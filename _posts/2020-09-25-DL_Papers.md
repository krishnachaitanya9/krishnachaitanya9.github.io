---
title: Latest Deep Learning Papers
date: 2020-10-26 21:00:01 +0800
featured-img: DL-logo
categories: [Deep_Learning]
tags: [Deep_Learning]
mathjax: Yes
---

<h1>Your arXiv Feed</h1>
<h2>A discrete complex Ginzburg-Landau equation for a hydrodynamic active lattice. (arXiv:2010.12655v1 [nlin.PS])</h2>
<h3>Stuart J. Thomson, Matthew Durey, Rodolfo R. Rosales</h3>
<p>A discrete and periodic complex Ginzburg-Landau equation, coupled to a
discrete mean equation, is systematically derived from a driven and dissipative
oscillator model, close to the onset of a supercritical Hopf bifurcation. The
oscillator model is inspired by recent experiments exploring active vibrations
of quasi-one-dimensional lattices of self-propelled millimetric droplets
bouncing on a vertically vibrating fluid bath. Our systematic derivation
provides a direct link between the constitutive properties of the lattice
system and the coefficients of the resultant amplitude equations, paving the
way to compare the emergent nonlinear dynamics---namely discrete bright and
dark solitons, breathers, and traveling waves---against experiments. Further,
the amplitude equations allow us to rationalize the successive bifurcations
leading to these distinct dynamical states. The framework presented herein is
expected to be applicable to a wider class of oscillators characterized by the
presence of a dynamic coupling potential between particles. More broadly, our
results point to deeper connections between nonlinear oscillators and the
physics of active and driven matter.
</p>
<a href="http://arxiv.org/abs/2010.12655" target="_blank">arXiv:2010.12655</a> [<a href="http://arxiv.org/pdf/2010.12655" target="_blank">pdf</a>]

<h2>Jensen-Shannon Information Based Characterization of the Generalization Error of Learning Algorithms. (arXiv:2010.12664v1 [cs.IT])</h2>
<h3>Gholamali Aminian, Laura Toni, Miguel R. D. Rodrigues</h3>
<p>Generalization error bounds are critical to understanding the performance of
machine learning models. In this work, we propose a new information-theoretic
based generalization error upper bound applicable to supervised learning
scenarios. We show that our general bound can specialize in various previous
bounds. We also show that our general bound can be specialized under some
conditions to a new bound involving the Jensen-Shannon information between a
random variable modelling the set of training samples and another random
variable modelling the set of hypotheses. We also prove that our bound can be
tighter than mutual information-based bounds under some conditions.
</p>
<a href="http://arxiv.org/abs/2010.12664" target="_blank">arXiv:2010.12664</a> [<a href="http://arxiv.org/pdf/2010.12664" target="_blank">pdf</a>]

<h2>An Inexact Accelerated Stochastic ADMM for Separable Convex Optimization. (arXiv:2010.12765v1 [math.OC])</h2>
<h3>Jianchao Bai, William W. Hager, Hongchao Zhang</h3>
<p>An inexact accelerated stochastic Alternating Direction Method of Multipliers
(AS-ADMM) scheme is developed for solving structured separable convex
optimization problems with linear constraints. The objective function is the
sum of a possibly nonsmooth convex function and a smooth function which is an
average of many component convex functions. Problems having this structure
often arise in machine learning and data mining applications. AS-ADMM combines
the ideas of both ADMM and the stochastic gradient methods using variance
reduction techniques. One of the ADMM subproblems employs a linearization
technique while a similar linearization could be introduced for the other
subproblem. For a specified choice of the algorithm parameters, it is shown
that the objective error and the constraint violation are $\mathcal{O}(1/k)$
relative to the number of outer iterations $k$. Under a strong convexity
assumption, the expected iterate error converges to zero linearly. A linearized
variant of AS-ADMM and incremental sampling strategies are also discussed.
Numerical experiments with both stochastic and deterministic ADMM algorithms
show that AS-ADMM can be particularly effective for structured optimization
arising in big data applications.
</p>
<a href="http://arxiv.org/abs/2010.12765" target="_blank">arXiv:2010.12765</a> [<a href="http://arxiv.org/pdf/2010.12765" target="_blank">pdf</a>]

<h2>An Adiabatic Theorem for Policy Tracking with TD-learning. (arXiv:2010.12848v1 [cs.LG])</h2>
<h3>Neil Walton</h3>
<p>We evaluate the ability of temporal difference learning to track the reward
function of a policy as it changes over time. Our results apply a new adiabatic
theorem that bounds the mixing time of time-inhomogeneous Markov chains. We
derive finite-time bounds for tabular temporal difference learning and
$Q$-learning when the policy used for training changes in time. To achieve
this, we develop bounds for stochastic approximation under asynchronous
adiabatic updates.
</p>
<a href="http://arxiv.org/abs/2010.12848" target="_blank">arXiv:2010.12848</a> [<a href="http://arxiv.org/pdf/2010.12848" target="_blank">pdf</a>]

<h2>Fast Epigraphical Projection-based Incremental Algorithms for Wasserstein Distributionally Robust Support Vector Machine. (arXiv:2010.12865v1 [math.OC])</h2>
<h3>Jiajin Li, Caihua Chen, Anthony Man-Cho So</h3>
<p>Wasserstein \textbf{D}istributionally \textbf{R}obust \textbf{O}ptimization
(DRO) is concerned with finding decisions that perform well on data that are
drawn from the worst-case probability distribution within a Wasserstein ball
centered at a certain nominal distribution. In recent years, it has been shown
that various DRO formulations of learning models admit tractable convex
reformulations. However, most existing works propose to solve these convex
reformulations by general-purpose solvers, which are not well-suited for
tackling large-scale problems. In this paper, we focus on a family of
Wasserstein distributionally robust support vector machine (DRSVM) problems and
propose two novel epigraphical projection-based incremental algorithms to solve
them. The updates in each iteration of these algorithms can be computed in a
highly efficient manner. Moreover, we show that the DRSVM problems considered
in this paper satisfy a H\"olderian growth condition with explicitly determined
growth exponents. Consequently, we are able to establish the convergence rates
of the proposed incremental algorithms. Our numerical results indicate that the
proposed methods are orders of magnitude faster than the state-of-the-art, and
the performance gap grows considerably as the problem size increases.
</p>
<a href="http://arxiv.org/abs/2010.12865" target="_blank">arXiv:2010.12865</a> [<a href="http://arxiv.org/pdf/2010.12865" target="_blank">pdf</a>]

<h2>Nearly Optimal Variational Inference for High Dimensional Regression with Shrinkage Priors. (arXiv:2010.12887v1 [stat.ML])</h2>
<h3>Jincheng Bai, Qifan Song, Guang Cheng</h3>
<p>We propose a variational Bayesian (VB) procedure for high-dimensional linear
model inferences with heavy tail shrinkage priors, such as student-t prior.
Theoretically, we establish the consistency of the proposed VB method and prove
that under the proper choice of prior specifications, the contraction rate of
the VB posterior is nearly optimal. It justifies the validity of VB inference
as an alternative of Markov Chain Monte Carlo (MCMC) sampling. Meanwhile,
comparing to conventional MCMC methods, the VB procedure achieves much higher
computational efficiency, which greatly alleviates the computing burden for
modern machine learning applications such as massive data analysis. Through
numerical studies, we demonstrate that the proposed VB method leads to shorter
computing time, higher estimation accuracy, and lower variable selection error
than competitive sparse Bayesian methods.
</p>
<a href="http://arxiv.org/abs/2010.12887" target="_blank">arXiv:2010.12887</a> [<a href="http://arxiv.org/pdf/2010.12887" target="_blank">pdf</a>]

<h2>Modeling and Optimization Trade-off in Meta-learning. (arXiv:2010.12916v1 [cs.LG])</h2>
<h3>Katelyn Gao, Ozan Sener</h3>
<p>By searching for shared inductive biases across tasks, meta-learning promises
to accelerate learning on novel tasks, but with the cost of solving a complex
bilevel optimization problem. We introduce and rigorously define the trade-off
between accurate modeling and optimization ease in meta-learning. At one end,
classic meta-learning algorithms account for the structure of meta-learning but
solve a complex optimization problem, while at the other end domain randomized
search (otherwise known as joint training) ignores the structure of
meta-learning and solves a single level optimization problem. Taking MAML as
the representative meta-learning algorithm, we theoretically characterize the
trade-off for general non-convex risk functions as well as linear regression,
for which we are able to provide explicit bounds on the errors associated with
modeling and optimization. We also empirically study this trade-off for
meta-reinforcement learning benchmarks.
</p>
<a href="http://arxiv.org/abs/2010.12916" target="_blank">arXiv:2010.12916</a> [<a href="http://arxiv.org/pdf/2010.12916" target="_blank">pdf</a>]

<h2>Power Allocation for Relayed OFDM with Index Modulation Assisted by Artificial Neural Network. (arXiv:2010.12959v1 [cs.IT])</h2>
<h3>Jiusi Zhou, Shuping Dang, Basem Shihada, Mohamed-Slim Alouini</h3>
<p>In this letter, we propose a power allocation scheme for relayed orthogonal
frequency division multiplexing with index modulation (OFDM-IM) systems. The
proposed power allocation scheme replies on artificial neural network (ANN) and
deep learning to allocate transmit power among various subcarriers at the
source and relay nodes. The objective of the power allocation scheme is to
minimize the overall transmit power under a set of constraints. Without loss of
generality, we assume all subcarriers at source and relay nodes are
independently distributed with different statistical distribution parameters.
The relay node adopts the fixed-gain amplify-and-forward (FG AF) relaying
protocol. We employ the adaptive moment estimation method (Adam) to implement
back-propagation learning and simulate the proposed power allocation scheme.
The analytical and simulation results show that the proposed power allocation
scheme is able to provide comparable performance as the optimal solution but
with lower complexity.
</p>
<a href="http://arxiv.org/abs/2010.12959" target="_blank">arXiv:2010.12959</a> [<a href="http://arxiv.org/pdf/2010.12959" target="_blank">pdf</a>]

<h2>Deep neural network for solving differential equations motivated by Legendre-Galerkin approximation. (arXiv:2010.12975v1 [math.NA])</h2>
<h3>Bryce Chudomelka, Youngjoon Hong, Hyunwoo Kim, Jinyoung Park</h3>
<p>Nonlinear differential equations are challenging to solve numerically and are
important to understanding the dynamics of many physical systems. Deep neural
networks have been applied to help alleviate the computational cost that is
associated with solving these systems. We explore the performance and accuracy
of various neural architectures on both linear and nonlinear differential
equations by creating accurate training sets with the spectral element method.
Next, we implement a novel Legendre-Galerkin Deep Neural Network (LGNet)
algorithm to predict solutions to differential equations. By constructing a set
of a linear combination of the Legendre basis, we predict the corresponding
coefficients, $\alpha_i$ which successfully approximate the solution as a sum
of smooth basis functions $u \simeq \sum_{i=0}^{N} \alpha_i \varphi_i$. As a
computational example, linear and nonlinear models with Dirichlet or Neumann
boundary conditions are considered.
</p>
<a href="http://arxiv.org/abs/2010.12975" target="_blank">arXiv:2010.12975</a> [<a href="http://arxiv.org/pdf/2010.12975" target="_blank">pdf</a>]

<h2>Local Averaging Helps: Hierarchical Federated Learning and Convergence Analysis. (arXiv:2010.12998v1 [cs.LG])</h2>
<h3>Jiayi Wang, Shiqiang Wang, Rong-Rong Chen, Mingyue Ji</h3>
<p>Federated learning is an effective approach to realize collaborative learning
among edge devices without exchanging raw data. In practice, these devices may
connect to local hubs which are then connected to the global server
(aggregator). Due to the (possibly limited) computation capability of these
local hubs, it is reasonable to assume that they can perform simple averaging
operations. A natural question is whether such local averaging is beneficial
under different system parameters and how much gain can be obtained compared to
the case without such averaging. In this paper, we study hierarchical federated
learning with stochastic gradient descent (HF-SGD) and conduct a thorough
theoretical analysis to analyze its convergence behavior. The analysis
demonstrates the impact of local averaging precisely as a function of system
parameters. Due to the higher communication cost of global averaging, a
strategy of decreasing the global averaging frequency and increasing the local
averaging frequency is proposed. Experiments validate the proposed theoretical
analysis and the advantages of hierarchical federated learning.
</p>
<a href="http://arxiv.org/abs/2010.12998" target="_blank">arXiv:2010.12998</a> [<a href="http://arxiv.org/pdf/2010.12998" target="_blank">pdf</a>]

<h2>Performance Analysis of Coded OTFS Systems over High-Mobility Channels. (arXiv:2010.13008v1 [cs.IT])</h2>
<h3>Shuangyang Li, Jinhong Yuan, Weijie Yuan, Zhiqiang Wei, Baoming Bai, Derrick Wing Kwan Ng</h3>
<p>Orthogonal time frequency space (OTFS) modulation is a recently developed
multi-carrier multi-slot transmission scheme for wireless communications in
high-mobility environments. In this paper, the error performance of coded OTFS
modulation over high-mobility channels is investigated. We start from the study
of conditional pairwise-error probability (PEP) of the OTFS scheme, based on
which its performance upper bound of the coded OTFS system is derived. Then, we
show that the coding improvement for OTFS systems depends on the squared
Euclidean distance among codeword pairs and the number of independent
resolvable paths of the channel. More importantly, we show that there exists a
fundamental trade-off between the coding gain and the diversity gain for OTFS
systems, i.e., the diversity gain of OTFS systems improves with the number of
resolvable paths, while the coding gain declines. Furthermore, based on our
analysis, the impact of channel coding parameters on the performance of the
coded OTFS systems is unveiled. The error performance of various coded OTFS
systems over high-mobility channels is then evaluated. Simulation results
demonstrate a significant performance improvement for OTFS modulation over the
conventional orthogonal frequency division multiplexing (OFDM) modulation over
high-mobility channels. Analytical results and the effectiveness of the
proposed code design are also verified by simulations with the application of
both classical and modern codes for OTFS systems.
</p>
<a href="http://arxiv.org/abs/2010.13008" target="_blank">arXiv:2010.13008</a> [<a href="http://arxiv.org/pdf/2010.13008" target="_blank">pdf</a>]

<h2>Local SGD for Saddle-Point Problems. (arXiv:2010.13112v1 [cs.LG])</h2>
<h3>Aleksandr Beznosikov, Valentin Samokhin, Alexander Gasnikov</h3>
<p>GAN is one of the most popular and commonly used neural network models. When
the model is large and there is a lot of data, the learning process can be
delayed. The standard way out is to use multiple devices. Therefore, the
methods of distributed and federated training for GANs are an important
question. But from an optimization point of view, GANs are nothing more than a
classical saddle-point problem: $\min_x \max_y f(x,y)$. Therefore, this paper
focuses on the distributed optimization of the smooth stochastic saddle-point
problems using Local SGD. We present a new algorithm specifically for our
problem -- Extra Step Local SGD. The obtained theoretical bounds of
communication rounds are $\Omega(K^{2/3} M^{1/3})$ in
strongly-convex-strongly-concave case and $\Omega(K^{8/9} M^{4/9})$ in
convex-concave (here $M$ -- number of functions (nodes) and $K$ - number of
iterations).
</p>
<a href="http://arxiv.org/abs/2010.13112" target="_blank">arXiv:2010.13112</a> [<a href="http://arxiv.org/pdf/2010.13112" target="_blank">pdf</a>]

<h2>Weighted-CEL0 sparse regularisation for molecule localisation in super-resolution microscopy with Poisson data. (arXiv:2010.13173v1 [eess.IV])</h2>
<h3>Marta Lazzaretti, Luca Calatroni, Claudio Estatico</h3>
<p>We propose a continuous non-convex variational model for Single Molecule
Localisation Microscopy (SMLM) super-resolution in order to overcome light
diffraction barriers. Namely, we consider a variation of the Continuous Exact
$\ell_0$ (CEL0) penalty recently introduced to relax the $\ell_2-\ell_0$
problem where a weighted-$\ell_2$ data fidelity is considered to model
signal-dependent Poisson noise. For the numerical solution of the associated
minimisation problem, we consider an iterative reweighted $\ell_1$ (IRL1)
strategy for which we detail efficient parameter computation strategies. We
report qualitative and quantitative molecule localisation results showing that
the proposed weighted-CEL0 (wCEL0) model improves the results obtained by CEL0
and state-of-the art deep-learning approaches for the high-density SMLM ISBI
2013 dataset.
</p>
<a href="http://arxiv.org/abs/2010.13173" target="_blank">arXiv:2010.13173</a> [<a href="http://arxiv.org/pdf/2010.13173" target="_blank">pdf</a>]

<h2>Variance-Reduced Off-Policy TDC Learning: Non-Asymptotic Convergence Analysis. (arXiv:2010.13272v1 [cs.LG])</h2>
<h3>Shaocong Ma, Yi Zhou, Shaofeng Zhou</h3>
<p>Variance reduction techniques have been successfully applied to
temporal-difference (TD) learning and help to improve the sample complexity in
policy evaluation. However, the existing work applied variance reduction to
either the less popular one time-scale TD algorithm or the two time-scale GTD
algorithm but with a finite number of i.i.d.\ samples, and both algorithms
apply to only the on-policy setting. In this work, we develop a variance
reduction scheme for the two time-scale TDC algorithm in the off-policy setting
and analyze its non-asymptotic convergence rate over both i.i.d.\ and Markovian
samples. In the i.i.d.\ setting, our algorithm achieves a sample complexity
$O(\epsilon^{-\frac{3}{5}} \log{\epsilon}^{-1})$ that is lower than the
state-of-the-art result $O(\epsilon^{-1} \log {\epsilon}^{-1})$. In the
Markovian setting, our algorithm achieves the state-of-the-art sample
complexity $O(\epsilon^{-1} \log {\epsilon}^{-1})$ that is near-optimal.
Experiments demonstrate that the proposed variance-reduced TDC achieves a
smaller asymptotic convergence error than both the conventional TDC and the
variance-reduced TD.
</p>
<a href="http://arxiv.org/abs/2010.13272" target="_blank">arXiv:2010.13272</a> [<a href="http://arxiv.org/pdf/2010.13272" target="_blank">pdf</a>]

<h2>Asymptotic Behavior of Adversarial Training in Binary Classification. (arXiv:2010.13275v1 [stat.ML])</h2>
<h3>Hossein Taheri, Ramtin Pedarsani, Christos Thrampoulidis</h3>
<p>It is widely known that several machine learning models are susceptible to
adversarial attacks i.e., small adversarial perturbations applied to data
points causing the model to misclassify the data. Adversarial training using
empirical risk minimization methods, is the state-of-the-art method for defense
against adversarial attacks. Despite being successful, several problems in
understanding generalization performance of adversarial training remain open.
In this paper, we derive precise theoretical predictions for the performance of
adversarial training in binary linear classification. We consider the modern
high-dimensional regime where the dimension of data grows with the size of the
training dataset at a constant ratio. Our results provide exact asymptotics for
the performance of estimators obtained by adversarial training with
$\ell_q$-norm bounded perturbations ($q \ge 1$) and for binary labels and
Gaussian features. These sharp predictions enable us to explore the role of
various factors including over-parametrization ratio, data model and attack
budget on the performance of adversarial training.
</p>
<a href="http://arxiv.org/abs/2010.13275" target="_blank">arXiv:2010.13275</a> [<a href="http://arxiv.org/pdf/2010.13275" target="_blank">pdf</a>]

<h2>Iterative Ensemble Kalman Methods: A Unified Perspective with Some New Variants. (arXiv:2010.13299v1 [math.NA])</h2>
<h3>Neil K. Chada, Yuming Chen, Daniel Sanz-Alonso</h3>
<p>This paper provides a unified perspective of iterative ensemble Kalman
methods, a family of derivative-free algorithms for parameter reconstruction
and other related tasks. We identify, compare and develop three subfamilies of
ensemble methods that differ in the objective they seek to minimize and the
derivative-based optimization scheme they approximate through the ensemble. Our
work emphasizes two principles for the derivation and analysis of iterative
ensemble Kalman methods: statistical linearization and continuum limits.
Following these guiding principles, we introduce new iterative ensemble Kalman
methods that show promising numerical performance in Bayesian inverse problems,
data assimilation and machine learning tasks.
</p>
<a href="http://arxiv.org/abs/2010.13299" target="_blank">arXiv:2010.13299</a> [<a href="http://arxiv.org/pdf/2010.13299" target="_blank">pdf</a>]

<h2>Federated Learning in Multi-RIS Aided Systems. (arXiv:2010.13333v1 [cs.IT])</h2>
<h3>Wanli Ni, Yuanwei Liu, Zhaohui Yang, Hui Tian, Xuemin Shen</h3>
<p>This paper investigates the problem of model aggregation in the federated
learning system aided by multiple reconfigurable intelligent surfaces. The
effective combination of computation and communication is achieved by
over-the-air computation in federated learning. Since all local parameters are
transmitted over shared wireless channels, the undesirable propagation error
will inevitably deteriorate the learning performance. The objective of this
work is to reduce the signal distortion and improve the convergence rate of
federated learning. Thus, the mean-square-error and weighted cardinality are
minimized by optimizing the transmit power, controlling the receive scalar,
designing the phase shifts, and selecting devices in the model uploading
process. To address this challenging issue, the original mixed-integer
bi-criterion problem (P0) is decomposed into a non-convex problem (P1) with
continuous variables and a combinatorial problem (P2) with integer variables.
In an effort to tackle P1, the closed-form expressions for transceivers are
first derived, and the multi-antenna cases are addressed by the semidefinite
relaxation, then the problem of phase shifts design is tackled by invoking the
penalty method and successive convex approximation. In terms of P2, the
difference-of-convex programming is adopted to select devices judiciously for
convergence accelerating, while satisfying the aggregation error demand. After
that, an alternating optimization algorithm is proposed to find a suboptimal
solution for P0, where the corresponding convergence and complexity are
analyzed. Finally, simulation results demonstrate that the designed algorithm
can converge faster and aggregate model more accurately.
</p>
<a href="http://arxiv.org/abs/2010.13333" target="_blank">arXiv:2010.13333</a> [<a href="http://arxiv.org/pdf/2010.13333" target="_blank">pdf</a>]

<h2>Convergence Acceleration via Chebyshev Step: Plausible Interpretation of Deep-Unfolded Gradient Descent. (arXiv:2010.13335v1 [cs.LG])</h2>
<h3>Satoshi Takabe, Tadashi Wadayama</h3>
<p>Deep unfolding is a promising deep-learning technique, whose network
architecture is based on expanding the recursive structure of existing
iterative algorithms. Although convergence acceleration is a remarkable
advantage of deep unfolding, its theoretical aspects have not been revealed
yet. The first half of this study details the theoretical analysis of the
convergence acceleration in deep-unfolded gradient descent (DUGD) whose
trainable parameters are step sizes. We propose a plausible interpretation of
the learned step-size parameters in DUGD by introducing the principle of
Chebyshev steps derived from Chebyshev polynomials. The use of Chebyshev steps
in gradient descent (GD) enables us to bound the spectral radius of a matrix
governing the convergence speed of GD, leading to a tight upper bound on the
convergence rate. The convergence rate of GD using Chebyshev steps is shown to
be asymptotically optimal, although it has no momentum terms. We also show that
Chebyshev steps numerically explain the learned step-size parameters in DUGD
well. In the second half of the study, %we apply the theory of Chebyshev steps
and Chebyshev-periodical successive over-relaxation (Chebyshev-PSOR) is
proposed for accelerating linear/nonlinear fixed-point iterations. Theoretical
analysis and numerical experiments indicate that Chebyshev-PSOR exhibits
significantly faster convergence for various examples such as Jacobi method and
proximal gradient methods.
</p>
<a href="http://arxiv.org/abs/2010.13335" target="_blank">arXiv:2010.13335</a> [<a href="http://arxiv.org/pdf/2010.13335" target="_blank">pdf</a>]

<h2>The estimation of training accuracy for two-layer neural networks on random datasets without training. (arXiv:2010.13380v1 [cs.LG])</h2>
<h3>Shuyue Guan, Murray Loew</h3>
<p>Although the neural network (NN) technique plays an important role in machine
learning, understanding the mechanism of NN models and the transparency of deep
learning still require more basic research. In this study we propose a novel
theory based on space partitioning to estimate the approximate training
accuracy for two-layer neural networks on random datasets without training.
There appear to be no other studies that have proposed a method to estimate
training accuracy without using input data or trained models. Our method
estimates the training accuracy for two-layer fully-connected neural networks
on two-class random datasets using only three arguments: the dimensionality of
inputs (d), the number of inputs (N), and the number of neurons in the hidden
layer (L). We have verified our method using real training accuracies in our
experiments. The results indicate that the method will work for any dimension,
and the proposed theory could extend also to estimate deeper NN models. This
study may provide a starting point for a new way for researchers to make
progress on the difficult problem of understanding deep learning.
</p>
<a href="http://arxiv.org/abs/2010.13380" target="_blank">arXiv:2010.13380</a> [<a href="http://arxiv.org/pdf/2010.13380" target="_blank">pdf</a>]

<h2>Robust Optimization Approaches for Portfolio Selection: A Computational and Comparative Analysis. (arXiv:2010.13397v1 [q-fin.PM])</h2>
<h3>A. Georgantas</h3>
<p>The field of portfolio selection is an active research topic, which combines
elements and methodologies from various fields, such as optimization, decision
analysis, risk management, data science, forecasting, etc. The modeling and
treatment of deep uncertainties for future asset returns is a major issue for
the success of analytical portfolio selection models. Recently, robust
optimization (RO) models have attracted a lot of interest in this area. RO
provides a computationally tractable framework for portfolio optimization based
on relatively general assumptions on the probability distributions of the
uncertain risk parameters. Thus, RO extends the framework of traditional linear
and non-linear models (e.g., the well-known mean-variance model), incorporating
uncertainty through a formal and analytical approach into the modeling process.
Robust counterparts of existing models can be considered as worst-case
re-formulations as far as deviations of the uncertain parameters from their
nominal values are concerned. Although several RO models have been proposed in
the literature focusing on various risk measures and different types of
uncertainty sets about asset returns, analytical empirical assessments of their
performance have not been performed in a comprehensive manner. The objective of
this study is to fill in this gap in the literature. More specifically, we
consider different types of RO models based on popular risk measures and
conduct an extensive comparative analysis of their performance using data from
the US market during the period 2005-2016.
</p>
<a href="http://arxiv.org/abs/2010.13397" target="_blank">arXiv:2010.13397</a> [<a href="http://arxiv.org/pdf/2010.13397" target="_blank">pdf</a>]

<h2>Application of Deep Learning to Sphere Decoding for Large MIMO Systems. (arXiv:2010.13481v1 [cs.IT])</h2>
<h3>Nhan Thanh Nguyen, Kyungchun Lee, Huaiyu Dai</h3>
<p>Although the sphere decoder (SD) is a powerful detector for multiple-input
multiple-output (MIMO) systems, it has become computationally prohibitive in
massive MIMO systems, where a large number of antennas are employed. To
overcome this challenge, we propose fast deep learning (DL)-aided SD (FDL-SD)
and fast DL-aided $K$-best SD (KSD, FDL-KSD) algorithms. Therein, the major
application of DL is to generate a highly reliable initial candidate to
accelerate the search in SD and KSD in conjunction with candidate/layer
ordering and early rejection. Compared to existing DL-aided SD schemes, our
proposed schemes are more advantageous in both offline training and online
application phases. Specifically, unlike existing DL-aided SD schemes, they do
not require performing the conventional SD in the training phase. For a $24
\times 24$ MIMO system with QPSK, the proposed FDL-SD achieves a complexity
reduction of more than $90\%$ without any performance loss compared to
conventional SD schemes. For a $32 \times 32$ MIMO system with QPSK, the
proposed FDL-KSD only requires $K = 32$ to attain the performance of the
conventional KSD with $K=256$, where $K$ is the number of survival paths in
KSD. This implies a dramatic improvement in the performance--complexity
tradeoff of the proposed FDL-KSD scheme.
</p>
<a href="http://arxiv.org/abs/2010.13481" target="_blank">arXiv:2010.13481</a> [<a href="http://arxiv.org/pdf/2010.13481" target="_blank">pdf</a>]

<h2>Query Complexity of k-NN based Mode Estimation. (arXiv:2010.13491v1 [stat.ML])</h2>
<h3>Anirudh Singhal, Subham Pirojiwala, Nikhil Karamchandani</h3>
<p>Motivated by the mode estimation problem of an unknown multivariate
probability density function, we study the problem of identifying the point
with the minimum k-th nearest neighbor distance for a given dataset of n
points. We study the case where the pairwise distances are apriori unknown, but
we have access to an oracle which we can query to get noisy information about
the distance between any pair of points. For two natural oracle models, we
design a sequential learning algorithm, based on the idea of confidence
intervals, which adaptively decides which queries to send to the oracle and is
able to correctly solve the problem with high probability. We derive
instance-dependent upper bounds on the query complexity of our proposed scheme
and also demonstrate significant improvement over the performance of other
baselines via extensive numerical evaluations.
</p>
<a href="http://arxiv.org/abs/2010.13491" target="_blank">arXiv:2010.13491</a> [<a href="http://arxiv.org/pdf/2010.13491" target="_blank">pdf</a>]

<h2>Simplifying Hamiltonian and Lagrangian Neural Networks via Explicit Constraints. (arXiv:2010.13581v1 [cs.LG])</h2>
<h3>Marc Finzi, Ke Alexander Wang, Andrew Gordon Wilson</h3>
<p>Reasoning about the physical world requires models that are endowed with the
right inductive biases to learn the underlying dynamics. Recent works improve
generalization for predicting trajectories by learning the Hamiltonian or
Lagrangian of a system rather than the differential equations directly. While
these methods encode the constraints of the systems using generalized
coordinates, we show that embedding the system into Cartesian coordinates and
enforcing the constraints explicitly with Lagrange multipliers dramatically
simplifies the learning problem. We introduce a series of challenging chaotic
and extended-body systems, including systems with N-pendulums, spring coupling,
magnetic fields, rigid rotors, and gyroscopes, to push the limits of current
approaches. Our experiments show that Cartesian coordinates with explicit
constraints lead to a 100x improvement in accuracy and data efficiency.
</p>
<a href="http://arxiv.org/abs/2010.13581" target="_blank">arXiv:2010.13581</a> [<a href="http://arxiv.org/pdf/2010.13581" target="_blank">pdf</a>]

<h2>Tight last-iterate convergence rates for no-regret learning in multi-player games. (arXiv:2010.13724v1 [cs.LG])</h2>
<h3>Noah Golowich, Sarath Pattathil, Constantinos Daskalakis</h3>
<p>We study the question of obtaining last-iterate convergence rates for
no-regret learning algorithms in multi-player games. We show that the
optimistic gradient (OG) algorithm with a constant step-size, which is
no-regret, achieves a last-iterate rate of $O(1/\sqrt{T})$ with respect to the
gap function in smooth monotone games. This result addresses a question of
Mertikopoulos &amp; Zhou (2018), who asked whether extra-gradient approaches (such
as OG) can be applied to achieve improved guarantees in the multi-agent
learning setting. The proof of our upper bound uses a new technique centered
around an adaptive choice of potential function at each iteration. We also show
that the $O(1/\sqrt{T})$ rate is tight for all $p$-SCLI algorithms, which
includes OG as a special case. As a byproduct of our lower bound analysis we
additionally present a proof of a conjecture of Arjevani et al. (2015) which is
more direct than previous approaches.
</p>
<a href="http://arxiv.org/abs/2010.13724" target="_blank">arXiv:2010.13724</a> [<a href="http://arxiv.org/pdf/2010.13724" target="_blank">pdf</a>]

<h2>Diff-DAC: Distributed Actor-Critic for Average Multitask Deep Reinforcement Learning. (arXiv:1710.10363v6 [cs.LG] UPDATED)</h2>
<h3>Sergio Valcarcel Macua, Aleksi Tukiainen, Daniel Garc&#xed;a-Oca&#xf1;a Hern&#xe1;ndez, David Baldazo, Enrique Munoz de Cote, Santiago Zazo</h3>
<p>We propose a fully distributed actor-critic algorithm approximated by deep
neural networks, named \textit{Diff-DAC}, with application to single-task and
to average multitask reinforcement learning (MRL). Each agent has access to
data from its local task only, but it aims to learn a policy that performs well
on average for the whole set of tasks. During the learning process, agents
communicate their value-policy parameters to their neighbors, diffusing the
information across the network, so that they converge to a common policy, with
no need for a central node. The method is scalable, since the computational and
communication costs per agent grow with its number of neighbors. We derive
Diff-DAC's from duality theory and provide novel insights into the standard
actor-critic framework, showing that it is actually an instance of the dual
ascent method that approximates the solution of a linear program. Experiments
suggest that Diff-DAC can outperform the single previous distributed MRL
approach (i.e., Dist-MTLPS) and even the centralized architecture.
</p>
<a href="http://arxiv.org/abs/1710.10363" target="_blank">arXiv:1710.10363</a> [<a href="http://arxiv.org/pdf/1710.10363" target="_blank">pdf</a>]

<h2>Energy Storage Arbitrage in Real-Time Markets via Reinforcement Learning. (arXiv:1711.03127v3 [cs.SY] UPDATED)</h2>
<h3>Hao Wang, Baosen Zhang</h3>
<p>In this paper, we derive a temporal arbitrage policy for storage via
reinforcement learning. Real-time price arbitrage is an important source of
revenue for storage units, but designing good strategies have proven to be
difficult because of the highly uncertain nature of the prices. Instead of
current model predictive or dynamic programming approaches, we use
reinforcement learning to design an optimal arbitrage policy. This policy is
learned through repeated charge and discharge actions performed by the storage
unit through updating a value matrix. We design a reward function that does not
only reflect the instant profit of charge/discharge decisions but also
incorporate the history information. Simulation results demonstrate that our
designed reward function leads to significant performance improvement compared
with existing algorithms.
</p>
<a href="http://arxiv.org/abs/1711.03127" target="_blank">arXiv:1711.03127</a> [<a href="http://arxiv.org/pdf/1711.03127" target="_blank">pdf</a>]

<h2>Intrinsic Dimension of Geometric Data Sets. (arXiv:1801.07985v3 [cs.AI] UPDATED)</h2>
<h3>Tom Hanika, Friedrich Martin Schneider, Gerd Stumme</h3>
<p>The curse of dimensionality is a phenomenon frequently observed in machine
learning (ML) and knowledge discovery (KD). There is a large body of literature
investigating its origin and impact, using methods from mathematics as well as
from computer science. Among the mathematical insights into data
dimensionality, there is an intimate link between the dimension curse and the
phenomenon of measure concentration, which makes the former accessible to
methods of geometric analysis. The present work provides a comprehensive study
of the intrinsic geometry of a data set, based on Gromov's metric measure
geometry and Pestov's axiomatic approach to intrinsic dimension. In detail, we
define a concept of geometric data set and introduce a metric as well as a
partial order on the set of isomorphism classes of such data sets. Based on
these objects, we propose and investigate an axiomatic approach to the
intrinsic dimension of geometric data sets and establish a concrete dimension
function with the desired properties. Our model for data sets and their
intrinsic dimension is computationally feasible and, moreover, adaptable to
specific ML/KD-algorithms, as illustrated by various experiments.
</p>
<a href="http://arxiv.org/abs/1801.07985" target="_blank">arXiv:1801.07985</a> [<a href="http://arxiv.org/pdf/1801.07985" target="_blank">pdf</a>]

<h2>Deep Network Approximation Characterized by Number of Neurons. (arXiv:1906.05497v3 [math.NA] UPDATED)</h2>
<h3>Zuowei Shen, Haizhao Yang, Shijun Zhang</h3>
<p>This paper quantitatively characterizes the approximation power of deep
feed-forward neural networks (FNNs) in terms of the number of neurons, i.e.,
the product of the network width and depth. It is shown by construction that
ReLU FNNs with width $\mywidth$ and depth $9L+12$ can approximate an arbitrary
H\"older continuous function of order $\alpha$ with a Lipschitz constant $\nu$
on $[0,1]^d$ with a tight approximation rate $5(8\sqrt{d})^\alpha\nu
N^{-2\alpha/d}L^{-2\alpha/d}$ for any given $N,L\in \N^+$. The constructive
approximation is a corollary of a more general result for an arbitrary
continuous function $f$ in terms of its modulus of continuity
$\omega_f(\cdot)$. In particular, the approximation rate of ReLU FNNs with
width $\mywidth$ and depth $9L+12$ for a general continuous function $f$ is
$5\omega_f(8\sqrt{d} N^{-2/d}L^{-2/d})$. We also extend our analysis to the
case when the domain of $f$ is irregular or localized in an
$\epsilon$-neighborhood of a $d_{\mathcal{M}}$-dimensional smooth manifold
$\mathcal{M}\subseteq [0,1]^d$ with $d_{\mathcal{M}}\ll d$. Especially, in the
case of an essentially low-dimensional domain, we show an approximation rate
$3\omega_f\big(\tfrac{4\epsilon}{1-\delta}\sqrt{\tfrac{d}{d_\delta}}\big)+5\omega_f\big(\tfrac{16d}{(1-\delta)\sqrt{d_\delta}}N^{-2/d_\delta}L^{-2/d_\delta
}\big)$ for ReLU FNNs to approximate $f$ in the $\epsilon$-neighborhood, where
$d_\delta=\OO\big(d_{\mathcal{M}}\tfrac{\ln (d/\delta)}{\delta^2}\big)$ for any
given $\delta\in(0,1)$. Our analysis provides a general guide for selecting the
width and the depth of ReLU FNNs to approximate continuous functions especially
in parallel computing.
</p>
<a href="http://arxiv.org/abs/1906.05497" target="_blank">arXiv:1906.05497</a> [<a href="http://arxiv.org/pdf/1906.05497" target="_blank">pdf</a>]

<h2>Aggregating Privacy-Conscious Distributed Energy Resources for Grid Service Provision. (arXiv:1909.01215v2 [eess.SY] UPDATED)</h2>
<h3>Jun-Xing Chin, Andrey Bernstein, Gabriela Hug</h3>
<p>The increasing adoption of advanced metering infrastructure has led to
growing concerns regarding privacy risks stemming from the high resolution
measurements. This has given rise to privacy protection techniques that
physically alter the consumer's energy load profile, masking private
information by using localised devices, e.g. batteries or flexible loads.
Meanwhile, there has also been increasing interest in aggregating the
distributed energy resources (DERs) of residential consumers to provide
services to the grid. In this paper, we propose an online distributed algorithm
to aggregate the DERs of privacy-conscious consumers to provide services to the
grid, whilst preserving their privacy. Results show that the optimisation
solution from the distributed method converges to one close to the optimum
computed using an ideal centralised solution method, balancing between grid
service provision, consumer preferences and privacy protection. More
importantly, the distributed method preserves consumer privacy, and does not
require high-bandwidth two-way communications infrastructure.
</p>
<a href="http://arxiv.org/abs/1909.01215" target="_blank">arXiv:1909.01215</a> [<a href="http://arxiv.org/pdf/1909.01215" target="_blank">pdf</a>]

<h2>Election Coding for Distributed Learning: Protecting SignSGD against Byzantine Attacks. (arXiv:1910.06093v4 [cs.IT] UPDATED)</h2>
<h3>Jy-yong Sohn, Dong-Jun Han, Beongjun Choi, Jaekyun Moon</h3>
<p>Recent advances in large-scale distributed learning algorithms have enabled
communication-efficient training via SignSGD. Unfortunately, a major issue
continues to plague distributed learning: namely, Byzantine failures may incur
serious degradation in learning accuracy. This paper proposes Election Coding,
a coding-theoretic framework to guarantee Byzantine-robustness for SignSGD with
Majority Vote, which uses minimum worker-master communication in both
directions. The suggested framework explores new information-theoretic limits
of finding the majority opinion when some workers could be malicious, and paves
the road to implement robust and efficient distributed learning algorithms.
Under this framework, we construct two types of explicit codes, random
Bernoulli codes and deterministic algebraic codes, that can tolerate Byzantine
attacks with a controlled amount of computational redundancy. For the Bernoulli
codes, we provide upper bounds on the error probability in estimating the
majority opinion, which give useful insights into code design for tolerating
Byzantine attacks. As for deterministic codes, we construct an explicit code
which perfectly tolerates Byzantines, and provide tight upper/lower bounds on
the minimum required computational redundancy. Finally, the Byzantine-tolerance
of the suggested coding schemes is confirmed by deep learning experiments on
Amazon EC2 using Python with MPI4py package.
</p>
<a href="http://arxiv.org/abs/1910.06093" target="_blank">arXiv:1910.06093</a> [<a href="http://arxiv.org/pdf/1910.06093" target="_blank">pdf</a>]

<h2>Fast classification rates without standard margin assumptions. (arXiv:1910.12756v2 [cs.LG] UPDATED)</h2>
<h3>Olivier Bousquet, Nikita Zhivotovskiy</h3>
<p>We consider the classical problem of learning rates for classes with finite
VC dimension. It is well known that fast learning rates up to
$O\left(\frac{d}{n}\right)$ are achievable by the empirical risk minimization
algorithm (ERM) if low noise or margin assumptions are satisfied. These usually
require the optimal Bayes classifier to be in the class, and it has been shown
that when this is not the case, the fast rates cannot be achieved even in the
noise free case. In this paper, we further investigate the question of the fast
rates under the misspecification, when the Bayes classifier is not in the class
(also called the agnostic setting).

First, we consider classification with a reject option, namely Chow's reject
option model, and show that by slightly lowering the impact of hard instances,
a learning rate of order $O\left(\frac{d}{n}\log \frac{n}{d}\right)$ is always
achievable in the agnostic setting by a specific learning algorithm. Similar
results were only known under special versions of margin assumptions. We also
show that the performance of the proposed algorithm is never worse than the
performance of ERM.

Based on those results, we derive the necessary and sufficient conditions for
classification (without a reject option) with fast rates in the agnostic
setting achievable by improper learners. This simultaneously extends the work
of Massart and N\'{e}d\'{e}lec (Ann. of Statistics, 2006), which studied this
question in the case where the Bayesian optimal rule belongs to the class, and
the work of Ben-David and Urner (COLT, 2014), which allows the misspecification
but is limited to the no noise setting. Our result also provides the first
general setup in statistical learning theory in which an improper learning
algorithm may significantly improve the learning rate for non-convex losses.
</p>
<a href="http://arxiv.org/abs/1910.12756" target="_blank">arXiv:1910.12756</a> [<a href="http://arxiv.org/pdf/1910.12756" target="_blank">pdf</a>]

<h2>Minimax Value Interval for Off-Policy Evaluation and Policy Optimization. (arXiv:2002.02081v5 [cs.LG] UPDATED)</h2>
<h3>Nan Jiang, Jiawei Huang</h3>
<p>We study minimax methods for off-policy evaluation (OPE) using value
functions and marginalized importance weights. Despite that they hold promises
of overcoming the exponential variance in traditional importance sampling,
several key problems remain:

(1) They require function approximation and are generally biased. For the
sake of trustworthy OPE, is there anyway to quantify the biases?

(2) They are split into two styles ("weight-learning" vs "value-learning").
Can we unify them?

In this paper we answer both questions positively. By slightly altering the
derivation of previous methods (one from each style; Uehara et al., 2020), we
unify them into a single value interval that comes with a special type of
double robustness: when either the value-function or the importance-weight
class is well specified, the interval is valid and its length quantifies the
misspecification of the other class. Our interval also provides a unified view
of and new insights to some recent methods, and we further explore the
implications of our results on exploration and exploitation in off-policy
policy optimization with insufficient data coverage.
</p>
<a href="http://arxiv.org/abs/2002.02081" target="_blank">arXiv:2002.02081</a> [<a href="http://arxiv.org/pdf/2002.02081" target="_blank">pdf</a>]

<h2>Provably Efficient Safe Exploration via Primal-Dual Policy Optimization. (arXiv:2003.00534v2 [cs.LG] UPDATED)</h2>
<h3>Dongsheng Ding, Xiaohan Wei, Zhuoran Yang, Zhaoran Wang, Mihailo R. Jovanovi&#x107;</h3>
<p>We study the Safe Reinforcement Learning (SRL) problem using the Constrained
Markov Decision Process (CMDP) formulation in which an agent aims to maximize
the expected total reward subject to a safety constraint on the expected total
value of a utility function. We focus on an episodic setting with the function
approximation where the Markov transition kernels have a linear structure but
do not impose any additional assumptions on the sampling model. Designing SRL
algorithms with provable computational and statistical efficiency is
particularly challenging under this setting because of the need to incorporate
both the safety constraint and the function approximation into the fundamental
exploitation/exploration tradeoff. To this end, we present an
\underline{O}ptimistic \underline{P}rimal-\underline{D}ual Proximal Policy
\underline{OP}timization (OPDOP) algorithm where the value function is
estimated by combining the least-squares policy evaluation and an additional
bonus term for safe exploration. We prove that the proposed algorithm achieves
an $\tilde{O}(d H^{2.5}\sqrt{T})$ regret and an $\tilde{O}(d H^{2.5}\sqrt{T})$
constraint violation, where $d$ is the dimension of the feature mapping, $H$ is
the horizon of each episode, and $T$ is the total number of steps. These bounds
hold when the reward/utility functions are fixed but the feedback after each
episode is bandit. Our bounds depend on the capacity of the state-action space
only through the dimension of the feature mapping and thus our results hold
even when the number of states goes to infinity. To the best of our knowledge,
we provide the first provably efficient online policy optimization algorithm
for CMDP with safe exploration in the function approximation setting.
</p>
<a href="http://arxiv.org/abs/2003.00534" target="_blank">arXiv:2003.00534</a> [<a href="http://arxiv.org/pdf/2003.00534" target="_blank">pdf</a>]

<h2>Machine learning based non-Newtonian fluid model with molecular fidelity. (arXiv:2003.03672v2 [physics.comp-ph] UPDATED)</h2>
<h3>Huan Lei, Lei Wu, Weinan E</h3>
<p>We introduce a machine-learning-based framework for constructing continuum
non-Newtonian fluid dynamics model directly from a micro-scale description.
Dumbbell polymer solutions are used as examples to demonstrate the essential
ideas. To faithfully retain molecular fidelity, we establish a micro-macro
correspondence via a set of encoders for the micro-scale polymer configurations
and their macro-scale counterparts, a set of nonlinear conformation tensors.
The dynamics of these conformation tensors can be derived from the micro-scale
model and the relevant terms can be parametrized using machine learning. The
final model named the deep non-Newtonian model (DeePN$^2$), takes the form of
conventional non-Newtonian fluid dynamics models, with a new form of the
objective tensor derivative. Both the formulation of the dynamic equation and
the neural network representation rigorously preserve the rotational
invariance, which ensures the admissibility of the constructed model. Numerical
results demonstrate the accuracy of DeePN$^2$, where models based on empirical
closures show limitations.
</p>
<a href="http://arxiv.org/abs/2003.03672" target="_blank">arXiv:2003.03672</a> [<a href="http://arxiv.org/pdf/2003.03672" target="_blank">pdf</a>]

<h2>Spectral methods for nonlinear functionals and functional differential equations. (arXiv:2003.14308v3 [math.NA] UPDATED)</h2>
<h3>Daniele Venturi, Alec Dektor</h3>
<p>We present a rigorous convergence analysis for cylindrical approximations of
nonlinear functionals, functional derivatives, and functional differential
equations (FDEs). The purpose of this analysis is twofold: first, we prove that
continuous nonlinear functionals, functional derivatives and FDEs can be
approximated uniformly on any compact subset of a real separable Hilbert space
by high-dimensional multivariate functions and high-dimensional partial
differential equations (PDEs), respectively. Second, we show that the
convergence rate of such functional approximations can be exponential,
depending on the regularity of the functional (in particular its Fr\'echet
differentiability), and its domain. We also provide necessary and sufficient
conditions for consistency, stability and convergence of functional
approximations to linear FDEs. These results open the possibility to utilize
numerical techniques for high-dimensional model representation such as deep
neural networks and numerical tensor methods to approximate nonlinear
functionals in terms of high-dimensional functions, and compute approximate
solutions to FDEs by solving high-dimensional PDEs. Numerical demonstrations
are presented and discussed for prototype nonlinear functionals in the space of
square-integrable functions and for an initial value problem involving a linear
FDE.
</p>
<a href="http://arxiv.org/abs/2003.14308" target="_blank">arXiv:2003.14308</a> [<a href="http://arxiv.org/pdf/2003.14308" target="_blank">pdf</a>]

<h2>Learning Optical Flow for Fast MRI Reconstruction. (arXiv:2004.10464v3 [math.NA] UPDATED)</h2>
<h3>T. Schmoderer, A.I Aviles-Rivero, V. Corona, N. Debroux, C-B. Sch&#xf6;nlieb</h3>
<p>Reconstructing high-quality magnetic resonance images (MRI) from undersampled
raw data is of great interest from both technical and clinical point of views.
To this date, however, it is still a mathematically and computationally
challenging problem due to its severe ill-posedness, resulting from the highly
undersampled data leading to significantly missing information. Whilst a number
of techniques have been presented to improve image reconstruction, they only
account for spatio-temporal regularisation, which shows its limitations in
several relevant scenarios including dynamic data. In this work, we propose a
new mathematical model for the reconstruction of high-quality medical MRI from
few measurements. Our proposed approach combines - \textit{in a multi-task and
hybrid model} - the traditional compressed sensing formulation for the
reconstruction of dynamic MRI with motion compensation by learning an optical
flow approximation. More precisely, we propose to encode the dynamics in the
form of an optical flow model that is sparsely represented over a learned
dictionary. This has the advantage that ground truth data is not required in
the training of the optical flow term. Furthermore, we present an efficient
optimisation scheme to tackle the non-convex problem based on an alternating
splitting method. We demonstrate the potentials of our approach through an
extensive set of numerical results using different datasets and acceleration
factors. Our combined approach reaches and outperforms several state of the art
techniques. Finally, we show the ability of our technique to transfer phantom
based knowledge to real datasets.
</p>
<a href="http://arxiv.org/abs/2004.10464" target="_blank">arXiv:2004.10464</a> [<a href="http://arxiv.org/pdf/2004.10464" target="_blank">pdf</a>]

<h2>Directional convergence and alignment in deep learning. (arXiv:2006.06657v2 [cs.LG] UPDATED)</h2>
<h3>Ziwei Ji, Matus Telgarsky</h3>
<p>In this paper, we show that although the minimizers of cross-entropy and
related classification losses are off at infinity, network weights learned by
gradient flow converge in direction, with an immediate corollary that network
predictions, training errors, and the margin distribution also converge. This
proof holds for deep homogeneous networks -- a broad class of networks allowing
for ReLU, max-pooling, linear, and convolutional layers -- and we additionally
provide empirical support not just close to the theory (e.g., the AlexNet), but
also on non-homogeneous networks (e.g., the DenseNet). If the network further
has locally Lipschitz gradients, we show that these gradients also converge in
direction, and asymptotically align with the gradient flow path, with
consequences on margin maximization, convergence of saliency maps, and a few
other settings. Our analysis complements and is distinct from the well-known
neural tangent and mean-field theories, and in particular makes no requirements
on network width and initialization, instead merely requiring perfect
classification accuracy. The proof proceeds by developing a theory of unbounded
nonsmooth Kurdyka-{\L}ojasiewicz inequalities for functions definable in an
o-minimal structure, and is also applicable outside deep learning.
</p>
<a href="http://arxiv.org/abs/2006.06657" target="_blank">arXiv:2006.06657</a> [<a href="http://arxiv.org/pdf/2006.06657" target="_blank">pdf</a>]

<h2>Deep Network with Approximation Error Being Reciprocal of Width to Power of Square Root of Depth. (arXiv:2006.12231v5 [cs.LG] UPDATED)</h2>
<h3>Zuowei Shen, Haizhao Yang, Shijun Zhang</h3>
<p>A new network with super approximation power is introduced. This network is
built with Floor ($\lfloor x\rfloor$) or ReLU ($\max\{0,x\}$) activation
function in each neuron and hence we call such networks Floor-ReLU networks.
{For any hyper-parameters $N\in\mathbb{N}^+$ and $L\in\mathbb{N}^+$,} it is
shown that Floor-ReLU networks with a width $\max\{d,\, 5N+13\}$ and a depth
$64dL+3$ can {uniformly approximate a H{\"o}lder function $f$ on $[0,1]^d$ with
an approximation rate $3\lambda d^{\alpha/2}N^{-\alpha\sqrt{L}}$, where $\alpha
\in(0,1]$ and $\lambda$ are the H{\"o}lder order and constant, respectively.}
More generally for an arbitrary continuous function $f$ on $[0,1]^d$ with a
modulus of continuity $\omega_f(\cdot)$, the constructive approximation rate is
$\omega_f(\sqrt{d}\,N^{-\sqrt{L}})+2\omega_f(\sqrt{d}){N^{-\sqrt{L}}}$. As a
consequence, this new {class of networks} overcome{s} the curse of
dimensionality in approximation power when the variation of $\omega_f(r)$ as
$r\rightarrow 0$ is moderate (e.g., $\omega_f(r){\lesssim} r^\alpha$ for
H{\"o}lder continuous functions), since the major term to be concerned in our
approximation rate is {essentially $\sqrt{d}$ times a function of $N$ and $L$
independent of $d$ within the modulus of continuity. }
</p>
<a href="http://arxiv.org/abs/2006.12231" target="_blank">arXiv:2006.12231</a> [<a href="http://arxiv.org/pdf/2006.12231" target="_blank">pdf</a>]

<h2>Sample-Efficient Reinforcement Learning of Undercomplete POMDPs. (arXiv:2006.12484v2 [cs.LG] UPDATED)</h2>
<h3>Chi Jin, Sham M. Kakade, Akshay Krishnamurthy, Qinghua Liu</h3>
<p>Partial observability is a common challenge in many reinforcement learning
applications, which requires an agent to maintain memory, infer latent states,
and integrate this past information into exploration. This challenge leads to a
number of computational and statistical hardness results for learning general
Partially Observable Markov Decision Processes (POMDPs). This work shows that
these hardness barriers do not preclude efficient reinforcement learning for
rich and interesting subclasses of POMDPs. In particular, we present a
sample-efficient algorithm, OOM-UCB, for episodic finite undercomplete POMDPs,
where the number of observations is larger than the number of latent states and
where exploration is essential for learning, thus distinguishing our results
from prior works. OOM-UCB achieves an optimal sample complexity of
$\tilde{\mathcal{O}}(1/\varepsilon^2)$ for finding an $\varepsilon$-optimal
policy, along with being polynomial in all other relevant quantities. As an
interesting special case, we also provide a computationally and statistically
efficient algorithm for POMDPs with deterministic state transitions.
</p>
<a href="http://arxiv.org/abs/2006.12484" target="_blank">arXiv:2006.12484</a> [<a href="http://arxiv.org/pdf/2006.12484" target="_blank">pdf</a>]

<h2>Average-case Complexity of Teaching Convex Polytopes via Halfspace Queries. (arXiv:2006.14677v2 [cs.LG] UPDATED)</h2>
<h3>Akash Kumar, Adish Singla, Yisong Yue, Yuxin Chen</h3>
<p>We examine the task of locating a target region among those induced by
intersections of $n$ halfspaces in $\mathbb{R}^d$. This generic task connects
to fundamental machine learning problems, such as training a perceptron and
learning a $\phi$-separable dichotomy. We investigate the average teaching
complexity of the task, i.e., the minimal number of samples (halfspace queries)
required by a teacher to help a version-space learner in locating a randomly
selected target. As our main result, we show that the average-case teaching
complexity is $\Theta(d)$, which is in sharp contrast to the worst-case
teaching complexity of $\Theta(n)$. If instead, we consider the average-case
learning complexity, the bounds have a dependency on $n$ as $\Theta(n)$ for
\tt{i.i.d.} queries and $\Theta(d \log(n))$ for actively chosen queries by the
learner. Our proof techniques are based on novel insights from computational
geometry, which allow us to count the number of convex polytopes and faces in a
Euclidean space depending on the arrangement of halfspaces. Our insights allow
us to establish a tight bound on the average-case complexity for
$\phi$-separable dichotomies, which generalizes the known $\mathcal{O}(d)$
bound on the average number of "extreme patterns" in the classical
computational geometry literature (Cover, 1965).
</p>
<a href="http://arxiv.org/abs/2006.14677" target="_blank">arXiv:2006.14677</a> [<a href="http://arxiv.org/pdf/2006.14677" target="_blank">pdf</a>]

<h2>Hybrid Variance-Reduced SGD Algorithms For Nonconvex-Concave Minimax Problems. (arXiv:2006.15266v2 [math.OC] UPDATED)</h2>
<h3>Quoc Tran-Dinh, Deyi Liu, Lam M. Nguyen</h3>
<p>We develop a novel and single-loop variance-reduced algorithm to solve a
class of stochastic nonconvex-convex minimax problems involving a
nonconvex-linear objective function, which has various applications in
different fields such as machine learning and robust optimization. This problem
class has several computational challenges due to its nonsmoothness,
nonconvexity, nonlinearity, and non-separability of the objective functions.
Our approach relies on a new combination of recent ideas, including smoothing
and hybrid biased variance-reduced techniques. Our algorithm and its variants
can achieve $\mathcal{O}(T^{-2/3})$-convergence rate and the best known oracle
complexity under standard assumptions, where $T$ is the iteration counter. They
have several computational advantages compared to existing methods such as
simple to implement and less parameter tuning requirements. They can also work
with both single sample or mini-batch on derivative estimators, and with
constant or diminishing step-sizes. We demonstrate the benefits of our
algorithms over existing methods through two numerical examples, including a
nonsmooth and nonconvex-non-strongly concave minimax model.
</p>
<a href="http://arxiv.org/abs/2006.15266" target="_blank">arXiv:2006.15266</a> [<a href="http://arxiv.org/pdf/2006.15266" target="_blank">pdf</a>]

<h2>UAV Path Planning for Wireless Data Harvesting: A Deep Reinforcement Learning Approach. (arXiv:2007.00544v2 [cs.LG] UPDATED)</h2>
<h3>Harald Bayerlein, Mirco Theile, Marco Caccamo, David Gesbert</h3>
<p>Autonomous deployment of unmanned aerial vehicles (UAVs) supporting
next-generation communication networks requires efficient trajectory planning
methods. We propose a new end-to-end reinforcement learning (RL) approach to
UAV-enabled data collection from Internet of Things (IoT) devices in an urban
environment. An autonomous drone is tasked with gathering data from distributed
sensor nodes subject to limited flying time and obstacle avoidance. While
previous approaches, learning and non-learning based, must perform expensive
recomputations or relearn a behavior when important scenario parameters such as
the number of sensors, sensor positions, or maximum flying time, change, we
train a double deep Q-network (DDQN) with combined experience replay to learn a
UAV control policy that generalizes over changing scenario parameters. By
exploiting a multi-layer map of the environment fed through convolutional
network layers to the agent, we show that our proposed network architecture
enables the agent to make movement decisions for a variety of scenario
parameters that balance the data collection goal with flight time efficiency
and safety constraints. Considerable advantages in learning efficiency from
using a map centered on the UAV's position over a non-centered map are also
illustrated.
</p>
<a href="http://arxiv.org/abs/2007.00544" target="_blank">arXiv:2007.00544</a> [<a href="http://arxiv.org/pdf/2007.00544" target="_blank">pdf</a>]

<h2>Fictitious Play for Mean Field Games: Continuous Time Analysis and Applications. (arXiv:2007.03458v2 [math.OC] UPDATED)</h2>
<h3>Sarah Perrin, Julien Perolat, Mathieu Lauri&#xe8;re, Matthieu Geist, Romuald Elie, Olivier Pietquin</h3>
<p>In this paper, we deepen the analysis of continuous time Fictitious Play
learning algorithm to the consideration of various finite state Mean Field Game
settings (finite horizon, $\gamma$-discounted), allowing in particular for the
introduction of an additional common noise.

We first present a theoretical convergence analysis of the continuous time
Fictitious Play process and prove that the induced exploitability decreases at
a rate $O(\frac{1}{t})$. Such analysis emphasizes the use of exploitability as
a relevant metric for evaluating the convergence towards a Nash equilibrium in
the context of Mean Field Games. These theoretical contributions are supported
by numerical experiments provided in either model-based or model-free settings.
We provide hereby for the first time converging learning dynamics for Mean
Field Games in the presence of common noise.
</p>
<a href="http://arxiv.org/abs/2007.03458" target="_blank">arXiv:2007.03458</a> [<a href="http://arxiv.org/pdf/2007.03458" target="_blank">pdf</a>]

<h2>Predictive Information Accelerates Learning in RL. (arXiv:2007.12401v2 [cs.LG] UPDATED)</h2>
<h3>Kuang-Huei Lee, Ian Fischer, Anthony Liu, Yijie Guo, Honglak Lee, John Canny, Sergio Guadarrama</h3>
<p>The Predictive Information is the mutual information between the past and the
future, I(X_past; X_future). We hypothesize that capturing the predictive
information is useful in RL, since the ability to model what will happen next
is necessary for success on many tasks. To test our hypothesis, we train Soft
Actor-Critic (SAC) agents from pixels with an auxiliary task that learns a
compressed representation of the predictive information of the RL environment
dynamics using a contrastive version of the Conditional Entropy Bottleneck
(CEB) objective. We refer to these as Predictive Information SAC (PI-SAC)
agents. We show that PI-SAC agents can substantially improve sample efficiency
over challenging baselines on tasks from the DM Control suite of continuous
control environments. We evaluate PI-SAC agents by comparing against
uncompressed PI-SAC agents, other compressed and uncompressed agents, and SAC
agents directly trained from pixels. Our implementation is given on GitHub.
</p>
<a href="http://arxiv.org/abs/2007.12401" target="_blank">arXiv:2007.12401</a> [<a href="http://arxiv.org/pdf/2007.12401" target="_blank">pdf</a>]

<h2>WaveFuse: A Unified Deep Framework for Image Fusion with Discrete Wavelet Transform. (arXiv:2007.14110v3 [cs.CV] UPDATED)</h2>
<h3>Shaolei Liu, Manning Wang, Zhijian Song</h3>
<p>We propose an unsupervised image fusion architecture for multiple application
scenarios based on the combination of multi-scale discrete wavelet transform
through regional energy and deep learning. To our best knowledge, this is the
first time the conventional image fusion method has been combined with deep
learning. The useful information of feature maps can be utilized adequately
through multi-scale discrete wavelet transform in our proposed method.Compared
with other state-of-the-art fusion method, the proposed algorithm exhibits
better fusion performance in both subjective and objective evaluation.
Moreover, it's worth mentioning that comparable fusion performance trained in
COCO dataset can be obtained by training with a much smaller dataset with only
hundreds of images chosen randomly from COCO. Hence, the training time is
shortened substantially, leading to the improvement of the model's performance
both in practicality and training efficiency.
</p>
<a href="http://arxiv.org/abs/2007.14110" target="_blank">arXiv:2007.14110</a> [<a href="http://arxiv.org/pdf/2007.14110" target="_blank">pdf</a>]

<h2>A multi-scale DNN algorithm for nonlinear elliptic equations with multiple scales. (arXiv:2009.14597v2 [physics.comp-ph] UPDATED)</h2>
<h3>Xi-An Li, Zhi-Qin John Xu, Lei Zhang</h3>
<p>Algorithms based on deep neural networks (DNNs) have attracted increasing
attention from the scientific computing community. DNN based algorithms are
easy to implement, natural for nonlinear problems, and have shown great
potential to overcome the curse of dimensionality. In this work, we utilize the
multi-scale DNN-based algorithm (MscaleDNN) proposed by Liu, Cai and Xu (2020)
to solve multi-scale elliptic problems with possible nonlinearity, for example,
the p-Laplacian problem. We improve the MscaleDNN algorithm by a smooth and
localized activation function. Several numerical examples of multi-scale
elliptic problems with separable or non-separable scales in low-dimensional and
high-dimensional Euclidean spaces are used to demonstrate the effectiveness and
accuracy of the MscaleDNN numerical scheme.
</p>
<a href="http://arxiv.org/abs/2009.14597" target="_blank">arXiv:2009.14597</a> [<a href="http://arxiv.org/pdf/2009.14597" target="_blank">pdf</a>]

<h2>POMDPs in Continuous Time and Discrete Spaces. (arXiv:2010.01014v3 [cs.LG] UPDATED)</h2>
<h3>Bastian Alt, Matthias Schultheis, Heinz Koeppl</h3>
<p>Many processes, such as discrete event systems in engineering or population
dynamics in biology, evolve in discrete space and continuous time. We consider
the problem of optimal decision making in such discrete state and action space
systems under partial observability. This places our work at the intersection
of optimal filtering and optimal control. At the current state of research, a
mathematical description for simultaneous decision making and filtering in
continuous time with finite state and action spaces is still missing. In this
paper, we give a mathematical description of a continuous-time partial
observable Markov decision process (POMDP). By leveraging optimal filtering
theory we derive a Hamilton-Jacobi-Bellman (HJB) type equation that
characterizes the optimal solution. Using techniques from deep learning we
approximately solve the resulting partial integro-differential equation. We
present (i) an approach solving the decision problem offline by learning an
approximation of the value function and (ii) an online algorithm which provides
a solution in belief space using deep reinforcement learning. We show the
applicability on a set of toy examples which pave the way for future methods
providing solutions for high dimensional problems.
</p>
<a href="http://arxiv.org/abs/2010.01014" target="_blank">arXiv:2010.01014</a> [<a href="http://arxiv.org/pdf/2010.01014" target="_blank">pdf</a>]

<h2>Data-driven Distributionally Robust Optimal Stochastic Control Using theWasserstein Metric. (arXiv:2010.06794v2 [math.OC] UPDATED)</h2>
<h3>Feiran Zhao, Keyou You</h3>
<p>Optimal control of a stochastic dynamical system usually requires a good
dynamical model with probability distributions, which is difficult to obtain
due to limited measurements and/or complicated dynamics. To solve it, this work
proposes a data-driven distributionally robust control framework with the
Wasserstein metric via a constrained two-player zero-sum Markov game, where the
adversarial player selects the probability distribution from a Wasserstein ball
centered at an empirical distribution. Then, the game is approached by its
penalized version, an optimal stabilizing solution of which is derived
explicitly in a linear structure under the Riccati-type iterations. Moreover,
we design a model-free Q-learning algorithm with global convergence to learn
the optimal controller. Finally, we verify the effectiveness of the proposed
learning algorithm and demonstrate its robustness to the probability
distribution errors via numerical examples.
</p>
<a href="http://arxiv.org/abs/2010.06794" target="_blank">arXiv:2010.06794</a> [<a href="http://arxiv.org/pdf/2010.06794" target="_blank">pdf</a>]

<h2>Eigenvalues of two-phase quantum walks with one defect in one dimension. (arXiv:2010.08324v2 [math-ph] UPDATED)</h2>
<h3>Chusei Kiumi, Kei Saito</h3>
<p>We study space-inhomogeneous quantum walks (QWs) on the integer lattice which
we assign three different coin matrices to the positive part, the negative
part, and the origin, respectively. We call them two-phase QWs with one defect.
They cover one-defect and two-phase QWs, which have been intensively
researched. Localization is one of the most characteristic properties of QWs,
and various types of two-phase QWs with one defect exhibit localization.
Moreover, the existence of eigenvalues is deeply related to localization. In
this paper, we obtain a necessary and sufficient condition for the existence of
eigenvalues. Our analytical methods are mainly based on the transfer matrix, a
useful tool to generate the generalized eigenfunctions. Furthermore, we
explicitly derive eigenvalues for some classes of two-phase QWs with one
defect, and illustrate the range of eigenvalues on unit circles with figures.
Our results include some results in previous studies, e.g. Endo et al. (2020).
</p>
<a href="http://arxiv.org/abs/2010.08324" target="_blank">arXiv:2010.08324</a> [<a href="http://arxiv.org/pdf/2010.08324" target="_blank">pdf</a>]

<h2>Binary Choice with Asymmetric Loss in a Data-Rich Environment: Theory and an Application to Racial Justice. (arXiv:2010.08463v2 [econ.EM] UPDATED)</h2>
<h3>Andrii Babii, Xi Chen, Eric Ghysels, Rohit Kumar</h3>
<p>The importance of asymmetries in prediction problems arising in economics has
been recognized for a long time. In this paper, we focus on binary choice
problems in a data-rich environment with general loss functions. In contrast to
the asymmetric regression problems, the binary choice with general loss
functions and high-dimensional datasets is challenging and not well understood.
Econometricians have studied binary choice problems for a long time, but the
literature does not offer computationally attractive solutions in data-rich
environments. In contrast, the machine learning literature has many
computationally attractive algorithms that form the basis for much of the
automated procedures that are implemented in practice, but it is focused on
symmetric loss functions that are independent of individual characteristics.
One of the main contributions of our paper is to show that the theoretically
valid predictions of binary outcomes with arbitrary loss functions can be
achieved via a very simple reweighting of the logistic regression, or other
state-of-the-art machine learning techniques, such as boosting or (deep) neural
networks. We apply our analysis to racial justice in pretrial detention.
</p>
<a href="http://arxiv.org/abs/2010.08463" target="_blank">arXiv:2010.08463</a> [<a href="http://arxiv.org/pdf/2010.08463" target="_blank">pdf</a>]

<h2>Max-Min Power Control in Downlink Massive MIMO with Distributed Antenna Arrays. (arXiv:2010.08966v2 [cs.IT] UPDATED)</h2>
<h3>Noman Akbar, Emil Bjornson, Nan Yang, Erik G. Larsson</h3>
<p>In this paper, we investigate optimal downlink power allocation in massive
multiple-input multiple-output (MIMO) networks with distributed antenna arrays
(DAAs) under correlated and uncorrelated channel fading. In DAA massive MIMO,
the base station (BS) consists of multiple antenna sub-arrays. Notably, the
antenna sub-arrays are deployed in arbitrary locations within a DAA massive
MIMO cell. Consequently, the distance-dependent large-scale propagation
coefficients are different from a user to these different antenna sub-arrays,
which makes power control a challenging problem. We assume that the network
operates in time-division duplex mode, where each BS obtains the channel
estimates via uplink pilots. Based on the channel estimates, the BSs perform
maximum-ratio transmission in the downlink. We then derive a closed-form
signal-to-interference-plus-noise ratio (SINR) expression, where the channels
are subject to correlated fading. Based on the SINR expression, we propose a
networkwide max-min power control algorithm to ensure that each user in the
network receives a uniform quality of service. Numerical results demonstrate
the performance advantages offered by DAA massive MIMO. For some specific
scenarios, DAA massive MIMO can improve the average per-user throughput up to
55%. Furthermore, we demonstrate that channel fading covariance is an important
factor in determining the performance of DAA massive MIMO.
</p>
<a href="http://arxiv.org/abs/2010.08966" target="_blank">arXiv:2010.08966</a> [<a href="http://arxiv.org/pdf/2010.08966" target="_blank">pdf</a>]

<h2>Multi-Armed Bandits with Dependent Arms. (arXiv:2010.09478v2 [cs.LG] UPDATED)</h2>
<h3>Rahul Singh, Fang Liu, Yin Sun, Ness Shroff</h3>
<p>We study a variant of the classical multi-armed bandit problem (MABP) which
we call as Multi-Armed Bandits with dependent arms. More specifically, multiple
arms are grouped together to form a cluster, and the reward distributions of
arms belonging to the same cluster are known functions of an unknown parameter
that is a characteristic of the cluster. Thus, pulling an arm $i$ not only
reveals information about its own reward distribution, but also about all those
arms that share the same cluster with arm $i$. This "correlation" amongst the
arms complicates the exploration-exploitation trade-off that is encountered in
the MABP because the observation dependencies allow us to test simultaneously
multiple hypotheses regarding the optimality of an arm. We develop learning
algorithms based on the UCB principle which utilize these additional side
observations appropriately while performing exploration-exploitation trade-off.
We show that the regret of our algorithms grows as $O(K\log T)$, where $K$ is
the number of clusters. In contrast, for an algorithm such as the vanilla UCB
that is optimal for the classical MABP and does not utilize these dependencies,
the regret scales as $O(M\log T)$ where $M$ is the number of arms.
</p>
<a href="http://arxiv.org/abs/2010.09478" target="_blank">arXiv:2010.09478</a> [<a href="http://arxiv.org/pdf/2010.09478" target="_blank">pdf</a>]

<h2>Greedy Algorithms for Sparse Sensor Placement via Deep Learning. (arXiv:1809.06025v5 [cs.LG] CROSS LISTED)</h2>
<h3>Louis Ly, Yen-Hsi Richard Tsai</h3>
<p>We consider the exploration problem: an agent equipped with a depth sensor
must map out a previously unknown environment using as few sensor measurements
as possible. We propose an approach based on supervised learning of a greedy
algorithm. We provide a bound on the optimality of the greedy algorithm using
submodularity theory. Using a level set representation, we train a
convolutional neural network to determine vantage points that maximize
visibility. We show that this method drastically reduces the on-line
computational cost and determines a small set of vantage points that solve the
problem. This enables us to efficiently produce highly-resolved and
topologically accurate maps of complex 3D environments. Unlike traditional
next-best-view and frontier-based strategies, the proposed method accounts for
geometric priors while evaluating potential vantage points. While existing deep
learning approaches focus on obstacle avoidance and local navigation, our
method aims at finding near-optimal solutions to the more global exploration
problem. We present realistic simulations on 2D and 3D urban environments.
</p>
<a href="http://arxiv.org/abs/1809.06025" target="_blank">arXiv:1809.06025</a> [<a href="http://arxiv.org/pdf/1809.06025" target="_blank">pdf</a>]

<h2>Using machine learning to correct model error in data assimilation and forecast applications. (arXiv:2010.12605v1 [stat.ML])</h2>
<h3>Alban Farchi, Patrick Laloyaux, Massimo Bonavita, Marc Bocquet</h3>
<p>The idea of using machine learning (ML) methods to reconstruct the dynamics
of a system is the topic of recent studies in the geosciences, in which the key
output is a surrogate model meant to emulate the dynamical model. In order to
treat sparse and noisy observations in a rigorous way, ML can be combined to
data assimilation (DA). This yields a class of iterative methods in which, at
each iteration a DA step assimilates the observations, and alternates with a ML
step to learn the underlying dynamics of the DA analysis. In this article, we
propose to use this method to correct the error of an existent, knowledge-based
model. In practice, the resulting surrogate model is an hybrid model between
the original (knowledge-based) model and the ML model. We demonstrate
numerically the feasibility of the method using a two-layer, two-dimensional
quasi-geostrophic channel model. Model error is introduced by the means of
perturbed parameters. The DA step is performed using the strong-constraint
4D-Var algorithm, while the ML step is performed using deep learning tools. The
ML models are able to learn a substantial part of the model error and the
resulting hybrid surrogate models produce better short- to mid-range forecasts.
Furthermore, using the hybrid surrogate models for DA yields a significantly
better analysis than using the original model.
</p>
<a href="http://arxiv.org/abs/2010.12605" target="_blank">arXiv:2010.12605</a> [<a href="http://arxiv.org/pdf/2010.12605" target="_blank">pdf</a>]

<h2>Towards Co-execution on Commodity Heterogeneous Systems: Optimizations for Time-Constrained Scenarios. (arXiv:2010.12607v1 [cs.DC])</h2>
<h3>Ra&#xfa;l Nozal, Jose Luis Bosque, Ramon Beivide</h3>
<p>Heterogeneous systems are present from powerful supercomputers, to mobile
devices, including desktop computers, thanks to their excellent performance and
energy consumption. The ubiquity of these architectures in both desktop systems
and medium-sized service servers allow enough variability to exploit a wide
range of problems, such as multimedia workloads, video encoding, image
filtering and inference in machine learning. Due to the heterogeneity, some
efforts have been done to reduce the programming effort and preserve
performance portability, but these systems include a set of challenges. The
context in which applications offload the workload along with the management
overheads introduced when doing co-execution, penalize the performance gains
under time-constrained scenarios. Therefore, this paper proposes optimizations
for the EngineCL runtime to reduce the penalization when co-executing in
commodity systems, as well as algorithmic improvements when load balancing. An
exhaustive experimental evaluation is performed, showing optimization
improvements of 7.5\% and 17.4\% for binary and ROI-based offloading modes,
respectively. Thanks to all the optimizations, the new load balancing algorithm
is always the most efficient scheduling configuration, achieving an average
efficiency of 0.84 under a pessimistic scenario.
</p>
<a href="http://arxiv.org/abs/2010.12607" target="_blank">arXiv:2010.12607</a> [<a href="http://arxiv.org/pdf/2010.12607" target="_blank">pdf</a>]

<h2>Iterative Graph Self-Distillation. (arXiv:2010.12609v1 [cs.LG])</h2>
<h3>Hanlin Zhang, Shuai Lin, Weiyang Liu, Pan Zhou, Jian Tang, Xiaodan Liang, Eric P. Xing</h3>
<p>How to discriminatively vectorize graphs is a fundamental challenge that
attracts increasing attentions in recent years. Inspired by the recent success
of unsupervised contrastive learning, we aim to learn graph-level
representation in an unsupervised manner. Specifically, we propose a novel
unsupervised graph learning paradigm called Iterative Graph Self-Distillation
(IGSD) which iteratively performs the teacher-student distillation with graph
augmentations. Different from conventional knowledge distillation, IGSD
constructs the teacher with an exponential moving average of the student model
and distills the knowledge of itself. The intuition behind IGSD is to predict
the teacher network representation of the graph pairs under different augmented
views. As a natural extension, we also apply IGSD to semi-supervised scenarios
by jointly regularizing the network with both supervised and unsupervised
contrastive loss. Finally, we show that finetuning the IGSD-trained models with
self-training can further improve the graph representation power. Empirically,
we achieve significant and consistent performance gain on various graph
datasets in both unsupervised and semi-supervised settings, which well
validates the superiority of IGSD.
</p>
<a href="http://arxiv.org/abs/2010.12609" target="_blank">arXiv:2010.12609</a> [<a href="http://arxiv.org/pdf/2010.12609" target="_blank">pdf</a>]

<h2>Ranking Creative Language Characteristics in Small Data Scenarios. (arXiv:2010.12613v1 [cs.CL])</h2>
<h3>Julia Siekiera, Marius K&#xf6;ppel, Edwin Simpson, Kevin Stowe, Iryna Gurevych, Stefan Kramer</h3>
<p>The ability to rank creative natural language provides an important general
tool for downstream language understanding and generation. However, current
deep ranking models require substantial amounts of labeled data that are
difficult and expensive to obtain for different domains, languages and creative
characteristics. A recent neural approach, the DirectRanker, promises to reduce
the amount of training data needed but its application to text isn't fully
explored. We therefore adapt the DirectRanker to provide a new deep model for
ranking creative language with small data. We compare DirectRanker with a
Bayesian approach, Gaussian process preference learning (GPPL), which has
previously been shown to work well with sparse data. Our experiments with
sparse training data show that while the performance of standard neural ranking
approaches collapses with small training datasets, DirectRanker remains
effective. We find that combining DirectRanker with GPPL increases performance
across different settings by leveraging the complementary benefits of both
models. Our combined approach outperforms the previous state-of-the-art on
humor and metaphor novelty tasks, increasing Spearman's $\rho$ by 14% and 16%
on average.
</p>
<a href="http://arxiv.org/abs/2010.12613" target="_blank">arXiv:2010.12613</a> [<a href="http://arxiv.org/pdf/2010.12613" target="_blank">pdf</a>]

<h2>Counterfactual Representation Learning with Balancing Weights. (arXiv:2010.12618v1 [stat.ML])</h2>
<h3>Serge Assaad, Shuxi Zeng, Chenyang Tao, Shounak Datta, Nikhil Mehta, Ricardo Henao, Fan Li, Lawrence Carin</h3>
<p>A key to causal inference with observational data is achieving balance in
predictive features associated with each treatment type. Recent literature has
explored representation learning to achieve this goal. In this work, we discuss
the pitfalls of these strategies - such as a steep trade-off between achieving
balance and predictive power - and present a remedy via the integration of
balancing weights in causal learning. Specifically, we theoretically link
balance to the quality of propensity estimation, emphasize the importance of
identifying a proper target population, and elaborate on the complementary
roles of feature balancing and weight adjustments. Using these concepts, we
then develop an algorithm for flexible, scalable and accurate estimation of
causal effects. Finally, we show how the learned weighted representations may
serve to facilitate alternative causal learning procedures with appealing
statistical features. We conduct an extensive set of experiments on both
synthetic examples and standard benchmarks, and report encouraging results
relative to state-of-the-art baselines.
</p>
<a href="http://arxiv.org/abs/2010.12618" target="_blank">arXiv:2010.12618</a> [<a href="http://arxiv.org/pdf/2010.12618" target="_blank">pdf</a>]

<h2>Learning Implicitly with Noisy Data in Linear Arithmetic. (arXiv:2010.12619v1 [cs.AI])</h2>
<h3>Alexander Philipp Rader, Ionela G. Mocanu, Vaishak Belle, Brendan Juba</h3>
<p>Robustly learning in expressive languages with real-world data continues to
be a challenging task. Numerous conventional methods appeal to heuristics
without any assurances of robustness. While PAC-Semantics offers strong
guarantees, learning explicit representations is not tractable even in a
propositional setting. However, recent work on so-called "implicit" learning
has shown tremendous promise in terms of obtaining polynomial-time results for
fragments of first-order logic. In this work, we extend implicit learning in
PAC-Semantics to handle noisy data in the form of intervals and threshold
uncertainty in the language of linear arithmetic. We prove that our extended
framework keeps the existing polynomial-time complexity guarantees.
Furthermore, we provide the first empirical investigation of this hitherto
purely theoretical framework. Using benchmark problems, we show that our
implicit approach to learning optimal linear programming objective constraints
significantly outperforms an explicit approach in practice.
</p>
<a href="http://arxiv.org/abs/2010.12619" target="_blank">arXiv:2010.12619</a> [<a href="http://arxiv.org/pdf/2010.12619" target="_blank">pdf</a>]

<h2>Learning to Execute Programs with Instruction Pointer Attention Graph Neural Networks. (arXiv:2010.12621v1 [cs.LG])</h2>
<h3>David Bieber, Charles Sutton, Hugo Larochelle, Daniel Tarlow</h3>
<p>Graph neural networks (GNNs) have emerged as a powerful tool for learning
software engineering tasks including code completion, bug finding, and program
repair. They benefit from leveraging program structure like control flow
graphs, but they are not well-suited to tasks like program execution that
require far more sequential reasoning steps than number of GNN propagation
steps. Recurrent neural networks (RNNs), on the other hand, are well-suited to
long sequential chains of reasoning, but they do not naturally incorporate
program structure and generally perform worse on the above tasks. Our aim is to
achieve the best of both worlds, and we do so by introducing a novel GNN
architecture, the Instruction Pointer Attention Graph Neural Networks
(IPA-GNN), which achieves improved systematic generalization on the task of
learning to execute programs using control flow graphs. The model arises by
considering RNNs operating on program traces with branch decisions as latent
variables. The IPA-GNN can be seen either as a continuous relaxation of the RNN
model or as a GNN variant more tailored to execution. To test the models, we
propose evaluating systematic generalization on learning to execute using
control flow graphs, which tests sequential reasoning and use of program
structure. More practically, we evaluate these models on the task of learning
to execute partial programs, as might arise if using the model as a heuristic
function in program synthesis. Results show that the IPA-GNN outperforms a
variety of RNN and GNN baselines on both tasks.
</p>
<a href="http://arxiv.org/abs/2010.12621" target="_blank">arXiv:2010.12621</a> [<a href="http://arxiv.org/pdf/2010.12621" target="_blank">pdf</a>]

<h2>S2cGAN: Semi-Supervised Training of Conditional GANs with Fewer Labels. (arXiv:2010.12622v1 [cs.LG])</h2>
<h3>Arunava Chakraborty, Rahul Ragesh, Mahir Shah, Nipun Kwatra</h3>
<p>Generative adversarial networks (GANs) have been remarkably successful in
learning complex high dimensional real word distributions and generating
realistic samples. However, they provide limited control over the generation
process. Conditional GANs (cGANs) provide a mechanism to control the generation
process by conditioning the output on a user defined input. Although training
GANs requires only unsupervised data, training cGANs requires labelled data
which can be very expensive to obtain. We propose a framework for
semi-supervised training of cGANs which utilizes sparse labels to learn the
conditional mapping, and at the same time leverages a large amount of
unsupervised data to learn the unconditional distribution. We demonstrate
effectiveness of our method on multiple datasets and different conditional
tasks.
</p>
<a href="http://arxiv.org/abs/2010.12622" target="_blank">arXiv:2010.12622</a> [<a href="http://arxiv.org/pdf/2010.12622" target="_blank">pdf</a>]

<h2>Anchor-based Bilingual Word Embeddings for Low-Resource Languages. (arXiv:2010.12627v1 [cs.CL])</h2>
<h3>Tobias Eder, Viktor Hangya, Alexander Fraser</h3>
<p>Bilingual word embeddings (BWEs) are useful for many cross-lingual
applications, such as bilingual lexicon induction (BLI) and cross-lingual
transfer learning. While recent methods have led to good quality BWEs for
different language pairs using only weak bilingual signals, they still rely on
an abundance of monolingual training data in both languages for their
performance. This becomes a problem especially in the case of low resource
languages where neither parallel bilingual corpora nor large monolingual
training data are available. This paper proposes a new approach for building
BWEs in which the vector space of the high resource source language is used as
a starting point for training an embedding space for the low resource target
language. By using the source vectors as anchors the vector spaces are
automatically aligned. We evaluate the resulting BWEs on BLI and show the
proposed method outperforms previous approaches in the low-resource setting by
a large margin. We show strong results on the standard English-German test pair
(using German to simulate low resource). We also show we can build useful BWEs
for English-Hiligaynon, a true low-resource language, where previous approaches
failed.
</p>
<a href="http://arxiv.org/abs/2010.12627" target="_blank">arXiv:2010.12627</a> [<a href="http://arxiv.org/pdf/2010.12627" target="_blank">pdf</a>]

<h2>Bio-NICA: A biologically inspired single-layer network for Nonnegative Independent Component Analysis. (arXiv:2010.12632v1 [eess.SP])</h2>
<h3>David Lipshutz, Dmitri B. Chklovskii</h3>
<p>Blind source separation, the problem of separating mixtures of unknown
signals into their distinct sources, is an important problem for both
biological and engineered signal processing systems. Nonnegative Independent
Component Analysis (NICA) is a special case of blind source separation that
assumes the mixture is a linear combination of independent, nonnegative
sources. In this work, we derive a single-layer neural network implementation
of NICA satisfying the following 3 constraints, which are relevant for
biological systems and the design of neuromorphic hardware: (i) the network
operates in the online setting, (ii) the synaptic learning rules are local, and
(iii) the neural outputs are nonnegative.
</p>
<a href="http://arxiv.org/abs/2010.12632" target="_blank">arXiv:2010.12632</a> [<a href="http://arxiv.org/pdf/2010.12632" target="_blank">pdf</a>]

<h2>Not Half Bad: Exploring Half-Precision in Graph Convolutional Neural Networks. (arXiv:2010.12635v1 [cs.LG])</h2>
<h3>John Brennan, Stephen Bonner, Amir Atapour-Abarghouei, Philip T Jackson, Boguslaw Obara, Andrew Stephen McGough</h3>
<p>With the growing significance of graphs as an effective representation of
data in numerous applications, efficient graph analysis using modern machine
learning is receiving a growing level of attention. Deep learning approaches
often operate over the entire adjacency matrix -- as the input and intermediate
network layers are all designed in proportion to the size of the adjacency
matrix -- leading to intensive computation and large memory requirements as the
graph size increases. It is therefore desirable to identify efficient measures
to reduce both run-time and memory requirements allowing for the analysis of
the largest graphs possible. The use of reduced precision operations within the
forward and backward passes of a deep neural network along with novel
specialised hardware in modern GPUs can offer promising avenues towards
efficiency. In this paper, we provide an in-depth exploration of the use of
reduced-precision operations, easily integrable into the highly popular PyTorch
framework, and an analysis of the effects of Tensor Cores on graph
convolutional neural networks. We perform an extensive experimental evaluation
of three GPU architectures and two widely-used graph analysis tasks (vertex
classification and link prediction) using well-known benchmark and
synthetically generated datasets. Thus allowing us to make important
observations on the effects of reduced-precision operations and Tensor Cores on
computational and memory usage of graph convolutional neural networks -- often
neglected in the literature.
</p>
<a href="http://arxiv.org/abs/2010.12635" target="_blank">arXiv:2010.12635</a> [<a href="http://arxiv.org/pdf/2010.12635" target="_blank">pdf</a>]

<h2>Nonseparable Symplectic Neural Networks. (arXiv:2010.12636v1 [cs.LG])</h2>
<h3>Shiying Xiong, Yunjin Tong, Xingzhe He, Cheng Yang, Shuqi Yang, Bo Zhu</h3>
<p>Predicting the behaviors of Hamiltonian systems has been drawing increasing
attention in scientific machine learning. However, the vast majority of the
literature was focused on predicting separable Hamiltonian systems with their
kinematic and potential energy terms being explicitly decoupled, while building
data-driven paradigms to predict nonseparable Hamiltonian systems that are
ubiquitous in fluid dynamics and quantum mechanics were rarely explored. The
main computational challenge lies in the effective embedding of symplectic
priors to describe the inherently coupled evolution of position and momentum,
which typically exhibits intricate dynamics with many degrees of freedom. To
solve the problem, we propose a novel neural network architecture, Nonseparable
Symplectic Neural Networks (NSSNNs), to uncover and embed the symplectic
structure of a nonseparable Hamiltonian system from limited observation data.
The enabling mechanics of our approach is an augmented symplectic time
integrator to decouple the position and momentum energy terms and facilitate
their evolution. We demonstrated the efficacy and versatility of our method by
predicting a wide range of Hamiltonian systems, both separable and
nonseparable, including vortical flow and quantum system. We showed the unique
computational merits of our approach to yield long-term, accurate, and robust
predictions for large-scale Hamiltonian systems by rigorously enforcing
symplectomorphism.
</p>
<a href="http://arxiv.org/abs/2010.12636" target="_blank">arXiv:2010.12636</a> [<a href="http://arxiv.org/pdf/2010.12636" target="_blank">pdf</a>]

<h2>Avoiding Occupancy Detection from Smart Meter using Adversarial Machine Learning. (arXiv:2010.12640v1 [cs.LG])</h2>
<h3>ibrahim Yilmaz, Ambareen Siraj</h3>
<p>More and more conventional electromechanical meters are being replaced with
smart meters because of their substantial benefits such as providing faster
bi-directional communication between utility services and end users, enabling
direct load control for demand response, energy saving, and so on. However, the
fine-grained usage data provided by smart meter brings additional
vulnerabilities from users to companies. Occupancy detection is one such
example which causes privacy violation of smart meter users. Detecting the
occupancy of a home is straightforward with time of use information as there is
a strong correlation between occupancy and electricity usage. In this work, our
major contributions are twofold. First, we validate the viability of an
occupancy detection attack based on a machine learning technique called Long
Short Term Memory (LSTM) method and demonstrate improved results. In addition,
we introduce an Adversarial Machine Learning Occupancy Detection Avoidance
(AMLODA) framework as a counter attack in order to prevent abuse of energy
consumption. Essentially, the proposed privacy-preserving framework is designed
to mask real-time or near real-time electricity usage information using
calculated optimum noise without compromising users' billing systems
functionality. Our results show that the proposed privacy-aware billing
technique upholds users' privacy strongly.
</p>
<a href="http://arxiv.org/abs/2010.12640" target="_blank">arXiv:2010.12640</a> [<a href="http://arxiv.org/pdf/2010.12640" target="_blank">pdf</a>]

<h2>Instance-Wise Minimax-Optimal Algorithms for Logistic Bandits. (arXiv:2010.12642v1 [cs.LG])</h2>
<h3>Marc Abeille, Louis Faury, Cl&#xe9;ment Calauz&#xe8;nes</h3>
<p>Logistic Bandits have recently attracted substantial attention, by providing
an uncluttered yet challenging framework for understanding the impact of
non-linearity in parametrized bandits. It was shown by Faury et al. (2020) that
the learning-theoretic difficulties of Logistic Bandits can be embodied by a
large (sometimes prohibitively) problem-dependent constant $\kappa$,
characterizing the magnitude of the reward's non-linearity. In this paper we
introduce a novel algorithm for which we provide a refined analysis. This
allows for a better characterization of the effect of non-linearity and yields
improved problem-dependent guarantees. In most favorable cases this leads to a
regret upper-bound scaling as $\tilde{\mathcal{O}}(d\sqrt{T/\kappa})$, which
dramatically improves over the $\tilde{\mathcal{O}}(d\sqrt{T}+\kappa)$
state-of-the-art guarantees. We prove that this rate is minimax-optimal by
deriving a $\Omega(d\sqrt{T/\kappa})$ problem-dependent lower-bound. Our
analysis identifies two regimes (permanent and transitory) of the regret, which
ultimately re-conciliates Faury et al. (2020) with the Bayesian approach of
Dong et al. (2019). In contrast to previous works, we find that in the
permanent regime non-linearity can dramatically ease the
exploration-exploitation trade-off. While it also impacts the length of the
transitory phase in a problem-dependent fashion, we show that this impact is
mild in most reasonable configurations.
</p>
<a href="http://arxiv.org/abs/2010.12642" target="_blank">arXiv:2010.12642</a> [<a href="http://arxiv.org/pdf/2010.12642" target="_blank">pdf</a>]

<h2>Synthetic Data Augmentation for Zero-Shot Cross-Lingual Question Answering. (arXiv:2010.12643v1 [cs.CL])</h2>
<h3>Arij Riabi, Thomas Scialom, Rachel Keraron, Beno&#xee;t Sagot, Djam&#xe9; Seddah, Jacopo Staiano</h3>
<p>Coupled with the availability of large scale datasets, deep learning
architectures have enabled rapid progress on the Question Answering task.
However, most of those datasets are in English, and the performances of
state-of-the-art multilingual models are significantly lower when evaluated on
non-English data. Due to high data collection costs, it is not realistic to
obtain annotated data for each language one desires to support.

We propose a method to improve the Cross-lingual Question Answering
performance without requiring additional annotated data, leveraging Question
Generation models to produce synthetic samples in a cross-lingual fashion. We
show that the proposed method allows to significantly outperform the baselines
trained on English data only. We report a new state-of-the-art on four
multilingual datasets: MLQA, XQuAD, SQuAD-it and PIAF (fr).
</p>
<a href="http://arxiv.org/abs/2010.12643" target="_blank">arXiv:2010.12643</a> [<a href="http://arxiv.org/pdf/2010.12643" target="_blank">pdf</a>]

<h2>A biologically plausible neural network for Slow Feature Analysis. (arXiv:2010.12644v1 [q-bio.NC])</h2>
<h3>David Lipshutz, Charlie Windolf, Siavash Golkar, Dmitri B. Chklovskii</h3>
<p>Learning latent features from time series data is an important problem in
both machine learning and brain function. One approach, called Slow Feature
Analysis (SFA), leverages the slowness of many salient features relative to the
rapidly varying input signals. Furthermore, when trained on naturalistic
stimuli, SFA reproduces interesting properties of cells in the primary visual
cortex and hippocampus, suggesting that the brain uses temporal slowness as a
computational principle for learning latent features. However, despite the
potential relevance of SFA for modeling brain function, there is currently no
SFA algorithm with a biologically plausible neural network implementation, by
which we mean an algorithm operates in the online setting and can be mapped
onto a neural network with local synaptic updates. In this work, starting from
an SFA objective, we derive an SFA algorithm, called Bio-SFA, with a
biologically plausible neural network implementation. We validate Bio-SFA on
naturalistic stimuli.
</p>
<a href="http://arxiv.org/abs/2010.12644" target="_blank">arXiv:2010.12644</a> [<a href="http://arxiv.org/pdf/2010.12644" target="_blank">pdf</a>]

<h2>Towards Safe Policy Improvement for Non-Stationary MDPs. (arXiv:2010.12645v1 [cs.LG])</h2>
<h3>Yash Chandak, Scott M. Jordan, Georgios Theocharous, Martha White, Philip S. Thomas</h3>
<p>Many real-world sequential decision-making problems involve critical systems
with financial risks and human-life risks. While several works in the past have
proposed methods that are safe for deployment, they assume that the underlying
problem is stationary. However, many real-world problems of interest exhibit
non-stationarity, and when stakes are high, the cost associated with a false
stationarity assumption may be unacceptable. We take the first steps towards
ensuring safety, with high confidence, for smoothly-varying non-stationary
decision problems. Our proposed method extends a type of safe algorithm, called
a Seldonian algorithm, through a synthesis of model-free reinforcement learning
with time-series analysis. Safety is ensured using sequential hypothesis
testing of a policy's forecasted performance, and confidence intervals are
obtained using wild bootstrap.
</p>
<a href="http://arxiv.org/abs/2010.12645" target="_blank">arXiv:2010.12645</a> [<a href="http://arxiv.org/pdf/2010.12645" target="_blank">pdf</a>]

<h2>Extracting Body Text from Academic PDF Documents for Text Mining. (arXiv:2010.12647v1 [cs.IR])</h2>
<h3>Changfeng Yu, Cheng Zhang, Jie Wang</h3>
<p>Accurate extraction of body text from PDF-formatted academic documents is
essential in text-mining applications for deeper semantic understandings. The
objective is to extract complete sentences in the body text into a txt file
with the original sentence flow and paragraph boundaries. Existing tools for
extracting text from PDF documents would often mix body and nonbody texts. We
devise and implement a system called PDFBoT to detect multiple-column layouts
using a line-sweeping technique, remove nonbody text using computed text
features and syntactic tagging in backward traversal, and align the remaining
text back to sentences and paragraphs. We show that PDFBoT is highly accurate
with average F1 scores of, respectively, 0.99 on extracting sentences, 0.96 on
extracting paragraphs, and 0.98 on removing text on tables, figures, and charts
over a corpus of PDF documents randomly selected from arXiv.org across multiple
academic disciplines.
</p>
<a href="http://arxiv.org/abs/2010.12647" target="_blank">arXiv:2010.12647</a> [<a href="http://arxiv.org/pdf/2010.12647" target="_blank">pdf</a>]

<h2>A Study of Transfer Learning in Music Source Separation. (arXiv:2010.12650v1 [cs.SD])</h2>
<h3>Andreas Bugler, Bryan Pardo, Prem Seetharaman</h3>
<p>Supervised deep learning methods for performing audio source separation can
be very effective in domains where there is a large amount of training data.
While some music domains have enough data suitable for training a separation
system, such as rock and pop genres, many musical domains do not, such as
classical music, choral music, and non-Western music traditions. It is well
known that transferring learning from related domains can result in a
performance boost for deep learning systems, but it is not always clear how
best to do pretraining. In this work we investigate the effectiveness of data
augmentation during pretraining, the impact on performance as a result of
pretraining and downstream datasets having similar content domains, and also
explore how much of a model must be retrained on the final target task, once
pretrained.
</p>
<a href="http://arxiv.org/abs/2010.12650" target="_blank">arXiv:2010.12650</a> [<a href="http://arxiv.org/pdf/2010.12650" target="_blank">pdf</a>]

<h2>A simple normative network approximates local non-Hebbian learning in the cortex. (arXiv:2010.12660v1 [q-bio.NC])</h2>
<h3>Siavash Golkar, David Lipshutz, Yanis Bahroun, Anirvan M. Sengupta, Dmitri B. Chklovskii</h3>
<p>To guide behavior, the brain extracts relevant features from high-dimensional
data streamed by sensory organs. Neuroscience experiments demonstrate that the
processing of sensory inputs by cortical neurons is modulated by instructive
signals which provide context and task-relevant information. Here, adopting a
normative approach, we model these instructive signals as supervisory inputs
guiding the projection of the feedforward data. Mathematically, we start with a
family of Reduced-Rank Regression (RRR) objective functions which include
Reduced Rank (minimum) Mean Square Error (RRMSE) and Canonical Correlation
Analysis (CCA), and derive novel offline and online optimization algorithms,
which we call Bio-RRR. The online algorithms can be implemented by neural
networks whose synaptic learning rules resemble calcium plateau potential
dependent plasticity observed in the cortex. We detail how, in our model, the
calcium plateau potential can be interpreted as a backpropagating error signal.
We demonstrate that, despite relying exclusively on biologically plausible
local learning rules, our algorithms perform competitively with existing
implementations of RRMSE and CCA.
</p>
<a href="http://arxiv.org/abs/2010.12660" target="_blank">arXiv:2010.12660</a> [<a href="http://arxiv.org/pdf/2010.12660" target="_blank">pdf</a>]

<h2>Short Video-based Advertisements Evaluation System: Self-Organizing Learning Approach. (arXiv:2010.12662v1 [cs.MM])</h2>
<h3>Yunjie Zhang, Fei Tao, Xudong Liu, Runze Su, Xiaorong Mei, Weicong Ding, Zhichen Zhao, Lei Yuan, Ji Liu</h3>
<p>With the rising of short video apps, such as TikTok, Snapchat and Kwai,
advertisement in short-term user-generated videos (UGVs) has become a trending
form of advertising. Prediction of user behavior without specific user profile
is required by advertisers, as they expect to acquire advertisement performance
in advance in the scenario of cold start. Current recommender system do not
take raw videos as input; additionally, most previous work of Multi-Modal
Machine Learning may not deal with unconstrained videos like UGVs. In this
paper, we proposed a novel end-to-end self-organizing framework for user
behavior prediction. Our model is able to learn the optimal topology of neural
network architecture, as well as optimal weights, through training data. We
evaluate our proposed method on our in-house dataset. The experimental results
reveal that our model achieves the best performance in all our experiments.
</p>
<a href="http://arxiv.org/abs/2010.12662" target="_blank">arXiv:2010.12662</a> [<a href="http://arxiv.org/pdf/2010.12662" target="_blank">pdf</a>]

<h2>A Simple Approach for Handling Out-of-Vocabulary Identifiers in Deep Learning for Source Code. (arXiv:2010.12663v1 [cs.SE])</h2>
<h3>Nadezhda Chirkova, Sergey Troshin</h3>
<p>There is an emerging interest in the application of deep learning models to
source code processing tasks. One of the major problems in applying deep
learning to software engineering is that source code often contains a lot of
rare identifiers resulting in huge vocabularies. We propose a simple yet
effective method based on identifier anonymization to handle out-of-vocabulary
(OOV) identifiers. Our method can be treated as a preprocessing step and
therefore allows an easy implementation. We show that the proposed OOV
anonymization method significantly improves the performance of the Transformer
in two code processing tasks: code completion and bug fixing.
</p>
<a href="http://arxiv.org/abs/2010.12663" target="_blank">arXiv:2010.12663</a> [<a href="http://arxiv.org/pdf/2010.12663" target="_blank">pdf</a>]

<h2>Overcoming Conflicting Data for Model Updates. (arXiv:2010.12675v1 [cs.CL])</h2>
<h3>David Gaddy, Alex Kouzemtchenko, Pavan Kumar Reddy, Prateek Kolhar, Rushin Shah</h3>
<p>In this paper, we explore how to use a small amount of new data to update a
model when the desired output for some examples has changed. When making
updates in this way, one potential problem that arises is the presence of
conflicting data, or out-of-date labels in the original training set. To
evaluate the impact of this problem, we propose an experimental setup for
simulating changes to a neural semantic parser. We show that the presence of
conflicting data greatly hinders learning of an update, then explore several
methods to mitigate its effect. Our methods lead to large improvements in model
accuracy compared to a naive mixing strategy, and our best method closes 86% of
the accuracy gap between this baseline and an oracle upper bound.
</p>
<a href="http://arxiv.org/abs/2010.12675" target="_blank">arXiv:2010.12675</a> [<a href="http://arxiv.org/pdf/2010.12675" target="_blank">pdf</a>]

<h2>Super-Resolution Reconstruction of Interval Energy Data. (arXiv:2010.12678v1 [eess.SP])</h2>
<h3>Jieyi Lu, Baihong Jin</h3>
<p>High-resolution data are desired in many data-driven applications; however,
in many cases only data whose resolution is lower than expected are available
due to various reasons. It is then a challenge how to obtain as much useful
information as possible from the low-resolution data. In this paper, we target
interval energy data collected by Advanced Metering Infrastructure (AMI), and
propose a Super-Resolution Reconstruction (SRR) approach to upsample
low-resolution (hourly) interval data into higher-resolution (15-minute) data
using deep learning. Our preliminary results show that the proposed SRR
approaches can achieve much improved performance compared to the baseline
model.
</p>
<a href="http://arxiv.org/abs/2010.12678" target="_blank">arXiv:2010.12678</a> [<a href="http://arxiv.org/pdf/2010.12678" target="_blank">pdf</a>]

<h2>Unsupervised Dense Shape Correspondence using Heat Kernels. (arXiv:2010.12682v1 [cs.CV])</h2>
<h3>Mehmet Ayg&#xfc;n, Zorah L&#xe4;hner, Daniel Cremers</h3>
<p>In this work, we propose an unsupervised method for learning dense
correspondences between shapes using a recent deep functional map framework.
Instead of depending on ground-truth correspondences or the computationally
expensive geodesic distances, we use heat kernels. These can be computed
quickly during training as the supervisor signal. Moreover, we propose a
curriculum learning strategy using different heat diffusion times which provide
different levels of difficulty during optimization without any sampling
mechanism or hard example mining. We present the results of our method on
different benchmarks which have various challenges like partiality, topological
noise and different connectivity.
</p>
<a href="http://arxiv.org/abs/2010.12682" target="_blank">arXiv:2010.12682</a> [<a href="http://arxiv.org/pdf/2010.12682" target="_blank">pdf</a>]

<h2>Stabilizing Transformer-Based Action Sequence Generation For Q-Learning. (arXiv:2010.12698v1 [cs.LG])</h2>
<h3>Gideon Stein, Andrey Filchenkov, Arip Asadulaev</h3>
<p>Since the publication of the original Transformer architecture (Vaswani et
al. 2017), Transformers revolutionized the field of Natural Language
Processing. This, mainly due to their ability to understand timely dependencies
better than competing RNN-based architectures. Surprisingly, this architecture
change does not affect the field of Reinforcement Learning (RL), even though
RNNs are quite popular in RL, and time dependencies are very common in RL.
Recently, (Parisotto et al. 2019) conducted the first promising research of
Transformers in RL. To support the findings of this work, this paper seeks to
provide an additional example of a Transformer-based RL method. Specifically,
the goal is a simple Transformer-based Deep Q-Learning method that is stable
over several environments. Due to the unstable nature of Transformers and RL,
an extensive method search was conducted to arrive at a final method that
leverages developments around Transformers as well as Q-learning. The proposed
method can match the performance of classic Q-learning on control environments
while showing potential on some selected Atari benchmarks. Furthermore, it was
critically evaluated to give additional insights into the relation between
Transformers and RL.
</p>
<a href="http://arxiv.org/abs/2010.12698" target="_blank">arXiv:2010.12698</a> [<a href="http://arxiv.org/pdf/2010.12698" target="_blank">pdf</a>]

<h2>Learning Assisted Side Channel Delay Test for Detection of Recycled ICs. (arXiv:2010.12704v1 [cs.CR])</h2>
<h3>Ashkan Vakil, Farzad Niknia, Ali Mirzaeian, Avesta Sasan, Naghmeh Karimi</h3>
<p>With the outsourcing of design flow, ensuring the security and
trustworthiness of integrated circuits has become more challenging. Among the
security threats, IC counterfeiting and recycled ICs have received a lot of
attention due to their inferior quality, and in turn, their negative impact on
the reliability and security of the underlying devices. Detecting recycled ICs
is challenging due to the effect of process variations and process drift
occurring during the chip fabrication. Moreover, relying on a golden chip as a
basis for comparison is not always feasible. Accordingly, this paper presents a
recycled IC detection scheme based on delay side-channel testing. The proposed
method relies on the features extracted during the design flow and the sample
delays extracted from the target chip to build a Neural Network model using
which the target chip can be truly identified as new or recycled. The proposed
method classifies the timing paths of the target chip into two groups based on
their vulnerability to aging using the information collected from the design
and detects the recycled ICs based on the deviation of the delay of these two
sets from each other.
</p>
<a href="http://arxiv.org/abs/2010.12704" target="_blank">arXiv:2010.12704</a> [<a href="http://arxiv.org/pdf/2010.12704" target="_blank">pdf</a>]

<h2>Learning to Recognize Dialect Features. (arXiv:2010.12707v1 [cs.CL])</h2>
<h3>Dorottya Demszky, Devyani Sharma, Jonathan H. Clark, Vinodkumar Prabhakaran, Jacob Eisenstein</h3>
<p>Linguists characterize dialects by the presence, absence, and frequency of
dozens of interpretable features. Detecting these features in text has
applications to social science and dialectology, and can be used to assess the
robustness of natural language processing systems to dialect differences. For
most dialects, large-scale annotated corpora for these features are
unavailable, making it difficult to train recognizers. Linguists typically
define dialect features by providing a small number of minimal pairs, which are
paired examples distinguished only by whether the feature is present, while
holding everything else constant. In this paper, we present two multitask
learning architectures for recognizing dialect features, both based on
pretrained transformers. We evaluate these models on two test sets of Indian
English, annotated for a total of 22 dialect features. We find these models
learn to recognize many features with high accuracy; crucially, a few minimal
pairs can be nearly as effective for training as thousands of labeled examples.
We also demonstrate the downstream applicability of our dialect feature
detection model as a dialect density measure and as a dialect classifier.
</p>
<a href="http://arxiv.org/abs/2010.12707" target="_blank">arXiv:2010.12707</a> [<a href="http://arxiv.org/pdf/2010.12707" target="_blank">pdf</a>]

<h2>Improving Classification through Weak Supervision in Context-specific Conversational Agent Development for Teacher Education. (arXiv:2010.12710v1 [cs.CL])</h2>
<h3>Debajyoti Datta, Maria Phillips, Jennifer Chiu, Ginger S. Watson, James P. Bywater, Laura Barnes, Donald Brown</h3>
<p>Machine learning techniques applied to the Natural Language Processing (NLP)
component of conversational agent development show promising results for
improved accuracy and quality of feedback that a conversational agent can
provide. The effort required to develop an educational scenario specific
conversational agent is time consuming as it requires domain experts to label
and annotate noisy data sources such as classroom videos. Previous approaches
to modeling annotations have relied on labeling thousands of examples and
calculating inter-annotator agreement and majority votes in order to model the
necessary scenarios. This method, while proven successful, ignores individual
annotator strengths in labeling a data point and under-utilizes examples that
do not have a majority vote for labeling. We propose using a multi-task weak
supervision method combined with active learning to address these concerns.
This approach requires less labeling than traditional methods and shows
significant improvements in precision, efficiency, and time-requirements than
the majority vote method (Ratner 2019). We demonstrate the validity of this
method on the Google Jigsaw data set and then propose a scenario to apply this
method using the Instructional Quality Assessment(IQA) to define the categories
for labeling. We propose using probabilistic modeling of annotator labeling to
generate active learning examples to further label the data. Active learning is
able to iteratively improve the training performance and accuracy of the
original classification model. This approach combines state-of-the art labeling
techniques of weak supervision and active learning to optimize results in the
educational domain and could be further used to lessen the data requirements
for expanded scenarios within the education domain through transfer learning.
</p>
<a href="http://arxiv.org/abs/2010.12710" target="_blank">arXiv:2010.12710</a> [<a href="http://arxiv.org/pdf/2010.12710" target="_blank">pdf</a>]

<h2>A Caption Is Worth A Thousand Images: Investigating Image Captions for Multimodal Named Entity Recognition. (arXiv:2010.12712v1 [cs.CL])</h2>
<h3>Shuguang Chen, Gustavo Aguilar, Leonardo Neves, Thamar Solorio</h3>
<p>Multimodal named entity recognition (MNER) requires to bridge the gap between
language understanding and visual context. Due to advances in natural language
processing (NLP) and computer vision (CV), many neural techniques have been
proposed to incorporate images into the NER task. In this work, we conduct a
detailed analysis of current state-of-the-art fusion techniques for MNER and
describe scenarios where adding information from the image does not always
result in boosts in performance. We also study the use of captions as a way to
enrich the context for MNER. We provide extensive empirical analysis and an
ablation study on three datasets from popular social platforms to expose the
situations where the approach is beneficial.
</p>
<a href="http://arxiv.org/abs/2010.12712" target="_blank">arXiv:2010.12712</a> [<a href="http://arxiv.org/pdf/2010.12712" target="_blank">pdf</a>]

<h2>Deep Learning for Radio-based Human Sensing: Recent Advances and Future Directions. (arXiv:2010.12717v1 [eess.SP])</h2>
<h3>Isura Nirmal, Abdelwahed Khamis, Mahbub Hassan, Wen Hu, Xiaoqing Zhu</h3>
<p>While decade-long research has clearly demonstrated the vast potential of
radio frequency (RF) for many human sensing tasks, scaling this technology to
large scenarios remained problematic with conventional approaches. Recently,
researchers have successfully applied deep learning to take radio-based sensing
to a new level. Many different types of deep learning models have been proposed
to achieve high sensing accuracy over a large population and activity set, as
well as in unseen environments. Deep learning has also enabled detection of
novel human sensing phenomena that were previously not possible. In this
survey, we provide a comprehensive review and taxonomy of recent research
efforts on deep learning based RF sensing. We also identify and compare several
publicly released labeled RF sensing datasets that can facilitate such deep
learning research. Finally, we summarize the lessons learned and discuss the
current limitations and future directions of deep learning based RF sensing.
</p>
<a href="http://arxiv.org/abs/2010.12717" target="_blank">arXiv:2010.12717</a> [<a href="http://arxiv.org/pdf/2010.12717" target="_blank">pdf</a>]

<h2>Learning Guidance Rewards with Trajectory-space Smoothing. (arXiv:2010.12718v1 [cs.LG])</h2>
<h3>Tanmay Gangwani, Yuan Zhou, Jian Peng</h3>
<p>Long-term temporal credit assignment is an important challenge in deep
reinforcement learning (RL). It refers to the ability of the agent to attribute
actions to consequences that may occur after a long time interval. Existing
policy-gradient and Q-learning algorithms typically rely on dense environmental
rewards that provide rich short-term supervision and help with credit
assignment. However, they struggle to solve tasks with delays between an action
and the corresponding rewarding feedback. To make credit assignment easier,
recent works have proposed algorithms to learn dense "guidance" rewards that
could be used in place of the sparse or delayed environmental rewards. This
paper is in the same vein -- starting with a surrogate RL objective that
involves smoothing in the trajectory-space, we arrive at a new algorithm for
learning guidance rewards. We show that the guidance rewards have an intuitive
interpretation, and can be obtained without training any additional neural
networks. Due to the ease of integration, we use the guidance rewards in a few
popular algorithms (Q-learning, Actor-Critic, Distributional-RL) and present
results in single-agent and multi-agent tasks that elucidate the benefit of our
approach when the environmental rewards are sparse or delayed.
</p>
<a href="http://arxiv.org/abs/2010.12718" target="_blank">arXiv:2010.12718</a> [<a href="http://arxiv.org/pdf/2010.12718" target="_blank">pdf</a>]

<h2>PEP: Parameter Ensembling by Perturbation. (arXiv:2010.12721v1 [cs.LG])</h2>
<h3>Alireza Mehrtash, Purang Abolmaesumi, Polina Golland, Tina Kapur, Demian Wassermann, William M. Wells III</h3>
<p>Ensembling is now recognized as an effective approach for increasing the
predictive performance and calibration of deep networks. We introduce a new
approach, Parameter Ensembling by Perturbation (PEP), that constructs an
ensemble of parameter values as random perturbations of the optimal parameter
set from training by a Gaussian with a single variance parameter. The variance
is chosen to maximize the log-likelihood of the ensemble average ($\mathbb{L}$)
on the validation data set. Empirically, and perhaps surprisingly, $\mathbb{L}$
has a well-defined maximum as the variance grows from zero (which corresponds
to the baseline model). Conveniently, calibration level of predictions also
tends to grow favorably until the peak of $\mathbb{L}$ is reached. In most
experiments, PEP provides a small improvement in performance, and, in some
cases, a substantial improvement in empirical calibration. We show that this
"PEP effect" (the gain in log-likelihood) is related to the mean curvature of
the likelihood function and the empirical Fisher information. Experiments on
ImageNet pre-trained networks including ResNet, DenseNet, and Inception showed
improved calibration and likelihood. We further observed a mild improvement in
classification accuracy on these networks. Experiments on classification
benchmarks such as MNIST and CIFAR-10 showed improved calibration and
likelihood, as well as the relationship between the PEP effect and overfitting;
this demonstrates that PEP can be used to probe the level of overfitting that
occurred during training. In general, no special training procedure or network
architecture is needed, and in the case of pre-trained networks, no additional
training is needed.
</p>
<a href="http://arxiv.org/abs/2010.12721" target="_blank">arXiv:2010.12721</a> [<a href="http://arxiv.org/pdf/2010.12721" target="_blank">pdf</a>]

<h2>Differentiate Quality of Experience Scheduling for Deep Learning Applications with Docker Containers in the Cloud. (arXiv:2010.12728v1 [cs.DC])</h2>
<h3>Ying Mao, Weifeng Yan, Yun Song, Yue Zeng, Ming Chen, Long Cheng, Qingzhi Liu</h3>
<p>With the prevalence of big-data-driven applications, such as face recognition
on smartphones and tailored recommendations from Google Ads, we are on the road
to a lifestyle with significantly more intelligence than ever before. For
example, Aipoly Vision [1] is an object and color recognizer that helps the
blind, visually impaired, and color blind understand their surroundings. At the
back end side of their intelligence, various neural networks powered models are
running to enable quick responses to users. Supporting those models requires
lots of cloud-based computational resources, e.g. CPUs and GPUs. The cloud
providers charge their clients by the amount of resources that they occupied.
From clients' perspective, they have to balance the budget and quality of
experiences (e.g. response time). The budget leans on individual business
owners and the required Quality of Experience (QoE) depends on usage scenarios
of different applications, for instance, an autonomous vehicle requires
realtime response, but, unlocking your smartphone can tolerate delays. However,
cloud providers fail to offer a QoE based option to their clients. In this
paper, we propose DQoES, a differentiate quality of experience scheduler for
deep learning applications. DQoES accepts client's specification on targeted
QoEs, and dynamically adjust resources to approach their targets. Through
extensive, cloud-based experiments, DQoES demonstrates that it can schedule
multiple concurrent jobs with respect to various QoEs and achieve up to 8x
times more satisfied models compared to the existing system.
</p>
<a href="http://arxiv.org/abs/2010.12728" target="_blank">arXiv:2010.12728</a> [<a href="http://arxiv.org/pdf/2010.12728" target="_blank">pdf</a>]

<h2>ANLIzing the Adversarial Natural Language Inference Dataset. (arXiv:2010.12729v1 [cs.CL])</h2>
<h3>Adina Williams, Tristan Thrush, Douwe Kiela</h3>
<p>We perform an in-depth error analysis of Adversarial NLI (ANLI), a recently
introduced large-scale human-and-model-in-the-loop natural language inference
dataset collected over multiple rounds. We propose a fine-grained annotation
scheme of the different aspects of inference that are responsible for the gold
classification labels, and use it to hand-code all three of the ANLI
development sets. We use these annotations to answer a variety of interesting
questions: which inference types are most common, which models have the highest
performance on each reasoning type, and which types are the most challenging
for state of-the-art models? We hope that our annotations will enable more
fine-grained evaluation of models trained on ANLI, provide us with a deeper
understanding of where models fail and succeed, and help us determine how to
train better models in future.
</p>
<a href="http://arxiv.org/abs/2010.12729" target="_blank">arXiv:2010.12729</a> [<a href="http://arxiv.org/pdf/2010.12729" target="_blank">pdf</a>]

<h2>Learning Fine-Grained Multimodal Alignment for Speech Emotion Recognition. (arXiv:2010.12733v1 [cs.SD])</h2>
<h3>Hang Li, Wenbiao Ding, Zhongqin Wu, Zitao Liu</h3>
<p>Speech emotion recognition is a challenging task because the emotion
expression is complex, multimodal and fine-grained. In this paper, we propose a
novel multimodal deep learning approach to perform fine-grained emotion
recognition from real-life speeches. We design a temporal alignment pooling
mechanism to capture the subtle and fine-grained emotions implied in every
utterance. In addition, we propose a cross modality excitation module to
conduct sample-specific activations on acoustic embedding dimensions and
adaptively recalibrate the corresponding values by latent semantic features.
The proposed model is evaluated on two well-known real-world speech emotion
recognition datasets. The results demonstrate that our approach is superior on
the prediction tasks for multimodal speech utterances, and it outperforms a
wide range of baselines in terms of prediction accuracy. In order to encourage
the research reproducibility, we make the code publicly available at
https://github.com/hzlihang99/icassp2021_CME.git.
</p>
<a href="http://arxiv.org/abs/2010.12733" target="_blank">arXiv:2010.12733</a> [<a href="http://arxiv.org/pdf/2010.12733" target="_blank">pdf</a>]

<h2>Scale-, shift- and rotation-invariant diffractive optical networks. (arXiv:2010.12747v1 [physics.optics])</h2>
<h3>Deniz Mengu, Yair Rivenson, Aydogan Ozcan</h3>
<p>Recent research efforts in optical computing have gravitated towards
developing optical neural networks that aim to benefit from the processing
speed and parallelism of optics/photonics in machine learning applications.
Among these endeavors, Diffractive Deep Neural Networks (D2NNs) harness
light-matter interaction over a series of trainable surfaces, designed using
deep learning, to compute a desired statistical inference task as the light
waves propagate from the input plane to the output field-of-view. Although,
earlier studies have demonstrated the generalization capability of diffractive
optical networks to unseen data, achieving e.g., &gt;98% image classification
accuracy for handwritten digits, these previous designs are in general
sensitive to the spatial scaling, translation and rotation of the input
objects. Here, we demonstrate a new training strategy for diffractive networks
that introduces input object translation, rotation and/or scaling during the
training phase as uniformly distributed random variables to build resilience in
their blind inference performance against such object transformations. This
training strategy successfully guides the evolution of the diffractive optical
network design towards a solution that is scale-, shift- and
rotation-invariant, which is especially important and useful for dynamic
machine vision applications in e.g., autonomous cars, in-vivo imaging of
biomedical specimen, among others.
</p>
<a href="http://arxiv.org/abs/2010.12747" target="_blank">arXiv:2010.12747</a> [<a href="http://arxiv.org/pdf/2010.12747" target="_blank">pdf</a>]

<h2>Model Extraction Attacks on Graph Neural Networks: Taxonomy and Realization. (arXiv:2010.12751v1 [cs.LG])</h2>
<h3>Bang Wu, Xiangwen Yang, Shirui Pan, Xingliang Yuan</h3>
<p>Graph neural networks (GNNs) have been widely used to analyze the
graph-structured data in various application domains, e.g., social networks,
molecular biology, and anomaly detection. With great power, the GNN models,
usually as valuable Intellectual Properties of their owners, also become
attractive targets of the attacker. Recent studies show that machine learning
models are facing a severe threat called Model Extraction Attacks, where a
well-trained private model owned by a service provider can be stolen by the
attacker pretending as a client. Unfortunately, existing works focus on the
models trained on the Euclidean space, e.g., images and texts, while how to
extract a GNN model that contains a graph structure and node features is yet to
be explored. In this paper, we explore and develop model extraction attacks
against GNN models. Given only black-box access to a target GNN model, the
attacker aims to reconstruct a duplicated one via several nodes he obtained
(called attacker nodes). We first systematically formalise the threat modeling
in the context of GNN model extraction and classify the adversarial threats
into seven categories by considering different background knowledge of the
attacker, e.g., attributes and/or neighbor connectives of the attacker nodes.
Then we present the detailed methods which utilize the accessible knowledge in
each threat to implement the attacks. By evaluating over three real-world
datasets, our attacks are shown to extract duplicated models effectively, i.e.,
more than 89% inputs in the target domain have the same output predictions as
the victim model.
</p>
<a href="http://arxiv.org/abs/2010.12751" target="_blank">arXiv:2010.12751</a> [<a href="http://arxiv.org/pdf/2010.12751" target="_blank">pdf</a>]

<h2>Temporal Reasoning on Implicit Events from Distant Supervision. (arXiv:2010.12753v1 [cs.CL])</h2>
<h3>Ben Zhou, Kyle Richardson, Qiang Ning, Tushar Khot, Ashish Sabharwal, Dan Roth</h3>
<p>Existing works on temporal reasoning among events described in text focus on
modeling relationships between explicitly mentioned events and do not handle
event end time effectively. However, human readers can infer from natural
language text many implicit events that help them better understand the
situation and, consequently, better reason about time. This work proposes a new
crowd-sourced dataset, TRACIE, which evaluates systems' understanding of
implicit events - events that are not mentioned explicitly in the text but can
be inferred from it. This is done via textual entailment instances querying
both start and end times of events. We show that TRACIE is challenging for
state-of-the-art language models. Our proposed model, SymTime, exploits distant
supervision signals from the text itself and reasons over events' start time
and duration to infer events' end time points. We show that our approach
improves over baseline language models, gaining 5% on the i.i.d. split and 9%
on an out-of-distribution test split. Our approach is also general to other
annotation schemes, gaining 2%-8% on MATRES, an extrinsic temporal relation
benchmark.
</p>
<a href="http://arxiv.org/abs/2010.12753" target="_blank">arXiv:2010.12753</a> [<a href="http://arxiv.org/pdf/2010.12753" target="_blank">pdf</a>]

<h2>Effective Distant Supervision for Temporal Relation Extraction. (arXiv:2010.12755v1 [cs.CL])</h2>
<h3>Xinyu Zhao, Shih-ting Lin, Greg Durrett</h3>
<p>A principal barrier to training temporal relation extraction models in new
domains is the lack of varied, high quality examples and the challenge of
collecting more. We present a method of automatically collecting
distantly-supervised examples of temporal relations. We scrape and
automatically label event pairs where the temporal relations are made explicit
in text, then mask out those explicit cues, forcing a model trained on this
data to learn other signals. We demonstrate that a pre-trained Transformer
model is able to transfer from the weakly labeled examples to human-annotated
benchmarks in both zero-shot and few-shot settings, and that the masking scheme
is important in improving generalization.
</p>
<a href="http://arxiv.org/abs/2010.12755" target="_blank">arXiv:2010.12755</a> [<a href="http://arxiv.org/pdf/2010.12755" target="_blank">pdf</a>]

<h2>NUANCED: Natural Utterance Annotation for Nuanced Conversation with Estimated Distributions. (arXiv:2010.12758v1 [cs.CL])</h2>
<h3>Zhiyu Chen, Honglei Liu, Hu Xu, Seungwhan Moon, Hao Zhou, Bing Liu</h3>
<p>Existing conversational systems are mostly agent-centric, which assumes the
user utterances would closely follow the system ontology (for NLU or dialogue
state tracking). However, in real-world scenarios, it is highly desirable that
the users can speak freely in their own way. It is extremely hard, if not
impossible, for the users to adapt to the unknown system ontology. In this
work, we attempt to build a user-centric dialogue system. As there is no clean
mapping for a user's free form utterance to an ontology, we first model the
user preferences as estimated distributions over the system ontology and map
the users' utterances to such distributions. Learning such a mapping poses new
challenges on reasoning over existing knowledge, ranging from factoid
knowledge, commonsense knowledge to the users' own situations. To this end, we
build a new dataset named NUANCED that focuses on such realistic settings for
conversational recommendation. Collected via dialogue simulation and
paraphrasing, NUANCED contains 5.1k dialogues, 26k turns of high-quality user
responses. We conduct experiments, showing both the usefulness and challenges
of our problem setting. We believe NUANCED can serve as a valuable resource to
push existing research from the agent-centric system to the user-centric
system. The code and data will be made publicly available.
</p>
<a href="http://arxiv.org/abs/2010.12758" target="_blank">arXiv:2010.12758</a> [<a href="http://arxiv.org/pdf/2010.12758" target="_blank">pdf</a>]

<h2>Gradient Flows in Dataset Space. (arXiv:2010.12760v1 [cs.LG])</h2>
<h3>David Alvarez-Melis, Nicol&#xf2; Fusi</h3>
<p>The current practice in machine learning is traditionally model-centric,
casting problems as optimization over model parameters, all the while assuming
the data is either fixed, or subject to extrinsic and inevitable change. On one
hand, this paradigm fails to capture important existing aspects of machine
learning, such as the substantial data manipulation (\emph{e.g.}, augmentation)
that goes into most state-of-the-art pipelines. On the other hand, this
viewpoint is ill-suited to formalize novel data-centric problems, such as
model-agnostic transfer learning or dataset synthesis. In this work, we view
these and other problems through the lens of \textit{dataset optimization},
casting them as optimization over data-generating distributions. We approach
this class of problems through Wasserstein gradient flows in probability space,
and derive practical and efficient particle-based methods for a flexible but
well-behaved class of objective functions. Through various experiments on
synthetic and real datasets, we show that this framework provides a principled
and effective approach to dataset shaping, transfer, and interpolation.
</p>
<a href="http://arxiv.org/abs/2010.12760" target="_blank">arXiv:2010.12760</a> [<a href="http://arxiv.org/pdf/2010.12760" target="_blank">pdf</a>]

<h2>Measuring Association Between Labels and Free-Text Rationales. (arXiv:2010.12762v1 [cs.CL])</h2>
<h3>Sarah Wiegreffe, Ana Marasovic, Noah A. Smith</h3>
<p>Interpretable NLP has taking increasing interest in ensuring that
explanations are faithful to the model's decision-making process. This property
is crucial for machine learning researchers and practitioners using
explanations to better understand models. While prior work focuses primarily on
extractive rationales (a subset of the input elements), we investigate their
less-studied counterpart: free-text natural language rationales. We demonstrate
that existing models for faithful interpretability do not extend cleanly to
tasks where free-text rationales are needed. We turn to models that jointly
predict and rationalize, a common class of models for free-text rationalization
whose faithfulness is not yet established. We propose measurements of
label-rationale association, a necessary property of faithful rationales, for
these models. Using our measurements, we show that a state-of-the-art joint
model based on T5 has strengths and weaknesses for producing faithful
rationales.
</p>
<a href="http://arxiv.org/abs/2010.12762" target="_blank">arXiv:2010.12762</a> [<a href="http://arxiv.org/pdf/2010.12762" target="_blank">pdf</a>]

<h2>Federated Bandit: A Gossiping Approach. (arXiv:2010.12763v1 [cs.LG])</h2>
<h3>Zhaowei Zhu, Jingxuan Zhu, Ji Liu, Yang Liu</h3>
<p>In this paper, we study \emph{Federated Bandit}, a decentralized Multi-Armed
Bandit problem with a set of $N$ agents, who can only communicate their local
data with neighbors described by a connected graph $G$. Each agent makes a
sequence of decisions on selecting an arm from $M$ candidates, yet they only
have access to local and potentially biased feedback/evaluation of the true
reward for each action taken. Learning only locally will lead agents to
sub-optimal actions while converging to a no-regret strategy requires a
collection of distributed data. Motivated by the proposal of federated
learning, we aim for a solution with which agents will never share their local
observations with a central entity, and will be allowed to only share a private
copy of his/her own information with their neighbors. We first propose a
decentralized bandit algorithm \texttt{Gossip\_UCB}, which is a coupling of
variants of both the classical gossiping algorithm and the celebrated Upper
Confidence Bound (UCB) bandit algorithm. We show that \texttt{Gossip\_UCB}
successfully adapts local bandit learning into a global gossiping process for
sharing information among connected agents, and achieves guaranteed regret at
the order of $O(\max\{ \texttt{poly}(N,M) \log T,
\texttt{poly}(N,M)\log_{\lambda_2^{-1}} N\})$ for all $N$ agents, where
$\lambda_2\in(0,1)$ is the second largest eigenvalue of the expected gossip
matrix, which is a function of $G$. We then propose \texttt{Fed\_UCB}, a
differentially private version of \texttt{Gossip\_UCB}, in which the agents
preserve $\epsilon$-differential privacy of their local data while achieving
$O(\max \{\frac{\texttt{poly}(N,M)}{\epsilon}\log^{2.5} T, \texttt{poly}(N,M)
(\log_{\lambda_2^{-1}} N + \log T) \})$ regret.
</p>
<a href="http://arxiv.org/abs/2010.12763" target="_blank">arXiv:2010.12763</a> [<a href="http://arxiv.org/pdf/2010.12763" target="_blank">pdf</a>]

<h2>Modularity Improves Out-of-Domain Instruction Following. (arXiv:2010.12764v1 [cs.CL])</h2>
<h3>Rodolfo Corona, Daniel Fried, Coline Devin, Dan Klein, Trevor Darrell</h3>
<p>We propose a modular architecture for following natural language instructions
that describe sequences of diverse subgoals, such as navigating to landmarks or
picking up objects. Standard, non-modular, architectures used in instruction
following do not exploit subgoal compositionality and often struggle on
out-of-distribution tasks and environments. In our approach, subgoal modules
each carry out natural language instructions for a specific subgoal type. A
sequence of modules to execute is chosen by learning to segment the
instructions and predicting a subgoal type for each segment. When compared to
standard sequence-to-sequence approaches on ALFRED, a challenging instruction
following benchmark, we find that modularization improves generalization to
environments unseen in training and to novel tasks.
</p>
<a href="http://arxiv.org/abs/2010.12764" target="_blank">arXiv:2010.12764</a> [<a href="http://arxiv.org/pdf/2010.12764" target="_blank">pdf</a>]

<h2>On Learning Text Style Transfer with Direct Rewards. (arXiv:2010.12771v1 [cs.CL])</h2>
<h3>Yixin Liu, Graham Neubig, John Wieting</h3>
<p>In most cases, the lack of parallel corpora makes it impossible to directly
train supervised models for text style transfer task. In this paper, we explore
training algorithms that instead optimize reward functions that explicitly
consider different aspects of the style-transferred outputs. In particular, we
leverage semantic similarity metrics originally used for fine-tuning neural
machine translation models to explicitly assess the preservation of content
between system outputs and input texts. We also investigate the potential
weaknesses of the existing automatic metrics and propose efficient strategies
of using these metrics for training. The experimental results show that our
model provides significant gains in both automatic and human evaluation over
strong baselines, indicating the effectiveness of our proposed methods and
training strategies.
</p>
<a href="http://arxiv.org/abs/2010.12771" target="_blank">arXiv:2010.12771</a> [<a href="http://arxiv.org/pdf/2010.12771" target="_blank">pdf</a>]

<h2>Structure-Grounded Pretraining for Text-to-SQL. (arXiv:2010.12773v1 [cs.CL])</h2>
<h3>Xiang Deng, Ahmed Hassan Awadallah, Christopher Meek, Oleksandr Polozov, Huan Sun, Matthew Richardson</h3>
<p>Learning to capture text-table alignment is essential for table related tasks
like text-to-SQL. The model needs to correctly recognize natural language
references to columns and values and to ground them in the given database
schema. In this paper, we present a novel weakly supervised Structure-Grounded
pretraining framework (StruG) for text-to-SQL that can effectively learn to
capture text-table alignment based on a parallel text-table corpus. We identify
a set of novel prediction tasks: column grounding, value grounding and
column-value mapping, and train them using weak supervision without requiring
complex SQL annotation. Additionally, to evaluate the model under a more
realistic setting, we create a new evaluation set Spider-Realistic based on
Spider with explicit mentions of column names removed, and adopt two existing
single-database text-to-SQL datasets. StruG significantly outperforms
BERT-LARGE on Spider and the realistic evaluation sets, while bringing
consistent improvement on the large-scale WikiSQL benchmark.
</p>
<a href="http://arxiv.org/abs/2010.12773" target="_blank">arXiv:2010.12773</a> [<a href="http://arxiv.org/pdf/2010.12773" target="_blank">pdf</a>]

<h2>Uncertainty Aware Semi-Supervised Learning on Graph Data. (arXiv:2010.12783v1 [cs.LG])</h2>
<h3>Xujiang Zhao, Feng Chen, Shu Hu, Jin-Hee Cho</h3>
<p>Thanks to graph neural networks (GNNs), semi-supervised node classification
has shown the state-of-the-art performance in graph data. However, GNNs have
not considered different types of uncertainties associated with class
probabilities to minimize risk of increasing misclassification under
uncertainty in real life. In this work, we propose a multi-source uncertainty
framework using a GNN that reflects various types of predictive uncertainties
in both deep learning and belief/evidence theory domains for node
classification predictions. By collecting evidence from the given labels of
training nodes, the Graph-based Kernel Dirichlet distribution Estimation (GKDE)
method is designed for accurately predicting node-level Dirichlet distributions
and detecting out-of-distribution (OOD) nodes. We validated the outperformance
of our proposed model compared to the state-of-the-art counterparts in terms of
misclassification detection and OOD detection based on six real network
datasets. We found that dissonance-based detection yielded the best results on
misclassification detection while vacuity-based detection was the best for OOD
detection. To clarify the reasons behind the results, we provided the
theoretical proof that explains the relationships between different types of
uncertainties considered in this work.
</p>
<a href="http://arxiv.org/abs/2010.12783" target="_blank">arXiv:2010.12783</a> [<a href="http://arxiv.org/pdf/2010.12783" target="_blank">pdf</a>]

<h2>Clustering Contextualized Representations of Text for Unsupervised Syntax Induction. (arXiv:2010.12784v1 [cs.CL])</h2>
<h3>Vikram Gupta, Haoyue Shi, Kevin Gimpel, Mrinmaya Sachan</h3>
<p>We explore clustering of contextualized text representations for two
unsupervised syntax induction tasks: part of speech induction (POSI) and
constituency labelling (CoLab). We propose a deep embedded clustering approach
which jointly transforms these representations into a lower dimension cluster
friendly space and clusters them. We further enhance these representations by
augmenting them with task-specific representations. We also explore the
effectiveness of multilingual representations for different tasks and
languages. With this work, we establish the first strong baselines for
unsupervised syntax induction using contextualized text representations. We
report competitive performance on 45-tag POSI, state-of-the-art performance on
12-tag POSI across 10 languages, and competitive results on CoLab.
</p>
<a href="http://arxiv.org/abs/2010.12784" target="_blank">arXiv:2010.12784</a> [<a href="http://arxiv.org/pdf/2010.12784" target="_blank">pdf</a>]

<h2>ShiftAddNet: A Hardware-Inspired Deep Network. (arXiv:2010.12785v1 [cs.LG])</h2>
<h3>Haoran You, Xiaohan Chen, Yongan Zhang, Chaojian Li, Sicheng Li, Zihao Liu, Zhangyang Wang, Yingyan Lin</h3>
<p>Multiplication (e.g., convolution) is arguably a cornerstone of modern deep
neural networks (DNNs). However, intensive multiplications cause expensive
resource costs that challenge DNNs' deployment on resource-constrained edge
devices, driving several attempts for multiplication-less deep networks. This
paper presented ShiftAddNet, whose main inspiration is drawn from a common
practice in energy-efficient hardware implementation, that is, multiplication
can be instead performed with additions and logical bit-shifts. We leverage
this idea to explicitly parameterize deep networks in this way, yielding a new
type of deep network that involves only bit-shift and additive weight layers.
This hardware-inspired ShiftAddNet immediately leads to both energy-efficient
inference and training, without compromising the expressive capacity compared
to standard DNNs. The two complementary operation types (bit-shift and add)
additionally enable finer-grained control of the model's learning capacity,
leading to more flexible trade-off between accuracy and (training) efficiency,
as well as improved robustness to quantization and pruning. We conduct
extensive experiments and ablation studies, all backed up by our FPGA-based
ShiftAddNet implementation and energy measurements. Compared to existing DNNs
or other multiplication-less models, ShiftAddNet aggressively reduces over 80%
hardware-quantified energy cost of DNNs training and inference, while offering
comparable or better accuracies. Codes and pre-trained models are available at
https://github.com/RICE-EIC/ShiftAddNet.
</p>
<a href="http://arxiv.org/abs/2010.12785" target="_blank">arXiv:2010.12785</a> [<a href="http://arxiv.org/pdf/2010.12785" target="_blank">pdf</a>]

<h2>Efficient End-to-end Learning of Cross-event Dependencies for Document-level Event Extraction. (arXiv:2010.12787v1 [cs.CL])</h2>
<h3>Kung-Hsiang Huang, Nanyun Peng</h3>
<p>Document-level event extraction is important for indexing the most important
information in a document to facilitate downstream tasks such as information
retrieval or question answering. However, it is a challenging task because it
requires the understanding of event and entity coreference, and capturing
arguments that span across different sentences. Existing works on event
extraction generally confine on extracting events from single sentences, which
fail to capture the relationships between the event mentions at the scale of a
document, as well as the event arguments that appear in a different sentence
than the event trigger. In this paper, we propose an end-to-end model
leveraging Deep Value Networks (DVN), a structured prediction algorithm, to
efficiently capture cross-event dependencies for document-level event
extraction. Experimental results show that our approach achieves comparable
performance to CRF-based model on ACE05, while enjoys significantly higher
efficiency.
</p>
<a href="http://arxiv.org/abs/2010.12787" target="_blank">arXiv:2010.12787</a> [<a href="http://arxiv.org/pdf/2010.12787" target="_blank">pdf</a>]

<h2>X-Class: Text Classification with Extremely Weak Supervision. (arXiv:2010.12794v1 [cs.CL])</h2>
<h3>Zihan Wang, Dheeraj Mekala, Jingbo Shang</h3>
<p>In this paper, we explore to conduct text classification with extremely weak
supervision, i.e., only relying on the surface text of class names. This is a
more challenging setting than the seed-driven weak supervision, which allows a
few seed words per class. We opt to attack this problem from a representation
learning perspective -- ideal document representations should lead to very
close results between clustering and the desired classification. In particular,
one can classify the same corpus differently (e.g., based on topics and
locations), so document representations must be adaptive to the given class
names. We propose a novel framework X-Class to realize it. Specifically, we
first estimate comprehensive class representations by incrementally adding the
most similar word to each class until inconsistency appears. Following a
tailored mixture of class attention mechanisms, we obtain the document
representation via a weighted average of contextualized token representations.
We then cluster and align the documents to classes with the prior of each
document assigned to its nearest class. Finally, we pick the most confident
documents from each cluster to train a text classifier. Extensive experiments
demonstrate that X-Class can rival and even outperform seed-driven weakly
supervised methods on 7 benchmark datasets.
</p>
<a href="http://arxiv.org/abs/2010.12794" target="_blank">arXiv:2010.12794</a> [<a href="http://arxiv.org/pdf/2010.12794" target="_blank">pdf</a>]

<h2>Improving the generalization of network based relative pose regression: dimension reduction as a regularizer. (arXiv:2010.12796v1 [cs.CV])</h2>
<h3>Xiaqing Ding, Yue Wang, Li Tang, Yanmei Jiao, Rong Xiong</h3>
<p>Visual localization occupies an important position in many areas such as
Augmented Reality, robotics and 3D reconstruction. The state-of-the-art visual
localization methods perform pose estimation using geometry based solver within
the RANSAC framework. However, these methods require accurate pixel-level
matching at high image resolution, which is hard to satisfy under significant
changes from appearance, dynamics or perspective of view. End-to-end learning
based regression networks provide a solution to circumvent the requirement for
precise pixel-level correspondences, but demonstrate poor performance towards
cross-scene generalization. In this paper, we explicitly add a learnable
matching layer within the network to isolate the pose regression solver from
the absolute image feature values, and apply dimension regularization on both
the correlation feature channel and the image scale to further improve
performance towards generalization and large viewpoint change. We implement
this dimension regularization strategy within a two-layer pyramid based
framework to regress the localization results from coarse to fine. In addition,
the depth information is fused for absolute translational scale recovery.
Through experiments on real world RGBD datasets we validate the effectiveness
of our design in terms of improving both generalization performance and
robustness towards viewpoint change, and also show the potential of regression
based visual localization networks towards challenging occasions that are
difficult for geometry based visual localization methods.
</p>
<a href="http://arxiv.org/abs/2010.12796" target="_blank">arXiv:2010.12796</a> [<a href="http://arxiv.org/pdf/2010.12796" target="_blank">pdf</a>]

<h2>Collaborative Machine Learning with Incentive-Aware Model Rewards. (arXiv:2010.12797v1 [cs.LG])</h2>
<h3>Rachael Hwee Ling Sim, Yehong Zhang, Mun Choon Chan, Bryan Kian Hsiang Low</h3>
<p>Collaborative machine learning (ML) is an appealing paradigm to build
high-quality ML models by training on the aggregated data from many parties.
However, these parties are only willing to share their data when given enough
incentives, such as a guaranteed fair reward based on their contributions. This
motivates the need for measuring a party's contribution and designing an
incentive-aware reward scheme accordingly. This paper proposes to value a
party's reward based on Shapley value and information gain on model parameters
given its data. Subsequently, we give each party a model as a reward. To
formally incentivize the collaboration, we define some desirable properties
(e.g., fairness and stability) which are inspired by cooperative game theory
but adapted for our model reward that is uniquely freely replicable. Then, we
propose a novel model reward scheme to satisfy fairness and trade off between
the desirable properties via an adjustable parameter. The value of each party's
model reward determined by our scheme is attained by injecting Gaussian noise
to the aggregated training data with an optimized noise variance. We
empirically demonstrate interesting properties of our scheme and evaluate its
performance using synthetic and real-world datasets.
</p>
<a href="http://arxiv.org/abs/2010.12797" target="_blank">arXiv:2010.12797</a> [<a href="http://arxiv.org/pdf/2010.12797" target="_blank">pdf</a>]

<h2>Content-Based Personalized Recommender System Using Entity Embeddings. (arXiv:2010.12798v1 [cs.IR])</h2>
<h3>Xavier Thomas</h3>
<p>Recommender systems are a class of machine learning algorithms that provide
relevant recommendations to a user based on the user's interaction with similar
items or based on the content of the item. In settings where the content of the
item is to be preserved, a content-based approach would be beneficial. This
paper aims to highlight the advantages of the content-based approach through
learned embeddings and leveraging these advantages to provide better and
personalised movie recommendations based on user preferences to various movie
features such as genre and keyword tags.
</p>
<a href="http://arxiv.org/abs/2010.12798" target="_blank">arXiv:2010.12798</a> [<a href="http://arxiv.org/pdf/2010.12798" target="_blank">pdf</a>]

<h2>Attentive Autoencoders for Multifaceted Preference Learning in One-class Collaborative Filtering. (arXiv:2010.12803v1 [cs.IR])</h2>
<h3>Zheda Mai, Ga Wu, Kai Luo, Scott Sanner</h3>
<p>Most existing One-Class Collaborative Filtering (OC-CF) algorithms estimate a
user's preference as a latent vector by encoding their historical interactions.
However, users often show diverse interests, which significantly increases the
learning difficulty. In order to capture multifaceted user preferences,
existing recommender systems either increase the encoding complexity or extend
the latent representation dimension. Unfortunately, these changes inevitably
lead to increased training difficulty and exacerbate scalability issues. In
this paper, we propose a novel and efficient CF framework called Attentive
Multi-modal AutoRec (AMA) that explicitly tracks multiple facets of user
preferences. Specifically, we extend the Autoencoding-based recommender AutoRec
to learn user preferences with multi-modal latent representations, where each
mode captures one facet of a user's preferences. By leveraging the attention
mechanism, each observed interaction can have different contributions to the
preference facets. Through extensive experiments on three real-world datasets,
we show that AMA is competitive with state-of-the-art models under the OC-CF
setting. Also, we demonstrate how the proposed model improves interpretability
by providing explanations using the attention mechanism.
</p>
<a href="http://arxiv.org/abs/2010.12803" target="_blank">arXiv:2010.12803</a> [<a href="http://arxiv.org/pdf/2010.12803" target="_blank">pdf</a>]

<h2>REDE: End-to-end Object 6D Pose Robust Estimation Using Differentiable Outliers Elimination. (arXiv:2010.12807v1 [cs.CV])</h2>
<h3>Weitong Hua, Zhongxiang Zhou, Jun Wu, Yue Wang, Rong Xiong</h3>
<p>Object 6D pose estimation is a fundamental task in many applications.
Conventional methods solve the task by detecting and matching the keypoints,
then estimating the pose. Recent efforts bringing deep learning into the
problem mainly overcome the vulnerability of conventional methods to
environmental variation due to the hand-crafted feature design. However, these
methods cannot achieve end-to-end learning and good interpretability at the
same time. In this paper, we propose REDE, a novel end-to-end object pose
estimator using RGB-D data, which utilizes network for keypoint regression, and
a differentiable geometric pose estimator for pose error back-propagation.
Besides, to achieve better robustness when outlier keypoint prediction occurs,
we further propose a differentiable outliers elimination method that regresses
the candidate result and the confidence simultaneously. Via confidence weighted
aggregation of multiple candidates, we can reduce the effect from the outliers
in the final estimation. Finally, following the conventional method, we apply a
learnable refinement process to further improve the estimation. The
experimental results on three benchmark datasets show that REDE slightly
outperforms the state-of-the-art approaches and is more robust to object
occlusion.
</p>
<a href="http://arxiv.org/abs/2010.12807" target="_blank">arXiv:2010.12807</a> [<a href="http://arxiv.org/pdf/2010.12807" target="_blank">pdf</a>]

<h2>Paired Representation Learning for Event and Entity Coreference. (arXiv:2010.12808v1 [cs.CL])</h2>
<h3>Xiaodong Yu, Wenpeng Yin, Dan Roth</h3>
<p>Co-reference of Events and of Entities are commonly formulated as binary
classification problems, given a pair of events or entities as input. Earlier
work addressed the main challenge in these problems -- the representation of
each element in the input pair by: (i) modelling the representation of one
element (event or entity) without considering the other element in the pair;
(ii) encoding all attributes of one element (e.g., arguments of an event) into
a single non-interpretable vector, thus losing the ability to compare
cross-element attributes. In this work we propose paired representation
learning (PairedRL) for coreference resolution. Given a pair of elements
(Events or Entities) our model treats the pair's sentences as a single sequence
so that each element in the pair learns its representation by encoding its own
context as well the other element's context. In addition, when representing
events, PairedRL is structured in that it represents the event's arguments to
facilitate their individual contribution to the final prediction. As we show,
in both (within-document &amp; cross-document) event and entity coreference
benchmarks, our unified approach, PairedRL, outperforms prior state of the art
systems with a large margin.
</p>
<a href="http://arxiv.org/abs/2010.12808" target="_blank">arXiv:2010.12808</a> [<a href="http://arxiv.org/pdf/2010.12808" target="_blank">pdf</a>]

<h2>Stop Bugging Me! Evading Modern-Day Wiretapping Using Adversarial Perturbations. (arXiv:2010.12809v1 [cs.SD])</h2>
<h3>Tal Ben Senior, Yael Mathov, Asaf Shabtai, Yuval Elovici</h3>
<p>Mass surveillance systems for voice over IP (VoIP) conversations pose a huge
risk to privacy. These automated systems use learning models to analyze
conversations, and upon detecting calls that involve specific topics, route
them to a human agent. In this study, we present an adversarial learning-based
framework for privacy protection for VoIP conversations. We present a novel
algorithm that finds a universal adversarial perturbation (UAP), which, when
added to the audio stream, prevents an eavesdropper from automatically
detecting the conversation's topic. As shown in our experiments, the UAP is
agnostic to the speaker or audio length, and its volume can be changed in
real-time, as needed. In a real-world demonstration, we use a Teensy
microcontroller that acts as an external microphone and adds the UAP to the
audio in real-time. We examine different speakers, VoIP applications (Skype,
Zoom), audio lengths, and speech-to-text models (Deep Speech, Kaldi). Our
results in the real world suggest that our approach is a feasible solution for
privacy protection.
</p>
<a href="http://arxiv.org/abs/2010.12809" target="_blank">arXiv:2010.12809</a> [<a href="http://arxiv.org/pdf/2010.12809" target="_blank">pdf</a>]

<h2>Graph Information Bottleneck. (arXiv:2010.12811v1 [cs.LG])</h2>
<h3>Tailin Wu, Hongyu Ren, Pan Li, Jure Leskovec</h3>
<p>Representation learning of graph-structured data is challenging because both
graph structure and node features carry important information. Graph Neural
Networks (GNNs) provide an expressive way to fuse information from network
structure and node features. However, GNNs are prone to adversarial attacks.
Here we introduce Graph Information Bottleneck (GIB), an information-theoretic
principle that optimally balances expressiveness and robustness of the learned
representation of graph-structured data. Inheriting from the general
Information Bottleneck (IB), GIB aims to learn the minimal sufficient
representation for a given task by maximizing the mutual information between
the representation and the target, and simultaneously constraining the mutual
information between the representation and the input data. Different from the
general IB, GIB regularizes the structural as well as the feature information.
We design two sampling algorithms for structural regularization and instantiate
the GIB principle with two new models: GIB-Cat and GIB-Bern, and demonstrate
the benefits by evaluating the resilience to adversarial attacks. We show that
our proposed models are more robust than state-of-the-art graph defense models.
GIB-based models empirically achieve up to 31% improvement with adversarial
perturbation of the graph structure as well as node features.
</p>
<a href="http://arxiv.org/abs/2010.12811" target="_blank">arXiv:2010.12811</a> [<a href="http://arxiv.org/pdf/2010.12811" target="_blank">pdf</a>]

<h2>A Frustratingly Easy Approach for Joint Entity and Relation Extraction. (arXiv:2010.12812v1 [cs.CL])</h2>
<h3>Zexuan Zhong, Danqi Chen</h3>
<p>End-to-end relation extraction aims to identify named entities and extract
relations between them simultaneously. Most recent work models these two
subtasks jointly, either by unifying them in one structured prediction
framework, or multi-task learning through shared representations. In this work,
we describe a very simple approach for joint entity and relation extraction,
and establish the new state-of-the-art on standard benchmarks (ACE04, ACE05,
and SciERC). Our approach essentially builds on two independent pre-trained
encoders and merely uses the entity model to provide input features for the
relation model. Through a series of careful examinations, we validate the
importance of learning distinct contextual representations for entities and
relations, fusing entity information at the input layer of the relation model,
and incorporating global context. Finally, we also present an efficient
approximation to our approach which requires only one pass of both encoders at
inference time, obtaining a 8-16$\times$ speedup with a small accuracy drop.
</p>
<a href="http://arxiv.org/abs/2010.12812" target="_blank">arXiv:2010.12812</a> [<a href="http://arxiv.org/pdf/2010.12812" target="_blank">pdf</a>]

<h2>Keyphrase Extraction with Dynamic Graph Convolutional Networks and Diversified Inference. (arXiv:2010.12828v1 [cs.CL])</h2>
<h3>Haoyu Zhang, Dingkun Long, Guangwei Xu, Pengjun Xie, Fei Huang, Ji Wang</h3>
<p>Keyphrase extraction (KE) aims to summarize a set of phrases that accurately
express a concept or a topic covered in a given document. Recently,
Sequence-to-Sequence (Seq2Seq) based generative framework is widely used in KE
task, and it has obtained competitive performance on various benchmarks. The
main challenges of Seq2Seq methods lie in acquiring informative latent document
representation and better modeling the compositionality of the target
keyphrases set, which will directly affect the quality of generated keyphrases.
In this paper, we propose to adopt the Dynamic Graph Convolutional Networks
(DGCN) to solve the above two problems simultaneously. Concretely, we explore
to integrate dependency trees with GCN for latent representation learning.
Moreover, the graph structure in our model is dynamically modified during the
learning process according to the generated keyphrases. To this end, our
approach is able to explicitly learn the relations within the keyphrases
collection and guarantee the information interchange between encoder and
decoder in both directions. Extensive experiments on various KE benchmark
datasets demonstrate the effectiveness of our approach.
</p>
<a href="http://arxiv.org/abs/2010.12828" target="_blank">arXiv:2010.12828</a> [<a href="http://arxiv.org/pdf/2010.12828" target="_blank">pdf</a>]

<h2>Cross-Modal Transfer Learning for Multilingual Speech-to-Text Translation. (arXiv:2010.12829v1 [cs.CL])</h2>
<h3>Chau Tran, Changhan Wang, Yuqing Tang, Yun Tang, Juan Pino, Xian Li</h3>
<p>We propose an effective approach to utilize pretrained speech and text models
to perform speech-to-text translation (ST). Our recipe to achieve cross-modal
and cross-lingual transfer learning (XMTL) is simple and generalizable: using
an adaptor module to bridge the modules pretrained in different modalities, and
an efficient finetuning step which leverages the knowledge from pretrained
modules yet making it work on a drastically different downstream task. With
this approach, we built a multilingual speech-to-text translation model with
pretrained audio encoder (wav2vec) and multilingual text decoder (mBART), which
achieves new state-of-the-art on CoVoST 2 ST benchmark [1] for English into 15
languages as well as 6 Romance languages into English with on average +2.8 BLEU
and +3.9 BLEU, respectively. On low-resource languages (with less than 10 hours
training data), our approach significantly improves the quality of
speech-to-text translation with +9.0 BLEU on Portuguese-English and +5.2 BLEU
on Dutch-English.
</p>
<a href="http://arxiv.org/abs/2010.12829" target="_blank">arXiv:2010.12829</a> [<a href="http://arxiv.org/pdf/2010.12829" target="_blank">pdf</a>]

<h2>Weakly-supervised VisualBERT: Pre-training without Parallel Images and Captions. (arXiv:2010.12831v1 [cs.CL])</h2>
<h3>Liunian Harold Li, Haoxuan You, Zhecan Wang, Alireza Zareian, Shih-Fu Chang, Kai-Wei Chang</h3>
<p>Pre-trained contextual vision-and-language (V&amp;L) models have brought
impressive performance improvement on various benchmarks. However, the paired
text-image data required for pre-training are hard to collect and scale up. We
investigate if a strong V&amp;L representation model can be learned without
text-image pairs. We propose Weakly-supervised VisualBERT with the key idea of
conducting "mask-and-predict" pre-training on language-only and image-only
corpora. Additionally, we introduce the object tags detected by an object
recognition model as anchor points to bridge two modalities. Evaluation on four
V&amp;L benchmarks shows that Weakly-supervised VisualBERT achieves similar
performance with a model pre-trained with paired data. Besides, pre-training on
more image-only data further improves a model that already has access to
aligned data, suggesting the possibility of utilizing billions of raw images
available to enhance V&amp;L models.
</p>
<a href="http://arxiv.org/abs/2010.12831" target="_blank">arXiv:2010.12831</a> [<a href="http://arxiv.org/pdf/2010.12831" target="_blank">pdf</a>]

<h2>Unclicked User Behaviors Enhanced SequentialRecommendation. (arXiv:2010.12837v1 [cs.IR])</h2>
<h3>Fuyu Lv, Mengxue Li, Tonglei Guo, Changlong Yu, Fei Sun, Taiwei Jin, Keping Yang</h3>
<p>Deep learning-based sequential recommender systems have recently attracted
increasing attention from both academia and industry. Among them, how to
comprehensively capture sequential user interest is a fundamental problem.
However, most existing sequential recommendation models take as input clicked
or purchased behavior sequences from user-item interactions. This leads to
incomprehensive user representation and sub-optimal model performance, since
they ignore the complete user behavior exposure data, i.e., impressed yet
unclicked items. In this work, we attempt to incorporate and model those
unclicked item sequences using a new learning approach in order to explore
better sequential recommendation technique. An efficient triplet metric
learning algorithm is proposed to appropriately learn the representation of
unclicked items. Our method can be simply integrated with existing sequential
recommendation models by a confidence fusion network and further gain better
user representation. We name our algorithm SRU2B (short for Sequential
Recommendation with Unclicked User Behaviors). The experimental results based
on real-world E-commerce data demonstrate the effectiveness of SRU2B and verify
the importance of unclicked items in sequential recommendation.
</p>
<a href="http://arxiv.org/abs/2010.12837" target="_blank">arXiv:2010.12837</a> [<a href="http://arxiv.org/pdf/2010.12837" target="_blank">pdf</a>]

<h2>Stochastic Gradient Descent Meets Distribution Regression. (arXiv:2010.12842v1 [stat.ML])</h2>
<h3>Nicole M&#xfc;cke</h3>
<p>Stochastic gradient descent (SGD) provides a simple and efficient way to
solve a broad range of machine learning problems. Here, we focus on
distribution regression (DR), involving two stages of sampling: Firstly, we
regress from probability measures to real-valued responses. Secondly, we sample
bags from these distributions for utilizing them to solve the overall
regression problem. Recently, DR has been tackled by applying kernel ridge
regression and the learning properties of this approach are well understood.
However, nothing is known about the learning properties of SGD for two stage
sampling problems. We fill this gap and provide theoretical guarantees for the
performance of SGD for DR. Our bounds are optimal in a mini-max sense under
standard assumptions.
</p>
<a href="http://arxiv.org/abs/2010.12842" target="_blank">arXiv:2010.12842</a> [<a href="http://arxiv.org/pdf/2010.12842" target="_blank">pdf</a>]

<h2>Towards Benchmark Datasets for Machine Learning Based Website Phishing Detection: An experimental study. (arXiv:2010.12847v1 [cs.CR])</h2>
<h3>Abdelhakim Hannousse, Salima Yahiouche</h3>
<p>In this paper, we present a general scheme for building reproducible and
extensible datasets for website phishing detection. The aim is to (1) enable
comparison of systems using different features, (2) overtake the short-lived
nature of phishing websites, and (3) keep track of the evolution of phishing
tactics. For experimenting the proposed scheme, we start by adopting a refined
classification of website phishing features and we systematically select a
total of 87 commonly recognized ones, we classify them, and we made them
subjects for relevance and runtime analysis. We use the collected set of
features to build a dataset in light of the proposed scheme. Thereafter, we use
a conceptual replication approach to check the genericity of former findings
for the built dataset. Specifically, we evaluate the performance of classifiers
on individual classes and on combinations of classes, we investigate different
combinations of models, and we explore the effects of filter and wrapper
methods on the selection of discriminative features. The results show that
Random Forest is the most predictive classifier. Features gathered from
external services are found the most discriminative where features extracted
from web page contents are found less distinguishing. Besides external service
based features, some web page content features are found time consuming and not
suitable for runtime detection. The use of hybrid features provided the best
accuracy score of 96.61%. By investigating different feature selection methods,
filter-based ranking together with incremental removal of less important
features improved the performance up to 96.83% better than wrapper methods.
</p>
<a href="http://arxiv.org/abs/2010.12847" target="_blank">arXiv:2010.12847</a> [<a href="http://arxiv.org/pdf/2010.12847" target="_blank">pdf</a>]

<h2>ReadOnce Transformers: Reusable Representations of Text for Transformers. (arXiv:2010.12854v1 [cs.CL])</h2>
<h3>Shih-Ting Lin, Ashish Sabharwal, Tushar Khot</h3>
<p>While large-scale language models are extremely effective when directly
fine-tuned on many end-tasks, such models learn to extract information and
solve the task simultaneously from end-task supervision. This is wasteful, as
the general problem of gathering information from a document is mostly
task-independent and need not be re-learned from scratch each time. Moreover,
once the information has been captured in a computable representation, it can
now be re-used across examples, leading to faster training and evaluation of
models. We present a transformer-based approach, ReadOnce Transformers, that is
trained to build such information-capturing representations of text. Our model
compresses the document into a variable-length task-independent representation
that can now be re-used in different examples and tasks, thereby requiring a
document to only be read once. Additionally, we extend standard text-to-text
models to consume our ReadOnce Representations along with text to solve
multiple downstream tasks. We show our task-independent representations can be
used for multi-hop QA, abstractive QA, and summarization. We observe 2x-5x
speedups compared to standard text-to-text models, while also being able to
handle long documents that would normally exceed the length limit of current
models.
</p>
<a href="http://arxiv.org/abs/2010.12854" target="_blank">arXiv:2010.12854</a> [<a href="http://arxiv.org/pdf/2010.12854" target="_blank">pdf</a>]

<h2>When Being Unseen from mBERT is just the Beginning: Handling New Languages With Multilingual Language Models. (arXiv:2010.12858v1 [cs.CL])</h2>
<h3>Benjamin Muller, Antonis Anastasopoulos, Beno&#xee;t Sagot, Djam&#xe9; Seddah</h3>
<p>Transfer learning based on pretraining language models on a large amount of
raw data has become a new norm to reach state-of-the-art performance in NLP.
Still, it remains unclear how this approach should be applied for unseen
languages that are not covered by any available large-scale multilingual
language model and for which only a small amount of raw data is generally
available. In this work, by comparing multilingual and monolingual models, we
show that such models behave in multiple ways on unseen languages. Some
languages greatly benefit from transfer learning and behave similarly to
closely related high resource languages whereas others apparently do not.
Focusing on the latter, we show that this failure to transfer is largely
related to the impact of the script used to write such languages.
Transliterating those languages improves very significantly the ability of
large-scale multilingual language models on downstream tasks.
</p>
<a href="http://arxiv.org/abs/2010.12858" target="_blank">arXiv:2010.12858</a> [<a href="http://arxiv.org/pdf/2010.12858" target="_blank">pdf</a>]

<h2>Stable ResNet. (arXiv:2010.12859v1 [cs.LG])</h2>
<h3>Soufiane Hayou, Eugenio Clerico, Bobby He, George Deligiannidis, Arnaud Doucet, Judith Rousseau</h3>
<p>Deep ResNet architectures have achieved state of the art performance on many
tasks. While they solve the problem of gradient vanishing, they might suffer
from gradient exploding as the depth becomes large (Yang et al. 2017).
Moreover, recent results have shown that ResNet might lose expressivity as the
depth goes to infinity (Yang et al. 2017, Hayou et al. 2019). To resolve these
issues, we introduce a new class of ResNet architectures, called Stable ResNet,
that have the property of stabilizing the gradient while ensuring expressivity
in the infinite depth limit.
</p>
<a href="http://arxiv.org/abs/2010.12859" target="_blank">arXiv:2010.12859</a> [<a href="http://arxiv.org/pdf/2010.12859" target="_blank">pdf</a>]

<h2>MARS: Multi-macro Architecture SRAM CIM-Based Accelerator with Co-designed Compressed Neural Networks. (arXiv:2010.12861v1 [cs.AR])</h2>
<h3>Syuan-Hao Sie, Jye-Luen Lee, Yi-Ren Chen, Chih-Cheng Lu, Chih-Cheng Hsieh, Meng-Fan Chang, Kea-Tiong Tang</h3>
<p>Convolutional neural networks (CNNs) play a key role in deep learning
applications. However, the large storage overheads and the substantial
computation cost of CNNs are problematic in hardware accelerators.
Computing-in-memory (CIM) architecture has demonstrated great potential to
effectively compute large-scale matrix-vector multiplication. However, the
intensive multiply and accumulation (MAC) operations executed at the crossbar
array and the limited capacity of CIM macros remain bottlenecks for further
improvement of energy efficiency and throughput. To reduce computation costs,
network pruning and quantization are two widely studied compression methods to
shrink the model size. However, most of the model compression algorithms can
only be implemented in digital-based CNN accelerators. For implementation in a
static random access memory (SRAM) CIM-based accelerator, the model compression
algorithm must consider the hardware limitations of CIM macros, such as the
number of word lines and bit lines that can be turned on at the same time, as
well as how to map the weight to the SRAM CIM macro. In this study, a software
and hardware co-design approach is proposed to design an SRAM CIM-based CNN
accelerator and an SRAM CIM-aware model compression algorithm. To lessen the
high-precision MAC required by batch normalization (BN), a quantization
algorithm that can fuse BN into the weights is proposed. Furthermore, to reduce
the number of network parameters, a sparsity algorithm that considers a CIM
architecture is proposed. Last, MARS, a CIM-based CNN accelerator that can
utilize multiple SRAM CIM macros as processing units and support a sparsity
neural network, is proposed.
</p>
<a href="http://arxiv.org/abs/2010.12861" target="_blank">arXiv:2010.12861</a> [<a href="http://arxiv.org/pdf/2010.12861" target="_blank">pdf</a>]

<h2>Efficiently Mitigating Classification Bias via Transfer Learning. (arXiv:2010.12864v1 [cs.CL])</h2>
<h3>Xisen Jin, Francesco Barbieri, Aida Mostafazadeh Davani, Brendan Kennedy, Leonardo Neves, Xiang Ren</h3>
<p>Prediction bias in machine learning models refers to unintended model
behaviors that discriminate against inputs mentioning or produced by certain
groups; for example, hate speech classifiers predict more false positives for
neutral text mentioning specific social groups. Mitigating bias for each task
or domain is inefficient, as it requires repetitive model training, data
annotation (e.g., demographic information), and evaluation. In pursuit of a
more accessible solution, we propose the Upstream Bias Mitigation for
Downstream Fine-Tuning (UBM) framework, which mitigate one or multiple bias
factors in downstream classifiers by transfer learning from an upstream model.
In the upstream bias mitigation stage, explanation regularization and
adversarial training are applied to mitigate multiple bias factors. In the
downstream fine-tuning stage, the classifier layer of the model is
re-initialized, and the entire model is fine-tuned to downstream tasks in
potentially novel domains without any further bias mitigation. We expect
downstream classifiers to be less biased by transfer learning from de-biased
upstream models. We conduct extensive experiments varying the similarity
between the source and target data, as well as varying the number of dimensions
of bias (e.g., discrimination against specific social groups or dialects). Our
results indicate the proposed UBM framework can effectively reduce bias in
downstream classifiers.
</p>
<a href="http://arxiv.org/abs/2010.12864" target="_blank">arXiv:2010.12864</a> [<a href="http://arxiv.org/pdf/2010.12864" target="_blank">pdf</a>]

<h2>Multi-Task Learning with Shared Encoder for Non-Autoregressive Machine Translation. (arXiv:2010.12868v1 [cs.CL])</h2>
<h3>Yongchang Hao, Shilin He, Wenxiang Jiao, Zhaopeng Tu, Michael Lyu, Xing Wang</h3>
<p>Non-Autoregressive machine Translation (NAT) models have demonstrated
significant inference speedup but suffer from inferior translation accuracy.
The common practice to tackle the problem is transferring the Autoregressive
machine Translation (AT) knowledge to NAT models, e.g., with knowledge
distillation. In this work, we hypothesize and empirically verify that AT and
NAT encoders capture different linguistic properties and representations of
source sentences. Therefore, we propose to adopt the multi-task learning to
transfer the AT knowledge to NAT models through the encoder sharing.
Specifically, we take the AT model as an auxiliary task to enhance NAT model
performance. Experimental results on WMT14 English-&gt;German and WMT16
English-&gt;Romanian datasets show that the proposed multi-task NAT achieves
significant improvements over the baseline NAT models. In addition,
experimental results demonstrate that our multi-task NAT is complementary to
the standard knowledge transfer method, knowledge distillation. Code is
publicly available at https://github.com/yongchanghao/multi-task-nat
</p>
<a href="http://arxiv.org/abs/2010.12868" target="_blank">arXiv:2010.12868</a> [<a href="http://arxiv.org/pdf/2010.12868" target="_blank">pdf</a>]

<h2>ExPAN(N)D: Exploring Posits for Efficient Artificial Neural Network Design in FPGA-based Systems. (arXiv:2010.12869v1 [cs.AR])</h2>
<h3>Suresh Nambi, Salim Ullah, Aditya Lohana, Siva Satyendra Sahoo, Farhad Merchant, Akash Kumar</h3>
<p>The recent advances in machine learning, in general, and Artificial Neural
Networks (ANN), in particular, has made smart embedded systems an attractive
option for a larger number of application areas. However, the high
computational complexity, memory footprints, and energy requirements of machine
learning models hinder their deployment on resource-constrained embedded
systems. Most state-of-the-art works have considered this problem by proposing
various low bit-width data representation schemes, optimized arithmetic
operators' implementations, and different complexity reduction techniques such
as network pruning. To further elevate the implementation gains offered by
these individual techniques, there is a need to cross-examine and combine these
techniques' unique features. This paper presents ExPAN(N)D, a framework to
analyze and ingather the efficacy of the \textit{Posit} number representation
scheme and the efficiency of \textit{fixed-point} arithmetic implementations
for ANNs. The Posit scheme offers a better dynamic range and higher precision
for various applications than IEEE $754$ single-precision floating-point
format. However, due to the dynamic nature of the various fields of the Posit
scheme, the corresponding arithmetic circuits have higher critical path delay
and resource requirements than the single-precision-based arithmetic units.
Towards this end, we propose a novel \textit{Posit to fixed-point converter}
for enabling high-performance and energy-efficient hardware implementations for
ANNs with minimal drop in the output accuracy. We also propose a modified
Posit-based representation to store the trained parameters of a network.
Compared to an $8$-bit fixed-point-based inference accelerator, our proposed
implementation offers $\approx46\%$ and $\approx18\%$ reductions in the storage
requirements of the parameters and energy consumption of the MAC units,
respectively.
</p>
<a href="http://arxiv.org/abs/2010.12869" target="_blank">arXiv:2010.12869</a> [<a href="http://arxiv.org/pdf/2010.12869" target="_blank">pdf</a>]

<h2>Efficient Learning in Non-Stationary Linear Markov Decision Processes. (arXiv:2010.12870v1 [cs.LG])</h2>
<h3>Ahmed Touati, Pascal Vincent</h3>
<p>We study episodic reinforcement learning in non-stationary linear (a.k.a.
low-rank) Markov Decision Processes (MDPs), i.e, both the reward and transition
kernel are linear with respect to a given feature map and are allowed to evolve
either slowly or abruptly over time. For this problem setting, we propose
OPT-WLSVI an optimistic model-free algorithm based on weighted least squares
value iteration which uses exponential weights to smoothly forget data that are
far in the past. We show that our algorithm, when competing against the best
policy at each time, achieves a regret that is upped bounded by
$\widetilde{\mathcal{O}}(d^{7/6}H^2 \Delta^{1/3} K^{2/3})$ where $d$ is the
dimension of the feature space, $H$ is the planning horizon, $K$ is the number
of episodes and $\Delta$ is a suitable measure of non-stationarity of the MDP.
This is the first regret bound for non-stationary reinforcement learning with
linear function approximation.
</p>
<a href="http://arxiv.org/abs/2010.12870" target="_blank">arXiv:2010.12870</a> [<a href="http://arxiv.org/pdf/2010.12870" target="_blank">pdf</a>]

<h2>Large Scale Legal Text Classification Using Transformer Models. (arXiv:2010.12871v1 [cs.CL])</h2>
<h3>Zein Shaheen, Gerhard Wohlgenannt, Erwin Filtz</h3>
<p>Large multi-label text classification is a challenging Natural Language
Processing (NLP) problem that is concerned with text classification for
datasets with thousands of labels. We tackle this problem in the legal domain,
where datasets, such as JRC-Acquis and EURLEX57K labeled with the EuroVoc
vocabulary were created within the legal information systems of the European
Union. The EuroVoc taxonomy includes around 7000 concepts. In this work, we
study the performance of various recent transformer-based models in combination
with strategies such as generative pretraining, gradual unfreezing and
discriminative learning rates in order to reach competitive classification
performance, and present new state-of-the-art results of 0.661 (F1) for
JRC-Acquis and 0.754 for EURLEX57K. Furthermore, we quantify the impact of
individual steps, such as language model fine-tuning or gradual unfreezing in
an ablation study, and provide reference dataset splits created with an
iterative stratification algorithm.
</p>
<a href="http://arxiv.org/abs/2010.12871" target="_blank">arXiv:2010.12871</a> [<a href="http://arxiv.org/pdf/2010.12871" target="_blank">pdf</a>]

<h2>Learning to Deceive Knowledge Graph Augmented Models via Targeted Perturbation. (arXiv:2010.12872v1 [cs.CL])</h2>
<h3>Mrigank Raman, Siddhant Agarwal, Peifeng Wang, Aaron Chan, Hansen Wang, Sungchul Kim, Ryan Rossi, Handong Zhao, Nedim Lipka, Xiang Ren</h3>
<p>Symbolic knowledge (e.g., entities, relations, and facts in a knowledge
graph) has become an increasingly popular component of neural-symbolic models
applied to machine learning tasks, such as question answering and recommender
systems. Besides improving downstream performance, these symbolic structures
(and their associated attention weights) are often used to help explain the
model's predictions and provide "insights" to practitioners. In this paper, we
question the faithfulness of such symbolic explanations. We demonstrate that,
through a learned strategy (or even simple heuristics), one can produce
deceptively perturbed symbolic structures which maintain the downstream
performance of the original structure while significantly deviating from the
original semantics. In particular, we train a reinforcement learning policy to
manipulate relation types or edge connections in a knowledge graph, such that
the resulting downstream performance is maximally preserved. Across multiple
models and tasks, our approach drastically alters knowledge graphs with little
to no drop in performance. These results raise doubts about the faithfulness of
explanations provided by learned symbolic structures and the reliability of
current neural-symbolic models in leveraging symbolic knowledge.
</p>
<a href="http://arxiv.org/abs/2010.12872" target="_blank">arXiv:2010.12872</a> [<a href="http://arxiv.org/pdf/2010.12872" target="_blank">pdf</a>]

<h2>Learning Contextualized Knowledge Structures for Commonsense Reasoning. (arXiv:2010.12873v1 [cs.CL])</h2>
<h3>Jun Yan, Mrigank Raman, Tianyu Zhang, Ryan Rossi, Handong Zhao, Sungchul Kim, Nedim Lipka, Xiang Ren</h3>
<p>Recently, neural-symbolic architectures have achieved success on commonsense
reasoning through effectively encoding relational structures retrieved from
external knowledge graphs (KGs) and obtained state-of-the-art results in tasks
such as (commonsense) question answering and natural language inference.
However, these methods rely on quality and contextualized knowledge structures
(i.e., fact triples) that are retrieved at the pre-processing stage but
overlook challenges caused by incompleteness of a KG, limited expressiveness of
its relations, and retrieved facts irrelevant to the reasoning context. In this
paper, we present a novel neural-symbolic model, named Hybrid Graph Network
(HGN), which jointly generates feature representations for new triples (as a
complement to existing edges in the KG), determines the relevance of the
triples to the reasoning context, and learns graph module parameters for
encoding the relational information. Our model learns a compact graph structure
(comprising both extracted and generated edges) through filtering edges that
are unhelpful to the reasoning process. We show marked improvement on three
commonsense reasoning benchmarks and demonstrate the superiority of the learned
graph structures with user studies.
</p>
<a href="http://arxiv.org/abs/2010.12873" target="_blank">arXiv:2010.12873</a> [<a href="http://arxiv.org/pdf/2010.12873" target="_blank">pdf</a>]

<h2>Electromagnetic Source Imaging via a Data-Synthesis-Based Denoising Autoencoder. (arXiv:2010.12876v1 [eess.IV])</h2>
<h3>Gexin Huang, Zhu Liang Yu, Wei Wu, Ke Liu, Zheng Hui Gu, Feifei Qi, YuanQing Li, Jiawen Liang</h3>
<p>Electromagnetic source imaging (ESI) is a highly ill-posed inverse problem.
To find a unique solution, traditional ESI methods impose a variety of priors
that may not reflect the actual source properties. Such limitations of
traditional ESI methods hinder their further applications. Inspired by deep
learning approaches, a novel data-synthesized spatio-temporal denoising
autoencoder method (DST-DAE) method was proposed to solve the ESI inverse
problem. Unlike the traditional methods, we utilize a neural network to
directly seek generalized mapping from the measured E/MEG signals to the
cortical sources. A novel data synthesis strategy is employed by introducing
the prior information of sources to the generated large-scale samples using the
forward model of ESI. All the generated data are used to drive the neural
network to automatically learn inverse mapping. To achieve better estimation
performance, a denoising autoencoder (DAE) architecture with spatio-temporal
feature extraction blocks is designed. Compared with the traditional methods,
we show (1) that the novel deep learning approach provides an effective and
easy-to-apply way to solve the ESI problem, that (2) compared to traditional
methods, DST-DAE with the data synthesis strategy can better consider the
characteristics of real sources than the mathematical formulation of prior
assumptions, and that (3) the specifically designed architecture of DAE can not
only provide a better estimation of source signals but also be robust to noise
pollution. Extensive numerical experiments show that the proposed method is
superior to the traditional knowledge-driven ESI methods.
</p>
<a href="http://arxiv.org/abs/2010.12876" target="_blank">arXiv:2010.12876</a> [<a href="http://arxiv.org/pdf/2010.12876" target="_blank">pdf</a>]

<h2>EEGsig machine learning-based toolbox for End-to-End EEG signal processing. (arXiv:2010.12877v1 [eess.SP])</h2>
<h3>Fardin Ghorbani, Soheil Hashemi, Ali Abdolali, Mohammad Soleimani</h3>
<p>In the quest to realize comprehensive EEG signal processing toolbox, in this
paper, we demonstrate the first toolbox contain three states of EEG signal
processing (preprocessing, feature extraction, classification) together. Our
goal is to provide a comprehensive toolbox for EEG signal processing. Using
MATLAB software, we have developed an open-source toolbox for end-to-end
processing of the EEG signal. As we know, in many research work in the field of
neuroscience and EEG signal processing, we first clear the signal and remove
noise, artifact, etc. Which we know as preprocessing, and then extract the
feature from the relevant signal, and finally Machine learning classifiers used
to classification of signal. We have tried to provide all the above steps in
the form of EEGsig as a graphical user interface(GUI) so that there is no need
for programming for all the above steps and reduce the time to complete these
projects to a desirable level.
</p>
<a href="http://arxiv.org/abs/2010.12877" target="_blank">arXiv:2010.12877</a> [<a href="http://arxiv.org/pdf/2010.12877" target="_blank">pdf</a>]

<h2>Pathfinder Discovery Networks for Neural Message Passing. (arXiv:2010.12878v1 [cs.LG])</h2>
<h3>Benedek Rozemberczki, Peter Englert, Amol Kapoor, Martin Blais, Bryan Perozzi</h3>
<p>In this work we propose Pathfinder Discovery Networks (PDNs), a method for
jointly learning a message passing graph over a multiplex network with a
downstream semi-supervised model. PDNs inductively learn an aggregated weight
for each edge, optimized to produce the best outcome for the downstream
learning task. PDNs are a generalization of attention mechanisms on graphs
which allow flexible construction of similarity functions between nodes, edge
convolutions, and cheap multiscale mixing layers. We show that PDNs overcome
weaknesses of existing methods for graph attention (e.g. Graph Attention
Networks), such as the diminishing weight problem. Our experimental results
demonstrate competitive predictive performance on academic node classification
tasks. Additional results from a challenging suite of node classification
experiments show how PDNs can learn a wider class of functions than existing
baselines. We analyze the relative computational complexity of PDNs, and show
that PDN runtime is not considerably higher than static-graph models. Finally,
we discuss how PDNs can be used to construct an easily interpretable attention
mechanism that allows users to understand information propagation in the graph.
</p>
<a href="http://arxiv.org/abs/2010.12878" target="_blank">arXiv:2010.12878</a> [<a href="http://arxiv.org/pdf/2010.12878" target="_blank">pdf</a>]

<h2>Persian Handwritten Digit, Character, and Words Recognition by Using Deep Learning Methods. (arXiv:2010.12880v1 [cs.CV])</h2>
<h3>Mehdi Bonyani, Simindokht Jahangard</h3>
<p>Digit, character, and word recognition of a particular script play a key role
in the field of pattern recognition. These days, Optical Character Recognition
(OCR) systems are widely used in commercial market in various applications. In
recent years, there are intensive research studies on optical character, digit,
and word recognition. However, only a limited number of works are offered for
numeral, character, and word recognition of Persian scripts. In this paper, we
have used deep neural network and investigated different versions of DensNet
models and Xception and compare our results with the state-of-the-art methods
and approaches in recognizing Persian character, number, and word. Two holistic
Persian handwritten datasets, HODA and Sadri, have been used. For a comparison
of our proposed deep neural network with previously published research studies,
the best state-of-the-art results have been considered. We used accuracy as our
criteria for evaluation. For HODA dataset, we achieved 99.72% and 89.99% for
digit and character, respectively. For Sadri dataset, we obtained accuracy
rates of 99.72%, 98.32%, and 98.82% for digit, character, and words,
respectively.
</p>
<a href="http://arxiv.org/abs/2010.12880" target="_blank">arXiv:2010.12880</a> [<a href="http://arxiv.org/pdf/2010.12880" target="_blank">pdf</a>]

<h2>FedE: Embedding Knowledge Graphs in Federated Setting. (arXiv:2010.12882v1 [cs.CL])</h2>
<h3>Mingyang Chen, Wen Zhang, Zonggang Yuan, Yantao Jia, Huajun Chen</h3>
<p>Knowledge graphs (KGs) consisting of triples are always incomplete, so it's
important to do Knowledge Graph Completion (KGC) by predicting missing triples.
Multi-Source KG is a common situation in real KG applications which can be
viewed as a set of related individual KGs where different KGs contains
relations of different aspects of entities. It's intuitive that, for each
individual KG, its completion could be greatly contributed by the triples
defined and labeled in other ones. However, because of the data privacy and
sensitivity, a set of relevant knowledge graphs cannot complement each other's
KGC by just collecting data from different knowledge graphs together.
Therefore, in this paper, we introduce federated setting to keep their privacy
without triple transferring between KGs and apply it in embedding knowledge
graph, a typical method which have proven effective for KGC in the past decade.
We propose a Federated Knowledge Graph Embedding framework FedE, focusing on
learning knowledge graph embeddings by aggregating locally-computed updates.
Finally, we conduct extensive experiments on datasets derived from KGE
benchmark datasets and results show the effectiveness of our proposed FedE.
</p>
<a href="http://arxiv.org/abs/2010.12882" target="_blank">arXiv:2010.12882</a> [<a href="http://arxiv.org/pdf/2010.12882" target="_blank">pdf</a>]

<h2>Variational Bayesian Unlearning. (arXiv:2010.12883v1 [cs.LG])</h2>
<h3>Quoc Phong Nguyen, Bryan Kian Hsiang Low, Patrick Jaillet</h3>
<p>This paper studies the problem of approximately unlearning a Bayesian model
from a small subset of the training data to be erased. We frame this problem as
one of minimizing the Kullback-Leibler divergence between the approximate
posterior belief of model parameters after directly unlearning from erased data
vs. the exact posterior belief from retraining with remaining data. Using the
variational inference (VI) framework, we show that it is equivalent to
minimizing an evidence upper bound which trades off between fully unlearning
from erased data vs. not entirely forgetting the posterior belief given the
full data (i.e., including the remaining data); the latter prevents
catastrophic unlearning that can render the model useless. In model training
with VI, only an approximate (instead of exact) posterior belief given the full
data can be obtained, which makes unlearning even more challenging. We propose
two novel tricks to tackle this challenge. We empirically demonstrate our
unlearning methods on Bayesian models such as sparse Gaussian process and
logistic regression using synthetic and real-world datasets.
</p>
<a href="http://arxiv.org/abs/2010.12883" target="_blank">arXiv:2010.12883</a> [<a href="http://arxiv.org/pdf/2010.12883" target="_blank">pdf</a>]

<h2>Abduction and Argumentation for Explainable Machine Learning: A Position Survey. (arXiv:2010.12896v1 [cs.AI])</h2>
<h3>Antonis Kakas, Loizos Michael</h3>
<p>This paper presents Abduction and Argumentation as two principled forms for
reasoning, and fleshes out the fundamental role that they can play within
Machine Learning. It reviews the state-of-the-art work over the past few
decades on the link of these two reasoning forms with machine learning work,
and from this it elaborates on how the explanation-generating role of Abduction
and Argumentation makes them naturally-fitting mechanisms for the development
of Explainable Machine Learning and AI systems. Abduction contributes towards
this goal by facilitating learning through the transformation, preparation, and
homogenization of data. Argumentation, as a conservative extension of classical
deductive reasoning, offers a flexible prediction and coverage mechanism for
learning -- an associated target language for learned knowledge -- that
explicitly acknowledges the need to deal, in the context of learning, with
uncertain, incomplete and inconsistent data that are incompatible with any
classically-represented logical theory.
</p>
<a href="http://arxiv.org/abs/2010.12896" target="_blank">arXiv:2010.12896</a> [<a href="http://arxiv.org/pdf/2010.12896" target="_blank">pdf</a>]

<h2>Adaptive In-network Collaborative Caching for Enhanced Ensemble Deep Learning at Edge. (arXiv:2010.12899v1 [cs.NI])</h2>
<h3>Yana Qin, Danye Wu, Zhiwei Xu, Jie Tian, Yujun Zhang</h3>
<p>To enhance the quality and speed of data processing and protect the privacy
and security of the data, edge computing has been extensively applied to
support data-intensive intelligent processing services at edge. Among these
data-intensive services, ensemble learning-based services can in natural
leverage the distributed computation and storage resources at edge devices to
achieve efficient data collection, processing, analysis.

Collaborative caching has been applied in edge computing to support services
close to the data source, in order to take the limited resources at edge
devices to support high-performance ensemble learning solutions. To achieve
this goal, we propose an adaptive in-network collaborative caching scheme for
ensemble learning at edge. First, an efficient data representation structure is
proposed to record cached data among different nodes. In addition, we design a
collaboration scheme to facilitate edge nodes to cache valuable data for local
ensemble learning, by scheduling local caching according to a summarization of
data representations from different edge nodes. Our extensive simulations
demonstrate the high performance of the proposed collaborative caching scheme,
which significantly reduces the learning latency and the transmission overhead.
</p>
<a href="http://arxiv.org/abs/2010.12899" target="_blank">arXiv:2010.12899</a> [<a href="http://arxiv.org/pdf/2010.12899" target="_blank">pdf</a>]

<h2>ATRO: Adversarial Training with a Rejection Option. (arXiv:2010.12905v1 [cs.LG])</h2>
<h3>Masahiro Kato, Zhenghang Cui, Yoshihiro Fukuhara</h3>
<p>This paper proposes a classification framework with a rejection option to
mitigate the performance deterioration caused by adversarial examples. While
recent machine learning algorithms achieve high prediction performance, they
are empirically vulnerable to adversarial examples, which are slightly
perturbed data samples that are wrongly classified. In real-world applications,
adversarial attacks using such adversarial examples could cause serious
problems. To this end, various methods are proposed to obtain a classifier that
is robust against adversarial examples. Adversarial training is one of them,
which trains a classifier to minimize the worst-case loss under adversarial
attacks. In this paper, in order to acquire a more reliable classifier against
adversarial attacks, we propose the method of Adversarial Training with a
Rejection Option (ATRO). Applying the adversarial training objective to both a
classifier and a rejection function simultaneously, classifiers trained by ATRO
can choose to abstain from classification when it has insufficient confidence
to classify a test data point. We examine the feasibility of the framework
using the surrogate maximum hinge loss and establish a generalization bound for
linear models. Furthermore, we empirically confirmed the effectiveness of ATRO
using various models and real-world datasets.
</p>
<a href="http://arxiv.org/abs/2010.12905" target="_blank">arXiv:2010.12905</a> [<a href="http://arxiv.org/pdf/2010.12905" target="_blank">pdf</a>]

<h2>Deep Graph Matching and Searching for Semantic Code Retrieval. (arXiv:2010.12908v1 [cs.AI])</h2>
<h3>Xiang Ling, Lingfei Wu, Saizhuo Wang, Gaoning Pan, Tengfei Ma, Fangli Xu, Alex X. Liu, Chunming Wu, Shouling Ji</h3>
<p>Code retrieval is to find the code snippet from a large corpus of source code
repositories that highly matches the query of natural language description.
Recent work mainly uses natural language processing techniques to process both
query texts (i.e., human natural language) and code snippets (i.e., machine
programming language), however neglecting the deep structured features of
natural language query texts and source codes, both of which contain rich
semantic information. In this paper, we propose an end-to-end deep graph
matching and searching (DGMS) model based on graph neural networks for semantic
code retrieval. To this end, we first represent both natural language query
texts and programming language codes with the unified graph-structured data,
and then use the proposed graph matching and searching model to retrieve the
best matching code snippet. In particular, DGMS not only captures more
structural information for individual query texts or code snippets but also
learns the fine-grained similarity between them by a cross-attention based
semantic matching operation. We evaluate the proposed DGMS model on two public
code retrieval datasets from two representative programming languages (i.e.,
Java and Python). The experiment results demonstrate that DGMS significantly
outperforms state-of-the-art baseline models by a large margin on both
datasets. Moreover, our extensive ablation studies systematically investigate
and illustrate the impact of each part of DGMS.
</p>
<a href="http://arxiv.org/abs/2010.12908" target="_blank">arXiv:2010.12908</a> [<a href="http://arxiv.org/pdf/2010.12908" target="_blank">pdf</a>]

<h2>Inductive Bias of Gradient Descent for Exponentially Weight Normalized Smooth Homogeneous Neural Nets. (arXiv:2010.12909v1 [cs.LG])</h2>
<h3>Depen Morwani, Harish G. Ramaswamy</h3>
<p>We analyze the inductive bias of gradient descent for weight normalized
smooth homogeneous neural nets, when trained on exponential or cross-entropy
loss. Our analysis focuses on exponential weight normalization (EWN), which
encourages weight updates along the radial direction. This paper shows that the
gradient flow path with EWN is equivalent to gradient flow on standard networks
with an adaptive learning rate, and hence causes the weights to be updated in a
way that prefers asymptotic relative sparsity. These results can be extended to
hold for gradient descent via an appropriate adaptive learning rate. The
asymptotic convergence rate of the loss in this setting is given by
$\Theta(\frac{1}{t(\log t)^2})$, and is independent of the depth of the
network. We contrast these results with the inductive bias of standard weight
normalization (SWN) and unnormalized architectures, and demonstrate their
implications on synthetic data sets.Experimental results on simple data sets
and architectures support our claim on sparse EWN solutions, even with SGD.
This demonstrates its potential applications in learning prunable neural
networks.
</p>
<a href="http://arxiv.org/abs/2010.12909" target="_blank">arXiv:2010.12909</a> [<a href="http://arxiv.org/pdf/2010.12909" target="_blank">pdf</a>]

<h2>Planning with Exploration: Addressing Dynamics Bottleneck in Model-based Reinforcement Learning. (arXiv:2010.12914v1 [cs.LG])</h2>
<h3>Xiyao Wang, Junge Zhang, Wenzhen Huang, Qiyue Yin</h3>
<p>Model-based reinforcement learning is a framework in which an agent learns an
environment model, makes planning and decision-making in this model, and
finally interacts with the real environment. Model-based reinforcement learning
has high sample efficiency compared with model-free reinforcement learning, and
shows great potential in the real-world application. However, model-based
reinforcement learning has been plagued by dynamics bottleneck. Dynamics
bottleneck is the phenomenon that when the timestep to interact with the
environment increases, the reward of the agent falls into the local optimum
instead of increasing. In this paper, we analyze and explain how the coupling
relationship between model and policy causes the dynamics bottleneck and shows
improving the exploration ability of the agent can alleviate this issue. We
then propose a new planning algorithm called Maximum Entropy Cross-Entropy
Method (MECEM). MECEM can improve the agent's exploration ability by maximizing
the distribution of action entropy in the planning process. We conduct
experiments on fourteen well-recognized benchmark environments such as
HalfCheetah, Ant and Swimmer. The results verify that our approach obtains the
state-of-the-art performance on eleven benchmark environments and can
effectively alleviate dynamics bottleneck on HalfCheetah, Ant and Walker2D.
</p>
<a href="http://arxiv.org/abs/2010.12914" target="_blank">arXiv:2010.12914</a> [<a href="http://arxiv.org/pdf/2010.12914" target="_blank">pdf</a>]

<h2>On Testing of Samplers. (arXiv:2010.12918v1 [cs.DS])</h2>
<h3>Kuldeep S. Meel, Yash Pote, Sourav Chakraborty</h3>
<p>Given a set of items $\mathcal{F}$ and a weight function $\mathtt{wt}:
\mathcal{F} \mapsto (0,1)$, the problem of sampling seeks to sample an item
proportional to its weight. Sampling is a fundamental problem in machine
learning. The daunting computational complexity of sampling with formal
guarantees leads designers to propose heuristics-based techniques for which no
rigorous theoretical analysis exists to quantify the quality of generated
distributions.

This poses a challenge in designing a testing methodology to test whether a
sampler under test generates samples according to a given distribution. Only
recently, Chakraborty and Meel (2019) designed the first scalable verifier,
called Barbarik1, for samplers in the special case when the weight function
$\mathtt{wt}$ is constant, that is, when the sampler is supposed to sample
uniformly from $\mathcal{F}$ . The techniques in Barbarik1, however, fail to
handle general weight functions.

The primary contribution of this paper is an affirmative answer to the above
challenge: motivated by Barbarik1 but using different techniques and analysis,
we design Barbarik2 an algorithm to test whether the distribution generated by
a sampler is $\varepsilon$-close or $\eta$-far from any target distribution. In
contrast to black-box sampling techniques that require a number of samples
proportional to $|\mathcal{F}|$ , Barbarik2 requires only
$\tilde{O}(tilt(\mathtt{wt},\varphi)^2/\eta(\eta - 6\varepsilon)^3)$ samples,
where the $tilt$ is the maximum ratio of weights of two satisfying assignments.
Barbarik2 can handle any arbitrary weight function. We present a prototype
implementation of Barbarik2 and use it to test three state-of-the-art samplers.
</p>
<a href="http://arxiv.org/abs/2010.12918" target="_blank">arXiv:2010.12918</a> [<a href="http://arxiv.org/pdf/2010.12918" target="_blank">pdf</a>]

<h2>Causal Effects of Linguistic Properties. (arXiv:2010.12919v1 [cs.CL])</h2>
<h3>Reid Pryzant, Dallas Card, Dan Jurafsky, Victor Veitch, Dhanya Sridhar</h3>
<p>We consider the problem of estimating the causal effects of linguistic
properties on downstream outcomes. For example, does writing a complaint
politely lead to a faster response time? How much will a positive product
review increase sales? This paper focuses on two challenges related to the
problem. First, we formalize the causal quantity of interest as the effect of a
writer's intent, and establish the assumptions necessary to identify this from
observational data. Second, in practice we only have access to noisy proxies
for these linguistic properties---e.g., predictions from classifiers and
lexicons. We propose an estimator for this setting and prove that its bias is
bounded when we perform an adjustment for the text. The method leverages (1) a
pre-trained language model (BERT) to adjust for the text, and (2) distant
supervision to improve the quality of noisy proxies. We show that our algorithm
produces better causal estimates than related methods on two datasets:
predicting the effect of music review sentiment on sales, and complaint
politeness on response time.
</p>
<a href="http://arxiv.org/abs/2010.12919" target="_blank">arXiv:2010.12919</a> [<a href="http://arxiv.org/pdf/2010.12919" target="_blank">pdf</a>]

<h2>Non-local Meets Global: An Iterative Paradigm for Hyperspectral Image Restoration. (arXiv:2010.12921v1 [eess.IV])</h2>
<h3>Wei He, Quanming Yao, Chao Li, Naoto Yokoya, Qibin Zhao, Hongyan Zhang, Liangpei Zhang</h3>
<p>Non-local low-rank tensor approximation has been developed as a
state-of-the-art method for hyperspectral image (HSI) restoration, which
includes the tasks of denoising, compressed HSI reconstruction and inpainting.
Unfortunately, while its restoration performance benefits from more spectral
bands, its runtime also substantially increases. In this paper, we claim that
the HSI lies in a global spectral low-rank subspace, and the spectral subspaces
of each full band patch group should lie in this global low-rank subspace. This
motivates us to propose a unified paradigm combining the spatial and spectral
properties for HSI restoration. The proposed paradigm enjoys performance
superiority from the non-local spatial denoising and light computation
complexity from the low-rank orthogonal basis exploration. An efficient
alternating minimization algorithm with rank adaptation is developed. It is
done by first solving a fidelity term-related problem for the update of a
latent input image, and then learning a low-dimensional orthogonal basis and
the related reduced image from the latent input image. Subsequently, non-local
low-rank denoising is developed to refine the reduced image and orthogonal
basis iteratively. Finally, the experiments on HSI denoising, compressed
reconstruction, and inpainting tasks, with both simulated and real datasets,
demonstrate its superiority with respect to state-of-the-art HSI restoration
methods.
</p>
<a href="http://arxiv.org/abs/2010.12921" target="_blank">arXiv:2010.12921</a> [<a href="http://arxiv.org/pdf/2010.12921" target="_blank">pdf</a>]

<h2>Disease Normalization with Graph Embeddings. (arXiv:2010.12925v1 [cs.CL])</h2>
<h3>Dhruba Pujary, Camilo Thorne, Wilker Aziz</h3>
<p>The detection and normalization of diseases in biomedical texts are key
biomedical natural language processing tasks. Disease names need not only be
identified, but also normalized or linked to clinical taxonomies describing
diseases such as MeSH. In this paper we describe deep learning methods that
tackle both tasks. We train and test our methods on the known NCBI disease
benchmark corpus. We propose to represent disease names by leveraging MeSH's
graphical structure together with the lexical information available in the
taxonomy using graph embeddings. We also show that combining neural named
entity recognition models with our graph-based entity linking methods via
multitask learning leads to improved disease recognition in the NCBI corpus.
</p>
<a href="http://arxiv.org/abs/2010.12925" target="_blank">arXiv:2010.12925</a> [<a href="http://arxiv.org/pdf/2010.12925" target="_blank">pdf</a>]

<h2>Neural Compound-Word (Sandhi) Generation and Splitting in Sanskrit Language. (arXiv:2010.12940v1 [cs.CL])</h2>
<h3>Sushant Dave, Arun Kumar Singh, Dr. Prathosh A. P., Prof. Brejesh Lall</h3>
<p>This paper describes neural network based approaches to the process of the
formation and splitting of word-compounding, respectively known as the Sandhi
and Vichchhed, in Sanskrit language. Sandhi is an important idea essential to
morphological analysis of Sanskrit texts. Sandhi leads to word transformations
at word boundaries. The rules of Sandhi formation are well defined but complex,
sometimes optional and in some cases, require knowledge about the nature of the
words being compounded. Sandhi split or Vichchhed is an even more difficult
task given its non uniqueness and context dependence. In this work, we propose
the route of formulating the problem as a sequence to sequence prediction task,
using modern deep learning techniques. Being the first fully data driven
technique, we demonstrate that our model has an accuracy better than the
existing methods on multiple standard datasets, despite not using any
additional lexical or morphological resources. The code is being made available
at https://github.com/IITD-DataScience/Sandhi_Prakarana
</p>
<a href="http://arxiv.org/abs/2010.12940" target="_blank">arXiv:2010.12940</a> [<a href="http://arxiv.org/pdf/2010.12940" target="_blank">pdf</a>]

<h2>DeepAtrophy: Teaching a Neural Network to Differentiate Progressive Changes from Noise on Longitudinal MRI in Alzheimer's Disease. (arXiv:2010.12948v1 [cs.LG])</h2>
<h3>Mengjin Dong, Long Xie, Sandhitsu R. Das, Jiancong Wang, Laura E.M. Wisse, Robin deFlores, David A. Wolk, Paul Yushkevich (for the Alzheimer&#x27;s Disease Neuroimaging Initiative)</h3>
<p>Volume change measures derived from longitudinal MRI (e.g. hippocampal
atrophy) are a well-studied biomarker of disease progression in Alzheimer's
Disease (AD) and are used in clinical trials to track the therapeutic efficacy
of disease-modifying treatments. However, longitudinal MRI change measures can
be confounded by non-biological factors, such as different degrees of head
motion and susceptibility artifact between pairs of MRI scans. We hypothesize
that deep learning methods applied directly to pairs of longitudinal MRI scans
can be trained to differentiate between biological changes and non-biological
factors better than conventional approaches based on deformable image
registration. To achieve this, we make a simplifying assumption that biological
factors are associated with time (i.e. the hippocampus shrinks overtime in the
aging population) whereas non-biological factors are independent of time. We
then formulate deep learning networks to infer the temporal order of
same-subject MRI scans input to the network in arbitrary order; as well as to
infer ratios between interscan intervals for two pairs of same-subject MRI
scans. In the test dataset, these networks perform better in tasks of temporal
ordering (89.3%) and interscan interval inference (86.1%) than a
state-of-the-art deformation-based morphometry method ALOHA (76.6% and 76.1%
respectively) (Das et al., 2012). Furthermore, we derive a disease progression
score from the network that is able to detect a group difference between 58
preclinical AD and 75 beta-amyloid-negative cognitively normal individuals
within one year, compared to two years for ALOHA. This suggests that deep
learning can be trained to differentiate MRI changes due to biological factors
(tissue loss) from changes due to non-biological factors, leading to novel
biomarkers that are more sensitive to longitudinal changes at the earliest
stages of AD.
</p>
<a href="http://arxiv.org/abs/2010.12948" target="_blank">arXiv:2010.12948</a> [<a href="http://arxiv.org/pdf/2010.12948" target="_blank">pdf</a>]

<h2>Advancing Non-Contact Vital Sign Measurement using Synthetic Avatars. (arXiv:2010.12949v1 [cs.CV])</h2>
<h3>Daniel McDuff, Javier Hernandez, Erroll Wood, Xin Liu, Tadas Baltrusaitis</h3>
<p>Non-contact physiological measurement has the potential to provide low-cost,
non-invasive health monitoring. However, machine vision approaches are often
limited by the availability and diversity of annotated video datasets resulting
in poor generalization to complex real-life conditions. To address these
challenges, this work proposes the use of synthetic avatars that display facial
blood flow changes and allow for systematic generation of samples under a wide
variety of conditions. Our results show that training on both simulated and
real video data can lead to performance gains under challenging conditions. We
show state-of-the-art performance on three large benchmark datasets and
improved robustness to skin type and motion.
</p>
<a href="http://arxiv.org/abs/2010.12949" target="_blank">arXiv:2010.12949</a> [<a href="http://arxiv.org/pdf/2010.12949" target="_blank">pdf</a>]

<h2>Raw-x-vector: Multi-scale Time Domain Speaker Embedding Network. (arXiv:2010.12951v1 [eess.AS])</h2>
<h3>Ge Zhu, Fei Jiang, Zhiyao Duan</h3>
<p>State-of-the-art text-independent speaker verification systems typically use
cepstral features or filter bank energies of speech utterances as input
features. With the ability of deep neural networks to learn representations
from raw data, recent studies attempted to extract speaker embeddings directly
from raw waveforms and showed competitive results. In this paper, we propose a
new speaker embedding called raw-x-vector for speaker verification in the time
domain, combining a multi-scale waveform encoder and an x-vector network
architecture. We show that the proposed approach outperforms existing
raw-waveform-based speaker verification systems by a large margin. We also show
that the proposed multi-scale encoder improves over single-scale encoders for
both the proposed system and another state-of-the-art raw-waveform-based
speaker verification systems. A further analysis of the learned filters shows
that the multi-scale encoder focuses on different frequency bands at its
different scales while resulting in a more flat overall frequency response than
any of the single-scale counterparts.
</p>
<a href="http://arxiv.org/abs/2010.12951" target="_blank">arXiv:2010.12951</a> [<a href="http://arxiv.org/pdf/2010.12951" target="_blank">pdf</a>]

<h2>Automated triage of COVID-19 from various lung abnormalities using chest CT features. (arXiv:2010.12967v1 [eess.IV])</h2>
<h3>Dor Amran, Maayan Frid-Adar, Nimrod Sagie, Jannette Nassar, Asher Kabakovitch, Hayit Greenspan</h3>
<p>The outbreak of COVID-19 has lead to a global effort to decelerate the
pandemic spread. For this purpose chest computed-tomography (CT) based
screening and diagnosis of COVID-19 suspected patients is utilized, either as a
support or replacement to reverse transcription-polymerase chain reaction
(RT-PCR) test. In this paper, we propose a fully automated AI based system that
takes as input chest CT scans and triages COVID-19 cases. More specifically, we
produce multiple descriptive features, including lung and infections
statistics, texture, shape and location, to train a machine learning based
classifier that distinguishes between COVID-19 and other lung abnormalities
(including community acquired pneumonia). We evaluated our system on a dataset
of 2191 CT cases and demonstrated a robust solution with 90.8% sensitivity at
85.4% specificity with 94.0% ROC-AUC. In addition, we present an elaborated
feature analysis and ablation study to explore the importance of each feature.
</p>
<a href="http://arxiv.org/abs/2010.12967" target="_blank">arXiv:2010.12967</a> [<a href="http://arxiv.org/pdf/2010.12967" target="_blank">pdf</a>]

<h2>Video Understanding based on Human Action and Group Activity Recognition. (arXiv:2010.12968v1 [cs.CV])</h2>
<h3>Zijian Kuang, Xinran Tie</h3>
<p>A lot of previous work, such as video captioning, has shown promising
performance in producing general video understanding. However, it is still
challenging to generate a fine-grained description of human actions and their
interactions using state-of-the-art video captioning techniques. The detailed
description of human actions and group activities is essential information,
which can be used in real-time CCTV video surveillance, health care, sports
video analysis, etc. In this study, we will propose and improve the video
understanding method based on the Group Activity Recognition model by learning
Actor Relation Graph (ARG).We will enhance the functionality and the
performance of the ARG based model to perform a better video understanding by
applying approaches such as increasing human object detection accuracy with
YOLO, increasing process speed by reducing the input image size, and applying
ResNet in the CNN layer.We will also introduce a visualization model that will
visualize each input video frame with predicted bounding boxes on each human
object and predicted "video captioning" to describe each individual's action
and their collective activity.
</p>
<a href="http://arxiv.org/abs/2010.12968" target="_blank">arXiv:2010.12968</a> [<a href="http://arxiv.org/pdf/2010.12968" target="_blank">pdf</a>]

<h2>Deep Denoising For Scientific Discovery: A Case Study In Electron Microscopy. (arXiv:2010.12970v1 [cs.CV])</h2>
<h3>Sreyas Mohan, Ramon Manzorro, Joshua L. Vincent, Binh Tang, Dev Yashpal Sheth, Eero P. Simoncelli, David S. Matteson, Peter A. Crozier, Carlos Fernandez-Granda</h3>
<p>Denoising is a fundamental challenge in scientific imaging. Deep
convolutional neural networks (CNNs) provide the current state of the art in
denoising natural images, where they produce impressive results. However, their
potential has barely been explored in the context of scientific imaging.
Denoising CNNs are typically trained on real natural images artificially
corrupted with simulated noise. In contrast, in scientific applications,
noiseless ground-truth images are usually not available. To address this issue,
we propose a simulation-based denoising (SBD) framework, in which CNNs are
trained on simulated images. We test the framework on data obtained from
transmission electron microscopy (TEM), an imaging technique with widespread
applications in material science, biology, and medicine. SBD outperforms
existing techniques by a wide margin on a simulated benchmark dataset, as well
as on real data. Apart from the denoised images, SBD generates likelihood maps
to visualize the agreement between the structure of the denoised image and the
observed data. Our results reveal shortcomings of state-of-the-art denoising
architectures, such as their small field-of-view: substantially increasing the
field-of-view of the CNNs allows them to exploit non-local periodic patterns in
the data, which is crucial at high noise levels. In addition, we analyze the
generalization capability of SBD, demonstrating that the trained networks are
robust to variations of imaging parameters and of the underlying signal
structure. Finally, we release the first publicly available benchmark dataset
of TEM images, containing 18,000 examples.
</p>
<a href="http://arxiv.org/abs/2010.12970" target="_blank">arXiv:2010.12970</a> [<a href="http://arxiv.org/pdf/2010.12970" target="_blank">pdf</a>]

<h2>Blind Deinterleaving of Signals in Time Series with Self-attention Based Soft Min-cost Flow Learning. (arXiv:2010.12972v1 [eess.SP])</h2>
<h3>O&#x11f;ul Can, Yeti Z. G&#xfc;rb&#xfc;z, Berkin Y&#x131;ld&#x131;r&#x131;m, A. Ayd&#x131;n Alatan</h3>
<p>We propose an end-to-end learning approach to address deinterleaving of
patterns in time series, in particular, radar signals. We link signal
clustering problem to min-cost flow as an equivalent problem once the proper
costs exist. We formulate a bi-level optimization problem involving min-cost
flow as a sub-problem to learn such costs from the supervised training data. We
then approximate the lower level optimization problem by self-attention based
neural networks and provide a trainable framework that clusters the patterns in
the input as the distinct flows. We evaluate our method with extensive
experiments on a large dataset with several challenging scenarios to show the
efficiency.
</p>
<a href="http://arxiv.org/abs/2010.12972" target="_blank">arXiv:2010.12972</a> [<a href="http://arxiv.org/pdf/2010.12972" target="_blank">pdf</a>]

<h2>Unsupervised Learning of Disentangled Speech Content and Style Representation. (arXiv:2010.12973v1 [cs.CL])</h2>
<h3>Andros Tjandra, Ruoming Pang, Yu Zhang, Shigeki Karita</h3>
<p>We present an approach for unsupervised learning of speech representation
disentangling contents and styles. Our model consists of: (1) a local encoder
that captures per-frame information; (2) a global encoder that captures
per-utterance information; and (3) a conditional decoder that reconstructs
speech given local and global latent variables. Our experiments show that (1)
the local latent variables encode speech contents, as reconstructed speech can
be recognized by ASR with low word error rates (WER), even with a different
global encoding; (2) the global latent variables encode speaker style, as
reconstructed speech shares speaker identity with the source utterance of the
global encoding. Additionally, we demonstrate an useful application from our
pre-trained model, where we can train a speaker recognition model from the
global latent variables and achieve high accuracy by fine-tuning with as few
data as one label per speaker.
</p>
<a href="http://arxiv.org/abs/2010.12973" target="_blank">arXiv:2010.12973</a> [<a href="http://arxiv.org/pdf/2010.12973" target="_blank">pdf</a>]

<h2>Improving the Exploration of Deep Reinforcement Learning in Continuous Domains using Planning for Policy Search. (arXiv:2010.12974v1 [cs.LG])</h2>
<h3>Jakob J. Hollenstein, Erwan Renaudo, Matteo Saveriano, Justus Piater</h3>
<p>Local policy search is performed by most Deep Reinforcement Learning (D-RL)
methods, which increases the risk of getting trapped in a local minimum.
Furthermore, the availability of a simulation model is not fully exploited in
D-RL even in simulation-based training, which potentially decreases efficiency.
To better exploit simulation models in policy search, we propose to integrate a
kinodynamic planner in the exploration strategy and to learn a control policy
in an offline fashion from the generated environment interactions. We call the
resulting model-based reinforcement learning method PPS (Planning for Policy
Search). We compare PPS with state-of-the-art D-RL methods in typical RL
settings including underactuated systems. The comparison shows that PPS, guided
by the kinodynamic planner, collects data from a wider region of the state
space. This generates training data that helps PPS discover better policies.
</p>
<a href="http://arxiv.org/abs/2010.12974" target="_blank">arXiv:2010.12974</a> [<a href="http://arxiv.org/pdf/2010.12974" target="_blank">pdf</a>]

<h2>Adam with Bandit Sampling for Deep Learning. (arXiv:2010.12986v1 [cs.LG])</h2>
<h3>Rui Liu, Tianyi Wu, Barzan Mozafari</h3>
<p>Adam is a widely used optimization method for training deep learning models.
It computes individual adaptive learning rates for different parameters. In
this paper, we propose a generalization of Adam, called Adambs, that allows us
to also adapt to different training examples based on their importance in the
model's convergence. To achieve this, we maintain a distribution over all
examples, selecting a mini-batch in each iteration by sampling according to
this distribution, which we update using a multi-armed bandit algorithm. This
ensures that examples that are more beneficial to the model training are
sampled with higher probabilities. We theoretically show that Adambs improves
the convergence rate of Adam---$O(\sqrt{\frac{\log n}{T} })$ instead of
$O(\sqrt{\frac{n}{T}})$ in some cases. Experiments on various models and
datasets demonstrate Adambs's fast convergence in practice.
</p>
<a href="http://arxiv.org/abs/2010.12986" target="_blank">arXiv:2010.12986</a> [<a href="http://arxiv.org/pdf/2010.12986" target="_blank">pdf</a>]

<h2>Multi-task Supervised Learning via Cross-learning. (arXiv:2010.12993v1 [cs.LG])</h2>
<h3>Juan Cervino, Juan Andres Bazerque, Miguel Calvo-Fullana, Alejandro Ribeiro</h3>
<p>In this paper we consider a problem known as multi-task learning, consisting
of fitting a set of classifier or regression functions intended for solving
different tasks. In our novel formulation, we couple the parameters of these
functions, so that they learn in their task specific domains while staying
close to each other. This facilitates cross-fertilization in which data
collected across different domains help improving the learning performance at
each other task. First, we present a simplified case in which the goal is to
estimate the means of two Gaussian variables, for the purpose of gaining some
insights on the advantage of the proposed cross-learning strategy. Then we
provide a stochastic projected gradient algorithm to perform cross-learning
over a generic loss function. If the number of parameters is large, then the
projection step becomes computationally expensive. To avoid this situation, we
derive a primal-dual algorithm that exploits the structure of the dual problem,
achieving a formulation whose complexity only depends on the number of tasks.
Preliminary numerical experiments for image classification by neural networks
trained on a dataset divided in different domains corroborate that the
cross-learned function outperforms both the task-specific and the consensus
approaches.
</p>
<a href="http://arxiv.org/abs/2010.12993" target="_blank">arXiv:2010.12993</a> [<a href="http://arxiv.org/pdf/2010.12993" target="_blank">pdf</a>]

<h2>Implicit Variational Inference: the Parameter and the Predictor Space. (arXiv:2010.12995v1 [cs.LG])</h2>
<h3>Yann Pequignot, Mathieu Alain, Patrick Dallaire, Alireza Yeganehparast, Pascal Germain, Jos&#xe9;e Desharnais, Fran&#xe7;ois Laviolette</h3>
<p>Having access to accurate confidence levels along with the predictions allows
to determine whether making a decision is worth the risk. Under the Bayesian
paradigm, the posterior distribution over parameters is used to capture model
uncertainty, a valuable information that can be translated into predictive
uncertainty. However, computing the posterior distribution for high capacity
predictors, such as neural networks, is generally intractable, making
approximate methods such as variational inference a promising alternative.
While most methods perform inference in the space of parameters, we explore the
benefits of carrying inference directly in the space of predictors. Relying on
a family of distributions given by a deep generative neural network, we present
two ways of carrying variational inference: one in \emph{parameter space}, one
in \emph{predictor space}. Importantly, the latter requires us to choose a
distribution of inputs, therefore allowing us at the same time to explicitly
address the question of \emph{out-of-distribution} uncertainty. We explore from
various perspectives the implications of working in the predictor space induced
by neural networks as opposed to the parameter space, focusing mainly on the
quality of uncertainty estimation for data lying outside of the training
distribution. We compare posterior approximations obtained with these two
methods to several standard methods and present results showing that
variational approximations learned in the predictor space distinguish
themselves positively from those trained in the parameter space.
</p>
<a href="http://arxiv.org/abs/2010.12995" target="_blank">arXiv:2010.12995</a> [<a href="http://arxiv.org/pdf/2010.12995" target="_blank">pdf</a>]

<h2>Discriminative Nearest Neighbor Few-Shot Intent Detection by Transferring Natural Language Inference. (arXiv:2010.13009v1 [cs.CL])</h2>
<h3>Jian-Guo Zhang, Kazuma Hashimoto, Wenhao Liu, Chien-Sheng Wu, Yao Wan, Philip S. Yu, Richard Socher, Caiming Xiong</h3>
<p>Intent detection is one of the core components of goal-oriented dialog
systems, and detecting out-of-scope (OOS) intents is also a practically
important skill. Few-shot learning is attracting much attention to mitigate
data scarcity, but OOS detection becomes even more challenging. In this paper,
we present a simple yet effective approach, discriminative nearest neighbor
classification with deep self-attention. Unlike softmax classifiers, we
leverage BERT-style pairwise encoding to train a binary classifier that
estimates the best matched training example for a user input. We propose to
boost the discriminative ability by transferring a natural language inference
(NLI) model. Our extensive experiments on a large-scale multi-domain intent
detection task show that our method achieves more stable and accurate in-domain
and OOS detection accuracy than RoBERTa-based classifiers and embedding-based
nearest neighbor approaches. More notably, the NLI transfer enables our 10-shot
model to perform competitively with 50-shot or even full-shot classifiers,
while we can keep the inference time constant by leveraging a faster embedding
retrieval model.
</p>
<a href="http://arxiv.org/abs/2010.13009" target="_blank">arXiv:2010.13009</a> [<a href="http://arxiv.org/pdf/2010.13009" target="_blank">pdf</a>]

<h2>An Approximate Dynamic Programming Approach to Adversarial Online Learning. (arXiv:1603.04981v6 [cs.GT] UPDATED)</h2>
<h3>Vijay Kamble, Patrick Loiseau, Jean Walrand</h3>
<p>We describe an approximate dynamic programming (ADP) approach to compute
approximations of the optimal strategies and of the minimal losses that can be
guaranteed in discounted repeated games with vector-valued losses. Such games
prominently arise in the analysis of regret in repeated decision-making in
adversarial environments, also known as adversarial online learning. At the
core of our approach is a characterization of the lower Pareto frontier of the
set of expected losses that a player can guarantee in these games as the unique
fixed point of a set-valued dynamic programming operator. When applied to the
problem of regret minimization with discounted losses, our approach yields
algorithms that achieve markedly improved performance bounds compared to
off-the-shelf online learning algorithms like Hedge. These results thus suggest
the significant potential of ADP-based approaches in adversarial online
learning.
</p>
<a href="http://arxiv.org/abs/1603.04981" target="_blank">arXiv:1603.04981</a> [<a href="http://arxiv.org/pdf/1603.04981" target="_blank">pdf</a>]

<h2>The Perception-Distortion Tradeoff. (arXiv:1711.06077v4 [cs.CV] UPDATED)</h2>
<h3>Yochai Blau, Tomer Michaeli</h3>
<p>Image restoration algorithms are typically evaluated by some distortion
measure (e.g. PSNR, SSIM, IFC, VIF) or by human opinion scores that quantify
perceived perceptual quality. In this paper, we prove mathematically that
distortion and perceptual quality are at odds with each other. Specifically, we
study the optimal probability for correctly discriminating the outputs of an
image restoration algorithm from real images. We show that as the mean
distortion decreases, this probability must increase (indicating worse
perceptual quality). As opposed to the common belief, this result holds true
for any distortion measure, and is not only a problem of the PSNR or SSIM
criteria. We also show that generative-adversarial-nets (GANs) provide a
principled way to approach the perception-distortion bound. This constitutes
theoretical support to their observed success in low-level vision tasks. Based
on our analysis, we propose a new methodology for evaluating image restoration
methods, and use it to perform an extensive comparison between recent
super-resolution algorithms.
</p>
<a href="http://arxiv.org/abs/1711.06077" target="_blank">arXiv:1711.06077</a> [<a href="http://arxiv.org/pdf/1711.06077" target="_blank">pdf</a>]

<h2>Audio Cover Song Identification using Convolutional Neural Network. (arXiv:1712.00166v2 [cs.SD] UPDATED)</h2>
<h3>Sungkyun Chang, Juheon Lee, Sang Keun Choe, Kyogu Lee</h3>
<p>In this paper, we propose a new approach to cover song identification using a
CNN (convolutional neural network). Most previous studies extract the feature
vectors that characterize the cover song relation from a pair of songs and used
it to compute the (dis)similarity between the two songs. Based on the
observation that there is a meaningful pattern between cover songs and that
this can be learned, we have reformulated the cover song identification problem
in a machine learning framework. To do this, we first build the CNN using as an
input a cross-similarity matrix generated from a pair of songs. We then
construct the data set composed of cover song pairs and non-cover song pairs,
which are used as positive and negative training samples, respectively. The
trained CNN outputs the probability of being in the cover song relation given a
cross-similarity matrix generated from any two pieces of music and identifies
the cover song by ranking on the probability. Experimental results show that
the proposed algorithm achieves performance better than or comparable to the
state-of-the-art.
</p>
<a href="http://arxiv.org/abs/1712.00166" target="_blank">arXiv:1712.00166</a> [<a href="http://arxiv.org/pdf/1712.00166" target="_blank">pdf</a>]

<h2>End-to-End Physics Event Classification with CMS Open Data: Applying Image-Based Deep Learning to Detector Data for the Direct Classification of Collision Events at the LHC. (arXiv:1807.11916v3 [hep-ex] UPDATED)</h2>
<h3>Michael Andrews, Manfred Paulini, Sergei Gleyzer, Barnabas Poczos</h3>
<p>This paper describes the construction of novel end-to-end image-based
classifiers that directly leverage low-level simulated detector data to
discriminate signal and background processes in pp collision events at the
Large Hadron Collider at CERN. To better understand what end-to-end classifiers
are capable of learning from the data and to address a number of associated
challenges, we distinguish the decay of the standard model Higgs boson into two
photons from its leading background sources using high-fidelity simulated CMS
Open Data. We demonstrate the ability of end-to-end classifiers to learn from
the angular distribution of the photons recorded as electromagnetic showers,
their intrinsic shapes, and the energy of their constituent hits, even when the
underlying particles are not fully resolved, delivering a clear advantage in
such cases over purely kinematics-based classifiers.
</p>
<a href="http://arxiv.org/abs/1807.11916" target="_blank">arXiv:1807.11916</a> [<a href="http://arxiv.org/pdf/1807.11916" target="_blank">pdf</a>]

<h2>Sequential Skip Prediction with Few-shot in Streamed Music Contents. (arXiv:1901.08203v2 [cs.IR] UPDATED)</h2>
<h3>Sungkyun Chang, Seungjin Lee, Kyogu Lee</h3>
<p>This paper provides an outline of the algorithms submitted for the WSDM Cup
2019 Spotify Sequential Skip Prediction Challenge (team name: mimbres). In the
challenge, complete information including acoustic features and user
interaction logs for the first half of a listening session is provided. Our
goal is to predict whether the individual tracks in the second half of the
session will be skipped or not, only given acoustic features. We proposed two
different kinds of algorithms that were based on metric learning and sequence
learning. The experimental results showed that the sequence learning approach
performed significantly better than the metric learning approach. Moreover, we
conducted additional experiments to find that significant performance gain can
be achieved using complete user log information.
</p>
<a href="http://arxiv.org/abs/1901.08203" target="_blank">arXiv:1901.08203</a> [<a href="http://arxiv.org/pdf/1901.08203" target="_blank">pdf</a>]

<h2>Quadratic Decomposable Submodular Function Minimization: Theory and Practice (Computation and Analysis of PageRank over Hypergraphs). (arXiv:1902.10132v4 [cs.LG] UPDATED)</h2>
<h3>Pan Li, Niao He, Olgica Milenkovic</h3>
<p>We introduce a new convex optimization problem, termed quadratic decomposable
submodular function minimization (QDSFM), which allows to model a number of
learning tasks on graphs and hypergraphs. The problem exhibits close ties to
decomposable submodular function minimization (DSFM), yet is much more
challenging to solve. We approach the problem via a new dual strategy and
formulate an objective that can be optimized through a number of double-loop
algorithms. The outer-loop uses either random coordinate descent (RCD) or
alternative projection (AP) methods, for both of which we prove linear
convergence rates. The inner-loop computes projections onto cones generated by
base polytopes of the submodular functions, via the modified min-norm-point or
Frank-Wolfe algorithm. We also describe two new applications of QDSFM:
hypergraph-adapted PageRank and semi-supervised learning. The proposed
hypergraph-based PageRank algorithm can be used for local hypergraph
partitioning, and comes with provable performance guarantees. For
hypergraph-adapted semi-supervised learning, we provide numerical experiments
demonstrating the efficiency of our QDSFM solvers and their significant
improvements on prediction accuracy when compared to state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/1902.10132" target="_blank">arXiv:1902.10132</a> [<a href="http://arxiv.org/pdf/1902.10132" target="_blank">pdf</a>]

<h2>Optimal Obfuscation Mechanisms via Machine Learning. (arXiv:1904.01059v5 [cs.LG] UPDATED)</h2>
<h3>Marco Romanelli, Konstantinos Chatzikokolakis, Catuscia Palamidessi</h3>
<p>We consider the problem of obfuscating sensitive information while preserving
utility, and we propose a machine learning approach inspired by the generative
adversarial networks paradigm. The idea is to set up two nets: the generator,
that tries to produce an optimal obfuscation mechanism to protect the data, and
the classifier, that tries to de-obfuscate the data. By letting the two nets
compete against each other, the mechanism improves its degree of protection,
until an equilibrium is reached. We apply our method to the case of location
privacy, and we perform experiments on synthetic data and on real data from the
Gowalla dataset. We evaluate the privacy of the mechanism not only by its
capacity to defeat the classifier, but also in terms of the Bayes error, which
represents the strongest possible adversary. We compare the privacy-utility
tradeoff of our method to that of the planar Laplace mechanism used in
geo-indistinguishability, showing favorable results. Like the Laplace
mechanism, our system can be deployed at the user end for protecting his
location.
</p>
<a href="http://arxiv.org/abs/1904.01059" target="_blank">arXiv:1904.01059</a> [<a href="http://arxiv.org/pdf/1904.01059" target="_blank">pdf</a>]

<h2>Vision-based Robotic Grasping From Object Localization, Object Pose Estimation to Grasp Estimation for Parallel Grippers: A Review. (arXiv:1905.06658v4 [cs.RO] UPDATED)</h2>
<h3>Guoguang Du, Kai Wang, Shiguo Lian, Kaiyong Zhao</h3>
<p>This paper presents a comprehensive survey on vision-based robotic grasping.
We conclude three key tasks during vision-based robotic grasping, which are
object localization, object pose estimation and grasp estimation. In detail,
the object localization task contains object localization without
classification, object detection and object instance segmentation. This task
provides the regions of the target object in the input data. The object pose
estimation task mainly refers to estimating the 6D object pose and includes
correspondence-based methods, template-based methods and voting-based methods,
which affords the generation of grasp poses for known objects. The grasp
estimation task includes 2D planar grasp methods and 6DoF grasp methods, where
the former is constrained to grasp from one direction. These three tasks could
accomplish the robotic grasping with different combinations. Lots of object
pose estimation methods need not object localization, and they conduct object
localization and object pose estimation jointly. Lots of grasp estimation
methods need not object localization and object pose estimation, and they
conduct grasp estimation in an end-to-end manner. Both traditional methods and
latest deep learning-based methods based on the RGB-D image inputs are reviewed
elaborately in this survey. Related datasets and comparisons between
state-of-the-art methods are summarized as well. In addition, challenges about
vision-based robotic grasping and future directions in addressing these
challenges are also pointed out.
</p>
<a href="http://arxiv.org/abs/1905.06658" target="_blank">arXiv:1905.06658</a> [<a href="http://arxiv.org/pdf/1905.06658" target="_blank">pdf</a>]

<h2>Accelerating DNN Training in Wireless Federated Edge Learning Systems. (arXiv:1905.09712v3 [cs.LG] UPDATED)</h2>
<h3>Jinke Ren, Guanding Yu, Guangyao Ding</h3>
<p>Training task in classical machine learning models, such as deep neural
networks, is generally implemented at a remote cloud center for centralized
learning, which is typically time-consuming and resource-hungry. It also incurs
serious privacy issue and long communication latency since a large amount of
data are transmitted to the centralized node. To overcome these shortcomings,
we consider a newly-emerged framework, namely federated edge learning, to
aggregate local learning updates at the network edge in lieu of users' raw
data. Aiming at accelerating the training process, we first define a novel
performance evaluation criterion, called learning efficiency. We then formulate
a training acceleration optimization problem in the CPU scenario, where each
user device is equipped with CPU. The closed-form expressions for joint
batchsize selection and communication resource allocation are developed and
some insightful results are highlighted. Further, we extend our learning
framework to the GPU scenario. The optimal solution in this scenario is
manifested to have the similar structure as that of the CPU scenario,
recommending that our proposed algorithm is applicable in more general systems.
Finally, extensive experiments validate the theoretical analysis and
demonstrate that the proposed algorithm can reduce the training time and
improve the learning accuracy simultaneously.
</p>
<a href="http://arxiv.org/abs/1905.09712" target="_blank">arXiv:1905.09712</a> [<a href="http://arxiv.org/pdf/1905.09712" target="_blank">pdf</a>]

<h2>Beyond Exponentially Discounted Sum: Automatic Learning of Return Function. (arXiv:1905.11591v2 [cs.LG] UPDATED)</h2>
<h3>Yufei Wang, Qiwei Ye, Tie-Yan Liu</h3>
<p>In reinforcement learning, Return, which is the weighted accumulated future
rewards, and Value, which is the expected return, serve as the objective that
guides the learning of the policy. In classic RL, return is defined as the
exponentially discounted sum of future rewards. One key insight is that there
could be many feasible ways to define the form of the return function (and thus
the value), from which the same optimal policy can be derived, yet these
different forms might render dramatically different speeds of learning this
policy. In this paper, we research how to modify the form of the return
function to enhance the learning towards the optimal policy. We propose to use
a general mathematical form for return function, and employ meta-learning to
learn the optimal return function in an end-to-end manner. We test our methods
on a specially designed maze environment and several Atari games, and our
experimental results clearly indicate the advantages of automatically learning
optimal return functions in reinforcement learning.
</p>
<a href="http://arxiv.org/abs/1905.11591" target="_blank">arXiv:1905.11591</a> [<a href="http://arxiv.org/pdf/1905.11591" target="_blank">pdf</a>]

<h2>Multimodal End-to-End Autonomous Driving. (arXiv:1906.03199v2 [cs.CV] UPDATED)</h2>
<h3>Yi Xiao, Felipe Codevilla, Akhil Gurram, Onay Urfalioglu, Antonio M. L&#xf3;pez</h3>
<p>A crucial component of an autonomous vehicle (AV) is the artificial
intelligence (AI) is able to drive towards a desired destination. Today, there
are different paradigms addressing the development of AI drivers. On the one
hand, we find modular pipelines, which divide the driving task into sub-tasks
such as perception and maneuver planning and control. On the other hand, we
find end-to-end driving approaches that try to learn a direct mapping from
input raw sensor data to vehicle control signals. The later are relatively less
studied, but are gaining popularity since they are less demanding in terms of
sensor data annotation. This paper focuses on end-to-end autonomous driving. So
far, most proposals relying on this paradigm assume RGB images as input sensor
data. However, AVs will not be equipped only with cameras, but also with active
sensors providing accurate depth information (e.g., LiDARs). Accordingly, this
paper analyses whether combining RGB and depth modalities, i.e. using RGBD
data, produces better end-to-end AI drivers than relying on a single modality.
We consider multimodality based on early, mid and late fusion schemes, both in
multisensory and single-sensor (monocular depth estimation) settings. Using the
CARLA simulator and conditional imitation learning (CIL), we show how, indeed,
early fusion multimodality outperforms single-modality.
</p>
<a href="http://arxiv.org/abs/1906.03199" target="_blank">arXiv:1906.03199</a> [<a href="http://arxiv.org/pdf/1906.03199" target="_blank">pdf</a>]

<h2>Deep Smoothing of the Implied Volatility Surface. (arXiv:1906.05065v3 [q-fin.PR] UPDATED)</h2>
<h3>Damien Ackerer, Natasa Tagasovska, Thibault Vatter</h3>
<p>We present a neural network (NN) approach to fit and predict implied
volatility surfaces (IVSs). Atypically to standard NN applications, financial
industry practitioners use such models equally to replicate market prices and
to value other financial instruments. In other words, low training losses are
as important as generalization capabilities. Importantly, IVS models need to
generate realistic arbitrage-free option prices, meaning that no portfolio can
lead to risk-free profits. We propose an approach guaranteeing the absence of
arbitrage opportunities by penalizing the loss using soft constraints.
Furthermore, our method can be combined with standard IVS models in
quantitative finance, thus providing a NN-based correction when such models
fail at replicating observed market prices. This lets practitioners use our
approach as a plug-in on top of classical methods. Empirical results show that
this approach is particularly useful when only sparse or erroneous data are
available. We also quantify the uncertainty of the model predictions in regions
with few or no observations. We further explore how deeper NNs improve over
shallower ones, as well as other properties of the network architecture. We
benchmark our method against standard IVS models. By evaluating our method on
both training sets, and testing sets, namely, we highlight both their capacity
to reproduce observed prices and predict new ones.
</p>
<a href="http://arxiv.org/abs/1906.05065" target="_blank">arXiv:1906.05065</a> [<a href="http://arxiv.org/pdf/1906.05065" target="_blank">pdf</a>]

<h2>Trade-offs and Guarantees of Adversarial Representation Learning for Information Obfuscation. (arXiv:1906.07902v3 [cs.LG] UPDATED)</h2>
<h3>Han Zhao, Jianfeng Chi, Yuan Tian, Geoffrey J. Gordon</h3>
<p>Crowdsourced data used in machine learning services might carry sensitive
information about attributes that users do not want to share. Various methods
have been proposed to minimize the potential information leakage of sensitive
attributes while maximizing the task accuracy. However, little is known about
the theory behind these methods. In light of this gap, we develop a novel
theoretical framework for attribute obfuscation. Under our framework, we
propose a minimax optimization formulation to protect the given attribute and
analyze its inference guarantees against worst-case adversaries. Meanwhile, it
is clear that in general there is a tension between minimizing information
leakage and maximizing task accuracy. To understand this, we prove an
information-theoretic lower bound to precisely characterize the fundamental
trade-off between accuracy and information leakage. We conduct experiments on
two real-world datasets to corroborate the inference guarantees and validate
this trade-off. Our results indicate that, among several alternatives, the
adversarial learning approach achieves the best trade-off in terms of attribute
obfuscation and accuracy maximization.
</p>
<a href="http://arxiv.org/abs/1906.07902" target="_blank">arXiv:1906.07902</a> [<a href="http://arxiv.org/pdf/1906.07902" target="_blank">pdf</a>]

<h2>Rethinking Formal Models of Partially Observable Multiagent Decision Making. (arXiv:1906.11110v3 [cs.AI] UPDATED)</h2>
<h3>Vojt&#x11b;ch Kova&#x159;&#xed;k, Martin Schmid, Neil Burch, Michael Bowling, Viliam Lis&#xfd;</h3>
<p>Multiagent decision-making in partially observable environments is usually
modelled as either an extensive-form game (EFG) in game theory or a partially
observable stochastic game (POSG) in multiagent reinforcement learning (MARL).
One issue with the current situation is that while most practical problems can
be modelled in both formalisms, the relationship of the two models is unclear,
which hinders the transfer of ideas between the two communities. A second issue
is that while EFGs have recently seen significant algorithmic progress, their
classical formalization is unsuitable for efficient presentation of the
underlying ideas, such as those around decomposition.

To solve the first issue, we introduce factored-observation stochastic games
(FOSGs), a minor modification of the POSG formalism which distinguishes between
private and public observation and thereby greatly simplifies decomposition. To
remedy the second issue, we show that FOSGs and POSGs are naturally connected
to EFGs: by "unrolling" a FOSG into its tree form, we obtain an EFG.
Conversely, any perfect-recall timeable EFG corresponds to some underlying FOSG
in this manner. Moreover, this relationship justifies several minor
modifications to the classical EFG formalization that recently appeared as an
implicit response to the model's issues with decomposition. Finally, we
illustrate the transfer of ideas between EFGs and MARL by presenting three key
EFG techniques -- counterfactual regret minimization, sequence form, and
decomposition -- in the FOSG framework.
</p>
<a href="http://arxiv.org/abs/1906.11110" target="_blank">arXiv:1906.11110</a> [<a href="http://arxiv.org/pdf/1906.11110" target="_blank">pdf</a>]

<h2>Stochastic Latent Actor-Critic: Deep Reinforcement Learning with a Latent Variable Model. (arXiv:1907.00953v4 [cs.LG] UPDATED)</h2>
<h3>Alex X. Lee, Anusha Nagabandi, Pieter Abbeel, Sergey Levine</h3>
<p>Deep reinforcement learning (RL) algorithms can use high-capacity deep
networks to learn directly from image observations. However, these
high-dimensional observation spaces present a number of challenges in practice,
since the policy must now solve two problems: representation learning and task
learning. In this work, we tackle these two problems separately, by explicitly
learning latent representations that can accelerate reinforcement learning from
images. We propose the stochastic latent actor-critic (SLAC) algorithm: a
sample-efficient and high-performing RL algorithm for learning policies for
complex continuous control tasks directly from high-dimensional image inputs.
SLAC provides a novel and principled approach for unifying stochastic
sequential models and RL into a single method, by learning a compact latent
representation and then performing RL in the model's learned latent space. Our
experimental evaluation demonstrates that our method outperforms both
model-free and model-based alternatives in terms of final performance and
sample efficiency, on a range of difficult image-based control tasks. Our code
and videos of our results are available at our website.
</p>
<a href="http://arxiv.org/abs/1907.00953" target="_blank">arXiv:1907.00953</a> [<a href="http://arxiv.org/pdf/1907.00953" target="_blank">pdf</a>]

<h2>Trading via Image Classification. (arXiv:1907.10046v3 [cs.CV] UPDATED)</h2>
<h3>Naftali Cohen, Tucker Balch, Manuela Veloso</h3>
<p>The art of systematic financial trading evolved with an array of approaches,
ranging from simple strategies to complex algorithms all relying, primary, on
aspects of time-series analysis. Recently, after visiting the trading floor of
a leading financial institution, we noticed that traders always execute their
trade orders while observing images of financial time-series on their screens.
In this work, we built upon the success in image recognition and examine the
value in transforming the traditional time-series analysis to that of image
classification. We create a large sample of financial time-series images
encoded as candlestick (Box and Whisker) charts and label the samples following
three algebraically-defined binary trade strategies. Using the images, we train
over a dozen machine-learning classification models and find that the
algorithms are very efficient in recovering the complicated, multiscale
label-generating rules when the data is represented visually. We suggest that
the transformation of continuous numeric time-series classification problem to
a vision problem is useful for recovering signals typical of technical
analysis.
</p>
<a href="http://arxiv.org/abs/1907.10046" target="_blank">arXiv:1907.10046</a> [<a href="http://arxiv.org/pdf/1907.10046" target="_blank">pdf</a>]

<h2>Deep Kinematic Models for Kinematically Feasible Vehicle Trajectory Predictions. (arXiv:1908.00219v3 [cs.RO] UPDATED)</h2>
<h3>Henggang Cui, Thi Nguyen, Fang-Chieh Chou, Tsung-Han Lin, Jeff Schneider, David Bradley, Nemanja Djuric</h3>
<p>Self-driving vehicles (SDVs) hold great potential for improving traffic
safety and are poised to positively affect the quality of life of millions of
people. To unlock this potential one of the critical aspects of the autonomous
technology is understanding and predicting future movement of vehicles
surrounding the SDV. This work presents a deep-learning-based method for
kinematically feasible motion prediction of such traffic actors. Previous work
did not explicitly encode vehicle kinematics and instead relied on the models
to learn the constraints directly from the data, potentially resulting in
kinematically infeasible, suboptimal trajectory predictions. To address this
issue we propose a method that seamlessly combines ideas from the AI with
physically grounded vehicle motion models. In this way we employ best of the
both worlds, coupling powerful learning models with strong feasibility
guarantees for their outputs. The proposed approach is general, being
applicable to any type of learning method. Extensive experiments using deep
convnets on real-world data strongly indicate its benefits, outperforming the
existing state-of-the-art.
</p>
<a href="http://arxiv.org/abs/1908.00219" target="_blank">arXiv:1908.00219</a> [<a href="http://arxiv.org/pdf/1908.00219" target="_blank">pdf</a>]

<h2>Reinforcement learning with world model. (arXiv:1908.11494v4 [cs.AI] UPDATED)</h2>
<h3>Jingbin Liu, Xinyang Gu, Shuai Liu</h3>
<p>Nowadays, model-free reinforcement learning algorithms have achieved
remarkable performance on many decision making and control tasks, but high
sample complexity and low sample efficiency still hinder the wide use of
model-free reinforcement learning algorithms. In this paper, we argue that if
we intend to design an intelligent agent that learns fast and transfers well,
the agent must be able to reflect key elements of intelligence, like intuition,
Memory, PredictionandCuriosity. We propose an agent framework that integrates
off-policy reinforcement learning with world model learning, so as to embody
the important features of intelligence in our algorithm design. We adopt the
state-of-art model-free reinforcement learning algorithm, Soft Actor-Critic, as
the agent intuition, and world model learning through RNN to endow the agent
with memory, curiosity, and the ability to predict. We show that these ideas
can work collaboratively with each other and our agent (RMC) can give new
state-of-art results while maintaining sample efficiency and training
stability. Moreover, our agent framework can be easily extended from MDP to
POMDP problems without performance loss.
</p>
<a href="http://arxiv.org/abs/1908.11494" target="_blank">arXiv:1908.11494</a> [<a href="http://arxiv.org/pdf/1908.11494" target="_blank">pdf</a>]

<h2>Learning Elastic Constitutive Material and Damping Models. (arXiv:1909.01875v2 [cs.GR] UPDATED)</h2>
<h3>Bin Wang, Yuanmin Deng, Paul Kry, Uri Ascher, Hui Huang, Baoquan Chen</h3>
<p>Commonly used linear and nonlinear constitutive material models in
deformation simulation contain many simplifications and only cover a tiny part
of possible material behavior. In this work we propose a framework for learning
customized models of deformable materials from example surface trajectories.
The key idea is to iteratively improve a correction to a nominal model of the
elastic and damping properties of the object, which allows new forward
simulations with the learned correction to more accurately predict the behavior
of a given soft object. Space-time optimization is employed to identify gentle
control forces with which we extract necessary data for model inference and to
finally encapsulate the material correction into a compact parametric form.
Furthermore, a patch based position constraint is proposed to tackle the
challenge of handling incomplete and noisy observations arising in real-world
examples. We demonstrate the effectiveness of our method with a set of
synthetic examples, as well with data captured from real world homogeneous
elastic objects.
</p>
<a href="http://arxiv.org/abs/1909.01875" target="_blank">arXiv:1909.01875</a> [<a href="http://arxiv.org/pdf/1909.01875" target="_blank">pdf</a>]

<h2>Population-based Gradient Descent Weight Learning for Graph Coloring Problems. (arXiv:1909.02261v4 [cs.LG] UPDATED)</h2>
<h3>Olivier Goudet, B&#xe9;atrice Duval, Jin-Kao Hao</h3>
<p>Graph coloring involves assigning colors to the vertices of a graph such that
two vertices linked by an edge receive different colors. Graph coloring
problems are general models that are very useful to formulate many relevant
applications and, however, are computationally difficult. In this work, a
general population-based weight learning framework for solving graph coloring
problems is presented. Unlike existing methods for graph coloring that are
specific to the considered problem, the presented work targets a generic
objective by introducing a unified method that can be applied to different
graph coloring problems. This work distinguishes itself by its solving approach
that formulates the search of a solution as a continuous weight tensor
optimization problem and takes advantage of a gradient descent method computed
in parallel on graphics processing units. The proposed approach is also
characterized by its general global loss function that can easily be adapted to
different graph coloring problems. The usefulness of the proposed approach is
demonstrated by applying it to solve two typical graph coloring problems and
performing large computational studies on popular benchmarks. Improved
best-known results (new upper bounds) are reported for several large graphs.
</p>
<a href="http://arxiv.org/abs/1909.02261" target="_blank">arXiv:1909.02261</a> [<a href="http://arxiv.org/pdf/1909.02261" target="_blank">pdf</a>]

<h2>Branch and Bound for Piecewise Linear Neural Network Verification. (arXiv:1909.06588v4 [cs.LG] UPDATED)</h2>
<h3>Rudy Bunel, Jingyue Lu, Ilker Turkaslan, Philip H.S. Torr, Pushmeet Kohli, M. Pawan Kumar</h3>
<p>The success of Deep Learning and its potential use in many safety-critical
applications has motivated research on formal verification of Neural Network
(NN) models. In this context, verification involves proving or disproving that
an NN model satisfies certain input-output properties. Despite the reputation
of learned NN models as black boxes, and the theoretical hardness of proving
useful properties about them, researchers have been successful in verifying
some classes of models by exploiting their piecewise linear structure and
taking insights from formal methods such as Satisifiability Modulo Theory.
However, these methods are still far from scaling to realistic neural networks.
To facilitate progress on this crucial area, we exploit the Mixed Integer
Linear Programming (MIP) formulation of verification to propose a family of
algorithms based on Branch-and-Bound (BaB). We show that our family contains
previous verification methods as special cases. With the help of the BaB
framework, we make three key contributions. Firstly, we identify new methods
that combine the strengths of multiple existing approaches, accomplishing
significant performance improvements over previous state of the art. Secondly,
we introduce an effective branching strategy on ReLU non-linearities. This
branching strategy allows us to efficiently and successfully deal with high
input dimensional problems with convolutional network architecture, on which
previous methods fail frequently. Finally, we propose comprehensive test data
sets and benchmarks which includes a collection of previously released
testcases. We use the data sets to conduct a thorough experimental comparison
of existing and new algorithms and to provide an inclusive analysis of the
factors impacting the hardness of verification problems.
</p>
<a href="http://arxiv.org/abs/1909.06588" target="_blank">arXiv:1909.06588</a> [<a href="http://arxiv.org/pdf/1909.06588" target="_blank">pdf</a>]

<h2>Learning Invariants through Soft Unification. (arXiv:1909.07328v2 [cs.LG] UPDATED)</h2>
<h3>Nuri Cingillioglu, Alessandra Russo</h3>
<p>Human reasoning involves recognising common underlying principles across many
examples. The by-products of such reasoning are invariants that capture
patterns such as "if someone went somewhere then they are there", expressed
using variables "someone" and "somewhere" instead of mentioning specific people
or places. Humans learn what variables are and how to use them at a young age.
This paper explores whether machines can also learn and use variables solely
from examples without requiring human pre-engineering. We propose Unification
Networks, an end-to-end differentiable neural network approach capable of
lifting examples into invariants and using those invariants to solve a given
task. The core characteristic of our architecture is soft unification between
examples that enables the network to generalise parts of the input into
variables, thereby learning invariants. We evaluate our approach on five
datasets to demonstrate that learning invariants captures patterns in the data
and can improve performance over baselines.
</p>
<a href="http://arxiv.org/abs/1909.07328" target="_blank">arXiv:1909.07328</a> [<a href="http://arxiv.org/pdf/1909.07328" target="_blank">pdf</a>]

<h2>Multi-task Batch Reinforcement Learning with Metric Learning. (arXiv:1909.11373v6 [cs.LG] UPDATED)</h2>
<h3>Jiachen Li, Quan Vuong, Shuang Liu, Minghua Liu, Kamil Ciosek, Keith Ross, Henrik Iskov Christensen, Hao Su</h3>
<p>We tackle the Multi-task Batch Reinforcement Learning problem. Given multiple
datasets collected from different tasks, we train a multi-task policy to
perform well in unseen tasks sampled from the same distribution. The task
identities of the unseen tasks are not provided. To perform well, the policy
must infer the task identity from collected transitions by modelling its
dependency on states, actions and rewards. Because the different datasets may
have state-action distributions with large divergence, the task inference
module can learn to ignore the rewards and spuriously correlate $\textit{only}$
state-action pairs to the task identity, leading to poor test time performance.
To robustify task inference, we propose a novel application of the triplet
loss. To mine hard negative examples, we relabel the transitions from the
training tasks by approximating their reward functions. When we allow further
training on the unseen tasks, using the trained policy as an initialization
leads to significantly faster convergence compared to randomly initialized
policies (up to $80\%$ improvement and across 5 different Mujoco task
distributions). We name our method $\textbf{MBML}$
($\textbf{M}\text{ulti-task}$ $\textbf{B}\text{atch}$ RL with
$\textbf{M}\text{etric}$ $\textbf{L}\text{earning}$).
</p>
<a href="http://arxiv.org/abs/1909.11373" target="_blank">arXiv:1909.11373</a> [<a href="http://arxiv.org/pdf/1909.11373" target="_blank">pdf</a>]

<h2>Learning an Action-Conditional Model for Haptic Texture Generation. (arXiv:1909.13025v2 [cs.RO] UPDATED)</h2>
<h3>Negin Heravi, Wenzhen Yuan, Allison M. Okamura, Jeannette Bohg</h3>
<p>Rich haptic sensory feedback in response to user interactions is desirable
for an effective, immersive virtual reality or teleoperation system. However,
this feedback depends on material properties and user interactions in a
complex, non-linear manner. Therefore, it is challenging to model the mapping
from material and user interactions to haptic feedback in a way that
generalizes over many variations of the user's input. Current methodologies are
typically conditioned on user interactions, but require a separate model for
each material. In this paper, we present a learned action-conditional model
that uses data from a vision-based tactile sensor (GelSight) and user's action
as input. This model predicts an induced acceleration that could be used to
provide haptic vibration feedback to a user. We trained our proposed model on a
publicly available dataset (Penn Haptic Texture Toolkit) that we augmented with
GelSight measurements of the different materials. We show that a unified model
over all materials outperforms previous methods and generalizes to new actions
and new instances of the material categories in the dataset.
</p>
<a href="http://arxiv.org/abs/1909.13025" target="_blank">arXiv:1909.13025</a> [<a href="http://arxiv.org/pdf/1909.13025" target="_blank">pdf</a>]

<h2>On the Detection of Digital Face Manipulation. (arXiv:1910.01717v5 [cs.CV] UPDATED)</h2>
<h3>Hao Dang, Feng Liu, Joel Stehouwer, Xiaoming Liu, Anil Jain</h3>
<p>Detecting manipulated facial images and videos is an increasingly important
topic in digital media forensics. As advanced face synthesis and manipulation
methods are made available, new types of fake face representations are being
created which have raised significant concerns for their use in social media.
Hence, it is crucial to detect manipulated face images and localize manipulated
regions. Instead of simply using multi-task learning to simultaneously detect
manipulated images and predict the manipulated mask (regions), we propose to
utilize an attention mechanism to process and improve the feature maps for the
classification task. The learned attention maps highlight the informative
regions to further improve the binary classification (genuine face v. fake
face), and also visualize the manipulated regions. To enable our study of
manipulated face detection and localization, we collect a large-scale database
that contains numerous types of facial forgeries. With this dataset, we perform
a thorough analysis of data-driven fake face detection. We show that the use of
an attention mechanism improves facial forgery detection and manipulated region
localization.
</p>
<a href="http://arxiv.org/abs/1910.01717" target="_blank">arXiv:1910.01717</a> [<a href="http://arxiv.org/pdf/1910.01717" target="_blank">pdf</a>]

<h2>Shooting Labels: 3D Semantic Labeling by Virtual Reality. (arXiv:1910.05021v2 [cs.CV] UPDATED)</h2>
<h3>Pierluigi Zama Ramirez, Claudio Paternesi, Luca De Luigi, Luigi Lella, Daniele De Gregorio, Luigi Di Stefano</h3>
<p>Availability of a few, large-size, annotated datasets, like ImageNet, Pascal
VOC and COCO, has lead deep learning to revolutionize computer vision research
by achieving astonishing results in several vision tasks.We argue that new
tools to facilitate generation of annotated datasets may help spreading
data-driven AI throughout applications and domains. In this work we propose
Shooting Labels, the first 3D labeling tool for dense 3D semantic segmentation
which exploits Virtual Reality to render the labeling task as easy and fun as
playing a video-game. Our tool allows for semantically labeling large scale
environments very expeditiously, whatever the nature of the 3D data at hand
(e.g. point clouds, mesh). Furthermore, Shooting Labels efficiently integrates
multiusers annotations to improve the labeling accuracy automatically and
compute a label uncertainty map. Besides, within our framework the 3D
annotations can be projected into 2D images, thereby speeding up also a
notoriously slow and expensive task such as pixel-wise semantic labeling. We
demonstrate the accuracy and efficiency of our tool in two different scenarios:
an indoor workspace provided by Matterport3D and a large-scale outdoor
environment reconstructed from 1000+ KITTI images.
</p>
<a href="http://arxiv.org/abs/1910.05021" target="_blank">arXiv:1910.05021</a> [<a href="http://arxiv.org/pdf/1910.05021" target="_blank">pdf</a>]

<h2>Interstellar: Searching Recurrent Architecture for Knowledge Graph Embedding. (arXiv:1911.07132v2 [cs.LG] UPDATED)</h2>
<h3>Yongqi Zhang, Quanming Yao, Lei Chen</h3>
<p>Knowledge graph (KG) embedding is well-known in learning representations of
KGs. Many models have been proposed to learn the interactions between entities
and relations of the triplets. However, long-term information among multiple
triplets is also important to KG. In this work, based on the relational paths,
which are composed of a sequence of triplets, we define the Interstellar as a
recurrent neural architecture search problem for the short-term and long-term
information along the paths. First, we analyze the difficulty of using a
unified model to work as the Interstellar. Then, we propose to search for
recurrent architecture as the Interstellar for different KG tasks. A case study
on synthetic data illustrates the importance of the defined search problem.
Experiments on real datasets demonstrate the effectiveness of the searched
models and the efficiency of the proposed hybrid-search algorithm.
</p>
<a href="http://arxiv.org/abs/1911.07132" target="_blank">arXiv:1911.07132</a> [<a href="http://arxiv.org/pdf/1911.07132" target="_blank">pdf</a>]

<h2>Casimir effect with machine learning. (arXiv:1911.07571v2 [hep-lat] UPDATED)</h2>
<h3>M. N. Chernodub, Harold Erbin, I. V. Grishmanovskii, V. A. Goy, A. V. Molochkov</h3>
<p>Vacuum fluctuations of quantum fields between physical objects depend on the
shapes, positions, and internal composition of the latter. For objects of
arbitrary shapes, even made from idealized materials, the calculation of the
associated zero-point (Casimir) energy is an analytically intractable
challenge. We propose a new numerical approach to this problem based on
machine-learning techniques and illustrate the effectiveness of the method in a
(2+1) dimensional scalar field theory. The Casimir energy is first calculated
numerically using a Monte-Carlo algorithm for a set of the Dirichlet boundaries
of various shapes. Then, a neural network is trained to compute this energy
given the Dirichlet domain, treating the latter as black-and-white pixelated
images. We show that after the learning phase, the neural network is able to
quickly predict the Casimir energy for new boundaries of general shapes with
reasonable accuracy.
</p>
<a href="http://arxiv.org/abs/1911.07571" target="_blank">arXiv:1911.07571</a> [<a href="http://arxiv.org/pdf/1911.07571" target="_blank">pdf</a>]

<h2>Self-Supervised Learning by Cross-Modal Audio-Video Clustering. (arXiv:1911.12667v3 [cs.CV] UPDATED)</h2>
<h3>Humam Alwassel, Dhruv Mahajan, Bruno Korbar, Lorenzo Torresani, Bernard Ghanem, Du Tran</h3>
<p>Visual and audio modalities are highly correlated, yet they contain different
information. Their strong correlation makes it possible to predict the
semantics of one from the other with good accuracy. Their intrinsic differences
make cross-modal prediction a potentially more rewarding pretext task for
self-supervised learning of video and audio representations compared to
within-modality learning. Based on this intuition, we propose Cross-Modal Deep
Clustering (XDC), a novel self-supervised method that leverages unsupervised
clustering in one modality (e.g., audio) as a supervisory signal for the other
modality (e.g., video). This cross-modal supervision helps XDC utilize the
semantic correlation and the differences between the two modalities. Our
experiments show that XDC outperforms single-modality clustering and other
multi-modal variants. XDC achieves state-of-the-art accuracy among
self-supervised methods on multiple video and audio benchmarks. Most
importantly, our video model pretrained on large-scale unlabeled data
significantly outperforms the same model pretrained with full-supervision on
ImageNet and Kinetics for action recognition on HMDB51 and UCF101. To the best
of our knowledge, XDC is the first self-supervised learning method that
outperforms large-scale fully-supervised pretraining for action recognition on
the same architecture.
</p>
<a href="http://arxiv.org/abs/1911.12667" target="_blank">arXiv:1911.12667</a> [<a href="http://arxiv.org/pdf/1911.12667" target="_blank">pdf</a>]

<h2>SEEF-ALDR: A Speaker Embedding Enhancement Framework via Adversarial Learning based Disentangled Representation. (arXiv:1912.02608v4 [eess.AS] UPDATED)</h2>
<h3>Jianwei Tai, Xiaoqi Jia, Qingjia Huang, Weijuan Zhang, Haichao Du, Shengzhi Zhang</h3>
<p>Speaker verification, as a biometric authentication mechanism, has been
widely used due to the pervasiveness of voice control on smart devices.
However, the task of "in-the-wild" speaker verification is still challenging,
considering the speech samples may contain lots of identity-unrelated
information, e.g., background noise, reverberation, emotion, etc. Previous
works focus on optimizing the model to improve verification accuracy, without
taking into account the elimination of the impact from the identity-unrelated
information. To solve the above problem, we propose SEEF-ALDR, a novel Speaker
Embedding Enhancement Framework via Adversarial Learning based Disentangled
Representation, to reinforce the performance of existing models on speaker
verification. The key idea is to retrieve as much speaker identity information
as possible from the original speech, thus minimizing the impact of
identity-unrelated information on the speaker verification task by using
adversarial learning. Experimental results demonstrate that the proposed
framework can significantly improve the performance of speaker verification by
20.3% and 23.8% on average over 13 tested baselines on dataset Voxceleb1 and 8
tested baselines on dataset Voxceleb2 respectively, without adjusting the
structure or hyper-parameters of them. Furthermore, the ablation study was
conducted to evaluate the contribution of each module in SEEF-ALDR. Finally,
porting an existing model into the proposed framework is straightforward and
cost-efficient, with very little effort from the model owners due to the
modular design of the framework.
</p>
<a href="http://arxiv.org/abs/1912.02608" target="_blank">arXiv:1912.02608</a> [<a href="http://arxiv.org/pdf/1912.02608" target="_blank">pdf</a>]

<h2>Multimodal Self-Supervised Learning for Medical Image Analysis. (arXiv:1912.05396v2 [cs.CV] UPDATED)</h2>
<h3>Aiham Taleb, Christoph Lippert, Tassilo Klein, Moin Nabi</h3>
<p>Self-supervised learning approaches leverage unlabeled samples to acquire
generic knowledge about different concepts, hence allowing for
annotation-efficient downstream task learning. In this paper, we propose a
novel self-supervised method that leverages multiple imaging modalities. We
introduce the multimodal puzzle task, which facilitates rich representation
learning from multiple image modalities. The learned representations allow for
subsequent fine-tuning on different downstream tasks. To achieve that, we learn
a modality-agnostic feature embedding by confusing image modalities at the
data-level. Together with the Sinkhorn operator, with which we formulate the
puzzle solving optimization as permutation matrix inference instead of
classification, they allow for efficient solving of multimodal puzzles with
varying levels of complexity. In addition, we also propose to utilize
cross-modal generation techniques for multimodal data augmentation used for
training self-supervised tasks. In other words, we exploit synthetic images for
self-supervised pretraining, instead of downstream tasks directly, in order to
circumvent quality issues associated with synthetic images, while improving
data-efficiency and representations quality. Our experimental results, which
assess the gains in downstream performance and data-efficiency, show that
solving our multimodal puzzles yields better semantic representations, compared
to treating each modality independently. Our results also highlight the
benefits of exploiting synthetic images for self-supervised pretraining. We
showcase our approach on four downstream tasks: Brain tumor segmentation and
survival days prediction using four MRI modalities, Prostate segmentation using
two MRI modalities, and Liver segmentation using unregistered CT and MRI
modalities. We outperform many previous solutions, and achieve results
competitive to state-of-the-art.
</p>
<a href="http://arxiv.org/abs/1912.05396" target="_blank">arXiv:1912.05396</a> [<a href="http://arxiv.org/pdf/1912.05396" target="_blank">pdf</a>]

<h2>Joint Architecture and Knowledge Distillation in CNN for Chinese Text Recognition. (arXiv:1912.07806v3 [cs.CV] UPDATED)</h2>
<h3>Zi-Rui Wang, Jun Du</h3>
<p>The technique of distillation helps transform cumbersome neural network into
compact network so that the model can be deployed on alternative hardware
devices. The main advantages of distillation based approaches include simple
training process, supported by most off-the-shelf deep learning softwares and
no special requirement of hardwares. In this paper, we propose a guideline to
distill the architecture and knowledge of pre-trained standard CNNs
simultaneously. We first make a quantitative analysis of the baseline network,
including computational cost and storage overhead in different components. And
then, according to the analysis results, optional strategies can be adopted to
the compression of fully-connected layers. For vanilla convolution layers, the
proposed parsimonious convolution (ParConv) block only consisting of depthwise
separable convolution and pointwise convolution is used as a direct replacement
without other adjustments such as the widths and depths in the network.
Finally, the knowledge distillation with multiple losses is adopted to improve
performance of the compact CNN. The proposed algorithm is first verified on
offline handwritten Chinese text recognition (HCTR) where the CNNs are
characterized by tens of thousands of output nodes and trained by hundreds of
millions of training samples. Compared with the CNN in the state-of-the-art
system, our proposed joint architecture and knowledge distillation can reduce
the computational cost by &gt;10x and model size by &gt;8x with negligible accuracy
loss. And then, by conducting experiments on one of the most popular data sets:
MNIST, we demonstrate the proposed approach can also be successfully applied on
mainstream backbone networks.
</p>
<a href="http://arxiv.org/abs/1912.07806" target="_blank">arXiv:1912.07806</a> [<a href="http://arxiv.org/pdf/1912.07806" target="_blank">pdf</a>]

<h2>Relative Flatness and Generalization in the Interpolation Regime. (arXiv:2001.00939v3 [cs.LG] UPDATED)</h2>
<h3>Henning Petzka, Michael Kamp, Linara Adilova, Mario Boley, Cristian Sminchisescu</h3>
<p>Traditional generalization bounds are based on analyzing the limits of the
model capacity. Therefore, they become vacuous in the \emph{interpolation}
(over-parameterized) regime of modern machine learning models where training
data can be fitted perfectly. This paper proposes a new approach to meaningful
generalization bounds in the interpolation regime by decomposing the
generalization gap into a notion of \emph{representativeness} and \emph{feature
robustness}. Representativeness captures properties of the data distribution
and mitigates the dependence on the data dimension by exploiting the
low-dimensional feature representation used implicitly by the model, and
feature robustness captures the expected change in loss resulting from
perturbations of these implicit features. We show that feature robustness can
be bounded by a relative flatness measure of the empirical loss surface for
models that locally minimize the training loss. This yields an
algorithm-agnostic bound potentially explaining the abundance of empirical
observations that flatness of the loss surface is correlated with
generalization.
</p>
<a href="http://arxiv.org/abs/2001.00939" target="_blank">arXiv:2001.00939</a> [<a href="http://arxiv.org/pdf/2001.00939" target="_blank">pdf</a>]

<h2>MONSTOR: An Inductive Approach for Estimating and Maximizing Influence over Unseen Networks. (arXiv:2001.08853v5 [cs.SI] UPDATED)</h2>
<h3>Jihoon Ko, Kyuhan Lee, Kijung Shin, Noseong Park</h3>
<p>Influence maximization (IM) is one of the most important problems in social
network analysis. Its objective is to find a given number of seed nodes that
maximize the spread of information through a social network. Since it is an
NP-hard problem, many approximate/heuristic methods have been developed, and a
number of them repeat Monte Carlo (MC) simulations over and over to reliably
estimate the influence (i.e., the number of infected nodes) of a seed set. In
this work, we present an inductive machine learning method, called Monte Carlo
Simulator (MONSTOR), for estimating the influence of given seed nodes in social
networks unseen during training. To the best of our knowledge, MONSTOR is the
first inductive method for this purpose. MONSTOR can greatly accelerate
existing IM algorithms by replacing repeated MC simulations. In our
experiments, MONSTOR provided highly accurate estimates, achieving 0.998 or
higher Pearson and Spearman correlation coefficients in unseen real-world
social networks. Moreover, IM algorithms equipped with MONSTOR are more
accurate than state-of-the-art competitors in 63% of IM use cases.
</p>
<a href="http://arxiv.org/abs/2001.08853" target="_blank">arXiv:2001.08853</a> [<a href="http://arxiv.org/pdf/2001.08853" target="_blank">pdf</a>]

<h2>Transforming Spectrum and Prosody for Emotional Voice Conversion with Non-Parallel Training Data. (arXiv:2002.00198v5 [eess.AS] UPDATED)</h2>
<h3>Kun Zhou, Berrak Sisman, Haizhou Li</h3>
<p>Emotional voice conversion aims to convert the spectrum and prosody to change
the emotional patterns of speech, while preserving the speaker identity and
linguistic content. Many studies require parallel speech data between different
emotional patterns, which is not practical in real life. Moreover, they often
model the conversion of fundamental frequency (F0) with a simple linear
transform. As F0 is a key aspect of intonation that is hierarchical in nature,
we believe that it is more adequate to model F0 in different temporal scales by
using wavelet transform. We propose a CycleGAN network to find an optimal
pseudo pair from non-parallel training data by learning forward and inverse
mappings simultaneously using adversarial and cycle-consistency losses. We also
study the use of continuous wavelet transform (CWT) to decompose F0 into ten
temporal scales, that describes speech prosody at different time resolution,
for effective F0 conversion. Experimental results show that our proposed
framework outperforms the baselines both in objective and subjective
evaluations.
</p>
<a href="http://arxiv.org/abs/2002.00198" target="_blank">arXiv:2002.00198</a> [<a href="http://arxiv.org/pdf/2002.00198" target="_blank">pdf</a>]

<h2>SuperDTI: Ultrafast diffusion tensor imaging and fiber tractography with deep learning. (arXiv:2002.01031v3 [eess.IV] UPDATED)</h2>
<h3>Hongyu Li, Zifei Liang, Chaoyi Zhang, Ruiying Liu, Jing Li, Weihong Zhang, Dong Liang, Bowen Shen, Xiaoliang Zhang, Yulin Ge, Jiangyang Zhang, Leslie Ying</h3>
<p>Purpose: To propose a deep learning-based reconstruction framework for
ultrafast and robust diffusion tensor imaging and fiber tractography. Methods:
We propose SuperDTI to learn the nonlinear relationship between
diffusion-weighted images (DWIs) (reduced in q-space and k-space) and the
corresponding tensor-derived quantitative maps as well as the fiber
tractography. Super DTI bypasses the tensor fitting procedure, which is well
known to be highly susceptible to noise and artifacts in DWIs. The network is
trained and tested using datasets from Human Connectome Project and patients
with ischemic stroke. The noise robustness of the network and the lesion
detectability of the reconstructed maps are evaluated. SuperDTI is compared
against the state-of-the-art methods for diffusion map reconstruction and fiber
tracking. Results: The proposed technique is able to generate fractional
anisotropy and mean diffusivity maps, as well as fiber tractography, from as
few as six undersampled raw DWIs. SuperDTI achieves a quantification error of
less than 5% in all regions of interest in white matter and gray matter
structures. In addition, we demonstrate that the trained neural network is
robust to additional noise in the testing data, and the network trained using
healthy volunteer data can be directly applied to stroke patient data without
compromising the lesion detectability. Conclusion: This paper demonstrates the
feasibility of superfast diffusion tensor imaging and fiber tractography using
deep learning with as few as six corrupted DWIs (up to 30-fold). Such a
significant reduction in scan time will allow the inclusion of DTI into
clinical routine for many potential applications.
</p>
<a href="http://arxiv.org/abs/2002.01031" target="_blank">arXiv:2002.01031</a> [<a href="http://arxiv.org/pdf/2002.01031" target="_blank">pdf</a>]

<h2>PLLay: Efficient Topological Layer based on Persistence Landscapes. (arXiv:2002.02778v3 [cs.LG] UPDATED)</h2>
<h3>Kwangho Kim, Jisu Kim, Manzil Zaheer, Joon Sik Kim, Frederic Chazal, Larry Wasserman</h3>
<p>We propose PLLay, a novel topological layer for general deep learning models
based on persistence landscapes, in which we can efficiently exploit the
underlying topological features of the input data structure. In this work, we
show differentiability with respect to layer inputs, for a general persistent
homology with arbitrary filtration. Thus, our proposed layer can be placed
anywhere in the network and feed critical information on the topological
features of input data into subsequent layers to improve the learnability of
the networks toward a given task. A task-optimal structure of PLLay is learned
during training via backpropagation, without requiring any input featurization
or data preprocessing. We provide a novel adaptation for the DTM function-based
filtration, and show that the proposed layer is robust against noise and
outliers through a stability analysis. We demonstrate the effectiveness of our
approach by classification experiments on various datasets.
</p>
<a href="http://arxiv.org/abs/2002.02778" target="_blank">arXiv:2002.02778</a> [<a href="http://arxiv.org/pdf/2002.02778" target="_blank">pdf</a>]

<h2>Self-Distillation Amplifies Regularization in Hilbert Space. (arXiv:2002.05715v3 [cs.LG] UPDATED)</h2>
<h3>Hossein Mobahi, Mehrdad Farajtabar, Peter L. Bartlett</h3>
<p>Knowledge distillation introduced in the deep learning context is a method to
transfer knowledge from one architecture to another. In particular, when the
architectures are identical, this is called self-distillation. The idea is to
feed in predictions of the trained model as new target values for retraining
(and iterate this loop possibly a few times). It has been empirically observed
that the self-distilled model often achieves higher accuracy on held out data.
Why this happens, however, has been a mystery: the self-distillation dynamics
does not receive any new information about the task and solely evolves by
looping over training. To the best of our knowledge, there is no rigorous
understanding of this phenomenon. This work provides the first theoretical
analysis of self-distillation. We focus on fitting a nonlinear function to
training data, where the model space is Hilbert space and fitting is subject to
$\ell_2$ regularization in this function space. We show that self-distillation
iterations modify regularization by progressively limiting the number of basis
functions that can be used to represent the solution. This implies (as we also
verify empirically) that while a few rounds of self-distillation may reduce
over-fitting, further rounds may lead to under-fitting and thus worse
performance.
</p>
<a href="http://arxiv.org/abs/2002.05715" target="_blank">arXiv:2002.05715</a> [<a href="http://arxiv.org/pdf/2002.05715" target="_blank">pdf</a>]

<h2>First Order Constrained Optimization in Policy Space. (arXiv:2002.06506v2 [cs.LG] UPDATED)</h2>
<h3>Yiming Zhang, Quan Vuong, Keith W. Ross</h3>
<p>In reinforcement learning, an agent attempts to learn high-performing
behaviors through interacting with the environment, such behaviors are often
quantified in the form of a reward function. However some aspects of
behavior-such as ones which are deemed unsafe and to be avoided-are best
captured through constraints. We propose a novel approach called First Order
Constrained Optimization in Policy Space (FOCOPS) which maximizes an agent's
overall reward while ensuring the agent satisfies a set of cost constraints.
Using data generated from the current policy, FOCOPS first finds the optimal
update policy by solving a constrained optimization problem in the
nonparameterized policy space. FOCOPS then projects the update policy back into
the parametric policy space. Our approach has an approximate upper bound for
worst-case constraint violation throughout training and is first-order in
nature therefore simple to implement. We provide empirical evidence that our
simple approach achieves better performance on a set of constrained robotics
locomotive tasks.
</p>
<a href="http://arxiv.org/abs/2002.06506" target="_blank">arXiv:2002.06506</a> [<a href="http://arxiv.org/pdf/2002.06506" target="_blank">pdf</a>]

<h2>Stochastic Normalizing Flows. (arXiv:2002.06707v3 [stat.ML] UPDATED)</h2>
<h3>Hao Wu, Jonas K&#xf6;hler, Frank No&#xe9;</h3>
<p>The sampling of probability distributions specified up to a normalization
constant is an important problem in both machine learning and statistical
mechanics. While classical stochastic sampling methods such as Markov Chain
Monte Carlo (MCMC) or Langevin Dynamics (LD) can suffer from slow mixing times
there is a growing interest in using normalizing flows in order to learn the
transformation of a simple prior distribution to the given target distribution.
Here we propose a generalized and combined approach to sample target densities:
Stochastic Normalizing Flows (SNF) -- an arbitrary sequence of deterministic
invertible functions and stochastic sampling blocks. We show that stochasticity
overcomes expressivity limitations of normalizing flows resulting from the
invertibility constraint, whereas trainable transformations between sampling
steps improve efficiency of pure MCMC/LD along the flow. By invoking ideas from
non-equilibrium statistical mechanics we derive an efficient training procedure
by which both the sampler's and the flow's parameters can be optimized
end-to-end, and by which we can compute exact importance weights without having
to marginalize out the randomness of the stochastic blocks. We illustrate the
representational power, sampling efficiency and asymptotic correctness of SNFs
on several benchmarks including applications to sampling molecular systems in
equilibrium.
</p>
<a href="http://arxiv.org/abs/2002.06707" target="_blank">arXiv:2002.06707</a> [<a href="http://arxiv.org/pdf/2002.06707" target="_blank">pdf</a>]

<h2>Learning Group Structure and Disentangled Representations of Dynamical Environments. (arXiv:2002.06991v2 [cs.LG] UPDATED)</h2>
<h3>Robin Quessard, Thomas D. Barrett, William R. Clements</h3>
<p>Learning disentangled representations is a key step towards effectively
discovering and modelling the underlying structure of environments. In the
natural sciences, physics has found great success by describing the universe in
terms of symmetry preserving transformations. Inspired by this formalism, we
propose a framework, built upon the theory of group representation, for
learning representations of a dynamical environment structured around the
transformations that generate its evolution. Experimentally, we learn the
structure of explicitly symmetric environments without supervision from
observational data generated by sequential interactions. We further introduce
an intuitive disentanglement regularisation to ensure the interpretability of
the learnt representations. We show that our method enables accurate
long-horizon predictions, and demonstrate a correlation between the quality of
predictions and disentanglement in the latent space.
</p>
<a href="http://arxiv.org/abs/2002.06991" target="_blank">arXiv:2002.06991</a> [<a href="http://arxiv.org/pdf/2002.06991" target="_blank">pdf</a>]

<h2>Calibrating Deep Neural Networks using Focal Loss. (arXiv:2002.09437v2 [cs.LG] UPDATED)</h2>
<h3>Jishnu Mukhoti, Viveka Kulharia, Amartya Sanyal, Stuart Golodetz, Philip H.S. Torr, Puneet K. Dokania</h3>
<p>Miscalibration - a mismatch between a model's confidence and its correctness
- of Deep Neural Networks (DNNs) makes their predictions hard to rely on.
Ideally, we want networks to be accurate, calibrated and confident. We show
that, as opposed to the standard cross-entropy loss, focal loss [Lin et. al.,
2017] allows us to learn models that are already very well calibrated. When
combined with temperature scaling, whilst preserving accuracy, it yields
state-of-the-art calibrated models. We provide a thorough analysis of the
factors causing miscalibration, and use the insights we glean from this to
justify the empirically excellent performance of focal loss. To facilitate the
use of focal loss in practice, we also provide a principled approach to
automatically select the hyperparameter involved in the loss function. We
perform extensive experiments on a variety of computer vision and NLP datasets,
and with a wide variety of network architectures, and show that our approach
achieves state-of-the-art calibration without compromising on accuracy in
almost all cases. Code is available at
https://github.com/torrvision/focal_calibration.
</p>
<a href="http://arxiv.org/abs/2002.09437" target="_blank">arXiv:2002.09437</a> [<a href="http://arxiv.org/pdf/2002.09437" target="_blank">pdf</a>]

<h2>Handling the Positive-Definite Constraint in the Bayesian Learning Rule. (arXiv:2002.10060v13 [stat.ML] UPDATED)</h2>
<h3>Wu Lin, Mark Schmidt, Mohammad Emtiyaz Khan</h3>
<p>The Bayesian learning rule is a natural-gradient variational inference
method, which not only contains many existing learning algorithms as special
cases but also enables the design of new algorithms. Unfortunately, when
variational parameters lie in an open constraint set, the rule may not satisfy
the constraint and requires line-searches which could slow down the algorithm.
In this work, we address this issue for positive-definite constraints by
proposing an improved rule that naturally handles the constraints. Our
modification is obtained by using Riemannian gradient methods, and is valid
when the approximation attains a \emph{block-coordinate natural
parameterization} (e.g., Gaussian distributions and their mixtures). We propose
a principled way to derive Riemannian gradients and retractions from scratch.
Our method outperforms existing methods without any significant increase in
computation. Our work makes it easier to apply the rule in the presence of
positive-definite constraints in parameter spaces.
</p>
<a href="http://arxiv.org/abs/2002.10060" target="_blank">arXiv:2002.10060</a> [<a href="http://arxiv.org/pdf/2002.10060" target="_blank">pdf</a>]

<h2>Temporal Spike Sequence Learning via Backpropagation for Deep Spiking Neural Networks. (arXiv:2002.10085v3 [cs.NE] UPDATED)</h2>
<h3>Wenrui Zhang, Peng Li</h3>
<p>Spiking neural networks (SNNs) are well suited for spatio-temporal learning
and implementations on energy-efficient event-driven neuromorphic processors.
However, existing SNN error backpropagation (BP) methods lack proper handling
of spiking discontinuities and suffer from low performance compared with the BP
methods for traditional artificial neural networks. In addition, a large number
of time steps are typically required to achieve decent performance, leading to
high latency and rendering spike-based computation unscalable to deep
architectures. We present a novel Temporal Spike Sequence Learning
Backpropagation (TSSL-BP) method for training deep SNNs, which breaks down
error backpropagation across two types of inter-neuron and intra-neuron
dependencies and leads to improved temporal learning precision. It captures
inter-neuron dependencies through presynaptic firing times by considering the
all-or-none characteristics of firing activities and captures intra-neuron
dependencies by handling the internal evolution of each neuronal state in time.
TSSL-BP efficiently trains deep SNNs within a much shortened temporal window of
a few steps while improving the accuracy for various image classification
datasets including CIFAR10.
</p>
<a href="http://arxiv.org/abs/2002.10085" target="_blank">arXiv:2002.10085</a> [<a href="http://arxiv.org/pdf/2002.10085" target="_blank">pdf</a>]

<h2>xAI-GAN: Enhancing Generative Adversarial Networks via Explainable AI Systems. (arXiv:2002.10438v2 [cs.LG] UPDATED)</h2>
<h3>Vineel Nagisetty, Laura Graves, Joseph Scott, Vijay Ganesh</h3>
<p>Generative Adversarial Networks (GANs) are a revolutionary class of Deep
Neural Networks (DNNs) that have been successfully used to generate realistic
images, music, text, and other data. However, GAN training presents many
challenges, notably it can be very resource-intensive. A potential weakness in
GANs is that it requires a lot of data for successful training and data
collection can be an expensive process. Typically, the corrective feedback from
discriminator DNNs to generator DNNs (namely, the discriminator's assessment of
the generated example) is calculated using only one real-numbered value (loss).
By contrast, we propose a new class of GAN we refer to as xAI-GAN that
leverages recent advances in explainable AI (xAI) systems to provide a "richer"
form of corrective feedback from discriminators to generators. Specifically, we
modify the gradient descent process using xAI systems that specify the reason
as to why the discriminator made the classification it did, thus providing the
"richer" corrective feedback that helps the generator to better fool the
discriminator. Using our approach, we observe xAI-GANs provide an improvement
of up to 23.18% in the quality of generated images on both MNIST and FMNIST
datasets over standard GANs as measured by Fr\'echet Inception Distance (FID).
We further compare xAI-GAN trained on 20% of the data with standard GAN trained
on 100% of data on the CIFAR10 dataset and find that xAI-GAN still shows an
improvement in FID score. Further, we compare our work with Differentiable
Augmentation - which has been shown to make GANs data-efficient - and show that
xAI-GANs outperform GANs trained on Differentiable Augmentation. Moreover, both
techniques can be combined to produce even better results. Finally, we argue
that xAI-GAN enables users greater control over how models learn than standard
GANs.
</p>
<a href="http://arxiv.org/abs/2002.10438" target="_blank">arXiv:2002.10438</a> [<a href="http://arxiv.org/pdf/2002.10438" target="_blank">pdf</a>]

<h2>ICE-BeeM: Identifiable Conditional Energy-Based Deep Models Based on Nonlinear ICA. (arXiv:2002.11537v4 [stat.ML] UPDATED)</h2>
<h3>Ilyes Khemakhem, Ricardo Pio Monti, Diederik P. Kingma, Aapo Hyv&#xe4;rinen</h3>
<p>We consider the identifiability theory of probabilistic models and establish
sufficient conditions under which the representations learned by a very broad
family of conditional energy-based models are unique in function space, up to a
simple transformation. In our model family, the energy function is the
dot-product between two feature extractors, one for the dependent variable, and
one for the conditioning variable. We show that under mild conditions, the
features are unique up to scaling and permutation. Our results extend recent
developments in nonlinear ICA, and in fact, they lead to an important
generalization of ICA models. In particular, we show that our model can be used
for the estimation of the components in the framework of Independently
Modulated Component Analysis (IMCA), a new generalization of nonlinear ICA that
relaxes the independence assumption. A thorough empirical study shows that
representations learned by our model from real-world image datasets are
identifiable, and improve performance in transfer learning and semi-supervised
learning tasks.
</p>
<a href="http://arxiv.org/abs/2002.11537" target="_blank">arXiv:2002.11537</a> [<a href="http://arxiv.org/pdf/2002.11537" target="_blank">pdf</a>]

<h2>DP-MERF: Differentially Private Mean Embeddings with Random Features for Practical Privacy-Preserving Data Generation. (arXiv:2002.11603v4 [cs.LG] UPDATED)</h2>
<h3>Frederik Harder, Kamil Adamczewski, Mijung Park</h3>
<p>We propose a differentially private data generation paradigm using random
feature representations of kernel mean embeddings when comparing the
distribution of true data with that of synthetic data. We exploit the random
feature representations for two important benefits. First, we require a minimal
privacy cost for training deep generative models. This is because unlike
kernel-based distance metrics that require computing the kernel matrix on all
pairs of true and synthetic data points, we can detach the data-dependent term
from the term solely dependent on synthetic data. Hence, we need to perturb the
data-dependent term once and for all and then use it repeatedly during the
generator training. Second, we can obtain an analytic sensitivity of the kernel
mean embedding as the random features are norm bounded by construction. This
removes the necessity of hyper-parameter search for a clipping norm to handle
the unknown sensitivity of a generator network. We provide several variants of
our algorithm, differentially private mean embeddings with random features
(DP-MERF) to jointly generate labels and input features for datasets such as
heterogeneous tabular data and image data. Our algorithm achieves drastically
better privacy-utility trade-offs than existing methods when tested on several
datasets.
</p>
<a href="http://arxiv.org/abs/2002.11603" target="_blank">arXiv:2002.11603</a> [<a href="http://arxiv.org/pdf/2002.11603" target="_blank">pdf</a>]

<h2>PushNet: Efficient and Adaptive Neural Message Passing. (arXiv:2003.02228v3 [cs.LG] UPDATED)</h2>
<h3>Julian Busch, Jiaxing Pi, Thomas Seidl</h3>
<p>Message passing neural networks have recently evolved into a state-of-the-art
approach to representation learning on graphs. Existing methods perform
synchronous message passing along all edges in multiple subsequent rounds and
consequently suffer from various shortcomings: Propagation schemes are
inflexible since they are restricted to $k$-hop neighborhoods and insensitive
to actual demands of information propagation. Further, long-range dependencies
cannot be modeled adequately and learned representations are based on
correlations of fixed locality. These issues prevent existing methods from
reaching their full potential in terms of prediction performance. Instead, we
consider a novel asynchronous message passing approach where information is
pushed only along the most relevant edges until convergence. Our proposed
algorithm can equivalently be formulated as a single synchronous message
passing iteration using a suitable neighborhood function, thus sharing the
advantages of existing methods while addressing their central issues. The
resulting neural network utilizes a node-adaptive receptive field derived from
meaningful sparse node neighborhoods. In addition, by learning and combining
node representations over differently sized neighborhoods, our model is able to
capture correlations on multiple scales. We further propose variants of our
base model with different inductive bias. Empirical results are provided for
semi-supervised node classification on five real-world datasets following a
rigorous evaluation protocol. We find that our models outperform competitors on
all datasets in terms of accuracy with statistical significance. In some cases,
our models additionally provide faster runtime.
</p>
<a href="http://arxiv.org/abs/2003.02228" target="_blank">arXiv:2003.02228</a> [<a href="http://arxiv.org/pdf/2003.02228" target="_blank">pdf</a>]

<h2>Low-viewpoint forest depth dataset for sparse rover swarms. (arXiv:2003.04359v2 [cs.RO] UPDATED)</h2>
<h3>Chaoyue Niu, Danesh Tarapore, Klaus-Peter Zauner</h3>
<p>Rapid progress in embedded computing hardware increasingly enables on-board
image processing on small robots. This development opens the path to replacing
costly sensors with sophisticated computer vision techniques. A case in point
is the prediction of scene depth information from a monocular camera for
autonomous navigation. Motivated by the aim to develop a robot swarm suitable
for sensing, monitoring, and search applications in forests, we have collected
a set of RGB images and corresponding depth maps. Over 100k images were
recorded with a custom rig from the perspective of a small ground rover moving
through a forest. Taken under different weather and lighting conditions, the
images include scenes with grass, bushes, standing and fallen trees, tree
branches, leafs, and dirt. In addition GPS, IMU, and wheel encoder data was
recorded. From the calibrated, synchronized, aligned and timestamped frames
about 9700 image-depth map pairs were selected for sharpness and variety. We
provide this dataset to the community to fill a need identified in our own
research and hope it will accelerate progress in robots navigating the
challenging forest environment. This paper describes our custom hardware and
methodology to collect the data, subsequent processing and quality of the data,
and how to access it.
</p>
<a href="http://arxiv.org/abs/2003.04359" target="_blank">arXiv:2003.04359</a> [<a href="http://arxiv.org/pdf/2003.04359" target="_blank">pdf</a>]

<h2>ARAE: Adversarially Robust Training of Autoencoders Improves Novelty Detection. (arXiv:2003.05669v2 [cs.CV] UPDATED)</h2>
<h3>Mohammadreza Salehi, Atrin Arya, Barbod Pajoum, Mohammad Otoofi, Amirreza Shaeiri, Mohammad Hossein Rohban, Hamid R. Rabiee</h3>
<p>Autoencoders (AE) have recently been widely employed to approach the novelty
detection problem. Trained only on the normal data, the AE is expected to
reconstruct the normal data effectively while fail to regenerate the anomalous
data, which could be utilized for novelty detection. However, in this paper, it
is demonstrated that this does not always hold. AE often generalizes so
perfectly that it can also reconstruct the anomalous data well. To address this
problem, we propose a novel AE that can learn more semantically meaningful
features. Specifically, we exploit the fact that adversarial robustness
promotes learning of meaningful features. Therefore, we force the AE to learn
such features by penalizing networks with a bottleneck layer that is unstable
against adversarial perturbations. We show that despite using a much simpler
architecture in comparison to the prior methods, the proposed AE outperforms or
is competitive to state-of-the-art on three benchmark datasets.
</p>
<a href="http://arxiv.org/abs/2003.05669" target="_blank">arXiv:2003.05669</a> [<a href="http://arxiv.org/pdf/2003.05669" target="_blank">pdf</a>]

<h2>Towards a General Theory of Infinite-Width Limits of Neural Classifiers. (arXiv:2003.05884v3 [stat.ML] UPDATED)</h2>
<h3>Eugene A. Golikov</h3>
<p>Obtaining theoretical guarantees for neural networks training appears to be a
hard problem in a general case. Recent research has been focused on studying
this problem in the limit of infinite width and two different theories have
been developed: a mean-field (MF) and a constant kernel (NTK) limit theories.
We propose a general framework that provides a link between these seemingly
distinct theories. Our framework out of the box gives rise to a discrete-time
MF limit which was not previously explored in the literature. We prove a
convergence theorem for it and show that it provides a more reasonable
approximation for finite-width nets compared to the NTK limit if learning rates
are not very small. Also, our framework suggests a limit model that coincides
neither with the MF limit nor with the NTK one. We show that for networks with
more than two hidden layers RMSProp training has a non-trivial discrete-time MF
limit but GD training does not have one. Overall, our framework demonstrates
that both MF and NTK limits have considerable limitations in approximating
finite-sized neural nets, indicating the need for designing more accurate
infinite-width approximations for them.
</p>
<a href="http://arxiv.org/abs/2003.05884" target="_blank">arXiv:2003.05884</a> [<a href="http://arxiv.org/pdf/2003.05884" target="_blank">pdf</a>]

<h2>Can Implicit Bias Explain Generalization? Stochastic Convex Optimization as a Case Study. (arXiv:2003.06152v2 [cs.LG] UPDATED)</h2>
<h3>Assaf Dauber, Meir Feder, Tomer Koren, Roi Livni</h3>
<p>The notion of implicit bias, or implicit regularization, has been suggested
as a means to explain the surprising generalization ability of modern-days
overparameterized learning algorithms. This notion refers to the tendency of
the optimization algorithm towards a certain structured solution that often
generalizes well. Recently, several papers have studied implicit regularization
and were able to identify this phenomenon in various scenarios. We revisit this
paradigm in arguably the simplest non-trivial setup, and study the implicit
bias of Stochastic Gradient Descent (SGD) in the context of Stochastic Convex
Optimization. As a first step, we provide a simple construction that rules out
the existence of a \emph{distribution-independent} implicit regularizer that
governs the generalization ability of SGD. We then demonstrate a learning
problem that rules out a very general class of \emph{distribution-dependent}
implicit regularizers from explaining generalization, which includes strongly
convex regularizers as well as non-degenerate norm-based regularizations.
Certain aspects of our constructions point out to significant difficulties in
providing a comprehensive explanation of an algorithm's generalization
performance by solely arguing about its implicit regularization properties.
</p>
<a href="http://arxiv.org/abs/2003.06152" target="_blank">arXiv:2003.06152</a> [<a href="http://arxiv.org/pdf/2003.06152" target="_blank">pdf</a>]

<h2>Universal Function Approximation on Graphs. (arXiv:2003.06706v3 [cs.DS] UPDATED)</h2>
<h3>Rickard Br&#xfc;el-Gabrielsson</h3>
<p>In this work we produce a framework for constructing universal function
approximators on graph isomorphism classes. We prove how this framework comes
with a collection of theoretically desirable properties and enables novel
analysis. We show how this allows us to achieve state-of-the-art performance on
four different well-known datasets in graph classification and separate classes
of graphs that other graph-learning methods cannot. Our approach is inspired by
persistent homology, dependency parsing for NLP, and multivalued functions. The
complexity of the underlying algorithm is O(#edges x #nodes) and code is
publicly available
(https://github.com/bruel-gabrielsson/universal-function-approximation-on-graphs).
</p>
<a href="http://arxiv.org/abs/2003.06706" target="_blank">arXiv:2003.06706</a> [<a href="http://arxiv.org/pdf/2003.06706" target="_blank">pdf</a>]

<h2>Unsupervised deep learning for text line segmentation. (arXiv:2003.08632v2 [cs.CV] UPDATED)</h2>
<h3>Berat Kurar Barakat, Ahmad Droby, Rym Alasam, Boraq Madi, Irina Rabaev, Raed Shammes, Jihad El-Sana</h3>
<p>We present an unsupervised deep learning method for text line segmentation
that is inspired by the relative variance between text lines and spaces among
text lines. Handwritten text line segmentation is important for the efficiency
of further processing. A common method is to train a deep learning network for
embedding the document image into an image of blob lines that are tracing the
text lines. Previous methods learned such embedding in a supervised manner,
requiring the annotation of many document images. This paper presents an
unsupervised embedding of document image patches without a need for
annotations. The number of foreground pixels over the text lines is relatively
different from the number of foreground pixels over the spaces among text
lines. Generating similar and different pairs relying on this principle
definitely leads to outliers. However, as the results show, the outliers do not
harm the convergence and the network learns to discriminate the text lines from
the spaces between text lines. Remarkably, with a challenging Arabic
handwritten text line segmentation dataset, VML-AHTE, we achieved superior
performance over the supervised methods. Additionally, the proposed method was
evaluated on the ICDAR 2017 and ICFHR 2010 handwritten text line segmentation
datasets.
</p>
<a href="http://arxiv.org/abs/2003.08632" target="_blank">arXiv:2003.08632</a> [<a href="http://arxiv.org/pdf/2003.08632" target="_blank">pdf</a>]

<h2>SOLOv2: Dynamic and Fast Instance Segmentation. (arXiv:2003.10152v3 [cs.CV] UPDATED)</h2>
<h3>Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li, Chunhua Shen</h3>
<p>In this work, we aim at building a simple, direct, and fast instance
segmentation framework with strong performance. We follow the principle of the
SOLO method of Wang et al. "SOLO: segmenting objects by locations".
Importantly, we take one step further by dynamically learning the mask head of
the object segmenter such that the mask head is conditioned on the location.
Specifically, the mask branch is decoupled into a mask kernel branch and mask
feature branch, which are responsible for learning the convolution kernel and
the convolved features respectively. Moreover, we propose Matrix NMS (non
maximum suppression) to significantly reduce the inference time overhead due to
NMS of masks. Our Matrix NMS performs NMS with parallel matrix operations in
one shot, and yields better results. We demonstrate a simple direct instance
segmentation system, outperforming a few state-of-the-art methods in both speed
and accuracy. A light-weight version of SOLOv2 executes at 31.3 FPS and yields
37.1% AP. Moreover, our state-of-the-art results in object detection (from our
mask byproduct) and panoptic segmentation show the potential to serve as a new
strong baseline for many instance-level recognition tasks besides instance
segmentation. Code is available at: https://git.io/AdelaiDet
</p>
<a href="http://arxiv.org/abs/2003.10152" target="_blank">arXiv:2003.10152</a> [<a href="http://arxiv.org/pdf/2003.10152" target="_blank">pdf</a>]

<h2>Adversarial System Variant Approximation to Quantify Process Model Generalization. (arXiv:2003.12168v2 [cs.AI] UPDATED)</h2>
<h3>Julian Theis, Houshang Darabi</h3>
<p>In process mining, process models are extracted from event logs using process
discovery algorithms and are commonly assessed using multiple quality
dimensions. While the metrics that measure the relationship of an extracted
process model to its event log are well-studied, quantifying the level by which
a process model can describe the unobserved behavior of its underlying system
falls short in the literature. In this paper, a novel deep learning-based
methodology called Adversarial System Variant Approximation (AVATAR) is
proposed to overcome this issue. Sequence Generative Adversarial Networks are
trained on the variants contained in an event log with the intention to
approximate the underlying variant distribution of the system behavior.
Unobserved realistic variants are sampled either directly from the Sequence
Generative Adversarial Network or by leveraging the Metropolis-Hastings
algorithm. The degree by which a process model relates to its underlying
unknown system behavior is then quantified based on the realistic observed and
estimated unobserved variants using established process model quality metrics.
Significant performance improvements in revealing realistic unobserved variants
are demonstrated in a controlled experiment on 15 ground truth systems.
Additionally, the proposed methodology is experimentally tested and evaluated
to quantify the generalization of 60 discovered process models with respect to
their systems.
</p>
<a href="http://arxiv.org/abs/2003.12168" target="_blank">arXiv:2003.12168</a> [<a href="http://arxiv.org/pdf/2003.12168" target="_blank">pdf</a>]

<h2>Log-Likelihood Ratio Minimizing Flows: Towards Robust and Quantifiable Neural Distribution Alignment. (arXiv:2003.12170v2 [cs.LG] UPDATED)</h2>
<h3>Ben Usman, Avneesh Sud, Nick Dufour, Kate Saenko</h3>
<p>Distribution alignment has many applications in deep learning, including
domain adaptation and unsupervised image-to-image translation. Most prior work
on unsupervised distribution alignment relies either on minimizing simple
non-parametric statistical distances such as maximum mean discrepancy or on
adversarial alignment. However, the former fails to capture the structure of
complex real-world distributions, while the latter is difficult to train and
does not provide any universal convergence guarantees or automatic quantitative
validation procedures. In this paper, we propose a new distribution alignment
method based on a log-likelihood ratio statistic and normalizing flows. We show
that, under certain assumptions, this combination yields a deep neural
likelihood-based minimization objective that attains a known lower bound upon
convergence. We experimentally verify that minimizing the resulting objective
results in domain alignment that preserves the local structure of input
domains.
</p>
<a href="http://arxiv.org/abs/2003.12170" target="_blank">arXiv:2003.12170</a> [<a href="http://arxiv.org/pdf/2003.12170" target="_blank">pdf</a>]

<h2>A Close Look at Deep Learning with Small Data. (arXiv:2003.12843v3 [cs.LG] UPDATED)</h2>
<h3>L. Brigato, L. Iocchi</h3>
<p>In this work, we perform a wide variety of experiments with different deep
learning architectures on datasets of limited size. According to our study, we
show that model complexity is a critical factor when only a few samples per
class are available. Differently from the literature, we show that in some
configurations, the state of the art can be improved using low complexity
models. For instance, in problems with scarce training samples and without data
augmentation, low-complexity convolutional neural networks perform comparably
well or better than state-of-the-art architectures. Moreover, we show that even
standard data augmentation can boost recognition performance by large margins.
This result suggests the development of more complex data
generation/augmentation pipelines for cases when data is limited. Finally, we
show that dropout, a widely used regularization technique, maintains its role
as a good regularizer even when data is scarce. Our findings are empirically
validated on the sub-sampled versions of popular CIFAR-10, Fashion-MNIST and,
SVHN benchmarks.
</p>
<a href="http://arxiv.org/abs/2003.12843" target="_blank">arXiv:2003.12843</a> [<a href="http://arxiv.org/pdf/2003.12843" target="_blank">pdf</a>]

<h2>Compositional Visual Generation and Inference with Energy Based Models. (arXiv:2004.06030v2 [cs.CV] UPDATED)</h2>
<h3>Yilun Du, Shuang Li, Igor Mordatch</h3>
<p>A vital aspect of human intelligence is the ability to compose increasingly
complex concepts out of simpler ideas, enabling both rapid learning and
adaptation of knowledge. In this paper we show that energy-based models can
exhibit this ability by directly combining probability distributions. Samples
from the combined distribution correspond to compositions of concepts. For
example, given a distribution for smiling faces, and another for male faces, we
can combine them to generate smiling male faces. This allows us to generate
natural images that simultaneously satisfy conjunctions, disjunctions, and
negations of concepts. We evaluate compositional generation abilities of our
model on the CelebA dataset of natural faces and synthetic 3D scene images. We
also demonstrate other unique advantages of our model, such as the ability to
continually learn and incorporate new concepts, or infer compositions of
concept properties underlying an image.
</p>
<a href="http://arxiv.org/abs/2004.06030" target="_blank">arXiv:2004.06030</a> [<a href="http://arxiv.org/pdf/2004.06030" target="_blank">pdf</a>]

<h2>Smart Inference for Multidigit Convolutional Neural Network based Barcode Decoding. (arXiv:2004.06297v2 [cs.CV] UPDATED)</h2>
<h3>Thao Do, Yalew Tolcha, Tae Joon Jun, Daeyoung Kim</h3>
<p>Barcodes are ubiquitous and have been used in most of critical daily
activities for decades. However, most of traditional decoders require
well-founded barcode under a relatively standard condition. While wilder
conditioned barcodes such as underexposed, occluded, blurry, wrinkled and
rotated are commonly captured in reality, those traditional decoders show
weakness of recognizing. Several works attempted to solve those challenging
barcodes, but many limitations still exist. This work aims to solve the
decoding problem using deep convolutional neural network with the possibility
of running on portable devices. Firstly, we proposed a special modification of
inference based on the feature of having checksum and test-time augmentation,
named as Smart Inference (SI) in prediction phase of a trained model. SI
considerably boosts accuracy and reduces the false prediction for trained
models. Secondly, we have created a large practical evaluation dataset of real
captured 1D barcode under various challenging conditions to test our methods
vigorously, which is publicly available for other researchers. The experiments'
results demonstrated the SI effectiveness with the highest accuracy of 95.85%
which outperformed many existing decoders on the evaluation set. Finally, we
successfully minimized the best model by knowledge distillation to a shallow
model which is shown to have high accuracy (90.85%) with good inference speed
of 34.2 ms per image on a real edge device.
</p>
<a href="http://arxiv.org/abs/2004.06297" target="_blank">arXiv:2004.06297</a> [<a href="http://arxiv.org/pdf/2004.06297" target="_blank">pdf</a>]

<h2>Learning from Aggregate Observations. (arXiv:2004.06316v2 [stat.ML] UPDATED)</h2>
<h3>Yivan Zhang, Nontawat Charoenphakdee, Zhenguo Wu, Masashi Sugiyama</h3>
<p>We study the problem of learning from aggregate observations where
supervision signals are given to sets of instances instead of individual
instances, while the goal is still to predict labels of unseen individuals. A
well-known example is multiple instance learning (MIL). In this paper, we
extend MIL beyond binary classification to other problems such as multiclass
classification and regression. We present a general probabilistic framework
that accommodates a variety of aggregate observations, e.g., pairwise
similarity/triplet comparison for classification and mean/difference/rank
observation for regression. Simple maximum likelihood solutions can be applied
to various differentiable models such as deep neural networks and gradient
boosting machines. Moreover, we develop the concept of consistency up to an
equivalence relation to characterize our estimator and show that it has nice
convergence properties under mild assumptions. Experiments on three problem
settings -- classification via triplet comparison and regression via mean/rank
observation indicate the effectiveness of the proposed method.
</p>
<a href="http://arxiv.org/abs/2004.06316" target="_blank">arXiv:2004.06316</a> [<a href="http://arxiv.org/pdf/2004.06316" target="_blank">pdf</a>]

<h2>Motion and Region Aware Adversarial Learning for Fall Detection with Thermal Imaging. (arXiv:2004.08352v2 [cs.CV] UPDATED)</h2>
<h3>Vineet Mehta, Abhinav Dhall, Sujata Pal, Shehroz S. Khan</h3>
<p>Automatic fall detection is a vital technology for ensuring the health and
safety of people. Home-based camera systems for fall detection often put
people's privacy at risk. Thermal cameras can partially or fully obfuscate
facial features, thus preserving the privacy of a person. Another challenge is
the less occurrence of falls in comparison to the normal activities of daily
living. As fall occurs rarely, it is non-trivial to learn algorithms due to
class imbalance. To handle these problems, we formulate fall detection as an
anomaly detection within an adversarial framework using thermal imaging. We
present a novel adversarial network that comprises of two-channel 3D
convolutional autoencoders which reconstructs the thermal data and the optical
flow input sequences respectively. We introduce a technique to track the region
of interest, a region-based difference constraint, and a joint discriminator to
compute the reconstruction error. A larger reconstruction error indicates the
occurrence of a fall. The experiments on a publicly available thermal fall
dataset show the superior results obtained compared to the standard baseline.
</p>
<a href="http://arxiv.org/abs/2004.08352" target="_blank">arXiv:2004.08352</a> [<a href="http://arxiv.org/pdf/2004.08352" target="_blank">pdf</a>]

<h2>Extending DeepSDF for automatic 3D shape retrieval and similarity transform estimation. (arXiv:2004.09048v3 [cs.CV] UPDATED)</h2>
<h3>Oladapo Afolabi, Allen Y. Yang, S. Shankar Sastry</h3>
<p>Recent advances in computer graphics and computer vision have found
successful application of deep neural network models for 3D shapes based on
signed distance functions (SDFs) that are useful for shape representation,
retrieval, and completion. However, this approach has been limited by the need
to have query shapes in the same canonical scale and pose as those observed
during training, restricting its effectiveness on real world scenes. We present
a formulation to overcome this issue by jointly estimating shape and similarity
transform parameters. We conduct experiments to demonstrate the effectiveness
of this formulation on synthetic and real datasets and report favorable
comparisons to the state of the art. Finally, we also emphasize the viability
of this approach as a form of 3D model compression.
</p>
<a href="http://arxiv.org/abs/2004.09048" target="_blank">arXiv:2004.09048</a> [<a href="http://arxiv.org/pdf/2004.09048" target="_blank">pdf</a>]

<h2>A Comprehensive Overview and Survey of Recent Advances in Meta-Learning. (arXiv:2004.11149v7 [cs.LG] UPDATED)</h2>
<h3>Huimin Peng</h3>
<p>This article reviews meta-learning also known as learning-to-learn which
seeks rapid and accurate model adaptation to unseen tasks with applications in
highly automated AI, few-shot learning, natural language processing and
robotics. Unlike deep learning, meta-learning can be applied to few-shot
high-dimensional datasets and considers further improving model generalization
to unseen tasks. Deep learning is focused upon in-sample prediction and
meta-learning concerns model adaptation for out-of-sample prediction.
Meta-learning can continually perform self-improvement to achieve highly
autonomous AI. Meta-learning may serve as an additional generalization block
complementary for original deep learning model. Meta-learning seeks adaptation
of machine learning models to unseen tasks which are vastly different from
trained tasks. Meta-learning with coevolution between agent and environment
provides solutions for complex tasks unsolvable by training from scratch.
Meta-learning methodology covers a wide range of great minds and thoughts. We
briefly introduce meta-learning methodologies in the following categories:
black-box meta-learning, metric-based meta-learning, layered meta-learning and
Bayesian meta-learning framework. Recent applications concentrate upon the
integration of meta-learning with other machine learning framework to provide
feasible integrated problem solutions. We briefly present recent meta-learning
advances and discuss potential future research directions.
</p>
<a href="http://arxiv.org/abs/2004.11149" target="_blank">arXiv:2004.11149</a> [<a href="http://arxiv.org/pdf/2004.11149" target="_blank">pdf</a>]

<h2>GIMP-ML: Python Plugins for using Computer Vision Models in GIMP. (arXiv:2004.13060v3 [cs.CV] UPDATED)</h2>
<h3>Kritik Soman</h3>
<p>This paper introduces GIMP-ML v1.1, a set of Python plugins for the widely
popular GNU Image Manipulation Program (GIMP). It enables the use of recent
advances in computer vision to the conventional image editing pipeline.
Applications from deep learning such as monocular depth estimation, semantic
segmentation, mask generative adversarial networks, image super-resolution,
de-noising, de-hazing, matting, enlightening and coloring have been
incorporated with GIMP through Python-based plugins. Additionally, operations
on images such as k-means based color clustering have also been added. GIMP-ML
relies on standard Python packages such as numpy, pytorch, open-cv, scipy.
Apart from these, several image manipulation techniques using these plugins
have been compiled and demonstrated in the YouTube channel
(https://youtube.com/user/kritiksoman) with the objective of demonstrating the
use-cases for machine learning based image modification. In addition, GIMP-ML
also aims to bring the benefits of using deep learning networks used for
computer vision tasks to routine image processing workflows. The code and
installation procedure for configuring these plugins is available at
https://github.com/kritiksoman/GIMP-ML.
</p>
<a href="http://arxiv.org/abs/2004.13060" target="_blank">arXiv:2004.13060</a> [<a href="http://arxiv.org/pdf/2004.13060" target="_blank">pdf</a>]

<h2>Selecting Data Augmentation for Simulating Interventions. (arXiv:2005.01856v4 [stat.ML] UPDATED)</h2>
<h3>Maximilian Ilse, Jakub M. Tomczak, Patrick Forr&#xe9;</h3>
<p>Machine learning models trained with purely observational data and the
principle of empirical risk minimization \citep{vapnik_principles_1992} can
fail to generalize to unseen domains. In this paper, we focus on the case where
the problem arises through spurious correlation between the observed domains
and the actual task labels. We find that many domain generalization methods do
not explicitly take this spurious correlation into account. Instead, especially
in more application-oriented research areas like medical imaging or robotics,
data augmentation techniques that are based on heuristics are used to learn
domain invariant features. To bridge the gap between theory and practice, we
develop a causal perspective on the problem of domain generalization. We argue
that causal concepts can be used to explain the success of data augmentation by
describing how they can weaken the spurious correlation between the observed
domains and the task labels. We demonstrate that data augmentation can serve as
a tool for simulating interventional data. We use these theoretical insights to
derive a simple algorithm that is able to select data augmentation techniques
that will lead to better domain generalization.
</p>
<a href="http://arxiv.org/abs/2005.01856" target="_blank">arXiv:2005.01856</a> [<a href="http://arxiv.org/pdf/2005.01856" target="_blank">pdf</a>]

<h2>A Comparison of Few-Shot Learning Methods for Underwater Optical and Sonar Image Classification. (arXiv:2005.04621v2 [cs.CV] UPDATED)</h2>
<h3>Mateusz Ochal, Jose Vazquez, Yvan Petillot, Sen Wang</h3>
<p>Deep convolutional neural networks generally perform well in underwater
object recognition tasks on both optical and sonar images. Many such methods
require hundreds, if not thousands, of images per class to generalize well to
unseen examples. However, obtaining and labeling sufficiently large volumes of
data can be relatively costly and time-consuming, especially when observing
rare objects or performing real-time operations. Few-Shot Learning (FSL)
efforts have produced many promising methods to deal with low data
availability. However, little attention has been given in the underwater
domain, where the style of images poses additional challenges for object
recognition algorithms. To the best of our knowledge, this is the first paper
to evaluate and compare several supervised and semi-supervised Few-Shot
Learning (FSL) methods using underwater optical and side-scan sonar imagery.
Our results show that FSL methods offer a significant advantage over the
traditional transfer learning methods that fine-tune pre-trained models. We
hope that our work will help apply FSL to autonomous underwater systems and
expand their learning capabilities.
</p>
<a href="http://arxiv.org/abs/2005.04621" target="_blank">arXiv:2005.04621</a> [<a href="http://arxiv.org/pdf/2005.04621" target="_blank">pdf</a>]

<h2>Multi-Graph Convolutional Network for Relationship-Driven Stock Movement Prediction. (arXiv:2005.04955v3 [q-fin.ST] UPDATED)</h2>
<h3>Jiexia Ye, Juanjuan Zhao, Kejiang Ye, Chengzhong Xu</h3>
<p>Stock price movement prediction is commonly accepted as a very challenging
task due to the volatile nature of financial markets. Previous works typically
predict the stock price mainly based on its own information, neglecting the
cross effect among involved stocks. However, it is well known that an
individual stock price is correlated with prices of other stocks in complex
ways. To take the cross effect into consideration, we propose a deep learning
framework, called Multi-GCGRU, which comprises graph convolutional network
(GCN) and gated recurrent unit (GRU) to predict stock movement. Specifically,
we first encode multiple relationships among stocks into graphs based on
financial domain knowledge and utilize GCN to extract the cross effect based on
these pre-defined graphs. To further get rid of prior knowledge, we explore an
adaptive relationship learned by data automatically. The cross-correlation
features produced by GCN are concatenated with historical records and then fed
into GRU to model the temporal dependency of stock prices. Experiments on two
stock indexes in China market show that our model outperforms other baselines.
Note that our model is rather feasible to incorporate more effective stock
relationships containing expert knowledge, as well as learn data-driven
relationship.
</p>
<a href="http://arxiv.org/abs/2005.04955" target="_blank">arXiv:2005.04955</a> [<a href="http://arxiv.org/pdf/2005.04955" target="_blank">pdf</a>]

<h2>Dynamic Cognition Applied to Value Learning in Artificial Intelligence. (arXiv:2005.05538v4 [cs.AI] UPDATED)</h2>
<h3>Nicholas Kluge Corr&#xea;a, Nythamar de Oliveira</h3>
<p>Experts in Artificial Intelligence (AI) development predict that advances in
the development of intelligent systems and agents will reshape vital areas in
our society. Nevertheless, if such an advance isn't done with prudence, it can
result in negative outcomes for humanity. For this reason, several researchers
in the area are trying to develop a robust, beneficial, and safe concept of
artificial intelligence. Currently, several of the open problems in the field
of AI research arise from the difficulty of avoiding unwanted behaviors of
intelligent agents, and at the same time specifying what we want such systems
to do. It is of utmost importance that artificial intelligent agents have their
values aligned with human values, given the fact that we cannot expect an AI to
develop our moral preferences simply because of its intelligence, as discussed
in the Orthogonality Thesis. Perhaps this difficulty comes from the way we are
addressing the problem of expressing objectives, values, and ends, using
representational cognitive methods. A solution to this problem would be the
dynamic cognitive approach proposed by Dreyfus, whose phenomenological
philosophy defends that the human experience of being-in-the-world cannot be
represented by the symbolic or connectionist cognitive methods. A possible
approach to this problem would be to use theoretical models such as SED
(situated embodied dynamics) to address the values learning problem in AI.
</p>
<a href="http://arxiv.org/abs/2005.05538" target="_blank">arXiv:2005.05538</a> [<a href="http://arxiv.org/pdf/2005.05538" target="_blank">pdf</a>]

<h2>Continuous LWE. (arXiv:2005.09595v2 [cs.CC] UPDATED)</h2>
<h3>Joan Bruna, Oded Regev, Min Jae Song, Yi Tang</h3>
<p>We introduce a continuous analogue of the Learning with Errors (LWE) problem,
which we name CLWE. We give a polynomial-time quantum reduction from worst-case
lattice problems to CLWE, showing that CLWE enjoys similar hardness guarantees
to those of LWE. Alternatively, our result can also be seen as opening new
avenues of (quantum) attacks on lattice problems. Our work resolves an open
problem regarding the computational complexity of learning mixtures of
Gaussians without separability assumptions (Diakonikolas 2016, Moitra 2018). As
an additional motivation, (a slight variant of) CLWE was considered in the
context of robust machine learning (Diakonikolas et al.~FOCS 2017), where
hardness in the statistical query (SQ) model was shown; our work addresses the
open question regarding its computational hardness (Bubeck et al.~ICML 2019).
</p>
<a href="http://arxiv.org/abs/2005.09595" target="_blank">arXiv:2005.09595</a> [<a href="http://arxiv.org/pdf/2005.09595" target="_blank">pdf</a>]

<h2>Graph Random Neural Network for Semi-Supervised Learning on Graphs. (arXiv:2005.11079v3 [cs.LG] UPDATED)</h2>
<h3>Wenzheng Feng, Jie Zhang, Yuxiao Dong, Yu Han, Huanbo Luan, Qian Xu, Qiang Yang, Evgeny Kharlamov, Jie Tang</h3>
<p>We study the problem of semi-supervised learning on graphs, for which graph
neural networks (GNNs) have been extensively explored. However, most existing
GNNs inherently suffer from the limitations of over-smoothing, non-robustness,
and weak-generalization when labeled nodes are scarce. In this paper, we
propose a simple yet effective framework---GRAPH RANDOM NEURAL NETWORKS
(GRAND)---to address these issues. In GRAND, we first design a random
propagation strategy to perform graph data augmentation. Then we leverage
consistency regularization to optimize the prediction consistency of unlabeled
nodes across different data augmentations. Extensive experiments on graph
benchmark datasets suggest that GRAND significantly outperforms
state-of-the-art GNN baselines on semi-supervised node classification. Finally,
we show that GRAND mitigates the issues of over-smoothing and non-robustness,
exhibiting better generalization behavior than existing GNNs. The source code
of GRAND is publicly available at https://github.com/Grand20/grand.
</p>
<a href="http://arxiv.org/abs/2005.11079" target="_blank">arXiv:2005.11079</a> [<a href="http://arxiv.org/pdf/2005.11079" target="_blank">pdf</a>]

<h2>Submodular Bandit Problem Under Multiple Constraints. (arXiv:2006.00661v4 [cs.LG] UPDATED)</h2>
<h3>Sho Takemori, Masahiro Sato, Takashi Sonoda, Janmajay Singh, Tomoko Ohkuma</h3>
<p>The linear submodular bandit problem was proposed to simultaneously address
diversified retrieval and online learning in a recommender system. If there is
no uncertainty, this problem is equivalent to a submodular maximization problem
under a cardinality constraint. However, in some situations, recommendation
lists should satisfy additional constraints such as budget constraints, other
than a cardinality constraint. Thus, motivated by diversified retrieval
considering budget constraints, we introduce a submodular bandit problem under
the intersection of $l$ knapsacks and a $k$-system constraint. Here $k$-system
constraints form a very general class of constraints including cardinality
constraints and the intersection of $k$ matroid constraints. To solve this
problem, we propose a non-greedy algorithm that adaptively focuses on a
standard or modified upper-confidence bound. We provide a high-probability
upper bound of an approximation regret, where the approximation ratio matches
that of a fast offline algorithm. Moreover, we perform experiments under
various combinations of constraints using a synthetic and two real-world
datasets and demonstrate that our proposed methods outperform the existing
baselines.
</p>
<a href="http://arxiv.org/abs/2006.00661" target="_blank">arXiv:2006.00661</a> [<a href="http://arxiv.org/pdf/2006.00661" target="_blank">pdf</a>]

<h2>Equivariant Flows: Exact Likelihood Generative Learning for Symmetric Densities. (arXiv:2006.02425v2 [stat.ML] UPDATED)</h2>
<h3>Jonas K&#xf6;hler, Leon Klein, Frank No&#xe9;</h3>
<p>Normalizing flows are exact-likelihood generative neural networks which
approximately transform samples from a simple prior distribution to samples of
the probability distribution of interest. Recent work showed that such
generative models can be utilized in statistical mechanics to sample
equilibrium states of many-body systems in physics and chemistry. To scale and
generalize these results, it is essential that the natural symmetries in the
probability density -- in physics defined by the invariances of the target
potential -- are built into the flow. We provide a theoretical sufficient
criterion showing that the distribution generated by \textit{equivariant}
normalizing flows is invariant with respect to these symmetries by design.
Furthermore, we propose building blocks for flows which preserve symmetries
which are usually found in physical/chemical many-body particle systems. Using
benchmark systems motivated from molecular physics, we demonstrate that those
symmetry preserving flows can provide better generalization capabilities and
sampling efficiency.
</p>
<a href="http://arxiv.org/abs/2006.02425" target="_blank">arXiv:2006.02425</a> [<a href="http://arxiv.org/pdf/2006.02425" target="_blank">pdf</a>]

<h2>Serving DNNs like Clockwork: Performance Predictability from the Bottom Up. (arXiv:2006.02464v2 [cs.DC] UPDATED)</h2>
<h3>Arpan Gujarati, Reza Karimi, Safya Alzayat, Wei Hao, Antoine Kaufmann, Ymir Vigfusson, Jonathan Mace</h3>
<p>Machine learning inference is becoming a core building block for interactive
web applications. As a result, the underlying model serving systems on which
these applications depend must consistently meet low latency targets. Existing
model serving architectures use well-known reactive techniques to alleviate
common-case sources of latency, but cannot effectively curtail tail latency
caused by unpredictable execution times. Yet the underlying execution times are
not fundamentally unpredictable - on the contrary we observe that inference
using Deep Neural Network (DNN) models has deterministic performance. Here,
starting with the predictable execution times of individual DNN inferences, we
adopt a principled design methodology to successively build a fully distributed
model serving system that achieves predictable end-to-end performance. We
evaluate our implementation, Clockwork, using production trace workloads, and
show that Clockwork can support thousands of models while simultaneously
meeting 100ms latency targets for 99.9999% of requests. We further demonstrate
that Clockwork exploits predictable execution times to achieve tight
request-level service-level objectives (SLOs) as well as a high degree of
request-level performance isolation.
</p>
<a href="http://arxiv.org/abs/2006.02464" target="_blank">arXiv:2006.02464</a> [<a href="http://arxiv.org/pdf/2006.02464" target="_blank">pdf</a>]

<h2>High-level Modeling of Manufacturing Faults in Deep Neural Network Accelerators. (arXiv:2006.03616v2 [cs.LG] UPDATED)</h2>
<h3>Shamik Kundu, Ahmet Soyyi&#x11f;it, Khaza Anuarul Hoque, Kanad Basu</h3>
<p>The advent of data-driven real-time applications requires the implementation
of Deep Neural Networks (DNNs) on Machine Learning accelerators. Google's
Tensor Processing Unit (TPU) is one such neural network accelerator that uses
systolic array-based matrix multiplication hardware for computation in its
crux. Manufacturing faults at any state element of the matrix multiplication
unit can cause unexpected errors in these inference networks. In this paper, we
propose a formal model of permanent faults and their propagation in a TPU using
the Discrete-Time Markov Chain (DTMC) formalism. The proposed model is analyzed
using the probabilistic model checking technique to reason about the likelihood
of faulty outputs. The obtained quantitative results show that the
classification accuracy is sensitive to the type of permanent faults as well as
their location, bit position and the number of layers in the neural network.
The conclusions from our theoretical model have been validated using
experiments on a digit recognition-based DNN.
</p>
<a href="http://arxiv.org/abs/2006.03616" target="_blank">arXiv:2006.03616</a> [<a href="http://arxiv.org/pdf/2006.03616" target="_blank">pdf</a>]

<h2>Contextual Bandits with Side-Observations. (arXiv:2006.03951v2 [cs.LG] UPDATED)</h2>
<h3>Rahul Singh, Fang Liu, Xin Liu, Ness Shroff</h3>
<p>We investigate contextual bandits in the presence of side-observations across
arms in order to design recommendation algorithms for users connected via
social networks. Users in social networks respond to their friends' activity,
and hence provide information about each other's preferences. In our model,
when a learning algorithm recommends an article to a user, not only does it
observe his/her response (e.g. an ad click), but also the side-observations,
i.e., the response of his neighbors if they were presented with the same
article. We model these observation dependencies by a graph $\mathcal{G}$ in
which nodes correspond to users, and edges correspond to social links. We
derive a problem/instance-dependent lower-bound on the regret of any consistent
algorithm. We propose an optimization (linear programming) based data-driven
learning algorithm that utilizes the structure of $\mathcal{G}$ in order to
make recommendations to users and show that it is asymptotically optimal, in
the sense that its regret matches the lower-bound as the number of rounds
$T\to\infty$. We show that this asymptotically optimal regret is upper-bounded
as $O\left(|\chi(\mathcal{G})|\log T\right)$, where $|\chi(\mathcal{G})|$ is
the domination number of $\mathcal{G}$. In contrast, a naive application of the
existing learning algorithms results in $O\left(N\log T\right)$ regret, where
$N$ is the number of users.
</p>
<a href="http://arxiv.org/abs/2006.03951" target="_blank">arXiv:2006.03951</a> [<a href="http://arxiv.org/pdf/2006.03951" target="_blank">pdf</a>]

<h2>Modeling Long-horizon Tasks as Sequential Interaction Landscapes. (arXiv:2006.04843v2 [cs.RO] UPDATED)</h2>
<h3>S&#xf6;ren Pirk, Karol Hausman, Alexander Toshev, Mohi Khansari</h3>
<p>Complex object manipulation tasks often span over long sequences of
operations. Task planning over long-time horizons is a challenging and open
problem in robotics, and its complexity grows exponentially with an increasing
number of subtasks. In this paper we present a deep learning network that
learns dependencies and transitions across subtasks solely from a set of
demonstration videos. We represent each subtask as an action symbol (e.g. move
cup), and show that these symbols can be learned and predicted directly from
image observations. Learning from demonstrations and visual observations are
two main pillars of our approach. The former makes the learning tractable as it
provides the network with information about the most frequent transitions and
relevant dependency between subtasks (instead of exploring all possible
combination), while the latter allows the network to continuously monitor the
task progress and thus to interactively adapt to changes in the environment. We
evaluate our framework on two long horizon tasks: (1) block stacking of puzzle
pieces being executed by humans, and (2) a robot manipulation task involving
pick and place of objects and sliding a cabinet door with a 7-DoF robot arm. We
show that complex plans can be carried out when executing the robotic task and
the robot can interactively adapt to changes in the environment and recover
from failure cases.
</p>
<a href="http://arxiv.org/abs/2006.04843" target="_blank">arXiv:2006.04843</a> [<a href="http://arxiv.org/pdf/2006.04843" target="_blank">pdf</a>]

<h2>Learning-to-Rank with Partitioned Preference: Fast Estimation for the Plackett-Luce Model. (arXiv:2006.05067v2 [cs.LG] UPDATED)</h2>
<h3>Jiaqi Ma, Xinyang Yi, Weijing Tang, Zhe Zhao, Lichan Hong, Ed H. Chi, Qiaozhu Mei</h3>
<p>We investigate the Plackett-Luce (PL) model based listwise learning-to-rank
(LTR) on data with partitioned preference, where a set of items are sliced into
ordered and disjoint partitions, but the ranking of items within a partition is
unknown. Given $N$ items with $M$ partitions, calculating the likelihood of
data with partitioned preference under the PL model has a time complexity of
$O(N+S!)$, where $S$ is the maximum size of the top $M-1$ partitions. This
computational challenge restrains most existing PL-based listwise LTR methods
to a special case of partitioned preference, top-$K$ ranking, where the exact
order of the top $K$ items is known. In this paper, we exploit a random utility
model formulation of the PL model, and propose an efficient numerical
integration approach for calculating the likelihood and its gradients with a
time complexity $O(N+S^3)$. We demonstrate that the proposed method outperforms
well-known LTR baselines and remains scalable through both simulation
experiments and applications to real-world eXtreme Multi-Label classification
tasks.
</p>
<a href="http://arxiv.org/abs/2006.05067" target="_blank">arXiv:2006.05067</a> [<a href="http://arxiv.org/pdf/2006.05067" target="_blank">pdf</a>]

<h2>Provable tradeoffs in adversarially robust classification. (arXiv:2006.05161v3 [cs.LG] UPDATED)</h2>
<h3>Edgar Dobriban, Hamed Hassani, David Hong, Alexander Robey</h3>
<p>It is well known that machine learning methods can be vulnerable to
adversarially-chosen perturbations of their inputs. Despite significant
progress in the area, foundational open problems remain. Here we address
several of these key questions. We derive exact and approximate Bayes-optimal
robust classifiers for the important setting of two- and three-class Gaussian
classification problems with arbitrary imbalance, for $\ell_2$ and
$\ell_\infty$ adversaries. In contrast to classical Bayes-optimal classifiers,
decisions here cannot be made pointwise and new theoretical approaches are
needed. We develop and leverage new tools, including recent breakthroughs from
probability theory on robust isoperimetry (Cianci et al, 2011, Mossel and
Neeman 2015), which, to our knowledge, have not yet been used in the area. Our
results reveal tradeoffs between standard and robust accuracy that grow when
data is imbalanced. We also show further foundational results, including an
analysis of the loss landscape, classification calibration for convex losses in
certain models, and finite sample rates for the robust risk.
</p>
<a href="http://arxiv.org/abs/2006.05161" target="_blank">arXiv:2006.05161</a> [<a href="http://arxiv.org/pdf/2006.05161" target="_blank">pdf</a>]

<h2>Wide and Deep Graph Neural Networks with Distributed Online Learning. (arXiv:2006.06376v2 [cs.LG] UPDATED)</h2>
<h3>Zhan Gao, Fernando Gama, Alejandro Ribeiro</h3>
<p>Graph neural networks (GNNs) learn representations from network data with
naturally distributed architectures, rendering them well-suited candidates for
decentralized learning. Oftentimes, this decentralized graph support changes
with time due to link failures or topology variations. These changes create a
mismatch between the graphs on which GNNs were trained and the ones on which
they are tested. Online learning can be used to retrain GNNs at testing time,
overcoming this issue. However, most online algorithms are centralized and work
on convex problems (which GNNs rarely lead to). This paper proposes the Wide
and Deep GNN (WD-GNN), a novel architecture that can be easily updated with
distributed online learning mechanisms. The WD-GNN comprises two components:
the wide part is a bank of linear graph filters and the deep part is a GNN. At
training time, the joint architecture learns a nonlinear representation from
data. At testing time, the deep part (nonlinear) is left unchanged, while the
wide part is retrained online, leading to a convex problem. We derive
convergence guarantees for this online retraining procedure and further propose
a decentralized alternative. Experiments on the robot swarm control for
flocking corroborate theory and show potential of the proposed architecture for
distributed online learning.
</p>
<a href="http://arxiv.org/abs/2006.06376" target="_blank">arXiv:2006.06376</a> [<a href="http://arxiv.org/pdf/2006.06376" target="_blank">pdf</a>]

<h2>Learning to Extrapolate Knowledge: Transductive Few-shot Out-of-Graph Link Prediction. (arXiv:2006.06648v2 [cs.LG] UPDATED)</h2>
<h3>Jinheon Baek, Dong Bok Lee, Sung Ju Hwang</h3>
<p>Many practical graph problems, such as knowledge graph construction and
drug-drug interaction prediction, require to handle multi-relational graphs.
However, handling real-world multi-relational graphs with Graph Neural Networks
(GNNs) is often challenging due to their evolving nature, as new entities
(nodes) can emerge over time. Moreover, newly emerged entities often have few
links, which makes the learning even more difficult. Motivated by this
challenge, we introduce a realistic problem of few-shot out-of-graph link
prediction, where we not only predict the links between the seen and unseen
nodes as in a conventional out-of-knowledge link prediction task but also
between the unseen nodes, with only few edges per node. We tackle this problem
with a novel transductive meta-learning framework which we refer to as Graph
Extrapolation Networks (GEN). GEN meta-learns both the node embedding network
for inductive inference (seen-to-unseen) and the link prediction network for
transductive inference (unseen-to-unseen). For transductive link prediction, we
further propose a stochastic embedding layer to model uncertainty in the link
prediction between unseen entities. We validate our model on multiple benchmark
datasets for knowledge graph completion and drug-drug interaction prediction.
The results show that our model significantly outperforms relevant baselines
for out-of-graph link prediction tasks.
</p>
<a href="http://arxiv.org/abs/2006.06648" target="_blank">arXiv:2006.06648</a> [<a href="http://arxiv.org/pdf/2006.06648" target="_blank">pdf</a>]

<h2>An Unsupervised Information-Theoretic Perceptual Quality Metric. (arXiv:2006.06752v2 [cs.CV] UPDATED)</h2>
<h3>Sangnie Bhardwaj, Ian Fischer, Johannes Ball&#xe9;, Troy Chinen</h3>
<p>Tractable models of human perception have proved to be challenging to build.
Hand-designed models such as MS-SSIM remain popular predictors of human image
quality judgements due to their simplicity and speed. Recent modern deep
learning approaches can perform better, but they rely on supervised data which
can be costly to gather: large sets of class labels such as ImageNet, image
quality ratings, or both. We combine recent advances in information-theoretic
objective functions with a computational architecture informed by the
physiology of the human visual system and unsupervised training on pairs of
video frames, yielding our Perceptual Information Metric (PIM). We show that
PIM is competitive with supervised metrics on the recent and challenging BAPPS
image quality assessment dataset and outperforms them in predicting the ranking
of image compression methods in CLIC 2020. We also perform qualitative
experiments using the ImageNet-C dataset, and establish that PIM is robust with
respect to architectural details.
</p>
<a href="http://arxiv.org/abs/2006.06752" target="_blank">arXiv:2006.06752</a> [<a href="http://arxiv.org/pdf/2006.06752" target="_blank">pdf</a>]

<h2>On Correctness of Automatic Differentiation for Non-Differentiable Functions. (arXiv:2006.06903v2 [cs.LG] UPDATED)</h2>
<h3>Wonyeol Lee, Hangyeol Yu, Xavier Rival, Hongseok Yang</h3>
<p>Differentiation lies at the core of many machine-learning algorithms, and is
well-supported by popular autodiff systems, such as TensorFlow and PyTorch.
Originally, these systems have been developed to compute derivatives of
differentiable functions, but in practice, they are commonly applied to
functions with non-differentiabilities. For instance, neural networks using
ReLU define non-differentiable functions in general, but the gradients of
losses involving those functions are computed using autodiff systems in
practice. This status quo raises a natural question: are autodiff systems
correct in any formal sense when they are applied to such non-differentiable
functions? In this paper, we provide a positive answer to this question. Using
counterexamples, we first point out flaws in often-used informal arguments,
such as: non-differentiabilities arising in deep learning do not cause any
issues because they form a measure-zero set. We then investigate a class of
functions, called PAP functions, that includes nearly all (possibly
non-differentiable) functions in deep learning nowadays. For these PAP
functions, we propose a new type of derivatives, called intensional
derivatives, and prove that these derivatives always exist and coincide with
standard derivatives for almost all inputs. We also show that these intensional
derivatives are what most autodiff systems compute or try to compute
essentially. In this way, we formally establish the correctness of autodiff
systems applied to non-differentiable functions.
</p>
<a href="http://arxiv.org/abs/2006.06903" target="_blank">arXiv:2006.06903</a> [<a href="http://arxiv.org/pdf/2006.06903" target="_blank">pdf</a>]

<h2>Does Unsupervised Architecture Representation Learning Help Neural Architecture Search?. (arXiv:2006.06936v2 [cs.CV] UPDATED)</h2>
<h3>Shen Yan, Yu Zheng, Wei Ao, Xiao Zeng, Mi Zhang</h3>
<p>Existing Neural Architecture Search (NAS) methods either encode neural
architectures using discrete encodings that do not scale well, or adopt
supervised learning-based methods to jointly learn architecture representations
and optimize architecture search on such representations which incurs search
bias. Despite the widespread use, architecture representations learned in NAS
are still poorly understood. We observe that the structural properties of
neural architectures are hard to preserve in the latent space if architecture
representation learning and search are coupled, resulting in less effective
search performance. In this work, we find empirically that pre-training
architecture representations using only neural architectures without their
accuracies as labels considerably improve the downstream architecture search
efficiency. To explain these observations, we visualize how unsupervised
architecture representation learning better encourages neural architectures
with similar connections and operators to cluster together. This helps to map
neural architectures with similar performance to the same regions in the latent
space and makes the transition of architectures in the latent space relatively
smooth, which considerably benefits diverse downstream search strategies.
</p>
<a href="http://arxiv.org/abs/2006.06936" target="_blank">arXiv:2006.06936</a> [<a href="http://arxiv.org/pdf/2006.06936" target="_blank">pdf</a>]

<h2>Deep Reinforcement and InfoMax Learning. (arXiv:2006.07217v2 [cs.LG] UPDATED)</h2>
<h3>Bogdan Mazoure, Remi Tachet des Combes, Thang Doan, Philip Bachman, R Devon Hjelm</h3>
<p>We begin with the hypothesis that a model-free agent whose representations
are predictive of properties of future states (beyond expected rewards) will be
more capable of solving and adapting to new RL problems. To test that
hypothesis, we introduce an objective based on Deep InfoMax (DIM) which trains
the agent to predict the future by maximizing the mutual information between
its internal representation of successive timesteps. We test our approach in
several synthetic settings, where it successfully learns representations that
are predictive of the future. Finally, we augment C51, a strong RL baseline,
with our temporal DIM objective and demonstrate improved performance on a
continual learning task and on the recently introduced Procgen environment.
</p>
<a href="http://arxiv.org/abs/2006.07217" target="_blank">arXiv:2006.07217</a> [<a href="http://arxiv.org/pdf/2006.07217" target="_blank">pdf</a>]

<h2>Online Bayesian Goal Inference for Boundedly-Rational Planning Agents. (arXiv:2006.07532v2 [cs.AI] UPDATED)</h2>
<h3>Tan Zhi-Xuan, Jordyn L. Mann, Tom Silver, Joshua B. Tenenbaum, Vikash K. Mansinghka</h3>
<p>People routinely infer the goals of others by observing their actions over
time. Remarkably, we can do so even when those actions lead to failure,
enabling us to assist others when we detect that they might not achieve their
goals. How might we endow machines with similar capabilities? Here we present
an architecture capable of inferring an agent's goals online from both optimal
and non-optimal sequences of actions. Our architecture models agents as
boundedly-rational planners that interleave search with execution by
replanning, thereby accounting for sub-optimal behavior. These models are
specified as probabilistic programs, allowing us to represent and perform
efficient Bayesian inference over an agent's goals and internal planning
processes. To perform such inference, we develop Sequential Inverse Plan Search
(SIPS), a sequential Monte Carlo algorithm that exploits the online replanning
assumption of these models, limiting computation by incrementally extending
inferred plans as new actions are observed. We present experiments showing that
this modeling and inference architecture outperforms Bayesian inverse
reinforcement learning baselines, accurately inferring goals from both optimal
and non-optimal trajectories involving failure and back-tracking, while
generalizing across domains with compositional structure and sparse rewards.
</p>
<a href="http://arxiv.org/abs/2006.07532" target="_blank">arXiv:2006.07532</a> [<a href="http://arxiv.org/pdf/2006.07532" target="_blank">pdf</a>]

<h2>Adversarial Self-Supervised Contrastive Learning. (arXiv:2006.07589v2 [cs.LG] UPDATED)</h2>
<h3>Minseon Kim, Jihoon Tack, Sung Ju Hwang</h3>
<p>Existing adversarial learning approaches mostly use class labels to generate
adversarial samples that lead to incorrect predictions, which are then used to
augment the training of the model for improved robustness. While some recent
works propose semi-supervised adversarial learning methods that utilize
unlabeled data, they still require class labels. However, do we really need
class labels at all, for adversarially robust training of deep neural networks?
In this paper, we propose a novel adversarial attack for unlabeled data, which
makes the model confuse the instance-level identities of the perturbed data
samples. Further, we present a self-supervised contrastive learning framework
to adversarially train a robust neural network without labeled data, which aims
to maximize the similarity between a random augmentation of a data sample and
its instance-wise adversarial perturbation. We validate our method, Robust
Contrastive Learning (RoCL), on multiple benchmark datasets, on which it
obtains comparable robust accuracy over state-of-the-art supervised adversarial
learning methods, and significantly improved robustness against the black box
and unseen types of attacks. Moreover, with further joint fine-tuning with
supervised adversarial loss, RoCL obtains even higher robust accuracy over
using self-supervised learning alone. Notably, RoCL also demonstrate impressive
results in robust transfer learning.
</p>
<a href="http://arxiv.org/abs/2006.07589" target="_blank">arXiv:2006.07589</a> [<a href="http://arxiv.org/pdf/2006.07589" target="_blank">pdf</a>]

<h2>Mitigating Gender Bias in Captioning Systems. (arXiv:2006.08315v2 [cs.CV] UPDATED)</h2>
<h3>Ruixiang Tang, Mengnan Du, Yuening Li, Zirui Liu, Xia Hu</h3>
<p>Image captioning has made substantial progress with huge supporting image
collections sourced from the web. However, recent studies have pointed out that
captioning datasets, such as COCO, contain gender bias found in web corpora. As
a result, learning models could heavily rely on the learned priors and image
context for gender identification, leading to incorrect or even offensive
errors. To encourage models to learn correct gender features, we reorganize the
COCO dataset and present two new splits COCO-GB V1 and V2 datasets where the
train and test sets have different gender-context joint distribution. Models
relying on contextual cues will suffer from huge gender prediction errors on
the anti-stereotypical test data. Benchmarking experiments reveal that most
captioning models learn gender bias, leading to high gender prediction errors,
especially for women. To alleviate the unwanted bias, we propose a new Guided
Attention Image Captioning model (GAIC) which provides self-guidance on visual
attention to encourage the model to capture correct gender visual evidence.
Experimental results validate that GAIC can significantly reduce gender
prediction errors with a competitive caption quality. Our codes and the
designed benchmark datasets are available at
https://github.com/CaptionGenderBias2020.
</p>
<a href="http://arxiv.org/abs/2006.08315" target="_blank">arXiv:2006.08315</a> [<a href="http://arxiv.org/pdf/2006.08315" target="_blank">pdf</a>]

<h2>Reciprocal Adversarial Learning via Characteristic Functions. (arXiv:2006.08413v2 [cs.LG] UPDATED)</h2>
<h3>Shengxi Li, Zeyang Yu, Min Xiang, Danilo Mandic</h3>
<p>Generative adversarial nets (GANs) have become a preferred tool for tasks
involving complicated distributions. To stabilise the training and reduce the
mode collapse of GANs, one of their main variants employs the integral
probability metric (IPM) as the loss function. This provides extensive IPM-GANs
with theoretical support for basically comparing moments in an embedded domain
of the \textit{critic}. We generalise this by comparing the distributions
rather than their moments via a powerful tool, i.e., the characteristic
function (CF), which uniquely and universally comprising all the information
about a distribution. For rigour, we first establish the physical meaning of
the phase and amplitude in CF, and show that this provides a feasible way of
balancing the accuracy and diversity of generation. We then develop an
efficient sampling strategy to calculate the CFs. Within this framework, we
further prove an equivalence between the embedded and data domains when a
reciprocal exists, where we naturally develop the GAN in an auto-encoder
structure, in a way of comparing everything in the embedded space (a
semantically meaningful manifold). This efficient structure uses only two
modules, together with a simple training strategy, to achieve bi-directionally
generating clear images, which is referred to as the reciprocal CF GAN
(RCF-GAN). Experimental results demonstrate the superior performances of the
proposed RCF-GAN in terms of both generation and reconstruction.
</p>
<a href="http://arxiv.org/abs/2006.08413" target="_blank">arXiv:2006.08413</a> [<a href="http://arxiv.org/pdf/2006.08413" target="_blank">pdf</a>]

<h2>Strongly local p-norm-cut algorithms for semi-supervised learning and local graph clustering. (arXiv:2006.08569v2 [cs.SI] UPDATED)</h2>
<h3>Meng Liu, David F. Gleich</h3>
<p>Graph based semi-supervised learning is the problem of learning a labeling
function for the graph nodes given a few example nodes, often called seeds,
usually under the assumption that the graph's edges indicate similarity of
labels. This is closely related to the local graph clustering or community
detection problem of finding a cluster or community of nodes around a given
seed. For this problem, we propose a novel generalization of random walk,
diffusion, or smooth function methods in the literature to a convex p-norm cut
function. The need for our p-norm methods is that, in our study of existing
methods, we find those principled methods based on eigenvector, spectral,
random walk, or linear system often have difficulty capturing the correct
boundary of a target label or target cluster. In contrast, 1-norm or
maxflow-mincut based methods capture the boundary, but cannot grow from small
seed set; hybrid procedures that use both have many hard to set parameters. In
this paper, we propose a generalization of the objective function behind these
methods involving p-norms. To solve the p-norm cut problem we give a strongly
local algorithm -- one whose runtime depends on the size of the output rather
than the size of the graph. Our method can be thought as a nonlinear
generalization of the Anderson-Chung-Lang push procedure to approximate a
personalized PageRank vector efficiently. Our procedure is general and can
solve other types of nonlinear objective functions, such as p-norm variants of
Huber losses. We provide a theoretical analysis of finding planted target
clusters with our method and show that the p-norm cut functions improve on the
standard Cheeger inequalities for random walk and spectral methods. Finally, we
demonstrate the speed and accuracy of our new method in synthetic and real
world datasets. Our code is available at this http URL
</p>
<a href="http://arxiv.org/abs/2006.08569" target="_blank">arXiv:2006.08569</a> [<a href="http://arxiv.org/pdf/2006.08569" target="_blank">pdf</a>]

<h2>Preference-based Reinforcement Learning with Finite-Time Guarantees. (arXiv:2006.08910v2 [cs.LG] UPDATED)</h2>
<h3>Yichong Xu, Ruosong Wang, Lin F. Yang, Aarti Singh, Artur Dubrawski</h3>
<p>Preference-based Reinforcement Learning (PbRL) replaces reward values in
traditional reinforcement learning by preferences to better elicit human
opinion on the target objective, especially when numerical reward values are
hard to design or interpret. Despite promising results in applications, the
theoretical understanding of PbRL is still in its infancy. In this paper, we
present the first finite-time analysis for general PbRL problems. We first show
that a unique optimal policy may not exist if preferences over trajectories are
deterministic for PbRL. If preferences are stochastic, and the preference
probability relates to the hidden reward values, we present algorithms for
PbRL, both with and without a simulator, that are able to identify the best
policy up to accuracy $\varepsilon$ with high probability. Our method explores
the state space by navigating to under-explored states, and solves PbRL using a
combination of dueling bandits and policy search. Experiments show the efficacy
of our method when it is applied to real-world problems.
</p>
<a href="http://arxiv.org/abs/2006.08910" target="_blank">arXiv:2006.08910</a> [<a href="http://arxiv.org/pdf/2006.08910" target="_blank">pdf</a>]

<h2>Improved Techniques for Training Score-Based Generative Models. (arXiv:2006.09011v2 [cs.LG] UPDATED)</h2>
<h3>Yang Song, Stefano Ermon</h3>
<p>Score-based generative models can produce high quality image samples
comparable to GANs, without requiring adversarial optimization. However,
existing training procedures are limited to images of low resolution (typically
below 32x32), and can be unstable under some settings. We provide a new
theoretical analysis of learning and sampling from score models in high
dimensional spaces, explaining existing failure modes and motivating new
solutions that generalize across datasets. To enhance stability, we also
propose to maintain an exponential moving average of model weights. With these
improvements, we can effortlessly scale score-based generative models to images
with unprecedented resolutions ranging from 64x64 to 256x256. Our score-based
models can generate high-fidelity samples that rival best-in-class GANs on
various image datasets, including CelebA, FFHQ, and multiple LSUN categories.
</p>
<a href="http://arxiv.org/abs/2006.09011" target="_blank">arXiv:2006.09011</a> [<a href="http://arxiv.org/pdf/2006.09011" target="_blank">pdf</a>]

<h2>Topological defects and confinement with machine learning: the case of monopoles in compact electrodynamics. (arXiv:2006.09113v2 [hep-lat] UPDATED)</h2>
<h3>M. N. Chernodub, Harold Erbin, V. A. Goy, A. V. Molochkov</h3>
<p>We investigate the advantages of machine learning techniques to recognize the
dynamics of topological objects in quantum field theories. We consider the
compact U(1) gauge theory in three spacetime dimensions as the simplest example
of a theory that exhibits confinement and mass gap phenomena generated by
monopoles. We train a neural network with a generated set of monopole
configurations to distinguish between confinement and deconfinement phases,
from which it is possible to determine the deconfinement transition point and
to predict several observables. The model uses a supervised learning approach
and treats the monopole configurations as three-dimensional images (holograms).
We show that the model can determine the transition temperature with accuracy,
which depends on the criteria implemented in the algorithm. More importantly,
we train the neural network with configurations from a single lattice size
before making predictions for configurations from other lattice sizes, from
which a reliable estimation of the critical temperatures are obtained.
</p>
<a href="http://arxiv.org/abs/2006.09113" target="_blank">arXiv:2006.09113</a> [<a href="http://arxiv.org/pdf/2006.09113" target="_blank">pdf</a>]

<h2>Sample-Efficient Optimization in the Latent Space of Deep Generative Models via Weighted Retraining. (arXiv:2006.09191v2 [cs.LG] UPDATED)</h2>
<h3>Austin Tripp, Erik Daxberger, Jos&#xe9; Miguel Hern&#xe1;ndez-Lobato</h3>
<p>Many important problems in science and engineering, such as drug design,
involve optimizing an expensive black-box objective function over a complex,
high-dimensional, and structured input space. Although machine learning
techniques have shown promise in solving such problems, existing approaches
substantially lack sample efficiency. We introduce an improved method for
efficient black-box optimization, which performs the optimization in the
low-dimensional, continuous latent manifold learned by a deep generative model.
In contrast to previous approaches, we actively steer the generative model to
maintain a latent manifold that is highly useful for efficiently optimizing the
objective. We achieve this by periodically retraining the generative model on
the data points queried along the optimization trajectory, as well as weighting
those data points according to their objective function value. This weighted
retraining can be easily implemented on top of existing methods, and is
empirically shown to significantly improve their efficiency and performance on
synthetic and real-world optimization problems.
</p>
<a href="http://arxiv.org/abs/2006.09191" target="_blank">arXiv:2006.09191</a> [<a href="http://arxiv.org/pdf/2006.09191" target="_blank">pdf</a>]

<h2>Learning About Objects by Learning to Interact with Them. (arXiv:2006.09306v2 [cs.CV] UPDATED)</h2>
<h3>Martin Lohmann, Jordi Salvador, Aniruddha Kembhavi, Roozbeh Mottaghi</h3>
<p>Much of the remarkable progress in computer vision has been focused around
fully supervised learning mechanisms relying on highly curated datasets for a
variety of tasks. In contrast, humans often learn about their world with little
to no external supervision. Taking inspiration from infants learning from their
environment through play and interaction, we present a computational framework
to discover objects and learn their physical properties along this paradigm of
Learning from Interaction. Our agent, when placed within the near
photo-realistic and physics-enabled AI2-THOR environment, interacts with its
world and learns about objects, their geometric extents and relative masses,
without any external guidance. Our experiments reveal that this agent learns
efficiently and effectively; not just for objects it has interacted with
before, but also for novel instances from seen categories as well as novel
object categories.
</p>
<a href="http://arxiv.org/abs/2006.09306" target="_blank">arXiv:2006.09306</a> [<a href="http://arxiv.org/pdf/2006.09306" target="_blank">pdf</a>]

<h2>Learning Partially Known Stochastic Dynamics with Empirical PAC Bayes. (arXiv:2006.09914v2 [cs.LG] UPDATED)</h2>
<h3>Manuel Haussmann, Sebastian Gerwinn, Andreas Look, Barbara Rakitsch, Melih Kandemir</h3>
<p>Neural Stochastic Differential Equations model a dynamical environment with
neural nets assigned to their drift and diffusion terms. The high expressive
power of their nonlinearity comes at the expense of instability in the
identification of the large set of free parameters. This paper presents a
recipe to improve the prediction accuracy of such models in three steps: i)
accounting for epistemic uncertainty by assuming probabilistic weights, ii)
incorporation of partial knowledge on the state dynamics, and iii) training the
resultant hybrid model by an objective derived from a PAC Bayesian
generalization bound. We observe in our experiments that this recipe
effectively translates partial and noisy prior knowledge into an improved model
fit.
</p>
<a href="http://arxiv.org/abs/2006.09914" target="_blank">arXiv:2006.09914</a> [<a href="http://arxiv.org/pdf/2006.09914" target="_blank">pdf</a>]

<h2>Big Self-Supervised Models are Strong Semi-Supervised Learners. (arXiv:2006.10029v2 [cs.LG] UPDATED)</h2>
<h3>Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, Geoffrey Hinton</h3>
<p>One paradigm for learning from few labeled examples while making best use of
a large amount of unlabeled data is unsupervised pretraining followed by
supervised fine-tuning. Although this paradigm uses unlabeled data in a
task-agnostic way, in contrast to common approaches to semi-supervised learning
for computer vision, we show that it is surprisingly effective for
semi-supervised learning on ImageNet. A key ingredient of our approach is the
use of big (deep and wide) networks during pretraining and fine-tuning. We find
that, the fewer the labels, the more this approach (task-agnostic use of
unlabeled data) benefits from a bigger network. After fine-tuning, the big
network can be further improved and distilled into a much smaller one with
little loss in classification accuracy by using the unlabeled examples for a
second time, but in a task-specific way. The proposed semi-supervised learning
algorithm can be summarized in three steps: unsupervised pretraining of a big
ResNet model using SimCLRv2, supervised fine-tuning on a few labeled examples,
and distillation with unlabeled examples for refining and transferring the
task-specific knowledge. This procedure achieves 73.9% ImageNet top-1 accuracy
with just 1% of the labels ($\le$13 labeled images per class) using ResNet-50,
a $10\times$ improvement in label efficiency over the previous
state-of-the-art. With 10% of labels, ResNet-50 trained with our method
achieves 77.5% top-1 accuracy, outperforming standard supervised training with
all of the labels.
</p>
<a href="http://arxiv.org/abs/2006.10029" target="_blank">arXiv:2006.10029</a> [<a href="http://arxiv.org/pdf/2006.10029" target="_blank">pdf</a>]

<h2>Simple and Principled Uncertainty Estimation with Deterministic Deep Learning via Distance Awareness. (arXiv:2006.10108v2 [cs.LG] UPDATED)</h2>
<h3>Jeremiah Zhe Liu, Zi Lin, Shreyas Padhy, Dustin Tran, Tania Bedrax-Weiss, Balaji Lakshminarayanan</h3>
<p>Bayesian neural networks (BNN) and deep ensembles are principled approaches
to estimate the predictive uncertainty of a deep learning model. However their
practicality in real-time, industrial-scale applications are limited due to
their heavy memory and inference cost. This motivates us to study principled
approaches to high-quality uncertainty estimation that require only a single
deep neural network (DNN). By formalizing the uncertainty quantification as a
minimax learning problem, we first identify input distance awareness, i.e., the
model's ability to quantify the distance of a testing example from the training
data in the input space, as a necessary condition for a DNN to achieve
high-quality (i.e., minimax optimal) uncertainty estimation. We then propose
Spectral-normalized Neural Gaussian Process (SNGP), a simple method that
improves the distance-awareness ability of modern DNNs, by adding a weight
normalization step during training and replacing the output layer with a
Gaussian process. On a suite of vision and language understanding tasks and on
modern architectures (Wide-ResNet and BERT), SNGP is competitive with deep
ensembles in prediction, calibration and out-of-domain detection, and
outperforms the other single-model approaches.
</p>
<a href="http://arxiv.org/abs/2006.10108" target="_blank">arXiv:2006.10108</a> [<a href="http://arxiv.org/pdf/2006.10108" target="_blank">pdf</a>]

<h2>Advances in Black-Box VI: Normalizing Flows, Importance Weighting, and Optimization. (arXiv:2006.10343v2 [cs.LG] UPDATED)</h2>
<h3>Abhinav Agrawal, Daniel Sheldon, Justin Domke</h3>
<p>Recent research has seen several advances relevant to black-box VI, but the
current state of automatic posterior inference is unclear. One such advance is
the use of normalizing flows to define flexible posterior densities for deep
latent variable models. Another direction is the integration of Monte-Carlo
methods to serve two purposes; first, to obtain tighter variational objectives
for optimization, and second, to define enriched variational families through
sampling. However, both flows and variational Monte-Carlo methods remain
relatively unexplored for black-box VI. Moreover, on a pragmatic front, there
are several optimization considerations like step-size scheme, parameter
initialization, and choice of gradient estimators, for which there are no clear
guidance in the existing literature. In this paper, we postulate that black-box
VI is best addressed through a careful combination of numerous algorithmic
components. We evaluate components relating to optimization, flows, and
Monte-Carlo methods on a benchmark of 30 models from the Stan model library.
The combination of these algorithmic components significantly advances the
state-of-the-art "out of the box" variational inference.
</p>
<a href="http://arxiv.org/abs/2006.10343" target="_blank">arXiv:2006.10343</a> [<a href="http://arxiv.org/pdf/2006.10343" target="_blank">pdf</a>]

<h2>Compositional Generalization by Learning Analytical Expressions. (arXiv:2006.10627v2 [cs.AI] UPDATED)</h2>
<h3>Qian Liu, Shengnan An, Jian-Guang Lou, Bei Chen, Zeqi Lin, Yan Gao, Bin Zhou, Nanning Zheng, Dongmei Zhang</h3>
<p>Compositional generalization is a basic and essential intellective capability
of human beings, which allows us to recombine known parts readily. However,
existing neural network based models have been proven to be extremely deficient
in such a capability. Inspired by work in cognition which argues
compositionality can be captured by variable slots with symbolic functions, we
present a refreshing view that connects a memory-augmented neural model with
analytical expressions, to achieve compositional generalization. Our model
consists of two cooperative neural modules, Composer and Solver, fitting well
with the cognitive argument while being able to be trained in an end-to-end
manner via a hierarchical reinforcement learning algorithm. Experiments on the
well-known benchmark SCAN demonstrate that our model seizes a great ability of
compositional generalization, solving all challenges addressed by previous
works with 100% accuracies.
</p>
<a href="http://arxiv.org/abs/2006.10627" target="_blank">arXiv:2006.10627</a> [<a href="http://arxiv.org/pdf/2006.10627" target="_blank">pdf</a>]

<h2>Spin-Weighted Spherical CNNs. (arXiv:2006.10731v2 [cs.CV] UPDATED)</h2>
<h3>Carlos Esteves, Ameesh Makadia, Kostas Daniilidis</h3>
<p>Learning equivariant representations is a promising way to reduce sample and
model complexity and improve the generalization performance of deep neural
networks. The spherical CNNs are successful examples, producing
SO(3)-equivariant representations of spherical inputs. There are two main types
of spherical CNNs. The first type lifts the inputs to functions on the rotation
group SO(3) and applies convolutions on the group, which are computationally
expensive since SO(3) has one extra dimension. The second type applies
convolutions directly on the sphere, which are limited to zonal (isotropic)
filters, and thus have limited expressivity. In this paper, we present a new
type of spherical CNN that allows anisotropic filters in an efficient way,
without ever leaving the spherical domain. The key idea is to consider
spin-weighted spherical functions, which were introduced in physics in the
study of gravitational waves. These are complex-valued functions on the sphere
whose phases change upon rotation. We define a convolution between
spin-weighted functions and build a CNN based on it. The spin-weighted
functions can also be interpreted as spherical vector fields, allowing
applications to tasks where the inputs or outputs are vector fields.
Experiments show that our method outperforms previous methods on tasks like
classification of spherical images, classification of 3D shapes and semantic
segmentation of spherical panoramas.
</p>
<a href="http://arxiv.org/abs/2006.10731" target="_blank">arXiv:2006.10731</a> [<a href="http://arxiv.org/pdf/2006.10731" target="_blank">pdf</a>]

<h2>Generating Adjacency-Constrained Subgoals in Hierarchical Reinforcement Learning. (arXiv:2006.11485v2 [cs.LG] UPDATED)</h2>
<h3>Tianren Zhang, Shangqi Guo, Tian Tan, Xiaolin Hu, Feng Chen</h3>
<p>Goal-conditioned hierarchical reinforcement learning (HRL) is a promising
approach for scaling up reinforcement learning (RL) techniques. However, it
often suffers from training inefficiency as the action space of the high-level,
i.e., the goal space, is often large. Searching in a large goal space poses
difficulties for both high-level subgoal generation and low-level policy
learning. In this paper, we show that this problem can be effectively
alleviated by restricting the high-level action space from the whole goal space
to a $k$-step adjacent region of the current state using an adjacency
constraint. We theoretically prove that the proposed adjacency constraint
preserves the optimal hierarchical policy in deterministic MDPs, and show that
this constraint can be practically implemented by training an adjacency network
that can discriminate between adjacent and non-adjacent subgoals. Experimental
results on discrete and continuous control tasks show that incorporating the
adjacency constraint improves the performance of state-of-the-art HRL
approaches in both deterministic and stochastic environments.
</p>
<a href="http://arxiv.org/abs/2006.11485" target="_blank">arXiv:2006.11485</a> [<a href="http://arxiv.org/pdf/2006.11485" target="_blank">pdf</a>]

<h2>Enhancing Factorization Machines with Generalized Metric Learning. (arXiv:2006.11600v2 [cs.IR] UPDATED)</h2>
<h3>Yangyang Guo, Zhiyong Cheng, Jiazheng Jing, Yanpeng Lin, Liqiang Nie, Meng Wang</h3>
<p>Factorization Machines (FMs) are effective in incorporating side information
to overcome the cold-start and data sparsity problems in recommender systems.
Traditional FMs adopt the inner product to model the second-order interactions
between different attributes, which are represented via feature vectors. The
problem is that the inner product violates the triangle inequality property of
feature vectors. As a result, it cannot well capture fine-grained attribute
interactions, resulting in sub-optimal performance. Recently, the Euclidean
distance is exploited in FMs to replace the inner product and has delivered
better performance. However, previous FM methods including the ones equipped
with the Euclidean distance all focus on the attribute-level interaction
modeling, ignoring the critical intrinsic feature correlations inside
attributes. Thereby, they fail to model the complex and rich interactions
exhibited in the real-world data. To tackle this problem, in this paper, we
propose a FM framework equipped with generalized metric learning techniques to
better capture these feature correlations. In particular, based on this
framework, we present a Mahalanobis distance and a deep neural network (DNN)
methods, which can effectively model the linear and non-linear correlations
between features, respectively. Besides, we design an efficient approach for
simplifying the model functions. Experiments on several benchmark datasets
demonstrate that our proposed framework outperforms several state-of-the-art
baselines by a large margin. Moreover, we collect a new large-scale dataset on
second-hand trading to justify the effectiveness of our method over cold-start
and data sparsity problems in recommender systems.
</p>
<a href="http://arxiv.org/abs/2006.11600" target="_blank">arXiv:2006.11600</a> [<a href="http://arxiv.org/pdf/2006.11600" target="_blank">pdf</a>]

<h2>Fanoos: Multi-Resolution, Multi-Strength, Interactive Explanations for Learned Systems. (arXiv:2006.12453v3 [cs.AI] UPDATED)</h2>
<h3>David Bayani (1), Stefan Mitsch (1) ((1) Carnegie Mellon University)</h3>
<p>Machine learning becomes increasingly important to tune or even synthesize
the behavior of safety-critical components in highly non-trivial environments,
where the inability to understand learned components in general, and neural
nets in particular, poses serious obstacles to their adoption. Explainability
and interpretability methods for learned systems have gained considerable
academic attention, but the focus of current approaches on only one aspect of
explanation, at a fixed level of abstraction, and limited if any formal
guarantees, prevents those explanations from being digestible by the relevant
stakeholders (e.g., end users, certification authorities, engineers) with their
diverse backgrounds and situation-specific needs. We introduce Fanoos, a
flexible framework for combining formal verification techniques, heuristic
search, and user interaction to explore explanations at the desired level of
granularity and fidelity. We demonstrate the ability of Fanoos to produce and
adjust the abstractness of explanations in response to user requests on a
learned controller for an inverted double pendulum and on a learned CPU usage
model.
</p>
<a href="http://arxiv.org/abs/2006.12453" target="_blank">arXiv:2006.12453</a> [<a href="http://arxiv.org/pdf/2006.12453" target="_blank">pdf</a>]

<h2>Unsupervised Sound Separation Using Mixture Invariant Training. (arXiv:2006.12701v2 [eess.AS] UPDATED)</h2>
<h3>Scott Wisdom, Efthymios Tzinis, Hakan Erdogan, Ron J. Weiss, Kevin Wilson, John R. Hershey</h3>
<p>In recent years, rapid progress has been made on the problem of
single-channel sound separation using supervised training of deep neural
networks. In such supervised approaches, a model is trained to predict the
component sources from synthetic mixtures created by adding up isolated
ground-truth sources. Reliance on this synthetic training data is problematic
because good performance depends upon the degree of match between the training
data and real-world audio, especially in terms of the acoustic conditions and
distribution of sources. The acoustic properties can be challenging to
accurately simulate, and the distribution of sound types may be hard to
replicate. In this paper, we propose a completely unsupervised method, mixture
invariant training (MixIT), that requires only single-channel acoustic
mixtures. In MixIT, training examples are constructed by mixing together
existing mixtures, and the model separates them into a variable number of
latent sources, such that the separated sources can be remixed to approximate
the original mixtures. We show that MixIT can achieve competitive performance
compared to supervised methods on speech separation. Using MixIT in a
semi-supervised learning setting enables unsupervised domain adaptation and
learning from large amounts of real world data without ground-truth source
waveforms. In particular, we significantly improve reverberant speech
separation performance by incorporating reverberant mixtures, train a speech
enhancement system from noisy mixtures, and improve universal sound separation
by incorporating a large amount of in-the-wild data.
</p>
<a href="http://arxiv.org/abs/2006.12701" target="_blank">arXiv:2006.12701</a> [<a href="http://arxiv.org/pdf/2006.12701" target="_blank">pdf</a>]

<h2>PAC-Bayes Analysis Beyond the Usual Bounds. (arXiv:2006.13057v2 [stat.ML] UPDATED)</h2>
<h3>Omar Rivasplata, Ilja Kuzborskij, Csaba Szepesvari, John Shawe-Taylor</h3>
<p>We focus on a stochastic learning model where the learner observes a finite
set of training examples and the output of the learning process is a
data-dependent distribution over a space of hypotheses. The learned
data-dependent distribution is then used to make randomized predictions, and
the high-level theme addressed here is guaranteeing the quality of predictions
on examples that were not seen during training, i.e. generalization. In this
setting the unknown quantity of interest is the expected risk of the
data-dependent randomized predictor, for which upper bounds can be derived via
a PAC-Bayes analysis, leading to PAC-Bayes bounds.

Specifically, we present a basic PAC-Bayes inequality for stochastic kernels,
from which one may derive extensions of various known PAC-Bayes bounds as well
as novel bounds. We clarify the role of the requirements of fixed 'data-free'
priors, bounded losses, and i.i.d. data. We highlight that those requirements
were used to upper-bound an exponential moment term, while the basic PAC-Bayes
theorem remains valid without those restrictions. We present three bounds that
illustrate the use of data-dependent priors, including one for the unbounded
square loss.
</p>
<a href="http://arxiv.org/abs/2006.13057" target="_blank">arXiv:2006.13057</a> [<a href="http://arxiv.org/pdf/2006.13057" target="_blank">pdf</a>]

<h2>Hyperparameter Ensembles for Robustness and Uncertainty Quantification. (arXiv:2006.13570v2 [cs.LG] UPDATED)</h2>
<h3>Florian Wenzel, Jasper Snoek, Dustin Tran, Rodolphe Jenatton</h3>
<p>Ensembles over neural network weights trained from different random
initialization, known as deep ensembles, achieve state-of-the-art accuracy and
calibration. The recently introduced batch ensembles provide a drop-in
replacement that is more parameter efficient. In this paper, we design
ensembles not only over weights, but over hyperparameters to improve the state
of the art in both settings. For best performance independent of budget, we
propose hyper-deep ensembles, a simple procedure that involves a random search
over different hyperparameters, themselves stratified across multiple random
initializations. Its strong performance highlights the benefit of combining
models with both weight and hyperparameter diversity. We further propose a
parameter efficient version, hyper-batch ensembles, which builds on the layer
structure of batch ensembles and self-tuning networks. The computational and
memory costs of our method are notably lower than typical ensembles. On image
classification tasks, with MLP, LeNet, ResNet 20 and Wide ResNet 28-10
architectures, we improve upon both deep and batch ensembles.
</p>
<a href="http://arxiv.org/abs/2006.13570" target="_blank">arXiv:2006.13570</a> [<a href="http://arxiv.org/pdf/2006.13570" target="_blank">pdf</a>]

<h2>Labelling unlabelled videos from scratch with multi-modal self-supervision. (arXiv:2006.13662v2 [cs.CV] UPDATED)</h2>
<h3>Yuki M. Asano, Mandela Patrick, Christian Rupprecht, Andrea Vedaldi</h3>
<p>A large part of the current success of deep learning lies in the
effectiveness of data -- more precisely: labelled data. Yet, labelling a
dataset with human annotation continues to carry high costs, especially for
videos. While in the image domain, recent methods have allowed to generate
meaningful (pseudo-) labels for unlabelled datasets without supervision, this
development is missing for the video domain where learning feature
representations is the current focus. In this work, we a) show that
unsupervised labelling of a video dataset does not come for free from strong
feature encoders and b) propose a novel clustering method that allows
pseudo-labelling of a video dataset without any human annotations, by
leveraging the natural correspondence between the audio and visual modalities.
An extensive analysis shows that the resulting clusters have high semantic
overlap to ground truth human labels. We further introduce the first
benchmarking results on unsupervised labelling of common video datasets
Kinetics, Kinetics-Sound, VGG-Sound and AVE.
</p>
<a href="http://arxiv.org/abs/2006.13662" target="_blank">arXiv:2006.13662</a> [<a href="http://arxiv.org/pdf/2006.13662" target="_blank">pdf</a>]

<h2>Is SGD a Bayesian sampler? Well, almost. (arXiv:2006.15191v2 [cs.LG] UPDATED)</h2>
<h3>Chris Mingard, Guillermo Valle-P&#xe9;rez, Joar Skalse, Ard A. Louis</h3>
<p>Overparameterised deep neural networks (DNNs) are highly expressive and so
can, in principle, generate almost any function that fits a training dataset
with zero error. The vast majority of these functions will perform poorly on
unseen data, and yet in practice DNNs often generalise remarkably well. This
success suggests that a trained DNN must have a strong inductive bias towards
functions with low generalisation error. Here we empirically investigate this
inductive bias by calculating, for a range of architectures and datasets, the
probability $P_{SGD}(f\mid S)$ that an overparameterised DNN, trained with
stochastic gradient descent (SGD) or one of its variants, converges on a
function $f$ consistent with a training set $S$. We also use Gaussian processes
to estimate the Bayesian posterior probability $P_B(f\mid S)$ that the DNN
expresses $f$ upon random sampling of its parameters, conditioned on $S$.

Our main findings are that $P_{SGD}(f\mid S)$ correlates remarkably well with
$P_B(f\mid S)$ and that $P_B(f\mid S)$ is strongly biased towards low-error and
low complexity functions. These results imply that strong inductive bias in the
parameter-function map (which determines $P_B(f\mid S)$), rather than a special
property of SGD, is the primary explanation for why DNNs generalise so well in
the overparameterised regime.

While our results suggest that the Bayesian posterior $P_B(f\mid S)$ is the
first order determinant of $P_{SGD}(f\mid S)$, there remain second order
differences that are sensitive to hyperparameter tuning. A function probability
picture, based on $P_{SGD}(f\mid S)$ and/or $P_B(f\mid S)$, can shed new light
on the way that variations in architecture or hyperparameter settings such as
batch size, learning rate, and optimiser choice, affect DNN performance.
</p>
<a href="http://arxiv.org/abs/2006.15191" target="_blank">arXiv:2006.15191</a> [<a href="http://arxiv.org/pdf/2006.15191" target="_blank">pdf</a>]

<h2>Model-based Reinforcement Learning for Semi-Markov Decision Processes with Neural ODEs. (arXiv:2006.16210v2 [cs.LG] UPDATED)</h2>
<h3>Jianzhun Du, Joseph Futoma, Finale Doshi-Velez</h3>
<p>We present two elegant solutions for modeling continuous-time dynamics, in a
novel model-based reinforcement learning (RL) framework for semi-Markov
decision processes (SMDPs), using neural ordinary differential equations
(ODEs). Our models accurately characterize continuous-time dynamics and enable
us to develop high-performing policies using a small amount of data. We also
develop a model-based approach for optimizing time schedules to reduce
interaction rates with the environment while maintaining the near-optimal
performance, which is not possible for model-free methods. We experimentally
demonstrate the efficacy of our methods across various continuous-time domains.
</p>
<a href="http://arxiv.org/abs/2006.16210" target="_blank">arXiv:2006.16210</a> [<a href="http://arxiv.org/pdf/2006.16210" target="_blank">pdf</a>]

<h2>Applying Machine Learning Techniques for Caching in Edge Networks: A Comprehensive Survey. (arXiv:2006.16864v3 [cs.NI] UPDATED)</h2>
<h3>Junaid Shuja, Kashif Bilal, Eisa Alanazi, Waleed Alasmary, Abdulaziz Alashaikh, Albert Y. Zomaya</h3>
<p>Edge networks provide access to a group of proximate users who may have
similar content interests. Caching popular content at the edge networks leads
to lower latencies while reducing the load on backhaul and core networks with
the emergence of high-speed 5G networks. User mobility, preferences, and
content popularity are the dominant dynamic features of the edge networks.
Temporal and social features of content, such as the number of views and likes
are applied to estimate the popularity of content from a global perspective.
However, such estimates may not be mapped to an edge network with particular
social and geographic characteristics. In edge networks, machine learning
techniques can be applied to predict content popularity based on user
preferences, user mobility based on user location history, cluster users based
on similar content interests, and optimize cache placement strategies provided
a set of constraints and predictions about the state of the network. These
applications of machine learning can help identify relevant content for an edge
network to lower latencies and increase cache hits. This article surveys the
application of machine learning techniques for caching content in edge
networks. We survey recent state-of-the-art literature and formulate a
comprehensive taxonomy based on (a) machine learning technique, (b) caching
strategy, and edge network. We further survey supporting concepts for optimal
edge caching decisions that require the application of machine learning. These
supporting concepts are social-awareness, popularity prediction, and community
detection in edge networks. A comparative analysis of the state-of-the-art
literature is presented with respect to the parameters identified in the
taxonomy. Moreover, we debate research challenges and future directions for
optimal caching decisions and the application of machine learning towards
caching in edge networks.
</p>
<a href="http://arxiv.org/abs/2006.16864" target="_blank">arXiv:2006.16864</a> [<a href="http://arxiv.org/pdf/2006.16864" target="_blank">pdf</a>]

<h2>Ultrahyperbolic Representation Learning. (arXiv:2007.00211v3 [cs.LG] UPDATED)</h2>
<h3>Marc T. Law, Jos Stam</h3>
<p>In machine learning, data is usually represented in a (flat) Euclidean space
where distances between points are along straight lines. Researchers have
recently considered more exotic (non-Euclidean) Riemannian manifolds such as
hyperbolic space which is well suited for tree-like data. In this paper, we
propose a representation living on a pseudo-Riemannian manifold of constant
nonzero curvature. It is a generalization of hyperbolic and spherical
geometries where the non-degenerate metric tensor need not be positive
definite. We provide the necessary learning tools in this geometry and extend
gradient method optimization techniques. More specifically, we provide
closed-form expressions for distances via geodesics and define a descent
direction to minimize some objective function. Our novel framework is applied
to graph representations.
</p>
<a href="http://arxiv.org/abs/2007.00211" target="_blank">arXiv:2007.00211</a> [<a href="http://arxiv.org/pdf/2007.00211" target="_blank">pdf</a>]

<h2>Synergistic saliency and depth prediction for RGB-D saliency detection. (arXiv:2007.01711v2 [cs.CV] UPDATED)</h2>
<h3>Yue Wang, Yuke Li, James H. Elder, Huchuan Lu, Runmin Wu, Lu Zhang</h3>
<p>Depth information available from an RGB-D camera can be useful in segmenting
salient objects when figure/ground cues from RGB channels are weak. This has
motivated the development of several RGB-D saliency datasets and algorithms
that use all four channels of the RGB-D data for both training and inference.
Unfortunately, existing RGB-D saliency datasets are small, which may lead to
overfitting and limited generalization for diverse scenarios. Here we propose a
semi-supervised system for RGB-D saliency detection that can be trained on
smaller RGB-D saliency datasets without saliency ground truth, while also make
effective joint use of a large RGB saliency dataset with saliency ground truth
together. To generalize our method on RGB-D saliency datasets, a novel
prediction-guided cross-refinement module which jointly estimates both saliency
and depth by mutual refinement between two respective tasks, and an adversarial
learning approach are employed. Critically, our system does not require
saliency ground-truth for the RGB-D datasets, which saves the massive human
labor for hand labeling, and does not require the depth data for inference,
allowing the method to be used for the much broader range of applications where
only RGB data are available. Evaluation on seven RGB-D datasets demonstrates
that even without saliency ground truth for RGB-D datasets and using only the
RGB data of RGB-D datasets at inference, our semi-supervised system performs
favorable against state-of-the-art fully-supervised RGB-D saliency detection
methods that use saliency ground truth for RGB-D datasets at training and depth
data at inference on two largest testing datasets. Our approach also achieves
comparable results on other popular RGB-D saliency benchmarks.
</p>
<a href="http://arxiv.org/abs/2007.01711" target="_blank">arXiv:2007.01711</a> [<a href="http://arxiv.org/pdf/2007.01711" target="_blank">pdf</a>]

<h2>Efficient Marginalization of Discrete and Structured Latent Variables via Sparsity. (arXiv:2007.01919v2 [cs.LG] UPDATED)</h2>
<h3>Gon&#xe7;alo M. Correia, Vlad Niculae, Wilker Aziz, Andr&#xe9; F. T. Martins</h3>
<p>Training neural network models with discrete (categorical or structured)
latent variables can be computationally challenging, due to the need for
marginalization over large or combinatorial sets. To circumvent this issue, one
typically resorts to sampling-based approximations of the true marginal,
requiring noisy gradient estimators (e.g., score function estimator) or
continuous relaxations with lower-variance reparameterized gradients (e.g.,
Gumbel-Softmax). In this paper, we propose a new training strategy which
replaces these estimators by an exact yet efficient marginalization. To achieve
this, we parameterize discrete distributions over latent assignments using
differentiable sparse mappings: sparsemax and its structured counterparts. In
effect, the support of these distributions is greatly reduced, which enables
efficient marginalization. We report successful results in three tasks covering
a range of latent variable modeling applications: a semisupervised deep
generative model, a latent communication game, and a generative model with a
bit-vector latent representation. In all cases, we obtain good performance
while still achieving the practicality of sampling-based approximations.
</p>
<a href="http://arxiv.org/abs/2007.01919" target="_blank">arXiv:2007.01919</a> [<a href="http://arxiv.org/pdf/2007.01919" target="_blank">pdf</a>]

<h2>Understanding and Improving Fast Adversarial Training. (arXiv:2007.02617v2 [cs.LG] UPDATED)</h2>
<h3>Maksym Andriushchenko, Nicolas Flammarion</h3>
<p>A recent line of work focused on making adversarial training computationally
efficient for deep learning models. In particular, Wong et al. (2020) showed
that $\ell_\infty$-adversarial training with fast gradient sign method (FGSM)
can fail due to a phenomenon called "catastrophic overfitting", when the model
quickly loses its robustness over a single epoch of training. We show that
adding a random step to FGSM, as proposed in Wong et al. (2020), does not
prevent catastrophic overfitting, and that randomness is not important per se
-- its main role being simply to reduce the magnitude of the perturbation.
Moreover, we show that catastrophic overfitting is not inherent to deep and
overparametrized networks, but can occur in a single-layer convolutional
network with a few filters. In an extreme case, even a single filter can make
the network highly non-linear locally, which is the main reason why FGSM
training fails. Based on this observation, we propose a new regularization
method, GradAlign, that prevents catastrophic overfitting by explicitly
maximizing the gradient alignment inside the perturbation set and improves the
quality of the FGSM solution. As a result, GradAlign allows to successfully
apply FGSM training also for larger $\ell_\infty$-perturbations and reduce the
gap to multi-step adversarial training. The code of our experiments is
available at https://github.com/tml-epfl/understanding-fast-adv-training.
</p>
<a href="http://arxiv.org/abs/2007.02617" target="_blank">arXiv:2007.02617</a> [<a href="http://arxiv.org/pdf/2007.02617" target="_blank">pdf</a>]

<h2>Curriculum learning for multilevel budgeted combinatorial problems. (arXiv:2007.03151v2 [cs.LG] UPDATED)</h2>
<h3>Adel Nabli, Margarida Carvalho</h3>
<p>Learning heuristics for combinatorial optimization problems through graph
neural networks have recently shown promising results on some classic NP-hard
problems. These are single-level optimization problems with only one player.
Multilevel combinatorial optimization problems are their generalization,
encompassing situations with multiple players taking decisions sequentially. By
framing them in a multi-agent reinforcement learning setting, we devise a
value-based method to learn to solve multilevel budgeted combinatorial problems
involving two players in a zero-sum game over a graph. Our framework is based
on a simple curriculum: if an agent knows how to estimate the value of
instances with budgets up to $B$, then solving instances with budget $B+1$ can
be done in polynomial time regardless of the direction of the optimization by
checking the value of every possible afterstate. Thus, in a bottom-up approach,
we generate datasets of heuristically solved instances with increasingly larger
budgets to train our agent. We report results close to optimality on graphs up
to $100$ nodes and a $185 \times$ speedup on average compared to the quickest
exact solver known for the Multilevel Critical Node problem, a max-min-max
trilevel problem that has been shown to be at least $\Sigma_2^p$-hard.
</p>
<a href="http://arxiv.org/abs/2007.03151" target="_blank">arXiv:2007.03151</a> [<a href="http://arxiv.org/pdf/2007.03151" target="_blank">pdf</a>]

<h2>Necessary and Sufficient Conditions for Inverse Reinforcement Learning of Bayesian Stopping Time Problems. (arXiv:2007.03481v2 [cs.LG] UPDATED)</h2>
<h3>Vikram Krishnamurthy, Kunal Pattanayak</h3>
<p>This paper presents an inverse reinforcement learning (IRL) framework for
Bayesian stopping time problems. By observing the actions of a Bayesian
decision maker, we provide a necessary and sufficient condition to identify if
these actions are consistent with optimizing a cost function; then we construct
set valued estimates of the cost function. To achieve this IRL objective, we
use novel ideas from Bayesian revealed preferences stemming from
microeconomics. To illustrate our IRL scheme,we consider two important examples
of stopping time problems, namely, sequential hypothesis testing and Bayesian
search. Finally, for finite datasets, we propose an IRL detection algorithm and
give finite sample bounds on its error probabilities. Also we discuss how to
identify $\epsilon$-optimal Bayesian decision makers and perform IRL.
</p>
<a href="http://arxiv.org/abs/2007.03481" target="_blank">arXiv:2007.03481</a> [<a href="http://arxiv.org/pdf/2007.03481" target="_blank">pdf</a>]

<h2>PIE-NET: Parametric Inference of Point Cloud Edges. (arXiv:2007.04883v2 [cs.CV] UPDATED)</h2>
<h3>Xiaogang Wang, Yuelang Xu, Kai Xu, Andrea Tagliasacchi, Bin Zhou, Ali Mahdavi-Amiri, Hao Zhang</h3>
<p>We introduce an end-to-end learnable technique to robustly identify feature
edges in 3D point cloud data. We represent these edges as a collection of
parametric curves (i.e.,lines, circles, and B-splines). Accordingly, our deep
neural network, coined PIE-NET, is trained for parametric inference of edges.
The network relies on a "region proposal" architecture, where a first module
proposes an over-complete collection of edge and corner points, and a second
module ranks each proposal to decide whether it should be considered. We train
and evaluate our method on the ABC dataset, a large dataset of CAD models, and
compare our results to those produced by traditional (non-learning) processing
pipelines, as well as a recent deep learning based edge detector (EC-NET). Our
results significantly improve over the state-of-the-art from both a
quantitative and qualitative standpoint.
</p>
<a href="http://arxiv.org/abs/2007.04883" target="_blank">arXiv:2007.04883</a> [<a href="http://arxiv.org/pdf/2007.04883" target="_blank">pdf</a>]

<h2>Coarse-to-Fine Pseudo-Labeling Guided Meta-Learning for Inexactly-Supervised Few-Shot Classification. (arXiv:2007.05675v2 [cs.CV] UPDATED)</h2>
<h3>Jinhai Yang, Hua Yang, Lin Chen</h3>
<p>Meta-learning has recently emerged as a promising technique to address the
challenge of few-shot learning. However, most existing meta-learning algorithms
require fine-grained supervision, thereby involving prohibitive annotation
cost. In this paper, we present a new problem named inexactly-supervised
meta-learning to alleviate such limitation, focusing on tackling few-shot
classification tasks with only coarse-grained supervision. Accordingly, we
propose a Coarse-to-Fine (C2F) pseudo-labeling process to construct
pseudo-tasks from coarsely-labeled data by grouping each coarse-class into
pseudo-fine-classes via similarity matching. Moreover, we develop a Bi-level
Discriminative Embedding (BDE) to obtain a good image similarity measure in
both visual and semantic aspects with inexact supervision. Experiments across
representative benchmarks indicate that our approach shows profound advantages
over baseline models.
</p>
<a href="http://arxiv.org/abs/2007.05675" target="_blank">arXiv:2007.05675</a> [<a href="http://arxiv.org/pdf/2007.05675" target="_blank">pdf</a>]

<h2>Bayesian Deep Ensembles via the Neural Tangent Kernel. (arXiv:2007.05864v2 [stat.ML] UPDATED)</h2>
<h3>Bobby He, Balaji Lakshminarayanan, Yee Whye Teh</h3>
<p>We explore the link between deep ensembles and Gaussian processes (GPs)
through the lens of the Neural Tangent Kernel (NTK): a recent development in
understanding the training dynamics of wide neural networks (NNs). Previous
work has shown that even in the infinite width limit, when NNs become GPs,
there is no GP posterior interpretation to a deep ensemble trained with squared
error loss. We introduce a simple modification to standard deep ensembles
training, through addition of a computationally-tractable, randomised and
untrainable function to each ensemble member, that enables a posterior
interpretation in the infinite width limit. When ensembled together, our
trained NNs give an approximation to a posterior predictive distribution, and
we prove that our Bayesian deep ensembles make more conservative predictions
than standard deep ensembles in the infinite width limit. Finally, using finite
width NNs we demonstrate that our Bayesian deep ensembles faithfully emulate
the analytic posterior predictive when available, and can outperform standard
deep ensembles in various out-of-distribution settings, for both regression and
classification tasks.
</p>
<a href="http://arxiv.org/abs/2007.05864" target="_blank">arXiv:2007.05864</a> [<a href="http://arxiv.org/pdf/2007.05864" target="_blank">pdf</a>]

<h2>GRADE: Graph Dynamic Embedding. (arXiv:2007.08060v2 [cs.LG] UPDATED)</h2>
<h3>Simeon Spasov, Alessandro Di Stefano, Pietro Lio, Jian Tang</h3>
<p>Representation learning of static and more recently dynamically evolving
graphs has gained noticeable attention. Existing approaches for modelling graph
dynamics focus extensively on the evolution of individual nodes independently
of the evolution of mesoscale community structures. As a result, current
methods do not provide useful tools to study and cannot explicitly capture
temporal community dynamics. To address this challenge, we propose GRADE - a
probabilistic model that learns to generate evolving node and community
representations by imposing a random walk prior over their trajectories. Our
model also learns node community membership which is updated between time steps
via a transition matrix. At each time step link generation is performed by
first assigning node membership from a distribution over the communities, and
then sampling a neighbor from a distribution over the nodes for the assigned
community. We parametrize the node and community distributions with neural
networks and learn their parameters via variational inference. Experiments
demonstrate GRADE outperforms baselines in dynamic link prediction, shows
favourable performance on dynamic community detection, and identifies coherent
and interpretable evolving communities.
</p>
<a href="http://arxiv.org/abs/2007.08060" target="_blank">arXiv:2007.08060</a> [<a href="http://arxiv.org/pdf/2007.08060" target="_blank">pdf</a>]

<h2>Backdoor Learning: A Survey. (arXiv:2007.08745v3 [cs.CR] UPDATED)</h2>
<h3>Yiming Li, Baoyuan Wu, Yong Jiang, Zhifeng Li, Shu-Tao Xia</h3>
<p>Backdoor attack intends to embed hidden backdoor into deep neural networks
(DNNs), such that the attacked model performs well on benign samples, whereas
its prediction will be maliciously changed if the hidden backdoor is activated
by the attacker-defined trigger. Backdoor attack could happen when the training
process is not fully controlled by the user, such as training on third-party
datasets or adopting third-party models, which poses a new and realistic
threat. Although backdoor learning is an emerging and rapidly growing research
area, its systematic review, however, remains blank. In this paper, we present
the first comprehensive survey of this realm. We summarize and categorize
existing backdoor attacks and defenses based on their characteristics, and
provide a unified framework for analyzing poisoning-based backdoor attacks.
Besides, we also analyze the relation between backdoor attacks and the relevant
fields ($i.e.,$ adversarial attack and data poisoning), and summarize the
benchmark datasets. Finally, we briefly outline certain future research
directions relying upon reviewed works.
</p>
<a href="http://arxiv.org/abs/2007.08745" target="_blank">arXiv:2007.08745</a> [<a href="http://arxiv.org/pdf/2007.08745" target="_blank">pdf</a>]

<h2>Self-Supervised Representation Learning for Vocal Music Context. (arXiv:2007.09060v2 [cs.SD] UPDATED)</h2>
<h3>Camille Noufi, Prateek Verma, Jonathan Berger</h3>
<p>In music and speech, meaning is derived at multiple levels of context.
Affect, for example, can be inferred both by a short sound token and by sonic
patterns over a longer temporal window such as an entire recording. In this
paper we focus on inferring meaning from this dichotomy of contexts. We show
how contextual representations of short sung vocal lines can be implicitly
learned from fundamental frequency ($F_0$) and thus be used as a meaningful
feature space for downstream Music Information Retrieval (MIR) tasks. We
propose three self-supervised deep learning paradigms which leverage pseudotask
learning of these two levels of context to produce latent representation
spaces. We evaluate the usefulness of these representations by embedding unseen
vocal contours into each space and conducting downstream classification tasks.
Our results show that contextual representation can enhance downstream
classification by as much as 15 % as compared to using traditional statistical
contour features.
</p>
<a href="http://arxiv.org/abs/2007.09060" target="_blank">arXiv:2007.09060</a> [<a href="http://arxiv.org/pdf/2007.09060" target="_blank">pdf</a>]

<h2>Multi-Stage Hybrid Federated Learning over Large-Scale D2D-Enabled Fog Networks. (arXiv:2007.09511v4 [cs.NI] UPDATED)</h2>
<h3>Seyyedali Hosseinalipour, Sheikh Shams Azam, Christopher G. Brinton, Nicolo Michelusi, Vaneet Aggarwal, David J. Love, Huaiyu Dai</h3>
<p>Federated learning has generated significant interest, with nearly all works
focused on a ``star" topology where nodes/devices are each connected to a
central server. We migrate away from this architecture and extend it through
the network dimension to the case where there are multiple layers of nodes
between the end devices and the server. Specifically, we develop multi-stage
hybrid federated learning (MH-FL), a hybrid of intra- and inter-layer model
learning that considers the network as a multi-layer cluster-based structure,
each layer of which consists of multiple device clusters. MH-FL considers the
topology structures among the nodes in the clusters, including local networks
formed via device-to-device (D2D) communications. It orchestrates the devices
at different network layers in a collaborative/cooperative manner (i.e., using
D2D interactions) to form local consensus on the model parameters, and combines
it with multi-stage parameter relaying between layers of the tree-shaped
hierarchy. We derive the upper bound of convergence for MH-FL with respect to
parameters of the network topology (e.g., the spectral radius) and the learning
algorithm (e.g., the number of D2D rounds in different clusters). We obtain a
set of policies for the D2D rounds at different clusters to guarantee either a
finite optimality gap or convergence to the global optimum. We then develop a
distributed control algorithm for MH-FL to tune the D2D rounds in each cluster
over time to meet specific convergence criteria. Our experiments on real-world
datasets verify our analytical results and demonstrate the advantages of MH-FL
in terms of resource utilization metrics.
</p>
<a href="http://arxiv.org/abs/2007.09511" target="_blank">arXiv:2007.09511</a> [<a href="http://arxiv.org/pdf/2007.09511" target="_blank">pdf</a>]

<h2>A Systematic Literature Review on Federated Machine Learning: From A Software Engineering Perspective. (arXiv:2007.11354v5 [cs.SE] UPDATED)</h2>
<h3>Sin Kit Lo, Qinghua Lu, Chen Wang, Hye-Young Paik, Liming Zhu</h3>
<p>Federated learning is an emerging machine learning paradigm where multiple
clients train models locally and formulate a global model based on the local
model updates. To identify the state-of-the-art in federated learning from a
software engineering perspective, we performed a systematic literature review
with the extracted 231 primary studies. The results show that most of the known
motivations of federated learning appear to be the most studied federated
learning challenges, such as communication efficiency and statistical
heterogeneity. Also, there are only a few real-world applications of federated
learning. Hence, more studies in this area are needed before the actual
industrial-level adoption of federated learning.
</p>
<a href="http://arxiv.org/abs/2007.11354" target="_blank">arXiv:2007.11354</a> [<a href="http://arxiv.org/pdf/2007.11354" target="_blank">pdf</a>]

<h2>Time-Reversal Symmetric ODE Network. (arXiv:2007.11362v2 [cs.LG] UPDATED)</h2>
<h3>In Huh, Eunho Yang, Sung Ju Hwang, Jinwoo Shin</h3>
<p>Time-reversal symmetry, which requires that the dynamics of a system should
not change with the reversal of time axis, is a fundamental property that
frequently holds in classical and quantum mechanics. In this paper, we propose
a novel loss function that measures how well our ordinary differential equation
(ODE) networks comply with this time-reversal symmetry; it is formally defined
by the discrepancy in the time evolutions of ODE networks between forward and
backward dynamics. Then, we design a new framework, which we name as
Time-Reversal Symmetric ODE Networks (TRS-ODENs), that can learn the dynamics
of physical systems more sample-efficiently by learning with the proposed loss
function. We evaluate TRS-ODENs on several classical dynamics, and find they
can learn the desired time evolution from observed noisy and complex
trajectories. We also show that, even for systems that do not possess the full
time-reversal symmetry, TRS-ODENs can achieve better predictive performances
over baselines.
</p>
<a href="http://arxiv.org/abs/2007.11362" target="_blank">arXiv:2007.11362</a> [<a href="http://arxiv.org/pdf/2007.11362" target="_blank">pdf</a>]

<h2>Deep Variational Instance Segmentation. (arXiv:2007.11576v2 [cs.CV] UPDATED)</h2>
<h3>Jialin Yuan, Chao Chen, Li Fuxin</h3>
<p>Instance Segmentation, which seeks to obtain both class and instance labels
for each pixel in the input image, is a challenging task in computer vision.
State-of-the-art algorithms often employ two separate stages, the first one
generating object proposals and the second one recognizing and refining the
boundaries. Further, proposals are usually based on detectors such as faster
R-CNN which search for boxes in the entire image exhaustively. In this paper,
we propose a novel algorithm that directly utilizes a fully convolutional
network (FCN) to predict instance labels. Specifically, we propose a
variational relaxation of instance segmentation as minimizing an optimization
functional for a piecewise-constant segmentation problem, which can be used to
train an FCN end-to-end. It extends the classical Mumford-Shah variational
segmentation problem to be able to handle permutation-invariant labels in the
ground truth of instance segmentation. Experiments on PASCAL VOC 2012, Semantic
Boundaries dataset(SBD), and the MSCOCO 2017 dataset show that the proposed
approach efficiently tackle the instance segmentation task. The source code and
trained models will be released with the paper.
</p>
<a href="http://arxiv.org/abs/2007.11576" target="_blank">arXiv:2007.11576</a> [<a href="http://arxiv.org/pdf/2007.11576" target="_blank">pdf</a>]

<h2>Bridging the Imitation Gap by Adaptive Insubordination. (arXiv:2007.12173v2 [cs.LG] UPDATED)</h2>
<h3>Luca Weihs, Unnat Jain, Jordi Salvador, Svetlana Lazebnik, Aniruddha Kembhavi, Alexander Schwing</h3>
<p>When expert supervision is available, practitioners often use imitation
learning with varying degrees of success. We show that when an expert has
access to privileged information that is unavailable to the student, this
information is marginalized in the student policy during imitation learning
resulting in an "imitation gap" and, potentially, poor results. Prior work
bridges this gap via a progression from imitation learning to reinforcement
learning. While often successful, gradual progression fails for tasks that
require frequent switches between exploration and memorization skills. To
better address these tasks and alleviate the imitation gap we propose 'Adaptive
Insubordination' (ADVISOR), which dynamically weights imitation and
reward-based reinforcement learning losses during training, enabling switching
between imitation and exploration. On a suite of challenging didactic and
MiniGrid tasks, we show that ADVISOR outperforms pure imitation, pure
reinforcement learning, as well as their sequential and parallel combinations.
</p>
<a href="http://arxiv.org/abs/2007.12173" target="_blank">arXiv:2007.12173</a> [<a href="http://arxiv.org/pdf/2007.12173" target="_blank">pdf</a>]

<h2>FedML: A Research Library and Benchmark for Federated Machine Learning. (arXiv:2007.13518v2 [cs.LG] UPDATED)</h2>
<h3>Chaoyang He, Songze Li, Jinhyun So, Xiao Zeng, Mi Zhang, Hongyi Wang, Xiaoyang Wang, Praneeth Vepakomma, Abhishek Singh, Hang Qiu, Xinghua Zhu, Jianzong Wang, Li Shen, Peilin Zhao, Yan Kang, Yang Liu, Ramesh Raskar, Qiang Yang, Murali Annavaram, Salman Avestimehr</h3>
<p>Federated learning (FL) is a rapidly growing research field in machine
learning. However, existing FL libraries cannot adequately support diverse
algorithmic development; inconsistent dataset and model usage make fair
algorithm comparison challenging. In this work, we introduce FedML, an open
research library and benchmark to facilitate FL algorithm development and fair
performance comparison. FedML supports three computing paradigms: on-device
training for edge devices, distributed computing, and single-machine
simulation. FedML also promotes diverse algorithmic research with flexible and
generic API design and comprehensive reference baseline implementations
(optimizer, models, and datasets). We hope FedML could provide an efficient and
reproducible means for developing and evaluating FL algorithms that would
benefit the FL research community. We maintain the source code, documents, and
user community at https://fedml.ai.
</p>
<a href="http://arxiv.org/abs/2007.13518" target="_blank">arXiv:2007.13518</a> [<a href="http://arxiv.org/pdf/2007.13518" target="_blank">pdf</a>]

<h2>Maximum Mutation Reinforcement Learning for Scalable Control. (arXiv:2007.13690v3 [cs.LG] UPDATED)</h2>
<h3>Karush Suri, Xiao Qi Shi, Konstantinos N. Plataniotis, Yuri A. Lawryshyn</h3>
<p>Advances in Reinforcement Learning (RL) have successfully tackled sample
efficiency and overestimation bias. However, these methods often fall short of
scalable performance. On the other hand, genetic methods provide scalability
but depict hyperparameter sensitivity to evolutionary operations. We present
the Evolution-based Soft Actor-Critic (ESAC), a scalable RL algorithm. Our
contributions are threefold; ESAC (1) abstracts exploration from exploitation
by combining Evolution Strategies (ES) with Soft Actor-Critic (SAC), (2)
provides dominant skill transfer between offsprings by making use of soft
winner selections and genetic crossovers in hindsight and (3) improves
hyperparameter sensitivity in evolutions using Automatic Mutation Tuning (AMT).
AMT gradually replaces the entropy framework of SAC allowing the population to
succeed at the task while acting as randomly as possible, without making use of
backpropagation updates. On a range of challenging control tasks consisting of
high-dimensional action spaces and sparse rewards, ESAC demonstrates
state-of-the-art performance and sample efficiency equivalent to SAC. ESAC
demonstrates scalability comparable to ES on the basis of hardware resources
and algorithm overhead. A complete implementation of ESAC with notes on
reproducibility and videos can be found at the project website
https://karush17.github.io/esac-web/.
</p>
<a href="http://arxiv.org/abs/2007.13690" target="_blank">arXiv:2007.13690</a> [<a href="http://arxiv.org/pdf/2007.13690" target="_blank">pdf</a>]

<h2>Group Knowledge Transfer: Federated Learning of Large CNNs at the Edge. (arXiv:2007.14513v3 [cs.LG] UPDATED)</h2>
<h3>Chaoyang He, Murali Annavaram, Salman Avestimehr</h3>
<p>Scaling up the convolutional neural network (CNN) size (e.g., width, depth,
etc.) is known to effectively improve model accuracy. However, the large model
size impedes training on resource-constrained edge devices. For instance,
federated learning (FL) may place undue burden on the compute capability of
edge nodes, even though there is a strong practical need for FL due to its
privacy and confidentiality properties. To address the resource-constrained
reality of edge devices, we reformulate FL as a group knowledge transfer
training algorithm, called FedGKT. FedGKT designs a variant of the alternating
minimization approach to train small CNNs on edge nodes and periodically
transfer their knowledge by knowledge distillation to a large server-side CNN.
FedGKT consolidates several advantages into a single framework: reduced demand
for edge computation, lower communication bandwidth for large CNNs, and
asynchronous training, all while maintaining model accuracy comparable to
FedAvg. We train CNNs designed based on ResNet-56 and ResNet-110 using three
distinct datasets (CIFAR-10, CIFAR-100, and CINIC-10) and their non-I.I.D.
variants. Our results show that FedGKT can obtain comparable or even slightly
higher accuracy than FedAvg. More importantly, FedGKT makes edge training
affordable. Compared to the edge training using FedAvg, FedGKT demands 9 to 17
times less computational power (FLOPs) on edge devices and requires 54 to 105
times fewer parameters in the edge CNN. Our source code is released at FedML
(https://fedml.ai).
</p>
<a href="http://arxiv.org/abs/2007.14513" target="_blank">arXiv:2007.14513</a> [<a href="http://arxiv.org/pdf/2007.14513" target="_blank">pdf</a>]

<h2>A Survey on Text Classification: From Shallow to Deep Learning. (arXiv:2008.00364v5 [cs.CL] UPDATED)</h2>
<h3>Qian Li, Hao Peng, Jianxin Li, Congying Xia, Renyu Yang, Lichao Sun, Philip S. Yu, Lifang He</h3>
<p>Text classification is the most fundamental and essential task in natural
language processing. The last decade has seen a surge of research in this area
due to the unprecedented success of deep learning. Numerous methods, datasets,
and evaluation metrics have been proposed in the literature, raising the need
for a comprehensive and updated survey. This paper fills the gap by reviewing
the state of the art approaches from 1961 to 2020, focusing on models from
shallow to deep learning. We create a taxonomy for text classification
according to the text involved and the models used for feature extraction and
classification. We then discuss each of these categories in detail, dealing
with both the technical developments and benchmark datasets that support tests
of predictions. A comprehensive comparison between different techniques, as
well as identifying the pros and cons of various evaluation metrics are also
provided in this survey. Finally, we conclude by summarizing key implications,
future research directions, and the challenges facing the research area.
</p>
<a href="http://arxiv.org/abs/2008.00364" target="_blank">arXiv:2008.00364</a> [<a href="http://arxiv.org/pdf/2008.00364" target="_blank">pdf</a>]

<h2>Active Object Search. (arXiv:2008.00923v5 [cs.CV] UPDATED)</h2>
<h3>Jie Wu, Tianshui Chen, Lishan Huang, Hefeng Wu, Guanbin Li, Ling Tian, Liang Lin</h3>
<p>In this work, we investigate an Active Object Search (AOS) task that is not
explicitly addressed in the literature; it aims to actively perform as few
action steps as possible to search and locate the target object in a 3D indoor
scene. Different from classic object detection that passively receives visual
information, this task encourages an intelligent agent to perform active search
via reasonable action planning; thus it can better recall the target objects,
especially for the challenging situations that the target is far from the
agent, blocked by an obstacle and out of view. To handle this cross-modal task,
we formulate a reinforcement learning framework that consists of a 3D object
detector, a state controller and a cross-modal action planner to work
cooperatively to find out the target object with minimal action steps. During
training, we design a novel cost-sensitive active search reward that penalizes
inaccurate object search and redundant action steps. To evaluate this novel
task, we construct an Active Object Search (AOS) benchmark that contains 5,845
samples from 30 diverse indoor scenes. We conduct extensive qualitative and
quantitative evaluations on this benchmark to demonstrate the effectiveness of
the proposed approach and analyze the key factors that contribute more to
address this task.
</p>
<a href="http://arxiv.org/abs/2008.00923" target="_blank">arXiv:2008.00923</a> [<a href="http://arxiv.org/pdf/2008.00923" target="_blank">pdf</a>]

<h2>Multiple instance learning on deep features for weakly supervised object detection with extreme domain shifts. (arXiv:2008.01178v4 [cs.CV] UPDATED)</h2>
<h3>Nicolas Gonthier, Sa&#xef;d Ladjal, Yann Gousseau</h3>
<p>Weakly supervised object detection (WSOD) using only image-level annotations
has attracted a growing attention over the past few years. Whereas such task is
typically addressed with a domain-specific solution focused on natural images,
we show that a simple multiple instance approach applied on pre-trained deep
features yields excellent performances on non-photographic datasets, possibly
including new classes. The approach does not include any fine-tuning or
cross-domain learning and is therefore efficient and possibly applicable to
arbitrary datasets and classes. We investigate several flavors of the proposed
approach, some including multi-layers perceptron and polyhedral classifiers.
Despite its simplicity, our method shows competitive results on a range of
publicly available datasets, including paintings (People-Art, IconArt),
watercolors, cliparts and comics and allows to quickly learn unseen visual
categories.
</p>
<a href="http://arxiv.org/abs/2008.01178" target="_blank">arXiv:2008.01178</a> [<a href="http://arxiv.org/pdf/2008.01178" target="_blank">pdf</a>]

<h2>Fast Adaptive Task Offloading in Edge Computing based on Meta Reinforcement Learning. (arXiv:2008.02033v5 [cs.DC] UPDATED)</h2>
<h3>Jin Wang, Jia Hu, Geyong Min, Albert Y. Zomaya, Nektarios Georgalas</h3>
<p>Multi-access edge computing (MEC) aims to extend cloud service to the network
edge to reduce network traffic and service latency. A fundamental problem in
MEC is how to efficiently offload heterogeneous tasks of mobile applications
from user equipment (UE) to MEC hosts. Recently, many deep reinforcement
learning (DRL) based methods have been proposed to learn offloading policies
through interacting with the MEC environment that consists of UE, wireless
channels, and MEC hosts. However, these methods have weak adaptability to new
environments because they have low sample efficiency and need full retraining
to learn updated policies for new environments. To overcome this weakness, we
propose a task offloading method based on meta reinforcement learning, which
can adapt fast to new environments with a small number of gradient updates and
samples. We model mobile applications as Directed Acyclic Graphs (DAGs) and the
offloading policy by a custom sequence-to-sequence (seq2seq) neural network. To
efficiently train the seq2seq network, we propose a method that synergizes the
first order approximation and clipped surrogate objective. The experimental
results demonstrate that this new offloading method can reduce the latency by
up to 25% compared to three baselines while being able to adapt fast to new
environments.
</p>
<a href="http://arxiv.org/abs/2008.02033" target="_blank">arXiv:2008.02033</a> [<a href="http://arxiv.org/pdf/2008.02033" target="_blank">pdf</a>]

<h2>Spatiotemporal Attention for Multivariate Time Series Prediction and Interpretation. (arXiv:2008.04882v2 [cs.LG] UPDATED)</h2>
<h3>Tryambak Gangopadhyay, Sin Yong Tan, Zhanhong Jiang, Rui Meng, Soumik Sarkar</h3>
<p>Multivariate time series modeling and prediction problems are abundant in
many machine learning application domains. Accurate interpretation of such
prediction outcomes from a machine learning model that explicitly captures
temporal correlations can significantly benefit the domain experts. In this
context, temporal attention has been successfully applied to isolate the
important time steps for the input time series. However, in multivariate time
series problems, spatial interpretation is also critical to understand the
contributions of different variables on the model outputs. We propose a novel
deep learning architecture, called spatiotemporal attention mechanism (STAM)
for simultaneous learning of the most important time steps and variables. STAM
is a causal (i.e., only depends on past inputs and does not use future inputs)
and scalable (i.e., scales well with an increase in the number of variables)
approach that is comparable to the state-of-the-art models in terms of
computational tractability. We demonstrate our models' performance on two
popular public datasets and a domain-specific dataset. When compared with the
baseline models, the results show that STAM maintains state-of-the-art
prediction accuracy while offering the benefit of accurate spatiotemporal
interpretability. The learned attention weights are validated from a domain
knowledge perspective for these real-world datasets.
</p>
<a href="http://arxiv.org/abs/2008.04882" target="_blank">arXiv:2008.04882</a> [<a href="http://arxiv.org/pdf/2008.04882" target="_blank">pdf</a>]

<h2>Quaternion Graph Neural Networks. (arXiv:2008.05089v2 [cs.LG] UPDATED)</h2>
<h3>Dai Quoc Nguyen, Tu Dinh Nguyen, Dinh Phung</h3>
<p>Recently, graph neural networks (GNNs) become a principal research direction
to learn low-dimensional continuous embeddings of nodes and graphs to predict
node and graph labels, respectively. However, Euclidean embeddings have high
distortion when using GNNs to model complex graphs such as social networks.
Furthermore, existing GNNs are not very efficient with the high number of model
parameters when increasing the number of hidden layers. Therefore, we move
beyond the Euclidean space to a hyper-complex vector space to improve graph
representation quality and reduce the number of model parameters. To this end,
we propose quaternion graph neural networks (QGNN) to generalize GCNs within
the Quaternion space to learn quaternion embeddings for nodes and graphs. The
Quaternion space, a hyper-complex vector space, provides highly meaningful
computations through Hamilton product compared to the Euclidean and complex
vector spaces. As a result, our QGNN can reduce the model size up to four times
and enhance learning better graph representations. Experimental results show
that the proposed QGNN produces state-of-the-art accuracies on a range of
well-known benchmark datasets for three downstream tasks, including graph
classification, semi-supervised node classification, and text (node)
classification.
</p>
<a href="http://arxiv.org/abs/2008.05089" target="_blank">arXiv:2008.05089</a> [<a href="http://arxiv.org/pdf/2008.05089" target="_blank">pdf</a>]

<h2>Deep Phase Correlation for End-to-End Heterogeneous Sensor Measurements Matching. (arXiv:2008.09474v3 [cs.CV] UPDATED)</h2>
<h3>Zexi Chen, Xuecheng Xu, Yue Wang, Rong Xiong</h3>
<p>The crucial step for localization is to match the current observation to the
map. When the two sensor modalities are significantly different, matching
becomes challenging. In this paper, we present an end-to-end deep phase
correlation network (DPCN) to match heterogeneous sensor measurements. In DPCN,
the primary component is a differentiable correlation-based estimator that
back-propagates the pose error to learnable feature extractors, which addresses
the problem that there are no direct common features for supervision. Also, it
eliminates the exhaustive evaluation in some previous methods, improving
efficiency. With the interpretable modeling, the network is light-weighted and
promising for better generalization. We evaluate the system on both the
simulation data and Aero-Ground Dataset which consists of heterogeneous sensor
images and aerial images acquired by satellites or aerial robots. The results
show that our method is able to match the heterogeneous sensor measurements,
outperforming the comparative traditional phase correlation and other
learning-based methods.
</p>
<a href="http://arxiv.org/abs/2008.09474" target="_blank">arXiv:2008.09474</a> [<a href="http://arxiv.org/pdf/2008.09474" target="_blank">pdf</a>]

<h2>Who ya gonna call? (Alerting Authorities): Measuring Namespaces, Web Certificates, and DNSSEC. (arXiv:2008.10497v2 [cs.CR] UPDATED)</h2>
<h3>Pouyan Fotouhi Tehrani, Eric Osterweil, Jochen H. Schiller, Thomas C. Schmidt, Matthias W&#xe4;hlisch</h3>
<p>During disasters, crisis, and emergencies the public relies on online
services provided by official authorities to receive timely alerts, trustworthy
information, and access to relief programs. It is therefore crucial for the
authorities to reduce risks when accessing their online services. This includes
catering to secure identification of service, secure resolution of name to
network service, and content security and privacy as a minimum base for
trustworthy communication.

In this paper, we take a first look at Alerting Authorities (AA) in the US
and investigate security measures related to trustworthy and secure
communication. We study the domain namespace structure, DNSSEC penetration, and
web certificates. We introduce an integrative threat model to better understand
whether and how the online presence and services of AAs are harmed. As an
illustrative example, we investigate 1,388 Alerting Authorities, backed by the
United States Federal Emergency Management Agency (US FEMA). We observe partial
heightened security relative to the global Internet trends, yet find cause for
concern as about 80% of service providers fail to deploy measures of
trustworthy service provision. Our analysis shows two major shortcomings: About
50% of organizations do not own their dedicated domain names and are dependent
on others, 55% opt for unrestricted-use namespaces, which simplifies phishing,
and less than 0.4% of unique AA domain names are secured by DNSSEC, which can
lead to DNS poisoning and possibly to certificate misissuance. Furthermore, 15%
of all hosts provide none or invalid certificates, thus cannot cater to
confidentiality and data integrity, 64% of the hosts provide domain validation
certificates that lack any identity information, and shared certificates have
gained on popularity, which leads to fate-sharing and can be a cause for
instability.
</p>
<a href="http://arxiv.org/abs/2008.10497" target="_blank">arXiv:2008.10497</a> [<a href="http://arxiv.org/pdf/2008.10497" target="_blank">pdf</a>]

<h2>LMSCNet: Lightweight Multiscale 3D Semantic Completion. (arXiv:2008.10559v2 [cs.CV] UPDATED)</h2>
<h3>Luis Rold&#xe3;o, Raoul de Charette, Anne Verroust-Blondet</h3>
<p>We introduce a new approach for multiscale 3Dsemantic scene completion from
voxelized sparse 3D LiDAR scans. As opposed to the literature, we use a 2D UNet
backbone with comprehensive multiscale skip connections to enhance feature
flow, along with 3D segmentation heads. On the SemanticKITTI benchmark, our
method performs on par on semantic completion and better on occupancy
completion than all other published methods -- while being significantly
lighter and faster. As such it provides a great performance/speed trade-off for
mobile-robotics applications. The ablation studies demonstrate our method is
robust to lower density inputs, and that it enables very high speed semantic
completion at the coarsest level. Our code is available at
https://github.com/cv-rits/LMSCNet.
</p>
<a href="http://arxiv.org/abs/2008.10559" target="_blank">arXiv:2008.10559</a> [<a href="http://arxiv.org/pdf/2008.10559" target="_blank">pdf</a>]

<h2>Interactive Annotation of 3D Object Geometry using 2D Scribbles. (arXiv:2008.10719v2 [cs.CV] UPDATED)</h2>
<h3>Tianchang Shen, Jun Gao, Amlan Kar, Sanja Fidler</h3>
<p>Inferring detailed 3D geometry of the scene is crucial for robotics
applications, simulation, and 3D content creation. However, such information is
hard to obtain, and thus very few datasets support it. In this paper, we
propose an interactive framework for annotating 3D object geometry from both
point cloud data and RGB imagery. The key idea behind our approach is to
exploit strong priors that humans have about the 3D world in order to
interactively annotate complete 3D shapes. Our framework targets naive users
without artistic or graphics expertise. We introduce two simple-to-use
interaction modules. First, we make an automatic guess of the 3D shape and
allow the user to provide feedback about large errors by drawing scribbles in
desired 2D views. Next, we aim to correct minor errors, in which users drag and
drop mesh vertices, assisted by a neural interactive module implemented as a
Graph Convolutional Network. Experimentally, we show that only a few user
interactions are needed to produce good quality 3D shapes on popular benchmarks
such as ShapeNet, Pix3D and ScanNet. We implement our framework as a web
service and conduct a user study, where we show that user annotated data using
our method effectively facilitates real-world learning tasks. Web service:
this http URL
</p>
<a href="http://arxiv.org/abs/2008.10719" target="_blank">arXiv:2008.10719</a> [<a href="http://arxiv.org/pdf/2008.10719" target="_blank">pdf</a>]

<h2>Estimating Example Difficulty Using Variance of Gradients. (arXiv:2008.11600v2 [cs.CV] UPDATED)</h2>
<h3>Chirag Agarwal, Sara Hooker</h3>
<p>In machine learning, a question of great interest is understanding what
examples are challenging for a model to classify. Identifying atypical examples
helps inform safe deployment of models, isolates examples that require further
human inspection, and provides interpretability into model behavior. In this
work, we propose Variance of Gradients ($VOG$) as a valuable and efficient
proxy metric for detecting outliers in the data distribution. We provide
quantitative and qualitative support that $VOG$ is a meaningful way to rank
data by difficulty and to surface a tractable subset of the most challenging
examples for human-in-the-loop auditing. Data points with high $VOG$ scores are
far more difficult for the model to learn and over-index on corrupted or
memorized examples.
</p>
<a href="http://arxiv.org/abs/2008.11600" target="_blank">arXiv:2008.11600</a> [<a href="http://arxiv.org/pdf/2008.11600" target="_blank">pdf</a>]

<h2>Adversarially Training for Audio Classifiers. (arXiv:2008.11618v2 [eess.AS] UPDATED)</h2>
<h3>Raymel Alfonso Sallo, Mohammad Esmaeilpour, Patrick Cardinal</h3>
<p>In this paper, we investigate the potential effect of the adversarially
training on the robustness of six advanced deep neural networks against a
variety of targeted and non-targeted adversarial attacks. We firstly show that,
the ResNet-56 model trained on the 2D representation of the discrete wavelet
transform appended with the tonnetz chromagram outperforms other models in
terms of recognition accuracy. Then we demonstrate the positive impact of
adversarially training on this model as well as other deep architectures
against six types of attack algorithms (white and black-box) with the cost of
the reduced recognition accuracy and limited adversarial perturbation. We run
our experiments on two benchmarking environmental sound datasets and show that
without any imposed limitations on the budget allocations for the adversary,
the fooling rate of the adversarially trained models can exceed 90\%. In other
words, adversarial attacks exist in any scales, but they might require higher
adversarial perturbations compared to non-adversarially trained models.
</p>
<a href="http://arxiv.org/abs/2008.11618" target="_blank">arXiv:2008.11618</a> [<a href="http://arxiv.org/pdf/2008.11618" target="_blank">pdf</a>]

<h2>Deep Reinforcement Learning for Contact-Rich Skills Using Compliant Movement Primitives. (arXiv:2008.13223v2 [cs.RO] UPDATED)</h2>
<h3>Oren Spector, Miriam Zacksenhouse</h3>
<p>In recent years, industrial robots have been installed in various industries
to handle advanced manufacturing and high precision tasks. However, further
integration of industrial robots is hampered by their limited flexibility,
adaptability and decision making skills compared to human operators. Assembly
tasks are especially challenging for robots since they are contact-rich and
sensitive to even small uncertainties. While reinforcement learning (RL) offers
a promising framework to learn contact-rich control policies from scratch, its
applicability to high-dimensional continuous state-action spaces remains rather
limited due to high brittleness and sample complexity. To address those issues,
we propose different pruning methods that facilitate convergence and
generalization. In particular, we divide the task into free and contact-rich
sub-tasks, perform the control in Cartesian rather than joint space, and
parameterize the control policy. Those pruning methods are naturally
implemented within the framework of dynamic movement primitives (DMP). To
handle contact-rich tasks, we extend the DMP framework by introducing a
coupling term that acts like the human wrist and provides active compliance
under contact with the environment. We demonstrate that the proposed method can
learn insertion skills that are invariant to space, size, shape, and closely
related scenarios, while handling large uncertainties. Finally we demonstrate
that the learned policy can be easily transferred from simulations to real
world and achieve similar performance on UR5e robot.
</p>
<a href="http://arxiv.org/abs/2008.13223" target="_blank">arXiv:2008.13223</a> [<a href="http://arxiv.org/pdf/2008.13223" target="_blank">pdf</a>]

<h2>Distance Encoding: Design Provably More Powerful Neural Networks for Graph Representation Learning. (arXiv:2009.00142v3 [cs.LG] UPDATED)</h2>
<h3>Pan Li, Yanbang Wang, Hongwei Wang, Jure Leskovec</h3>
<p>Learning representations of sets of nodes in a graph is crucial for
applications ranging from node-role discovery to link prediction and molecule
classification. Graph Neural Networks (GNNs) have achieved great success in
graph representation learning. However, expressive power of GNNs is limited by
the 1-Weisfeiler-Lehman (WL) test and thus GNNs generate identical
representations for graph substructures that may in fact be very different.
More powerful GNNs, proposed recently by mimicking higher-order-WL tests, only
focus on representing entire graphs and they are computationally inefficient as
they cannot utilize sparsity of the underlying graph. Here we propose and
mathematically analyze a general class of structure-related features, termed
Distance Encoding (DE). DE assists GNNs in representing any set of nodes, while
providing strictly more expressive power than the 1-WL test. DE captures the
distance between the node set whose representation is to be learned and each
node in the graph. To capture the distance DE can apply various graph-distance
measures such as shortest path distance or generalized PageRank scores. We
propose two ways for GNNs to use DEs (1) as extra node features, and (2) as
controllers of message aggregation in GNNs. Both approaches can utilize the
sparse structure of the underlying graph, which leads to computational
efficiency and scalability. We also prove that DE can distinguish node sets
embedded in almost all regular graphs where traditional GNNs always fail. We
evaluate DE on three tasks over six real networks: structural role prediction,
link prediction, and triangle prediction. Results show that our models
outperform GNNs without DE by up-to 15\% in accuracy and AUROC. Furthermore,
our models also significantly outperform other state-of-the-art methods
especially designed for the above tasks.
</p>
<a href="http://arxiv.org/abs/2009.00142" target="_blank">arXiv:2009.00142</a> [<a href="http://arxiv.org/pdf/2009.00142" target="_blank">pdf</a>]

<h2>Learning explanations that are hard to vary. (arXiv:2009.00329v3 [cs.LG] UPDATED)</h2>
<h3>Giambattista Parascandolo, Alexander Neitz, Antonio Orvieto, Luigi Gresele, Bernhard Sch&#xf6;lkopf</h3>
<p>In this paper, we investigate the principle that `good explanations are hard
to vary' in the context of deep learning. We show that averaging gradients
across examples -- akin to a logical OR of patterns -- can favor memorization
and `patchwork' solutions that sew together different strategies, instead of
identifying invariances. To inspect this, we first formalize a notion of
consistency for minima of the loss surface, which measures to what extent a
minimum appears only when examples are pooled. We then propose and
experimentally validate a simple alternative algorithm based on a logical AND,
that focuses on invariances and prevents memorization in a set of real-world
tasks. Finally, using a synthetic dataset with a clear distinction between
invariant and spurious mechanisms, we dissect learning signals and compare this
approach to well-established regularizers.
</p>
<a href="http://arxiv.org/abs/2009.00329" target="_blank">arXiv:2009.00329</a> [<a href="http://arxiv.org/pdf/2009.00329" target="_blank">pdf</a>]

<h2>GazeMAE: General Representations of Eye Movements using a Micro-Macro Autoencoder. (arXiv:2009.02437v2 [cs.CV] UPDATED)</h2>
<h3>Louise Gillian C. Bautista, Prospero C. Naval Jr</h3>
<p>Eye movements are intricate and dynamic events that contain a wealth of
information about the subject and the stimuli. We propose an abstract
representation of eye movements that preserve the important nuances in gaze
behavior while being stimuli-agnostic. We consider eye movements as raw
position and velocity signals and train separate deep temporal convolutional
autoencoders. The autoencoders learn micro-scale and macro-scale
representations that correspond to the fast and slow features of eye movements.
We evaluate the joint representations with a linear classifier fitted on
various classification tasks. Our work accurately discriminates between gender
and age groups, and outperforms previous works on biometrics and stimuli
clasification. Further experiments highlight the validity and generalizability
of this method, bringing eye tracking research closer to real-world
applications.
</p>
<a href="http://arxiv.org/abs/2009.02437" target="_blank">arXiv:2009.02437</a> [<a href="http://arxiv.org/pdf/2009.02437" target="_blank">pdf</a>]

<h2>not-so-BigGAN: Generating High-Fidelity Images on Small Compute with Wavelet-based Super-Resolution. (arXiv:2009.04433v2 [eess.IV] UPDATED)</h2>
<h3>Seungwook Han, Akash Srivastava, Cole Hurwitz, Prasanna Sattigeri, David D. Cox</h3>
<p>State-of-the-art models for high-resolution image generation, such as BigGAN
and VQVAE-2, require an incredible amount of compute resources and/or time (512
TPU-v3 cores) to train, putting them out of reach for the larger research
community. On the other hand, GAN-based image super-resolution models, such as
ESRGAN, can not only upscale images to high dimensions, but also are efficient
to train. In this paper, we present not-so-big-GAN (nsb-GAN), a simple yet
cost-effective two-step training framework for deep generative models (DGMs) of
high-dimensional natural images. First, we generate images in low-frequency
bands by training a sampler in the wavelet domain. Then, we super-resolve these
images from the wavelet domain back to the pixel-space with our novel wavelet
super-resolution decoder network. Wavelet-based down-sampling method preserves
more structural information than pixel-based methods, leading to significantly
better generative quality of the low-resolution sampler (e.g., 64x64). Since
the sampler and decoder can be trained in parallel and operate on much lower
dimensional spaces than end-to-end models, the training cost is substantially
reduced. On ImageNet 512x512, our model achieves a Fr\'echet Inception Distance
(FID) of 10.59 -- beating the baseline BigGAN model -- at half the compute (256
TPU-v3 cores).
</p>
<a href="http://arxiv.org/abs/2009.04433" target="_blank">arXiv:2009.04433</a> [<a href="http://arxiv.org/pdf/2009.04433" target="_blank">pdf</a>]

<h2>Differentially Private Language Models Benefit from Public Pre-training. (arXiv:2009.05886v2 [cs.LG] UPDATED)</h2>
<h3>Gavin Kerrigan, Dylan Slack, Jens Tuyls</h3>
<p>Language modeling is a keystone task in natural language processing. When
training a language model on sensitive information, differential privacy (DP)
allows us to quantify the degree to which our private data is protected.
However, training algorithms which enforce differential privacy often lead to
degradation in model quality. We study the feasibility of learning a language
model which is simultaneously high-quality and privacy preserving by tuning a
public base model on a private corpus. We find that DP fine-tuning boosts the
performance of language models in the private domain, making the training of
such models possible.
</p>
<a href="http://arxiv.org/abs/2009.05886" target="_blank">arXiv:2009.05886</a> [<a href="http://arxiv.org/pdf/2009.05886" target="_blank">pdf</a>]

<h2>EasyASR: A Distributed Machine Learning Platform for End-to-end Automatic Speech Recognition. (arXiv:2009.06487v2 [cs.CL] UPDATED)</h2>
<h3>Chengyu Wang, Mengli Cheng, Xu Hu, Jun Huang</h3>
<p>We present EasyASR, a distributed machine learning platform for training and
serving large-scale Automatic Speech Recognition (ASR) models, as well as
collecting and processing audio data at scale. Our platform is built upon the
Machine Learning Platform for AI of Alibaba Cloud. Its main functionality is to
support efficient learning and inference for end-to-end ASR models on
distributed GPU clusters. It allows users to learn ASR models with either
pre-defined or user-customized network architectures via simple user interface.
On EasyASR, we have produced state-of-the-art results over several public
datasets for Mandarin speech recognition.
</p>
<a href="http://arxiv.org/abs/2009.06487" target="_blank">arXiv:2009.06487</a> [<a href="http://arxiv.org/pdf/2009.06487" target="_blank">pdf</a>]

<h2>Domain-invariant Similarity Activation Map Metric Learning for Retrieval-based Long-term Visual Localization. (arXiv:2009.07719v2 [cs.CV] UPDATED)</h2>
<h3>Hanjiang Hu, Zhijian Qiao, Hesheng Wang, Zhe Liu, Weidong Chen</h3>
<p>Visual localization is a crucial component in the application of mobile robot
and autonomous driving. Image retrieval is an efficient and effective technique
in image-based localization methods. Due to the drastic variability of
environmental conditions, e.g. illumination, seasonal and weather changes,
retrieval-based visual localization is severely affected and becomes a
challenging problem. In this work, a general architecture is first formulated
probabilistically to extract domain-invariant feature through multi-domain
image translation. And then a novel gradient-weighted similarity activation
mapping loss (Grad-SAM) is incorporated for finer localization with high
accuracy. We also propose a new adaptive triplet loss to boost the metric
learning of the embedding in a self-supervised manner. The final coarse-to-fine
image retrieval pipeline is implemented as the sequential combination of models
without and with Grad-SAM loss. Extensive experiments have been conducted to
validate the effectiveness of the proposed approach on the CMU-Seasons dataset.
The strong generalization ability of our approach is verified on RobotCar
dataset using models pre-trained on urban part of CMU-Seasons dataset. Our
performance is on par with or even outperforms the state-of-the-art image-based
localization baselines in medium or high precision, especially under the
challenging environments with illumination variance, vegetation and night-time
images. Moreover, real-site experiments have been conducted to validate the
efficiency and effectiveness of the coarse-to-fine strategy for localization.
</p>
<a href="http://arxiv.org/abs/2009.07719" target="_blank">arXiv:2009.07719</a> [<a href="http://arxiv.org/pdf/2009.07719" target="_blank">pdf</a>]

<h2>Online Semi-Supervised Learning in Contextual Bandits with Episodic Reward. (arXiv:2009.08457v2 [cs.LG] UPDATED)</h2>
<h3>Baihan Lin</h3>
<p>We considered a novel practical problem of online learning with episodically
revealed rewards, motivated by several real-world applications, where the
contexts are nonstationary over different episodes and the reward feedbacks are
not always available to the decision making agents. For this online
semi-supervised learning setting, we introduced Background Episodic Reward
LinUCB (BerlinUCB), a solution that easily incorporates clustering as a
self-supervision module to provide useful side information when rewards are not
observed. Our experiments on a variety of datasets, both in stationary and
nonstationary environments of six different scenarios, demonstrated clear
advantages of the proposed approach over the standard contextual bandit.
Lastly, we introduced a relevant real-life example where this problem setting
is especially useful.
</p>
<a href="http://arxiv.org/abs/2009.08457" target="_blank">arXiv:2009.08457</a> [<a href="http://arxiv.org/pdf/2009.08457" target="_blank">pdf</a>]

<h2>TODS: An Automated Time Series Outlier Detection System. (arXiv:2009.09822v2 [cs.DB] UPDATED)</h2>
<h3>Kwei-Herng Lai, Daochen Zha, Guanchu Wang, Junjie Xu, Yue Zhao, Devesh Kumar, Yile Chen, Purav Zumkhawaka, Minyang Wan, Diego Martinez, Xia Hu</h3>
<p>We present TODS, an automated Time Series Outlier Detection System for
research and industrial applications. TODS is a highly modular system that
supports easy pipeline construction. The basic building block of TODS is
primitive, which is an implementation of a function with hyperparameters. TODS
currently supports 70 primitives, including data processing, time series
processing, feature analysis, detection algorithms, and a reinforcement module.
Users can freely construct a pipeline using these primitives and perform end-
to-end outlier detection with the constructed pipeline. TODS provides a
Graphical User Interface (GUI), where users can flexibly design a pipeline with
drag-and-drop. Moreover, a data-driven searcher is provided to automatically
discover the most suitable pipelines given a dataset. TODS is released under
Apache 2.0 license at https://github.com/datamllab/tods.
</p>
<a href="http://arxiv.org/abs/2009.09822" target="_blank">arXiv:2009.09822</a> [<a href="http://arxiv.org/pdf/2009.09822" target="_blank">pdf</a>]

<h2>Dynamic Fusion based Federated Learning for COVID-19 Detection. (arXiv:2009.10401v4 [cs.DC] UPDATED)</h2>
<h3>Weishan Zhang, Tao Zhou, Qinghua Lu, Xiao Wang, Chunsheng Zhu, Haoyun Sun, Zhipeng Wang, Sin Kit Lo, Fei-Yue Wang</h3>
<p>Medical diagnostic image analysis (e.g., CT scan or X-Ray) using machine
learning is an efficient and accurate way to detect COVID-19 infections.
However, sharing diagnostic images across medical institutions is usually not
allowed due to the concern of patients' privacy. This causes the issue of
insufficient datasets for training the image classification model. Federated
learning is an emerging privacy-preserving machine learning paradigm that
produces an unbiased global model based on the received updates of local models
trained by clients without exchanging clients' local data. Nevertheless, the
default setting of federated learning introduces huge communication cost of
transferring model updates and can hardly ensure model performance when data
heterogeneity of clients heavily exists. To improve communication efficiency
and model performance, in this paper, we propose a novel dynamic fusion-based
federated learning approach for medical diagnostic image analysis to detect
COVID-19 infections. First, we design an architecture for dynamic fusion-based
federated learning systems to analyse medical diagnostic images. Further, we
present a dynamic fusion method to dynamically decide the participating clients
according to their local model performance and schedule the model fusion-based
on participating clients' training time. In addition, we summarise a category
of medical diagnostic image datasets for COVID-19 detection, which can be used
by the machine learning community for image analysis. The evaluation results
show that the proposed approach is feasible and performs better than the
default setting of federated learning in terms of model performance,
communication efficiency and fault tolerance.
</p>
<a href="http://arxiv.org/abs/2009.10401" target="_blank">arXiv:2009.10401</a> [<a href="http://arxiv.org/pdf/2009.10401" target="_blank">pdf</a>]

<h2>Dual Encoder Fusion U-Net (DEFU-Net) for Cross-manufacturer Chest X-ray Segmentation. (arXiv:2009.10608v3 [eess.IV] UPDATED)</h2>
<h3>Lipei Zhang, Aozhi Liu, Jing Xiao, Paul Taylor</h3>
<p>A number of methods based on deep learning have been applied to medical image
segmentation and have achieved state-of-the-art performance. Due to the
importance of chest x-ray data in studying COVID-19, there is a demand for
state-of-the-art models capable of precisely segmenting soft tissue on the
chest x-rays. The dataset for exploring best segmentation model is from
Montgomery and Shenzhen hospital which had opened in 2014. The most famous
technique is U-Net which has been used to many medical datasets including the
Chest X-rays. However, most variant U-Nets mainly focus on extraction of
contextual information and skip connections. There is still a large space for
improving extraction of spatial features. In this paper, we propose a dual
encoder fusion U-Net framework for Chest X-rays based on Inception
Convolutional Neural Network with dilation, Densely Connected Recurrent
Convolutional Neural Network, which is named DEFU-Net. The densely connected
recurrent path extends the network deeper for facilitating contextual feature
extraction. In order to increase the width of network and enrich representation
of features, the inception blocks with dilation are adopted. The inception
blocks can capture globally and locally spatial information from various
receptive fields. At the same time, the two paths are fused by summing
features, thus preserving the contextual and spatial information for decoding
part. This multi-learning-scale model is benefiting in Chest X-ray dataset from
two different manufacturers (Montgomery and Shenzhen hospital). The DEFU-Net
achieves the better performance than basic U-Net, residual U-Net, BCDU-Net,
R2U-Net and attention R2U-Net. This model has proved the feasibility for mixed
dataset and approaches state-of-the-art. The source code for this proposed
framework is public https://github.com/uceclz0/DEFU-Net.
</p>
<a href="http://arxiv.org/abs/2009.10608" target="_blank">arXiv:2009.10608</a> [<a href="http://arxiv.org/pdf/2009.10608" target="_blank">pdf</a>]

<h2>Machine-learning physics from unphysics: Finding deconfinement temperature in lattice Yang-Mills theories from outside the scaling window. (arXiv:2009.10971v2 [hep-lat] UPDATED)</h2>
<h3>D.L. Boyda, M.N. Chernodub, N.V. Gerasimeniuk, V.A. Goy, S.D. Liubimov, A.V. Molochkov</h3>
<p>We study the machine learning techniques applied to the lattice gauge
theory's critical behavior, particularly to the confinement/deconfinement phase
transition in the SU(2) and SU(3) gauge theories. We find that the neural
network, trained on lattice configurations of gauge fields at an unphysical
value of the lattice parameters as an input, builds up a gauge-invariant
function, and finds correlations with the target observable that is valid in
the physical region of the parameter space. In particular, if the algorithm
aimed to predict the Polyakov loop as the deconfining order parameter, it
builds a trace of the gauge group matrices along a closed loop in the time
direction. As a result, the neural network, trained at one unphysical value of
the lattice coupling $\beta$ predicts the order parameter in the whole region
of the $\beta$ values with good precision. We thus demonstrate that the machine
learning techniques may be used as a numerical analog of the analytical
continuation from easily accessible but physically uninteresting regions of the
coupling space to the interesting but potentially not accessible regions.
</p>
<a href="http://arxiv.org/abs/2009.10971" target="_blank">arXiv:2009.10971</a> [<a href="http://arxiv.org/pdf/2009.10971" target="_blank">pdf</a>]

<h2>Neurosymbolic Reinforcement Learning with Formally Verified Exploration. (arXiv:2009.12612v2 [cs.LG] UPDATED)</h2>
<h3>Greg Anderson, Abhinav Verma, Isil Dillig, Swarat Chaudhuri</h3>
<p>We present Revel, a partially neural reinforcement learning (RL) framework
for provably safe exploration in continuous state and action spaces. A key
challenge for provably safe deep RL is that repeatedly verifying neural
networks within a learning loop is computationally infeasible. We address this
challenge using two policy classes: a general, neurosymbolic class with
approximate gradients and a more restricted class of symbolic policies that
allows efficient verification. Our learning algorithm is a mirror descent over
policies: in each iteration, it safely lifts a symbolic policy into the
neurosymbolic space, performs safe gradient updates to the resulting policy,
and projects the updated policy into the safe symbolic subset, all without
requiring explicit verification of neural networks. Our empirical results show
that Revel enforces safe exploration in many scenarios in which Constrained
Policy Optimization does not, and that it can discover policies that outperform
those learned through prior approaches to verified exploration.
</p>
<a href="http://arxiv.org/abs/2009.12612" target="_blank">arXiv:2009.12612</a> [<a href="http://arxiv.org/pdf/2009.12612" target="_blank">pdf</a>]

<h2>Learning to Improve Image Compression without Changing the Standard Decoder. (arXiv:2009.12927v3 [eess.IV] UPDATED)</h2>
<h3>Yannick Str&#xfc;mpler, Ren Yang, Radu Timofte</h3>
<p>In recent years we have witnessed an increasing interest in applying Deep
Neural Networks (DNNs) to improve the rate-distortion performance in image
compression. However, the existing approaches either train a post-processing
DNN on the decoder side, or propose learning for image compression in an
end-to-end manner. This way, the trained DNNs are required in the decoder,
leading to the incompatibility to the standard image decoders (e.g., JPEG) in
personal computers and mobiles. Therefore, we propose learning to improve the
encoding performance with the standard decoder. In this paper, We work on JPEG
as an example. Specifically, a frequency-domain pre-editing method is proposed
to optimize the distribution of DCT coefficients, aiming at facilitating the
JPEG compression. Moreover, we propose learning the JPEG quantization table
jointly with the pre-editing network. Most importantly, we do not modify the
JPEG decoder and therefore our approach is applicable when viewing images with
the widely used standard JPEG decoder. The experiments validate that our
approach successfully improves the rate-distortion performance of JPEG in terms
of various quality metrics, such as PSNR, MS-SSIM and LPIPS. Visually, this
translates to better overall color retention especially when strong compression
is applied. The codes are available at
https://github.com/YannickStruempler/LearnedJPEG.
</p>
<a href="http://arxiv.org/abs/2009.12927" target="_blank">arXiv:2009.12927</a> [<a href="http://arxiv.org/pdf/2009.12927" target="_blank">pdf</a>]

<h2>Deep Composer Classification Using Symbolic Representation. (arXiv:2010.00823v2 [cs.SD] UPDATED)</h2>
<h3>Sunghyeon Kim, Hyeyoon Lee, Sunjong Park, Jinho Lee, Keunwoo Choi</h3>
<p>In this study, we train deep neural networks to classify composer on a
symbolic domain. The model takes a two-channel two-dimensional input, i.e.,
onset and note activations of time-pitch representation, which is converted
from MIDI recordings and performs a single-label classification. On the
experiments conducted on MAESTRO dataset, we report an F1 value of 0.8333 for
the classification of 13~classical composers.
</p>
<a href="http://arxiv.org/abs/2010.00823" target="_blank">arXiv:2010.00823</a> [<a href="http://arxiv.org/pdf/2010.00823" target="_blank">pdf</a>]

<h2>TeRo: A Time-aware Knowledge Graph Embedding via Temporal Rotation. (arXiv:2010.01029v2 [cs.CL] UPDATED)</h2>
<h3>Chengjin Xu, Mojtaba Nayyeri, Fouad Alkhoury, Hamed Shariat Yazdi, Jens Lehmann</h3>
<p>In the last few years, there has been a surge of interest in learning
representations of entitiesand relations in knowledge graph (KG). However, the
recent availability of temporal knowledgegraphs (TKGs) that contain time
information for each fact created the need for reasoning overtime in such TKGs.
In this regard, we present a new approach of TKG embedding, TeRo, which defines
the temporal evolution of entity embedding as a rotation from the initial time
to the currenttime in the complex vector space. Specially, for facts involving
time intervals, each relation isrepresented as a pair of dual complex
embeddings to handle the beginning and the end of therelation, respectively. We
show our proposed model overcomes the limitations of the existing KG embedding
models and TKG embedding models and has the ability of learning and
inferringvarious relation patterns over time. Experimental results on four
different TKGs show that TeRo significantly outperforms existing
state-of-the-art models for link prediction. In addition, we analyze the effect
of time granularity on link prediction over TKGs, which as far as we know
hasnot been investigated in previous literature.
</p>
<a href="http://arxiv.org/abs/2010.01029" target="_blank">arXiv:2010.01029</a> [<a href="http://arxiv.org/pdf/2010.01029" target="_blank">pdf</a>]

<h2>Rank Position Forecasting in Car Racing. (arXiv:2010.01707v2 [cs.LG] UPDATED)</h2>
<h3>Bo Peng, Jiayu Li, Selahattin Akkas, Fugang Wang, Takuya Araki, Ohno Yoshiyuki, Judy Qiu</h3>
<p>Forecasting is challenging since uncertainty resulted from exogenous factors
exists. This work investigates the rank position forecasting problem in car
racing, which predicts the rank positions at the future laps for cars. Among
the many factors that bring changes to the rank positions, pit stops are
critical but irregular and rare. We found existing methods, including
statistical models, machine learning regression models, and state-of-the-art
deep forecasting model based on encoder-decoder architecture, all have
limitations in the forecasting. By elaborative analysis of pit stops events, we
propose a deep model, RankNet, with the cause effects decomposition that
modeling the rank position sequence and pit stop events separately. It also
incorporates probabilistic forecasting to model the uncertainty inside each
sub-model. Through extensive experiments, RankNet demonstrates a strong
performance improvement over the baselines, e.g., MAE improves more than 10%
consistently, and is also more stable when adapting to unseen new data. Details
of model optimization, performance profiling are presented. It is promising to
provide useful forecasting tools for the car racing analysis and shine a light
on solutions to similar challenging issues in general forecasting problems.
</p>
<a href="http://arxiv.org/abs/2010.01707" target="_blank">arXiv:2010.01707</a> [<a href="http://arxiv.org/pdf/2010.01707" target="_blank">pdf</a>]

<h2>RANP: Resource Aware Neuron Pruning at Initialization for 3D CNNs. (arXiv:2010.02488v3 [cs.CV] UPDATED)</h2>
<h3>Zhiwei Xu, Thalaiyasingam Ajanthan, Vibhav Vineet, Richard Hartley</h3>
<p>Although 3D Convolutional Neural Networks (CNNs) are essential for most
learning based applications involving dense 3D data, their applicability is
limited due to excessive memory and computational requirements. Compressing
such networks by pruning therefore becomes highly desirable. However, pruning
3D CNNs is largely unexplored possibly because of the complex nature of typical
pruning algorithms that embeds pruning into an iterative optimization paradigm.
In this work, we introduce a Resource Aware Neuron Pruning (RANP) algorithm
that prunes 3D CNNs at initialization to high sparsity levels. Specifically,
the core idea is to obtain an importance score for each neuron based on their
sensitivity to the loss function. This neuron importance is then reweighted
according to the neuron resource consumption related to FLOPs or memory. We
demonstrate the effectiveness of our pruning method on 3D semantic segmentation
with widely used 3D-UNets on ShapeNet and BraTS'18 as well as on video
classification with MobileNetV2 and I3D on UCF101 dataset. In these
experiments, our RANP leads to roughly 50-95 reduction in FLOPs and 35-80
reduction in memory with negligible loss in accuracy compared to the unpruned
networks. This significantly reduces the computational resources required to
train 3D CNNs. The pruned network obtained by our algorithm can also be easily
scaled up and transferred to another dataset for training.
</p>
<a href="http://arxiv.org/abs/2010.02488" target="_blank">arXiv:2010.02488</a> [<a href="http://arxiv.org/pdf/2010.02488" target="_blank">pdf</a>]

<h2>Transcending Transcend: Revisiting Malware Classification with Conformal Evaluation. (arXiv:2010.03856v2 [cs.CR] UPDATED)</h2>
<h3>Federico Barbero, Feargus Pendlebury, Fabio Pierazzi, Lorenzo Cavallaro</h3>
<p>Machine learning for malware classification shows encouraging results, but
real deployments suffer from performance degradation as malware authors adapt
their techniques to evade detection. This phenomenon, known as concept drift,
occurs as new malware examples evolve and become less and less like the
original training examples. One promising method to cope with concept drift is
classification with rejection in which examples that are likely to be
misclassified are instead quarantined until they can be expertly analyzed.

We revisit Transcend, a recently proposed framework for performing rejection
based on conformal prediction theory. In particular, we provide a formal
treatment of Transcend, enabling us to refine conformal evaluation theory---its
underlying statistical engine---and gain a better understanding of the
theoretical reasons for its effectiveness. In the process, we develop two
additional conformal evaluators that match or surpass the performance of the
original while significantly decreasing the computational overhead. We evaluate
our extension on a large dataset that removes sources of experimental bias
present in the original evaluation.

Finally, to aid practitioners, we determine the optimal operational settings
for a Transcend deployment and show how it can be applied to many popular
learning algorithms. These insights support both old and new empirical
findings, making Transcend a sound and practical solution for the first time.
To this end, we release our implementation of Transcend as open source, to aid
the adoption of rejection strategies by the security community.
</p>
<a href="http://arxiv.org/abs/2010.03856" target="_blank">arXiv:2010.03856</a> [<a href="http://arxiv.org/pdf/2010.03856" target="_blank">pdf</a>]

<h2>MIMO ILC for Precision SEA robots using Input-weighted Complex-Kernel Regression. (arXiv:2010.04487v2 [eess.SY] UPDATED)</h2>
<h3>Leon Yan, Nathan Banka, Parker Owan, Walter Tony Piaskowy, Joseph Garbini, Santosh Devasia</h3>
<p>This work improves the positioning precision of lightweight robots with
series elastic actuators (SEAs). Lightweight SEA robots, along with
low-impedance control, can maneuver without causing damage in uncertain,
confined spaces such as inside an aircraft wing during aircraft assembly.
Nevertheless, substantial modeling uncertainties in SEA robots reduce the
precision achieved by model-based approaches such as inversion-based
feedforward. Therefore, this article improves the precision of SEA robots
around specified operating points, through a multi-input multi-output (MIMO),
iterative learning control (ILC) approach. The main contributions of this
article are to (i) introduce an input-weighted complex kernel to estimate local
MIMO models using complex Gaussian process regression (c-GPR) (ii) develop
Ger\v{s}gorin-theorem-based conditions on the iteration gains for ensuring ILC
convergence to precision within noise-related limits, even with errors in the
estimated model; and (iii) demonstrate precision positioning with an
experimental SEA robot. Comparative experimental results, with and without ILC,
show around 90% improvement in the positioning precision (close to the
repeatability limit of the robot) and a 10-times increase in the SEA robot's
operating speed with the use of the MIMO ILC.
</p>
<a href="http://arxiv.org/abs/2010.04487" target="_blank">arXiv:2010.04487</a> [<a href="http://arxiv.org/pdf/2010.04487" target="_blank">pdf</a>]

<h2>The emergence of Explainability of Intelligent Systems: Delivering Explainable and Personalised Recommendations for Energy Efficiency. (arXiv:2010.04990v2 [cs.AI] UPDATED)</h2>
<h3>Christos Sardianos, Iraklis Varlamis, Christos Chronis, George Dimitrakopoulos, Abdullah Alsalemi, Yassine Himeur, Faycal Bensaali, Abbes Amira</h3>
<p>The recent advances in artificial intelligence namely in machine learning and
deep learning, have boosted the performance of intelligent systems in several
ways. This gave rise to human expectations, but also created the need for a
deeper understanding of how intelligent systems think and decide. The concept
of explainability appeared, in the extent of explaining the internal system
mechanics in human terms. Recommendation systems are intelligent systems that
support human decision making, and as such, they have to be explainable in
order to increase user trust and improve the acceptance of recommendations. In
this work, we focus on a context-aware recommendation system for energy
efficiency and develop a mechanism for explainable and persuasive
recommendations, which are personalized to user preferences and habits. The
persuasive facts either emphasize on the economical saving prospects (Econ) or
on a positive ecological impact (Eco) and explanations provide the reason for
recommending an energy saving action. Based on a study conducted using a
Telegram bot, different scenarios have been validated with actual data and
human feedback. Current results show a total increase of 19\% on the
recommendation acceptance ratio when both economical and ecological persuasive
facts are employed. This revolutionary approach on recommendation systems,
demonstrates how intelligent recommendations can effectively encourage energy
saving behavior.
</p>
<a href="http://arxiv.org/abs/2010.04990" target="_blank">arXiv:2010.04990</a> [<a href="http://arxiv.org/pdf/2010.04990" target="_blank">pdf</a>]

<h2>Total heat flux convergence in the calculation of 2d and 3d heat losses through building elements. (arXiv:2010.05207v2 [cs.CE] UPDATED)</h2>
<h3>Sanjin Gumbarevi&#x107;, Bojan Milovanovi&#x107;, Mergim Ga&#x161;i, Marina Bagari&#x107;</h3>
<p>Heat losses through the building envelope is one of the key factors in the
calculation of the building energy balance. If steady-state heat conduction is
observed, which is commonly used to assess the heat losses in building, there
is an analytical solution for one-dimensional problem. For two and
three-dimensional problems, especially for the complex geometry cases, one must
use numerical methods to solve the heat conduction equation. To standardise two
and three-dimensional calculation of heat losses through building elements, ISO
10211 standard can be used. The standard has four benchmark examples with
criteria that must be satisfied to declare a method as a high-precision
calculation method. A problem occurs for Case 1 of benchmark test because the
analysed problem has a singular point due to discretely assigned Dirichlet
boundary conditions. The reliability of the results around the singular point
could be improved by the refinement of the mesh in the area around the singular
point, but as a point of interest is the total heat flux that is entering the
building element, and it must converge between subdivisions, this method is not
good since the reliable result cannot be reached. The problem for the
convergence is in the marginal node because the temperature gradient in it
increases as the temperature difference remains constant and the distance
between the corresponding nodes decreases. For that reason, Case 1 from the
benchmark is inadequate because even if there is a discontinuity in temperature
field on the boundary, there is an interval in which this change is to happen,
and the heat flux has a theoretical limit which is not infinity. From the
results of this research, it is shown that one should neglect a certain number
of singular points in order to achieve the tolerance given by the standard
since the temperature further from the marginal node is stable for any
subdivision.
</p>
<a href="http://arxiv.org/abs/2010.05207" target="_blank">arXiv:2010.05207</a> [<a href="http://arxiv.org/pdf/2010.05207" target="_blank">pdf</a>]

<h2>How to Stop Epidemics: Controlling Graph Dynamics with Reinforcement Learning and Graph Neural Networks. (arXiv:2010.05313v2 [cs.LG] UPDATED)</h2>
<h3>Eli A. Meirom, Haggai Maron, Shie Mannor, Gal Chechik</h3>
<p>We consider the problem of monitoring and controlling a partially-observed
dynamic process that spreads over a graph. This problem naturally arises in
contexts such as scheduling virus tests or quarantining individuals to curb a
spreading epidemic; detecting fake news spreading on online networks by
manually inspecting posted articles; and targeted marketing where the objective
is to encourage the spread of a product. Curbing the spread and constraining
the fraction of infected population becomes challenging when only a fraction of
the population can be tested or quarantined.

To address this challenge, we formulate this setup as a sequential decision
problem over a graph. In face of an exponential state space, combinatorial
action space and partial observability, we design RLGN, a novel tractable
Reinforcement Learning (RL) scheme to prioritize which nodes should be tested,
using Graph Neural Networks (GNNs) to rank the graph nodes. We evaluate this
approach in three types of social-networks: community-structured, preferential
attachment, and based on statistics from real cellular tracking. RLGN
consistently outperforms all baselines in our experiments. It suggests that
prioritizing tests using RL on temporal graphs can increase the number of
healthy people by $25\%$ and contain the epidemic $30\%$ more often than
supervised approaches and $2.5\times$ more often than non-learned baselines
using the same resources.
</p>
<a href="http://arxiv.org/abs/2010.05313" target="_blank">arXiv:2010.05313</a> [<a href="http://arxiv.org/pdf/2010.05313" target="_blank">pdf</a>]

<h2>Are all negatives created equal in contrastive instance discrimination?. (arXiv:2010.06682v2 [cs.CV] UPDATED)</h2>
<h3>Tiffany Tianhui Cai, Jonathan Frankle, David J. Schwab, Ari S. Morcos</h3>
<p>Self-supervised learning has recently begun to rival supervised learning on
computer vision tasks. Many of the recent approaches have been based on
contrastive instance discrimination (CID), in which the network is trained to
recognize two augmented versions of the same instance (a query and positive)
while discriminating against a pool of other instances (negatives). The learned
representation is then used on downstream tasks such as image classification.
Using methodology from MoCo v2 (Chen et al., 2020), we divided negatives by
their difficulty for a given query and studied which difficulty ranges were
most important for learning useful representations. We found a minority of
negatives -- the hardest 5% -- were both necessary and sufficient for the
downstream task to reach nearly full accuracy. Conversely, the easiest 95% of
negatives were unnecessary and insufficient. Moreover, the very hardest 0.1% of
negatives were unnecessary and sometimes detrimental. Finally, we studied the
properties of negatives that affect their hardness, and found that hard
negatives were more semantically similar to the query, and that some negatives
were more consistently easy or hard than we would expect by chance. Together,
our results indicate that negatives vary in importance and that CID may benefit
from more intelligent negative treatment.
</p>
<a href="http://arxiv.org/abs/2010.06682" target="_blank">arXiv:2010.06682</a> [<a href="http://arxiv.org/pdf/2010.06682" target="_blank">pdf</a>]

<h2>GreedyFool: An Imperceptible Black-box Adversarial Example Attack against Neural Networks. (arXiv:2010.06855v2 [cs.LG] UPDATED)</h2>
<h3>Hui Liu, Bo Zhao, Jiabao Guo, Yang An, Peng Liu</h3>
<p>Deep neural networks (DNNs) are inherently vulnerable to well-designed input
samples called adversarial examples. The adversary can easily fool DNNs by
adding slight perturbations to the input. In this paper, we propose a novel
black-box adversarial example attack named GreedyFool, which synthesizes
adversarial examples based on the differential evolution and the greedy
approximation. The differential evolution is utilized to evaluate the effects
of perturbed pixels on the confidence of the DNNs-based classifier. The greedy
approximation is an approximate optimization algorithm to automatically get
adversarial perturbations. Existing works synthesize the adversarial examples
by leveraging simple metrics to penalize the perturbations, which lack
sufficient consideration of the human visual system (HVS), resulting in
noticeable artifacts. In order to sufficient imperceptibility, we launch a lot
of investigations into the HVS and design an integrated metric considering just
noticeable distortion (JND), Weber-Fechner law, texture masking and channel
modulation, which is proven to be a better metric to measure the perceptual
distance between the benign examples and the adversarial ones. The experimental
results demonstrate that the GreedyFool has several remarkable properties
including black-box, 100% success rate, flexibility, automation and can
synthesize the more imperceptible adversarial examples than the
state-of-the-art pixel-wise methods.
</p>
<a href="http://arxiv.org/abs/2010.06855" target="_blank">arXiv:2010.06855</a> [<a href="http://arxiv.org/pdf/2010.06855" target="_blank">pdf</a>]

<h2>AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients. (arXiv:2010.07468v2 [cs.LG] UPDATED)</h2>
<h3>Juntang Zhuang, Tommy Tang, Yifan Ding, Sekhar Tatikonda, Nicha Dvornek, Xenophon Papademetris, James S. Duncan</h3>
<p>Most popular optimizers for deep learning can be broadly categorized as
adaptive methods (e.g. Adam) and accelerated schemes (e.g. stochastic gradient
descent (SGD) with momentum). For many models such as convolutional neural
networks (CNNs), adaptive methods typically converge faster but generalize
worse compared to SGD; for complex settings such as generative adversarial
networks (GANs), adaptive methods are typically the default because of their
stability.We propose AdaBelief to simultaneously achieve three goals: fast
convergence as in adaptive methods, good generalization as in SGD, and training
stability. The intuition for AdaBelief is to adapt the stepsize according to
the "belief" in the current gradient direction. Viewing the exponential moving
average (EMA) of the noisy gradient as the prediction of the gradient at the
next time step, if the observed gradient greatly deviates from the prediction,
we distrust the current observation and take a small step; if the observed
gradient is close to the prediction, we trust it and take a large step. We
validate AdaBelief in extensive experiments, showing that it outperforms other
methods with fast convergence and high accuracy on image classification and
language modeling. Specifically, on ImageNet, AdaBelief achieves comparable
accuracy to SGD. Furthermore, in the training of a GAN on Cifar10, AdaBelief
demonstrates high stability and improves the quality of generated samples
compared to a well-tuned Adam optimizer. Code is available at
https://github.com/juntang-zhuang/Adabelief-Optimizer
</p>
<a href="http://arxiv.org/abs/2010.07468" target="_blank">arXiv:2010.07468</a> [<a href="http://arxiv.org/pdf/2010.07468" target="_blank">pdf</a>]

<h2>Neograd: Gradient Descent with a Near-Ideal Learning Rate. (arXiv:2010.07873v2 [cs.LG] UPDATED)</h2>
<h3>Michael F. Zimmer</h3>
<p>Since its inception by Cauchy in 1847, the gradient descent algorithm has
been without guidance as to how to efficiently set the learning rate. This
paper identifies a concept, defines metrics, and introduces algorithms to
provide such guidance. The result is a family of algorithms (Neograd) based on
a {\em constant $\rho$ ansatz}, where $\rho$ is a metric based on the error of
the updates. This allows one to adjust the learning rate at each step, using a
formulaic estimate based on $\rho$. It is now no longer necessary to do trial
runs beforehand to estimate a single learning rate for an entire optimization
run. The additional costs to operate this metric are trivial. One member of
this family of algorithms, NeogradM, can quickly reach much lower cost function
values than other first order algorithms. Comparisons are made mainly between
NeogradM and Adam on an array of test functions and on a neural network model
for identifying hand-written digits. The results show great performance
improvements with NeogradM.
</p>
<a href="http://arxiv.org/abs/2010.07873" target="_blank">arXiv:2010.07873</a> [<a href="http://arxiv.org/pdf/2010.07873" target="_blank">pdf</a>]

<h2>SIGTYP 2020 Shared Task: Prediction of Typological Features. (arXiv:2010.08246v2 [cs.CL] UPDATED)</h2>
<h3>Johannes Bjerva, Elizabeth Salesky, Sabrina J. Mielke, Aditi Chaudhary, Giuseppe G. A. Celano, Edoardo M. Ponti, Ekaterina Vylomova, Ryan Cotterell, Isabelle Augenstein</h3>
<p>Typological knowledge bases (KBs) such as WALS (Dryer and Haspelmath, 2013)
contain information about linguistic properties of the world's languages. They
have been shown to be useful for downstream applications, including
cross-lingual transfer learning and linguistic probing. A major drawback
hampering broader adoption of typological KBs is that they are sparsely
populated, in the sense that most languages only have annotations for some
features, and skewed, in that few features have wide coverage. As typological
features often correlate with one another, it is possible to predict them and
thus automatically populate typological KBs, which is also the focus of this
shared task. Overall, the task attracted 8 submissions from 5 teams, out of
which the most successful methods make use of such feature correlations.
However, our error analysis reveals that even the strongest submitted systems
struggle with predicting feature values for languages where few features are
known.
</p>
<a href="http://arxiv.org/abs/2010.08246" target="_blank">arXiv:2010.08246</a> [<a href="http://arxiv.org/pdf/2010.08246" target="_blank">pdf</a>]

<h2>VolumeNet: A Lightweight Parallel Network for Super-Resolution of Medical Volumetric Data. (arXiv:2010.08357v2 [eess.IV] UPDATED)</h2>
<h3>Yinhao Li, Yutaro Iwamoto, Lanfen Lin, Rui Xu, Yen-Wei Chen</h3>
<p>Deep learning-based super-resolution (SR) techniques have generally achieved
excellent performance in the computer vision field. Recently, it has been
proven that three-dimensional (3D) SR for medical volumetric data delivers
better visual results than conventional two-dimensional (2D) processing.
However, deepening and widening 3D networks increases training difficulty
significantly due to the large number of parameters and small number of
training samples. Thus, we propose a 3D convolutional neural network (CNN) for
SR of medical volumetric data called ParallelNet using parallel connections. We
construct a parallel connection structure based on the group convolution and
feature aggregation to build a 3D CNN that is as wide as possible with few
parameters. As a result, the model thoroughly learns more feature maps with
larger receptive fields. In addition, to further improve accuracy, we present
an efficient version of ParallelNet (called VolumeNet), which reduces the
number of parameters and deepens ParallelNet using a proposed lightweight
building block module called the Queue module. Unlike most lightweight CNNs
based on depthwise convolutions, the Queue module is primarily constructed
using separable 2D cross-channel convolutions. As a result, the number of
network parameters and computational complexity can be reduced significantly
while maintaining accuracy due to full channel fusion. Experimental results
demonstrate that the proposed VolumeNet significantly reduces the number of
model parameters and achieves high precision results compared to
state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2010.08357" target="_blank">arXiv:2010.08357</a> [<a href="http://arxiv.org/pdf/2010.08357" target="_blank">pdf</a>]

<h2>Ordinal Neural Network Transformation Models: Deep and interpretable regression models for ordinal outcomes. (arXiv:2010.08376v2 [stat.ML] UPDATED)</h2>
<h3>Lucas Kook, Lisa Herzog, Torsten Hothorn, Oliver D&#xfc;rr, Beate Sick</h3>
<p>Outcomes with a natural order commonly occur in prediction tasks and
oftentimes the available input data are a mixture of complex data, like images,
and tabular predictors. Although deep Learning (DL) methods have shown
outstanding performance on image classification, most models treat ordered
outcomes as unordered and lack interpretability. In contrast, classical ordinal
regression models yield interpretable predictor effects but are limited to
tabular input data. Here, we present the highly modular class of ordinal neural
network transformation models (ONTRAMs). Transformation models use a parametric
transformation function and a simple distribution to trade off flexibility and
interpretability of individual model components. In ONTRAMs, this trade-off is
achieved by additively decomposing the transformation function into terms for
the tabular and image data using a set of jointly trained neural networks. We
show that the most flexible ONTRAMs achieve on-par performance with DL
classifiers while outperforming them in training speed. We discuss how to
interpret components of ONTRAMs in general and in the case of correlated
tabular and image data. Taken together, ONTRAMs join benefits of DL and
distributional regression to create interpretable prediction models for ordinal
outcomes.
</p>
<a href="http://arxiv.org/abs/2010.08376" target="_blank">arXiv:2010.08376</a> [<a href="http://arxiv.org/pdf/2010.08376" target="_blank">pdf</a>]

<h2>An efficient representation of chronological events in medical texts. (arXiv:2010.08433v2 [cs.CL] UPDATED)</h2>
<h3>Andrey Kormilitzin, Nemanja Vaci, Qiang Liu, Hao Ni, Goran Nenadic, Alejo Nevado-Holgado</h3>
<p>In this work we addressed the problem of capturing sequential information
contained in longitudinal electronic health records (EHRs). Clinical notes,
which is a particular type of EHR data, are a rich source of information and
practitioners often develop clever solutions how to maximise the sequential
information contained in free-texts. We proposed a systematic methodology for
learning from chronological events available in clinical notes. The proposed
methodological {\it path signature} framework creates a non-parametric
hierarchical representation of sequential events of any type and can be used as
features for downstream statistical learning tasks. The methodology was
developed and externally validated using the largest in the UK secondary care
mental health EHR data on a specific task of predicting survival risk of
patients diagnosed with Alzheimer's disease. The signature-based model was
compared to a common survival random forest model. Our results showed a
15.4$\%$ increase of risk prediction AUC at the time point of 20 months after
the first admission to a specialist memory clinic and the signature method
outperformed the baseline mixed-effects model by 13.2 $\%$.
</p>
<a href="http://arxiv.org/abs/2010.08433" target="_blank">arXiv:2010.08433</a> [<a href="http://arxiv.org/pdf/2010.08433" target="_blank">pdf</a>]

<h2>Tensor-based Intrinsic Subspace Representation Learning for Multi-view Clustering. (arXiv:2010.09193v2 [cs.LG] UPDATED)</h2>
<h3>Qinghai Zheng, Jihua Zhu, Zhongyu Li, Haoyu Tang, Shuangxun Ma</h3>
<p>Multi-view subspace clustering is an important and hot topic in machine
learning field, which aims to promote clustering results based on multi-view
data, which are collected from different domains or various measurements. In
this paper, we propose a novel tensor-based intrinsic subspace representation
learning for multi-view clustering. Specifically, to investigate the underlying
subspace representation, the rank preserving decomposition accompanied with the
tensor-singular value decomposition based low-rank tensor constraint is
introduced and applied on the subspace representation matrices of multiple
views. The specific information of different views can be considered by the
rank preserving decomposition and the high-order correlations of multi-view
data are fully explored by the low-rank tensor constraint in our method. Based
on the learned subspace representation, clustering results can be obtained by
employing the standard spectral clustering algorithm. The objective function is
efficiently optimized by utilizing the augmented Lagrangian multiplier based
alternating direction minimization algorithm. Experimental results on nine
real-world datasets illustrate the superiority of our method compared to
several state-of-the-arts.
</p>
<a href="http://arxiv.org/abs/2010.09193" target="_blank">arXiv:2010.09193</a> [<a href="http://arxiv.org/pdf/2010.09193" target="_blank">pdf</a>]

<h2>MimicNorm: Weight Mean and Last BN Layer Mimic the Dynamic of Batch Normalization. (arXiv:2010.09278v2 [cs.LG] UPDATED)</h2>
<h3>Wen Fei, Wenrui Dai, Chenglin Li, Junni Zou, Hongkai Xiong</h3>
<p>Substantial experiments have validated the success of Batch Normalization
(BN) Layer in benefiting convergence and generalization. However, BN requires
extra memory and float-point calculation. Moreover, BN would be inaccurate on
micro-batch, as it depends on batch statistics. In this paper, we address these
problems by simplifying BN regularization while keeping two fundamental impacts
of BN layers, i.e., data decorrelation and adaptive learning rate. We propose a
novel normalization method, named MimicNorm, to improve the convergence and
efficiency in network training. MimicNorm consists of only two light
operations, including modified weight mean operations (subtract mean values
from weight parameter tensor) and one BN layer before loss function (last BN
layer). We leverage the neural tangent kernel (NTK) theory to prove that our
weight mean operation whitens activations and transits network into the chaotic
regime like BN layer, and consequently, leads to an enhanced convergence. The
last BN layer provides autotuned learning rates and also improves accuracy.
Experimental results show that MimicNorm achieves similar accuracy for various
network structures, including ResNets and lightweight networks like ShuffleNet,
with a reduction of about 20% memory consumption. The code is publicly
available at https://github.com/Kid-key/MimicNorm.
</p>
<a href="http://arxiv.org/abs/2010.09278" target="_blank">arXiv:2010.09278</a> [<a href="http://arxiv.org/pdf/2010.09278" target="_blank">pdf</a>]

<h2>Survey on Causal-based Machine Learning Fairness Notions. (arXiv:2010.09553v2 [cs.LG] UPDATED)</h2>
<h3>Karima Makhlouf, Sami Zhioua, Catuscia Palamidessi</h3>
<p>Addressing the problem of fairness is crucial to safely use machine learning
algorithms to support decisions with a critical impact on people's lives such
as job hiring, child maltreatment, disease diagnosis, loan granting, etc.
Several notions of fairness have been defined and examined in the past decade,
such as, statistical parity and equalized odds. The most recent fairness
notions, however, are causal-based and reflect the now widely accepted idea
that using causality is necessary to appropriately address the problem of
fairness. This paper examines an exhaustive list of causal-based fairness
notions, in particular their applicability in real-world scenarios. As the
majority of causal-based fairness notions are defined in terms of
non-observable quantities (e.g. interventions and counterfactuals), their
applicability depends heavily on the identifiability of those quantities from
observational data. In this paper, we compile the most relevant identifiability
criteria for the problem of fairness from the extensive literature on
identifiability theory. These criteria are then used to decide about the
applicability of causal-based fairness notions in concrete discrimination
scenarios.
</p>
<a href="http://arxiv.org/abs/2010.09553" target="_blank">arXiv:2010.09553</a> [<a href="http://arxiv.org/pdf/2010.09553" target="_blank">pdf</a>]

<h2>American Sign Language Identification Using Hand Trackpoint Analysis. (arXiv:2010.10590v2 [cs.CV] UPDATED)</h2>
<h3>Yugam Bajaj, Puru Malhotra</h3>
<p>Sign Language helps people with Speaking and Hearing Disabilities communicate
with others efficiently. Sign Language identification is a challenging area in
the field of computer vision and recent developments have been able to achieve
near perfect results for the task, though some challenges are yet to be solved.
In this paper we propose a novel machine learning based pipeline for American
Sign Language identification using hand track points. We convert a hand gesture
into a series of hand track point coordinates that serve as an input to our
system. In order to make the solution more efficient, we experimented with 28
different combinations of pre-processing techniques, each run on three
different machine learning algorithms namely k-Nearest Neighbours, Random
Forests and a Neural Network. Their performance was contrasted to determine the
best pre-processing scheme and Algorithm Pair. Our system achieved an Accuracy
of 95.66% to identify American sign language gestures.
</p>
<a href="http://arxiv.org/abs/2010.10590" target="_blank">arXiv:2010.10590</a> [<a href="http://arxiv.org/pdf/2010.10590" target="_blank">pdf</a>]

<h2>Open-Domain Frame Semantic Parsing Using Transformers. (arXiv:2010.10998v2 [cs.CL] UPDATED)</h2>
<h3>Aditya Kalyanpur, Or Biran, Tom Breloff, Jennifer Chu-Carroll, Ariel Diertani, Owen Rambow, Mark Sammons</h3>
<p>Frame semantic parsing is a complex problem which includes multiple
underlying subtasks. Recent approaches have employed joint learning of subtasks
(such as predicate and argument detection), and multi-task learning of related
tasks (such as syntactic and semantic parsing). In this paper, we explore
multi-task learning of all subtasks with transformer-based models. We show that
a purely generative encoder-decoder architecture handily beats the previous
state of the art in FrameNet 1.7 parsing, and that a mixed decoding multi-task
approach achieves even better performance. Finally, we show that the multi-task
model also outperforms recent state of the art systems for PropBank SRL parsing
on the CoNLL 2012 benchmark.
</p>
<a href="http://arxiv.org/abs/2010.10998" target="_blank">arXiv:2010.10998</a> [<a href="http://arxiv.org/pdf/2010.10998" target="_blank">pdf</a>]

<h2>Secure Software Leasing from Standard Assumptions. (arXiv:2010.11186v2 [quant-ph] UPDATED)</h2>
<h3>Fuyuki Kitagawa, Ryo Nishimaki, Takashi Yamakawa</h3>
<p>Secure software leasing (SSL) is a quantum cryptographic primitive that
enables users to execute software only during the software is leased. It
prevents users from executing leased software after they return the leased
software to its owner. SSL can make software distribution more flexible and
controllable. Although SSL is an attractive cryptographic primitive, the
existing SSL scheme is based on public key quantum money, which is not
instantiated with standard cryptographic assumptions so far. Moreover, the
existing SSL scheme only supports a subclass of evasive functions. In this
work, we present SSL schemes based on the learning with errors assumption
(LWE). Specifically, our contributions consist of the following.

- We construct an SSL scheme for pseudorandom functions from the LWE
assumption against quantum adversaries.

- We construct an SSL scheme for a subclass of evasive functions from the LWE
assumption against sub-exponential quantum adversaries.

- We construct SSL schemes for the functionalities above with classical
communication from the LWE assumption against (sub-exponential) quantum
adversaries. SSL with classical communication means that entities exchange only
classical information though they run quantum computation locally.

Our crucial tool is two-tier quantum lightning, which is introduced in this
work and a relaxed version of quantum lighting. In two-tier quantum lightning
schemes, we have a public verification algorithm called semi-verification and a
private verification algorithm called full-verification. An adversary cannot
generate possibly entangled two quantum states whose serial numbers are the
same such that one passes the semi-verification, and the other also passes the
full-verification. We show that we can construct a two-tier quantum lightning
scheme from the LWE assumption.
</p>
<a href="http://arxiv.org/abs/2010.11186" target="_blank">arXiv:2010.11186</a> [<a href="http://arxiv.org/pdf/2010.11186" target="_blank">pdf</a>]

<h2>MicroNets: Neural Network Architectures for Deploying TinyML Applications on Commodity Microcontrollers. (arXiv:2010.11267v2 [cs.LG] UPDATED)</h2>
<h3>Colby Banbury, Chuteng Zhou, Igor Fedorov, Ramon Matas Navarro, Urmish Thakker, Dibakar Gope, Vijay Janapa Reddi, Matthew Mattina, Paul N. Whatmough</h3>
<p>Executing machine learning workloads locally on resource constrained
microcontrollers (MCUs) promises to drastically expand the application space of
IoT. However, so-called TinyML presents severe technical challenges, as deep
neural network inference demands a large compute and memory budget. To address
this challenge, neural architecture search (NAS) promises to help design
accurate ML models that meet the tight MCU memory, latency and energy
constraints. A key component of NAS algorithms is their latency/energy model,
i.e., the mapping from a given neural network architecture to its inference
latency/energy on an MCU. In this paper, we observe an intriguing property of
NAS search spaces for MCU model design: on average, model latency varies
linearly with model operation (op) count under a uniform prior over models in
the search space. Exploiting this insight, we employ differentiable NAS (DNAS)
to search for models with low memory usage and low op count, where op count is
treated as a viable proxy to latency. Experimental results validate our
methodology, yielding our MicroNet models, which we deploy on MCUs using
Tensorflow Lite Micro, a standard open-source NN inference runtime widely used
in the TinyML community. MicroNets demonstrate state-of-the-art results for all
three TinyMLperf industry-standard benchmark tasks: visual wake words, audio
keyword spotting, and anomaly detection.
</p>
<a href="http://arxiv.org/abs/2010.11267" target="_blank">arXiv:2010.11267</a> [<a href="http://arxiv.org/pdf/2010.11267" target="_blank">pdf</a>]

<h2>Meta-Learning Guarantees for Online Receding Horizon Control. (arXiv:2010.11327v2 [eess.SY] UPDATED)</h2>
<h3>Deepan Muthirayan, Pramod P. Khargonekar</h3>
<p>In this paper we provide provable regret guarantees for an online
meta-learning receding horizon control algorithm in an iterative control
setting, where in each iteration the system to be controlled is a linear
deterministic system that is different and unknown, the cost for the controller
in an iteration is a general additive cost function and the control input is
required to be constrained, which if violated incurs an additional cost. We
prove (i) that the algorithm achieves a regret for the controller cost and
constraint violation that are $O(T^{3/4})$ for an episode of duration $T$ with
respect to the best policy that satisfies the control input control constraints
and (ii) that the average of the regret for the controller cost and constraint
violation with respect to the same policy vary as $O((1+1/\sqrt{N})T^{3/4})$
with the number of iterations $N$, showing that the worst regret for the
learning within an iteration continuously improves with experience of more
iterations.
</p>
<a href="http://arxiv.org/abs/2010.11327" target="_blank">arXiv:2010.11327</a> [<a href="http://arxiv.org/pdf/2010.11327" target="_blank">pdf</a>]

<h2>TeX-Graph: Coupled tensor-matrix knowledge-graph embedding for COVID-19 drug repurposing. (arXiv:2010.11367v2 [cs.SI] UPDATED)</h2>
<h3>Charilaos I. Kanatsoulis, Nicholas D. Sidiropoulos</h3>
<p>Knowledge graphs (KGs) are powerful tools that codify relational behaviour
between entities in knowledge bases. KGs can simultaneously model many
different types of subject-predicate-object and higher-order relations. As
such, they offer a flexible modeling framework that has been applied to many
areas, including biology and pharmacology -- most recently, in the fight
against COVID-19. The flexibility of KG modeling is both a blessing and a
challenge from the learning point of view. In this paper we propose a novel
coupled tensor-matrix framework for KG embedding. We leverage tensor
factorization tools to learn concise representations of entities and relations
in knowledge bases and employ these representations to perform drug repurposing
for COVID-19. Our proposed framework is principled, elegant, and achieves 100%
improvement over the best baseline in the COVID-19 drug repurposing task using
a recently developed biological KG.
</p>
<a href="http://arxiv.org/abs/2010.11367" target="_blank">arXiv:2010.11367</a> [<a href="http://arxiv.org/pdf/2010.11367" target="_blank">pdf</a>]

<h2>DPD-InfoGAN: Differentially Private Distributed InfoGAN. (arXiv:2010.11398v2 [cs.LG] UPDATED)</h2>
<h3>Vaikkunth Mugunthan, Vignesh Gokul, Lalana Kagal, Shlomo Dubnov</h3>
<p>Generative Adversarial Networks (GANs) are deep learning architectures
capable of generating synthetic datasets. Despite producing high-quality
synthetic images, the default GAN has no control over the kinds of images it
generates. The Information Maximizing GAN (InfoGAN) is a variant of the default
GAN that introduces feature-control variables that are automatically learned by
the framework, hence providing greater control over the different kinds of
images produced. Due to the high model complexity of InfoGAN, the generative
distribution tends to be concentrated around the training data points. This is
a critical problem as the models may inadvertently expose the sensitive and
private information present in the dataset. To address this problem, we propose
a differentially private version of InfoGAN (DP-InfoGAN). We also extend our
framework to a distributed setting (DPD-InfoGAN) to allow clients to learn
different attributes present in other clients' datasets in a privacy-preserving
manner. In our experiments, we show that both DP-InfoGAN and DPD-InfoGAN can
synthesize high-quality images with flexible control over image attributes
while preserving privacy.
</p>
<a href="http://arxiv.org/abs/2010.11398" target="_blank">arXiv:2010.11398</a> [<a href="http://arxiv.org/pdf/2010.11398" target="_blank">pdf</a>]

<h2>Confidence Estimation for Attention-based Sequence-to-sequence Models for Speech Recognition. (arXiv:2010.11428v2 [eess.AS] UPDATED)</h2>
<h3>Qiujia Li, David Qiu, Yu Zhang, Bo Li, Yanzhang He, Philip C. Woodland, Liangliang Cao, Trevor Strohman</h3>
<p>For various speech-related tasks, confidence scores from a speech recogniser
are a useful measure to assess the quality of transcriptions. In traditional
hidden Markov model-based automatic speech recognition (ASR) systems,
confidence scores can be reliably obtained from word posteriors in decoding
lattices. However, for an ASR system with an auto-regressive decoder, such as
an attention-based sequence-to-sequence model, computing word posteriors is
difficult. An obvious alternative is to use the decoder softmax probability as
the model confidence. In this paper, we first examine how some commonly used
regularisation methods influence the softmax-based confidence scores and study
the overconfident behaviour of end-to-end models. Then we propose a lightweight
and effective approach named confidence estimation module (CEM) on top of an
existing end-to-end ASR model. Experiments on LibriSpeech show that CEM can
mitigate the overconfidence problem and can produce more reliable confidence
scores with and without shallow fusion of a language model. Further analysis
shows that CEM generalises well to speech from a moderately mismatched domain
and can potentially improve downstream tasks such as semi-supervised learning.
</p>
<a href="http://arxiv.org/abs/2010.11428" target="_blank">arXiv:2010.11428</a> [<a href="http://arxiv.org/pdf/2010.11428" target="_blank">pdf</a>]

<h2>Object-Attribute Biclustering for Elimination of Missing Genotypes in Ischemic Stroke Genome-Wide Data. (arXiv:2010.11641v2 [q-bio.GN] UPDATED)</h2>
<h3>Dmitry I. Ignatov, Gennady V. Khvorykh, Andrey V. Khrunin, Stefan Nikoli&#x107;, Makhmud Shaban, Elizaveta A. Petrova, Evgeniya A. Koltsova, Fouzi Takelait, Dmitrii Egurnov</h3>
<p>Missing genotypes can affect the efficacy of machine learning approaches to
identify the risk genetic variants of common diseases and traits. The problem
occurs when genotypic data are collected from different experiments with
different DNA microarrays, each being characterised by its pattern of uncalled
(missing) genotypes. This can prevent the machine learning classifier from
assigning the classes correctly. To tackle this issue, we used well-developed
notions of object-attribute biclusters and formal concepts that correspond to
dense subrelations in the binary relation $\textit{patients} \times
\textit{SNPs}$. The paper contains experimental results on applying a
biclustering algorithm to a large real-world dataset collected for studying the
genetic bases of ischemic stroke. The algorithm could identify large dense
biclusters in the genotypic matrix for further processing, which in return
significantly improved the quality of machine learning classifiers. The
proposed algorithm was also able to generate biclusters for the whole dataset
without size constraints in comparison to the In-Close4 algorithm for
generation of formal concepts.
</p>
<a href="http://arxiv.org/abs/2010.11641" target="_blank">arXiv:2010.11641</a> [<a href="http://arxiv.org/pdf/2010.11641" target="_blank">pdf</a>]

<h2>Restoring Negative Information in Few-Shot Object Detection. (arXiv:2010.11714v2 [cs.CV] UPDATED)</h2>
<h3>Yukuan Yang, Fangyun Wei, Miaojing Shi, Guoqi Li</h3>
<p>Few-shot learning has recently emerged as a new challenge in the deep
learning field: unlike conventional methods that train the deep neural networks
(DNNs) with a large number of labeled data, it asks for the generalization of
DNNs on new classes with few annotated samples. Recent advances in few-shot
learning mainly focus on image classification while in this paper we focus on
object detection. The initial explorations in few-shot object detection tend to
simulate a classification scenario by using the positive proposals in images
with respect to certain object class while discarding the negative proposals of
that class. Negatives, especially hard negatives, however, are essential to the
embedding space learning in few-shot object detection. In this paper, we
restore the negative information in few-shot object detection by introducing a
new negative- and positive-representative based metric learning framework and a
new inference scheme with negative and positive representatives. We build our
work on a recent few-shot pipeline RepMet with several new modules to encode
negative information for both training and testing. Extensive experiments on
ImageNet-LOC and PASCAL VOC show our method substantially improves the
state-of-the-art few-shot object detection solutions. Our code is available at
https://github.com/yang-yk/NP-RepMet.
</p>
<a href="http://arxiv.org/abs/2010.11714" target="_blank">arXiv:2010.11714</a> [<a href="http://arxiv.org/pdf/2010.11714" target="_blank">pdf</a>]

<h2>Multifaceted Context Representation using Dual Attention for Ontology Alignment. (arXiv:2010.11721v2 [cs.AI] UPDATED)</h2>
<h3>Vivek Iyer, Arvind Agarwal, Harshit Kumar</h3>
<p>Ontology Alignment is an important research problem that finds application in
various fields such as data integration, data transfer, data preparation etc.
State-of-the-art (SOTA) architectures in Ontology Alignment typically use naive
domain-dependent approaches with handcrafted rules and manually assigned
values, making them unscalable and inefficient. Deep Learning approaches for
ontology alignment use domain-specific architectures that are not only
in-extensible to other datasets and domains, but also typically perform worse
than rule-based approaches due to various limitations including over-fitting of
models, sparsity of datasets etc. In this work, we propose VeeAlign, a Deep
Learning based model that uses a dual-attention mechanism to compute the
contextualized representation of a concept in order to learn alignments. By
doing so, not only does our approach exploit both syntactic and semantic
structure of ontologies, it is also, by design, flexible and scalable to
different domains with minimal effort. We validate our approach on various
datasets from different domains and in multilingual settings, and show its
superior performance over SOTA methods.
</p>
<a href="http://arxiv.org/abs/2010.11721" target="_blank">arXiv:2010.11721</a> [<a href="http://arxiv.org/pdf/2010.11721" target="_blank">pdf</a>]

<h2>Neural Audio Fingerprint for High-specific Audio Retrieval based on Contrastive Learning. (arXiv:2010.11910v2 [cs.SD] UPDATED)</h2>
<h3>Sungkyun Chang, Donmoon Lee, Jeongsoo Park, Hyungui Lim, Kyogu Lee, Karam Ko, Yoonchang Han</h3>
<p>Most of existing audio fingerprinting systems have limitations to be used for
high-specific audio retrieval at scale. In this work, we generate a
low-dimensional representation from a short unit segment of audio, and couple
this fingerprint with a fast maximum inner-product search. To this end, we
present a contrastive learning framework that derives from the segment-level
search objective. Each update in training uses a batch consisting of a set of
pseudo labels, randomly selected original samples, and their augmented
replicas. These replicas can simulate the degrading effects on original audio
signals by applying small time offsets and various types of distortions, such
as background noise and room/microphone impulse responses. In the segment-level
search task, where the conventional audio fingerprinting systems used to fail,
our system using 10x smaller storage has shown promising results. Our code and
dataset will be available.
</p>
<a href="http://arxiv.org/abs/2010.11910" target="_blank">arXiv:2010.11910</a> [<a href="http://arxiv.org/pdf/2010.11910" target="_blank">pdf</a>]

<h2>Approximation Methods for Kernelized Bandits. (arXiv:2010.12167v2 [cs.LG] UPDATED)</h2>
<h3>Sho Takemori, Masahiro Sato</h3>
<p>The RKHS bandit problem (also called kernelized multi-armed bandit problem)
is an online optimization problem of non-linear functions with noisy feedbacks.
Most of the existing methods for the problem have sub-linear regret guarantee
at the cost of high computational complexity. For example, IGP-UCB requires at
least quadratic time in the number of observed samples at each round. In this
paper, using deep results provided by the approximation theory, we
approximately reduce the problem to the well-studied linear bandit problem of
an appropriate dimension. Then, we propose several algorithms and prove that
they achieve comparable regret guarantee to the existing methods (GP-UCB,
IGP-UCB) with less computational complexity. Specifically, our proposed methods
require polylogarithmic time to select an arm at each round for kernels with
"infinite smoothness" (e.g. the rational quadratic and squared exponential
kernels). Furthermore, we empirically show our proposed method has comparable
regret to the existing method and its running time is much shorter.
</p>
<a href="http://arxiv.org/abs/2010.12167" target="_blank">arXiv:2010.12167</a> [<a href="http://arxiv.org/pdf/2010.12167" target="_blank">pdf</a>]

<h2>Reducing Bias in Modeling Real-world Password Strength via Deep Learning and Dynamic Dictionaries. (arXiv:2010.12269v2 [cs.CR] UPDATED)</h2>
<h3>Dario Pasquini, Marco Cianfriglia, Giuseppe Ateniese, Massimo Bernaschi</h3>
<p>Password security hinges on an accurate understanding of the techniques
adopted by attackers. However, current studies mostly rely on probabilistic
password models that are imperfect proxies of real-world guessing strategies.
The main reason is that attackers rely on very pragmatic approaches such as
dictionary attacks. Unfortunately, it is inherently difficult to correctly
model those methods. To be representative, dictionary attacks must be
thoughtfully configured according to a process that requires an expertise that
cannot be easily replicated in password studies. The consequence of
inaccurately calibrating those attacks is the unreliability of password
security estimates, impaired by measurement bias.

In the present work, we introduce new guessing techniques that make
dictionary attacks consistently more resilient to inadequate configurations.
Our framework allows dictionary attacks to self-heal and converge towards
optimal attacks' performance, requiring no supervision or domain-knowledge. To
achieve this: (1) We use a deep neural network to model and then simulate the
proficiency of expert adversaries. (2) Then, we introduce automatic dynamic
strategies within dictionary attacks to mimic experts' ability to adapt their
guessing strategies on the fly by incorporating knowledge on their targets. Our
techniques enable robust and sound password strength estimates, eventually
reducing bias in modeling real-world threats in password security.
</p>
<a href="http://arxiv.org/abs/2010.12269" target="_blank">arXiv:2010.12269</a> [<a href="http://arxiv.org/pdf/2010.12269" target="_blank">pdf</a>]

<h2>Topic Space Trajectories: A case study on machine learning literature. (arXiv:2010.12294v2 [cs.LG] UPDATED)</h2>
<h3>Bastian Sch&#xe4;fermeier, Gerd Stumme, Tom Hanika</h3>
<p>The annual number of publications at scientific venues, for example,
conferences and journals, is growing quickly. Hence, even for researchers it
becomes harder and harder to keep track of research topics and their progress.
In this task, researchers can be supported by automated publication analysis.
Yet, many such methods result in uninterpretable, purely numerical
representations. As an attempt to support human analysts, we present
\emph{topic space trajectories}, a structure that allows for the comprehensible
tracking of research topics. We demonstrate how these trajectories can be
interpreted based on eight different analysis approaches. To obtain
comprehensible results, we employ non-negative matrix factorization as well as
suitable visualization techniques. We show the applicability of our approach on
a publication corpus spanning 50 years of machine learning research from 32
publication venues. Our novel analysis method may be employed for paper
classification, for the prediction of future research topics, and for the
recommendation of fitting conferences and journals for submitting unpublished
work.
</p>
<a href="http://arxiv.org/abs/2010.12294" target="_blank">arXiv:2010.12294</a> [<a href="http://arxiv.org/pdf/2010.12294" target="_blank">pdf</a>]

<h2>Learning Implicit Functions for Topology-Varying Dense 3D Shape Correspondence. (arXiv:2010.12320v2 [cs.CV] UPDATED)</h2>
<h3>Feng Liu, Xiaoming Liu</h3>
<p>The goal of this paper is to learn dense 3D shape correspondence for
topology-varying objects in an unsupervised manner. Conventional implicit
functions estimate the occupancy of a 3D point given a shape latent code.
Instead, our novel implicit function produces a part embedding vector for each
3D point, which is assumed to be similar to its densely corresponded point in
another 3D shape of the same object category. Furthermore, we implement dense
correspondence through an inverse function mapping from the part embedding to a
corresponded 3D point. Both functions are jointly learned with several
effective loss functions to realize our assumption, together with the encoder
generating the shape latent code. During inference, if a user selects an
arbitrary point on the source shape, our algorithm can automatically generate a
confidence score indicating whether there is a correspondence on the target
shape, as well as the corresponding semantic point if there is one. Such a
mechanism inherently benefits man-made objects with different part
constitutions. The effectiveness of our approach is demonstrated through
unsupervised 3D semantic correspondence and shape segmentation.
</p>
<a href="http://arxiv.org/abs/2010.12320" target="_blank">arXiv:2010.12320</a> [<a href="http://arxiv.org/pdf/2010.12320" target="_blank">pdf</a>]

<h2>A Review of Deep Learning Methods for Irregularly Sampled Medical Time Series Data. (arXiv:2010.12493v2 [cs.LG] UPDATED)</h2>
<h3>Chenxi Sun, Shenda Hong, Moxian Song, Hongyan Li</h3>
<p>Irregularly sampled time series (ISTS) data has irregular temporal intervals
between observations and different sampling rates between sequences. ISTS
commonly appears in healthcare, economics, and geoscience. Especially in the
medical environment, the widely used Electronic Health Records (EHRs) have
abundant typical irregularly sampled medical time series (ISMTS) data.
Developing deep learning methods on EHRs data is critical for personalized
treatment, precise diagnosis and medical management. However, it is challenging
to directly use deep learning models for ISMTS data. On the one hand, ISMTS
data has the intra-series and inter-series relations. Both the local and global
structures should be considered. On the other hand, methods should consider the
trade-off between task accuracy and model complexity and remain generality and
interpretability. So far, many existing works have tried to solve the above
problems and have achieved good results. In this paper, we review these deep
learning methods from the perspectives of technology and task. Under the
technology-driven perspective, we summarize them into two categories - missing
data-based methods and raw data-based methods. Under the task-driven
perspective, we also summarize them into two categories - data
imputation-oriented and downstream task-oriented. For each of them, we point
out their advantages and disadvantages. Moreover, we implement some
representative methods and compare them on four medical datasets with two
tasks. Finally, we discuss the challenges and opportunities in this area.
</p>
<a href="http://arxiv.org/abs/2010.12493" target="_blank">arXiv:2010.12493</a> [<a href="http://arxiv.org/pdf/2010.12493" target="_blank">pdf</a>]

<h2>Bipartite Graph Neural Networks for Efficient Node Representation Learning. (arXiv:1906.11994v2 [cs.SI] CROSS LISTED)</h2>
<h3>Chaoyang He, Tian Xie, Yu Rong, Wenbing Huang, Yanfang Li, Junzhou Huang, Xiang Ren, Cyrus Shahabi</h3>
<p>Existing Graph Neural Networks (GNNs) mainly focus on general structures,
while the specific architecture on bipartite graphs---a crucial practical data
form that consists of two distinct domains of nodes---is seldom studied. In
this paper, we propose Bipartite Graph Neural Network (BGNN), a novel model
that is domain-consistent, unsupervised, and efficient. At its core, BGNN
utilizes the proposed Inter-domain Message Passing (IDMP) for message
aggregation and Intra-domain Alignment (IDA) towards information fusion over
domains, both of which are trained without requiring any supervision. Moreover,
we formulate a multi-layer BGNN in a cascaded manner to enable multi-hop
relation modeling while enjoying promising efficiency in training. Extensive
experiments on several datasets of varying scales verify the effectiveness of
BGNN compared to other counterparts. Particularly for the experiment on a
large-scale bipartite graph dataset, the scalability of our BGNN is validated
in terms of fast training speed and low memory cost.
</p>
<a href="http://arxiv.org/abs/1906.11994" target="_blank">arXiv:1906.11994</a> [<a href="http://arxiv.org/pdf/1906.11994" target="_blank">pdf</a>]

<h2>Byzantine Resilient Distributed Multi-Task Learning. (arXiv:2010.13032v1 [cs.LG])</h2>
<h3>Jiani Li, Waseem Abbas, Xenofon Koutsoukos</h3>
<p>Distributed multi-task learning provides significant advantages in
multi-agent networks with heterogeneous data sources where agents aim to learn
distinct but correlated models simultaneously. However, distributed algorithms
for learning relatedness among tasks are not resilient in the presence of
Byzantine agents. In this paper, we present an approach for Byzantine resilient
distributed multi-task learning. We propose an efficient online weight
assignment rule by measuring the accumulated loss using an agent's data and its
neighbors' models. A small accumulated loss indicates a large similarity
between the two tasks. In order to ensure the Byzantine resilience of the
aggregation at a normal agent, we introduce a step for filtering out larger
losses. We analyze the approach for convex models and show that normal agents
converge resiliently towards their true targets. Further, an agent's learning
performance using the proposed weight assignment rule is guaranteed to be at
least as good as in the non-cooperative case as measured by the expected
regret. Finally, we demonstrate the approach using three case studies,
including regression and classification problems, and show that our method
exhibits good empirical performance for non-convex models, such as
convolutional neural networks.
</p>
<a href="http://arxiv.org/abs/2010.13032" target="_blank">arXiv:2010.13032</a> [<a href="http://arxiv.org/pdf/2010.13032" target="_blank">pdf</a>]

<h2>Regularizing Towards Permutation Invariance in Recurrent Models. (arXiv:2010.13055v1 [cs.LG])</h2>
<h3>Edo Cohen-Karlik, Avichai Ben David, Amir Globerson</h3>
<p>In many machine learning problems the output should not depend on the order
of the input. Such "permutation invariant" functions have been studied
extensively recently. Here we argue that temporal architectures such as RNNs
are highly relevant for such problems, despite the inherent dependence of RNNs
on order. We show that RNNs can be regularized towards permutation invariance,
and that this can result in compact models, as compared to non-recurrent
architectures. We implement this idea via a novel form of stochastic
regularization.

Existing solutions mostly suggest restricting the learning problem to
hypothesis classes which are permutation invariant by design. Our approach of
enforcing permutation invariance via regularization gives rise to models which
are \textit{semi permutation invariant} (e.g. invariant to some permutations
and not to others). We show that our method outperforms other permutation
invariant approaches on synthetic and real world datasets.
</p>
<a href="http://arxiv.org/abs/2010.13055" target="_blank">arXiv:2010.13055</a> [<a href="http://arxiv.org/pdf/2010.13055" target="_blank">pdf</a>]

<h2>Further Analysis of Outlier Detection with Deep Generative Models. (arXiv:2010.13064v1 [stat.ML])</h2>
<h3>Ziyu Wang, Bin Dai, David Wipf, Jun Zhu</h3>
<p>The recent, counter-intuitive discovery that deep generative models (DGMs)
can frequently assign a higher likelihood to outliers has implications for both
outlier detection applications as well as our overall understanding of
generative modeling. In this work, we present a possible explanation for this
phenomenon, starting from the observation that a model's typical set and
high-density region may not conincide. From this vantage point we propose a
novel outlier test, the empirical success of which suggests that the failure of
existing likelihood-based outlier tests does not necessarily imply that the
corresponding generative model is uncalibrated. We also conduct additional
experiments to help disentangle the impact of low-level texture versus
high-level semantics in differentiating outliers. In aggregate, these results
suggest that modifications to the standard evaluation practices and benchmarks
commonly applied in the literature are needed.
</p>
<a href="http://arxiv.org/abs/2010.13064" target="_blank">arXiv:2010.13064</a> [<a href="http://arxiv.org/pdf/2010.13064" target="_blank">pdf</a>]

<h2>XLVIN: eXecuted Latent Value Iteration Nets. (arXiv:2010.13146v1 [cs.LG])</h2>
<h3>Andreea Deac, Petar Veli&#x10d;kovi&#x107;, Ognjen Milinkovi&#x107;, Pierre-Luc Bacon, Jian Tang, Mladen Nikoli&#x107;</h3>
<p>Value Iteration Networks (VINs) have emerged as a popular method to
incorporate planning algorithms within deep reinforcement learning, enabling
performance improvements on tasks requiring long-range reasoning and
understanding of environment dynamics. This came with several limitations,
however: the model is not incentivised in any way to perform meaningful
planning computations, the underlying state space is assumed to be discrete,
and the Markov decision process (MDP) is assumed fixed and known. We propose
eXecuted Latent Value Iteration Networks (XLVINs), which combine recent
developments across contrastive self-supervised learning, graph representation
learning and neural algorithmic reasoning to alleviate all of the above
limitations, successfully deploying VIN-style models on generic environments.
XLVINs match the performance of VIN-like models when the underlying MDP is
discrete, fixed and known, and provides significant improvements to model-free
baselines across three general MDP setups.
</p>
<a href="http://arxiv.org/abs/2010.13146" target="_blank">arXiv:2010.13146</a> [<a href="http://arxiv.org/pdf/2010.13146" target="_blank">pdf</a>]

<h2>Improving the Reconstruction of Disentangled Representation Learners via Multi-Stage Modelling. (arXiv:2010.13187v1 [stat.ML])</h2>
<h3>Akash Srivastava, Yamini Bansal, Yukun Ding, Cole Hurwitz, Kai Xu, Bernhard Egger, Prasanna Sattigeri, Josh Tenenbaum, David D. Cox, Dan Gutfreund</h3>
<p>Current autoencoder-based disentangled representation learning methods
achieve disentanglement by penalizing the (aggregate) posterior to encourage
statistical independence of the latent factors. This approach introduces a
trade-off between disentangled representation learning and reconstruction
quality since the model does not have enough capacity to learn correlated
latent variables that capture detail information present in most image data. To
overcome this trade-off, we present a novel multi-stage modelling approach
where the disentangled factors are first learned using a preexisting
disentangled representation learning method (such as $\beta$-TCVAE); then, the
low-quality reconstruction is improved with another deep generative model that
is trained to model the missing correlated latent variables, adding detail
information while maintaining conditioning on the previously learned
disentangled factors. Taken together, our multi-stage modelling approach
results in a single, coherent probabilistic model that is theoretically
justified by the principal of D-separation and can be realized with a variety
of model classes including likelihood-based models such as variational
autoencoders, implicit models such as generative adversarial networks, and
tractable models like normalizing flows or mixtures of Gaussians. We
demonstrate that our multi-stage model has much higher reconstruction quality
than current state-of-the-art methods with equivalent disentanglement
performance across multiple standard benchmarks.
</p>
<a href="http://arxiv.org/abs/2010.13187" target="_blank">arXiv:2010.13187</a> [<a href="http://arxiv.org/pdf/2010.13187" target="_blank">pdf</a>]

<h2>Machine Learning Based Network Coverage Guidance System. (arXiv:2010.13190v1 [cs.LG])</h2>
<h3>Srikanth Chandar, Muvazima Mansoor, Mohina Ahmadi, Hrishikesh Badve, Deepesh Sahoo, Bharath Katragadda</h3>
<p>With the advent of 4G, there has been a huge consumption of data and the
availability of mobile networks has become paramount. Also, with the burst of
network traffic based on user consumption, data availability and network
anomalies have increased substantially. In this paper, we introduce a novel
approach, to identify the regions that have poor network connectivity thereby
providing feedback to both the service providers to improve the coverage as
well as to the customers to choose the network judiciously. In addition to
this, the solution enables customers to navigate to a better mobile network
coverage area with stronger signal strength location using Machine Learning
Clustering Algorithms, whilst deploying it as a Mobile Application. It also
provides a dynamic visual representation of varying network strength and range
across nearby geographical areas.
</p>
<a href="http://arxiv.org/abs/2010.13190" target="_blank">arXiv:2010.13190</a> [<a href="http://arxiv.org/pdf/2010.13190" target="_blank">pdf</a>]

<h2>Revisiting convolutional neural network on graphs with polynomial approximations of Laplace-Beltrami spectral filtering. (arXiv:2010.13269v1 [cs.LG])</h2>
<h3>Shih-Gu Huang, Moo K. Chung, Anqi Qiu, Alzheimer&#x27;s Disease Neuroimaging Initiative</h3>
<p>This paper revisits spectral graph convolutional neural networks (graph-CNNs)
given in Defferrard (2016) and develops the Laplace-Beltrami CNN (LB-CNN) by
replacing the graph Laplacian with the LB operator. We then define spectral
filters via the LB operator on a graph. We explore the feasibility of
Chebyshev, Laguerre, and Hermite polynomials to approximate LB-based spectral
filters and define an update of the LB operator for pooling in the LBCNN. We
employ the brain image data from Alzheimer's Disease Neuroimaging Initiative
(ADNI) and demonstrate the use of the proposed LB-CNN. Based on the cortical
thickness of the ADNI dataset, we showed that the LB-CNN didn't improve
classification accuracy compared to the spectral graph-CNN. The three
polynomials had a similar computational cost and showed comparable
classification accuracy in the LB-CNN or spectral graph-CNN. Our findings
suggest that even though the shapes of the three polynomials are different,
deep learning architecture allows us to learn spectral filters such that the
classification performance is not dependent on the type of the polynomials or
the operators (graph Laplacian and LB operator).
</p>
<a href="http://arxiv.org/abs/2010.13269" target="_blank">arXiv:2010.13269</a> [<a href="http://arxiv.org/pdf/2010.13269" target="_blank">pdf</a>]

<h2>Theory-Oriented Deep Leakage from Gradients via Linear Equation Solver. (arXiv:2010.13356v1 [cs.CR])</h2>
<h3>Xudong Pan, Mi Zhang, Yifan Yan, Jiaming Zhu, Min Yang</h3>
<p>In this paper, we take a theory-oriented approach to systematically study the
privacy properties of gradients from a broad class of neural networks with
rectified linear units (ReLU), probably the most popular activation function
used in current deep learning practices. By utilizing some intrinsic properties
of neural networks with ReLU, we prove the existence of exclusively activated
neurons is critical to the separability of the activation patterns of different
samples. Intuitively, an activation pattern is like the fingerprint of the
corresponding sample during the training process. With the separated activation
patterns, we for the first time show the equivalence of data reconstruction
attacks with a sparse linear equation system.

In practice, we propose a novel data reconstruction attack on fully-connected
neural networks and extend the attack to more commercial convolutional neural
network architectures. Our systematic evaluations cover more than $10$
representative neural network architectures (e.g., GoogLeNet, VGGNet and $6$
more), on various real-world scenarios related with healthcare, medical
imaging, location, face recognition and shopping behaviors. In the majority of
test cases, our proposed attack is able to infer ground-truth labels in the
training batch with near $100\%$ accuracy, reconstruct the input data to
fully-connected neural networks with lower than $10^{-6}$ MSE error, and
provide better reconstruction results on both shallow and deep convolutional
neural networks than previous attacks.
</p>
<a href="http://arxiv.org/abs/2010.13356" target="_blank">arXiv:2010.13356</a> [<a href="http://arxiv.org/pdf/2010.13356" target="_blank">pdf</a>]

<h2>Integrating end-to-end neural and clustering-based diarization: Getting the best of both worlds. (arXiv:2010.13366v1 [eess.AS])</h2>
<h3>Keisuke Kinoshita, Marc Delcroix, Naohiro Tawara</h3>
<p>Recent diarization technologies can be categorized into two approaches, i.e.,
clustering and end-to-end neural approaches, which have different pros and
cons. The clustering-based approaches assign speaker labels to speech regions
by clustering speaker embeddings such as x-vectors. While it can be seen as a
current state-of-the-art approach that works for various challenging data with
reasonable robustness and accuracy, it has a critical disadvantage that it
cannot handle overlapped speech that is inevitable in natural conversational
data. In contrast, the end-to-end neural diarization (EEND), which directly
predicts diarization labels using a neural network, was devised to handle the
overlapped speech. While the EEND, which can easily incorporate emerging
deep-learning technologies, has started outperforming the x-vector clustering
approach in some realistic database, it is difficult to make it work for `long'
recordings (e.g., recordings longer than 10 minutes) because of, e.g., its huge
memory consumption. Block-wise independent processing is also difficult because
it poses an inter-block label permutation problem, i.e., an ambiguity of the
speaker label assignments between blocks. In this paper, we propose a simple
but effective hybrid diarization framework that works with overlapped speech
and for long recordings containing an arbitrary number of speakers. It modifies
the conventional EEND framework to simultaneously output global speaker
embeddings so that speaker clustering can be performed across blocks to solve
the permutation problem. With experiments based on simulated noisy reverberant
2-speaker meeting-like data, we show that the proposed framework works
significantly better than the original EEND especially when the input data is
long.
</p>
<a href="http://arxiv.org/abs/2010.13366" target="_blank">arXiv:2010.13366</a> [<a href="http://arxiv.org/pdf/2010.13366" target="_blank">pdf</a>]

<h2>Behavioral decision-making for urban autonomous driving in the presence of pedestrians using Deep Recurrent Q-Network. (arXiv:2010.13407v1 [cs.NE])</h2>
<h3>Niranjan Deshpande (CHROMA), Dominique Vaufreydaz (LIG), Anne Spalanzani (CHROMA)</h3>
<p>Decision making for autonomous driving in urban environments is challenging
due to the complexity of the road structure and the uncertainty in the behavior
of diverse road users. Traditional methods consist of manually designed rules
as the driving policy, which require expert domain knowledge, are difficult to
generalize and might give sub-optimal results as the environment gets complex.
Whereas, using reinforcement learning, optimal driving policy could be learned
and improved automatically through several interactions with the environment.
However, current research in the field of reinforcement learning for autonomous
driving is mainly focused on highway setup with little to no emphasis on urban
environments. In this work, a deep reinforcement learning based decision-making
approach for high-level driving behavior is proposed for urban environments in
the presence of pedestrians. For this, the use of Deep Recurrent Q-Network
(DRQN) is explored, a method combining state-of-the art Deep Q-Network (DQN)
with a long term short term memory (LSTM) layer helping the agent gain a memory
of the environment. A 3-D state representation is designed as the input
combined with a well defined reward function to train the agent for learning an
appropriate behavior policy in a real-world like urban simulator. The proposed
method is evaluated for dense urban scenarios and compared with a rule-based
approach and results show that the proposed DRQN based driving behavior
decision maker outperforms the rule-based approach.
</p>
<a href="http://arxiv.org/abs/2010.13407" target="_blank">arXiv:2010.13407</a> [<a href="http://arxiv.org/pdf/2010.13407" target="_blank">pdf</a>]

<h2>High Acceleration Reinforcement Learning for Real-World Juggling with Binary Rewards. (arXiv:2010.13483v1 [cs.RO])</h2>
<h3>Kai Ploeger, Michael Lutter, Jan Peters</h3>
<p>Robots that can learn in the physical world will be important to en-able
robots to escape their stiff and pre-programmed movements. For dynamic
high-acceleration tasks, such as juggling, learning in the real-world is
particularly challenging as one must push the limits of the robot and its
actuation without harming the system, amplifying the necessity of sample
efficiency and safety for robot learning algorithms. In contrast to prior work
which mainly focuses on the learning algorithm, we propose a learning system,
that directly incorporates these requirements in the design of the policy
representation, initialization, and optimization. We demonstrate that this
system enables the high-speed Barrett WAM manipulator to learn juggling two
balls from 56 minutes of experience with a binary reward signal. The final
policy juggles continuously for up to 33 minutes or about 4500 repeated
catches. The videos documenting the learning process and the evaluation can be
found at https://sites.google.com/view/jugglingbot
</p>
<a href="http://arxiv.org/abs/2010.13483" target="_blank">arXiv:2010.13483</a> [<a href="http://arxiv.org/pdf/2010.13483" target="_blank">pdf</a>]

<h2>Scalable Bayesian neural networks by layer-wise input augmentation. (arXiv:2010.13498v1 [stat.ML])</h2>
<h3>Trung Trinh, Samuel Kaski, Markus Heinonen</h3>
<p>We introduce implicit Bayesian neural networks, a simple and scalable
approach for uncertainty representation in deep learning. Standard Bayesian
approach to deep learning requires the impractical inference of the posterior
distribution over millions of parameters. Instead, we propose to induce a
distribution that captures the uncertainty over neural networks by augmenting
each layer's inputs with latent variables. We present appropriate input
distributions and demonstrate state-of-the-art performance in terms of
calibration, robustness and uncertainty characterisation over large-scale,
multi-million parameter image classification tasks.
</p>
<a href="http://arxiv.org/abs/2010.13498" target="_blank">arXiv:2010.13498</a> [<a href="http://arxiv.org/pdf/2010.13498" target="_blank">pdf</a>]

<h2>An Efficient Newton Method for Extreme Similarity Learning with Nonlinear Embeddings. (arXiv:2010.13511v1 [stat.ML])</h2>
<h3>Bowen Yuan, Yu-Sheng Li, Pengrui Quan, Chih-Jen Lin</h3>
<p>We study the problem of learning similarity by using nonlinear embedding
models (e.g., neural networks) from all possible pairs. This problem is
well-known for its difficulty of training with the extreme number of pairs.
Existing optimization methods extended from stochastic gradient methods suffer
from slow convergence and high complexity per pass of all possible pairs.
Inspired by some recent works reporting that Newton methods are competitive for
training certain types of neural networks, in this work, we novelly apply the
Newton method for this problem. A prohibitive cost depending on the extreme
number of pairs occurs if the Newton method is directly applied. We propose an
efficient algorithm which successfully eliminates the cost. Our proposed
algorithm can take advantage of second-order information and lower time
complexity per pass of all possible pairs. Experiments conducted on large-scale
data sets demonstrate that the proposed algorithm is more efficient than
existing algorithms.
</p>
<a href="http://arxiv.org/abs/2010.13511" target="_blank">arXiv:2010.13511</a> [<a href="http://arxiv.org/pdf/2010.13511" target="_blank">pdf</a>]

<h2>Kernel Smoothing, Mean Shift, and Their Learning Theory with Directional Data. (arXiv:2010.13523v1 [stat.ML])</h2>
<h3>Yikun Zhang (1), Yen-Chi Chen (2) ((1) University of Washington, Seattle, (2) Department of Statistics, University of Washington, Seattle)</h3>
<p>Directional data consist of observations distributed on a (hyper)sphere, and
appear in many applied fields, such as astronomy, ecology, and environmental
science. This paper studies both statistical and computational problems of
kernel smoothing for directional data. We generalize the classical mean shift
algorithm to directional data, which allows us to identify local modes of the
directional kernel density estimator (KDE). The statistical convergence rates
of the directional KDE and its derivatives are derived, and the problem of mode
estimation is examined. We also prove the ascending property of our directional
mean shift algorithm and investigate a general problem of gradient ascent on
the unit hypersphere. To demonstrate the applicability of our proposed
algorithm, we evaluate it as a mode clustering method on both simulated and
real-world datasets.
</p>
<a href="http://arxiv.org/abs/2010.13523" target="_blank">arXiv:2010.13523</a> [<a href="http://arxiv.org/pdf/2010.13523" target="_blank">pdf</a>]

<h2>Robust Disentanglement of a Few Factors at a Time. (arXiv:2010.13527v1 [cs.LG])</h2>
<h3>Benjamin Estermann, Markus Marks, Mehmet Fatih Yanik</h3>
<p>Disentanglement is at the forefront of unsupervised learning, as disentangled
representations of data improve generalization, interpretability, and
performance in downstream tasks. Current unsupervised approaches remain
inapplicable for real-world datasets since they are highly variable in their
performance and fail to reach levels of disentanglement of (semi-)supervised
approaches. We introduce population-based training (PBT) for improving
consistency in training variational autoencoders (VAEs) and demonstrate the
validity of this approach in a supervised setting (PBT-VAE). We then use
Unsupervised Disentanglement Ranking (UDR) as an unsupervised heuristic to
score models in our PBT-VAE training and show how models trained this way tend
to consistently disentangle only a subset of the generative factors. Building
on top of this observation we introduce the recursive rPU-VAE approach. We
train the model until convergence, remove the learned factors from the dataset
and reiterate. In doing so, we can label subsets of the dataset with the
learned factors and consecutively use these labels to train one model that
fully disentangles the whole dataset. With this approach, we show striking
improvement in state-of-the-art unsupervised disentanglement performance and
robustness across multiple datasets and metrics.
</p>
<a href="http://arxiv.org/abs/2010.13527" target="_blank">arXiv:2010.13527</a> [<a href="http://arxiv.org/pdf/2010.13527" target="_blank">pdf</a>]

<h2>From the Expectation Maximisation Algorithm to Autoencoded Variational Bayes. (arXiv:2010.13551v1 [stat.ML])</h2>
<h3>Graham W. Pulford</h3>
<p>Although the expectation maximisation (EM) algorithm was introduced in 1970,
it remains somewhat inaccessible to machine learning practitioners due to its
obscure notation, terse proofs and lack of concrete links to modern machine
learning techniques like autoencoded variational Bayes. This has resulted in
gaps in the AI literature concerning the meaning of such concepts like ``latent
variables'' and ``variational lower bound,'' which are frequently used but
often not clearly explained. The roots of these ideas lie in the EM algorithm.
We first give a tutorial presentation of the EM algorithm for estimating the
parameters of a $K$-component mixture density. The Gaussian mixture case is
presented in detail using $K$-ary scalar hidden (or latent) variables rather
than the more traditional binary valued $K$-dimenional vectors. This
presentation is motivated by mixture modelling from the target tracking
literature. In a similar style to Bishop's 2009 book, we present variational
Bayesian inference as a generalised EM algorithm stemming from the variational
(or evidential) lower bound, as well as the technique of mean field
approximation (or product density transform). We continue the evolution from EM
to variational autoencoders, developed by Kingma &amp; Welling in 2014. In so
doing, we establish clear links between the EM algorithm and its variational
counterparts, hence clarifying the meaning of ``latent variables.'' We provide
a detailed coverage of the ``reparametrisation trick'' and focus on how the
AEVB differs from conventional variational Bayesian inference. Throughout the
tutorial, consistent notational conventions are used. This unifies the
narrative and clarifies the concepts. Some numerical examples are given to
further illustrate the algorithms.
</p>
<a href="http://arxiv.org/abs/2010.13551" target="_blank">arXiv:2010.13551</a> [<a href="http://arxiv.org/pdf/2010.13551" target="_blank">pdf</a>]

<h2>Forethought and Hindsight in Credit Assignment. (arXiv:2010.13685v1 [cs.LG])</h2>
<h3>Veronica Chelu, Doina Precup, Hado van Hasselt</h3>
<p>We address the problem of credit assignment in reinforcement learning and
explore fundamental questions regarding the way in which an agent can best use
additional computation to propagate new information, by planning with internal
models of the world to improve its predictions. Particularly, we work to
understand the gains and peculiarities of planning employed as forethought via
forward models or as hindsight operating with backward models. We establish the
relative merits, limitations and complementary properties of both planning
mechanisms in carefully constructed scenarios. Further, we investigate the best
use of models in planning, primarily focusing on the selection of states in
which predictions should be (re)-evaluated. Lastly, we discuss the issue of
model estimation and highlight a spectrum of methods that stretch from explicit
environment-dynamics predictors to more abstract planner-aware models.
</p>
<a href="http://arxiv.org/abs/2010.13685" target="_blank">arXiv:2010.13685</a> [<a href="http://arxiv.org/pdf/2010.13685" target="_blank">pdf</a>]

<h2>Meaningful uncertainties from deep neural network surrogates of large-scale numerical simulations. (arXiv:2010.13749v1 [stat.ML])</h2>
<h3>Gemma J. Anderson, Jim A. Gaffney, Brian K. Spears, Peer-Timo Bremer, Rushil Anirudh, Jayaraman J. Thiagarajan</h3>
<p>Large-scale numerical simulations are used across many scientific disciplines
to facilitate experimental development and provide insights into underlying
physical processes, but they come with a significant computational cost. Deep
neural networks (DNNs) can serve as highly-accurate surrogate models, with the
capacity to handle diverse datatypes, offering tremendous speed-ups for
prediction and many other downstream tasks. An important use-case for these
surrogates is the comparison between simulations and experiments; prediction
uncertainty estimates are crucial for making such comparisons meaningful, yet
standard DNNs do not provide them. In this work we define the fundamental
requirements for a DNN to be useful for scientific applications, and
demonstrate a general variational inference approach to equip predictions of
scalar and image data from a DNN surrogate model trained on inertial
confinement fusion simulations with calibrated Bayesian uncertainties.
Critically, these uncertainties are interpretable, meaningful and preserve
physics-correlations in the predicted quantities.
</p>
<a href="http://arxiv.org/abs/2010.13749" target="_blank">arXiv:2010.13749</a> [<a href="http://arxiv.org/pdf/2010.13749" target="_blank">pdf</a>]

<h2>Enforcing Interpretability and its Statistical Impacts: Trade-offs between Accuracy and Interpretability. (arXiv:2010.13764v1 [cs.LG])</h2>
<h3>Gintare Karolina Dziugaite, Shai Ben-David, Daniel M. Roy</h3>
<p>To date, there has been no formal study of the statistical cost of
interpretability in machine learning. As such, the discourse around potential
trade-offs is often informal and misconceptions abound. In this work, we aim to
initiate a formal study of these trade-offs. A seemingly insurmountable
roadblock is the lack of any agreed upon definition of interpretability.
Instead, we propose a shift in perspective. Rather than attempt to define
interpretability, we propose to model the \emph{act} of \emph{enforcing}
interpretability. As a starting point, we focus on the setting of empirical
risk minimization for binary classification, and view interpretability as a
constraint placed on learning. That is, we assume we are given a subset of
hypothesis that are deemed to be interpretable, possibly depending on the data
distribution and other aspects of the context. We then model the act of
enforcing interpretability as that of performing empirical risk minimization
over the set of interpretable hypotheses. This model allows us to reason about
the statistical implications of enforcing interpretability, using known results
in statistical learning theory. Focusing on accuracy, we perform a case
analysis, explaining why one may or may not observe a trade-off between
accuracy and interpretability when the restriction to interpretable classifiers
does or does not come at the cost of some excess statistical risk. We close
with some worked examples and some open problems, which we hope will spur
further theoretical development around the tradeoffs involved in
interpretability.
</p>
<a href="http://arxiv.org/abs/2010.13764" target="_blank">arXiv:2010.13764</a> [<a href="http://arxiv.org/pdf/2010.13764" target="_blank">pdf</a>]

<h2>Structural Agnostic Modeling: Adversarial Learning of Causal Graphs. (arXiv:1803.04929v3 [stat.ML] UPDATED)</h2>
<h3>Diviyan Kalainathan, Olivier Goudet, Isabelle Guyon, David Lopez-Paz, Mich&#xe8;le Sebag</h3>
<p>A new causal discovery method, Structural Agnostic Modeling (SAM), is
presented in this paper. Leveraging both conditional independencies and
distributional asymmetries in the data, SAM aims to find the underlying causal
structure from observational data. The approach is based on a game between
different players estimating each variable distribution conditionally to the
others as a neural net, and an adversary aimed at discriminating the overall
joint conditional distribution, and that of the original data. A learning
criterion combining distribution estimation, sparsity and acyclicity
constraints is used to enforce the end-to-end optimization of the graph
structure and parameters through stochastic gradient descent. Besides a
theoretical analysis of the approach in the large sample limit, SAM is
extensively experimentally validated on synthetic and real data.
</p>
<a href="http://arxiv.org/abs/1803.04929" target="_blank">arXiv:1803.04929</a> [<a href="http://arxiv.org/pdf/1803.04929" target="_blank">pdf</a>]

<h2>Robust and Efficient Semi-Supervised Estimation of Average Treatment Effects with Application to Electronic Health Records Data. (arXiv:1804.00195v2 [stat.ME] UPDATED)</h2>
<h3>David Cheng, Ashwin Ananthakrishnan, Tianxi Cai</h3>
<p>We consider the problem of estimating the average treatment effect (ATE) in a
semi-supervised learning setting, where a very small proportion of the entire
set of observations are labeled with the true outcome but features predictive
of the outcome are available among all observations. This problem arises, for
example, when estimating treatment effects in electronic health records (EHR)
data because gold-standard outcomes are often not directly observable from the
records but are observed for a limited number of patients through small-scale
manual chart review. We develop an imputation-based approach for estimating the
ATE that is robust to misspecification of the imputation model. This
effectively allows information from the predictive features to be safely
leveraged to improve efficiency in estimating the ATE. The estimator is
additionally doubly-robust in that it is consistent under correct specification
of either an initial propensity score model or a baseline outcome model. It is
also locally semiparametric efficient under an ideal semi-supervised model
where the distribution of the unlabeled data is known. Simulations exhibit the
efficiency and robustness of the proposed method compared to existing
approaches in finite samples.We illustrate the method by comparing rates of
treatment response to two biologic agents for treatment inflammatory bowel
disease using EHR data from Partner's Healthcare.
</p>
<a href="http://arxiv.org/abs/1804.00195" target="_blank">arXiv:1804.00195</a> [<a href="http://arxiv.org/pdf/1804.00195" target="_blank">pdf</a>]

<h2>Modelling spine locations on dendrite trees using inhomogeneous Cox point processes. (arXiv:1907.12283v3 [stat.ME] UPDATED)</h2>
<h3>Heidi S. Christensen, Jesper M&#xf8;ller</h3>
<p>Dendritic spines, which are small protrusions on the dendrites of a neuron,
are of interest in neuroscience as they are related to cognitive processes such
as learning and memory. We analyse the distribution of spine locations on six
different dendrite trees from mouse neurons using point process theory for
linear networks. Besides some possible small-scale repulsion, { we find that
two of the spine point pattern data sets may be described by inhomogeneous
Poisson process models}, while the other point pattern data sets exhibit
clustering between spines at a larger scale. To model this we propose an
inhomogeneous Cox process model constructed by thinning a Poisson process on a
linear network with retention probabilities determined by a spatially
correlated random field. For model checking we consider network analogues of
the empirical $F$-, $G$-, and $J$-functions originally introduced for
inhomogeneous point processes on a Euclidean space. The fitted Cox process
models seem to catch the clustering of spine locations between spines, but also
posses a large variance in the number of points for some of the data sets
causing large confidence regions for the empirical $F$- and $G$-functions.
</p>
<a href="http://arxiv.org/abs/1907.12283" target="_blank">arXiv:1907.12283</a> [<a href="http://arxiv.org/pdf/1907.12283" target="_blank">pdf</a>]

<h2>Estimating NBA players salary share according to their performance on court: A machine learning approach. (arXiv:2007.14694v2 [stat.AP] UPDATED)</h2>
<h3>Ioanna Papadaki, Michail Tsagris</h3>
<p>It is customary for researchers and practitioners to fit linear models in
order to predict NBA player's salary based on the players' performance on
court. On the contrary, we focus on the players salary share (with regards to
the team payroll) by first selecting the most important determinants or
statistics (years of experience in the league, games played, etc.) and then
utilise them to predict the player salaries by employing a non linear Random
Forest machine learning algorithm. We externally evaluate our salary
predictions, thus we avoid the phenomenon of over-fitting observed in most
papers. Overall, using data from three distinct periods, 2017-2019 we identify
the important factors that achieve very satisfactory salary predictions and we
draw useful conclusions.
</p>
<a href="http://arxiv.org/abs/2007.14694" target="_blank">arXiv:2007.14694</a> [<a href="http://arxiv.org/pdf/2007.14694" target="_blank">pdf</a>]

<h2>Estimating heterogeneous survival treatment effect in observational data using machine learning. (arXiv:2008.07044v2 [stat.AP] UPDATED)</h2>
<h3>Liangyuan Hu, Jiayi Ji, Fan Li</h3>
<p>Methods for estimating heterogeneous treatment effect in observational data
have largely focused on continuous or binary outcomes, and have been relatively
less vetted with survival outcomes. Using flexible machine learning methods in
the counterfactual framework is a promising approach to address challenges due
to complex individual characteristics, to which treatments need to be tailored.
To evaluate the operating characteristics of recent survival machine learning
methods for the estimation of TEH and inform better practice, we carry out a
comprehensive simulation study representing a variety of confounded
heterogeneous survival treatment effect settings and varying degrees of
covariate overlap. Our results indicate that the nonparametric Bayesian
Additive Regression Trees within the framework of accelerated failure time
model (AFT-BART-NP) consistently carries the best performance, both in terms of
bias and precision. Moreover, the credible interval estimators from AFT-BART-NP
provide close to nominal frequentist coverage for the individual survival
treatment effect when the covariate overlap is at least moderate. Under lack of
overlap, where accurate estimation of the average treatment effect becomes
challenging, the credible intervals from AFT-BART-NP still provide nominal
frequentist coverage among units near the centroid of the propensity score
distribution. Finally, we demonstrate the application of these machine learning
methods through a comprehensive case study examining the heterogeneous survival
effects of two radiotherapy approaches for localized high-risk prostate cancer.
</p>
<a href="http://arxiv.org/abs/2008.07044" target="_blank">arXiv:2008.07044</a> [<a href="http://arxiv.org/pdf/2008.07044" target="_blank">pdf</a>]

<h2>KrigHedge: GP Surrogates for Delta Hedging. (arXiv:2010.08407v2 [q-fin.CP] UPDATED)</h2>
<h3>Mike Ludkovski, Yuri Saporito</h3>
<p>We investigate a machine learning approach to option Greeks approximation
based on Gaussian process (GP) surrogates. The method takes in noisily observed
option prices, fits a nonparametric input-output map and then analytically
differentiates the latter to obtain the various price sensitivities. Our
motivation is to compute Greeks in cases where direct computation is expensive,
such as in local volatility models, or can only ever be done approximately. We
provide a detailed analysis of numerous aspects of GP surrogates, including
choice of kernel family, simulation design, choice of trend function and impact
of noise.

We further discuss the application to Delta hedging, including a new Lemma
that relates quality of the Delta approximation to discrete-time hedging loss.
Results are illustrated with two extensive case studies that consider
estimation of Delta, Theta and Gamma and benchmark approximation quality and
uncertainty quantification using a variety of statistical metrics. Among our
key take-aways are the recommendation to use Matern kernels, the benefit of
including virtual training points to capture boundary conditions, and the
significant loss of fidelity when training on stock-path-based datasets.
</p>
<a href="http://arxiv.org/abs/2010.08407" target="_blank">arXiv:2010.08407</a> [<a href="http://arxiv.org/pdf/2010.08407" target="_blank">pdf</a>]

