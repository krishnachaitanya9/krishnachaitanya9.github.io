---
title: Latest Deep Learning Papers
date: 2020-10-25 21:00:01 +0800
featured-img: DL-logo
categories: [Deep_Learning]
tags: [Deep_Learning]
mathjax: Yes
---

<h1>Your arXiv Feed</h1>
<h2>Reinforcement Learning with Combinatorial Actions: An Application to Vehicle Routing. (arXiv:2010.12001v1 [cs.LG])</h2>
<h3>Arthur Delarue, Ross Anderson, Christian Tjandraatmadja</h3>
<p>Value-function-based methods have long played an important role in
reinforcement learning. However, finding the best next action given a value
function of arbitrary complexity is nontrivial when the action space is too
large for enumeration. We develop a framework for value-function-based deep
reinforcement learning with a combinatorial action space, in which the action
selection problem is explicitly formulated as a mixed-integer optimization
problem. As a motivating example, we present an application of this framework
to the capacitated vehicle routing problem (CVRP), a combinatorial optimization
problem in which a set of locations must be covered by a single vehicle with
limited capacity. On each instance, we model an action as the construction of a
single route, and consider a deterministic policy which is improved through a
simple policy iteration algorithm. Our approach is competitive with other
reinforcement learning methods and achieves an average gap of 1.7% with
state-of-the-art OR methods on standard library instances of medium size.
</p>
<a href="http://arxiv.org/abs/2010.12001" target="_blank">arXiv:2010.12001</a> [<a href="http://arxiv.org/pdf/2010.12001" target="_blank">pdf</a>]

<h2>Channel Estimation for Full-Duplex RIS-assisted HAPS Backhauling with Graph Attention Networks. (arXiv:2010.12004v1 [cs.IT])</h2>
<h3>K&#xfc;r&#x15f;at Tekb&#x131;y&#x131;k, G&#xfc;ne&#x15f; Karabulut Kurt, Chongwen Huang, Ali R&#x131;za Ekti, Halim Yanikomeroglu</h3>
<p>In this paper, the graph attention network (GAT) is firstly utilized for the
channel estimation. In accordance with the 6G expectations, we consider a
high-altitude platform station (HAPS) mounted reconfigurable intelligent
surface-assisted two-way communications and obtain a low overhead and a high
normalized mean square error performance. The performance of the proposed
method is investigated on the two-way backhauling link over the RIS-integrated
HAPS. The simulation results denote that the GAT estimator overperforms the
least square in full-duplex channel estimation. Contrary to the previously
introduced methods, GAT at one of the nodes can separately estimate the
cascaded channel coefficients. Thus, there is no need to use time-division
duplex mode during pilot signaling in full-duplex communication. Moreover, it
is shown that the GAT estimator is robust to hardware imperfections and changes
in small scale fading characteristics even if the training data do not include
all these variations.
</p>
<a href="http://arxiv.org/abs/2010.12004" target="_blank">arXiv:2010.12004</a> [<a href="http://arxiv.org/pdf/2010.12004" target="_blank">pdf</a>]

<h2>Regret Bounds without Lipschitz Continuity: Online Learning with Relative-Lipschitz Losses. (arXiv:2010.12033v1 [cs.LG])</h2>
<h3>Yihan Zhou, Victor S. Portella, Mark Schmidt, Nicholas J. A. Harvey</h3>
<p>In online convex optimization (OCO), Lipschitz continuity of the functions is
commonly assumed in order to obtain sublinear regret. Moreover, many algorithms
have only logarithmic regret when these functions are also strongly convex.
Recently, researchers from convex optimization proposed the notions of
"relative Lipschitz continuity" and "relative strong convexity". Both of the
notions are generalizations of their classical counterparts. It has been shown
that subgradient methods in the relative setting have performance analogous to
their performance in the classical setting.

In this work, we consider OCO for relative Lipschitz and relative strongly
convex functions. We extend the known regret bounds for classical OCO
algorithms to the relative setting. Specifically, we show regret bounds for the
follow the regularized leader algorithms and a variant of online mirror
descent. Due to the generality of these methods, these results yield regret
bounds for a wide variety of OCO algorithms. Furthermore, we further extend the
results to algorithms with extra regularization such as regularized dual
averaging.
</p>
<a href="http://arxiv.org/abs/2010.12033" target="_blank">arXiv:2010.12033</a> [<a href="http://arxiv.org/pdf/2010.12033" target="_blank">pdf</a>]

<h2>Recovery of sparse linear classifiers from mixture of responses. (arXiv:2010.12087v1 [stat.ML])</h2>
<h3>Venkata Gandikota, Arya Mazumdar, Soumyabrata Pal</h3>
<p>In the problem of learning a {\em mixture of linear classifiers}, the aim is
to learn a collection of hyperplanes from a sequence of binary responses. Each
response is a result of querying with a vector and indicates the side of a
randomly chosen hyperplane from the collection the query vector belong to. This
model provides a rich representation of heterogeneous data with categorical
labels and has only been studied in some special settings. We look at a
hitherto unstudied problem of query complexity upper bound of recovering all
the hyperplanes, especially for the case when the hyperplanes are sparse. This
setting is a natural generalization of the extreme quantization problem known
as 1-bit compressed sensing. Suppose we have a set of $\ell$ unknown $k$-sparse
vectors. We can query the set with another vector $\boldsymbol{a}$, to obtain
the sign of the inner product of $\boldsymbol{a}$ and a randomly chosen vector
from the $\ell$-set. How many queries are sufficient to identify all the $\ell$
unknown vectors? This question is significantly more challenging than both the
basic 1-bit compressed sensing problem (i.e., $\ell=1$ case) and the analogous
regression problem (where the value instead of the sign is provided). We
provide rigorous query complexity results (with efficient algorithms) for this
problem.
</p>
<a href="http://arxiv.org/abs/2010.12087" target="_blank">arXiv:2010.12087</a> [<a href="http://arxiv.org/pdf/2010.12087" target="_blank">pdf</a>]

<h2>Adaptive extra-gradient methods for min-max optimization and games. (arXiv:2010.12100v1 [math.OC])</h2>
<h3>Kimon Antonakopoulos, E. Veronica Belmega, Panayotis Mertikopoulos</h3>
<p>We present a new family of min-max optimization algorithms that automatically
exploit the geometry of the gradient data observed at earlier iterations to
perform more informative extra-gradient steps in later ones. Thanks to this
adaptation mechanism, the proposed methods automatically detect the problem's
smoothness properties, without requiring any prior tuning by the optimizer. As
a result, the algorithm simultaneously achieves the optimal smooth/non-smooth
min-max convergence rates, i.e., it converges to an $\varepsilon$-optimal
solution within $\mathcal{O}(1/\varepsilon)$ iterations in smooth problems, and
within $\mathcal{O}(1/\varepsilon^2)$ iterations in non-smooth ones.
Importantly, these guarantees do not require any of the standard boundedness or
Lipschitz continuity conditions that are typically assumed in the literature;
in particular, they apply even to problems with divergent loss functions (such
as log-likelihood learning and the like). This "off-the-shelf" adaptation is
achieved through the use of a geometric apparatus based on Finsler metrics and
a suitably chosen mirror-prox template that allows us to derive sharp
convergence rates for the family of methods at hand.
</p>
<a href="http://arxiv.org/abs/2010.12100" target="_blank">arXiv:2010.12100</a> [<a href="http://arxiv.org/pdf/2010.12100" target="_blank">pdf</a>]

<h2>An Inertial Block Majorization Minimization Framework for Nonsmooth Nonconvex Optimization. (arXiv:2010.12133v1 [math.OC])</h2>
<h3>Le Thi Khanh Hien, Duy Nhat Phan, Nicolas Gillis</h3>
<p>In this paper, we introduce TITAN, a novel inerTial block majorIzation
minimization framework for non-smooth non-convex opTimizAtioN problems. TITAN
is a block coordinate method (BCM) that embeds inertial force to each
majorization-minimization step of the block updates. The inertial force is
obtained via an extrapolation operator that subsumes heavy-ball and
Nesterov-type accelerations for block proximal gradient methods as special
cases. By choosing various surrogate functions, such as proximal, Lipschitz
gradient, Bregman, quadratic, and composite surrogate functions, and by varying
the extrapolation operator, TITAN produces a rich set of inertial BCMs. We
study sub-sequential convergence as well as global convergence for the
generated sequence of TITAN. We illustrate the effectiveness of TITAN on two
important machine learning problems, namely sparse non-negative matrix
factorization and matrix completion.
</p>
<a href="http://arxiv.org/abs/2010.12133" target="_blank">arXiv:2010.12133</a> [<a href="http://arxiv.org/pdf/2010.12133" target="_blank">pdf</a>]

<h2>A Feasible Level Proximal Point Method for Nonconvex Sparse Constrained Optimization. (arXiv:2010.12169v1 [math.OC])</h2>
<h3>Digvijay Boob, Qi Deng, Guanghui Lan, Yilin Wang</h3>
<p>Nonconvex sparse models have received significant attention in
high-dimensional machine learning. In this paper, we study a new model
consisting of a general convex or nonconvex objectives and a variety of
continuous nonconvex sparsity-inducing constraints. For this constrained model,
we propose a novel proximal point algorithm that solves a sequence of convex
subproblems with gradually relaxed constraint levels. Each subproblem, having a
proximal point objective and a convex surrogate constraint, can be efficiently
solved based on a fast routine for projection onto the surrogate constraint. We
establish the asymptotic convergence of the proposed algorithm to the
Karush-Kuhn-Tucker (KKT) solutions. We also establish new convergence
complexities to achieve an approximate KKT solution when the objective can be
smooth/nonsmooth, deterministic/stochastic and convex/nonconvex with complexity
that is on a par with gradient descent for unconstrained optimization problems
in respective cases. To the best of our knowledge, this is the first study of
the first-order methods with complexity guarantee for nonconvex
sparse-constrained problems. We perform numerical experiments to demonstrate
the effectiveness of our new model and efficiency of the proposed algorithm for
large scale problems.
</p>
<a href="http://arxiv.org/abs/2010.12169" target="_blank">arXiv:2010.12169</a> [<a href="http://arxiv.org/pdf/2010.12169" target="_blank">pdf</a>]

<h2>Morse shellings on products. (arXiv:2010.12206v1 [math.AG])</h2>
<h3>Jean-Yves Welschinger (AGL)</h3>
<p>We recently defined a property of Morse shellability (and tileability) of
finite simplicial complexes which extends the classical one and its relations
with discrete Morse theory. We now prove that the product of two Morse tileable
or shellable simplicial complexes carries Morse tileable or shellable
triangulations under some tameness condition, and that any tiling or shelling
becomes tame after one barycentric subdivision. We deduce that any finite
product of closed manifolds of dimensions less than four carries Morse
shellable triangulations whose critical and h-vectors are palindromic. We also
prove that the h-vector of a Morse tiling is always palindromic in dimension
less than four or in the case of an h-tiling, provided its critical vector is
palindromic.
</p>
<a href="http://arxiv.org/abs/2010.12206" target="_blank">arXiv:2010.12206</a> [<a href="http://arxiv.org/pdf/2010.12206" target="_blank">pdf</a>]

<h2>Learning to Optimise General TSP Instances. (arXiv:2010.12214v1 [cs.LG])</h2>
<h3>Nasrin Sultana, Jeffrey Chan, A. K. Qin</h3>
<p>The Travelling Salesman Problem (TSP) is a classical combinatorial
optimisation problem. Deep learning has been successfully extended to
meta-learning, where previous solving efforts assist in learning how to
optimise future optimisation instances. In recent years, learning to optimise
approaches have shown success in solving TSP problems. However, they focus on
one type of TSP problem, namely ones where the points are uniformly distributed
in Euclidean spaces and have issues in generalising to other embedding spaces,
e.g., spherical distance spaces, and to TSP instances where the points are
distributed in a non-uniform manner. An aim of learning to optimise is to train
once and solve across a broad spectrum of (TSP) problems. Although supervised
learning approaches have shown to achieve more optimal solutions than
unsupervised approaches, they do require the generation of training data and
running a solver to obtain solutions to learn from, which can be time-consuming
and difficult to find reasonable solutions for harder TSP instances. Hence this
paper introduces a new learning-based approach to solve a variety of different
and common TSP problems that are trained on easier instances which are faster
to train and are easier to obtain better solutions. We name this approach the
non-Euclidean TSP network (NETSP-Net). The approach is evaluated on various TSP
instances using the benchmark TSPLIB dataset and popular instance generator
used in the literature. We performed extensive experiments that indicate our
approach generalises across many types of instances and scales to instances
that are larger than what was used during training.
</p>
<a href="http://arxiv.org/abs/2010.12214" target="_blank">arXiv:2010.12214</a> [<a href="http://arxiv.org/pdf/2010.12214" target="_blank">pdf</a>]

<h2>Exponential ReLU Neural Network Approximation Rates for Point and Edge Singularities. (arXiv:2010.12217v1 [math.NA])</h2>
<h3>Carlo Marcati, Joost A. A. Opschoor, Philipp C. Petersen, Christoph Schwab</h3>
<p>We prove exponential expressivity with stable ReLU Neural Networks (ReLU NNs)
in $H^1(\Omega)$ for weighted analytic function classes in certain polytopal
domains $\Omega$, in space dimension $d=2,3$. Functions in these classes are
locally analytic on open subdomains $D\subset \Omega$, but may exhibit isolated
point singularities in the interior of $\Omega$ or corner and edge
singularities at the boundary $\partial \Omega$. The exponential expression
rate bounds proved here imply uniform exponential expressivity by ReLU NNs of
solution families for several elliptic boundary and eigenvalue problems with
analytic data. The exponential approximation rates are shown to hold in space
dimension $d = 2$ on Lipschitz polygons with straight sides, and in space
dimension $d=3$ on Fichera-type polyhedral domains with plane faces. The
constructive proofs indicate in particular that NN depth and size increase
poly-logarithmically with respect to the target NN approximation accuracy
$\varepsilon&gt;0$ in $H^1(\Omega)$. The results cover in particular solution sets
of linear, second order elliptic PDEs with analytic data and certain nonlinear
elliptic eigenvalue problems with analytic nonlinearities and singular,
weighted analytic potentials as arise in electron structure models. In the
latter case, the functions correspond to electron densities that exhibit
isolated point singularities at the positions of the nuclei. Our findings
provide in particular mathematical foundation of recently reported, successful
uses of deep neural networks in variational electron structure algorithms.
</p>
<a href="http://arxiv.org/abs/2010.12217" target="_blank">arXiv:2010.12217</a> [<a href="http://arxiv.org/pdf/2010.12217" target="_blank">pdf</a>]

<h2>Learning from missing data with the Latent Block Model. (arXiv:2010.12222v1 [cs.LG])</h2>
<h3>Gabriel Frisch (Heudiasyc), Jean-Benoist L&#xe9;ger (Heudiasyc), Yves Grandvalet (Heudiasyc)</h3>
<p>Missing data can be informative. Ignoring this information can lead to
misleading conclusions when the data model does not allow information to be
extracted from the missing data. We propose a co-clustering model, based on the
Latent Block Model, that aims to take advantage of this nonignorable
nonresponses, also known as Missing Not At Random data (MNAR). A variational
expectation-maximization algorithm is derived to perform inference and a model
selection criterion is presented. We assess the proposed approach on a
simulation study, before using our model on the voting records from the lower
house of the French Parliament, where our analysis brings out relevant groups
of MPs and texts, together with a sensible interpretation of the behavior of
non-voters.
</p>
<a href="http://arxiv.org/abs/2010.12222" target="_blank">arXiv:2010.12222</a> [<a href="http://arxiv.org/pdf/2010.12222" target="_blank">pdf</a>]

<h2>Throughput-Optimal Topology Design for Cross-Silo Federated Learning. (arXiv:2010.12229v1 [cs.LG])</h2>
<h3>Othmane Marfoq, Chuan Xu, Giovanni Neglia, Richard Vidal</h3>
<p>Federated learning usually employs a client-server architecture where an
orchestrator iteratively aggregates model updates from remote clients and
pushes them back a refined model. This approach may be inefficient in
cross-silo settings, as close-by data silos with high-speed access links may
exchange information faster than with the orchestrator, and the orchestrator
may become a communication bottleneck. In this paper we define the problem of
topology design for cross-silo federated learning using the theory of max-plus
linear systems to compute the system throughput---number of communication
rounds per time unit. We also propose practical algorithms that, under the
knowledge of measurable network characteristics, find a topology with the
largest throughput or with provable throughput guarantees. In realistic
Internet networks with 10 Gbps access links for silos, our algorithms speed up
training by a factor 9 and 1.5 in comparison to the master-slave architecture
and to state-of-the-art MATCHA, respectively. Speedups are even larger with
slower access links.
</p>
<a href="http://arxiv.org/abs/2010.12229" target="_blank">arXiv:2010.12229</a> [<a href="http://arxiv.org/pdf/2010.12229" target="_blank">pdf</a>]

<h2>Coping with Label Shift via Distributionally Robust Optimisation. (arXiv:2010.12230v1 [cs.LG])</h2>
<h3>Jingzhao Zhang, Aditya Menon, Andreas Veit, Srinadh Bhojanapalli, Sanjiv Kumar, Suvrit Sra</h3>
<p>The label shift problem refers to the supervised learning setting where the
train and test label distributions do not match. Existing work addressing label
shift usually assumes access to an \emph{unlabelled} test sample. This sample
may be used to estimate the test label distribution, and to then train a
suitably re-weighted classifier. While approaches using this idea have proven
effective, their scope is limited as it is not always feasible to access the
target domain; further, they require repeated retraining if the model is to be
deployed in \emph{multiple} test environments. Can one instead learn a
\emph{single} classifier that is robust to arbitrary label shifts from a broad
family? In this paper, we answer this question by proposing a model that
minimises an objective based on distributionally robust optimisation (DRO). We
then design and analyse a gradient descent-proximal mirror ascent algorithm
tailored for large-scale problems to optimise the proposed objective. %, and
establish its convergence. Finally, through experiments on CIFAR-100 and
ImageNet, we show that our technique can significantly improve performance over
a number of baselines in settings where label shift is present.
</p>
<a href="http://arxiv.org/abs/2010.12230" target="_blank">arXiv:2010.12230</a> [<a href="http://arxiv.org/pdf/2010.12230" target="_blank">pdf</a>]

<h2>Graph-Homomorphic Perturbations for Private Decentralized Learning. (arXiv:2010.12288v1 [cs.LG])</h2>
<h3>Stefan Vlaski, Ali H. Sayed</h3>
<p>Decentralized algorithms for stochastic optimization and learning rely on the
diffusion of information as a result of repeated local exchanges of
intermediate estimates. Such structures are particularly appealing in
situations where agents may be hesitant to share raw data due to privacy
concerns. Nevertheless, in the absence of additional privacy-preserving
mechanisms, the exchange of local estimates, which are generated based on
private data can allow for the inference of the data itself. The most common
mechanism for guaranteeing privacy is the addition of perturbations to local
estimates before broadcasting. These perturbations are generally chosen
independently at every agent, resulting in a significant performance loss. We
propose an alternative scheme, which constructs perturbations according to a
particular nullspace condition, allowing them to be invisible (to first order
in the step-size) to the network centroid, while preserving privacy guarantees.
The analysis allows for general nonconvex loss functions, and is hence
applicable to a large number of machine learning and signal processing
problems, including deep learning.
</p>
<a href="http://arxiv.org/abs/2010.12288" target="_blank">arXiv:2010.12288</a> [<a href="http://arxiv.org/pdf/2010.12288" target="_blank">pdf</a>]

<h2>Linearly Converging Error Compensated SGD. (arXiv:2010.12292v1 [math.OC])</h2>
<h3>Eduard Gorbunov, Dmitry Kovalev, Dmitry Makarenko, Peter Richt&#xe1;rik</h3>
<p>In this paper, we propose a unified analysis of variants of distributed SGD
with arbitrary compressions and delayed updates. Our framework is general
enough to cover different variants of quantized SGD, Error-Compensated SGD
(EC-SGD) and SGD with delayed updates (D-SGD). Via a single theorem, we derive
the complexity results for all the methods that fit our framework. For the
existing methods, this theorem gives the best-known complexity results.
Moreover, using our general scheme, we develop new variants of SGD that combine
variance reduction or arbitrary sampling with error feedback and quantization
and derive the convergence rates for these methods beating the state-of-the-art
results. In order to illustrate the strength of our framework, we develop 16
new methods that fit this. In particular, we propose the first method called
EC-SGD-DIANA that is based on error-feedback for biased compression operator
and quantization of gradient differences and prove the convergence guarantees
showing that EC-SGD-DIANA converges to the exact optimum asymptotically in
expectation with constant learning rate for both convex and strongly convex
objectives when workers compute full gradients of their loss functions.
Moreover, for the case when the loss function of the worker has the form of
finite sum, we modified the method and got a new one called EC-LSVRG-DIANA
which is the first distributed stochastic method with error feedback and
variance reduction that converges to the exact optimum asymptotically in
expectation with a constant learning rate.
</p>
<a href="http://arxiv.org/abs/2010.12292" target="_blank">arXiv:2010.12292</a> [<a href="http://arxiv.org/pdf/2010.12292" target="_blank">pdf</a>]

<h2>Deep Reinforcement Learning for IoT Networks: Age of Information and Energy Cost Tradeoff. (arXiv:2010.12297v1 [cs.IT])</h2>
<h3>Xiongwei Wu, Xiuhua Li, Jun Li, P. C. Ching, H. Vincent Poor</h3>
<p>In most Internet of Things (IoT) networks, edge nodes are commonly used as to
relays to cache sensing data generated by IoT sensors as well as provide
communication services for data consumers. However, a critical issue of IoT
sensing is that data are usually transient, which necessitates temporal updates
of caching content items while frequent cache updates could lead to
considerable energy cost and challenge the lifetime of IoT sensors. To address
this issue, we adopt the Age of Information (AoI) to quantify data freshness
and propose an online cache update scheme to obtain an effective tradeoff
between the average AoI and energy cost. Specifically, we first develop a
characterization of transmission energy consumption at IoT sensors by
incorporating a successful transmission condition. Then, we model cache
updating as a Markov decision process to minimize average weighted cost with
judicious definitions of state, action, and reward. Since user preference
towards content items is usually unknown and often temporally evolving, we
therefore develop a deep reinforcement learning (DRL) algorithm to enable
intelligent cache updates. Through trial-and-error explorations, an effective
caching policy can be learned without requiring exact knowledge of content
popularity. Simulation results demonstrate the superiority of the proposed
framework.
</p>
<a href="http://arxiv.org/abs/2010.12297" target="_blank">arXiv:2010.12297</a> [<a href="http://arxiv.org/pdf/2010.12297" target="_blank">pdf</a>]

<h2>Model-Based Machine Learning for Joint Digital Backpropagation and PMD Compensation. (arXiv:2010.12313v1 [eess.SP])</h2>
<h3>Rick M. B&#xfc;tler, Christian H&#xe4;ger, Henry D. Pfister, Gabriele Liga, Alex Alvarado</h3>
<p>In this paper, we propose a model-based machine-learning approach for
dual-polarization systems by parameterizing the split-step Fourier method for
the Manakov-PMD equation. The resulting method combines hardware-friendly
time-domain nonlinearity mitigation via the recently proposed learned digital
backpropagation (LDBP) with distributed compensation of polarization-mode
dispersion (PMD). We refer to the resulting approach as LDBP-PMD. We train
LDBP-PMD on multiple PMD realizations and show that it converges within 1% of
its peak dB performance after 428 training iterations on average, yielding a
peak effective signal-to-noise ratio of only 0.30 dB below the PMD-free case.
Similar to state-of-the-art lumped PMD compensation algorithms in practical
systems, our approach does not assume any knowledge about the particular PMD
realization along the link, nor any knowledge about the total accumulated PMD.
This is a significant improvement compared to prior work on distributed PMD
compensation, where knowledge about the accumulated PMD is typically assumed.
We also compare different parameterization choices in terms of performance,
complexity, and convergence behavior. Lastly, we demonstrate that the learned
models can be successfully retrained after an abrupt change of the PMD
realization along the fiber.
</p>
<a href="http://arxiv.org/abs/2010.12313" target="_blank">arXiv:2010.12313</a> [<a href="http://arxiv.org/pdf/2010.12313" target="_blank">pdf</a>]

<h2>Data-driven Regularized Inference Privacy. (arXiv:2010.12346v1 [cs.CR])</h2>
<h3>Chong Xiao Wang, Wee Peng Tay</h3>
<p>Data is used widely by service providers as input to inference systems to
perform decision making for authorized tasks. The raw data however allows a
service provider to infer other sensitive information it has not been
authorized for. We propose a data-driven inference privacy preserving framework
to sanitize data so as to prevent leakage of sensitive information that is
present in the raw data, while ensuring that the sanitized data is still
compatible with the service provider's legacy inference system. We develop an
inference privacy framework based on the variational method and include maximum
mean discrepancy and domain adaption as techniques to regularize the domain of
the sanitized data to ensure its legacy compatibility. However, the variational
method leads to weak privacy in cases where the underlying data distribution is
hard to approximate. It may also face difficulties when handling continuous
private variables. To overcome this, we propose an alternative formulation of
the privacy metric using maximal correlation and we present empirical methods
to estimate it. Finally, we develop a deep learning model as an example of the
proposed inference privacy framework. Numerical experiments verify the
feasibility of our approach.
</p>
<a href="http://arxiv.org/abs/2010.12346" target="_blank">arXiv:2010.12346</a> [<a href="http://arxiv.org/pdf/2010.12346" target="_blank">pdf</a>]

<h2>Sub-linear convergence of a stochastic proximal iteration method in Hilbert space. (arXiv:2010.12348v1 [math.OC])</h2>
<h3>M&#xe5;ns Williamson, Monika Eisenmann, Tony Stillfjord</h3>
<p>We consider a stochastic version of the proximal point algorithm for
optimization problems posed on a Hilbert space. A typical application of this
is supervised learning. While the method is not new, it has not been
extensively analyzed in this form. Indeed, most related results are confined to
the finite-dimensional setting, where error bounds could depend on the
dimension of the space. On the other hand, the few existing results in the
infinite-dimensional setting only prove very weak types of convergence, owing
to weak assumptions on the problem. In particular, there are no results that
show convergence with a rate. In this article, we bridge these two worlds by
assuming more regularity of the optimization problem, which allows us to prove
convergence with an (optimal) sub-linear rate also in an infinite-dimensional
setting. We illustrate these results by discretizing a concrete
infinite-dimensional classification problem with varying degrees of accuracy.
</p>
<a href="http://arxiv.org/abs/2010.12348" target="_blank">arXiv:2010.12348</a> [<a href="http://arxiv.org/pdf/2010.12348" target="_blank">pdf</a>]

<h2>Multi-UAV Path Planning for Wireless Data Harvesting with Deep Reinforcement Learning. (arXiv:2010.12461v1 [cs.MA])</h2>
<h3>Harald Bayerlein, Mirco Theile, Marco Caccamo, David Gesbert</h3>
<p>Harvesting data from distributed Internet of Things (IoT) devices with
multiple autonomous unmanned aerial vehicles (UAVs) is a challenging problem
requiring flexible path planning methods. We propose a multi-agent
reinforcement learning (MARL) approach that, in contrast to previous work, can
adapt to profound changes in the scenario parameters defining the data
harvesting mission, such as the number of deployed UAVs, number and position of
IoT devices, or the maximum flying time, without the need to perform expensive
recomputations or relearn control policies. We formulate the path planning
problem for a cooperative, non-communicating, and homogeneous team of UAVs
tasked with maximizing collected data from distributed IoT sensor nodes subject
to flying time and collision avoidance constraints. The path planning problem
is translated into a decentralized partially observable Markov decision process
(Dec-POMDP), which we solve by training a double deep Q-network (DDQN) to
approximate the optimal UAV control policy. By exploiting global-local maps of
the environment that are fed into convolutional layers of the agents, we show
that our proposed network architecture enables the agents to cooperate
effectively by carefully dividing the data collection task among themselves,
adapt to large state spaces, and make movement decisions that balance data
collection goals, flight-time efficiency, and navigation constraints.
</p>
<a href="http://arxiv.org/abs/2010.12461" target="_blank">arXiv:2010.12461</a> [<a href="http://arxiv.org/pdf/2010.12461" target="_blank">pdf</a>]

<h2>Divide and Conquer: One-Bit MIMO-OFDM Detection by Inexact Expectation Maximization. (arXiv:2010.12492v1 [cs.IT])</h2>
<h3>Mingjie Shao, Wing-Kin Ma</h3>
<p>Adopting one-bit analog-to-digital convertors (ADCs) for massive
multiple-input multiple-output (MIMO) implementations has great potential in
reducing the hardware cost and power consumption. However, distortions caused
by quantization raise great challenges. In MIMO orthogonal frequency-division
modulation (OFDM) detection, coarse quantization renders the orthogonal
separation among subcarriers inapplicable, forcing us to deal with a problem
that has a very large problem size. In this paper we study the
expectation-maximization (EM) approach for one-bit MIMO-OFDM detection. The
idea is to iteratively decouple the MIMO-OFDM detection problem among
subcarriers. Using the perspective of block coordinate descent, we describe
inexact variants of the classical EM method for providing more flexible and
computationally efficient designs. Simulation results are provided to
illustrate the potential of the divide-and-conquer strategy enabled by EM.
</p>
<a href="http://arxiv.org/abs/2010.12492" target="_blank">arXiv:2010.12492</a> [<a href="http://arxiv.org/pdf/2010.12492" target="_blank">pdf</a>]

<h2>Train simultaneously, generalize better: Stability of gradient-based minimax learners. (arXiv:2010.12561v1 [cs.LG])</h2>
<h3>Farzan Farnia, Asuman Ozdaglar</h3>
<p>The success of minimax learning problems of generative adversarial networks
(GANs) has been observed to depend on the minimax optimization algorithm used
for their training. This dependence is commonly attributed to the convergence
speed and robustness properties of the underlying optimization algorithm. In
this paper, we show that the optimization algorithm also plays a key role in
the generalization performance of the trained minimax model. To this end, we
analyze the generalization properties of standard gradient descent ascent (GDA)
and proximal point method (PPM) algorithms through the lens of algorithmic
stability under both convex concave and non-convex non-concave minimax
settings. While the GDA algorithm is not guaranteed to have a vanishing excess
risk in convex concave problems, we show the PPM algorithm enjoys a bounded
excess risk in the same setup. For non-convex non-concave problems, we compare
the generalization performance of stochastic GDA and GDmax algorithms where the
latter fully solves the maximization subproblem at every iteration. Our
generalization analysis suggests the superiority of GDA provided that the
minimization and maximization subproblems are solved simultaneously with
similar learning rates. We discuss several numerical results indicating the
role of optimization algorithms in the generalization of the learned minimax
models.
</p>
<a href="http://arxiv.org/abs/2010.12561" target="_blank">arXiv:2010.12561</a> [<a href="http://arxiv.org/pdf/2010.12561" target="_blank">pdf</a>]

<h2>Non-convex Super-resolution of OCT images via sparse representation. (arXiv:2010.12576v1 [eess.IV])</h2>
<h3>Gabriele Scrivanti, Luca Calatroni, Serena Morigi, Lindsay Nicholson, Alin Achim</h3>
<p>We propose a non-convex variational model for the super-resolution of Optical
Coherence Tomography (OCT) images of the murine eye, by enforcing sparsity with
respect to suitable dictionaries learnt from high-resolution OCT data. The
statistical characteristics of OCT images motivate the use of {\alpha}-stable
distributions for learning dictionaries, by considering the non-Gaussian case,
{\alpha}=1. The sparsity-promoting cost function relies on a non-convex penalty
- Cauchy-based or Minimax Concave Penalty (MCP) - which makes the problem
particularly challenging. We propose an efficient algorithm for minimizing the
function based on the forward-backward splitting strategy which guarantees at
each iteration the existence and uniqueness of the proximal point. Comparisons
with standard convex L1-based reconstructions show the better performance of
non-convex models, especially in view of further OCT image analysis
</p>
<a href="http://arxiv.org/abs/2010.12576" target="_blank">arXiv:2010.12576</a> [<a href="http://arxiv.org/pdf/2010.12576" target="_blank">pdf</a>]

<h2>Rationally Biased Learning. (arXiv:1709.02256v2 [cs.AI] UPDATED)</h2>
<h3>Michel de Lara (CERMICS)</h3>
<p>Humans display a tendency to pay more attention to bad outcomes, often in a
disproportionate way relative to their statistical occurrence. They also
display euphorism, as well as a preference for the current state of affairs
(status quo bias). Based on the analysis of optimal solutions of infinite
horizon stationary optimization problems under imperfect state observation, we
show that such human perception and decision biases can be grounded in a form
of rationality. We also provide conditions (boundaries) for their possible
occurence and an analysis of their robustness.Thus, biases can be the product
of rational behavior.
</p>
<a href="http://arxiv.org/abs/1709.02256" target="_blank">arXiv:1709.02256</a> [<a href="http://arxiv.org/pdf/1709.02256" target="_blank">pdf</a>]

<h2>Deep Learning for Beamspace Channel Estimation in Millimeter-Wave Massive MIMO Systems. (arXiv:1910.12455v3 [eess.SP] UPDATED)</h2>
<h3>Xiuhong Wei, Chen Hu, Linglong Dai</h3>
<p>Millimeter-wave massive multiple-input multiple-output (MIMO) can use a lens
antenna array to considerably reduce the number of radio frequency (RF) chains,
but channel estimation is challenging due to the number of RF chains is much
smaller than that of antennas. By exploiting the sparsity of beamspace
channels, the beamspace channel estimation can be formulated as a sparse signal
recovery problem, which can be solved by the classical iterative algorithm
named approximate message passing (AMP), and its corresponding version learned
AMP (LAMP) realized by a deep neural network (DNN). However, these existing
schemes cannot achieve satisfactory estimation accuracy. To improve the channel
estimation performance, we propose a prior-aided Gaussian mixture LAMP
(GM-LAMP) based beamspace channel estimation scheme in this paper.
Specifically, based on the prior information that beamspace channel elements
can be modeled by the Gaussian mixture distribution, we first derive a new
shrinkage function to refine the AMP algorithm. Then, by replacing the original
shrinkage function in the LAMP network with the derived Gaussian mixture
shrinkage function, a prior-aided GM-LAMP network is developed to estimate the
beamspace channel more accurately. Simulation results by using both the
theoretical channel model and the ray-tracing based channel dataset show that,
the proposed GM-LAMP network can achieve better channel estimation accuracy.
</p>
<a href="http://arxiv.org/abs/1910.12455" target="_blank">arXiv:1910.12455</a> [<a href="http://arxiv.org/pdf/1910.12455" target="_blank">pdf</a>]

<h2>Sequential Classification with Empirically Observed Statistics. (arXiv:1912.01170v2 [stat.ML] UPDATED)</h2>
<h3>Mahdi Haghifam, Vincent Y. F. Tan, Ashish Khisti</h3>
<p>Motivated by real-world machine learning applications, we consider a
statistical classification task in a sequential setting where test samples
arrive sequentially. In addition, the generating distributions are unknown and
only a set of empirically sampled sequences are available to a decision maker.
The decision maker is tasked to classify a test sequence which is known to be
generated according to either one of the distributions. In particular, for the
binary case, the decision maker wishes to perform the classification task with
minimum number of the test samples, so, at each step, she declares that either
hypothesis 1 is true, hypothesis 2 is true, or she requests for an additional
test sample. We propose a classifier and analyze the type-I and type-II error
probabilities. We demonstrate the significant advantage of our sequential
scheme compared to an existing non-sequential classifier proposed by Gutman.
Finally, we extend our setup and results to the multi-class classification
scenario and again demonstrate that the variable-length nature of the problem
affords significant advantages as one can achieve the same set of exponents as
Gutman's fixed-length setting but without having the rejection option.
</p>
<a href="http://arxiv.org/abs/1912.01170" target="_blank">arXiv:1912.01170</a> [<a href="http://arxiv.org/pdf/1912.01170" target="_blank">pdf</a>]

<h2>Why are Adaptive Methods Good for Attention Models?. (arXiv:1912.03194v2 [math.OC] UPDATED)</h2>
<h3>Jingzhao Zhang, Sai Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sashank J Reddi, Sanjiv Kumar, Suvrit Sra</h3>
<p>While stochastic gradient descent (SGD) is still the \emph{de facto}
algorithm in deep learning, adaptive methods like Clipped SGD/Adam have been
observed to outperform SGD across important tasks, such as attention models.
The settings under which SGD performs poorly in comparison to adaptive methods
are not well understood yet. In this paper, we provide empirical and
theoretical evidence that a heavy-tailed distribution of the noise in
stochastic gradients is one cause of SGD's poor performance. We provide the
first tight upper and lower convergence bounds for adaptive gradient methods
under heavy-tailed noise. Further, we demonstrate how gradient clipping plays a
key role in addressing heavy-tailed gradient noise. Subsequently, we show how
clipping can be applied in practice by developing an \emph{adaptive}
coordinate-wise clipping algorithm (ACClip) and demonstrate its superior
performance on BERT pretraining and finetuning tasks.
</p>
<a href="http://arxiv.org/abs/1912.03194" target="_blank">arXiv:1912.03194</a> [<a href="http://arxiv.org/pdf/1912.03194" target="_blank">pdf</a>]

<h2>Distributed Reinforcement Learning for Decentralized Linear Quadratic Control: A Derivative-Free Policy Optimization Approach. (arXiv:1912.09135v3 [eess.SY] UPDATED)</h2>
<h3>Yingying Li, Yujie Tang, Runyu Zhang, Na Li</h3>
<p>This paper considers a distributed reinforcement learning problem for
decentralized linear quadratic control with partial state observations and
local costs. We propose a Zero-Order Distributed Policy Optimization algorithm
(ZODPO) that learns linear local controllers in a distributed fashion,
leveraging the ideas of policy gradient, zero-order optimization and consensus
algorithms. In ZODPO, each agent estimates the global cost by consensus, and
then conducts local policy gradient in parallel based on zero-order gradient
estimation. ZODPO only requires limited communication and storage even in
large-scale systems. Further, we investigate the nonasymptotic performance of
ZODPO and show that the sample complexity to approach a stationary point is
polynomial with the error tolerance's inverse and the problem dimensions,
demonstrating the scalability of ZODPO. We also show that the controllers
generated throughout ZODPO are stabilizing controllers with high probability.
Lastly, we numerically test ZODPO on multi-zone HVAC systems.
</p>
<a href="http://arxiv.org/abs/1912.09135" target="_blank">arXiv:1912.09135</a> [<a href="http://arxiv.org/pdf/1912.09135" target="_blank">pdf</a>]

<h2>Stochastic Recursive Gradient Descent Ascent for Stochastic Nonconvex-Strongly-Concave Minimax Problems. (arXiv:2001.03724v2 [cs.LG] UPDATED)</h2>
<h3>Luo Luo, Haishan Ye, Zhichao Huang, Tong Zhang</h3>
<p>We consider nonconvex-concave minimax optimization problems of the form
$\min_{\bf x}\max_{\bf y\in{\mathcal Y}} f({\bf x},{\bf y})$, where $f$ is
strongly-concave in $\bf y$ but possibly nonconvex in $\bf x$ and ${\mathcal
Y}$ is a convex and compact set. We focus on the stochastic setting, where we
can only access an unbiased stochastic gradient estimate of $f$ at each
iteration. This formulation includes many machine learning applications as
special cases such as robust optimization and adversary training. We are
interested in finding an ${\mathcal O}(\varepsilon)$-stationary point of the
function $\Phi(\cdot)=\max_{\bf y\in{\mathcal Y}} f(\cdot, {\bf y})$. The most
popular algorithm to solve this problem is stochastic gradient decent ascent,
which requires $\mathcal O(\kappa^3\varepsilon^{-4})$ stochastic gradient
evaluations, where $\kappa$ is the condition number. In this paper, we propose
a novel method called Stochastic Recursive gradiEnt Descent Ascent (SREDA),
which estimates gradients more efficiently using variance reduction. This
method achieves the best known stochastic gradient complexity of ${\mathcal
O}(\kappa^3\varepsilon^{-3})$, and its dependency on $\varepsilon$ is optimal
for this problem.
</p>
<a href="http://arxiv.org/abs/2001.03724" target="_blank">arXiv:2001.03724</a> [<a href="http://arxiv.org/pdf/2001.03724" target="_blank">pdf</a>]

<h2>Pruning Neural Belief Propagation Decoders. (arXiv:2001.07464v2 [cs.IT] UPDATED)</h2>
<h3>Andreas Buchberger, Christian H&#xe4;ger, Henry D. Pfister, Laurent Schmalen, Alexandre Graell i Amat</h3>
<p>We consider near maximum-likelihood (ML) decoding of short linear block codes
based on neural belief propagation (BP) decoding recently introduced by
Nachmani et al.. While this method significantly outperforms conventional BP
decoding, the underlying parity-check matrix may still limit the overall
performance. In this paper, we introduce a method to tailor an overcomplete
parity-check matrix to (neural) BP decoding using machine learning. We consider
the weights in the Tanner graph as an indication of the importance of the
connected check nodes (CNs) to decoding and use them to prune unimportant CNs.
As the pruning is not tied over iterations, the final decoder uses a different
parity-check matrix in each iteration. For Reed-Muller and short low-density
parity-check codes, we achieve performance within 0.27 dB and 1.5 dB of the ML
performance while reducing the complexity of the decoder.
</p>
<a href="http://arxiv.org/abs/2001.07464" target="_blank">arXiv:2001.07464</a> [<a href="http://arxiv.org/pdf/2001.07464" target="_blank">pdf</a>]

<h2>Optimal Iterative Sketching with the Subsampled Randomized Hadamard Transform. (arXiv:2002.00864v5 [math.OC] UPDATED)</h2>
<h3>Jonathan Lacotte, Sifan Liu, Edgar Dobriban, Mert Pilanci</h3>
<p>Random projections or sketching are widely used in many algorithmic and
learning contexts. Here we study the performance of iterative Hessian sketch
for least-squares problems. By leveraging and extending recent results from
random matrix theory on the limiting spectrum of matrices randomly projected
with the subsampled randomized Hadamard transform, and truncated Haar matrices,
we can study and compare the resulting algorithms to a level of precision that
has not been possible before. Our technical contributions include a novel
formula for the second moment of the inverse of projected matrices. We also
find simple closed-form expressions for asymptotically optimal step-sizes and
convergence rates. These show that the convergence rate for Haar and randomized
Hadamard matrices are identical, and asymptotically improve upon Gaussian
random projections. These techniques may be applied to other algorithms that
employ randomized dimension reduction.
</p>
<a href="http://arxiv.org/abs/2002.00864" target="_blank">arXiv:2002.00864</a> [<a href="http://arxiv.org/pdf/2002.00864" target="_blank">pdf</a>]

<h2>Minimax optimal goodness-of-fit testing for densities and multinomials under a local differential privacy constraint. (arXiv:2002.04254v2 [math.ST] UPDATED)</h2>
<h3>Joseph Lam-Weil, B&#xe9;atrice Laurent, Jean-Michel Loubes</h3>
<p>Finding anonymization mechanisms to protect personal data is at the heart of
recent machine learning research. Here, we consider the consequences of local
differential privacy constraints on goodness-of-fit testing, i.e. the
statistical problem assessing whether sample points are generated from a fixed
density $f_0$, or not. The observations are kept hidden and replaced by a
stochastic transformation satisfying the local differential privacy constraint.
In this setting, we propose a testing procedure which is based on an estimation
of the quadratic distance between the density $f$ of the unobserved samples and
$f_0$. We establish minimax separation rates for our test in the discrete and
continuous settings. To the best of our knowledge, we provide the first minimax
optimal test and associated private transformation under a local differential
privacy constraint over Besov balls in the continuous setting, quantifying the
price to pay for data privacy. We also present a test that is adaptive to the
smoothness parameter of the unknown density and remains minimax optimal up to a
logarithmic factor. Finally, we note that our results can be translated to the
discrete case, where the treatment of probability vectors is shown to be
equivalent to that of piecewise constant densities in our setting. That is why
we work with a unified setting for both the continuous and the discrete cases.
</p>
<a href="http://arxiv.org/abs/2002.04254" target="_blank">arXiv:2002.04254</a> [<a href="http://arxiv.org/pdf/2002.04254" target="_blank">pdf</a>]

<h2>Personalized Federated Learning: A Meta-Learning Approach. (arXiv:2002.07948v4 [cs.LG] UPDATED)</h2>
<h3>Alireza Fallah, Aryan Mokhtari, Asuman Ozdaglar</h3>
<p>In Federated Learning, we aim to train models across multiple computing units
(users), while users can only communicate with a common central server, without
exchanging their data samples. This mechanism exploits the computational power
of all users and allows users to obtain a richer model as their models are
trained over a larger set of data points. However, this scheme only develops a
common output for all the users, and, therefore, it does not adapt the model to
each user. This is an important missing feature, especially given the
heterogeneity of the underlying data distribution for various users. In this
paper, we study a personalized variant of the federated learning in which our
goal is to find an initial shared model that current or new users can easily
adapt to their local dataset by performing one or a few steps of gradient
descent with respect to their own data. This approach keeps all the benefits of
the federated learning architecture, and, by structure, leads to a more
personalized model for each user. We show this problem can be studied within
the Model-Agnostic Meta-Learning (MAML) framework. Inspired by this connection,
we study a personalized variant of the well-known Federated Averaging algorithm
and evaluate its performance in terms of gradient norm for non-convex loss
functions. Further, we characterize how this performance is affected by the
closeness of underlying distributions of user data, measured in terms of
distribution distances such as Total Variation and 1-Wasserstein metric.
</p>
<a href="http://arxiv.org/abs/2002.07948" target="_blank">arXiv:2002.07948</a> [<a href="http://arxiv.org/pdf/2002.07948" target="_blank">pdf</a>]

<h2>Competition between slow and fast regimes for extreme first passage times of diffusion. (arXiv:2004.05414v2 [math.PR] UPDATED)</h2>
<h3>Jacob B. Madrid, Sean D. Lawley</h3>
<p>Many physical, chemical, and biological systems depend on the first passage
time (FPT) of a diffusive searcher to a target. Typically, this FPT is much
slower than the characteristic diffusion timescale. For example, this is the
case if the target is small (the narrow escape problem) or if the searcher must
escape a potential well. However, many systems depend on the first time a
searcher finds the target out of a large group of searchers, which is the
so-called extreme FPT. Since this extreme FPT vanishes in the limit of many
searchers, the prohibitively slow FPTs of diffusive search can be negated by
deploying enough searchers. However, the notion of "enough searchers" is poorly
understood. How can one determine if a system is in the slow regime (dominated
by small targets or a deep potential, for example) or the fast regime
(dominated by many searchers)? How can one estimate the extreme FPT in these
different regimes? In this paper, we answer these questions by deriving
conditions which ensure that a system is in either regime and finding
approximations of the full distribution and all the moments of the extreme FPT
in these regimes. Our analysis reveals the critical effect that initial
searcher distribution and target reactivity can have on extreme FPTs.
</p>
<a href="http://arxiv.org/abs/2004.05414" target="_blank">arXiv:2004.05414</a> [<a href="http://arxiv.org/pdf/2004.05414" target="_blank">pdf</a>]

<h2>A novel embedded min-max approach for feature selection in nonlinear support vector machine classification. (arXiv:2004.09863v3 [cs.LG] UPDATED)</h2>
<h3>Asunci&#xf3;n Jim&#xe9;nez-Cordero, Juan Miguel Morales, Salvador Pineda</h3>
<p>In recent years, feature selection has become a challenging problem in
several machine learning fields, such as classification problems. Support
Vector Machine (SVM) is a well-known technique applied in classification tasks.
Various methodologies have been proposed in the literature to select the most
relevant features in SVM. Unfortunately, all of them either deal with the
feature selection problem in the linear classification setting or propose
ad-hoc approaches that are difficult to implement in practice. In contrast, we
propose an embedded feature selection method based on a min-max optimization
problem, where a trade-off between model complexity and classification accuracy
is sought. By leveraging duality theory, we equivalently reformulate the
min-max problem and solve it without further ado using off-the-shelf software
for nonlinear optimization. The efficiency and usefulness of our approach are
tested on several benchmark data sets in terms of accuracy, number of selected
features and interpretability.
</p>
<a href="http://arxiv.org/abs/2004.09863" target="_blank">arXiv:2004.09863</a> [<a href="http://arxiv.org/pdf/2004.09863" target="_blank">pdf</a>]

<h2>Sharpened Generalization Bounds based on Conditional Mutual Information and an Application to Noisy, Iterative Algorithms. (arXiv:2004.12983v2 [stat.ML] UPDATED)</h2>
<h3>Mahdi Haghifam, Jeffrey Negrea, Ashish Khisti, Daniel M. Roy, Gintare Karolina Dziugaite</h3>
<p>The information-theoretic framework of Russo and J. Zou (2016) and Xu and
Raginsky (2017) provides bounds on the generalization error of a learning
algorithm in terms of the mutual information between the algorithm's output and
the training sample. In this work, we study the proposal, by Steinke and
Zakynthinou (2020), to reason about the generalization error of a learning
algorithm by introducing a super sample that contains the training sample as a
random subset and computing mutual information conditional on the super sample.
We first show that these new bounds based on the conditional mutual information
are tighter than those based on the unconditional mutual information. We then
introduce yet tighter bounds, building on the "individual sample" idea of Bu,
S. Zou, and Veeravalli (2019) and the "data dependent" ideas of Negrea et al.
(2019), using disintegrated mutual information. Finally, we apply these bounds
to the study of Langevin dynamics algorithm, showing that conditioning on the
super sample allows us to exploit information in the optimization trajectory to
obtain tighter bounds based on hypothesis tests.
</p>
<a href="http://arxiv.org/abs/2004.12983" target="_blank">arXiv:2004.12983</a> [<a href="http://arxiv.org/pdf/2004.12983" target="_blank">pdf</a>]

<h2>Oriented Matroids from Triangulations of Products of Simplices. (arXiv:2005.01787v2 [math.CO] UPDATED)</h2>
<h3>Marcel Celaya, Georg Loho, Chi Ho Yuen</h3>
<p>We introduce a construction of oriented matroids from a triangulation of a
product of two simplices. For this, we use the structure of such a
triangulation in terms of polyhedral matching fields. The oriented matroid is
composed of compatible chirotopes on the cells in a matroid subdivision of the
hypersimplex, which might be of independent interest. In particular, we
generalize this using the language of matroids over hyperfields, which gives a
new approach to construct matroids over hyperfields. A recurring theme in our
work is that various tropical constructions can be extended beyond
tropicalization with new formulations and proof methods.
</p>
<a href="http://arxiv.org/abs/2005.01787" target="_blank">arXiv:2005.01787</a> [<a href="http://arxiv.org/pdf/2005.01787" target="_blank">pdf</a>]

<h2>Tree! I am no Tree! I am a Low Dimensional Hyperbolic Embedding. (arXiv:2005.03847v4 [cs.LG] UPDATED)</h2>
<h3>Rishi Sonthalia, Anna C. Gilbert</h3>
<p>Given data, finding a faithful low-dimensional hyperbolic embedding of the
data is a key method by which we can extract hierarchical information or learn
representative geometric features of the data. In this paper, we explore a new
method for learning hyperbolic representations by taking a metric-first
approach. Rather than determining the low-dimensional hyperbolic embedding
directly, we learn a tree structure on the data. This tree structure can then
be used directly to extract hierarchical information, embedded into a
hyperbolic manifold using Sarkar's construction \cite{sarkar}, or used as a
tree approximation of the original metric. To this end, we present a novel fast
algorithm \textsc{TreeRep} such that, given a $\delta$-hyperbolic metric (for
any $\delta \geq 0$), the algorithm learns a tree structure that approximates
the original metric. In the case when $\delta = 0$, we show analytically that
\textsc{TreeRep} exactly recovers the original tree structure. We show
empirically that \textsc{TreeRep} is not only many orders of magnitude faster
than previously known algorithms, but also produces metrics with lower average
distortion and higher mean average precision than most previous algorithms for
learning hyperbolic embeddings, extracting hierarchical information, and
approximating metrics via tree metrics.
</p>
<a href="http://arxiv.org/abs/2005.03847" target="_blank">arXiv:2005.03847</a> [<a href="http://arxiv.org/pdf/2005.03847" target="_blank">pdf</a>]

<h2>Non-Euclidean Universal Approximation. (arXiv:2006.02341v2 [cs.LG] UPDATED)</h2>
<h3>Anastasis Kratsios, Ievgen Bilokopytov</h3>
<p>Modifications to a neural network's input and output layers are often
required to accommodate the specificities of most practical learning tasks.
However, the impact of such changes on architecture's approximation
capabilities is largely not understood. We present general conditions
describing feature and readout maps that preserve an architecture's ability to
approximate any continuous functions uniformly on compacts. As an application,
we show that if an architecture is capable of universal approximation, then
modifying its final layer to produce binary values creates a new architecture
capable of deterministically approximating any classifier. In particular, we
obtain guarantees for deep CNNs and deep feed-forward networks. Our results
also have consequences within the scope of geometric deep learning.
Specifically, when the input and output spaces are Cartan-Hadamard manifolds,
we obtain geometrically meaningful feature and readout maps satisfying our
criteria. Consequently, commonly used non-Euclidean regression models between
spaces of symmetric positive definite matrices are extended to universal DNNs.
The same result allows us to show that the hyperbolic feed-forward networks,
used for hierarchical learning, are universal. Our result is also used to show
that the common practice of randomizing all but the last two layers of a DNN
produces a universal family of functions with probability one. We also provide
conditions on a DNN's first (resp. last) few layer's connections and activation
function which guarantee that these layers can have a width equal to the input
(resp. output) space's dimension while not negatively affecting the
architecture's approximation capabilities.
</p>
<a href="http://arxiv.org/abs/2006.02341" target="_blank">arXiv:2006.02341</a> [<a href="http://arxiv.org/pdf/2006.02341" target="_blank">pdf</a>]

<h2>Lipschitz Bounds and Provably Robust Training by Laplacian Smoothing. (arXiv:2006.03712v3 [cs.LG] UPDATED)</h2>
<h3>Vishaal Krishnan, Abed AlRahman Al Makdah, Fabio Pasqualetti</h3>
<p>In this work we propose a graph-based learning framework to train models with
provable robustness to adversarial perturbations. In contrast to
regularization-based approaches, we formulate the adversarially robust learning
problem as one of loss minimization with a Lipschitz constraint, and show that
the saddle point of the associated Lagrangian is characterized by a Poisson
equation with weighted Laplace operator. Further, the weighting for the Laplace
operator is given by the Lagrange multiplier for the Lipschitz constraint,
which modulates the sensitivity of the minimizer to perturbations. We then
design a provably robust training scheme using graph-based discretization of
the input space and a primal-dual algorithm to converge to the Lagrangian's
saddle point. Our analysis establishes a novel connection between elliptic
operators with constraint-enforced weighting and adversarial learning. We also
study the complementary problem of improving the robustness of minimizers with
a margin on their loss, formulated as a loss-constrained minimization problem
of the Lipschitz constant. We propose a technique to obtain robustified
minimizers, and evaluate fundamental Lipschitz lower bounds by approaching
Lipschitz constant minimization via a sequence of gradient $p$-norm
minimization problems. Ultimately, our results show that, for a desired nominal
performance, there exists a fundamental lower bound on the sensitivity to
adversarial perturbations that depends only on the loss function and the data
distribution, and that improvements in robustness beyond this bound can only be
made at the expense of nominal performance. Our training schemes provably
achieve these bounds both under constraints on performance and~robustness.
</p>
<a href="http://arxiv.org/abs/2006.03712" target="_blank">arXiv:2006.03712</a> [<a href="http://arxiv.org/pdf/2006.03712" target="_blank">pdf</a>]

<h2>Convergence of Meta-Learning with Task-Specific Adaptation over Partial Parameters. (arXiv:2006.09486v3 [cs.LG] UPDATED)</h2>
<h3>Kaiyi Ji, Jason D. Lee, Yingbin Liang, H. Vincent Poor</h3>
<p>Although model-agnostic meta-learning (MAML) is a very successful algorithm
in meta-learning practice, it can have high computational cost because it
updates all model parameters over both the inner loop of task-specific
adaptation and the outer-loop of meta initialization training. A more efficient
algorithm ANIL (which refers to almost no inner loop) was proposed recently by
Raghu et al. 2019, which adapts only a small subset of parameters in the inner
loop and thus has substantially less computational cost than MAML as
demonstrated by extensive experiments. However, the theoretical convergence of
ANIL has not been studied yet. In this paper, we characterize the convergence
rate and the computational complexity for ANIL under two representative
inner-loop loss geometries, i.e., strongly-convexity and nonconvexity. Our
results show that such a geometric property can significantly affect the
overall convergence performance of ANIL. For example, ANIL achieves a faster
convergence rate for a strongly-convex inner-loop loss as the number $N$ of
inner-loop gradient descent steps increases, but a slower convergence rate for
a nonconvex inner-loop loss as $N$ increases. Moreover, our complexity analysis
provides a theoretical quantification on the improved efficiency of ANIL over
MAML. The experiments on standard few-shot meta-learning benchmarks validate
our theoretical findings.
</p>
<a href="http://arxiv.org/abs/2006.09486" target="_blank">arXiv:2006.09486</a> [<a href="http://arxiv.org/pdf/2006.09486" target="_blank">pdf</a>]

<h2>Hybrid Models for Learning to Branch. (arXiv:2006.15212v3 [cs.LG] UPDATED)</h2>
<h3>Prateek Gupta, Maxime Gasse, Elias B. Khalil, M. Pawan Kumar, Andrea Lodi, Yoshua Bengio</h3>
<p>A recent Graph Neural Network (GNN) approach for learning to branch has been
shown to successfully reduce the running time of branch-and-bound algorithms
for Mixed Integer Linear Programming (MILP). While the GNN relies on a GPU for
inference, MILP solvers are purely CPU-based. This severely limits its
application as many practitioners may not have access to high-end GPUs. In this
work, we ask two key questions. First, in a more realistic setting where only a
CPU is available, is the GNN model still competitive? Second, can we devise an
alternate computationally inexpensive model that retains the predictive power
of the GNN architecture? We answer the first question in the negative, and
address the second question by proposing a new hybrid architecture for
efficient branching on CPU machines. The proposed architecture combines the
expressive power of GNNs with computationally inexpensive multi-layer
perceptrons (MLP) for branching. We evaluate our methods on four classes of
MILP problems, and show that they lead to up to 26% reduction in solver running
time compared to state-of-the-art methods without a GPU, while extrapolating to
harder problems than it was trained on. The code for this project is publicly
available at https://github.com/pg2455/Hybrid-learn2branch.
</p>
<a href="http://arxiv.org/abs/2006.15212" target="_blank">arXiv:2006.15212</a> [<a href="http://arxiv.org/pdf/2006.15212" target="_blank">pdf</a>]

<h2>A Decentralized Approach to Bayesian Learning. (arXiv:2007.06799v2 [stat.ML] UPDATED)</h2>
<h3>Anjaly Parayil, He Bai, Jemin George, Prudhvi Gurram</h3>
<p>Motivated by decentralized approaches to machine learning, we propose a
collaborative Bayesian learning algorithm taking the form of decentralized
Langevin dynamics in a non-convex setting. Our analysis show that the initial
KL-divergence between the Markov Chain and the target posterior distribution is
exponentially decreasing while the error contributions to the overall
KL-divergence from the additive noise is decreasing in polynomial time. We
further show that the polynomial-term experiences speed-up with number of
agents and provide sufficient conditions on the time-varying step-sizes to
guarantee convergence to the desired distribution. The performance of the
proposed algorithm is evaluated on a wide variety of machine learning tasks.
The empirical results show that the performance of individual agents with
locally available data is on par with the centralized setting with considerable
improvement in the convergence rate.
</p>
<a href="http://arxiv.org/abs/2007.06799" target="_blank">arXiv:2007.06799</a> [<a href="http://arxiv.org/pdf/2007.06799" target="_blank">pdf</a>]

<h2>POMDPs in Continuous Time and Discrete Spaces. (arXiv:2010.01014v2 [cs.LG] UPDATED)</h2>
<h3>Bastian Alt, Matthias Schultheis, Heinz Koeppl</h3>
<p>Many processes, such as discrete event systems in engineering or population
dynamics in biology, evolve in discrete space and continuous time. We consider
the problem of optimal decision making in such discrete state and action space
systems under partial observability. This places our work at the intersection
of optimal filtering and optimal control. At the current state of research, a
mathematical description for simultaneous decision making and filtering in
continuous time with finite state and action spaces is still missing. In this
paper, we give a mathematical description of a continuous-time partial
observable Markov decision process (POMDP). By leveraging optimal filtering
theory we derive a Hamilton-Jacobi-Bellman (HJB) type equation that
characterizes the optimal solution. Using techniques from deep learning we
approximately solve the resulting partial integro-differential equation. We
present (i) an approach solving the decision problem offline by learning an
approximation of the value function and (ii) an online algorithm which provides
a solution in belief space using deep reinforcement learning. We show the
applicability on a set of toy examples which pave the way for future methods
providing solutions for high dimensional problems.
</p>
<a href="http://arxiv.org/abs/2010.01014" target="_blank">arXiv:2010.01014</a> [<a href="http://arxiv.org/pdf/2010.01014" target="_blank">pdf</a>]

<h2>On the Universality of the Double Descent Peak in Ridgeless Regression. (arXiv:2010.01851v3 [stat.ML] UPDATED)</h2>
<h3>David Holzm&#xfc;ller</h3>
<p>We prove a non-asymptotic distribution-independent lower bound for the
expected mean squared generalization error caused by label noise in ridgeless
linear regression. Our lower bound generalizes a similar known result to the
overparameterized (interpolating) regime. In contrast to most previous works,
our analysis applies to a broad class of input distributions with almost surely
full-rank feature matrices, which allows us to cover various types of
deterministic or random feature maps. Our lower bound is asymptotically sharp
and implies that in the presence of label noise, ridgeless linear regression
does not perform well around the interpolation threshold for any of these
feature maps. We analyze the imposed assumptions in detail and provide a theory
for analytic (random) feature maps. Using this theory, we can show that our
assumptions are satisfied for input distributions with a (Lebesgue) density and
feature maps given by random deep neural networks with analytic activation
functions like sigmoid, tanh, softplus or GELU. As further examples, we show
that feature maps from random Fourier features and polynomial kernels also
satisfy our assumptions. We complement our theory with further experimental and
analytic results.
</p>
<a href="http://arxiv.org/abs/2010.01851" target="_blank">arXiv:2010.01851</a> [<a href="http://arxiv.org/pdf/2010.01851" target="_blank">pdf</a>]

<h2>Probabilistic Linear Solvers for Machine Learning. (arXiv:2010.09691v2 [cs.LG] UPDATED)</h2>
<h3>Jonathan Wenger, Philipp Hennig</h3>
<p>Linear systems are the bedrock of virtually all numerical computation.
Machine learning poses specific challenges for the solution of such systems due
to their scale, characteristic structure, stochasticity and the central role of
uncertainty in the field. Unifying earlier work we propose a class of
probabilistic linear solvers which jointly infer the matrix, its inverse and
the solution from matrix-vector product observations. This class emerges from a
fundamental set of desiderata which constrains the space of possible algorithms
and recovers the method of conjugate gradients under certain conditions. We
demonstrate how to incorporate prior spectral information in order to calibrate
uncertainty and experimentally showcase the potential of such solvers for
machine learning.
</p>
<a href="http://arxiv.org/abs/2010.09691" target="_blank">arXiv:2010.09691</a> [<a href="http://arxiv.org/pdf/2010.09691" target="_blank">pdf</a>]

<h2>A Differentially Private Text Perturbation Method Using a Regularized Mahalanobis Metric. (arXiv:2010.11947v1 [cs.CL])</h2>
<h3>Zekun Xu, Abhinav Aggarwal, Oluwaseyi Feyisetan, Nathanael Teissier</h3>
<p>Balancing the privacy-utility tradeoff is a crucial requirement of many
practical machine learning systems that deal with sensitive customer data. A
popular approach for privacy-preserving text analysis is noise injection, in
which text data is first mapped into a continuous embedding space, perturbed by
sampling a spherical noise from an appropriate distribution, and then projected
back to the discrete vocabulary space. While this allows the perturbation to
admit the required metric differential privacy, often the utility of downstream
tasks modeled on this perturbed data is low because the spherical noise does
not account for the variability in the density around different words in the
embedding space. In particular, words in a sparse region are likely unchanged
even when the noise scale is large. %Using the global sensitivity of the
mechanism can potentially add too much noise to the words in the dense regions
of the embedding space, causing a high utility loss, whereas using local
sensitivity can leak information through the scale of the noise added.

In this paper, we propose a text perturbation mechanism based on a carefully
designed regularized variant of the Mahalanobis metric to overcome this
problem. For any given noise scale, this metric adds an elliptical noise to
account for the covariance structure in the embedding space. This heterogeneity
in the noise scale along different directions helps ensure that the words in
the sparse region have sufficient likelihood of replacement without sacrificing
the overall utility. We provide a text-perturbation algorithm based on this
metric and formally prove its privacy guarantees. Additionally, we empirically
show that our mechanism improves the privacy statistics to achieve the same
level of utility as compared to the state-of-the-art Laplace mechanism.
</p>
<a href="http://arxiv.org/abs/2010.11947" target="_blank">arXiv:2010.11947</a> [<a href="http://arxiv.org/pdf/2010.11947" target="_blank">pdf</a>]

<h2>Language Models are Open Knowledge Graphs. (arXiv:2010.11967v1 [cs.CL])</h2>
<h3>Chenguang Wang, Xiao Liu, Dawn Song</h3>
<p>This paper shows how to construct knowledge graphs (KGs) from pre-trained
language models (e.g., BERT, GPT-2/3), without human supervision. Popular KGs
(e.g, Wikidata, NELL) are built in either a supervised or semi-supervised
manner, requiring humans to create knowledge. Recent deep language models
automatically acquire knowledge from large-scale corpora via pre-training. The
stored knowledge has enabled the language models to improve downstream NLP
tasks, e.g., answering questions, and writing code and articles. In this paper,
we propose an unsupervised method to cast the knowledge contained within
language models into KGs. We show that KGs are constructed with a single
forward pass of the pre-trained language models (without fine-tuning) over the
corpora. We demonstrate the quality of the constructed KGs by comparing to two
KGs (Wikidata, TAC KBP) created by humans. Our KGs also provide open factual
knowledge that is new in the existing KGs. Our code and KGs will be made
publicly available.
</p>
<a href="http://arxiv.org/abs/2010.11967" target="_blank">arXiv:2010.11967</a> [<a href="http://arxiv.org/pdf/2010.11967" target="_blank">pdf</a>]

<h2>Two-sample Test using Projected Wasserstein Distance: Breaking the Curse of Dimensionality. (arXiv:2010.11970v1 [stat.ML])</h2>
<h3>Jie Wang, Rui Gao, Yao Xie</h3>
<p>We develop a projected Wasserstein distance for the two-sample test, a
fundamental problem in statistics and machine learning: given two sets of
samples, to determine whether they are from the same distribution. In
particular, we aim to circumvent the curse of dimensionality in Wasserstein
distance: when the dimension is high, it has diminishing testing power, which
is inherently due to the slow concentration property of Wasserstein metrics in
the high dimension space. A key contribution is to couple optimal projection to
find the low dimensional linear mapping to maximize the Wasserstein distance
between projected probability distributions. We characterize the theoretical
property of the finite-sample convergence rate on IPMs and present practical
algorithms for computing this metric. Numerical examples validate our
theoretical results.
</p>
<a href="http://arxiv.org/abs/2010.11970" target="_blank">arXiv:2010.11970</a> [<a href="http://arxiv.org/pdf/2010.11970" target="_blank">pdf</a>]

<h2>Rediscovering the Slavic Continuum in Representations Emerging from Neural Models of Spoken Language Identification. (arXiv:2010.11973v1 [cs.CL])</h2>
<h3>Badr M. Abdullah, Jacek Kudera, Tania Avgustinova, Bernd M&#xf6;bius, Dietrich Klakow</h3>
<p>Deep neural networks have been employed for various spoken language
recognition tasks, including tasks that are multilingual by definition such as
spoken language identification. In this paper, we present a neural model for
Slavic language identification in speech signals and analyze its emergent
representations to investigate whether they reflect objective measures of
language relatedness and/or non-linguists' perception of language similarity.
While our analysis shows that the language representation space indeed captures
language relatedness to a great extent, we find perceptual confusability
between languages in our study to be the best predictor of the language
representation similarity.
</p>
<a href="http://arxiv.org/abs/2010.11973" target="_blank">arXiv:2010.11973</a> [<a href="http://arxiv.org/pdf/2010.11973" target="_blank">pdf</a>]

<h2>Deep Convolutional Neural Networks Model-based Brain Tumor Detection in Brain MRI Images. (arXiv:2010.11978v1 [eess.IV])</h2>
<h3>Md. Abu Bakr Siddique, Shadman Sakib, Mohammad Mahmudur Rahman Khan, Abyaz Kader Tanzeem, Madiha Chowdhury, Nowrin Yasmin</h3>
<p>Diagnosing Brain Tumor with the aid of Magnetic Resonance Imaging (MRI) has
gained enormous prominence over the years, primarily in the field of medical
science. Detection and/or partitioning of brain tumors solely with the aid of
MR imaging is achieved at the cost of immense time and effort and demands a lot
of expertise from engaged personnel. This substantiates the necessity of
fabricating an autonomous model brain tumor diagnosis. Our work involves
implementing a deep convolutional neural network (DCNN) for diagnosing brain
tumors from MR images. The dataset used in this paper consists of 253 brain MR
images where 155 images are reported to have tumors. Our model can single out
the MR images with tumors with an overall accuracy of 96%. The model
outperformed the existing conventional methods for the diagnosis of brain tumor
in the test dataset (Precision = 0.93, Sensitivity = 1.00, and F1-score =
0.97). Moreover, the proposed model's average precision-recall score is 0.93,
Cohen's Kappa 0.91, and AUC 0.95. Therefore, the proposed model can help
clinical experts verify whether the patient has a brain tumor and,
consequently, accelerate the treatment procedure.
</p>
<a href="http://arxiv.org/abs/2010.11978" target="_blank">arXiv:2010.11978</a> [<a href="http://arxiv.org/pdf/2010.11978" target="_blank">pdf</a>]

<h2>A Joint Learning Approach based on Self-Distillation for Keyphrase Extraction from Scientific Documents. (arXiv:2010.11980v1 [cs.CL])</h2>
<h3>Tuan Manh Lai, Trung Bui, Doo Soon Kim, Quan Hung Tran</h3>
<p>Keyphrase extraction is the task of extracting a small set of phrases that
best describe a document. Most existing benchmark datasets for the task
typically have limited numbers of annotated documents, making it challenging to
train increasingly complex neural networks. In contrast, digital libraries
store millions of scientific articles online, covering a wide range of topics.
While a significant portion of these articles contain keyphrases provided by
their authors, most other articles lack such kind of annotations. Therefore, to
effectively utilize these large amounts of unlabeled articles, we propose a
simple and efficient joint learning approach based on the idea of
self-distillation. Experimental results show that our approach consistently
improves the performance of baseline models for keyphrase extraction.
Furthermore, our best models outperform previous methods for the task,
achieving new state-of-the-art results on two public benchmarks: Inspec and
SemEval-2017.
</p>
<a href="http://arxiv.org/abs/2010.11980" target="_blank">arXiv:2010.11980</a> [<a href="http://arxiv.org/pdf/2010.11980" target="_blank">pdf</a>]

<h2>The Turking Test: Can Language Models Understand Instructions?. (arXiv:2010.11982v1 [cs.CL])</h2>
<h3>Avia Efrat, Omer Levy</h3>
<p>Supervised machine learning provides the learner with a set of input-output
examples of the target task. Humans, however, can also learn to perform new
tasks from instructions in natural language. Can machines learn to understand
instructions as well? We present the Turking Test, which examines a model's
ability to follow natural language instructions of varying complexity. These
range from simple tasks, like retrieving the nth word of a sentence, to ones
that require creativity, such as generating examples for SNLI and SQuAD in
place of human intelligence workers ("turkers"). Despite our lenient evaluation
methodology, we observe that a large pretrained language model performs poorly
across all tasks. Analyzing the model's error patterns reveals that the model
tends to ignore explicit instructions and often generates outputs that cannot
be construed as an attempt to solve the task. While it is not yet clear whether
instruction understanding can be captured by traditional language models, the
sheer expressivity of instruction understanding makes it an appealing
alternative to the rising few-shot inference paradigm.
</p>
<a href="http://arxiv.org/abs/2010.11982" target="_blank">arXiv:2010.11982</a> [<a href="http://arxiv.org/pdf/2010.11982" target="_blank">pdf</a>]

<h2>Learnability and Complexity of Quantum Samples. (arXiv:2010.11983v1 [quant-ph])</h2>
<h3>Murphy Yuezhen Niu, Andrew M. Dai, Li Li, Augustus Odena, Zhengli Zhao, Vadim Smelyanskyi, Hartmut Neven, Sergio Boixo</h3>
<p>Given a quantum circuit, a quantum computer can sample the output
distribution exponentially faster in the number of bits than classical
computers. A similar exponential separation has yet to be established in
generative models through quantum sample learning: given samples from an
n-qubit computation, can we learn the underlying quantum distribution using
models with training parameters that scale polynomial in n under a fixed
training time? We study four kinds of generative models: Deep Boltzmann machine
(DBM), Generative Adversarial Networks (GANs), Long Short-Term Memory (LSTM)
and Autoregressive GAN, on learning quantum data set generated by deep random
circuits. We demonstrate the leading performance of LSTM in learning quantum
samples, and thus the autoregressive structure present in the underlying
quantum distribution from random quantum circuits. Both numerical experiments
and a theoretical proof in the case of the DBM show exponentially growing
complexity of learning-agent parameters required for achieving a fixed accuracy
as n increases. Finally, we establish a connection between learnability and the
complexity of generative models by benchmarking learnability against different
sets of samples drawn from probability distributions of variable degrees of
complexities in their quantum and classical representations.
</p>
<a href="http://arxiv.org/abs/2010.11983" target="_blank">arXiv:2010.11983</a> [<a href="http://arxiv.org/pdf/2010.11983" target="_blank">pdf</a>]

<h2>MTGAT: Multimodal Temporal Graph Attention Networks for Unaligned Human Multimodal Language Sequences. (arXiv:2010.11985v1 [cs.CL])</h2>
<h3>Jianing Yang, Yongxin Wang, Ruitao Yi, Yuying Zhu, Azaan Rehman, Amir Zadeh, Soujanya Poria, Louis-Philippe Morency</h3>
<p>Human communication is multimodal in nature; it is through multiple
modalities, i.e., language, voice, and facial expressions, that opinions and
emotions are expressed. Data in this domain exhibits complex multi-relational
and temporal interactions. Learning from this data is a fundamentally
challenging research problem. In this paper, we propose Multimodal Temporal
Graph Attention Networks (MTGAT). MTGAT is an interpretable graph-based neural
model that provides a suitable framework for analyzing this type of multimodal
sequential data. We first introduce a procedure to convert unaligned multimodal
sequence data into a graph with heterogeneous nodes and edges that captures the
rich interactions between different modalities through time. Then, a novel
graph operation, called Multimodal Temporal Graph Attention, along with a
dynamic pruning and read-out technique is designed to efficiently process this
multimodal temporal graph. By learning to focus only on the important
interactions within the graph, our MTGAT is able to achieve state-of-the-art
performance on multimodal sentiment analysis and emotion recognition benchmarks
including IEMOCAP and CMU-MOSI, while utilizing significantly fewer
computations.
</p>
<a href="http://arxiv.org/abs/2010.11985" target="_blank">arXiv:2010.11985</a> [<a href="http://arxiv.org/pdf/2010.11985" target="_blank">pdf</a>]

<h2>Meta-Learning for Domain Generalization in Semantic Parsing. (arXiv:2010.11988v1 [cs.CL])</h2>
<h3>Bailin Wang, Mirella Lapata, Ivan Titov</h3>
<p>The importance of building semantic parsers which can be applied to new
domains and generate programs unseen at training has long been acknowledged,
and datasets testing out-of-domain performance are becoming increasingly
available. However, little or no attention has been devoted to studying
learning algorithms or objectives which promote domain generalization, with
virtually all existing approaches relying on standard supervised learning. In
this work, we use a meta-learning framework which targets specifically
zero-shot domain generalization for semantic parsing. We apply a model-agnostic
training algorithm that simulates zero-shot parsing by constructing virtual
train and test sets from disjoint domains. The learning objective capitalizes
on the intuition that gradient steps that improve source-domain performance
should also improve target-domain performance, thus encouraging a parser to
generalize well to unseen target domains. Experimental results on the (English)
Spider and Chinese Spider datasets show that the meta-learning objective
significantly boosts the performance of a baseline parser.
</p>
<a href="http://arxiv.org/abs/2010.11988" target="_blank">arXiv:2010.11988</a> [<a href="http://arxiv.org/pdf/2010.11988" target="_blank">pdf</a>]

<h2>Atlas Fusion -- Modern Framework for Autonomous Agent Sensor Data Fusion. (arXiv:2010.11991v1 [cs.RO])</h2>
<h3>Adam Ligocki, Ales Jelinek, Ludek Zalud</h3>
<p>In this paper, we present our new sensor fusion framework for self-driving
cars and other autonomous robots. We have designed our framework as a universal
and scalable platform for building up a robust 3D model of the agent's
surrounding environment by fusing a wide range of various sensors into the data
model that we can use as a basement for the decision making and planning
algorithms. Our software currently covers the data fusion of the RGB and
thermal cameras, 3D LiDARs, 3D IMU, and a GNSS positioning. The framework
covers a complete pipeline from data loading, filtering, preprocessing,
environment model construction, visualization, and data storage. The
architecture allows the community to modify the existing setup or to extend our
solution with new ideas. The entire software is fully compatible with ROS
(Robotic Operation System), which allows the framework to cooperate with other
ROS-based software. The source codes are fully available as an open-source
under the MIT license. See https://github.com/Robotics-BUT/Atlas-Fusion.
</p>
<a href="http://arxiv.org/abs/2010.11991" target="_blank">arXiv:2010.11991</a> [<a href="http://arxiv.org/pdf/2010.11991" target="_blank">pdf</a>]

<h2>Unsupervised deep learning for grading of age-related macular degeneration using retinal fundus images. (arXiv:2010.11993v1 [cs.CV])</h2>
<h3>Baladitya Yellapragada, Sascha Hornhauer, Kiersten Snyder, Stella Yu, Glenn Yiu</h3>
<p>Many diseases are classified based on human-defined rubrics that are prone to
bias. Supervised neural networks can automate the grading of retinal fundus
images, but require labor-intensive annotations and are restricted to the
specific trained task. Here, we employed an unsupervised network with
Non-Parametric Instance Discrimination (NPID) to grade age-related macular
degeneration (AMD) severity using fundus photographs from the Age-Related Eye
Disease Study (AREDS). Our unsupervised algorithm demonstrated versatility
across different AMD classification schemes without retraining, and achieved
unbalanced accuracies comparable to supervised networks and human
ophthalmologists in classifying advanced or referable AMD, or on the 4-step AMD
severity scale. Exploring the networks behavior revealed disease-related fundus
features that drove predictions and unveiled the susceptibility of more
granular human-defined AMD severity schemes to misclassification by both
ophthalmologists and neural networks. Importantly, unsupervised learning
enabled unbiased, data-driven discovery of AMD features such as geographic
atrophy, as well as other ocular phenotypes of the choroid, vitreous, and lens,
such as visually-impairing cataracts, that were not pre-defined by human
labels.
</p>
<a href="http://arxiv.org/abs/2010.11993" target="_blank">arXiv:2010.11993</a> [<a href="http://arxiv.org/pdf/2010.11993" target="_blank">pdf</a>]

<h2>On the impact of publicly available news and information transfer to financial markets. (arXiv:2010.12002v1 [q-fin.ST])</h2>
<h3>Metod Jazbec, Barna P&#xe1;sztor, Felix Faltings, Nino Antulov-Fantulin, Petter N. Kolm</h3>
<p>We quantify the propagation and absorption of large-scale publicly available
news articles from the World Wide Web to financial markets. To extract publicly
available information, we use the news archives from the Common Crawl, a
nonprofit organization that crawls a large part of the web. We develop a
processing pipeline to identify news articles associated with the constituent
companies in the S\&amp;P 500 index, an equity market index that measures the stock
performance of U.S. companies. Using machine learning techniques, we extract
sentiment scores from the Common Crawl News data and employ tools from
information theory to quantify the information transfer from public news
articles to the U.S. stock market. Furthermore, we analyze and quantify the
economic significance of the news-based information with a simple
sentiment-based portfolio trading strategy. Our findings provides support for
that information in publicly available news on the World Wide Web has a
statistically and economically significant impact on events in financial
markets.
</p>
<a href="http://arxiv.org/abs/2010.12002" target="_blank">arXiv:2010.12002</a> [<a href="http://arxiv.org/pdf/2010.12002" target="_blank">pdf</a>]

<h2>CellCycleGAN: Spatiotemporal Microscopy Image Synthesis of Cell Populations using Statistical Shape Models and Conditional GANs. (arXiv:2010.12011v1 [eess.IV])</h2>
<h3>Dennis B&#xe4;hr, Dennis Eschweiler, Anuk Bhattacharyya, Daniel Moreno-Andr&#xe9;s, Wolfram Antonin, Johannes Stegmaier</h3>
<p>Automatic analysis of spatio-temporal microscopy images is inevitable for
state-of-the-art research in the life sciences. Recent developments in deep
learning provide powerful tools for automatic analyses of such image data, but
heavily depend on the amount and quality of provided training data to perform
well. To this end, we developed a new method for realistic generation of
synthetic 2D+t microscopy image data of fluorescently labeled cellular nuclei.
The method combines spatiotemporal statistical shape models of different cell
cycle stages with a conditional GAN to generate time series of cell populations
and provides instance-level control of cell cycle stage and the fluorescence
intensity of generated cells. We show the effect of the GAN conditioning and
create a set of synthetic images that can be readily used for training and
benchmarking of cell segmentation and tracking approaches.
</p>
<a href="http://arxiv.org/abs/2010.12011" target="_blank">arXiv:2010.12011</a> [<a href="http://arxiv.org/pdf/2010.12011" target="_blank">pdf</a>]

<h2>Deep neural networks for collaborative learning analytics: Evaluating team collaborations using student gaze point prediction. (arXiv:2010.12012v1 [cs.CY])</h2>
<h3>Zang Guo, Roghayeh Barmaki</h3>
<p>Automatic assessment and evaluation of team performance during collaborative
tasks is key to the learning analytics and computer-supported cooperative work
research. There is a growing interest in the use of gaze-oriented cues for
evaluating the collaboration and cooperativeness of teams. However, collecting
gaze data using eye-trackers is not always feasible due to time and cost
constraints. In this paper, we introduce an automated team assessment tool
based on gaze points and joint visual attention (JVA) information extracted by
computer vision solutions. We then evaluate team collaborations in an
undergraduate anatomy learning activity (N=60, 30 teams) as a test user-study.
The results indicate that higher JVA was positively associated with student
learning outcomes (r(30)=0.50,p&lt;0.005). Moreover, teams who participated in two
experimental groups, and used interactive 3-D anatomy models, had higher JVA
(F(1,28)=6.65,p&lt;0.05) and better knowledge retention (F(1,28) =7.56,p&lt;0.05)
than those in the control group. Also, no significant difference was observed
based on JVA for different gender compositions of teams. The findings from this
work offer implications in learning sciences and collaborative computing by
providing a novel mutual attention-based measure to objectively evaluate team
collaboration dynamics.
</p>
<a href="http://arxiv.org/abs/2010.12012" target="_blank">arXiv:2010.12012</a> [<a href="http://arxiv.org/pdf/2010.12012" target="_blank">pdf</a>]

<h2>Listening to Sounds of Silence for Speech Denoising. (arXiv:2010.12013v1 [cs.SD])</h2>
<h3>Ruilin Xu, Rundi Wu, Yuko Ishiwaka, Carl Vondrick, Changxi Zheng</h3>
<p>We introduce a deep learning model for speech denoising, a long-standing
challenge in audio analysis arising in numerous applications. Our approach is
based on a key observation about human speech: there is often a short pause
between each sentence or word. In a recorded speech signal, those pauses
introduce a series of time periods during which only noise is present. We
leverage these incidental silent intervals to learn a model for automatic
speech denoising given only mono-channel audio. Detected silent intervals over
time expose not just pure noise but its time-varying features, allowing the
model to learn noise dynamics and suppress it from the speech signal.
Experiments on multiple datasets confirm the pivotal role of silent interval
detection for speech denoising, and our method outperforms several
state-of-the-art denoising methods, including those that accept only audio
input (like ours) and those that denoise based on audiovisual input (and hence
require more information). We also show that our method enjoys excellent
generalization properties, such as denoising spoken languages not seen during
training.
</p>
<a href="http://arxiv.org/abs/2010.12013" target="_blank">arXiv:2010.12013</a> [<a href="http://arxiv.org/pdf/2010.12013" target="_blank">pdf</a>]

<h2>Towards falsifiable interpretability research. (arXiv:2010.12016v1 [cs.CY])</h2>
<h3>Matthew L. Leavitt, Ari Morcos</h3>
<p>Methods for understanding the decisions of and mechanisms underlying deep
neural networks (DNNs) typically rely on building intuition by emphasizing
sensory or semantic features of individual examples. For instance, methods aim
to visualize the components of an input which are "important" to a network's
decision, or to measure the semantic properties of single neurons. Here, we
argue that interpretability research suffers from an over-reliance on
intuition-based approaches that risk-and in some cases have caused-illusory
progress and misleading conclusions. We identify a set of limitations that we
argue impede meaningful progress in interpretability research, and examine two
popular classes of interpretability methods-saliency and single-neuron-based
approaches-that serve as case studies for how overreliance on intuition and
lack of falsifiability can undermine interpretability research. To address
these concerns, we propose a strategy to address these impediments in the form
of a framework for strongly falsifiable interpretability research. We encourage
researchers to use their intuitions as a starting point to develop and test
clear, falsifiable hypotheses, and hope that our framework yields robust,
evidence-based interpretability methods that generate meaningful advances in
our understanding of DNNs.
</p>
<a href="http://arxiv.org/abs/2010.12016" target="_blank">arXiv:2010.12016</a> [<a href="http://arxiv.org/pdf/2010.12016" target="_blank">pdf</a>]

<h2>AutoPruning for Deep Neural Network with Dynamic Channel Masking. (arXiv:2010.12021v1 [cs.CV])</h2>
<h3>Baopu Li, Yanwen Fan, Zhihong Pan, Teng Xi, Gang Zhang</h3>
<p>Modern deep neural network models are large and computationally intensive.
One typical solution to this issue is model pruning. However, most current
pruning algorithms depend on hand crafted rules or domain expertise. To
overcome this problem, we propose a learning based auto pruning algorithm for
deep neural network, which is inspired by recent automatic machine
learning(AutoML). A two objectives' problem that aims for the the weights and
the best channels for each layer is first formulated. An alternative
optimization approach is then proposed to derive the optimal channel numbers
and weights simultaneously. In the process of pruning, we utilize a searchable
hyperparameter, remaining ratio, to denote the number of channels in each
convolution layer, and then a dynamic masking process is proposed to describe
the corresponding channel evolution. To control the trade-off between the
accuracy of a model and the pruning ratio of floating point operations, a novel
loss function is further introduced. Preliminary experimental results on
benchmark datasets demonstrate that our scheme achieves competitive results for
neural network pruning.
</p>
<a href="http://arxiv.org/abs/2010.12021" target="_blank">arXiv:2010.12021</a> [<a href="http://arxiv.org/pdf/2010.12021" target="_blank">pdf</a>]

<h2>Comprehensive Attention Self-Distillation for Weakly-Supervised Object Detection. (arXiv:2010.12023v1 [cs.CV])</h2>
<h3>Zeyi Huang, Yang Zou, Vijayakumar Bhagavatula, Dong Huang</h3>
<p>Weakly Supervised Object Detection (WSOD) has emerged as an effective tool to
train object detectors using only the image-level category labels. However,
without object-level labels, WSOD detectors are prone to detect bounding boxes
on salient objects, clustered objects and discriminative object parts.
Moreover, the image-level category labels do not enforce consistent object
detection across different transformations of the same images. To address the
above issues, we propose a Comprehensive Attention Self-Distillation (CASD)
training approach for WSOD. To balance feature learning among all object
instances, CASD computes the comprehensive attention aggregated from multiple
transformations and feature layers of the same images. To enforce consistent
spatial supervision on objects, CASD conducts self-distillation on the WSOD
networks, such that the comprehensive attention is approximated simultaneously
by multiple transformations and feature layers of the same images. CASD
produces new state-of-the-art WSOD results on standard benchmarks such as
PASCAL VOC 2007/2012 and MS-COCO.
</p>
<a href="http://arxiv.org/abs/2010.12023" target="_blank">arXiv:2010.12023</a> [<a href="http://arxiv.org/pdf/2010.12023" target="_blank">pdf</a>]

<h2>Combination of Deep Speaker Embeddings for Diarisation. (arXiv:2010.12025v1 [cs.SD])</h2>
<h3>Guangzhi Sun, Chao Zhang, Phil Woodland</h3>
<p>Recently, significant progress has been made in speaker diarisation after the
introduction of d-vectors as speaker embeddings extracted from the neural
network (NN) speaker classifiers for clustering speech segments. To extract
better-performing and more robust speaker embeddings, this paper proposes a
c-vector method by combining multiple sets of complementary d-vectors derived
from systems with different NN components. Three structures are used to
implement the c-vectors, namely 2D self-attentive, gated additive, and bilinear
pooling structures, relying on attention mechanisms, a gating mechanism, and a
low-rank bilinear pooling mechanism respectively. Furthermore, a neural-based
single-pass speaker diarisation pipeline is also proposed in this paper, which
uses NNs to achieve voice activity detection, speaker change point detection,
and speaker embedding extraction. Experiments and detailed analyses are
conducted on the challenging AMI and NIST RT05 datasets which consist of real
meetings with 4--10 speakers and a wide range of acoustic conditions.
Consistent improvements are obtained by using c-vectors instead of d-vectors,
and similar relative improvements in diarisation error rates are observed on
both AMI and RT05, which shows the robustness of the proposed methods.
</p>
<a href="http://arxiv.org/abs/2010.12025" target="_blank">arXiv:2010.12025</a> [<a href="http://arxiv.org/pdf/2010.12025" target="_blank">pdf</a>]

<h2>"Healthy surveillance": Designing a concept for privacy-preserving mask recognition AI in the age of pandemics. (arXiv:2010.12026v1 [cs.CY])</h2>
<h3>Niklas K&#xfc;hl, Dominik Martin, Clemens Wolff, Melanie Volkamer</h3>
<p>The obligation to wear masks in times of pandemics reduces the risk of
spreading viruses. In case of the COVID-19 pandemic in 2020, many governments
recommended or even obligated their citizens to wear masks as an effective
countermeasure. In order to continuously monitor the compliance of this policy
measure in public spaces like restaurants or tram stations by public
authorities, one scalable and automatable option depicts the application of
surveillance systems, i.e., CCTV. However, large-scale monitoring of mask
recognition does not only require a well-performing Artificial Intelligence,
but also ensure that no privacy issues are introduced, as surveillance is a
deterrent for citizens and regulations like General Data Protection Regulation
(GDPR) demand strict regulations of such personal data. In this work, we show
how a privacy-preserving mask recognition artifact could look like, demonstrate
different options for implementation and evaluate performances. Our conceptual
deep-learning based Artificial Intelligence is able to achieve detection
performances between 95% and 99% in a privacy-friendly setting. On that basis,
we elaborate on the trade-off between the level of privacy preservation and
Artificial Intelligence performance, i.e. the "price of privacy".
</p>
<a href="http://arxiv.org/abs/2010.12026" target="_blank">arXiv:2010.12026</a> [<a href="http://arxiv.org/pdf/2010.12026" target="_blank">pdf</a>]

<h2>Predicting future state for adaptive clinical pathway management. (arXiv:2010.12027v1 [cs.CY])</h2>
<h3>Hong Sun, D&#xf6;rthe Arndt, Jos De Roo, Erik Mannens</h3>
<p>Clinical decision support systems are assisting physicians in providing care
to patients. However, in the context of clinical pathway management such
systems are rather limited as they only take the current state of the patient
into account and ignore the possible evolvement of that state in the future. In
the past decade, the availability of big data in the healthcare domain did open
a new era for clinical decision support. Machine learning technologies are now
widely used in the clinical domain, nevertheless, mostly as a tool for disease
prediction. A tool that not only predicts future states, but also enables
adaptive clinical pathway management based on these predictions is still in
need. This paper introduces weighted state transition logic, a logic to model
state changes based on actions planned in clinical pathways. Weighted state
transition logic extends linear logic by taking weights -- numerical values
indicating the quality of an action or an entire clinical pathway -- into
account. It allows us to predict the future states of a patient and it enables
adaptive clinical pathway management based on these predictions. We provide an
implementation of weighted state transition logic using semantic web
technologies, which makes it easy to integrate semantic data and rules as
background knowledge. Executed by a semantic reasoner, it is possible to
generate a clinical pathway towards a target state, as well as to detect
potential conflicts in the future when multiple pathways are coexisting. The
transitions from the current state to the predicted future state are traceable,
which builds trust from human users on the generated pathway.
</p>
<a href="http://arxiv.org/abs/2010.12027" target="_blank">arXiv:2010.12027</a> [<a href="http://arxiv.org/pdf/2010.12027" target="_blank">pdf</a>]

<h2>Automating Abnormality Detection in Musculoskeletal Radiographs through Deep Learning. (arXiv:2010.12030v1 [eess.IV])</h2>
<h3>Goodarz Mehr</h3>
<p>This paper introduces MuRAD (Musculoskeletal Radiograph Abnormality Detection
tool), a tool that can help radiologists automate the detection of
abnormalities in musculoskeletal radiographs (bone X-rays). MuRAD utilizes a
Convolutional Neural Network (CNN) that can accurately predict whether a bone
X-ray is abnormal, and leverages Class Activation Map (CAM) to localize the
abnormality in the image. MuRAD achieves an F1 score of 0.822 and a Cohen's
kappa of 0.699, which is comparable to the performance of expert radiologists.
</p>
<a href="http://arxiv.org/abs/2010.12030" target="_blank">arXiv:2010.12030</a> [<a href="http://arxiv.org/pdf/2010.12030" target="_blank">pdf</a>]

<h2>Keep your Eyes on the Lane: Attention-guided Lane Detection. (arXiv:2010.12035v1 [cs.CV])</h2>
<h3>Lucas Tabelini, Rodrigo Berriel, Thiago M. Paix&#xe3;o, Claudine Badue, Alberto F. De Souza, Thiago Olivera-Santos</h3>
<p>Modern lane detection methods have achieved remarkable performances in
complex real-world scenarios, but many have issues maintaining real-time
efficiency, which is important for autonomous vehicles. In this work, we
propose LaneATT: an anchor-based deep lane detection model, which, akin to
other generic deep object detectors, uses the anchors for the feature pooling
step. Since lanes follow a regular pattern and are highly correlated, we
hypothesize that in some cases global information may be crucial to infer their
positions, especially in conditions such as occlusion, missing lane markers,
and others. Thus, we propose a novel anchor-based attention mechanism that
aggregates global information. The model was evaluated extensively on two of
the most widely used datasets in the literature. The results show that our
method outperforms the current state-of-the-art methods showing both a higher
efficacy and efficiency. Moreover, we perform an ablation study and discuss
efficiency trade-off options that are useful in practice. To reproduce our
findings, source code and pretrained models are available at
https://github.com/lucastabelini/LaneATT
</p>
<a href="http://arxiv.org/abs/2010.12035" target="_blank">arXiv:2010.12035</a> [<a href="http://arxiv.org/pdf/2010.12035" target="_blank">pdf</a>]

<h2>Supporting Tool for The Transition of Existing Small and Medium Enterprises Towards Industry 4.0. (arXiv:2010.12038v1 [cs.CY])</h2>
<h3>Miguel Baritto, Md Mashum Billal, S. M. Muntasir Nasim, Rumana Afroz Sultana, Mohammad Arani, Ahmed Jawad Qureshi</h3>
<p>The rapid growth of Industry 4.0 technologies such as big data, cloud
computing, smart sensors, machine learning (ML), radio-frequency identification
(RFID), robotics, 3D-printing, and Internet of Things (IoT) offers Small and
Medium Enterprises (SMEs) the chance to improve productivity and efficiency,
reduce cost and provide better customer experience, among other benefits. The
main purpose of this work is to propose a methodology to support SMEs managers
in better understanding the specific requirements for the implementation of
Industry 4.0 solutions and the derived benefits within their firms. A proposed
methodology will be helpful for SMEs manager to take a decision regarding when
and how to migrate to Industry 4.0.
</p>
<a href="http://arxiv.org/abs/2010.12038" target="_blank">arXiv:2010.12038</a> [<a href="http://arxiv.org/pdf/2010.12038" target="_blank">pdf</a>]

<h2>Reducing Adversarially Robust Learning to Non-Robust PAC Learning. (arXiv:2010.12039v1 [cs.LG])</h2>
<h3>Omar Montasser, Steve Hanneke, Nathan Srebro</h3>
<p>We study the problem of reducing adversarially robust learning to standard
PAC learning, i.e. the complexity of learning adversarially robust predictors
using access to only a black-box non-robust learner. We give a reduction that
can robustly learn any hypothesis class $\mathcal{C}$ using any non-robust
learner $\mathcal{A}$ for $\mathcal{C}$. The number of calls to $\mathcal{A}$
depends logarithmically on the number of allowed adversarial perturbations per
example, and we give a lower bound showing this is unavoidable.
</p>
<a href="http://arxiv.org/abs/2010.12039" target="_blank">arXiv:2010.12039</a> [<a href="http://arxiv.org/pdf/2010.12039" target="_blank">pdf</a>]

<h2>Deep Image Prior for Sparse-sampling Photoacoustic Microscopy. (arXiv:2010.12041v1 [eess.IV])</h2>
<h3>Tri Vu, Anthony DiSpirito III, Daiwei Li, Zixuan Zhang, Xiaoyi Zhu, Maomao Chen, Dong Zhang, Jianwen Luo, Yu Shrike Zhang, Roarke Horstmeyer, Junjie Yao</h3>
<p>Photoacoustic microscopy (PAM) is an emerging method for imaging both
structural and functional information without the need for exogenous contrast
agents. However, state-of-the-art PAM faces a tradeoff between imaging speed
and spatial sampling density within the same field-of-view (FOV). Limited by
the pulsed laser's repetition rate, the imaging speed is inversely proportional
to the total number of effective pixels. To cover the same FOV in a shorter
amount of time with the same PAM hardware, there is currently no other option
than to decrease spatial sampling density (i.e., sparse sampling). Deep
learning methods have recently been used to improve sparsely sampled PAM
images; however, these methods often require time-consuming pre-training and a
large training dataset that has fully sampled, co-registered ground truth. In
this paper, we propose using a method known as "deep image prior" to improve
the image quality of sparsely sampled PAM images. The network does not need
prior learning or fully sampled ground truth, making its implementation more
flexible and much quicker. Our results show promising improvement in PA
vasculature images with as few as 2% of the effective pixels. Our deep image
prior approach produces results that outperform interpolation methods and can
be readily translated to other high-speed, sparse-sampling imaging modalities.
</p>
<a href="http://arxiv.org/abs/2010.12041" target="_blank">arXiv:2010.12041</a> [<a href="http://arxiv.org/pdf/2010.12041" target="_blank">pdf</a>]

<h2>SAINT+: Integrating Temporal Features for EdNet Correctness Prediction. (arXiv:2010.12042v1 [cs.CY])</h2>
<h3>Dongmin Shin, Yugeun Shim, Hangyeol Yu, Seewoo Lee, Byungsoo Kim, Youngduck Choi</h3>
<p>We propose SAINT+, a successor of SAINT which is a Transformer based
knowledge tracing model that separately processes exercise information and
student response information. Following the architecture of SAINT, SAINT+ has
an encoder-decoder structure where the encoder applies self-attention layers to
a stream of exercise embeddings, and the decoder alternately applies
self-attention layers and encoder-decoder attention layers to streams of
response embeddings and encoder output. Moreover, SAINT+ incorporates two
temporal feature embeddings into the response embeddings: elapsed time, the
time taken for a student to answer, and lag time, the time interval between
adjacent learning activities. We empirically evaluate the effectiveness of
SAINT+ on EdNet, the largest publicly available benchmark dataset in the
education domain. Experimental results show that SAINT+ achieves
state-of-the-art performance in knowledge tracing with an improvement of 1.25%
in area under receiver operating characteristic curve compared to SAINT, the
current state-of-the-art model in EdNet dataset.
</p>
<a href="http://arxiv.org/abs/2010.12042" target="_blank">arXiv:2010.12042</a> [<a href="http://arxiv.org/pdf/2010.12042" target="_blank">pdf</a>]

<h2>Using Deep Image Priors to Generate Counterfactual Explanations. (arXiv:2010.12046v1 [cs.LG])</h2>
<h3>Vivek Narayanaswamy, Jayaraman J. Thiagarajan, Andreas Spanias</h3>
<p>Through the use of carefully tailored convolutional neural network
architectures, a deep image prior (DIP) can be used to obtain pre-images from
latent representation encodings. Though DIP inversion has been known to be
superior to conventional regularized inversion strategies such as total
variation, such an over-parameterized generator is able to effectively
reconstruct even images that are not in the original data distribution. This
limitation makes it challenging to utilize such priors for tasks such as
counterfactual reasoning, wherein the goal is to generate small, interpretable
changes to an image that systematically leads to changes in the model
prediction. To this end, we propose a novel regularization strategy based on an
auxiliary loss estimator jointly trained with the predictor, which efficiently
guides the prior to recover natural pre-images. Our empirical studies with a
real-world ISIC skin lesion detection problem clearly evidence the
effectiveness of the proposed approach in synthesizing meaningful
counterfactuals. In comparison, we find that the standard DIP inversion often
proposes visually imperceptible perturbations to irrelevant parts of the image,
thus providing no additional insights into the model behavior.
</p>
<a href="http://arxiv.org/abs/2010.12046" target="_blank">arXiv:2010.12046</a> [<a href="http://arxiv.org/pdf/2010.12046" target="_blank">pdf</a>]

<h2>Contrastive Learning with Adversarial Examples. (arXiv:2010.12050v1 [cs.CV])</h2>
<h3>Chih-Hui Ho, Nuno Vasconcelos</h3>
<p>Contrastive learning (CL) is a popular technique for self-supervised learning
(SSL) of visual representations. It uses pairs of augmentations of unlabeled
training examples to define a classification task for pretext learning of a
deep embedding. Despite extensive works in augmentation procedures, prior works
do not address the selection of challenging negative pairs, as images within a
sampled batch are treated independently. This paper addresses the problem, by
introducing a new family of adversarial examples for constrastive learning and
using these examples to define a new adversarial training algorithm for SSL,
denoted as CLAE. When compared to standard CL, the use of adversarial examples
creates more challenging positive pairs and adversarial training produces
harder negative pairs by accounting for all images in a batch during the
optimization. CLAE is compatible with many CL methods in the literature.
Experiments show that it improves the performance of several existing CL
baselines on multiple datasets.
</p>
<a href="http://arxiv.org/abs/2010.12050" target="_blank">arXiv:2010.12050</a> [<a href="http://arxiv.org/pdf/2010.12050" target="_blank">pdf</a>]

<h2>Analysis of three dimensional potential problems in non-homogeneous media with deep learning based collocation method. (arXiv:2010.12060v1 [cs.LG])</h2>
<h3>Hongwei Guo, Xiaoying Zhuang, Xiaoyu Meng, Timon Rabczuk</h3>
<p>A deep learning based collocation method is presented in this paper to solve
the three dimensional potential problems in non-homogeneous media. Based on the
universal approximation theorem, the neural network can be utilized to
approximate solutions for different PDEs in different geometries. The
performance of deep learning based method depends on the configurations of the
network and other hyper-parameter settings. This makes the choice of neural
network configurations extremely important. The configuration of this deep
collocation method is setup by comparing different schemes of smooth activation
functions, sampling methods for collocation points generation, combined
optimizers. Besides, a convergence proof of this deep collocation method in
solving non-homogeneous potential problems is performed. Then the deep
collocation method is applied to the analysis of different material variations,
and it can be concluded that the deep collocation method predicts the
temperature and flux accurately for different material variations, especially
the exponential material variations. As a result, the deep learning based
collocation method shows a great potential in approximating solutions to PDEs.
</p>
<a href="http://arxiv.org/abs/2010.12060" target="_blank">arXiv:2010.12060</a> [<a href="http://arxiv.org/pdf/2010.12060" target="_blank">pdf</a>]

<h2>Explaining Neural Network Predictions for Functional Data Using Principal Component Analysis and Feature Importance. (arXiv:2010.12063v1 [cs.LG])</h2>
<h3>Katherine Goode, Daniel Ries, Joshua Zollweg</h3>
<p>Optical spectral-temporal signatures extracted from videos of explosions
provide information for identifying characteristics of the corresponding
explosive devices. Currently, the identification is done using heuristic
algorithms and direct subject matter expert review. An improvement in
predictive performance may be obtained by using machine learning, but this
application lends itself to high consequence national security decisions, so it
is not only important to provide high accuracy but clear explanations for the
predictions to garner confidence in the model. While much work has been done to
develop explainability methods for machine learning models, not much of the
work focuses on situations with input variables of the form of functional data
such optical spectral-temporal signatures. We propose a procedure for
explaining machine learning models fit using functional data that accounts for
the functional nature the data. Our approach makes use of functional principal
component analysis (fPCA) and permutation feature importance (PFI). fPCA is
used to transform the functions to create uncorrelated functional principal
components (fPCs). The model is trained using the fPCs as inputs, and PFI is
applied to identify the fPCs important to the model for prediction.
Visualizations are used to interpret the variability explained by the fPCs that
are found to be important by PFI to determine the aspects of the functions that
are important for prediction. We demonstrate the technique by explaining neural
networks fit to explosion optical spectral-temporal signatures for predicting
characteristics of the explosive devices.
</p>
<a href="http://arxiv.org/abs/2010.12063" target="_blank">arXiv:2010.12063</a> [<a href="http://arxiv.org/pdf/2010.12063" target="_blank">pdf</a>]

<h2>Cloud Energy Micro-Moment Data Classification: A Platform Study. (arXiv:2010.12064v1 [cs.DC])</h2>
<h3>Abdullah Alsalemi, Ayman Al-Kababji, Yassine Himeur, Faycal Bensaali, Abbes Amira</h3>
<p>Energy efficiency is a crucial factor in the well-being of our planet. In
parallel, Machine Learning (ML) plays an instrumental role in automating our
lives and creating convenient workflows for enhancing behavior. So, analyzing
energy behavior can help understand weak points and lay the path towards better
interventions. Moving towards higher performance, cloud platforms can assist
researchers in conducting classification trials that need high computational
power. Under the larger umbrella of the Consumer Engagement Towards Energy
Saving Behavior by means of Exploiting Micro Moments and Mobile Recommendation
Systems (EM)3 framework, we aim to influence consumers behavioral change via
improving their power consumption consciousness. In this paper, common cloud
artificial intelligence platforms are benchmarked and compared for micro-moment
classification. The Amazon Web Services, Google Cloud Platform, Google Colab,
and Microsoft Azure Machine Learning are employed on simulated and real energy
consumption datasets. The KNN, DNN, and SVM classifiers have been employed.
Superb performance has been observed in the selected cloud platforms, showing
relatively close performance. Yet, the nature of some algorithms limits the
training performance.
</p>
<a href="http://arxiv.org/abs/2010.12064" target="_blank">arXiv:2010.12064</a> [<a href="http://arxiv.org/pdf/2010.12064" target="_blank">pdf</a>]

<h2>A generalized deep learning model for multi-disease Chest X-Ray diagnostics. (arXiv:2010.12065v1 [q-bio.QM])</h2>
<h3>Nabit Bajwa, Kedar Bajwa, Atif Rana, M. Faique Shakeel, Kashif Haqqi, Suleiman Ali Khan</h3>
<p>We investigate the generalizability of deep convolutional neural network
(CNN) on the task of disease classification from chest x-rays collected over
multiple sites. We systematically train the model using datasets from three
independent sites with different patient populations: National Institute of
Health (NIH), Stanford University Medical Centre (CheXpert), and Shifa
International Hospital (SIH). We formulate a sequential training approach and
demonstrate that the model produces generalized prediction performance using
held out test sets from the three sites. Our model generalizes better when
trained on multiple datasets, with the CheXpert-Shifa-NET model performing
significantly better (p-values &lt; 0.05) than the models trained on individual
datasets for 3 out of the 4 distinct disease classes. The code for training the
model will be made available open source at: www.github.com/link-to-code at the
time of publication.
</p>
<a href="http://arxiv.org/abs/2010.12065" target="_blank">arXiv:2010.12065</a> [<a href="http://arxiv.org/pdf/2010.12065" target="_blank">pdf</a>]

<h2>Learning Patterns in Imaginary Vowels for an Intelligent Brain Computer Interface (BCI) Design. (arXiv:2010.12066v1 [cs.LG])</h2>
<h3>Parisa Ghane, Gahangir Hossain</h3>
<p>Technology advancements made it easy to measure non-invasive and high-quality
electroencephalograph (EEG) signals from human's brain. Hence, development of
robust and high-performance AI algorithms becomes crucial to properly process
the EEG signals and recognize the patterns, which lead to an appropriate
control signal. Despite the advancements in processing the motor imagery EEG
signals, the healthcare applications, such as emotion detection, are still in
the early stages of AI design. In this paper, we propose a modular framework
for the recognition of vowels as the AI part of a brain computer interface
system. We carefully designed the modules to discriminate the English vowels
given the raw EEG signals, and meanwhile avoid the typical issued with the
data-poor environments like most of the healthcare applications. The proposed
framework consists of appropriate signal segmentation, filtering, extraction of
spectral features, reducing the dimensions by means of principle component
analysis, and finally a multi-class classification by decision-tree-based
support vector machine (DT-SVM). The performance of our framework was evaluated
by a combination of test-set and resubstitution (also known as apparent) error
rates. We provide the algorithms of the proposed framework to make it easy for
future researchers and developers who want to follow the same workflow.
</p>
<a href="http://arxiv.org/abs/2010.12066" target="_blank">arXiv:2010.12066</a> [<a href="http://arxiv.org/pdf/2010.12066" target="_blank">pdf</a>]

<h2>Dynamics and Domain Randomized Gait Modulation with Bezier Curves for Sim-to-Real Legged Locomotion. (arXiv:2010.12070v1 [cs.RO])</h2>
<h3>Maurice Rahme, Ian Abraham, Matthew L. Elwin, Todd D. Murphey</h3>
<p>We present a sim-to-real framework that uses dynamics and domain randomized
offline reinforcement learning to enhance open-loop gaits for legged robots,
allowing them to traverse uneven terrain without sensing foot impacts. Our
approach, D$^2$-Randomized Gait Modulation with Bezier Curves (D$^2$-GMBC),
uses augmented random search with randomized dynamics and terrain to train, in
simulation, a policy that modifies the parameters and output of an open-loop
Bezier curve gait generator for quadrupedal robots. The policy, using only
inertial measurements, enables the robot to traverse unknown rough terrain,
even when the robot's physical parameters do not match the open-loop model.

We compare the resulting policy to hand-tuned Bezier Curve gaits and to
policies trained without randomization, both in simulation and on a real
quadrupedal robot. With D$^2$-GMBC, across a variety of experiments on
unobserved and unknown uneven terrain, the robot walks significantly farther
than with either hand-tuned gaits or gaits learned without domain
randomization. Additionally, using D$^2$-GMBC, the robot can walk laterally and
rotate while on the rough terrain, even though it was trained only for forward
walking.
</p>
<a href="http://arxiv.org/abs/2010.12070" target="_blank">arXiv:2010.12070</a> [<a href="http://arxiv.org/pdf/2010.12070" target="_blank">pdf</a>]

<h2>Getting Passive Aggressive About False Positives: Patching Deployed Malware Detectors. (arXiv:2010.12080v1 [cs.LG])</h2>
<h3>Edward Raff, Bobby Filar, James Holt</h3>
<p>False positives (FPs) have been an issue of extreme importance for anti-virus
(AV) systems for decades. As more security vendors turn to machine learning,
alert deluge has hit critical mass with over 20% of all alerts resulting in FPs
and, in some organizations, the number reaches half of all alerts. This
increase has resulted in fatigue, frustration, and, worst of all, neglect from
security workers on SOC teams. A foundational cause for FPs is that vendors
must build one global system to try and satisfy all customers, but have no
method to adjust to individual local environments. This leads to outrageous,
albeit technically correct, characterization of their platforms being 99.9%
effective. Once these systems are deployed the idiosyncrasies of individual,
local environments expose blind spots that lead to FPs and uncertainty.

We propose a strategy for fixing false positives in production after a model
has already been deployed. For too long the industry has tried to combat these
problems with inefficient, and at times, dangerous allowlist techniques and
excessive model retraining which is no longer enough. We propose using a
technique called passive-aggressive learning to alter a malware detection model
to an individual's environment, eliminating false positives without sharing any
customer sensitive information. We will show how to use passive-aggressive
learning to solve a collection of notoriously difficult false positives from a
production environment without compromising the malware model's accuracy,
reducing the total number of FP alerts by an average of 23x.
</p>
<a href="http://arxiv.org/abs/2010.12080" target="_blank">arXiv:2010.12080</a> [<a href="http://arxiv.org/pdf/2010.12080" target="_blank">pdf</a>]

<h2>A Multilinear Sampling Algorithm to Estimate Shapley Values. (arXiv:2010.12082v1 [cs.LG])</h2>
<h3>Ramin Okhrati, Aldo Lipani</h3>
<p>Shapley values are great analytical tools in game theory to measure the
importance of a player in a game. Due to their axiomatic and desirable
properties such as efficiency, they have become popular for feature importance
analysis in data science and machine learning. However, the time complexity to
compute Shapley values based on the original formula is exponential, and as the
number of features increases, this becomes infeasible. Castro et al. [1]
developed a sampling algorithm, to estimate Shapley values. In this work, we
propose a new sampling method based on a multilinear extension technique as
applied in game theory. The aim is to provide a more efficient (sampling)
method for estimating Shapley values. Our method is applicable to any machine
learning model, in particular for either multi-class classifications or
regression problems. We apply the method to estimate Shapley values for
multilayer perceptrons (MLPs) and through experimentation on two datasets, we
demonstrate that our method provides more accurate estimations of the Shapley
values by reducing the variance of the sampling statistics.
</p>
<a href="http://arxiv.org/abs/2010.12082" target="_blank">arXiv:2010.12082</a> [<a href="http://arxiv.org/pdf/2010.12082" target="_blank">pdf</a>]

<h2>Language-Conditioned Imitation Learning for Robot Manipulation Tasks. (arXiv:2010.12083v1 [cs.RO])</h2>
<h3>Simon Stepputtis, Joseph Campbell, Mariano Phielipp, Stefan Lee, Chitta Baral, Heni Ben Amor</h3>
<p>Imitation learning is a popular approach for teaching motor skills to robots.
However, most approaches focus on extracting policy parameters from execution
traces alone (i.e., motion trajectories and perceptual data). No adequate
communication channel exists between the human expert and the robot to describe
critical aspects of the task, such as the properties of the target object or
the intended shape of the motion. Motivated by insights into the human teaching
process, we introduce a method for incorporating unstructured natural language
into imitation learning. At training time, the expert can provide
demonstrations along with verbal descriptions in order to describe the
underlying intent (e.g., "go to the large green bowl"). The training process
then interrelates these two modalities to encode the correlations between
language, perception, and motion. The resulting language-conditioned visuomotor
policies can be conditioned at runtime on new human commands and instructions,
which allows for more fine-grained control over the trained policies while also
reducing situational ambiguity. We demonstrate in a set of simulation
experiments how our approach can learn language-conditioned manipulation
policies for a seven-degree-of-freedom robot arm and compare the results to a
variety of alternative methods.
</p>
<a href="http://arxiv.org/abs/2010.12083" target="_blank">arXiv:2010.12083</a> [<a href="http://arxiv.org/pdf/2010.12083" target="_blank">pdf</a>]

<h2>Few-shot Image Recognition with Manifolds. (arXiv:2010.12084v1 [cs.CV])</h2>
<h3>Debasmit Das, J.H. Moon, C. S. George Lee</h3>
<p>In this paper, we extend the traditional few-shot learning (FSL) problem to
the situation when the source-domain data is not accessible but only high-level
information in the form of class prototypes is available. This limited
information setup for the FSL problem deserves much attention due to its
implication of privacy-preserving inaccessibility to the source-domain data but
it has rarely been addressed before. Because of limited training data, we
propose a non-parametric approach to this FSL problem by assuming that all the
class prototypes are structurally arranged on a manifold. Accordingly, we
estimate the novel-class prototype locations by projecting the few-shot samples
onto the average of the subspaces on which the surrounding classes lie. During
classification, we again exploit the structural arrangement of the categories
by inducing a Markov chain on the graph constructed with the class prototypes.
This manifold distance obtained using the Markov chain is expected to produce
better results compared to a traditional nearest-neighbor-based Euclidean
distance. To evaluate our proposed framework, we have tested it on two image
datasets - the large-scale ImageNet and the small-scale but fine-grained
CUB-200. We have also studied parameter sensitivity to better understand our
framework.
</p>
<a href="http://arxiv.org/abs/2010.12084" target="_blank">arXiv:2010.12084</a> [<a href="http://arxiv.org/pdf/2010.12084" target="_blank">pdf</a>]

<h2>Adversarial Robustness of Supervised Sparse Coding. (arXiv:2010.12088v1 [cs.LG])</h2>
<h3>Jeremias Sulam, Ramchandran Muthumukar, Raman Arora</h3>
<p>Several recent results provide theoretical insights into the phenomena of
adversarial examples. Existing results, however, are often limited due to a gap
between the simplicity of the models studied and the complexity of those
deployed in practice. In this work, we strike a better balance by considering a
model that involves learning a representation while at the same time giving a
precise generalization bound and a robustness certificate. We focus on the
hypothesis class obtained by combining a sparsity-promoting encoder coupled
with a linear classifier, and show an interesting interplay between the
expressivity and stability of the (supervised) representation map and a notion
of margin in the feature space. We bound the robust risk (to $\ell_2$-bounded
perturbations) of hypotheses parameterized by dictionaries that achieve a mild
encoder gap on training data. Furthermore, we provide a robustness certificate
for end-to-end classification. We demonstrate the applicability of our analysis
by computing certified accuracy on real data, and compare with other
alternatives for certified robustness.
</p>
<a href="http://arxiv.org/abs/2010.12088" target="_blank">arXiv:2010.12088</a> [<a href="http://arxiv.org/pdf/2010.12088" target="_blank">pdf</a>]

<h2>Improving Streaming Automatic Speech Recognition With Non-Streaming Model Distillation On Unsupervised Data. (arXiv:2010.12096v1 [cs.SD])</h2>
<h3>Thibault Doutre, Wei Han, Min Ma, Zhiyun Lu, Chung-Cheng Chiu, Ruoming Pang, Arun Narayanan, Ananya Misra, Yu Zhang, Liangliang Cao</h3>
<p>Streaming end-to-end automatic speech recognition (ASR) models are widely
used on smart speakers and on-device applications. Since these models are
expected to transcribe speech with minimal latency, they are constrained to be
causal with no future context, compared to their non-streaming counterparts.
Consequently, streaming models usually perform worse than non-streaming models.
We propose a novel and effective learning method by leveraging a non-streaming
ASR model as a teacher to generate transcripts on an arbitrarily large data
set, which is then used to distill knowledge into streaming ASR models. This
way, we scale the training of streaming models to up to 3 million hours of
YouTube audio. Experiments show that our approach can significantly reduce the
word error rate (WER) of RNNT models not only on LibriSpeech but also on
YouTube data in four languages. For example, in French, we are able to reduce
the WER by 16.4% relatively to a baseline streaming model by leveraging a
non-streaming teacher model trained on the same amount of labeled data as the
baseline.
</p>
<a href="http://arxiv.org/abs/2010.12096" target="_blank">arXiv:2010.12096</a> [<a href="http://arxiv.org/pdf/2010.12096" target="_blank">pdf</a>]

<h2>Differentially Private Learning Does Not Bound Membership Inference. (arXiv:2010.12112v1 [cs.CR])</h2>
<h3>Thomas Humphries, Matthew Rafuse, Lindsey Tulloch, Simon Oya, Ian Goldberg, Florian Kerschbaum</h3>
<p>Training machine learning models on privacy-sensitive data has become a
popular practice, driving innovation in ever-expanding fields. This has opened
the door to a series of new attacks, such as Membership Inference Attacks
(MIAs), that exploit vulnerabilities in ML models in order to expose the
privacy of individual training samples. A growing body of literature holds up
Differential Privacy (DP) as an effective defense against such attacks, and
companies like Google and Amazon include this privacy notion in their
machine-learning-as-a-service products. However, little scrutiny has been given
to how underlying correlations within the datasets used for training these
models can impact the privacy guarantees provided by DP. In this work, we
challenge prior findings that suggest DP provides a strong defense against
MIAs. We provide theoretical and experimental evidence for cases where the
theoretical bounds of DP are violated by MIAs using the same attacks described
in prior work. We show this hypothetically with artificial, pathological
datasets as well as with real-world datasets carefully split to create a
distinction between member and non-member samples. Our findings suggest that
certain properties of datasets, such as bias or data correlation, play a
critical role in determining the effectiveness of DP as a privacy preserving
mechanism against MIAs. Further, ensuring that a given dataset is resilient
against these MIAs may be virtually impossible for a data analyst to determine.
</p>
<a href="http://arxiv.org/abs/2010.12112" target="_blank">arXiv:2010.12112</a> [<a href="http://arxiv.org/pdf/2010.12112" target="_blank">pdf</a>]

<h2>Knowledge Graph Embedding with Atrous Convolution and Residual Learning. (arXiv:2010.12121v1 [cs.AI])</h2>
<h3>Feiliang Ren, Juchen Li, Huihui Zhang, Shilei Liu, Bochao Li, Ruicheng Ming, Yujia Bai</h3>
<p>Knowledge graph embedding is an important task and it will benefit lots of
downstream applications. Currently, deep neural networks based methods achieve
state-of-the-art performance. However, most of these existing methods are very
complex and need much time for training and inference. To address this issue,
we propose a simple but effective atrous convolution based knowledge graph
embedding method. Compared with existing state-of-the-art methods, our method
has following main characteristics. First, it effectively increases feature
interactions by using atrous convolutions. Second, to address the original
information forgotten issue and vanishing/exploding gradient issue, it uses the
residual learning method. Third, it has simpler structure but much higher
parameter efficiency. We evaluate our method on six benchmark datasets with
different evaluation metrics. Extensive experiments show that our model is very
effective. On these diverse datasets, it achieves better results than the
compared state-of-the-art methods on most of evaluation metrics. The source
codes of our model could be found at https://github.com/neukg/AcrE.
</p>
<a href="http://arxiv.org/abs/2010.12121" target="_blank">arXiv:2010.12121</a> [<a href="http://arxiv.org/pdf/2010.12121" target="_blank">pdf</a>]

<h2>On the Number of Linear Functions Composing Deep Neural Network: Towards a Refined Definition of Neural Networks Complexity. (arXiv:2010.12125v1 [cs.LG])</h2>
<h3>Yuuki Takai, Akiyoshi Sannai, Matthieu Cordonnier</h3>
<p>The classical approach to measure the expressive power of deep neural
networks with piecewise linear activations is based on counting their maximum
number of linear regions. However, when considering the two different models
which are the fully connected and the permutation invariant ones, this measure
is unable to distinguish them clearly in term of expressivity. To tackle this,
we propose a refined definition of deep neural networks complexity. Instead of
counting the number of linear regions directly, we first introduce an
equivalence relation between the linear functions composing a DNN and then
count those functions relatively to that equivalence relation. We continue with
a study of our new complexity measure and verify that it has the good expected
properties. It is able to distinguish clearly between the two models mentioned
above, it is consistent with the classical measure, and it increases
exponentially with depth. That last point confirms the high expressive power of
deep networks.
</p>
<a href="http://arxiv.org/abs/2010.12125" target="_blank">arXiv:2010.12125</a> [<a href="http://arxiv.org/pdf/2010.12125" target="_blank">pdf</a>]

<h2>Beyond the Deep Metric Learning: Enhance the Cross-Modal Matching with Adversarial Discriminative Domain Regularization. (arXiv:2010.12126v1 [cs.CV])</h2>
<h3>Li Ren, Kai Li, LiQiang Wang, Kien Hua</h3>
<p>Matching information across image and text modalities is a fundamental
challenge for many applications that involve both vision and natural language
processing. The objective is to find efficient similarity metrics to compare
the similarity between visual and textual information. Existing approaches
mainly match the local visual objects and the sentence words in a shared space
with attention mechanisms. The matching performance is still limited because
the similarity computation is based on simple comparisons of the matching
features, ignoring the characteristics of their distribution in the data. In
this paper, we address this limitation with an efficient learning objective
that considers the discriminative feature distributions between the visual
objects and sentence words. Specifically, we propose a novel Adversarial
Discriminative Domain Regularization (ADDR) learning framework, beyond the
paradigm metric learning objective, to construct a set of discriminative data
domains within each image-text pairs. Our approach can generally improve the
learning efficiency and the performance of existing metrics learning frameworks
by regulating the distribution of the hidden space between the matching pairs.
The experimental results show that this new approach significantly improves the
overall performance of several popular cross-modal matching techniques (SCAN,
VSRN, BFAN) on the MS-COCO and Flickr30K benchmarks.
</p>
<a href="http://arxiv.org/abs/2010.12126" target="_blank">arXiv:2010.12126</a> [<a href="http://arxiv.org/pdf/2010.12126" target="_blank">pdf</a>]

<h2>Lamina-specific neuronal properties promote robust, stable signal propagation in feedforward networks. (arXiv:2010.12127v1 [q-bio.NC])</h2>
<h3>Dongqi Han, Erik De Schutter, Sungho Hong</h3>
<p>Feedforward networks (FFN) are ubiquitous structures in neural systems and
have been studied to understand mechanisms of reliable signal and information
transmission. In many FFNs, neurons in one layer have intrinsic properties that
are distinct from those in their pre-/postsynaptic layers, but how this affects
network-level information processing remains unexplored. Here we show that
layer-to-layer heterogeneity arising from lamina-specific cellular properties
facilitates signal and information transmission in FFNs. Specifically, we found
that signal transformations, made by each layer of neurons on an input-driven
spike signal, demodulate signal distortions introduced by preceding layers.
This mechanism boosts information transfer carried by a propagating spike
signal and thereby supports reliable spike signal and information transmission
in a deep FFN. Our study suggests that distinct cell types in neural circuits,
performing different computational functions, facilitate information processing
on the whole.
</p>
<a href="http://arxiv.org/abs/2010.12127" target="_blank">arXiv:2010.12127</a> [<a href="http://arxiv.org/pdf/2010.12127" target="_blank">pdf</a>]

<h2>Graph Geometry Interaction Learning. (arXiv:2010.12135v1 [cs.LG])</h2>
<h3>Shichao Zhu, Shirui Pan, Chuan Zhou, Jia Wu, Yanan Cao, Bin Wang</h3>
<p>While numerous approaches have been developed to embed graphs into either
Euclidean or hyperbolic spaces, they do not fully utilize the information
available in graphs, or lack the flexibility to model intrinsic complex graph
geometry. To utilize the strength of both Euclidean and hyperbolic geometries,
we develop a novel Geometry Interaction Learning (GIL) method for graphs, a
well-suited and efficient alternative for learning abundant geometric
properties in graph. GIL captures a more informative internal structural
features with low dimensions while maintaining conformal invariance of each
space. Furthermore, our method endows each node the freedom to determine the
importance of each geometry space via a flexible dual feature interaction
learning and probability assembling mechanism. Promising experimental results
are presented for five benchmark datasets on node classification and link
prediction tasks.
</p>
<a href="http://arxiv.org/abs/2010.12135" target="_blank">arXiv:2010.12135</a> [<a href="http://arxiv.org/pdf/2010.12135" target="_blank">pdf</a>]

<h2>Rethinking the competition between detection and ReID in Multi-Object Tracking. (arXiv:2010.12138v1 [cs.CV])</h2>
<h3>Chao Liang, Zhipeng Zhang, Yi Lu, Xue Zhou, Bing Li, Xiyong Ye, Jianxiao Zou</h3>
<p>Due to balanced accuracy and speed, joint learning detection and ReID-based
one-shot models have drawn great attention in multi-object tracking(MOT).
However, the differences between the above two tasks in the one-shot tracking
paradigm are unconsciously overlooked, leading to inferior performance than the
two-stage methods. In this paper, we dissect the reasoning process of the
aforementioned two tasks. Our analysis reveals that the competition of them
inevitably hurts the learning of task-dependent representations, which further
impedes the tracking performance. To remedy this issue, we propose a novel
cross-correlation network that can effectively impel the separate branches to
learn task-dependent representations. Furthermore, we introduce a scale-aware
attention network that learns discriminative embeddings to improve the ReID
capability. We integrate the delicately designed networks into a one-shot
online MOT system, dubbed CSTrack. Without bells and whistles, our model
achieves new state-of-the-art performances on MOT16 and MOT17. We will release
our code to facilitate further work.
</p>
<a href="http://arxiv.org/abs/2010.12138" target="_blank">arXiv:2010.12138</a> [<a href="http://arxiv.org/pdf/2010.12138" target="_blank">pdf</a>]

<h2>GSEP: A robust vocal and accompaniment separation system using gated CBHG module and loudness normalization. (arXiv:2010.12139v1 [cs.SD])</h2>
<h3>Soochul Park, Ben Sangbae Chon</h3>
<p>In the field of audio signal processing research, source separation has been
a popular research topic for a long time and the recent adoption of the deep
neural networks have shown a significant improvement in performance. The
improvement vitalizes the industry to productize audio deep learning based
products and services including Karaoke in the music streaming apps and
dialogue enhancement in the UHDTV. For these early markets, we defined a set of
design principles of the vocal and accompaniment separation model in terms of
robustness, quality, and cost. In this paper, we introduce GSEP (Gaudio source
SEParation system), a robust vocal and accompaniment separation system using a
Gated- CBHG module, mask warping, and loudness normalization and it was
verified that the proposed system satisfies all three principles and
outperforms the state-of-the-art systems both in objective measure and
subjective assessment through experiments.
</p>
<a href="http://arxiv.org/abs/2010.12139" target="_blank">arXiv:2010.12139</a> [<a href="http://arxiv.org/pdf/2010.12139" target="_blank">pdf</a>]

<h2>Bridging Imagination and Reality for Model-Based Deep Reinforcement Learning. (arXiv:2010.12142v1 [cs.LG])</h2>
<h3>Guangxiang Zhu, Minghao Zhang, Honglak Lee, Chongjie Zhang</h3>
<p>Sample efficiency has been one of the major challenges for deep reinforcement
learning. Recently, model-based reinforcement learning has been proposed to
address this challenge by performing planning on imaginary trajectories with a
learned world model. However, world model learning may suffer from overfitting
to training trajectories, and thus model-based value estimation and policy
search will be pone to be sucked in an inferior local policy. In this paper, we
propose a novel model-based reinforcement learning algorithm, called BrIdging
Reality and Dream (BIRD). It maximizes the mutual information between imaginary
and real trajectories so that the policy improvement learned from imaginary
trajectories can be easily generalized to real trajectories. We demonstrate
that our approach improves sample efficiency of model-based planning, and
achieves state-of-the-art performance on challenging visual control benchmarks.
</p>
<a href="http://arxiv.org/abs/2010.12142" target="_blank">arXiv:2010.12142</a> [<a href="http://arxiv.org/pdf/2010.12142" target="_blank">pdf</a>]

<h2>One-shot Learning for Temporal Knowledge Graphs. (arXiv:2010.12144v1 [cs.LG])</h2>
<h3>Mehrnoosh Mirtaheri, Mohammad Rostami, Xiang Ren, Fred Morstatter, Aram Galstyan</h3>
<p>Most real-world knowledge graphs are characterized by a long-tail relation
frequency distribution where a significant fraction of relations occurs only a
handful of times. This observation has given rise to recent interest in
low-shot learning methods that are able to generalize from only a few examples.
The existing approaches, however, are tailored to static knowledge graphs and
not easily generalized to temporal settings, where data scarcity poses even
bigger problems, e.g., due to occurrence of new, previously unseen relations.
We address this shortcoming by proposing a one-shot learning framework for link
prediction in temporal knowledge graphs. Our proposed method employs a
self-attention mechanism to effectively encode temporal interactions between
entities, and a network to compute a similarity score between a given query and
a (one-shot) example. Our experiments show that the proposed algorithm
outperforms the state of the art baselines for two well-studied benchmarks
while achieving significantly better performance for sparse relations.
</p>
<a href="http://arxiv.org/abs/2010.12144" target="_blank">arXiv:2010.12144</a> [<a href="http://arxiv.org/pdf/2010.12144" target="_blank">pdf</a>]

<h2>ERNIE-Gram: Pre-Training with Explicitly N-Gram Masked Language Modeling for Natural Language Understanding. (arXiv:2010.12148v1 [cs.CL])</h2>
<h3>Dongling Xiao, Yu-Kun Li, Han Zhang, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang</h3>
<p>Coarse-grained linguistic information, such as name entities or phrases,
facilitates adequately representation learning in pre-training. Previous works
mainly focus on extending the objective of BERT's Masked Language Modeling
(MLM) from masking individual tokens to contiguous sequences of n tokens. We
argue that such continuously masking method neglects to model the
inner-dependencies and inter-relation of coarse-grained information. As an
alternative, we propose ERNIE-Gram, an explicitly n-gram masking method to
enhance the integration of coarse-grained information for pre-training. In
ERNIE-Gram, n-grams are masked and predicted directly using explicit n-gram
identities rather than contiguous sequences of tokens. Furthermore, ERNIE-Gram
employs a generator model to sample plausible n-gram identities as optional
n-gram masks and predict them in both coarse-grained and fine-grained manners
to enable comprehensive n-gram prediction and relation modeling. We pre-train
ERNIE-Gram on English and Chinese text corpora and fine-tune on 19 downstream
tasks. Experimental results show that ERNIE-Gram outperforms previous
pre-training models like XLNet and RoBERTa by a large margin, and achieves
comparable results with state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2010.12148" target="_blank">arXiv:2010.12148</a> [<a href="http://arxiv.org/pdf/2010.12148" target="_blank">pdf</a>]

<h2>DeFuzz: Deep Learning Guided Directed Fuzzing. (arXiv:2010.12149v1 [cs.CR])</h2>
<h3>Xiaogang Zhu, Shigang Liu, Xian Li, Sheng Wen, Jun Zhang, Camtepe Seyit, Yang Xiang</h3>
<p>Fuzzing is one of the most effective technique to identify potential software
vulnerabilities. Most of the fuzzers aim to improve the code coverage, and
there is lack of directedness (e.g., fuzz the specified path in a software). In
this paper, we proposed a deep learning (DL) guided directed fuzzing for
software vulnerability detection, named DeFuzz. DeFuzz includes two main
schemes: (1) we employ a pre-trained DL prediction model to identify the
potentially vulnerable functions and the locations (i.e., vulnerable
addresses). Precisely, we employ Bidirectional-LSTM (BiLSTM) to identify
attention words, and the vulnerabilities are associated with these attention
words in functions. (2) then we employ directly fuzzing to fuzz the potential
vulnerabilities by generating inputs that tend to arrive the predicted
locations. To evaluate the effectiveness and practical of the proposed DeFuzz
technique, we have conducted experiments on real-world data sets. Experimental
results show that our DeFuzz can discover coverage more and faster than AFL.
Moreover, DeFuzz exposes 43 more bugs than AFL on real-world applications.
</p>
<a href="http://arxiv.org/abs/2010.12149" target="_blank">arXiv:2010.12149</a> [<a href="http://arxiv.org/pdf/2010.12149" target="_blank">pdf</a>]

<h2>Generative Neurosymbolic Machines. (arXiv:2010.12152v1 [cs.LG])</h2>
<h3>Jindong Jiang, Sungjin Ahn</h3>
<p>Reconciling symbolic and distributed representations is a crucial challenge
that can potentially resolve the limitations of current deep learning.
Remarkable advances in this direction have been achieved recently via
generative object-centric representation models. While learning a recognition
model that infers object-centric symbolic representations like bounding boxes
from raw images in an unsupervised way, no such model can provide another
important ability of a generative model, i.e., generating (sampling) according
to the structure of learned world density. In this paper, we propose Generative
Neurosymbolic Machines, a generative model that combines the benefits of
distributed and symbolic representations to support both structured
representations of symbolic components and density-based generation. These two
crucial properties are achieved by a two-layer latent hierarchy with the global
distributed latent for flexible density modeling and the structured symbolic
latent map. To increase the model flexibility in this hierarchical structure,
we also propose the StructDRAW prior. In experiments, we show that the proposed
model significantly outperforms the previous structured representation models
as well as the state-of-the-art non-structured generative models in terms of
both structure accuracy and image generation quality.
</p>
<a href="http://arxiv.org/abs/2010.12152" target="_blank">arXiv:2010.12152</a> [<a href="http://arxiv.org/pdf/2010.12152" target="_blank">pdf</a>]

<h2>Improved Worst-Case Regret Bounds for Randomized Least-Squares Value Iteration. (arXiv:2010.12163v1 [cs.LG])</h2>
<h3>Priyank Agrawal, Jinglin Chen, Nan Jiang</h3>
<p>This paper studies regret minimization with randomized value functions in
reinforcement learning. In tabular finite-horizon Markov Decision Processes, we
introduce a clipping variant of one classical Thompson Sampling (TS)-like
algorithm, randomized least-squares value iteration (RLSVI). We analyze the
algorithm using a novel intertwined regret decomposition. Our
$\tilde{\mathrm{O}}(H^2S\sqrt{AT})$ high-probability worst-case regret bound
improves the previous sharpest worst-case regret bounds for RLSVI and matches
the existing state-of-the-art worst-case TS-based regret bounds.
</p>
<a href="http://arxiv.org/abs/2010.12163" target="_blank">arXiv:2010.12163</a> [<a href="http://arxiv.org/pdf/2010.12163" target="_blank">pdf</a>]

<h2>Approximation Methods for Kernelized Bandits. (arXiv:2010.12167v1 [cs.LG])</h2>
<h3>Sho Takemori, Masahiro Sato</h3>
<p>The RKHS bandit problem (also called kernelized multi-armed bandit problem)
is an online optimization problem of non-linear functions with noisy feedbacks.
Most of the existing methods for the problem have sub-linear regret guarantee
at the cost of high computational complexity. For example, IGP-UCB requires at
least quadratic time in the number of observed samples at each round. In this
paper, using deep results provided by the approximation theory, we
approximately reduce the problem to the well-studied linear bandit problem of
an appropriate dimension. Then, we propose several algorithms and prove that
they achieve comparable regret guarantee to the existing methods (GP-UCB,
IGP-UCB) with less computational complexity. Specifically, our proposed methods
require polylogarithmic time to select an arm at each round for kernels with
"infinite smoothness" (e.g. the rational quadratic and squared exponential
kernels). Furthermore, we empirically show our proposed method has comparable
regret to the existing method and its running time is much shorter.
</p>
<a href="http://arxiv.org/abs/2010.12167" target="_blank">arXiv:2010.12167</a> [<a href="http://arxiv.org/pdf/2010.12167" target="_blank">pdf</a>]

<h2>Trustworthy Digital Twins in the Industrial Internet of Things with Blockchain. (arXiv:2010.12168v1 [cs.CR])</h2>
<h3>Sabah Suhail, Rasheed Hussain, Raja Jurdak, Choong Seon Hong</h3>
<p>Industrial processes rely on sensory data for critical decision-making
processes. Extracting actionable insights from the collected data calls for an
infrastructure that can ensure the trustworthiness of data. To this end, we
envision a blockchain-based framework for the Industrial Internet of Things
(IIoT) to address the issues of data management and security. Once the data
collected from trustworthy sources are recorded in the blockchain, product
lifecycle events can be fed into data-driven systems for process monitoring,
diagnostics, and optimized control. In this regard, we leverage Digital Twins
(DTs) that can draw intelligent conclusions from data by identifying the faults
and recommending precautionary measures ahead of critical events. Furthermore,
we discuss the integration of DTs and blockchain to target key challenges of
disparate data repositories, untrustworthy data dissemination, and fault
diagnosis. Finally, we identify outstanding challenges faced by the IIoT and
future research directions while leveraging blockchain and DTs.
</p>
<a href="http://arxiv.org/abs/2010.12168" target="_blank">arXiv:2010.12168</a> [<a href="http://arxiv.org/pdf/2010.12168" target="_blank">pdf</a>]

<h2>DualNet: Locate Then Detect Effective Payload with Deep Attention Network. (arXiv:2010.12171v1 [cs.CR])</h2>
<h3>Shiyi Yang, Peilun Wu, Hui Guo</h3>
<p>Network intrusion detection (NID) is an essential defense strategy that is
used to discover the trace of suspicious user behaviour in large-scale
cyberspace, and machine learning (ML), due to its capability of automation and
intelligence, has been gradually adopted as a mainstream hunting method in
recent years. However, traditional ML based network intrusion detection systems
(NIDSs) are not effective to recognize unknown threats and their high detection
rate often comes with the cost of high false alarms, which leads to the problem
of alarm fatigue. To address the above problems, in this paper, we propose a
novel neural network based detection system, DualNet, which is constructed with
a general feature extraction stage and a crucial feature learning stage.
DualNet can rapidly reuse the spatial-temporal features in accordance with
their importance to facilitate the entire learning process and simultaneously
mitigate several optimization problems occurred in deep learning (DL). We
evaluate the DualNet on two benchmark cyber attack datasets, NSL-KDD and
UNSW-NB15. Our experiment shows that DualNet outperforms classical ML based
NIDSs and is more effective than existing DL methods for NID in terms of
accuracy, detection rate and false alarm rate.
</p>
<a href="http://arxiv.org/abs/2010.12171" target="_blank">arXiv:2010.12171</a> [<a href="http://arxiv.org/pdf/2010.12171" target="_blank">pdf</a>]

<h2>A Cross-Verification Approach for Protecting World Leaders from Fake and Tampered Audio. (arXiv:2010.12173v1 [eess.AS])</h2>
<h3>Mengyi Shan, TJ Tsai</h3>
<p>This paper tackles the problem of verifying the authenticity of speech
recordings from world leaders. Whereas previous work on detecting deep fake or
tampered audio focus on scrutinizing an audio recording in isolation, we
instead reframe the problem and focus on cross-verifying a questionable
recording against trusted references. We present a method for cross-verifying a
speech recording against a reference that consists of two steps: aligning the
two recordings and then classifying each query frame as matching or
non-matching. We propose a subsequence alignment method based on the
Needleman-Wunsch algorithm and show that it significantly outperforms dynamic
time warping in handling common tampering operations. We also explore several
binary classification models based on LSTM and Transformer architectures to
verify content at the frame level. Through extensive experiments on tampered
speech recordings of Donald Trump, we show that our system can reliably detect
audio tampering operations of different types and durations. Our best model
achieves 99.7% accuracy for the alignment task at an error tolerance of 50 ms
and a 0.43% equal error rate in classifying audio frames as matching or
non-matching.
</p>
<a href="http://arxiv.org/abs/2010.12173" target="_blank">arXiv:2010.12173</a> [<a href="http://arxiv.org/pdf/2010.12173" target="_blank">pdf</a>]

<h2>KINNEWS and KIRNEWS: Benchmarking Cross-Lingual Text Classification for Kinyarwanda and Kirundi. (arXiv:2010.12174v1 [cs.CL])</h2>
<h3>Rubungo Andre Niyongabo, Hong Qu, Julia Kreutzer, Li Huang</h3>
<p>Recent progress in text classification has been focused on high-resource
languages such as English and Chinese. For low-resource languages, amongst them
most African languages, the lack of well-annotated data and effective
preprocessing, is hindering the progress and the transfer of successful
methods. In this paper, we introduce two news datasets (KINNEWS and KIRNEWS)
for multi-class classification of news articles in Kinyarwanda and Kirundi, two
low-resource African languages. The two languages are mutually intelligible,
but while Kinyarwanda has been studied in Natural Language Processing (NLP) to
some extent, this work constitutes the first study on Kirundi. Along with the
datasets, we provide statistics, guidelines for preprocessing, and monolingual
and cross-lingual baseline models. Our experiments show that training
embeddings on the relatively higher-resourced Kinyarwanda yields successful
cross-lingual transfer to Kirundi. In addition, the design of the created
datasets allows for a wider use in NLP beyond text classification in future
studies, such as representation learning, cross-lingual learning with more
distant languages, or as base for new annotations for tasks such as parsing,
POS tagging, and NER. The datasets, stopwords, and pre-trained embeddings are
publicly available at https://github.com/Andrews2017/KINNEWS-and-KIRNEWS-Corpus .
</p>
<a href="http://arxiv.org/abs/2010.12174" target="_blank">arXiv:2010.12174</a> [<a href="http://arxiv.org/pdf/2010.12174" target="_blank">pdf</a>]

<h2>Don't shoot butterfly with rifles: Multi-channel Continuous Speech Separation with Early Exit Transformer. (arXiv:2010.12180v1 [cs.SD])</h2>
<h3>Sanyuan Chen, Yu Wu, Zhuo Chen, Takuya Yoshioka, Shujie Liu, Jinyu Li</h3>
<p>With its strong modeling capacity that comes from a multi-head and
multi-layer structure, Transformer is a very powerful model for learning a
sequential representation and has been successfully applied to speech
separation recently. However, multi-channel speech separation sometimes does
not necessarily need such a heavy structure for all time frames especially when
the cross-talker challenge happens only occasionally. For example, in
conversation scenarios, most regions contain only a single active speaker,
where the separation task downgrades to a single speaker enhancement problem.
It turns out that using a very deep network structure for dealing with signals
with a low overlap ratio not only negatively affects the inference efficiency
but also hurts the separation performance. To deal with this problem, we
propose an early exit mechanism, which enables the Transformer model to handle
different cases with adaptive depth. Experimental results indicate that not
only does the early exit mechanism accelerate the inference, but it also
improves the accuracy.
</p>
<a href="http://arxiv.org/abs/2010.12180" target="_blank">arXiv:2010.12180</a> [<a href="http://arxiv.org/pdf/2010.12180" target="_blank">pdf</a>]

<h2>Towards Fair Knowledge Transfer for Imbalanced Domain Adaptation. (arXiv:2010.12184v1 [cs.CV])</h2>
<h3>Taotao Jing Name, Bingrong Xu, Jingjing Li, Zhengming Ding</h3>
<p>Domain adaptation (DA) becomes an up-and-coming technique to address the
insufficient or no annotation issue by exploiting external source knowledge.
Existing DA algorithms mainly focus on practical knowledge transfer through
domain alignment. Unfortunately, they ignore the fairness issue when the
auxiliary source is extremely imbalanced across different categories, which
results in severe under-presented knowledge adaptation of minority source set.
To this end, we propose a Towards Fair Knowledge Transfer (TFKT) framework to
handle the fairness challenge in imbalanced cross-domain learning.
Specifically, a novel cross-domain mixup generation is exploited to augment the
minority source set with target information to enhance fairness. Moreover, dual
distinct classifiers and cross-domain prototype alignment are developed to seek
a more robust classifier boundary and mitigate the domain shift. Such three
strategies are formulated into a unified framework to address the fairness
issue and domain shift challenge. Extensive experiments over two popular
benchmarks have verified the effectiveness of our proposed model by comparing
to existing state-of-the-art DA models, and especially our model significantly
improves over 20% on two benchmarks in terms of the overall accuracy.
</p>
<a href="http://arxiv.org/abs/2010.12184" target="_blank">arXiv:2010.12184</a> [<a href="http://arxiv.org/pdf/2010.12184" target="_blank">pdf</a>]

<h2>On Evaluating Neural Network Backdoor Defenses. (arXiv:2010.12186v1 [cs.LG])</h2>
<h3>Akshaj Veldanda, Siddharth Garg</h3>
<p>Deep neural networks (DNNs) demonstrate superior performance in various
fields, including scrutiny and security. However, recent studies have shown
that DNNs are vulnerable to backdoor attacks. Several defenses were proposed in
the past to defend DNNs against such backdoor attacks. In this work, we conduct
a critical analysis and identify common pitfalls in these existing defenses,
prepare a comprehensive database of backdoor attacks, conduct a side-by-side
evaluation of existing defenses against this database. Finally, we layout some
general guidelines to help researchers develop more robust defenses in the
future and avoid common mistakes from the past.
</p>
<a href="http://arxiv.org/abs/2010.12186" target="_blank">arXiv:2010.12186</a> [<a href="http://arxiv.org/pdf/2010.12186" target="_blank">pdf</a>]

<h2>Learn Robust Features via Orthogonal Multi-Path. (arXiv:2010.12190v1 [cs.CV])</h2>
<h3>Kun Fang, Yingwen Wu, Tao Li, Xiaolin Huang, Jie Yang</h3>
<p>It is now widely known that by adversarial attacks, clean images with
invisible perturbations can fool deep neural networks. To defend adversarial
attacks, we design a block containing multiple paths to learn robust features
and the parameters of these paths are required to be orthogonal with each
other. The so-called Orthogonal Multi-Path (OMP) block could be posed in any
layer of a neural network. Via forward learning and backward correction, one
OMP block makes the neural networks learn features that are appropriate for all
the paths and hence are expected to be robust. With careful design and thorough
experiments on e.g., the positions of imposing orthogonality constraint, and
the trade-off between the variety and accuracy, the robustness of the neural
networks is significantly improved. For example, under white-box PGD attack
with $l_\infty$ bound ${8}/{255}$ (this is a fierce attack that can make the
accuracy of many vanilla neural networks drop to nearly $10\%$ on CIFAR10),
VGG16 with the proposed OMP block could keep over $50\%$ accuracy. For
black-box attacks, neural networks equipped with an OMP block have accuracy
over $80\%$. The performance under both white-box and black-box attacks is much
better than the existing state-of-the-art adversarial defenders.
</p>
<a href="http://arxiv.org/abs/2010.12190" target="_blank">arXiv:2010.12190</a> [<a href="http://arxiv.org/pdf/2010.12190" target="_blank">pdf</a>]

<h2>Quantum Superposition Spiking Neural Network. (arXiv:2010.12197v1 [cs.NE])</h2>
<h3>Yinqian Sun, Yi Zeng, Tielin Zhang</h3>
<p>Quantum brain as a novel hypothesis states that some non-trivial mechanisms
in quantum computation, such as superposition and entanglement, may have
important influence for the formation of brain functions. Inspired by this
idea, we propose Quantum Superposition Spiking Neural Network (QS-SNN), which
introduce quantum superposition to spiking neural network models to handel
challenges which are hard for other state-of-the-art machine learning models.
For human brain, grasping the main information no matter how the background
changes is necessary to interact efficiently with diverse environments. As an
example, it is easy for human to recognize the digits whether it is white
character with black background or inversely black character with white
background. While if the current machine learning models are trained with one
of the cases (e.g. white character with black background), it will be nearly
impossible for them to recognize the color inverted version. To handel this
challenge, we propose two-compartment spiking neural network with superposition
states encoding, which is inspired by quantum information theory and
spatial-temporal spiking property from neuron information encoding in the
brain. Typical network structures like fully-connected ANN, VGG, ResNet and
DenseNet are challenged with the same task. We train these networks on original
image dataset and then invert the background color to test their
generalization. Result shows that artificial neural network can not deal with
this condition while the quantum superposition spiking neural network(QS-SNN)
which we proposed in this paper recognizes the color-inverse image
successfully. Further the QS-SNN shows its robustness when noises are added on
inputs.
</p>
<a href="http://arxiv.org/abs/2010.12197" target="_blank">arXiv:2010.12197</a> [<a href="http://arxiv.org/pdf/2010.12197" target="_blank">pdf</a>]

<h2>Domain Divergences: a Survey and Empirical Analysis. (arXiv:2010.12198v1 [cs.CL])</h2>
<h3>Abhinav Ramesh Kashyap, Devamanyu Hazarika, Min-Yen Kan, Roger Zimmermann</h3>
<p>Domain divergence plays a significant role in estimating the performance of a
model when applied to new domains. While there is significant literature on
divergence measures, choosing an appropriate divergence measures remains
difficult for researchers. We address this shortcoming by both surveying the
literature and through an empirical study. We contribute a taxonomy of
divergence measures consisting of three groups -- Information-theoretic,
Geometric, and Higher-order measures -- and identify the relationships between
them. We then ground the use of divergence measures in three different
application groups -- 1) Data Selection, 2) Learning Representation, and 3)
Decisions in the Wild. From this, we identify that Information-theoretic
measures are prevalent for 1) and 3), and higher-order measures are common for
2). To further help researchers, we validate these uses empirically through a
correlation analysis of performance drops. We consider the current contextual
word representations (CWR) to contrast with the older word distribution based
representations for this analysis. We find that traditional measures over word
distributions still serve as strong baselines, while higher-order measures with
CWR are effective.
</p>
<a href="http://arxiv.org/abs/2010.12198" target="_blank">arXiv:2010.12198</a> [<a href="http://arxiv.org/pdf/2010.12198" target="_blank">pdf</a>]

<h2>Feature matching in Ultrasound images. (arXiv:2010.12216v1 [cs.CV])</h2>
<h3>Hang Zhu, Zihao Wang</h3>
<p>Feature matching is an important technique to identify a single object in
different images. It helps machines to construct recognition of a specific
object from multiple perspectives. For years, feature matching has been
commonly used in various computer vision applications, like traffic
surveillance, self-driving, and other systems. With the arise of Computer-Aided
Diagnosis(CAD), the need for feature matching techniques also emerges in the
medical imaging field. In this paper, we present a deep learning-based method
specially for ultrasound images. It will be examined against existing methods
that have outstanding results on regular images. As the ultrasound images are
different from regular images in many fields like texture, noise type, and
dimension, traditional methods will be evaluated and optimized to be applied to
ultrasound images.
</p>
<a href="http://arxiv.org/abs/2010.12216" target="_blank">arXiv:2010.12216</a> [<a href="http://arxiv.org/pdf/2010.12216" target="_blank">pdf</a>]

<h2>When the Open Source Community Meets COVID-19: Characterizing COVID-19 themed GitHub Repositories. (arXiv:2010.12218v1 [cs.SE])</h2>
<h3>Liu Wang, Ruiqing Li, Jiaxin Zhu, Guangdong Bai, Haoyu Wang</h3>
<p>Ever since the beginning of the outbreak of the COVID-19 pandemic,
researchers from interdisciplinary domains have worked together to fight
against the crisis. The open source community, plays a vital role in coping
with the pandemic which is inherently a collaborative process. Plenty of
COVID-19 related datasets, tools, software, deep learning models, are created
and shared in research communities with great efforts. However, COVID-19 themed
open source projects have not been systematically studied, and we are still
unaware how the open source community helps combat COVID-19 in practice. To
fill this void, in this paper, we take the first step to study COVID-19 themed
repositories in GitHub, one of the most popular collaborative platforms. We
have collected over 67K COVID-19 themed GitHub repositories till July 2020. We
then characterize them from a number of aspects and classify them into six
categories. We further investigate the contribution patterns of the
contributors, and development and maintenance patterns of the repositories.
This study sheds light on the promising direction of adopting open source
technologies and resources to rapidly tackle the worldwide public health
emergency in practice, and reveals existing challenges for improvement.
</p>
<a href="http://arxiv.org/abs/2010.12218" target="_blank">arXiv:2010.12218</a> [<a href="http://arxiv.org/pdf/2010.12218" target="_blank">pdf</a>]

<h2>A Teacher-Student Framework for Semi-supervised Medical Image Segmentation From Mixed Supervision. (arXiv:2010.12219v1 [cs.CV])</h2>
<h3>Liyan Sun, Jianxiong Wu, Xinghao Ding, Yue Huang, Guisheng Wang, Yizhou Yu</h3>
<p>Standard segmentation of medical images based on full-supervised
convolutional networks demands accurate dense annotations. Such learning
framework is built on laborious manual annotation with restrict demands for
expertise, leading to insufficient high-quality labels. To overcome such
limitation and exploit massive weakly labeled data, we relaxed the rigid
labeling requirement and developed a semi-supervised learning framework based
on a teacher-student fashion for organ and lesion segmentation with partial
dense-labeled supervision and supplementary loose bounding-box supervision
which are easier to acquire. Observing the geometrical relation of an organ and
its inner lesions in most cases, we propose a hierarchical organ-to-lesion
(O2L) attention module in a teacher segmentor to produce pseudo-labels. Then a
student segmentor is trained with combinations of manual-labeled and
pseudo-labeled annotations. We further proposed a localization branch realized
via an aggregation of high-level features in a deep decoder to predict
locations of organ and lesion, which enriches student segmentor with precise
localization information. We validated each design in our model on LiTS
challenge datasets by ablation study and showed its state-of-the-art
performance compared with recent methods. We show our model is robust to the
quality of bounding box and achieves comparable performance compared with
full-supervised learning methods.
</p>
<a href="http://arxiv.org/abs/2010.12219" target="_blank">arXiv:2010.12219</a> [<a href="http://arxiv.org/pdf/2010.12219" target="_blank">pdf</a>]

<h2>Temporal Attention-Augmented Graph Convolutional Network for Efficient Skeleton-Based Human Action Recognition. (arXiv:2010.12221v1 [cs.CV])</h2>
<h3>Negar Heidari, Alexandros Iosifidis</h3>
<p>Graph convolutional networks (GCNs) have been very successful in modeling
non-Euclidean data structures, like sequences of body skeletons forming actions
modeled as spatio-temporal graphs. Most GCN-based action recognition methods
use deep feed-forward networks with high computational complexity to process
all skeletons in an action. This leads to a high number of floating point
operations (ranging from 16G to 100G FLOPs) to process a single sample, making
their adoption in restricted computation application scenarios infeasible. In
this paper, we propose a temporal attention module (TAM) for increasing the
efficiency in skeleton-based action recognition by selecting the most
informative skeletons of an action at the early layers of the network. We
incorporate the TAM in a light-weight GCN topology to further reduce the
overall number of computations. Experimental results on two benchmark datasets
show that the proposed method outperforms with a large margin the baseline
GCN-based method while having 2.9 times less number of computations. Moreover,
it performs on par with the state-of-the-art with up to 9.6 times less number
of computations.
</p>
<a href="http://arxiv.org/abs/2010.12221" target="_blank">arXiv:2010.12221</a> [<a href="http://arxiv.org/pdf/2010.12221" target="_blank">pdf</a>]

<h2>A Perspective on Machine Learning Methods in Turbulence Modelling. (arXiv:2010.12226v1 [cs.CE])</h2>
<h3>Andrea Beck, Marius Kurz</h3>
<p>This work presents a review of the current state of research in data-driven
turbulence closure modeling. It offers a perspective on the challenges and open
issues, but also on the advantages and promises of machine learning methods
applied to parameter estimation, model identification, closure term
reconstruction and beyond, mostly from the perspective of Large Eddy Simulation
and related techniques. We stress that consistency of the training data, the
model, the underlying physics and the discretization is a key issue that needs
to be considered for a successful ML-augmented modeling strategy. In order to
make the discussion useful for non-experts in either field, we introduce both
the modeling problem in turbulence as well as the prominent ML paradigms and
methods in a concise and self-consistent manner. Following, we present a survey
of the current data-driven model concepts and methods, highlight important
developments and put them into the context of the discussed challenges.
</p>
<a href="http://arxiv.org/abs/2010.12226" target="_blank">arXiv:2010.12226</a> [<a href="http://arxiv.org/pdf/2010.12226" target="_blank">pdf</a>]

<h2>Hard Example Generation by Texture Synthesis for Cross-domain Shape Similarity Learning. (arXiv:2010.12238v1 [cs.CV])</h2>
<h3>Huan Fu, Shunming Li, Rongfei Jia, Mingming Gong, Binqiang Zhao, Dacheng Tao</h3>
<p>Image-based 3D shape retrieval (IBSR) aims to find the corresponding 3D shape
of a given 2D image from a large 3D shape database. The common routine is to
map 2D images and 3D shapes into an embedding space and define (or learn) a
shape similarity measure. While metric learning with some adaptation techniques
seems to be a natural solution to shape similarity learning, the performance is
often unsatisfactory for fine-grained shape retrieval. In the paper, we
identify the source of the poor performance and propose a practical solution to
this problem. We find that the shape difference between a negative pair is
entangled with the texture gap, making metric learning ineffective in pushing
away negative pairs. To tackle this issue, we develop a geometry-focused
multi-view metric learning framework empowered by texture synthesis. The
synthesis of textures for 3D shape models creates hard triplets, which suppress
the adverse effects of rich texture in 2D images, thereby push the network to
focus more on discovering geometric characteristics. Our approach shows
state-of-the-art performance on a recently released large-scale 3D-FUTURE[1]
repository, as well as three widely studied benchmarks, including Pix3D[2],
Stanford Cars[3], and Comp Cars[4]. Codes will be made public available at:
https://github.com/3D-FRONT-FUTURE/IBSR-texture
</p>
<a href="http://arxiv.org/abs/2010.12238" target="_blank">arXiv:2010.12238</a> [<a href="http://arxiv.org/pdf/2010.12238" target="_blank">pdf</a>]

<h2>Domain Adaptation in LiDAR Semantic Segmentation. (arXiv:2010.12239v1 [cs.CV])</h2>
<h3>Inigo Alonso, Luis Riazuelo. Luis Montesano, Ana C. Murillo</h3>
<p>LiDAR semantic segmentation provides 3D semantic information about the
environment, an essential cue for intelligent systems during their decision
making processes. Deep neural networks are achieving state-of-the-art results
on large public benchmarks on this task. Unfortunately, finding models that
generalize well or adapt to additional domains, where data distribution is
different, remains a major challenge. This work addresses the problem of
unsupervised domain adaptation for LiDAR semantic segmentation models. Our
approach combines novel ideas on top of the current state-of-the-art approaches
and yields new state-of-the-art results. We propose simple but effective
strategies to reduce the domain shift by aligning the data distribution on the
input space. Besides, we propose a learning-based approach that aligns the
distribution of the semantic classes of the target domain to the source domain.
The presented ablation study shows how each part contributes to the final
performance. Our strategy is shown to outperform previous approaches for domain
adaptation with comparisons run on three different domains.
</p>
<a href="http://arxiv.org/abs/2010.12239" target="_blank">arXiv:2010.12239</a> [<a href="http://arxiv.org/pdf/2010.12239" target="_blank">pdf</a>]

<h2>Option Hedging with Risk Averse Reinforcement Learning. (arXiv:2010.12245v1 [q-fin.TR])</h2>
<h3>Edoardo Vittori, Michele Trapletti, Marcello Restelli</h3>
<p>In this paper we show how risk-averse reinforcement learning can be used to
hedge options. We apply a state-of-the-art risk-averse algorithm: Trust Region
Volatility Optimization (TRVO) to a vanilla option hedging environment,
considering realistic factors such as discrete time and transaction costs.
Realism makes the problem twofold: the agent must both minimize volatility and
contain transaction costs, these tasks usually being in competition. We use the
algorithm to train a sheaf of agents each characterized by a different risk
aversion, so to be able to span an efficient frontier on the volatility-p\&amp;l
space. The results show that the derived hedging strategy not only outperforms
the Black \&amp; Scholes delta hedge, but is also extremely robust and flexible, as
it can efficiently hedge options with different characteristics and work on
markets with different behaviors than what was used in training.
</p>
<a href="http://arxiv.org/abs/2010.12245" target="_blank">arXiv:2010.12245</a> [<a href="http://arxiv.org/pdf/2010.12245" target="_blank">pdf</a>]

<h2>An Asymptotically Optimal Primal-Dual Incremental Algorithm for Contextual Linear Bandits. (arXiv:2010.12247v1 [cs.LG])</h2>
<h3>Andrea Tirinzoni, Matteo Pirotta, Marcello Restelli, Alessandro Lazaric</h3>
<p>In the contextual linear bandit setting, algorithms built on the optimism
principle fail to exploit the structure of the problem and have been shown to
be asymptotically suboptimal. In this paper, we follow recent approaches of
deriving asymptotically optimal algorithms from problem-dependent regret lower
bounds and we introduce a novel algorithm improving over the state-of-the-art
along multiple dimensions. We build on a reformulation of the lower bound,
where context distribution and exploration policy are decoupled, and we obtain
an algorithm robust to unbalanced context distributions. Then, using an
incremental primal-dual approach to solve the Lagrangian relaxation of the
lower bound, we obtain a scalable and computationally efficient algorithm.
Finally, we remove forced exploration and build on confidence intervals of the
optimization problem to encourage a minimum level of exploration that is better
adapted to the problem structure. We demonstrate the asymptotic optimality of
our algorithm, while providing both problem-dependent and worst-case
finite-time regret guarantees. Our bounds scale with the logarithm of the
number of arms, thus avoiding the linear dependence common in all related prior
works. Notably, we establish minimax optimality for any learning horizon in the
special case of non-contextual linear bandits. Finally, we verify that our
algorithm obtains better empirical performance than state-of-the-art baselines.
</p>
<a href="http://arxiv.org/abs/2010.12247" target="_blank">arXiv:2010.12247</a> [<a href="http://arxiv.org/pdf/2010.12247" target="_blank">pdf</a>]

<h2>IPU-Net: Multi Scale Identity-Preserved U-Net for Low Resolution Face Recognition. (arXiv:2010.12249v1 [cs.CV])</h2>
<h3>Vahid Reza Khazaie, Nicky Bayat, Yalda Mohsenzadeh</h3>
<p>State-of-the-art deep neural network models have reached near perfect face
recognition accuracy rates on controlled high resolution face images. However,
their performance is drastically degraded when they are tested with very low
resolution face images. This is particularly critical in surveillance systems,
where a low resolution probe image is to be matched with high resolution
gallery images. Super resolution techniques aim at producing high resolution
face images from low resolution counterparts. While they are capable of
reconstructing images that are visually appealing, the identity-related
information is not preserved. Here, we propose an identity-preserved U-Net
which is capable of super-resolving very low resolution faces to their high
resolution counterparts while preserving identity-related information. We
achieve this by training a U-Net with a combination of a reconstruction and an
identity-preserving loss, on multi-scale low resolution conditions. Extensive
quantitative evaluations of our proposed model demonstrated that it outperforms
competing super resolution and low resolution face recognition methods on
natural and artificial low resolution face data sets and even unseen
identities.
</p>
<a href="http://arxiv.org/abs/2010.12249" target="_blank">arXiv:2010.12249</a> [<a href="http://arxiv.org/pdf/2010.12249" target="_blank">pdf</a>]

<h2>A Scalable Framework for Learning From Implicit User Feedback to Improve Natural Language Understanding in Large-Scale Conversational AI Systems. (arXiv:2010.12251v1 [cs.CL])</h2>
<h3>Sunghyun Park, Han Li, Ameen Patel, Sidharth Mudgal, Sungjin Lee, Young-Bum Kim, Spyros Matsoukas, Ruhi Sarikaya</h3>
<p>Natural Language Understanding (NLU) is an established component within a
conversational AI or digital assistant system, and it is responsible for
producing semantic understanding of a user request. We propose a scalable and
automatic approach for improving NLU in a large-scale conversational AI system
by leveraging implicit user feedback, with an insight that user interaction
data and dialog context have rich information embedded from which user
satisfaction and intention can be inferred. In particular, we propose a general
domain-agnostic framework for curating new supervision data for improving NLU
from live production traffic. With an extensive set of experiments, we show the
results of applying the framework and improving NLU for a large-scale
production system and show its impact across 10 domains.
</p>
<a href="http://arxiv.org/abs/2010.12251" target="_blank">arXiv:2010.12251</a> [<a href="http://arxiv.org/pdf/2010.12251" target="_blank">pdf</a>]

<h2>NGAT4Rec: Neighbor-Aware Graph Attention Network For Recommendation. (arXiv:2010.12256v1 [cs.IR])</h2>
<h3>Jinbo Song, Chao Chang, Fei Sun, Xinbo Song, Peng Jiang</h3>
<p>Learning informative representations (aka. embeddings) of users and items is
the core of modern recommender systems. Previous works exploit user-item
relationships of one-hop neighbors in the user-item interaction graph to
improve the quality of representation. Recently, the research of Graph Neural
Network (GNN) for recommendation considers the implicit collaborative
information of multi-hop neighbors to enrich the representation. However, most
works of GNN for recommendation systems do not consider the relational
information which implies the expression differences of different neighbors in
the neighborhood explicitly. The influence of each neighboring item to the
representation of the user's preference can be represented by the correlation
between the item and neighboring items of the user. Symmetrically, for a given
item, the correlation between one neighboring user and neighboring users can
reflect the strength of signal about the item's characteristic. To modeling the
implicit correlations of neighbors in graph embedding aggregating, we propose a
Neighbor-Aware Graph Attention Network for recommendation task, termed
NGAT4Rec. It employs a novel neighbor-aware graph attention layer that assigns
different neighbor-aware attention coefficients to different neighbors of a
given node by computing the attention among these neighbors pairwisely. Then
NGAT4Rec aggregates the embeddings of neighbors according to the corresponding
neighbor-aware attention coefficients to generate next layer embedding for
every node. Furthermore, we combine more neighbor-aware graph attention layer
to gather the influential signals from multi-hop neighbors. We remove feature
transformation and nonlinear activation that proved to be useless on
collaborative filtering. Extensive experiments on three benchmark datasets show
that our model outperforms various state-of-the-art models consistently.
</p>
<a href="http://arxiv.org/abs/2010.12256" target="_blank">arXiv:2010.12256</a> [<a href="http://arxiv.org/pdf/2010.12256" target="_blank">pdf</a>]

<h2>State space models for building control: how deep should you go?. (arXiv:2010.12257v1 [eess.SY])</h2>
<h3>Baptiste Schubnel, Rafael E. Carrillo, Paolo Taddeo, Lluc Canals Casals, Jaume Salom, Yves Stauffer, Pierre-Jean Alet</h3>
<p>Power consumption in buildings show non-linear behaviors that linear models
cannot capture whereas recurrent neural networks (RNNs) can. This ability makes
RNNs attractive alternatives for the model-predictive control (MPC) of
buildings. However RNN models lack mathematical regularity which makes their
use challenging in optimization problems. This work therefore systematically
investigates whether using RNNs for building control provides net gains in an
MPC framework. It compares the representation power and control performance of
two architectures: a fully non-linear RNN architecture and a linear state-space
model with non-linear regressor. The comparison covers five instances of each
architecture over two months of simulated operation in identical conditions.
The error on the one-hour forecast of temperature is 69% lower with the RNN
model than with the linear one. In control the linear state-space model
outperforms by 10% on the objective function, shows 2.8 times higher average
temperature violations, and needs a third of the computation time the RNN model
requires. This work therefore demonstrates that in their current form RNNs do
improve accuracy but on balance well-designed linear state-space models with
non-linear regressors are best in most cases of MPC.
</p>
<a href="http://arxiv.org/abs/2010.12257" target="_blank">arXiv:2010.12257</a> [<a href="http://arxiv.org/pdf/2010.12257" target="_blank">pdf</a>]

<h2>Population Gradients improve performance across data-sets and architectures in object classification. (arXiv:2010.12260v1 [cs.LG])</h2>
<h3>Yurika Sakai, Andrey Kormilitzin, Qiang Liu, Alejo Nevado-Holgado</h3>
<p>The most successful methods such as ReLU transfer functions, batch
normalization, Xavier initialization, dropout, learning rate decay, or dynamic
optimizers, have become standards in the field due, particularly, to their
ability to increase the performance of Neural Networks (NNs) significantly and
in almost all situations. Here we present a new method to calculate the
gradients while training NNs, and show that it significantly improves final
performance across architectures, data-sets, hyper-parameter values, training
length, and model sizes, including when it is being combined with other common
performance-improving methods (such as the ones mentioned above). Besides being
effective in the wide array situations that we have tested, the increase in
performance (e.g. F1) it provides is as high or higher than this one of all the
other widespread performance-improving methods that we have compared against.
We call our method Population Gradients (PG), and it consists on using a
population of NNs to calculate a non-local estimation of the gradient, which is
closer to the theoretical exact gradient (i.e. this one obtainable only with an
infinitely big data-set) of the error function than the empirical gradient
(i.e. this one obtained with the real finite data-set).
</p>
<a href="http://arxiv.org/abs/2010.12260" target="_blank">arXiv:2010.12260</a> [<a href="http://arxiv.org/pdf/2010.12260" target="_blank">pdf</a>]

<h2>Model Interpretability through the Lens of Computational Complexity. (arXiv:2010.12265v1 [cs.AI])</h2>
<h3>Pablo Barcel&#xf3;, Mika&#xeb;l Monet, Jorge P&#xe9;rez, Bernardo Subercaseaux</h3>
<p>In spite of several claims stating that some models are more interpretable
than others -- e.g., "linear models are more interpretable than deep neural
networks" -- we still lack a principled notion of interpretability to formally
compare among different classes of models. We make a step towards such a notion
by studying whether folklore interpretability claims have a correlate in terms
of computational complexity theory. We focus on local post-hoc explainability
queries that, intuitively, attempt to answer why individual inputs are
classified in a certain way by a given model. In a nutshell, we say that a
class $\mathcal{C}_1$ of models is more interpretable than another class
$\mathcal{C}_2$, if the computational complexity of answering post-hoc queries
for models in $\mathcal{C}_2$ is higher than for those in $\mathcal{C}_1$. We
prove that this notion provides a good theoretical counterpart to current
beliefs on the interpretability of models; in particular, we show that under
our definition and assuming standard complexity-theoretical assumptions (such
as P$\neq$NP), both linear and tree-based models are strictly more
interpretable than neural networks. Our complexity analysis, however, does not
provide a clear-cut difference between linear and tree-based models, as we
obtain different results depending on the particular post-hoc explanations
considered. Finally, by applying a finer complexity analysis based on
parameterized complexity, we are able to prove a theoretical result suggesting
that shallow neural networks are more interpretable than deeper ones.
</p>
<a href="http://arxiv.org/abs/2010.12265" target="_blank">arXiv:2010.12265</a> [<a href="http://arxiv.org/pdf/2010.12265" target="_blank">pdf</a>]

<h2>A Combinatorial Perspective on Transfer Learning. (arXiv:2010.12268v1 [cs.LG])</h2>
<h3>Jianan Wang, Eren Sezener, David Budden, Marcus Hutter, Joel Veness</h3>
<p>Human intelligence is characterized not only by the capacity to learn complex
skills, but the ability to rapidly adapt and acquire new skills within an
ever-changing environment. In this work we study how the learning of modular
solutions can allow for effective generalization to both unseen and potentially
differently distributed data. Our main postulate is that the combination of
task segmentation, modular learning and memory-based ensembling can give rise
to generalization on an exponentially growing number of unseen tasks. We
provide a concrete instantiation of this idea using a combination of: (1) the
Forget-Me-Not Process, for task segmentation and memory based ensembling; and
(2) Gated Linear Networks, which in contrast to contemporary deep learning
techniques use a modular and local learning mechanism. We demonstrate that this
system exhibits a number of desirable continual learning properties: robustness
to catastrophic forgetting, no negative transfer and increasing levels of
positive transfer as more tasks are seen. We show competitive performance
against both offline and online methods on standard continual learning
benchmarks.
</p>
<a href="http://arxiv.org/abs/2010.12268" target="_blank">arXiv:2010.12268</a> [<a href="http://arxiv.org/pdf/2010.12268" target="_blank">pdf</a>]

<h2>Reducing Bias in Modeling Real-world Password Strength via Deep Learning and Dynamic Dictionaries. (arXiv:2010.12269v1 [cs.CR])</h2>
<h3>Dario Pasquini, Marco Cianfriglia, Giuseppe Ateniese, Massimo Bernaschi</h3>
<p>Password security hinges on an accurate understanding of the techniques
adopted by attackers. However, current studies mostly rely on probabilistic
password models that are imperfect proxies of real-world guessing strategies.
The main reason is that attackers rely on very pragmatic methods such as
dictionary attacks. Unfortunately, it is inherently difficult correctly
modeling those strategies. To be representative of the actual threat,
dictionary attacks must be thoughtfully configured according to a process that
requires domain-knowledge and expertise that cannot be easily replicated by
researchers and security practitioners. The consequence of inaccurately
calibrating those attacks is the unreliability of password security analysis,
impaired by measurement bias.

In the present work, we introduce new guessing techniques that make
dictionary attacks consistently more resilient to inadequate configurations.
Our framework allows dictionary attacks to self-heal and converge towards
optimal attacks' performance, requiring no supervision or domain-knowledge. To
achieve this: (1) We use a deep neural network to model and then simulate the
proficiency of expert adversaries. (2) Then, we introduce automatic dynamic
strategies within dictionary attacks to mimic experts' ability to adapt their
guessing strategies on the fly by incorporating knowledge on their targets. Our
techniques enable robust and sound password strength estimates, eventually
reducing bias in modeling real-world threats in password security.
</p>
<a href="http://arxiv.org/abs/2010.12269" target="_blank">arXiv:2010.12269</a> [<a href="http://arxiv.org/pdf/2010.12269" target="_blank">pdf</a>]

<h2>Pre-trained Model for Chinese Word Segmentation with Meta Learning. (arXiv:2010.12272v1 [cs.CL])</h2>
<h3>Zhen Ke, Liang Shi, Erli Meng, Bin Wang, Xipeng Qiu</h3>
<p>Recent researches show that pre-trained models such as BERT (Devlin et al.,
2019) are beneficial for Chinese Word Segmentation tasks. However, existing
approaches usually finetune pre-trained models directly on a separate
downstream Chinese Word Segmentation corpus. These recent methods don't fully
utilize the prior knowledge of existing segmentation corpora, and don't regard
the discrepancy between the pre-training tasks and the downstream Chinese Word
Segmentation tasks. In this work, we propose a Pre-Trained Model for Chinese
Word Segmentation, which can be abbreviated as PTM-CWS. PTM-CWS model employs a
unified architecture for different segmentation criteria, and is pre-trained on
a joint multi-criteria corpus with meta learning algorithm. Empirical results
show that our PTM-CWS model can utilize the existing prior segmentation
knowledge, reduce the discrepancy between the pre-training tasks and the
downstream Chinese Word Segmentation tasks, and achieve new state-of-the-art
performance on twelve Chinese Word Segmentation corpora.
</p>
<a href="http://arxiv.org/abs/2010.12272" target="_blank">arXiv:2010.12272</a> [<a href="http://arxiv.org/pdf/2010.12272" target="_blank">pdf</a>]

<h2>A Pre-training Strategy for Recommendation. (arXiv:2010.12284v1 [cs.IR])</h2>
<h3>Susen Yang, Yong Liu, Chenyi Lei, Guoxin Wang, Haihong Tang, Juyong Zhang, Chunyan Miao</h3>
<p>The side information of items has been shown to be effective in building the
recommendation systems. Various methods have been developed to exploit the item
side information for learning users' preferences on items. Differing from
previous work, this paper focuses on developing an unsupervised pre-training
strategy, which can exploit the items' multimodality side information (e.g.,
text and images) to learn the item representations that may benefit downstream
applications, such as personalized item recommendation and click-through ratio
prediction. Firstly, we employ a multimodal graph to describe the relationships
between items and their multimodal feature information. Then, we propose a
novel graph neural network, named Multimodal Graph-BERT (MG-BERT), to learn the
item representations based on the item multimodal graph. Specifically, MG-BERT
is trained by solving the following two graph reconstruction problems, i.e.,
graph structure reconstruction and masked node feature reconstruction.
Experimental results on real datasets demonstrate that the proposed MG-BERT can
effectively exploit the multimodality information of items to help downstream
applications.
</p>
<a href="http://arxiv.org/abs/2010.12284" target="_blank">arXiv:2010.12284</a> [<a href="http://arxiv.org/pdf/2010.12284" target="_blank">pdf</a>]

<h2>Topic Space Trajectories: A case study on machine learning literature. (arXiv:2010.12294v1 [cs.LG])</h2>
<h3>Bastian Sch&#xe4;fermeier, Gerd Stumme, Tom Hanika</h3>
<p>The annual number of publications at scientific venues, for example,
conferences and journals, is growing quickly. Hence, even for researchers
becomes harder and harder to keep track of research topics and their progress.
In this task, researchers can be supported by automated publication analysis.
Yet, many such methods result in uninterpretable, purely numerical
representations. As an attempt to support human analysts, we present
\emph{topic space trajectories}, a structure that allows for the comprehensible
tracking of research topics. We demonstrate how these trajectories can be
interpreted based on eight different analysis approaches. To obtain
comprehensible results, we employ non-negative matrix factorization as well as
suitable visualization techniques. We show the applicability of our approach on
a publication corpus spanning 50 years of machine learning research from 32
publication venues. Our novel analysis method may be employed for paper
classification, for the prediction of future research topics, and for the
recommendation of fitting conferences and journals for submitting unpublished
work.
</p>
<a href="http://arxiv.org/abs/2010.12294" target="_blank">arXiv:2010.12294</a> [<a href="http://arxiv.org/pdf/2010.12294" target="_blank">pdf</a>]

<h2>Graph Learning for Clustering Multi-view Data. (arXiv:2010.12301v1 [cs.LG])</h2>
<h3>Sravanthi Gurugubelli, Sundeep Prabhakar Chepuri</h3>
<p>In this paper, we focus on graph learning from multi-view data of shared
entities for clustering. We can explain interactions between the entities in
multi-view data using a multi-layer graph with a common vertex set representing
the shared entities. The edges on different layers capture the relationships of
the entities. Assuming a smoothness data model, we estimate the graph Laplacian
matrices of the individual graph layers by constraining their ranks to obtain
multi-component graph layers for clustering. We also learn low-dimensional node
embeddings, common to all the views, that assimilate the complementary
information present in the views. We propose an efficient solver based on
alternating minimization to solve the proposed multi-layer multi-component
graph learning problem. Numerical experiments on synthetic and real datasets
demonstrate that the proposed algorithm outperforms state-of-the-art multi-view
clustering techniques.
</p>
<a href="http://arxiv.org/abs/2010.12301" target="_blank">arXiv:2010.12301</a> [<a href="http://arxiv.org/pdf/2010.12301" target="_blank">pdf</a>]

<h2>Adversarial Learning of Feature-based Meta-Embeddings. (arXiv:2010.12305v1 [cs.CL])</h2>
<h3>Lukas Lange, Heike Adel, Jannik Str&#xf6;tgen, Dietrich Klakow</h3>
<p>Certain embedding types outperform others in different scenarios, e.g.,
subword-based embeddings can model rare words well and domain-specific
embeddings can better represent in-domain terms. Therefore, recent works
consider attention-based meta-embeddings to combine different embedding types.
We demonstrate that these methods have two shortcomings: First, the attention
weights are calculated without knowledge of word properties. Second, the
different embedding types can form clusters in the common embedding space,
preventing the computation of a meaningful average of different embeddings and
thus, reducing performance. We propose to solve these problems by using
feature-based meta-embeddings learned with adversarial training. Our
experiments and analysis on sentence classification and sequence tagging tasks
show that our approach is effective. We set the new state of the art on various
datasets across languages and domains.
</p>
<a href="http://arxiv.org/abs/2010.12305" target="_blank">arXiv:2010.12305</a> [<a href="http://arxiv.org/pdf/2010.12305" target="_blank">pdf</a>]

<h2>Network Classifiers Based on Social Learning. (arXiv:2010.12306v1 [eess.SP])</h2>
<h3>Virginia Bordignon, Stefan Vlaski, Vincenzo Matta, Ali H. Sayed</h3>
<p>This work proposes a new way of combining independently trained classifiers
over space and time. Combination over space means that the outputs of spatially
distributed classifiers are aggregated. Combination over time means that the
classifiers respond to streaming data during testing and continue to improve
their performance even during this phase. By doing so, the proposed
architecture is able to improve prediction performance over time with unlabeled
data. Inspired by social learning algorithms, which require prior knowledge of
the observations distribution, we propose a Social Machine Learning (SML)
paradigm that is able to exploit the imperfect models generated during the
learning phase. We show that this strategy results in consistent learning with
high probability, and it yields a robust structure against poorly trained
classifiers. Simulations with an ensemble of feedforward neural networks are
provided to illustrate the theoretical results.
</p>
<a href="http://arxiv.org/abs/2010.12306" target="_blank">arXiv:2010.12306</a> [<a href="http://arxiv.org/pdf/2010.12306" target="_blank">pdf</a>]

<h2>Self-Learning Transformations for Improving Gaze and Head Redirection. (arXiv:2010.12307v1 [cs.CV])</h2>
<h3>Yufeng Zheng, Seonwook Park, Xucong Zhang, Shalini De Mello, Otmar Hilliges</h3>
<p>Many computer vision tasks rely on labeled data. Rapid progress in generative
modeling has led to the ability to synthesize photorealistic images. However,
controlling specific aspects of the generation process such that the data can
be used for supervision of downstream tasks remains challenging. In this paper
we propose a novel generative model for images of faces, that is capable of
producing high-quality images under fine-grained control over eye gaze and head
orientation angles. This requires the disentangling of many appearance related
factors including gaze and head orientation but also lighting, hue etc. We
propose a novel architecture which learns to discover, disentangle and encode
these extraneous variations in a self-learned manner. We further show that
explicitly disentangling task-irrelevant factors results in more accurate
modelling of gaze and head orientation. A novel evaluation scheme shows that
our method improves upon the state-of-the-art in redirection accuracy and
disentanglement between gaze direction and head orientation changes.
Furthermore, we show that in the presence of limited amounts of real-world
training data, our method allows for improvements in the downstream task of
semi-supervised cross-dataset gaze estimation. Please check our project page
at: https://ait.ethz.ch/projects/2020/STED-gaze/
</p>
<a href="http://arxiv.org/abs/2010.12307" target="_blank">arXiv:2010.12307</a> [<a href="http://arxiv.org/pdf/2010.12307" target="_blank">pdf</a>]

<h2>A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios. (arXiv:2010.12309v1 [cs.CL])</h2>
<h3>Michael A. Hedderich, Lukas Lange, Heike Adel, Jannik Str&#xf6;tgen, Dietrich Klakow</h3>
<p>Current developments in natural language processing offer challenges and
opportunities for low-resource languages and domains. Deep neural networks are
known for requiring large amounts of training data which might not be available
in resource-lean scenarios. However, there is also a growing body of works to
improve the performance in low-resource settings. Motivated by fundamental
changes towards neural models and the currently popular pre-train and fine-tune
paradigm, we give an overview of promising approaches for low-resource natural
language processing. After a discussion about the definition of low-resource
scenarios and the different dimensions of data availability, we then examine
methods that enable learning when training data is sparse. This includes
mechanisms to create additional labeled data like data augmentation and distant
supervision as well as transfer learning settings that reduce the need for
target supervision. The survey closes with a brief look into methods suggested
in non-NLP machine learning communities, which might be beneficial for NLP in
low-resource scenarios
</p>
<a href="http://arxiv.org/abs/2010.12309" target="_blank">arXiv:2010.12309</a> [<a href="http://arxiv.org/pdf/2010.12309" target="_blank">pdf</a>]

<h2>Matching the Clinical Reality: Accurate OCT-Based Diagnosis From Few Labels. (arXiv:2010.12316v1 [cs.CV])</h2>
<h3>Valentyn Melnychuk, Evgeniy Faerman, Ilja Manakov, Thomas Seidl</h3>
<p>Unlabeled data is often abundant in the clinic, making machine learning
methods based on semi-supervised learning a good match for this setting.
Despite this, they are currently receiving relatively little attention in
medical image analysis literature. Instead, most practitioners and researchers
focus on supervised or transfer learning approaches. The recently proposed
MixMatch and FixMatch algorithms have demonstrated promising results in
extracting useful representations while requiring very few labels. Motivated by
these recent successes, we apply MixMatch and FixMatch in an ophthalmological
diagnostic setting and investigate how they fare against standard transfer
learning. We find that both algorithms outperform the transfer learning
baseline on all fractions of labelled data. Furthermore, our experiments show
that exponential moving average (EMA) of model parameters, which is a component
of both algorithms, is not needed for our classification problem, as disabling
it leaves the outcome unchanged. Our code is available online:
https://github.com/Valentyn1997/oct-diagn-semi-supervised
</p>
<a href="http://arxiv.org/abs/2010.12316" target="_blank">arXiv:2010.12316</a> [<a href="http://arxiv.org/pdf/2010.12316" target="_blank">pdf</a>]

<h2>Learning Implicit Functions for Topology-Varying Dense 3D Shape Correspondence. (arXiv:2010.12320v1 [cs.CV])</h2>
<h3>Feng Liu, Xiaoming Liu</h3>
<p>The goal of this paper is to learn dense 3D shape correspondence for
topology-varying objects in an unsupervised manner. Conventional implicit
functions estimate the occupancy of a 3D point given a shape latent code.
Instead, our novel implicit function produces a part embedding vector for each
3D point, which is assumed to be similar to its densely corresponded point in
another 3D shape of the same object category. Furthermore, we implement dense
correspondence through an inverse function mapping from the part embedding to a
corresponded 3D point. Both functions are jointly learned with several
effective loss functions to realize our assumption, together with the encoder
generating the shape latent code. During inference, if a user selects an
arbitrary point on the source shape, our algorithm can automatically generate a
confidence score indicating whether there is a correspondence on the target
shape, as well as the corresponding semantic point if there is one. Such a
mechanism inherently benefits man-made objects with different part
constitutions. The effectiveness of our approach is demonstrated through
unsupervised 3D semantic correspondence and shape segmentation.
</p>
<a href="http://arxiv.org/abs/2010.12320" target="_blank">arXiv:2010.12320</a> [<a href="http://arxiv.org/pdf/2010.12320" target="_blank">pdf</a>]

<h2>BARThez: a Skilled Pretrained French Sequence-to-Sequence Model. (arXiv:2010.12321v1 [cs.CL])</h2>
<h3>Moussa Kamal Eddine, Antoine J.-P. Tixier, Michalis Vazirgiannis</h3>
<p>Inductive transfer learning, enabled by self-supervised learning, have taken
the entire Natural Language Processing (NLP) field by storm, with models such
as BERT and BART setting new state of the art on countless natural language
understanding tasks. While there are some notable exceptions, most of the
available models and research have been conducted for the English language. In
this work, we introduce BARThez, the first BART model for the French language
(to the best of our knowledge). BARThez was pretrained on a very large
monolingual French corpus from past research that we adapted to suit BART's
perturbation schemes. Unlike already existing BERT-based French language models
such as CamemBERT and FlauBERT, BARThez is particularly well-suited for
generative tasks, since not only its encoder but also its decoder is
pretrained. In addition to discriminative tasks from the FLUE benchmark, we
evaluate BARThez on a novel summarization dataset, OrangeSum, that we release
with this paper. We also continue the pretraining of an already pretrained
multilingual BART on BARThez's corpus, and we show that the resulting model,
which we call mBARTHez, provides a significant boost over vanilla BARThez, and
is on par with or outperforms CamemBERT and FlauBERT.
</p>
<a href="http://arxiv.org/abs/2010.12321" target="_blank">arXiv:2010.12321</a> [<a href="http://arxiv.org/pdf/2010.12321" target="_blank">pdf</a>]

<h2>Stochastic groundwater flow analysis in heterogeneous aquifer with modified neural architecture search (NAS) based physics-informed neural networks using transfer learning. (arXiv:2010.12344v1 [cs.LG])</h2>
<h3>Hongwei Guo, Xiaoying Zhuang, Dawei Liang, Timon Rabczuk</h3>
<p>In this work, a modified neural architecture search method (NAS) based
physics-informed deep learning model is presented to solve the groundwater flow
problems in porous media. Monte Carlo method based on a randomized spectral
representation is first employed to construct a stochastic model for simulation
of flow through porous media. The desired hydraulic conductivity fields are
assumed to be log-normally distributed with exponential and Gaussian
correlations. To analyze the Darcy equation with the random hydraulic
conductivity in this case when its intensity of fluctuations is small, the
lowest-order perturbation theory is used to reduce the difficulty of
calculations, by neglecting the higher-order nonlinear part. To solve the
governing equations for groundwater flow problem, we build a modified NAS model
based on physics-informed neural networks (PINNs) with transfer learning in
this paper that will be able to fit different partial differential equations
(PDEs) with less calculation. The performance estimation strategies adopted is
constructed from an error estimation model using the method of manufactured
solutions. Since the configuration selection of the neural network has a strong
influence on the simulation results, we apply sensitivity analysis to obtain
the prior knowledge of the PINNs model and narrow down the range of parameters
for search space and use hyper-parameter optimization algorithms to further
determine the values of the parameters. Further the NAS based PINNs model also
saves the weights and biases of the most favorable architectures, which is then
used in the fine-tuning process. The proposed NAS model based deep collocation
method is verified to be effective and accurate through numerical examples in
different dimensions using different manufactured solutions.
</p>
<a href="http://arxiv.org/abs/2010.12344" target="_blank">arXiv:2010.12344</a> [<a href="http://arxiv.org/pdf/2010.12344" target="_blank">pdf</a>]

<h2>Online Algorithm for Unsupervised Sequential Selection with Contextual Information. (arXiv:2010.12353v1 [cs.LG])</h2>
<h3>Arun Verma, Manjesh K. Hanawal, Csaba Szepesv&#xe1;ri, Venkatesh Saligrama</h3>
<p>In this paper, we study Contextual Unsupervised Sequential Selection (USS), a
new variant of the stochastic contextual bandits problem where the loss of an
arm cannot be inferred from the observed feedback. In our setup, arms are
associated with fixed costs and are ordered, forming a cascade. In each round,
a context is presented, and the learner selects the arms sequentially till some
depth. The total cost incurred by stopping at an arm is the sum of fixed costs
of arms selected and the stochastic loss associated with the arm. The learner's
goal is to learn a decision rule that maps contexts to arms with the goal of
minimizing the total expected loss. The problem is challenging as we are faced
with an unsupervised setting as the total loss cannot be estimated. Clearly,
learning is feasible only if the optimal arm can be inferred (explicitly or
implicitly) from the problem structure. We observe that learning is still
possible when the problem instance satisfies the so-called 'Contextual Weak
Dominance' (CWD) property. Under CWD, we propose an algorithm for the
contextual USS problem and demonstrate that it has sub-linear regret.
Experiments on synthetic and real datasets validate our algorithm.
</p>
<a href="http://arxiv.org/abs/2010.12353" target="_blank">arXiv:2010.12353</a> [<a href="http://arxiv.org/pdf/2010.12353" target="_blank">pdf</a>]

<h2>Regret in Online Recommendation Systems. (arXiv:2010.12363v1 [stat.ML])</h2>
<h3>Kaito Ariu, Narae Ryu, Se-Young Yun, Alexandre Prouti&#xe8;re</h3>
<p>This paper proposes a theoretical analysis of recommendation systems in an
online setting, where items are sequentially recommended to users over time. In
each round, a user, randomly picked from a population of $m$ users, requests a
recommendation. The decision-maker observes the user and selects an item from a
catalogue of $n$ items. Importantly, an item cannot be recommended twice to the
same user. The probabilities that a user likes each item are unknown. The
performance of the recommendation algorithm is captured through its regret,
considering as a reference an Oracle algorithm aware of these probabilities. We
investigate various structural assumptions on these probabilities: we derive
for each structure regret lower bounds, and devise algorithms achieving these
limits. Interestingly, our analysis reveals the relative weights of the
different components of regret: the component due to the constraint of not
presenting the same item twice to the same user, that due to learning the
chances users like items, and finally that arising when learning the underlying
structure.
</p>
<a href="http://arxiv.org/abs/2010.12363" target="_blank">arXiv:2010.12363</a> [<a href="http://arxiv.org/pdf/2010.12363" target="_blank">pdf</a>]

<h2>Learning to Dispatch for Job Shop Scheduling via Deep Reinforcement Learning. (arXiv:2010.12367v1 [cs.LG])</h2>
<h3>Cong Zhang, Wen Song, Zhiguang Cao, Jie Zhang, Puay Siew Tan, Chi Xu</h3>
<p>Priority dispatching rule (PDR) is widely used for solving real-world
Job-shop scheduling problem (JSSP). However, the design of effective PDRs is a
tedious task, requiring a myriad of specialized knowledge and often delivering
limited performance. In this paper, we propose to automatically learn PDRs via
an end-to-end deep reinforcement learning agent. We exploit the disjunctive
graph representation of JSSP, and propose a Graph Neural Network based scheme
to embed the states encountered during solving. The resulting policy network is
size-agnostic, effectively enabling generalization on large-scale instances.
Experiments show that the agent can learn high-quality PDRs from scratch with
elementary raw features, and demonstrates strong performance against the best
existing PDRs. The learned policies also perform well on much larger instances
that are unseen in training.
</p>
<a href="http://arxiv.org/abs/2010.12367" target="_blank">arXiv:2010.12367</a> [<a href="http://arxiv.org/pdf/2010.12367" target="_blank">pdf</a>]

<h2>A Simulation-based Education Approach for the Electromagnetic and Electromechanical Transient Waves in Power Systems. (arXiv:2010.12379v1 [eess.SY])</h2>
<h3>Abdulelah Alharbi, Shutang You</h3>
<p>Power systems usually go through electromagnetic and electromechanical
transient processes after different disturbances. Learning the characteristics
and the differences between them are important but not easy for students
majoring in power systems. This paper presents a simulation-based approach to
comprehensively study the two types of transient waves, constituting the
experimental part of the power system transient and stability course. In this
approach, three models with different levels of complexity are developed to
simultaneously show the two types of transient waves in the time domain. The
developed models are then demonstrated as testbeds for investigating various
aspects related to the two types of transient, such as waveforms induced by
different disturbances, influencing factors on the propagation speed, and the
interaction between incident waves and the reflective waves. In addition, a
theory-to-practice engineering research process is demonstrated through
developing a power system event-location application, which is inspired by
electromechanical wave propagation study. The proposed education process and
models at various complexity levels provide a creative and interactive way for
power system transients and dynamics study.
</p>
<a href="http://arxiv.org/abs/2010.12379" target="_blank">arXiv:2010.12379</a> [<a href="http://arxiv.org/pdf/2010.12379" target="_blank">pdf</a>]

<h2>Efficient grouping for keypoint detection. (arXiv:2010.12390v1 [cs.CV])</h2>
<h3>Alexey Sidnev, Ekaterina Krasikova, Maxim Kazakov</h3>
<p>The success of deep neural networks in the traditional keypoint detection
task encourages researchers to solve new problems and collect more complex
datasets. The size of the DeepFashion2 dataset poses a new challenge on the
keypoint detection task, as it comprises 13 clothing categories that span a
wide range of keypoints (294 in total). The direct prediction of all keypoints
leads to huge memory consumption, slow training, and a slow inference time.
This paper studies the keypoint grouping approach and how it affects the
performance of the CenterNet architecture. We propose a simple and efficient
automatic grouping technique with a powerful post-processing method and apply
it to the DeepFashion2 fashion landmark task and the MS COCO pose estimation
task. This reduces memory consumption and processing time during inference by
up to 19% and 30% respectively, and during the training stage by 28% and 26%
respectively, without compromising accuracy.
</p>
<a href="http://arxiv.org/abs/2010.12390" target="_blank">arXiv:2010.12390</a> [<a href="http://arxiv.org/pdf/2010.12390" target="_blank">pdf</a>]

<h2>Segmentation of the cortical plate in fetal brain MRI with a topological loss. (arXiv:2010.12391v1 [eess.IV])</h2>
<h3>Priscille de Dumast, Hamza Kebiri, Chirine Atat, Vincent Dunet, M&#xe9;riam Koob, Meritxell Bach Cuadra</h3>
<p>The fetal cortical plate undergoes drastic morphological changes throughout
early in utero development that can be observed using magnetic resonance (MR)
imaging. An accurate MR image segmentation, and more importantly a
topologically correct delineation of the cortical gray matter, is a key
baseline to perform further quantitative analysis of brain development. In this
paper, we propose for the first time the integration of a topological
constraint, as an additional loss function, to enhance the morphological
consistency of a deep learning-based segmentation of the fetal cortical plate.
We quantitatively evaluate our method on 18 fetal brain atlases ranging from 21
to 38 weeks of gestation, showing the significant benefits of our method
through all gestational ages as compared to a baseline method. Furthermore,
qualitative evaluation by three different experts on 130 randomly selected
slices from 26 clinical MRIs evidences the out-performance of our method
independently of the MR reconstruction quality.
</p>
<a href="http://arxiv.org/abs/2010.12391" target="_blank">arXiv:2010.12391</a> [<a href="http://arxiv.org/pdf/2010.12391" target="_blank">pdf</a>]

<h2>RSKDD-Net: Random Sample-based Keypoint Detector and Descriptor. (arXiv:2010.12394v1 [cs.CV])</h2>
<h3>Fan Lu, Guang Chen, Yinlong Liu, Zhongnan Qu, Alois Knoll</h3>
<p>Keypoint detector and descriptor are two main components of point cloud
registration. Previous learning-based keypoint detectors rely on saliency
estimation for each point or farthest point sample (FPS) for candidate points
selection, which are inefficient and not applicable in large scale scenes. This
paper proposes Random Sample-based Keypoint Detector and Descriptor Network
(RSKDD-Net) for large scale point cloud registration. The key idea is using
random sampling to efficiently select candidate points and using a
learning-based method to jointly generate keypoints and descriptors. To tackle
the information loss of random sampling, we exploit a novel random dilation
cluster strategy to enlarge the receptive field of each sampled point and an
attention mechanism to aggregate the positions and features of neighbor points.
Furthermore, we propose a matching loss to train the descriptor in a weakly
supervised manner. Extensive experiments on two large scale outdoor LiDAR
datasets show that the proposed RSKDD-Net achieves state-of-the-art performance
with more than 15 times faster than existing methods. Our code is available at
https://github.com/ispc-lab/RSKDD-Net.
</p>
<a href="http://arxiv.org/abs/2010.12394" target="_blank">arXiv:2010.12394</a> [<a href="http://arxiv.org/pdf/2010.12394" target="_blank">pdf</a>]

<h2>On the Equivalence of Decoupled Graph Convolution Network and Label Propagation. (arXiv:2010.12408v1 [cs.LG])</h2>
<h3>Hande Dong, Jiawei Chen, Fuli Feng, Xiangnan He, Shuxian Bi, Zhaolin Ding, Peng Cui</h3>
<p>The original design of Graph Convolution Network (GCN) couples feature
transformation and neighborhood aggregation for node representation learning.
Recently, some work shows that coupling is inferior to decoupling, which
supports deep graph propagation and has become the latest paradigm of GCN
(e.g., APPNP and SGCN). Despite effectiveness, the working mechanisms of the
decoupled GCN are not well understood. In this paper, we explore the decoupled
GCN for semi-supervised node classification from a novel and fundamental
perspective -- label propagation. We conduct thorough theoretical analyses,
proving that the decoupled GCN is essentially the same as the two-step label
propagation: first, propagating the known labels along the graph to generate
pseudo-labels for the unlabeled nodes, and second, training normal neural
network classifiers on the augmented pseudo-labeled data. More interestingly,
we reveal the effectiveness of decoupled GCN: going beyond the conventional
label propagation, it could automatically assign structure- and model- aware
weights to the pseudo-label data. This explains why the decoupled GCN is
relatively robust to the structure noise and over-smoothing, but sensitive to
the label noise and model initialization. Based on this insight, we propose a
new label propagation method named Propagation then Training Adaptively (PTA),
which overcomes the flaws of the decoupled GCN with a dynamic and adaptive
weighting strategy. Our PTA is simple yet more effective and robust than
decoupled GCN. We empirically validate our findings on four benchmark datasets,
demonstrating the advantages of our method.
</p>
<a href="http://arxiv.org/abs/2010.12408" target="_blank">arXiv:2010.12408</a> [<a href="http://arxiv.org/pdf/2010.12408" target="_blank">pdf</a>]

<h2>SAHDL: Sparse Attention Hypergraph Regularized Dictionary Learning. (arXiv:2010.12416v1 [cs.CV])</h2>
<h3>Shuai Shao, Rui Xu, Yan-Jiang Wang, Weifeng Liu, Bao-Di Liu</h3>
<p>In recent years, the attention mechanism contributes significantly to
hypergraph based neural networks. However, these methods update the attention
weights with the network propagating. That is to say, this type of attention
mechanism is only suitable for deep learning-based methods while not applicable
to the traditional machine learning approaches. In this paper, we propose a
hypergraph based sparse attention mechanism to tackle this issue and embed it
into dictionary learning. More specifically, we first construct a sparse
attention hypergraph, asset attention weights to samples by employing the
$\ell_1$-norm sparse regularization to mine the high-order relationship among
sample features. Then, we introduce the hypergraph Laplacian operator to
preserve the local structure for subspace transformation in dictionary
learning. Besides, we incorporate the discriminative information into the
hypergraph as the guidance to aggregate samples. Unlike previous works, our
method updates attention weights independently, does not rely on the deep
network. We demonstrate the efficacy of our approach on four benchmark
datasets.
</p>
<a href="http://arxiv.org/abs/2010.12416" target="_blank">arXiv:2010.12416</a> [<a href="http://arxiv.org/pdf/2010.12416" target="_blank">pdf</a>]

<h2>DLDL: Dynamic Label Dictionary Learning via Hypergraph Regularization. (arXiv:2010.12417v1 [cs.CV])</h2>
<h3>Shuai Shao, Mengke Wang, Rui Xu, Yan-Jiang Wang, Bao-Di Liu</h3>
<p>For classification tasks, dictionary learning based methods have attracted
lots of attention in recent years. One popular way to achieve this purpose is
to introduce label information to generate a discriminative dictionary to
represent samples. However, compared with traditional dictionary learning, this
category of methods only achieves significant improvements in supervised
learning, and has little positive influence on semi-supervised or unsupervised
learning. To tackle this issue, we propose a Dynamic Label Dictionary Learning
(DLDL) algorithm to generate the soft label matrix for unlabeled data.
Specifically, we employ hypergraph manifold regularization to keep the
relations among original data, transformed data, and soft labels consistent. We
demonstrate the efficiency of the proposed DLDL approach on two remote sensing
datasets.
</p>
<a href="http://arxiv.org/abs/2010.12417" target="_blank">arXiv:2010.12417</a> [<a href="http://arxiv.org/pdf/2010.12417" target="_blank">pdf</a>]

<h2>Deep Learning Framework for Measuring the Digital Strategy of Companies from Earnings Calls. (arXiv:2010.12418v1 [cs.CL])</h2>
<h3>Ahmed Ghanim Al-Ali, Robert Phaal, Donald Sull</h3>
<p>Companies today are racing to leverage the latest digital technologies, such
as artificial intelligence, blockchain, and cloud computing. However, many
companies report that their strategies did not achieve the anticipated business
results. This study is the first to apply state of the art NLP models on
unstructured data to understand the different clusters of digital strategy
patterns that companies are Adopting. We achieve this by analyzing earnings
calls from Fortune Global 500 companies between 2015 and 2019. We use
Transformer based architecture for text classification which show a better
understanding of the conversation context. We then investigate digital strategy
patterns by applying clustering analysis. Our findings suggest that Fortune 500
companies use four distinct strategies which are product led, customer
experience led, service led, and efficiency led. This work provides an
empirical baseline for companies and researchers to enhance our understanding
of the field.
</p>
<a href="http://arxiv.org/abs/2010.12418" target="_blank">arXiv:2010.12418</a> [<a href="http://arxiv.org/pdf/2010.12418" target="_blank">pdf</a>]

<h2>Progressive Training of Multi-level Wavelet Residual Networks for Image Denoising. (arXiv:2010.12422v1 [eess.IV])</h2>
<h3>Yali Peng, Yue Cao, Shigang Liu, Jian Yang, Wangmeng Zuo</h3>
<p>Recent years have witnessed the great success of deep convolutional neural
networks (CNNs) in image denoising. Albeit deeper network and larger model
capacity generally benefit performance, it remains a challenging practical
issue to train a very deep image denoising network. Using multilevel
wavelet-CNN (MWCNN) as an example, we empirically find that the denoising
performance cannot be significantly improved by either increasing wavelet
decomposition levels or increasing convolution layers within each level. To
cope with this issue, this paper presents a multi-level wavelet residual
network (MWRN) architecture as well as a progressive training (PTMWRN) scheme
to improve image denoising performance. In contrast to MWCNN, our MWRN
introduces several residual blocks after each level of discrete wavelet
transform (DWT) and before inverse discrete wavelet transform (IDWT). For
easing the training difficulty, scale-specific loss is applied to each level of
MWRN by requiring the intermediate output to approximate the corresponding
wavelet subbands of ground-truth clean image. To ensure the effectiveness of
scale-specific loss, we also take the wavelet subbands of noisy image as the
input to each scale of the encoder. Furthermore, progressive training scheme is
adopted for better learning of MWRN by beigining with training the lowest level
of MWRN and progressively training the upper levels to bring more fine details
to denoising results. Experiments on both synthetic and real-world noisy images
show that our PT-MWRN performs favorably against the state-of-the-art denoising
methods in terms both quantitative metrics and visual quality.
</p>
<a href="http://arxiv.org/abs/2010.12422" target="_blank">arXiv:2010.12422</a> [<a href="http://arxiv.org/pdf/2010.12422" target="_blank">pdf</a>]

<h2>Training Noisy Single-Channel Speech Separation With Noisy Oracle Sources: A Large Gap and A Small Step. (arXiv:2010.12430v1 [eess.AS])</h2>
<h3>Matthew Maciejewski, Jing Shi, Shinji Watanabe, Sanjeev Khudanpur</h3>
<p>As the performance of single-channel speech separation systems has improved,
there has been a desire to move to more challenging conditions than the clean,
near-field speech that initial systems were developed on. When training deep
learning separation models, a need for ground truth leads to training on
synthetic mixtures. As such, training in noisy conditions requires either using
noise synthetically added to clean speech, preventing the use of in-domain data
for a noisy-condition task, or training using mixtures of noisy speech,
requiring the network to additionally separate the noise. We demonstrate the
relative inseparability of noise and that this noisy speech paradigm leads to
significant degradation of system performance. We also propose an
SI-SDR-inspired training objective that tries to exploit the inseparability of
noise to implicitly partition the signal and discount noise separation errors,
enabling the training of better separation systems with noisy oracle sources.
</p>
<a href="http://arxiv.org/abs/2010.12430" target="_blank">arXiv:2010.12430</a> [<a href="http://arxiv.org/pdf/2010.12430" target="_blank">pdf</a>]

<h2>Pathological Visual Question Answering. (arXiv:2010.12435v1 [cs.CV])</h2>
<h3>Xuehai He, Zhuo Cai, Wenlan Wei, Yichen Zhang, Luntian Mou, Eric Xing, Pengtao Xie</h3>
<p>Is it possible to develop an "AI Pathologist" to pass the board-certified
examination of the American Board of Pathology (ABP)? To build such a system,
three challenges need to be addressed. First, we need to create a visual
question answering (VQA) dataset where the AI agent is presented with a
pathology image together with a question and is asked to give the correct
answer. Due to privacy concerns, pathology images are usually not publicly
available. Besides, only well-trained pathologists can understand pathology
images, but they barely have time to help create datasets for AI research. The
second challenge is: since it is difficult to hire highly experienced
pathologists to create pathology visual questions and answers, the resulting
pathology VQA dataset may contain errors. Training pathology VQA models using
these noisy or even erroneous data will lead to problematic models that cannot
generalize well on unseen images. The third challenge is: the medical concepts
and knowledge covered in pathology question-answer (QA) pairs are very diverse
while the number of QA pairs available for modeling training is limited. How to
learn effective representations of diverse medical concepts based on limited
data is technically demanding. In this paper, we aim to address these three
challenges. To our best knowledge, our work represents the first one addressing
the pathology VQA problem. To deal with the issue that a publicly available
pathology VQA dataset is lacking, we create PathVQA dataset. To address the
second challenge, we propose a learning-by-ignoring approach. To address the
third challenge, we propose to use cross-modal self-supervised learning. We
perform experiments on our created PathVQA dataset and the results demonstrate
the effectiveness of our proposed learning-by-ignoring method and cross-modal
self-supervised learning methods.
</p>
<a href="http://arxiv.org/abs/2010.12435" target="_blank">arXiv:2010.12435</a> [<a href="http://arxiv.org/pdf/2010.12435" target="_blank">pdf</a>]

<h2>Transferable Graph Optimizers for ML Compilers. (arXiv:2010.12438v1 [cs.LG])</h2>
<h3>Yanqi Zhou, Sudip Roy, Amirali Abdolrashidi, Daniel Wong, Peter Ma, Qiumin Xu, Hanxiao Liu, Mangpo Phitchaya Phothilimtha, Shen Wang, Anna Goldie, Azalia Mirhoseini, James Laudon</h3>
<p>Most compilers for machine learning (ML) frameworks need to solve many
correlated optimization problems to generate efficient machine code. Current ML
compilers rely on heuristics based algorithms to solve these optimization
problems one at a time. However, this approach is not only hard to maintain but
often leads to sub-optimal solutions especially for newer model architectures.
Existing learning based approaches in the literature are sample inefficient,
tackle a single optimization problem, and do not generalize to unseen graphs
making them infeasible to be deployed in practice. To address these
limitations, we propose an end-to-end, transferable deep reinforcement learning
method for computational graph optimization (GO), based on a scalable
sequential attention mechanism over an inductive graph neural network. GO
generates decisions on the entire graph rather than on each individual node
autoregressively, drastically speeding up the search compared to prior methods.
Moreover, we propose recurrent attention layers to jointly optimize dependent
graph optimization tasks and demonstrate 33%-60% speedup on three graph
optimization tasks compared to TensorFlow default optimization. On a diverse
set of representative graphs consisting of up to 80,000 nodes, including
Inception-v3, Transformer-XL, and WaveNet, GO achieves on average 21%
improvement over human experts and 18% improvement over the prior state of the
art with 15x faster convergence, on a device placement task evaluated in real
systems.
</p>
<a href="http://arxiv.org/abs/2010.12438" target="_blank">arXiv:2010.12438</a> [<a href="http://arxiv.org/pdf/2010.12438" target="_blank">pdf</a>]

<h2>Importance-Aware Semantic Segmentation in Self-Driving with Discrete Wasserstein Training. (arXiv:2010.12440v1 [cs.CV])</h2>
<h3>Xiaofeng Liu, Yuzhuo Han, Song Bai, Yi Ge, Tianxing Wang, Xu Han, Site Li, Jane You, Ju Lu</h3>
<p>Semantic segmentation (SS) is an important perception manner for self-driving
cars and robotics, which classifies each pixel into a pre-determined class. The
widely-used cross entropy (CE) loss-based deep networks has achieved
significant progress w.r.t. the mean Intersection-over Union (mIoU). However,
the cross entropy loss can not take the different importance of each class in
an self-driving system into account. For example, pedestrians in the image
should be much more important than the surrounding buildings when make a
decisions in the driving, so their segmentation results are expected to be as
accurate as possible. In this paper, we propose to incorporate the
importance-aware inter-class correlation in a Wasserstein training framework by
configuring its ground distance matrix. The ground distance matrix can be
pre-defined following a priori in a specific task, and the previous
importance-ignored methods can be the particular cases. From an optimization
perspective, we also extend our ground metric to a linear, convex or concave
increasing function $w.r.t.$ pre-defined ground distance. We evaluate our
method on CamVid and Cityscapes datasets with different backbones (SegNet,
ENet, FCN and Deeplab) in a plug and play fashion. In our extenssive
experiments, Wasserstein loss demonstrates superior segmentation performance on
the predefined critical classes for safe-driving.
</p>
<a href="http://arxiv.org/abs/2010.12440" target="_blank">arXiv:2010.12440</a> [<a href="http://arxiv.org/pdf/2010.12440" target="_blank">pdf</a>]

<h2>Estimation of Cardiac Valve Annuli Motion with Deep Learning. (arXiv:2010.12446v1 [eess.IV])</h2>
<h3>Eric Kerfoot, Carlos Escudero King, Tefvik Ismail, David Nordsletten, Renee Miller</h3>
<p>Valve annuli motion and morphology, measured from non-invasive imaging, can
be used to gain a better understanding of healthy and pathological heart
function. Measurements such as long-axis strain as well as peak strain rates
provide markers of systolic function. Likewise, early and late-diastolic
filling velocities are used as indicators of diastolic function. Quantifying
global strains, however, requires a fast and precise method of tracking
long-axis motion throughout the cardiac cycle. Valve landmarks such as the
insertion of leaflets into the myocardial wall provide features that can be
tracked to measure global long-axis motion. Feature tracking methods require
initialisation, which can be time-consuming in studies with large cohorts.
Therefore, this study developed and trained a neural network to identify ten
features from unlabeled long-axis MR images: six mitral valve points from three
long-axis views, two aortic valve points and two tricuspid valve points. This
study used manual annotations of valve landmarks in standard 2-, 3- and
4-chamber long-axis images collected in clinical scans to train the network.
The accuracy in the identification of these ten features, in pixel distance,
was compared with the accuracy of two commonly used feature tracking methods as
well as the inter-observer variability of manual annotations. Clinical
measures, such as valve landmark strain and motion between end-diastole and
end-systole, are also presented to illustrate the utility and robustness of the
method.
</p>
<a href="http://arxiv.org/abs/2010.12446" target="_blank">arXiv:2010.12446</a> [<a href="http://arxiv.org/pdf/2010.12446" target="_blank">pdf</a>]

<h2>LoopReg: Self-supervised Learning of Implicit Surface Correspondences, Pose and Shape for 3D Human Mesh Registration. (arXiv:2010.12447v1 [cs.CV])</h2>
<h3>Bharat Lal Bhatnagar, Cristian Sminchisescu, Christian Theobalt, Gerard Pons-Moll</h3>
<p>We address the problem of fitting 3D human models to 3D scans of dressed
humans. Classical methods optimize both the data-to-model correspondences and
the human model parameters (pose and shape), but are reliable only when
initialized close to the solution. Some methods initialize the optimization
based on fully supervised correspondence predictors, which is not
differentiable end-to-end, and can only process a single scan at a time. Our
main contribution is LoopReg, an end-to-end learning framework to register a
corpus of scans to a common 3D human model. The key idea is to create a
self-supervised loop. A backward map, parameterized by a Neural Network,
predicts the correspondence from every scan point to the surface of the human
model. A forward map, parameterized by a human model, transforms the
corresponding points back to the scan based on the model parameters (pose and
shape), thus closing the loop. Formulating this closed loop is not
straightforward because it is not trivial to force the output of the NN to be
on the surface of the human model - outside this surface the human model is not
even defined. To this end, we propose two key innovations. First, we define the
canonical surface implicitly as the zero level set of a distance field in R3,
which in contrast to morecommon UV parameterizations, does not require cutting
the surface, does not have discontinuities, and does not induce distortion.
Second, we diffuse the human model to the 3D domain R3. This allows to map the
NN predictions forward,even when they slightly deviate from the zero level set.
Results demonstrate that we can train LoopRegmainly self-supervised - following
a supervised warm-start, the model becomes increasingly more accurate as
additional unlabelled raw scans are processed. Our code and pre-trained models
can be downloaded for research.
</p>
<a href="http://arxiv.org/abs/2010.12447" target="_blank">arXiv:2010.12447</a> [<a href="http://arxiv.org/pdf/2010.12447" target="_blank">pdf</a>]

<h2>Primal-Dual Mesh Convolutional Neural Networks. (arXiv:2010.12455v1 [cs.CV])</h2>
<h3>Francesco Milano, Antonio Loquercio, Antoni Rosinol, Davide Scaramuzza, Luca Carlone</h3>
<p>Recent works in geometric deep learning have introduced neural networks that
allow performing inference tasks on three-dimensional geometric data by
defining convolution, and sometimes pooling, operations on triangle meshes.
These methods, however, either consider the input mesh as a graph, and do not
exploit specific geometric properties of meshes for feature aggregation and
downsampling, or are specialized for meshes, but rely on a rigid definition of
convolution that does not properly capture the local topology of the mesh. We
propose a method that combines the advantages of both types of approaches,
while addressing their limitations: we extend a primal-dual framework drawn
from the graph-neural-network literature to triangle meshes, and define
convolutions on two types of graphs constructed from an input mesh. Our method
takes features for both edges and faces of a 3D mesh as input and dynamically
aggregates them using an attention mechanism. At the same time, we introduce a
pooling operation with a precise geometric interpretation, that allows handling
variations in the mesh connectivity by clustering mesh faces in a task-driven
fashion. We provide theoretical insights of our approach using tools from the
mesh-simplification literature. In addition, we validate experimentally our
method in the tasks of shape classification and shape segmentation, where we
obtain comparable or superior performance to the state of the art.
</p>
<a href="http://arxiv.org/abs/2010.12455" target="_blank">arXiv:2010.12455</a> [<a href="http://arxiv.org/pdf/2010.12455" target="_blank">pdf</a>]

<h2>Adaptive Gradient Quantization for Data-Parallel SGD. (arXiv:2010.12460v1 [cs.LG])</h2>
<h3>Fartash Faghri, Iman Tabrizian, Ilia Markov, Dan Alistarh, Daniel Roy, Ali Ramezani-Kebrya</h3>
<p>Many communication-efficient variants of SGD use gradient quantization
schemes. These schemes are often heuristic and fixed over the course of
training. We empirically observe that the statistics of gradients of deep
models change during the training. Motivated by this observation, we introduce
two adaptive quantization schemes, ALQ and AMQ. In both schemes, processors
update their compression schemes in parallel by efficiently computing
sufficient statistics of a parametric distribution. We improve the validation
accuracy by almost 2% on CIFAR-10 and 1% on ImageNet in challenging low-cost
communication setups. Our adaptive methods are also significantly more robust
to the choice of hyperparameters.
</p>
<a href="http://arxiv.org/abs/2010.12460" target="_blank">arXiv:2010.12460</a> [<a href="http://arxiv.org/pdf/2010.12460" target="_blank">pdf</a>]

<h2>Learning to Noise: Application-Agnostic Data Sharing with Local Differential Privacy. (arXiv:2010.12464v1 [cs.LG])</h2>
<h3>Alex Mansbridge, Gregory Barbour, Davide Piras, Christopher Frye, Ilya Feige, David Barber</h3>
<p>In recent years, the collection and sharing of individuals' private data has
become commonplace in many industries. Local differential privacy (LDP) is a
rigorous approach which uses a randomized algorithm to preserve privacy even
from the database administrator, unlike the more standard central differential
privacy. For LDP, when applying noise directly to high-dimensional data, the
level of noise required all but entirely destroys data utility. In this paper
we introduce a novel, application-agnostic privatization mechanism that
leverages representation learning to overcome the prohibitive noise
requirements of direct methods, while maintaining the strict guarantees of LDP.
We further demonstrate that this privatization mechanism can be used to train
machine learning algorithms across a range of applications, including private
data collection, private novel-class classification, and the augmentation of
clean datasets with additional privatized features. We achieve significant
gains in performance on downstream classification tasks relative to benchmarks
that noise the data directly, which are state-of-the-art in the context of
application-agnostic LDP mechanisms for high-dimensional data.
</p>
<a href="http://arxiv.org/abs/2010.12464" target="_blank">arXiv:2010.12464</a> [<a href="http://arxiv.org/pdf/2010.12464" target="_blank">pdf</a>]

<h2>The IDLAB VoxCeleb Speaker Recognition Challenge 2020 System Description. (arXiv:2010.12468v1 [eess.AS])</h2>
<h3>Jenthe Thienpondt, Brecht Desplanques, Kris Demuynck</h3>
<p>In this technical report we describe the IDLAB top-scoring submissions for
the VoxCeleb Speaker Recognition Challenge 2020 (VoxSRC-20) in the supervised
and unsupervised speaker verification tracks. For the supervised verification
tracks we trained 6 state-of-the-art ECAPA-TDNN systems and 4 Resnet34 based
systems with architectural variations. On all models we apply a large margin
fine-tuning strategy, which enables the training procedure to use higher margin
penalties by using longer training utterances. In addition, we use
quality-aware score calibration which introduces quality metrics in the
calibration system to generate more consistent scores across varying levels of
utterance conditions. A fusion of all systems with both enhancements applied
led to the first place on the open and closed supervised verification tracks.
The unsupervised system is trained through contrastive learning. Subsequent
pseudo-label generation by iterative clustering of the training embeddings
allows the use of supervised techniques. This procedure led to the winning
submission on the unsupervised track, and its performance is closing in on
supervised training.
</p>
<a href="http://arxiv.org/abs/2010.12468" target="_blank">arXiv:2010.12468</a> [<a href="http://arxiv.org/pdf/2010.12468" target="_blank">pdf</a>]

<h2>Intrinsic Quality Assessment of Arguments. (arXiv:2010.12473v1 [cs.CL])</h2>
<h3>Henning Wachsmuth, Till Werner</h3>
<p>Several quality dimensions of natural language arguments have been
investigated. Some are likely to be reflected in linguistic features (e.g., an
argument's arrangement), whereas others depend on context (e.g., relevance) or
topic knowledge (e.g., acceptability). In this paper, we study the intrinsic
computational assessment of 15 dimensions, i.e., only learning from an
argument's text. In systematic experiments with eight feature types on an
existing corpus, we observe moderate but significant learning success for most
dimensions. Rhetorical quality seems hardest to assess, and subjectivity
features turn out strong, although length bias in the corpus impedes full
validity. We also find that human assessors differ more clearly to each other
than to our approach.
</p>
<a href="http://arxiv.org/abs/2010.12473" target="_blank">arXiv:2010.12473</a> [<a href="http://arxiv.org/pdf/2010.12473" target="_blank">pdf</a>]

<h2>Speech enhancement aided end-to-end multi-task learning for voice activity detection. (arXiv:2010.12484v1 [eess.AS])</h2>
<h3>Xu Tan, Xiao-Lei Zhang</h3>
<p>Robust voice activity detection (VAD) is a challenging task in low
signal-to-noise (SNR) environments. Recent studies show that speech enhancement
is helpful to VAD, but the performance improvement is limited. To address this
issue, here we propose a speech enhancement aided end-to-end multi-task model
for VAD. The model has two decoders, one for speech enhancement and the other
for VAD. The two decoders share the same encoder and speech separation network.
Unlike the direct thought that takes two separated objectives for VAD and
speech enhancement respectively, here we propose a new joint optimization
objective---VAD-masked scale-invariant source-to-noise ratio (mSI-SDR). mSI-SDR
uses VAD information to mask the output of the speech enhancement decoder in
the training process. It makes the VAD and speech enhancement tasks jointly
optimized not only at the shared encoder and separation network, but also at
the objective level. Experimental results show that the multi-task method
significantly outperforms its single-task VAD counterpart. Moreover, mSI-SDR
outperforms SI-SDR in the same multi-task setting. Finally, the model performs
well in real-time conditions.
</p>
<a href="http://arxiv.org/abs/2010.12484" target="_blank">arXiv:2010.12484</a> [<a href="http://arxiv.org/pdf/2010.12484" target="_blank">pdf</a>]

<h2>An Analysis of LIME for Text Data. (arXiv:2010.12487v1 [stat.ML])</h2>
<h3>Dina Mardaoui, Damien Garreau</h3>
<p>Text data are increasingly handled in an automated fashion by machine
learning algorithms. But the models handling these data are not always
well-understood due to their complexity and are more and more often referred to
as "black-boxes." Interpretability methods aim to explain how these models
operate. Among them, LIME has become one of the most popular in recent years.
However, it comes without theoretical guarantees: even for simple models, we
are not sure that LIME behaves accurately. In this paper, we provide a first
theoretical analysis of LIME for text data. As a consequence of our theoretical
findings, we show that LIME indeed provides meaningful explanations for simple
models, namely decision trees and linear models.
</p>
<a href="http://arxiv.org/abs/2010.12487" target="_blank">arXiv:2010.12487</a> [<a href="http://arxiv.org/pdf/2010.12487" target="_blank">pdf</a>]

<h2>CLOUD: Contrastive Learning of Unsupervised Dynamics. (arXiv:2010.12488v1 [cs.RO])</h2>
<h3>Jianren Wang, Yujie Lu, Hang Zhao</h3>
<p>Developing agents that can perform complex control tasks from high
dimensional observations such as pixels is challenging due to difficulties in
learning dynamics efficiently. In this work, we propose to learn forward and
inverse dynamics in a fully unsupervised manner via contrastive estimation.
Specifically, we train a forward dynamics model and an inverse dynamics model
in the feature space of states and actions with data collected from random
exploration. Unlike most existing deterministic models, our energy-based model
takes into account the stochastic nature of agent-environment interactions. We
demonstrate the efficacy of our approach across a variety of tasks including
goal-directed planning and imitation from observations. Project videos and code
are at https://jianrenw.github.io/cloud/.
</p>
<a href="http://arxiv.org/abs/2010.12488" target="_blank">arXiv:2010.12488</a> [<a href="http://arxiv.org/pdf/2010.12488" target="_blank">pdf</a>]

<h2>A Review of Deep Learning Methods for Irregularly Sampled Medical Time Series Data. (arXiv:2010.12493v1 [cs.LG])</h2>
<h3>Chenxi Sun, Hongda Shen, Moxian Song, Hongyan Li</h3>
<p>Irregularly sampled time series (ISTS) data has irregular temporal intervals
between observations and different sampling rates between sequences. ISTS
commonly appears in healthcare, economics, and geoscience. Especially in the
medical environment, the widely used Electronic Health Records (EHRs) have
abundant typical irregularly sampled medical time series (ISMTS) data.
Developing deep learning methods on EHRs data is critical for personalized
treatment, precise diagnosis and medical management. However, it is challenging
to directly use deep learning models for ISMTS data. On the one hand, ISMTS
data has the intra-series and inter-series relations. Both the local and global
structures should be considered. On the other hand, methods should consider the
trade-off between task accuracy and model complexity and remain generality and
interpretability. So far, many existing works have tried to solve the above
problems and have achieved good results. In this paper, we review these deep
learning methods from the perspectives of technology and task. Under the
technology-driven perspective, we summarize them into two categories - missing
data-based methods and raw data-based methods. Under the task-driven
perspective, we also summarize them into two categories - data
imputation-oriented and downstream task-oriented. For each of them, we point
out their advantages and disadvantages. Moreover, we implement some
representative methods and compare them on four medical datasets with two
tasks. Finally, we discuss the challenges and opportunities in this area.
</p>
<a href="http://arxiv.org/abs/2010.12493" target="_blank">arXiv:2010.12493</a> [<a href="http://arxiv.org/pdf/2010.12493" target="_blank">pdf</a>]

<h2>ResNet or DenseNet? Introducing Dense Shortcuts to ResNet. (arXiv:2010.12496v1 [cs.CV])</h2>
<h3>Chaoning Zhang, Philipp Benz, Dawit Mureja Argaw, Seokju Lee, Junsik Kim, Francois Rameau, Jean-Charles Bazin, In So Kweon</h3>
<p>ResNet or DenseNet? Nowadays, most deep learning based approaches are
implemented with seminal backbone networks, among them the two arguably most
famous ones are ResNet and DenseNet. Despite their competitive performance and
overwhelming popularity, inherent drawbacks exist for both of them. For ResNet,
the identity shortcut that stabilizes training also limits its representation
capacity, while DenseNet has a higher capacity with multi-layer feature
concatenation. However, the dense concatenation causes a new problem of
requiring high GPU memory and more training time. Partially due to this, it is
not a trivial choice between ResNet and DenseNet. This paper provides a unified
perspective of dense summation to analyze them, which facilitates a better
understanding of their core difference. We further propose dense weighted
normalized shortcuts as a solution to the dilemma between them. Our proposed
dense shortcut inherits the design philosophy of simple design in ResNet and
DenseNet. On several benchmark datasets, the experimental results show that the
proposed DSNet achieves significantly better results than ResNet, and achieves
comparable performance as DenseNet but requiring fewer computation resources.
</p>
<a href="http://arxiv.org/abs/2010.12496" target="_blank">arXiv:2010.12496</a> [<a href="http://arxiv.org/pdf/2010.12496" target="_blank">pdf</a>]

<h2>Few-shot Learning for Decoding Brain Signals. (arXiv:2010.12500v1 [cs.LG])</h2>
<h3>Myriam Bontonou, Nicolas Farrugia, Vincent Gripon</h3>
<p>Few-shot learning consists in addressing data-thrifty (inductive few-shot) or
label-thrifty (transductive few-shot) problems. So far, the field has been
mostly driven by applications in computer vision. In this work, we are
interested in stressing the ability of recently introduced few-shot methods to
solve problems dealing with neuroimaging data, a promising application field.
To this end, we propose a benchmark dataset and compare multiple learning
paradigms, including meta-learning, as well as various backbone networks. Our
experiments show that few-shot methods are able to efficiently decode brain
signals using few examples, and that graph-based backbones do not outperform
simple structure-agnostic solutions, such as multi-layer perceptrons.
</p>
<a href="http://arxiv.org/abs/2010.12500" target="_blank">arXiv:2010.12500</a> [<a href="http://arxiv.org/pdf/2010.12500" target="_blank">pdf</a>]

<h2>Beating the market with a bad predictive model. (arXiv:2010.12508v1 [cs.CE])</h2>
<h3>Ond&#x159;ej Hub&#xe1;&#x10d;ek, Gustav &#x160;&#xed;r</h3>
<p>It is a common misconception that in order to make consistent profits as a
trader, one needs to posses some extra information leading to an asset value
estimation more accurate than that reflected by the current market price. While
the idea makes intuitive sense and is also well substantiated by the widely
popular Kelly criterion, we prove that it is generally possible to make
systematic profits with a completely inferior price-predicting model. The key
idea is to alter the training objective of the predictive models to explicitly
decorrelate them from the market, enabling to exploit inconspicuous biases in
market maker's pricing, and profit on the inherent advantage of the market
taker. We introduce the problem setting throughout the diverse domains of stock
trading and sports betting to provide insights into the common underlying
properties of profitable predictive models, their connections to standard
portfolio optimization strategies, and the, commonly overlooked, advantage of
the market taker. Consequently, we prove desirability of the decorrelation
objective across common market distributions, translate the concept into a
practical machine learning setting, and demonstrate its viability with real
world market data.
</p>
<a href="http://arxiv.org/abs/2010.12508" target="_blank">arXiv:2010.12508</a> [<a href="http://arxiv.org/pdf/2010.12508" target="_blank">pdf</a>]

<h2>Generating Plausible Counterfactual Explanations for Deep Transformers in Financial Text Classification. (arXiv:2010.12512v1 [cs.CL])</h2>
<h3>Linyi Yang, Eoin M. Kenny, Tin Lok James Ng, Yi Yang, Barry Smyth, Ruihai Dong</h3>
<p>Corporate mergers and acquisitions (M&amp;A) account for billions of dollars of
investment globally every year, and offer an interesting and challenging domain
for artificial intelligence. However, in these highly sensitive domains, it is
crucial to not only have a highly robust and accurate model, but be able to
generate useful explanations to garner a user's trust in the automated system.
Regrettably, the recent research regarding eXplainable AI (XAI) in financial
text classification has received little to no attention, and many current
methods for generating textual-based explanations result in highly implausible
explanations, which damage a user's trust in the system. To address these
issues, this paper proposes a novel methodology for producing plausible
counterfactual explanations, whilst exploring the regularization benefits of
adversarial training on language models in the domain of FinTech. Exhaustive
quantitative experiments demonstrate that not only does this approach improve
the model accuracy when compared to the current state-of-the-art and human
performance, but it also generates counterfactual explanations which are
significantly more plausible based on human trials.
</p>
<a href="http://arxiv.org/abs/2010.12512" target="_blank">arXiv:2010.12512</a> [<a href="http://arxiv.org/pdf/2010.12512" target="_blank">pdf</a>]

<h2>TAMPC: A Controller for Escaping Traps in Novel Environments. (arXiv:2010.12516v1 [cs.RO])</h2>
<h3>Sheng Zhong (1), Zhenyuan Zhang (1), Nima Fazeli (1), Dmitry Berenson (1) ((1) Robotics Institute, University of Michigan)</h3>
<p>We propose an approach to online model adaptation and control in the
challenging case of hybrid and discontinuous dynamics where actions may lead to
difficult-to-escape "trap" states. We first learn dynamics for a given system
from training data which does not contain unexpected traps (since we do not
know what traps will be encountered online). These "nominal" dynamics allow us
to perform tasks under ideal conditions, but when unexpected traps arise in
execution, we must find a way to adapt our dynamics and control strategy and
continue attempting the task. Our approach, Trap-Aware Model Predictive Control
(TAMPC), is a two-level hierarchical control algorithm that reasons about traps
and non-nominal dynamics to decide between goal-seeking and recovery policies.
An important requirement of our method is the ability to recognize nominal
dynamics even when we encounter data that is out-of-distribution w.r.t the
training data. We achieve this by learning a representation for dynamics that
exploits invariance in the nominal environment, thus allowing better
generalization. We evaluate our method on simulated planar pushing and
peg-in-hole as well as real robot peg-in-hole problems against adaptive control
and reinforcement learning baselines, where traps arise due to unexpected
obstacles that we only observe through contact. Our results show that our
method significantly outperforms the baselines in all tested scenarios.
</p>
<a href="http://arxiv.org/abs/2010.12516" target="_blank">arXiv:2010.12516</a> [<a href="http://arxiv.org/pdf/2010.12516" target="_blank">pdf</a>]

<h2>Graph and graphon neural network stability. (arXiv:2010.12529v1 [cs.LG])</h2>
<h3>Luana Ruiz, Zhiyang Wang, Alejandro Ribeiro</h3>
<p>Graph neural networks (GNNs) are learning architectures that rely on
knowledge of the graph structure to generate meaningful representations of
large-scale network data. GNN stability is thus important as in real-world
scenarios there are typically uncertainties associated with the graph. We
analyze GNN stability using kernel objects called graphons. Graphons are both
limits of convergent graph sequences and generating models for deterministic
and stochastic graphs. Building upon the theory of graphon signal processing,
we define graphon neural networks and analyze their stability to graphon
perturbations. We then extend this analysis by interpreting the graphon neural
network as a generating model for GNNs on deterministic and stochastic graphs
instantiated from the original and perturbed graphons. We observe that GNNs are
stable to graphon perturbations with a stability bound that decreases
asymptotically with the size of the graph. This asymptotic behavior is further
demonstrated in an experiment of movie recommendation.
</p>
<a href="http://arxiv.org/abs/2010.12529" target="_blank">arXiv:2010.12529</a> [<a href="http://arxiv.org/pdf/2010.12529" target="_blank">pdf</a>]

<h2>Predicting Infectiousness for Proactive Contact Tracing. (arXiv:2010.12536v1 [cs.LG])</h2>
<h3>Yoshua Bengio, Prateek Gupta, Tegan Maharaj, Nasim Rahaman, Martin Weiss, Tristan Deleu, Eilif Muller, Meng Qu, Victor Schmidt, Pierre-Luc St-Charles, Hannah Alsdurf, Olexa Bilanuik, David Buckeridge, G&#xe1;etan Marceau Caron, Pierre-Luc Carrier, Joumana Ghosn, Satya Ortiz-Gagne, Chris Pal, Irina Rish, Bernhard Sch&#xf6;lkopf, Abhinav Sharma, Jian Tang, Andrew Williams</h3>
<p>The COVID-19 pandemic has spread rapidly worldwide, overwhelming manual
contact tracing in many countries and resulting in widespread lockdowns for
emergency containment. Large-scale digital contact tracing (DCT) has emerged as
a potential solution to resume economic and social activity while minimizing
spread of the virus. Various DCT methods have been proposed, each making
trade-offs between privacy, mobility restrictions, and public health. The most
common approach, binary contact tracing (BCT), models infection as a binary
event, informed only by an individual's test results, with corresponding binary
recommendations that either all or none of the individual's contacts
quarantine. BCT ignores the inherent uncertainty in contacts and the infection
process, which could be used to tailor messaging to high-risk individuals, and
prompt proactive testing or earlier warnings. It also does not make use of
observations such as symptoms or pre-existing medical conditions, which could
be used to make more accurate infectiousness predictions. In this paper, we use
a recently-proposed COVID-19 epidemiological simulator to develop and test
methods that can be deployed to a smartphone to locally and proactively predict
an individual's infectiousness (risk of infecting others) based on their
contact history and other information, while respecting strong privacy
constraints. Predictions are used to provide personalized recommendations to
the individual via an app, as well as to send anonymized messages to the
individual's contacts, who use this information to better predict their own
infectiousness, an approach we call proactive contact tracing (PCT). We find a
deep-learning based PCT method which improves over BCT for equivalent average
mobility, suggesting PCT could help in safe re-opening and second-wave
prevention.
</p>
<a href="http://arxiv.org/abs/2010.12536" target="_blank">arXiv:2010.12536</a> [<a href="http://arxiv.org/pdf/2010.12536" target="_blank">pdf</a>]

<h2>Comprehensive Empirical Evaluation of Deep Learning Approaches for Session-based Recommendation in E-Commerce. (arXiv:2010.12540v1 [cs.IR])</h2>
<h3>Mohamed Maher (1), Perseverance Munga Ngoy (1), Aleksandrs Rebriks (1), Cagri Ozcinar (1), Josue Cuevas (3), Rajasekhar Sanagavarapu (3), Gholamreza Anbarjafari (1 and 2) ((1) iCV Lab, University of Tartu, Tartu, Estonia, (2) Faculty of Engineering, Hasan Kalyoncu University, Gaziantep, Turkey, (3) Rakuten Inc., Big Data Department, Machine Learning Group, Tokyo, Japan)</h3>
<p>Boosting sales of e-commerce services is guaranteed once users find more
matching items to their interests in a short time. Consequently, recommendation
systems have become a crucial part of any successful e-commerce services.
Although various recommendation techniques could be used in e-commerce, a
considerable amount of attention has been drawn to session-based recommendation
systems during the recent few years. This growing interest is due to the
security concerns in collecting personalized user behavior data, especially
after the recent general data protection regulations. In this work, we present
a comprehensive evaluation of the state-of-the-art deep learning approaches
used in the session-based recommendation. In session-based recommendation, a
recommendation system counts on the sequence of events made by a user within
the same session to predict and endorse other items that are more likely to
correlate with his/her preferences. Our extensive experiments investigate
baseline techniques (\textit{e.g.,} nearest neighbors and pattern mining
algorithms) and deep learning approaches (\textit{e.g.,} recurrent neural
networks, graph neural networks, and attention-based networks). Our evaluations
show that advanced neural-based models and session-based nearest neighbor
algorithms outperform the baseline techniques in most of the scenarios.
However, we found that these models suffer more in case of long sessions when
there exists drift in user interests, and when there is no enough data to model
different items correctly during training. Our study suggests that using hybrid
models of different approaches combined with baseline algorithms could lead to
substantial results in session-based recommendations based on dataset
characteristics. We also discuss the drawbacks of current session-based
recommendation algorithms and further open research directions in this field.
</p>
<a href="http://arxiv.org/abs/2010.12540" target="_blank">arXiv:2010.12540</a> [<a href="http://arxiv.org/pdf/2010.12540" target="_blank">pdf</a>]

<h2>Multilingual BERT Post-Pretraining Alignment. (arXiv:2010.12547v1 [cs.CL])</h2>
<h3>Lin Pan, Chung-Wei Hang, Haode Qi, Abhishek Shah, Mo Yu, Saloni Potdar</h3>
<p>We propose a simple method to align multilingual contextual embeddings as a
post-pretraining step for improved zero-shot cross-lingual transferability of
the pretrained models. Using parallel data, our method aligns embeddings on the
word level through the recently proposed Translation Language Modeling
objective as well as on the sentence level via contrastive learning and random
input shuffling. We also perform code-switching with English when finetuning on
downstream tasks. On XNLI, our best model (initialized from mBERT) improves
over mBERT by 4.7% in the zero-shot setting and achieves comparable result to
XLM for translate-train while using less than 18% of the same parallel data and
31% less model parameters. On MLQA, our model outperforms XLM-R_Base that has
57% more parameters than ours.
</p>
<a href="http://arxiv.org/abs/2010.12547" target="_blank">arXiv:2010.12547</a> [<a href="http://arxiv.org/pdf/2010.12547" target="_blank">pdf</a>]

<h2>The Case for Distance-Bounded Spatial Approximations. (arXiv:2010.12548v1 [cs.DB])</h2>
<h3>Eleni Tzirita Zacharatou, Andreas Kipf, Ibrahim Sabek, Varun Pandey, Harish Doraiswamy, Volker Markl</h3>
<p>Spatial approximations have been traditionally used in spatial databases to
accelerate the processing of complex geometric operations. However,
approximations are typically only used in a first filtering step to determine a
set of candidate spatial objects that may fulfill the query condition. To
provide accurate results, the exact geometries of the candidate objects are
tested against the query condition, which is typically an expensive operation.
Nevertheless, many emerging applications (e.g., visualization tools) require
interactive responses, while only needing approximate results. Besides,
real-world geospatial data is inherently imprecise, which makes exact data
processing unnecessary. Given the uncertainty associated with spatial data and
the relaxed precision requirements of many applications, this vision paper
advocates for approximate spatial data processing techniques that omit exact
geometric tests and provide final answers solely on the basis of (fine-grained)
approximations. Thanks to recent hardware advances, this vision can be realized
today. Furthermore, our approximate techniques employ a distance-based error
bound, i.e., a bound on the maximum spatial distance between false (or missing)
and exact results which is crucial for meaningful analyses. This bound allows
to control the precision of the approximation and trade accuracy for
performance.
</p>
<a href="http://arxiv.org/abs/2010.12548" target="_blank">arXiv:2010.12548</a> [<a href="http://arxiv.org/pdf/2010.12548" target="_blank">pdf</a>]

<h2>High-Throughput Image-Based Plant Stand Count Estimation Using Convolutional Neural Networks. (arXiv:2010.12552v1 [cs.CV])</h2>
<h3>Saeed Khaki, Hieu Pham, Ye Han, Wade Kent, Lizhi Wang</h3>
<p>The future landscape of modern farming and plant breeding is rapidly changing
due to the complex needs of our society. The explosion of collectable data has
started a revolution in agriculture to the point where innovation must occur.
To a commercial organization, the accurate and efficient collection of
information is necessary to ensure that optimal decisions are made at key
points of the breeding cycle. However, due to the shear size of a breeding
program and current resource limitations, the ability to collect precise data
on individual plants is not possible. In particular, efficient phenotyping of
crops to record its color, shape, chemical properties, disease susceptibility,
etc. is severely limited due to labor requirements and, oftentimes, expert
domain knowledge. In this paper, we propose a deep learning based approach,
named DeepStand, for image-based corn stand counting at early phenological
stages. The proposed method adopts a truncated VGG-16 network as a backbone
feature extractor and merges multiple feature maps with different scales to
make the network robust against scale variation. Our extensive computational
experiments suggest that our proposed method can successfully count corn stands
and out-perform other state-of-the-art methods. It is the goal of our work to
be used by the larger agricultural community as a way to enable high-throughput
phenotyping without the use of extensive time and labor requirements.
</p>
<a href="http://arxiv.org/abs/2010.12552" target="_blank">arXiv:2010.12552</a> [<a href="http://arxiv.org/pdf/2010.12552" target="_blank">pdf</a>]

<h2>DICT-MLM: Improved Multilingual Pre-Training using Bilingual Dictionaries. (arXiv:2010.12566v1 [cs.CL])</h2>
<h3>Aditi Chaudhary, Karthik Raman, Krishna Srinivasan, Jiecao Chen</h3>
<p>Pre-trained multilingual language models such as mBERT have shown immense
gains for several natural language processing (NLP) tasks, especially in the
zero-shot cross-lingual setting. Most, if not all, of these pre-trained models
rely on the masked-language modeling (MLM) objective as the key language
learning objective. The principle behind these approaches is that predicting
the masked words with the help of the surrounding text helps learn potent
contextualized representations. Despite the strong representation learning
capability enabled by MLM, we demonstrate an inherent limitation of MLM for
multilingual representation learning. In particular, by requiring the model to
predict the language-specific token, the MLM objective disincentivizes learning
a language-agnostic representation -- which is a key goal of multilingual
pre-training. Therefore to encourage better cross-lingual representation
learning we propose the DICT-MLM method. DICT-MLM works by incentivizing the
model to be able to predict not just the original masked word, but potentially
any of its cross-lingual synonyms as well. Our empirical analysis on multiple
downstream tasks spanning 30+ languages, demonstrates the efficacy of the
proposed approach and its ability to learn better multilingual representations.
</p>
<a href="http://arxiv.org/abs/2010.12566" target="_blank">arXiv:2010.12566</a> [<a href="http://arxiv.org/pdf/2010.12566" target="_blank">pdf</a>]

<h2>Online Semi-Supervised Learning with Bandit Feedback. (arXiv:2010.12574v1 [cs.LG])</h2>
<h3>Sohini Upadhyay, Mikhail Yurochkin, Mayank Agarwal, Yasaman Khazaeni, DjallelBouneffouf</h3>
<p>We formulate a new problem at the intersectionof semi-supervised learning and
contextual bandits,motivated by several applications including clini-cal trials
and ad recommendations. We demonstratehow Graph Convolutional Network (GCN), a
semi-supervised learning approach, can be adjusted tothe new problem
formulation. We also propose avariant of the linear contextual bandit with
semi-supervised missing rewards imputation. We thentake the best of both
approaches to develop multi-GCN embedded contextual bandit. Our algorithmsare
verified on several real world datasets.
</p>
<a href="http://arxiv.org/abs/2010.12574" target="_blank">arXiv:2010.12574</a> [<a href="http://arxiv.org/pdf/2010.12574" target="_blank">pdf</a>]

<h2>A modified Bayesian Convolutional Neural Network for Breast Histopathology Image Classification and Uncertainty Quantification. (arXiv:2010.12575v1 [cs.CV])</h2>
<h3>Pushkar Khairnar, Ponkrshnan Thiagarajan, Susanta Ghosh</h3>
<p>Convolutional neural network (CNN) based classification models have been
successfully used on histopathological images for the detection of diseases.
Despite its success, CNN may yield erroneous or overfitted results when the
data is not sufficiently large or is biased. To overcome these limitations of
CNN and to provide uncertainty quantification Bayesian CNN is recently
proposed. However, we show that Bayesian-CNN still suffers from inaccuracies,
especially in negative predictions. In the present work, we extend the
Bayesian-CNN to improve accuracy and the rate of convergence. The proposed
model is called modified Bayesian-CNN. The novelty of the proposed model lies
in an adaptive activation function that contains a learnable parameter for each
of the neurons. This adaptive activation function dynamically changes the loss
function thereby providing faster convergence and better accuracy. The
uncertainties associated with the predictions are obtained since the model
learns a probability distribution on the network parameters. It reduces
overfitting through an ensemble averaging over networks, which in turn improves
accuracy on the unknown data. The proposed model demonstrates significant
improvement by nearly eliminating overfitting and remarkably reducing (about
38%) the number of false-negative predictions. We found that the proposed model
predicts higher uncertainty for images having features of both the classes. The
uncertainty in the predictions of individual images can be used to decide when
further human-expert intervention is needed. These findings have the potential
to advance the state-of-the-art machine learning based automatic classification
for histopathological images.
</p>
<a href="http://arxiv.org/abs/2010.12575" target="_blank">arXiv:2010.12575</a> [<a href="http://arxiv.org/pdf/2010.12575" target="_blank">pdf</a>]

<h2>Generalisation in humans and deep neural networks. (arXiv:1808.08750v3 [cs.CV] UPDATED)</h2>
<h3>Robert Geirhos, Carlos R. Medina Temme, Jonas Rauber, Heiko H. Sch&#xfc;tt, Matthias Bethge, Felix A. Wichmann</h3>
<p>We compare the robustness of humans and current convolutional deep neural
networks (DNNs) on object recognition under twelve different types of image
degradations. First, using three well known DNNs (ResNet-152, VGG-19,
GoogLeNet) we find the human visual system to be more robust to nearly all of
the tested image manipulations, and we observe progressively diverging
classification error-patterns between humans and DNNs when the signal gets
weaker. Secondly, we show that DNNs trained directly on distorted images
consistently surpass human performance on the exact distortion types they were
trained on, yet they display extremely poor generalisation abilities when
tested on other distortion types. For example, training on salt-and-pepper
noise does not imply robustness on uniform white noise and vice versa. Thus,
changes in the noise distribution between training and testing constitutes a
crucial challenge to deep learning vision systems that can be systematically
addressed in a lifelong machine learning approach. Our new dataset consisting
of 83K carefully measured human psychophysical trials provide a useful
reference for lifelong robustness against image degradations set by the human
visual system.
</p>
<a href="http://arxiv.org/abs/1808.08750" target="_blank">arXiv:1808.08750</a> [<a href="http://arxiv.org/pdf/1808.08750" target="_blank">pdf</a>]

<h2>Greedy Algorithms for Sparse Sensor Placement via Deep Learning. (arXiv:1809.06025v5 [cs.LG] UPDATED)</h2>
<h3>Louis Ly, Yen-Hsi Richard Tsai</h3>
<p>We consider the exploration problem: an agent equipped with a depth sensor
must map out a previously unknown environment using as few sensor measurements
as possible. We propose an approach based on supervised learning of a greedy
algorithm. We provide a bound on the optimality of the greedy algorithm using
submodularity theory. Using a level set representation, we train a
convolutional neural network to determine vantage points that maximize
visibility. We show that this method drastically reduces the on-line
computational cost and determines a small set of vantage points that solve the
problem. This enables us to efficiently produce highly-resolved and
topologically accurate maps of complex 3D environments. Unlike traditional
next-best-view and frontier-based strategies, the proposed method accounts for
geometric priors while evaluating potential vantage points. While existing deep
learning approaches focus on obstacle avoidance and local navigation, our
method aims at finding near-optimal solutions to the more global exploration
problem. We present realistic simulations on 2D and 3D urban environments.
</p>
<a href="http://arxiv.org/abs/1809.06025" target="_blank">arXiv:1809.06025</a> [<a href="http://arxiv.org/pdf/1809.06025" target="_blank">pdf</a>]

<h2>End-to-End Classification of Reverberant Rooms using DNNs. (arXiv:1812.09324v5 [eess.AS] UPDATED)</h2>
<h3>Constantinos Papayiannis, Christine Evers, Patrick A. Naylor</h3>
<p>Reverberation is present in our workplaces, our homes, concert halls and
theatres. This paper investigates how deep learning can use the effect of
reverberation on speech to classify a recording in terms of the room in which
it was recorded. Existing approaches in the literature rely on domain expertise
to manually select acoustic parameters as inputs to classifiers. Estimation of
these parameters from reverberant speech is adversely affected by estimation
errors, impacting the classification accuracy. In order to overcome the
limitations of previously proposed methods, this paper shows how DNNs can
perform the classification by operating directly on reverberant speech spectra
and a CRNN with an attention-mechanism is proposed for the task. The
relationship is investigated between the reverberant speech representations
learned by the DNNs and acoustic parameters. For evaluation, AIRs are used from
the ACE-challenge dataset that were measured in 7 real rooms. The
classification accuracy of the CRNN classifier in the experiments is 78% when
using 5 hours of training data and 90% when using 10 hours.
</p>
<a href="http://arxiv.org/abs/1812.09324" target="_blank">arXiv:1812.09324</a> [<a href="http://arxiv.org/pdf/1812.09324" target="_blank">pdf</a>]

<h2>Sequential Adaptive Design for Jump Regression Estimation in Materials Discovery. (arXiv:1904.01648v3 [stat.ML] UPDATED)</h2>
<h3>Chiwoo Park, Peihua Qiu, Jennifer Carpena-N&#xfa;&#xf1;ez, Rahul Rao, Michael Susner, Benji Maruyama</h3>
<p>Selecting input variables or design points for statistical models has been of
great interest in adaptive design and active learning. Motivated by two
scientific examples, this paper presents a strategy of selecting the design
points for a regression model when the underlying regression function is
discontinuous. The first example we undertook was for the purpose of
accelerating imaging speed in a high resolution material imaging; the second
was use of sequential design for the purpose of mapping a chemical phase
diagram. In both examples, the underlying regression functions have
discontinuities, so many of the existing design optimization approaches cannot
be applied because they mostly assume a continuous regression function.
Although some existing adaptive design strategies developed from treed
regression models can handle the discontinuities, the Bayesian approaches come
with computationally expensive Markov Chain Monte Carlo techniques for
posterior inferences and subsequent design point selections, which is not
appropriate for the first motivating example that requires computation at least
faster than the original imaging speed. In addition, the treed models are based
on the domain partitioning that are inefficient when the discontinuities occurs
over complex sub-domain boundaries. We propose a simple and effective adaptive
design strategy for a regression analysis with discontinuities: some
statistical properties with a fixed design will be presented first, and then
these properties will be used to propose a new criterion of selecting the
design points for the regression analysis. Sequential design with the new
criterion will be presented with comprehensive simulated examples, and its
application to the two motivating examples will be presented.
</p>
<a href="http://arxiv.org/abs/1904.01648" target="_blank">arXiv:1904.01648</a> [<a href="http://arxiv.org/pdf/1904.01648" target="_blank">pdf</a>]

<h2>Class specific or shared? A cascaded dictionary learning framework for image classification. (arXiv:1904.08928v2 [cs.CV] UPDATED)</h2>
<h3>Yan-Jiang Wang, Shuai Shao, Rui Xu, Werifeng Liu, Bao-Di Liu</h3>
<p>Dictionary learning methods can be split into: i) class specific dictionary
learning ii) class shared dictionary learning. The difference between the two
categories is how to use discriminative information. With the first category,
samples of different classes are mapped into different subspaces, which leads
to some redundancy with the class specific base vectors. While for the second
category, the samples in each specific class can not be described accurately.
In this paper, we first propose a novel class shared dictionary learning method
named label embedded dictionary learning (LEDL). It is the improvement based on
LCKSVD, which is easier to find out the optimal solution. Then we propose a
novel framework named cascaded dictionary learning framework (CDLF) to combine
the specific dictionary learning with shared dictionary learning to describe
the feature to boost the performance of classification sufficiently. Extensive
experimental results on six benchmark datasets illustrate that our methods are
capable of achieving superior performance compared to several state-of-art
classification algorithms.
</p>
<a href="http://arxiv.org/abs/1904.08928" target="_blank">arXiv:1904.08928</a> [<a href="http://arxiv.org/pdf/1904.08928" target="_blank">pdf</a>]

<h2>Explicit approximation of stochastic optimal feedback control for combined therapy of cancer. (arXiv:1905.04937v2 [cs.SY] UPDATED)</h2>
<h3>Mazen Alamir</h3>
<p>In this paper, a tractable methodology is proposed to approximate stochastic
optimal feedback treatment in the context of mixed immuno-chemo therapy of
cancer. The method uses a fixed-point value iteration that approximately solves
a stochastic dynamic programming-like equation. It is in particular shown that
the introduction of a variance-related penalty in the latter induces better
results that cope with the consequences of softening the health safety
constraints in the cost function. The convergence of the value function
iteration is revisited in the presence of the variance related term. The
implementation involves some Machine Learning tools in order to represent the
optimal function and to perform complexity reduction by clustering.
Quantitative illustration is given using a commonly used model of combined
therapy involving twelve highly uncertain parameters.
</p>
<a href="http://arxiv.org/abs/1905.04937" target="_blank">arXiv:1905.04937</a> [<a href="http://arxiv.org/pdf/1905.04937" target="_blank">pdf</a>]

<h2>Adversarial Training is a Form of Data-dependent Operator Norm Regularization. (arXiv:1906.01527v5 [cs.LG] UPDATED)</h2>
<h3>Kevin Roth, Yannic Kilcher, Thomas Hofmann</h3>
<p>We establish a theoretical link between adversarial training and operator
norm regularization for deep neural networks. Specifically, we prove that
$\ell_p$-norm constrained projected gradient ascent based adversarial training
with an $\ell_q$-norm loss on the logits of clean and perturbed inputs is
equivalent to data-dependent (p, q) operator norm regularization. This
fundamental connection confirms the long-standing argument that a network's
sensitivity to adversarial examples is tied to its spectral properties and
hints at novel ways to robustify and defend against adversarial attacks. We
provide extensive empirical evidence on state-of-the-art network architectures
to support our theoretical results.
</p>
<a href="http://arxiv.org/abs/1906.01527" target="_blank">arXiv:1906.01527</a> [<a href="http://arxiv.org/pdf/1906.01527" target="_blank">pdf</a>]

<h2>Direct Policy Gradients: Direct Optimization of Policies in Discrete Action Spaces. (arXiv:1906.06062v2 [cs.LG] UPDATED)</h2>
<h3>Guy Lorberbom, Chris J. Maddison, Nicolas Heess, Tamir Hazan, Daniel Tarlow</h3>
<p>Direct optimization is an appealing framework that replaces integration with
optimization of a random objective for approximating gradients in models with
discrete random variables. A$^\star$ sampling is a framework for optimizing
such random objectives over large spaces. We show how to combine these
techniques to yield a reinforcement learning algorithm that approximates a
policy gradient by finding trajectories that optimize a random objective. We
call the resulting algorithms "direct policy gradient" (DirPG) algorithms. A
main benefit of DirPG algorithms is that they allow the insertion of domain
knowledge in the form of upper bounds on return-to-go at training time, like is
used in heuristic search, while still directly computing a policy gradient. We
further analyze their properties, showing there are cases where DirPG has an
exponentially larger probability of sampling informative gradients compared to
REINFORCE. We also show that there is a built-in variance reduction technique
and that a parameter that was previously viewed as a numerical approximation
can be interpreted as controlling risk sensitivity. Empirically, we evaluate
the effect of key degrees of freedom and show that the algorithm performs well
in illustrative domains compared to baselines.
</p>
<a href="http://arxiv.org/abs/1906.06062" target="_blank">arXiv:1906.06062</a> [<a href="http://arxiv.org/pdf/1906.06062" target="_blank">pdf</a>]

<h2>Exploration Through Reward Biasing: Reward-Biased Maximum Likelihood Estimation for Stochastic Multi-Armed Bandits. (arXiv:1907.01287v3 [cs.LG] UPDATED)</h2>
<h3>Xi Liu, Ping-Chun Hsieh, Anirban Bhattacharya, P. R. Kumar</h3>
<p>Inspired by the Reward-Biased Maximum Likelihood Estimate method of adaptive
control, we propose RBMLE -- a novel family of learning algorithms for
stochastic multi-armed bandits (SMABs). For a broad range of SMABs including
both the parametric Exponential Family as well as the non-parametric
sub-Gaussian/Exponential family, we show that RBMLE yields an index policy. To
choose the bias-growth rate $\alpha(t)$ in RBMLE, we reveal the nontrivial
interplay between $\alpha(t)$ and the regret bound that generally applies in
both the Exponential Family as well as the sub-Gaussian/Exponential family
bandits. To quantify the finite-time performance, we prove that RBMLE attains
order-optimality by adaptively estimating the unknown constants in the
expression of $\alpha(t)$ for Gaussian and sub-Gaussian bandits. Extensive
experiments demonstrate that the proposed RBMLE achieves empirical regret
performance competitive with the state-of-the-art methods, while being more
computationally efficient and scalable in comparison to the best-performing
ones among them.
</p>
<a href="http://arxiv.org/abs/1907.01287" target="_blank">arXiv:1907.01287</a> [<a href="http://arxiv.org/pdf/1907.01287" target="_blank">pdf</a>]

<h2>ACTNET: end-to-end learning of feature activations and multi-stream aggregation for effective instance image retrieval. (arXiv:1907.05794v3 [cs.CV] UPDATED)</h2>
<h3>Syed Sameed Husain, Eng-Jon Ong, Miroslaw Bober</h3>
<p>We propose a novel CNN architecture called ACTNET for robust instance image
retrieval from large-scale datasets. Our key innovation is a learnable
activation layer designed to improve the signal-to-noise ratio (SNR) of deep
convolutional feature maps. Further, we introduce a controlled multi-stream
aggregation, where complementary deep features from different convolutional
layers are optimally transformed and balanced using our novel activation
layers, before aggregation into a global descriptor. Importantly, the learnable
parameters of our activation blocks are explicitly trained, together with the
CNN parameters, in an end-to-end manner minimising triplet loss. This means
that our network jointly learns the CNN filters and their optimal activation
and aggregation for retrieval tasks. To our knowledge, this is the first time
parametric functions have been used to control and learn optimal aggregation.
We conduct an in-depth experimental study on three non-linear activation
functions: Sine-Hyperbolic, Exponential and modified Weibull, showing that
while all bring significant gains the Weibull function performs best thanks to
its ability to equalise strong activations. The results clearly demonstrate
that our ACTNET architecture significantly enhances the discriminative power of
deep features, improving significantly over the state-of-the-art retrieval
results on all datasets.
</p>
<a href="http://arxiv.org/abs/1907.05794" target="_blank">arXiv:1907.05794</a> [<a href="http://arxiv.org/pdf/1907.05794" target="_blank">pdf</a>]

<h2>Proof-of-Useful-Work as Dual-Purpose Mechanism for Blockchain and AI: Blockchain Consensus that Enables Privacy Preserving Data Mining. (arXiv:1907.08744v3 [cs.CR] UPDATED)</h2>
<h3>Hjalmar Turesson, Henry M. Kim, Marek Laskowski, Alexandra Roatis</h3>
<p>Blockchains rely on a consensus among participants to achieve
decentralization and security. However, reaching consensus in an online,
digital world where identities are not tied to physical users is a challenging
problem. Proof-of-work provides a solution by linking representation to a
valuable, physical resource. While this has worked well, it uses a tremendous
amount of specialized hardware and energy, with no utility beyond blockchain
security. Here, we propose an alternative consensus scheme that directs the
computational resources to the optimization of machine learning (ML) models, a
task with more general utility. This is achieved by a hybrid consensus scheme
relying on three parties: data providers, miners, and a committee. The data
provider makes data available and provides payment in return for the best
model, miners compete about the payment and access to the committee by
producing ML optimized models, and the committee controls the ML competition.
</p>
<a href="http://arxiv.org/abs/1907.08744" target="_blank">arXiv:1907.08744</a> [<a href="http://arxiv.org/pdf/1907.08744" target="_blank">pdf</a>]

<h2>Quantum adiabatic machine learning with zooming. (arXiv:1908.04480v2 [quant-ph] UPDATED)</h2>
<h3>Alexander Zlokapa, Alex Mott, Joshua Job, Jean-Roch Vlimant, Daniel Lidar, Maria Spiropulu</h3>
<p>Recent work has shown that quantum annealing for machine learning, referred
to as QAML, can perform comparably to state-of-the-art machine learning methods
with a specific application to Higgs boson classification. We propose QAML-Z, a
novel algorithm that iteratively zooms in on a region of the energy surface by
mapping the problem to a continuous space and sequentially applying quantum
annealing to an augmented set of weak classifiers. Results on a programmable
quantum annealer show that QAML-Z matches classical deep neural network
performance at small training set sizes and reduces the performance margin
between QAML and classical deep neural networks by almost 50% at large training
set sizes, as measured by area under the ROC curve. The significant improvement
of quantum annealing algorithms for machine learning and the use of a discrete
quantum algorithm on a continuous optimization problem both opens a new class
of problems that can be solved by quantum annealers and suggests the approach
in performance of near-term quantum machine learning towards classical
benchmarks.
</p>
<a href="http://arxiv.org/abs/1908.04480" target="_blank">arXiv:1908.04480</a> [<a href="http://arxiv.org/pdf/1908.04480" target="_blank">pdf</a>]

<h2>Learning Sub-Sampling and Signal Recovery with Applications in Ultrasound Imaging. (arXiv:1908.05764v5 [eess.IV] UPDATED)</h2>
<h3>Iris A.M. Huijben, Bastiaan S. Veeling, Kees Janse, Massimo Mischi, Ruud J.G. van Sloun</h3>
<p>Limitations on bandwidth and power consumption impose strict bounds on data
rates of diagnostic imaging systems. Consequently, the design of suitable (i.e.
task- and data-aware) compression and reconstruction techniques has attracted
considerable attention in recent years. Compressed sensing emerged as a popular
framework for sparse signal reconstruction from a small set of compressed
measurements. However, typical compressed sensing designs measure a
(non)linearly weighted combination of all input signal elements, which poses
practical challenges. These designs are also not necessarily task-optimal. In
addition, real-time recovery is hampered by the iterative and time-consuming
nature of sparse recovery algorithms. Recently, deep learning methods have
shown promise for fast recovery from compressed measurements, but the design of
adequate and practical sensing strategies remains a challenge. Here, we propose
a deep learning solution termed Deep Probabilistic Sub-sampling (DPS), that
learns a task-driven sub-sampling pattern, while jointly training a subsequent
task model. Once learned, the task-based sub-sampling patterns are fixed and
straightforwardly implementable, e.g. by non-uniform analog-to-digital
conversion, sparse array design, or slow-time ultrasound pulsing schemes. The
effectiveness of our framework is demonstrated in-silico for sparse signal
recovery from partial Fourier measurements, and in-vivo for both anatomical
image and tissue-motion (Doppler) reconstruction from sub-sampled medical
ultrasound imaging data.
</p>
<a href="http://arxiv.org/abs/1908.05764" target="_blank">arXiv:1908.05764</a> [<a href="http://arxiv.org/pdf/1908.05764" target="_blank">pdf</a>]

<h2>Adversarial shape perturbations on 3D point clouds. (arXiv:1908.06062v3 [cs.CV] UPDATED)</h2>
<h3>Daniel Liu, Ronald Yu, Hao Su</h3>
<p>The importance of training robust neural network grows as 3D data is
increasingly utilized in deep learning for vision tasks in robotics, drone
control, and autonomous driving. One commonly used 3D data type is 3D point
clouds, which describe shape information. We examine the problem of creating
robust models from the perspective of the attacker, which is necessary in
understanding how 3D neural networks can be exploited. We explore two
categories of attacks: distributional attacks that involve imperceptible
perturbations to the distribution of points, and shape attacks that involve
deforming the shape represented by a point cloud. We explore three possible
shape attacks for attacking 3D point cloud classification and show that some of
them are able to be effective even against preprocessing steps, like the
previously proposed point-removal defenses.
</p>
<a href="http://arxiv.org/abs/1908.06062" target="_blank">arXiv:1908.06062</a> [<a href="http://arxiv.org/pdf/1908.06062" target="_blank">pdf</a>]

<h2>NL-LinkNet: Toward Lighter but More AccurateRoad Extraction with Non-Local Operations. (arXiv:1908.08223v2 [cs.LG] UPDATED)</h2>
<h3>Yooseung Wang, Junghoon Seo, Taegyun Jeon</h3>
<p>Road extraction from very high resolution satellite (VHR) images is one of
the most important topics in the field of remote sensing. In this paper, we
propose an efficient Non-Local LinkNet with non-local blocks that can grasp
relations between global features. This enables each spatial feature point to
refer to all other contextual information and results in more accurate road
segmentation. In detail, our single model without any post-processing like CRF
refinement, performed better than any other published state-of-the-art ensemble
model in the official DeepGlobe Challenge. Moreover, our NL-LinkNet beat the
D-LinkNet, the winner of the DeepGlobe challenge \cite{demir2018deepglobe},
with 43 \% less parameters, less giga floating-point operations per seconds
(GFLOPs) and shorter training convergence time. We also present empirical
analyses on the proper usages of non-local blocks for the baseline model.
</p>
<a href="http://arxiv.org/abs/1908.08223" target="_blank">arXiv:1908.08223</a> [<a href="http://arxiv.org/pdf/1908.08223" target="_blank">pdf</a>]

<h2>On the Expressiveness of Approximate Inference in Bayesian Neural Networks. (arXiv:1909.00719v4 [stat.ML] UPDATED)</h2>
<h3>Andrew Y. K. Foong, David R. Burt, Yingzhen Li, Richard E. Turner</h3>
<p>While Bayesian neural networks (BNNs) hold the promise of being flexible,
well-calibrated statistical models, inference often requires approximations
whose consequences are poorly understood. We study the quality of common
variational methods in approximating the Bayesian predictive distribution. For
single-hidden layer ReLU BNNs, we prove a fundamental limitation in
function-space of two of the most commonly used distributions defined in
weight-space: mean-field Gaussian and Monte Carlo dropout. We find there are
simple cases where neither method can have substantially increased uncertainty
in between well-separated regions of low uncertainty. We provide strong
empirical evidence that exact inference does not have this pathology, hence it
is due to the approximation and not the model. In contrast, for deep networks,
we prove a universality result showing that there exist approximate posteriors
in the above classes which provide flexible uncertainty estimates. However, we
find empirically that pathologies of a similar form as in the single-hidden
layer case can persist when performing variational inference in deeper
networks. Our results motivate careful consideration of the implications of
approximate inference methods in BNNs.
</p>
<a href="http://arxiv.org/abs/1909.00719" target="_blank">arXiv:1909.00719</a> [<a href="http://arxiv.org/pdf/1909.00719" target="_blank">pdf</a>]

<h2>Universal Graph Transformer Self-Attention Networks. (arXiv:1909.11855v9 [cs.LG] UPDATED)</h2>
<h3>Dai Quoc Nguyen, Tu Dinh Nguyen, Dinh Phung</h3>
<p>The transformer self-attention network has been extensively used in research
domains such as computer vision, image processing, and natural language
processing. But it has not been actively used in graph neural networks (GNNs)
where constructing an advanced aggregation function is essential. To this end,
we present U2GNN, an effective GNN model leveraging a transformer
self-attention mechanism followed by a recurrent transition, to induce a
powerful aggregation function to learn graph representations. Experimental
results show that the proposed U2GNN achieves state-of-the-art accuracies on
well-known benchmark datasets for graph classification. Our code is available
at: https://github.com/daiquocnguyen/Graph-Transformer
</p>
<a href="http://arxiv.org/abs/1909.11855" target="_blank">arXiv:1909.11855</a> [<a href="http://arxiv.org/pdf/1909.11855" target="_blank">pdf</a>]

<h2>Model Pruning Enables Efficient Federated Learning on Edge Devices. (arXiv:1909.12326v4 [cs.LG] UPDATED)</h2>
<h3>Yuang Jiang, Shiqiang Wang, Victor Valls, Bong Jun Ko, Wei-Han Lee, Kin K. Leung, Leandros Tassiulas</h3>
<p>Federated learning (FL) allows model training from local data collected by
edge/mobile devices, while preserving data privacy. A challenge is that client
devices in FL usually have much more limited computation and communication
resources compared to servers in a datacenter. To overcome this challenge, we
propose PruneFL -- a novel FL approach with adaptive and distributed parameter
pruning, which adapts the model size during FL to reduce both communication and
computation overhead and minimize the overall training time, while maintaining
a similar accuracy as the original model. PruneFL includes initial pruning at a
selected client and further pruning as part of the FL process. The model size
is adapted during this process, which includes maximizing the approximate
empirical risk reduction divided by the time of one FL round. Our experiments
with various datasets on edge devices (e.g., Raspberry Pi) show that: (i) we
significantly reduce the training time compared to conventional FL and various
other pruning-based methods; (ii) the pruned model converges to an accuracy
that is very similar to the original model but has a much smaller size, and it
is also a lottery ticket of the original model.
</p>
<a href="http://arxiv.org/abs/1909.12326" target="_blank">arXiv:1909.12326</a> [<a href="http://arxiv.org/pdf/1909.12326" target="_blank">pdf</a>]

<h2>Self-attention for raw optical Satellite Time Series Classification. (arXiv:1910.10536v3 [cs.LG] UPDATED)</h2>
<h3>Marc Ru&#xdf;wurm, Marco K&#xf6;rner</h3>
<p>The amount of available Earth observation data has increased dramatically in
the recent years. Efficiently making use of the entire body information is a
current challenge in remote sensing and demands for light-weight
problem-agnostic models that do not require region- or problem-specific expert
knowledge. End-to-end trained deep learning models can make use of raw sensory
data by learning feature extraction and classification in one step solely from
data. Still, many methods proposed in remote sensing research require implicit
feature extraction through data preprocessing or explicit design of features.

In this work, we compare recent deep learning models on crop type
classification on raw and preprocessed Sentinel 2 data. We concentrate on the
common neural network architectures for time series, i.e., 1D-convolutions,
recurrence, a shallow random forest baseline, and focus on the novel
self-attention architecture. Our central findings are that data preprocessing
still increased the overall classification performance for all models while the
choice of model was less crucial. Self-attention and recurrent neural networks,
by their architecture, outperformed convolutional neural networks on raw
satellite time series. We explore this by a feature importance analysis based
on gradient back-propagation that exploits the differentiable nature of deep
learning models. Further, we qualitatively show how self-attention scores focus
selectively on few classification-relevant observations.
</p>
<a href="http://arxiv.org/abs/1910.10536" target="_blank">arXiv:1910.10536</a> [<a href="http://arxiv.org/pdf/1910.10536" target="_blank">pdf</a>]

<h2>RIGA: Covert and Robust White-Box Watermarking of Deep Neural Networks. (arXiv:1910.14268v3 [cs.CR] UPDATED)</h2>
<h3>Tianhao Wang, Florian Kerschbaum</h3>
<p>Watermarking of deep neural networks (DNN) can enable their tracing once
released by a data owner. In this paper, we generalize white-box watermarking
algorithms for DNNs, where the data owner needs white-box access to the model
to extract the watermark. White-box watermarking algorithms have the advantage
that they do not impact the accuracy of the watermarked model. We propose
Robust whIte-box GAn watermarking (RIGA), a novel white-box watermarking
algorithm that uses adversarial training. Our extensive experiments demonstrate
that the proposed watermarking algorithm not only does not impact accuracy, but
also significantly improves the covertness and robustness over the current
state-of-art.
</p>
<a href="http://arxiv.org/abs/1910.14268" target="_blank">arXiv:1910.14268</a> [<a href="http://arxiv.org/pdf/1910.14268" target="_blank">pdf</a>]

<h2>ImmuNeCS: Neural Committee Search by an Artificial Immune System. (arXiv:1911.07729v4 [cs.NE] UPDATED)</h2>
<h3>Luc Frachon, Wei Pang, George M. Coghill</h3>
<p>Current Neural Architecture Search techniques can suffer from a few
shortcomings, including high computational cost, excessive bias from the search
space, conceptual complexity or uncertain empirical benefits over random
search. In this paper, we present ImmuNeCS, an attempt at addressing these
issues with a method that offers a simple, flexible, and efficient way of
building deep learning models automatically, and we demonstrate its
effectiveness in the context of convolutional neural networks. Instead of
searching for the 1-best architecture for a given task, we focus on building a
population of neural networks that are then ensembled into a neural network
committee, an approach we dub 'Neural Committee Search'. To ensure sufficient
performance from the committee, our search algorithm is based on an artificial
immune system that balances individual performance with population diversity.
This allows us to stop the search when accuracy starts to plateau, and to
bridge the performance gap through ensembling. In order to justify our method,
we first verify that the chosen search space exhibits the locality property. To
further improve efficiency, we also combine partial evaluation, weight
inheritance, and progressive search. First, experiments are run to verify the
validity of these techniques. Then, preliminary experimental results on two
popular computer vision benchmarks show that our method consistently
outperforms random search and yields promising results within reasonable GPU
budgets. An additional experiment also shows that ImmuNeCS's solutions transfer
effectively to a more difficult task, where they achieve results comparable to
a direct search on the new task. We believe these findings can open the way for
new, accessible alternatives to traditional NAS.
</p>
<a href="http://arxiv.org/abs/1911.07729" target="_blank">arXiv:1911.07729</a> [<a href="http://arxiv.org/pdf/1911.07729" target="_blank">pdf</a>]

<h2>CURL: Neural Curve Layers for Global Image Enhancement. (arXiv:1911.13175v4 [eess.IV] UPDATED)</h2>
<h3>Sean Moran, Steven McDonagh, Gregory Slabaugh</h3>
<p>We present a novel approach to adjust global image properties such as colour,
saturation, and luminance using human-interpretable image enhancement curves,
inspired by the Photoshop curves tool. Our method, dubbed neural CURve Layers
(CURL), is designed as a multi-colour space neural retouching block trained
jointly in three different colour spaces (HSV, CIELab, RGB) guided by a novel
multi-colour space loss. The curves are fully differentiable and are trained
end-to-end for different computer vision problems including photo enhancement
(RGB-to-RGB) and as part of the image signal processing pipeline for image
formation (RAW-to-RGB). To demonstrate the effectiveness of CURL we combine
this global image transformation block with a pixel-level (local) image
multi-scale encoder-decoder backbone network. In an extensive experimental
evaluation we show that CURL produces state-of-the-art image quality versus
recently proposed deep learning approaches in both objective and perceptual
metrics, setting new state-of-the-art performance on multiple public datasets.
Our code is publicly available at: https://github.com/sjmoran/CURL.
</p>
<a href="http://arxiv.org/abs/1911.13175" target="_blank">arXiv:1911.13175</a> [<a href="http://arxiv.org/pdf/1911.13175" target="_blank">pdf</a>]

<h2>View-Invariant Probabilistic Embedding for Human Pose. (arXiv:1912.01001v4 [cs.CV] UPDATED)</h2>
<h3>Jennifer J. Sun, Jiaping Zhao, Liang-Chieh Chen, Florian Schroff, Hartwig Adam, Ting Liu</h3>
<p>Depictions of similar human body configurations can vary with changing
viewpoints. Using only 2D information, we would like to enable vision
algorithms to recognize similarity in human body poses across multiple views.
This ability is useful for analyzing body movements and human behaviors in
images and videos. In this paper, we propose an approach for learning a compact
view-invariant embedding space from 2D joint keypoints alone, without
explicitly predicting 3D poses. Since 2D poses are projected from 3D space,
they have an inherent ambiguity, which is difficult to represent through a
deterministic mapping. Hence, we use probabilistic embeddings to model this
input uncertainty. Experimental results show that our embedding model achieves
higher accuracy when retrieving similar poses across different camera views, in
comparison with 2D-to-3D pose lifting models. We also demonstrate the
effectiveness of applying our embeddings to view-invariant action recognition
and video alignment. Our code is available at
https://github.com/google-research/google-research/tree/master/poem.
</p>
<a href="http://arxiv.org/abs/1912.01001" target="_blank">arXiv:1912.01001</a> [<a href="http://arxiv.org/pdf/1912.01001" target="_blank">pdf</a>]

<h2>Gradient Surgery for Multi-Task Learning. (arXiv:2001.06782v3 [cs.LG] UPDATED)</h2>
<h3>Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, Chelsea Finn</h3>
<p>While deep learning and deep reinforcement learning (RL) systems have
demonstrated impressive results in domains such as image classification, game
playing, and robotic control, data efficiency remains a major challenge.
Multi-task learning has emerged as a promising approach for sharing structure
across multiple tasks to enable more efficient learning. However, the
multi-task setting presents a number of optimization challenges, making it
difficult to realize large efficiency gains compared to learning tasks
independently. The reasons why multi-task learning is so challenging compared
to single-task learning are not fully understood. In this work, we identify a
set of three conditions of the multi-task optimization landscape that cause
detrimental gradient interference, and develop a simple yet general approach
for avoiding such interference between task gradients. We propose a form of
gradient surgery that projects a task's gradient onto the normal plane of the
gradient of any other task that has a conflicting gradient. On a series of
challenging multi-task supervised and multi-task RL problems, this approach
leads to substantial gains in efficiency and performance. Further, it is
model-agnostic and can be combined with previously-proposed multi-task
architectures for enhanced performance.
</p>
<a href="http://arxiv.org/abs/2001.06782" target="_blank">arXiv:2001.06782</a> [<a href="http://arxiv.org/pdf/2001.06782" target="_blank">pdf</a>]

<h2>Re-Examining Linear Embeddings for High-Dimensional Bayesian Optimization. (arXiv:2001.11659v2 [stat.ML] UPDATED)</h2>
<h3>Benjamin Letham, Roberto Calandra, Akshara Rai, Eytan Bakshy</h3>
<p>Bayesian optimization (BO) is a popular approach to optimize
expensive-to-evaluate black-box functions. A significant challenge in BO is to
scale to high-dimensional parameter spaces while retaining sample efficiency. A
solution considered in existing literature is to embed the high-dimensional
space in a lower-dimensional manifold, often via a random linear embedding. In
this paper, we identify several crucial issues and misconceptions about the use
of linear embeddings for BO. We study the properties of linear embeddings from
the literature and show that some of the design choices in current approaches
adversely impact their performance. We show empirically that properly
addressing these issues significantly improves the efficacy of linear
embeddings for BO on a range of problems, including learning a gait policy for
robot locomotion.
</p>
<a href="http://arxiv.org/abs/2001.11659" target="_blank">arXiv:2001.11659</a> [<a href="http://arxiv.org/pdf/2001.11659" target="_blank">pdf</a>]

<h2>Optimized spiking neurons can classify images with high accuracy through temporal coding with two spikes. (arXiv:2002.00860v3 [cs.NE] UPDATED)</h2>
<h3>Christoph St&#xf6;ckl, Wolfgang Maass</h3>
<p>Spike-based neuromorphic hardware is a promising option for reducing the
energy consumption of image classification and other deep learning
applications. A drastic reduction of this energy consumption is especially
needed for implementing state-of-the-art results of deep learning in mobile
phones or other edge devices. However direct training of deep spiking neural
networks is difficult, and previous methods for converting trained artificial
neural networks to spiking neurons were inefficient because the neurons had to
emit too many spikes. We show that a substantially more efficient conversion
arises when one optimizes the spiking neuron model for that purpose, so that it
not only matters for information transmission how many spikes a neuron emits,
but also when it emits those spikes. This advances the accuracy that can be
achieved for image classification with spiking neurons, and the resulting
networks need on average just two spikes per neuron for classifying an image.
In addition, our new conversion method drastically improves latency and
throughput of the resulting spiking networks.
</p>
<a href="http://arxiv.org/abs/2002.00860" target="_blank">arXiv:2002.00860</a> [<a href="http://arxiv.org/pdf/2002.00860" target="_blank">pdf</a>]

<h2>Statistical Optimal Transport posed as Learning Kernel Embedding. (arXiv:2002.03179v5 [cs.LG] UPDATED)</h2>
<h3>J. Saketha Nath (IIT Hyderabad, INDIA), Pratik Jawanpuria (Microsoft IDC, INDIA)</h3>
<p>The objective in statistical Optimal Transport (OT) is to consistently
estimate the optimal transport plan/map solely using samples from the given
source and target marginal distributions. This work takes the novel approach of
posing statistical OT as that of learning the transport plan's kernel mean
embedding from sample based estimates of marginal embeddings. The proposed
estimator controls overfitting by employing maximum mean discrepancy based
regularization, which is complementary to $\phi$-divergence (entropy) based
regularization popularly employed in existing estimators. A key result is that,
under very mild conditions, $\epsilon$-optimal recovery of the transport plan
as well as the Barycentric-projection based transport map is possible with a
sample complexity that is completely dimension-free. Moreover, the implicit
smoothing in the kernel mean embeddings enables out-of-sample estimation. An
appropriate representer theorem is proved leading to a kernelized convex
formulation for the estimator, which can then be potentially used to perform OT
even in non-standard domains. Empirical results illustrate the efficacy of the
proposed approach.
</p>
<a href="http://arxiv.org/abs/2002.03179" target="_blank">arXiv:2002.03179</a> [<a href="http://arxiv.org/pdf/2002.03179" target="_blank">pdf</a>]

<h2>A Second look at Exponential and Cosine Step Sizes: Simplicity, Convergence, and Performance. (arXiv:2002.05273v3 [stat.ML] UPDATED)</h2>
<h3>Xiaoyu Li, Zhenxun Zhuang, Francesco Orabona</h3>
<p>Stochastic Gradient Descent (SGD) is a popular tool in training large-scale
machine learning models. Its performance, however, is highly variable,
depending crucially on the choice of the step sizes. Accordingly, a variety of
strategies for tuning the step sizes have been proposed. Yet, most of them lack
a theoretical guarantee, whereas those backed by theories often do not shine in
practice. In this paper, we study two heuristic step size schedules whose power
has been repeatedly confirmed in practice: the exponential and the cosine step
sizes. For the first time, we provide theoretical support for them: we prove
their (almost) optimal convergence rates for stochastic optimization of smooth
non-convex functions. Furthermore, if in addition, the Polyak-\L{}ojasiewicz
(PL) condition holds, they both automatically adapt to the level of noise, with
a rate interpolating between a linear rate for the noiseless case and a
sub-linear one for the noisy case. Finally, we conduct a fair and comprehensive
empirical evaluation of real-world datasets with deep learning architectures.
Results show that, even if only requiring at most two hyperparameters to tune,
they best or match the performance of various finely-tuned state-of-the-art
strategies.
</p>
<a href="http://arxiv.org/abs/2002.05273" target="_blank">arXiv:2002.05273</a> [<a href="http://arxiv.org/pdf/2002.05273" target="_blank">pdf</a>]

<h2>Do We Really Need to Access the Source Data? Source Hypothesis Transfer for Unsupervised Domain Adaptation. (arXiv:2002.08546v5 [cs.CV] UPDATED)</h2>
<h3>Jian Liang, Dapeng Hu, Jiashi Feng</h3>
<p>Unsupervised domain adaptation (UDA) aims to leverage the knowledge learned
from a labeled source dataset to solve similar tasks in a new unlabeled domain.
Prior UDA methods typically require to access the source data when learning to
adapt the model, making them risky and inefficient for decentralized private
data. This work tackles a practical setting where only a trained source model
is available and investigates how we can effectively utilize such a model
without source data to solve UDA problems. We propose a simple yet generic
representation learning framework, named \emph{Source HypOthesis Transfer}
(SHOT). SHOT freezes the classifier module (hypothesis) of the source model and
learns the target-specific feature extraction module by exploiting both
information maximization and self-supervised pseudo-labeling to implicitly
align representations from the target domains to the source hypothesis. To
verify its versatility, we evaluate SHOT in a variety of adaptation cases
including closed-set, partial-set, and open-set domain adaptation. Experiments
indicate that SHOT yields state-of-the-art results among multiple domain
adaptation benchmarks.
</p>
<a href="http://arxiv.org/abs/2002.08546" target="_blank">arXiv:2002.08546</a> [<a href="http://arxiv.org/pdf/2002.08546" target="_blank">pdf</a>]

<h2>Learning Dynamic Belief Graphs to Generalize on Text-Based Games. (arXiv:2002.09127v3 [cs.CL] UPDATED)</h2>
<h3>Ashutosh Adhikari, Xingdi Yuan, Marc-Alexandre C&#xf4;t&#xe9;, Mikul&#xe1;&#x161; Zelinka, Marc-Antoine Rondeau, Romain Laroche, Pascal Poupart, Jian Tang, Adam Trischler, William L. Hamilton</h3>
<p>Playing text-based games requires skills in processing natural language and
sequential decision making. Achieving human-level performance on text-based
games remains an open challenge, and prior research has largely relied on
hand-crafted structured representations and heuristics. In this work, we
investigate how an agent can plan and generalize in text-based games using
graph-structured representations learned end-to-end from raw text. We propose a
novel graph-aided transformer agent (GATA) that infers and updates latent
belief graphs during planning to enable effective action selection by capturing
the underlying game dynamics. GATA is trained using a combination of
reinforcement and self-supervised learning. Our work demonstrates that the
learned graph-based representations help agents converge to better policies
than their text-only counterparts and facilitate effective generalization
across game configurations. Experiments on 500+ unique games from the TextWorld
suite show that our best agent outperforms text-based baselines by an average
of 24.2%.
</p>
<a href="http://arxiv.org/abs/2002.09127" target="_blank">arXiv:2002.09127</a> [<a href="http://arxiv.org/pdf/2002.09127" target="_blank">pdf</a>]

<h2>Cross-Resolution Adversarial Dual Network for Person Re-Identification and Beyond. (arXiv:2002.09274v2 [cs.CV] UPDATED)</h2>
<h3>Yu-Jhe Li, Yun-Chun Chen, Yen-Yu Lin, Yu-Chiang Frank Wang</h3>
<p>Person re-identification (re-ID) aims at matching images of the same person
across camera views. Due to varying distances between cameras and persons of
interest, resolution mismatch can be expected, which would degrade re-ID
performance in real-world scenarios. To overcome this problem, we propose a
novel generative adversarial network to address cross-resolution person re-ID,
allowing query images with varying resolutions. By advancing adversarial
learning techniques, our proposed model learns resolution-invariant image
representations while being able to recover the missing details in
low-resolution input images. The resulting features can be jointly applied for
improving re-ID performance due to preserving resolution invariance and
recovering re-ID oriented discriminative details. Extensive experimental
results on five standard person re-ID benchmarks confirm the effectiveness of
our method and the superiority over the state-of-the-art approaches, especially
when the input resolutions are not seen during training. Furthermore, the
experimental results on two vehicle re-ID benchmarks also confirm the
generalization of our model on cross-resolution visual tasks. The extensions of
semi-supervised settings further support the use of our proposed approach to
real-world scenarios and applications.
</p>
<a href="http://arxiv.org/abs/2002.09274" target="_blank">arXiv:2002.09274</a> [<a href="http://arxiv.org/pdf/2002.09274" target="_blank">pdf</a>]

<h2>On the Search for Feedback in Reinforcement Learning. (arXiv:2002.09478v2 [cs.LG] UPDATED)</h2>
<h3>Ran Wang, Karthikeya S. Parunandi, Aayushman Sharma, Suman Chakravorty, Dileep Kalathil</h3>
<p>This paper addresses the problem of learning the optimal feedback policy for
a nonlinear stochastic dynamical system. Feedback policies typically need a
high dimensional parametrization, which makes Reinforcement Learning (RL)
algorithms that search for an optimum in this large parameter space, sample
inefficient and subject to high variance. We propose a "decoupling" principle
that drastically reduces the feedback parameter space while still remaining
locally optimal. A corollary of this result is a decoupled data-based control
(D2C) algorithm for RL: first, an open-loop deterministic trajectory
optimization problem is solved using a black-box simulation model of the
dynamical system. Then, a linear closed-loop control is developed around this
nominal trajectory using the simulation model. Empirical evidence suggests
highly significant reduction in training time, as well as the training
variance, without compromising on performance, compared to state of the art RL
algorithms.
</p>
<a href="http://arxiv.org/abs/2002.09478" target="_blank">arXiv:2002.09478</a> [<a href="http://arxiv.org/pdf/2002.09478" target="_blank">pdf</a>]

<h2>The Unreasonable Effectiveness of Greedy Algorithms in Multi-Armed Bandit with Many Arms. (arXiv:2002.10121v2 [cs.LG] UPDATED)</h2>
<h3>Mohsen Bayati, Nima Hamidi, Ramesh Johari, Khashayar Khosravi</h3>
<p>We study the structure of regret-minimizing policies in the many-armed
Bayesian multi-armed bandit problem: in particular, with k the number of arms
and T the time horizon, we consider the case where k &gt; \sqrt{T}. We first show
that subsampling is a critical step for designing optimal policies. In
particular, the standard UCB algorithm leads to sub-optimal regret bounds in
this regime. However, a subsampled UCB (SS-UCB), which samples \sqrt{T} arms
and executes UCB only on that subset, is rate-optimal. Despite theoretically
optimal regret, even SS-UCB performs poorly due to excessive exploration of
suboptimal arms. In fact, in numerical experiments SS-UCB performs worse than a
simple greedy algorithm (and its subsampled version) that pulls the current
empirical best arm at every time period. We show that these insights hold even
in a contextual setting, using real-world data. These empirical results suggest
a novel form of free exploration in the many-armed regime that benefits greedy
algorithms. We theoretically study this new source of free exploration and find
that it is deeply connected to the distribution of a certain tail event for the
prior distribution of arm rewards. This is a fundamentally distinct phenomenon
from free exploration as discussed in the recent literature on contextual
bandits, where free exploration arises due to variation in contexts. We prove
that the subsampled greedy algorithm is rate-optimal for Bernoulli bandits when
k &gt; \sqrt{T}, and achieves sublinear regret with more general distributions.
This is a case where theoretical rate optimality does not tell the whole story:
when complemented by the empirical observations of our paper, the power of
greedy algorithms becomes quite evident. Taken together, from a practical
standpoint, our results suggest that in applications it may be preferable to
use a variant of the greedy algorithm in the many-armed regime.
</p>
<a href="http://arxiv.org/abs/2002.10121" target="_blank">arXiv:2002.10121</a> [<a href="http://arxiv.org/pdf/2002.10121" target="_blank">pdf</a>]

<h2>RNNPool: Efficient Non-linear Pooling for RAM Constrained Inference. (arXiv:2002.11921v2 [cs.CV] UPDATED)</h2>
<h3>Oindrila Saha, Aditya Kusupati, Harsha Vardhan Simhadri, Manik Varma, Prateek Jain</h3>
<p>Standard Convolutional Neural Networks (CNNs) designed for computer vision
tasks tend to have large intermediate activation maps. These require large
working memory and are thus unsuitable for deployment on resource-constrained
devices typically used for inference on the edge. Aggressively downsampling the
images via pooling or strided convolutions can address the problem but leads to
a significant decrease in accuracy due to gross aggregation of the feature map
by standard pooling operators. In this paper, we introduce RNNPool, a novel
pooling operator based on Recurrent Neural Networks (RNNs), that efficiently
aggregates features over large patches of an image and rapidly downsamples
activation maps. Empirical evaluation indicates that an RNNPool layer can
effectively replace multiple blocks in a variety of architectures such as
MobileNets, DenseNet when applied to standard vision tasks like image
classification and face detection. That is, RNNPool can significantly decrease
computational complexity and peak memory usage for inference while retaining
comparable accuracy. We use RNNPool with the standard S3FD architecture to
construct a face detection method that achieves state-of-the-art MAP for tiny
ARM Cortex-M4 class microcontrollers with under 256 KB of RAM. Code is released
at https://github.com/Microsoft/EdgeML.
</p>
<a href="http://arxiv.org/abs/2002.11921" target="_blank">arXiv:2002.11921</a> [<a href="http://arxiv.org/pdf/2002.11921" target="_blank">pdf</a>]

<h2>Domain Adaptation with Conditional Distribution Matching and Generalized Label Shift. (arXiv:2003.04475v2 [cs.LG] UPDATED)</h2>
<h3>Remi Tachet des Combes, Han Zhao, Yu-Xiang Wang, Geoff Gordon</h3>
<p>Adversarial learning has demonstrated good performance in the unsupervised
domain adaptation setting, by learning domain-invariant representations.
However, recent work has shown limitations of this approach when label
distributions differ between the source and target domains. In this paper, we
propose a new assumption, \textit{generalized label shift} ($GLS$), to improve
robustness against mismatched label distributions. $GLS$ states that,
conditioned on the label, there exists a representation of the input that is
invariant between the source and target domains. Under $GLS$, we provide
theoretical guarantees on the transfer performance of any classifier. We also
devise necessary and sufficient conditions for $GLS$ to hold, by using an
estimation of the relative class weights between domains and an appropriate
reweighting of samples. Our weight estimation method could be straightforwardly
and generically applied in existing domain adaptation (DA) algorithms that
learn domain-invariant representations, with small computational overhead. In
particular, we modify three DA algorithms, JAN, DANN and CDAN, and evaluate
their performance on standard and artificial DA tasks. Our algorithms
outperform the base versions, with vast improvements for large label
distribution mismatches. Our code is available at https://tinyurl.com/y585xt6j.
</p>
<a href="http://arxiv.org/abs/2003.04475" target="_blank">arXiv:2003.04475</a> [<a href="http://arxiv.org/pdf/2003.04475" target="_blank">pdf</a>]

<h2>Learning Compositional Rules via Neural Program Synthesis. (arXiv:2003.05562v2 [cs.AI] UPDATED)</h2>
<h3>Maxwell I. Nye, Armando Solar-Lezama, Joshua B. Tenenbaum, Brenden M. Lake</h3>
<p>Many aspects of human reasoning, including language, require learning rules
from very little data. Humans can do this, often learning systematic rules from
very few examples, and combining these rules to form compositional rule-based
systems. Current neural architectures, on the other hand, often fail to
generalize in a compositional manner, especially when evaluated in ways that
vary systematically from training. In this work, we present a neuro-symbolic
model which learns entire rule systems from a small set of examples. Instead of
directly predicting outputs from inputs, we train our model to induce the
explicit system of rules governing a set of previously seen examples, drawing
upon techniques from the neural program synthesis literature. Our
rule-synthesis approach outperforms neural meta-learning techniques in three
domains: an artificial instruction-learning domain used to evaluate human
learning, the SCAN challenge datasets, and learning rule-based translations of
number words into integers for a wide range of human languages.
</p>
<a href="http://arxiv.org/abs/2003.05562" target="_blank">arXiv:2003.05562</a> [<a href="http://arxiv.org/pdf/2003.05562" target="_blank">pdf</a>]

<h2>Intra Order-preserving Functions for Calibration of Multi-Class Neural Networks. (arXiv:2003.06820v2 [cs.LG] UPDATED)</h2>
<h3>Amir Rahimi, Amirreza Shaban, Ching-An Cheng, Richard Hartley, Byron Boots</h3>
<p>Predicting calibrated confidence scores for multi-class deep networks is
important for avoiding rare but costly mistakes. A common approach is to learn
a post-hoc calibration function that transforms the output of the original
network into calibrated confidence scores while maintaining the network's
accuracy. However, previous post-hoc calibration techniques work only with
simple calibration functions, potentially lacking sufficient representation to
calibrate the complex function landscape of deep networks. In this work, we aim
to learn general post-hoc calibration functions that can preserve the top-k
predictions of any deep network. We call this family of functions intra
order-preserving functions. We propose a new neural network architecture that
represents a class of intra order-preserving functions by combining common
neural network components. Additionally, we introduce order-invariant and
diagonal sub-families, which can act as regularization for better
generalization when the training data size is small. We show the effectiveness
of the proposed method across a wide range of datasets and classifiers. Our
method outperforms state-of-the-art post-hoc calibration methods, namely
temperature scaling and Dirichlet calibration, in several evaluation metrics
for the task.
</p>
<a href="http://arxiv.org/abs/2003.06820" target="_blank">arXiv:2003.06820</a> [<a href="http://arxiv.org/pdf/2003.06820" target="_blank">pdf</a>]

<h2>Enhancing Social Recommendation with Adversarial Graph Convolutional Networks. (arXiv:2004.02340v4 [cs.IR] UPDATED)</h2>
<h3>Junliang Yu, Hongzhi Yin, Jundong Li, Min Gao, Zi Huang, Lizhen Cui</h3>
<p>Social recommender systems are expected to improve recommendation quality by
incorporating social information when there is little user-item interaction
data. However, recent reports from industry show that social recommender
systems consistently fail in practice. According to the negative findings, the
failure is attributed to: (1) A majority of users only have a very limited
number of neighbors in social networks and can hardly benefit from social
relations; (2) Social relations are noisy but they are indiscriminately used;
(3) Social relations are assumed to be universally applicable to multiple
scenarios while they are actually multi-faceted and show heterogeneous
strengths in different scenarios. Most existing social recommendation models
only consider the homophily in social networks and neglect these drawbacks. In
this paper we propose a deep adversarial framework based on graph convolutional
networks (GCN) to address these problems. Concretely, for (1) and (2), a
GCN-based autoencoder is developed to augment the relation data by encoding
high-order and complex connectivity patterns, and meanwhile is optimized
subject to the constraint of reconstructing the social profile to guarantee the
validity of the identified neighborhood. After obtaining enough purified social
relations for each user, a GCN-based attentive social recommendation module is
designed to address (3) by capturing the heterogeneous strengths of social
relations. Finally, we adopt adversarial training to unify all the components
by playing a Minimax game and ensure a coordinated effort to enhance
recommendation performance. Extensive experiments on multiple open datasets
demonstrate the superiority of our framework and the ablation study confirms
the importance and effectiveness of each component.
</p>
<a href="http://arxiv.org/abs/2004.02340" target="_blank">arXiv:2004.02340</a> [<a href="http://arxiv.org/pdf/2004.02340" target="_blank">pdf</a>]

<h2>SciWING -- A Software Toolkit for Scientific Document Processing. (arXiv:2004.03807v2 [cs.DL] UPDATED)</h2>
<h3>Abhinav Ramesh Kashyap, Min-Yen Kan</h3>
<p>We introduce SciWING, an open-source software toolkit which provides access
to pre-trained models for scientific document processing tasks, inclusive of
citation string parsing and logical structure recovery. SciWING enables
researchers to rapidly experiment with different models by swapping and
stacking different modules. It also enables them declare and run models from a
configuration file. It enables researchers to perform production-ready transfer
learning from general, pre-trained transformers (i.e., BERT, SciBERT etc), and
aids development of end-user applications. It includes ready-to-use web and
terminal-based applications and demonstrations (Available from
this http URL).
</p>
<a href="http://arxiv.org/abs/2004.03807" target="_blank">arXiv:2004.03807</a> [<a href="http://arxiv.org/pdf/2004.03807" target="_blank">pdf</a>]

<h2>Natural Disaster Classification using Aerial Photography Explainable for Typhoon Damaged Feature. (arXiv:2004.10130v4 [cs.CV] UPDATED)</h2>
<h3>Takato Yasuno, Masazumi Amakata, Masahiro Okano</h3>
<p>Recent years, typhoon damages has become social problem owing to climate
change. In 9 September 2019, Typhoon Faxai passed on the Chiba in Japan, whose
damages included with electric provision stop because of strong wind recorded
on the maximum 45 meter per second. A large amount of tree fell down, and the
neighbor electric poles also fell down at the same time. These disaster
features have caused that it took 18 days for recovery longer than past ones.
Immediate responses are important for faster recovery. As long as we can,
aerial survey for global screening of devastated region would be required for
decision support to respond where to recover ahead. This paper proposes a
practical method to visualize the damaged areas focused on the typhoon disaster
features using aerial photography. This method can classify eight classes which
contains land covers without damages and areas with disaster. Using target
feature class probabilities, we can visualize disaster features map to scale a
color range. Furthermore, we can realize disaster feature mapping on each unit
grid images to compute the convolutional activation map using Grad-CAM. We
demonstrate case studies applied to aerial photographs recorded at the Chiba
region after typhoon disaster. (198 words)
</p>
<a href="http://arxiv.org/abs/2004.10130" target="_blank">arXiv:2004.10130</a> [<a href="http://arxiv.org/pdf/2004.10130" target="_blank">pdf</a>]

<h2>Self-Paced Deep Reinforcement Learning. (arXiv:2004.11812v5 [cs.LG] UPDATED)</h2>
<h3>Pascal Klink, Carlo D&#x27;Eramo, Jan Peters, Joni Pajarinen</h3>
<p>Curriculum reinforcement learning (CRL) improves the learning speed and
stability of an agent by exposing it to a tailored series of tasks throughout
learning. Despite empirical successes, an open question in CRL is how to
automatically generate a curriculum for a given reinforcement learning (RL)
agent, avoiding manual design. In this paper, we propose an answer by
interpreting the curriculum generation as an inference problem, where
distributions over tasks are progressively learned to approach the target task.
This approach leads to an automatic curriculum generation, whose pace is
controlled by the agent, with solid theoretical motivation and easily
integrated with deep RL algorithms. In the conducted experiments, the curricula
generated with the proposed algorithm significantly improve learning
performance across several environments and deep RL algorithms, matching or
outperforming state-of-the-art existing CRL algorithms.
</p>
<a href="http://arxiv.org/abs/2004.11812" target="_blank">arXiv:2004.11812</a> [<a href="http://arxiv.org/pdf/2004.11812" target="_blank">pdf</a>]

<h2>VTGNet: A Vision-based Trajectory Generation Network for Autonomous Vehicles in Urban Environments. (arXiv:2004.12591v3 [cs.CV] UPDATED)</h2>
<h3>Peide Cai, Yuxiang Sun, Hengli Wang, Ming Liu</h3>
<p>Traditional methods for autonomous driving are implemented with many building
blocks from perception, planning and control, making them difficult to
generalize to varied scenarios due to complex assumptions and
interdependencies. Recently, the end-to-end driving method has emerged, which
performs well and generalizes to new environments by directly learning from
export-provided data. However, many existing methods on this topic neglect to
check the confidence of the driving actions and the ability to recover from
driving mistakes. In this paper, we develop an uncertainty-aware end-to-end
trajectory generation method based on imitation learning. It can extract
spatiotemporal features from the front-view camera images for scene
understanding, and then generate collision-free trajectories several seconds
into the future. The experimental results suggest that under various weather
and lighting conditions, our network can reliably generate trajectories in
different urban environments, such as turning at intersections and slowing down
for collision avoidance. Furthermore, closed-loop driving tests suggest that
the proposed method achieves better cross-scene/platform driving results than
the state-of-the-art (SOTA) end-to-end control method, where our model can
recover from off-center and off-orientation errors and capture 80% of dangerous
cases with high uncertainty estimations.
</p>
<a href="http://arxiv.org/abs/2004.12591" target="_blank">arXiv:2004.12591</a> [<a href="http://arxiv.org/pdf/2004.12591" target="_blank">pdf</a>]

<h2>Out-of-Sample Representation Learning for Multi-Relational Graphs. (arXiv:2004.13230v2 [cs.LG] UPDATED)</h2>
<h3>Marjan Albooyeh, Rishab Goel, Seyed Mehran Kazemi</h3>
<p>Many important problems can be formulated as reasoning in knowledge graphs.
Representation learning has proved extremely effective for transductive
reasoning, in which one needs to make new predictions for already observed
entities. This is true for both attributed graphs(where each entity has an
initial feature vector) and non-attributed graphs (where the only initial
information derives from known relations with other entities). For
out-of-sample reasoning, where one needs to make predictions for entities that
were unseen at training time, much prior work considers attributed graph.
However, this problem is surprisingly under-explored for non-attributed graphs.
In this paper, we study the out-of-sample representation learning problem for
non-attributed knowledge graphs, create benchmark datasets for this task,
develop several models and baselines, and provide empirical analyses and
comparisons of the proposed models and baselines.
</p>
<a href="http://arxiv.org/abs/2004.13230" target="_blank">arXiv:2004.13230</a> [<a href="http://arxiv.org/pdf/2004.13230" target="_blank">pdf</a>]

<h2>Value-based Engineering for Ethics by Design. (arXiv:2004.13676v2 [cs.CY] UPDATED)</h2>
<h3>Sarah Spiekermann, Till Winkler</h3>
<p>This article gives a methodological overview of Value-based Engineering for
ethics by design. It discusses key challenges and measures involved in
eliciting, conceptualizing, prioritizing and respecting values in system
design. Thereby it draws from software engineering, value sensitive design,
design thinking and participatory design as well as from philosophical sources,
especially Material Ethics of Value. The article recognizes timely challenges
for Value-based Engineering, such as compatibility with agile forms of system
development, responsibility in hardly controllable ecosystems of interconnected
services, fearless integration of external stakeholders and the difficulty in
measuring the ethicality of a system. Finally, the Value-based Engineering
methodology presented here benefits from learnings collected in the IEEE P7000
standardization process as well as from a case study. P7000 has been set up by
IEEE to establish a process model, which addresses ethical considerations
throughout the various stages of system initiation, analysis and design.
</p>
<a href="http://arxiv.org/abs/2004.13676" target="_blank">arXiv:2004.13676</a> [<a href="http://arxiv.org/pdf/2004.13676" target="_blank">pdf</a>]

<h2>Condensed Movies: Story Based Retrieval with Contextual Embeddings. (arXiv:2005.04208v2 [cs.CV] UPDATED)</h2>
<h3>Max Bain, Arsha Nagrani, Andrew Brown, Andrew Zisserman</h3>
<p>Our objective in this work is long range understanding of the narrative
structure of movies. Instead of considering the entire movie, we propose to
learn from the `key scenes' of the movie, providing a condensed look at the
full storyline. To this end, we make the following three contributions: (i) We
create the Condensed Movies Dataset (CMD) consisting of the key scenes from
over 3K movies: each key scene is accompanied by a high level semantic
description of the scene, character face-tracks, and metadata about the movie.
The dataset is scalable, obtained automatically from YouTube, and is freely
available for anybody to download and use. It is also an order of magnitude
larger than existing movie datasets in the number of movies; (ii) We provide a
deep network baseline for text-to-video retrieval on our dataset, combining
character, speech and visual cues into a single video embedding; and finally
(iii) We demonstrate how the addition of context from other video clips
improves retrieval performance.
</p>
<a href="http://arxiv.org/abs/2005.04208" target="_blank">arXiv:2005.04208</a> [<a href="http://arxiv.org/pdf/2005.04208" target="_blank">pdf</a>]

<h2>Movement Pruning: Adaptive Sparsity by Fine-Tuning. (arXiv:2005.07683v2 [cs.CL] UPDATED)</h2>
<h3>Victor Sanh, Thomas Wolf, Alexander M. Rush</h3>
<p>Magnitude pruning is a widely used strategy for reducing model size in pure
supervised learning; however, it is less effective in the transfer learning
regime that has become standard for state-of-the-art natural language
processing applications. We propose the use of movement pruning, a simple,
deterministic first-order weight pruning method that is more adaptive to
pretrained model fine-tuning. We give mathematical foundations to the method
and compare it to existing zeroth- and first-order pruning methods. Experiments
show that when pruning large pretrained language models, movement pruning shows
significant improvements in high-sparsity regimes. When combined with
distillation, the approach achieves minimal accuracy loss with down to only 3%
of the model parameters.
</p>
<a href="http://arxiv.org/abs/2005.07683" target="_blank">arXiv:2005.07683</a> [<a href="http://arxiv.org/pdf/2005.07683" target="_blank">pdf</a>]

<h2>Automating Turbulence Modeling by Multi-Agent Reinforcement Learning. (arXiv:2005.09023v2 [physics.comp-ph] UPDATED)</h2>
<h3>Guido Novati, Hugues Lascombes de Laroussilhe, Petros Koumoutsakos</h3>
<p>The modeling of turbulent flows is critical to scientific and engineering
problems ranging from aircraft design to weather forecasting and climate
prediction. Over the last sixty years numerous turbulence models have been
proposed, largely based on physical insight and engineering intuition. Recent
advances in machine learning and data science have incited new efforts to
complement these approaches. To date, all such efforts have focused on
supervised learning which, despite demonstrated promise, encounters
difficulties in generalizing beyond the distributions of the training data. In
this work we introduce multi-agent reinforcement learning (MARL) as an
automated discovery tool of turbulence models. We demonstrate the potential of
this approach on Large Eddy Simulations of homogeneous and isotropic turbulence
using as reward the recovery of the statistical properties of Direct Numerical
Simulations. Here, the closure model is formulated as a control policy enacted
by cooperating agents, which detect critical spatio-temporal patterns in the
flow field to estimate the unresolved sub-grid scale (SGS) physics. The present
results are obtained with state-of-the-art algorithms based on experience
replay and compare favorably with established dynamic SGS modeling approaches.
Moreover, we show that the present turbulence models generalize across grid
sizes and flow conditions as expressed by the Reynolds numbers.
</p>
<a href="http://arxiv.org/abs/2005.09023" target="_blank">arXiv:2005.09023</a> [<a href="http://arxiv.org/pdf/2005.09023" target="_blank">pdf</a>]

<h2>Stable and expressive recurrent vision models. (arXiv:2005.11362v2 [cs.CV] UPDATED)</h2>
<h3>Drew Linsley, Alekh Karkada Ashok, Lakshmi Narasimhan Govindarajan, Rex Liu, Thomas Serre</h3>
<p>Primate vision depends on recurrent processing for reliable perception. A
growing body of literature also suggests that recurrent connections improve the
learning efficiency and generalization of vision models on classic computer
vision challenges. Why then, are current large-scale challenges dominated by
feedforward networks? We posit that the effectiveness of recurrent vision
models is bottlenecked by the standard algorithm used for training them,
"back-propagation through time" (BPTT), which has O(N) memory-complexity for
training an N step model. Thus, recurrent vision model design is bounded by
memory constraints, forcing a choice between rivaling the enormous capacity of
leading feedforward models or trying to compensate for this deficit through
granular and complex dynamics. Here, we develop a new learning algorithm,
"contractor recurrent back-propagation" (C-RBP), which alleviates these issues
by achieving constant O(1) memory-complexity with steps of recurrent
processing. We demonstrate that recurrent vision models trained with C-RBP can
detect long-range spatial dependencies in a synthetic contour tracing task that
BPTT-trained models cannot. We further show that recurrent vision models
trained with C-RBP to solve the large-scale Panoptic Segmentation MS-COCO
challenge outperform the leading feedforward approach, with fewer free
parameters. C-RBP is a general-purpose learning algorithm for any application
that can benefit from expansive recurrent dynamics. Code and data are available
at https://github.com/c-rbp.
</p>
<a href="http://arxiv.org/abs/2005.11362" target="_blank">arXiv:2005.11362</a> [<a href="http://arxiv.org/pdf/2005.11362" target="_blank">pdf</a>]

<h2>Modeling the Distribution of Normal Data in Pre-Trained Deep Features for Anomaly Detection. (arXiv:2005.14140v2 [cs.CV] UPDATED)</h2>
<h3>Oliver Rippel, Patrick Mertens, Dorit Merhof</h3>
<p>Anomaly Detection (AD) in images is a fundamental computer vision problem and
refers to identifying images and image substructures that deviate significantly
from the norm. Popular AD algorithms commonly try to learn a model of normality
from scratch using task specific datasets, but are limited to semi-supervised
approaches employing mostly normal data due to the inaccessibility of anomalies
on a large scale combined with the ambiguous nature of anomaly appearance.

We follow an alternative approach and demonstrate that deep feature
representations learned by discriminative models on large natural image
datasets are well suited to describe normality and detect even subtle anomalies
in a transfer learning setting. Our model of normality is established by
fitting a multivariate Gaussian (MVG) to deep feature representations of
classification networks trained on ImageNet using normal data only. By
subsequently applying the Mahalanobis distance as the anomaly score we
outperform the current state of the art on the public MVTec AD dataset,
achieving an AUROC value of $95.8 \pm 1.2$ (mean $\pm$ SEM) over all 15
classes. We further investigate why the learned representations are
discriminative to the AD task using Principal Component Analysis. We find that
the principal components containing little variance in normal data are the ones
crucial for discriminating between normal and anomalous instances. This gives a
possible explanation to the often sub-par performance of AD approaches trained
from scratch using normal data only. By selectively fitting a MVG to these most
relevant components only, we are able to further reduce model complexity while
retaining AD performance. We also investigate setting the working point by
selecting acceptable False Positive Rate thresholds based on the MVG
assumption.

Code available at https://github.com/ORippler/gaussian-ad-mvtec
</p>
<a href="http://arxiv.org/abs/2005.14140" target="_blank">arXiv:2005.14140</a> [<a href="http://arxiv.org/pdf/2005.14140" target="_blank">pdf</a>]

<h2>Locally Differentially Private (Contextual) Bandits Learning. (arXiv:2006.00701v3 [cs.LG] UPDATED)</h2>
<h3>Kai Zheng, Tianle Cai, Weiran Huang, Zhenguo Li, Liwei Wang</h3>
<p>We study locally differentially private (LDP) bandits learning in this paper.
First, we propose simple black-box reduction frameworks that can solve a large
family of context-free bandits learning problems with LDP guarantee. Based on
our frameworks, we can improve previous best results for private bandits
learning with one-point feedback, such as private Bandits Convex Optimization,
and obtain the first result for Bandits Convex Optimization (BCO) with
multi-point feedback under LDP. LDP guarantee and black-box nature make our
frameworks more attractive in real applications compared with previous
specifically designed and relatively weaker differentially private (DP)
context-free bandits algorithms. Further, we extend our $(\varepsilon,
\delta)$-LDP algorithm to Generalized Linear Bandits, which enjoys a sub-linear
regret $\tilde{O}(T^{3/4}/\varepsilon)$ and is conjectured to be nearly
optimal. Note that given the existing $\Omega(T)$ lower bound for DP contextual
linear bandits (Shariff&amp;Sheffe, 2018), our result shows a fundamental
difference between LDP and DP contextual bandits learning.
</p>
<a href="http://arxiv.org/abs/2006.00701" target="_blank">arXiv:2006.00701</a> [<a href="http://arxiv.org/pdf/2006.00701" target="_blank">pdf</a>]

<h2>D-ACC: Dynamic Adaptive Cruise Control for Highways with Ramps Based on Deep Q-Learning. (arXiv:2006.01411v3 [cs.CY] UPDATED)</h2>
<h3>Lokesh Das, Myounggyu Won</h3>
<p>An Adaptive Cruise Control (ACC) system allows vehicles to maintain a desired
headway distance to a preceding vehicle automatically. ACC is increasingly
adopted by commercial vehicles. Recent research demonstrates that effective use
of ACC can improve the traffic flow by adjusting the headway distance in
response to dynamically changing traffic conditions. In this paper, we
demonstrate that a state-of-the-art real-time ACC system performs poorly on
highways with ramps because the simple model-based approach does not take into
account appropriately the dynamics of traffic on ramps in determining the
optimal headway distance. We then propose a dynamic adaptive cruise control
system (D-ACC) based on deep reinforcement learning that adapts the headway
distance effectively according to dynamically changing traffic conditions of
both the main road and ramp to optimize the traffic flow. Extensive simulations
are performed with a combination of a traffic simulator (SUMO) and
vehicle-to-everything communication (V2X) network simulator (Veins) under
numerous traffic scenarios. We demonstrate that D-ACC improves the traffic flow
by up to 70% compared with a state-of-the-art real-time ACC system in a highway
segment with a ramp.
</p>
<a href="http://arxiv.org/abs/2006.01411" target="_blank">arXiv:2006.01411</a> [<a href="http://arxiv.org/pdf/2006.01411" target="_blank">pdf</a>]

<h2>Unifying Activation- and Timing-based Learning Rules for Spiking Neural Networks. (arXiv:2006.02642v2 [cs.NE] UPDATED)</h2>
<h3>Jinseok Kim, Kyungsu Kim, Jae-Joon Kim</h3>
<p>For the gradient computation across the time domain in Spiking Neural
Networks (SNNs) training, two different approaches have been independently
studied. The first is to compute the gradients with respect to the change in
spike activation (activation-based methods), and the second is to compute the
gradients with respect to the change in spike timing (timing-based methods). In
this work, we present a comparative study of the two methods and propose a new
supervised learning method that combines them. The proposed method utilizes
each individual spike more effectively by shifting spike timings as in the
timing-based methods as well as generating and removing spikes as in the
activation-based methods. Experimental results showed that the proposed method
achieves higher performance in terms of both accuracy and efficiency than the
previous approaches.
</p>
<a href="http://arxiv.org/abs/2006.02642" target="_blank">arXiv:2006.02642</a> [<a href="http://arxiv.org/pdf/2006.02642" target="_blank">pdf</a>]

<h2>From Federated to Fog Learning: Distributed Machine Learning over Heterogeneous Wireless Networks. (arXiv:2006.03594v3 [cs.DC] UPDATED)</h2>
<h3>Seyyedali Hosseinalipour, Christopher G. Brinton, Vaneet Aggarwal, Huaiyu Dai, Mung Chiang</h3>
<p>Machine learning (ML) tasks are becoming ubiquitous in today's network
applications. Federated learning has emerged recently as a technique for
training ML models at the network edge by leveraging processing capabilities
across the nodes that collect the data. There are several challenges with
employing conventional federated learning in contemporary networks, due to the
significant heterogeneity in compute and communication capabilities that exist
across devices. To address this, we advocate a new learning paradigm called fog
learning which will intelligently distribute ML model training across the
continuum of nodes from edge devices to cloud servers. Fog learning enhances
federated learning along three major dimensions: network, heterogeneity, and
proximity. It considers a multi-layer hybrid learning framework consisting of
heterogeneous devices with various proximities. It accounts for the topology
structures of the local networks among the heterogeneous nodes at each network
layer, orchestrating them for collaborative/cooperative learning through
device-to-device (D2D) communications. This migrates from star network
topologies used for parameter transfers in federated learning to more
distributed topologies at scale. We discuss several open research directions to
realizing fog learning.
</p>
<a href="http://arxiv.org/abs/2006.03594" target="_blank">arXiv:2006.03594</a> [<a href="http://arxiv.org/pdf/2006.03594" target="_blank">pdf</a>]

<h2>Texture Interpolation for Probing Visual Perception. (arXiv:2006.03698v2 [q-bio.NC] UPDATED)</h2>
<h3>Jonathan Vacher, Aida Davila, Adam Kohn, Ruben Coen-Cagli</h3>
<p>Texture synthesis models are important tools for understanding visual
processing. In particular, statistical approaches based on neurally relevant
features have been instrumental in understanding aspects of visual perception
and of neural coding. New deep learning-based approaches further improve the
quality of synthetic textures. Yet, it is still unclear why deep texture
synthesis performs so well, and applications of this new framework to probe
visual perception are scarce. Here, we show that distributions of deep
convolutional neural network (CNN) activations of a texture are well described
by elliptical distributions and therefore, following optimal transport theory,
constraining their mean and covariance is sufficient to generate new texture
samples. Then, we propose the natural geodesics (ie the shortest path between
two points) arising with the optimal transport metric to interpolate between
arbitrary textures. Compared to other CNN-based approaches, our interpolation
method appears to match more closely the geometry of texture perception, and
our mathematical framework is better suited to study its statistical nature. We
apply our method by measuring the perceptual scale associated to the
interpolation parameter in human observers, and the neural sensitivity of
different areas of visual cortex in macaque monkeys.
</p>
<a href="http://arxiv.org/abs/2006.03698" target="_blank">arXiv:2006.03698</a> [<a href="http://arxiv.org/pdf/2006.03698" target="_blank">pdf</a>]

<h2>Towards large-scale, automated, accurate detection of CCTV camera objects using computer vision. Applications and implications for privacy, safety, and cybersecurity. (Preprint). (arXiv:2006.03870v2 [cs.CV] UPDATED)</h2>
<h3>Hannu Turtiainen, Andrei Costin, Timo Hamalainen, Tuomo Lahtinen</h3>
<p>In order to withstand the ever-increasing invasion of privacy by CCTV cameras
and technologies, on par CCTV-aware solutions must exist that provide privacy,
safety, and cybersecurity features. We argue that a first important step
towards such CCTV-aware solutions must be a mapping system (e.g., Google Maps,
OpenStreetMap) that provides both privacy and safety routing and navigation
options. However, this in turn requires that the mapping system contains
updated information on CCTV cameras' exact geo-location, coverage area, and
possibly other meta-data (e.g., resolution, facial recognition features,
operator). Such information is however missing from current mapping systems,
and there are several ways to fix this. One solution is to perform CCTV camera
detection on geo-location tagged images, e.g., street view imagery on various
platforms, user images publicly posted in image sharing platforms such as
Flickr. Unfortunately, to the best of our knowledge, there are no computer
vision models for CCTV camera object detection as well as no mapping system
that supports privacy and safety routing options.

To close these gaps, with this paper we introduce the first and only computer
vision MS COCO-compatible models that are able to accurately detect CCTV and
video surveillance cameras in images and video frames. To this end, our best
detectors were built using 8387 images that were manually reviewed and
annotated to contain 10419 CCTV camera instances, and achieve an accuracy of up
to 98.7%. Moreover, we build and evaluate multiple models, present a
comprehensive comparison of their performance, and outline core challenges
associated with such research.
</p>
<a href="http://arxiv.org/abs/2006.03870" target="_blank">arXiv:2006.03870</a> [<a href="http://arxiv.org/pdf/2006.03870" target="_blank">pdf</a>]

<h2>Memory-Efficient Learning of Stable Linear Dynamical Systems for Prediction and Control. (arXiv:2006.03937v3 [cs.LG] UPDATED)</h2>
<h3>Giorgos Mamakoukas, Orest Xherija, T. D. Murphey</h3>
<p>Learning a stable Linear Dynamical System (LDS) from data involves creating
models that both minimize reconstruction error and enforce stability of the
learned representation. We propose a novel algorithm for learning stable LDSs.
Using a recent characterization of stable matrices, we present an optimization
method that ensures stability at every step and iteratively improves the
reconstruction error using gradient directions derived in this paper. When
applied to LDSs with inputs, our approach---in contrast to current methods for
learning stable LDSs---updates both the state and control matrices, expanding
the solution space and allowing for models with lower reconstruction error. We
apply our algorithm in simulations and experiments to a variety of problems,
including learning dynamic textures from image sequences and controlling a
robotic manipulator. Compared to existing approaches, our proposed method
achieves an orders-of-magnitude improvement in reconstruction error and
superior results in terms of control performance. In addition, it is provably
more memory-efficient, with an O(n^2) space complexity compared to O(n^4) of
competing alternatives, thus scaling to higher-dimensional systems when the
other methods fail.
</p>
<a href="http://arxiv.org/abs/2006.03937" target="_blank">arXiv:2006.03937</a> [<a href="http://arxiv.org/pdf/2006.03937" target="_blank">pdf</a>]

<h2>Yield Loss Reduction and Test of AI and Deep Learning Accelerators. (arXiv:2006.04798v3 [cs.DC] UPDATED)</h2>
<h3>Mehdi Sadi, Ujjwal Guin</h3>
<p>With data-driven analytics becoming mainstream, the global demand for
dedicated AI and Deep Learning accelerator chips is soaring. These
accelerators, designed with densely packed Processing Elements (PE), are
especially vulnerable to the manufacturing defects and functional faults common
in the advanced semiconductor process nodes resulting in significant yield
loss. In this work, we demonstrate an application-driven methodology of binning
the AI accelerator chips, and yield loss reduction by correlating the circuit
faults in the PEs of the accelerator with the desired accuracy of the target AI
workload. We exploit the inherent fault tolerance features of trained deep
learning models and a strategy of selective deactivation of faulty PEs to
develop the presented yield loss reduction and test methodology. An analytical
relationship is derived between fault location, fault rate, and the AI task's
accuracy for deciding if the accelerator chip can pass the final yield test. A
yield-loss reduction aware fault isolation, ATPG, and test flow are presented
for the multiply and accumulate units of the PEs. Results obtained with widely
used AI/deep learning benchmarks demonstrate that the accelerators can sustain
5% fault-rate in PE arrays while suffering from less than 1% accuracy loss,
thus enabling product-binning and yield loss reduction of these chips.
</p>
<a href="http://arxiv.org/abs/2006.04798" target="_blank">arXiv:2006.04798</a> [<a href="http://arxiv.org/pdf/2006.04798" target="_blank">pdf</a>]

<h2>Knowledge Distillation: A Survey. (arXiv:2006.05525v4 [cs.LG] UPDATED)</h2>
<h3>Jianping Gou, Baosheng Yu, Stephen John Maybank, Dacheng Tao</h3>
<p>In recent years, deep neural networks have been successful in both industry
and academia, especially for computer vision tasks. The great success of deep
learning is mainly due to its scalability to encode large-scale data and to
maneuver billions of model parameters. However, it is a challenge to deploy
these cumbersome deep models on devices with limited resources, e.g., mobile
phones and embedded devices, not only because of the high computational
complexity but also the large storage requirements. To this end, a variety of
model compression and acceleration techniques have been developed. As a
representative type of model compression and acceleration, knowledge
distillation effectively learns a small student model from a large teacher
model. It has received rapid increasing attention from the community. This
paper provides a comprehensive survey of knowledge distillation from the
perspectives of knowledge categories, training schemes, teacher-student
architecture, distillation algorithms, performance comparison and applications.
Furthermore, challenges in knowledge distillation are briefly reviewed and
comments on future research are discussed and forwarded.
</p>
<a href="http://arxiv.org/abs/2006.05525" target="_blank">arXiv:2006.05525</a> [<a href="http://arxiv.org/pdf/2006.05525" target="_blank">pdf</a>]

<h2>Distribution Regression for Sequential Data. (arXiv:2006.05805v4 [cs.LG] UPDATED)</h2>
<h3>Maud Lemercier, Cristopher Salvi, Theodoros Damoulas, Edwin V. Bonilla, Terry Lyons</h3>
<p>Distribution regression refers to the supervised learning problem where
labels are only available for groups of inputs instead of individual inputs. In
this paper, we develop a rigorous mathematical framework for distribution
regression where inputs are complex data streams. Leveraging properties of the
expected signature and a recent signature kernel trick for sequential data from
stochastic analysis, we introduce two new learning techniques, one
feature-based and the other kernel-based. Each is suited to a different data
regime in terms of the number of data streams and the dimensionality of the
individual streams. We provide theoretical results on the universality of both
approaches and demonstrate empirically their robustness to irregularly sampled
multivariate time-series, achieving state-of-the-art performance on both
synthetic and real-world examples from thermodynamics, mathematical finance and
agricultural science.
</p>
<a href="http://arxiv.org/abs/2006.05805" target="_blank">arXiv:2006.05805</a> [<a href="http://arxiv.org/pdf/2006.05805" target="_blank">pdf</a>]

<h2>Effective Dimension Adaptive Sketching Methods for Faster Regularized Least-Squares Optimization. (arXiv:2006.05874v2 [cs.LG] UPDATED)</h2>
<h3>Jonathan Lacotte, Mert Pilanci</h3>
<p>We propose a new randomized algorithm for solving L2-regularized
least-squares problems based on sketching. We consider two of the most popular
random embeddings, namely, Gaussian embeddings and the Subsampled Randomized
Hadamard Transform (SRHT). While current randomized solvers for least-squares
optimization prescribe an embedding dimension at least greater than the data
dimension, we show that the embedding dimension can be reduced to the effective
dimension of the optimization problem, and still preserve high-probability
convergence guarantees. In this regard, we derive sharp matrix deviation
inequalities over ellipsoids for both Gaussian and SRHT embeddings.
Specifically, we improve on the constant of a classical Gaussian concentration
bound whereas, for SRHT embeddings, our deviation inequality involves a novel
technical approach. Leveraging these bounds, we are able to design a practical
and adaptive algorithm which does not require to know the effective dimension
beforehand. Our method starts with an initial embedding dimension equal to 1
and, over iterations, increases the embedding dimension up to the effective one
at most. Hence, our algorithm improves the state-of-the-art computational
complexity for solving regularized least-squares problems. Further, we show
numerically that it outperforms standard iterative solvers such as the
conjugate gradient method and its pre-conditioned version on several standard
machine learning datasets.
</p>
<a href="http://arxiv.org/abs/2006.05874" target="_blank">arXiv:2006.05874</a> [<a href="http://arxiv.org/pdf/2006.05874" target="_blank">pdf</a>]

<h2>Large-Scale Adversarial Training for Vision-and-Language Representation Learning. (arXiv:2006.06195v2 [cs.CV] UPDATED)</h2>
<h3>Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng, Jingjing Liu</h3>
<p>We present VILLA, the first known effort on large-scale adversarial training
for vision-and-language (V+L) representation learning. VILLA consists of two
training stages: (i) task-agnostic adversarial pre-training; followed by (ii)
task-specific adversarial finetuning. Instead of adding adversarial
perturbations on image pixels and textual tokens, we propose to perform
adversarial training in the embedding space of each modality. To enable
large-scale training, we adopt the "free" adversarial training strategy, and
combine it with KL-divergence-based regularization to promote higher invariance
in the embedding space. We apply VILLA to current best-performing V+L models,
and achieve new state of the art on a wide range of tasks, including Visual
Question Answering, Visual Commonsense Reasoning, Image-Text Retrieval,
Referring Expression Comprehension, Visual Entailment, and NLVR2.
</p>
<a href="http://arxiv.org/abs/2006.06195" target="_blank">arXiv:2006.06195</a> [<a href="http://arxiv.org/pdf/2006.06195" target="_blank">pdf</a>]

<h2>NanoFlow: Scalable Normalizing Flows with Sublinear Parameter Complexity. (arXiv:2006.06280v4 [cs.LG] UPDATED)</h2>
<h3>Sang-gil Lee, Sungwon Kim, Sungroh Yoon</h3>
<p>Normalizing flows (NFs) have become a prominent method for deep generative
models that allow for an analytic probability density estimation and efficient
synthesis. However, a flow-based network is considered to be inefficient in
parameter complexity because of reduced expressiveness of bijective mapping,
which renders the models unfeasibly expensive in terms of parameters. We
present an alternative parameterization scheme called NanoFlow, which uses a
single neural density estimator to model multiple transformation stages. Hence,
we propose an efficient parameter decomposition method and the concept of flow
indication embedding, which are key missing components that enable density
estimation from a single neural network. Experiments performed on audio and
image models confirm that our method provides a new parameter-efficient
solution for scalable NFs with significant sublinear parameter complexity.
</p>
<a href="http://arxiv.org/abs/2006.06280" target="_blank">arXiv:2006.06280</a> [<a href="http://arxiv.org/pdf/2006.06280" target="_blank">pdf</a>]

<h2>Deep Structural Causal Models for Tractable Counterfactual Inference. (arXiv:2006.06485v2 [stat.ML] UPDATED)</h2>
<h3>Nick Pawlowski, Daniel C. Castro, Ben Glocker</h3>
<p>We formulate a general framework for building structural causal models (SCMs)
with deep learning components. The proposed approach employs normalising flows
and variational inference to enable tractable inference of exogenous noise
variables - a crucial step for counterfactual inference that is missing from
existing deep causal learning methods. Our framework is validated on a
synthetic dataset built on MNIST as well as on a real-world medical dataset of
brain MRI scans. Our experimental results indicate that we can successfully
train deep SCMs that are capable of all three levels of Pearl's ladder of
causation: association, intervention, and counterfactuals, giving rise to a
powerful new approach for answering causal questions in imaging applications
and beyond. The code for all our experiments is available at
https://github.com/biomedia-mira/deepscm.
</p>
<a href="http://arxiv.org/abs/2006.06485" target="_blank">arXiv:2006.06485</a> [<a href="http://arxiv.org/pdf/2006.06485" target="_blank">pdf</a>]

<h2>Improving GAN Training with Probability Ratio Clipping and Sample Reweighting. (arXiv:2006.06900v3 [cs.LG] UPDATED)</h2>
<h3>Yue Wu, Pan Zhou, Andrew Gordon Wilson, Eric P. Xing, Zhiting Hu</h3>
<p>Despite success on a wide range of problems related to vision, generative
adversarial networks (GANs) often suffer from inferior performance due to
unstable training, especially for text generation. To solve this issue, we
propose a new variational GAN training framework which enjoys superior training
stability. Our approach is inspired by a connection of GANs and reinforcement
learning under a variational perspective. The connection leads to (1)
probability ratio clipping that regularizes generator training to prevent
excessively large updates, and (2) a sample re-weighting mechanism that
improves discriminator training by downplaying bad-quality fake samples.
Moreover, our variational GAN framework can provably overcome the training
issue in many GANs that an optimal discriminator cannot provide any informative
gradient to training generator. By plugging the training approach in diverse
state-of-the-art GAN architectures, we obtain significantly improved
performance over a range of tasks, including text generation, text style
transfer, and image generation.
</p>
<a href="http://arxiv.org/abs/2006.06900" target="_blank">arXiv:2006.06900</a> [<a href="http://arxiv.org/pdf/2006.06900" target="_blank">pdf</a>]

<h2>Kernelized information bottleneck leads to biologically plausible 3-factor Hebbian learning in deep networks. (arXiv:2006.07123v2 [cs.LG] UPDATED)</h2>
<h3>Roman Pogodin, Peter E. Latham</h3>
<p>The state-of-the art machine learning approach to training deep neural
networks, backpropagation, is implausible for real neural networks: neurons
need to know their outgoing weights; training alternates between a bottom-up
forward pass (computation) and a top-down backward pass (learning); and the
algorithm often needs precise labels of many data points. Biologically
plausible approximations to backpropagation, such as feedback alignment, solve
the weight transport problem, but not the other two. Thus, fully biologically
plausible learning rules have so far remained elusive. Here we present a family
of learning rules that does not suffer from any of these problems. It is
motivated by the information bottleneck principle (extended with kernel
methods), in which networks learn to compress the input as much as possible
without sacrificing prediction of the output. The resulting rules have a
3-factor Hebbian structure: they require pre- and post-synaptic firing rates
and an error signal - the third factor - consisting of a global teaching signal
and a layer-specific term, both available without a top-down pass. They do not
require precise labels; instead, they rely on the similarity between pairs of
desired outputs. Moreover, to obtain good performance on hard problems and
retain biological plausibility, our rules need divisive normalization - a known
feature of biological networks. Finally, simulations show that our rules
perform nearly as well as backpropagation on image classification tasks.
</p>
<a href="http://arxiv.org/abs/2006.07123" target="_blank">arXiv:2006.07123</a> [<a href="http://arxiv.org/pdf/2006.07123" target="_blank">pdf</a>]

<h2>Convolutional Generation of Textured 3D Meshes. (arXiv:2006.07660v2 [cs.CV] UPDATED)</h2>
<h3>Dario Pavllo, Graham Spinks, Thomas Hofmann, Marie-Francine Moens, Aurelien Lucchi</h3>
<p>While recent generative models for 2D images achieve impressive visual
results, they clearly lack the ability to perform 3D reasoning. This heavily
restricts the degree of control over generated objects as well as the possible
applications of such models. In this work, we bridge this gap by leveraging
recent advances in differentiable rendering. We design a framework that can
generate triangle meshes and associated high-resolution texture maps, using
only 2D supervision from single-view natural images. A key contribution of our
work is the encoding of the mesh and texture as 2D representations, which are
semantically aligned and can be easily modeled by a 2D convolutional GAN. We
demonstrate the efficacy of our method on Pascal3D+ Cars and CUB, both in an
unconditional setting and in settings where the model is conditioned on class
labels, attributes, and text. Finally, we propose an evaluation methodology
that assesses the mesh and texture quality separately.
</p>
<a href="http://arxiv.org/abs/2006.07660" target="_blank">arXiv:2006.07660</a> [<a href="http://arxiv.org/pdf/2006.07660" target="_blank">pdf</a>]

<h2>Graph Meta Learning via Local Subgraphs. (arXiv:2006.07889v3 [cs.LG] UPDATED)</h2>
<h3>Kexin Huang, Marinka Zitnik</h3>
<p>Prevailing methods for graphs require abundant label and edge information for
learning. When data for a new task are scarce, meta-learning can learn from
prior experiences and form much-needed inductive biases for fast adaption to
new tasks. Here, we introduce G-Meta, a novel meta-learning algorithm for
graphs. G-Meta uses local subgraphs to transfer subgraph-specific information
and learn transferable knowledge faster via meta gradients. G-Meta learns how
to quickly adapt to a new task using only a handful of nodes or edges in the
new task and does so by learning from data points in other graphs or related,
albeit disjoint label sets. G-Meta is theoretically justified as we show that
the evidence for a prediction can be found in the local subgraph surrounding
the target node or edge. Experiments on seven datasets and nine baseline
methods show that G-Meta outperforms existing methods by up to 16.3%. Unlike
previous methods, G-Meta successfully learns in challenging, few-shot learning
settings that require generalization to completely new graphs and
never-before-seen labels. Finally, G-Meta scales to large graphs, which we
demonstrate on a new Tree-of-Life dataset comprising of 1,840 graphs, a
two-orders of magnitude increase in the number of graphs used in prior work.
</p>
<a href="http://arxiv.org/abs/2006.07889" target="_blank">arXiv:2006.07889</a> [<a href="http://arxiv.org/pdf/2006.07889" target="_blank">pdf</a>]

<h2>Explaining Predictions by Approximating the Local Decision Boundary. (arXiv:2006.07985v2 [cs.LG] UPDATED)</h2>
<h3>Georgios Vlassopoulos, Tim van Erven, Henry Brighton, Vlado Menkovski</h3>
<p>Constructing accurate model-agnostic explanations for opaque machine learning
models remains a challenging task. Classification models for high-dimensional
data, like images, are often inherently complex. To reduce this complexity,
individual predictions may be explained locally, either in terms of a simpler
local surrogate model or by communicating how the predictions contrast with
those of another class. However, existing approaches still fall short in the
following ways: a) they measure locality using a (Euclidean) metric that is not
meaningful for non-linear high-dimensional data; or b) they do not attempt to
explain the decision boundary, which is the most relevant characteristic of
classifiers that are optimized for classification accuracy; or c) they do not
give the user any freedom in specifying attributes that are meaningful to them.
We address these issues in a new procedure for local decision boundary
approximation (DBA). To construct a meaningful metric, we train a variational
autoencoder to learn a Euclidean latent space of encoded data representations.
We impose interpretability by exploiting attribute annotations to map the
latent space to attributes that are meaningful to the user. A difficulty in
evaluating explainability approaches is the lack of a ground truth. We address
this by introducing a new benchmark data set with artificially generated Iris
images, and showing that we can recover the latent attributes that locally
determine the class. We further evaluate our approach on tabular data and on
the CelebA image data set.
</p>
<a href="http://arxiv.org/abs/2006.07985" target="_blank">arXiv:2006.07985</a> [<a href="http://arxiv.org/pdf/2006.07985" target="_blank">pdf</a>]

<h2>Neural Execution Engines: Learning to Execute Subroutines. (arXiv:2006.08084v3 [cs.LG] UPDATED)</h2>
<h3>Yujun Yan, Kevin Swersky, Danai Koutra, Parthasarathy Ranganathan, Milad Hashemi</h3>
<p>A significant effort has been made to train neural networks that replicate
algorithmic reasoning, but they often fail to learn the abstract concepts
underlying these algorithms. This is evidenced by their inability to generalize
to data distributions that are outside of their restricted training sets,
namely larger inputs and unseen data. We study these generalization issues at
the level of numerical subroutines that comprise common algorithms like
sorting, shortest paths, and minimum spanning trees. First, we observe that
transformer-based sequence-to-sequence models can learn subroutines like
sorting a list of numbers, but their performance rapidly degrades as the length
of lists grows beyond those found in the training set. We demonstrate that this
is due to attention weights that lose fidelity with longer sequences,
particularly when the input numbers are numerically similar. To address the
issue, we propose a learned conditional masking mechanism, which enables the
model to strongly generalize far outside of its training range with
near-perfect accuracy on a variety of algorithms. Second, to generalize to
unseen data, we show that encoding numbers with a binary representation leads
to embeddings with rich structure once trained on downstream tasks like
addition or multiplication. This allows the embedding to handle missing data by
faithfully interpolating numbers not seen during training.
</p>
<a href="http://arxiv.org/abs/2006.08084" target="_blank">arXiv:2006.08084</a> [<a href="http://arxiv.org/pdf/2006.08084" target="_blank">pdf</a>]

<h2>Depth Uncertainty in Neural Networks. (arXiv:2006.08437v2 [stat.ML] UPDATED)</h2>
<h3>Javier Antor&#xe1;n, James Urquhart Allingham, Jos&#xe9; Miguel Hern&#xe1;ndez-Lobato</h3>
<p>Existing methods for estimating uncertainty in deep learning tend to require
multiple forward passes, making them unsuitable for applications where
computational resources are limited. To solve this, we perform probabilistic
reasoning over the depth of neural networks. Different depths correspond to
subnetworks which share weights and whose predictions are combined via
marginalisation, yielding model uncertainty. By exploiting the sequential
structure of feed-forward networks, we are able to both evaluate our training
objective and make predictions with a single forward pass. We validate our
approach on real-world regression and image classification tasks. Our approach
provides uncertainty calibration, robustness to dataset shift, and accuracies
competitive with more computationally expensive baselines.
</p>
<a href="http://arxiv.org/abs/2006.08437" target="_blank">arXiv:2006.08437</a> [<a href="http://arxiv.org/pdf/2006.08437" target="_blank">pdf</a>]

<h2>Network Diffusions via Neural Mean-Field Dynamics. (arXiv:2006.09449v2 [cs.LG] UPDATED)</h2>
<h3>Shushan He, Hongyuan Zha, Xiaojing Ye</h3>
<p>We propose a novel learning framework based on neural mean-field dynamics for
inference and estimation problems of diffusion on networks. Our new framework
is derived from the Mori-Zwanzig formalism to obtain an exact evolution of the
node infection probabilities, which renders a delay differential equation with
memory integral approximated by learnable time convolution operators, resulting
in a highly structured and interpretable RNN. Directly using cascade data, our
framework can jointly learn the structure of the diffusion network and the
evolution of infection probabilities, which are cornerstone to important
downstream applications such as influence maximization. Connections between
parameter learning and optimal control are also established. Empirical study
shows that our approach is versatile and robust to variations of the underlying
diffusion network models, and significantly outperform existing approaches in
accuracy and efficiency on both synthetic and real-world data.
</p>
<a href="http://arxiv.org/abs/2006.09449" target="_blank">arXiv:2006.09449</a> [<a href="http://arxiv.org/pdf/2006.09449" target="_blank">pdf</a>]

<h2>On the Role of Sparsity and DAG Constraints for Learning Linear DAGs. (arXiv:2006.10201v2 [cs.LG] UPDATED)</h2>
<h3>Ignavier Ng, AmirEmad Ghassami, Kun Zhang</h3>
<p>Learning graphical structure based on Directed Acyclic Graphs (DAGs) is a
challenging problem, partly owing to the large search space of possible graphs.
A recent line of work formulates the structure learning problem as a continuous
constrained optimization task using the least squares objective and an
algebraic characterization of DAGs. However, the formulation requires a hard
DAG constraint and may lead to optimization difficulties. In this paper, we
study the asymptotic role of the sparsity and DAG constraints for learning DAG
models in the linear Gaussian and non-Gaussian cases, and investigate their
usefulness in the finite sample regime. Based on the theoretical results, we
formulate a likelihood-based score function, and show that one only has to
apply soft sparsity and DAG constraints to learn a DAG equivalent to the ground
truth DAG. This leads to an unconstrained optimization problem that is much
easier to solve. Using gradient-based optimization and GPU acceleration, our
procedure can easily handle thousands of nodes while retaining a high accuracy.
Extensive experiments validate the effectiveness of our proposed method and
show that the DAG-penalized likelihood objective is indeed favorable over the
least squares one with the hard DAG constraint.
</p>
<a href="http://arxiv.org/abs/2006.10201" target="_blank">arXiv:2006.10201</a> [<a href="http://arxiv.org/pdf/2006.10201" target="_blank">pdf</a>]

<h2>Beyond Homophily in Graph Neural Networks: Current Limitations and Effective Designs. (arXiv:2006.11468v2 [cs.LG] UPDATED)</h2>
<h3>Jiong Zhu, Yujun Yan, Lingxiao Zhao, Mark Heimann, Leman Akoglu, Danai Koutra</h3>
<p>We investigate the representation power of graph neural networks in the
semi-supervised node classification task under heterophily or low homophily,
i.e., in networks where connected nodes may have different class labels and
dissimilar features. Many popular GNNs fail to generalize to this setting, and
are even outperformed by models that ignore the graph structure (e.g.,
multilayer perceptrons). Motivated by this limitation, we identify a set of key
designs -- ego- and neighbor-embedding separation, higher-order neighborhoods,
and combination of intermediate representations -- that boost learning from the
graph structure under heterophily. We combine them into a graph neural network,
H2GCN, which we use as the base method to empirically evaluate the
effectiveness of the identified designs. Going beyond the traditional
benchmarks with strong homophily, our empirical analysis shows that the
identified designs increase the accuracy of GNNs by up to 40% and 27% over
models without them on synthetic and real networks with heterophily,
respectively, and yield competitive performance under homophily.
</p>
<a href="http://arxiv.org/abs/2006.11468" target="_blank">arXiv:2006.11468</a> [<a href="http://arxiv.org/pdf/2006.11468" target="_blank">pdf</a>]

<h2>Superpolynomial Lower Bounds for Learning One-Layer Neural Networks using Gradient Descent. (arXiv:2006.12011v2 [cs.LG] UPDATED)</h2>
<h3>Surbhi Goel, Aravind Gollakota, Zhihan Jin, Sushrut Karmalkar, Adam Klivans</h3>
<p>We prove the first superpolynomial lower bounds for learning one-layer neural
networks with respect to the Gaussian distribution using gradient descent. We
show that any classifier trained using gradient descent with respect to
square-loss will fail to achieve small test error in polynomial time given
access to samples labeled by a one-layer neural network. For classification, we
give a stronger result, namely that any statistical query (SQ) algorithm
(including gradient descent) will fail to achieve small test error in
polynomial time. Prior work held only for gradient descent run with small batch
sizes, required sharp activations, and applied to specific classes of queries.
Our lower bounds hold for broad classes of activations including ReLU and
sigmoid. The core of our result relies on a novel construction of a simple
family of neural networks that are exactly orthogonal with respect to all
spherically symmetric distributions.
</p>
<a href="http://arxiv.org/abs/2006.12011" target="_blank">arXiv:2006.12011</a> [<a href="http://arxiv.org/pdf/2006.12011" target="_blank">pdf</a>]

<h2>What shapes feature representations? Exploring datasets, architectures, and training. (arXiv:2006.12433v2 [cs.LG] UPDATED)</h2>
<h3>Katherine L. Hermann, Andrew K. Lampinen</h3>
<p>In naturalistic learning problems, a model's input contains a wide range of
features, some useful for the task at hand, and others not. Of the useful
features, which ones does the model use? Of the task-irrelevant features, which
ones does the model represent? Answers to these questions are important for
understanding the basis of models' decisions, as well as for building models
that learn versatile, adaptable representations useful beyond the original
training task. We study these questions using synthetic datasets in which the
task-relevance of input features can be controlled directly. We find that when
two features redundantly predict the labels, the model preferentially
represents one, and its preference reflects what was most linearly decodable
from the untrained model. Over training, task-relevant features are enhanced,
and task-irrelevant features are partially suppressed. Interestingly, in some
cases, an easier, weakly predictive feature can suppress a more strongly
predictive, but more difficult one. Additionally, models trained to recognize
both easy and hard features learn representations most similar to models that
use only the easy feature. Further, easy features lead to more consistent
representations across model runs than do hard features. Finally, models have
greater representational similarity to an untrained model than to models
trained on a different task. Our results highlight the complex processes that
determine which features a model represents.
</p>
<a href="http://arxiv.org/abs/2006.12433" target="_blank">arXiv:2006.12433</a> [<a href="http://arxiv.org/pdf/2006.12433" target="_blank">pdf</a>]

<h2>Iterative Deep Graph Learning for Graph Neural Networks: Better and Robust Node Embeddings. (arXiv:2006.13009v2 [cs.LG] UPDATED)</h2>
<h3>Yu Chen, Lingfei Wu, Mohammed J. Zaki</h3>
<p>In this paper, we propose an end-to-end graph learning framework, namely
Iterative Deep Graph Learning (IDGL), for jointly and iteratively learning
graph structure and graph embedding. The key rationale of IDGL is to learn a
better graph structure based on better node embeddings, and vice versa (i.e.,
better node embeddings based on a better graph structure). Our iterative method
dynamically stops when the learned graph structure approaches close enough to
the graph optimized for the downstream prediction task. In addition, we cast
the graph learning problem as a similarity metric learning problem and leverage
adaptive graph regularization for controlling the quality of the learned graph.
Finally, combining the anchor-based approximation technique, we further propose
a scalable version of IDGL, namely IDGL-Anch, which significantly reduces the
time and space complexity of IDGL without compromising the performance. Our
extensive experiments on nine benchmarks show that our proposed IDGL models can
consistently outperform or match the state-of-the-art baselines. Furthermore,
IDGL can be more robust to adversarial graphs and cope with both transductive
and inductive learning.
</p>
<a href="http://arxiv.org/abs/2006.13009" target="_blank">arXiv:2006.13009</a> [<a href="http://arxiv.org/pdf/2006.13009" target="_blank">pdf</a>]

<h2>Calibration of Shared Equilibria in General Sum Partially Observable Markov Games. (arXiv:2006.13085v5 [cs.MA] UPDATED)</h2>
<h3>Nelson Vadori, Sumitra Ganesh, Prashant Reddy, Manuela Veloso</h3>
<p>Training multi-agent systems (MAS) to achieve realistic equilibria gives us a
useful tool to understand and model real-world systems. We consider a general
sum partially observable Markov game where agents of different types share a
single policy network, conditioned on agent-specific information. This paper
aims at i) formally understanding equilibria reached by such agents, and ii)
matching emergent phenomena of such equilibria to real-world targets. Parameter
sharing with decentralized execution has been introduced as an efficient way to
train multiple agents using a single policy network. However, the nature of
resulting equilibria reached by such agents has not been yet studied: we
introduce the novel concept of Shared equilibrium as a symmetric pure Nash
equilibrium of a certain Functional Form Game (FFG) and prove convergence to
the latter for a certain class of games using self-play. In addition, it is
important that such equilibria satisfy certain constraints so that MAS are
calibrated to real world data for practical use: we solve this problem by
introducing a novel dual-Reinforcement Learning based approach that fits
emergent behaviors of agents in a Shared equilibrium to externally-specified
targets, and apply our methods to a n-player market example. We do so by
calibrating parameters governing distributions of agent types rather than
individual agents, which allows both behavior differentiation among agents and
coherent scaling of the shared policy network to multiple agents.
</p>
<a href="http://arxiv.org/abs/2006.13085" target="_blank">arXiv:2006.13085</a> [<a href="http://arxiv.org/pdf/2006.13085" target="_blank">pdf</a>]

<h2>Graph Policy Network for Transferable Active Learning on Graphs. (arXiv:2006.13463v2 [cs.LG] UPDATED)</h2>
<h3>Shengding Hu, Zheng Xiong, Meng Qu, Xingdi Yuan, Marc-Alexandre C&#xf4;t&#xe9;, Zhiyuan Liu, Jian Tang</h3>
<p>Graph neural networks (GNNs) have been attracting increasing popularity due
to their simplicity and effectiveness in a variety of fields. However, a large
number of labeled data is generally required to train these networks, which
could be very expensive to obtain in some domains. In this paper, we study
active learning for GNNs, i.e., how to efficiently label the nodes on a graph
to reduce the annotation cost of training GNNs. We formulate the problem as a
sequential decision process on graphs and train a GNN-based policy network with
reinforcement learning to learn the optimal query strategy. By jointly training
on several source graphs with full labels, we learn a transferable active
learning policy which can directly generalize to unlabeled target graphs.
Experimental results on multiple datasets from different domains prove the
effectiveness of the learned policy in promoting active learning performance in
both settings of transferring between graphs in the same domain and across
different domains.
</p>
<a href="http://arxiv.org/abs/2006.13463" target="_blank">arXiv:2006.13463</a> [<a href="http://arxiv.org/pdf/2006.13463" target="_blank">pdf</a>]

<h2>Building powerful and equivariant graph neural networks with structural message-passing. (arXiv:2006.15107v3 [cs.LG] UPDATED)</h2>
<h3>Clement Vignac, Andreas Loukas, Pascal Frossard</h3>
<p>Message-passing has proved to be an effective way to design graph neural
networks, as it is able to leverage both permutation equivariance and an
inductive bias towards learning local structures in order to achieve good
generalization. However, current message-passing architectures have a limited
representation power and fail to learn basic topological properties of graphs.
We address this problem and propose a powerful and equivariant message-passing
framework based on two ideas: first, we propagate a one-hot encoding of the
nodes, in addition to the features, in order to learn a local context matrix
around each node. This matrix contains rich local information about both
features and topology and can eventually be pooled to build node
representations. Second, we propose methods for the parametrization of the
message and update functions that ensure permutation equivariance. Having a
representation that is independent of the specific choice of the one-hot
encoding permits inductive reasoning and leads to better generalization
properties. Experimentally, our model can predict various graph topological
properties on synthetic data more accurately than previous methods and achieves
state-of-the-art results on molecular graph regression on the ZINC dataset.
</p>
<a href="http://arxiv.org/abs/2006.15107" target="_blank">arXiv:2006.15107</a> [<a href="http://arxiv.org/pdf/2006.15107" target="_blank">pdf</a>]

<h2>Statistical-Query Lower Bounds via Functional Gradients. (arXiv:2006.15812v2 [cs.LG] UPDATED)</h2>
<h3>Surbhi Goel, Aravind Gollakota, Adam Klivans</h3>
<p>We give the first statistical-query lower bounds for agnostically learning
any non-polynomial activation with respect to Gaussian marginals (e.g., ReLU,
sigmoid, sign). For the specific problem of ReLU regression (equivalently,
agnostically learning a ReLU), we show that any statistical-query algorithm
with tolerance $n^{-(1/\epsilon)^b}$ must use at least $2^{n^c} \epsilon$
queries for some constant $b, c &gt; 0$, where $n$ is the dimension and $\epsilon$
is the accuracy parameter. Our results rule out general (as opposed to
correlational) SQ learning algorithms, which is unusual for real-valued
learning problems. Our techniques involve a gradient boosting procedure for
"amplifying" recent lower bounds due to Diakonikolas et al. (COLT 2020) and
Goel et al. (ICML 2020) on the SQ dimension of functions computed by two-layer
neural networks. The crucial new ingredient is the use of a nonstandard convex
functional during the boosting procedure. This also yields a best-possible
reduction between two commonly studied models of learning: agnostic learning
and probabilistic concepts.
</p>
<a href="http://arxiv.org/abs/2006.15812" target="_blank">arXiv:2006.15812</a> [<a href="http://arxiv.org/pdf/2006.15812" target="_blank">pdf</a>]

<h2>Building Rule Hierarchies for Efficient Logical Rule Learning from Knowledge Graphs. (arXiv:2006.16171v3 [cs.AI] UPDATED)</h2>
<h3>Yulong Gu, Yu Guan, Paolo Missier</h3>
<p>Many systems have been developed in recent years to mine logical rules from
large-scale Knowledge Graphs (KGs), on the grounds that representing
regularities as rules enables both the interpretable inference of new facts,
and the explanation of known facts. Among these systems, the walk-based methods
that generate the instantiated rules containing constants by abstracting
sampled paths in KGs demonstrate strong predictive performance and
expressivity. However, due to the large volume of possible rules, these systems
do not scale well where computational resources are often wasted on generating
and evaluating unpromising rules. In this work, we address such scalability
issues by proposing new methods for pruning unpromising rules using rule
hierarchies. The approach consists of two phases. Firstly, since rule
hierarchies are not readily available in walk-based methods, we have built a
Rule Hierarchy Framework (RHF), which leverages a collection of subsumption
frameworks to build a proper rule hierarchy from a set of learned rules. And
secondly, we adapt RHF to an existing rule learner where we design and
implement two methods for Hierarchical Pruning (HPMs), which utilize the
generated hierarchies to remove irrelevant and redundant rules. Through
experiments over four public benchmark datasets, we show that the application
of HPMs is effective in removing unpromising rules, which leads to significant
reductions in the runtime as well as in the number of learned rules, without
compromising the predictive performance.
</p>
<a href="http://arxiv.org/abs/2006.16171" target="_blank">arXiv:2006.16171</a> [<a href="http://arxiv.org/pdf/2006.16171" target="_blank">pdf</a>]

<h2>Introducing Problem Schema with Hierarchical Exercise Graph for Knowledge Tracing. (arXiv:2006.16915v3 [cs.CY] UPDATED)</h2>
<h3>Hanshuang Tong, Yun Zhou, Zhen Wang, Ben Teng</h3>
<p>Knowledge tracing (KT) which aims at predicting learner's knowledge mastery
plays an important role in the computer-aided educational system. In recent
years, many deep learning models have been applied to tackle the KT task, which
has shown promising results. However, limitations still exist. Most existing
methods simplify the exercising records as knowledge sequence, which fails to
explore rich information existed in exercise texts. Besides, the latent
hierarchical graph nature of exercises and knowledge remains unexplored. Thus,
in this paper, we propose a hierarchical graph knowledge tracing model
framework (HGKT) which can leverage the advantages of hierarchical exercise
graph and of sequence model to enhance the ability of knowledge tracing.
Besides, we introduce the concept of problem schema to better represent a group
of similar exercises and propose a hierarchical graph neural network to learn
representations of problem schemas. Moreover, in the sequence model, we employ
two attention mechanisms to highlight important historical states of students.
In the testing stage, we present a K\&amp;S diagnosis matrix that could trace the
transition of mastery of knowledge and problem schema, which can be more easily
applied to different applications. Extensive experiments show the effectiveness
and interpretability of our proposed models.
</p>
<a href="http://arxiv.org/abs/2006.16915" target="_blank">arXiv:2006.16915</a> [<a href="http://arxiv.org/pdf/2006.16915" target="_blank">pdf</a>]

<h2>Improving robustness against common corruptions by covariate shift adaptation. (arXiv:2006.16971v2 [cs.LG] UPDATED)</h2>
<h3>Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, Matthias Bethge</h3>
<p>Today's state-of-the-art machine vision models are vulnerable to image
corruptions like blurring or compression artefacts, limiting their performance
in many real-world applications. We here argue that popular benchmarks to
measure model robustness against common corruptions (like ImageNet-C)
underestimate model robustness in many (but not all) application scenarios. The
key insight is that in many scenarios, multiple unlabeled examples of the
corruptions are available and can be used for unsupervised online adaptation.
Replacing the activation statistics estimated by batch normalization on the
training set with the statistics of the corrupted images consistently improves
the robustness across 25 different popular computer vision models. Using the
corrected statistics, ResNet-50 reaches 62.2% mCE on ImageNet-C compared to
76.7% without adaptation. With the more robust DeepAugment+AugMix model, we
improve the state of the art achieved by a ResNet50 model up to date from 53.6%
mCE to 45.4% mCE. Even adapting to a single sample improves robustness for the
ResNet-50 and AugMix models, and 32 samples are sufficient to improve the
current state of the art for a ResNet-50 architecture. We argue that results
with adapted statistics should be included whenever reporting scores in
corruption benchmarks and other out-of-distribution generalization settings.
</p>
<a href="http://arxiv.org/abs/2006.16971" target="_blank">arXiv:2006.16971</a> [<a href="http://arxiv.org/pdf/2006.16971" target="_blank">pdf</a>]

<h2>Early-Learning Regularization Prevents Memorization of Noisy Labels. (arXiv:2007.00151v2 [cs.LG] UPDATED)</h2>
<h3>Sheng Liu, Jonathan Niles-Weed, Narges Razavian, Carlos Fernandez-Granda</h3>
<p>We propose a novel framework to perform classification via deep learning in
the presence of noisy annotations. When trained on noisy labels, deep neural
networks have been observed to first fit the training data with clean labels
during an "early learning" phase, before eventually memorizing the examples
with false labels. We prove that early learning and memorization are
fundamental phenomena in high-dimensional classification tasks, even in simple
linear models, and give a theoretical explanation in this setting. Motivated by
these findings, we develop a new technique for noisy classification tasks,
which exploits the progress of the early learning phase. In contrast with
existing approaches, which use the model output during early learning to detect
the examples with clean labels, and either ignore or attempt to correct the
false labels, we take a different route and instead capitalize on early
learning via regularization. There are two key elements to our approach. First,
we leverage semi-supervised learning techniques to produce target probabilities
based on the model outputs. Second, we design a regularization term that steers
the model towards these targets, implicitly preventing memorization of the
false labels. The resulting framework is shown to provide robustness to noisy
annotations on several standard benchmarks and real-world datasets, where it
achieves results comparable to the state of the art.
</p>
<a href="http://arxiv.org/abs/2007.00151" target="_blank">arXiv:2007.00151</a> [<a href="http://arxiv.org/pdf/2007.00151" target="_blank">pdf</a>]

<h2>Ultrahyperbolic Representation Learning. (arXiv:2007.00211v2 [cs.LG] UPDATED)</h2>
<h3>Marc T. Law, Jos Stam</h3>
<p>In machine learning, data is usually represented in a (flat) Euclidean space
where distances between points are along straight lines. Researchers have
recently considered more exotic (non-Euclidean) Riemannian manifolds such as
hyperbolic space which is well suited for tree-like data. In this paper, we
propose a representation living on a pseudo-Riemannian manifold of constant
nonzero curvature. It is a generalization of hyperbolic and spherical
geometries where the non-degenerate metric tensor need not be positive
definite. We provide the necessary learning tools in this geometry and extend
gradient method optimization techniques. More specifically, we provide
closed-form expressions for distances via geodesics and define a descent
direction to minimize some objective function. Our novel framework is applied
to graph representations.
</p>
<a href="http://arxiv.org/abs/2007.00211" target="_blank">arXiv:2007.00211</a> [<a href="http://arxiv.org/pdf/2007.00211" target="_blank">pdf</a>]

<h2>Adaptive Discretization for Model-Based Reinforcement Learning. (arXiv:2007.00717v2 [cs.LG] UPDATED)</h2>
<h3>Sean R. Sinclair, Tianyu Wang, Gauri Jain, Siddhartha Banerjee, Christina Lee Yu</h3>
<p>We introduce the technique of adaptive discretization to design an efficient
model-based episodic reinforcement learning algorithm in large (potentially
continuous) state-action spaces. Our algorithm is based on optimistic one-step
value iteration extended to maintain an adaptive discretization of the space.
From a theoretical perspective we provide worst-case regret bounds for our
algorithm which are competitive compared to the state-of-the-art model-based
algorithms. Moreover, our bounds are obtained via a modular proof technique
which can potentially extend to incorporate additional structure on the
problem.

From an implementation standpoint, our algorithm has much lower storage and
computational requirements due to maintaining a more efficient partition of the
state and action spaces. We illustrate this via experiments on several
canonical control problems, which shows that our algorithm empirically performs
significantly better than fixed discretization in terms of both faster
convergence and lower memory usage. Interestingly, we observe empirically that
while fixed-discretization model-based algorithms vastly outperform their
model-free counterparts, the two achieve comparable performance with adaptive
discretization.
</p>
<a href="http://arxiv.org/abs/2007.00717" target="_blank">arXiv:2007.00717</a> [<a href="http://arxiv.org/pdf/2007.00717" target="_blank">pdf</a>]

<h2>CoPhy-PGNN: Learning Physics-guided Neural Networks withCompeting Loss Functions for Solving Eigenvalue Problems. (arXiv:2007.01420v3 [cs.LG] UPDATED)</h2>
<h3>Mohannad Elhamod, Jie Bu, Christopher Singh, Matthew Redell, Abantika Ghosh, Viktor Podolskiy, Wei-Cheng Lee, Anuj Karpatne</h3>
<p>Physics-guided Neural Networks (PGNNs) represent an emerging class of neural
networks that are trained using physics-guided (PG) loss functions (capturing
violations in network outputs with known physics), along with the supervision
contained in data. Existing work in PGNNs have demonstrated the efficacy of
adding single PG loss functions in the neural network objectives, using
constant trade-off parameters, to ensure better generalizability. However, in
the presence of multiple physics loss functions with competing gradient
directions, there is a need to adaptively tune the contribution of competing PG
loss functions during the course of training to arrive at generalizable
solutions. We demonstrate the presence of competing PG losses in the generic
neural network problem of solving for the lowest (or highest) eigenvector of a
physics-based eigenvalue equation, common to many scientific problems. We
present a novel approach to handle competing PG losses and demonstrate its
efficacy in learning generalizable solutions in two motivating applications of
quantum mechanics and electromagnetic propagation. All the code and data used
in this work is available at https://github.com/jayroxis/Cophy-PGNN.
</p>
<a href="http://arxiv.org/abs/2007.01420" target="_blank">arXiv:2007.01420</a> [<a href="http://arxiv.org/pdf/2007.01420" target="_blank">pdf</a>]

<h2>Unsupervised Learning of Lagrangian Dynamics from Images for Prediction and Control. (arXiv:2007.01926v2 [cs.LG] UPDATED)</h2>
<h3>Yaofeng Desmond Zhong, Naomi Ehrich Leonard</h3>
<p>Recent approaches for modelling dynamics of physical systems with neural
networks enforce Lagrangian or Hamiltonian structure to improve prediction and
generalization. However, when coordinates are embedded in high-dimensional data
such as images, these approaches either lose interpretability or can only be
applied to one particular example. We introduce a new unsupervised neural
network model that learns Lagrangian dynamics from images, with
interpretability that benefits prediction and control. The model infers
Lagrangian dynamics on generalized coordinates that are simultaneously learned
with a coordinate-aware variational autoencoder (VAE). The VAE is designed to
account for the geometry of physical systems composed of multiple rigid bodies
in the plane. By inferring interpretable Lagrangian dynamics, the model learns
physical system properties, such as kinetic and potential energy, which enables
long-term prediction of dynamics in the image space and synthesis of
energy-based controllers.
</p>
<a href="http://arxiv.org/abs/2007.01926" target="_blank">arXiv:2007.01926</a> [<a href="http://arxiv.org/pdf/2007.01926" target="_blank">pdf</a>]

<h2>Meta-Learning through Hebbian Plasticity in Random Networks. (arXiv:2007.02686v3 [cs.NE] UPDATED)</h2>
<h3>Elias Najarro, Sebastian Risi</h3>
<p>Lifelong learning and adaptability are two defining aspects of biological
agents. Modern reinforcement learning (RL) approaches have shown significant
progress in solving complex tasks, however once training is concluded, the
found solutions are typically static and incapable of adapting to new
information or perturbations. While it is still not completely understood how
biological brains learn and adapt so efficiently from experience, it is
believed that synaptic plasticity plays a prominent role in this process.
Inspired by this biological mechanism, we propose a search method that, instead
of optimizing the weight parameters of neural networks directly, only searches
for synapse-specific Hebbian learning rules that allow the network to
continuously self-organize its weights during the lifetime of the agent. We
demonstrate our approach on several reinforcement learning tasks with different
sensory modalities and more than 450K trainable plasticity parameters. We find
that starting from completely random weights, the discovered Hebbian rules
enable an agent to navigate a dynamical 2D-pixel environment; likewise they
allow a simulated 3D quadrupedal robot to learn how to walk while adapting to
morphological damage not seen during training and in the absence of any
explicit reward or error signal in less than 100 timesteps. Code is available
at https://github.com/enajx/HebbianMetaLearning.
</p>
<a href="http://arxiv.org/abs/2007.02686" target="_blank">arXiv:2007.02686</a> [<a href="http://arxiv.org/pdf/2007.02686" target="_blank">pdf</a>]

<h2>Learning Differential Equations that are Easy to Solve. (arXiv:2007.04504v2 [cs.LG] UPDATED)</h2>
<h3>Jacob Kelly, Jesse Bettencourt, Matthew James Johnson, David Duvenaud</h3>
<p>Differential equations parameterized by neural networks become expensive to
solve numerically as training progresses. We propose a remedy that encourages
learned dynamics to be easier to solve. Specifically, we introduce a
differentiable surrogate for the time cost of standard numerical solvers, using
higher-order derivatives of solution trajectories. These derivatives are
efficient to compute with Taylor-mode automatic differentiation. Optimizing
this additional objective trades model performance against the time cost of
solving the learned dynamics. We demonstrate our approach by training
substantially faster, while nearly as accurate, models in supervised
classification, density estimation, and time-series modelling tasks.
</p>
<a href="http://arxiv.org/abs/2007.04504" target="_blank">arXiv:2007.04504</a> [<a href="http://arxiv.org/pdf/2007.04504" target="_blank">pdf</a>]

<h2>Migratable AI: Effect of identity and information migration on users perception of conversational AI agents. (arXiv:2007.05801v2 [cs.CY] UPDATED)</h2>
<h3>Ravi Tejwani, Felipe Moreno, Sooyeon Jeong, Hae Won Park, Cynthia Breazeal</h3>
<p>Conversational AI agents are proliferating, embodying a range of devices such
as smart speakers, smart displays, robots, cars, and more. We can envision a
future where a personal conversational agent could migrate across different
form factors and environments to always accompany and assist its user to
support a far more continuous, personalized, and collaborative experience. This
opens the question of what properties of a conversational AI agent migrates
across forms, and how it would impact user perception. To explore this, we
developed a Migratable AI system where a user's information and/or the agent's
identity can be preserved as it migrates across form factors to help its user
with a task. We designed a 2x2 between-subjects study to explore the effects of
information migration and identity migration on user perceptions of trust,
competence, likeability, and social presence. Our results suggest that identity
migration had a positive effect on trust, competence, and social presence,
while information migration had a positive effect on trust, competence, and
likeability. Overall, users report the highest trust, competence, likeability,
and social presence towards the conversational agent when both identity and
information were migrated across embodiments.
</p>
<a href="http://arxiv.org/abs/2007.05801" target="_blank">arXiv:2007.05801</a> [<a href="http://arxiv.org/pdf/2007.05801" target="_blank">pdf</a>]

<h2>AdvFlow: Inconspicuous Black-box Adversarial Attacks using Normalizing Flows. (arXiv:2007.07435v2 [cs.LG] UPDATED)</h2>
<h3>Hadi M. Dolatabadi, Sarah Erfani, Christopher Leckie</h3>
<p>Deep learning classifiers are susceptible to well-crafted, imperceptible
variations of their inputs, known as adversarial attacks. In this regard, the
study of powerful attack models sheds light on the sources of vulnerability in
these classifiers, hopefully leading to more robust ones. In this paper, we
introduce AdvFlow: a novel black-box adversarial attack method on image
classifiers that exploits the power of normalizing flows to model the density
of adversarial examples around a given target image. We see that the proposed
method generates adversaries that closely follow the clean data distribution, a
property which makes their detection less likely. Also, our experimental
results show competitive performance of the proposed approach with some of the
existing attack methods on defended classifiers. The code is available at
https://github.com/hmdolatabadi/AdvFlow.
</p>
<a href="http://arxiv.org/abs/2007.07435" target="_blank">arXiv:2007.07435</a> [<a href="http://arxiv.org/pdf/2007.07435" target="_blank">pdf</a>]

<h2>Memory based fusion for multi-modal deep learning. (arXiv:2007.08076v3 [cs.LG] UPDATED)</h2>
<h3>Darshana Priyasad, Tharindu Fernando, Simon Denman, Sridha Sridharan, Clinton Fookes</h3>
<p>The use of multi-modal data for deep machine learning has shown promise when
compared to uni-modal approaches with fusion of multi-modal features resulting
in improved performance in several applications. However, most state-of-the-art
methods use naive fusion which processes feature streams independently,
ignoring possible long-term dependencies within the data during fusion. In this
paper, we present a novel Memory based Attentive Fusion layer, which fuses
modes by incorporating both the current features and longterm dependencies in
the data, thus allowing the model to understand the relative importance of
modes over time. We introduce an explicit memory block within the fusion layer
which stores features containing long-term dependencies of the fused data. The
feature inputs from uni-modal encoders are fused through attentive composition
and transformation followed by naive fusion of the resultant memory derived
features with layer inputs. Following state-of-the-art methods, we have
evaluated the performance and the generalizability of the proposed fusion
approach on two different datasets with different modalities. In our
experiments, we replace the naive fusion layer in benchmark networks with our
proposed layer to enable a fair comparison. Experimental results indicate that
the MBAF layer can generalise across different modalities and networks to
enhance fusion and improve performance.
</p>
<a href="http://arxiv.org/abs/2007.08076" target="_blank">arXiv:2007.08076</a> [<a href="http://arxiv.org/pdf/2007.08076" target="_blank">pdf</a>]

<h2>Provably Consistent Partial-Label Learning. (arXiv:2007.08929v2 [cs.LG] UPDATED)</h2>
<h3>Lei Feng, Jiaqi Lv, Bo Han, Miao Xu, Gang Niu, Xin Geng, Bo An, Masashi Sugiyama</h3>
<p>Partial-label learning (PLL) is a multi-class classification problem, where
each training example is associated with a set of candidate labels. Even though
many practical PLL methods have been proposed in the last two decades, there
lacks a theoretical understanding of the consistency of those methods-none of
the PLL methods hitherto possesses a generation process of candidate label
sets, and then it is still unclear why such a method works on a specific
dataset and when it may fail given a different dataset. In this paper, we
propose the first generation model of candidate label sets, and develop two
novel PLL methods that are guaranteed to be provably consistent, i.e., one is
risk-consistent and the other is classifier-consistent. Our methods are
advantageous, since they are compatible with any deep network or stochastic
optimizer. Furthermore, thanks to the generation model, we would be able to
answer the two questions above by testing if the generation model matches given
candidate label sets. Experiments on benchmark and real-world datasets validate
the effectiveness of the proposed generation model and two PLL methods.
</p>
<a href="http://arxiv.org/abs/2007.08929" target="_blank">arXiv:2007.08929</a> [<a href="http://arxiv.org/pdf/2007.08929" target="_blank">pdf</a>]

<h2>Probabilistic Active Meta-Learning. (arXiv:2007.08949v2 [cs.LG] UPDATED)</h2>
<h3>Jean Kaddour, Steind&#xf3;r S&#xe6;mundsson, Marc Peter Deisenroth</h3>
<p>Data-efficient learning algorithms are essential in many practical
applications where data collection is expensive, e.g., in robotics due to the
wear and tear. To address this problem, meta-learning algorithms use prior
experience about tasks to learn new, related tasks efficiently. Typically, a
set of training tasks is assumed given or randomly chosen. However, this
setting does not take into account the sequential nature that naturally arises
when training a model from scratch in real-life: how do we collect a set of
training tasks in a data-efficient manner? In this work, we introduce task
selection based on prior experience into a meta-learning algorithm by
conceptualizing the learner and the active meta-learning setting using a
probabilistic latent variable model. We provide empirical evidence that our
approach improves data-efficiency when compared to strong baselines on
simulated robotic experiments.
</p>
<a href="http://arxiv.org/abs/2007.08949" target="_blank">arXiv:2007.08949</a> [<a href="http://arxiv.org/pdf/2007.08949" target="_blank">pdf</a>]

<h2>Continuous Object Representation Networks: Novel View Synthesis without Target View Supervision. (arXiv:2007.15627v2 [cs.CV] UPDATED)</h2>
<h3>Nicolai H&#xe4;ni, Selim Engin, Jun-Jee Chao, Volkan Isler</h3>
<p>Novel View Synthesis (NVS) is concerned with synthesizing views under camera
viewpoint transformations from one or multiple input images. NVS requires
explicit reasoning about 3D object structure and unseen parts of the scene to
synthesize convincing results. As a result, current approaches typically rely
on supervised training with either ground truth 3D models or multiple target
images. We propose Continuous Object Representation Networks (CORN), a
conditional architecture that encodes an input image's geometry and appearance
that map to a 3D consistent scene representation. We can train CORN with only
two source images per object by combining our model with a neural renderer. A
key feature of CORN is that it requires no ground truth 3D models or target
view supervision. Regardless, CORN performs well on challenging tasks such as
novel view synthesis and single-view 3D reconstruction and achieves performance
comparable to state-of-the-art approaches that use direct supervision. For
up-to-date information, data, and code, please see our project page:
https://nicolaihaeni.github.io/corn/.
</p>
<a href="http://arxiv.org/abs/2007.15627" target="_blank">arXiv:2007.15627</a> [<a href="http://arxiv.org/pdf/2007.15627" target="_blank">pdf</a>]

<h2>Disentangling Human Error from the Ground Truth in Segmentation of Medical Images. (arXiv:2007.15963v5 [cs.CV] UPDATED)</h2>
<h3>Le Zhang, Ryutaro Tanno, Mou-Cheng Xu, Chen Jin, Joseph Jacob, Olga Ciccarelli, Frederik Barkhof, Daniel C. Alexander</h3>
<p>Recent years have seen increasing use of supervised learning methods for
segmentation tasks. However, the predictive performance of these algorithms
depends on the quality of labels. This problem is particularly pertinent in the
medical image domain, where both the annotation cost and inter-observer
variability are high. In a typical label acquisition process, different human
experts provide their estimates of the "true" segmentation labels under the
influence of their own biases and competence levels. Treating these noisy
labels blindly as the ground truth limits the performance that automatic
segmentation algorithms can achieve. In this work, we present a method for
jointly learning, from purely noisy observations alone, the reliability of
individual annotators and the true segmentation label distributions, using two
coupled CNNs. The separation of the two is achieved by encouraging the
estimated annotators to be maximally unreliable while achieving high fidelity
with the noisy training data. We first define a toy segmentation dataset based
on MNIST and study the properties of the proposed algorithm. We then
demonstrate the utility of the method on three public medical imaging
segmentation datasets with simulated (when necessary) and real diverse
annotations: 1) MSLSC (multiple-sclerosis lesions); 2) BraTS (brain tumours);
3) LIDC-IDRI (lung abnormalities). In all cases, our method outperforms
competing methods and relevant baselines particularly in cases where the number
of annotations is small and the amount of disagreement is large. The
experiments also show strong ability to capture the complex spatial
characteristics of annotators' mistakes.
</p>
<a href="http://arxiv.org/abs/2007.15963" target="_blank">arXiv:2007.15963</a> [<a href="http://arxiv.org/pdf/2007.15963" target="_blank">pdf</a>]

<h2>A Spectral Energy Distance for Parallel Speech Synthesis. (arXiv:2008.01160v2 [eess.AS] UPDATED)</h2>
<h3>Alexey A. Gritsenko, Tim Salimans, Rianne van den Berg, Jasper Snoek, Nal Kalchbrenner</h3>
<p>Speech synthesis is an important practical generative modeling problem that
has seen great progress over the last few years, with likelihood-based
autoregressive neural models now outperforming traditional concatenative
systems. A downside of such autoregressive models is that they require
executing tens of thousands of sequential operations per second of generated
audio, making them ill-suited for deployment on specialized deep learning
hardware. Here, we propose a new learning method that allows us to train highly
parallel models of speech, without requiring access to an analytical likelihood
function. Our approach is based on a generalized energy distance between the
distributions of the generated and real audio. This spectral energy distance is
a proper scoring rule with respect to the distribution over
magnitude-spectrograms of the generated waveform audio and offers statistical
consistency guarantees. The distance can be calculated from minibatches without
bias, and does not involve adversarial learning, yielding a stable and
consistent method for training implicit generative models. Empirically, we
achieve state-of-the-art generation quality among implicit generative models,
as judged by the recently-proposed cFDSD metric. When combining our method with
adversarial techniques, we also improve upon the recently-proposed GAN-TTS
model in terms of Mean Opinion Score as judged by trained human evaluators.
</p>
<a href="http://arxiv.org/abs/2008.01160" target="_blank">arXiv:2008.01160</a> [<a href="http://arxiv.org/pdf/2008.01160" target="_blank">pdf</a>]

<h2>Neural Complexity Measures. (arXiv:2008.02953v2 [cs.LG] UPDATED)</h2>
<h3>Yoonho Lee, Juho Lee, Sung Ju Hwang, Eunho Yang, Seungjin Choi</h3>
<p>While various complexity measures for deep neural networks exist, specifying
an appropriate measure capable of predicting and explaining generalization in
deep networks has proven challenging. We propose Neural Complexity (NC), a
meta-learning framework for predicting generalization. Our model learns a
scalar complexity measure through interactions with many heterogeneous tasks in
a data-driven way. The trained NC model can be added to the standard training
loss to regularize any task learner in a standard supervised learning scenario.
We contrast NC's approach against existing manually-designed complexity
measures and other meta-learning models, and we validate NC's performance on
multiple regression and classification tasks
</p>
<a href="http://arxiv.org/abs/2008.02953" target="_blank">arXiv:2008.02953</a> [<a href="http://arxiv.org/pdf/2008.02953" target="_blank">pdf</a>]

<h2>Joint Policy Search for Multi-agent Collaboration with Imperfect Information. (arXiv:2008.06495v4 [cs.LG] UPDATED)</h2>
<h3>Yuandong Tian, Qucheng Gong, Tina Jiang</h3>
<p>To learn good joint policies for multi-agent collaboration with imperfect
information remains a fundamental challenge. While for two-player zero-sum
games, coordinate-ascent approaches (optimizing one agent's policy at a time,
e.g., self-play) work with guarantees, in multi-agent cooperative setting they
often converge to sub-optimal Nash equilibrium. On the other hand, directly
modeling joint policy changes in imperfect information game is nontrivial due
to complicated interplay of policies (e.g., upstream updates affect downstream
state reachability). In this paper, we show global changes of game values can
be decomposed to policy changes localized at each information set, with a novel
term named policy-change density. Based on this, we propose Joint Policy
Search(JPS) that iteratively improves joint policies of collaborative agents in
imperfect information games, without re-evaluating the entire game. On
multi-agent collaborative tabular games, JPS is proven to never worsen
performance and can improve solutions provided by unilateral approaches (e.g,
CFR), outperforming algorithms designed for collaborative policy learning (e.g.
BAD). Furthermore, for real-world games, JPS has an online form that naturally
links with gradient updates. We test it to Contract Bridge, a 4-player
imperfect-information game where a team of $2$ collaborates to compete against
the other. In its bidding phase, players bid in turn to find a good contract
through a limited information channel. Based on a strong baseline agent that
bids competitive bridge purely through domain-agnostic self-play, JPS improves
collaboration of team players and outperforms WBridge5, a championship-winning
software, by $+0.63$ IMPs (International Matching Points) per board over 1k
games, substantially better than previous SoTA ($+0.41$ IMPs/b) under
Double-Dummy evaluation.
</p>
<a href="http://arxiv.org/abs/2008.06495" target="_blank">arXiv:2008.06495</a> [<a href="http://arxiv.org/pdf/2008.06495" target="_blank">pdf</a>]

<h2>HiPPO: Recurrent Memory with Optimal Polynomial Projections. (arXiv:2008.07669v2 [cs.LG] UPDATED)</h2>
<h3>Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, Christopher Re</h3>
<p>A central problem in learning from sequential data is representing cumulative
history in an incremental fashion as more data is processed. We introduce a
general framework (HiPPO) for the online compression of continuous signals and
discrete time series by projection onto polynomial bases. Given a measure that
specifies the importance of each time step in the past, HiPPO produces an
optimal solution to a natural online function approximation problem. As special
cases, our framework yields a short derivation of the recent Legendre Memory
Unit (LMU) from first principles, and generalizes the ubiquitous gating
mechanism of recurrent neural networks such as GRUs. This formal framework
yields a new memory update mechanism (HiPPO-LegS) that scales through time to
remember all history, avoiding priors on the timescale. HiPPO-LegS enjoys the
theoretical benefits of timescale robustness, fast updates, and bounded
gradients. By incorporating the memory dynamics into recurrent neural networks,
HiPPO RNNs can empirically capture complex temporal dependencies. On the
benchmark permuted MNIST dataset, HiPPO-LegS sets a new state-of-the-art
accuracy of 98.3%. Finally, on a novel trajectory classification task testing
robustness to out-of-distribution timescales and missing data, HiPPO-LegS
outperforms RNN and neural ODE baselines by 25-40% accuracy.
</p>
<a href="http://arxiv.org/abs/2008.07669" target="_blank">arXiv:2008.07669</a> [<a href="http://arxiv.org/pdf/2008.07669" target="_blank">pdf</a>]

<h2>Residual Learning from Demonstration: Adapting Dynamic Movement Primitives for Contact-rich Insertion Tasks. (arXiv:2008.07682v2 [cs.RO] UPDATED)</h2>
<h3>Todor Davchev, Kevin Sebastian Luck, Michael Burke, Franziska Meier, Stefan Schaal, Subramanian Ramamoorthy</h3>
<p>Contacts and friction are inherent to nearly all robotic manipulation tasks.
Through the motor skill of insertion, we study how robots can learn to cope
when these attributes play a salient role. In this work we study ways for
adapting dynamic movement primitives (DMP) to improve their performance in the
context of contact rich insertion. We propose a framework we refer to as
residual learning from demonstration (rLfD) that combines dynamic movement
primitives (DMP) that rely on behavioural cloning with a reinforcement learning
(RL) based residual correction policy. Our evaluation suggests that applying
residual learning directly in task space and operating on the full pose of the
robot can significantly improve the overall performance of DMPs. We show that
rLfD outperforms alternatives and improves the generalisation abilities of
DMPs. We evaluate this approach by training an agent to successfully perform
both simulated and real world insertions of pegs, gears and plugs into
respective sockets.
</p>
<a href="http://arxiv.org/abs/2008.07682" target="_blank">arXiv:2008.07682</a> [<a href="http://arxiv.org/pdf/2008.07682" target="_blank">pdf</a>]

<h2>Transductive Information Maximization For Few-Shot Learning. (arXiv:2008.11297v3 [cs.LG] UPDATED)</h2>
<h3>Malik Boudiaf, Ziko Imtiaz Masud, J&#xe9;r&#xf4;me Rony, Jos&#xe9; Dolz, Pablo Piantanida, Ismail Ben Ayed</h3>
<p>We introduce Transductive Infomation Maximization (TIM) for few-shot
learning. Our method maximizes the mutual information between the query
features and their label predictions for a given few-shot task, in conjunction
with a supervision loss based on the support set. Furthermore, we propose a new
alternating-direction solver for our mutual-information loss, which
substantially speeds up transductive-inference convergence over gradient-based
optimization, while yielding similar accuracy. TIM inference is modular: it can
be used on top of any base-training feature extractor. Following standard
transductive few-shot settings, our comprehensive experiments demonstrate that
TIM outperforms state-of-the-art methods significantly across various datasets
and networks, while used on top of a fixed feature extractor trained with
simple cross-entropy on the base classes, without resorting to complex
meta-learning schemes. It consistently brings between 2% and 5% improvement in
accuracy over the best performing method, not only on all the well-established
few-shot benchmarks but also on more challenging scenarios,with domain shifts
and larger numbers of classes.
</p>
<a href="http://arxiv.org/abs/2008.11297" target="_blank">arXiv:2008.11297</a> [<a href="http://arxiv.org/pdf/2008.11297" target="_blank">pdf</a>]

<h2>Multi-Attention-Network for Semantic Segmentation of High-Resolution Remote Sensing Images. (arXiv:2009.02130v3 [eess.IV] UPDATED)</h2>
<h3>Rui Li, Shunyi Zheng, Chenxi Duan, Ce Zhang, Jianlin Su</h3>
<p>Semantic segmentation of remote sensing images plays an important role in
land resource management, yield estimation, and economic assessment. Even
though the semantic segmentation of remote sensing images has been prominently
improved by convolutional neural networks, there are still several limitations
contained in standard models. First, for encoder-decoder architectures like
U-Net, the utilization of multi-scale features causes overuse of information,
where similar low-level features are exploited at multiple scales for multiple
times. Second, long-range dependencies of feature maps are not sufficiently
explored, leading to feature representations associated with each semantic
class are not optimal. Third, despite the dot-product attention mechanism has
been introduced and harnessed widely in semantic segmentation to model
long-range dependencies, the high time and space complexities of attention
impede the usage of attention in application scenarios with large input. In
this paper, we proposed a Multi-Attention-Network (MANet) to remedy these
drawbacks, which extracts contextual dependencies by multi efficient attention
mechanisms. A novel attention mechanism named kernel attention with linear
complexity is proposed to alleviate the high computational demand of attention.
Based on kernel attention and channel attention, we integrate local feature
maps extracted by ResNeXt-101 with their corresponding global dependencies, and
adaptively signalize interdependent channel maps. Experiments conducted on two
remote sensing image datasets captured by variant satellites demonstrate that
the performance of our MANet transcends the DeepLab V3+, PSPNet, FastFCN, and
other baseline algorithms.
</p>
<a href="http://arxiv.org/abs/2009.02130" target="_blank">arXiv:2009.02130</a> [<a href="http://arxiv.org/pdf/2009.02130" target="_blank">pdf</a>]

<h2>Estimating Individual Treatment Effects using Non-Parametric Regression Models: a Review. (arXiv:2009.06472v2 [stat.ME] UPDATED)</h2>
<h3>Alberto Caron, Ioanna Manolopoulou, Gianluca Baio</h3>
<p>Large observational data are increasingly available in disciplines such as
health, economic and social sciences, where researchers are interested in
causal questions rather than prediction. In this paper, we investigate the
problem of estimating heterogeneous treatment effects using non-parametric
regression-based methods. Firstly, we introduce the setup and the issues
related to conducting causal inference with observational or non-fully
randomized data, and how these issues can be tackled with the help of
statistical learning tools. Then, we provide a review of state-of-the-art
methods, with a particular focus on non-parametric modeling, and we cast them
under a unifying taxonomy. After presenting a brief overview on the problem of
model selection, we illustrate the performance of some of the methods on three
different simulated studies and on a real world example to investigate the
effect of participation in school meal programs on health indicators.
</p>
<a href="http://arxiv.org/abs/2009.06472" target="_blank">arXiv:2009.06472</a> [<a href="http://arxiv.org/pdf/2009.06472" target="_blank">pdf</a>]

<h2>Competing AI: How does competition feedback affect machine learning?. (arXiv:2009.06797v3 [cs.LG] UPDATED)</h2>
<h3>Antonio Ginart, Eva Zhang, Yongchan Kwon, James Zou</h3>
<p>This papers studies how competition affects machine learning (ML) predictors.
As ML becomes more ubiquitous, it is often deployed by companies to compete
over customers. For example, digital platforms like Yelp use ML to predict user
preference and make recommendations. A service that is more often queried by
users, perhaps because it more accurately anticipates user preferences, is also
more likely to obtain additional user data (e.g. in the form of a Yelp review).
Thus, competing predictors cause feedback loops whereby a predictor's
performance impacts what training data it receives and biases its predictions
over time. We introduce a flexible model of competing ML predictors that
enables both rapid experimentation and theoretical tractability. We show with
empirical and mathematical analysis that competition causes predictors to
specialize for specific sub-populations at the cost of worse performance over
the general population. We further analyze the impact of predictor
specialization on the overall prediction quality experienced by users. We show
that having too few or too many competing predictors in a market can hurt the
overall prediction quality. Our theory is complemented by experiments on
several real datasets using popular learning algorithms, such as neural
networks and nearest neighbor methods.
</p>
<a href="http://arxiv.org/abs/2009.06797" target="_blank">arXiv:2009.06797</a> [<a href="http://arxiv.org/pdf/2009.06797" target="_blank">pdf</a>]

<h2>Certifying Confidence via Randomized Smoothing. (arXiv:2009.08061v2 [cs.LG] UPDATED)</h2>
<h3>Aounon Kumar, Alexander Levine, Soheil Feizi, Tom Goldstein</h3>
<p>Randomized smoothing has been shown to provide good certified-robustness
guarantees for high-dimensional classification problems. It uses the
probabilities of predicting the top two most-likely classes around an input
point under a smoothing distribution to generate a certified radius for a
classifier's prediction. However, most smoothing methods do not give us any
information about the confidence with which the underlying classifier (e.g.,
deep neural network) makes a prediction. In this work, we propose a method to
generate certified radii for the prediction confidence of the smoothed
classifier. We consider two notions for quantifying confidence: average
prediction score of a class and the margin by which the average prediction
score of one class exceeds that of another. We modify the Neyman-Pearson lemma
(a key theorem in randomized smoothing) to design a procedure for computing the
certified radius where the confidence is guaranteed to stay above a certain
threshold. Our experimental results on CIFAR-10 and ImageNet datasets show that
using information about the distribution of the confidence scores allows us to
achieve a significantly better certified radius than ignoring it. Thus, we
demonstrate that extra information about the base classifier at the input point
can help improve certified guarantees for the smoothed classifier. Code for the
experiments is available at https://github.com/aounon/cdf-smoothing.
</p>
<a href="http://arxiv.org/abs/2009.08061" target="_blank">arXiv:2009.08061</a> [<a href="http://arxiv.org/pdf/2009.08061" target="_blank">pdf</a>]

<h2>Semi-supervised Semantic Segmentation of Prostate and Organs-at-Risk on 3D Pelvic CT Images. (arXiv:2009.09571v2 [cs.CV] UPDATED)</h2>
<h3>Zhuangzhuang Zhang, Tianyu Zhao, Hiram Gay, Baozhou Sun, Weixiong Zhang</h3>
<p>Automated segmentation of organs-at-risk (OARs) in pelvic computed tomography
(CT) images can assist radiotherapy treatment planning by saving efforts for
manual contouring and reducing intra-observer and inter-observer variations.
However, training effective deep-learning segmentation models usually require a
sufficient amount of high-quality labeled data, which are costly to collect.
Taking automated segmentation of OARs as a case study, we developed a novel
semi-supervised adversarial deep learning approach to support medical image
data processing and modeling where training data may be insufficient. Unlike
supervised deep learning methods, our new approach can utilize both annotated
and un-annotated data for training. Additionally, the new approach can generate
un-annotated data by the generative adversarial networks (GANs) aided data
augmentation scheme. We applied the new approach to segmenting tumors and
multiple OARs in male pelvic CT images. The new approach was evaluated on a
dataset of 100 training cases and 20 testing cases. Experimental results,
including four metrics (dice similarity coefficient, average Hausdorff
distance, average surface Hausdorff distance, and relative volume difference),
showed that the new method can achieve comparable performance with less
annotated data and better performance with the same amount of annotated data.
The performance of the new approach and its 3D Pelvic CT segmentation model
achieved comparable or better performance than other state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2009.09571" target="_blank">arXiv:2009.09571</a> [<a href="http://arxiv.org/pdf/2009.09571" target="_blank">pdf</a>]

<h2>Understanding Approximate Fisher Information for Fast Convergence of Natural Gradient Descent in Wide Neural Networks. (arXiv:2010.00879v2 [stat.ML] UPDATED)</h2>
<h3>Ryo Karakida, Kazuki Osawa</h3>
<p>Natural Gradient Descent (NGD) helps to accelerate the convergence of
gradient descent dynamics, but it requires approximations in large-scale deep
neural networks because of its high computational cost. Empirical studies have
confirmed that some NGD methods with approximate Fisher information converge
sufficiently fast in practice. Nevertheless, it remains unclear from the
theoretical perspective why and under what conditions such heuristic
approximations work well. In this work, we reveal that, under specific
conditions, NGD with approximate Fisher information achieves the same fast
convergence to global minima as exact NGD. We consider deep neural networks in
the infinite-width limit, and analyze the asymptotic training dynamics of NGD
in function space via the neural tangent kernel. In the function space, the
training dynamics with the approximate Fisher information are identical to
those with the exact Fisher information, and they converge quickly. The fast
convergence holds in layer-wise approximations; for instance, in block diagonal
approximation where each block corresponds to a layer as well as in block
tri-diagonal and K-FAC approximations. We also find that a unit-wise
approximation achieves the same fast convergence under some assumptions. All of
these different approximations have an isotropic gradient in the function
space, and this plays a fundamental role in achieving the same convergence
properties in training. Thus, the current study gives a novel and unified
theoretical foundation with which to understand NGD methods in deep learning.
</p>
<a href="http://arxiv.org/abs/2010.00879" target="_blank">arXiv:2010.00879</a> [<a href="http://arxiv.org/pdf/2010.00879" target="_blank">pdf</a>]

<h2>Goal-directed Generation of Discrete Structures with Conditional Generative Models. (arXiv:2010.02311v2 [cs.LG] UPDATED)</h2>
<h3>Amina Mollaysa, Brooks Paige, Alexandros Kalousis</h3>
<p>Despite recent advances, goal-directed generation of structured discrete data
remains challenging. For problems such as program synthesis (generating source
code) and materials design (generating molecules), finding examples which
satisfy desired constraints or exhibit desired properties is difficult. In
practice, expensive heuristic search or reinforcement learning algorithms are
often employed. In this paper we investigate the use of conditional generative
models which directly attack this inverse problem, by modeling the distribution
of discrete structures given properties of interest. Unfortunately, maximum
likelihood training of such models often fails with the samples from the
generative model inadequately respecting the input properties. To address this,
we introduce a novel approach to directly optimize a reinforcement learning
objective, maximizing an expected reward. We avoid high-variance score-function
estimators that would otherwise be required by sampling from an approximation
to the normalized rewards, allowing simple Monte Carlo estimation of model
gradients. We test our methodology on two tasks: generating molecules with
user-defined properties and identifying short python expressions which evaluate
to a given target value. In both cases, we find improvements over maximum
likelihood estimation and other baselines.
</p>
<a href="http://arxiv.org/abs/2010.02311" target="_blank">arXiv:2010.02311</a> [<a href="http://arxiv.org/pdf/2010.02311" target="_blank">pdf</a>]

<h2>Knowledge-enriched, Type-constrained and Grammar-guided Question Generation over Knowledge Bases. (arXiv:2010.03157v3 [cs.CL] UPDATED)</h2>
<h3>Sheng Bi, Xiya Cheng, Yuan-Fang Li, Yongzhen Wang, Guilin Qi</h3>
<p>Question generation over knowledge bases (KBQG) aims at generating
natural-language questions about a subgraph, i.e. a set of (connected) triples.
Two main challenges still face the current crop of encoder-decoder-based
methods, especially on small subgraphs: (1) low diversity and poor fluency due
to the limited information contained in the subgraphs, and (2) semantic drift
due to the decoder's oblivion of the semantics of the answer entity. We propose
an innovative knowledge-enriched, type-constrained and grammar-guided KBQG
model, named KTG, to addresses the above challenges. In our model, the encoder
is equipped with auxiliary information from the KB, and the decoder is
constrained with word types during QG. Specifically, entity domain and
description, as well as relation hierarchy information are considered to
construct question contexts, while a conditional copy mechanism is incorporated
to modulate question semantics according to current word types. Besides, a
novel reward function featuring grammatical similarity is designed to improve
both generative richness and syntactic correctness via reinforcement learning.
Extensive experiments show that our proposed model outperforms existing methods
by a significant margin on two widely-used benchmark datasets SimpleQuestion
and PathQuestion.
</p>
<a href="http://arxiv.org/abs/2010.03157" target="_blank">arXiv:2010.03157</a> [<a href="http://arxiv.org/pdf/2010.03157" target="_blank">pdf</a>]

<h2>Rotation-Invariant Local-to-Global Representation Learning for 3D Point Cloud. (arXiv:2010.03318v3 [cs.CV] UPDATED)</h2>
<h3>Seohyun Kim, Jaeyoo Park, Bohyung Han</h3>
<p>We propose a local-to-global representation learning algorithm for 3D point
cloud data, which is appropriate to handle various geometric transformations,
especially rotation, without explicit data augmentation with respect to the
transformations. Our model takes advantage of multi-level abstraction based on
graph convolutional neural networks, which constructs a descriptor hierarchy to
encode rotation-invariant shape information of an input object in a bottom-up
manner. The descriptors in each level are obtained from a neural network based
on a graph via stochastic sampling of 3D points, which is effective in making
the learned representations robust to the variations of input data. The
proposed algorithm presents the state-of-the-art performance on the
rotation-augmented 3D object recognition and segmentation benchmarks, and we
further analyze its characteristics through comprehensive ablative experiments.
</p>
<a href="http://arxiv.org/abs/2010.03318" target="_blank">arXiv:2010.03318</a> [<a href="http://arxiv.org/pdf/2010.03318" target="_blank">pdf</a>]

<h2>Nearly Minimax Optimal Reward-free Reinforcement Learning. (arXiv:2010.05901v2 [cs.LG] UPDATED)</h2>
<h3>Zihan Zhang, Simon S. Du, Xiangyang Ji</h3>
<p>We study the reward-free reinforcement learning framework, which is
particularly suitable for batch reinforcement learning and scenarios where one
needs policies for multiple reward functions. This framework has two phases. In
the exploration phase, the agent collects trajectories by interacting with the
environment without using any reward signal. In the planning phase, the agent
needs to return a near-optimal policy for arbitrary reward functions. We give a
new efficient algorithm, \textbf{S}taged \textbf{S}ampling + \textbf{T}runcated
\textbf{P}lanning (\algoname), which interacts with the environment at most
$O\left(
\frac{S^2A}{\epsilon^2}\text{poly}\log\left(\frac{SAH}{\epsilon}\right)
\right)$ episodes in the exploration phase, and guarantees to output a
near-optimal policy for arbitrary reward functions in the planning phase. Here,
$S$ is the size of state space, $A$ is the size of action space, $H$ is the
planning horizon, and $\epsilon$ is the target accuracy relative to the total
reward. Notably, our sample complexity scales only \emph{logarithmically} with
$H$, in contrast to all existing results which scale \emph{polynomially} with
$H$. Furthermore, this bound matches the minimax lower bound
$\Omega\left(\frac{S^2A}{\epsilon^2}\right)$ up to logarithmic factors.

Our results rely on three new techniques : 1) A new sufficient condition for
the dataset to plan for an $\epsilon$-suboptimal policy; 2) A new way to plan
efficiently under the proposed condition using soft-truncated planning; 3)
Constructing extended MDP to maximize the truncated accumulative rewards
efficiently.
</p>
<a href="http://arxiv.org/abs/2010.05901" target="_blank">arXiv:2010.05901</a> [<a href="http://arxiv.org/pdf/2010.05901" target="_blank">pdf</a>]

<h2>Cold-start Active Learning through Self-supervised Language Modeling. (arXiv:2010.09535v2 [cs.CL] UPDATED)</h2>
<h3>Michelle Yuan, Hsuan-Tien Lin, Jordan Boyd-Graber</h3>
<p>Active learning strives to reduce annotation costs by choosing the most
critical examples to label. Typically, the active learning strategy is
contingent on the classification model. For instance, uncertainty sampling
depends on poorly calibrated model confidence scores. In the cold-start
setting, active learning is impractical because of model instability and data
scarcity. Fortunately, modern NLP provides an additional source of information:
pre-trained language models. The pre-training loss can find examples that
surprise the model and should be labeled for efficient fine-tuning. Therefore,
we treat the language modeling loss as a proxy for classification uncertainty.
With BERT, we develop a simple strategy based on the masked language modeling
loss that minimizes labeling costs for text classification. Compared to other
baselines, our approach reaches higher accuracy within less sampling iterations
and computation time.
</p>
<a href="http://arxiv.org/abs/2010.09535" target="_blank">arXiv:2010.09535</a> [<a href="http://arxiv.org/pdf/2010.09535" target="_blank">pdf</a>]

<h2>ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction. (arXiv:2010.09885v2 [cs.LG] UPDATED)</h2>
<h3>Seyone Chithrananda, Gabriel Grand, Bharath Ramsundar</h3>
<p>GNNs and chemical fingerprints are the predominant approaches to representing
molecules for property prediction. However, in NLP, transformers have become
the de-facto standard for representation learning thanks to their strong
downstream task transfer. In parallel, the software ecosystem around
transformers is maturing rapidly, with libraries like HuggingFace and BertViz
enabling streamlined training and introspection. In this work, we make one of
the first attempts to systematically evaluate transformers on molecular
property prediction tasks via our ChemBERTa model. ChemBERTa scales well with
pretraining dataset size, offering competitive downstream performance on
MoleculeNet and useful attention-based visualization modalities. Our results
suggest that transformers offer a promising avenue of future work for molecular
representation learning and property prediction. To facilitate these efforts,
we release a curated dataset of 77M SMILES from PubChem suitable for
large-scale self-supervised pretraining.
</p>
<a href="http://arxiv.org/abs/2010.09885" target="_blank">arXiv:2010.09885</a> [<a href="http://arxiv.org/pdf/2010.09885" target="_blank">pdf</a>]

<h2>Sparse Gaussian Process Variational Autoencoders. (arXiv:2010.10177v2 [stat.ML] UPDATED)</h2>
<h3>Matthew Ashman, Jonathan So, Will Tebbutt, Vincent Fortuin, Michael Pearce, Richard E. Turner</h3>
<p>Large, multi-dimensional spatio-temporal datasets are omnipresent in modern
science and engineering. An effective framework for handling such data are
Gaussian process deep generative models (GP-DGMs), which employ GP priors over
the latent variables of DGMs. Existing approaches for performing inference in
GP-DGMs do not support sparse GP approximations based on inducing points, which
are essential for the computational efficiency of GPs, nor do they handle
missing data -- a natural occurrence in many spatio-temporal datasets -- in a
principled manner. We address these shortcomings with the development of the
sparse Gaussian process variational autoencoder (SGP-VAE), characterised by the
use of partial inference networks for parameterising sparse GP approximations.
Leveraging the benefits of amortised variational inference, the SGP-VAE enables
inference in multi-output sparse GPs on previously unobserved data with no
additional training. The SGP-VAE is evaluated in a variety of experiments where
it outperforms alternative approaches including multi-output GPs and structured
VAEs.
</p>
<a href="http://arxiv.org/abs/2010.10177" target="_blank">arXiv:2010.10177</a> [<a href="http://arxiv.org/pdf/2010.10177" target="_blank">pdf</a>]

<h2>Bootleg: Chasing the Tail with Self-Supervised Named Entity Disambiguation. (arXiv:2010.10363v3 [cs.CL] UPDATED)</h2>
<h3>Laurel Orr, Megan Leszczynski, Simran Arora, Sen Wu, Neel Guha, Xiao Ling, Christopher Re</h3>
<p>A challenge for named entity disambiguation (NED), the task of mapping
textual mentions to entities in a knowledge base, is how to disambiguate
entities that appear rarely in the training data, termed tail entities. Humans
use subtle reasoning patterns based on knowledge of entity facts, relations,
and types to disambiguate unfamiliar entities. Inspired by these patterns, we
introduce Bootleg, a self-supervised NED system that is explicitly grounded in
reasoning patterns for disambiguation. We define core reasoning patterns for
disambiguation, create a learning procedure to encourage the self-supervised
model to learn the patterns, and show how to use weak supervision to enhance
the signals in the training data. Encoding the reasoning patterns in a simple
Transformer architecture, Bootleg meets or exceeds state-of-the-art on three
NED benchmarks. We further show that the learned representations from Bootleg
successfully transfer to other non-disambiguation tasks that require
entity-based knowledge: we set a new state-of-the-art in the popular TACRED
relation extraction task by 1.0 F1 points and demonstrate up to 8% performance
lift in highly optimized production search and assistant tasks at a major
technology company
</p>
<a href="http://arxiv.org/abs/2010.10363" target="_blank">arXiv:2010.10363</a> [<a href="http://arxiv.org/pdf/2010.10363" target="_blank">pdf</a>]

<h2>Where Is the Normative Proof? Assumptions and Contradictions in ML Fairness Research. (arXiv:2010.10407v2 [cs.CY] UPDATED)</h2>
<h3>A. Feder Cooper</h3>
<p>Across machine learning (ML) sub-disciplines researchers make mathematical
assumptions to facilitate proof-writing. While such assumptions are necessary
for providing mathematical guarantees for how algorithms behave, they also
necessarily limit the applicability of these algorithms to different problem
settings. This practice is known--in fact, obvious--and accepted in ML
research. However, similar attention is not paid to the normative assumptions
that ground this work. I argue such assumptions are equally as important,
especially in areas of ML with clear social impact, such as fairness. This is
because, similar to how mathematical assumptions constrain applicability,
normative assumptions also limit algorithm applicability to certain problem
domains. I show that, in existing papers published in top venues, once
normative assumptions are clarified, it is often possible to get unclear or
contradictory results. While the mathematical assumptions and results are
sound, the implicit normative assumptions and accompanying normative results
contraindicate using these methods in practical fairness applications.
</p>
<a href="http://arxiv.org/abs/2010.10407" target="_blank">arXiv:2010.10407</a> [<a href="http://arxiv.org/pdf/2010.10407" target="_blank">pdf</a>]

<h2>A primer on model-guided exploration of fitness landscapes for biological sequence design. (arXiv:2010.10614v2 [q-bio.QM] UPDATED)</h2>
<h3>Sam Sinai, Eric D Kelsic</h3>
<p>Machine learning methods are increasingly employed to address challenges
faced by biologists. One area that will greatly benefit from this
cross-pollination is the problem of biological sequence design, which has
massive potential for therapeutic applications. However, significant
inefficiencies remain in communication between these fields which result in
biologists finding the progress in machine learning inaccessible, and hinder
machine learning scientists from contributing to impactful problems in
bioengineering. Sequence design can be seen as a search process on a discrete,
high-dimensional space, where each sequence is associated with a function. This
sequence-to-function map is known as a "Fitness Landscape". Designing a
sequence with a particular function is hence a matter of "discovering" such a
(often rare) sequence within this space. Today we can build predictive models
with good interpolation ability due to impressive progress in the synthesis and
testing of biological sequences in large numbers, which enables model training
and validation. However, it often remains a challenge to find useful sequences
with the properties that we like using these models. In particular, in this
primer we highlight that algorithms for experimental design, what we call
"exploration strategies", are a related, yet distinct problem from building
good models of sequence-to-function maps. We review advances and insights from
current literature -- by no means a complete treatment -- while highlighting
desirable features of optimal model-guided exploration, and cover potential
pitfalls drawn from our own experience. This primer can serve as a starting
point for researchers from different domains that are interested in the problem
of searching a sequence space with a model, but are perhaps unaware of
approaches that originate outside their field.
</p>
<a href="http://arxiv.org/abs/2010.10614" target="_blank">arXiv:2010.10614</a> [<a href="http://arxiv.org/pdf/2010.10614" target="_blank">pdf</a>]

<h2>Multi-Unit Transformers for Neural Machine Translation. (arXiv:2010.10743v2 [cs.CL] UPDATED)</h2>
<h3>Jianhao Yan, Fandong Meng, Jie Zhou</h3>
<p>Transformer models achieve remarkable success in Neural Machine Translation.
Many efforts have been devoted to deepening the Transformer by stacking several
units (i.e., a combination of Multihead Attentions and FFN) in a cascade, while
the investigation over multiple parallel units draws little attention. In this
paper, we propose the Multi-Unit Transformers (MUTE), which aim to promote the
expressiveness of the Transformer by introducing diverse and complementary
units. Specifically, we use several parallel units and show that modeling with
multiple units improves model performance and introduces diversity. Further, to
better leverage the advantage of the multi-unit setting, we design biased
module and sequential dependency that guide and encourage complementariness
among different units. Experimental results on three machine translation tasks,
the NIST Chinese-to-English, WMT'14 English-to-German and WMT'18
Chinese-to-English, show that the MUTE models significantly outperform the
Transformer-Base, by up to +1.52, +1.90 and +1.10 BLEU points, with only a mild
drop in inference speed (about 3.1%). In addition, our methods also surpass the
Transformer-Big model, with only 54\% of its parameters. These results
demonstrate the effectiveness of the MUTE, as well as its efficiency in both
the inference process and parameter usage.
</p>
<a href="http://arxiv.org/abs/2010.10743" target="_blank">arXiv:2010.10743</a> [<a href="http://arxiv.org/pdf/2010.10743" target="_blank">pdf</a>]

<h2>Mixed-Precision Embedding Using a Cache. (arXiv:2010.11305v2 [cs.LG] UPDATED)</h2>
<h3>Jie Amy Yang, Jianyu Huang, Jongsoo Park, Ping Tak Peter Tang, Andrew Tulloch</h3>
<p>In recommendation systems, practitioners observed that increase in the number
of embedding tables and their sizes often leads to significant improvement in
model performances. Given this and the business importance of these models to
major internet companies, embedding tables for personalization tasks have grown
to terabyte scale and continue to grow at a significant rate. Meanwhile, these
large-scale models are often trained with GPUs where high-performance memory is
a scarce resource, thus motivating numerous work on embedding table compression
during training. We propose a novel change to embedding tables using a cache
memory architecture, where the majority of rows in an embedding is trained in
low precision, and the most frequently or recently accessed rows cached and
trained in full precision. The proposed architectural change works in
conjunction with standard precision reduction and computer arithmetic
techniques such as quantization and stochastic rounding. For an open source
deep learning recommendation model (DLRM) running with Criteo-Kaggle dataset,
we achieve 3x memory reduction with INT8 precision embedding tables and
full-precision cache whose size are 5% of the embedding tables, while
maintaining accuracy. For an industrial scale model and dataset, we achieve
even higher &gt;7x memory reduction with INT4 precision and cache size 1% of
embedding tables, while maintaining accuracy, and 16% end-to-end training
speedup by reducing GPU-to-host data transfers.
</p>
<a href="http://arxiv.org/abs/2010.11305" target="_blank">arXiv:2010.11305</a> [<a href="http://arxiv.org/pdf/2010.11305" target="_blank">pdf</a>]

<h2>Efficient Generalized Spherical CNNs. (arXiv:2010.11661v2 [cs.CV] UPDATED)</h2>
<h3>Oliver J. Cobb, Christopher G. R. Wallis, Augustine N. Mavor-Parker, Augustin Marignier, Matthew A. Price, Mayeul d&#x27;Avezac, Jason D. McEwen</h3>
<p>Many problems across computer vision and the natural sciences require the
analysis of spherical data, for which representations may be learned
efficiently by encoding equivariance to rotational symmetries. We present a
generalized spherical CNN framework that encompasses various existing
approaches and allows them to be leveraged alongside each other. The only
existing non-linear spherical CNN layer that is strictly equivariant has
complexity $\mathcal{O}(C^2L^5)$, where $C$ is a measure of representational
capacity and $L$ the spherical harmonic bandlimit. Such a high computational
cost often prohibits the use of strictly equivariant spherical CNNs. We develop
two new strictly equivariant layers with reduced complexity $\mathcal{O}(CL^4)$
and $\mathcal{O}(CL^3 \log L)$, making larger, more expressive models
computationally feasible. Moreover, we adopt efficient sampling theory to
achieve further computational savings. We show that these developments allow
the construction of more expressive hybrid models that achieve state-of-the-art
accuracy and parameter efficiency on spherical benchmark problems.
</p>
<a href="http://arxiv.org/abs/2010.11661" target="_blank">arXiv:2010.11661</a> [<a href="http://arxiv.org/pdf/2010.11661" target="_blank">pdf</a>]

<h2>Deep Analysis of CNN-based Spatio-temporal Representations for Action Recognition. (arXiv:2010.11757v2 [cs.CV] UPDATED)</h2>
<h3>Chun-Fu Chen, Rameswar Panda, Kandan Ramakrishnan, Rogerio Feris, John Cohn, Aude Oliva, Quanfu Fan</h3>
<p>In recent years, a number of approaches based on 2D CNNs and 3D CNNs have
emerged for video action recognition, achieving state-of-the-art results on
several large-scale benchmark datasets. In this paper, we carry out an in-depth
comparative analysis to better understand the differences between these
approaches and the progress made by them. To this end, we develop a unified
framework for both 2D-CNN and 3D-CNN action models, which enables us to remove
bells and whistles and provides a common ground for a fair comparison. We then
conduct an effort towards a large-scale analysis involving over 300 action
recognition models. Our comprehensive analysis reveals that a) a significant
leap is made in efficiency for action recognition, but not in accuracy; b)
2D-CNN and 3D-CNN models behave similarly in terms of spatio-temporal
representation abilities and transferability. Our analysis also shows that
recent action models seem to be able to learn data-dependent temporality
flexibly as needed. Our codes and models are available on
https://github.com/IBM/action-recognition-pytorch.
</p>
<a href="http://arxiv.org/abs/2010.11757" target="_blank">arXiv:2010.11757</a> [<a href="http://arxiv.org/pdf/2010.11757" target="_blank">pdf</a>]

<h2>Deep Unsupervised Drum Transcription. (arXiv:1906.03697v2 [cs.SD] CROSS LISTED)</h2>
<h3>Keunwoo Choi, Kyunghyun Cho</h3>
<p>We introduce DrummerNet, a drum transcription system that is trained in an
unsupervised manner. DrummerNet does not require any ground-truth transcription
and, with the data-scalability of deep neural networks, learns from a large
unlabeled dataset. In DrummerNet, the target drum signal is first passed to a
(trainable) transcriber, then reconstructed in a (fixed) synthesizer according
to the transcription estimate. By training the system to minimize the distance
between the input and the output audio signals, the transcriber learns to
transcribe without ground truth transcription. Our experiment shows that
DrummerNet performs favorably compared to many other recent drum transcription
systems, both supervised and unsupervised.
</p>
<a href="http://arxiv.org/abs/1906.03697" target="_blank">arXiv:1906.03697</a> [<a href="http://arxiv.org/pdf/1906.03697" target="_blank">pdf</a>]

<h2>ByzShield: An Efficient and Robust System for Distributed Training. (arXiv:2010.04902v1 [cs.LG] CROSS LISTED)</h2>
<h3>Konstantinos Konstantinidis, Aditya Ramamoorthy</h3>
<p>Training of large scale models on distributed clusters is a critical
component of the machine learning pipeline. However, this training can easily
be made to fail if some workers behave in an adversarial (Byzantine) fashion
whereby they return arbitrary results to the parameter server (PS). A plethora
of existing papers consider a variety of attack models and propose robust
aggregation and/or computational redundancy to alleviate the effects of these
attacks. In this work we consider an omniscient attack model where the
adversary has full knowledge about the gradient computation assignments of the
workers and can choose to attack (up to) any q out of n worker nodes to induce
maximal damage. Our redundancy-based method ByzShield leverages the properties
of bipartite expander graphs for the assignment of tasks to workers; this helps
to effectively mitigate the effect of the Byzantine behavior. Specifically, we
demonstrate an upper bound on the worst case fraction of corrupted gradients
based on the eigenvalues of our constructions which are based on mutually
orthogonal Latin squares and Ramanujan graphs. Our numerical experiments
indicate over a 36% reduction on average in the fraction of corrupted gradients
compared to the state of the art. Likewise, our experiments on training
followed by image classification on the CIFAR-10 dataset show that ByzShield
has on average a 20% advantage in accuracy under the most sophisticated
attacks. ByzShield also tolerates a much larger fraction of adversarial nodes
compared to prior work.
</p>
<a href="http://arxiv.org/abs/2010.04902" target="_blank">arXiv:2010.04902</a> [<a href="http://arxiv.org/pdf/2010.04902" target="_blank">pdf</a>]

<h2>LowCon: A design-based subsampling approach in a misspecified linear modeL. (arXiv:2010.12178v1 [stat.ME])</h2>
<h3>Cheng Meng, Rui Xie, Abhyuday Mandal, Xinlian Zhang, Wenxuan Zhong, Ping Ma</h3>
<p>We consider a measurement constrained supervised learning problem, that is,
(1) full sample of the predictors are given; (2) the response observations are
unavailable and expensive to measure. Thus, it is ideal to select a subsample
of predictor observations, measure the corresponding responses, and then fit
the supervised learning model on the subsample of the predictors and responses.
However, model fitting is a trial and error process, and a postulated model for
the data could be misspecified. Our empirical studies demonstrate that most of
the existing subsampling methods have unsatisfactory performances when the
models are misspecified. In this paper, we develop a novel subsampling method,
called "LowCon", which outperforms the competing methods when the working
linear model is misspecified. Our method uses orthogonal Latin hypercube
designs to achieve a robust estimation. We show that the proposed design-based
estimator approximately minimizes the so-called "worst-case" bias with respect
to many possible misspecification terms. Both the simulated and real-data
analyses demonstrate the proposed estimator is more robust than several
subsample least squares estimators obtained by state-of-the-art subsampling
methods.
</p>
<a href="http://arxiv.org/abs/2010.12178" target="_blank">arXiv:2010.12178</a> [<a href="http://arxiv.org/pdf/2010.12178" target="_blank">pdf</a>]

<h2>Graph Neural Network for Large-Scale Network Localization. (arXiv:2010.11653v1 [cs.LG] CROSS LISTED)</h2>
<h3>Wenzhong Yan, Di Jin, Zhidi Lin, Feng Yin</h3>
<p>Graph neural networks (GNNs) are popular to use for classifying structured
data in the context of machine learning. But surprisingly, they are rarely
applied to regression problems. In this work, we adopt GNN for a classic but
challenging nonlinear regression problem, namely the network localization. Our
main findings are in order. First, GNN is potentially the best solution to
large-scale network localization in terms of accuracy, robustness and
computational time. Second, thresholding of the communication range is
essential to its superior performance. Simulation results corroborate that the
proposed GNN based method outperforms all benchmarks by far. Such inspiring
results are further justified theoretically in terms of data aggregation,
non-line-of-sight (NLOS) noise removal and lowpass filtering effect, all
affected by the threshold for neighbor selection. Code is available at
https://github.com/Yanzongzi/GNN-For-localization.
</p>
<a href="http://arxiv.org/abs/2010.11653" target="_blank">arXiv:2010.11653</a> [<a href="http://arxiv.org/pdf/2010.11653" target="_blank">pdf</a>]

