---
title: Latest Deep Learning Papers
date: 2020-11-09 21:00:01 +0800
featured-img: DL-logo
categories: [Deep_Learning]
tags: [Deep_Learning]
mathjax: Yes
---

<h1>Your arXiv Feed (295 Articles)</h1>
<h2>Single-Node Attack for Fooling Graph Neural Networks. (arXiv:2011.03574v1 [cs.LG])</h2>
<h3>Ben Finkelshtein, Chaim Baskin, Evgenii Zheltonozhskii, Uri Alon</h3>
<p>Graph neural networks (GNNs) have shown broad applicability in a variety of
domains. Some of these domains, such as social networks and product
recommendations, are fertile ground for malicious users and behavior. In this
paper, we show that GNNs are vulnerable to the extremely limited scenario of a
single-node adversarial example, where the node cannot be picked by the
attacker. That is, an attacker can force the GNN to classify any target node to
a chosen label by only slightly perturbing another single arbitrary node in the
graph, even when not being able to pick that specific attacker node. When the
adversary is allowed to pick a specific attacker node, the attack is even more
effective. We show that this attack is effective across various GNN types, such
as GraphSAGE, GCN, GAT, and GIN, across a variety of real-world datasets, and
as a targeted and a non-targeted attack. Our code is available at
https://github.com/benfinkelshtein/SINGLE .
</p>
<a href="http://arxiv.org/abs/2011.03574" target="_blank">arXiv:2011.03574</a> [<a href="http://arxiv.org/pdf/2011.03574" target="_blank">pdf</a>]

<h2>A Weakly Supervised Convolutional Network for Change Segmentation and Classification. (arXiv:2011.03577v1 [cs.CV])</h2>
<h3>Philipp Andermatt, Radu Timofte</h3>
<p>Fully supervised change detection methods require difficult to procure
pixel-level labels, while weakly supervised approaches can be trained with
image-level labels. However, most of these approaches require a combination of
changed and unchanged image pairs for training. Thus, these methods can not
directly be used for datasets where only changed image pairs are available. We
present W-CDNet, a novel weakly supervised change detection network that can be
trained with image-level semantic labels. Additionally, W-CDNet can be trained
with two different types of datasets, either containing changed image pairs
only or a mixture of changed and unchanged image pairs. Since we use
image-level semantic labels for training, we simultaneously create a change
mask and label the changed object for single-label images. W-CDNet employs a
W-shaped siamese U-net to extract feature maps from an image pair which then
get compared in order to create a raw change mask. The core part of our model,
the Change Segmentation and Classification (CSC) module, learns an accurate
change mask at a hidden layer by using a custom Remapping Block and then
segmenting the current input image with the change mask. The segmented image is
used to predict the image-level semantic label. The correct label can only be
predicted if the change mask actually marks relevant change. This forces the
model to learn an accurate change mask. We demonstrate the segmentation and
classification performance of our approach and achieve top results on AICD and
HRSCD, two public aerial imaging change detection datasets as well as on a Food
Waste change detection dataset. Our code is available at
https://github.com/PhiAbs/W-CDNet .
</p>
<a href="http://arxiv.org/abs/2011.03577" target="_blank">arXiv:2011.03577</a> [<a href="http://arxiv.org/pdf/2011.03577" target="_blank">pdf</a>]

<h2>Reactive motion planning with probabilistics safety guarantees. (arXiv:2011.03590v1 [cs.RO])</h2>
<h3>Yuxiao Chen, Ugo Rosolia, Chuchu Fan, Aaron D. Ames, Richard Murray</h3>
<p>Motion planning in environments with multiple agents is critical to many
important autonomous applications such as autonomous vehicles and assistive
robots. This paper considers the problem of motion planning, where the
controlled agent shares the environment with multiple uncontrolled agents.
First, a predictive model of the uncontrolled agents is trained to predict all
possible trajectories within a short horizon based on the scenario. The
prediction is then fed to a motion planning module based on model predictive
control. We proved generalization bound for the predictive model using three
different methods, post-bloating, support vector machine (SVM), and conformal
analysis, all capable of generating stochastic guarantees of the correctness of
the predictor. The proposed approach is demonstrated in simulation in a
scenario emulating autonomous highway driving.
</p>
<a href="http://arxiv.org/abs/2011.03590" target="_blank">arXiv:2011.03590</a> [<a href="http://arxiv.org/pdf/2011.03590" target="_blank">pdf</a>]

<h2>Fast Near-Optimal Heterogeneous Task Allocation via Flow Decomposition. (arXiv:2011.03603v1 [cs.RO])</h2>
<h3>Kiril Solovey, Saptarshi Bandyopadhyay, Federico Rossi, Michael T. Wolf, Marco Pavone</h3>
<p>Multi-robot systems are uniquely well-suited to performing complex tasks such
as patrolling and tracking, information gathering, and pick-up and delivery
problems, offering significantly higher performance than single-robot systems.
A fundamental building block in most multi-robot systems is task allocation:
assigning robots to tasks (e.g., patrolling an area, or servicing a
transportation request) as they appear based on the robots' states to maximize
reward. In many practical situations, the allocation must account for
heterogeneous capabilities (e.g., availability of appropriate sensors or
actuators) to ensure the feasibility of execution, and to promote a higher
reward, over a long time horizon. To this end, we present the FlowDec algorithm
for efficient heterogeneous task-allocation achieving an approximation factor
of at least 1/2 of the optimal reward. Our approach decomposes the
heterogeneous problem into several homogeneous subproblems that can be solved
efficiently using min-cost flow. Through simulation experiments, we show that
our algorithm is faster by several orders of magnitude than a MILP approach.
</p>
<a href="http://arxiv.org/abs/2011.03603" target="_blank">arXiv:2011.03603</a> [<a href="http://arxiv.org/pdf/2011.03603" target="_blank">pdf</a>]

<h2>Highly Available Data Parallel ML training on Mesh Networks. (arXiv:2011.03605v1 [cs.LG])</h2>
<h3>Sameer Kumar, Norm Jouppi</h3>
<p>Data parallel ML models can take several days or weeks to train on several
accelerators. The long duration of training relies on the cluster of resources
to be available for the job to keep running for the entire duration. On a mesh
network this is challenging because failures will create holes in the mesh.
Packets must be routed around the failed chips for full connectivity. In this
paper, we present techniques to route gradient summation allreduce traffic
around failed chips on 2-D meshes. We evaluate performance of our fault
tolerant allreduce techniques via the MLPerf-v0.7 ResNet-50 and BERT
benchmarks. Performance results show minimal impact to training throughput on
512 and 1024 TPU-v3 chips.
</p>
<a href="http://arxiv.org/abs/2011.03605" target="_blank">arXiv:2011.03605</a> [<a href="http://arxiv.org/pdf/2011.03605" target="_blank">pdf</a>]

<h2>Ridge Regression with Frequent Directions: Statistical and Optimization Perspectives. (arXiv:2011.03607v1 [cs.LG])</h2>
<h3>Charlie Dickens</h3>
<p>Despite its impressive theory \&amp; practical performance, Frequent Directions
(\acrshort{fd}) has not been widely adopted for large-scale regression tasks.
Prior work has shown randomized sketches (i) perform worse in estimating the
covariance matrix of the data than \acrshort{fd}; (ii) incur high error when
estimating the bias and/or variance on sketched ridge regression. We give the
first constant factor relative error bounds on the bias \&amp; variance for
sketched ridge regression using \acrshort{fd}. We complement these statistical
results by showing that \acrshort{fd} can be used in the optimization setting
through an iterative scheme which yields high-accuracy solutions. This improves
on randomized approaches which need to compromise the need for a new sketch
every iteration with speed of convergence. In both settings, we also show using
\emph{Robust Frequent Directions} further enhances performance.
</p>
<a href="http://arxiv.org/abs/2011.03607" target="_blank">arXiv:2011.03607</a> [<a href="http://arxiv.org/pdf/2011.03607" target="_blank">pdf</a>]

<h2>A Few Shot Adaptation of Visual Navigation Skills to New Observations using Meta-Learning. (arXiv:2011.03609v1 [cs.RO])</h2>
<h3>Qian Luo, Maks Sorokin, Sehoon Ha</h3>
<p>Target-driven visual navigation is a challenging problem that requires a
robot to find the goal using only visual inputs. Many researchers have
demonstrated promising results using deep reinforcement learning (deep RL) on
various robotic platforms, but typical end-to-end learning is known for its
poor extrapolation capability to new scenarios. Therefore, learning a
navigation policy for a new robot with a new sensor configuration or a new
target still remains a challenging problem. In this paper, we introduce a
learning algorithm that enables rapid adaptation to new sensor configurations
or target objects with a few shots. We design a policy architecture with latent
features between perception and inference networks and quickly adapt the
perception network via meta-learning while freezing the inference network. Our
experiments show that our algorithm adapts the learned navigation policy with
only three shots for unseen situations with different sensor configurations or
different target colors. We also analyze the proposed algorithm by
investigating various hyperparameters.
</p>
<a href="http://arxiv.org/abs/2011.03609" target="_blank">arXiv:2011.03609</a> [<a href="http://arxiv.org/pdf/2011.03609" target="_blank">pdf</a>]

<h2>Single and Multi-Agent Deep Reinforcement Learning for AI-Enabled Wireless Networks: A Tutorial. (arXiv:2011.03615v1 [cs.LG])</h2>
<h3>Amal Feriani, Ekram Hossain</h3>
<p>Deep Reinforcement Learning (DRL) has recently witnessed significant advances
that have led to multiple successes in solving sequential decision-making
problems in various domains, particularly in wireless communications. The
future sixth-generation (6G) networks are expected to provide scalable,
low-latency, ultra-reliable services empowered by the application of
data-driven Artificial Intelligence (AI). The key enabling technologies of
future 6G networks, such as intelligent meta-surfaces, aerial networks, and AI
at the edge, involve more than one agent which motivates the importance of
multi-agent learning techniques. Furthermore, cooperation is central to
establishing self-organizing, self-sustaining, and decentralized networks. In
this context, this tutorial focuses on the role of DRL with an emphasis on deep
Multi-Agent Reinforcement Learning (MARL) for AI-enabled 6G networks. The first
part of this paper will present a clear overview of the mathematical frameworks
for single-agent RL and MARL. The main idea of this work is to motivate the
application of RL beyond the model-free perspective which was extensively
adopted in recent years. Thus, we provide a selective description of RL
algorithms such as Model-Based RL (MBRL) and cooperative MARL and we highlight
their potential applications in 6G wireless networks. Finally, we overview the
state-of-the-art of MARL in fields such as Mobile Edge Computing (MEC),
Unmanned Aerial Vehicles (UAV) networks, and cell-free massive MIMO, and
identify promising future research directions. We expect this tutorial to
stimulate more research endeavors to build scalable and decentralized systems
based on MARL.
</p>
<a href="http://arxiv.org/abs/2011.03615" target="_blank">arXiv:2011.03615</a> [<a href="http://arxiv.org/pdf/2011.03615" target="_blank">pdf</a>]

<h2>Learning Human Search Behavior from Egocentric Visual Inputs. (arXiv:2011.03618v1 [cs.RO])</h2>
<h3>Maks Sorokin, Wenhao Yu, Sehoon Ha, C. Karen Liu</h3>
<p>"Looking for things" is a mundane but critical task we repeatedly carry on in
our daily life. We introduce a method to develop a human character capable of
searching for a randomly located target object in a detailed 3D scene using its
locomotion capability and egocentric vision perception represented as RGBD
images. By depriving the privileged 3D information from the human character, it
is forced to move and look around simultaneously to account for the restricted
sensing capability, resulting in natural navigation and search behaviors. Our
method consists of two components: 1) a search control policy based on an
abstract character model, and 2) an online replanning control module for
synthesizing detailed kinematic motion based on the trajectories planned by the
search policy. We demonstrate that the combined techniques enable the character
to effectively find often occluded household items in indoor environments. The
same search policy can be applied to different full-body characters without the
need for retraining. We evaluate our method quantitatively by testing it on
randomly generated scenarios. Our work is a first step toward creating
intelligent virtual agents with humanlike behaviors driven by onboard sensors,
paving the road toward future robotic applications.
</p>
<a href="http://arxiv.org/abs/2011.03618" target="_blank">arXiv:2011.03618</a> [<a href="http://arxiv.org/pdf/2011.03618" target="_blank">pdf</a>]

<h2>Feature Removal Is a Unifying Principle for Model Explanation Methods. (arXiv:2011.03623v1 [cs.LG])</h2>
<h3>Ian Covert, Scott Lundberg, Su-In Lee</h3>
<p>Researchers have proposed a wide variety of model explanation approaches, but
it remains unclear how most methods are related or when one method is
preferable to another. We examine the literature and find that many methods are
based on a shared principle of explaining by removing - essentially, measuring
the impact of removing sets of features from a model. These methods vary in
several respects, so we develop a framework for removal-based explanations that
characterizes each method along three dimensions: 1) how the method removes
features, 2) what model behavior the method explains, and 3) how the method
summarizes each feature's influence. Our framework unifies 25 existing methods,
including several of the most widely used approaches (SHAP, LIME, Meaningful
Perturbations, permutation tests). Exposing the fundamental similarities
between these methods empowers users to reason about which tools to use and
suggests promising directions for ongoing research in model explainability.
</p>
<a href="http://arxiv.org/abs/2011.03623" target="_blank">arXiv:2011.03623</a> [<a href="http://arxiv.org/pdf/2011.03623" target="_blank">pdf</a>]

<h2>Curse of Small Sample Size in Forecasting of the Active Cases in COVID-19 Outbreak. (arXiv:2011.03628v1 [cs.LG])</h2>
<h3>Mert Nak&#x131;p, Onur &#xc7;opur, C&#xfc;neyt G&#xfc;zeli&#x15f;</h3>
<p>During the COVID-19 pandemic, a massive number of attempts on the predictions
of the number of cases and the other future trends of this pandemic have been
made. However, they fail to predict, in a reliable way, the medium and long
term evolution of fundamental features of COVID-19 outbreak within acceptable
accuracy. This paper gives an explanation for the failure of machine learning
models in this particular forecasting problem. The paper shows that simple
linear regression models provide high prediction accuracy values reliably but
only for a 2-weeks period and that relatively complex machine learning models,
which have the potential of learning long term predictions with low errors,
cannot achieve to obtain good predictions with possessing a high generalization
ability. It is suggested in the paper that the lack of a sufficient number of
samples is the source of low prediction performance of the forecasting models.
The reliability of the forecasting results about the active cases is measured
in terms of the cross-validation prediction errors, which are used as
expectations for the generalization errors of the forecasters. To exploit the
information, which is of most relevant with the active cases, we perform
feature selection over a variety of variables. We apply different feature
selection methods, namely the Pairwise Correlation, Recursive Feature
Selection, and feature selection by using the Lasso regression and compare them
to each other and also with the models not employing any feature selection.
Furthermore, we compare Linear Regression, Multi-Layer Perceptron, and
Long-Short Term Memory models each of which is used for prediction active cases
together with the mentioned feature selection methods. Our results show that
the accurate forecasting of the active cases with high generalization ability
is possible up to 3 days only because of the small sample size of COVID-19
data.
</p>
<a href="http://arxiv.org/abs/2011.03628" target="_blank">arXiv:2011.03628</a> [<a href="http://arxiv.org/pdf/2011.03628" target="_blank">pdf</a>]

<h2>Efficient Robust Watermarking Based on Quaternion Singular Value Decomposition and Coefficient Pair Selection. (arXiv:2011.03631v1 [cs.CV])</h2>
<h3>Yong Chen, Zhi-Gang Jia, Ya-Xin Peng, Yan Peng</h3>
<p>Quaternion singular value decomposition (QSVD) is a robust technique of
digital watermarking which can extract high quality watermarks from watermarked
images with low distortion. In this paper, QSVD technique is further
investigated and an efficient robust watermarking scheme is proposed. The
improved algebraic structure-preserving method is proposed to handle the
problem of "explosion of complexity" occurred in the conventional QSVD design.
Secret information is transmitted blindly by incorporating in QSVD two new
strategies, namely, coefficient pair selection and adaptive embedding. Unlike
conventional QSVD which embeds watermarks in a single imaginary unit, we
propose to adaptively embed the watermark into the optimal hiding position
using the Normalized Cross-Correlation (NC) method. This avoids the selection
of coefficient pair with less correlation, and thus, it reduces embedding
impact by decreasing the maximum modification of coefficient values. In this
way, compared with conventional QSVD, the proposed watermarking strategy avoids
more modifications to a single color image layer and a better visual quality of
the watermarked image is observed. Meanwhile, adaptive QSVD resists some common
geometric attacks, and it improves the robustness of conventional QSVD. With
these improvements, our method outperforms conventional QSVD. Its superiority
over other state-of-the-art methods is also demonstrated experimentally.
</p>
<a href="http://arxiv.org/abs/2011.03631" target="_blank">arXiv:2011.03631</a> [<a href="http://arxiv.org/pdf/2011.03631" target="_blank">pdf</a>]

<h2>Augmented Equivariant Attention Networks for Electron Microscopy Image Super-Resolution. (arXiv:2011.03633v1 [cs.CV])</h2>
<h3>Yaochen Xie, Yu Ding, Shuiwang Ji</h3>
<p>Taking electron microscopy (EM) images in high-resolution is time-consuming
and expensive and could be detrimental to the integrity of the samples under
observation. Advances in deep learning enable us to perform super-resolution
computationally, so as to obtain high-resolution images from low-resolution
ones. When training super-resolution models on pairs of experimentally acquired
EM images, prior models suffer from performance loss while using the
pooled-training strategy due to their inability to capture inter-image
dependencies and common features shared among images. Although there exist
methods that take advantage of shared features among input instances in image
classification tasks, they in the current form cannot be applied to
super-resolution tasks because they fail to preserve an essential property in
image-to-image transformation problems, which is the equivariance property to
spatial permutations. To address these limitations, we propose the augmented
equivariant attention networks (AEANets) with better capability to capture
inter-image dependencies and shared features, while preserving the equivariance
to spatial permutations. The proposed AEANets captures inter-image dependencies
and common features shared among images via two augmentations on the attention
mechanism; namely, the shared references and the batch-aware attention during
training. We theoretically show the equivariance property of the proposed
augmented attention model and experimentally show that AEANets consistently
outperforms the baselines in both quantitative and visual results.
</p>
<a href="http://arxiv.org/abs/2011.03633" target="_blank">arXiv:2011.03633</a> [<a href="http://arxiv.org/pdf/2011.03633" target="_blank">pdf</a>]

<h2>Motion Prediction on Self-driving Cars: A Review. (arXiv:2011.03635v1 [cs.RO])</h2>
<h3>Shahrokh Paravarzar, Belqes Mohammad</h3>
<p>The autonomous vehicle motion prediction literature is reviewed. Motion
prediction is the most challenging task in autonomous vehicles and self-drive
cars. These challenges have been discussed. Later on, the state-of-theart has
reviewed based on the most recent literature and the current challenges are
discussed. The state-of-the-art consists of classical and physical methods,
deep learning networks, and reinforcement learning. prons and cons of the
methods and gap of the research presented in this review. Finally, the
literature surrounding object tracking and motion will be presented. As a
result, deep reinforcement learning is the best candidate to tackle
self-driving cars.
</p>
<a href="http://arxiv.org/abs/2011.03635" target="_blank">arXiv:2011.03635</a> [<a href="http://arxiv.org/pdf/2011.03635" target="_blank">pdf</a>]

<h2>Graph cuts always find a global optimum (with a catch). (arXiv:2011.03639v1 [stat.ML])</h2>
<h3>Hunter Lang, David Sontag, Aravindan Vijayaraghavan</h3>
<p>We prove that the alpha-expansion algorithm for MAP inference always returns
a globally optimal assignment for Markov Random Fields with Potts pairwise
potentials, with a catch: the returned assignment is only guaranteed to be
optimal in a small perturbation of the original problem instance. In other
words, all local minima with respect to expansion moves are global minima to
slightly perturbed versions of the problem. On "real-world" instances, MAP
assignments of small perturbations of the problem should be very similar to the
MAP assignment(s) of the original problem instance. We design an algorithm that
can certify whether this is the case in practice. On several MAP inference
problem instances from computer vision, this algorithm certifies that MAP
solutions to all of these perturbations are very close to solutions of the
original instance. These results taken together give a cohesive explanation for
the good performance of "graph cuts" algorithms in practice. Every local
expansion minimum is a global minimum in a small perturbation of the problem,
and all of these global minima are close to the original solution.
</p>
<a href="http://arxiv.org/abs/2011.03639" target="_blank">arXiv:2011.03639</a> [<a href="http://arxiv.org/pdf/2011.03639" target="_blank">pdf</a>]

<h2>Exploring the limits of Concurrency in ML Training on Google TPUs. (arXiv:2011.03641v1 [cs.LG])</h2>
<h3>Sameer Kumar, James Bradbury, Cliff Young, Yu Emma Wang, Anselm Levskaya, Blake Hechtman, Dehao Chen, HyoukJoong Lee, Mehmet Deveci, Naveen Kumar, Pankaj Kanwar, Shibo Wang, Skye Wanderman-Milne, Steve Lacy, Tao Wang, Tayo Oguntebi, Yazhou Zu, Yuanzhong Xu, Andy Swing</h3>
<p>Recent results in language understanding using neural networks have required
training hardware of unprecedentedscale, with thousands of chips cooperating on
a single training run. This paper presents techniques to scaleML models on the
Google TPU Multipod, a mesh with 4096 TPU-v3 chips. We discuss model
parallelism toovercome scaling limitations from the fixed batch size in data
parallelism, communication/collective optimizations,distributed evaluation of
training metrics, and host input processing scaling optimizations. These
techniques aredemonstrated in both the TensorFlow and JAX programming
frameworks. We also present performance resultsfrom the recent Google
submission to the MLPerf-v0.7 benchmark contest, achieving record training
times from16 to 28 seconds in four MLPerf models on the Google TPU-v3 Multipod
machine.
</p>
<a href="http://arxiv.org/abs/2011.03641" target="_blank">arXiv:2011.03641</a> [<a href="http://arxiv.org/pdf/2011.03641" target="_blank">pdf</a>]

<h2>Towards Spiral Brick Column Building Robots. (arXiv:2011.03643v1 [cs.RO])</h2>
<h3>Yaseer Ashraf, Ahmed Abdallah, Abdelhaleem Osman, Victor Parque, Samy Assal</h3>
<p>Automation in construction has the potential to expand the technological
landscape of labor intensive tasks, and bring gains in efficiency and
productivity to sustain global competitiveness. In this paper we propose a
task-level approach for assembly of spiral brick columns. Our extensive
computational simulations using the generalized models of spiral brick columns
show the feasibility, the effectiveness and efficiency of our proposed
approach. Our results offer the potential to use robots in automated
construction of spiral brick columns with utmost efficiency.
</p>
<a href="http://arxiv.org/abs/2011.03643" target="_blank">arXiv:2011.03643</a> [<a href="http://arxiv.org/pdf/2011.03643" target="_blank">pdf</a>]

<h2>A Reinforcement Learning Approach to the Orienteering Problem with Time Windows. (arXiv:2011.03647v1 [cs.LG])</h2>
<h3>Ricardo Gama, Hugo L. Fernandes</h3>
<p>The Orienteering Problem with Time Windows (OPTW) is a combinatorial
optimization problem where the goal is to maximize the total scores collected
from visited locations, under some time constraints. Several heuristics have
been proposed for the OPTW, yet in comparison with machine learning models, a
heuristic typically has a smaller potential for generalization and
personalization. The application of neural network models to combinatorial
optimization has recently shown promising results in similar problems like the
Travelling Salesman Problem. A neural network allows learning solutions using
reinforcement learning or in a supervised way, depending on the available data.
After learning, it can potentially generalize and be quickly fine-tuned to
further improve performance and personalization. This is advantageous since,
for real word applications, a solution's quality, personalization and execution
times are all important factors to be taken into account.

Here we explore the use of Pointer Network models trained with reinforcement
learning for solving the OPTW problem. Among its various applications, the OPTW
can be used to model the Tourist Trip Design Problem (TTDP). We train the
Pointer Network with the TTDP problem in mind, by sampling variables that can
change across tourists for a particular instance-region: starting position,
starting time, time available and the scores of each point of interest. After a
model-region is trained it can infer a solution for a particular tourist using
beam search. We evaluate our approach on several existing benchmark OPTW
instances. We show that it is able to generalize across different generated
tourists for each region and that it generally outperforms the most commonly
used heuristic while computing the solution in realistic times.
</p>
<a href="http://arxiv.org/abs/2011.03647" target="_blank">arXiv:2011.03647</a> [<a href="http://arxiv.org/pdf/2011.03647" target="_blank">pdf</a>]

<h2>Sliding on Manifolds: Geometric Attitude Control with Quaternions. (arXiv:2011.03648v1 [cs.RO])</h2>
<h3>Brett T. Lopez, Jean-Jacques E. Slotine</h3>
<p>This work proposes a quaternion-based sliding variable that describes
exponentially convergent error dynamics for any forward complete desired
attitude trajectory. The proposed sliding variable directly operates on the
non-Euclidean space formed by quaternions and explicitly handles the double
covering property to enable global attitude tracking when used in feedback.
In-depth analysis of the sliding variable is provided and compared to others in
the literature. Several feedback controllers including nonlinear PD, robust,
and adaptive sliding control are then derived. Simulation results of a rigid
body with uncertain dynamics demonstrate the effectiveness and superiority of
the approach.
</p>
<a href="http://arxiv.org/abs/2011.03648" target="_blank">arXiv:2011.03648</a> [<a href="http://arxiv.org/pdf/2011.03648" target="_blank">pdf</a>]

<h2>Strawberry Detection Using a Heterogeneous Multi-Processor Platform. (arXiv:2011.03651v1 [cs.RO])</h2>
<h3>Samuel Brandenburg, Pedro Machado, Nikesh Lama, T.M. McGinnity</h3>
<p>Over the last few years, the number of precision farming projects has
increased specifically in harvesting robots and many of which have made
continued progress from identifying crops to grasping the desired fruit or
vegetable. One of the most common issues found in precision farming projects is
that successful application is heavily dependent not just on identifying the
fruit but also on ensuring that localisation allows for accurate navigation.
These issues become significant factors when the robot is not operating in a
prearranged environment, or when vegetation becomes too thick, thus covering
crop. Moreover, running a state-of-the-art deep learning algorithm on an
embedded platform is also very challenging, resulting most of the times in low
frame rates. This paper proposes using the You Only Look Once version 3
(YOLOv3) Convolutional Neural Network (CNN) in combination with utilising image
processing techniques for the application of precision farming robots targeting
strawberry detection, accelerated on a heterogeneous multiprocessor platform.
The results show a performance acceleration by five times when implemented on a
Field-Programmable Gate Array (FPGA) when compared with the same algorithm
running on the processor side with an accuracy of 78.3\% over the test set
comprised of 146 images.
</p>
<a href="http://arxiv.org/abs/2011.03651" target="_blank">arXiv:2011.03651</a> [<a href="http://arxiv.org/pdf/2011.03651" target="_blank">pdf</a>]

<h2>ROBIN: a Graph-Theoretic Approach to Reject Outliers in Robust Estimation using Invariants. (arXiv:2011.03659v1 [cs.CV])</h2>
<h3>Jingnan Shi, Heng Yang, Luca Carlone</h3>
<p>Many estimation problems in robotics, computer vision, and learning require
estimating unknown quantities in the face of outliers. Outliers are typically
the result of incorrect data association or feature matching, and it is common
to have problems where more than 90% of the measurements used for estimation
are outliers. While current approaches for robust estimation are able to deal
with moderate amounts of outliers, they fail to produce accurate estimates in
the presence of many outliers. This paper develops an approach to prune
outliers. First, we develop a theory of invariance that allows us to quickly
check if a subset of measurements are mutually compatible without explicitly
solving the estimation problem. Second, we develop a graph-theoretic framework,
where measurements are modeled as vertices and mutual compatibility is captured
by edges. We generalize existing results showing that the inliers form a clique
in this graph and typically belong to the maximum clique. We also show that in
practice the maximum k-core of the compatibility graph provides an
approximation of the maximum clique, while being faster to compute in large
problems. These two contributions leads to ROBIN, our approach to Reject
Outliers Based on INvariants, which allows us to quickly prune outliers in
generic estimation problems. We demonstrate ROBIN in four geometric perception
problems and show it boosts robustness of existing solvers while running in
milliseconds in large problems.
</p>
<a href="http://arxiv.org/abs/2011.03659" target="_blank">arXiv:2011.03659</a> [<a href="http://arxiv.org/pdf/2011.03659" target="_blank">pdf</a>]

<h2>Search-Based Online Trajectory Planning for Car-like Robots in Highly Dynamic Environments. (arXiv:2011.03664v1 [cs.RO])</h2>
<h3>Jiahui Lin, Tong Zhou, Delong Zhu, Jianbang Liu, Max Q.-H. Meng</h3>
<p>This paper presents a search-based partial motion planner to generate
dynamically feasible trajectories for car-like robots in highly dynamic
environments. The planner searches for smooth, safe, and near-time-optimal
trajectories by exploring a state graph built on motion primitives, which are
generated by discretizing the time dimension and the control space. To enable
fast online planning, we first propose an efficient path searching algorithm
based on the aggregation and pruning of motion primitives. We then propose a
fast collision checking algorithm that takes into account the motions of moving
obstacles. The algorithm linearizes relative motions between the robot and
obstacles and then checks collisions by comparing a point-line distance.
Benefiting from the fast searching and collision checking algorithms, the
planner can effectively and safely explore the state-time space to generate
near-time-optimal solutions. The results through extensive experiments show
that the proposed method can generate feasible trajectories within milliseconds
while maintaining a higher success rate than up-to-date methods, which
significantly demonstrates its advantages.
</p>
<a href="http://arxiv.org/abs/2011.03664" target="_blank">arXiv:2011.03664</a> [<a href="http://arxiv.org/pdf/2011.03664" target="_blank">pdf</a>]

<h2>Identifying Mislabeled Images in Supervised Learning Utilizing Autoencoder. (arXiv:2011.03667v1 [cs.CV])</h2>
<h3>Yunhao Yang</h3>
<p>Supervised learning is based on the assumption that the ground truth in the
training data is accurate. However, this may not be guaranteed in real-world
settings. Inaccurate training data will result in some unexpected predictions.
In image classification, incorrect labels may cause the classification model to
be inaccurate as well. In this paper, I am going to apply unsupervised
techniques on the training data before training the classification network. A
convolutional autoencoder is applied to encode and reconstruct images. The
encoder will project the image data on to latent space. In the latent space,
image features are preserved in a lower dimension. Samples with similar
features are likely to have the same label. Noised samples can be classified in
the latent space. Image projections on the feature space are compared with its
neighbors. The images with different labels with their neighbors are considered
as incorrectly labeled data. These incorrectly labeled data are visualized as
outliers in the latent space. After incorrect labels are detected, I will
remove the samples which consist of incorrect labels. Thus the training data
can be directly used in training the supervised learning network.
</p>
<a href="http://arxiv.org/abs/2011.03667" target="_blank">arXiv:2011.03667</a> [<a href="http://arxiv.org/pdf/2011.03667" target="_blank">pdf</a>]

<h2>Domain-Aware Unsupervised Hyperspectral Reconstruction for Aerial Image Dehazing. (arXiv:2011.03677v1 [cs.CV])</h2>
<h3>Aditya Mehta, Harsh Sinha, Murari Mandal, Pratik Narang</h3>
<p>Haze removal in aerial images is a challenging problem due to considerable
variation in spatial details and varying contrast. Changes in particulate
matter density often lead to degradation in visibility. Therefore, several
approaches utilize multi-spectral data as auxiliary information for haze
removal. In this paper, we propose SkyGAN for haze removal in aerial images.
SkyGAN consists of 1) a domain-aware hazy-to-hyperspectral (H2H) module, and 2)
a conditional GAN (cGAN) based multi-cue image-to-image translation module
(I2I) for dehazing. The proposed H2H module reconstructs several visual bands
from RGB images in an unsupervised manner, which overcomes the lack of hazy
hyperspectral aerial image datasets. The module utilizes task supervision and
domain adaptation in order to create a "hyperspectral catalyst" for image
dehazing. The I2I module uses the hyperspectral catalyst along with a
12-channel multi-cue input and performs effective image dehazing by utilizing
the entire visual spectrum. In addition, this work introduces a new dataset,
called Hazy Aerial-Image (HAI) dataset, that contains more than 65,000 pairs of
hazy and ground truth aerial images with realistic, non-homogeneous haze of
varying density. The performance of SkyGAN is evaluated on the recent
SateHaze1k dataset as well as the HAI dataset. We also present a comprehensive
evaluation of HAI dataset with a representative set of state-of-the-art
techniques in terms of PSNR and SSIM.
</p>
<a href="http://arxiv.org/abs/2011.03677" target="_blank">arXiv:2011.03677</a> [<a href="http://arxiv.org/pdf/2011.03677" target="_blank">pdf</a>]

<h2>When Optimizing $f$-divergence is Robust with Label Noise. (arXiv:2011.03687v1 [cs.LG])</h2>
<h3>Jiaheng Wei, Yang Liu</h3>
<p>We show when maximizing a properly defined $f$-divergence measure with
respect to a classifier's predictions and the supervised labels is robust with
label noise. Leveraging its variational form, we derive a nice decoupling
property for this particular $f$-divergence when label noise presents, where
the divergence is shown to be a linear combination of the variational
difference defined on the clean distribution and a bias term introduced due to
the noise. The above derivation helps us analyze the robustness of this measure
for different $f$-divergence functions. With established robustness, this
family of $f$-divergence functions arises as useful metrics for the problem of
learning with noisy labels, which do not require the specification of the
labels' noise rate. When they are possibly not robust, we propose fixes to make
them so. In addition to the analytical results, we present thorough
experimental studies.
</p>
<a href="http://arxiv.org/abs/2011.03687" target="_blank">arXiv:2011.03687</a> [<a href="http://arxiv.org/pdf/2011.03687" target="_blank">pdf</a>]

<h2>Depthwise Multiception Convolution for Reducing Network Parameters without Sacrificing Accuracy. (arXiv:2011.03701v1 [cs.CV])</h2>
<h3>Guoqing Bao, Manuel B. Graeber, Xiuying Wang</h3>
<p>Deep convolutional neural networks have been proven successful in multiple
benchmark challenges in recent years. However, the performance improvements are
heavily reliant on increasingly complex network architecture and a high number
of parameters, which require ever increasing amounts of storage and memory
capacity. Depthwise separable convolution (DSConv) can effectively reduce the
number of required parameters through decoupling standard convolution into
spatial and cross-channel convolution steps. However, the method causes a
degradation of accuracy. To address this problem, we present depthwise
multiception convolution, termed Multiception, which introduces layer-wise
multiscale kernels to learn multiscale representations of all individual input
channels simultaneously. We have carried out the experiment on four benchmark
datasets, i.e. Cifar-10, Cifar-100, STL-10 and ImageNet32x32, using five
popular CNN models, Multiception achieved accuracy promotion in all models and
demonstrated higher accuracy performance compared to related works. Meanwhile,
Multiception significantly reduces the number of parameters of standard
convolution-based models by 32.48% on average while still preserving accuracy.
</p>
<a href="http://arxiv.org/abs/2011.03701" target="_blank">arXiv:2011.03701</a> [<a href="http://arxiv.org/pdf/2011.03701" target="_blank">pdf</a>]

<h2>TB-Net: A Three-Stream Boundary-Aware Network for Fine-Grained Pavement Disease Segmentation. (arXiv:2011.03703v1 [cs.CV])</h2>
<h3>Yujia Zhang, Qianzhong Li, Xiaoguang Zhao, Min Tan</h3>
<p>Regular pavement inspection plays a significant role in road maintenance for
safety assurance. Existing methods mainly address the tasks of crack detection
and segmentation that are only tailored for long-thin crack disease. However,
there are many other types of diseases with a wider variety of sizes and
patterns that are also essential to segment in practice, bringing more
challenges towards fine-grained pavement inspection. In this paper, our goal is
not only to automatically segment cracks, but also to segment other complex
pavement diseases as well as typical landmarks (markings, runway lights, etc.)
and commonly seen water/oil stains in a single model. To this end, we propose a
three-stream boundary-aware network (TB-Net). It consists of three streams
fusing the low-level spatial and the high-level contextual representations as
well as the detailed boundary information. Specifically, the spatial stream
captures rich spatial features. The context stream, where an attention
mechanism is utilized, models the contextual relationships over local features.
The boundary stream learns detailed boundaries using a global-gated convolution
to further refine the segmentation outputs. The network is trained using a
dual-task loss in an end-to-end manner, and experiments on a newly collected
fine-grained pavement disease dataset show the effectiveness of our TB-Net.
</p>
<a href="http://arxiv.org/abs/2011.03703" target="_blank">arXiv:2011.03703</a> [<a href="http://arxiv.org/pdf/2011.03703" target="_blank">pdf</a>]

<h2>Blind Motion Deblurring through SinGAN Architecture. (arXiv:2011.03705v1 [cs.CV])</h2>
<h3>Harshil Jain, Rohit Patil, Indra Deep Mastan, Shanmuganathan Raman</h3>
<p>Blind motion deblurring involves reconstructing a sharp image from an
observation that is blurry. It is a problem that is ill-posed and lies in the
categories of image restoration problems. The training data-based methods for
image deblurring mostly involve training models that take a lot of time. These
models are data-hungry i.e., they require a lot of training data to generate
satisfactory results. Recently, there are various image feature learning
methods developed which relieve us of the need for training data and perform
image restoration and image synthesis, e.g., DIP, InGAN, and SinGAN. SinGAN is
a generative model that is unconditional and could be learned from a single
natural image. This model primarily captures the internal distribution of the
patches which are present in the image and is capable of generating samples of
varied diversity while preserving the visual content of the image. Images
generated from the model are very much like real natural images. In this paper,
we focus on blind motion deblurring through SinGAN architecture.
</p>
<a href="http://arxiv.org/abs/2011.03705" target="_blank">arXiv:2011.03705</a> [<a href="http://arxiv.org/pdf/2011.03705" target="_blank">pdf</a>]

<h2>DeepCFL: Deep Contextual Features Learning from a Single Image. (arXiv:2011.03712v1 [cs.CV])</h2>
<h3>Indra Deep Mastan, Shanmuganathan Raman</h3>
<p>Recently, there is a vast interest in developing image feature learning
methods that are independent of the training data, such as deep image prior,
InGAN, SinGAN, and DCIL. These methods are unsupervised and are used to perform
low-level vision tasks such as image restoration, image editing, and image
synthesis. In this work, we proposed a new training data-independent framework,
called Deep Contextual Features Learning (DeepCFL), to perform image synthesis
and image restoration based on the semantics of the input image. The contextual
features are simply the high dimensional vectors representing the semantics of
the given image. DeepCFL is a single image GAN framework that learns the
distribution of the context vectors from the input image. We show the
performance of contextual learning in various challenging scenarios:
outpainting, inpainting, and restoration of randomly removed pixels. DeepCFL is
applicable when the input source image and the generated target image are not
aligned. We illustrate image synthesis using DeepCFL for the task of image
resizing.
</p>
<a href="http://arxiv.org/abs/2011.03712" target="_blank">arXiv:2011.03712</a> [<a href="http://arxiv.org/pdf/2011.03712" target="_blank">pdf</a>]

<h2>Coarse- and Fine-grained Attention Network with Background-aware Loss for Crowd Density Map Estimation. (arXiv:2011.03721v1 [cs.CV])</h2>
<h3>Liangzi Rong, Chunping Li</h3>
<p>In this paper, we present a novel method Coarse- and Fine-grained Attention
Network (CFANet) for generating high-quality crowd density maps and people
count estimation by incorporating attention maps to better focus on the crowd
area. We devise a from-coarse-to-fine progressive attention mechanism by
integrating Crowd Region Recognizer (CRR) and Density Level Estimator (DLE)
branch, which can suppress the influence of irrelevant background and assign
attention weights according to the crowd density levels, because generating
accurate fine-grained attention maps directly is normally difficult. We also
employ a multi-level supervision mechanism to assist the backpropagation of
gradient and reduce overfitting. Besides, we propose a Background-aware
Structural Loss (BSL) to reduce the false recognition ratio while improving the
structural similarity to groundtruth. Extensive experiments on commonly used
datasets show that our method can not only outperform previous state-of-the-art
methods in terms of count accuracy but also improve the image quality of
density maps as well as reduce the false recognition ratio.
</p>
<a href="http://arxiv.org/abs/2011.03721" target="_blank">arXiv:2011.03721</a> [<a href="http://arxiv.org/pdf/2011.03721" target="_blank">pdf</a>]

<h2>Template Controllable keywords-to-text Generation. (arXiv:2011.03722v1 [cs.AI])</h2>
<h3>Abhijit Mishra, Md Faisal Mahbub Chowdhury, Sagar Manohar, Dan Gutfreund, Karthik Sankaranarayanan</h3>
<p>This paper proposes a novel neural model for the understudied task of
generating text from keywords. The model takes as input a set of un-ordered
keywords, and part-of-speech (POS) based template instructions. This makes it
ideal for surface realization in any NLG setup. The framework is based on the
encode-attend-decode paradigm, where keywords and templates are encoded first,
and the decoder judiciously attends over the contexts derived from the encoded
keywords and templates to generate the sentences. Training exploits weak
supervision, as the model trains on a large amount of labeled data with
keywords and POS based templates prepared through completely automatic means.
Qualitative and quantitative performance analyses on publicly available
test-data in various domains reveal our system's superiority over baselines,
built using state-of-the-art neural machine translation and controllable
transfer techniques. Our approach is indifferent to the order of input
keywords.
</p>
<a href="http://arxiv.org/abs/2011.03722" target="_blank">arXiv:2011.03722</a> [<a href="http://arxiv.org/pdf/2011.03722" target="_blank">pdf</a>]

<h2>A Strong Baseline for Crowd Counting and Unsupervised People Localization. (arXiv:2011.03725v1 [cs.CV])</h2>
<h3>Liangzi Rong, Chunping Li</h3>
<p>In this paper, we explore a strong baseline for crowd counting and an
unsupervised people localization algorithm based on estimated density maps.
Firstly, existing methods achieve state-of-the-art performance based on
different backbones and kinds of training tricks. We collect different
backbones and training tricks and evaluate the impact of changing them and
develop an efficient pipeline for crowd counting, which decreases MAE and RMSE
significantly on multiple datasets. We also propose a clustering algorithm
named isolated KMeans to locate the heads in density maps. This method can
divide the density maps into subregions and find the centers under local count
constraints without training any parameter and can be integrated with existing
methods easily.
</p>
<a href="http://arxiv.org/abs/2011.03725" target="_blank">arXiv:2011.03725</a> [<a href="http://arxiv.org/pdf/2011.03725" target="_blank">pdf</a>]

<h2>Enhash: A Fast Streaming Algorithm For Concept Drift Detection. (arXiv:2011.03729v1 [cs.LG])</h2>
<h3>Aashi Jindal, Prashant Gupta, Debarka Sengupta, Jayadeva</h3>
<p>We propose Enhash, a fast ensemble learner that detects \textit{concept
drift} in a data stream. A stream may consist of abrupt, gradual, virtual, or
recurring events, or a mixture of various types of drift. Enhash employs
projection hash to insert an incoming sample. We show empirically that the
proposed method has competitive performance to existing ensemble learners in
much lesser time. Also, Enhash has moderate resource requirements. Experiments
relevant to performance comparison were performed on 6 artificial and 4 real
data sets consisting of various types of drifts.
</p>
<a href="http://arxiv.org/abs/2011.03729" target="_blank">arXiv:2011.03729</a> [<a href="http://arxiv.org/pdf/2011.03729" target="_blank">pdf</a>]

<h2>On the Privacy Risks of Algorithmic Fairness. (arXiv:2011.03731v1 [stat.ML])</h2>
<h3>Hongyan Chang, Reza Shokri</h3>
<p>Algorithmic fairness and privacy are essential elements of trustworthy
machine learning for critical decision making processes. Fair machine learning
algorithms are developed to minimize discrimination against protected groups in
machine learning. This is achieved, for example, by imposing a constraint on
the model to equalize its behavior across different groups. This can
significantly increase the influence of some training data points on the fair
model. We study how this change in influence can change the information leakage
of the model about its training data. We analyze the privacy risks of
statistical notions of fairness (i.e., equalized odds) through the lens of
membership inference attacks: inferring whether a data point was used for
training a model. We show that fairness comes at the cost of privacy. However,
this privacy cost is not distributed equally: the information leakage of fair
models increases significantly on the unprivileged subgroups, which suffer from
the discrimination in regular models. Furthermore, the more biased the
underlying data is, the higher the privacy cost of achieving fairness for the
unprivileged subgroups is. We demonstrate this effect on multiple datasets and
explain how fairness-aware learning impacts privacy.
</p>
<a href="http://arxiv.org/abs/2011.03731" target="_blank">arXiv:2011.03731</a> [<a href="http://arxiv.org/pdf/2011.03731" target="_blank">pdf</a>]

<h2>Human-Like Active Learning: Machines Simulating the Human Learning Process. (arXiv:2011.03733v1 [cs.LG])</h2>
<h3>Jaeseo Lim, Hwiyeol Jo, Byoung-Tak Zhang, Jooyong Park</h3>
<p>Although the use of active learning to increase learners' engagement has
recently been introduced in a variety of methods, empirical experiments are
lacking. In this study, we attempted to align two experiments in order to (1)
make a hypothesis for machine and (2) empirically confirm the effect of active
learning on learning. In Experiment 1, we compared the effect of a passive form
of learning to active form of learning. The results showed that active learning
had a greater learning outcomes than passive learning. In the machine
experiment based on the human result, we imitated the human active learning as
a form of knowledge distillation. The active learning framework performed
better than the passive learning framework. In the end, we showed not only that
we can make build better machine training framework through the human
experiment result, but also empirically confirm the result of human experiment
through imitated machine experiments; human-like active learning have crucial
effect on learning performance.
</p>
<a href="http://arxiv.org/abs/2011.03733" target="_blank">arXiv:2011.03733</a> [<a href="http://arxiv.org/pdf/2011.03733" target="_blank">pdf</a>]

<h2>Interventional Domain Adaptation. (arXiv:2011.03737v1 [cs.LG])</h2>
<h3>Jun Wen, Changjian Shui, Kun Kuang, Junsong Yuan, Zenan Huang, Zhefeng Gong, Nenggan Zheng</h3>
<p>Domain adaptation (DA) aims to transfer discriminative features learned from
source domain to target domain. Most of DA methods focus on enhancing feature
transferability through domain-invariance learning. However, source-learned
discriminability itself might be tailored to be biased and unsafely
transferable by spurious correlations, \emph{i.e.}, part of source-specific
features are correlated with category labels. We find that standard
domain-invariance learning suffers from such correlations and incorrectly
transfers the source-specifics. To address this issue, we intervene in the
learning of feature discriminability using unlabeled target data to guide it to
get rid of the domain-specific part and be safely transferable. Concretely, we
generate counterfactual features that distinguish the domain-specifics from
domain-sharable part through a novel feature intervention strategy. To prevent
the residence of domain-specifics, the feature discriminability is trained to
be invariant to the mutations in the domain-specifics of counterfactual
features. Experimenting on typical \emph{one-to-one} unsupervised domain
adaptation and challenging domain-agnostic adaptation tasks, the consistent
performance improvements of our method over state-of-the-art approaches
validate that the learned discriminative features are more safely transferable
and generalize well to novel domains.
</p>
<a href="http://arxiv.org/abs/2011.03737" target="_blank">arXiv:2011.03737</a> [<a href="http://arxiv.org/pdf/2011.03737" target="_blank">pdf</a>]

<h2>Fast 3D Modeling of Anthropomorphic Robotic Hands Based on A Multi-layer Deformable Design. (arXiv:2011.03742v1 [cs.RO])</h2>
<h3>Li Tian, Hanhui Li, Muhammad Faaiz Khan Bin Abdul Halil, Nadia Magnenat Thalmann, Daniel Thalmann, Jianmin Zheng</h3>
<p>Current anthropomorphic robotic hands mainly focus on improving their
dexterity by devising new mechanical structures and actuation systems. However,
most of them rely on a single structure/system (e.g., bone-only) and ignore the
fact that the human hand is composed of multiple functional structures (e.g.,
skin, bones, muscles, and tendons). This not only increases the difficulty of
the design process but also lowers the robustness and flexibility of the
fabricated hand. Besides, other factors like customization, the time and cost
for production, and the degree of resemblance between human hands and robotic
hands, remain omitted. To tackle these problems, this study proposes a 3D
printable multi-layer design that models the hand with the layers of skin,
tissues, and bones. The proposed design first obtains the 3D surface model of a
target hand via 3D scanning, and then generates the 3D bone models from the
surface model based on a fast template matching method. To overcome the
disadvantage of the rigid bone layer in deformation, the tissue layer is
introduced and represented by a concentric tube based structure, of which the
deformability can be explicitly controlled by a parameter. Besides, a low-cost
yet effective underactuated system is adopted to drive the fabricated hand. The
proposed design is tested with 33 widely used object grasping types, as well as
special objects like fragile silken tofu, and outperforms previous designs
remarkably. With the proposed design, anthropomorphic robotic hands can be
produced fast with low cost, and be customizable and deformable.
</p>
<a href="http://arxiv.org/abs/2011.03742" target="_blank">arXiv:2011.03742</a> [<a href="http://arxiv.org/pdf/2011.03742" target="_blank">pdf</a>]

<h2>B-GAP: Behavior-Guided Action Prediction for Autonomous Navigation. (arXiv:2011.03748v1 [cs.RO])</h2>
<h3>Angelos Mavrogiannis, Rohan Chandra, Dinesh Manocha</h3>
<p>We present a novel learning algorithm for action prediction and local
navigation for autonomous driving. Our approach classifies the driver behavior
of other vehicles or road-agents (aggressive or conservative) and takes that
into account for decision making and safe driving. We present a behavior-driven
simulator that can generate trajectories corresponding to different levels of
aggressive behaviors and use our simulator to train a policy using graph
convolutional networks. We use a reinforcement learning-based navigation scheme
that uses a proximity graph of traffic agents and computes a safe trajectory
for the ego-vehicle that accounts for aggressive driver maneuvers such as
overtaking, over-speeding, weaving, and sudden lane changes. We have integrated
our algorithm with OpenAI gym-based "Highway-Env" simulator and demonstrate the
benefits in terms of improved navigation in different scenarios.
</p>
<a href="http://arxiv.org/abs/2011.03748" target="_blank">arXiv:2011.03748</a> [<a href="http://arxiv.org/pdf/2011.03748" target="_blank">pdf</a>]

<h2>Robustness and Diversity Seeking Data-Free Knowledge Distillation. (arXiv:2011.03749v1 [cs.LG])</h2>
<h3>Pengchao Han, Jihong Park, Shiqiang Wang, Yejun Liu</h3>
<p>Knowledge distillation (KD) has enabled remarkable progress in model
compression and knowledge transfer. However, KD requires a large volume of
original data or their representation statistics that are not usually available
in practice. Data-free KD has recently been proposed to resolve this problem,
wherein teacher and student models are fed by a synthetic sample generator
trained from the teacher. Nonetheless, existing data-free KD methods rely on
fine-tuning of weights to balance multiple losses, and ignore the diversity of
generated samples, resulting in limited accuracy and robustness. To overcome
this challenge, we propose robustness and diversity seeking data-free KD
(RDSKD) in this paper. The generator loss function is crafted to produce
samples with high authenticity, class diversity, and inter-sample diversity.
Without real data, the objectives of seeking high sample authenticity and class
diversity often conflict with each other, causing frequent loss fluctuations.
We mitigate this by exponentially penalizing loss increments. With MNIST,
CIFAR-10, and SVHN datasets, our experiments show that RDSKD achieves higher
accuracy with more robustness over different hyperparameter settings, compared
to other data-free KD methods such as DAFL, MSKD, ZSKD, and DeepInversion.
</p>
<a href="http://arxiv.org/abs/2011.03749" target="_blank">arXiv:2011.03749</a> [<a href="http://arxiv.org/pdf/2011.03749" target="_blank">pdf</a>]

<h2>A Multi-stream Convolutional Neural Network for Micro-expression Recognition Using Optical Flow and EVM. (arXiv:2011.03756v1 [cs.CV])</h2>
<h3>Jinming Liu, Ke Li, Baolin Song, Li Zhao</h3>
<p>Micro-expression (ME) recognition plays a crucial role in a wide range of
applications, particularly in public security and psychotherapy. Recently,
traditional methods rely excessively on machine learning design and the
recognition rate is not high enough for its practical application because of
its short duration and low intensity. On the other hand, some methods based on
deep learning also cannot get high accuracy due to problems such as the
imbalance of databases. To address these problems, we design a multi-stream
convolutional neural network (MSCNN) for ME recognition in this paper.
Specifically, we employ EVM and optical flow to magnify and visualize subtle
movement changes in MEs and extract the masks from the optical flow images. And
then, we add the masks, optical flow images, and grayscale images into the
MSCNN. After that, in order to overcome the imbalance of databases, we added a
random over-sampler after the Dense Layer of the neural network. Finally,
extensive experiments are conducted on two public ME databases: CASME II and
SAMM. Compared with many recent state-of-the-art approaches, our method
achieves more promising recognition results.
</p>
<a href="http://arxiv.org/abs/2011.03756" target="_blank">arXiv:2011.03756</a> [<a href="http://arxiv.org/pdf/2011.03756" target="_blank">pdf</a>]

<h2>Text-to-Image Generation Grounded by Fine-Grained User Attention. (arXiv:2011.03775v1 [cs.CV])</h2>
<h3>Jing Yu Koh, Jason Baldridge, Honglak Lee, Yinfei Yang</h3>
<p>Localized Narratives is a dataset with detailed natural language descriptions
of images paired with mouse traces that provide a sparse, fine-grained visual
grounding for phrases. We propose TReCS, a sequential model that exploits this
grounding to generate images. TReCS uses descriptions to retrieve segmentation
masks and predict object labels aligned with mouse traces. These alignments are
used to select and position masks to generate a fully covered segmentation
canvas; the final image is produced by a segmentation-to-image generator using
this canvas. This multi-step, retrieval-based approach outperforms existing
direct text-to-image generation models on both automatic metrics and human
evaluations: overall, its generated images are more photo-realistic and better
match descriptions.
</p>
<a href="http://arxiv.org/abs/2011.03775" target="_blank">arXiv:2011.03775</a> [<a href="http://arxiv.org/pdf/2011.03775" target="_blank">pdf</a>]

<h2>Rapid Pose Label Generation through Sparse Representation of Unknown Objects. (arXiv:2011.03790v1 [cs.CV])</h2>
<h3>Rohan Pratap Singh, Mehdi Benallegue, Yusuke Yoshiyasu, Fumio Kanehiro</h3>
<p>Deep Convolutional Neural Networks (CNNs) have been successfully deployed on
robots for 6-DoF object pose estimation through visual perception. However,
obtaining labeled data on a scale required for the supervised training of CNNs
is a difficult task - exacerbated if the object is novel and a 3D model is
unavailable. To this end, this work presents an approach for rapidly generating
real-world, pose-annotated RGB-D data for unknown objects. Our method not only
circumvents the need for a prior 3D object model (textured or otherwise) but
also bypasses complicated setups of fiducial markers, turntables, and sensors.
With the help of a human user, we first source minimalistic labelings of an
ordered set of arbitrarily chosen keypoints over a set of RGB-D videos. Then,
by solving an optimization problem, we combine these labels under a world frame
to recover a sparse, keypoint-based representation of the object. The sparse
representation leads to the development of a dense model and the pose labels
for each image frame in the set of scenes. We show that the sparse model can
also be efficiently used for scaling to a large number of new scenes. We
demonstrate the practicality of the generated labeled dataset by training a
pipeline for 6-DoF object pose estimation and a pixel-wise segmentation
network.
</p>
<a href="http://arxiv.org/abs/2011.03790" target="_blank">arXiv:2011.03790</a> [<a href="http://arxiv.org/pdf/2011.03790" target="_blank">pdf</a>]

<h2>Deep Learning Analysis and Age Prediction from Shoeprints. (arXiv:2011.03794v1 [cs.CV])</h2>
<h3>Muhammad Hassan (1), Yan Wang (1), Di Wang (2), Daixi Li (3), Yanchun Liang (1), You Zhou (1,2), Dong Xu (4) ((1) Computer Science and Technology, Jilin University, Changchun, (2) Joint NTU-UBC Research Centre of Excellence in Active Living for the Elderly, Nanyang Technological University, Singapore, (3) Everspray Science and Technology Company Ltd., (4) Department of Electrical Engineering and Computer Science, University of Missouri, Columbia)</h3>
<p>Human walking and gaits involve several complex body parts and are influenced
by personality, mood, social and cultural traits, and aging. These factors are
reflected in shoeprints, which in turn can be used to predict age, a problem
not systematically addressed using any computational approach. We collected
100,000 shoeprints of subjects ranging from 7 to 80 years old and used the data
to develop a deep learning end-to-end model ShoeNet to analyze age-related
patterns and predict age. The model integrates various convolutional neural
network models together using a skip mechanism to extract age-related features,
especially in pressure and abrasion regions from pair-wise shoeprints. The
results show that 40.23% of the subjects had prediction errors within 5-years
of age and the prediction accuracy for gender classification reached 86.07%.
Interestingly, the age-related features mostly reside in the asymmetric
differences between left and right shoeprints. The analysis also reveals
interesting age-related and gender-related patterns in the pressure
distributions on shoeprints; in particular, the pressure forces spread from the
middle of the toe toward outside regions over age with gender-specific
variations on heel regions. Such statistics provide insight into new methods
for forensic investigations, medical studies of gait-pattern disorders,
biometrics, and sport studies.
</p>
<a href="http://arxiv.org/abs/2011.03794" target="_blank">arXiv:2011.03794</a> [<a href="http://arxiv.org/pdf/2011.03794" target="_blank">pdf</a>]

<h2>Symmetric Parallax Attention for Stereo Image Super-Resolution. (arXiv:2011.03802v1 [cs.CV])</h2>
<h3>Yingqian Wang, Xinyi Ying, Longguang Wang, Jungang Yang, Wei An, Yulan Guo</h3>
<p>Although recent years have witnessed the great advances in stereo image
super-resolution (SR), the beneficial information provided by binocular systems
has not been fully used. Since stereo images are highly symmetric under
epipolar constraint, in this paper, we improve the performance of stereo image
SR by exploiting symmetry cues in stereo image pairs. Specifically, we propose
a symmetric bi-directional parallax attention module (biPAM) and an inline
occlusion handling scheme to effectively interact cross-view information. Then,
we design a Siamese network equipped with a biPAM to super-resolve both sides
of views in a highly symmetric manner. Finally, we design several
illuminance-robust bilateral losses to enforce stereo consistency. Experiments
on four public datasets have demonstrated the superiority of our method. As
compared to PASSRnet, our method achieves notable performance improvements with
a comparable model size. Source codes are available at
https://github.com/YingqianWang/iPASSR.
</p>
<a href="http://arxiv.org/abs/2011.03802" target="_blank">arXiv:2011.03802</a> [<a href="http://arxiv.org/pdf/2011.03802" target="_blank">pdf</a>]

<h2>Sim-to-Real Transfer for Vision-and-Language Navigation. (arXiv:2011.03807v1 [cs.CV])</h2>
<h3>Peter Anderson, Ayush Shrivastava, Joanne Truong, Arjun Majumdar, Devi Parikh, Dhruv Batra, Stefan Lee</h3>
<p>We study the challenging problem of releasing a robot in a previously unseen
environment, and having it follow unconstrained natural language navigation
instructions. Recent work on the task of Vision-and-Language Navigation (VLN)
has achieved significant progress in simulation. To assess the implications of
this work for robotics, we transfer a VLN agent trained in simulation to a
physical robot. To bridge the gap between the high-level discrete action space
learned by the VLN agent, and the robot's low-level continuous action space, we
propose a subgoal model to identify nearby waypoints, and use domain
randomization to mitigate visual domain differences. For accurate sim and real
comparisons in parallel environments, we annotate a 325m2 office space with
1.3km of navigation instructions, and create a digitized replica in simulation.
We find that sim-to-real transfer to an environment not seen in training is
successful if an occupancy map and navigation graph can be collected and
annotated in advance (success rate of 46.8% vs. 55.9% in sim), but much more
challenging in the hardest setting with no prior mapping at all (success rate
of 22.5%).
</p>
<a href="http://arxiv.org/abs/2011.03807" target="_blank">arXiv:2011.03807</a> [<a href="http://arxiv.org/pdf/2011.03807" target="_blank">pdf</a>]

<h2>MAGIC: Learning Macro-Actions for Online POMDP Planning using Generator-Critic. (arXiv:2011.03813v1 [cs.RO])</h2>
<h3>Yiyuan Lee, Panpan Cai, David Hsu</h3>
<p>When robots operate in the real-world, they need to handle uncertainties in
sensing, acting, and the environment. Many tasks also require reasoning about
long-term consequences of robot decisions. The partially observable Markov
decision process (POMDP) offers a principled approach for planning under
uncertainty. However, its computational complexity grows exponentially with the
planning horizon. We propose to use temporally-extended macro-actions to cut
down the effective planning horizon and thus the exponential factor of the
complexity. We propose Macro-Action Generator-Critic (MAGIC), an algorithm that
learns a macro-action generator from data, and uses the learned macro-actions
to perform long-horizon planning. MAGIC learns the generator using experience
provided by an online planner, and in-turn conditions the planner using the
generated macro-actions. We evaluate MAGIC on several long-term planning tasks,
showing that it significantly outperforms planning using primitive actions,
hand-crafted macro-actions, as well as naive reinforcement learning in both
simulation and on a real robot.
</p>
<a href="http://arxiv.org/abs/2011.03813" target="_blank">arXiv:2011.03813</a> [<a href="http://arxiv.org/pdf/2011.03813" target="_blank">pdf</a>]

<h2>Towards Resolving the Challenge of Long-tail Distribution in UAV Images for Object Detection. (arXiv:2011.03822v1 [cs.CV])</h2>
<h3>Weiping Yu, Taojiannan Yang, Chen Chen</h3>
<p>Existing methods for object detection in UAV images ignored an important
challenge - imbalanced class distribution in UAV images - which leads to poor
performance on tail classes. We systematically investigate existing solutions
to long-tail problems and unveil that re-balancing methods that are effective
on natural image datasets cannot be trivially applied to UAV datasets. To this
end, we rethink long-tailed object detection in UAV images and propose the Dual
Sampler and Head detection Network (DSHNet), which is the first work that aims
to resolve long-tail distribution in UAV images. The key components in DSHNet
include Class-Biased Samplers (CBS) and Bilateral Box Heads (BBH), which are
developed to cope with tail classes and head classes in a dual-path manner.
Without bells and whistles, DSHNet significantly boosts the performance of tail
classes on different detection frameworks. Moreover, DSHNet significantly
outperforms base detectors and generic approaches for long-tail problems on
VisDrone and UAVDT datasets. It achieves new state-of-the-art performance when
combining with image cropping methods. Code is available at
https://github.com/we1pingyu/DSHNet
</p>
<a href="http://arxiv.org/abs/2011.03822" target="_blank">arXiv:2011.03822</a> [<a href="http://arxiv.org/pdf/2011.03822" target="_blank">pdf</a>]

<h2>Model-based Shape Control of Tensegrity Robotic Systems. (arXiv:2011.03829v1 [cs.RO])</h2>
<h3>Raman Goyal, Manoranjan Majji, Robert E. Skelton</h3>
<p>This paper proposes a model-based approach to control the shape of a
tensegrity system by driving its node position locations. The nonlinear
dynamics of the tensegrity system is used to regulate position, velocity, and
acceleration to the specified reference trajectory. State feedback control
design is used to obtain the solution for the control variable as a linear
programming problem. Shape control for the gyroscopic tensegrity systems is
discussed, and it is observed that these systems increase the reachable space
for the structure by providing independent control over certain rotational
degrees of freedom. Disturbance rejection of the tensegrity system is further
studied in the paper. A methodology to calculate the control gains to bound the
errors for five different types of problems is provided. The formulation uses a
Linear Matrix Inequality (LMI) approach to stipulate the desired performance
bounds on the error for $\mathcal{H}_\infty$, generalized $\mathcal{H}_2$, LQR,
covariance control and stabilizing control problem. A high degree of freedom
tensegrity $T_2D_1$ robotic arm is used as an example to show the efficacy of
the formulation.
</p>
<a href="http://arxiv.org/abs/2011.03829" target="_blank">arXiv:2011.03829</a> [<a href="http://arxiv.org/pdf/2011.03829" target="_blank">pdf</a>]

<h2>On the spatial attention in Spatio-Temporal Graph Convolutional Networks for skeleton-based human action recognition. (arXiv:2011.03833v1 [cs.CV])</h2>
<h3>Negar Heidari, Alexandros Iosifidis</h3>
<p>Graph convolutional networks (GCNs) achieved promising performance in
skeleton-based human action recognition by modeling a sequence of skeletons as
a spatio-temporal graph. Most of the recently proposed GCN-based methods
improve the performance by learning the graph structure at each layer of the
network using a spatial attention applied on a predefined graph Adjacency
matrix that is optimized jointly with model's parameters in an end-to-end
manner. In this paper, we analyze the spatial attention used in spatio-temporal
GCN layers and propose a symmetric spatial attention for better reflecting the
symmetric property of the relative positions of the human body joints when
executing actions. We also highlight the connection of spatio-temporal GCN
layers employing additive spatial attention to bilinear layers, and we propose
the spatio-temporal bilinear network (ST-BLN) which does not require the use of
predefined Adjacency matrices and allows for more flexible design of the model.
Experimental results show that the three models lead to effectively the same
performance. Moreover, by exploiting the flexibility provided by the proposed
ST-BLN, one can increase the efficiency of the model.
</p>
<a href="http://arxiv.org/abs/2011.03833" target="_blank">arXiv:2011.03833</a> [<a href="http://arxiv.org/pdf/2011.03833" target="_blank">pdf</a>]

<h2>Implementing Behavior Trees using Three-Valued Logic. (arXiv:2011.03835v1 [cs.AI])</h2>
<h3>Thibaud de Souza</h3>
<p>With consideration to behavior trees and their relevance to planning and
control, within and without game development, the distinction between stateful
and stateless models is discussed; a three-valued logic bridging traditional
control flow with behavior trees is introduced, and a C# implementation is
presented.
</p>
<a href="http://arxiv.org/abs/2011.03835" target="_blank">arXiv:2011.03835</a> [<a href="http://arxiv.org/pdf/2011.03835" target="_blank">pdf</a>]

<h2>SeqGenSQL -- A Robust Sequence Generation Model for Structured Query Language. (arXiv:2011.03836v1 [cs.AI])</h2>
<h3>Ning Li, Bethany Keller, Mark Butler, Daniel Cer</h3>
<p>We explore using T5 (Raffel et al. (2019)) to directly translate natural
language questions into SQL statements. General purpose natural language that
interfaces to information stored within databases requires flexibly translating
natural language questions into database queries. The best performing
text-to-SQL systems approach this task by first converting questions into an
intermediate logical form (LF) (Lyu et al. (2020)). While LFs provide a
convenient intermediate representation and simplify query generation, they
introduce an additional layer of complexity and annotation requirements.
However, weakly supervised modeling that directly converts questions to SQL
statements has proven more difficult without the scaffolding provided by LFs
(Min et al. (2019)). We approach direct conversion of questions to SQL
statements using T5 (Raffel et al. (2019)), a pre-trained textto-text
generation model, modified to support pointer-generator style decoding (See et
al. (2017)). We explore using question augmentation with table schema
information and the use of automatically generated silver training data. The
resulting model achieves 90.5% execution accuracy on the WikiSQL (Zhong et al.
(2017)) test data set, a new state-of-the-art on weakly supervised SQL
generation. The performance improvement is 6.6% absolute over the prior
state-of-the-art (Min et al. (2019)) and approaches the performance of
state-ofthe-art systems making use of LFs.
</p>
<a href="http://arxiv.org/abs/2011.03836" target="_blank">arXiv:2011.03836</a> [<a href="http://arxiv.org/pdf/2011.03836" target="_blank">pdf</a>]

<h2>Autonomous Intruder Detection Using a ROS-Based Multi-Robot System Equipped with 2D-LiDAR Sensors. (arXiv:2011.03838v1 [cs.RO])</h2>
<h3>Mashnoon Islam, Touhid Ahmed, Abu Tammam Bin Nuruddin, Mashuda Islam, Shahnewaz Siddique</h3>
<p>The application of autonomous mobile robots in robotic security platforms is
becoming a promising field of innovation due to their adaptive capability of
responding to potential disturbances perceived through a wide range of sensors.
Researchers have proposed systems that either focus on utilizing a single
mobile robot or a system of cooperative multiple robots. However, very few of
the proposed works, particularly in the field of multi-robot systems, are
completely dependent on LiDAR sensors for achieving various tasks. This is
essential when other sensors on a robot fail to provide peak performance in
particular conditions, such as a camera operating in the absence of light. This
paper proposes a multi-robot system that is developed using ROS (Robot
Operating System) for intruder detection in a single-range-sensor-per-robot
scenario with centralized processing of detections from all robots by our
central bot MIDNet (Multiple Intruder Detection Network). This work is aimed at
providing an autonomous multi-robot security solution for a warehouse in the
absence of human personnel.
</p>
<a href="http://arxiv.org/abs/2011.03838" target="_blank">arXiv:2011.03838</a> [<a href="http://arxiv.org/pdf/2011.03838" target="_blank">pdf</a>]

<h2>Deep traffic light detection by overlaying synthetic context on arbitrary natural images. (arXiv:2011.03841v1 [cs.CV])</h2>
<h3>Jean Pablo Vieira de Mello, Lucas Tabelini, Rodrigo F. Berriel, Thiago M. Paix&#xe3;o, Alberto F. de Souza, Claudine Badue, Nicu Sebe, Thiago Oliveira-Santos</h3>
<p>Deep neural networks come as an effective solution to many problems
associated with autonomous driving. By providing real image samples with
traffic context to the network, the model learns to detect and classify
elements of interest, such as pedestrians, traffic signs, and traffic lights.
However, acquiring and annotating real data can be extremely costly in terms of
time and effort. In this context, we propose a method to generate artificial
traffic-related training data for deep traffic light detectors. This data is
generated using basic non-realistic computer graphics to blend fake traffic
scenes on top of arbitrary image backgrounds that are not related to the
traffic domain. Thus, a large amount of training data can be generated without
annotation efforts. Furthermore, it also tackles the intrinsic data imbalance
problem in traffic light datasets, caused mainly by the low amount of samples
of the yellow state. Experiments show that it is possible to achieve results
comparable to those obtained with real training data from the problem domain,
yielding an average mAP and an average F1-score which are each nearly 4 p.p.
higher than the respective metrics obtained with a real-world reference model.
</p>
<a href="http://arxiv.org/abs/2011.03841" target="_blank">arXiv:2011.03841</a> [<a href="http://arxiv.org/pdf/2011.03841" target="_blank">pdf</a>]

<h2>Universal Activation Function For Machine Learning. (arXiv:2011.03842v1 [cs.LG])</h2>
<h3>Brosnan Yuen, Minh Tu Hoang, Xiaodai Dong, Tao Lu</h3>
<p>This article proposes a Universal Activation Function (UAF) that achieves
near optimal performance in quantification, classification, and reinforcement
learning (RL) problems. For any given problem, the optimization algorithms are
able to evolve the UAF to a suitable activation function by tuning the UAF's
parameters. For the CIFAR-10 classification and VGG-8, the UAF converges to the
Mish like activation function, which has near optimal performance $F_{1} =
0.9017\pm0.0040$ when compared to other activation functions. For the
quantification of simulated 9-gas mixtures in 30 dB signal-to-noise ratio (SNR)
environments, the UAF converges to the identity function, which has near
optimal root mean square error of $0.4888 \pm 0.0032$ $\mu M$. In the
BipedalWalker-v2 RL dataset, the UAF achieves the 250 reward in $961 \pm 193$
epochs, which proves that the UAF converges in the lowest number of epochs.
Furthermore, the UAF converges to a new activation function in the
BipedalWalker-v2 RL dataset.
</p>
<a href="http://arxiv.org/abs/2011.03842" target="_blank">arXiv:2011.03842</a> [<a href="http://arxiv.org/pdf/2011.03842" target="_blank">pdf</a>]

<h2>MaskBot: Real-time Robotic Projection Mapping with Head Motion Tracking. (arXiv:2011.03844v1 [cs.RO])</h2>
<h3>Miguel Altamirano-Cabrera, Igor Usachev, Juan Heredia, Jonathan Tirado, Aleksey Fedoseev, Dzmitry Tsetserukou</h3>
<p>The projection mapping systems on the human face is limited by the latency
and the movement of the users. The area of the projection is restricted by the
position of the projectors and the cameras. We are introducing MaskBot, a
real-time projection mapping system operated by a 6 Degrees of Freedom (DoF)
collaborative robot. The collaborative robot locates the projector and camera
in normal position to the face of the user to increase the projection area and
to reduce the latency of the system. A webcam is used to detect the face and to
sense the robot-user distance to modify the projection size and orientation.
MaskBot projects different images on the face of the user, such as face
modifications, make-up, and logos. In contrast to the existing methods, the
presented system is the first that introduces a robotic projection mapping. One
of the prospective applications is to acquire a dataset of adversarial images
to challenge face detection DNN systems, such as Face ID.
</p>
<a href="http://arxiv.org/abs/2011.03844" target="_blank">arXiv:2011.03844</a> [<a href="http://arxiv.org/pdf/2011.03844" target="_blank">pdf</a>]

<h2>ZoomTouch: Multi-User Remote Robot Control in Zoom by DNN-based Gesture Recognition. (arXiv:2011.03845v1 [cs.RO])</h2>
<h3>Ilya Zakharkin, Arman Tsaturyan, Miguel Altamirano-Cabrera, Jonathan Tirado, Dzmitry Tsetserukou</h3>
<p>We present ZoomTouch - a breakthrough technology for multi-user control of
robot from Zoom in real-time by DNN-based gesture recognition. The users from
digital world can have a video conferencing and manipulate the robot to make
the dexterous manipulations with tangible objects. As the scenario, we proposed
the remote COVID-19 test Laboratory to considerably reduce the time to receive
the data and substitute medical assistant working in protective gear in close
proximity with infected cells. The proposed technology suggests a new type of
reality, where multi-users can jointly interact with remote object, e.g. make a
new building design, joint cooking in robotic kitchen, etc, and discuss/modify
the results at the same time.
</p>
<a href="http://arxiv.org/abs/2011.03845" target="_blank">arXiv:2011.03845</a> [<a href="http://arxiv.org/pdf/2011.03845" target="_blank">pdf</a>]

<h2>Google Trends Analysis of COVID-19. (arXiv:2011.03847v1 [cs.LG])</h2>
<h3>Hoang Long Nguyen, Zhenhe Pan, Hashim Abu-gellban, Fang Jin, Yuanlin Zhang</h3>
<p>The World Health Organization (WHO) announced that COVID-19 was a pandemic
disease on the 11th of March as there were 118K cases in several countries and
territories. Numerous researchers worked on forecasting the number of confirmed
cases since anticipating the growth of the cases helps governments adopting
knotty decisions to ease the lockdowns orders for their countries. These orders
help several people who have lost their jobs and support gravely impacted
businesses. Our research aims to investigate the relation between Google search
trends and the spreading of the novel coronavirus (COVID-19) over countries
worldwide, to predict the number of cases. We perform a correlation analysis on
the keywords of the related Google search trends according to the number of
confirmed cases reported by the WHO. After that, we applied several machine
learning techniques (Multiple Linear Regression, Non-negative Integer
Regression, Deep Neural Network), to forecast the number of confirmed cases
globally based on historical data as well as the hybrid data (Google search
trends). Our results show that Google search trends are highly associated with
the number of reported confirmed cases, where the Deep Learning approach
outperforms other forecasting techniques. We believe that it is not only a
promising approach for forecasting the confirmed cases of COVID-19, but also
for similar forecasting problems that are associated with the related Google
trends.
</p>
<a href="http://arxiv.org/abs/2011.03847" target="_blank">arXiv:2011.03847</a> [<a href="http://arxiv.org/pdf/2011.03847" target="_blank">pdf</a>]

<h2>Graph Kernels: State-of-the-Art and Future Challenges. (arXiv:2011.03854v1 [cs.LG])</h2>
<h3>Karsten Borgwardt, Elisabetta Ghisu, Felipe Llinares-L&#xf3;pez, Leslie O&#x27;Bray, Bastian Rieck</h3>
<p>Graph-structured data are an integral part of many application domains,
including chemoinformatics, computational biology, neuroimaging, and social
network analysis. Over the last fifteen years, numerous graph kernels, i.e.
kernel functions between graphs, have been proposed to solve the problem of
assessing the similarity between graphs, thereby making it possible to perform
predictions in both classification and regression settings. This manuscript
provides a review of existing graph kernels, their applications, software plus
data resources, and an empirical comparison of state-of-the-art graph kernels.
</p>
<a href="http://arxiv.org/abs/2011.03854" target="_blank">arXiv:2011.03854</a> [<a href="http://arxiv.org/pdf/2011.03854" target="_blank">pdf</a>]

<h2>Learning to Model and Ignore Dataset Bias with Mixed Capacity Ensembles. (arXiv:2011.03856v1 [cs.LG])</h2>
<h3>Christopher Clark, Mark Yatskar, Luke Zettlemoyer</h3>
<p>Many datasets have been shown to contain incidental correlations created by
idiosyncrasies in the data collection process. For example, sentence entailment
datasets can have spurious word-class correlations if nearly all contradiction
sentences contain the word "not", and image recognition datasets can have
tell-tale object-background correlations if dogs are always indoors. In this
paper, we propose a method that can automatically detect and ignore these kinds
of dataset-specific patterns, which we call dataset biases. Our method trains a
lower capacity model in an ensemble with a higher capacity model. During
training, the lower capacity model learns to capture relatively shallow
correlations, which we hypothesize are likely to reflect dataset bias. This
frees the higher capacity model to focus on patterns that should generalize
better. We ensure the models learn non-overlapping approaches by introducing a
novel method to make them conditionally independent. Importantly, our approach
does not require the bias to be known in advance. We evaluate performance on
synthetic datasets, and four datasets built to penalize models that exploit
known biases on textual entailment, visual question answering, and image
recognition tasks. We show improvement in all settings, including a 10 point
gain on the visual question answering dataset.
</p>
<a href="http://arxiv.org/abs/2011.03856" target="_blank">arXiv:2011.03856</a> [<a href="http://arxiv.org/pdf/2011.03856" target="_blank">pdf</a>]

<h2>Leveraging Forward Model Prediction Error for Learning Control. (arXiv:2011.03859v1 [cs.RO])</h2>
<h3>Sarah Bechtle, Bilal Hammoud, Akshara Rai, Franziska Meier, Ludovic Righetti</h3>
<p>Learning for model based control can be sample-efficient and generalize well,
however successfully learning models and controllers that represent the problem
at hand can be challenging for complex tasks. Using inaccurate models for
learning can lead to sub-optimal solutions, that are unlikely to perform well
in practice. In this work, we present a learning approach which iterates
between model learning and data collection and leverages forward model
prediction error for learning control. We show how using the controller's
prediction as input to a forward model can create a differentiable connection
between the controller and the model, allowing us to formulate a loss in the
state space. This lets us include forward model prediction error during
controller learning and we show that this creates a loss objective that
significantly improves learning on different motor control tasks. We provide
empirical and theoretical results that show the benefits of our method and
present evaluations in simulation for learning control on a 7 DoF manipulator
and an underactuated 12 DoF quadruped. We show that our approach successfully
learns controllers for challenging motor control tasks involving contact
switching.
</p>
<a href="http://arxiv.org/abs/2011.03859" target="_blank">arXiv:2011.03859</a> [<a href="http://arxiv.org/pdf/2011.03859" target="_blank">pdf</a>]

<h2>Latent Neural Differential Equations for Video Generation. (arXiv:2011.03864v1 [cs.CV])</h2>
<h3>Cade Gordon, Natalie Parde</h3>
<p>Generative Adversarial Networks have recently shown promise for video
generation, building off of the success of image generation while also
addressing a new challenge: time. Although time was analyzed in some early
work, the literature has not adequately grown with temporal modeling
developments. We propose studying the effects of Neural Differential Equations
to model the temporal dynamics of video generation. The paradigm of Neural
Differential Equations presents many theoretical strengths including the first
continuous representation of time within video generation. In order to address
the effects of Neural Differential Equations, we will investigate how changes
in temporal models affect generated video quality.
</p>
<a href="http://arxiv.org/abs/2011.03864" target="_blank">arXiv:2011.03864</a> [<a href="http://arxiv.org/pdf/2011.03864" target="_blank">pdf</a>]

<h2>Learning Continuous System Dynamics from Irregularly-Sampled Partial Observations. (arXiv:2011.03880v1 [cs.LG])</h2>
<h3>Zijie Huang, Yizhou Sun, Wei Wang</h3>
<p>Many real-world systems, such as moving planets, can be considered as
multi-agent dynamic systems, where objects interact with each other and
co-evolve along with the time. Such dynamics is usually difficult to capture,
and understanding and predicting the dynamics based on observed trajectories of
objects become a critical research problem in many domains. Most existing
algorithms, however, assume the observations are regularly sampled and all the
objects can be fully observed at each sampling time, which is impractical for
many applications. In this paper, we propose to learn system dynamics from
irregularly-sampled partial observations with underlying graph structure for
the first time. To tackle the above challenge, we present LG-ODE, a latent
ordinary differential equation generative model for modeling multi-agent
dynamic system with known graph structure. It can simultaneously learn the
embedding of high dimensional trajectories and infer continuous latent system
dynamics. Our model employs a novel encoder parameterized by a graph neural
network that can infer initial states in an unsupervised way from
irregularly-sampled partial observations of structural objects and utilizes
neuralODE to infer arbitrarily complex continuous-time latent dynamics.
Experiments on motion capture, spring system, and charged particle datasets
demonstrate the effectiveness of our approach.
</p>
<a href="http://arxiv.org/abs/2011.03880" target="_blank">arXiv:2011.03880</a> [<a href="http://arxiv.org/pdf/2011.03880" target="_blank">pdf</a>]

<h2>Learning Extended Body Schemas from Visual Keypoints for Object Manipulation. (arXiv:2011.03882v1 [cs.RO])</h2>
<h3>Sarah Bechtle, Neha Das, Franziska Meier</h3>
<p>Humans have impressive generalization capabilities when it comes to
manipulating objects and tools in completely novel environments. These
capabilities are, at least partially, a result of humans having internal models
of their bodies and any grasped object. How to learn such body schemas for
robots remains an open problem. In this work, we develop an approach that can
extend a robot's kinematic model when grasping an object from visual latent
representations. Our framework comprises two components: 1) a structured
keypoint detector, which fuses proprioception and vision to predict visual key
points on an object; 2) Learning an adaptation of the kinematic chain by
regressing virtual joints from the predicted key points. Our evaluation shows
that our approach learns to consistently predict visual keypoints on objects,
and can easily adapt a kinematic chain to the object grasped in various
configurations, from a few seconds of data. Finally we show that this extended
kinematic chain lends itself for object manipulation tasks such as placing a
grasped object.
</p>
<a href="http://arxiv.org/abs/2011.03882" target="_blank">arXiv:2011.03882</a> [<a href="http://arxiv.org/pdf/2011.03882" target="_blank">pdf</a>]

<h2>Swarm Formation Morphing for Congestion Aware Collision Avoidance. (arXiv:2011.03883v1 [cs.RO])</h2>
<h3>Jawad N. Yasin, Mohammad-Hashem Haghbayan, Muhammad Mehboob Yasin, Juha Plosila</h3>
<p>The focus of this work is to present a novel methodology for optimal
distribution of a swarm formation on either side of an obstacle, when evading
the obstacle, to avoid overpopulation on the sides to reduce the agents'
waiting delays, resulting in a reduced overall mission time and lower energy
consumption. To handle this, the problem is divided into two main parts: 1) the
disturbance phase: how to morph the formation optimally to avoid the obstacle
in the least possible time in the situation at hand, and 2) the convergence
phase: how to optimally resume the intended formation shape once the threat of
potential collision has been eliminated. For the first problem, we develop a
methodology which tests different formation morphing combinations and finds the
optimal one, by utilizing trajectory, velocity, and coordinate information, to
bypass the obstacle. For the second problem, we utilize a thin-plate splines
(TPS) inspired temperature function minimization method to bring the agents
back from the distorted formation into the desired formation in an optimal
manner, after collision avoidance has been successfully performed. Experimental
results show that, in the considered test scenario, the traditional method
based on the shortest path results in 14.7% higher energy consumption as
compared to our proposed approach.
</p>
<a href="http://arxiv.org/abs/2011.03883" target="_blank">arXiv:2011.03883</a> [<a href="http://arxiv.org/pdf/2011.03883" target="_blank">pdf</a>]

<h2>Performative Prediction in a Stateful World. (arXiv:2011.03885v1 [cs.LG])</h2>
<h3>Gavin Brown, Shlomi Hod, Iden Kalemaj</h3>
<p>Deployed supervised machine learning models make predictions that interact
with and influence the world. This phenomenon is called "performative
prediction" by Perdomo et al. (2020), who investigated it in a stateless
setting. We generalize their results to the case where the response of the
population to the deployed classifier depends both on the classifier and the
previous distribution of the population. We also demonstrate such a setting
empirically, for the scenario of strategic manipulation.
</p>
<a href="http://arxiv.org/abs/2011.03885" target="_blank">arXiv:2011.03885</a> [<a href="http://arxiv.org/pdf/2011.03885" target="_blank">pdf</a>]

<h2>Channel Pruning Guided by Spatial and Channel Attention for DNNs in Intelligent Edge Computing. (arXiv:2011.03891v1 [cs.CV])</h2>
<h3>Mengran Liu, Weiwei Fang, Xiaodong Ma, Wenyuan Xu, Naixue Xiong, Yi Ding</h3>
<p>Deep Neural Networks (DNNs) have achieved remarkable success in many computer
vision tasks recently, but the huge number of parameters and the high
computation overhead hinder their deployments on resource-constrained edge
devices. It is worth noting that channel pruning is an effective approach for
compressing DNN models. A critical challenge is to determine which channels are
to be removed, so that the model accuracy will not be negatively affected. In
this paper, we first propose Spatial and Channel Attention (SCA), a new
attention module combining both spatial and channel attention that respectively
focuses on "where" and "what" are the most informative parts. Guided by the
scale values generated by SCA for measuring channel importance, we further
propose a new channel pruning approach called Channel Pruning guided by Spatial
and Channel Attention (CPSCA). Experimental results indicate that SCA achieves
the best inference accuracy, while incurring negligibly extra resource
consumption, compared to other state-of-the-art attention modules. Our
evaluation on two benchmark datasets shows that, with the guidance of SCA, our
CPSCA approach achieves higher inference accuracy than other state-of-the-art
pruning methods under the same pruning ratios.
</p>
<a href="http://arxiv.org/abs/2011.03891" target="_blank">arXiv:2011.03891</a> [<a href="http://arxiv.org/pdf/2011.03891" target="_blank">pdf</a>]

<h2>Multimodal Trajectory Prediction via Topological Invariance for Navigation at Uncontrolled Intersections. (arXiv:2011.03894v1 [cs.RO])</h2>
<h3>Junha Roh, Christoforos Mavrogiannis, Rishabh Madan, Dieter Fox, Siddhartha S. Srinivasa</h3>
<p>We focus on decentralized navigation among multiple non-communicating
rational agents at \emph{uncontrolled} intersections, i.e., street
intersections without traffic signs or signals. Avoiding collisions in such
domains relies on the ability of agents to predict each others' intentions
reliably, and react quickly. Multiagent trajectory prediction is NP-hard
whereas the sample complexity of existing data-driven approaches limits their
applicability. Our key insight is that the geometric structure of the
intersection and the incentive of agents to move efficiently and avoid
collisions (rationality) reduces the space of likely behaviors, effectively
relaxing the problem of trajectory prediction. In this paper, we collapse the
space of multiagent trajectories at an intersection into a set of modes
representing different classes of multiagent behavior, formalized using a
notion of topological invariance. Based on this formalism, we design Multiple
Topologies Prediction (MTP), a data-driven trajectory-prediction mechanism that
reconstructs trajectory representations of high-likelihood modes in multiagent
intersection scenes. We show that MTP outperforms a state-of-the-art multimodal
trajectory prediction baseline (MFP) in terms of prediction accuracy by 78.24%
on a challenging simulated dataset. Finally, we show that MTP enables our
optimization-based planner, MTPnav, to achieve collision-free and
time-efficient navigation across a variety of challenging intersection
scenarios on the CARLA simulator.
</p>
<a href="http://arxiv.org/abs/2011.03894" target="_blank">arXiv:2011.03894</a> [<a href="http://arxiv.org/pdf/2011.03894" target="_blank">pdf</a>]

<h2>Cooperative and Stochastic Multi-Player Multi-Armed Bandit: Optimal Regret With Neither Communication Nor Collisions. (arXiv:2011.03896v1 [cs.LG])</h2>
<h3>S&#xe9;bastien Bubeck, Thomas Budzinski, Mark Sellke</h3>
<p>We consider the cooperative multi-player version of the stochastic
multi-armed bandit problem. We study the regime where the players cannot
communicate but have access to shared randomness. In prior work by the first
two authors, a strategy for this regime was constructed for two players and
three arms, with regret $\tilde{O}(\sqrt{T})$, and with no collisions at all
between the players (with very high probability). In this paper we show that
these properties (near-optimal regret and no collisions at all) are achievable
for any number of players and arms. At a high level, the previous strategy
heavily relied on a $2$-dimensional geometric intuition that was difficult to
generalize in higher dimensions, while here we take a more combinatorial route
to build the new strategy.
</p>
<a href="http://arxiv.org/abs/2011.03896" target="_blank">arXiv:2011.03896</a> [<a href="http://arxiv.org/pdf/2011.03896" target="_blank">pdf</a>]

<h2>The Cost of Privacy in Generalized Linear Models: Algorithms and Minimax Lower Bounds. (arXiv:2011.03900v1 [stat.ML])</h2>
<h3>T. Tony Cai, Yichen Wang, Linjun Zhang</h3>
<p>The trade-off between differential privacy and statistical accuracy in
generalized linear models (GLMs) is studied. We propose differentially private
algorithms for parameter estimation in both low-dimensional and
high-dimensional sparse GLMs and characterize their statistical performance. We
establish privacy-constrained minimax lower bounds for GLMs, which imply that
the proposed algorithms are rate-optimal up to logarithmic factors in sample
size. The lower bounds are obtained via a novel technique, which is based on
Stein's Lemma and generalizes the tracing attack technique for
privacy-constrained lower bounds. This lower bound argument can be of
independent interest as it is applicable to general parametric models.
Simulated and real data experiments are conducted to demonstrate the numerical
performance of our algorithms.
</p>
<a href="http://arxiv.org/abs/2011.03900" target="_blank">arXiv:2011.03900</a> [<a href="http://arxiv.org/pdf/2011.03900" target="_blank">pdf</a>]

<h2>Adversarial Black-Box Attacks On Text Classifiers Using Multi-Objective Genetic Optimization Guided By Deep Networks. (arXiv:2011.03901v1 [cs.AI])</h2>
<h3>Alex Mathai, Shreya Khare, Srikanth Tamilselvam, Senthil Mani</h3>
<p>We propose a novel genetic-algorithm technique that generates black-box
adversarial examples which successfully fool neural network based text
classifiers. We perform a genetic search with multi-objective optimization
guided by deep learning based inferences and Seq2Seq mutation to generate
semantically similar but imperceptible adversaries. We compare our approach
with DeepWordBug (DWB) on SST and IMDB sentiment datasets by attacking three
trained models viz. char-LSTM, word-LSTM and elmo-LSTM. On an average, we
achieve an attack success rate of $65.67% for SST and 36.45% for IMDB across
the three models showing an improvement of 49.48% and 101% respectively.
Furthermore, our qualitative study indicates that $94\%$ of the time, the users
were not able to distinguish between an original and adversarial sample.
</p>
<a href="http://arxiv.org/abs/2011.03901" target="_blank">arXiv:2011.03901</a> [<a href="http://arxiv.org/pdf/2011.03901" target="_blank">pdf</a>]

<h2>Learning Neural Event Functions for Ordinary Differential Equations. (arXiv:2011.03902v1 [cs.LG])</h2>
<h3>Ricky T. Q. Chen, Brandon Amos, Maximilian Nickel</h3>
<p>The existing Neural ODE formulation relies on an explicit knowledge of the
termination time. We extend Neural ODEs to implicitly defined termination
criteria modeled by neural event functions, which can be chained together and
differentiated through. Neural Event ODEs are capable of modeling discrete
(instantaneous) changes in a continuous-time system, without prior knowledge of
when these changes should occur or how many such changes should exist. We test
our approach in modeling hybrid discrete- and continuous- systems such as
switching dynamical systems and collision in multi-body systems, and we propose
simulation-based training of point processes with applications in discrete
control.
</p>
<a href="http://arxiv.org/abs/2011.03902" target="_blank">arXiv:2011.03902</a> [<a href="http://arxiv.org/pdf/2011.03902" target="_blank">pdf</a>]

<h2>Locally Adaptive Nearest Neighbors. (arXiv:2011.03904v1 [cs.LG])</h2>
<h3>Jan Philip G&#xf6;pfert, Heiko Wersing, Barbara Hammer</h3>
<p>When training automated systems, it has been shown to be beneficial to adapt
the representation of data by learning a problem-specific metric. This metric
is global. We extend this idea and, for the widely used family of k nearest
neighbors algorithms, develop a method that allows learning locally adaptive
metrics. To demonstrate important aspects of how our approach works, we conduct
a number of experiments on synthetic data sets, and we show its usefulness on
real-world benchmark data sets.
</p>
<a href="http://arxiv.org/abs/2011.03904" target="_blank">arXiv:2011.03904</a> [<a href="http://arxiv.org/pdf/2011.03904" target="_blank">pdf</a>]

<h2>Reinforcement Learning for Assignment problem. (arXiv:2011.03909v1 [cs.AI])</h2>
<h3>Filipp Skomorokhov (1 and 2), George Ovchinnikov (2) ((1) Moscow Institute of Physics and Technology, (2) Skolkovo Institute of Science and Technology)</h3>
<p>This paper is dedicated to the application of reinforcement learning combined
with neural networks to the general formulation of user scheduling problem. Our
simulator resembles real world problems by means of stochastic changes in
environment. We applied Q-learning based method to the number of dynamic
simulations and outperformed analytical greedy-based solution in terms of total
reward, the aim of which is to get the lowest possible penalty throughout
simulation.
</p>
<a href="http://arxiv.org/abs/2011.03909" target="_blank">arXiv:2011.03909</a> [<a href="http://arxiv.org/pdf/2011.03909" target="_blank">pdf</a>]

<h2>Faster object tracking pipeline for real time tracking. (arXiv:2011.03910v1 [cs.CV])</h2>
<h3>Parthesh Soni, Falak Shah, Nisarg Vyas</h3>
<p>Multi-object tracking (MOT) is a challenging practical problem for vision
based applications. Most recent approaches for MOT use precomputed detections
from models such as Faster RCNN, performing fine-tuning of bounding boxes and
association in subsequent phases. However, this is not suitable for actual
industrial applications due to unavailability of detections upfront. In their
recent work, Wang et al. proposed a tracking pipeline that uses a Joint
detection and embedding model and performs target localization and association
in realtime. Upon investigating the tracking by detection paradigm, we find
that the tracking pipeline can be made faster by performing localization and
association tasks parallely with model prediction. This, and other
computational optimizations such as using mixed precision model and performing
batchwise detection result in a speed-up of the tracking pipeline by 57.8\% (19
FPS to 30 FPS) on FullHD resolution. Moreover, the speed is independent of the
object density in image sequence. The main contribution of this paper is
showcasing a generic pipeline which can be used to speed up detection based
object tracking methods. We also reviewed different batch sizes for optimal
performance, taking into consideration GPU memory usage and speed.
</p>
<a href="http://arxiv.org/abs/2011.03910" target="_blank">arXiv:2011.03910</a> [<a href="http://arxiv.org/pdf/2011.03910" target="_blank">pdf</a>]

<h2>Dynamic Movement Primitive based Motion Retargeting for Dual-Arm Sign Language Motions. (arXiv:2011.03914v1 [cs.RO])</h2>
<h3>Yuwei Liang, Weijie Li, Yue Wang, Rong Xiong</h3>
<p>We aim to develop an efficient programming method for equipping service
robots with the skill of performing sign language motions. This paper addresses
the problem of transferring complex dual-arm sign language motions
characterized by the coordination among arms and hands from human to robot,
which is seldom considered in previous studies of motion retargeting
techniques. In this paper, we propose a novel motion retargeting method that
leverages graph optimization and Dynamic Movement Primitives (DMPs) for this
problem. We employ DMPs in a leader-follower manner to parameterize the
original trajectories while preserving motion rhythm and relative movements
between human body parts, and adopt a three-step optimization procedure to find
deformed trajectories for robot motion planning while ensuring feasibility for
robot execution. Experimental results of several Chinese Sign Language (CSL)
motions have been successfully performed on ABB's YuMi dual-arm collaborative
robot (14-DOF) with two 6-DOF Inspire-Robotics' multi-fingered hands, a system
with 26 DOFs in total.
</p>
<a href="http://arxiv.org/abs/2011.03914" target="_blank">arXiv:2011.03914</a> [<a href="http://arxiv.org/pdf/2011.03914" target="_blank">pdf</a>]

<h2>Asymptotic Convergence of Thompson Sampling. (arXiv:2011.03917v1 [cs.LG])</h2>
<h3>Cem Kalkanli, Ayfer Ozgur</h3>
<p>Thompson sampling has been shown to be an effective policy across a variety
of online learning tasks. Many works have analyzed the finite time performance
of Thompson sampling, and proved that it achieves a sub-linear regret under a
broad range of probabilistic settings. However its asymptotic behavior remains
mostly underexplored. In this paper, we prove an asymptotic convergence result
for Thompson sampling under the assumption of a sub-linear Bayesian regret, and
show that the actions of a Thompson sampling agent provide a strongly
consistent estimator of the optimal action. Our results rely on the martingale
structure inherent in Thompson sampling.
</p>
<a href="http://arxiv.org/abs/2011.03917" target="_blank">arXiv:2011.03917</a> [<a href="http://arxiv.org/pdf/2011.03917" target="_blank">pdf</a>]

<h2>Integrating Human Gaze into Attention for Egocentric Activity Recognition. (arXiv:2011.03920v1 [cs.CV])</h2>
<h3>Kyle Min, Jason J. Corso</h3>
<p>It is well known that human gaze carries significant information about visual
attention. However, there are three main difficulties in incorporating the gaze
data in an attention mechanism of deep neural networks: 1) the gaze fixation
points are likely to have measurement errors due to blinking and rapid eye
movements; 2) it is unclear when and how much the gaze data is correlated with
visual attention; and 3) gaze data is not always available in many real-world
situations. In this work, we introduce an effective probabilistic approach to
integrate human gaze into spatiotemporal attention for egocentric activity
recognition. Specifically, we represent the locations of gaze fixation points
as structured discrete latent variables to model their uncertainties. In
addition, we model the distribution of gaze fixations using a variational
method. The gaze distribution is learned during the training process so that
the ground-truth annotations of gaze locations are no longer needed in testing
situations since they are predicted from the learned gaze distribution. The
predicted gaze locations are used to provide informative attentional cues to
improve the recognition performance. Our method outperforms all the previous
state-of-the-art approaches on EGTEA, which is a large-scale dataset for
egocentric activity recognition provided with gaze measurements. We also
perform an ablation study and qualitative analysis to demonstrate that our
attention mechanism is effective.
</p>
<a href="http://arxiv.org/abs/2011.03920" target="_blank">arXiv:2011.03920</a> [<a href="http://arxiv.org/pdf/2011.03920" target="_blank">pdf</a>]

<h2>PointTransformer for Shape Classification and Retrieval of 3D and ALS Roof PointClouds. (arXiv:2011.03921v1 [cs.CV])</h2>
<h3>Dimple A Shajahan, Mukund Varma T, Ramanathan Muthuganapathy</h3>
<p>Effective feature representation from Airborne Laser Scanning (ALS) point
clouds used for urban modeling was challenging until the advent of deep
learning and improved ALS techniques. Most deep learning techniques for 3-D
point clouds utilize convolutions that assume a uniform input distribution and
cannot learn long-range dependencies, leading to some limitations. Recent works
have already shown that adding attention on top of these methods improves
performance. This raises a question: can attention layers completely replace
convolutions? We propose a fully attentional model-PointTransformer for
deriving a rich point cloud representation. The model's shape classification
and retrieval performance are evaluated on a large-scale urban dataset -
RoofN3D and a standard benchmark dataset ModelNet40. Also, the model is tested
on various simulated point corruptions to analyze its effectiveness on real
datasets. The proposed method outperforms other state-of-the-art models in the
RoofN3D dataset, gives competitive results in the ModelNet40 benchmark, and
showcases high robustness to multiple point corruptions. Furthermore, the model
is both memory and space-efficient without compromising on performance.
</p>
<a href="http://arxiv.org/abs/2011.03921" target="_blank">arXiv:2011.03921</a> [<a href="http://arxiv.org/pdf/2011.03921" target="_blank">pdf</a>]

<h2>Learning World Transition Model for Socially Aware Robot Navigation. (arXiv:2011.03922v1 [cs.RO])</h2>
<h3>Yuxiang Cui, Haodong Zhang, Yue Wang, Rong Xiong</h3>
<p>Moving in dynamic pedestrian environments is one of the important
requirements for autonomous mobile robots. We present a model-based
reinforcement learning approach for robots to navigate through crowded
environments. The navigation policy is trained with both real interaction data
from multi-agent simulation and virtual data from a deep transition model that
predicts the evolution of surrounding dynamics of mobile robots. The model
takes laser scan sequence and robot's own state as input and outputs steering
control. The laser sequence is further transformed into stacked local obstacle
maps disentangled from robot's ego motion to separate the static and dynamic
obstacles, simplifying the model training. We observe that our method can be
trained with significantly less real interaction data in simulator but achieve
similar level of success rate in social navigation task compared with other
methods. Experiments were conducted in multiple social scenarios both in
simulation and on real robots, the learned policy can guide the robots to the
final targets successfully while avoiding pedestrians in a socially compliant
manner. Code is available at
https://github.com/YuxiangCui/model-based-social-navigation
</p>
<a href="http://arxiv.org/abs/2011.03922" target="_blank">arXiv:2011.03922</a> [<a href="http://arxiv.org/pdf/2011.03922" target="_blank">pdf</a>]

<h2>Right on Time: Multi-Temporal Convolutions for Human Action Recognition in Videos. (arXiv:2011.03949v1 [cs.CV])</h2>
<h3>Alexandros Stergiou, Ronald Poppe</h3>
<p>The variations in the temporal performance of human actions observed in
videos present challenges for their extraction using fixed-sized convolution
kernels in CNNs. We present an approach that is more flexible in terms of
processing the input at multiple timescales. We introduce Multi-Temporal
networks that model spatio-temporal patterns of different temporal durations at
each layer. To this end, they employ novel 3D convolution (MTConv) blocks that
consist of a short stream for local space-time features and a long stream for
features spanning across longer times. By aligning features of each stream with
respect to the global motion patterns using recurrent cells, we can discover
temporally coherent spatio-temporal features with varying durations. We further
introduce sub-streams within each of the block pathways to reduce the
computation requirements. The proposed MTNet architectures outperform
state-of-the-art 3D-CNNs on five action recognition benchmark datasets.
Notably, we achieve at 87.22% top-1 accuracy on HACS, and 58.39% top-1 at
Kinectics-700. We further demonstrate the favorable computational requirements.
Using sub-streams, we can further achieve a drastic reduction in parameters
(~60%) and GLOPs (~74%). Experiments using transfer learning finally verify the
generalization capabilities of the multi-temporal features
</p>
<a href="http://arxiv.org/abs/2011.03949" target="_blank">arXiv:2011.03949</a> [<a href="http://arxiv.org/pdf/2011.03949" target="_blank">pdf</a>]

<h2>FlowCaps: Optical Flow Estimation with Capsule Networks For Action Recognition. (arXiv:2011.03958v1 [cs.CV])</h2>
<h3>Vinoj Jayasundara, Debaditya Roy, Basura Fernando</h3>
<p>Capsule networks (CapsNets) have recently shown promise to excel in most
computer vision tasks, especially pertaining to scene understanding. In this
paper, we explore CapsNet's capabilities in optical flow estimation, a task at
which convolutional neural networks (CNNs) have already outperformed other
approaches. We propose a CapsNet-based architecture, termed FlowCaps, which
attempts to a) achieve better correspondence matching via finer-grained,
motion-specific, and more-interpretable encoding crucial for optical flow
estimation, b) perform better-generalizable optical flow estimation, c) utilize
lesser ground truth data, and d) significantly reduce the computational
complexity in achieving good performance, in comparison to its
CNN-counterparts.
</p>
<a href="http://arxiv.org/abs/2011.03958" target="_blank">arXiv:2011.03958</a> [<a href="http://arxiv.org/pdf/2011.03958" target="_blank">pdf</a>]

<h2>Fast-Tracker: A Robust Aerial System for Tracking Agile Target in Cluttered Environments. (arXiv:2011.03968v1 [cs.RO])</h2>
<h3>Zhichao Han, Ruibin Zhang, Neng Pan, Chao Xu, Fei Gao</h3>
<p>This paper proposes a systematic solution that uses an unmanned aerial
vehicle (UAV) to aggressively and safely track an agile target. The solution
properly handles the challenging situations where the intent of the target and
the dense environments are unknown to the UAV. Our work is divided into two
parts: target motion prediction and tracking trajectory planning. The target
motion prediction method utilizes target observations to reliably predict the
future motion of the target considering dynamic constraints. The tracking
trajectory planner follows the traditional hierarchical workflow.A target
informed kinodynamic searching method is adopted as the front-end, which
heuristically searches for a safe tracking trajectory. The back-end optimizer
then refines it into a spatial-temporal optimal and collision-free trajectory.
The proposed solution is integrated into an onboard quadrotor system. We fully
test the system in challenging real-world tracking missions.Moreover, benchmark
comparisons validate that the proposed method surpasses the cutting-edge
methods on time efficiency and tracking effectiveness.
</p>
<a href="http://arxiv.org/abs/2011.03968" target="_blank">arXiv:2011.03968</a> [<a href="http://arxiv.org/pdf/2011.03968" target="_blank">pdf</a>]

<h2>The quantization error in a Self-Organizing Map as a contrast and colour specific indicator of single-pixel change in large random patterns. (arXiv:2011.03970v1 [cs.CV])</h2>
<h3>John M Wandeto, Birgitta Dresp-Langley</h3>
<p>The quantization error in a fixed-size Self-Organizing Map (SOM) with
unsupervised winner-take-all learning has previously been used successfully to
detect, in minimal computation time, highly meaningful changes across images in
medical time series and in time series of satellite images. Here, the
functional properties of the quantization error in SOM are explored further to
show that the metric is capable of reliably discriminating between the finest
differences in local contrast intensities and contrast signs. While this
capability of the QE is akin to functional characteristics of a specific class
of retinal ganglion cells (the so-called Y-cells) in the visual systems of the
primate and the cat, the sensitivity of the QE surpasses the capacity limits of
human visual detection. Here, the quantization error in the SOM is found to
reliably signal changes in contrast or colour when contrast information is
removed from or added to the image, but not when the amount and relative weight
of contrast information is constant and only the local spatial position of
contrast elements in the pattern changes. While the RGB Mean reflects coarser
changes in colour or contrast well enough, the SOM-QE is shown to outperform
the RGB Mean in the detection of single-pixel changes in images with up to five
million pixels. This could have important implications in the context of
unsupervised image learning and computational building block approaches to
large sets of image data (big data), including deep learning blocks, and
automatic detection of contrast change at the nanoscale in Transmission or
Scanning Electron Micrographs (TEM, SEM), or at the subpixel level in
multispectral and hyper-spectral imaging data.
</p>
<a href="http://arxiv.org/abs/2011.03970" target="_blank">arXiv:2011.03970</a> [<a href="http://arxiv.org/pdf/2011.03970" target="_blank">pdf</a>]

<h2>Adaptive Linear Span Network for Object Skeleton Detection. (arXiv:2011.03972v1 [cs.CV])</h2>
<h3>Chang Liu, Yunjie Tian, Jianbin Jiao, Qixiang Ye</h3>
<p>Conventional networks for object skeleton detection are usually hand-crafted.
Although effective, they require intensive priori knowledge to configure
representative features for objects in different scale granularity.In this
paper, we propose adaptive linear span network (AdaLSN), driven by neural
architecture search (NAS), to automatically configure and integrate scale-aware
features for object skeleton detection. AdaLSN is formulated with the theory of
linear span, which provides one of the earliest explanations for multi-scale
deep feature fusion. AdaLSN is materialized by defining a mixed unit-pyramid
search space, which goes beyond many existing search spaces using unit-level or
pyramid-level features.Within the mixed space, we apply genetic architecture
search to jointly optimize unit-level operations and pyramid-level connections
for adaptive feature space expansion. AdaLSN substantiates its versatility by
achieving significantly higher accuracy and latency trade-off compared with
state-of-the-arts. It also demonstrates general applicability to image-to-mask
tasks such as edge detection and road extraction. Code is available at
\href{https://github.com/sunsmarterjie/SDL-Skeleton}{\color{magenta}github.com/sunsmarterjie/SDL-Skeleton}.
</p>
<a href="http://arxiv.org/abs/2011.03972" target="_blank">arXiv:2011.03972</a> [<a href="http://arxiv.org/pdf/2011.03972" target="_blank">pdf</a>]

<h2>Skewed Laplace Spectral Mixture kernels for long-term forecasting in Gaussian process. (arXiv:2011.03974v1 [cs.AI])</h2>
<h3>Kai Chen, Twan van Laarhoven, Elena Marchiori</h3>
<p>Long-term forecasting involves predicting a horizon that is far ahead of the
last observation. It is a problem of highly practical relevance, for instance
for companies in order to decide upon expensive long-term investments. Despite
recent progress and success of Gaussian Processes (GPs) based on Spectral
Mixture kernels, long-term forecasting remains a challenging problem for these
kernels because they decay exponentially at large horizons. This is mainly due
their use of a mixture of Gaussians to model spectral densities. The challenges
underlying long-term forecasting become evident by investigating the
distribution of the Fourier coefficients of (the training part of) the signal,
which is non-smooth, heavy-tailed, sparse and skewed. Notably the heavy tail
and skewness characteristics of such distribution in spectral domain allow to
capture long range covariance of the signal in the time domain. Motivated by
these observations, we propose to model spectral densities using a Skewed
Laplace Spectral Mixture (SLSM) due to the skewness of its peaks, sparsity,
non-smoothness, and heavy tail characteristics. By applying the inverse Fourier
Transform to this spectral density we obtain a new GP kernel for long-term
forecasting. Results of extensive experiments, including a multivariate time
series, show the beneficial effect of the proposed SLSM kernel for long-term
extrapolation and robustness to the choice of the number of mixture components.
</p>
<a href="http://arxiv.org/abs/2011.03974" target="_blank">arXiv:2011.03974</a> [<a href="http://arxiv.org/pdf/2011.03974" target="_blank">pdf</a>]

<h2>Mapless-Planner: A Robust and Fast Planning Framework for Aggressive Autonomous Flight without Map Fusion. (arXiv:2011.03975v1 [cs.RO])</h2>
<h3>Jialin Ji, Zhepei Wang, Yingjian Wang, Chao Xu, Fei Gao</h3>
<p>Maintaining a map online is resource-consuming while a robust navigation
system usually needs environment abstraction via a well-fused map. In this
paper, we propose a mapless planner which directly conducts such abstraction on
the unfused sensor data. A limited-memory data structure with a reliable
proximity query algorithm is proposed for maintaining raw historical
information. A sampling-based scheme is designed to extract the free-space
skeleton. A smart waypoint selection strategy enables to generate high-quality
trajectories within the resultant flight corridors. Our planner differs from
other mapless ones in that it can abstract and exploit the environment
information efficiently. The online replan consistency and success rate are
both significantly improved against conventional mapless methods.
</p>
<a href="http://arxiv.org/abs/2011.03975" target="_blank">arXiv:2011.03975</a> [<a href="http://arxiv.org/pdf/2011.03975" target="_blank">pdf</a>]

<h2>Extending the statistical software package Engine for Likelihood-Free Inference. (arXiv:2011.03977v1 [cs.LG])</h2>
<h3>Vasileios Gkolemis, Michael Gutmann</h3>
<p>Bayesian inference is a principled framework for dealing with uncertainty.
The practitioner can perform an initial assumption for the physical phenomenon
they want to model (prior belief), collect some data and then adjust the
initial assumption in the light of the new evidence (posterior belief).
Approximate Bayesian Computation (ABC) methods, also known as likelihood-free
inference techniques, are a class of models used for performing inference when
the likelihood is intractable. The unique requirement of these models is a
black-box sampling machine. Due to the modelling-freedom they provide these
approaches are particularly captivating. Robust Optimisation Monte Carlo (ROMC)
is one of the most recent techniques of the specific domain. It approximates
the posterior distribution by solving independent optimisation problems. This
dissertation focuses on the implementation of the ROMC method in the software
package Engine for Likelihood-Free Inference (ELFI). In the first chapters, we
provide the mathematical formulation and the algorithmic description of the
ROMC approach. In the following chapters, we describe our implementation; (a)
we present all the functionalities provided to the user and (b) we demonstrate
how to perform inference on some real examples. Our implementation provides a
robust and efficient solution to a practitioner who wants to perform inference
on a simulator-based model. Furthermore, it exploits parallel processing for
accelerating the inference wherever it is possible. Finally, it has been
designed to serve extensibility; the user can easily replace specific subparts
of the method without significant overhead on the development side. Therefore,
it can be used by a researcher for further experimentation.
</p>
<a href="http://arxiv.org/abs/2011.03977" target="_blank">arXiv:2011.03977</a> [<a href="http://arxiv.org/pdf/2011.03977" target="_blank">pdf</a>]

<h2>Learning-based 3D Occupancy Prediction for Autonomous Navigation in Occluded Environments. (arXiv:2011.03981v1 [cs.RO])</h2>
<h3>Lizi Wang, Hongkai Ye, Qianhao Wang, Yuman Gao, Chao Xu, Fei Gao</h3>
<p>In autonomous navigation of mobile robots, sensors suffer from massive
occlusion in cluttered environments, leaving significant amount of space
unknown during planning. In practice, treating the unknown space in optimistic
or pessimistic ways both set limitations on planning performance, thus
aggressiveness and safety cannot be satisfied at the same time. However, humans
can infer the exact shape of the obstacles from only partial observation and
generate non-conservative trajectories that avoid possible collisions in
occluded space. Mimicking human behavior, in this paper, we propose a method
based on deep neural network to predict occupancy distribution of unknown space
reliably. Specifically, the proposed method utilizes contextual information of
environments and learns from prior knowledge to predict obstacle distributions
in occluded space. We use unlabeled and no-ground-truth data to train our
network and successfully apply it to real-time navigation in unseen
environments without any refinement. Results show that our method leverages the
performance of a kinodynamic planner by improving security with no reduction of
speed in clustered environments.
</p>
<a href="http://arxiv.org/abs/2011.03981" target="_blank">arXiv:2011.03981</a> [<a href="http://arxiv.org/pdf/2011.03981" target="_blank">pdf</a>]

<h2>DyERNIE: Dynamic Evolution of Riemannian Manifold Embeddings for Temporal Knowledge Graph Completion. (arXiv:2011.03984v1 [cs.LG])</h2>
<h3>Zhen Han, Peng Chen, Yunpu Ma, Volker Tresp</h3>
<p>There has recently been increasing interest in learning representations of
temporal knowledge graphs (KGs), which record the dynamic relationships between
entities over time. Temporal KGs often exhibit multiple simultaneous
non-Euclidean structures, such as hierarchical and cyclic structures. However,
existing embedding approaches for temporal KGs typically learn entity
representations and their dynamic evolution in the Euclidean space, which might
not capture such intrinsic structures very well. To this end, we propose Dy-
ERNIE, a non-Euclidean embedding approach that learns evolving entity
representations in a product of Riemannian manifolds, where the composed spaces
are estimated from the sectional curvatures of underlying data. Product
manifolds enable our approach to better reflect a wide variety of geometric
structures on temporal KGs. Besides, to capture the evolutionary dynamics of
temporal KGs, we let the entity representations evolve according to a velocity
vector defined in the tangent space at each timestamp. We analyze in detail the
contribution of geometric spaces to representation learning of temporal KGs and
evaluate our model on temporal knowledge graph completion tasks. Extensive
experiments on three real-world datasets demonstrate significantly improved
performance, indicating that the dynamics of multi-relational graph data can be
more properly modeled by the evolution of embeddings on Riemannian manifolds.
</p>
<a href="http://arxiv.org/abs/2011.03984" target="_blank">arXiv:2011.03984</a> [<a href="http://arxiv.org/pdf/2011.03984" target="_blank">pdf</a>]

<h2>The NederDrone: A hybrid lift, hybrid energy hydrogen UAV. (arXiv:2011.03991v1 [cs.RO])</h2>
<h3>Christophe De Wagter, Bart Remes, Ewoud Smeur, Freek van Tienen, Rick Ruijsink, Kevin van Hecke, Erik van der Horst</h3>
<p>A lot of UAV applications require vertical take-off and landing (VTOL)
combined with very long-range or endurance. Transitioning UAVs have been
proposed to combine the VTOL capabilities of helicopters with the efficient
long-range flight properties of fixed-wing aircraft. But energy is still a
bottleneck for many electric long endurance applications. While solar power
technology and battery technology have improved a lot, in rougher conditions
they still respectively lack the power or total amount of energy required for
many real-world situations. In this paper, we introduce the NederDrone, a
hybrid lift, hybrid energy hydrogen-powered UAV which can perform vertical
take-off and landings using 12 propellers while flying efficiently in forward
flight thanks to its fixed wings. The energy is supplied from a mix of
hydrogen-driven fuel-cells to store large amounts of energy and battery power
for high power situations. The hydrogen is stored in a pressurized cylinder
around which the UAV is optimized. This paper analyses the selection of the
concept, the implemented safety elements, the electronics and flight control
and shows flight data including a 3h38 flight at sea, starting and landing on a
small moving ship.
</p>
<a href="http://arxiv.org/abs/2011.03991" target="_blank">arXiv:2011.03991</a> [<a href="http://arxiv.org/pdf/2011.03991" target="_blank">pdf</a>]

<h2>VID-Fusion: Robust Visual-Inertial-Dynamics Odometry for Accurate External Force Estimation. (arXiv:2011.03993v1 [cs.RO])</h2>
<h3>Ziming Ding, Tiankai Yang, Kunyi Zhang, Chao Xu, Fei Gao</h3>
<p>Recently, quadrotors are gaining significant attention in aerial
transportation and delivery. In these scenarios, an accurate estimation of the
external force is as essential as the 6 degree-of-freedom (DoF) pose since it
is of vital importance for planning and control of the vehicle. To this end, we
propose a tightly-coupled Visual-Inertial-Dynamics (VID) system that
simultaneously estimates the external force applied to the quadrotor along with
the 6 DoF pose. Our method builds on the state-of-the-art optimization-based
Visual-Inertial system, with a novel deduction of the dynamics and external
force factor extended from VIMO. Utilizing the proposed dynamics and external
force factor, our estimator robustly and accurately estimates the external
force even when it varies widely. Moreover, since we explicitly consider the
influence of the external force, when compared with VIMO and VINS-Mono, our
method shows comparable and superior pose accuracy, even when the external
force ranges from neglectable to significant. The robustness and effectiveness
of the proposed method are validated by extensive real-world experiments and
application scenario simulation. We will release an open-source package of this
method along with datasets with ground truth force measurements for the
reference of the community.
</p>
<a href="http://arxiv.org/abs/2011.03993" target="_blank">arXiv:2011.03993</a> [<a href="http://arxiv.org/pdf/2011.03993" target="_blank">pdf</a>]

<h2>Long Range Arena: A Benchmark for Efficient Transformers. (arXiv:2011.04006v1 [cs.LG])</h2>
<h3>Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, Donald Metzler</h3>
<p>Transformers do not scale very well to long sequence lengths largely because
of quadratic self-attention complexity. In the recent months, a wide spectrum
of efficient, fast Transformers have been proposed to tackle this problem, more
often than not claiming superior or comparable model quality to vanilla
Transformer models. To this date, there is no well-established consensus on how
to evaluate this class of models. Moreover, inconsistent benchmarking on a wide
spectrum of tasks and datasets makes it difficult to assess relative model
quality amongst many models. This paper proposes a systematic and unified
benchmark, LRA, specifically focused on evaluating model quality under
long-context scenarios. Our benchmark is a suite of tasks consisting of
sequences ranging from $1K$ to $16K$ tokens, encompassing a wide range of data
types and modalities such as text, natural, synthetic images, and mathematical
expressions requiring similarity, structural, and visual-spatial reasoning. We
systematically evaluate ten well-established long-range Transformer models
(Reformers, Linformers, Linear Transformers, Sinkhorn Transformers, Performers,
Synthesizers, Sparse Transformers, and Longformers) on our newly proposed
benchmark suite. LRA paves the way towards better understanding this class of
efficient Transformer models, facilitates more research in this direction, and
presents new challenging tasks to tackle. Our benchmark code will be released
at https://github.com/google-research/long-range-arena.
</p>
<a href="http://arxiv.org/abs/2011.04006" target="_blank">arXiv:2011.04006</a> [<a href="http://arxiv.org/pdf/2011.04006" target="_blank">pdf</a>]

<h2>Scout Algorithm For Fast Substring Matching. (arXiv:2011.04010v1 [cs.AI])</h2>
<h3>Anand Natrajan, Mallige Anand</h3>
<p>Exact substring matching is a common task in many software applications.
Despite the existence of several algorithms for finding whether or not a
pattern string is present in a target string, the most common implementation is
a na\"ive, brute force approach. Alternative approaches either do not provide
enough of a benefit for the added complexity, or are impractical for modern
character sets, e.g., Unicode. We present a new algorithm, Scout, that is
straightforward, quick and appropriate for all applications. We also compare
the performance characteristics of the Scout algorithm with several others.
</p>
<a href="http://arxiv.org/abs/2011.04010" target="_blank">arXiv:2011.04010</a> [<a href="http://arxiv.org/pdf/2011.04010" target="_blank">pdf</a>]

<h2>Provenance-Based Interpretation of Multi-Agent Information Analysis. (arXiv:2011.04016v1 [cs.AI])</h2>
<h3>Scott Friedman, Jeff Rye, David LaVergne, Dan Thomsen, Matthew Allen, Kyle Tunis</h3>
<p>Analytic software tools and workflows are increasing in capability,
complexity, number, and scale, and the integrity of our workflows is as
important as ever. Specifically, we must be able to inspect the process of
analytic workflows to assess (1) confidence of the conclusions, (2) risks and
biases of the operations involved, (3) sensitivity of the conclusions to
sources and agents, (4) impact and pertinence of various sources and agents,
and (5) diversity of the sources that support the conclusions. We present an
approach that tracks agents' provenance with PROV-O in conjunction with agents'
appraisals and evidence links (expressed in our novel DIVE ontology). Together,
PROV-O and DIVE enable dynamic propagation of confidence and counter-factual
refutation to improve human-machine trust and analytic integrity. We
demonstrate representative software developed for user interaction with that
provenance, and discuss key needs for organizations adopting such approaches.
We demonstrate all of these assessments in a multi-agent analysis scenario,
using an interactive web-based information validation UI.
</p>
<a href="http://arxiv.org/abs/2011.04016" target="_blank">arXiv:2011.04016</a> [<a href="http://arxiv.org/pdf/2011.04016" target="_blank">pdf</a>]

<h2>Online Sparse Reinforcement Learning. (arXiv:2011.04018v1 [cs.LG])</h2>
<h3>Botao Hao, Tor Lattimore, Csaba Szepesv&#xe1;ri, Mengdi Wang</h3>
<p>We investigate the hardness of online reinforcement learning in sparse linear
Markov decision process (MDP), with a special focus on the high-dimensional
regime where the ambient dimension is larger than the number of episodes. Our
contribution is two-fold. First, we provide a lower bound showing that linear
regret is generally unavoidable, even if there exists a policy that collects
well-conditioned data. Second, we show that if the learner has oracle access to
a policy that collects well-conditioned data, then a variant of Lasso fitted
Q-iteration enjoys a regret of $\tilde{O}(N^{2/3})$ where $N$ is the number of
episodes.
</p>
<a href="http://arxiv.org/abs/2011.04018" target="_blank">arXiv:2011.04018</a> [<a href="http://arxiv.org/pdf/2011.04018" target="_blank">pdf</a>]

<h2>Sparse Feature Selection Makes Batch Reinforcement Learning More Sample Efficient. (arXiv:2011.04019v1 [cs.LG])</h2>
<h3>Botao Hao, Yaqi Duan, Tor Lattimore, Csaba Szepesv&#xe1;ri, Mengdi Wang</h3>
<p>This paper provides a statistical analysis of high-dimensional batch
Reinforcement Learning (RL) using sparse linear function approximation. When
there is a large number of candidate features, our result sheds light on the
fact that sparsity-aware methods can make batch RL more sample efficient. We
first consider the off-policy policy evaluation problem. To evaluate a new
target policy, we analyze a Lasso fitted Q-evaluation method and establish a
finite-sample error bound that has no polynomial dependence on the ambient
dimension. To reduce the Lasso bias, we further propose a post model-selection
estimator that applies fitted Q-evaluation to the features selected via group
Lasso. Under an additional signal strength assumption, we derive a sharper
instance-dependent error bound that depends on a divergence function measuring
the distribution mismatch between the data distribution and occupancy measure
of the target policy. Further, we study the Lasso fitted Q-iteration for batch
policy optimization and establish a finite-sample error bound depending on the
ratio between the number of relevant features and restricted minimal eigenvalue
of the data's covariance. In the end, we complement the results with minimax
lower bounds for batch-data policy evaluation/optimization that nearly match
our upper bounds. The results suggest that having well-conditioned data is
crucial for sparse batch policy learning.
</p>
<a href="http://arxiv.org/abs/2011.04019" target="_blank">arXiv:2011.04019</a> [<a href="http://arxiv.org/pdf/2011.04019" target="_blank">pdf</a>]

<h2>High-Dimensional Sparse Linear Bandits. (arXiv:2011.04020v1 [stat.ML])</h2>
<h3>Botao Hao, Tor Lattimore, Mengdi Wang</h3>
<p>Stochastic linear bandits with high-dimensional sparse features are a
practical model for a variety of domains, including personalized medicine and
online advertising. We derive a novel $\Omega(n^{2/3})$ dimension-free minimax
regret lower bound for sparse linear bandits in the data-poor regime where the
horizon is smaller than the ambient dimension and where the feature vectors
admit a well-conditioned exploration distribution. This is complemented by a
nearly matching upper bound for an explore-then-commit algorithm showing that
that $\Theta(n^{2/3})$ is the optimal rate in the data-poor regime. The results
complement existing bounds for the data-rich regime and provide another example
where carefully balancing the trade-off between information and regret is
necessary. Finally, we prove a dimension-free $O(\sqrt{n})$ regret upper bound
under an additional assumption on the magnitude of the signal for relevant
features.
</p>
<a href="http://arxiv.org/abs/2011.04020" target="_blank">arXiv:2011.04020</a> [<a href="http://arxiv.org/pdf/2011.04020" target="_blank">pdf</a>]

<h2>On the role of planning in model-based deep reinforcement learning. (arXiv:2011.04021v1 [cs.AI])</h2>
<h3>Jessica B. Hamrick, Abram L. Friesen, Feryal Behbahani, Arthur Guez, Fabio Viola, Sims Witherspoon, Thomas Anthony, Lars Buesing, Petar Veli&#x10d;kovi&#x107;, Th&#xe9;ophane Weber</h3>
<p>Model-based planning is often thought to be necessary for deep, careful
reasoning and generalization in artificial agents. While recent successes of
model-based reinforcement learning (MBRL) with deep function approximation have
strengthened this hypothesis, the resulting diversity of model-based methods
has also made it difficult to track which components drive success and why. In
this paper, we seek to disentangle the contributions of recent methods by
focusing on three questions: (1) How does planning benefit MBRL agents? (2)
Within planning, what choices drive performance? (3) To what extent does
planning improve generalization? To answer these questions, we study the
performance of MuZero (Schrittwieser et al., 2019), a state-of-the-art MBRL
algorithm, under a number of interventions and ablations and across a wide
range of environments including control tasks, Atari, and 9x9 Go. Our results
suggest the following: (1) The primary benefit of planning is in driving policy
learning. (2) Using shallow trees with simple Monte-Carlo rollouts is as
performant as more complex methods, except in the most difficult reasoning
tasks. (3) Planning alone is insufficient to drive strong generalization. These
results indicate where and how to utilize planning in reinforcement learning
settings, and highlight a number of open questions for future MBRL research.
</p>
<a href="http://arxiv.org/abs/2011.04021" target="_blank">arXiv:2011.04021</a> [<a href="http://arxiv.org/pdf/2011.04021" target="_blank">pdf</a>]

<h2>Pathwise Conditioning of Gaussian Processes. (arXiv:2011.04026v1 [stat.ML])</h2>
<h3>James T. Wilson, Viacheslav Borovitskiy, Alexander Terenin, Peter Mostowsky, Marc Peter Deisenroth</h3>
<p>As Gaussian processes are integrated into increasingly complex problem
settings, analytic solutions to quantities of interest become scarcer and
scarcer. Monte Carlo methods act as a convenient bridge for connecting
intractable mathematical expressions with actionable estimates via sampling.
Conventional approaches for simulating Gaussian process posteriors view samples
as vectors drawn from marginal distributions over process values at a finite
number of input location. This distribution-based characterization leads to
generative strategies that scale cubically in the size of the desired random
vector. These methods are, therefore, prohibitively expensive in cases where
high-dimensional vectors - let alone continuous functions - are required. In
this work, we investigate a different line of reasoning. Rather than focusing
on distributions, we articulate Gaussian conditionals at the level of random
variables. We show how this pathwise interpretation of conditioning gives rise
to a general family of approximations that lend themselves to fast sampling
from Gaussian process posteriors. We analyze these methods, along with the
approximation errors they introduce, from first principles. We then complement
this theory, by exploring the practical ramifications of pathwise conditioning
in a various applied settings.
</p>
<a href="http://arxiv.org/abs/2011.04026" target="_blank">arXiv:2011.04026</a> [<a href="http://arxiv.org/pdf/2011.04026" target="_blank">pdf</a>]

<h2>Unwrapping The Black Box of Deep ReLU Networks: Interpretability, Diagnostics, and Simplification. (arXiv:2011.04041v1 [cs.LG])</h2>
<h3>Agus Sudjianto, William Knauth, Rahul Singh, Zebin Yang, Aijun Zhang</h3>
<p>The deep neural networks (DNNs) have achieved great success in learning
complex patterns with strong predictive power, but they are often thought of as
"black box" models without a sufficient level of transparency and
interpretability. It is important to demystify the DNNs with rigorous
mathematics and practical tools, especially when they are used for
mission-critical applications. This paper aims to unwrap the black box of deep
ReLU networks through local linear representation, which utilizes the
activation pattern and disentangles the complex network into an equivalent set
of local linear models (LLMs). We develop a convenient LLM-based toolkit for
interpretability, diagnostics, and simplification of a pre-trained deep ReLU
network. We propose the local linear profile plot and other visualization
methods for interpretation and diagnostics, and an effective merging strategy
for network simplification. The proposed methods are demonstrated by simulation
examples, benchmark datasets, and a real case study in home lending credit risk
assessment.
</p>
<a href="http://arxiv.org/abs/2011.04041" target="_blank">arXiv:2011.04041</a> [<a href="http://arxiv.org/pdf/2011.04041" target="_blank">pdf</a>]

<h2>FairLens: Auditing Black-box Clinical Decision Support Systems. (arXiv:2011.04049v1 [cs.LG])</h2>
<h3>Cecilia Panigutti, Alan Perotti, Andr&#xe8; Panisson, Paolo Bajardi, Dino Pedreschi</h3>
<p>The pervasive application of algorithmic decision-making is raising concerns
on the risk of unintended bias in AI systems deployed in critical settings such
as healthcare. The detection and mitigation of biased models is a very delicate
task which should be tackled with care and involving domain experts in the
loop. In this paper we introduce FairLens, a methodology for discovering and
explaining biases. We show how our tool can be used to audit a fictional
commercial black-box model acting as a clinical decision support system. In
this scenario, the healthcare facility experts can use FairLens on their own
historical data to discover the model's biases before incorporating it into the
clinical decision flow. FairLens first stratifies the available patient data
according to attributes such as age, ethnicity, gender and insurance; it then
assesses the model performance on such subgroups of patients identifying those
in need of expert evaluation. Finally, building on recent state-of-the-art XAI
(eXplainable Artificial Intelligence) techniques, FairLens explains which
elements in patients' clinical history drive the model error in the selected
subgroup. Therefore, FairLens allows experts to investigate whether to trust
the model and to spotlight group-specific biases that might constitute
potential fairness issues.
</p>
<a href="http://arxiv.org/abs/2011.04049" target="_blank">arXiv:2011.04049</a> [<a href="http://arxiv.org/pdf/2011.04049" target="_blank">pdf</a>]

<h2>Adaptive Federated Dropout: Improving Communication Efficiency and Generalization for Federated Learning. (arXiv:2011.04050v1 [cs.LG])</h2>
<h3>Nader Bouacida, Jiahui Hou, Hui Zang, Xin Liu</h3>
<p>With more regulations tackling users' privacy-sensitive data protection in
recent years, access to such data has become increasingly restricted and
controversial. To exploit the wealth of data generated and located at
distributed entities such as mobile phones, a revolutionary decentralized
machine learning setting, known as Federated Learning, enables multiple clients
located at different geographical locations to collaboratively learn a machine
learning model while keeping all their data on-device. However, the scale and
decentralization of federated learning present new challenges. Communication
between the clients and the server is considered a main bottleneck in the
convergence time of federated learning.

In this paper, we propose and study Adaptive Federated Dropout (AFD), a novel
technique to reduce the communication costs associated with federated learning.
It optimizes both server-client communications and computation costs by
allowing clients to train locally on a selected subset of the global model. We
empirically show that this strategy, combined with existing compression
methods, collectively provides up to 57x reduction in convergence time. It also
outperforms the state-of-the-art solutions for communication efficiency.
Furthermore, it improves model generalization by up to 1.7%.
</p>
<a href="http://arxiv.org/abs/2011.04050" target="_blank">arXiv:2011.04050</a> [<a href="http://arxiv.org/pdf/2011.04050" target="_blank">pdf</a>]

<h2>Predictive Analysis of Diabetic Retinopathy with Transfer Learning. (arXiv:2011.04052v1 [cs.CV])</h2>
<h3>Shreyas Rajesh Labhsetwar, Raj Sunil Salvi, Piyush Arvind Kolte, Veerasai Subramaniam venkatesh, Alistair Michael Baretto</h3>
<p>With the prevalence of Diabetes, the Diabetes Mellitus Retinopathy (DR) is
becoming a major health problem across the world. The long-term medical
complications arising due to DR have a significant impact on the patient as
well as the society, as the disease mostly affects individuals in their most
productive years. Early detection and treatment can help reduce the extent of
damage to the patients. The rise of Convolutional Neural Networks for
predictive analysis in the medical field paves the way for a robust solution to
DR detection. This paper studies the performance of several highly efficient
and scalable CNN architectures for Diabetic Retinopathy Classification with the
help of Transfer Learning. The research focuses on VGG16, Resnet50 V2 and
EfficientNet B0 models. The classification performance is analyzed using
several performance metrics including True Positive Rate, False Positive Rate,
Accuracy, etc. Also, several performance graphs are plotted for visualizing the
architecture performance including Confusion Matrix, ROC Curve, etc. The
results indicate that Transfer Learning with ImageNet weights using VGG 16
model demonstrates the best classification performance with the best Accuracy
of 95%. It is closely followed by ResNet50 V2 architecture with the best
Accuracy of 93%. This paper shows that predictive analysis of DR from retinal
images is achieved with Transfer Learning on Convolutional Neural Networks.
</p>
<a href="http://arxiv.org/abs/2011.04052" target="_blank">arXiv:2011.04052</a> [<a href="http://arxiv.org/pdf/2011.04052" target="_blank">pdf</a>]

<h2>Fourier-based and Rational Graph Filters for Spectral Processing. (arXiv:2011.04055v1 [cs.LG])</h2>
<h3>Giuseppe Patan&#xe8;</h3>
<p>Data are represented as graphs in a wide range of applications, such as
Computer Vision (e.g., images) and Graphics (e.g., 3D meshes), network analysis
(e.g., social networks), and bio-informatics (e.g., molecules). In this
context, our overall goal is the definition of novel Fourier-based and graph
filters induced by rational polynomials for graph processing, which generalise
polynomial filters and the Fourier transform to non-Euclidean domains. For the
efficient evaluation of discrete spectral Fourier-based and wavelet operators,
we introduce a spectrum-free approach, which requires the solution of a small
set of sparse, symmetric, well-conditioned linear systems and is oblivious of
the evaluation of the Laplacian or kernel spectrum. Approximating arbitrary
graph filters with rational polynomials provides a more accurate and
numerically stable alternative with respect to polynomials. To achieve these
goals, we also study the link between spectral operators, wavelets, and
filtered convolution with integral operators induced by spectral kernels.
According to our tests, main advantages of the proposed approach are (i) its
generality with respect to the input data (e.g., graphs, 3D shapes),
applications (e.g., signal reconstruction and smoothing, shape correspondence),
and filters (e.g., polynomial, rational polynomial), and (ii) a spectrum-free
computation with a generally low computational cost and storage overhead.
</p>
<a href="http://arxiv.org/abs/2011.04055" target="_blank">arXiv:2011.04055</a> [<a href="http://arxiv.org/pdf/2011.04055" target="_blank">pdf</a>]

<h2>Performance Analysis of Optimizers for Plant Disease Classification with Convolutional Neural Networks. (arXiv:2011.04056v1 [cs.CV])</h2>
<h3>Shreyas Rajesh Labhsetwar, Soumya Haridas, Riyali Panmand, Rutuja Deshpande, Piyush Arvind Kolte, Sandhya Pati</h3>
<p>Crop failure owing to pests &amp; diseases are inherent within Indian
agriculture, leading to annual losses of 15 to 25% of productivity, resulting
in a huge economic loss. This research analyzes the performance of various
optimizers for predictive analysis of plant diseases with deep learning
approach. The research uses Convolutional Neural Networks for classification of
farm or plant leaf samples of 3 crops into 15 classes. The various optimizers
used in this research include RMSprop, Adam and AMSgrad. Optimizers Performance
is visualised by plotting the Training and Validation Accuracy and Loss curves,
ROC curves and Confusion Matrix. The best performance is achieved using Adam
optimizer, with the maximum validation accuracy being 98%. This paper focuses
on the research analysis proving that plant diseases can be predicted and
pre-empted using deep learning methodology with the help of satellite, drone
based or mobile based images that result in reducing crop failure and
agricultural losses.
</p>
<a href="http://arxiv.org/abs/2011.04056" target="_blank">arXiv:2011.04056</a> [<a href="http://arxiv.org/pdf/2011.04056" target="_blank">pdf</a>]

<h2>Analysis of Dimensional Influence of Convolutional Neural Networks for Histopathological Cancer Classification. (arXiv:2011.04057v1 [cs.CV])</h2>
<h3>Shreyas Rajesh Labhsetwar, Alistair Michael Baretto, Raj Sunil Salvi, Piyush Arvind Kolte, Veerasai Subramaniam Venkatesh</h3>
<p>Convolutional Neural Networks can be designed with different levels of
complexity depending upon the task at hand. This paper analyzes the effect of
dimensional changes to the CNN architecture on its performance on the task of
Histopathological Cancer Classification. The research starts with a baseline
10-layer CNN model with (3 X 3) convolution filters. Thereafter, the baseline
architecture is scaled in multiple dimensions including width, depth,
resolution and a combination of all of these. Width scaling involves
inculcating greater number of neurons per CNN layer, whereas depth scaling
involves deepening the hierarchical layered structure. Resolution scaling is
performed by increasing the dimensions of the input image, and compound scaling
involves a hybrid combination of width, depth and resolution scaling. The
results indicate that histopathological cancer scans are very complex in nature
and hence require high resolution images fed to a large hierarchy of
Convolution, MaxPooling, Dropout and Batch Normalization layers to extract all
the intricacies and perform perfect classification. Since compound scaling the
baseline model ensures that all the three dimensions: width, depth and
resolution are scaled, the best performance is obtained with compound scaling.
This research shows that better performance of CNN models is achieved by
compound scaling of the baseline model for the task of Histopathological Cancer
Classification.
</p>
<a href="http://arxiv.org/abs/2011.04057" target="_blank">arXiv:2011.04057</a> [<a href="http://arxiv.org/pdf/2011.04057" target="_blank">pdf</a>]

<h2>MLAS: Metric Learning on Attributed Sequences. (arXiv:2011.04062v1 [cs.LG])</h2>
<h3>Zhongfang Zhuang, Xiangnan Kong, Elke Rundensteiner, Jihane Zouaoui, Aditya Arora</h3>
<p>Distance metric learning has attracted much attention in recent years, where
the goal is to learn a distance metric based on user feedback. Conventional
approaches to metric learning mainly focus on learning the Mahalanobis distance
metric on data attributes. Recent research on metric learning has been extended
to sequential data, where we only have structural information in the sequences,
but no attribute is available. However, real-world applications often involve
attributed sequence data (e.g., clickstreams), where each instance consists of
not only a set of attributes (e.g., user session context) but also a sequence
of categorical items (e.g., user actions). In this paper, we study the problem
of metric learning on attributed sequences. Unlike previous work on metric
learning, we now need to go beyond the Mahalanobis distance metric in the
attribute feature space while also incorporating the structural information in
sequences. We propose a deep learning framework, called MLAS (Metric Learning
on Attributed Sequences), to learn a distance metric that effectively measures
dissimilarities between attributed sequences. Empirical results on real-world
datasets demonstrate that the proposed MLAS framework significantly improves
the performance of metric learning compared to state-of-the-art methods on
attributed sequences.
</p>
<a href="http://arxiv.org/abs/2011.04062" target="_blank">arXiv:2011.04062</a> [<a href="http://arxiv.org/pdf/2011.04062" target="_blank">pdf</a>]

<h2>AI on the Bog: Monitoring and Evaluating Cranberry Crop Risk. (arXiv:2011.04064v1 [cs.CV])</h2>
<h3>Peri Akiva, Benjamin Planche, Aditi Roy, Kristin Dana, Peter Oudemans, Michael Mars</h3>
<p>Machine vision for precision agriculture has attracted considerable research
interest in recent years. The goal of this paper is to develop an end-to-end
cranberry health monitoring system to enable and support real time cranberry
over-heating assessment to facilitate informed decisions that may sustain the
economic viability of the farm. Toward this goal, we propose two main deep
learning-based modules for: 1) cranberry fruit segmentation to delineate the
exact fruit regions in the cranberry field image that are exposed to sun, 2)
prediction of cloud coverage conditions and sun irradiance to estimate the
inner temperature of exposed cranberries. We develop drone-based field data and
ground-based sky data collection systems to collect video imagery at multiple
time points for use in crop health analysis. Extensive evaluation on the data
set shows that it is possible to predict exposed fruit's inner temperature with
high accuracy (0.02% MAPE). The sun irradiance prediction error was found to be
8.41-20.36% MAPE in the 5-20 minutes time horizon. With 62.54% mIoU for
segmentation and 13.46 MAE for counting accuracies in exposed fruit
identification, this system is capable of giving informed feedback to growers
to take precautionary action (e.g. irrigation) in identified crop field regions
with higher risk of sunburn in the near future. Though this novel system is
applied for cranberry health monitoring, it represents a pioneering step
forward for efficient farming and is useful in precision agriculture beyond the
problem of cranberry overheating.
</p>
<a href="http://arxiv.org/abs/2011.04064" target="_blank">arXiv:2011.04064</a> [<a href="http://arxiv.org/pdf/2011.04064" target="_blank">pdf</a>]

<h2>Bait and Switch: Online Training Data Poisoning of Autonomous Driving Systems. (arXiv:2011.04065v1 [cs.LG])</h2>
<h3>Naman Patel, Prashanth Krishnamurthy, Siddharth Garg, Farshad Khorrami</h3>
<p>We show that by controlling parts of a physical environment in which a
pre-trained deep neural network (DNN) is being fine-tuned online, an adversary
can launch subtle data poisoning attacks that degrade the performance of the
system. While the attack can be applied in general to any perception task, we
consider a DNN based traffic light classifier for an autonomous car that has
been trained in one city and is being fine-tuned online in another city. We
show that by injecting environmental perturbations that do not modify the
traffic lights themselves or ground-truth labels, the adversary can cause the
deep network to learn spurious concepts during the online learning phase. The
attacker can leverage the introduced spurious concepts in the environment to
cause the model's accuracy to degrade during operation; therefore, causing the
system to malfunction.
</p>
<a href="http://arxiv.org/abs/2011.04065" target="_blank">arXiv:2011.04065</a> [<a href="http://arxiv.org/pdf/2011.04065" target="_blank">pdf</a>]

<h2>An HVS-Oriented Saliency Map Prediction Modeling. (arXiv:2011.04076v1 [cs.CV])</h2>
<h3>Qiang Li</h3>
<p>Visual attention is one of the most significant characteristics for selecting
and understanding the outside world. The nature complex scenes, including
larger redundancy and human vision, can't be processing all information
simultaneously because of the information bottleneck. The visual system mainly
focuses on dominant parts of the scenes to reduce the input visual redundancy
information. It's commonly known as visual attention prediction or visual
saliency map. This paper proposes a new saliency prediction architecture
inspired by human low-level visual cortex function. The model considered the
opponent color channel, wavelet energy map, and contrast sensitivity function
for extract image features and maximum approach to real visual neural network
function in the brain. The proposed model is evaluated several datasets,
including MIT1003, MIT300, TORONTO, and SID4VAM to explain its efficiency. The
proposed model results are quantitatively and qualitatively compared to other
state-of-the-art salience prediction models and their achieved out-performing
of visual saliency prediction.
</p>
<a href="http://arxiv.org/abs/2011.04076" target="_blank">arXiv:2011.04076</a> [<a href="http://arxiv.org/pdf/2011.04076" target="_blank">pdf</a>]

<h2>A Semantic Framework for Enabling Radio Spectrum Policy Management and Evaluation. (arXiv:2011.04085v1 [cs.AI])</h2>
<h3>H. Santos, A. Mulvehill, J. S. Erickson, J. P. McCusker, M. Gordon, O. Xie, S. Stouffer, G. Capraro, A. Pidwerbetsky, J. Burgess, A. Berlinsky, K. Turck, J. Ashdown, D. L. McGuinness</h3>
<p>Because radio spectrum is a finite resource, its usage and sharing is
regulated by government agencies. These agencies define policies to manage
spectrum allocation and assignment across multiple organizations, systems, and
devices. With more portions of the radio spectrum being licensed for commercial
use, the importance of providing an increased level of automation when
evaluating such policies becomes crucial for the efficiency and efficacy of
spectrum management. We introduce our Dynamic Spectrum Access Policy Framework
for supporting the United States government's mission to enable both federal
and non-federal entities to compatibly utilize available spectrum. The DSA
Policy Framework acts as a machine-readable policy repository providing policy
management features and spectrum access request evaluation. The framework
utilizes a novel policy representation using OWL and PROV-O along with a
domain-specific reasoning implementation that mixes GeoSPARQL, OWL reasoning,
and knowledge graph traversal to evaluate incoming spectrum access requests and
explain how applicable policies were used. The framework is currently being
used to support live, over-the-air field exercises involving a diverse set of
federal and commercial radios, as a component of a prototype spectrum
management system.
</p>
<a href="http://arxiv.org/abs/2011.04085" target="_blank">arXiv:2011.04085</a> [<a href="http://arxiv.org/pdf/2011.04085" target="_blank">pdf</a>]

<h2>Kimera-Multi: a System for Distributed Multi-Robot Metric-Semantic Simultaneous Localization and Mapping. (arXiv:2011.04087v1 [cs.RO])</h2>
<h3>Yun Chang, Yulun Tian, Jonathan P. How, Luca Carlone</h3>
<p>We present the first fully distributed multi-robot system for dense
metric-semantic Simultaneous Localization and Mapping (SLAM). Our system,
dubbed Kimera-Multi, is implemented by a team of robots equipped with
visual-inertial sensors, and builds a 3D mesh model of the environment in
real-time, where each face of the mesh is annotated with a semantic label
(e.g., building, road, objects). In Kimera-Multi, each robot builds a local
trajectory estimate and a local mesh using Kimera. Then, when two robots are
within communication range, they initiate a distributed place recognition and
robust pose graph optimization protocol with a novel incremental maximum clique
outlier rejection; the protocol allows the robots to improve their local
trajectory estimates by leveraging inter-robot loop closures. Finally, each
robot uses its improved trajectory estimate to correct the local mesh using
mesh deformation techniques. We demonstrate Kimera-Multi in photo-realistic
simulations and real data. Kimera-Multi (i) is able to build accurate 3D
metric-semantic meshes, (ii) is robust to incorrect loop closures while
requiring less computation than state-of-the-art distributed SLAM back-ends,
and (iii) is efficient, both in terms of computation at each robot as well as
communication bandwidth.
</p>
<a href="http://arxiv.org/abs/2011.04087" target="_blank">arXiv:2011.04087</a> [<a href="http://arxiv.org/pdf/2011.04087" target="_blank">pdf</a>]

<h2>Exploiting collisions for sampling-based multicopter motion planning. (arXiv:2011.04091v1 [cs.RO])</h2>
<h3>Jiaming Zha, Mark W. Mueller</h3>
<p>Multicopters with collision-resilient designs can operate with trajectories
involving collisions. This paper presents a sampling-based method that can
exploit collisions for better motion planning. The method is built upon the
basis of the RRT* algorithm and takes advantages of fast motion primitive
generation and collision checking for multicopters. It generates collision
states by detecting potential intersections between motion primitives and
obstacles, and connects these states with other sampled states to form
collision-inclusive trajectories. We show that allowing collision helps improve
the performance of the sampling-based planner in narrow spaces like tunnels.
Finally, an experiment of tracking the trajectory generated by the
collision-inclusive planner is presented.
</p>
<a href="http://arxiv.org/abs/2011.04091" target="_blank">arXiv:2011.04091</a> [<a href="http://arxiv.org/pdf/2011.04091" target="_blank">pdf</a>]

<h2>Image Clustering using an Augmented Generative Adversarial Network and Information Maximization. (arXiv:2011.04094v1 [cs.CV])</h2>
<h3>Foivos Ntelemis, Yaochu Jin, Spencer A. Thomas</h3>
<p>Image clustering has recently attracted significant attention due to the
increased availability of unlabelled datasets. The efficiency of traditional
clustering algorithms heavily depends on the distance functions used and the
dimensionality of the features. Therefore, performance degradation is often
observed when tackling either unprocessed images or high-dimensional features
extracted from processed images. To deal with these challenges, we propose a
deep clustering framework consisting of a modified generative adversarial
network (GAN) and an auxiliary classifier. The modification employs Sobel
operations prior to the discriminator of the GAN to enhance the separability of
the learned features. The discriminator is then leveraged to generate
representations as the input to an auxiliary classifier. An adaptive objective
function is utilised to train the auxiliary classifier for clustering the
representations, aiming to increase the robustness by minimizing the divergence
of multiple representations generated by the discriminator. The auxiliary
classifier is implemented with a group of multiple cluster-heads, where a
tolerance hyper-parameter is used to tackle imbalanced data. Our results
indicate that the proposed method significantly outperforms state-of-the-art
clustering methods on CIFAR-10 and CIFAR-100, and is competitive on the STL10
and MNIST datasets.
</p>
<a href="http://arxiv.org/abs/2011.04094" target="_blank">arXiv:2011.04094</a> [<a href="http://arxiv.org/pdf/2011.04094" target="_blank">pdf</a>]

<h2>Reliable Off-policy Evaluation for Reinforcement Learning. (arXiv:2011.04102v1 [cs.LG])</h2>
<h3>Jie Wang, Rui Gao, Hongyuan Zha</h3>
<p>In a sequential decision-making problem, off-policy evaluation (OPE)
estimates the expected cumulative reward of a target policy using logged
transition data generated from a different behavior policy, without execution
of the target policy. Reinforcement learning in high-stake environments, such
as healthcare and education, is often limited to off-policy settings due to
safety or ethical concerns, or inability of exploration. Hence it is imperative
to quantify the uncertainty of the off-policy estimate before deployment of the
target policy. In this paper, we propose a novel framework that provides robust
and optimistic cumulative reward estimates with statistical guarantees and
develop non-asymptotic as well as asymptotic confidence intervals for OPE,
leveraging methodologies from distributionally robust optimization. Our
theoretical results are also supported by empirical analysis.
</p>
<a href="http://arxiv.org/abs/2011.04102" target="_blank">arXiv:2011.04102</a> [<a href="http://arxiv.org/pdf/2011.04102" target="_blank">pdf</a>]

<h2>Evolution of Artificial Intelligent Plane. (arXiv:2011.04105v1 [cs.AI])</h2>
<h3>Puneet Kumar</h3>
<p>With the growth of the internet, it is becoming hard to manage, configure and
monitor networks. Recent trends to control and operate them is artificial
intelligence based automation to minimize human intervention. Albeit this
concept has been introduced since a decade with several different names, but
the underlying goal remains the same, which is to make network intelligent
enough to assemble, reassemble if configuration changes, and detect a problem
on its own and fix it. As a result, in addition to Data Plane, Control Plane
and Management Plane, a new plane called Artificial Intelligence (AI) Plane is
introduced. Our main objective is to analyze all major AI plane techniques,
frameworks and algorithms proposed in various types of networks. We propose a
comprehensive and network independent framework to cover all aspects of AI
plane, in particular we provide a systematically means of comparison. In
conjunction to make AI plane understand simpler, this framework highlights
relevant challenges and design considerations for future research. To the best
of our knowledge this is the first survey report which represents a complete
comparison of AI planes with their investigation issues in several types of
networks.
</p>
<a href="http://arxiv.org/abs/2011.04105" target="_blank">arXiv:2011.04105</a> [<a href="http://arxiv.org/pdf/2011.04105" target="_blank">pdf</a>]

<h2>Ensembled CTR Prediction via Knowledge Distillation. (arXiv:2011.04106v1 [cs.LG])</h2>
<h3>Jieming Zhu, Jinyang Liu, Weiqi Li, Jincai Lai, Xiuqiang He, Liang Chen, Zibin Zheng</h3>
<p>Recently, deep learning-based models have been widely studied for
click-through rate (CTR) prediction and lead to improved prediction accuracy in
many industrial applications. However, current research focuses primarily on
building complex network architectures to better capture sophisticated feature
interactions and dynamic user behaviors. The increased model complexity may
slow down online inference and hinder its adoption in real-time applications.
Instead, our work targets at a new model training strategy based on knowledge
distillation (KD). KD is a teacher-student learning framework to transfer
knowledge learned from a teacher model to a student model. The KD strategy not
only allows us to simplify the student model as a vanilla DNN model but also
achieves significant accuracy improvements over the state-of-the-art teacher
models. The benefits thus motivate us to further explore the use of a powerful
ensemble of teachers for more accurate student model training. We also propose
some novel techniques to facilitate ensembled CTR prediction, including teacher
gating and early stopping by distillation loss. We conduct comprehensive
experiments against 12 existing models and across three industrial datasets.
Both offline and online A/B testing results show the effectiveness of our
KD-based training strategy.
</p>
<a href="http://arxiv.org/abs/2011.04106" target="_blank">arXiv:2011.04106</a> [<a href="http://arxiv.org/pdf/2011.04106" target="_blank">pdf</a>]

<h2>Joint Estimation of Expertise and Reward Preferences From Human Demonstrations. (arXiv:2011.04118v1 [cs.RO])</h2>
<h3>Pamela Carreno-Medrano, Stephen L. Smith, Dana Kulic</h3>
<p>When a robot learns from human examples, most approaches assume that the
human partner provides examples of optimal behavior. However, there are
applications in which the robot learns from non-expert humans. We argue that
the robot should learn not only about the human's objectives, but also about
their expertise level. The robot could then leverage this joint information to
reduce or increase the frequency at which it provides assistance to its human's
partner or be more cautious when learning new skills from novice users.
Similarly, by taking into account the human's expertise, the robot would also
be able of inferring a human's true objectives even when the human's fails to
properly demonstrate these objectives due to a lack of expertise. In this
paper, we propose to jointly infer the expertise level and objective function
of a human given observations of their (possibly) non-optimal demonstrations.
Two inference approaches are proposed. In the first approach, inference is done
over a finite, discrete set of possible objective functions and expertise
levels. In the second approach, the robot optimizes over the space of all
possible hypotheses and finds the objective function and expertise level that
best explain the observed human behavior. We demonstrate our proposed
approaches both in simulation and with real user data.
</p>
<a href="http://arxiv.org/abs/2011.04118" target="_blank">arXiv:2011.04118</a> [<a href="http://arxiv.org/pdf/2011.04118" target="_blank">pdf</a>]

<h2>Distance-Based Anomaly Detection for Industrial Surfaces Using Triplet Networks. (arXiv:2011.04121v1 [cs.CV])</h2>
<h3>Tareq Tayeh, Sulaiman Aburakhia, Ryan Myers, Abdallah Shami</h3>
<p>Surface anomaly detection plays an important quality control role in many
manufacturing industries to reduce scrap production. Machine-based visual
inspections have been utilized in recent years to conduct this task instead of
human experts. In particular, deep learning Convolutional Neural Networks
(CNNs) have been at the forefront of these image processing-based solutions due
to their predictive accuracy and efficiency. Training a CNN on a classification
objective requires a sufficiently large amount of defective data, which is
often not available. In this paper, we address that challenge by training the
CNN on surface texture patches with a distance-based anomaly detection
objective instead. A deep residual-based triplet network model is utilized, and
defective training samples are synthesized exclusively from non-defective
samples via random erasing techniques to directly learn a similarity metric
between the same-class samples and out-of-class samples. Evaluation results
demonstrate the approach's strength in detecting different types of anomalies,
such as bent, broken, or cracked surfaces, for known surfaces that are part of
the training data and unseen novel surfaces.
</p>
<a href="http://arxiv.org/abs/2011.04121" target="_blank">arXiv:2011.04121</a> [<a href="http://arxiv.org/pdf/2011.04121" target="_blank">pdf</a>]

<h2>Localising In Complex Scenes Using Balanced Adversarial Adaptation. (arXiv:2011.04122v1 [cs.CV])</h2>
<h3>Gil Avraham, Yan Zuo, Tom Drummond</h3>
<p>Domain adaptation and generative modelling have collectively mitigated the
expensive nature of data collection and labelling by leveraging the rich
abundance of accurate, labelled data in simulation environments. In this work,
we study the performance gap that exists between representations optimised for
localisation on simulation environments and the application of such
representations in a real-world setting. Our method exploits the shared
geometric similarities between simulation and real-world environments whilst
maintaining invariance towards visual discrepancies. This is achieved by
optimising a representation extractor to project both simulated and real
representations into a shared representation space. Our method uses a
symmetrical adversarial approach which encourages the representation extractor
to conceal the domain that features are extracted from and simultaneously
preserves robust attributes between source and target domains that are
beneficial for localisation. We evaluate our method by adapting representations
optimised for indoor Habitat simulated environments (Matterport3D and Replica)
to a real-world indoor environment (Active Vision Dataset), showing that it
compares favourably against fully-supervised approaches.
</p>
<a href="http://arxiv.org/abs/2011.04122" target="_blank">arXiv:2011.04122</a> [<a href="http://arxiv.org/pdf/2011.04122" target="_blank">pdf</a>]

<h2>Deep Learning based Monocular Depth Prediction: Datasets, Methods and Applications. (arXiv:2011.04123v1 [cs.CV])</h2>
<h3>Qing Li, Jiasong Zhu, Jun Liu, Rui Cao, Qingquan Li, Sen Jia, Guoping Qiu</h3>
<p>Estimating depth from RGB images can facilitate many computer vision tasks,
such as indoor localization, height estimation, and simultaneous localization
and mapping (SLAM). Recently, monocular depth estimation has obtained great
progress owing to the rapid development of deep learning techniques. They
surpass traditional machine learning-based methods by a large margin in terms
of accuracy and speed. Despite the rapid progress in this topic, there are
lacking of a comprehensive review, which is needed to summarize the current
progress and provide the future directions. In this survey, we first introduce
the datasets for depth estimation, and then give a comprehensive introduction
of the methods from three perspectives: supervised learning-based methods,
unsupervised learning-based methods, and sparse samples guidance-based methods.
In addition, downstream applications that benefit from the progress have also
been illustrated. Finally, we point out the future directions and conclude the
paper.
</p>
<a href="http://arxiv.org/abs/2011.04123" target="_blank">arXiv:2011.04123</a> [<a href="http://arxiv.org/pdf/2011.04123" target="_blank">pdf</a>]

<h2>What Does CNN Shift Invariance Look Like? A Visualization Study. (arXiv:2011.04127v1 [cs.LG])</h2>
<h3>Jake Lee, Junfeng Yang, Zhangyang Wang</h3>
<p>Feature extraction with convolutional neural networks (CNNs) is a popular
method to represent images for machine learning tasks. These representations
seek to capture global image content, and ideally should be independent of
geometric transformations. We focus on measuring and visualizing the shift
invariance of extracted features from popular off-the-shelf CNN models. We
present the results of three experiments comparing representations of millions
of images with exhaustively shifted objects, examining both local invariance
(within a few pixels) and global invariance (across the image frame). We
conclude that features extracted from popular networks are not globally
invariant, and that biases and artifacts exist within this variance.
Additionally, we determine that anti-aliased models significantly improve local
invariance but do not impact global invariance. Finally, we provide a code
repository for experiment reproduction, as well as a website to interact with
our results at https://jakehlee.github.io/visualize-invariance.
</p>
<a href="http://arxiv.org/abs/2011.04127" target="_blank">arXiv:2011.04127</a> [<a href="http://arxiv.org/pdf/2011.04127" target="_blank">pdf</a>]

<h2>Stable predictions for health related anticausal prediction tasks affected by selection biases: the need to deconfound the test set features. (arXiv:2011.04128v1 [stat.ML])</h2>
<h3>Elias Chaibub Neto, Phil Snyder, Solveig K Sieberts, Larsson Omberg</h3>
<p>In health related machine learning applications, the training data often
corresponds to a non-representative sample from the target populations where
the learners will be deployed. In anticausal prediction tasks, selection biases
often make the associations between confounders and the outcome variable
unstable across different target environments. As a consequence, the
predictions from confounded learners are often unstable, and might fail to
generalize in shifted test environments. Stable prediction approaches aim to
solve this problem by producing predictions that are stable across unknown test
environments. These approaches, however, are sometimes applied to the training
data alone with the hope that training an unconfounded model will be enough to
generate stable predictions in shifted test sets. Here, we show that this is
insufficient, and that improved stability can be achieved by deconfounding the
test set features as well. We illustrate these observations using both
synthetic data and real world data from a mobile health study.
</p>
<a href="http://arxiv.org/abs/2011.04128" target="_blank">arXiv:2011.04128</a> [<a href="http://arxiv.org/pdf/2011.04128" target="_blank">pdf</a>]

<h2>Virtual Model Control for Wheel-legged Robotic Systems with Prescribed Transient Performance. (arXiv:2011.04138v1 [cs.RO])</h2>
<h3>Dongchen Liu, Junzheng Wang, Shoukun Wang, Dawei Shi, Huaihang Zheng, Yuan Huang</h3>
<p>This work proposes a posture adjustment strategy for wheel-legged mechanisms
via virtual model control with prescribed transient performance. A simple model
of a rigid block subjected to a 6-dimensional force at the center of gravity
(CoG) is introduced to be the virtual model of the wheel-legged control system.
The force tracking of the wheel-legs is realized with prescribed transient
performance based on the funnel control strategy. To improve the robustness of
the scheme, an event-triggering condition is designed for on-line segment of
the funnel function, such that the force tracking error evolves inside the
performance funnel with proved convergence. The absence of Zeno behavior for
the event-based mechanism is also guaranteed. With the force references of the
wheel-legs are planned for the vector sum tracks the 6-dimensional force from
the virtual model, the posture adjustment is achieved on uneven roads by the
force tracking of wheel-legs. Experimental results are presented to validate
the stability and effectiveness of the proposed method.
</p>
<a href="http://arxiv.org/abs/2011.04138" target="_blank">arXiv:2011.04138</a> [<a href="http://arxiv.org/pdf/2011.04138" target="_blank">pdf</a>]

<h2>Uncertainty-Aware Constraint Learning for Adaptive Safe Motion Planning from Demonstrations. (arXiv:2011.04141v1 [cs.RO])</h2>
<h3>Glen Chou, Necmiye Ozay, Dmitry Berenson</h3>
<p>We present a method for learning to satisfy uncertain constraints from
demonstrations. Our method uses robust optimization to obtain a belief over the
potentially infinite set of possible constraints consistent with the
demonstrations, and then uses this belief to plan trajectories that trade off
performance with satisfying the possible constraints. We use these trajectories
in a closed-loop policy that executes and replans using belief updates, which
incorporate data gathered during execution. We derive guarantees on the
accuracy of our constraint belief and probabilistic guarantees on plan safety.
We present results on a 7-DOF arm and 12D quadrotor, showing our method can
learn to satisfy high-dimensional (up to 30D) uncertain constraints, and
outperforms baselines in safety and efficiency.
</p>
<a href="http://arxiv.org/abs/2011.04141" target="_blank">arXiv:2011.04141</a> [<a href="http://arxiv.org/pdf/2011.04141" target="_blank">pdf</a>]

<h2>Sinkhorn Natural Gradient for Generative Models. (arXiv:2011.04162v1 [stat.ML])</h2>
<h3>Zebang Shen, Zhenfu Wang, Alejandro Ribeiro, Hamed Hassani</h3>
<p>We consider the problem of minimizing a functional over a parametric family
of probability measures, where the parameterization is characterized via a
push-forward structure. An important application of this problem is in training
generative adversarial networks. In this regard, we propose a novel Sinkhorn
Natural Gradient (SiNG) algorithm which acts as a steepest descent method on
the probability space endowed with the Sinkhorn divergence. We show that the
Sinkhorn information matrix (SIM), a key component of SiNG, has an explicit
expression and can be evaluated accurately in complexity that scales
logarithmically with respect to the desired accuracy. This is in sharp contrast
to existing natural gradient methods that can only be carried out
approximately. Moreover, in practical applications when only Monte-Carlo type
integration is available, we design an empirical estimator for SIM and provide
the stability analysis. In our experiments, we quantitatively compare SiNG with
state-of-the-art SGD-type solvers on generative tasks to demonstrate its
efficiency and efficacy of our method.
</p>
<a href="http://arxiv.org/abs/2011.04162" target="_blank">arXiv:2011.04162</a> [<a href="http://arxiv.org/pdf/2011.04162" target="_blank">pdf</a>]

<h2>Distant Supervision for E-commerce Query Segmentation via Attention Network. (arXiv:2011.04166v1 [cs.AI])</h2>
<h3>Zhao Li, Donghui Ding, Pengcheng Zou, Yu Gong, Xi Chen, Ji Zhang, Jianliang Gao, Youxi Wu, Yucong Duan</h3>
<p>The booming online e-commerce platforms demand highly accurate approaches to
segment queries that carry the product requirements of consumers. Recent works
have shown that the supervised methods, especially those based on deep
learning, are attractive for achieving better performance on the problem of
query segmentation. However, the lack of labeled data is still a big challenge
for training a deep segmentation network, and the problem of Out-of-Vocabulary
(OOV) also adversely impacts the performance of query segmentation. Different
from query segmentation task in an open domain, e-commerce scenario can provide
external documents that are closely related to these queries. Thus, to deal
with the two challenges, we employ the idea of distant supervision and design a
novel method to find contexts in external documents and extract features from
these contexts. In this work, we propose a BiLSTM-CRF based model with an
attention module to encode external features, such that external contexts
information, which can be utilized naturally and effectively to help query
segmentation. Experiments on two datasets show the effectiveness of our
approach compared with several kinds of baselines.
</p>
<a href="http://arxiv.org/abs/2011.04166" target="_blank">arXiv:2011.04166</a> [<a href="http://arxiv.org/pdf/2011.04166" target="_blank">pdf</a>]

<h2>Synthetic Over-sampling with the Minority and Majority classes for imbalance problems. (arXiv:2011.04170v1 [cs.LG])</h2>
<h3>Hadi A. Khorshidi, Uwe Aickelin</h3>
<p>Class imbalance is a substantial challenge in classifying many real-world
cases. Synthetic over-sampling methods have been effective to improve the
performance of classifiers for imbalance problems. However, most synthetic
over-sampling methods generate non-diverse synthetic instances within the
convex hull formed by the existing minority instances as they only concentrate
on the minority class and ignore the vast information provided by the majority
class. They also often do not perform well for extremely imbalanced data as the
fewer the minority instances, the less information to generate synthetic
instances. Moreover, existing methods that generate synthetic instances using
distributional information of the majority class cannot perform effectively
when the majority class has a multi-modal distribution. We propose a new method
to generate diverse and adaptable synthetic instances using Synthetic
Over-sampling with the Minority and Majority classes (SOMM). SOMM generates
synthetic instances diversely within the minority data space. It updates the
generated instances adaptively to the neighbourhood including both classes.
Thus, SOMM performs well for both binary and multiclass imbalance problems. We
examine the performance of SOMM for binary and multiclass problems using
benchmark data sets for different imbalance levels. The empirical results show
the superiority of SOMM compared to other existing methods.
</p>
<a href="http://arxiv.org/abs/2011.04170" target="_blank">arXiv:2011.04170</a> [<a href="http://arxiv.org/pdf/2011.04170" target="_blank">pdf</a>]

<h2>Geometric Structure Aided Visual Inertial Localization. (arXiv:2011.04173v1 [cs.RO])</h2>
<h3>Huaiyang Huang, Haoyang Ye, Jianhao Jiao, Yuxiang Sun, Ming Liu</h3>
<p>Visual Localization is an essential component in autonomous navigation.
Existing approaches are either based on the visual structure from SLAM/SfM or
the geometric structure from dense mapping. To take the advantages of both, in
this work, we present a complete visual inertial localization system based on a
hybrid map representation to reduce the computational cost and increase the
positioning accuracy. Specially, we propose two modules for data association
and batch optimization, respectively. To this end, we develop an efficient data
association module to associate map components with local features, which takes
only $2$ms to generate temporal landmarks. For batch optimization, instead of
using visual factors, we develop a module to estimate a pose prior from the
instant localization results to constrain poses. The experimental results on
the EuRoC MAV dataset demonstrate a competitive performance compared to the
state of the arts. Specially, our system achieves an average position error in
1.7 cm with 100% recall. The timings show that the proposed modules reduce the
computational cost by 20-30%. We will make our implementation open source at
this http URL
</p>
<a href="http://arxiv.org/abs/2011.04173" target="_blank">arXiv:2011.04173</a> [<a href="http://arxiv.org/pdf/2011.04173" target="_blank">pdf</a>]

<h2>Multi-label Causal Variable Discovery: Learning Common Causal Variables and Label-specific Causal Variables. (arXiv:2011.04176v1 [cs.LG])</h2>
<h3>Xingyu Wu, Bingbing Jiang, Yan Zhong, Huanhuan Chen</h3>
<p>Causal variables in Markov boundary (MB) have been widely applied in
extensive single-label tasks. While few researches focus on the causal variable
discovery in multi-label data due to the complex causal relationships. Since
some variables in multi-label scenario might contain causal information about
multiple labels, this paper investigates the problem of multi-label causal
variable discovery as well as the distinguishing between common causal
variables shared by multiple labels and label-specific causal variables
associated with some single labels. Considering the multiple MBs under the
non-positive joint probability distribution, we explore the relationships
between common causal variables and equivalent information phenomenon, and find
that the solutions are influenced by equivalent information following different
mechanisms with or without existence of label causality. Analyzing these
mechanisms, we provide the theoretical property of common causal variables,
based on which the discovery and distinguishing algorithm is designed to
identify these two types of variables. Similar to single-label problem, causal
variables for multiple labels also have extensive application prospects. To
demonstrate this, we apply the proposed causal mechanism to multi-label feature
selection and present an interpretable algorithm, which is proved to achieve
the minimal redundancy and the maximum relevance. Extensive experiments
demonstrate the efficacy of these contributions.
</p>
<a href="http://arxiv.org/abs/2011.04176" target="_blank">arXiv:2011.04176</a> [<a href="http://arxiv.org/pdf/2011.04176" target="_blank">pdf</a>]

<h2>Two-Stream Appearance Transfer Network for Person Image Generation. (arXiv:2011.04181v1 [cs.CV])</h2>
<h3>Chengkang Shen, Peiyan Wang, Wei Tang</h3>
<p>Pose guided person image generation means to generate a photo-realistic
person image conditioned on an input person image and a desired pose. This task
requires spatial manipulation of the source image according to the target pose.
However, the generative adversarial networks (GANs) widely used for image
generation and translation rely on spatially local and translation equivariant
operators, i.e., convolution, pooling and unpooling, which cannot handle large
image deformation. This paper introduces a novel two-stream appearance transfer
network (2s-ATN) to address this challenge. It is a multi-stage architecture
consisting of a source stream and a target stream. Each stage features an
appearance transfer module and several two-stream feature fusion modules. The
former finds the dense correspondence between the two-stream feature maps and
then transfers the appearance information from the source stream to the target
stream. The latter exchange local information between the two streams and
supplement the non-local appearance transfer. Both quantitative and qualitative
results indicate the proposed 2s-ATN can effectively handle large spatial
deformation and occlusion while retaining the appearance details. It
outperforms prior states of the art on two widely used benchmarks.
</p>
<a href="http://arxiv.org/abs/2011.04181" target="_blank">arXiv:2011.04181</a> [<a href="http://arxiv.org/pdf/2011.04181" target="_blank">pdf</a>]

<h2>Improving Classifier Confidence using Lossy Label-Invariant Transformations. (arXiv:2011.04182v1 [cs.LG])</h2>
<h3>Sooyong Jang, Insup Lee, James Weimer</h3>
<p>Providing reliable model uncertainty estimates is imperative to enabling
robust decision making by autonomous agents and humans alike. While recently
there have been significant advances in confidence calibration for trained
models, examples with poor calibration persist in most calibrated models.
Consequently, multiple techniques have been proposed that leverage
label-invariant transformations of the input (i.e., an input manifold) to
improve worst-case confidence calibration. However, manifold-based confidence
calibration techniques generally do not scale and/or require expensive
retraining when applied to models with large input spaces (e.g., ImageNet). In
this paper, we present the recursive lossy label-invariant calibration (ReCal)
technique that leverages label-invariant transformations of the input that
induce a loss of discriminatory information to recursively group (and
calibrate) inputs - without requiring model retraining. We show that ReCal
outperforms other calibration methods on multiple datasets, especially, on
large-scale datasets such as ImageNet.
</p>
<a href="http://arxiv.org/abs/2011.04182" target="_blank">arXiv:2011.04182</a> [<a href="http://arxiv.org/pdf/2011.04182" target="_blank">pdf</a>]

<h2>EGO-Swarm: A Fully Autonomous and Decentralized Quadrotor Swarm System in Cluttered Environments. (arXiv:2011.04183v1 [cs.RO])</h2>
<h3>Xin Zhou, Jiangchao Zhu, Hongyu Zhou, Chao Xu, Fei Gao</h3>
<p>This paper presents a decentralized and asynchronous systematic solution for
multi-robot autonomous navigation in unknown obstacle-rich scenes using merely
onboard resources. The planning system is formulated under gradient-based local
planning framework, where collision avoidance is achieved by formulating the
collision risk as a penalty of a nonlinear optimization problem. In order to
improve robustness and escape local minima, we incorporate a lightweight
topological trajectory generation method. Then agents generate safe, smooth,
and dynamically feasible trajectories in only several milliseconds using an
unreliable trajectory sharing network. Relative localization drift among agents
is corrected by using agent detection in depth images. Our method is
demonstrated in both simulation and real-world experiments. The source code is
released for the reference of the community.
</p>
<a href="http://arxiv.org/abs/2011.04183" target="_blank">arXiv:2011.04183</a> [<a href="http://arxiv.org/pdf/2011.04183" target="_blank">pdf</a>]

<h2>Numerical Exploration of Training Loss Level-Sets in Deep Neural Networks. (arXiv:2011.04189v1 [cs.LG])</h2>
<h3>Naveed Tahir, Garrett E. Katz</h3>
<p>We present a computational method for empirically characterizing the training
loss level-sets of deep neural networks. Our method numerically constructs a
path in parameter space that is constrained to a set with a fixed near-zero
training loss. By measuring regularization functions and test loss at different
points within this path, we examine how different points in the parameter space
with the same fixed training loss compare in terms of generalization ability.
We also compare this method for finding regularized points with the more
typical method, that uses objective functions which are weighted sums of
training loss and regularization terms. We apply dimensionality reduction to
the traversed paths in order to visualize the loss level sets in a
well-regularized region of parameter space. Our results provide new information
about the loss landscape of deep neural networks, as well as a new strategy for
reducing test loss.
</p>
<a href="http://arxiv.org/abs/2011.04189" target="_blank">arXiv:2011.04189</a> [<a href="http://arxiv.org/pdf/2011.04189" target="_blank">pdf</a>]

<h2>Micron-level Optimal Obstacle-avoidance Trajectory Planning for a Free-floating Space Robot with Predefined-time Convergence. (arXiv:2011.04193v1 [cs.RO])</h2>
<h3>Wen Yan, Yicheng Liu</h3>
<p>With the development of human space exploration, the space environment is
gradually filled with abandoned satellite debris and unknown micrometeorites,
which will seriously affect capture motion of space robot. Hence, a novel fast
collision-avoidance trajectory planning strategy for a dual-arm free-floating
space robot (FFSR) with predefined-time pose feedback will be mainly studied to
achieve micron-level tracking accuracy of end-effector in this paper. However,
similar to control, the exponential feedback results in larger initial joint
angular velocity relative to proportional feedback. Therefore, a GA-based
optimization algorithm is used to reduce the control input, which is just the
joint angular velocity. Firstly, a pose-error-based kinematic model of the FFSR
will be derived from a control perspective. Then, a cumulative dangerous field
(CDF) collision-avoidance algorithm is applied in predefined-time trajectory
planning to achieve micron-level collision-avoidance trajectory tracking
precision. In the end, a GA-based optimization algorithm is used to optimize
the predefined-time parameter to obtain a motion trajectory of low joint
angular velocity of robotic arms. The simulation results verify our conjecture
and conclusion.
</p>
<a href="http://arxiv.org/abs/2011.04193" target="_blank">arXiv:2011.04193</a> [<a href="http://arxiv.org/pdf/2011.04193" target="_blank">pdf</a>]

<h2>LADA: Look-Ahead Data Acquisition via Augmentation for Active Learning. (arXiv:2011.04194v1 [cs.LG])</h2>
<h3>Yoon-Yeong Kim, Kyungwoo Song, JoonHo Jang, Il-Chul Moon</h3>
<p>Active learning effectively collects data instances for training deep
learning models when the labeled dataset is limited and the annotation cost is
high. Besides active learning, data augmentation is also an effective technique
to enlarge the limited amount of labeled instances. However, the potential gain
from virtual instances generated by data augmentation has not been considered
in the acquisition process of active learning yet. Looking ahead the effect of
data augmentation in the process of acquisition would select and generate the
data instances that are informative for training the model. Hence, this paper
proposes Look-Ahead Data Acquisition via augmentation, or LADA, to integrate
data acquisition and data augmentation. LADA considers both 1) unlabeled data
instance to be selected and 2) virtual data instance to be generated by data
augmentation, in advance of the acquisition process. Moreover, to enhance the
informativeness of the virtual data instances, LADA optimizes the data
augmentation policy to maximize the predictive acquisition score, resulting in
the proposal of InfoMixup and InfoSTN. As LADA is a generalizable framework, we
experiment with the various combinations of acquisition and augmentation
methods. The performance of LADA shows a significant improvement over the
recent augmentation and acquisition baselines which were independently applied
to the benchmark datasets.
</p>
<a href="http://arxiv.org/abs/2011.04194" target="_blank">arXiv:2011.04194</a> [<a href="http://arxiv.org/pdf/2011.04194" target="_blank">pdf</a>]

<h2>Detecting Outliers with Foreign Patch Interpolation. (arXiv:2011.04197v1 [cs.CV])</h2>
<h3>Jeremy Tan, Benjamin Hou, James Batten, Huaqi Qiu, Bernhard Kainz</h3>
<p>In medical imaging, outliers can contain hypo/hyper-intensities, minor
deformations, or completely altered anatomy. To detect these irregularities it
is helpful to learn the features present in both normal and abnormal images.
However this is difficult because of the wide range of possible abnormalities
and also the number of ways that normal anatomy can vary naturally. As such, we
leverage the natural variations in normal anatomy to create a range of
synthetic abnormalities. Specifically, the same patch region is extracted from
two independent samples and replaced with an interpolation between both
patches. The interpolation factor, patch size, and patch location are randomly
sampled from uniform distributions. A wide residual encoder decoder is trained
to give a pixel-wise prediction of the patch and its interpolation factor. This
encourages the network to learn what features to expect normally and to
identify where foreign patterns have been introduced. The estimate of the
interpolation factor lends itself nicely to the derivation of an outlier score.
Meanwhile the pixel-wise output allows for pixel- and subject- level
predictions using the same model.
</p>
<a href="http://arxiv.org/abs/2011.04197" target="_blank">arXiv:2011.04197</a> [<a href="http://arxiv.org/pdf/2011.04197" target="_blank">pdf</a>]

<h2>An improved helmet detection method for YOLOv3 on an unbalanced dataset. (arXiv:2011.04214v1 [cs.CV])</h2>
<h3>Rui Geng, Yixuan Ma, Wanhong Huang</h3>
<p>The YOLOv3 target detection algorithm is widely used in industry due to its
high speed and high accuracy, but it has some limitations, such as the accuracy
degradation of unbalanced datasets. The YOLOv3 target detection algorithm is
based on a Gaussian fuzzy data augmentation approach to pre-process the data
set and improve the YOLOv3 target detection algorithm. Through the efficient
pre-processing, the confidence level of YOLOv3 is generally improved by
0.01-0.02 without changing the recognition speed of YOLOv3, and the processed
images also perform better in image localization due to effective feature
fusion, which is more in line with the requirement of recognition speed and
accuracy in production.
</p>
<a href="http://arxiv.org/abs/2011.04214" target="_blank">arXiv:2011.04214</a> [<a href="http://arxiv.org/pdf/2011.04214" target="_blank">pdf</a>]

<h2>Positive-Unlabeled Classification under Class Prior Shift and Asymmetric Error. (arXiv:1809.07011v4 [stat.ML] UPDATED)</h2>
<h3>Nontawat Charoenphakdee, Masashi Sugiyama</h3>
<p>Bottlenecks of binary classification from positive and unlabeled data (PU
classification) are the requirements that given unlabeled patterns are drawn
from the test marginal distribution, and the penalty of the false positive
error is identical to the false negative error. However, such requirements are
often not fulfilled in practice. In this paper, we generalize PU classification
to the class prior shift and asymmetric error scenarios. Based on the analysis
of the Bayes optimal classifier, we show that given a test class prior, PU
classification under class prior shift is equivalent to PU classification with
asymmetric error. Then, we propose two different frameworks to handle these
problems, namely, a risk minimization framework and density ratio estimation
framework. Finally, we demonstrate the effectiveness of the proposed frameworks
and compare both frameworks through experiments using benchmark datasets.
</p>
<a href="http://arxiv.org/abs/1809.07011" target="_blank">arXiv:1809.07011</a> [<a href="http://arxiv.org/pdf/1809.07011" target="_blank">pdf</a>]

<h2>Cloud Chaser: Real Time Deep Learning Computer Vision on Low Computing Power Devices. (arXiv:1810.01069v2 [cs.CV] UPDATED)</h2>
<h3>Zhengyi Luo, Austin Small, Liam Dugan, Stephen Lane</h3>
<p>Internet of Things(IoT) devices, mobile phones, and robotic systems are often
denied the power of deep learning algorithms due to their limited computing
power. However, to provide time-critical services such as emergency response,
home assistance, surveillance, etc, these devices often need real-time analysis
of their camera data. This paper strives to offer a viable approach to
integrate high-performance deep learning-based computer vision algorithms with
low-resource and low-power devices by leveraging the computing power of the
cloud. By offloading the computation work to the cloud, no dedicated hardware
is needed to enable deep neural networks on existing low computing power
devices. A Raspberry Pi based robot, Cloud Chaser, is built to demonstrate the
power of using cloud computing to perform real-time vision tasks. Furthermore,
to reduce latency and improve real-time performance, compression algorithms are
proposed and evaluated for streaming real-time video frames to the cloud.
</p>
<a href="http://arxiv.org/abs/1810.01069" target="_blank">arXiv:1810.01069</a> [<a href="http://arxiv.org/pdf/1810.01069" target="_blank">pdf</a>]

<h2>Precise Performance Analysis of the Box-Elastic Net under Matrix Uncertainties. (arXiv:1901.04469v4 [cs.LG] UPDATED)</h2>
<h3>Ayed M.Alrashdi, Ismail Ben Atitallah, Tareq Y. Al-Naffouri</h3>
<p>In this letter, we consider the problem of recovering an unknown sparse
signal from noisy linear measurements, using an enhanced version of the popular
Elastic-Net (EN) method. We modify the EN by adding a box-constraint, and we
call it the Box-Elastic Net (Box-EN). We assume independent identically
distributed (iid) real Gaussian measurement matrix with additive Gaussian
noise. In many practical situations, the measurement matrix is not perfectly
known, and so we only have a noisy estimate of it. In this work, we precisely
characterize the mean squared error and the probability of support recovery of
the Box-Elastic Net in the high-dimensional asymptotic regime. Numerical
simulations validate the theoretical predictions derived in the paper and also
show that the boxed variant outperforms the standard EN.
</p>
<a href="http://arxiv.org/abs/1901.04469" target="_blank">arXiv:1901.04469</a> [<a href="http://arxiv.org/pdf/1901.04469" target="_blank">pdf</a>]

<h2>Regularizing Black-box Models for Improved Interpretability. (arXiv:1902.06787v6 [cs.LG] UPDATED)</h2>
<h3>Gregory Plumb, Maruan Al-Shedivat, Angel Alexander Cabrera, Adam Perer, Eric Xing, Ameet Talwalkar</h3>
<p>Most of the work on interpretable machine learning has focused on designing
either inherently interpretable models, which typically trade-off accuracy for
interpretability, or post-hoc explanation systems, whose explanation quality
can be unpredictable. Our method, ExpO, is a hybridization of these approaches
that regularizes a model for explanation quality at training time. Importantly,
these regularizers are differentiable, model agnostic, and require no domain
knowledge to define. We demonstrate that post-hoc explanations for
ExpO-regularized models have better explanation quality, as measured by the
common fidelity and stability metrics. We verify that improving these metrics
leads to significantly more useful explanations with a user study on a
realistic task.
</p>
<a href="http://arxiv.org/abs/1902.06787" target="_blank">arXiv:1902.06787</a> [<a href="http://arxiv.org/pdf/1902.06787" target="_blank">pdf</a>]

<h2>Accelerating Self-Play Learning in Go. (arXiv:1902.10565v5 [cs.LG] UPDATED)</h2>
<h3>David J. Wu</h3>
<p>By introducing several improvements to the AlphaZero process and
architecture, we greatly accelerate self-play learning in Go, achieving a 50x
reduction in computation over comparable methods. Like AlphaZero and
replications such as ELF OpenGo and Leela Zero, our bot KataGo only learns from
neural-net-guided Monte Carlo tree search self-play. But whereas AlphaZero
required thousands of TPUs over several days and ELF required thousands of GPUs
over two weeks, KataGo surpasses ELF's final model after only 19 days on fewer
than 30 GPUs. Much of the speedup involves non-domain-specific improvements
that might directly transfer to other problems. Further gains from
domain-specific techniques reveal the remaining efficiency gap between the best
methods and purely general methods such as AlphaZero. Our work is a step
towards making learning in state spaces as large as Go possible without
large-scale computational resources.
</p>
<a href="http://arxiv.org/abs/1902.10565" target="_blank">arXiv:1902.10565</a> [<a href="http://arxiv.org/pdf/1902.10565" target="_blank">pdf</a>]

<h2>RefineLoc: Iterative Refinement for Weakly-Supervised Action Localization. (arXiv:1904.00227v3 [cs.CV] UPDATED)</h2>
<h3>Alejandro Pardo, Humam Alwassel, Fabian Caba Heilbron, Ali Thabet, Bernard Ghanem</h3>
<p>Video action detectors are usually trained using datasets with
fully-supervised temporal annotations. Building such datasets is an expensive
task. To alleviate this problem, recent methods have tried to leverage weak
labeling, where videos are untrimmed and only a video-level label is available.
In this paper, we propose RefineLoc, a novel weakly-supervised temporal action
localization method. RefineLoc uses an iterative refinement approach by
estimating and training on snippet-level pseudo ground truth at every
iteration. We show the benefit of this iterative approach and present an
extensive analysis of five different pseudo ground truth generators. We show
the effectiveness of our model on two standard action datasets, ActivityNet
v1.2 and THUMOS14. RefineLoc shows competitive results with the
state-of-the-art in weakly-supervised temporal localization. Additionally, our
iterative refinement process is able to significantly improve the performance
of two state-of-the-art methods, setting a new state-of-the-art on THUMOS14.
</p>
<a href="http://arxiv.org/abs/1904.00227" target="_blank">arXiv:1904.00227</a> [<a href="http://arxiv.org/pdf/1904.00227" target="_blank">pdf</a>]

<h2>Semisupervised Clustering by Queries and Locally Encodable Source Coding. (arXiv:1904.00507v2 [stat.ML] UPDATED)</h2>
<h3>Arya Mazumdar, Soumyabrata Pal</h3>
<p>Source coding is the canonical problem of data compression in information
theory. In a locally encodable source coding, each compressed bit depends on
only few bits of the input. In this paper, we show that a recently popular
model of semi-supervised clustering is equivalent to locally encodable source
coding. In this model, the task is to perform multiclass labeling of unlabeled
elements. At the beginning, we can ask in parallel a set of simple queries to
an oracle who provides (possibly erroneous) binary answers to the queries. The
queries cannot involve more than two (or a fixed constant number of) elements.
Now the labeling of all the elements (or clustering) must be performed based on
the noisy query answers. The goal is to recover all the correct labelings while
minimizing the number of such queries. The equivalence to locally encodable
source codes leads us to find lower bounds on the number of queries required in
a variety of scenarios. We provide querying schemes based on pairwise `same
cluster' queries - and pairwise AND queries and show provable performance
guarantees for each of the schemes.
</p>
<a href="http://arxiv.org/abs/1904.00507" target="_blank">arXiv:1904.00507</a> [<a href="http://arxiv.org/pdf/1904.00507" target="_blank">pdf</a>]

<h2>Statistical feature embedding for heart sound classification. (arXiv:1904.11914v3 [cs.LG] UPDATED)</h2>
<h3>Mohammad Adiban, Bagher BabaAli, Saeedreza Shehnepoor</h3>
<p>Cardiovascular Disease (CVD) is considered as one of the principal causes of
death in the world. Over recent years, this field of study has attracted
researchers' attention to investigate heart sounds' patterns for disease
diagnostics. In this study, an approach is proposed for normal/abnormal heart
sound classification on the Physionet challenge 2016 dataset. For the first
time, a fixed-length feature vector; called i-vector; is extracted from each
heart sound using Mel Frequency Cepstral Coefficient (MFCC) features.
Afterwards, Principal Component Analysis (PCA) transform and Variational
Autoencoder (VAE) are applied on the i-vector to achieve dimension reduction.
Eventually, the reduced size vector is fed to Gaussian Mixture Models (GMMs)
and Support Vector Machine (SVM) for classification purpose. Experimental
results demonstrate the proposed method could achieve a performance improvement
of 16% based on Modified Accuracy (MAcc) compared with the baseline system on
the Physoinet dataset.
</p>
<a href="http://arxiv.org/abs/1904.11914" target="_blank">arXiv:1904.11914</a> [<a href="http://arxiv.org/pdf/1904.11914" target="_blank">pdf</a>]

<h2>Dynamic Routing Networks. (arXiv:1905.04849v5 [cs.LG] UPDATED)</h2>
<h3>Shaofeng Cai, Yao Shu, Wei Wang, Beng Chin Ooi</h3>
<p>The deployment of deep neural networks in real-world applications is mostly
restricted by their high inference costs. Extensive efforts have been made to
improve the accuracy with expert-designed or algorithm-searched architectures.
However, the incremental improvement is typically achieved with increasingly
more expensive models that only a small portion of input instances really need.
Inference with a static architecture that processes all input instances via the
same transformation would thus incur unnecessary computational costs.
Therefore, customizing the model capacity in an instance-aware manner is much
needed for higher inference efficiency. In this paper, we propose Dynamic
Routing Networks (DRNets), which support efficient instance-aware inference by
routing the input instance to only necessary transformation branches selected
from a candidate set of branches for each connection between transformation
nodes. The branch selection is dynamically determined via the corresponding
branch importance weights, which are first generated from lightweight
hypernetworks (RouterNets) and then recalibrated with Gumbel-Softmax before the
selection. Extensive experiments show that DRNets can reduce a substantial
amount of parameter size and FLOPs during inference with prediction performance
comparable to state-of-the-art architectures.
</p>
<a href="http://arxiv.org/abs/1905.04849" target="_blank">arXiv:1905.04849</a> [<a href="http://arxiv.org/pdf/1905.04849" target="_blank">pdf</a>]

<h2>Towards a Quantum-Like Cognitive Architecture for Decision-Making. (arXiv:1905.05176v2 [cs.AI] UPDATED)</h2>
<h3>Catarina Moreira, Lauren Fell, Shahram Dehdashti, Peter Bruza, Andreas Wichert</h3>
<p>We propose an alternative and unifying framework for decision-making that, by
using quantum mechanics, provides more generalised cognitive and decision
models with the ability to represent more information than classical models.
This framework can accommodate and predict several cognitive biases reported in
Lieder &amp; Griffiths without heavy reliance on heuristics nor on assumptions of
the computational resources of the mind.
</p>
<a href="http://arxiv.org/abs/1905.05176" target="_blank">arXiv:1905.05176</a> [<a href="http://arxiv.org/pdf/1905.05176" target="_blank">pdf</a>]

<h2>The Variational InfoMax AutoEncoder. (arXiv:1905.10549v2 [cs.LG] UPDATED)</h2>
<h3>Vincenzo Crescimanna, Bruce Graham</h3>
<p>The Variational AutoEncoder (VAE) learns simultaneously an inference and a
generative model, but only one of these models can be learned at optimum, this
behaviour is associated to the ELBO learning objective, that is optimised by a
non-informative generator. In order to solve such an issue, we provide a
learning objective, learning a maximal informative generator while maintaining
bounded the network capacity: the Variational InfoMax (VIM). The contribution
of the VIM derivation is twofold: an objective learning both an optimal
inference and generative model and the explicit definition of the network
capacity, an estimation of the network robustness.
</p>
<a href="http://arxiv.org/abs/1905.10549" target="_blank">arXiv:1905.10549</a> [<a href="http://arxiv.org/pdf/1905.10549" target="_blank">pdf</a>]

<h2>Training Data Subset Search with Ensemble Active Learning. (arXiv:1905.12737v3 [cs.LG] UPDATED)</h2>
<h3>Kashyap Chitta, Jose M. Alvarez, Elmar Haussmann, Clement Farabet</h3>
<p>Deep Neural Networks (DNNs) often rely on very large datasets for training.
Given the large size of such datasets, it is conceivable that they contain
certain samples that either do not contribute or negatively impact the DNN's
optimization. Modifying the training distribution in a way that excludes such
samples could provide an effective solution to both improve performance and
reduce training time. In this paper, we propose to scale up ensemble Active
Learning (AL) methods to perform acquisition at a large scale (10k to 500k
samples at a time). We do this with ensembles of hundreds of models, obtained
at a minimal computational cost by reusing intermediate training checkpoints.
This allows us to automatically and efficiently perform a training data subset
search for large labeled datasets. We observe that our approach obtains
favorable subsets of training data, which can be used to train more accurate
DNNs than training with the entire dataset. We perform an extensive
experimental study of this phenomenon on three image classification benchmarks
(CIFAR-10, CIFAR-100 and ImageNet), as well as an internal object detection
benchmark for prototyping perception models for autonomous driving. Unlike
existing studies, our experiments on object detection are at the scale required
for production-ready autonomous driving systems. We provide insights on the
impact of different initialization schemes, acquisition functions and ensemble
configurations at this scale. Our results provide strong empirical evidence
that optimizing the training data distribution can provide significant benefits
on large scale vision tasks.
</p>
<a href="http://arxiv.org/abs/1905.12737" target="_blank">arXiv:1905.12737</a> [<a href="http://arxiv.org/pdf/1905.12737" target="_blank">pdf</a>]

<h2>A Review of Robot Learning for Manipulation: Challenges, Representations, and Algorithms. (arXiv:1907.03146v3 [cs.RO] UPDATED)</h2>
<h3>Oliver Kroemer, Scott Niekum, George Konidaris</h3>
<p>A key challenge in intelligent robotics is creating robots that are capable
of directly interacting with the world around them to achieve their goals. The
last decade has seen substantial growth in research on the problem of robot
manipulation, which aims to exploit the increasing availability of affordable
robot arms and grippers to create robots capable of directly interacting with
the world to achieve their goals. Learning will be central to such autonomous
systems, as the real world contains too much variation for a robot to expect to
have an accurate model of its environment, the objects in it, or the skills
required to manipulate them, in advance. We aim to survey a representative
subset of that research which uses machine learning for manipulation. We
describe a formalization of the robot manipulation learning problem that
synthesizes existing research into a single coherent framework and highlight
the many remaining research opportunities and challenges.
</p>
<a href="http://arxiv.org/abs/1907.03146" target="_blank">arXiv:1907.03146</a> [<a href="http://arxiv.org/pdf/1907.03146" target="_blank">pdf</a>]

<h2>And the Bit Goes Down: Revisiting the Quantization of Neural Networks. (arXiv:1907.05686v5 [cs.CV] UPDATED)</h2>
<h3>Pierre Stock, Armand Joulin, R&#xe9;mi Gribonval, Benjamin Graham, Herv&#xe9; J&#xe9;gou</h3>
<p>In this paper, we address the problem of reducing the memory footprint of
convolutional network architectures. We introduce a vector quantization method
that aims at preserving the quality of the reconstruction of the network
outputs rather than its weights. The principle of our approach is that it
minimizes the loss reconstruction error for in-domain inputs. Our method only
requires a set of unlabelled data at quantization time and allows for efficient
inference on CPU by using byte-aligned codebooks to store the compressed
weights. We validate our approach by quantizing a high performing ResNet-50
model to a memory size of 5MB (20x compression factor) while preserving a top-1
accuracy of 76.1% on ImageNet object classification and by compressing a Mask
R-CNN with a 26x factor.
</p>
<a href="http://arxiv.org/abs/1907.05686" target="_blank">arXiv:1907.05686</a> [<a href="http://arxiv.org/pdf/1907.05686" target="_blank">pdf</a>]

<h2>A Group-Theoretic Framework for Data Augmentation. (arXiv:1907.10905v4 [stat.ML] UPDATED)</h2>
<h3>Shuxiao Chen, Edgar Dobriban, Jane H Lee</h3>
<p>Data augmentation is a widely used trick when training deep neural networks:
in addition to the original data, properly transformed data are also added to
the training set. However, to the best of our knowledge, a clear mathematical
framework to explain the performance benefits of data augmentation is not
available. In this paper, we develop such a theoretical framework. We show data
augmentation is equivalent to an averaging operation over the orbits of a
certain group that keeps the data distribution approximately invariant. We
prove that it leads to variance reduction. We study empirical risk
minimization, and the examples of exponential families, linear regression, and
certain two-layer neural networks. We also discuss how data augmentation could
be used in problems with symmetry where other approaches are prevalent, such as
in cryo-electron microscopy (cryo-EM).
</p>
<a href="http://arxiv.org/abs/1907.10905" target="_blank">arXiv:1907.10905</a> [<a href="http://arxiv.org/pdf/1907.10905" target="_blank">pdf</a>]

<h2>Specular- and Diffuse-reflection-based Face Spoofing Detection for Mobile Devices. (arXiv:1907.12400v4 [cs.CV] UPDATED)</h2>
<h3>Akinori F. Ebihara, Kazuyuki Sakurai, Hitoshi Imaoka</h3>
<p>In light of the rising demand for biometric-authentication systems,
preventing face spoofing attacks is a critical issue for the safe deployment of
face recognition systems. Here, we propose an efficient face presentation
attack detection (PAD) algorithm that requires minimal hardware and only a
small database, making it suitable for resource-constrained devices such as
mobile phones. Utilizing one monocular visible light camera, the proposed
algorithm takes two facial photos, one taken with a flash, the other without a
flash. The proposed $SpecDiff$ descriptor is constructed by leveraging two
types of reflection: (i) specular reflections from the iris region that have a
specific intensity distribution depending on liveness, and (ii) diffuse
reflections from the entire face region that represents the 3D structure of a
subject's face. Classifiers trained with $SpecDiff$ descriptor outperforms
other flash-based PAD algorithms on both an in-house database and on publicly
available NUAA, Replay-Attack, and SiW databases. Moreover, the proposed
algorithm achieves statistically significantly better accuracy to that of an
end-to-end, deep neural network classifier, while being approximately six-times
faster execution speed. The code is publicly available at
https://github.com/Akinori-F-Ebihara/SpecDiff-spoofing-detector.
</p>
<a href="http://arxiv.org/abs/1907.12400" target="_blank">arXiv:1907.12400</a> [<a href="http://arxiv.org/pdf/1907.12400" target="_blank">pdf</a>]

<h2>Promoting Coordination through Policy Regularization in Multi-Agent Deep Reinforcement Learning. (arXiv:1908.02269v4 [cs.LG] UPDATED)</h2>
<h3>Julien Roy, Paul Barde, F&#xe9;lix G. Harvey, Derek Nowrouzezahrai, Christopher Pal</h3>
<p>In multi-agent reinforcement learning, discovering successful collective
behaviors is challenging as it requires exploring a joint action space that
grows exponentially with the number of agents. While the tractability of
independent agent-wise exploration is appealing, this approach fails on tasks
that require elaborate group strategies. We argue that coordinating the agents'
policies can guide their exploration and we investigate techniques to promote
such an inductive bias. We propose two policy regularization methods: TeamReg,
which is based on inter-agent action predictability and CoachReg that relies on
synchronized behavior selection. We evaluate each approach on four challenging
continuous control tasks with sparse rewards that require varying levels of
coordination as well as on the discrete action Google Research Football
environment. Our experiments show improved performance across many cooperative
multi-agent problems. Finally, we analyze the effects of our proposed methods
on the policies that our agents learn and show that our methods successfully
enforce the qualities that we propose as proxies for coordinated behaviors.
</p>
<a href="http://arxiv.org/abs/1908.02269" target="_blank">arXiv:1908.02269</a> [<a href="http://arxiv.org/pdf/1908.02269" target="_blank">pdf</a>]

<h2>A Method to Learn Embedding of a Probabilistic Medical Knowledge Graph: Algorithm Development. (arXiv:1909.00672v2 [cs.AI] UPDATED)</h2>
<h3>Linfeng Li, Peng Wang, Yao Wang, Jinpeng Jiang, Buzhou Tang, Jun Yan, Shenghui Wang, Yuting Liu</h3>
<p>This paper proposes an algorithm named as PrTransH to learn embedding vectors
from real world EMR data based medical knowledge. The unique challenge in
embedding medical knowledge graph from real world EMR data is that the
uncertainty of knowledge triplets blurs the border between "correct triplet"
and "wrong triplet", changing the fundamental assumption of many existing
algorithms. To address the challenge, some enhancements are made to existing
TransH algorithm, including: 1) involve probability of medical knowledge
triplet into training objective; 2) replace the margin-based ranking loss with
unified loss calculation considering both valid and corrupted triplets; 3)
augment training data set with medical background knowledge. Verifications on
real world EMR data based medical knowledge graph prove that PrTransH
outperforms TransH in link prediction task. To the best of our survey, this
paper is the first one to learn and verify knowledge embedding on probabilistic
knowledge graphs.
</p>
<a href="http://arxiv.org/abs/1909.00672" target="_blank">arXiv:1909.00672</a> [<a href="http://arxiv.org/pdf/1909.00672" target="_blank">pdf</a>]

<h2>TORM: Fast and Accurate Trajectory Optimization of Redundant Manipulator given an End-Effector Path. (arXiv:1909.12517v2 [cs.RO] UPDATED)</h2>
<h3>Mincheul Kang, Heechan Shin, Donghyuk Kim, Sung-Eui Yoon</h3>
<p>A redundant manipulator has multiple inverse kinematics solutions per
end-effector pose. Accordingly, there can be many trajectories for joints that
follow a given endeffector path in the Cartesian space. In this paper, we
present a trajectory optimization of a redundant manipulator (TORM) to
synthesize a trajectory that follows a given end-effector path accurately,
while achieving smoothness and collisionfree manipulation. Our method
holistically incorporates three desired properties into the trajectory
optimization process by integrating the Jacobian-based inverse kinematics
solving method and an optimization-based motion planning approach.
Specifically, we optimize a trajectory using two-stage gradient descent to
reduce potential competition between different properties during the update. To
avoid falling into local minima, we iteratively explore different candidate
trajectories with our local update. We compare our method with state-of-the-art
methods in test scenes including external obstacles and two non-obstacle
problems. Our method robustly minimizes the pose error in a progressive manner
while satisfying various desirable properties.
</p>
<a href="http://arxiv.org/abs/1909.12517" target="_blank">arXiv:1909.12517</a> [<a href="http://arxiv.org/pdf/1909.12517" target="_blank">pdf</a>]

<h2>Meta Module Network for Compositional Visual Reasoning. (arXiv:1910.03230v5 [cs.CV] UPDATED)</h2>
<h3>Wenhu Chen, Zhe Gan, Linjie Li, Yu Cheng, William Wang, Jingjing Liu</h3>
<p>Neural Module Network (NMN) exhibits strong interpretability and
compositionality thanks to its handcrafted neural modules with explicit
multi-hop reasoning capability. However, most NMNs suffer from two critical
drawbacks: 1) scalability: customized module for specific function renders it
impractical when scaling up to a larger set of functions in complex tasks; 2)
generalizability: rigid pre-defined module inventory makes it difficult to
generalize to unseen functions in new tasks/domains. To design a more powerful
NMN architecture for practical use, we propose Meta Module Network (MMN)
centered on a novel meta module, which can take in function recipes and morph
into diverse instance modules dynamically. The instance modules are then woven
into an execution graph for complex visual reasoning, inheriting the strong
explainability and compositionality of NMN. With such a flexible instantiation
mechanism, the parameters of instance modules are inherited from the central
meta module, retaining the same model complexity as the function set grows,
which promises better scalability. Meanwhile, as functions are encoded into the
embedding space, unseen functions can be readily represented based on its
structural similarity with previously observed ones, which ensures better
generalizability. Experiments on GQA and CLEVR datasets validate the
superiority of MMN over state-of-the-art NMN designs. Synthetic experiments on
held-out unseen functions from GQA dataset also demonstrate the strong
generalizability of MMN. Our code and model are released in Github
https://github.com/wenhuchen/Meta-Module-Network.
</p>
<a href="http://arxiv.org/abs/1910.03230" target="_blank">arXiv:1910.03230</a> [<a href="http://arxiv.org/pdf/1910.03230" target="_blank">pdf</a>]

<h2>SurReal: Complex-Valued Learning as Principled Transformations on a Scaling and Rotation Manifold. (arXiv:1910.11334v3 [cs.CV] UPDATED)</h2>
<h3>Rudrasis Chakraborty, Yifei Xing, Stella Yu</h3>
<p>Complex-valued data is ubiquitous in signal and image processing
applications, and complex-valued representations in deep learning have
appealing theoretical properties. While these aspects have long been
recognized, complex-valued deep learning continues to lag far behind its
real-valued counterpart.

We propose a principled geometric approach to complex-valued deep learning.
Complex-valued data could often be subject to arbitrary complex-valued scaling;
as a result, real and imaginary components could co-vary. Instead of treating
complex values as two independent channels of real values, we recognize their
underlying geometry: We model the space of complex numbers as a product
manifold of non-zero scaling and planar rotations. Arbitrary complex-valued
scaling naturally becomes a group of transitive actions on this manifold.

We propose to extend the property instead of the form of real-valued
functions to the complex domain. We define convolution as weighted Fr\'echet
mean on the manifold that is equivariant to the group of scaling/rotation
actions, and define distance transform on the manifold that is invariant to the
action group. The manifold perspective also allows us to define nonlinear
activation functions such as tangent ReLU and G-transport, as well as residual
connections on the manifold-valued data.

We dub our model SurReal, as our experiments on MSTAR and RadioML deliver
high performance with only a fractional size of real-valued and complex-valued
baseline models.
</p>
<a href="http://arxiv.org/abs/1910.11334" target="_blank">arXiv:1910.11334</a> [<a href="http://arxiv.org/pdf/1910.11334" target="_blank">pdf</a>]

<h2>Method to Characterize Potential UAS Encounters Using Open Source Data. (arXiv:1911.00110v3 [cs.RO] UPDATED)</h2>
<h3>Andrew Weinert</h3>
<p>As unmanned aerial systems (UASs) increasingly integrate into the US national
airspace system, there is an increasing need to characterize how commercial and
recreational UASs may encounter each other. To inform the development and
evaluation of safety critical technologies, we demonstrate a methodology to
analytically calculate all potential relative geometries between different UAS
operations performing inspection missions. This method is based on a previously
demonstrated technique that leverages open source geospatial information to
generate representative unmanned aircraft trajectories. Using open source data
and parallel processing techniques, we performed trillions of calculations to
estimate the relative horizontal distance between geospatial points across
sixteen locations.
</p>
<a href="http://arxiv.org/abs/1911.00110" target="_blank">arXiv:1911.00110</a> [<a href="http://arxiv.org/pdf/1911.00110" target="_blank">pdf</a>]

<h2>Online matrix factorization for Markovian data and applications to Network Dictionary Learning. (arXiv:1911.01931v6 [cs.LG] UPDATED)</h2>
<h3>Hanbaek Lyu, Deanna Needell, Laura Balzano</h3>
<p>Online Matrix Factorization (OMF) is a fundamental tool for dictionary
learning problems, giving an approximate representation of complex data sets in
terms of a reduced number of extracted features. Convergence guarantees for
most of the OMF algorithms in the literature assume independence between data
matrices, and the case of dependent data streams remains largely unexplored. In
this paper, we show that a non-convex generalization of the well-known OMF
algorithm for i.i.d. stream of data in \citep{mairal2010online} converges
almost surely to the set of critical points of the expected loss function, even
when the data matrices are functions of some underlying Markov chain satisfying
a mild mixing condition. This allows one to extract features more efficiently
from dependent data streams, as there is no need to subsample the data sequence
to approximately satisfy the independence assumption. As the main application,
by combining online non-negative matrix factorization and a recent MCMC
algorithm for sampling motifs from networks, we propose a novel framework of
Network Dictionary Learning, which extracts ``network dictionary patches' from
a given network in an online manner that encodes main features of the network.
We demonstrate this technique and its application to network denoising problems
on real-world network data.
</p>
<a href="http://arxiv.org/abs/1911.01931" target="_blank">arXiv:1911.01931</a> [<a href="http://arxiv.org/pdf/1911.01931" target="_blank">pdf</a>]

<h2>Incentive-aware Contextual Pricing with Non-parametric Market Noise. (arXiv:1911.03508v2 [cs.LG] UPDATED)</h2>
<h3>Negin Golrezaei, Patrick Jaillet, Jason Cheuk Nam Liang</h3>
<p>We consider a dynamic pricing problem for repeated contextual second-price
auctions with strategic buyers whose goals are to maximize their long-term time
discounted utility. The seller has very limited information about buyers'
overall demand curves, which depends on $d$-dimensional context vectors
characterizing auctioned items, and a non-parametric market noise distribution
that captures buyers' idiosyncratic tastes. The noise distribution and the
relationship between the context vectors and buyers' demand curves are both
unknown to the seller. We focus on designing the seller's learning policy to
set contextual reserve prices where the seller's goal is to minimize his regret
for revenue. We first propose a pricing policy when buyers are truthful and
show that it achieves a $T$-period regret bound of
$\tilde{\mathcal{O}}(\sqrt{dT})$ against a clairvoyant policy that has full
information of the buyers' demand. Next, under the setting where buyers bid
strategically to maximize their long-term discounted utility, we develop a
variant of our first policy that is robust to strategic (corrupted) bids. This
policy incorporates randomized "isolation" periods, during which a buyer is
randomly chosen to solely participate in the auction. We show that this design
allows the seller to control the number of periods in which buyers
significantly corrupt their bids. Because of this nice property, our robust
policy enjoys a $T$-period regret of $\tilde{\mathcal{O}}(\sqrt{dT})$, matching
that under the truthful setting up to a constant factor that depends on the
utility discount factor.
</p>
<a href="http://arxiv.org/abs/1911.03508" target="_blank">arXiv:1911.03508</a> [<a href="http://arxiv.org/pdf/1911.03508" target="_blank">pdf</a>]

<h2>XPipe: Efficient Pipeline Model Parallelism for Multi-GPU DNN Training. (arXiv:1911.04610v3 [cs.LG] UPDATED)</h2>
<h3>Lei Guan, Wotao Yin, Dongsheng Li, Xicheng Lu</h3>
<p>We propose XPipe, an efficient asynchronous pipeline model parallelism
approach for multi-GPU DNN training. XPipe is designed to use multiple GPUs to
concurrently and continuously train different parts of a DNN model. To improve
GPU utilization and achieve high throughput, it splits a mini-batch into a set
of micro-batches. It allows the overlapping of the pipelines of multiple
micro-batches, including those belonging to different mini-batches. Most
importantly, the novel weight prediction strategy adopted by XPipe enables it
to effectively address the weight inconsistency and staleness issues incurred
by the asynchronous pipeline parallelism. As a result, XPipe incorporates the
advantages of both synchronous and asynchronous pipeline model parallelism
approaches. Concretely, it can achieve very comparable (even slightly better)
model accuracy as its synchronous counterpart while obtaining higher throughput
than it. Experimental results show that XPipe outperforms other
state-of-the-art synchronous and asynchronous model parallelism approaches.
</p>
<a href="http://arxiv.org/abs/1911.04610" target="_blank">arXiv:1911.04610</a> [<a href="http://arxiv.org/pdf/1911.04610" target="_blank">pdf</a>]

<h2>Continuous Adaptation for Interactive Object Segmentation by Learning from Corrections. (arXiv:1911.12709v4 [cs.CV] UPDATED)</h2>
<h3>Theodora Kontogianni, Michael Gygli, Jasper Uijlings, Vittorio Ferrari</h3>
<p>In interactive object segmentation a user collaborates with a computer vision
model to segment an object. Recent works employ convolutional neural networks
for this task: Given an image and a set of corrections made by the user as
input, they output a segmentation mask. These approaches achieve strong
performance by training on large datasets but they keep the model parameters
unchanged at test time. Instead, we recognize that user corrections can serve
as sparse training examples and we propose a method that capitalizes on that
idea to update the model parameters on-the-fly to the data at hand. Our
approach enables the adaptation to a particular object and its background, to
distributions shifts in a test set, to specific object classes, and even to
large domain changes, where the imaging modality changes between training and
testing. We perform extensive experiments on 8 diverse datasets and show:
Compared to a model with frozen parameters, our method reduces the required
corrections (i) by 9%-30% when distribution shifts are small between training
and testing; (ii) by 12%-44% when specializing to a specific class; (iii) and
by 60% and 77% when we completely change domain between training and testing.
</p>
<a href="http://arxiv.org/abs/1911.12709" target="_blank">arXiv:1911.12709</a> [<a href="http://arxiv.org/pdf/1911.12709" target="_blank">pdf</a>]

<h2>Occlusion-Robust Online Multi-Object Visual Tracking using a GM-PHD Filter with CNN-Based Re-Identification. (arXiv:1912.05949v4 [cs.CV] UPDATED)</h2>
<h3>Nathanael L. Baisa</h3>
<p>We propose a novel online multi-object visual tracking algorithm via a
tracking-by-detection paradigm using a Gaussian mixture Probability Hypothesis
Density (GM-PHD) filter and deep Convolutional Neural Network (CNN) appearance
representations learning. The GM-PHD filter has a linear complexity with the
number of objects and observations while estimating the states and cardinality
of unknown and time-varying number of objects in the scene. Though it handles
object birth, death and clutter in a unified framework, it is susceptible to
miss-detections and does not include the identity of objects. We use
visual-spatio-temporal information obtained from object bounding boxes and
deeply learned appearance representations to perform estimates-to-tracks data
association for labeling of each target as well as formulate an augmented
likelihood and then integrate into the update step of the GM-PHD filter. We
learn the deep CNN appearance representations by training an identification
network (IdNet) on large-scale person re-identification data sets. We also
employ additional unassigned tracks prediction after the data association step
to overcome the susceptibility of the GM-PHD filter towards miss-detections
caused by occlusion. Our tracker which runs in real-time is applied to track
multiple objects in video sequences acquired under varying environmental
conditions and objects density. Lastly, we make extensive evaluations on
Multiple Object Tracking 2016 (MOT16) and 2017 (MOT17) benchmark data sets and
find out that our online tracker significantly outperforms several
state-of-the-art trackers in terms of tracking accuracy and identification.
</p>
<a href="http://arxiv.org/abs/1912.05949" target="_blank">arXiv:1912.05949</a> [<a href="http://arxiv.org/pdf/1912.05949" target="_blank">pdf</a>]

<h2>Deep Poisoning: Towards Robust Image Data Sharing against Visual Disclosure. (arXiv:1912.06895v2 [cs.CV] UPDATED)</h2>
<h3>Hao Guo, Brian Dolhansky, Eric Hsin, Phong Dinh, Cristian Canton Ferrer, Song Wang</h3>
<p>Due to respectively limited training data, different entities addressing the
same vision task based on certain sensitive images may not train a robust deep
network. This paper introduces a new vision task where various entities share
task-specific image data to enlarge each other's training data volume without
visually disclosing sensitive contents (e.g. illegal images). Then, we present
a new structure-based training regime to enable different entities learn
task-specific and reconstruction-proof image representations for image data
sharing. Specifically, each entity learns a private Deep Poisoning Module (DPM)
and insert it to a pre-trained deep network, which is designed to perform the
specific vision task. The DPM deliberately poisons convolutional image features
to prevent image reconstructions, while ensuring that the altered image data is
functionally equivalent to the non-poisoned data for the specific vision task.
Given this equivalence, the poisoned features shared from one entity could be
used by another entity for further model refinement. Experimental results on
image classification prove the efficacy of the proposed method.
</p>
<a href="http://arxiv.org/abs/1912.06895" target="_blank">arXiv:1912.06895</a> [<a href="http://arxiv.org/pdf/1912.06895" target="_blank">pdf</a>]

<h2>Identifying and Compensating for Feature Deviation in Imbalanced Deep Learning. (arXiv:2001.01385v3 [cs.LG] UPDATED)</h2>
<h3>Han-Jia Ye, Hong-You Chen, De-Chuan Zhan, Wei-Lun Chao</h3>
<p>We investigate learning a ConvNet classifier with class-imbalanced data. We
found that a ConvNet significantly over-fits the minor classes that do not have
sufficient training instances, which is quite opposite to a traditional machine
learning model like logistic regression that often under-fits minor classes. We
conduct a series of analysis and argue that feature deviation between the
training and test instances serves as the main cause. We propose to incorporate
class-dependent temperatures (CDT) in learning a ConvNet: CDT forces the
minor-class instances to have larger decision values in the training phase, so
as to compensate for the effect of feature deviation in the test data. We
validate our approach on several benchmark datasets and achieve promising
performance. We hope that our insights can inspire new ways of thinking in
resolving class-imbalanced deep learning.
</p>
<a href="http://arxiv.org/abs/2001.01385" target="_blank">arXiv:2001.01385</a> [<a href="http://arxiv.org/pdf/2001.01385" target="_blank">pdf</a>]

<h2>A Unified Framework for Coupled Tensor Completion. (arXiv:2001.02810v4 [cs.LG] UPDATED)</h2>
<h3>Huyan Huang, Yipeng Liu, Ce Zhu</h3>
<p>Coupled tensor decomposition reveals the joint data structure by
incorporating priori knowledge that come from the latent coupled factors. The
tensor ring (TR) decomposition is invariant under the permutation of tensors
with different mode properties, which ensures the uniformity of decomposed
factors and mode attributes. The TR has powerful expression ability and
achieves success in some multi-dimensional data processing applications. To let
coupled tensors help each other for missing component estimation, in this paper
we utilize TR for coupled completion by sharing parts of the latent factors.
The optimization model for coupled TR completion is developed with a novel
Frobenius norm. It is solved by the block coordinate descent algorithm which
efficiently solves a series of quadratic problems resulted from sampling
pattern. The excess risk bound for this optimization model shows the
theoretical performance enhancement in comparison with other coupled nuclear
norm based methods. The proposed method is validated on numerical experiments
on synthetic data, and experimental results on real-world data demonstrate its
superiority over the state-of-the-art methods in terms of recovery accuracy.
</p>
<a href="http://arxiv.org/abs/2001.02810" target="_blank">arXiv:2001.02810</a> [<a href="http://arxiv.org/pdf/2001.02810" target="_blank">pdf</a>]

<h2>Combining Offline Causal Inference and Online Bandit Learning for Data Driven Decision. (arXiv:2001.05699v2 [cs.LG] UPDATED)</h2>
<h3>Li Ye, Yishi Lin, Hong Xie, John C.S. Lui</h3>
<p>A fundamental question for companies with large amount of logged data is: How
to use such logged data together with incoming streaming data to make good
decisions? Many companies currently make decisions via online A/B tests, but
wrong decisions during testing hurt users' experiences and cause irreversible
damage. A typical alternative is offline causal inference, which analyzes
logged data alone to make decisions. However, these decisions are not adaptive
to the new incoming data, and so a wrong decision will continuously hurt users'
experiences. To overcome the aforementioned limitations, we propose a framework
to unify offline causal inference algorithms (e.g., weighting, matching) and
online learning algorithms (e.g., UCB, LinUCB). We propose novel algorithms and
derive bounds on the decision accuracy via the notion of "regret". We derive
the first upper regret bound for forest-based online bandit algorithms.
Experiments on two real datasets show that our algorithms outperform other
algorithms that use only logged data or online feedbacks, or algorithms that do
not use the data properly.
</p>
<a href="http://arxiv.org/abs/2001.05699" target="_blank">arXiv:2001.05699</a> [<a href="http://arxiv.org/pdf/2001.05699" target="_blank">pdf</a>]

<h2>Evaluating approval-based multiwinner voting in terms of robustness to noise. (arXiv:2002.01776v2 [cs.AI] UPDATED)</h2>
<h3>Ioannis Caragiannis, Christos Kaklamanis, Nikos Karanikolas, George A. Krimpas</h3>
<p>Approval-based multiwinner voting rules have recently received much attention
in the Computational Social Choice literature. Such rules aggregate approval
ballots and determine a winning committee of alternatives. To assess
effectiveness, we propose to employ new noise models that are specifically
tailored for approval votes and committees. These models take as input a ground
truth committee and return random approval votes to be thought of as noisy
estimates of the ground truth. A minimum robustness requirement for an
approval-based multiwinner voting rule is to return the ground truth when
applied to profiles with sufficiently many noisy votes. Our results indicate
that approval-based multiwinner voting is always robust to reasonable noise. We
further refine this finding by presenting a hierarchy of rules in terms of how
robust to noise they are.
</p>
<a href="http://arxiv.org/abs/2002.01776" target="_blank">arXiv:2002.01776</a> [<a href="http://arxiv.org/pdf/2002.01776" target="_blank">pdf</a>]

<h2>Watch out! Motion is Blurring the Vision of Your Deep Neural Networks. (arXiv:2002.03500v3 [cs.CV] UPDATED)</h2>
<h3>Qing Guo, Felix Juefei-Xu, Xiaofei Xie, Lei Ma, Jian Wang, Bing Yu, Wei Feng, Yang Liu</h3>
<p>The state-of-the-art deep neural networks (DNNs) are vulnerable against
adversarial examples with additive random-like noise perturbations. While such
examples are hardly found in the physical world, the image blurring effect
caused by object motion, on the other hand, commonly occurs in practice, making
the study of which greatly important especially for the widely adopted
real-time image processing tasks (e.g., object detection, tracking). In this
paper, we initiate the first step to comprehensively investigate the potential
hazards of the blur effect for DNN, caused by object motion. We propose a novel
adversarial attack method that can generate visually natural motion-blurred
adversarial examples, named motion-based adversarial blur attack (ABBA). To
this end, we first formulate the kernel-prediction-based attack where an input
image is convolved with kernels in a pixel-wise way, and the misclassification
capability is achieved by tuning the kernel weights. To generate visually more
natural and plausible examples, we further propose the saliency-regularized
adversarial kernel prediction, where the salient region serves as a moving
object, and the predicted kernel is regularized to achieve naturally visual
effects. Besides, the attack is further enhanced by adaptively tuning the
translations of object and background. A comprehensive evaluation on the
NeurIPS'17 adversarial competition dataset demonstrates the effectiveness of
ABBA by considering various kernel sizes, translations, and regions. The
in-depth study further confirms that our method shows more effective
penetrating capability to the state-of-the-art GAN-based deblurring mechanisms
compared with other blurring methods. We release the code to
https://github.com/tsingqguo/ABBA.
</p>
<a href="http://arxiv.org/abs/2002.03500" target="_blank">arXiv:2002.03500</a> [<a href="http://arxiv.org/pdf/2002.03500" target="_blank">pdf</a>]

<h2>Compositional Embeddings for Multi-Label One-Shot Learning. (arXiv:2002.04193v4 [cs.LG] UPDATED)</h2>
<h3>Zeqian Li, Michael C. Mozer, Jacob Whitehill</h3>
<p>We present a compositional embedding framework that infers not just a single
class per input image, but a set of classes, in the setting of one-shot
learning. Specifically, we propose and evaluate several novel models consisting
of (1) an embedding function f trained jointly with a "composition" function g
that computes set union operations between the classes encoded in two embedding
vectors; and (2) embedding f trained jointly with a "query" function h that
computes whether the classes encoded in one embedding subsume the classes
encoded in another embedding. In contrast to prior work, these models must both
perceive the classes associated with the input examples and encode the
relationships between different class label sets, and they are trained using
only weak one-shot supervision consisting of the label-set relationships among
training examples. Experiments on the OmniGlot, Open Images, and COCO datasets
show that the proposed compositional embedding models outperform existing
embedding methods. Our compositional embedding models have applications to
multi-label object recognition for both one-shot and supervised learning.
</p>
<a href="http://arxiv.org/abs/2002.04193" target="_blank">arXiv:2002.04193</a> [<a href="http://arxiv.org/pdf/2002.04193" target="_blank">pdf</a>]

<h2>Learning from Positive and Unlabeled Data with Arbitrary Positive Shift. (arXiv:2002.10261v4 [cs.LG] UPDATED)</h2>
<h3>Zayd Hammoudeh, Daniel Lowd</h3>
<p>Positive-unlabeled (PU) learning trains a binary classifier using only
positive and unlabeled data. A common simplifying assumption is that the
positive data is representative of the target positive class. This assumption
rarely holds in practice due to temporal drift, domain shift, and/or
adversarial manipulation. This paper shows that PU learning is possible even
with arbitrarily non-representative positive data given unlabeled data from the
source and target distributions. Our key insight is that only the negative
class's distribution need be fixed. We integrate this into two statistically
consistent methods to address arbitrary positive bias - one approach combines
negative-unlabeled learning with unlabeled-unlabeled learning while the other
uses a novel, recursive risk estimator. Experimental results demonstrate our
methods' effectiveness across numerous real-world datasets and forms of
positive bias, including disjoint positive class-conditional supports.
Additionally, we propose a general, simplified approach to address PU risk
estimation overfitting.
</p>
<a href="http://arxiv.org/abs/2002.10261" target="_blank">arXiv:2002.10261</a> [<a href="http://arxiv.org/pdf/2002.10261" target="_blank">pdf</a>]

<h2>Contextual Search in the Presence of Irrational Agents. (arXiv:2002.11650v3 [cs.LG] UPDATED)</h2>
<h3>Akshay Krishnamurthy, Thodoris Lykouris, Chara Podimata, Robert Schapire</h3>
<p>We study contextual search, a generalization of binary search in higher
dimensions, which captures settings such as feature-based dynamic pricing.
Standard game-theoretic formulations of this problem assume that agents act in
accordance with a specific behavioral model. In practice, however, some agents
may not prescribe to the dominant behavioral model or may act in ways that are
seemingly arbitrarily irrational. Existing algorithms heavily depend on the
behavioral model being (approximately) accurate for all agents and have poor
performance in the presence of even a few such arbitrarily irrational agents.

We initiate the study of contextual search when some of the agents can behave
in ways inconsistent with the underlying behavioral model. In particular, we
provide two algorithms, one built on robustifying multidimensional binary
search methods and one on translating the setting to a proxy setting
appropriate for gradient descent. Our techniques draw inspiration from learning
theory, game theory, high-dimensional geometry, and convex analysis.
</p>
<a href="http://arxiv.org/abs/2002.11650" target="_blank">arXiv:2002.11650</a> [<a href="http://arxiv.org/pdf/2002.11650" target="_blank">pdf</a>]

<h2>The Spectral Underpinning of word2vec. (arXiv:2002.12317v2 [cs.LG] UPDATED)</h2>
<h3>Ariel Jaffe, Yuval Kluger, Ofir Lindenbaum, Jonathan Patsenker, Erez Peterfreund, Stefan Steinerberger</h3>
<p>word2vec due to Mikolov \textit{et al.} (2013) is a word embedding method
that is widely used in natural language processing. Despite its great success
and frequent use, theoretical justification is still lacking. The main
contribution of our paper is to propose a rigorous analysis of the highly
nonlinear functional of word2vec. Our results suggest that word2vec may be
primarily driven by an underlying spectral method. This insight may open the
door to obtaining provable guarantees for word2vec. We support these findings
by numerical simulations. One fascinating open question is whether the
nonlinear properties of word2vec that are not captured by the spectral method
are beneficial and, if so, by what mechanism.
</p>
<a href="http://arxiv.org/abs/2002.12317" target="_blank">arXiv:2002.12317</a> [<a href="http://arxiv.org/pdf/2002.12317" target="_blank">pdf</a>]

<h2>Adversarial Deepfakes: Evaluating Vulnerability of Deepfake Detectors to Adversarial Examples. (arXiv:2002.12749v3 [cs.CV] UPDATED)</h2>
<h3>Shehzeen Hussain, Paarth Neekhara, Malhar Jere, Farinaz Koushanfar, Julian McAuley</h3>
<p>Recent advances in video manipulation techniques have made the generation of
fake videos more accessible than ever before. Manipulated videos can fuel
disinformation and reduce trust in media. Therefore detection of fake videos
has garnered immense interest in academia and industry. Recently developed
Deepfake detection methods rely on deep neural networks (DNNs) to distinguish
AI-generated fake videos from real videos. In this work, we demonstrate that it
is possible to bypass such detectors by adversarially modifying fake videos
synthesized using existing Deepfake generation methods. We further demonstrate
that our adversarial perturbations are robust to image and video compression
codecs, making them a real-world threat. We present pipelines in both white-box
and black-box attack scenarios that can fool DNN based Deepfake detectors into
classifying fake videos as real.
</p>
<a href="http://arxiv.org/abs/2002.12749" target="_blank">arXiv:2002.12749</a> [<a href="http://arxiv.org/pdf/2002.12749" target="_blank">pdf</a>]

<h2>Bounded Regret for Finitely Parameterized Multi-Armed Bandits. (arXiv:2003.01328v5 [cs.LG] UPDATED)</h2>
<h3>Kishan Panaganti, Dileep Kalathil</h3>
<p>We consider the problem of finitely parameterized multi-armed bandits where
the model of the underlying stochastic environment can be characterized based
on a common unknown parameter. The true parameter is unknown to the learning
agent. However, the set of possible parameters, which is finite, is known a
priori. We propose an algorithm that is simple and easy to implement, which we
call Finitely Parameterized Upper Confidence Bound (FP-UCB) algorithm, which
uses the information about the underlying parameter set for faster learning. In
particular, we show that the FP-UCB algorithm achieves a bounded regret under
some structural condition on the underlying parameter set. We also show that,
if the underlying parameter set does not satisfy the necessary structural
condition, the FP-UCB algorithm achieves a logarithmic regret, but with a
smaller preceding constant compared to the standard UCB algorithm. We also
validate the superior performance of the FP-UCB algorithm through extensive
numerical simulations.
</p>
<a href="http://arxiv.org/abs/2003.01328" target="_blank">arXiv:2003.01328</a> [<a href="http://arxiv.org/pdf/2003.01328" target="_blank">pdf</a>]

<h2>Getting Better from Worse: Augmented Bagging and a Cautionary Tale of Variable Importance. (arXiv:2003.03629v2 [stat.ML] UPDATED)</h2>
<h3>Lucas Mentch, Siyu Zhou</h3>
<p>As the size, complexity, and availability of data continues to grow,
scientists are increasingly relying upon black-box learning algorithms that can
often provide accurate predictions with minimal a priori model specifications.
Tools like random forests have an established track record of off-the-shelf
success and even offer various strategies for analyzing the underlying
relationships among variables. Here, motivated by recent insights into random
forest behavior, we introduce the simple idea of augmented bagging (AugBagg), a
procedure that operates in an identical fashion to classical bagging and random
forests, but which operates on a larger, augmented space containing additional
randomly generated noise features. Surprisingly, we demonstrate that this
simple act of including extra noise variables in the model can lead to dramatic
improvements in out-of-sample predictive accuracy, sometimes outperforming even
an optimally tuned traditional random forest. As a result, intuitive notions of
variable importance based on improved model accuracy may be deeply flawed, as
even purely random noise can routinely register as statistically significant.
Numerous demonstrations on both real and synthetic data are provided along with
a proposed solution.
</p>
<a href="http://arxiv.org/abs/2003.03629" target="_blank">arXiv:2003.03629</a> [<a href="http://arxiv.org/pdf/2003.03629" target="_blank">pdf</a>]

<h2>Unified Image and Video Saliency Modeling. (arXiv:2003.05477v3 [cs.CV] UPDATED)</h2>
<h3>Richard Droste, Jianbo Jiao, J. Alison Noble</h3>
<p>Visual saliency modeling for images and videos is treated as two independent
tasks in recent computer vision literature. While image saliency modeling is a
well-studied problem and progress on benchmarks like SALICON and MIT300 is
slowing, video saliency models have shown rapid gains on the recent DHF1K
benchmark. Here, we take a step back and ask: Can image and video saliency
modeling be approached via a unified model, with mutual benefit? We identify
different sources of domain shift between image and video saliency data and
between different video saliency datasets as a key challenge for effective
joint modelling. To address this we propose four novel domain adaptation
techniques - Domain-Adaptive Priors, Domain-Adaptive Fusion, Domain-Adaptive
Smoothing and Bypass-RNN - in addition to an improved formulation of learned
Gaussian priors. We integrate these techniques into a simple and lightweight
encoder-RNN-decoder-style network, UNISAL, and train it jointly with image and
video saliency data. We evaluate our method on the video saliency datasets
DHF1K, Hollywood-2 and UCF-Sports, and the image saliency datasets SALICON and
MIT300. With one set of parameters, UNISAL achieves state-of-the-art
performance on all video saliency datasets and is on par with the
state-of-the-art for image saliency datasets, despite faster runtime and a 5 to
20-fold smaller model size compared to all competing deep methods. We provide
retrospective analyses and ablation studies which confirm the importance of the
domain shift modeling. The code is available at
https://github.com/rdroste/unisal
</p>
<a href="http://arxiv.org/abs/2003.05477" target="_blank">arXiv:2003.05477</a> [<a href="http://arxiv.org/pdf/2003.05477" target="_blank">pdf</a>]

<h2>GraphTCN: Spatio-Temporal Interaction Modeling for Human Trajectory Prediction. (arXiv:2003.07167v5 [cs.CV] UPDATED)</h2>
<h3>Chengxin Wang, Shaofeng Cai, Gary Tan</h3>
<p>Predicting the future paths of an agent's neighbors accurately and in a
timely manner is central to the autonomous applications for collision
avoidance. Conventional approaches, e.g., LSTM-based models, take considerable
computational costs in the prediction, especially for the long sequence
prediction. To support more efficient and accurate trajectory predictions, we
propose a novel CNN-based spatial-temporal graph framework GraphTCN, which
models the spatial interactions as social graphs and captures the
spatio-temporal interactions with a modified temporal convolutional network. In
contrast to conventional models, both the spatial and temporal modeling of our
model are computed within each local time window. Therefore, it can be executed
in parallel for much higher efficiency, and meanwhile with accuracy comparable
to best-performing approaches. Experimental results confirm that our model
achieves better performance in terms of both efficiency and accuracy as
compared with state-of-the-art models on various trajectory prediction
benchmark datasets.
</p>
<a href="http://arxiv.org/abs/2003.07167" target="_blank">arXiv:2003.07167</a> [<a href="http://arxiv.org/pdf/2003.07167" target="_blank">pdf</a>]

<h2>Inverting the Pose Forecasting Pipeline with SPF2: Sequential Pointcloud Forecasting for Sequential Pose Forecasting. (arXiv:2003.08376v3 [cs.CV] UPDATED)</h2>
<h3>Xinshuo Weng, Jianren Wang, Sergey Levine, Kris Kitani, Nicholas Rhinehart</h3>
<p>Many autonomous systems forecast aspects of the future in order to aid
decision-making. For example, self-driving vehicles and robotic manipulation
systems often forecast future object poses by first detecting and tracking
objects. However, this detect-then-forecast pipeline is expensive to scale, as
pose forecasting algorithms typically require labeled sequences of object
poses, which are costly to obtain in 3D space. Can we scale performance without
requiring additional labels? We hypothesize yes, and propose inverting the
detect-then-forecast pipeline. Instead of detecting, tracking and then
forecasting the objects, we propose to first forecast 3D sensor data (e.g.,
point clouds with $100$k points) and then detect/track objects on the predicted
point cloud sequences to obtain future poses, i.e., a forecast-then-detect
pipeline. This inversion makes it less expensive to scale pose forecasting, as
the sensor data forecasting task requires no labels. Part of this work's focus
is on the challenging first step -- Sequential Pointcloud Forecasting (SPF),
for which we also propose an effective approach, SPFNet. To compare our
forecast-then-detect pipeline relative to the detect-then-forecast pipeline, we
propose an evaluation procedure and two metrics. Through experiments on a
robotic manipulation dataset and two driving datasets, we show that SPFNet is
effective for the SPF task, our forecast-then-detect pipeline outperforms the
detect-then-forecast approaches to which we compared, and that pose forecasting
performance improves with the addition of unlabeled data.
</p>
<a href="http://arxiv.org/abs/2003.08376" target="_blank">arXiv:2003.08376</a> [<a href="http://arxiv.org/pdf/2003.08376" target="_blank">pdf</a>]

<h2>Goal-Conditioned End-to-End Visuomotor Control for Versatile Skill Primitives. (arXiv:2003.08854v2 [cs.RO] UPDATED)</h2>
<h3>Oliver Groth, Chia-Man Hung, Andrea Vedaldi, Ingmar Posner</h3>
<p>Visuomotor control (VMC) is an effective means of achieving basic
manipulation tasks such as pushing or pick-and-place from raw images.
Conditioning VMC on desired goal states is a promising way of achieving
versatile skill primitives. However, common conditioning schemes either rely on
task-specific fine tuning - e.g. using one-shot imitation learning (IL) - or on
sampling approaches using a forward model of scene dynamics i.e.
model-predictive control (MPC), leaving deployability and planning horizon
severely limited. In this paper we propose a conditioning scheme which avoids
these pitfalls by learning the controller and its conditioning in an end-to-end
manner. Our model predicts complex action sequences based directly on a dynamic
image representation of the robot motion and the distance to a given target
observation. In contrast to related works, this enables our approach to
efficiently perform complex manipulation tasks from raw image observations
without predefined control primitives or test time demonstrations. We report
significant improvements in task success over representative MPC and IL
baselines. We also demonstrate our model's generalisation capabilities in
challenging, unseen tasks featuring visual noise, cluttered scenes and unseen
object geometries.
</p>
<a href="http://arxiv.org/abs/2003.08854" target="_blank">arXiv:2003.08854</a> [<a href="http://arxiv.org/pdf/2003.08854" target="_blank">pdf</a>]

<h2>ACNMP: Skill Transfer and Task Extrapolation through Learning from Demonstration and Reinforcement Learning via Representation Sharing. (arXiv:2003.11334v3 [cs.RO] UPDATED)</h2>
<h3>M.Tuluhan Akbulut, Erhan Oztop, M.Yunus Seker, Honghu Xue, Ahmet E. Tekden, Emre Ugur</h3>
<p>To equip robots with dexterous skills, an effective approach is to first
transfer the desired skill via Learning from Demonstration (LfD), then let the
robot improve it by self-exploration via Reinforcement Learning (RL). In this
paper, we propose a novel LfD+RL framework, namely Adaptive Conditional Neural
Movement Primitives (ACNMP), that allows efficient policy improvement in novel
environments and effective skill transfer between different agents. This is
achieved through exploiting the latent representation learned by the underlying
Conditional Neural Process (CNP) model, and simultaneous training of the model
with supervised learning (SL) for acquiring the demonstrated trajectories and
via RL for new trajectory discovery. Through simulation experiments, we show
that (i) ACNMP enables the system to extrapolate to situations where pure LfD
fails; (ii) Simultaneous training of the system through SL and RL preserves the
shape of demonstrations while adapting to novel situations due to the shared
representations used by both learners; (iii) ACNMP enables order-of-magnitude
sample-efficient RL in extrapolation of reaching tasks compared to the existing
approaches; (iv) ACNMPs can be used to implement skill transfer between robots
having different morphology, with competitive learning speeds and importantly
with less number of assumptions compared to the state-of-the-art approaches.
Finally, we show the real-world suitability of ACNMPs through real robot
experiments that involve obstacle avoidance, pick and place and pouring
actions.
</p>
<a href="http://arxiv.org/abs/2003.11334" target="_blank">arXiv:2003.11334</a> [<a href="http://arxiv.org/pdf/2003.11334" target="_blank">pdf</a>]

<h2>Meta-Learning in Neural Networks: A Survey. (arXiv:2004.05439v2 [cs.LG] UPDATED)</h2>
<h3>Timothy Hospedales, Antreas Antoniou, Paul Micaelli, Amos Storkey</h3>
<p>The field of meta-learning, or learning-to-learn, has seen a dramatic rise in
interest in recent years. Contrary to conventional approaches to AI where tasks
are solved from scratch using a fixed learning algorithm, meta-learning aims to
improve the learning algorithm itself, given the experience of multiple
learning episodes. This paradigm provides an opportunity to tackle many
conventional challenges of deep learning, including data and computation
bottlenecks, as well as generalization. This survey describes the contemporary
meta-learning landscape. We first discuss definitions of meta-learning and
position it with respect to related fields, such as transfer learning and
hyperparameter optimization. We then propose a new taxonomy that provides a
more comprehensive breakdown of the space of meta-learning methods today. We
survey promising applications and successes of meta-learning such as few-shot
learning and reinforcement learning. Finally, we discuss outstanding challenges
and promising areas for future research.
</p>
<a href="http://arxiv.org/abs/2004.05439" target="_blank">arXiv:2004.05439</a> [<a href="http://arxiv.org/pdf/2004.05439" target="_blank">pdf</a>]

<h2>SSP: Single Shot Future Trajectory Prediction. (arXiv:2004.05846v2 [cs.CV] UPDATED)</h2>
<h3>Isht Dwivedi, Srikanth Malla, Behzad Dariush, Chiho Choi</h3>
<p>We propose a robust solution to future trajectory forecast, which can be
practically applicable to autonomous agents in highly crowded environments. For
this, three aspects are particularly addressed in this paper. First, we use
composite fields to predict future locations of all road agents in a
single-shot, which results in a constant time complexity, regardless of the
number of agents in the scene. Second, interactions between agents are modeled
as a non-local response, enabling spatial relationships between different
locations to be captured temporally as well (i.e., in spatio-temporal
interactions). Third, the semantic context of the scene are modeled and take
into account the environmental constraints that potentially influence the
future motion. To this end, we validate the robustness of the proposed approach
using the ETH, UCY, and SDD datasets and highlight its practical functionality
compared to the current state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2004.05846" target="_blank">arXiv:2004.05846</a> [<a href="http://arxiv.org/pdf/2004.05846" target="_blank">pdf</a>]

<h2>Privacy in Deep Learning: A Survey. (arXiv:2004.12254v5 [cs.LG] UPDATED)</h2>
<h3>Fatemehsadat Mireshghallah, Mohammadkazem Taram, Praneeth Vepakomma, Abhishek Singh, Ramesh Raskar, Hadi Esmaeilzadeh</h3>
<p>The ever-growing advances of deep learning in many areas including vision,
recommendation systems, natural language processing, etc., have led to the
adoption of Deep Neural Networks (DNNs) in production systems. The availability
of large datasets and high computational power are the main contributors to
these advances. The datasets are usually crowdsourced and may contain sensitive
information. This poses serious privacy concerns as this data can be misused or
leaked through various vulnerabilities. Even if the cloud provider and the
communication link is trusted, there are still threats of inference attacks
where an attacker could speculate properties of the data used for training, or
find the underlying model architecture and parameters. In this survey, we
review the privacy concerns brought by deep learning, and the mitigating
techniques introduced to tackle these issues. We also show that there is a gap
in the literature regarding test-time inference privacy, and propose possible
future research directions.
</p>
<a href="http://arxiv.org/abs/2004.12254" target="_blank">arXiv:2004.12254</a> [<a href="http://arxiv.org/pdf/2004.12254" target="_blank">pdf</a>]

<h2>Towards Visually Explaining Video Understanding Networks with Perturbation. (arXiv:2005.00375v2 [cs.CV] UPDATED)</h2>
<h3>Zhenqiang Li, Weimin Wang, Zuoyue Li, Yifei Huang, Yoichi Sato</h3>
<p>''Making black box models explainable'' is a vital problem that accompanies
the development of deep learning networks. For networks taking visual
information as input, one basic but challenging explanation method is to
identify and visualize the input pixels/regions that dominate the network's
prediction. However, most existing works focus on explaining networks taking a
single image as input and do not consider the temporal relationship that exists
in videos. Providing an easy-to-use visual explanation method that is
applicable to diversified structures of video understanding networks still
remains an open challenge. In this paper, we investigate a generic
perturbation-based method for visually explaining video understanding networks.
Besides, we propose a novel loss function to enhance the method by constraining
the smoothness of its results in both spatial and temporal dimensions. The
method enables the comparison of explanation results between different network
structures to become possible and can also avoid generating the pathological
adversarial explanations for video inputs. Experimental comparison results
verified the effectiveness of our method.
</p>
<a href="http://arxiv.org/abs/2005.00375" target="_blank">arXiv:2005.00375</a> [<a href="http://arxiv.org/pdf/2005.00375" target="_blank">pdf</a>]

<h2>Reference Pose Generation for Long-term Visual Localization via Learned Features and View Synthesis. (arXiv:2005.05179v3 [cs.CV] UPDATED)</h2>
<h3>Zichao Zhang, Torsten Sattler, Davide Scaramuzza</h3>
<p>Visual Localization is one of the key enabling technologies for autonomous
driving and augmented reality. High quality datasets with accurate 6
Degree-of-Freedom (DoF) reference poses are the foundation for benchmarking and
improving existing methods. Traditionally, reference poses have been obtained
via Structure-from-Motion (SfM). However, SfM itself relies on local features
which are prone to fail when images were taken under different conditions,
e.g., day/ night changes. At the same time, manually annotating feature
correspondences is not scalable and potentially inaccurate. In this work, we
propose a semi-automated approach to generate reference poses based on feature
matching between renderings of a 3D model and real images via learned features.
Given an initial pose estimate, our approach iteratively refines the pose based
on feature matches against a rendering of the model from the current pose
estimate. We significantly improve the nighttime reference poses of the popular
Aachen Day-Night dataset, showing that state-of-the-art visual localization
methods perform better (up to $47\%$) than predicted by the original reference
poses. We extend the dataset with new nighttime test images, provide
uncertainty estimates for our new reference poses, and introduce a new
evaluation criterion. We will make our reference poses and our framework
publicly available upon publication.
</p>
<a href="http://arxiv.org/abs/2005.05179" target="_blank">arXiv:2005.05179</a> [<a href="http://arxiv.org/pdf/2005.05179" target="_blank">pdf</a>]

<h2>Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere. (arXiv:2005.10242v8 [cs.LG] UPDATED)</h2>
<h3>Tongzhou Wang, Phillip Isola</h3>
<p>Contrastive representation learning has been outstandingly successful in
practice. In this work, we identify two key properties related to the
contrastive loss: (1) alignment (closeness) of features from positive pairs,
and (2) uniformity of the induced distribution of the (normalized) features on
the hypersphere. We prove that, asymptotically, the contrastive loss optimizes
these properties, and analyze their positive effects on downstream tasks.
Empirically, we introduce an optimizable metric to quantify each property.
Extensive experiments on standard vision and language datasets confirm the
strong agreement between both metrics and downstream task performance.
Remarkably, directly optimizing for these two metrics leads to representations
with comparable or better performance at downstream tasks than contrastive
learning.

Project Page: https://ssnl.github.io/hypersphere

Code: https://github.com/SsnL/align_uniform ,
https://github.com/SsnL/moco_align_uniform
</p>
<a href="http://arxiv.org/abs/2005.10242" target="_blank">arXiv:2005.10242</a> [<a href="http://arxiv.org/pdf/2005.10242" target="_blank">pdf</a>]

<h2>Invariant Policy Optimization: Towards Stronger Generalization in Reinforcement Learning. (arXiv:2006.01096v3 [cs.LG] UPDATED)</h2>
<h3>Anoopkumar Sonar, Vincent Pacelli, Anirudha Majumdar</h3>
<p>A fundamental challenge in reinforcement learning is to learn policies that
generalize beyond the operating domains experienced during training. In this
paper, we approach this challenge through the following invariance principle:
an agent must find a representation such that there exists an action-predictor
built on top of this representation that is simultaneously optimal across all
training domains. Intuitively, the resulting invariant policy enhances
generalization by finding causes of successful actions. We propose a novel
learning algorithm, Invariant Policy Optimization (IPO), that implements this
principle and learns an invariant policy during training. We compare our
approach with standard policy gradient methods and demonstrate significant
improvements in generalization performance on unseen domains for linear
quadratic regulator and grid-world problems, and an example where a robot must
learn to open doors with varying physical properties.
</p>
<a href="http://arxiv.org/abs/2006.01096" target="_blank">arXiv:2006.01096</a> [<a href="http://arxiv.org/pdf/2006.01096" target="_blank">pdf</a>]

<h2>Non-Euclidean Universal Approximation. (arXiv:2006.02341v3 [cs.LG] UPDATED)</h2>
<h3>Anastasis Kratsios, Eugene Bilokopytov</h3>
<p>Modifications to a neural network's input and output layers are often
required to accommodate the specificities of most practical learning tasks.
However, the impact of such changes on architecture's approximation
capabilities is largely not understood. We present general conditions
describing feature and readout maps that preserve an architecture's ability to
approximate any continuous functions uniformly on compacts. As an application,
we show that if an architecture is capable of universal approximation, then
modifying its final layer to produce binary values creates a new architecture
capable of deterministically approximating any classifier. In particular, we
obtain guarantees for deep CNNs and deep feed-forward networks. Our results
also have consequences within the scope of geometric deep learning.
Specifically, when the input and output spaces are Cartan-Hadamard manifolds,
we obtain geometrically meaningful feature and readout maps satisfying our
criteria. Consequently, commonly used non-Euclidean regression models between
spaces of symmetric positive definite matrices are extended to universal DNNs.
The same result allows us to show that the hyperbolic feed-forward networks,
used for hierarchical learning, are universal. Our result is also used to show
that the common practice of randomizing all but the last two layers of a DNN
produces a universal family of functions with probability one. We also provide
conditions on a DNN's first (resp. last) few layer's connections and activation
function which guarantee that these layers can have a width equal to the input
(resp. output) space's dimension while not negatively affecting the
architecture's approximation capabilities.
</p>
<a href="http://arxiv.org/abs/2006.02341" target="_blank">arXiv:2006.02341</a> [<a href="http://arxiv.org/pdf/2006.02341" target="_blank">pdf</a>]

<h2>Pruning neural networks without any data by iteratively conserving synaptic flow. (arXiv:2006.05467v2 [cs.LG] UPDATED)</h2>
<h3>Hidenori Tanaka, Daniel Kunin, Daniel L. K. Yamins, Surya Ganguli</h3>
<p>Pruning the parameters of deep neural networks has generated intense interest
due to potential savings in time, memory and energy both during training and at
test time. Recent works have identified, through an expensive sequence of
training and pruning cycles, the existence of winning lottery tickets or sparse
trainable subnetworks at initialization. This raises a foundational question:
can we identify highly sparse trainable subnetworks at initialization, without
ever training, or indeed without ever looking at the data? We provide an
affirmative answer to this question through theory driven algorithm design. We
first mathematically formulate and experimentally verify a conservation law
that explains why existing gradient-based pruning algorithms at initialization
suffer from layer-collapse, the premature pruning of an entire layer rendering
a network untrainable. This theory also elucidates how layer-collapse can be
entirely avoided, motivating a novel pruning algorithm Iterative Synaptic Flow
Pruning (SynFlow). This algorithm can be interpreted as preserving the total
flow of synaptic strengths through the network at initialization subject to a
sparsity constraint. Notably, this algorithm makes no reference to the training
data and consistently competes with or outperforms existing state-of-the-art
pruning algorithms at initialization over a range of models (VGG and ResNet),
datasets (CIFAR-10/100 and Tiny ImageNet), and sparsity constraints (up to
99.99 percent). Thus our data-agnostic pruning algorithm challenges the
existing paradigm that, at initialization, data must be used to quantify which
synapses are important.
</p>
<a href="http://arxiv.org/abs/2006.05467" target="_blank">arXiv:2006.05467</a> [<a href="http://arxiv.org/pdf/2006.05467" target="_blank">pdf</a>]

<h2>0-MMS: Zero-Shot Multi-Motion Segmentation With A Monocular Event Camera. (arXiv:2006.06158v2 [cs.CV] UPDATED)</h2>
<h3>Chethan M. Parameshwara, Nitin J. Sanket, Chahat Deep Singh, Cornelia Ferm&#xfc;ller, Yiannis Aloimonos</h3>
<p>Segmentation of moving objects in dynamic scenes is a key process in scene
understanding for navigation tasks. Classical cameras suffer from motion blur
in such scenarios rendering them effete. On the contrary, event cameras,
because of their high temporal resolution and lack of motion blur, are
tailor-made for this problem. We present an approach for monocular multi-motion
segmentation, which combines bottom-up feature tracking and top-down motion
compensation into a unified pipeline, which is the first of its kind to our
knowledge. Using the events within a time-interval, our method segments the
scene into multiple motions by splitting and merging. We further speed up our
method by using the concept of motion propagation and cluster keyslices.

The approach was successfully evaluated on both challenging real-world and
synthetic scenarios from the EV-IMO, EED, and MOD datasets and outperformed the
state-of-the-art detection rate by 12\%, achieving a new state-of-the-art
average detection rate of 81.06%, 94.2% and 82.35% on the aforementioned
datasets. To enable further research and systematic evaluation of multi-motion
segmentation, we present and open-source a new dataset/benchmark called MOD++,
which includes challenging sequences and extensive data stratification in-terms
of camera and object motion, velocity magnitudes, direction, and rotational
speeds.
</p>
<a href="http://arxiv.org/abs/2006.06158" target="_blank">arXiv:2006.06158</a> [<a href="http://arxiv.org/pdf/2006.06158" target="_blank">pdf</a>]

<h2>Generalization error in high-dimensional perceptrons: Approaching Bayes error with convex optimization. (arXiv:2006.06560v2 [stat.ML] UPDATED)</h2>
<h3>Benjamin Aubin, Florent Krzakala, Yue M. Lu, Lenka Zdeborov&#xe1;</h3>
<p>We consider a commonly studied supervised classification of a synthetic
dataset whose labels are generated by feeding a one-layer neural network with
random iid inputs. We study the generalization performances of standard
classifiers in the high-dimensional regime where $\alpha=n/d$ is kept finite in
the limit of a high dimension $d$ and number of samples $n$. Our contribution
is three-fold: First, we prove a formula for the generalization error achieved
by $\ell_2$ regularized classifiers that minimize a convex loss. This formula
was first obtained by the heuristic replica method of statistical physics.
Secondly, focussing on commonly used loss functions and optimizing the $\ell_2$
regularization strength, we observe that while ridge regression performance is
poor, logistic and hinge regression are surprisingly able to approach the
Bayes-optimal generalization error extremely closely. As $\alpha \to \infty$
they lead to Bayes-optimal rates, a fact that does not follow from predictions
of margin-based generalization error bounds. Third, we design an optimal loss
and regularizer that provably leads to Bayes-optimal generalization error.
</p>
<a href="http://arxiv.org/abs/2006.06560" target="_blank">arXiv:2006.06560</a> [<a href="http://arxiv.org/pdf/2006.06560" target="_blank">pdf</a>]

<h2>On Worst-case Regret of Linear Thompson Sampling. (arXiv:2006.06790v2 [cs.LG] UPDATED)</h2>
<h3>Nima Hamidi, Mohsen Bayati</h3>
<p>In this paper, we consider the worst-case regret of Linear Thompson Sampling
(LinTS) for the linear bandit problem. \citet{russo2014learning} show that the
Bayesian regret of LinTS is bounded above by
$\widetilde{\mathcal{O}}(d\sqrt{T})$ where $T$ is the time horizon and $d$ is
the number of parameters. While this bound matches the minimax lower-bounds for
this problem up to logarithmic factors, the existence of a similar worst-case
regret bound is still unknown. The only known worst-case regret bound for
LinTS, due to \cite{agrawal2013thompson,abeille2017linear}, is
$\widetilde{\mathcal{O}}(d\sqrt{dT})$ which requires the posterior variance to
be inflated by a factor of $\widetilde{\mathcal{O}}(\sqrt{d})$. While this
bound is far from the minimax optimal rate by a factor of $\sqrt{d}$, in this
paper we show that it is the best possible one can get, settling an open
problem stated in \cite{russo2018tutorial}. Specifically, we construct examples
to show that, without the inflation, LinTS can incur linear regret up to time
$\exp(\Omega(d))$. We then demonstrate that, under mild conditions, a slightly
modified version of LinTS requires only an $\widetilde{\mathcal{O}}(1)$
inflation where the constant depends on the diversity of the optimal arm.
</p>
<a href="http://arxiv.org/abs/2006.06790" target="_blank">arXiv:2006.06790</a> [<a href="http://arxiv.org/pdf/2006.06790" target="_blank">pdf</a>]

<h2>Temporal Variability in Implicit Online Learning. (arXiv:2006.07503v2 [cs.LG] UPDATED)</h2>
<h3>Nicol&#xf2; Campolongo, Francesco Orabona</h3>
<p>In the setting of online learning, Implicit algorithms turn out to be highly
successful from a practical standpoint. However, the tightest regret analyses
only show marginal improvements over Online Mirror Descent. In this work, we
shed light on this behavior carrying out a careful regret analysis. We prove a
novel static regret bound that depends on the temporal variability of the
sequence of loss functions, a quantity which is often encountered when
considering dynamic competitors. We show, for example, that the regret can be
constant if the temporal variability is constant and the learning rate is tuned
appropriately, without the need of smooth losses. Moreover, we present an
adaptive algorithm that achieves this regret bound without prior knowledge of
the temporal variability and prove a matching lower bound. Finally, we validate
our theoretical findings on classification and regression datasets.
</p>
<a href="http://arxiv.org/abs/2006.07503" target="_blank">arXiv:2006.07503</a> [<a href="http://arxiv.org/pdf/2006.07503" target="_blank">pdf</a>]

<h2>Nonasymptotic Guarantees for Spiked Matrix Recovery with Generative Priors. (arXiv:2006.07953v2 [stat.ML] UPDATED)</h2>
<h3>Jorio Cocola, Paul Hand, Vladislav Voroninski</h3>
<p>Many problems in statistics and machine learning require the reconstruction
of a rank-one signal matrix from noisy data. Enforcing additional prior
information on the rank-one component is often key to guaranteeing good
recovery performance. One such prior on the low-rank component is sparsity,
giving rise to the sparse principal component analysis problem. Unfortunately,
there is strong evidence that this problem suffers from a
computational-to-statistical gap, which may be fundamental. In this work, we
study an alternative prior where the low-rank component is in the range of a
trained generative network. We provide a non-asymptotic analysis with optimal
sample complexity, up to logarithmic factors, for rank-one matrix recovery
under an expansive-Gaussian network prior. Specifically, we establish a
favorable global optimization landscape for a nonlinear least squares
objective, provided the number of samples is on the order of the dimensionality
of the input to the generative model. This result suggests that generative
priors have no computational-to-statistical gap for structured rank-one matrix
recovery in the finite data, nonasymptotic regime. We present this analysis in
the case of both the Wishart and Wigner spiked matrix models.
</p>
<a href="http://arxiv.org/abs/2006.07953" target="_blank">arXiv:2006.07953</a> [<a href="http://arxiv.org/pdf/2006.07953" target="_blank">pdf</a>]

<h2>On Lipschitz Regularization of Convolutional Layers using Toeplitz Matrix Theory. (arXiv:2006.08391v2 [cs.LG] UPDATED)</h2>
<h3>Alexandre Araujo, Benjamin Negrevergne, Yann Chevaleyre, Jamal Atif</h3>
<p>This paper tackles the problem of Lipschitz regularization of Convolutional
Neural Networks. Lipschitz regularity is now established as a key property of
modern deep learning with implications in training stability, generalization,
robustness against adversarial examples, etc. However, computing the exact
value of the Lipschitz constant of a neural network is known to be NP-hard.
Recent attempts from the literature introduce upper bounds to approximate this
constant that are either efficient but loose or accurate but computationally
expensive. In this work, by leveraging the theory of Toeplitz matrices, we
introduce a new upper bound for convolutional layers that is both tight and
easy to compute. Based on this result we devise an algorithm to train Lipschitz
regularized Convolutional Neural Networks.
</p>
<a href="http://arxiv.org/abs/2006.08391" target="_blank">arXiv:2006.08391</a> [<a href="http://arxiv.org/pdf/2006.08391" target="_blank">pdf</a>]

<h2>Learning Rates as a Function of Batch Size: A Random Matrix Theory Approach to Neural Network Training. (arXiv:2006.09092v2 [stat.ML] UPDATED)</h2>
<h3>Diego Granziol, Stefan Zohren, Stephen Roberts</h3>
<p>We study the effect of mini-batching on the loss landscape of deep neural
networks using spiked, field-dependent random matrix theory. We demonstrate
that the magnitude of the extremal values of the batch Hessian are larger than
those of the empirical Hessian. We also derive similar results for the
Generalised Gauss-Newton matrix approximation of the Hessian. As a consequence
of our theorems we derive an analytical expressions for the maximal learning
rates as a function of batch size, informing practical optimisation schemes for
both stochastic gradient descent (linear scaling) and adaptive algorithms such
as Adam (square root scaling). Whilst the linear scaling for stochastic
gradient descent has been derived under more restrictive conditions, which we
generalise, the square root scaling rule for adaptive optimisers is, to our
knowledge, completely novel. We validate our claims on the VGG/WideResNet
architectures on the CIFAR-100 and ImageNet datasets.
</p>
<a href="http://arxiv.org/abs/2006.09092" target="_blank">arXiv:2006.09092</a> [<a href="http://arxiv.org/pdf/2006.09092" target="_blank">pdf</a>]

<h2>HyNet: Learning Local Descriptor with Hybrid Similarity Measure and Triplet Loss. (arXiv:2006.10202v3 [cs.CV] UPDATED)</h2>
<h3>Yurun Tian, Axel Barroso-Laguna, Tony Ng, Vassileios Balntas, Krystian Mikolajczyk</h3>
<p>Recent works show that local descriptor learning benefits from the use of L2
normalisation, however, an in-depth analysis of this effect lacks in the
literature. In this paper, we investigate how L2 normalisation affects the
back-propagated descriptor gradients during training. Based on our
observations, we propose HyNet, a new local descriptor that leads to
state-of-the-art results in matching. HyNet introduces a hybrid similarity
measure for triplet margin loss, a regularisation term constraining the
descriptor norm, and a new network architecture that performs L2 normalisation
of all intermediate feature maps and the output descriptors. HyNet surpasses
previous methods by a significant margin on standard benchmarks that include
patch matching, verification, and retrieval, as well as outperforming full
end-to-end methods on 3D reconstruction tasks.
</p>
<a href="http://arxiv.org/abs/2006.10202" target="_blank">arXiv:2006.10202</a> [<a href="http://arxiv.org/pdf/2006.10202" target="_blank">pdf</a>]

<h2>Subgraph Neural Networks. (arXiv:2006.10538v3 [cs.LG] UPDATED)</h2>
<h3>Emily Alsentzer, Samuel G. Finlayson, Michelle M. Li, Marinka Zitnik</h3>
<p>Deep learning methods for graphs achieve remarkable performance on many
node-level and graph-level prediction tasks. However, despite the proliferation
of the methods and their success, prevailing Graph Neural Networks (GNNs)
neglect subgraphs, rendering subgraph prediction tasks challenging to tackle in
many impactful applications. Further, subgraph prediction tasks present several
unique challenges: subgraphs can have non-trivial internal topology, but also
carry a notion of position and external connectivity information relative to
the underlying graph in which they exist. Here, we introduce SubGNN, a subgraph
neural network to learn disentangled subgraph representations. We propose a
novel subgraph routing mechanism that propagates neural messages between the
subgraph's components and randomly sampled anchor patches from the underlying
graph, yielding highly accurate subgraph representations. SubGNN specifies
three channels, each designed to capture a distinct aspect of subgraph
topology, and we provide empirical evidence that the channels encode their
intended properties. We design a series of new synthetic and real-world
subgraph datasets. Empirical results for subgraph classification on eight
datasets show that SubGNN achieves considerable performance gains,
outperforming strong baseline methods, including node-level and graph-level
GNNs, by 19.8% over the strongest baseline. SubGNN performs exceptionally well
on challenging biomedical datasets where subgraphs have complex topology and
even comprise multiple disconnected components.
</p>
<a href="http://arxiv.org/abs/2006.10538" target="_blank">arXiv:2006.10538</a> [<a href="http://arxiv.org/pdf/2006.10538" target="_blank">pdf</a>]

<h2>Differentiable Augmentation for Data-Efficient GAN Training. (arXiv:2006.10738v3 [cs.CV] UPDATED)</h2>
<h3>Shengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, Song Han</h3>
<p>The performance of generative adversarial networks (GANs) heavily
deteriorates given a limited amount of training data. This is mainly because
the discriminator is memorizing the exact training set. To combat it, we
propose Differentiable Augmentation (DiffAugment), a simple method that
improves the data efficiency of GANs by imposing various types of
differentiable augmentations on both real and fake samples. Previous attempts
to directly augment the training data manipulate the distribution of real
images, yielding little benefit; DiffAugment enables us to adopt the
differentiable augmentation for the generated samples, effectively stabilizes
training, and leads to better convergence. Experiments demonstrate consistent
gains of our method over a variety of GAN architectures and loss functions for
both unconditional and class-conditional generation. With DiffAugment, we
achieve a state-of-the-art FID of 6.80 with an IS of 100.8 on ImageNet 128x128
and 2-4x reductions of FID given 1,000 images on FFHQ and LSUN. Furthermore,
with only 20% training data, we can match the top performance on CIFAR-10 and
CIFAR-100. Finally, our method can generate high-fidelity images using only 100
images without pre-training, while being on par with existing transfer learning
algorithms. Code is available at
https://github.com/mit-han-lab/data-efficient-gans.
</p>
<a href="http://arxiv.org/abs/2006.10738" target="_blank">arXiv:2006.10738</a> [<a href="http://arxiv.org/pdf/2006.10738" target="_blank">pdf</a>]

<h2>Adversarial Soft Advantage Fitting: Imitation Learning without Policy Optimization. (arXiv:2006.13258v3 [cs.LG] UPDATED)</h2>
<h3>Paul Barde, Julien Roy, Wonseok Jeon, Joelle Pineau, Christopher Pal, Derek Nowrouzezahrai</h3>
<p>Adversarial Imitation Learning alternates between learning a discriminator --
which tells apart expert's demonstrations from generated ones -- and a
generator's policy to produce trajectories that can fool this discriminator.
This alternated optimization is known to be delicate in practice since it
compounds unstable adversarial training with brittle and sample-inefficient
reinforcement learning. We propose to remove the burden of the policy
optimization steps by leveraging a novel discriminator formulation.
Specifically, our discriminator is explicitly conditioned on two policies: the
one from the previous generator's iteration and a learnable policy. When
optimized, this discriminator directly learns the optimal generator's policy.
Consequently, our discriminator's update solves the generator's optimization
problem for free: learning a policy that imitates the expert does not require
an additional optimization loop. This formulation effectively cuts by half the
implementation and computational burden of Adversarial Imitation Learning
algorithms by removing the Reinforcement Learning phase altogether. We show on
a variety of tasks that our simpler approach is competitive to prevalent
Imitation Learning methods.
</p>
<a href="http://arxiv.org/abs/2006.13258" target="_blank">arXiv:2006.13258</a> [<a href="http://arxiv.org/pdf/2006.13258" target="_blank">pdf</a>]

<h2>A Flexible Framework for Designing Trainable Priors with Adaptive Smoothing and Game Encoding. (arXiv:2006.14859v2 [cs.CV] UPDATED)</h2>
<h3>Bruno Lecouat, Jean Ponce, Julien Mairal</h3>
<p>We introduce a general framework for designing and training neural network
layers whose forward passes can be interpreted as solving non-smooth convex
optimization problems, and whose architectures are derived from an optimization
algorithm. We focus on convex games, solved by local agents represented by the
nodes of a graph and interacting through regularization functions. This
approach is appealing for solving imaging problems, as it allows the use of
classical image priors within deep models that are trainable end to end. The
priors used in this presentation include variants of total variation, Laplacian
regularization, bilateral filtering, sparse coding on learned dictionaries, and
non-local self similarities. Our models are fully interpretable as well as
parameter and data efficient. Our experiments demonstrate their effectiveness
on a large diversity of tasks ranging from image denoising and compressed
sensing for fMRI to dense stereo matching.
</p>
<a href="http://arxiv.org/abs/2006.14859" target="_blank">arXiv:2006.14859</a> [<a href="http://arxiv.org/pdf/2006.14859" target="_blank">pdf</a>]

<h2>RELATE: Physically Plausible Multi-Object Scene Synthesis Using Structured Latent Spaces. (arXiv:2007.01272v2 [cs.CV] UPDATED)</h2>
<h3>Sebastien Ehrhardt, Oliver Groth, Aron Monszpart, Martin Engelcke, Ingmar Posner, Niloy Mitra, Andrea Vedaldi</h3>
<p>We present RELATE, a model that learns to generate physically plausible
scenes and videos of multiple interacting objects. Similar to other generative
approaches, RELATE is trained end-to-end on raw, unlabeled data. RELATE
combines an object-centric GAN formulation with a model that explicitly
accounts for correlations between individual objects. This allows the model to
generate realistic scenes and videos from a physically-interpretable
parameterization. Furthermore, we show that modeling the object correlation is
necessary to learn to disentangle object positions and identity. We find that
RELATE is also amenable to physically realistic scene editing and that it
significantly outperforms prior art in object-centric scene generation in both
synthetic (CLEVR, ShapeStacks) and real-world data (cars). In addition, in
contrast to state-of-the-art methods in object-centric generative modeling,
RELATE also extends naturally to dynamic scenes and generates videos of high
visual fidelity. Source code, datasets and more results are available at
this http URL
</p>
<a href="http://arxiv.org/abs/2007.01272" target="_blank">arXiv:2007.01272</a> [<a href="http://arxiv.org/pdf/2007.01272" target="_blank">pdf</a>]

<h2>RGBT Salient Object Detection: A Large-scale Dataset and Benchmark. (arXiv:2007.03262v3 [cs.CV] UPDATED)</h2>
<h3>Zhengzheng Tu, Yan Ma, Zhun Li, Chenglong Li, Jieming Xu, Yongtao Liu</h3>
<p>Salient object detection in complex scenes and environments is a challenging
research topic. Most works focus on RGB-based salient object detection, which
limits its performance of real-life applications when confronted with adverse
conditions such as dark environments and complex backgrounds. Taking advantage
of RGB and thermal infrared images becomes a new research direction for
detecting salient object in complex scenes recently, as thermal infrared
spectrum imaging provides the complementary information and has been applied to
many computer vision tasks. However, current research for RGBT salient object
detection is limited by the lack of a large-scale dataset and comprehensive
benchmark. This work contributes such a RGBT image dataset named VT5000,
including 5000 spatially aligned RGBT image pairs with ground truth
annotations. VT5000 has 11 challenges collected in different scenes and
environments for exploring the robustness of algorithms. With this dataset, we
propose a powerful baseline approach, which extracts multi-level features
within each modality and aggregates these features of all modalities with the
attention mechanism, for accurate RGBT salient object detection. Extensive
experiments show that the proposed baseline approach outperforms the
state-of-the-art methods on VT5000 dataset and other two public datasets. In
addition, we carry out a comprehensive analysis of different algorithms of RGBT
salient object detection on VT5000 dataset, and then make several valuable
conclusions and provide some potential research directions for RGBT salient
object detection.
</p>
<a href="http://arxiv.org/abs/2007.03262" target="_blank">arXiv:2007.03262</a> [<a href="http://arxiv.org/pdf/2007.03262" target="_blank">pdf</a>]

<h2>Synthetic-to-Real Domain Adaptation for Lane Detection. (arXiv:2007.04023v2 [cs.CV] UPDATED)</h2>
<h3>Noa Garnett, Roy Uziel, Netalee Efrat, Dan Levi</h3>
<p>Accurate lane detection, a crucial enabler for autonomous driving, currently
relies on obtaining a large and diverse labeled training dataset. In this work,
we explore learning from abundant, randomly generated synthetic data, together
with unlabeled or partially labeled target domain data, instead. Randomly
generated synthetic data has the advantage of controlled variability in the
lane geometry and lighting, but it is limited in terms of photo-realism. This
poses the challenge of adapting models learned on the unrealistic synthetic
domain to real images. To this end we develop a novel autoencoder-based
approach that uses synthetic labels unaligned with particular images for
adapting to target domain data. In addition, we explore existing domain
adaptation approaches, such as image translation and self-supervision, and
adjust them to the lane detection task. We test all approaches in the
unsupervised domain adaptation setting in which no target domain labels are
available and in the semi-supervised setting in which a small portion of the
target images are labeled. In extensive experiments using three different
datasets, we demonstrate the possibility to save costly target domain labeling
efforts. For example, using our proposed autoencoder approach on the llamas and
tuSimple lane datasets, we can almost recover the fully supervised accuracy
with only 10% of the labeled data. In addition, our autoencoder approach
outperforms all other methods in the semi-supervised domain adaptation
scenario.
</p>
<a href="http://arxiv.org/abs/2007.04023" target="_blank">arXiv:2007.04023</a> [<a href="http://arxiv.org/pdf/2007.04023" target="_blank">pdf</a>]

<h2>Differentiable Unsupervised Feature Selection based on a Gated Laplacian. (arXiv:2007.04728v3 [cs.LG] UPDATED)</h2>
<h3>Ofir Lindenbaum, Uri Shaham, Jonathan Svirsky, Erez Peterfreund, Yuval Kluger</h3>
<p>Scientific observations may consist of a large number of variables
(features). Identifying a subset of meaningful features is often ignored in
unsupervised learning, despite its potential for unraveling clear patterns
hidden in the ambient space. In this paper, we present a method for
unsupervised feature selection, and we demonstrate its use for the task of
clustering. We propose a differentiable loss function that combines the
Laplacian score, which favors low-frequency features, with a gating mechanism
for feature selection. We improve the Laplacian score, by replacing it with a
gated variant computed on a subset of features. This subset is obtained using a
continuous approximation of Bernoulli variables whose parameters are trained to
gate the full feature space. We mathematically motivate the proposed approach
and demonstrate that in the high noise regime, it is crucial to compute the
Laplacian on the gated inputs, rather than on the full feature set.
Experimental demonstration of the efficacy of the proposed approach and its
advantage over current baselines is provided using several real-world examples.
</p>
<a href="http://arxiv.org/abs/2007.04728" target="_blank">arXiv:2007.04728</a> [<a href="http://arxiv.org/pdf/2007.04728" target="_blank">pdf</a>]

<h2>To Filter Prune, or to Layer Prune, That Is The Question. (arXiv:2007.05667v3 [cs.CV] UPDATED)</h2>
<h3>Sara Elkerdawy, Mostafa Elhoushi, Abhineet Singh, Hong Zhang, Nilanjan Ray</h3>
<p>Recent advances in pruning of neural networks have made it possible to remove
a large number of filters or weights without any perceptible drop in accuracy.
The number of parameters and that of FLOPs are usually the reported metrics to
measure the quality of the pruned models. However, the gain in speed for these
pruned models is often overlooked in the literature due to the complex nature
of latency measurements. In this paper, we show the limitation of filter
pruning methods in terms of latency reduction and propose LayerPrune framework.
LayerPrune presents a set of layer pruning methods based on different criteria
that achieve higher latency reduction than filter pruning methods on similar
accuracy. The advantage of layer pruning over filter pruning in terms of
latency reduction is a result of the fact that the former is not constrained by
the original model's depth and thus allows for a larger range of latency
reduction. For each filter pruning method we examined, we use the same filter
importance criterion to calculate a per-layer importance score in one-shot. We
then prune the least important layers and fine-tune the shallower model which
obtains comparable or better accuracy than its filter-based pruning
counterpart. This one-shot process allows to remove layers from single path
networks like VGG before fine-tuning, unlike in iterative filter pruning, a
minimum number of filters per layer is required to allow for data flow which
constraint the search space. To the best of our knowledge, we are the first to
examine the effect of pruning methods on latency metric instead of FLOPs for
multiple networks, datasets and hardware targets. LayerPrune also outperforms
handcrafted architectures such as Shufflenet, MobileNet, MNASNet and ResNet18
by 7.3%, 4.6%, 2.8% and 0.5% respectively on similar latency budget on ImageNet
dataset.
</p>
<a href="http://arxiv.org/abs/2007.05667" target="_blank">arXiv:2007.05667</a> [<a href="http://arxiv.org/pdf/2007.05667" target="_blank">pdf</a>]

<h2>Transferable Calibration with Lower Bias and Variance in Domain Adaptation. (arXiv:2007.08259v2 [cs.LG] UPDATED)</h2>
<h3>Ximei Wang, Mingsheng Long, Jianmin Wang, Michael I. Jordan</h3>
<p>Domain Adaptation (DA) enables transferring a learning machine from a labeled
source domain to an unlabeled target one. While remarkable advances have been
made, most of the existing DA methods focus on improving the target accuracy at
inference. How to estimate the predictive uncertainty of DA models is vital for
decision-making in safety-critical scenarios but remains the boundary to
explore. In this paper, we delve into the open problem of Calibration in DA,
which is extremely challenging due to the coexistence of domain shift and the
lack of target labels. We first reveal the dilemma that DA models learn higher
accuracy at the expense of well-calibrated probabilities. Driven by this
finding, we propose Transferable Calibration (TransCal) to achieve more
accurate calibration with lower bias and variance in a unified
hyperparameter-free optimization framework. As a general post-hoc calibration
method, TransCal can be easily applied to recalibrate existing DA methods. Its
efficacy has been justified both theoretically and empirically.
</p>
<a href="http://arxiv.org/abs/2007.08259" target="_blank">arXiv:2007.08259</a> [<a href="http://arxiv.org/pdf/2007.08259" target="_blank">pdf</a>]

<h2>Resolution Switchable Networks for Runtime Efficient Image Recognition. (arXiv:2007.09558v3 [cs.CV] UPDATED)</h2>
<h3>Yikai Wang, Fuchun Sun, Duo Li, Anbang Yao</h3>
<p>We propose a general method to train a single convolutional neural network
which is capable of switching image resolutions at inference. Thus the running
speed can be selected to meet various computational resource limits. Networks
trained with the proposed method are named Resolution Switchable Networks
(RS-Nets). The basic training framework shares network parameters for handling
images which differ in resolution, yet keeps separate batch normalization
layers. Though it is parameter-efficient in design, it leads to inconsistent
accuracy variations at different resolutions, for which we provide a detailed
analysis from the aspect of the train-test recognition discrepancy. A
multi-resolution ensemble distillation is further designed, where a teacher is
learnt on the fly as a weighted ensemble over resolutions. Thanks to the
ensemble and knowledge distillation, RS-Nets enjoy accuracy improvements at a
wide range of resolutions compared with individually trained models. Extensive
experiments on the ImageNet dataset are provided, and we additionally consider
quantization problems. Code and models are available at
https://github.com/yikaiw/RS-Nets.
</p>
<a href="http://arxiv.org/abs/2007.09558" target="_blank">arXiv:2007.09558</a> [<a href="http://arxiv.org/pdf/2007.09558" target="_blank">pdf</a>]

<h2>Bayesian Robust Optimization for Imitation Learning. (arXiv:2007.12315v3 [cs.LG] UPDATED)</h2>
<h3>Daniel S. Brown, Scott Niekum, Marek Petrik</h3>
<p>One of the main challenges in imitation learning is determining what action
an agent should take when outside the state distribution of the demonstrations.
Inverse reinforcement learning (IRL) can enable generalization to new states by
learning a parameterized reward function, but these approaches still face
uncertainty over the true reward function and corresponding optimal policy.
Existing safe imitation learning approaches based on IRL deal with this
uncertainty using a maxmin framework that optimizes a policy under the
assumption of an adversarial reward function, whereas risk-neutral IRL
approaches either optimize a policy for the mean or MAP reward function. While
completely ignoring risk can lead to overly aggressive and unsafe policies,
optimizing in a fully adversarial sense is also problematic as it can lead to
overly conservative policies that perform poorly in practice. To provide a
bridge between these two extremes, we propose Bayesian Robust Optimization for
Imitation Learning (BROIL). BROIL leverages Bayesian reward function inference
and a user specific risk tolerance to efficiently optimize a robust policy that
balances expected return and conditional value at risk. Our empirical results
show that BROIL provides a natural way to interpolate between return-maximizing
and risk-minimizing behaviors and outperforms existing risk-sensitive and
risk-neutral inverse reinforcement learning algorithms. Code is available at
https://github.com/dsbrown1331/broil.
</p>
<a href="http://arxiv.org/abs/2007.12315" target="_blank">arXiv:2007.12315</a> [<a href="http://arxiv.org/pdf/2007.12315" target="_blank">pdf</a>]

<h2>CAMPs: Learning Context-Specific Abstractions for Efficient Planning in Factored MDPs. (arXiv:2007.13202v3 [cs.LG] UPDATED)</h2>
<h3>Rohan Chitnis, Tom Silver, Beomjoon Kim, Leslie Pack Kaelbling, Tomas Lozano-Perez</h3>
<p>Meta-planning, or learning to guide planning from experience, is a promising
approach to improving the computational cost of planning. A general
meta-planning strategy is to learn to impose constraints on the states
considered and actions taken by the agent. We observe that (1) imposing a
constraint can induce context-specific independences that render some aspects
of the domain irrelevant, and (2) an agent can take advantage of this fact by
imposing constraints on its own behavior. These observations lead us to propose
the context-specific abstract Markov decision process (CAMP), an abstraction of
a factored MDP that affords efficient planning. We then describe how to learn
constraints to impose so the CAMP optimizes a trade-off between rewards and
computational cost. Our experiments consider five planners across four domains,
including robotic navigation among movable obstacles (NAMO), robotic task and
motion planning for sequential manipulation, and classical planning. We find
planning with learned CAMPs to consistently outperform baselines, including
Stilman's NAMO-specific algorithm. Video: https://youtu.be/wTXt6djcAd4 Code:
https://git.io/JTnf6
</p>
<a href="http://arxiv.org/abs/2007.13202" target="_blank">arXiv:2007.13202</a> [<a href="http://arxiv.org/pdf/2007.13202" target="_blank">pdf</a>]

<h2>FedML: A Research Library and Benchmark for Federated Machine Learning. (arXiv:2007.13518v4 [cs.LG] UPDATED)</h2>
<h3>Chaoyang He, Songze Li, Jinhyun So, Xiao Zeng, Mi Zhang, Hongyi Wang, Xiaoyang Wang, Praneeth Vepakomma, Abhishek Singh, Hang Qiu, Xinghua Zhu, Jianzong Wang, Li Shen, Peilin Zhao, Yan Kang, Yang Liu, Ramesh Raskar, Qiang Yang, Murali Annavaram, Salman Avestimehr</h3>
<p>Federated learning (FL) is a rapidly growing research field in machine
learning. However, existing FL libraries cannot adequately support diverse
algorithmic development; inconsistent dataset and model usage make fair
algorithm comparison challenging. In this work, we introduce FedML, an open
research library and benchmark to facilitate FL algorithm development and fair
performance comparison. FedML supports three computing paradigms: on-device
training for edge devices, distributed computing, and single-machine
simulation. FedML also promotes diverse algorithmic research with flexible and
generic API design and comprehensive reference baseline implementations
(optimizer, models, and datasets). We hope FedML could provide an efficient and
reproducible means for developing and evaluating FL algorithms that would
benefit the FL research community. We maintain the source code, documents, and
user community at https://fedml.ai.
</p>
<a href="http://arxiv.org/abs/2007.13518" target="_blank">arXiv:2007.13518</a> [<a href="http://arxiv.org/pdf/2007.13518" target="_blank">pdf</a>]

<h2>Maximum Mutation Reinforcement Learning for Scalable Control. (arXiv:2007.13690v4 [cs.LG] UPDATED)</h2>
<h3>Karush Suri, Xiao Qi Shi, Konstantinos N. Plataniotis, Yuri A. Lawryshyn</h3>
<p>Advances in Reinforcement Learning (RL) have successfully tackled sample
efficiency and overestimation bias. However, these methods often fall short of
scalable performance. On the other hand, genetic methods provide scalability
but depict hyperparameter sensitivity to evolutionary operations. We present
the Evolution-based Soft Actor-Critic (ESAC), a scalable RL algorithm. Our
contributions are threefold; ESAC (1) abstracts exploration from exploitation
by combining Evolution Strategies (ES) with Soft Actor-Critic (SAC), (2)
provides dominant skill transfer between offsprings by making use of soft
winner selections and genetic crossovers in hindsight and (3) improves
hyperparameter sensitivity in evolutions using Automatic Mutation Tuning (AMT).
AMT gradually replaces the entropy framework of SAC allowing the population to
succeed at the task while acting as randomly as possible, without making use of
backpropagation updates. On a range of challenging control tasks consisting of
high-dimensional action spaces and sparse rewards, ESAC demonstrates
state-of-the-art performance and sample efficiency equivalent to SAC. ESAC
demonstrates scalability comparable to ES on the basis of hardware resources
and algorithm overhead. A complete implementation of ESAC with notes on
reproducibility and videos can be found at the project website
https://karush17.github.io/esac-web/.
</p>
<a href="http://arxiv.org/abs/2007.13690" target="_blank">arXiv:2007.13690</a> [<a href="http://arxiv.org/pdf/2007.13690" target="_blank">pdf</a>]

<h2>F*: An Interpretable Transformation of the F-measure. (arXiv:2008.00103v2 [cs.LG] UPDATED)</h2>
<h3>David J. Hand, Peter Christen, Nishadi Kirielle</h3>
<p>The F-measure, also known as the F1-score, is widely used to assess the
performance of classification algorithms. However, some researchers find it
lacking in intuitive interpretation, questioning the appropriateness of
combining two aspects of performance as conceptually distinct as precision and
recall, and also questioning whether the harmonic mean is the best way to
combine them. To ease this concern, we describe a simple transformation of the
F-measure, which we call F* (F-star), which has an immediate practical
interpretation.
</p>
<a href="http://arxiv.org/abs/2008.00103" target="_blank">arXiv:2008.00103</a> [<a href="http://arxiv.org/pdf/2008.00103" target="_blank">pdf</a>]

<h2>COALESCE: Component Assembly by Learning to Synthesize Connections. (arXiv:2008.01936v2 [cs.CV] UPDATED)</h2>
<h3>Kangxue Yin, Zhiqin Chen, Siddhartha Chaudhuri, Matthew Fisher, Vladimir G. Kim, Hao Zhang</h3>
<p>We introduce COALESCE, the first data-driven framework for component-based
shape assembly which employs deep learning to synthesize part connections. To
handle geometric and topological mismatches between parts, we remove the
mismatched portions via erosion, and rely on a joint synthesis step, which is
learned from data, to fill the gap and arrive at a natural and plausible part
joint. Given a set of input parts extracted from different objects, COALESCE
automatically aligns them and synthesizes plausible joints to connect the parts
into a coherent 3D object represented by a mesh. The joint synthesis network,
designed to focus on joint regions, reconstructs the surface between the parts
by predicting an implicit shape representation that agrees with existing parts,
while generating a smooth and topologically meaningful connection. We employ
test-time optimization to further ensure that the synthesized joint region
closely aligns with the input parts to create realistic component assemblies
from diverse input parts. We demonstrate that our method significantly
outperforms prior approaches including baseline deep models for 3D shape
synthesis, as well as state-of-the-art methods for shape completion.
</p>
<a href="http://arxiv.org/abs/2008.01936" target="_blank">arXiv:2008.01936</a> [<a href="http://arxiv.org/pdf/2008.01936" target="_blank">pdf</a>]

<h2>Contrastive Variational Reinforcement Learning for Complex Observations. (arXiv:2008.02430v2 [cs.LG] UPDATED)</h2>
<h3>Xiao Ma, Siwei Chen, David Hsu, Wee Sun Lee</h3>
<p>Deep reinforcement learning (DRL) has achieved significant success in various
robot tasks: manipulation, navigation, etc. However, complex visual
observations in natural environments remains a major challenge. This paper
presents Contrastive Variational Reinforcement Learning (CVRL), a model-based
method that tackles complex visual observations in DRL. CVRL learns a
contrastive variational model by maximizing the mutual information between
latent states and observations discriminatively, through contrastive learning.
It avoids modeling the complex observation space unnecessarily, as the commonly
used generative observation model often does, and is significantly more robust.
CVRL achieves comparable performance with state-of-the-art model-based DRL
methods on standard Mujoco tasks. It significantly outperforms them on Natural
Mujoco tasks and a robot box-pushing task with complex observations, e.g.,
dynamic shadows. The CVRL code is available publicly at
https://github.com/Yusufma03/CVRL.
</p>
<a href="http://arxiv.org/abs/2008.02430" target="_blank">arXiv:2008.02430</a> [<a href="http://arxiv.org/pdf/2008.02430" target="_blank">pdf</a>]

<h2>TGK-Planner: An Efficient Topology Guided Kinodynamic Planner for Autonomous Quadrotors. (arXiv:2008.03468v2 [cs.RO] UPDATED)</h2>
<h3>Hongkai Ye, Xin Zhou, Zhepei Wang, Chao Xu, Jian Chu, Fei Gao</h3>
<p>In this paper, we propose a lightweight yet effective Topology Guided
Kinodynamic planner (TGK-Planner) for quadrotor aggressive flights with limited
onboard computing resources. The proposed system follows the traditional
hierarchical planning workflow, with novel designs to improve the robustness
and efficiency in both the pathfinding and trajectory optimization sub-modules.
Firstly, we propose the topology guided graph, which roughly captures the
topological structure of the environment and guides the state sampling of a
sampling-based kinodynamic planner. In this way, we significantly improve the
efficiency of finding a safe and dynamically feasible trajectory. Then, we
refine the smoothness and continuity of the trajectory in an optimization
framework, which incorporates the homotopy constraint to guarantee the safety
of the trajectory. The optimization program is formulated as a sequence of
quadratic programmings (QPs) and can be iteratively solved in a few
milliseconds. Finally, the proposed system is integrated into a fully
autonomous quadrotor and validated in various simulated and real-world
scenarios. Benchmark comparisons show that our method outperforms
state-of-the-art methods with regard to efficiency and trajectory quality.
Moreover, we will release our code as an open-source package.
</p>
<a href="http://arxiv.org/abs/2008.03468" target="_blank">arXiv:2008.03468</a> [<a href="http://arxiv.org/pdf/2008.03468" target="_blank">pdf</a>]

<h2>Deep learning for photoacoustic imaging: a survey. (arXiv:2008.04221v3 [cs.CV] UPDATED)</h2>
<h3>Changchun Yang, Hengrong Lan, Feng Gao, Fei Gao</h3>
<p>Machine learning has been developed dramatically and witnessed a lot of
applications in various fields over the past few years. This boom originated in
2009, when a new model emerged, that is, the deep artificial neural network,
which began to surpass other established mature models on some important
benchmarks. Later, it was widely used in academia and industry. Ranging from
image analysis to natural language processing, it fully exerted its magic and
now become the state-of-the-art machine learning models. Deep neural networks
have great potential in medical imaging technology, medical data analysis,
medical diagnosis and other healthcare issues, and is promoted in both
pre-clinical and even clinical stages. In this review, we performed an overview
of some new developments and challenges in the application of machine learning
to medical image analysis, with a special focus on deep learning in
photoacoustic imaging. The aim of this review is threefold: (i) introducing
deep learning with some important basics, (ii) reviewing recent works that
apply deep learning in the entire ecological chain of photoacoustic imaging,
from image reconstruction to disease diagnosis, (iii) providing some open
source materials and other resources for researchers interested in applying
deep learning to photoacoustic imaging.
</p>
<a href="http://arxiv.org/abs/2008.04221" target="_blank">arXiv:2008.04221</a> [<a href="http://arxiv.org/pdf/2008.04221" target="_blank">pdf</a>]

<h2>Hardware as Policy: Mechanical and Computational Co-Optimization using Deep Reinforcement Learning. (arXiv:2008.04460v2 [cs.RO] UPDATED)</h2>
<h3>Tianjian Chen, Zhanpeng He, Matei Ciocarlie</h3>
<p>Deep Reinforcement Learning (RL) has shown great success in learning complex
control policies for a variety of applications in robotics. However, in most
such cases, the hardware of the robot has been considered immutable, modeled as
part of the environment. In this study, we explore the problem of learning
hardware and control parameters together in a unified RL framework. To achieve
this, we propose to model the robot body as a "hardware policy", analogous to
and optimized jointly with its computational counterpart. We show that, by
modeling such hardware policies as auto-differentiable computational graphs,
the ensuing optimization problem can be solved efficiently by gradient-based
algorithms from the Policy Optimization family. We present two such design
examples: a toy mass-spring problem, and a real-world problem of designing an
underactuated hand. We compare our method against traditional co-optimization
approaches, and also demonstrate its effectiveness by building a physical
prototype based on the learned hardware parameters. Videos and more details are
available at https://roamlab.github.io/hwasp/ .
</p>
<a href="http://arxiv.org/abs/2008.04460" target="_blank">arXiv:2008.04460</a> [<a href="http://arxiv.org/pdf/2008.04460" target="_blank">pdf</a>]

<h2>R-MNet: A Perceptual Adversarial Network for Image Inpainting. (arXiv:2008.04621v3 [cs.CV] UPDATED)</h2>
<h3>Jireh Jam, Connah Kendrick, Vincent Drouard, Kevin Walker, Gee-Sern Hsu, Moi Hoon Yap</h3>
<p>Facial image inpainting is a problem that is widely studied, and in recent
years the introduction of Generative Adversarial Networks, has led to
improvements in the field. Unfortunately some issues persists, in particular
when blending the missing pixels with the visible ones. We address the problem
by proposing a Wasserstein GAN combined with a new reverse mask operator,
namely Reverse Masking Network (R-MNet), a perceptual adversarial network for
image inpainting. The reverse mask operator transfers the reverse masked image
to the end of the encoder-decoder network leaving only valid pixels to be
inpainted. Additionally, we propose a new loss function computed in feature
space to target only valid pixels combined with adversarial training. These
then capture data distributions and generate images similar to those in the
training data with achieved realism (realistic and coherent) on the output
images. We evaluate our method on publicly available dataset, and compare with
state-of-the-art methods. We show that our method is able to generalize to
high-resolution inpainting task, and further show more realistic outputs that
are plausible to the human visual system when compared with the
state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2008.04621" target="_blank">arXiv:2008.04621</a> [<a href="http://arxiv.org/pdf/2008.04621" target="_blank">pdf</a>]

<h2>PLACE: Proximity Learning of Articulation and Contact in 3D Environments. (arXiv:2008.05570v3 [cs.CV] UPDATED)</h2>
<h3>Siwei Zhang, Yan Zhang, Qianli Ma, Michael J. Black, Siyu Tang</h3>
<p>High fidelity digital 3D environments have been proposed in recent years,
however, it remains extremely challenging to automatically equip such
environment with realistic human bodies. Existing work utilizes images, depth
or semantic maps to represent the scene, and parametric human models to
represent 3D bodies. While being straightforward, their generated human-scene
interactions are often lack of naturalness and physical plausibility. Our key
observation is that humans interact with the world through body-scene contact.
To synthesize realistic human-scene interactions, it is essential to
effectively represent the physical contact and proximity between the body and
the world. To that end, we propose a novel interaction generation method, named
PLACE (Proximity Learning of Articulation and Contact in 3D Environments),
which explicitly models the proximity between the human body and the 3D scene
around it. Specifically, given a set of basis points on a scene mesh, we
leverage a conditional variational autoencoder to synthesize the minimum
distances from the basis points to the human body surface. The generated
proximal relationship exhibits which region of the scene is in contact with the
person. Furthermore, based on such synthesized proximity, we are able to
effectively obtain expressive 3D human bodies that interact with the 3D scene
naturally. Our perceptual study shows that PLACE significantly improves the
state-of-the-art method, approaching the realism of real human-scene
interaction. We believe our method makes an important step towards the fully
automatic synthesis of realistic 3D human bodies in 3D scenes. The code and
model are available for research at
https://sanweiliti.github.io/PLACE/PLACE.html.
</p>
<a href="http://arxiv.org/abs/2008.05570" target="_blank">arXiv:2008.05570</a> [<a href="http://arxiv.org/pdf/2008.05570" target="_blank">pdf</a>]

<h2>SMPLpix: Neural Avatars from 3D Human Models. (arXiv:2008.06872v2 [cs.CV] UPDATED)</h2>
<h3>Sergey Prokudin, Michael J. Black, Javier Romero</h3>
<p>Recent advances in deep generative models have led to an unprecedented level
of realism for synthetically generated images of humans. However, one of the
remaining fundamental limitations of these models is the ability to flexibly
control the generative process, e.g.~change the camera and human pose while
retaining the subject identity. At the same time, deformable human body models
like SMPL and its successors provide full control over pose and shape but rely
on classic computer graphics pipelines for rendering. Such rendering pipelines
require explicit mesh rasterization that (a) does not have the potential to fix
artifacts or lack of realism in the original 3D geometry and (b) until
recently, were not fully incorporated into deep learning frameworks. In this
work, we propose to bridge the gap between classic geometry-based rendering and
the latest generative networks operating in pixel space. We train a network
that directly converts a sparse set of 3D mesh vertices into photorealistic
images, alleviating the need for traditional rasterization mechanism. We train
our model on a large corpus of human 3D models and corresponding real photos,
and show the advantage over conventional differentiable renderers both in terms
of the level of photorealism and rendering efficiency.
</p>
<a href="http://arxiv.org/abs/2008.06872" target="_blank">arXiv:2008.06872</a> [<a href="http://arxiv.org/pdf/2008.06872" target="_blank">pdf</a>]

<h2>Action-Based Representation Learning for Autonomous Driving. (arXiv:2008.09417v2 [cs.CV] UPDATED)</h2>
<h3>Yi Xiao, Felipe Codevilla, Christopher Pal, Antonio M. Lopez</h3>
<p>Human drivers produce a vast amount of data which could, in principle, be
used to improve autonomous driving systems. Unfortunately, seemingly
straightforward approaches for creating end-to-end driving models that map
sensor data directly into driving actions are problematic in terms of
interpretability, and typically have significant difficulty dealing with
spurious correlations. Alternatively, we propose to use this kind of
action-based driving data for learning representations. Our experiments show
that an affordance-based driving model pre-trained with this approach can
leverage a relatively small amount of weakly annotated imagery and outperform
pure end-to-end driving models, while being more interpretable. Further, we
demonstrate how this strategy outperforms previous methods based on learning
inverse dynamics models as well as other methods based on heavy human
supervision (ImageNet).
</p>
<a href="http://arxiv.org/abs/2008.09417" target="_blank">arXiv:2008.09417</a> [<a href="http://arxiv.org/pdf/2008.09417" target="_blank">pdf</a>]

<h2>Variational Autoencoder for Anti-Cancer Drug Response Prediction. (arXiv:2008.09763v5 [cs.LG] UPDATED)</h2>
<h3>Hongyuan Dong, Jiaqing Xie, Zhi Jing, Dexin Ren</h3>
<p>Cancer has long been a main cause of human death, and the discovery of new
drugs and the customization of cancer therapy have puzzled people for a long
time. In order to facilitate the discovery of new anti-cancer drugs and the
customization of treatment strategy, we seek to predict the response of
different anti-cancer drugs with variational autoencoders (VAE) and multi-layer
perceptron (MLP).Our model takes as input gene expression data of cancer cell
lines and anti-cancer drug molecular data, and encode these data with {\sc
{GeneVae}} model, which is an ordinary VAE, and rectified junction tree
variational autoencoder ({\sc JtVae}) (\cite{jin2018junction}) model,
respectively. Encoded features are processes by a Multi-layer Perceptron (MLP)
model to produce a final prediction. We reach an average coefficient of
determination ($R^{2} = 0.83$) in predicting drug response on breast cancer
cell lines and an average $R^{2} &gt; 0.84$ on pan-cancer cell lines.
Additionally, we show that our model can generate unseen effective drug
compounds for specific cancer cell lines.
</p>
<a href="http://arxiv.org/abs/2008.09763" target="_blank">arXiv:2008.09763</a> [<a href="http://arxiv.org/pdf/2008.09763" target="_blank">pdf</a>]

<h2>Self-Supervised Learning for Large-Scale Unsupervised Image Clustering. (arXiv:2008.10312v2 [cs.CV] UPDATED)</h2>
<h3>Evgenii Zheltonozhskii, Chaim Baskin, Alex M. Bronstein, Avi Mendelson</h3>
<p>Unsupervised learning has always been appealing to machine learning
researchers and practitioners, allowing them to avoid an expensive and
complicated process of labeling the data. However, unsupervised learning of
complex data is challenging, and even the best approaches show much weaker
performance than their supervised counterparts. Self-supervised deep learning
has become a strong instrument for representation learning in computer vision.
However, those methods have not been evaluated in a fully unsupervised setting.
In this paper, we propose a simple scheme for unsupervised classification based
on self-supervised representations. We evaluate the proposed approach with
several recent self-supervised methods showing that it achieves competitive
results for ImageNet classification (39% accuracy on ImageNet with 1000
clusters and 46% with overclustering). We suggest adding the unsupervised
evaluation to a set of standard benchmarks for self-supervised learning. The
code is available at https://github.com/Randl/kmeans_selfsuper
</p>
<a href="http://arxiv.org/abs/2008.10312" target="_blank">arXiv:2008.10312</a> [<a href="http://arxiv.org/pdf/2008.10312" target="_blank">pdf</a>]

<h2>Learning Obstacle Representations for Neural Motion Planning. (arXiv:2008.11174v4 [cs.RO] UPDATED)</h2>
<h3>Robin Strudel, Ricardo Garcia, Justin Carpentier, Jean-Paul Laumond, Ivan Laptev, Cordelia Schmid</h3>
<p>Motion planning and obstacle avoidance is a key challenge in robotics
applications. While previous work succeeds to provide excellent solutions for
known environments, sensor-based motion planning in new and dynamic
environments remains difficult. In this work we address sensor-based motion
planning from a learning perspective. Motivated by recent advances in visual
recognition, we argue the importance of learning appropriate representations
for motion planning. We propose a new obstacle representation based on the
PointNet architecture and train it jointly with policies for obstacle
avoidance. We experimentally evaluate our approach for rigid body motion
planning in challenging environments and demonstrate significant improvements
of the state of the art in terms of accuracy and efficiency.
</p>
<a href="http://arxiv.org/abs/2008.11174" target="_blank">arXiv:2008.11174</a> [<a href="http://arxiv.org/pdf/2008.11174" target="_blank">pdf</a>]

<h2>Identifying Critical States by the Action-Based Variance of Expected Return. (arXiv:2008.11332v2 [stat.ML] UPDATED)</h2>
<h3>Izumi Karino, Yoshiyuki Ohmura, Yasuo Kuniyoshi</h3>
<p>The balance of exploration and exploitation plays a crucial role in
accelerating reinforcement learning (RL). To deploy an RL agent in human
society, its explainability is also essential. However, basic RL approaches
have difficulties in deciding when to choose exploitation as well as in
extracting useful points for a brief explanation of its operation. One reason
for the difficulties is that these approaches treat all states the same way.
Here, we show that identifying critical states and treating them specially is
commonly beneficial to both problems. These critical states are the states at
which the action selection changes the potential of success and failure
substantially. We propose to identify the critical states using the variance in
the Q-function for the actions and to perform exploitation with high
probability on the identified states. These simple methods accelerate RL in a
grid world with cliffs and two baseline tasks of deep RL. Our results also
demonstrate that the identified critical states are intuitively interpretable
regarding the crucial nature of the action selection. Furthermore, our analysis
of the relationship between the timing of the identification of especially
critical states and the rapid progress of learning suggests there are a few
especially critical states that have important information for accelerating RL
rapidly.
</p>
<a href="http://arxiv.org/abs/2008.11332" target="_blank">arXiv:2008.11332</a> [<a href="http://arxiv.org/pdf/2008.11332" target="_blank">pdf</a>]

<h2>$\beta$-Cores: Robust Large-Scale Bayesian Data Summarization in the Presence of Outliers. (arXiv:2008.13600v2 [cs.LG] UPDATED)</h2>
<h3>Dionysis Manousakas, Cecilia Mascolo</h3>
<p>Modern machine learning applications should be able to address the intrinsic
challenges arising over inference on massive real-world datasets, including
scalability and robustness to outliers. Despite the multiple benefits of
Bayesian methods (such as uncertainty-aware predictions, incorporation of
experts knowledge, and hierarchical modeling), the quality of classic Bayesian
inference depends critically on whether observations conform with the assumed
data generating model, which is impossible to guarantee in practice. In this
work, we propose a variational inference method that, in a principled way, can
simultaneously scale to large datasets, and robustify the inferred posterior
with respect to the existence of outliers in the observed data. Reformulating
Bayes theorem via the $\beta$-divergence, we posit a robustified
pseudo-Bayesian posterior as the target of inference. Moreover, relying on the
recent formulations of Riemannian coresets for scalable Bayesian inference, we
propose a sparse variational approximation of the robustified posterior and an
efficient stochastic black-box algorithm to construct it. Overall our method
allows releasing cleansed data summaries that can be applied broadly in
scenarios including structured data corruption. We illustrate the applicability
of our approach in diverse simulated and real datasets, and various statistical
models, including Gaussian mean inference, logistic and neural linear
regression, demonstrating its superiority to existing Bayesian summarization
methods in the presence of outliers.
</p>
<a href="http://arxiv.org/abs/2008.13600" target="_blank">arXiv:2008.13600</a> [<a href="http://arxiv.org/pdf/2008.13600" target="_blank">pdf</a>]

<h2>Overcoming Negative Transfer: A Survey. (arXiv:2009.00909v2 [cs.LG] UPDATED)</h2>
<h3>Wen Zhang, Lingfei Deng, Lei Zhang, Dongrui Wu</h3>
<p>Transfer learning (TL) tries to utilize data or knowledge from one or more
source domains to facilitate the learning in a target domain. It is
particularly useful when the target domain has few or no labeled data, due to
annotation expense, privacy concerns, etc. Unfortunately, the effectiveness of
TL is not always guaranteed. Negative transfer (NT), i.e., the source domain
data/knowledge cause reduced learning performance in the target domain, has
been a long-standing and challenging problem in TL. Various approaches to
overcome NT have been proposed in the literature. However, there has not been a
systematic survey on overcoming NT. This paper fills the gap, by categorizing
and reviewing near 100 approaches for combating NT, from four perspectives:
source data quality, target data quality, domain divergence, and integrated
algorithms. NT in related fields, e.g., multi-task learning, multilingual
models, and lifelong learning, is also discussed.
</p>
<a href="http://arxiv.org/abs/2009.00909" target="_blank">arXiv:2009.00909</a> [<a href="http://arxiv.org/pdf/2009.00909" target="_blank">pdf</a>]

<h2>Bounded Risk-Sensitive Markov Game and Its Inverse Reward Learning Problem. (arXiv:2009.01495v3 [cs.LG] UPDATED)</h2>
<h3>Ran Tian, Liting Sun, Masayoshi Tomizuka</h3>
<p>Classical game-theoretic approaches for multi-agent systems in both the
forward policy design problem and the inverse reward learning problem often
make strong rationality assumptions: agents perfectly maximize expected
utilities under uncertainties. Such assumptions, however, substantially
mismatch with observed humans' behaviors such as satisficing with sub-optimal,
risk-seeking, and loss-aversion decisions. In this paper, we investigate the
problem of bounded risk-sensitive Markov Game (BRSMG) and its inverse reward
learning problem. {Drawing on iterative reasoning models and cumulative
prospect theory, we embrace that humans have bounded intelligence and maximize
risk-sensitive utilities in BRSMGs.} Convergence analysis for both the forward
policy design and the inverse reward learning problems are established under
the BRSMG framework. We also validate the proposed forward policy design and
inverse reward learning algorithms in a navigation scenario. The results show
that the behaviors of agents demonstrate both risk-averse and risk-seeking
characteristics. Moreover, in the inverse reward learning task, the proposed
bounded risk-sensitive inverse learning algorithm outperforms a baseline
risk-neutral inverse learning algorithm by effectively recovering not only more
accurate reward values but also the intelligence levels and the risk-measure
parameters given demonstrations of agents' interactive behaviors.
</p>
<a href="http://arxiv.org/abs/2009.01495" target="_blank">arXiv:2009.01495</a> [<a href="http://arxiv.org/pdf/2009.01495" target="_blank">pdf</a>]

<h2>Neuro-symbolic Neurodegenerative Disease Modeling as Probabilistic Programmed Deep Kernels. (arXiv:2009.07738v2 [cs.LG] UPDATED)</h2>
<h3>Alexander Lavin</h3>
<p>We present a probabilistic programmed deep kernel learning approach to
personalized, predictive modeling of neurodegenerative diseases. Our analysis
considers a spectrum of neural and symbolic machine learning approaches, which
we assess for predictive performance and important medical AI properties such
as interpretability, uncertainty reasoning, data-efficiency, and leveraging
domain knowledge. Our Bayesian approach combines the flexibility of Gaussian
processes with the structural power of neural networks to model biomarker
progressions, without needing clinical labels for training. We run evaluations
on the problem of Alzheimer's disease prediction, yielding results surpassing
deep learning and with the practical advantages of Bayesian non-parametrics and
probabilistic programming.
</p>
<a href="http://arxiv.org/abs/2009.07738" target="_blank">arXiv:2009.07738</a> [<a href="http://arxiv.org/pdf/2009.07738" target="_blank">pdf</a>]

<h2>MFIF-GAN: A New Generative Adversarial Network for Multi-Focus Image Fusion. (arXiv:2009.09718v4 [cs.CV] UPDATED)</h2>
<h3>Yicheng Wang, Shuang Xu, Junmin Liu, Zixiang Zhao, Chunxia Zhang, Jiangshe Zhang</h3>
<p>Multi-Focus Image Fusion (MFIF) is a promising image enhancement technique to
obtain all-in-focus images meeting visual needs and it is a precondition of
other computer vision tasks. One of the research trends of MFIF is to avoid the
defocus spread effect (DSE) around the focus/defocus boundary (FDB). In this
paper,we propose a network termed MFIF-GAN to attenuate the DSE by generating
focus maps in which the foreground region are correctly larger than the
corresponding objects. The Squeeze and Excitation Residual module is employed
in the network. By combining the prior knowledge of training condition, this
network is trained on a synthetic dataset based on an {\alpha}-matte model. In
addition, the reconstruction and gradient regularization terms are combined in
the loss functions to enhance the boundary details and improve the quality of
fused images. Extensive experiments demonstrate that the MFIF-GAN outperforms
several state-of-the-art (SOTA) methods in visual perception, quantitative
analysis as well as efficiency. Moreover, the edge diffusion and contraction
module is firstly proposed to verify that focus maps generated by our method
are accurate at the pixel level.
</p>
<a href="http://arxiv.org/abs/2009.09718" target="_blank">arXiv:2009.09718</a> [<a href="http://arxiv.org/pdf/2009.09718" target="_blank">pdf</a>]

<h2>Improving Point Cloud Semantic Segmentation by Learning 3D Object Detection. (arXiv:2009.10569v3 [cs.CV] UPDATED)</h2>
<h3>Ozan Unal, Luc Van Gool, Dengxin Dai</h3>
<p>Point cloud semantic segmentation plays an essential role in autonomous
driving, providing vital information about drivable surfaces and nearby objects
that can aid higher level tasks such as path planning and collision avoidance.
While current 3D semantic segmentation networks focus on convolutional
architectures that perform great for well represented classes, they show a
significant drop in performance for underrepresented classes that share similar
geometric features. We propose a novel Detection Aware 3D Semantic Segmentation
(DASS) framework that explicitly leverages localization features from an
auxiliary 3D object detection task. By utilizing multitask training, the shared
feature representation of the network is guided to be aware of per class
detection features that aid tackling the differentiation of geometrically
similar classes. We additionally provide a pipeline that uses DASS to generate
high recall proposals for existing 2-stage detectors and demonstrate that the
added supervisory signal can be used to improve 3D orientation estimation
capabilities. Extensive experiments on both the SemanticKITTI and KITTI object
datasets show that DASS can improve 3D semantic segmentation results of
geometrically similar classes up to 37.8% IoU in image FOV while maintaining
high precision bird's-eye view (BEV) detection results.
</p>
<a href="http://arxiv.org/abs/2009.10569" target="_blank">arXiv:2009.10569</a> [<a href="http://arxiv.org/pdf/2009.10569" target="_blank">pdf</a>]

<h2>Online Action Learning in High Dimensions: A New Exploration Rule for Contextual $\epsilon_t$-Greedy Heuristics. (arXiv:2009.13961v2 [stat.ML] UPDATED)</h2>
<h3>Claudio Cardoso Flores, Marcelo Cunha Medeiros</h3>
<p>Bandit problems are pervasive in various fields of research and are also
present in several practical applications. Examples, including dynamic pricing
and assortment and the design of auctions and incentives, permeate a large
number of sequential treatment experiments. Different applications impose
distinct levels of restrictions on viable actions. Some favor diversity of
outcomes, while others require harmful actions to be closely monitored or
mainly avoided. In this paper, we extend one of the most popular bandit
solutions, the original $\epsilon_t$-greedy heuristics, to high-dimensional
contexts. Moreover, we introduce a competing exploration mechanism that counts
with searching sets based on order statistics. We view our proposals as
alternatives for cases where pluralism is valued or, in the opposite direction,
cases where the end-user should carefully tune the range of exploration of new
actions. We find reasonable bounds for the cumulative regret of a decaying
$\epsilon_t$-greedy heuristic in both cases and we provide an upper bound for
the initialization phase that implies the regret bounds when order statistics
are considered to be at most equal but mostly better than the case when random
searching is the sole exploration mechanism. Additionally, we show that
end-users have sufficient flexibility to avoid harmful actions since any
cardinality for the higher-order statistics can be used to achieve an stricter
upper bound. We illustrate the algorithms proposed in this paper both with
simulated and real data.
</p>
<a href="http://arxiv.org/abs/2009.13961" target="_blank">arXiv:2009.13961</a> [<a href="http://arxiv.org/pdf/2009.13961" target="_blank">pdf</a>]

<h2>Attentional Feature Fusion. (arXiv:2009.14082v2 [cs.CV] UPDATED)</h2>
<h3>Yimian Dai, Fabian Gieseke, Stefan Oehmcke, Yiquan Wu, Kobus Barnard</h3>
<p>Feature fusion, the combination of features from different layers or
branches, is an omnipresent part of modern network architectures. It is often
implemented via simple operations, such as summation or concatenation, but this
might not be the best choice. In this work, we propose a uniform and general
scheme, namely attentional feature fusion, which is applicable for most common
scenarios, including feature fusion induced by short and long skip connections
as well as within Inception layers. To better fuse features of inconsistent
semantics and scales, we propose a multi-scale channel attention module, which
addresses issues that arise when fusing features given at different scales. We
also demonstrate that the initial integration of feature maps can become a
bottleneck and that this issue can be alleviated by adding another level of
attention, which we refer to as iterative attentional feature fusion. With
fewer layers or parameters, our models outperform state-of-the-art networks on
both CIFAR-100 and ImageNet datasets, which suggests that more sophisticated
attention mechanisms for feature fusion hold great potential to consistently
yield better results compared to their direct counterparts. Our codes and
trained models are available online.
</p>
<a href="http://arxiv.org/abs/2009.14082" target="_blank">arXiv:2009.14082</a> [<a href="http://arxiv.org/pdf/2009.14082" target="_blank">pdf</a>]

<h2>Constraint Monotonicity, Epistemic Splitting and Foundedness Could in General Be Too Strong in Answer Set Programming. (arXiv:2010.00191v2 [cs.AI] UPDATED)</h2>
<h3>Yi-Dong Shen, Thomas Eiter</h3>
<p>Recently, the notions of subjective constraint monotonicity, epistemic
splitting, and foundedness have been introduced for epistemic logic programs,
with the aim to use them as main criteria respectively intuitions to compare
different answer set semantics proposed in the literature on how they comply
with these intuitions. In this note, we consider these three notions and
demonstrate on some examples that they may be too strong in general and may
exclude some desired answer sets respectively world views. In conclusion, these
properties should not be regarded as mandatory properties that every answer set
semantics must satisfy in general.
</p>
<a href="http://arxiv.org/abs/2010.00191" target="_blank">arXiv:2010.00191</a> [<a href="http://arxiv.org/pdf/2010.00191" target="_blank">pdf</a>]

<h2>A Variational Information Bottleneck Based Method to Compress Sequential Networks for Human Action Recognition. (arXiv:2010.01343v2 [cs.CV] UPDATED)</h2>
<h3>Ayush Srivastava, Oshin Dutta, Prathosh AP, Sumeet Agarwal, Jigyasa Gupta</h3>
<p>In the last few years, compression of deep neural networks has become an
important strand of machine learning and computer vision research. Deep models
require sizeable computational complexity and storage, when used for instance
for Human Action Recognition (HAR) from videos, making them unsuitable to be
deployed on edge devices. In this paper, we address this issue and propose a
method to effectively compress Recurrent Neural Networks (RNNs) such as Gated
Recurrent Units (GRUs) and Long-Short-Term-Memory Units (LSTMs) that are used
for HAR. We use a Variational Information Bottleneck (VIB) theory-based pruning
approach to limit the information flow through the sequential cells of RNNs to
a small subset. Further, we combine our pruning method with a specific
group-lasso regularization technique that significantly improves compression.
The proposed techniques reduce model parameters and memory footprint from
latent representations, with little or no reduction in the validation accuracy
while increasing the inference speed several-fold. We perform experiments on
the three widely used Action Recognition datasets, viz. UCF11, HMDB51, and
UCF101, to validate our approach. It is shown that our method achieves over 70
times greater compression than the nearest competitor with comparable accuracy
for the task of action recognition on UCF11.
</p>
<a href="http://arxiv.org/abs/2010.01343" target="_blank">arXiv:2010.01343</a> [<a href="http://arxiv.org/pdf/2010.01343" target="_blank">pdf</a>]

<h2>Feature Whitening via Gradient Transformation for Improved Convergence. (arXiv:2010.01546v2 [cs.LG] UPDATED)</h2>
<h3>Shmulik Markovich-Golan, Barak Battash, Amit Bleiweiss</h3>
<p>Feature whitening is a known technique for speeding up training of DNN. Under
certain assumptions, whitening the activations reduces the Fisher information
matrix to a simple identity matrix, in which case stochastic gradient descent
is equivalent to the faster natural gradient descent. Due to the additional
complexity resulting from transforming the layer inputs and their corresponding
gradients in the forward and backward propagation, and from repeatedly
computing the Eigenvalue decomposition (EVD), this method is not commonly used
to date. In this work, we address the complexity drawbacks of feature
whitening. Our contribution is twofold. First, we derive an equivalent method,
which replaces the sample transformations by a transformation to the weight
gradients, applied to every batch of B samples. The complexity is reduced by a
factor of S=(2B), where S denotes the feature dimension of the layer output. As
the batch size increases with distributed training, the benefit of using the
proposed method becomes more compelling. Second, motivated by the theoretical
relation between the condition number of the sample covariance matrix and the
convergence speed, we derive an alternative sub-optimal algorithm which
recursively reduces the condition number of the latter matrix. Compared to EVD,
complexity is reduced by a factor of the input feature dimension M. We
exemplify the proposed algorithms with ResNet-based networks for image
classification demonstrated on the CIFAR and Imagenet datasets. Parallelizing
the proposed algorithms is straightforward and we implement a distributed
version thereof. Improved convergence, in terms of speed and attained accuracy,
can be observed in our experiments.
</p>
<a href="http://arxiv.org/abs/2010.01546" target="_blank">arXiv:2010.01546</a> [<a href="http://arxiv.org/pdf/2010.01546" target="_blank">pdf</a>]

<h2>Painting Outside as Inside: Edge Guided Image Outpainting via Bidirectional Rearrangement with Progressive Step Learning. (arXiv:2010.01810v2 [cs.CV] UPDATED)</h2>
<h3>Kyunghun Kim, Yeohun Yun, Keon-Woo Kang, Kyeongbo Kong, Siyeong Lee, Suk-Ju Kang</h3>
<p>Image outpainting is a very intriguing problem as the outside of a given
image can be continuously filled by considering as the context of the image.
This task has two main challenges. The first is to maintain the spatial
consistency in contents of generated regions and the original input. The second
is to generate a high-quality large image with a small amount of adjacent
information. Conventional image outpainting methods generate inconsistent,
blurry, and repeated pixels. To alleviate the difficulty of an outpainting
problem, we propose a novel image outpainting method using bidirectional
boundary region rearrangement. We rearrange the image to benefit from the image
inpainting task by reflecting more directional information. The bidirectional
boundary region rearrangement enables the generation of the missing region
using bidirectional information similar to that of the image inpainting task,
thereby generating the higher quality than the conventional methods using
unidirectional information. Moreover, we use the edge map generator that
considers images as original input with structural information and hallucinates
the edges of unknown regions to generate the image. Our proposed method is
compared with other state-of-the-art outpainting and inpainting methods both
qualitatively and quantitatively. We further compared and evaluated them using
BRISQUE, one of the No-Reference image quality assessment (IQA) metrics, to
evaluate the naturalness of the output. The experimental results demonstrate
that our method outperforms other methods and generates new images with
360{\deg}panoramic characteristics.
</p>
<a href="http://arxiv.org/abs/2010.01810" target="_blank">arXiv:2010.01810</a> [<a href="http://arxiv.org/pdf/2010.01810" target="_blank">pdf</a>]

<h2>Is Information Theory Inherently a Theory of Causation?. (arXiv:2010.01932v4 [stat.ML] UPDATED)</h2>
<h3>David Sigtermans</h3>
<p>Information theory gives rise to a novel method for causal skeleton discovery
by expressing associations between variables as tensors. This tensor-based
approach reduces the dimensionality of the data needed to test for conditional
independence, e.g., for systems comprising three variables, the causal skeleton
can be determined using pair-wise determined tensors. To arrive at this result,
an additional information measure, path information, is proposed.
</p>
<a href="http://arxiv.org/abs/2010.01932" target="_blank">arXiv:2010.01932</a> [<a href="http://arxiv.org/pdf/2010.01932" target="_blank">pdf</a>]

<h2>Revisiting Batch Normalization for Improving Corruption Robustness. (arXiv:2010.03630v2 [cs.CV] UPDATED)</h2>
<h3>Philipp Benz, Chaoning Zhang, Adil Karjauv, In So Kweon</h3>
<p>The performance of DNNs trained on clean images has been shown to decrease
when the test images have common corruptions. In this work, we interpret
corruption robustness as a domain shift and propose to rectify batch
normalization (BN) statistics for improving model robustness. This is motivated
by perceiving the shift from the clean domain to the corruption domain as a
style shift that is represented by the BN statistics. We find that simply
estimating and adapting the BN statistics on a few (32 for instance)
representation samples, without retraining the model, improves the corruption
robustness by a large margin on several benchmark datasets with a wide range of
model architectures. For example, on ImageNet-C, statistics adaptation improves
the top1 accuracy of ResNet50 from 39.2% to 48.7%. Moreover, we find that this
technique can further improve state-of-the-art robust models from 58.1% to
63.3%.
</p>
<a href="http://arxiv.org/abs/2010.03630" target="_blank">arXiv:2010.03630</a> [<a href="http://arxiv.org/pdf/2010.03630" target="_blank">pdf</a>]

<h2>Deep Learning Superpixel Semantic Segmentation with Transparent Initialization and Sparse Encoder. (arXiv:2010.04363v2 [cs.CV] UPDATED)</h2>
<h3>Zhiwei Xu, Thalaiyasingam Ajanthan, Richard Hartley</h3>
<p>Even though deep learning greatly improves the performance of semantic
segmentation, its success mainly lies in object central areas but without
accurate edges. As superpixel is a popular and effective auxiliary to preserve
object edges, in this paper, we jointly learn semantic segmentation with
trainable superpixels. We achieve it by adding fully-connected layers with
transparent initialization and an efficient logit consistency with a sparse
encoder. Specifically, the proposed transparent initialization reserves the
effects of learned parameters from pretrained networks, one for semantic
segmentation and the other for superpixel, by a linear data recovery. This
avoids a significant loss increase by using the pretrained networks, which
otherwise can be caused by an inappropriate parameter initialization on the
added layers. Meanwhile, consistent assignments to all pixels in each
superpixel can be guaranteed by the logit consistency with a sparse encoder.
This sparse encoder with sparse matrix operations substantially improves the
training efficiency by reducing the large computational complexity arising from
indexing pixels by superpixels. We demonstrate the effectiveness of our
proposal by transparent initialization and sparse encoder on semantic
segmentation on PASCAL VOC 2012 dataset with enhanced labeling on the object
edges. Moreover, the proposed transparent initialization can also be used to
jointly finetune multiple or a deeper pretrained network on other tasks.
</p>
<a href="http://arxiv.org/abs/2010.04363" target="_blank">arXiv:2010.04363</a> [<a href="http://arxiv.org/pdf/2010.04363" target="_blank">pdf</a>]

<h2>Image Generation With Neural Cellular Automatas. (arXiv:2010.04949v2 [cs.AI] UPDATED)</h2>
<h3>Mingxiang Chen, Zhecheng Wang</h3>
<p>In this paper, we propose a novel approach to generate images (or other
artworks) by using neural cellular automatas (NCAs). Rather than training NCAs
based on single images one by one, we combined the idea with variational
autoencoders (VAEs), and hence explored some applications, such as image
restoration and style fusion. The code for model implementation is available
online.
</p>
<a href="http://arxiv.org/abs/2010.04949" target="_blank">arXiv:2010.04949</a> [<a href="http://arxiv.org/pdf/2010.04949" target="_blank">pdf</a>]

<h2>Hybrid-S2S: Video Object Segmentation with Recurrent Networks and Correspondence Matching. (arXiv:2010.05069v2 [cs.CV] UPDATED)</h2>
<h3>Fatemeh Azimi, Stanislav Frolov, Federico Raue, Joern Hees, Andreas Dengel</h3>
<p>One-shot Video Object Segmentation~(VOS) is the task of pixel-wise tracking
an object of interest within a video sequence, where the segmentation mask of
the first frame is given at inference time. In recent years, Recurrent Neural
Networks~(RNNs) have been widely used for VOS tasks, but they often suffer from
limitations such as drift and error propagation. In this work, we study an
RNN-based architecture and address some of these issues by proposing a hybrid
sequence-to-sequence architecture named HS2S, utilizing a dual mask propagation
strategy that allows incorporating the information obtained from correspondence
matching. Our experiments show that augmenting the RNN with correspondence
matching is a highly effective solution to reduce the drift problem. The
additional information helps the model to predict more accurate masks and makes
it robust against error propagation. We evaluate our HS2S model on the
DAVIS2017 dataset as well as Youtube-VOS. On the latter, we achieve an
improvement of 11.2pp in the overall segmentation accuracy over RNN-based
state-of-the-art methods in VOS. We analyze our model's behavior in challenging
cases such as occlusion and long sequences and show that our hybrid
architecture significantly enhances the segmentation quality in these difficult
scenarios.
</p>
<a href="http://arxiv.org/abs/2010.05069" target="_blank">arXiv:2010.05069</a> [<a href="http://arxiv.org/pdf/2010.05069" target="_blank">pdf</a>]

<h2>Explaining Neural Matrix Factorization with Gradient Rollback. (arXiv:2010.05516v3 [cs.LG] UPDATED)</h2>
<h3>Carolin Lawrence, Timo Sztyler, Mathias Niepert</h3>
<p>Explaining the predictions of neural black-box models is an important
problem, especially when such models are used in applications where user trust
is crucial. Estimating the influence of training examples on a learned neural
model's behavior allows us to identify training examples most responsible for a
given prediction and, therefore, to faithfully explain the output of a
black-box model. The most generally applicable existing method is based on
influence functions, which scale poorly for larger sample sizes and models.

We propose gradient rollback, a general approach for influence estimation,
applicable to neural models where each parameter update step during gradient
descent touches a smaller number of parameters, even if the overall number of
parameters is large. Neural matrix factorization models trained with gradient
descent are part of this model class. These models are popular and have found a
wide range of applications in industry. Especially knowledge graph embedding
methods, which belong to this class, are used extensively. We show that
gradient rollback is highly efficient at both training and test time. Moreover,
we show theoretically that the difference between gradient rollback's influence
approximation and the true influence on a model's behavior is smaller than
known bounds on the stability of stochastic gradient descent. This establishes
that gradient rollback is robustly estimating example influence. We also
conduct experiments which show that gradient rollback provides faithful
explanations for knowledge base completion and recommender datasets.
</p>
<a href="http://arxiv.org/abs/2010.05516" target="_blank">arXiv:2010.05516</a> [<a href="http://arxiv.org/pdf/2010.05516" target="_blank">pdf</a>]

<h2>An Information-Theoretic Perspective on Overfitting and Underfitting. (arXiv:2010.06076v2 [cs.LG] UPDATED)</h2>
<h3>Daniel Bashir, George D. Montanez, Sonia Sehra, Pedro Sandoval Segura, Julius Lauw</h3>
<p>We present an information-theoretic framework for understanding overfitting
and underfitting in machine learning and prove the formal undecidability of
determining whether an arbitrary classification algorithm will overfit a
dataset. Measuring algorithm capacity via the information transferred from
datasets to models, we consider mismatches between algorithm capacities and
datasets to provide a signature for when a model can overfit or underfit a
dataset. We present results upper-bounding algorithm capacity, establish its
relationship to quantities in the algorithmic search framework for machine
learning, and relate our work to recent information-theoretic approaches to
generalization.
</p>
<a href="http://arxiv.org/abs/2010.06076" target="_blank">arXiv:2010.06076</a> [<a href="http://arxiv.org/pdf/2010.06076" target="_blank">pdf</a>]

<h2>AMPA-Net: Optimization-Inspired Attention Neural Network for Deep Compressed Sensing. (arXiv:2010.06907v5 [cs.CV] UPDATED)</h2>
<h3>Nanyu Li, Charles C. Zhou</h3>
<p>Compressed sensing (CS) is a challenging problem in image processing due to
reconstructing an almost complete image from a limited measurement. To achieve
fast and accurate CS reconstruction, we synthesize the advantages of two
well-known methods (neural network and optimization algorithm) to propose a
novel optimization inspired neural network which dubbed AMP-Net. AMP-Net
realizes the fusion of the Approximate Message Passing (AMP) algorithm and
neural network. All of its parameters are learned automatically. Furthermore,
we propose an AMPA-Net which uses three attention networks to improve the
representation ability of AMP-Net. Finally, We demonstrate the effectiveness of
AMP-Net and AMPA-Net on four standard CS reconstruction benchmark data sets.
</p>
<a href="http://arxiv.org/abs/2010.06907" target="_blank">arXiv:2010.06907</a> [<a href="http://arxiv.org/pdf/2010.06907" target="_blank">pdf</a>]

<h2>Selective Classification via One-Sided Prediction. (arXiv:2010.07853v2 [cs.LG] UPDATED)</h2>
<h3>Aditya Gangrade, Anil Kag, Venkatesh Saligrama</h3>
<p>We propose a novel method for selective classification (SC), a problem which
allows a classifier to abstain from predicting some instances, thus trading off
accuracy against coverage (the fraction of instances predicted). In contrast to
prior gating or confidence-set based work, our proposed method optimises a
collection of class-wise decoupled one-sided empirical risks, and is in essence
a method for explicitly finding the largest decision sets for each class that
have few false positives. This one-sided prediction (OSP) based relaxation
yields an SC scheme that attains near-optimal coverage in the practically
relevant high target accuracy regime, and further admits efficient
implementation, leading to a flexible and principled method for SC. We
theoretically derive generalization bounds for SC and OSP, and empirically we
show that our scheme strongly outperforms state of the art methods in coverage
at small error levels.
</p>
<a href="http://arxiv.org/abs/2010.07853" target="_blank">arXiv:2010.07853</a> [<a href="http://arxiv.org/pdf/2010.07853" target="_blank">pdf</a>]

<h2>Variational (Gradient) Estimate of the Score Function in Energy-based Latent Variable Models. (arXiv:2010.08258v2 [cs.LG] UPDATED)</h2>
<h3>Fan Bao, Kun Xu, Chongxuan Li, Lanqing Hong, Jun Zhu, Bo Zhang</h3>
<p>The learning and evaluation of energy-based latent variable models (EBLVMs)
without any structural assumptions are highly challenging, because the true
posteriors and the partition functions in such models are generally
intractable. This paper presents variational estimates of the score function
and its gradient with respect to the model parameters in a general EBLVM,
referred to as VaES and VaGES respectively. The variational posterior is
trained to minimize a certain divergence to the true model posterior and the
bias in both estimates can be bounded by the divergence theoretically. With a
minimal model assumption, VaES and VaGES can be applied to the kernelized Stein
discrepancy (KSD) and score matching (SM)-based methods to learn EBLVMs.
Besides, VaES can also be used to estimate the exact Fisher divergence between
the data and general EBLVMs.
</p>
<a href="http://arxiv.org/abs/2010.08258" target="_blank">arXiv:2010.08258</a> [<a href="http://arxiv.org/pdf/2010.08258" target="_blank">pdf</a>]

<h2>GFL: A Decentralized Federated Learning Framework Based On Blockchain. (arXiv:2010.10996v2 [cs.LG] UPDATED)</h2>
<h3>Yifan Hu, Wei Xia, Jun Xiao, Chao Wu</h3>
<p>Due to people's emerging concern about data privacy, federated learning(FL)
is currently being widely used. Conventional federated learning uses a highly
centralized architecture, but in a real federated learning scenario, due to the
highly distributed of data nodes and the existence of malicious data nodes, It
is of great challenges for conventional federated learning to improve the
utilization of network bandwidth and maintained the security and robustness of
federated learning under malicious node attacks. In this paper, we propose an
innovative Ring decentralized federated learning algorithm(RDFL) that not only
makes full use of the bandwidth of the network but also improves the security
and robustness of federated learning under malicious node attacks. At the same
time, we encapsulated RDFL into a blochain-based federated learning framework
called Galaxy Federated Learning framework\cite{GFL} and used real data to
perform experiments on the GFL to verify the effectiveness of the GFL.
</p>
<a href="http://arxiv.org/abs/2010.10996" target="_blank">arXiv:2010.10996</a> [<a href="http://arxiv.org/pdf/2010.10996" target="_blank">pdf</a>]

<h2>Recovery of sparse linear classifiers from mixture of responses. (arXiv:2010.12087v2 [stat.ML] UPDATED)</h2>
<h3>Venkata Gandikota, Arya Mazumdar, Soumyabrata Pal</h3>
<p>In the problem of learning a mixture of linear classifiers, the aim is to
learn a collection of hyperplanes from a sequence of binary responses. Each
response is a result of querying with a vector and indicates the side of a
randomly chosen hyperplane from the collection the query vector belongs to.
This model provides a rich representation of heterogeneous data with
categorical labels and has only been studied in some special settings. We look
at a hitherto unstudied problem of query complexity upper bound of recovering
all the hyperplanes, especially for the case when the hyperplanes are sparse.
This setting is a natural generalization of the extreme quantization problem
known as 1-bit compressed sensing. Suppose we have a set of $\ell$ unknown
$k$-sparse vectors. We can query the set with another vector $\boldsymbol{a}$,
to obtain the sign of the inner product of $\boldsymbol{a}$ and a randomly
chosen vector from the $\ell$-set. How many queries are sufficient to identify
all the $\ell$ unknown vectors? This question is significantly more challenging
than both the basic 1-bit compressed sensing problem (i.e., $\ell=1$ case) and
the analogous regression problem (where the value instead of the sign is
provided). We provide rigorous query complexity results (with efficient
algorithms) for this problem.
</p>
<a href="http://arxiv.org/abs/2010.12087" target="_blank">arXiv:2010.12087</a> [<a href="http://arxiv.org/pdf/2010.12087" target="_blank">pdf</a>]

<h2>Loss-analysis via Attention-scale for Physiologic Time Series. (arXiv:2010.12690v2 [cs.LG] UPDATED)</h2>
<h3>Jiawei Yang, Jeffrey M. Hausdorff</h3>
<p>Physiologic signals have properties across multiple spatial and temporal
scales, which can be shown by the complexity-analysis of the coarse-grained
physiologic signals by scaling techniques such as the multiscale.
Unfortunately, the results obtained from the coarse-grained signals by the
multiscale may not fully reflect the properties of the original signals because
there is a loss caused by scaling techniques and the same scaling technique may
bring different losses to different signals. Another problem is that multiscale
does not consider the key observations inherent in the signal. Here, we show a
new analysis method for time series called the loss-analysis via
attention-scale. We show that multiscale is a special case of attention-scale.
The loss-analysis can complement to the complexity-analysis to capture aspects
of the signals that are not captured using previously developed measures. This
can be used to study ageing, diseases, and other physiologic phenomenon.
</p>
<a href="http://arxiv.org/abs/2010.12690" target="_blank">arXiv:2010.12690</a> [<a href="http://arxiv.org/pdf/2010.12690" target="_blank">pdf</a>]

<h2>EDNet: Efficient Disparity Estimation with Combination Volume and Spatial Attention based Residual Learning. (arXiv:2010.13338v2 [cs.CV] UPDATED)</h2>
<h3>Songyan Zhang, Zhicheng Wang</h3>
<p>Existing state-of-the-art disparity estimation works mostly leverage the 4D
concatenation volume and construct a very deep 3D convolution neural network
for disparity regression, which is inefficient considering the high memory
consumption and slow inference speed. In this paper, we propose a network named
EDNet for efficient disparity estimation. To be specific, we construct a
combination volume which incorporates contextual information from the
concatenation volume and feature similarity measurement from the correlation
volume. The combination volume can be aggregated by 2D convolutions which
require less running memory. We further propose a spatial attention based
residual learning module to generate attention-aware residual features.
Accurate disparity correction can be provided even in low-texture regions as
the residual learning process can specifically concentrate on inaccurate
regions. Extensive experiments on Scene Flow and KITTI datasets show that our
network outperforms previous 3D convolution based works and achieves
state-of-the-art performance with significantly faster speed and less memory
consumption, demonstrating the effectiveness of our proposed method.
</p>
<a href="http://arxiv.org/abs/2010.13338" target="_blank">arXiv:2010.13338</a> [<a href="http://arxiv.org/pdf/2010.13338" target="_blank">pdf</a>]

<h2>KFC: A Scalable Approximation Algorithm for $k$-center Fair Clustering. (arXiv:2010.13949v2 [cs.LG] UPDATED)</h2>
<h3>Elfarouk Harb, Ho Shan Lam</h3>
<p>In this paper, we study the problem of fair clustering on the $k-$center
objective. In fair clustering, the input is $N$ points, each belonging to at
least one of $l$ protected groups, e.g. male, female, Asian, Hispanic. The
objective is to cluster the $N$ points into $k$ clusters to minimize a
classical clustering objective function. However, there is an additional
constraint that each cluster needs to be fair, under some notion of fairness.
This ensures that no group is either "over-represented" or "under-represented"
in any cluster. Our work builds on the work of Chierichetti et al. (NIPS 2017),
Bera et al. (NeurIPS 2019), Ahmadian et al. (KDD 2019), and Bercea et al.
(APPROX 2019). We obtain a randomized $3-$approximation algorithm for the
$k-$center objective function, beating the previous state of the art
($4-$approximation). We test our algorithm on real datasets, and show that our
algorithm is effective in finding good clusters without over-representation or
under-representation, surpassing the current state of the art in runtime speed,
clustering cost, while achieving similar fairness violations.
</p>
<a href="http://arxiv.org/abs/2010.13949" target="_blank">arXiv:2010.13949</a> [<a href="http://arxiv.org/pdf/2010.13949" target="_blank">pdf</a>]

<h2>An Experimentation Platform for Explainable Coalition Situational Understanding. (arXiv:2010.14388v2 [cs.AI] UPDATED)</h2>
<h3>Katie Barrett-Powell, Jack Furby, Liam Hiley, Marc Roig Vilamala, Harrison Taylor, Federico Cerutti, Alun Preece, Tianwei Xing, Luis Garcia, Mani Srivastava, Dave Braines</h3>
<p>We present an experimentation platform for coalition situational
understanding research that highlights capabilities in explainable artificial
intelligence/machine learning (AI/ML) and integration of symbolic and
subsymbolic AI/ML approaches for event processing. The Situational
Understanding Explorer (SUE) platform is designed to be lightweight, to easily
facilitate experiments and demonstrations, and open. We discuss our
requirements to support coalition multi-domain operations with emphasis on
asset interoperability and ad hoc human-machine teaming in a dense urban
terrain setting. We describe the interface functionality and give examples of
SUE applied to coalition situational understanding tasks.
</p>
<a href="http://arxiv.org/abs/2010.14388" target="_blank">arXiv:2010.14388</a> [<a href="http://arxiv.org/pdf/2010.14388" target="_blank">pdf</a>]

<h2>Stereo Frustums: A Siamese Pipeline for 3D Object Detection. (arXiv:2010.14599v2 [cs.CV] UPDATED)</h2>
<h3>Xi Mo, Usman Sajid, Guanghui Wang</h3>
<p>The paper proposes a light-weighted stereo frustums matching module for 3D
objection detection. The proposed framework takes advantage of a
high-performance 2D detector and a point cloud segmentation network to regress
3D bounding boxes for autonomous driving vehicles. Instead of performing
traditional stereo matching to compute disparities, the module directly takes
the 2D proposals from both the left and the right views as input. Based on the
epipolar constraints recovered from the well-calibrated stereo cameras, we
propose four matching algorithms to search for the best match for each proposal
between the stereo image pairs. Each matching pair proposes a segmentation of
the scene which is then fed into a 3D bounding box regression network. Results
of extensive experiments on KITTI dataset demonstrate that the proposed Siamese
pipeline outperforms the state-of-the-art stereo-based 3D bounding box
regression methods.
</p>
<a href="http://arxiv.org/abs/2010.14599" target="_blank">arXiv:2010.14599</a> [<a href="http://arxiv.org/pdf/2010.14599" target="_blank">pdf</a>]

<h2>Generative Adversarial Networks in Human Emotion Synthesis:A Review. (arXiv:2010.15075v2 [cs.CV] UPDATED)</h2>
<h3>Noushin Hajarolasvadi, Miguel Arjona Ram&#xed;rez, Hasan Demirel</h3>
<p>Synthesizing realistic data samples is of great value for both academic and
industrial communities. Deep generative models have become an emerging topic in
various research areas like computer vision and signal processing. Affective
computing, a topic of a broad interest in computer vision society, has been no
exception and has benefited from generative models. In fact, affective
computing observed a rapid derivation of generative models during the last two
decades. Applications of such models include but are not limited to emotion
recognition and classification, unimodal emotion synthesis, and cross-modal
emotion synthesis. As a result, we conducted a review of recent advances in
human emotion synthesis by studying available databases, advantages, and
disadvantages of the generative models along with the related training
strategies considering two principal human communication modalities, namely
audio and video. In this context, facial expression synthesis, speech emotion
synthesis, and the audio-visual (cross-modal) emotion synthesis is reviewed
extensively under different application scenarios. Gradually, we discuss open
research problems to push the boundaries of this research area for future
works.
</p>
<a href="http://arxiv.org/abs/2010.15075" target="_blank">arXiv:2010.15075</a> [<a href="http://arxiv.org/pdf/2010.15075" target="_blank">pdf</a>]

<h2>LIFI: Towards Linguistically Informed Frame Interpolation. (arXiv:2010.16078v2 [cs.CV] UPDATED)</h2>
<h3>Aradhya Neeraj Mathur, Devansh Batra, Yaman Kumar, Rajiv Ratn Shah, Roger Zimmermann, Amanda Stent</h3>
<p>In this work, we explore a new problem of frame interpolation for speech
videos. Such content today forms the major form of online communication. We try
to solve this problem by using several deep learning video generation
algorithms to generate the missing frames. We also provide examples where
computer vision models despite showing high performance on conventional
non-linguistic metrics fail to accurately produce faithful interpolation of
speech. With this motivation, we provide a new set of linguistically-informed
metrics specifically targeted to the problem of speech videos interpolation. We
also release several datasets to test computer vision video generation models
of their speech understanding.
</p>
<a href="http://arxiv.org/abs/2010.16078" target="_blank">arXiv:2010.16078</a> [<a href="http://arxiv.org/pdf/2010.16078" target="_blank">pdf</a>]

<h2>HOI Analysis: Integrating and Decomposing Human-Object Interaction. (arXiv:2010.16219v2 [cs.CV] UPDATED)</h2>
<h3>Yong-Lu Li, Xinpeng Liu, Xiaoqian Wu, Yizhuo Li, Cewu Lu</h3>
<p>Human-Object Interaction (HOI) consists of human, object and implicit
interaction/verb. Different from previous methods that directly map pixels to
HOI semantics, we propose a novel perspective for HOI learning in an analytical
manner. In analogy to Harmonic Analysis, whose goal is to study how to
represent the signals with the superposition of basic waves, we propose the HOI
Analysis. We argue that coherent HOI can be decomposed into isolated human and
object. Meanwhile, isolated human and object can also be integrated into
coherent HOI again. Moreover, transformations between human-object pairs with
the same HOI can also be easier approached with integration and decomposition.
As a result, the implicit verb will be represented in the transformation
function space. In light of this, we propose an Integration-Decomposition
Network (IDN) to implement the above transformations and achieve
state-of-the-art performance on widely-used HOI detection benchmarks. Code is
available at
https://github.com/DirtyHarryLYL/HAKE-Action-Torch/tree/IDN-(Integrating-Decomposing-Network).
</p>
<a href="http://arxiv.org/abs/2010.16219" target="_blank">arXiv:2010.16219</a> [<a href="http://arxiv.org/pdf/2010.16219" target="_blank">pdf</a>]

<h2>Unsupervised Monocular Depth Learning in Dynamic Scenes. (arXiv:2010.16404v2 [cs.CV] UPDATED)</h2>
<h3>Hanhan Li, Ariel Gordon, Hang Zhao, Vincent Casser, Anelia Angelova</h3>
<p>We present a method for jointly training the estimation of depth, ego-motion,
and a dense 3D translation field of objects relative to the scene, with
monocular photometric consistency being the sole source of supervision. We show
that this apparently heavily underdetermined problem can be regularized by
imposing the following prior knowledge about 3D translation fields: they are
sparse, since most of the scene is static, and they tend to be constant for
rigid moving objects. We show that this regularization alone is sufficient to
train monocular depth prediction models that exceed the accuracy achieved in
prior work for dynamic scenes, including methods that require semantic input.
Code is at
https://github.com/google-research/google-research/tree/master/depth_and_motion_learning .
</p>
<a href="http://arxiv.org/abs/2010.16404" target="_blank">arXiv:2010.16404</a> [<a href="http://arxiv.org/pdf/2010.16404" target="_blank">pdf</a>]

<h2>Self-paced and self-consistent co-training for semi-supervised image segmentation. (arXiv:2011.00325v2 [cs.CV] UPDATED)</h2>
<h3>Ping Wang, Jizong Peng, Marco Pedersoli, Yuanfeng Zhou, Caiming Zhang, Christian Desrosiers</h3>
<p>Deep co-training has recently been proposed as an effective approach for
image segmentation when annotated data is scarce. In this paper, we improve
existing approaches for semi-supervised segmentation with a self-paced and
self-consistent co-training method. To help distillate information from
unlabeled images, we first design a self-paced learning strategy for
co-training that lets jointly-trained neural networks focus on
easier-to-segment regions first, and then gradually consider harder ones.This
is achieved via an end-to-end differentiable loss inthe form of a generalized
Jensen Shannon Divergence(JSD). Moreover, to encourage predictions from
different networks to be both consistent and confident, we enhance this
generalized JSD loss with an uncertainty regularizer based on entropy. The
robustness of individual models is further improved using a self-ensembling
loss that enforces their prediction to be consistent across different training
iterations. We demonstrate the potential of our method on three challenging
image segmentation problems with different image modalities, using small
fraction of labeled data. Results show clear advantages in terms of performance
compared to the standard co-training baselines and recently proposed
state-of-the-art approaches for semi-supervised segmentation
</p>
<a href="http://arxiv.org/abs/2011.00325" target="_blank">arXiv:2011.00325</a> [<a href="http://arxiv.org/pdf/2011.00325" target="_blank">pdf</a>]

<h2>Differentially Private Bayesian Inference for Generalized Linear Models. (arXiv:2011.00467v2 [cs.LG] UPDATED)</h2>
<h3>Tejas Kulkarni, Joonas J&#xe4;lk&#xf6;, Antti Koskela, Samuel Kaski, Antti Honkela</h3>
<p>The framework of differential privacy (DP) upper bounds the information
disclosure risk involved in using sensitive datasets for statistical analysis.
A DP mechanism typically operates by adding carefully calibrated noise to the
data release procedure. Generalized linear models (GLMs) are among the most
widely used arms in data analyst's repertoire. In this work, with logistic and
Poisson regression as running examples, we propose a generic noise-aware
Bayesian framework to quantify the parameter uncertainty for a GLM at hand,
given noisy sufficient statistics. We perform a tight privacy analysis and
experimentally demonstrate that the posteriors obtained from our model, while
adhering to strong privacy guarantees, are similar to the non-private
posteriors.
</p>
<a href="http://arxiv.org/abs/2011.00467" target="_blank">arXiv:2011.00467</a> [<a href="http://arxiv.org/pdf/2011.00467" target="_blank">pdf</a>]

<h2>FusiformNet: Extracting Discriminative Facial Features on Different Levels. (arXiv:2011.00577v2 [cs.CV] UPDATED)</h2>
<h3>Kyo Takano</h3>
<p>Over the last several years, research on facial recognition based on Deep
Neural Network has evolved with approaches like task-specific loss functions,
image normalization and augmentation, network architectures, etc. However,
there have been few approaches with attention to how human faces differ from
person to person. Premising that inter-personal differences are found both
generally and locally on the human face, I propose FusiformNet, a novel
framework for feature extraction that leverages the nature of discriminative
facial features. Tested on Image-Unrestricted setting of Labeled Face in the
Wild benchmark, this method achieved a state-of-the-art accuracy of 96.67%
without labeled outside data, image augmentation, normalization, or special
loss functions. Likewise, the method also performed on par with previous
state-of-the-arts when pre-trained on CASIA-WebFace dataset. Considering its
ability to extract both general and local facial features, the utility of
FusiformNet may not be limited to facial recognition but also extend to other
DNN-based tasks.
</p>
<a href="http://arxiv.org/abs/2011.00577" target="_blank">arXiv:2011.00577</a> [<a href="http://arxiv.org/pdf/2011.00577" target="_blank">pdf</a>]

<h2>PAC Confidence Predictions for Deep Neural Network Classifiers. (arXiv:2011.00716v3 [cs.LG] UPDATED)</h2>
<h3>Sangdon Park, Shuo Li, Osbert Bastani, Insup Lee</h3>
<p>A key challenge for deploying deep neural networks (DNNs) in safety critical
settings is the need to provide rigorous ways to quantify their uncertainty. In
this paper, we propose a novel algorithm for constructing predicted
classification confidences for DNNs that comes with provable correctness
guarantees. Our approach uses Clopper-Pearson confidence intervals for the
Binomial distribution in conjunction with the histogram binning approach to
calibrated prediction. In addition, we demonstrate how our predicted
confidences can be used to enable downstream guarantees in two settings: (i)
fast DNN inference, where we demonstrate how to compose a fast but inaccurate
DNN with an accurate but slow DNN in a rigorous way to improve performance
without sacrificing accuracy, and (ii) safe planning, where we guarantee safety
when using a DNN to predict whether a given action is safe based on visual
observations. In our experiments, we demonstrate that our approach can be used
to provide guarantees for state-of-the-art DNNs.
</p>
<a href="http://arxiv.org/abs/2011.00716" target="_blank">arXiv:2011.00716</a> [<a href="http://arxiv.org/pdf/2011.00716" target="_blank">pdf</a>]

<h2>Set Augmented Triplet Loss for Video Person Re-Identification. (arXiv:2011.00774v2 [cs.CV] UPDATED)</h2>
<h3>Pengfei Fang, Pan Ji, Lars Petersson, Mehrtash Harandi</h3>
<p>Modern video person re-identification (re-ID) machines are often trained
using a metric learning approach, supervised by a triplet loss. The triplet
loss used in video re-ID is usually based on so-called clip features, each
aggregated from a few frame features. In this paper, we propose to model the
video clip as a set and instead study the distance between sets in the
corresponding triplet loss. In contrast to the distance between clip
representations, the distance between clip sets considers the pair-wise
similarity of each element (i.e., frame representation) between two sets. This
allows the network to directly optimize the feature representation at a frame
level. Apart from the commonly-used set distance metrics (e.g., ordinary
distance and Hausdorff distance), we further propose a hybrid distance metric,
tailored for the set-aware triplet loss. Also, we propose a hard positive set
construction strategy using the learned class prototypes in a batch. Our
proposed method achieves state-of-the-art results across several standard
benchmarks, demonstrating the advantages of the proposed method.
</p>
<a href="http://arxiv.org/abs/2011.00774" target="_blank">arXiv:2011.00774</a> [<a href="http://arxiv.org/pdf/2011.00774" target="_blank">pdf</a>]

<h2>Receptive Field Size Optimization with Continuous Time Pooling. (arXiv:2011.00869v2 [cs.CV] UPDATED)</h2>
<h3>D&#xf3;ra Babicz, Soma Kont&#xe1;r, M&#xe1;rk Pet&#x151;, Andr&#xe1;s F&#xfc;l&#xf6;p, Gergely Szab&#xf3;, Andr&#xe1;s Horv&#xe1;th</h3>
<p>The pooling operation is a cornerstone element of convolutional neural
networks. These elements generate receptive fields for neurons, in which local
perturbations should have minimal effect on the output activations, increasing
robustness and invariance of the network. In this paper we will present an
altered version of the most commonly applied method, maximum pooling, where
pooling in theory is substituted by a continuous time differential equation,
which generates a location sensitive pooling operation, more similar to
biological receptive fields. We will present how this continuous method can be
approximated numerically using discrete operations which fit ideally on a GPU.
In our approach the kernel size is substituted by diffusion strength which is a
continuous valued parameter, this way it can be optimized by gradient descent
algorithms. We will evaluate the effect of continuous pooling on accuracy and
computational need using commonly applied network architectures and datasets.
</p>
<a href="http://arxiv.org/abs/2011.00869" target="_blank">arXiv:2011.00869</a> [<a href="http://arxiv.org/pdf/2011.00869" target="_blank">pdf</a>]

<h2>Reducing Neural Network Parameter Initialization Into an SMT Problem. (arXiv:2011.01191v3 [cs.LG] UPDATED)</h2>
<h3>Mohamad H. Danesh</h3>
<p>Training a neural network (NN) depends on multiple factors, including but not
limited to the initial weights. In this paper, we focus on initializing deep NN
parameters such that it performs better, comparing to random or zero
initialization. We do this by reducing the process of initialization into an
SMT solver. Previous works consider certain activation functions on small NNs,
however the studied NN is a deep network with different activation functions.
Our experiments show that the proposed approach for parameter initialization
achieves better performance comparing to randomly initialized networks.
</p>
<a href="http://arxiv.org/abs/2011.01191" target="_blank">arXiv:2011.01191</a> [<a href="http://arxiv.org/pdf/2011.01191" target="_blank">pdf</a>]

<h2>Autoencoding Features for Aviation Machine Learning Problems. (arXiv:2011.01464v2 [cs.LG] UPDATED)</h2>
<h3>Liya Wang, Panta Lucic, Keith Campbell, Craig Wanke</h3>
<p>The current practice of manually processing features for high-dimensional and
heterogeneous aviation data is labor-intensive, does not scale well to new
problems, and is prone to information loss, affecting the effectiveness and
maintainability of machine learning (ML) procedures. This research explored an
unsupervised learning method, autoencoder, to extract effective features for
aviation machine learning problems. The study explored variants of autoencoders
with the aim of forcing the learned representations of the input to assume
useful properties. A flight track anomaly detection autoencoder was developed
to demonstrate the versatility of the technique. The research results show that
the autoencoder can not only automatically extract effective features for the
flight track data, but also efficiently deep clean data, thereby reducing the
workload of data scientists. Moreover, the research leveraged transfer learning
to efficiently train models for multiple airports. Transfer learning can reduce
model training times from days to hours, as well as improving model
performance. The developed applications and techniques are shared with the
whole aviation community to improve effectiveness of ongoing and future machine
learning studies.
</p>
<a href="http://arxiv.org/abs/2011.01464" target="_blank">arXiv:2011.01464</a> [<a href="http://arxiv.org/pdf/2011.01464" target="_blank">pdf</a>]

<h2>Design Paradigms Based on Spring Agonists for Underactuated Robot Hands: Concepts and Application. (arXiv:2011.01483v2 [cs.RO] UPDATED)</h2>
<h3>Tianjian Chen, Tianyi Zhang, Matei Ciocarlie</h3>
<p>In this paper, we focus on a rarely used paradigm in the design of
underactuated robot hands: the use of springs as agonists and tendons as
antagonists. We formalize this approach in a design matrix also considering its
interplay with the underactuation method used (one tendon for multiple joints
vs. multiple tendons on one motor shaft). We then show how different cells in
this design matrix can be combined in order to facilitate the implementation of
desired postural synergies with a single motor. Furthermore, we show that when
agonist and antagonist tendons are combined on the same motor shaft, the
resulting spring force cancellation can be leveraged to produce multiple
desirable behaviors, which we demonstrate in a physical prototype.
</p>
<a href="http://arxiv.org/abs/2011.01483" target="_blank">arXiv:2011.01483</a> [<a href="http://arxiv.org/pdf/2011.01483" target="_blank">pdf</a>]

<h2>An Improved Attention for Visual Question Answering. (arXiv:2011.02164v2 [cs.CV] UPDATED)</h2>
<h3>Tanzila Rahman, Shih-Han Chou, Leonid Sigal, Giuseppe Carenini</h3>
<p>We consider the problem of Visual Question Answering (VQA). Given an image
and a free-form, open-ended, question, expressed in natural language, the goal
of VQA system is to provide accurate answer to this question with respect to
the image. The task is challenging because it requires simultaneous and
intricate understanding of both visual and textual information. Attention,
which captures intra- and inter-modal dependencies, has emerged as perhaps the
most widely used mechanism for addressing these challenges. In this paper, we
propose an improved attention-based architecture to solve VQA. We incorporate
an Attention on Attention (AoA) module within encoder-decoder framework, which
is able to determine the relation between attention results and queries.
Attention module generates weighted average for each query. On the other hand,
AoA module first generates an information vector and an attention gate using
attention results and current context; and then adds another attention to
generate final attended information by multiplying the two. We also propose
multimodal fusion module to combine both visual and textual information. The
goal of this fusion module is to dynamically decide how much information should
be considered from each modality. Extensive experiments on VQA-v2 benchmark
dataset show that our method achieves the state-of-the-art performance.
</p>
<a href="http://arxiv.org/abs/2011.02164" target="_blank">arXiv:2011.02164</a> [<a href="http://arxiv.org/pdf/2011.02164" target="_blank">pdf</a>]

<h2>Effective Fusion Factor in FPN for Tiny Object Detection. (arXiv:2011.02298v2 [cs.CV] UPDATED)</h2>
<h3>Yuqi Gong, Xuehui Yu, Yao Ding, Xiaoke Peng, Jian Zhao, Zhenjun Han</h3>
<p>FPN-based detectors have made significant progress in general object
detection, e.g., MS COCO and PASCAL VOC. However, these detectors fail in
certain application scenarios, e.g., tiny object detection. In this paper, we
argue that the top-down connections between adjacent layers in FPN bring
two-side influences for tiny object detection, not only positive. We propose a
novel concept, fusion factor, to control information that deep layers deliver
to shallow layers, for adapting FPN to tiny object detection. After series of
experiments and analysis, we explore how to estimate an effective value of
fusion factor for a particular dataset by a statistical method. The estimation
is dependent on the number of objects distributed in each layer. Comprehensive
experiments are conducted on tiny object detection datasets, e.g., TinyPerson
and Tiny CityPersons. Our results show that when configuring FPN with a proper
fusion factor, the network is able to achieve significant performance gains
over the baseline on tiny object detection datasets. Codes and models will be
released.
</p>
<a href="http://arxiv.org/abs/2011.02298" target="_blank">arXiv:2011.02298</a> [<a href="http://arxiv.org/pdf/2011.02298" target="_blank">pdf</a>]

<h2>Hypersim: A Photorealistic Synthetic Dataset for Holistic Indoor Scene Understanding. (arXiv:2011.02523v2 [cs.CV] UPDATED)</h2>
<h3>Mike Roberts, Nathan Paczan</h3>
<p>For many fundamental scene understanding tasks, it is difficult or impossible
to obtain per-pixel ground truth labels from real images. We address this
challenge by introducing Hypersim, a photorealistic synthetic dataset for
holistic indoor scene understanding. To create our dataset, we leverage a large
repository of synthetic scenes created by professional artists, and we generate
77,400 images of 461 indoor scenes with detailed per-pixel labels and
corresponding ground truth geometry. Our dataset: (1) relies exclusively on
publicly available 3D assets; (2) includes complete scene geometry, material
information, and lighting information for every scene; (3) includes dense
per-pixel semantic instance segmentations for every image; and (4) factors
every image into diffuse reflectance, diffuse illumination, and a non-diffuse
residual term that captures view-dependent lighting effects. Together, these
features make our dataset well-suited for geometric learning problems that
require direct 3D supervision, multi-task learning problems that require
reasoning jointly over multiple input and output modalities, and inverse
rendering problems. We analyze our dataset at the level of scenes, objects, and
pixels, and we analyze costs in terms of money, annotation effort, and
computation time. Remarkably, we find that it is possible to generate our
entire dataset from scratch, for roughly half the cost of training a
state-of-the-art natural language processing model. All the code we used to
generate our dataset will be made available online.
</p>
<a href="http://arxiv.org/abs/2011.02523" target="_blank">arXiv:2011.02523</a> [<a href="http://arxiv.org/pdf/2011.02523" target="_blank">pdf</a>]

<h2>Defense-friendly Images in Adversarial Attacks: Dataset and Metrics for Perturbation Difficulty. (arXiv:2011.02675v2 [cs.CV] UPDATED)</h2>
<h3>Camilo Pestana, Wei Liu, David Glance, Ajmal Mian</h3>
<p>Dataset bias is a problem in adversarial machine learning, especially in the
evaluation of defenses. An adversarial attack or defense algorithm may show
better results on the reported dataset than can be replicated on other
datasets. Even when two algorithms are compared, their relative performance can
vary depending on the dataset. Deep learning offers state-of-the-art solutions
for image recognition, but deep models are vulnerable even to small
perturbations. Research in this area focuses primarily on adversarial attacks
and defense algorithms. In this paper, we report for the first time, a class of
robust images that are both resilient to attacks and that recover better than
random images under adversarial attacks using simple defense techniques. Thus,
a test dataset with a high proportion of robust images gives a misleading
impression about the performance of an adversarial attack or defense. We
propose three metrics to determine the proportion of robust images in a dataset
and provide scoring to determine the dataset bias. We also provide an
ImageNet-R dataset of 15000+ robust images to facilitate further research on
this intriguing phenomenon of image strength under attack. Our dataset,
combined with the proposed metrics, is valuable for unbiased benchmarking of
adversarial attack and defense algorithms.
</p>
<a href="http://arxiv.org/abs/2011.02675" target="_blank">arXiv:2011.02675</a> [<a href="http://arxiv.org/pdf/2011.02675" target="_blank">pdf</a>]

<h2>Transfer Meta-Learning: Information-Theoretic Bounds and Information Meta-Risk Minimization. (arXiv:2011.02872v2 [cs.LG] UPDATED)</h2>
<h3>Sharu Theresa Jose, Osvaldo Simeone, Giuseppe Durisi</h3>
<p>Meta-learning automatically infers an inductive bias by observing data from a
number of related tasks. The inductive bias is encoded by hyperparameters that
determine aspects of the model class or training algorithm, such as
initialization or learning rate. Meta-learning assumes that the learning tasks
belong to a task environment, and that tasks are drawn from the same task
environment both during meta-training and meta-testing. This, however, may not
hold true in practice. In this paper, we introduce the problem of transfer
meta-learning, in which tasks are drawn from a target task environment during
meta-testing that may differ from the source task environment observed during
meta-training. Novel information-theoretic upper bounds are obtained on the
transfer meta-generalization gap, which measures the difference between the
meta-training loss, available at the meta-learner, and the average loss on
meta-test data from a new, randomly selected, task in the target task
environment. The first bound, on the average transfer meta-generalization gap,
captures the meta-environment shift between source and target task environments
via the KL divergence between source and target data distributions. The second,
PAC-Bayesian bound, and the third, single-draw bound, account for this shift
via the log-likelihood ratio between source and target task distributions.
Furthermore, two transfer meta-learning solutions are introduced. For the
first, termed Empirical Meta-Risk Minimization (EMRM), we derive bounds on the
average optimality gap. The second, referred to as Information Meta-Risk
Minimization (IMRM), is obtained by minimizing the PAC-Bayesian bound. IMRM is
shown via experiments to potentially outperform EMRM.
</p>
<a href="http://arxiv.org/abs/2011.02872" target="_blank">arXiv:2011.02872</a> [<a href="http://arxiv.org/pdf/2011.02872" target="_blank">pdf</a>]

<h2>Measuring Data Collection Quality for Community Healthcare. (arXiv:2011.02962v3 [cs.LG] UPDATED)</h2>
<h3>Ramesha Karunasena, Mohammad Sarparajul Ambiya, Arunesh Sinha, Ruchit Nagar, Saachi Dalal, Divy Thakkar, Milind Tambe</h3>
<p>Machine learning has tremendous potential to provide targeted interventions
in low-resource communities, however the availability of high-quality public
health data is a significant challenge. In this work, we partner with field
experts at a non-governmental organization (NGO) in India to define and test a
data collection quality score for each health worker who collects data. This
challenging unlabeled data problem is handled by building upon domain-expert's
guidance to design a useful data representation that is then clustered to infer
a data quality score. We also provide a more interpretable version of the
score. These scores already provide for a measurement of data collection
quality; in addition, we also predict the quality for future time steps and
find our results to be very accurate. Our work was successfully field tested
and is in the final stages of deployment in Rajasthan, India.
</p>
<a href="http://arxiv.org/abs/2011.02962" target="_blank">arXiv:2011.02962</a> [<a href="http://arxiv.org/pdf/2011.02962" target="_blank">pdf</a>]

<h2>Illumination Normalization by Partially Impossible Encoder-Decoder Cost Function. (arXiv:2011.03428v2 [cs.CV] UPDATED)</h2>
<h3>Steve Dias Da Cruz, Bertram Taetz, Thomas Stifter, Didier Stricker</h3>
<p>Images recorded during the lifetime of computer vision based systems undergo
a wide range of illumination and environmental conditions affecting the
reliability of previously trained machine learning models. Image normalization
is hence a valuable preprocessing component to enhance the models' robustness.
To this end, we introduce a new strategy for the cost function formulation of
encoder-decoder networks to average out all the unimportant information in the
input images (e.g. environmental features and illumination changes) to focus on
the reconstruction of the salient features (e.g. class instances). Our method
exploits the availability of identical sceneries under different illumination
and environmental conditions for which we formulate a partially impossible
reconstruction target: the input image will not convey enough information to
reconstruct the target in its entirety. Its applicability is assessed on three
publicly available datasets. We combine the triplet loss as a regularizer in
the latent space representation and a nearest neighbour search to improve the
generalization to unseen illuminations and class instances. The importance of
the aforementioned post-processing is highlighted on an automotive application.
To this end, we release a synthetic dataset of sceneries from three different
passenger compartments where each scenery is rendered under ten different
illumination and environmental conditions: see https://sviro.kl.dfki.de
</p>
<a href="http://arxiv.org/abs/2011.03428" target="_blank">arXiv:2011.03428</a> [<a href="http://arxiv.org/pdf/2011.03428" target="_blank">pdf</a>]

<h2>Multi-Agent Reinforcement Learning in Time-varying Networked Systems. (arXiv:2006.06555v2 [cs.LG] CROSS LISTED)</h2>
<h3>Yiheng Lin, Guannan Qu, Longbo Huang, Adam Wierman</h3>
<p>We study multi-agent reinforcement learning (MARL) in a time-varying network
of agents. The objective is to find localized policies that maximize the
(discounted) global reward. In general, scalability is a challenge in this
setting because the size of the global state/action space can be exponential in
the number of agents. Scalable algorithms are only known in cases where
dependencies are static, fixed and local, e.g., between neighbors in a fixed,
time-invariant underlying graph. In this work, we propose a Scalable Actor
Critic framework that applies in settings where the dependencies can be
non-local and time-varying, and provide a finite-time error bound that shows
how the convergence rate depends on the speed of information spread in the
network. Additionally, as a byproduct of our analysis, we obtain novel
finite-time convergence results for a general stochastic approximation scheme
and for temporal difference learning with state aggregation, which apply beyond
the setting of RL in networked systems.
</p>
<a href="http://arxiv.org/abs/2006.06555" target="_blank">arXiv:2006.06555</a> [<a href="http://arxiv.org/pdf/2006.06555" target="_blank">pdf</a>]

<h2>SplitEasy: A Practical Approach for Training ML models on Mobile Devices in a split second. (arXiv:2011.04232v1 [cs.LG])</h2>
<h3>Kamalesh Palanisamy, Vivek Khimani, Moin Hussain Moti, Dimitris Chatzopoulos</h3>
<p>Modern mobile devices, although resourceful, cannot train state-of-the-art
machine learning models without the assistance of servers, which require access
to privacy-sensitive user data. Split learning has recently emerged as a
promising technique for training complex deep learning (DL) models on
low-powered mobile devices. The core idea behind this technique is to train the
sensitive layers of a DL model on the mobile devices while offloading the
computationally intensive layers to a server. Although a lot of works have
already explored the effectiveness of split learning in simulated settings, a
usable toolkit for this purpose does not exist. In this work, we propose
SplitEasy, a framework for training ML models on mobile devices using split
learning. Using the abstraction provided by SplitEasy, developers can run
various DL models under split learning setting by making minimal modifications.
We provide a detailed explanation of SplitEasy and perform experiments under
varying networks to demonstrate its versatility. We demonstrate how SplitEasy
can be used to train state-of-the-art models while incurring nearly constant
computational cost on mobile devices.
</p>
<a href="http://arxiv.org/abs/2011.04232" target="_blank">arXiv:2011.04232</a> [<a href="http://arxiv.org/pdf/2011.04232" target="_blank">pdf</a>]

<h2>BayGo: Joint Bayesian Learning and Information-Aware Graph Optimization. (arXiv:2011.04345v1 [cs.LG])</h2>
<h3>Tamara Alshammari, Sumudu Samarakoon, Anis Elgabli, Mehdi Bennis</h3>
<p>This article deals with the problem of distributed machine learning, in which
agents update their models based on their local datasets, and aggregate the
updated models collaboratively and in a fully decentralized manner. In this
paper, we tackle the problem of information heterogeneity arising in
multi-agent networks where the placement of informative agents plays a crucial
role in the learning dynamics. Specifically, we propose BayGo, a novel fully
decentralized joint Bayesian learning and graph optimization framework with
proven fast convergence over a sparse graph. Under our framework, agents are
able to learn and communicate with the most informative agent to their own
learning. Unlike prior works, our framework assumes no prior knowledge of the
data distribution across agents nor does it assume any knowledge of the true
parameter of the system. The proposed alternating minimization based framework
ensures global connectivity in a fully decentralized way while minimizing the
number of communication links. We theoretically show that by optimizing the
proposed objective function, the estimation error of the posterior probability
distribution decreases exponentially at each iteration. Via extensive
simulations, we show that our framework achieves faster convergence and higher
accuracy compared to fully-connected and star topology graphs.
</p>
<a href="http://arxiv.org/abs/2011.04345" target="_blank">arXiv:2011.04345</a> [<a href="http://arxiv.org/pdf/2011.04345" target="_blank">pdf</a>]

<h2>Community Detection by Principle Components Clustering Methods. (arXiv:2011.04377v1 [stat.ML])</h2>
<h3>Huan Qing, Jingli Wang</h3>
<p>Based on the classical Degree Corrected Stochastic Blockmodel (DCSBM) model
for network community detection problem, we propose two novel approaches:
principal component clustering (PCC) and normalized principal component
clustering (NPCC). Without any parameters to be estimated, the PCC method is
simple to be implemented. Under mild conditions, we show that PCC yields
consistent community detection. NPCC is designed based on the combination of
the PCC and the RSC method (Qin &amp; Rohe 2013). Population analysis for NPCC
shows that NPCC returns perfect clustering for the ideal case under DCSBM. PCC
and NPCC is illustrated through synthetic and real-world datasets. Numerical
results show that NPCC provides a significant improvement compare with PCC and
RSC. Moreover, NPCC inherits nice properties of PCC and RSC such that NPCC is
insensitive to the number of eigenvectors to be clustered and the choosing of
the tuning parameter. When dealing with two weak signal networks Simmons and
Caltech, by considering one more eigenvectors for clustering, we provide two
refinements PCC+ and NPCC+ of PCC and NPCC, respectively. Both two refinements
algorithms provide improvement performances compared with their original
algorithms. Especially, NPCC+ provides satisfactory performances on Simmons and
Caltech, with error rates of 121/1137 and 96/590, respectively.
</p>
<a href="http://arxiv.org/abs/2011.04377" target="_blank">arXiv:2011.04377</a> [<a href="http://arxiv.org/pdf/2011.04377" target="_blank">pdf</a>]

<h2>Dual regularized Laplacian spectral clustering methods on community detection. (arXiv:2011.04392v1 [stat.ML])</h2>
<h3>Huan Qing, Jingli Wang</h3>
<p>Spectral clustering methods are widely used for detecting clusters in
networks for community detection, while a small change on the graph Laplacian
matrix could bring a dramatic improvement. In this paper, we propose a dual
regularized graph Laplacian matrix and then employ it to three classical
spectral clustering approaches under the degree-corrected stochastic block
model. If the number of communities is known as $K$, we consider more than $K$
leading eigenvectors and weight them by their corresponding eigenvalues in the
spectral clustering procedure to improve the performance. Three improved
spectral clustering methods are dual regularized spectral clustering (DRSC)
method, dual regularized spectral clustering on Ratios-of-eigenvectors
(DRSCORE) method, and dual regularized symmetrized Laplacian inverse matrix
(DRSLIM) method. Theoretical analysis of DRSC and DRSLIM show that under mild
conditions DRSC and DRSLIM yield stable consistent community detection,
moreover, DRSCORE returns perfect clustering under the ideal case. We compare
the performances of DRSC, DRSCORE and DRSLIM with several spectral methods by
substantial simulated networks and eight real-world networks.
</p>
<a href="http://arxiv.org/abs/2011.04392" target="_blank">arXiv:2011.04392</a> [<a href="http://arxiv.org/pdf/2011.04392" target="_blank">pdf</a>]

<h2>Towards Domain-Agnostic Contrastive Learning. (arXiv:2011.04419v1 [cs.LG])</h2>
<h3>Vikas Verma, Minh-Thang Luong, Kenji Kawaguchi, Hieu Pham, Quoc V. Le</h3>
<p>Despite recent success, most contrastive self-supervised learning methods are
domain-specific, relying heavily on data augmentation techniques that require
knowledge about a particular domain, such as image cropping and rotation. To
overcome such limitation, we propose a novel domain-agnostic approach to
contrastive learning, named DACL, that is applicable to domains where
invariances, and thus, data augmentation techniques, are not readily available.
Key to our approach is the use of Mixup noise to create similar and dissimilar
examples by mixing data samples differently either at the input or hidden-state
levels. To demonstrate the effectiveness of DACL, we conduct experiments across
various domains such as tabular data, images, and graphs. Our results show that
DACL not only outperforms other domain-agnostic noising methods, such as
Gaussian-noise, but also combines well with domain-specific methods, such as
SimCLR, to improve self-supervised visual representation learning. Finally, we
theoretically analyze our method and show advantages over the Gaussian-noise
based contrastive learning approach.
</p>
<a href="http://arxiv.org/abs/2011.04419" target="_blank">arXiv:2011.04419</a> [<a href="http://arxiv.org/pdf/2011.04419" target="_blank">pdf</a>]

<h2>A contribution to Optimal Transport on incomparable spaces. (arXiv:2011.04447v1 [stat.ML])</h2>
<h3>Titouan Vayer</h3>
<p>Optimal Transport is a theory that allows to define geometrical notions of
distance between probability distributions and to find correspondences,
relationships, between sets of points. Many machine learning applications are
derived from this theory, at the frontier between mathematics and optimization.
This thesis proposes to study the complex scenario in which the different data
belong to incomparable spaces. In particular we address the following
questions: how to define and apply Optimal Transport between graphs, between
structured data? How can it be adapted when the data are varied and not
embedded in the same metric space? This thesis proposes a set of Optimal
Transport tools for these different cases. An important part is notably devoted
to the study of the Gromov-Wasserstein distance whose properties allow to
define interesting transport problems on incomparable spaces. More broadly, we
analyze the mathematical properties of the various proposed tools, we establish
algorithmic solutions to compute them and we study their applicability in
numerous machine learning scenarii which cover, in particular, classification,
simplification, partitioning of structured data, as well as heterogeneous
domain adaptation.
</p>
<a href="http://arxiv.org/abs/2011.04447" target="_blank">arXiv:2011.04447</a> [<a href="http://arxiv.org/pdf/2011.04447" target="_blank">pdf</a>]

<h2>A Theory of Universal Learning. (arXiv:2011.04483v1 [cs.LG])</h2>
<h3>Olivier Bousquet, Steve Hanneke, Shay Moran, Ramon van Handel, Amir Yehudayoff</h3>
<p>How quickly can a given class of concepts be learned from examples? It is
common to measure the performance of a supervised machine learning algorithm by
plotting its "learning curve", that is, the decay of the error rate as a
function of the number of training examples. However, the classical theoretical
framework for understanding learnability, the PAC model of Vapnik-Chervonenkis
and Valiant, does not explain the behavior of learning curves: the
distribution-free PAC model of learning can only bound the upper envelope of
the learning curves over all possible data distributions. This does not match
the practice of machine learning, where the data source is typically fixed in
any given scenario, while the learner may choose the number of training
examples on the basis of factors such as computational resources and desired
accuracy.

In this paper, we study an alternative learning model that better captures
such practical aspects of machine learning, but still gives rise to a complete
theory of the learnable in the spirit of the PAC model. More precisely, we
consider the problem of universal learning, which aims to understand the
performance of learning algorithms on every data distribution, but without
requiring uniformity over the distribution. The main result of this paper is a
remarkable trichotomy: there are only three possible rates of universal
learning. More precisely, we show that the learning curves of any given concept
class decay either at an exponential, linear, or arbitrarily slow rates.
Moreover, each of these cases is completely characterized by appropriate
combinatorial parameters, and we exhibit optimal learning algorithms that
achieve the best possible rate in each case.

For concreteness, we consider in this paper only the realizable case, though
analogous results are expected to extend to more general learning scenarios.
</p>
<a href="http://arxiv.org/abs/2011.04483" target="_blank">arXiv:2011.04483</a> [<a href="http://arxiv.org/pdf/2011.04483" target="_blank">pdf</a>]

<h2>Particles to Partial Differential Equations Parsimoniously. (arXiv:2011.04517v1 [stat.ML])</h2>
<h3>Hassan Arbabi, Ioannis Kevrekidis</h3>
<p>Equations governing physico-chemical processes are usually known at
microscopic spatial scales, yet one suspects that there exist equations, e.g.
in the form of Partial Differential Equations (PDEs), that can explain the
system evolution at much coarser, meso- or macroscopic length scales.
Discovering those coarse-grained effective PDEs can lead to considerable
savings in computation-intensive tasks like prediction or control. We propose a
framework combining artificial neural networks with multiscale computation, in
the form of equation-free numerics, for efficient discovery of such macro-scale
PDEs directly from microscopic simulations. Gathering sufficient microscopic
data for training neural networks can be computationally prohibitive;
equation-free numerics enable a more parsimonious collection of training data
by only operating in a sparse subset of the space-time domain. We also propose
using a data-driven approach, based on manifold learning and unnormalized
optimal transport of distributions, to identify macro-scale dependent
variable(s) suitable for the data-driven discovery of said PDEs. This approach
can corroborate physically motivated candidate variables, or introduce new
data-driven variables, in terms of which the coarse-grained effective PDE can
be formulated. We illustrate our approach by extracting coarse-grained
evolution equations from particle-based simulations with a priori unknown
macro-scale variable(s), while significantly reducing the requisite data
collection computational effort.
</p>
<a href="http://arxiv.org/abs/2011.04517" target="_blank">arXiv:2011.04517</a> [<a href="http://arxiv.org/pdf/2011.04517" target="_blank">pdf</a>]

<h2>Spectral clustering on spherical coordinates under the degree-corrected stochastic blockmodel. (arXiv:2011.04558v1 [stat.ML])</h2>
<h3>Francesco Sanna Passino, Nicholas A. Heard, Patrick Rubin-Delanchy</h3>
<p>Spectral clustering is a popular method for community detection in networks
under the assumption of the standard stochastic blockmodel. Taking a matrix
representation of the graph such as the adjacency matrix, the nodes are
clustered on a low dimensional projection obtained from a truncated spectral
decomposition of the matrix. Estimating the number of communities and the
dimension of the reduced latent space well is crucial for good performance of
spectral clustering algorithms. Real-world networks, such as computer networks
studied in cyber-security applications, often present heterogeneous
within-community degree distributions which are better addressed by the
degree-corrected stochastic blockmodel. A novel, model-based method is proposed
in this article for simultaneous and automated selection of the number of
communities and latent dimension for spectral clustering under the
degree-corrected stochastic blockmodel. The method is based on a transformation
to spherical coordinates of the spectral embedding, and on a novel modelling
assumption in the transformed space, which is then embedded into an existing
model selection framework for estimating the number of communities and the
latent dimension. Results show improved performance over competing methods on
simulated and real-world computer network data.
</p>
<a href="http://arxiv.org/abs/2011.04558" target="_blank">arXiv:2011.04558</a> [<a href="http://arxiv.org/pdf/2011.04558" target="_blank">pdf</a>]

<h2>Stable Sample Compression Schemes: New Applications and an Optimal SVM Margin Bound. (arXiv:2011.04586v1 [cs.LG])</h2>
<h3>Steve Hanneke, Aryeh Kontorovich</h3>
<p>We analyze a family of supervised learning algorithms based on sample
compression schemes that are stable, in the sense that removing points from the
training set which were not selected for the compression set does not alter the
resulting classifier. We use this technique to derive a variety of novel or
improved data-dependent generalization bounds for several learning algorithms.
In particular, we prove a new margin bound for SVM, removing a log factor. The
new bound is provably optimal. This resolves a long-standing open question
about the PAC margin bounds achievable by SVM.
</p>
<a href="http://arxiv.org/abs/2011.04586" target="_blank">arXiv:2011.04586</a> [<a href="http://arxiv.org/pdf/2011.04586" target="_blank">pdf</a>]

<h2>Geometry Perspective Of Estimating Learning Capability Of Neural Networks. (arXiv:2011.04588v1 [cs.LG])</h2>
<h3>Ankan Dutta, Arnab Rakshit</h3>
<p>The paper uses statistical and differential geometric motivation to acquire
prior information about the learning capability of an artificial neural network
on a given dataset. The paper considers a broad class of neural networks with
generalized architecture performing simple least square regression with
stochastic gradient descent (SGD). The system characteristics at two critical
epochs in the learning trajectory are analyzed. During some epochs of the
training phase, the system reaches equilibrium with the generalization
capability attaining a maximum. The system can also be coherent with localized,
non-equilibrium states, which is characterized by the stabilization of the
Hessian matrix. The paper proves that neural networks with higher
generalization capability will have a slower convergence rate. The relationship
between the generalization capability with the stability of the neural network
has also been discussed. By correlating the principles of high-energy physics
with the learning theory of neural networks, the paper establishes a variant of
the Complexity-Action conjecture from an artificial neural network perspective.
</p>
<a href="http://arxiv.org/abs/2011.04588" target="_blank">arXiv:2011.04588</a> [<a href="http://arxiv.org/pdf/2011.04588" target="_blank">pdf</a>]

<h2>Numerically Solving Parametric Families of High-Dimensional Kolmogorov Partial Differential Equations via Deep Learning. (arXiv:2011.04602v1 [cs.LG])</h2>
<h3>Julius Berner, Markus Dablander, Philipp Grohs</h3>
<p>We present a deep learning algorithm for the numerical solution of parametric
families of high-dimensional linear Kolmogorov partial differential equations
(PDEs). Our method is based on reformulating the numerical approximation of a
whole family of Kolmogorov PDEs as a single statistical learning problem using
the Feynman-Kac formula. Successful numerical experiments are presented, which
empirically confirm the functionality and efficiency of our proposed algorithm
in the case of heat equations and Black-Scholes option pricing models
parametrized by affine-linear coefficient functions. We show that a single deep
neural network trained on simulated data is capable of learning the solution
functions of an entire family of PDEs on a full space-time region. Most
notably, our numerical observations and theoretical results also demonstrate
that the proposed method does not suffer from the curse of dimensionality,
distinguishing it from almost all standard numerical methods for PDEs.
</p>
<a href="http://arxiv.org/abs/2011.04602" target="_blank">arXiv:2011.04602</a> [<a href="http://arxiv.org/pdf/2011.04602" target="_blank">pdf</a>]

<h2>Causality-aware counterfactual confounding adjustment as an alternative to linear residualization in anticausal prediction tasks based on linear learners. (arXiv:2011.04605v1 [stat.ML])</h2>
<h3>Elias Chaibub Neto</h3>
<p>Linear residualization is a common practice for confounding adjustment in
machine learning (ML) applications. Recently, causality-aware predictive
modeling has been proposed as an alternative causality-inspired approach for
adjusting for confounders. The basic idea is to simulate counterfactual data
that is free from the spurious associations generated by the observed
confounders. In this paper, we compare the linear residualization approach
against the causality-aware confounding adjustment in anticausal prediction
tasks, and show that the causality-aware approach tends to (asymptotically)
outperform the residualization adjustment in terms of predictive performance in
linear learners. Importantly, our results still holds even when the true model
is not linear. We illustrate our results in both regression and classification
tasks, where we compared the causality-aware and residualization approaches
using mean squared errors and classification accuracy in synthetic data
experiments where the linear regression model is mispecified, as well as, when
the linear model is correctly specified. Furthermore, we illustrate how the
causality-aware approach is more stable than residualization with respect to
dataset shifts in the joint distribution of the confounders and outcome
variables.
</p>
<a href="http://arxiv.org/abs/2011.04605" target="_blank">arXiv:2011.04605</a> [<a href="http://arxiv.org/pdf/2011.04605" target="_blank">pdf</a>]

<h2>Bridging Exploration and General Function Approximation in Reinforcement Learning: Provably Efficient Kernel and Neural Value Iterations. (arXiv:2011.04622v1 [cs.LG])</h2>
<h3>Zhuoran Yang, Chi Jin, Zhaoran Wang, Mengdi Wang, Michael I. Jordan</h3>
<p>Reinforcement learning (RL) algorithms combined with modern function
approximators such as kernel functions and deep neural networks have achieved
significant empirical successes in large-scale application problems with a
massive number of states. From a theoretical perspective, however, RL with
functional approximation poses a fundamental challenge to developing algorithms
with provable computational and statistical efficiency, due to the need to take
into consideration both the exploration-exploitation tradeoff that is inherent
in RL and the bias-variance tradeoff that is innate in statistical estimation.
To address such a challenge, focusing on the episodic setting where the
action-value functions are represented by a kernel function or
over-parametrized neural network, we propose the first provable RL algorithm
with both polynomial runtime and sample complexity, without additional
assumptions on the data-generating model. In particular, for both the kernel
and neural settings, we prove that an optimistic modification of the
least-squares value iteration algorithm incurs an
$\tilde{\mathcal{O}}(\delta_{\mathcal{F}} H^2 \sqrt{T})$ regret, where
$\delta_{\mathcal{F}}$ characterizes the intrinsic complexity of the function
class $\mathcal{F}$, $H$ is the length of each episode, and $T$ is the total
number of episodes. Our regret bounds are independent of the number of states
and therefore even allows it to diverge, which exhibits the benefit of function
approximation.
</p>
<a href="http://arxiv.org/abs/2011.04622" target="_blank">arXiv:2011.04622</a> [<a href="http://arxiv.org/pdf/2011.04622" target="_blank">pdf</a>]

<h2>Learning Undirected Graphs in Financial Markets. (arXiv:2005.09958v4 [stat.ML] UPDATED)</h2>
<h3>Jos&#xe9; Vin&#xed;cius de Miranda Cardoso, Daniel P. Palomar</h3>
<p>We investigate the problem of learning undirected graphical models under
Laplacian structural constraints from the point of view of financial market
data. We show that Laplacian constraints have meaningful physical
interpretations related to the market index factor and to the conditional
correlations between stocks. Those interpretations lead to a set of guidelines
that users should be aware of when estimating graphs in financial markets. In
addition, we propose algorithms to learn undirected graphs that account for
stylized facts and tasks intrinsic to financial data such as non-stationarity
and stock clustering.
</p>
<a href="http://arxiv.org/abs/2005.09958" target="_blank">arXiv:2005.09958</a> [<a href="http://arxiv.org/pdf/2005.09958" target="_blank">pdf</a>]

<h2>Multi-Agent Reinforcement Learning in Time-varying Networked Systems. (arXiv:2006.06555v2 [cs.LG] UPDATED)</h2>
<h3>Yiheng Lin, Guannan Qu, Longbo Huang, Adam Wierman</h3>
<p>We study multi-agent reinforcement learning (MARL) in a time-varying network
of agents. The objective is to find localized policies that maximize the
(discounted) global reward. In general, scalability is a challenge in this
setting because the size of the global state/action space can be exponential in
the number of agents. Scalable algorithms are only known in cases where
dependencies are static, fixed and local, e.g., between neighbors in a fixed,
time-invariant underlying graph. In this work, we propose a Scalable Actor
Critic framework that applies in settings where the dependencies can be
non-local and time-varying, and provide a finite-time error bound that shows
how the convergence rate depends on the speed of information spread in the
network. Additionally, as a byproduct of our analysis, we obtain novel
finite-time convergence results for a general stochastic approximation scheme
and for temporal difference learning with state aggregation, which apply beyond
the setting of RL in networked systems.
</p>
<a href="http://arxiv.org/abs/2006.06555" target="_blank">arXiv:2006.06555</a> [<a href="http://arxiv.org/pdf/2006.06555" target="_blank">pdf</a>]

