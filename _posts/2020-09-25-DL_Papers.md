---
title: Latest Deep Learning Papers
date: 2021-03-02 21:00:01 +0800
featured-img: DL-logo
categories: [Deep_Learning]
tags: [Deep_Learning]
mathjax: Yes
---

<h1>Your arXiv Feed (107 Articles)</h1>
<h2>Multiple Convolutional Features in Siamese Networks for Object Tracking. (arXiv:2103.01222v1 [cs.CV])</h2>
<h3>Zhenxi Li, Guillaume-Alexandre Bilodeau, Wassim Bouachir</h3>
<p>Siamese trackers demonstrated high performance in object tracking due to
their balance between accuracy and speed. Unlike classification-based CNNs,
deep similarity networks are specifically designed to address the image
similarity problem, and thus are inherently more appropriate for the tracking
task. However, Siamese trackers mainly use the last convolutional layers for
similarity analysis and target search, which restricts their performance. In
this paper, we argue that using a single convolutional layer as feature
representation is not an optimal choice in a deep similarity framework. We
present a Multiple Features-Siamese Tracker (MFST), a novel tracking algorithm
exploiting several hierarchical feature maps for robust tracking. Since
convolutional layers provide several abstraction levels in characterizing an
object, fusing hierarchical features allows to obtain a richer and more
efficient representation of the target. Moreover, we handle the target
appearance variations by calibrating the deep features extracted from two
different CNN models. Based on this advanced feature representation, our method
achieves high tracking accuracy, while outperforming the standard siamese
tracker on object tracking benchmarks. The source code and trained models are
available at https://github.com/zhenxili96/MFST.
</p>
<a href="http://arxiv.org/abs/2103.01222" target="_blank">arXiv:2103.01222</a> [<a href="http://arxiv.org/pdf/2103.01222" target="_blank">pdf</a>]

<h2>Auto-Exposure Fusion for Single-Image Shadow Removal. (arXiv:2103.01255v1 [cs.CV])</h2>
<h3>Lan Fu, Changqing Zhou, Qing Guo, Felix Juefei-Xu, Hongkai Yu, Wei Feng, Yang Liu, Song Wang</h3>
<p>Shadow removal is still a challenging task due to its inherent
background-dependent and spatial-variant properties, leading to unknown and
diverse shadow patterns. Even powerful state-of-the-art deep neural networks
could hardly recover traceless shadow-removed background. This paper proposes a
new solution for this task by formulating it as an exposure fusion problem to
address the challenges. Intuitively, we can first estimate multiple
over-exposure images w.r.t. the input image to let the shadow regions in these
images have the same color with shadow-free areas in the input image. Then, we
fuse the original input with the over-exposure images to generate the final
shadow-free counterpart. Nevertheless, the spatial-variant property of the
shadow requires the fusion to be sufficiently `smart', that is, it should
automatically select proper over-exposure pixels from different images to make
the final output natural. To address this challenge, we propose the {\bf
shadow-aware FusionNet} that takes the shadow image as input to generate fusion
weight maps across all the over-exposure images. Moreover, we propose the {\bf
boundary-aware RefineNet} to eliminate the remaining shadow trace further. We
conduct extensive experiments on the ISTD, ISTD+, and SRD datasets to validate
our method's effectiveness and show better performance in shadow regions and
comparable performance in non-shadow regions over the state-of-the-art methods.
We release the model and code in
https://github.com/tsingqguo/exposure-fusion-shadow-removal.
</p>
<a href="http://arxiv.org/abs/2103.01255" target="_blank">arXiv:2103.01255</a> [<a href="http://arxiv.org/pdf/2103.01255" target="_blank">pdf</a>]

<h2>A Deep Emulator for Secondary Motion of 3D Characters. (arXiv:2103.01261v1 [cs.CV])</h2>
<h3>Mianlun Zheng, Yi Zhou, Duygu Ceylan, Jernej Barbic</h3>
<p>Fast and light-weight methods for animating 3D characters are desirable in
various applications such as computer games. We present a learning-based
approach to enhance skinning-based animations of 3D characters with vivid
secondary motion effects. We design a neural network that encodes each local
patch of a character simulation mesh where the edges implicitly encode the
internal forces between the neighboring vertices. The network emulates the
ordinary differential equations of the character dynamics, predicting new
vertex positions from the current accelerations, velocities and positions.
Being a local method, our network is independent of the mesh topology and
generalizes to arbitrarily shaped 3D character meshes at test time. We further
represent per-vertex constraints and material properties such as stiffness,
enabling us to easily adjust the dynamics in different parts of the mesh. We
evaluate our method on various character meshes and complex motion sequences.
Our method can be over 30 times more efficient than ground-truth physically
based simulation, and outperforms alternative solutions that provide fast
approximations.
</p>
<a href="http://arxiv.org/abs/2103.01261" target="_blank">arXiv:2103.01261</a> [<a href="http://arxiv.org/pdf/2103.01261" target="_blank">pdf</a>]

<h2>Geometry-Based Grasping of Vine Tomatoes. (arXiv:2103.01272v1 [cs.RO])</h2>
<h3>Taeke de Haan, Padmaja Kulkarni, Robert Babuska</h3>
<p>We propose a geometry-based grasping method for vine tomatoes. It relies on a
computer-vision pipeline to identify the required geometric features of the
tomatoes and of the truss stem. The grasping method then uses a geometric model
of the robotic hand and the truss to determine a suitable grasping location on
the stem. This approach allows for grasping tomato trusses without requiring
delicate contact sensors or complex mechanistic models and under minimal risk
of damaging the tomatoes. Lab experiments were conducted to validate the
proposed methods, using an RGB-D camera and a low-cost robotic manipulator. The
success rate was 83% to 92%, depending on the type of truss.
</p>
<a href="http://arxiv.org/abs/2103.01272" target="_blank">arXiv:2103.01272</a> [<a href="http://arxiv.org/pdf/2103.01272" target="_blank">pdf</a>]

<h2>Continuous control of an underground loader using deep reinforcement learning. (arXiv:2103.01283v1 [cs.RO])</h2>
<h3>Sofi Backman, Daniel Lindmark, Kenneth Bodin, Martin Servin, Joakim M&#xf6;rk, H&#xe5;kan L&#xf6;fgren</h3>
<p>Reinforcement learning control of an underground loader is investigated in
simulated environment, using a multi-agent deep neural network approach. At the
start of each loading cycle, one agent selects the dig position from a depth
camera image of the pile of fragmented rock. A second agent is responsible for
continuous control of the vehicle, with the goal of filling the bucket at the
selected loading point, while avoiding collisions, getting stuck, or losing
ground traction. It relies on motion and force sensors, as well as on camera
and lidar. Using a soft actor-critic algorithm the agents learn policies for
efficient bucket filling over many subsequent loading cycles, with clear
ability to adapt to the changing environment. The best results, on average 75%
of the max capacity, are obtained when including a penalty for energy usage in
the reward.
</p>
<a href="http://arxiv.org/abs/2103.01283" target="_blank">arXiv:2103.01283</a> [<a href="http://arxiv.org/pdf/2103.01283" target="_blank">pdf</a>]

<h2>Maximal function pooling with applications. (arXiv:2103.01292v1 [cs.CV])</h2>
<h3>Wojciech Czaja, Weilin Li, Yiran Li, Mike Pekala</h3>
<p>Inspired by the Hardy-Littlewood maximal function, we propose a novel pooling
strategy which is called maxfun pooling. It is presented both as a viable
alternative to some of the most popular pooling functions, such as max pooling
and average pooling, and as a way of interpolating between these two
algorithms. We demonstrate the features of maxfun pooling with two
applications: first in the context of convolutional sparse coding, and then for
image classification.
</p>
<a href="http://arxiv.org/abs/2103.01292" target="_blank">arXiv:2103.01292</a> [<a href="http://arxiv.org/pdf/2103.01292" target="_blank">pdf</a>]

<h2>Learning Multimodal Contact-Rich Skills from Demonstrations Without Reward Engineering. (arXiv:2103.01296v1 [cs.RO])</h2>
<h3>Mythra V. Balakuntala, Upinder Kaur, Xin Ma, Juan Wachs, Richard M. Voyles</h3>
<p>Everyday contact-rich tasks, such as peeling, cleaning, and writing, demand
multimodal perception for effective and precise task execution. However, these
present a novel challenge to robots as they lack the ability to combine these
multimodal stimuli for performing contact-rich tasks. Learning-based methods
have attempted to model multi-modal contact-rich tasks, but they often require
extensive training examples and task-specific reward functions which limits
their practicality and scope. Hence, we propose a generalizable model-free
learning-from-demonstration framework for robots to learn contact-rich skills
without explicit reward engineering. We present a novel multi-modal sensor data
representation which improves the learning performance for contact-rich skills.
We performed training and experiments using the real-life Sawyer robot for
three everyday contact-rich skills -- cleaning, writing, and peeling. Notably,
the framework achieves a success rate of 100% for the peeling and writing
skill, and 80% for the cleaning skill. Hence, this skill learning framework can
be extended for learning other physical manipulation skills.
</p>
<a href="http://arxiv.org/abs/2103.01296" target="_blank">arXiv:2103.01296</a> [<a href="http://arxiv.org/pdf/2103.01296" target="_blank">pdf</a>]

<h2>Coarse-Fine Networks for Temporal Activity Detection in Videos. (arXiv:2103.01302v1 [cs.CV])</h2>
<h3>Kumara Kahatapitiya, Michael S. Ryoo</h3>
<p>In this paper, we introduce 'Coarse-Fine Networks', a two-stream architecture
which benefits from different abstractions of temporal resolution to learn
better video representations for long-term motion. Traditional Video models
process inputs at one (or few) fixed temporal resolution without any dynamic
frame selection. However, we argue that, processing multiple temporal
resolutions of the input and doing so dynamically by learning to estimate the
importance of each frame can largely improve video representations, specially
in the domain of temporal activity localization. To this end, we propose (1)
`Grid Pool', a learned temporal downsampling layer to extract coarse features,
and, (2) `Multi-stage Fusion', a spatio-temporal attention mechanism to fuse a
fine-grained context with the coarse features. We show that our method can
outperform the state-of-the-arts for action detection in public datasets
including Charades with a significantly reduced compute and memory footprint.
</p>
<a href="http://arxiv.org/abs/2103.01302" target="_blank">arXiv:2103.01302</a> [<a href="http://arxiv.org/pdf/2103.01302" target="_blank">pdf</a>]

<h2>Exploring the high dimensional geometry of HSI features. (arXiv:2103.01303v1 [cs.CV])</h2>
<h3>Wojciech Czaja, Ilya Kavalerov, Weilin Li</h3>
<p>We explore feature space geometries induced by the 3-D Fourier scattering
transform and deep neural network with extended attribute profiles on four
standard hyperspectral images. We examine the distances and angles of class
means, the variability of classes, and their low-dimensional structures. These
statistics are compared to that of raw features, and our results provide
insight into the vastly different properties of these two methods. We also
explore a connection with the newly observed deep learning phenomenon of neural
collapse.
</p>
<a href="http://arxiv.org/abs/2103.01303" target="_blank">arXiv:2103.01303</a> [<a href="http://arxiv.org/pdf/2103.01303" target="_blank">pdf</a>]

<h2>Scalable Scene Flow from Point Clouds in the Real World. (arXiv:2103.01306v1 [cs.CV])</h2>
<h3>Philipp Jund, Chris Sweeney, Nichola Abdo, Zhifeng Chen, Jonathon Shlens</h3>
<p>Autonomous vehicles operate in highly dynamic environments necessitating an
accurate assessment of which aspects of a scene are moving and where they are
moving to. A popular approach to 3D motion estimation -- termed scene flow --
is to employ 3D point cloud data from consecutive LiDAR scans, although such
approaches have been limited by the small size of real-world, annotated LiDAR
data. In this work, we introduce a new large scale benchmark for scene flow
based on the Waymo Open Dataset. The dataset is $\sim$1,000$\times$ larger than
previous real-world datasets in terms of the number of annotated frames and is
derived from the corresponding tracked 3D objects. We demonstrate how previous
works were bounded based on the amount of real LiDAR data available, suggesting
that larger datasets are required to achieve state-of-the-art predictive
performance. Furthermore, we show how previous heuristics for operating on
point clouds such as artificial down-sampling heavily degrade performance,
motivating a new class of models that are tractable on the full point cloud. To
address this issue, we introduce the model architecture \modelname~that
provides real time inference on the full point cloud. Finally, we demonstrate
that this problem is amenable to techniques from semi-supervised learning by
highlighting open problems for generalizing methods for predicting motion on
unlabeled objects. We hope that this dataset may provide new opportunities for
developing real world scene flow systems and motivate a new class of machine
learning problems.
</p>
<a href="http://arxiv.org/abs/2103.01306" target="_blank">arXiv:2103.01306</a> [<a href="http://arxiv.org/pdf/2103.01306" target="_blank">pdf</a>]

<h2>Exploring Complementary Strengths of Invariant and Equivariant Representations for Few-Shot Learning. (arXiv:2103.01315v1 [cs.CV])</h2>
<h3>Mamshad Nayeem Rizve, Salman Khan, Fahad Shahbaz Khan, Mubarak Shah</h3>
<p>In many real-world problems, collecting a large number of labeled samples is
infeasible. Few-shot learning (FSL) is the dominant approach to address this
issue, where the objective is to quickly adapt to novel categories in presence
of a limited number of samples. FSL tasks have been predominantly solved by
leveraging the ideas from gradient-based meta-learning and metric learning
approaches. However, recent works have demonstrated the significance of
powerful feature representations with a simple embedding network that can
outperform existing sophisticated FSL algorithms. In this work, we build on
this insight and propose a novel training mechanism that simultaneously
enforces equivariance and invariance to a general set of geometric
transformations. Equivariance or invariance has been employed standalone in the
previous works; however, to the best of our knowledge, they have not been used
jointly. Simultaneous optimization for both of these contrasting objectives
allows the model to jointly learn features that are not only independent of the
input transformation but also the features that encode the structure of
geometric transformations. These complementary sets of features help generalize
well to novel classes with only a few data samples. We achieve additional
improvements by incorporating a novel self-supervised distillation objective.
Our extensive experimentation shows that even without knowledge distillation
our proposed method can outperform current state-of-the-art FSL methods on five
popular benchmark datasets.
</p>
<a href="http://arxiv.org/abs/2103.01315" target="_blank">arXiv:2103.01315</a> [<a href="http://arxiv.org/pdf/2103.01315" target="_blank">pdf</a>]

<h2>LTO: Lazy Trajectory Optimization with Graph-Search Planning for High DOF Robots in Cluttered Environments. (arXiv:2103.01333v1 [cs.RO])</h2>
<h3>Yuki Shirai, Xuan Lin, Ankur Mehta, Dennis Hong</h3>
<p>Although Trajectory Optimization (TO) is one of the most powerful motion
planning tools, it suffers from expensive computational complexity as a time
horizon increases in cluttered environments. It can also fail to converge to a
globally optimal solution. In this paper, we present Lazy Trajectory
Optimization (LTO) that unifies local short-horizon TO and global Graph-Search
Planning (GSP) to generate a long-horizon global optimal trajectory. LTO solves
TO with the same constraints as the original long-horizon TO with improved time
complexity. We also propose a TO-aware cost function that can balance both
solution cost and planning time. Since LTO solves many nearly identical TO in a
roadmap, it can provide an informed warm-start for TO to accelerate the
planning process. We also present proofs of the computational complexity and
optimality of LTO. Finally, we demonstrate LTO's performance on motion planning
problems for a 2 DOF free-flying robot and a 21 DOF legged robot, showing that
LTO outperforms existing algorithms in terms of its runtime and reliability.
</p>
<a href="http://arxiv.org/abs/2103.01333" target="_blank">arXiv:2103.01333</a> [<a href="http://arxiv.org/pdf/2103.01333" target="_blank">pdf</a>]

<h2>Hierarchical and Partially Observable Goal-driven Policy Learning with Goals Relational Graph. (arXiv:2103.01350v1 [cs.CV])</h2>
<h3>Xin Ye, Yezhou Yang</h3>
<p>We present a novel two-layer hierarchical reinforcement learning approach
equipped with a Goals Relational Graph (GRG) for tackling the partially
observable goal-driven task, such as goal-driven visual navigation. Our GRG
captures the underlying relations of all goals in the goal space through a
Dirichlet-categorical process that facilitates: 1) the high-level network
raising a sub-goal towards achieving a designated final goal; 2) the low-level
network towards an optimal policy; and 3) the overall system generalizing
unseen environments and goals. We evaluate our approach with two settings of
partially observable goal-driven tasks -- a grid-world domain and a robotic
object search task. Our experimental results show that our approach exhibits
superior generalization performance on both unseen environments and new goals.
</p>
<a href="http://arxiv.org/abs/2103.01350" target="_blank">arXiv:2103.01350</a> [<a href="http://arxiv.org/pdf/2103.01350" target="_blank">pdf</a>]

<h2>There is More than Meets the Eye: Self-Supervised Multi-Object Detection and Tracking with Sound by Distilling Multimodal Knowledge. (arXiv:2103.01353v1 [cs.CV])</h2>
<h3>Francisco Rivera Valverde, Juana Valeria Hurtado, Abhinav Valada</h3>
<p>Attributes of sound inherent to objects can provide valuable cues to learn
rich representations for object detection and tracking. Furthermore, the
co-occurrence of audiovisual events in videos can be exploited to localize
objects over the image field by solely monitoring the sound in the environment.
Thus far, this has only been feasible in scenarios where the camera is static
and for single object detection. Moreover, the robustness of these methods has
been limited as they primarily rely on RGB images which are highly susceptible
to illumination and weather changes. In this work, we present the novel
self-supervised MM-DistillNet framework consisting of multiple teachers that
leverage diverse modalities including RGB, depth and thermal images, to
simultaneously exploit complementary cues and distill knowledge into a single
audio student network. We propose the new MTA loss function that facilitates
the distillation of information from multimodal teachers in a self-supervised
manner. Additionally, we propose a novel self-supervised pretext task for the
audio student that enables us to not rely on labor-intensive manual
annotations. We introduce a large-scale multimodal dataset with over 113,000
time-synchronized frames of RGB, depth, thermal, and audio modalities.
Extensive experiments demonstrate that our approach outperforms
state-of-the-art methods while being able to detect multiple objects using only
sound during inference and even while moving.
</p>
<a href="http://arxiv.org/abs/2103.01353" target="_blank">arXiv:2103.01353</a> [<a href="http://arxiv.org/pdf/2103.01353" target="_blank">pdf</a>]

<h2>Brain Programming is Immune to Adversarial Attacks: Towards Accurate and Robust Image Classification using Symbolic Learning. (arXiv:2103.01359v1 [cs.CV])</h2>
<h3>Gerardo Ibarra-Vazquez, Gustavo Olague, Mariana Chan-Ley, Cesar Puente, Carlos Soubervielle-Montalvo</h3>
<p>In recent years, the security concerns about the vulnerability of Deep
Convolutional Neural Networks (DCNN) to Adversarial Attacks (AA) in the form of
small modifications to the input image almost invisible to human vision make
their predictions untrustworthy. Therefore, it is necessary to provide
robustness to adversarial examples in addition to an accurate score when
developing a new classifier. In this work, we perform a comparative study of
the effects of AA on the complex problem of art media categorization, which
involves a sophisticated analysis of features to classify a fine collection of
artworks. We tested a prevailing bag of visual words approach from computer
vision, four state-of-the-art DCNN models (AlexNet, VGG, ResNet, ResNet101),
and the Brain Programming (BP) algorithm. In this study, we analyze the
algorithms' performance using accuracy. Besides, we use the accuracy ratio
between adversarial examples and clean images to measure robustness. Moreover,
we propose a statistical analysis of each classifier's predictions' confidence
to corroborate the results. We confirm that BP predictions' change was below
2\% using adversarial examples computed with the fast gradient sign method.
Also, considering the multiple pixel attack, BP obtained four out of seven
classes without changes and the rest with a maximum error of 4\% in the
predictions. Finally, BP also gets four categories using adversarial patches
without changes and for the remaining three classes with a variation of 1\%.
Additionally, the statistical analysis showed that the predictions' confidence
of BP were not significantly different for each pair of clean and perturbed
images in every experiment. These results prove BP's robustness against
adversarial examples compared to DCNN and handcrafted features methods, whose
performance on the art media classification was compromised with the proposed
perturbations.
</p>
<a href="http://arxiv.org/abs/2103.01359" target="_blank">arXiv:2103.01359</a> [<a href="http://arxiv.org/pdf/2103.01359" target="_blank">pdf</a>]

<h2>Multiclass Burn Wound Image Classification Using Deep Convolutional Neural Networks. (arXiv:2103.01361v1 [cs.CV])</h2>
<h3>Behrouz Rostami, Jeffrey Niezgoda, Sandeep Gopalakrishnan, Zeyun Yu</h3>
<p>Millions of people are affected by acute and chronic wounds yearly across the
world. Continuous wound monitoring is important for wound specialists to allow
more accurate diagnosis and optimization of management protocols. Machine
Learning-based classification approaches provide optimal care strategies
resulting in more reliable outcomes, cost savings, healing time reduction, and
improved patient satisfaction. In this study, we use a deep learning-based
method to classify burn wound images into two or three different categories
based on the wound conditions. A pre-trained deep convolutional neural network,
AlexNet, is fine-tuned using a burn wound image dataset and utilized as the
classifier. The classifier's performance is evaluated using classification
metrics such as accuracy, precision, and recall as well as confusion matrix. A
comparison with previous works that used the same dataset showed that our
designed classifier improved the classification accuracy by more than 8%.
</p>
<a href="http://arxiv.org/abs/2103.01361" target="_blank">arXiv:2103.01361</a> [<a href="http://arxiv.org/pdf/2103.01361" target="_blank">pdf</a>]

<h2>BPActuators: Lightweight and Low-Cost Soft Actuators by Balloons and Plastics. (arXiv:2103.01409v1 [cs.RO])</h2>
<h3>Qiukai Qi, Shogo Yoshida, Genki Kakihana, Takuma Torii, Van Anh Ho, Haoran Xie</h3>
<p>To increase the awareness and impact, soft robotics needs to go beyond the
lab environment and should be readily accessible to those even with no robotic
expertise. However, most prevailing manufacturing methodologies require either
professional equipment or materials that are not usually available to common
people, thereby constraining the accessibility of soft robotics. In this
communication, we propose a lightweight and low-cost soft bending actuator,
called BPActuator, that can be easily fabricated with plastics and balloons. We
fabricated a range of actuators with various morphology for characterization in
terms of deformation and load-bearing capacity, and demonstrated that they can
bend up to 35 degrees and exert force at the tip around 0.070$\pm$0.015N, which
is over 5 times higher than their average gravity. We further implemented a
gripper with three fingers using the proposed actuators, and found that the
gripper can realize human-like grasp of a range of daily objects. The gripper
can lift objects at least 8 times heavier than its own weight. Furthermore, the
BPActuator is cost effective and each costs about 0.22 USD. Given these
advantages, the BPActuators are expected to significantly improve the
accessibility of soft robotics to a wider group without robotic expertise.
</p>
<a href="http://arxiv.org/abs/2103.01409" target="_blank">arXiv:2103.01409</a> [<a href="http://arxiv.org/pdf/2103.01409" target="_blank">pdf</a>]

<h2>A Survey of Deep Learning Techniques for Weed Detection from Images. (arXiv:2103.01415v1 [cs.CV])</h2>
<h3>A S M Mahmudul Hasan, Ferdous Sohel, Dean Diepeveen, Hamid Laga, Michael G.K. Jones</h3>
<p>The rapid advances in Deep Learning (DL) techniques have enabled rapid
detection, localisation, and recognition of objects from images or videos. DL
techniques are now being used in many applications related to agriculture and
farming. Automatic detection and classification of weeds can play an important
role in weed management and so contribute to higher yields. Weed detection in
crops from imagery is inherently a challenging problem because both weeds and
crops have similar colours ('green-on-green'), and their shapes and texture can
be very similar at the growth phase. Also, a crop in one setting can be
considered a weed in another. In addition to their detection, the recognition
of specific weed species is essential so that targeted controlling mechanisms
(e.g. appropriate herbicides and correct doses) can be applied. In this paper,
we review existing deep learning-based weed detection and classification
techniques. We cover the detailed literature on four main procedures, i.e.,
data acquisition, dataset preparation, DL techniques employed for detection,
location and classification of weeds in crops, and evaluation metrics
approaches. We found that most studies applied supervised learning techniques,
they achieved high classification accuracy by fine-tuning pre-trained models on
any plant dataset, and past experiments have already achieved high accuracy
when a large amount of labelled data is available.
</p>
<a href="http://arxiv.org/abs/2103.01415" target="_blank">arXiv:2103.01415</a> [<a href="http://arxiv.org/pdf/2103.01415" target="_blank">pdf</a>]

<h2>Learning Robotic Manipulation Tasks through Visual Planning. (arXiv:2103.01434v1 [cs.RO])</h2>
<h3>Sulabh Kumra, Shirin Josh, Ferat Sahin</h3>
<p>Multi-step manipulation tasks in unstructured environments are extremely
challenging for a robot to learn. Such tasks interlace high-level reasoning
that consists of the expected states that can be attained to achieve an overall
task and low-level reasoning that decides what actions will yield these states.
We propose a model-free deep reinforcement learning method to learn these
multi-step manipulation tasks. We introduce a Robotic Manipulation Network
(RoManNet) which is a vision-based deep reinforcement learning algorithm to
learn the action-value functions and project manipulation action candidates. We
define a Task Progress based Gaussian (TPG) reward function that computes the
reward based on actions that lead to successful motion primitives and progress
towards the overall task goal. We further introduce a Loss Adjusted Exploration
(LAE) policy that determines actions from the action candidates according to
the Boltzmann distribution of loss estimates. We demonstrate the effectiveness
of our approaches by training RoManNet to learn several challenging multi-step
robotic manipulation tasks. Empirical results show that our method outperforms
the existing methods and achieves state-of-the-art results. The ablation
studies show that TPG and LAE are especially beneficial for tasks like multiple
block stacking. Code is available at: https://github.com/skumra/romannet
</p>
<a href="http://arxiv.org/abs/2103.01434" target="_blank">arXiv:2103.01434</a> [<a href="http://arxiv.org/pdf/2103.01434" target="_blank">pdf</a>]

<h2>All at Once Network Quantization via Collaborative Knowledge Transfer. (arXiv:2103.01435v1 [cs.CV])</h2>
<h3>Ximeng Sun, Rameswar Panda, Chun-Fu Chen, Naigang Wang, Bowen Pan Kailash Gopalakrishnan, Aude Oliva, Rogerio Feris, Kate Saenko</h3>
<p>Network quantization has rapidly become one of the most widely used methods
to compress and accelerate deep neural networks on edge devices. While existing
approaches offer impressive results on common benchmark datasets, they
generally repeat the quantization process and retrain the low-precision network
from scratch, leading to different networks tailored for different resource
constraints. This limits scalable deployment of deep networks in many
real-world applications, where in practice dynamic changes in bit-width are
often desired. All at Once quantization addresses this problem, by flexibly
adjusting the bit-width of a single deep network during inference, without
requiring re-training or additional memory to store separate models, for
instant adaptation in different scenarios. In this paper, we develop a novel
collaborative knowledge transfer approach for efficiently training the
all-at-once quantization network. Specifically, we propose an adaptive
selection strategy to choose a high-precision \enquote{teacher} for
transferring knowledge to the low-precision student while jointly optimizing
the model with all bit-widths. Furthermore, to effectively transfer knowledge,
we develop a dynamic block swapping method by randomly replacing the blocks in
the lower-precision student network with the corresponding blocks in the
higher-precision teacher network. Extensive experiments on several challenging
and diverse datasets for both image and video classification well demonstrate
the efficacy of our proposed approach over state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2103.01435" target="_blank">arXiv:2103.01435</a> [<a href="http://arxiv.org/pdf/2103.01435" target="_blank">pdf</a>]

<h2>Interpretable Hyperspectral AI: When Non-Convex Modeling meets Hyperspectral Remote Sensing. (arXiv:2103.01449v1 [cs.CV])</h2>
<h3>Danfeng Hong, Wei He, Naoto Yokoya, Jing Yao, Lianru Gao, Liangpei Zhang, Jocelyn Chanussot, Xiao Xiang Zhu</h3>
<p>Hyperspectral imaging, also known as image spectrometry, is a landmark
technique in geoscience and remote sensing (RS). In the past decade, enormous
efforts have been made to process and analyze these hyperspectral (HS) products
mainly by means of seasoned experts. However, with the ever-growing volume of
data, the bulk of costs in manpower and material resources poses new challenges
on reducing the burden of manual labor and improving efficiency. For this
reason, it is, therefore, urgent to develop more intelligent and automatic
approaches for various HS RS applications. Machine learning (ML) tools with
convex optimization have successfully undertaken the tasks of numerous
artificial intelligence (AI)-related applications. However, their ability in
handling complex practical problems remains limited, particularly for HS data,
due to the effects of various spectral variabilities in the process of HS
imaging and the complexity and redundancy of higher dimensional HS signals.
Compared to the convex models, non-convex modeling, which is capable of
characterizing more complex real scenes and providing the model
interpretability technically and theoretically, has been proven to be a
feasible solution to reduce the gap between challenging HS vision tasks and
currently advanced intelligent data processing models.
</p>
<a href="http://arxiv.org/abs/2103.01449" target="_blank">arXiv:2103.01449</a> [<a href="http://arxiv.org/pdf/2103.01449" target="_blank">pdf</a>]

<h2>AttriMeter: An Attribute-guided Metric Interpreter for Person Re-Identification. (arXiv:2103.01451v1 [cs.CV])</h2>
<h3>Xiaodong Chen, Xinchen Liu, Wu Liu, Xiao-Ping Zhang, Yongdong Zhang, Tao Mei</h3>
<p>Person Re-identification (ReID) has achieved significant improvement due to
the adoption of Convolutional Neural Networks (CNNs). However, person ReID
systems only provide a distance or similarity when matching two persons, which
makes users hardly understand why they are similar or not. Therefore, we
propose an Attribute-guided Metric Interpreter, named AttriMeter, to
semantically and quantitatively explain the results of CNN-based ReID models.
The AttriMeter has a pluggable structure that can be grafted on arbitrary
target models, i.e., the ReID models that need to be interpreted. With an
attribute decomposition head, it can learn to generate a group of
attribute-guided attention maps (AAMs) from the target model. By applying AAMs
to features of two persons from the target model, their distance will be
decomposed into a set of attribute-guided components that can measure the
contributions of individual attributes. Moreover, we design a distance
distillation loss to guarantee the consistency between the results from the
target model and the decomposed components from AttriMeter, and an attribute
prior loss to eliminate the biases caused by the unbalanced distribution of
attributes. Finally, extensive experiments and analysis on a variety of ReID
models and datasets show the effectiveness of AttriMeter.
</p>
<a href="http://arxiv.org/abs/2103.01451" target="_blank">arXiv:2103.01451</a> [<a href="http://arxiv.org/pdf/2103.01451" target="_blank">pdf</a>]

<h2>Image-to-image Translation via Hierarchical Style Disentanglement. (arXiv:2103.01456v1 [cs.CV])</h2>
<h3>Xinyang Li, Shengchuan Zhang, Jie Hu, Liujuan Cao, Xiaopeng Hong, Xudong Mao, Feiyue Huang, Yongjian Wu, Rongrong Ji</h3>
<p>Recently, image-to-image translation has made significant progress in
achieving both multi-label (\ie, translation conditioned on different labels)
and multi-style (\ie, generation with diverse styles) tasks. However, due to
the unexplored independence and exclusiveness in the labels, existing endeavors
are defeated by involving uncontrolled manipulations to the translation
results. In this paper, we propose Hierarchical Style Disentanglement (HiSD) to
address this issue. Specifically, we organize the labels into a hierarchical
tree structure, in which independent tags, exclusive attributes, and
disentangled styles are allocated from top to bottom. Correspondingly, a new
translation process is designed to adapt the above structure, in which the
styles are identified for controllable translations. Both qualitative and
quantitative results on the CelebA-HQ dataset verify the ability of the
proposed HiSD. We hope our method will serve as a solid baseline and provide
fresh insights with the hierarchically organized annotations for future
research in image-to-image translation. The code has been released at
https://github.com/imlixinyang/HiSD.
</p>
<a href="http://arxiv.org/abs/2103.01456" target="_blank">arXiv:2103.01456</a> [<a href="http://arxiv.org/pdf/2103.01456" target="_blank">pdf</a>]

<h2>Diffusion Probabilistic Models for 3D Point Cloud Generation. (arXiv:2103.01458v1 [cs.CV])</h2>
<h3>Shitong Luo, Wei Hu</h3>
<p>We present a probabilistic model for point cloud generation, which is
critical for various 3D vision tasks such as shape completion, upsampling,
synthesis and data augmentation. Inspired by the diffusion process in
non-equilibrium thermodynamics, we view points in point clouds as particles in
a thermodynamic system in contact with a heat bath, which diffuse from the
original distribution to a noise distribution. Point cloud generation thus
amounts to learning the reverse diffusion process that transforms the noise
distribution to the distribution of a desired shape. Specifically, we propose
to model the reverse diffusion process for point clouds as a Markov chain
conditioned on certain shape latent. We derive the variational bound in closed
form for training and provide implementations of the model. Experimental
results demonstrate that our model achieves the state-of-the-art performance in
point cloud generation and auto-encoding. The code is available at
\url{https://github.com/luost26/diffusion-point-cloud}.
</p>
<a href="http://arxiv.org/abs/2103.01458" target="_blank">arXiv:2103.01458</a> [<a href="http://arxiv.org/pdf/2103.01458" target="_blank">pdf</a>]

<h2>NavTuner: Learning a Scene-Sensitive Family of Navigation Policies. (arXiv:2103.01464v1 [cs.RO])</h2>
<h3>Haoxin Ma, Justin S. Smith, Patricio A. Vela</h3>
<p>The advent of deep learning has inspired research into end-to-end learning
for a variety of problem domains in robotics. For navigation, the resulting
methods may not have the generalization properties desired let alone match the
performance of traditional methods. Instead of learning a navigation policy, we
explore learning an adaptive policy in the parameter space of an existing
navigation module. Having adaptive parameters provides the navigation module
with a family of policies that can be dynamically reconfigured based on the
local scene structure, and addresses the common assertion in machine learning
that engineered solutions are inflexible. Of the methods tested, reinforcement
learning (RL) is shown to provide a significant performance boost to a modern
navigation method through reduced sensitivity of its success rate to
environmental clutter. The outcomes indicate that RL as a meta-policy learner,
or dynamic parameter tuner, effectively robustifies algorithms sensitive to
external, measurable nuisance factors.
</p>
<a href="http://arxiv.org/abs/2103.01464" target="_blank">arXiv:2103.01464</a> [<a href="http://arxiv.org/pdf/2103.01464" target="_blank">pdf</a>]

<h2>Depth from Camera Motion and Object Detection. (arXiv:2103.01468v1 [cs.CV])</h2>
<h3>Brent A. Griffin, Jason J. Corso</h3>
<p>This paper addresses the problem of learning to estimate the depth of
detected objects given some measurement of camera motion (e.g., from robot
kinematics or vehicle odometry). We achieve this by 1) designing a recurrent
neural network (DBox) that estimates the depth of objects using a generalized
representation of bounding boxes and uncalibrated camera movement and 2)
introducing the Object Depth via Motion and Detection Dataset (ODMD). ODMD
training data are extensible and configurable, and the ODMD benchmark includes
21,600 examples across four validation and test sets. These sets include mobile
robot experiments using an end-effector camera to locate objects from the YCB
dataset and examples with perturbations added to camera motion or bounding box
data. In addition to the ODMD benchmark, we evaluate DBox in other monocular
application domains, achieving state-of-the-art results on existing driving and
robotics benchmarks and estimating the depth of objects using a camera phone.
</p>
<a href="http://arxiv.org/abs/2103.01468" target="_blank">arXiv:2103.01468</a> [<a href="http://arxiv.org/pdf/2103.01468" target="_blank">pdf</a>]

<h2>Patch-NetVLAD: Multi-Scale Fusion of Locally-Global Descriptors for Place Recognition. (arXiv:2103.01486v1 [cs.CV])</h2>
<h3>Stephen Hausler, Sourav Garg, Ming Xu, Michael Milford, Tobias Fischer</h3>
<p>Visual Place Recognition is a challenging task for robotics and autonomous
systems, which must deal with the twin problems of appearance and viewpoint
change in an always changing world. This paper introduces Patch-NetVLAD, which
provides a novel formulation for combining the advantages of both local and
global descriptor methods by deriving patch-level features from NetVLAD
residuals. Unlike the fixed spatial neighborhood regime of existing local
keypoint features, our method enables aggregation and matching of deep-learned
local features defined over the feature-space grid. We further introduce a
multi-scale fusion of patch features that have complementary scales (i.e. patch
sizes) via an integral feature space and show that the fused features are
highly invariant to both condition (season, structure, and illumination) and
viewpoint (translation and rotation) changes. Patch-NetVLAD outperforms both
global and local feature descriptor-based methods with comparable compute,
achieving state-of-the-art visual place recognition results on a range of
challenging real-world datasets, including winning the Facebook Mapillary
Visual Place Recognition Challenge at ECCV2020. It is also adaptable to user
requirements, with a speed-optimised version operating over an order of
magnitude faster than the state-of-the-art. By combining superior performance
with improved computational efficiency in a configurable framework,
Patch-NetVLAD is well suited to enhance both stand-alone place recognition
capabilities and the overall performance of SLAM systems.
</p>
<a href="http://arxiv.org/abs/2103.01486" target="_blank">arXiv:2103.01486</a> [<a href="http://arxiv.org/pdf/2103.01486" target="_blank">pdf</a>]

<h2>Avoiding Degeneracy for Monocular Visual SLAM with Point and Line Features. (arXiv:2103.01501v1 [cs.RO])</h2>
<h3>Hyunjun Lim, Yeeun Kim, Kwangik Jung, Sumin Hu, Hyun Myung</h3>
<p>In this paper, a degeneracy avoidance method for a point and line based
visual SLAM algorithm is proposed. Visual SLAM predominantly uses point
features. However, point features lack robustness in low texture and
illuminance variant environments. Therefore, line features are used to
compensate the weaknesses of point features. In addition, point features are
poor in representing discernable features for the naked eye, meaning mapped
point features cannot be recognized. To overcome the limitations above, line
features were actively employed in previous studies. However, since degeneracy
arises in the process of using line features, this paper attempts to solve this
problem. First, a simple method to identify degenerate lines is presented. In
addition, a novel structural constraint is proposed to avoid the degeneracy
problem. At last, a point and line based monocular SLAM system using a robust
optical-flow based lien tracking method is implemented. The results are
verified using experiments with the EuRoC dataset and compared with other
state-of-the-art algorithms. It is proven that our method yields more accurate
localization as well as mapping results.
</p>
<a href="http://arxiv.org/abs/2103.01501" target="_blank">arXiv:2103.01501</a> [<a href="http://arxiv.org/pdf/2103.01501" target="_blank">pdf</a>]

<h2>When Age-Invariant Face Recognition Meets Face Age Synthesis: A Multi-Task Learning Framework. (arXiv:2103.01520v1 [cs.CV])</h2>
<h3>Zhizhong Huang, Junping Zhang, Hongming Shan</h3>
<p>To minimize the effects of age variation in face recognition, previous work
either extracts identity-related discriminative features by minimizing the
correlation between identity- and age-related features, called age-invariant
face recognition (AIFR), or removes age variation by transforming the faces of
different age groups into the same age group, called face age synthesis (FAS);
however, the former lacks visual results for model interpretation while the
latter suffers from artifacts compromising downstream recognition. Therefore,
this paper proposes a unified, multi-task framework to jointly handle these two
tasks, termed \methodname, which can learn age-invariant identity-related
representation while achieving pleasing face synthesis.

Specifically, we first decompose the mixed face feature into two uncorrelated
components -- identity- and age-related feature -- through an attention
mechanism, and then decorrelate these two components using multi-task training
and continuous domain adaption. In contrast to the conventional one-hot
encoding that achieves group-level FAS, we propose a novel identity conditional
module to achieve identity-level FAS, with a weight-sharing strategy to improve
the age smoothness of synthesized faces. In addition, we collect and release a
large cross-age face dataset with age and gender annotations to advance the
development of the AIFR and FAS.

Extensive experiments on five benchmark cross-age datasets demonstrate the
superior performance of our proposed \methodname over existing state-of-the-art
methods for AIFR and FAS. We further validate \methodname on two popular
general face recognition datasets, showing competitive performance for face
recognition in the wild. The source code and dataset are available
at~\url{https://github.com/Hzzone/MTLFace}.
</p>
<a href="http://arxiv.org/abs/2103.01520" target="_blank">arXiv:2103.01520</a> [<a href="http://arxiv.org/pdf/2103.01520" target="_blank">pdf</a>]

<h2>A Pose-only Solution to Visual Reconstruction and Navigation. (arXiv:2103.01530v1 [cs.CV])</h2>
<h3>Qi Cai, Lilian Zhang, Yuanxin Wu, Wenxian Yu, Dewen Hu</h3>
<p>Visual navigation and three-dimensional (3D) scene reconstruction are
essential for robotics to interact with the surrounding environment.
Large-scale scenes and critical camera motions are great challenges facing the
research community to achieve this goal. We raised a pose-only imaging geometry
framework and algorithms that can help solve these challenges. The
representation is a linear function of camera global translations, which allows
for efficient and robust camera motion estimation. As a result, the spatial
feature coordinates can be analytically reconstructed and do not require
nonlinear optimization. Experiments demonstrate that the computational
efficiency of recovering the scene and associated camera poses is significantly
improved by 2-4 orders of magnitude. This solution might be promising to unlock
real-time 3D visual computing in many forefront applications.
</p>
<a href="http://arxiv.org/abs/2103.01530" target="_blank">arXiv:2103.01530</a> [<a href="http://arxiv.org/pdf/2103.01530" target="_blank">pdf</a>]

<h2>Few-shot Open-set Recognition by Transformation Consistency. (arXiv:2103.01537v1 [cs.CV])</h2>
<h3>Minki Jeong, Seokeon Choi, Changick Kim</h3>
<p>In this paper, we attack a few-shot open-set recognition (FSOSR) problem,
which is a combination of few-shot learning (FSL) and open-set recognition
(OSR). It aims to quickly adapt a model to a given small set of labeled samples
while rejecting unseen class samples. Since OSR requires rich data and FSL
considers closed-set classification, existing OSR and FSL methods show poor
performances in solving FSOSR problems. The previous FSOSR method follows the
pseudo-unseen class sample-based methods, which collect pseudo-unseen samples
from the other dataset or synthesize samples to model unseen class
representations. However, this approach is heavily dependent on the composition
of the pseudo samples. In this paper, we propose a novel unknown class sample
detector, named SnaTCHer, that does not require pseudo-unseen samples. Based on
the transformation consistency, our method measures the difference between the
transformed prototypes and a modified prototype set. The modified set is
composed by replacing a query feature and its predicted class prototype.
SnaTCHer rejects samples with large differences to the transformed prototypes.
Our method alters the unseen class distribution estimation problem to a
relative feature transformation problem, independent of pseudo-unseen class
samples. We investigate our SnaTCHer with various prototype transformation
methods and observe that our method consistently improves unseen class sample
detection performance without closed-set classification reduction.
</p>
<a href="http://arxiv.org/abs/2103.01537" target="_blank">arXiv:2103.01537</a> [<a href="http://arxiv.org/pdf/2103.01537" target="_blank">pdf</a>]

<h2>TransTailor: Pruning the Pre-trained Model for Improved Transfer Learning. (arXiv:2103.01542v1 [cs.CV])</h2>
<h3>Bingyan Liu, Yifeng Cai, Yao Guo, Xiangqun Chen</h3>
<p>The increasing of pre-trained models has significantly facilitated the
performance on limited data tasks with transfer learning. However, progress on
transfer learning mainly focuses on optimizing the weights of pre-trained
models, which ignores the structure mismatch between the model and the target
task. This paper aims to improve the transfer performance from another angle -
in addition to tuning the weights, we tune the structure of pre-trained models,
in order to better match the target task. To this end, we propose TransTailor,
targeting at pruning the pre-trained model for improved transfer learning.
Different from traditional pruning pipelines, we prune and fine-tune the
pre-trained model according to the target-aware weight importance, generating
an optimal sub-model tailored for a specific target task. In this way, we
transfer a more suitable sub-structure that can be applied during fine-tuning
to benefit the final performance. Extensive experiments on multiple pre-trained
models and datasets demonstrate that TransTailor outperforms the traditional
pruning methods and achieves competitive or even better performance than other
state-of-the-art transfer learning methods while using a smaller model.
Notably, on the Stanford Dogs dataset, TransTailor can achieve 2.7% accuracy
improvement over other transfer methods with 20% fewer FLOPs.
</p>
<a href="http://arxiv.org/abs/2103.01542" target="_blank">arXiv:2103.01542</a> [<a href="http://arxiv.org/pdf/2103.01542" target="_blank">pdf</a>]

<h2>Real Masks and Fake Faces: On the Masked Face Presentation Attack Detection. (arXiv:2103.01546v1 [cs.CV])</h2>
<h3>Meiling Fang, Naser Damer, Florian Kirchbuchner, Arjan Kuijper</h3>
<p>The ongoing COVID-19 pandemic has lead to massive public health issues. Face
masks have become one of the most efficient ways to reduce coronavirus
transmission. This makes face recognition (FR) a challenging task as several
discriminative features are hidden. Moreover, face presentation attack
detection (PAD) is crucial to ensure the security of FR systems. In contrast to
growing numbers of masked FR studies, the impact of masked attacks on PAD has
not been explored. Therefore, we present novel attacks with real masks placed
on presentations and attacks with subjects wearing masks to reflect the current
real-world situation. Furthermore, this study investigates the effect of masked
attacks on PAD performance by using seven state-of-the-art PAD algorithms under
intra- and cross-database scenarios. We also evaluate the vulnerability of FR
systems on masked attacks. The experiments show that real masked attacks pose a
serious threat to the operation and security of FR systems.
</p>
<a href="http://arxiv.org/abs/2103.01546" target="_blank">arXiv:2103.01546</a> [<a href="http://arxiv.org/pdf/2103.01546" target="_blank">pdf</a>]

<h2>Careful with That! Observation of Human Movements to Estimate Objects Properties. (arXiv:2103.01555v1 [cs.RO])</h2>
<h3>Linda Lastrico, Alessandro Carf&#xec;, Alessia Vignolo, Alessandra Sciutti, Fulvio Mastrogiovanni, Francesco Rea</h3>
<p>Humans are very effective at interpreting subtle properties of the partner's
movement and use this skill to promote smooth interactions. Therefore, robotic
platforms that support human partners in daily activities should acquire
similar abilities. In this work we focused on the features of human motor
actions that communicate insights on the weight of an object and the
carefulness required in its manipulation. Our final goal is to enable a robot
to autonomously infer the degree of care required in object handling and to
discriminate whether the item is light or heavy, just by observing a human
manipulation. This preliminary study represents a promising step towards the
implementation of those abilities on a robot observing the scene with its
camera. Indeed, we succeeded in demonstrating that it is possible to reliably
deduct if the human operator is careful when handling an object, through
machine learning algorithms relying on the stream of visual acquisition from
either a robot camera or from a motion capture system. On the other hand, we
observed that the same approach is inadequate to discriminate between light and
heavy objects.
</p>
<a href="http://arxiv.org/abs/2103.01555" target="_blank">arXiv:2103.01555</a> [<a href="http://arxiv.org/pdf/2103.01555" target="_blank">pdf</a>]

<h2>Model-based Safe Reinforcement Learning using Generalized Control Barrier Function. (arXiv:2103.01556v1 [cs.RO])</h2>
<h3>Haitong Ma, Jianyu Chen, Shengbo Eben Li, Ziyu Lin, Sifa Zheng</h3>
<p>Model information can be used to predict future trajectories, so it has huge
potential to avoid dangerous region when implementing reinforcement learning
(RL) on real-world tasks, like autonomous driving. However, existing studies
mostly use model-free constrained RL, which causes inevitable constraint
violations. This paper proposes a model-based feasibility enhancement technique
of constrained RL, which enhances the feasibility of policy using generalized
control barrier function (GCBF) defined on the distance to constraint boundary.
By using the model information, the policy can be optimized safely without
violating actual safety constraints, and the sample efficiency is increased.
The major difficulty of infeasibility in solving the constrained policy
gradient is handled by an adaptive coefficient mechanism. We evaluate the
proposed method in both simulations and real vehicle experiments in a complex
autonomous driving collision avoidance task. The proposed method achieves up to
four times fewer constraint violations and converges 3.36 times faster than
baseline constrained RL approaches.
</p>
<a href="http://arxiv.org/abs/2103.01556" target="_blank">arXiv:2103.01556</a> [<a href="http://arxiv.org/pdf/2103.01556" target="_blank">pdf</a>]

<h2>Inter-class Discrepancy Alignment for Face Recognition. (arXiv:2103.01559v1 [cs.CV])</h2>
<h3>Jiaheng Liu, Yudong Wu, Yichao Wu, Zhenmao Li, Chen Ken, Ding Liang, Junjie Yan</h3>
<p>The field of face recognition (FR) has witnessed great progress with the
surge of deep learning. Existing methods mainly focus on extracting
discriminative features, and directly compute the cosine or L2 distance by the
point-to-point way without considering the context information. In this study,
we make a key observation that the local con-text represented by the
similarities between the instance and its inter-class neighbors1plays an
important role forFR. Specifically, we attempt to incorporate the local
in-formation in the feature space into the metric, and pro-pose a unified
framework calledInter-class DiscrepancyAlignment(IDA), with two dedicated
modules, Discrepancy Alignment Operator(IDA-DAO) andSupport Set
Estimation(IDA-SSE). IDA-DAO is used to align the similarity scores considering
the discrepancy between the images and its neighbors, which is defined by
adaptive support sets on the hypersphere. For practical inference, it is
difficult to acquire support set during online inference. IDA-SSE can provide
convincing inter-class neighbors by introducing virtual candidate images
generated with GAN. Further-more, we propose the learnable IDA-SSE, which can
implicitly give estimation without the need of any other images in the
evaluation process. The proposed IDA can be incorporated into existing FR
systems seamlessly and efficiently. Extensive experiments demonstrate that this
frame-work can 1) significantly improve the accuracy, and 2) make the model
robust to the face images of various distributions.Without bells and whistles,
our method achieves state-of-the-art performance on multiple standard FR
benchmarks.
</p>
<a href="http://arxiv.org/abs/2103.01559" target="_blank">arXiv:2103.01559</a> [<a href="http://arxiv.org/pdf/2103.01559" target="_blank">pdf</a>]

<h2>Contextually Guided Convolutional Neural Networks for Learning Most Transferable Representations. (arXiv:2103.01566v1 [cs.CV])</h2>
<h3>Olcay Kursun, Semih Dinc, Oleg V. Favorov</h3>
<p>Deep Convolutional Neural Networks (CNNs), trained extensively on very large
labeled datasets, learn to recognize inferentially powerful features in their
input patterns and represent efficiently their objective content. Such
objectivity of their internal representations enables deep CNNs to readily
transfer and successfully apply these representations to new classification
tasks. Deep CNNs develop their internal representations through a challenging
process of error backpropagation-based supervised training. In contrast, deep
neural networks of the cerebral cortex develop their even more powerful
internal representations in an unsupervised process, apparently guided at a
local level by contextual information. Implementing such local contextual
guidance principles in a single-layer CNN architecture, we propose an efficient
algorithm for developing broad-purpose representations (i.e., representations
transferable to new tasks without additional training) in shallow CNNs trained
on limited-size datasets. A contextually guided CNN (CG-CNN) is trained on
groups of neighboring image patches picked at random image locations in the
dataset. Such neighboring patches are likely to have a common context and
therefore are treated for the purposes of training as belonging to the same
class. Across multiple iterations of such training on different context-sharing
groups of image patches, CNN features that are optimized in one iteration are
then transferred to the next iteration for further optimization, etc. In this
process, CNN features acquire higher pluripotency, or inferential utility for
any arbitrary classification task, which we quantify as a transfer utility. In
our application to natural images, we find that CG-CNN features show the same,
if not higher, transfer utility and classification accuracy as comparable
transferable features in the first CNN layer of the well-known deep networks.
</p>
<a href="http://arxiv.org/abs/2103.01566" target="_blank">arXiv:2103.01566</a> [<a href="http://arxiv.org/pdf/2103.01566" target="_blank">pdf</a>]

<h2>A Comprehensive Study on Face Recognition Biases Beyond Demographics. (arXiv:2103.01592v1 [cs.CV])</h2>
<h3>Philipp Terh&#xf6;rst, Jan Niklas Kolf, Marco Huber, Florian Kirchbuchner, Naser Damer, Aythami Morales, Julian Fierrez, Arjan Kuijper</h3>
<p>Face recognition (FR) systems have a growing effect on critical
decision-making processes. Recent works have shown that FR solutions show
strong performance differences based on the user's demographics. However, to
enable a trustworthy FR technology, it is essential to know the influence of an
extended range of facial attributes on FR beyond demographics. Therefore, in
this work, we analyse FR bias over a wide range of attributes. We investigate
the influence of 47 attributes on the verification performance of two popular
FR models. The experiments were performed on the publicly available MAADFace
attribute database with over 120M high-quality attribute annotations. To
prevent misleading statements about biased performances, we introduced control
group based validity values to decide if unbalanced test data causes the
performance differences. The results demonstrate that also many non-demographic
attributes strongly affect the recognition performance, such as accessories,
hair-styles and colors, face shapes, or facial anomalies. The observations of
this work show the strong need for further advances in making FR system more
robust, explainable, and fair. Moreover, our findings might help to a better
understanding of how FR networks work, to enhance the robustness of these
networks, and to develop more generalized bias-mitigating face recognition
solutions.
</p>
<a href="http://arxiv.org/abs/2103.01592" target="_blank">arXiv:2103.01592</a> [<a href="http://arxiv.org/pdf/2103.01592" target="_blank">pdf</a>]

<h2>Simulation-to-Real domain adaptation with teacher-student learning for endoscopic instrument segmentation. (arXiv:2103.01593v1 [cs.CV])</h2>
<h3>Manish Sahu, Anirban Mukhopadhyay, Stefan Zachow</h3>
<p>Purpose: Segmentation of surgical instruments in endoscopic videos is
essential for automated surgical scene understanding and process modeling.
However, relying on fully supervised deep learning for this task is challenging
because manual annotation occupies valuable time of the clinical experts.

Methods: We introduce a teacher-student learning approach that learns jointly
from annotated simulation data and unlabeled real data to tackle the erroneous
learning problem of the current consistency-based unsupervised domain
adaptation framework.

Results: Empirical results on three datasets highlight the effectiveness of
the proposed framework over current approaches for the endoscopic instrument
segmentation task. Additionally, we provide analysis of major factors affecting
the performance on all datasets to highlight the strengths and failure modes of
our approach.

Conclusion: We show that our proposed approach can successfully exploit the
unlabeled real endoscopic video frames and improve generalization performance
over pure simulation-based training and the previous state-of-the-art. This
takes us one step closer to effective segmentation of surgical tools in the
annotation scarce setting.
</p>
<a href="http://arxiv.org/abs/2103.01593" target="_blank">arXiv:2103.01593</a> [<a href="http://arxiv.org/pdf/2103.01593" target="_blank">pdf</a>]

<h2>Spatial Attention Point Network for Deep-learning-based Robust Autonomous Robot Motion Generation. (arXiv:2103.01598v1 [cs.RO])</h2>
<h3>Hideyuki Ichiwara, Hiroshi Ito, Kenjiro Yamamoto, Hiroki Mori, Tetsuya Ogata</h3>
<p>Deep learning provides a powerful framework for automated acquisition of
complex robotic motions. However, despite a certain degree of generalization,
the need for vast amounts of training data depending on the work-object
position is an obstacle to industrial applications. Therefore, a robot
motion-generation model that can respond to a variety of work-object positions
with a small amount of training data is necessary. In this paper, we propose a
method robust to changes in object position by automatically extracting spatial
attention points in the image for the robot task and generating motions on the
basis of their positions. We demonstrate our method with an LBR iiwa 7R1400
robot arm on a picking task and a pick-and-place task at various positions in
various situations. In each task, the spatial attention points are obtained for
the work objects that are important to the task. Our method is robust to
changes in object position. Further, it is robust to changes in background,
lighting, and obstacles that are not important to the task because it only
focuses on positions that are important to the task.
</p>
<a href="http://arxiv.org/abs/2103.01598" target="_blank">arXiv:2103.01598</a> [<a href="http://arxiv.org/pdf/2103.01598" target="_blank">pdf</a>]

<h2>Path continuity for multi-wheeled AGVs. (arXiv:2103.01619v1 [cs.RO])</h2>
<h3>Mirko Kokot, Damjan Mikli&#x107;, Tamara Petrovi&#x107;</h3>
<p>Notwithstanding the growing presence of AGVs in the industry, there is a lack
of research about multi-wheeled AGVs which offer higher maneuverability and
space efficiency. In this paper, we present generalized path continuity
conditions as a continuation of previous research done for vehicles with more
constrained kinematic capabilities. We propose a novel approach for
analytically defining various kinematic modes (motion modes), that AGVs with
multiple steer and drive wheels can utilize. This approach enables deriving
vehicle kinematic equations based on the vehicle configuration and its
constraints, path shape, and corresponding motion mode. Finally, we derive
general continuity conditions for paths that multi-wheeled AGVs can follow, and
show through examples how they can be utilized in layout design methods.
</p>
<a href="http://arxiv.org/abs/2103.01619" target="_blank">arXiv:2103.01619</a> [<a href="http://arxiv.org/pdf/2103.01619" target="_blank">pdf</a>]

<h2>Geometry-Guided Street-View Panorama Synthesis from Satellite Imagery. (arXiv:2103.01623v1 [cs.CV])</h2>
<h3>Yujiao Shi, Dylan Campbell, Xin Yu, Hongdong Li</h3>
<p>This paper presents a new approach for synthesizing a novel street-view
panorama given an overhead satellite image. Taking a small satellite image
patch as input, our method generates a Google's omnidirectional street-view
type panorama, as if it is captured from the same geographical location as the
center of the satellite patch. Existing works tackle this task as an image
generation problem which adopts generative adversarial networks to implicitly
learn the cross-view transformations, while ignoring the domain relevance. In
this paper, we propose to explicitly establish the geometric correspondences
between the two-view images so as to facilitate the cross-view transformation
learning. Specifically, we observe that when a 3D point in the real world is
visible in both views, there is a deterministic mapping between the projected
points in the two-view images given the height information of this 3D point.
Motivated by this, we develop a novel Satellite to Street-view image Projection
(S2SP) module which explicitly establishes such geometric correspondences and
projects the satellite images to the street viewpoint. With these projected
satellite images as network input, we next employ a generator to synthesize
realistic street-view panoramas that are geometrically consistent with the
satellite images. Our S2SP module is differentiable and the whole framework is
trained in an end-to-end manner. Extensive experimental results on two
cross-view benchmark datasets demonstrate that our method generates images that
better respect the scene geometry than existing approaches.
</p>
<a href="http://arxiv.org/abs/2103.01623" target="_blank">arXiv:2103.01623</a> [<a href="http://arxiv.org/pdf/2103.01623" target="_blank">pdf</a>]

<h2>Reachability-based Identification, Analysis, and Control Synthesis of Robot Systems. (arXiv:2103.01626v1 [cs.RO])</h2>
<h3>Stefan B. Liu, Bastian Sch&#xfc;rmann, Matthias Althoff</h3>
<p>We introduce reachability analysis for the formal examination of robots. We
propose a novel identification method, which preserves reachset conformance of
linear systems. We additionally propose a simultaneous identification and
control synthesis scheme to obtain optimal controllers with formal guarantees.
In a case study, we examine the effectiveness of using reachability analysis to
synthesize a state-feedback controller, a velocity observer, and an output
feedback controller.
</p>
<a href="http://arxiv.org/abs/2103.01626" target="_blank">arXiv:2103.01626</a> [<a href="http://arxiv.org/pdf/2103.01626" target="_blank">pdf</a>]

<h2>Pixel-level Extrinsic Self Calibration of High Resolution LiDAR and Camera in Targetless Environments. (arXiv:2103.01627v1 [cs.RO])</h2>
<h3>Chongjian Yuan, Xiyuan Liu, Xiaoping Hong, Fu Zhang</h3>
<p>In this letter, we present a novel method for automatic extrinsic calibration
of high-resolution LiDARs and RGB cameras in targetless environments. Our
approach does not require checkerboards but can achieve pixel-level accuracy by
aligning natural edge features in the two sensors. On the theory level, we
analyze the constraints imposed by edge features and the sensitivity of
calibration accuracy with respect to edge distribution in the scene. On the
implementation level, we carefully investigate the physical measuring
principles of LiDARs and propose an efficient and accurate LiDAR edge
extraction method based on point cloud voxel cutting and plane fitting. Due to
the edges' richness in natural scenes, we have carried out experiments in many
indoor and outdoor scenes. The results show that this method has high
robustness, accuracy, and consistency. It can promote the research and
application of the fusion between LiDAR and camera. We have open-sourced our
code on GitHub to benefit the community.
</p>
<a href="http://arxiv.org/abs/2103.01627" target="_blank">arXiv:2103.01627</a> [<a href="http://arxiv.org/pdf/2103.01627" target="_blank">pdf</a>]

<h2>Using CNNs to Identify the Origin of Finger Vein Image. (arXiv:2103.01632v1 [cs.CV])</h2>
<h3>Babak Maser, Andreas Uhl</h3>
<p>We study the finger vein (FV) sensor model identification task using a deep
learning approach. So far, for this biometric modality, only correlation-based
PRNU and texture descriptor-based methods have been applied. We employ five
prominent CNN architectures covering a wide range of CNN family models,
including VGG16, ResNet, and the Xception model. In addition, a novel
architecture termed FV2021 is proposed in this work, which excels by its
compactness and a low number of parameters to be trained. Original samples, as
well as the region of interest data from eight publicly accessible FV datasets,
are used in experimentation. An excellent sensor identification AUC-ROC score
of 1.0 for patches of uncropped samples and 0.9997 for ROI samples have been
achieved. The comparison with former methods shows that the CNN-based approach
is superior and improved the results.
</p>
<a href="http://arxiv.org/abs/2103.01632" target="_blank">arXiv:2103.01632</a> [<a href="http://arxiv.org/pdf/2103.01632" target="_blank">pdf</a>]

<h2>Brain-inspired algorithms for processing of visual data. (arXiv:2103.01634v1 [cs.CV])</h2>
<h3>Nicola Strisciuglio</h3>
<p>The study of the visual system of the brain has attracted the attention and
interest of many neuro-scientists, that derived computational models of some
types of neuron that compose it. These findings inspired researchers in image
processing and computer vision to deploy such models to solve problems of
visual data processing. In this paper, we review approaches for image
processing and computer vision, the design of which is based on
neuro-scientific findings about the functions of some neurons in the visual
cortex. Furthermore, we analyze the connection between the hierarchical
organization of the visual system of the brain and the structure of
Convolutional Networks (ConvNets). We pay particular attention to the
mechanisms of inhibition of the responses of some neurons, which provide the
visual system with improved stability to changing input stimuli, and discuss
their implementation in image processing operators and in ConvNets.
</p>
<a href="http://arxiv.org/abs/2103.01634" target="_blank">arXiv:2103.01634</a> [<a href="http://arxiv.org/pdf/2103.01634" target="_blank">pdf</a>]

<h2>Exploiting latent representation of sparse semantic layers for improved short-term motion prediction with Capsule Networks. (arXiv:2103.01644v1 [cs.CV])</h2>
<h3>Albert Dulian, John C. Murray</h3>
<p>As urban environments manifest high levels of complexity it is of vital
importance that safety systems embedded within autonomous vehicles (AVs) are
able to accurately anticipate short-term future motion of nearby agents. This
problem can be further understood as generating a sequence of coordinates
describing the future motion of the tracked agent. Various proposed approaches
demonstrate significant benefits of using a rasterised top-down image of the
road, with a combination of Convolutional Neural Networks (CNNs), for
extraction of relevant features that define the road structure (eg. driveable
areas, lanes, walkways). In contrast, this paper explores use of Capsule
Networks (CapsNets) in the context of learning a hierarchical representation of
sparse semantic layers corresponding to small regions of the High-Definition
(HD) map. Each region of the map is dismantled into separate geometrical layers
that are extracted with respect to the agent's current position. By using an
architecture based on CapsNets the model is able to retain hierarchical
relationships between detected features within images whilst also preventing
loss of spatial data often caused by the pooling operation. We train and
evaluate our model on publicly available dataset nuTonomy scenes and compare it
to recently published methods. We show that our model achieves significant
improvement over recently published works on deterministic prediction, whilst
drastically reducing the overall size of the network.
</p>
<a href="http://arxiv.org/abs/2103.01644" target="_blank">arXiv:2103.01644</a> [<a href="http://arxiv.org/pdf/2103.01644" target="_blank">pdf</a>]

<h2>Part2Whole: Iteratively Enrich Detail for Cross-Modal Retrieval with Partial Query. (arXiv:2103.01654v1 [cs.CV])</h2>
<h3>Guanyu Cai, Xinyang Jiang, Jun Zhang, Yifei Gong, Lianghua He, Pai Peng, Xiaowei Guo, Xing Sun</h3>
<p>Text-based image retrieval has seen considerable progress in recent years.
However, the performance of existing methods suffers in real life since the
user is likely to provide an incomplete description of a complex scene, which
often leads to results filled with false positives that fit the incomplete
description. In this work, we introduce the partial-query problem and
extensively analyze its influence on text-based image retrieval. We then
propose an interactive retrieval framework called Part2Whole to tackle this
problem by iteratively enriching the missing details. Specifically, an
Interactive Retrieval Agent is trained to build an optimal policy to refine the
initial query based on a user-friendly interaction and statistical
characteristics of the gallery. Compared to other dialog-based methods that
rely heavily on the user to feed back differentiating information, we let AI
take over the optimal feedback searching process and hint the user with
confirmation-based questions about details. Furthermore, since fully-supervised
training is often infeasible due to the difficulty of obtaining human-machine
dialog data, we present a weakly-supervised reinforcement learning method that
needs no human-annotated data other than the text-image dataset. Experiments
show that our framework significantly improves the performance of text-based
image retrieval under complex scenes.
</p>
<a href="http://arxiv.org/abs/2103.01654" target="_blank">arXiv:2103.01654</a> [<a href="http://arxiv.org/pdf/2103.01654" target="_blank">pdf</a>]

<h2>Run Your Visual-Inertial Odometry on NVIDIA Jetson: Benchmark Tests on a Micro Aerial Vehicle. (arXiv:2103.01655v1 [cs.RO])</h2>
<h3>Jinwoo Jeon, Sungwook Jung, Eungchang Lee, Duckyu Choi, Hyun Myung</h3>
<p>This paper presents benchmark tests of various visual(-inertial) odometry
algorithms on NVIDIA Jetson platforms. The compared algorithms include mono and
stereo, covering Visual Odometry (VO) and Visual-Inertial Odometry (VIO):
VINS-Mono, VINS-Fusion, Kimera, ALVIO, Stereo-MSCKF, ORB-SLAM2 stereo, and
ROVIO. As these methods are mainly used for unmanned aerial vehicles (UAVs),
they must perform well in situations where the size of the processing board and
weight is limited. Jetson boards released by NVIDIA satisfy these constraints
as they have a sufficiently powerful central processing unit (CPU) and graphics
processing unit (GPU) for image processing. However, in existing studies, the
performance of Jetson boards as a processing platform for executing VO/VIO has
not been compared extensively in terms of the usage of computing resources and
accuracy. Therefore, this study compares representative VO/VIO algorithms on
several NVIDIA Jetson platforms, namely NVIDIA Jetson TX2, Xavier NX, and AGX
Xavier, and introduces a novel dataset 'KAIST VIO dataset' for UAVs. Including
pure rotations, the dataset has several geometric trajectories that are harsh
to visual(-inertial) state estimation. The evaluation is performed in terms of
the accuracy of estimated odometry, CPU usage, and memory usage on various
Jetson boards, algorithms, and trajectories. We present the {results of the}
comprehensive benchmark test and release the dataset for the computer vision
and robotics applications.
</p>
<a href="http://arxiv.org/abs/2103.01655" target="_blank">arXiv:2103.01655</a> [<a href="http://arxiv.org/pdf/2103.01655" target="_blank">pdf</a>]

<h2>A Novel CNN-LSTM-based Approach to Predict Urban Expansion. (arXiv:2103.01695v1 [cs.CV])</h2>
<h3>Wadii Boulila, Hamza Ghandorh, Mehshan Ahmed Khan, Fawad Ahmed, Jawad Ahmad</h3>
<p>Time-series remote sensing data offer a rich source of information that can
be used in a wide range of applications, from monitoring changes in land cover
to surveilling crops, coastal changes, flood risk assessment, and urban sprawl.
This paper addresses the challenge of using time-series satellite images to
predict urban expansion. Building upon previous work, we propose a novel
two-step approach based on semantic image segmentation in order to predict
urban expansion. The first step aims to extract information about urban regions
at different time scales and prepare them for use in the training step. The
second step combines Convolutional Neural Networks (CNN) with Long Short Term
Memory (LSTM) methods in order to learn temporal features and thus predict
urban expansion. In this paper, experimental results are conducted using
several multi-date satellite images representing the three largest cities in
Saudi Arabia, namely: Riyadh, Jeddah, and Dammam. We empirically evaluated our
proposed technique, and examined its results by comparing them with
state-of-the-art approaches. Following this evaluation, we determined that our
results reveal improved performance for the new-coupled CNN-LSTM approach,
particularly in terms of assessments based on Mean Square Error, Root Mean
Square Error, Peak Signal to Noise Ratio, Structural Similarity Index, and
overall classification accuracy.
</p>
<a href="http://arxiv.org/abs/2103.01695" target="_blank">arXiv:2103.01695</a> [<a href="http://arxiv.org/pdf/2103.01695" target="_blank">pdf</a>]

<h2>An Interpretable Multiple-Instance Approach for the Detection of referable Diabetic Retinopathy from Fundus Images. (arXiv:2103.01702v1 [cs.CV])</h2>
<h3>Alexandros Papadopoulos, Fotis Topouzis, Anastasios Delopoulos</h3>
<p>Diabetic Retinopathy (DR) is a leading cause of vision loss globally. Yet
despite its prevalence, the majority of affected people lack access to the
specialized ophthalmologists and equipment required for assessing their
condition. This can lead to delays in the start of treatment, thereby lowering
their chances for a successful outcome. Machine learning systems that
automatically detect the disease in eye fundus images have been proposed as a
means of facilitating access to DR severity estimates for patients in remote
regions or even for complementing the human expert's diagnosis. In this paper,
we propose a machine learning system for the detection of referable DR in
fundus images that is based on the paradigm of multiple-instance learning. By
extracting local information from image patches and combining it efficiently
through an attention mechanism, our system is able to achieve high
classification accuracy. Moreover, it can highlight potential image regions
where DR manifests through its characteristic lesions. We evaluate our approach
on publicly available retinal image datasets, in which it exhibits near
state-of-the-art performance, while also producing interpretable visualizations
of its predictions.
</p>
<a href="http://arxiv.org/abs/2103.01702" target="_blank">arXiv:2103.01702</a> [<a href="http://arxiv.org/pdf/2103.01702" target="_blank">pdf</a>]

<h2>Unmasking Face Embeddings by Self-restrained Triplet Loss for Accurate Masked Face Recognition. (arXiv:2103.01716v1 [cs.CV])</h2>
<h3>Fadi Boutros, Naser Damer, Florian Kirchbuchner, Arjan Kuijper</h3>
<p>Using the face as a biometric identity trait is motivated by the contactless
nature of the capture process and the high accuracy of the recognition
algorithms. After the current COVID-19 pandemic, wearing a face mask has been
imposed in public places to keep the pandemic under control. However, face
occlusion due to wearing a mask presents an emerging challenge for face
recognition systems. In this paper, we presented a solution to improve the
masked face recognition performance. Specifically, we propose the Embedding
Unmasking Model (EUM) operated on top of existing face recognition models. We
also propose a novel loss function, the Self-restrained Triplet (SRT), which
enabled the EUM to produce embeddings similar to these of unmasked faces of the
same identities. The achieved evaluation results on two face recognition models
and two real masked datasets proved that our proposed approach significantly
improves the performance in most experimental settings.
</p>
<a href="http://arxiv.org/abs/2103.01716" target="_blank">arXiv:2103.01716</a> [<a href="http://arxiv.org/pdf/2103.01716" target="_blank">pdf</a>]

<h2>Transportation Density Reduction Caused by City Lockdowns Across the World during the COVID-19 Epidemic: From the View of High-resolution Remote Sensing Imagery. (arXiv:2103.01717v1 [cs.CV])</h2>
<h3>Chen Wu, Sihan Zhu, Jiaqi Yang, Meiqi Hu, Bo Du, Liangpei Zhang, Lefei Zhang, Chengxi Han, Meng Lan</h3>
<p>As the COVID-19 epidemic began to worsen in the first months of 2020,
stringent lockdown policies were implemented in numerous cities throughout the
world to control human transmission and mitigate its spread. Although
transportation density reduction inside the city was felt subjectively, there
has thus far been no objective and quantitative study of its variation to
reflect the intracity population flows and their corresponding relationship
with lockdown policy stringency from the view of remote sensing images with the
high resolution under 1m. Accordingly, we here provide a quantitative
investigation of the transportation density reduction before and after lockdown
was implemented in six epicenter cities (Wuhan, Milan, Madrid, Paris, New York,
and London) around the world during the COVID-19 epidemic, which is
accomplished by extracting vehicles from the multi-temporal high-resolution
remote sensing images. A novel vehicle detection model combining unsupervised
vehicle candidate extraction and deep learning identification was specifically
proposed for the images with the resolution of 0.5m. Our results indicate that
transportation densities were reduced by an average of approximately 50% (and
as much as 75.96%) in these six cities following lockdown. The influences on
transportation density reduction rates are also highly correlated with policy
stringency, with an R^2 value exceeding 0.83. Even within a specific city, the
transportation density changes differed and tended to be distributed in
accordance with the city's land-use patterns. Considering that public
transportation was mostly reduced or even forbidden, our results indicate that
city lockdown policies are effective at limiting human transmission within
cities.
</p>
<a href="http://arxiv.org/abs/2103.01717" target="_blank">arXiv:2103.01717</a> [<a href="http://arxiv.org/pdf/2103.01717" target="_blank">pdf</a>]

<h2>On the Generalisation Capabilities of Fisher Vector based Face Presentation Attack Detection. (arXiv:2103.01721v1 [cs.CV])</h2>
<h3>L&#xe1;zaro J. Gonz&#xe1;lez-Soler, Marta Gomez-Barrero, Christoph Busch</h3>
<p>In the last decades, the broad development experienced by biometric systems
has unveiled several threats which may decrease their trustworthiness. Those
are attack presentations which can be easily carried out by a non-authorised
subject to gain access to the biometric system. In order to mitigate those
security concerns, most face Presentation Attack Detection techniques have
reported a good detection performance when they are evaluated on known
Presentation Attack Instruments (PAI) and acquisition conditions, in contrast
to more challenging scenarios where unknown attacks are included in the test
set. For those more realistic scenarios, the existing algorithms face
difficulties to detect unknown PAI species in many cases. In this work, we use
a new feature space based on Fisher Vectors, computed from compact Binarised
Statistical Image Features histograms, which allow discovering semantic feature
subsets from known samples in order to enhance the detection of unknown
attacks. This new representation, evaluated for challenging unknown attacks
taken from freely available facial databases, shows promising results: a
BPCER100 under 17% together with an AUC over 98% can be achieved in the
presence of unknown attacks. In addition, by training a limited number of
parameters, our method is able to achieve state-of-the-art deep learning-based
approaches for cross-dataset scenarios.
</p>
<a href="http://arxiv.org/abs/2103.01721" target="_blank">arXiv:2103.01721</a> [<a href="http://arxiv.org/pdf/2103.01721" target="_blank">pdf</a>]

<h2>Image/Video Deep Anomaly Detection: A Survey. (arXiv:2103.01739v1 [cs.CV])</h2>
<h3>Bahram Mohammadi, Mahmood Fathy, Mohammad Sabokrou</h3>
<p>The considerable significance of Anomaly Detection (AD) problem has recently
drawn the attention of many researchers. Consequently, the number of proposed
methods in this research field has been increased steadily. AD strongly
correlates with the important computer vision and image processing tasks such
as image/video anomaly, irregularity and sudden event detection. More recently,
Deep Neural Networks (DNNs) offer a high performance set of solutions, but at
the expense of a heavy computational cost. However, there is a noticeable gap
between the previously proposed methods and an applicable real-word approach.
Regarding the raised concerns about AD as an ongoing challenging problem,
notably in images and videos, the time has come to argue over the pitfalls and
prospects of methods have attempted to deal with visual AD tasks. Hereupon, in
this survey we intend to conduct an in-depth investigation into the
images/videos deep learning based AD methods. We also discuss current
challenges and future research directions thoroughly.
</p>
<a href="http://arxiv.org/abs/2103.01739" target="_blank">arXiv:2103.01739</a> [<a href="http://arxiv.org/pdf/2103.01739" target="_blank">pdf</a>]

<h2>IdentityDP: Differential Private Identification Protection for Face Images. (arXiv:2103.01745v1 [cs.CV])</h2>
<h3>Yunqian Wen, Li Song, Bo Liu, Ming Ding, Rong Xie</h3>
<p>Because of the explosive growth of face photos as well as their widespread
dissemination and easy accessibility in social media, the security and privacy
of personal identity information becomes an unprecedented challenge. Meanwhile,
the convenience brought by advanced identity-agnostic computer vision
technologies is attractive. Therefore, it is important to use face images while
taking careful consideration in protecting people's identities. Given a face
image, face de-identification, also known as face anonymization, refers to
generating another image with similar appearance and the same background, while
the real identity is hidden. Although extensive efforts have been made,
existing face de-identification techniques are either insufficient in
photo-reality or incapable of well-balancing privacy and utility. In this
paper, we focus on tackling these challenges to improve face de-identification.
We propose IdentityDP, a face anonymization framework that combines a
data-driven deep neural network with a differential privacy (DP) mechanism.
This framework encompasses three stages: facial representations
disentanglement, $\epsilon$-IdentityDP perturbation and image reconstruction.
Our model can effectively obfuscate the identity-related information of faces,
preserve significant visual similarity, and generate high-quality images that
can be used for identity-agnostic computer vision tasks, such as detection,
tracking, etc. Different from the previous methods, we can adjust the balance
of privacy and utility through the privacy budget according to pratical demands
and provide a diversity of results without pre-annotations. Extensive
experiments demonstrate the effectiveness and generalization ability of our
proposed anonymization framework.
</p>
<a href="http://arxiv.org/abs/2103.01745" target="_blank">arXiv:2103.01745</a> [<a href="http://arxiv.org/pdf/2103.01745" target="_blank">pdf</a>]

<h2>Comparison of Methods Generalizing Max- and Average-Pooling. (arXiv:2103.01746v1 [cs.CV])</h2>
<h3>Florentin Bieder, Robin Sandk&#xfc;hler, Philippe C. Cattin</h3>
<p>Max- and average-pooling are the most popular pooling methods for
downsampling in convolutional neural networks. In this paper, we compare
different pooling methods that generalize both max- and average-pooling.
Furthermore, we propose another method based on a smooth approximation of the
maximum function and put it into context with related methods. For the
comparison, we use a VGG16 image classification network and train it on a large
dataset of natural high-resolution images (Google Open Images v5). The results
show that none of the more sophisticated methods perform significantly better
in this classification task than standard max- or average-pooling.
</p>
<a href="http://arxiv.org/abs/2103.01746" target="_blank">arXiv:2103.01746</a> [<a href="http://arxiv.org/pdf/2103.01746" target="_blank">pdf</a>]

<h2>A region-based descriptor network for uniformly sampled keypoints. (arXiv:2103.01780v1 [cs.CV])</h2>
<h3>Kai Lv, Zongqing Lu, Qingmin Liao</h3>
<p>Matching keypoint pairs of different images is a basic task of computer
vision. Most methods require customized extremum point schemes to obtain the
coordinates of feature points with high confidence, which often need complex
algorithmic design or a network with higher training difficulty and also ignore
the possibility that flat regions can be used as candidate regions of matching
points. In this paper, we design a region-based descriptor by combining the
context features of a deep network. The new descriptor can give a robust
representation of a point even in flat regions. By the new descriptor, we can
obtain more high confidence matching points without extremum operation. The
experimental results show that our proposed method achieves a performance
comparable to state-of-the-art.
</p>
<a href="http://arxiv.org/abs/2103.01780" target="_blank">arXiv:2103.01780</a> [<a href="http://arxiv.org/pdf/2103.01780" target="_blank">pdf</a>]

<h2>Context Decoupling Augmentation for Weakly Supervised Semantic Segmentation. (arXiv:2103.01795v1 [cs.CV])</h2>
<h3>Yukun Su, Ruizhou Sun, Guosheng Lin, Qingyao Wu</h3>
<p>Data augmentation is vital for deep learning neural networks. By providing
massive training samples, it helps to improve the generalization ability of the
model. Weakly supervised semantic segmentation (WSSS) is a challenging problem
that has been deeply studied in recent years, conventional data augmentation
approaches for WSSS usually employ geometrical transformations, random cropping
and color jittering. However, merely increasing the same contextual semantic
data does not bring much gain to the networks to distinguish the objects, e.g.,
the correct image-level classification of "aeroplane" may be not only due to
the recognition of the object itself, but also its co-occurrence context like
"sky", which will cause the model to focus less on the object features. To this
end, we present a Context Decoupling Augmentation (CDA) method, to change the
inherent context in which the objects appear and thus drive the network to
remove the dependence between object instances and contextual information. To
validate the effectiveness of the proposed method, extensive experiments on
PASCAL VOC 2012 dataset with several alternative network architectures
demonstrate that CDA can boost various popular WSSS methods to the new
state-of-the-art by a large margin.
</p>
<a href="http://arxiv.org/abs/2103.01795" target="_blank">arXiv:2103.01795</a> [<a href="http://arxiv.org/pdf/2103.01795" target="_blank">pdf</a>]

<h2>A Safety-Aware Kinodynamic Architecture for Human-Robot Collaboration. (arXiv:2103.01818v1 [cs.RO])</h2>
<h3>Andrea Pupa, Mohammad Arrfou, Gildo Andreoni, Cristian Secchi</h3>
<p>The new paradigm of human-robot collaboration has led to the creation of
shared work environments in which humans and robots work in close contact with
each other. Consequently, the safety regulations have been updated addressing
these new scenarios. The mere application of these regulations may lead to a
very inefficient behavior of the robot. In order to preserve safety for the
human operators and allow the robot to reach a desired configuration in a safe
and efficient way, a two layers architecture for trajectory planning and
scaling is proposed. The first layer calculates the nominal trajectory and
continuously adapts it based on the human behavior. The second layer, which
explicitly considers the safety regulations, scales the robot velocity and
requests for a new trajectory if the robot speed drops. The proposed
architecture is experimentally validated on a Pilz PRBT manipulator.
</p>
<a href="http://arxiv.org/abs/2103.01818" target="_blank">arXiv:2103.01818</a> [<a href="http://arxiv.org/pdf/2103.01818" target="_blank">pdf</a>]

<h2>A Structurally Regularized Convolutional Neural Network for Image Classification using Wavelet-based SubBand Decomposition. (arXiv:2103.01823v1 [cs.CV])</h2>
<h3>Pavel Sinha, Ioannis Psaromiligkos, Zeljko Zilic</h3>
<p>We propose a convolutional neural network (CNN) architecture for image
classification based on subband decomposition of the image using wavelets. The
proposed architecture decomposes the input image spectra into multiple
critically sampled subbands, extracts features using a single CNN per subband,
and finally, performs classification by combining the extracted features using
a fully connected layer. Processing each of the subbands by an individual CNN,
thereby limiting the learning scope of each CNN to a single subband, imposes a
form of structural regularization. This provides better generalization
capability as seen by the presented results. The proposed architecture achieves
best-in-class performance in terms of total multiply-add-accumulator operations
and nearly best-in-class performance in terms of total parameters required, yet
it maintains competitive classification performance. We also show the proposed
architecture is more robust than the regular full-band CNN to noise caused by
weight-and-bias quantization and input quantization.
</p>
<a href="http://arxiv.org/abs/2103.01823" target="_blank">arXiv:2103.01823</a> [<a href="http://arxiv.org/pdf/2103.01823" target="_blank">pdf</a>]

<h2>Human-Centered Dynamic Scheduling Architecture for Collaborative Application. (arXiv:2103.01831v1 [cs.RO])</h2>
<h3>Andrea Pupa, Wietse Van Dijk, Cristian Secchi</h3>
<p>In collaborative robotic applications, human and robot have to work together
during a whole shift for executing a sequence of jobs. The performance of the
human robot team can be enhanced by scheduling the right tasks to the human and
the robot. The scheduling should consider the task execution constraints, the
variability in the task execution by the human, and the job quality of the
human. Therefore, it is necessary to dynamically schedule the assigned tasks.
In this paper, we propose a two-layered architecture for task allocation and
scheduling in a collaborative cell. Job quality is explicitly considered during
the allocation of the tasks and over a sequence of jobs. The tasks are
dynamically scheduled based on the real time monitoring of the human's
activities. The effectiveness of the proposed architecture is experimentally
validated.
</p>
<a href="http://arxiv.org/abs/2103.01831" target="_blank">arXiv:2103.01831</a> [<a href="http://arxiv.org/pdf/2103.01831" target="_blank">pdf</a>]

<h2>Multi-robot task allocation for safe planning under dynamic uncertainties. (arXiv:2103.01840v1 [cs.RO])</h2>
<h3>Daniel Tihanyi, Yimeng Lu, Orcun Karaca, Maryam Kamgarpour</h3>
<p>This paper considers the problem of multi-robot safe mission planning in
uncertain dynamic environments. This problem arises in several applications
including safety-critical exploration, surveillance, and emergency rescue
missions. Computation of a multi-robot optimal control policy is challenging
not only because of the complexity of incorporating dynamic uncertainties while
planning, but also because of the exponential growth in problem size as a
function of the number of robots. Leveraging recent works obtaining a tractable
safety maximizing plan for a single robot, we propose a scalable two-stage
framework to solve the problem at hand. Specifically, the problem is split into
a low-level single-agent planning problem and a high-level task allocation
problem. The low-level problem uses an efficient approximation of stochastic
reachability for a Markov decision process to handle the dynamic uncertainty.
The task allocation, on the other hand, is solved using polynomial-time forward
and reverse greedy heuristics. The safety objective of our multi-robot safe
planning problem allows an implementation of the greedy heuristics through a
distributed auction-based approach. Moreover, by leveraging the properties of
the safety objective function, we ensure provable performance bounds on the
safety of the approximate solutions proposed by these two heuristics. Our
result is illustrated through case studies.
</p>
<a href="http://arxiv.org/abs/2103.01840" target="_blank">arXiv:2103.01840</a> [<a href="http://arxiv.org/pdf/2103.01840" target="_blank">pdf</a>]

<h2>Square Root Bundle Adjustment for Large-Scale Reconstruction. (arXiv:2103.01843v1 [cs.CV])</h2>
<h3>Nikolaus Demmel, Christiane Sommer, Daniel Cremers, Vladyslav Usenko</h3>
<p>We propose a new formulation for the bundle adjustment problem which relies
on nullspace marginalization of landmark variables by QR decomposition. Our
approach, which we call square root bundle adjustment, is algebraically
equivalent to the commonly used Schur complement trick, improves the numeric
stability of computations and allows for solving large-scale bundle adjustment
problems with single precision floating point numbers. We show in real-world
experiments with the BAL datasets that even in single precision the proposed
solver achieves on average equally accurate solutions compared to Schur
complement solvers using double precision. It runs significantly faster, but
can require larger amounts of memory on dense problems. The proposed
formulation relies on simple linear algebra operations and opens the way for
efficient implementations of bundle adjustment on hardware platforms optimized
for single precision linear algebra processing.
</p>
<a href="http://arxiv.org/abs/2103.01843" target="_blank">arXiv:2103.01843</a> [<a href="http://arxiv.org/pdf/2103.01843" target="_blank">pdf</a>]

<h2>Network Pruning via Resource Reallocation. (arXiv:2103.01847v1 [cs.CV])</h2>
<h3>Yuenan Hou, Zheng Ma, Chunxiao Liu, Zhe Wang, Chen Change Loy</h3>
<p>Channel pruning is broadly recognized as an effective approach to obtain a
small compact model through eliminating unimportant channels from a large
cumbersome network. Contemporary methods typically perform iterative pruning
procedure from the original over-parameterized model, which is both tedious and
expensive especially when the pruning is aggressive. In this paper, we propose
a simple yet effective channel pruning technique, termed network Pruning via
rEsource rEalLocation (PEEL), to quickly produce a desired slim model with
negligible cost. Specifically, PEEL first constructs a predefined backbone and
then conducts resource reallocation on it to shift parameters from less
informative layers to more important layers in one round, thus amplifying the
positive effect of these informative layers. To demonstrate the effectiveness
of PEEL , we perform extensive experiments on ImageNet with ResNet-18,
ResNet-50, MobileNetV2, MobileNetV3-small and EfficientNet-B0. Experimental
results show that structures uncovered by PEEL exhibit competitive performance
with state-of-the-art pruning algorithms under various pruning settings. Our
code is available at https://github.com/cardwing/Codes-for-PEEL.
</p>
<a href="http://arxiv.org/abs/2103.01847" target="_blank">arXiv:2103.01847</a> [<a href="http://arxiv.org/pdf/2103.01847" target="_blank">pdf</a>]

<h2>HED-UNet: Combined Segmentation and Edge Detection for Monitoring the Antarctic Coastline. (arXiv:2103.01849v1 [cs.CV])</h2>
<h3>Konrad Heidler, Lichao Mou, Celia Baumhoer, Andreas Dietz, Xiao Xiang Zhu</h3>
<p>Deep learning-based coastline detection algorithms have begun to outshine
traditional statistical methods in recent years. However, they are usually
trained only as single-purpose models to either segment land and water or
delineate the coastline. In contrast to this, a human annotator will usually
keep a mental map of both segmentation and delineation when performing manual
coastline detection. To take into account this task duality, we therefore
devise a new model to unite these two approaches in a deep learning model. By
taking inspiration from the main building blocks of a semantic segmentation
framework (UNet) and an edge detection framework (HED), both tasks are combined
in a natural way. Training is made efficient by employing deep supervision on
side predictions at multiple resolutions. Finally, a hierarchical attention
mechanism is introduced to adaptively merge these multiscale predictions into
the final model output. The advantages of this approach over other traditional
and deep learning-based methods for coastline detection are demonstrated on a
dataset of Sentinel-1 imagery covering parts of the Antarctic coast, where
coastline detection is notoriously difficult. An implementation of our method
is available at \url{https://github.com/khdlr/HED-UNet}.
</p>
<a href="http://arxiv.org/abs/2103.01849" target="_blank">arXiv:2103.01849</a> [<a href="http://arxiv.org/pdf/2103.01849" target="_blank">pdf</a>]

<h2>Spatial-Phase Shallow Learning: Rethinking Face Forgery Detection in Frequency Domain. (arXiv:2103.01856v1 [cs.CV])</h2>
<h3>Honggu Liu, Xiaodan Li, Wenbo Zhou, Yuefeng Chen, Yuan He, Hui Xue, Weiming Zhang, Nenghai Yu</h3>
<p>The remarkable success in face forgery techniques has received considerable
attention in computer vision due to security concerns. We observe that
up-sampling is a necessary step of most face forgery techniques, and cumulative
up-sampling will result in obvious changes in the frequency domain, especially
in the phase spectrum. According to the property of natural images, the phase
spectrum preserves abundant frequency components that provide extra information
and complement the loss of the amplitude spectrum. To this end, we present a
novel Spatial-Phase Shallow Learning (SPSL) method, which combines spatial
image and phase spectrum to capture the up-sampling artifacts of face forgery
to improve the transferability, for face forgery detection. And we also
theoretically analyze the validity of utilizing the phase spectrum. Moreover,
we notice that local texture information is more crucial than high-level
semantic information for the face forgery detection task. So we reduce the
receptive fields by shallowing the network to suppress high-level features and
focus on the local region. Extensive experiments show that SPSL can achieve the
state-of-the-art performance on cross-datasets evaluation as well as
multi-class classification and obtain comparable results on single dataset
evaluation.
</p>
<a href="http://arxiv.org/abs/2103.01856" target="_blank">arXiv:2103.01856</a> [<a href="http://arxiv.org/pdf/2103.01856" target="_blank">pdf</a>]

<h2>Exploring Imitation Learning for Autonomous Driving with Feedback Synthesizer and Differentiable Rasterization. (arXiv:2103.01882v1 [cs.RO])</h2>
<h3>Jinyun Zhou, Rui Wang, Xu Liu, Yifei Jiang, Shu Jiang, Jiaming Tao, Jinghao Miao, Shiyu Song</h3>
<p>We present a learning-based planner that aims to robustly drive a vehicle by
mimicking human drivers' driving behavior. We leverage a mid-to-mid approach
that allows us to manipulate the input to our imitation learning network
freely. With that in mind, we propose a novel feedback synthesizer for data
augmentation. It allows our agent to gain more driving experience in various
previously unseen environments that are likely to encounter, thus improving
overall performance. This is in contrast to prior works that rely purely on
random synthesizers. Furthermore, rather than completely commit to imitating,
we introduce task losses that penalize undesirable behaviors, such as
collision, off-road, and so on. Unlike prior works, this is done by introducing
a differentiable vehicle rasterizer that directly converts the waypoints output
by the network into images. This effectively avoids the usage of heavyweight
ConvLSTM networks, therefore, yields a faster model inference time. About the
network architecture, we exploit an attention mechanism that allows the network
to reason critical objects in the scene and produce better interpretable
attention heatmaps. To further enhance the safety and robustness of the
network, we add an optional optimization-based post-processing planner
improving the driving comfort. We comprehensively validate our method's
effectiveness in different scenarios that are specifically created for
evaluating self-driving vehicles. Results demonstrate that our learning-based
planner achieves high intelligence and can handle complex situations. Detailed
ablation and visualization analysis are included to further demonstrate each of
our proposed modules' effectiveness in our method.
</p>
<a href="http://arxiv.org/abs/2103.01882" target="_blank">arXiv:2103.01882</a> [<a href="http://arxiv.org/pdf/2103.01882" target="_blank">pdf</a>]

<h2>Prognostics-Informed Battery Reconfiguration in a Multi-Battery Small UAS Energy System. (arXiv:2103.01883v1 [cs.RO])</h2>
<h3>Prashin Sharma, Ella Atkins</h3>
<p>Batteries have been identified as one most likely small UAS (sUAS) components
to fail in flight. sUAS safety will therefore be improved with redundant or
backup batteries. This paper presents a prognostics-informed Markov Decision
Process (MDP) model for managing multi-battery reconfiguration for sUAS
missions. Typical lithium polymer (Lipo) battery properties are experimentally
characterized and used in Monte Carlo simulations to establish battery dynamics
in sUAS flights of varying duration. Case studies illustrate the trade off
between multi-battery system increased complexity/weight and resilience to
non-ideal battery performance.
</p>
<a href="http://arxiv.org/abs/2103.01883" target="_blank">arXiv:2103.01883</a> [<a href="http://arxiv.org/pdf/2103.01883" target="_blank">pdf</a>]

<h2>Learning-based Bias Correction for Time Difference of Arrival Ultra-wideband Localization of Resource-constrained Mobile Robots. (arXiv:2103.01885v1 [cs.RO])</h2>
<h3>Wenda Zhao, Jacopo Panerati, Angela P. Schoellig (University of Toronto Institute for Aerospace Studies, Vector Institute for Artificial Intelligence)</h3>
<p>Accurate indoor localization is a crucial enabling technology for many
robotics applications, from warehouse management to monitoring tasks.
Ultra-wideband (UWB) time difference of arrival (TDOA)-based localization is a
promising lightweight, low-cost solution that can scale to a large number of
devices -- making it especially suited for resource-constrained multi-robot
applications. However, the localization accuracy of standard, commercially
available UWB radios is often insufficient due to significant measurement bias
and outliers. In this letter, we address these issues by proposing a robust UWB
TDOA localization framework comprising of (i) learning-based bias correction
and (ii) M-estimation-based robust filtering to handle outliers. The key
properties of our approach are that (i) the learned biases generalize to
different UWB anchor setups and (ii) the approach is computationally efficient
enough to run on resource-constrained hardware. We demonstrate our approach on
a Crazyflie nano-quadcopter. Experimental results show that the proposed
localization framework, relying only on the onboard IMU and UWB, provides an
average of 42.08 percent localization error reduction (in three different
anchor setups) compared to the baseline approach without bias compensation. {We
also show autonomous trajectory tracking on a quadcopter using our UWB TDOA
localization approach.}
</p>
<a href="http://arxiv.org/abs/2103.01885" target="_blank">arXiv:2103.01885</a> [<a href="http://arxiv.org/pdf/2103.01885" target="_blank">pdf</a>]

<h2>Semantic Relation Reasoning for Shot-Stable Few-Shot Object Detection. (arXiv:2103.01903v1 [cs.CV])</h2>
<h3>Chenchen Zhu, Fangyi Chen, Uzair Ahmed, Marios Savvides</h3>
<p>Few-shot object detection is an imperative and long-lasting problem due to
the inherent long-tail distribution of real-world data. Its performance is
largely affected by the data scarcity of novel classes. But the semantic
relation between the novel classes and the base classes is constant regardless
of the data availability. In this work, we investigate utilizing this semantic
relation together with the visual information and introduce explicit relation
reasoning into the learning of novel object detection. Specifically, we
represent each class concept by a semantic embedding learned from a large
corpus of text. The detector is trained to project the image representations of
objects into this embedding space. We also identify the problems of trivially
using the raw embeddings with a heuristic knowledge graph and propose to
augment the embeddings with a dynamic relation graph. As a result, our few-shot
detector, termed SRR-FSD, is robust and stable to the variation of shots of
novel objects. Experiments show that SRR-FSD can achieve competitive results at
higher shots, and more importantly, a significantly better performance given
both lower explicit and implicit shots. The proposed benchmark protocol with
implicit shots removed from the pretrained classification dataset can serve as
a more realistic setting for future research.
</p>
<a href="http://arxiv.org/abs/2103.01903" target="_blank">arXiv:2103.01903</a> [<a href="http://arxiv.org/pdf/2103.01903" target="_blank">pdf</a>]

<h2>WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning. (arXiv:2103.01913v1 [cs.CV])</h2>
<h3>Krishna Srinivasan, Karthik Raman, Jiecao Chen, Michael Bendersky, Marc Najork</h3>
<p>The milestone improvements brought about by deep representation learning and
pre-training techniques have led to large performance gains across downstream
NLP, IR and Vision tasks. Multimodal modeling techniques aim to leverage large
high-quality visio-linguistic datasets for learning complementary information
(across image and text modalities). In this paper, we introduce the
Wikipedia-based Image Text (WIT)
Dataset\footnote{\url{https://github.com/google-research-datasets/wit}} to
better facilitate multimodal, multilingual learning. WIT is composed of a
curated set of 37.6 million entity rich image-text examples with 11.5 million
unique images across 108 Wikipedia languages. Its size enables WIT to be used
as a pretraining dataset for multimodal models, as we show when applied to
downstream tasks such as image-text retrieval. WIT has four main and unique
advantages. First, WIT is the largest multimodal dataset by the number of
image-text examples by 3x (at the time of writing). Second, WIT is massively
multilingual (first of its kind) with coverage over 100+ languages (each of
which has at least 12K examples) and provides cross-lingual texts for many
images. Third, WIT represents a more diverse set of concepts and real world
entities relative to what previous datasets cover. Lastly, WIT provides a very
challenging real-world test set, as we empirically illustrate using an
image-text retrieval task as an example.
</p>
<a href="http://arxiv.org/abs/2103.01913" target="_blank">arXiv:2103.01913</a> [<a href="http://arxiv.org/pdf/2103.01913" target="_blank">pdf</a>]

<h2>Masked Face Recognition: Human vs. Machine. (arXiv:2103.01924v1 [cs.CV])</h2>
<h3>Naser Damer, Fadi Boutros, Marius S&#xfc;&#xdf;milch, Meiling Fang, Florian Kirchbuchner, Arjan Kuijper</h3>
<p>The recent COVID-19 pandemic has increased the focus on hygienic and
contactless identity verification methods. However, the pandemic led to the
wide use of face masks, essential to keep the pandemic under control. The
effect of wearing a mask on face recognition in a collaborative environment is
currently sensitive yet understudied issue. Recent reports have tackled this by
evaluating the masked probe effect on the performance of automatic face
recognition solutions. However, such solutions can fail in certain processes,
leading to performing the verification task by a human expert. This work
provides a joint evaluation and in-depth analyses of the face verification
performance of human experts in comparison to state-of-the-art automatic face
recognition solutions. This involves an extensive evaluation with 12 human
experts and 4 automatic recognition solutions. The study concludes with a set
of take-home-messages on different aspects of the correlation between the
verification behavior of human and machine.
</p>
<a href="http://arxiv.org/abs/2103.01924" target="_blank">arXiv:2103.01924</a> [<a href="http://arxiv.org/pdf/2103.01924" target="_blank">pdf</a>]

<h2>Meta-Learning-Based Robust Adaptive Flight Control Under Uncertain Wind Conditions. (arXiv:2103.01932v1 [cs.RO])</h2>
<h3>Michael O&#x27;Connell, Guanya Shi, Xichen Shi, Soon-Jo Chung</h3>
<p>Realtime model learning proves challenging for complex dynamical systems,
such as drones flying in variable wind conditions. Machine learning technique
such as deep neural networks have high representation power but is often too
slow to update onboard. On the other hand, adaptive control relies on simple
linear parameter models can update as fast as the feedback control loop. We
propose an online composite adaptation method that treats outputs from a deep
neural network as a set of basis functions capable of representing different
wind conditions. To help with training, meta-learning techniques are used to
optimize the network output useful for adaptation. We validate our approach by
flying a drone in an open air wind tunnel under varying wind conditions and
along challenging trajectories. We compare the result with other adaptive
controller with different basis function sets and show improvement over
tracking and prediction errors.
</p>
<a href="http://arxiv.org/abs/2103.01932" target="_blank">arXiv:2103.01932</a> [<a href="http://arxiv.org/pdf/2103.01932" target="_blank">pdf</a>]

<h2>Fixing Data Augmentation to Improve Adversarial Robustness. (arXiv:2103.01946v1 [cs.CV])</h2>
<h3>Sylvestre-Alvise Rebuffi, Sven Gowal, Dan A. Calian, Florian Stimberg, Olivia Wiles, Timothy Mann</h3>
<p>Adversarial training suffers from robust overfitting, a phenomenon where the
robust test accuracy starts to decrease during training. In this paper, we
focus on both heuristics-driven and data-driven augmentations as a means to
reduce robust overfitting. First, we demonstrate that, contrary to previous
findings, when combined with model weight averaging, data augmentation can
significantly boost robust accuracy. Second, we explore how state-of-the-art
generative models can be leveraged to artificially increase the size of the
training set and further improve adversarial robustness. Finally, we evaluate
our approach on CIFAR-10 against $\ell_\infty$ and $\ell_2$ norm-bounded
perturbations of size $\epsilon = 8/255$ and $\epsilon = 128/255$,
respectively. We show large absolute improvements of +7.06% and +5.88% in
robust accuracy compared to previous state-of-the-art methods. In particular,
against $\ell_\infty$ norm-bounded perturbations of size $\epsilon = 8/255$,
our model reaches 64.20% robust accuracy without using any external data,
beating most prior works that use external data.
</p>
<a href="http://arxiv.org/abs/2103.01946" target="_blank">arXiv:2103.01946</a> [<a href="http://arxiv.org/pdf/2103.01946" target="_blank">pdf</a>]

<h2>Predicting Video with VQVAE. (arXiv:2103.01950v1 [cs.CV])</h2>
<h3>Jacob Walker, Ali Razavi, A&#xe4;ron van den Oord</h3>
<p>In recent years, the task of video prediction-forecasting future video given
past video frames-has attracted attention in the research community. In this
paper we propose a novel approach to this problem with Vector Quantized
Variational AutoEncoders (VQ-VAE). With VQ-VAE we compress high-resolution
videos into a hierarchical set of multi-scale discrete latent variables.
Compared to pixels, this compressed latent space has dramatically reduced
dimensionality, allowing us to apply scalable autoregressive generative models
to predict video. In contrast to previous work that has largely emphasized
highly constrained datasets, we focus on very diverse, large-scale datasets
such as Kinetics-600. We predict video at a higher resolution on unconstrained
videos, 256x256, than any other previous method to our knowledge. We further
validate our approach against prior work via a crowdsourced human evaluation.
</p>
<a href="http://arxiv.org/abs/2103.01950" target="_blank">arXiv:2103.01950</a> [<a href="http://arxiv.org/pdf/2103.01950" target="_blank">pdf</a>]

<h2>Robustness of classifiers to universal perturbations: a geometric perspective. (arXiv:1705.09554v2 [cs.CV] UPDATED)</h2>
<h3>Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, Pascal Frossard, Stefano Soatto</h3>
<p>Deep networks have recently been shown to be vulnerable to universal
perturbations: there exist very small image-agnostic perturbations that cause
most natural images to be misclassified by such classifiers. In this paper, we
propose the first quantitative analysis of the robustness of classifiers to
universal perturbations, and draw a formal link between the robustness to
universal perturbations, and the geometry of the decision boundary.
Specifically, we establish theoretical bounds on the robustness of classifiers
under two decision boundary models (flat and curved models). We show in
particular that the robustness of deep networks to universal perturbations is
driven by a key property of their curvature: there exists shared directions
along which the decision boundary of deep networks is systematically positively
curved. Under such conditions, we prove the existence of small universal
perturbations. Our analysis further provides a novel geometric method for
computing universal perturbations, in addition to explaining their properties.
</p>
<a href="http://arxiv.org/abs/1705.09554" target="_blank">arXiv:1705.09554</a> [<a href="http://arxiv.org/pdf/1705.09554" target="_blank">pdf</a>]

<h2>ACE: Adaptive Confusion Energy for Natural World Data Distribution. (arXiv:1910.12423v2 [cs.CV] UPDATED)</h2>
<h3>Yen-Chi Hsu, Cheng-Yao Hong, Wan-Cyuan Fan, Ming-Sui Lee, Davi Geiger, Tyng-Luh Liu</h3>
<p>With the development of deep learning, standard classification problems have
achieved good results. However, conventional classification problems are often
too idealistic. Most data in the natural world usually have imbalanced
distribution and fine-grained characteristics. Recently, many state-of-the-art
approaches tend to focus on one or another separately, but rarely on both. In
this paper, we introduce a novel and adaptive batch-wise regularization based
on the proposed Adaptive Confusion Energy (ACE) to flexibly address the nature
world distribution, which usually involves fine-grained and long-tailed
properties at the same time. ACE increases the difficulty of the training
process and further alleviates the overfitting problem. Through the datasets
with the technical issue in fine-grained (CUB, CAR, AIR) and long-tailed
(ImageNet-LT), or comprehensive issues (CUB-LT, iNaturalist), the result shows
that the ACE is not only competitive to some state-of-the-art on performance
but also demonstrates the effectiveness of training.
</p>
<a href="http://arxiv.org/abs/1910.12423" target="_blank">arXiv:1910.12423</a> [<a href="http://arxiv.org/pdf/1910.12423" target="_blank">pdf</a>]

<h2>Road Network and Travel Time Extraction from Multiple Look Angles with SpaceNet Data. (arXiv:2001.05923v2 [cs.CV] UPDATED)</h2>
<h3>Adam Van Etten, Jacob Shermeyer, Daniel Hogan, Nicholas Weir, Ryan Lewis</h3>
<p>Identification of road networks and optimal routes directly from remote
sensing is of critical importance to a broad array of humanitarian and
commercial applications. Yet while identification of road pixels has been
attempted before, estimation of route travel times from overhead imagery
remains a novel problem, particularly for off-nadir overhead imagery. To this
end, we extract road networks with travel time estimates from the SpaceNet MVOI
dataset. Utilizing the CRESIv2 framework, we demonstrate the ability to extract
road networks in various observation angles and quantify performance at 27
unique nadir angles with the graph-theoretic APLS_length and APLS_time metrics.
A minimal gap of 0.03 between APLS_length and APLS_time scores indicates that
our approach yields speed limits and travel times with very high fidelity. We
also explore the utility of incorporating all available angles during model
training, and find a peak score of APLS_time = 0.56. The combined model
exhibits greatly improved robustness over angle-specific models, despite the
very different appearance of road networks at extremely oblique off-nadir
angles versus images captured from directly overhead.
</p>
<a href="http://arxiv.org/abs/2001.05923" target="_blank">arXiv:2001.05923</a> [<a href="http://arxiv.org/pdf/2001.05923" target="_blank">pdf</a>]

<h2>AlignSeg: Feature-Aligned Segmentation Networks. (arXiv:2003.00872v2 [cs.CV] UPDATED)</h2>
<h3>Zilong Huang, Yunchao Wei, Xinggang Wang, Wenyu Liu, Thomas S. Huang, Humphrey Shi</h3>
<p>Aggregating features in terms of different convolutional blocks or contextual
embeddings has been proven to be an effective way to strengthen feature
representations for semantic segmentation. However, most of the current popular
network architectures tend to ignore the misalignment issues during the feature
aggregation process caused by 1) step-by-step downsampling operations, and 2)
indiscriminate contextual information fusion. In this paper, we explore the
principles in addressing such feature misalignment issues and inventively
propose Feature-Aligned Segmentation Networks (AlignSeg). AlignSeg consists of
two primary modules, i.e., the Aligned Feature Aggregation (AlignFA) module and
the Aligned Context Modeling (AlignCM) module. First, AlignFA adopts a simple
learnable interpolation strategy to learn transformation offsets of pixels,
which can effectively relieve the feature misalignment issue caused by
multiresolution feature aggregation. Second, with the contextual embeddings in
hand, AlignCM enables each pixel to choose private custom contextual
information in an adaptive manner, making the contextual embeddings aligned
better to provide appropriate guidance. We validate the effectiveness of our
AlignSeg network with extensive experiments on Cityscapes and ADE20K, achieving
new state-of-the-art mIoU scores of 82.6% and 45.95%, respectively. Our source
code will be made available.
</p>
<a href="http://arxiv.org/abs/2003.00872" target="_blank">arXiv:2003.00872</a> [<a href="http://arxiv.org/pdf/2003.00872" target="_blank">pdf</a>]

<h2>A flow disturbance estimation and rejection strategy for multirotors with round-trip trajectories. (arXiv:2003.02974v2 [cs.RO] UPDATED)</h2>
<h3>Jaeseung Byun, Simo A. M&#xe4;kiharju, Mark W. Mueller</h3>
<p>This paper presents a round-trip strategy of multirotors subject to unknown
flow disturbances. During the outbound flight, the vehicle immediately utilizes
the wind disturbance estimations in feedback control, as an attempt to reduce
the tracking error. During this phase, the disturbance estimations with respect
to the position are also recorded for future use. For the return flight, the
disturbances previously collected are then routed through a feedforward
controller. The major assumption here is that the disturbances may vary over
space, but not over time during the same mission. We demonstrate the
effectiveness of this feedforward strategy via experiments with two different
types of wind flows; a simple jet flow and a more complex flow. To use as a
baseline case, a cascaded PD controller with an additional feedback loop for
disturbance estimation was employed for outbound flights. To display our
contributions regarding the additional feedforward approach, an additional
feedforward correction term obtained via prerecorded data was integrated for
the return flight. Compared to the baseline controller, the feedforward
controller was observed to produce 43% less RMSE position error at a vehicle
ground velocity of 1 m/s with 6 m/s of environmental wind velocity. This
feedforward approach also produced 14% less RMSE position error for the complex
flows as well.
</p>
<a href="http://arxiv.org/abs/2003.02974" target="_blank">arXiv:2003.02974</a> [<a href="http://arxiv.org/pdf/2003.02974" target="_blank">pdf</a>]

<h2>BeCAPTCHA-Mouse: Synthetic Mouse Trajectories and Improved Bot Detection. (arXiv:2005.00890v2 [cs.CV] UPDATED)</h2>
<h3>Alejandro Acien, Aythami Morales, Julian Fierrez, Ruben Vera-Rodriguez</h3>
<p>We first study the suitability of behavioral biometrics to distinguish
between computers and humans, commonly named as bot detection. We then present
BeCAPTCHA-Mouse, a bot detector based on: i) a neuromotor model of mouse
dynamics to obtain a novel feature set for the classification of human and bot
samples; and ii) a learning framework involving real and synthetically
generated mouse trajectories. We propose two new mouse trajectory synthesis
methods for generating realistic data: a) a function-based method based on
heuristic functions, and b) a data-driven method based on Generative
Adversarial Networks (GANs) in which a Generator synthesizes human-like
trajectories from a Gaussian noise input. Experiments are conducted on a new
testbed also introduced here and available in GitHub: BeCAPTCHA-Mouse
Benchmark; useful for research in bot detection and other mouse-based HCI
applications. Our benchmark data consists of 15,000 mouse trajectories
including real data from 58 users and bot data with various levels of realism.
Our experiments show that BeCAPTCHA-Mouse is able to detect bot trajectories of
high realism with 93% of accuracy in average using only one mouse trajectory.
When our approach is fused with state-of-the-art mouse dynamic features, the
bot detection accuracy increases relatively by more than 36%, proving that
mouse-based bot detection is a fast, easy, and reliable tool to complement
traditional CAPTCHA systems.
</p>
<a href="http://arxiv.org/abs/2005.00890" target="_blank">arXiv:2005.00890</a> [<a href="http://arxiv.org/pdf/2005.00890" target="_blank">pdf</a>]

<h2>VirTex: Learning Visual Representations from Textual Annotations. (arXiv:2006.06666v2 [cs.CV] UPDATED)</h2>
<h3>Karan Desai, Justin Johnson</h3>
<p>The de-facto approach to many vision tasks is to start from pretrained visual
representations, typically learned via supervised training on ImageNet. Recent
methods have explored unsupervised pretraining to scale to vast quantities of
unlabeled images. In contrast, we aim to learn high-quality visual
representations from fewer images. To this end, we revisit supervised
pretraining, and seek data-efficient alternatives to classification-based
pretraining. We propose VirTex -- a pretraining approach using semantically
dense captions to learn visual representations. We train convolutional networks
from scratch on COCO Captions, and transfer them to downstream recognition
tasks including image classification, object detection, and instance
segmentation. On all tasks, VirTex yields features that match or exceed those
learned on ImageNet -- supervised or unsupervised -- despite using up to ten
times fewer images.
</p>
<a href="http://arxiv.org/abs/2006.06666" target="_blank">arXiv:2006.06666</a> [<a href="http://arxiv.org/pdf/2006.06666" target="_blank">pdf</a>]

<h2>MgSvF: Multi-Grained Slow vs. Fast Framework for Few-Shot Class-Incremental Learning. (arXiv:2006.15524v3 [cs.CV] UPDATED)</h2>
<h3>Hanbin Zhao, Yongjian Fu, Mintong Kang, Qi Tian, Fei Wu, Xi Li</h3>
<p>As a challenging problem, few-shot class-incremental learning (FSCIL)
continually learns a sequence of tasks, confronting the dilemma between slow
forgetting of old knowledge and fast adaptation to new knowledge. In this
paper, we concentrate on this "slow vs. fast" (SvF) dilemma to determine which
knowledge components to be updated in a slow fashion or a fast fashion, and
thereby balance old-knowledge preservation and new-knowledge adaptation. We
propose a multi-grained SvF learning strategy to cope with the SvF dilemma from
two different grains: intra-space (within the same feature space) and
inter-space (between two different feature spaces). The proposed strategy
designs a novel frequency-aware regularization to boost the intra-space SvF
capability, and meanwhile develops a new feature space composition operation to
enhance the inter-space SvF learning performance. With the multi-grained SvF
learning strategy, our method outperforms the state-of-the-art approaches by a
large margin.
</p>
<a href="http://arxiv.org/abs/2006.15524" target="_blank">arXiv:2006.15524</a> [<a href="http://arxiv.org/pdf/2006.15524" target="_blank">pdf</a>]

<h2>Artificial Intelligence in the Creative Industries: A Review. (arXiv:2007.12391v4 [cs.CV] UPDATED)</h2>
<h3>Nantheera Anantrasirichai, David Bull</h3>
<p>This paper reviews the current state of the art in Artificial Intelligence
(AI) technologies and applications in the context of the creative industries. A
brief background of AI, and specifically Machine Learning (ML) algorithms, is
provided including Convolutional Neural Network (CNNs), Generative Adversarial
Networks (GANs), Recurrent Neural Networks (RNNs) and Deep Reinforcement
Learning (DRL). We categorise creative applications into five groups related to
how AI technologies are used: i) content creation, ii) information analysis,
iii) content enhancement and post production workflows, iv) information
extraction and enhancement, and v) data compression. We critically examine the
successes and limitations of this rapidly advancing technology in each of these
areas. We further differentiate between the use of AI as a creative tool and
its potential as a creator in its own right. We foresee that, in the near
future, machine learning-based AI will be adopted widely as a tool or
collaborative assistant for creativity. In contrast, we observe that the
successes of machine learning in domains with fewer constraints, where AI is
the `creator', remain modest. The potential of AI (or its developers) to win
awards for its original creations in competition with human creatives is also
limited, based on contemporary technologies. We therefore conclude that, in the
context of creative industries, maximum benefit from AI will be derived where
its focus is human centric -- where it is designed to augment, rather than
replace, human creativity.
</p>
<a href="http://arxiv.org/abs/2007.12391" target="_blank">arXiv:2007.12391</a> [<a href="http://arxiv.org/pdf/2007.12391" target="_blank">pdf</a>]

<h2>Deep Co-Training with Task Decomposition for Semi-Supervised Domain Adaptation. (arXiv:2007.12684v3 [cs.CV] UPDATED)</h2>
<h3>Luyu Yang, Yan Wang, Mingfei Gao, Abhinav Shrivastava, Kilian Q. Weinberger, Wei-Lun Chao, Ser-Nam Lim</h3>
<p>Semi-supervised domain adaptation (SSDA) aims to adapt models from a labeled
source domain to a different but related target domain, from which unlabeled
data and a small set of labeled data are provided. In this paper we propose a
new approach for SSDA, which is to explicitly decompose the SSDA task into two
sub-tasks: a semi-supervised learning (SSL) task in the target domain and an
unsupervised domain adaptation (UDA) task across domains. We show that these
two sub-tasks yield very different classifiers and thus naturally fits into the
well established co-training framework, in which the two classifiers exchange
their high confident predictions to iteratively "teach each other" so that both
classifiers can excel in the target domain. We call our approach Deep
Co-Training with Task Decomposition (DeCoTa). DeCoTa requires no adversarial
training, making it fairly easy to implement. DeCoTa achieves state-of-the-art
results on several SSDA datasets, outperforming the prior art by a notable 4%
margin on DomainNet.
</p>
<a href="http://arxiv.org/abs/2007.12684" target="_blank">arXiv:2007.12684</a> [<a href="http://arxiv.org/pdf/2007.12684" target="_blank">pdf</a>]

<h2>Fine-grained Visual Textual Alignment for Cross-Modal Retrieval using Transformer Encoders. (arXiv:2008.05231v2 [cs.CV] UPDATED)</h2>
<h3>Nicola Messina, Giuseppe Amato, Andrea Esuli, Fabrizio Falchi, Claudio Gennaro, St&#xe9;phane Marchand-Maillet</h3>
<p>Despite the evolution of deep-learning-based visual-textual processing
systems, precise multi-modal matching remains a challenging task. In this work,
we tackle the task of cross-modal retrieval through image-sentence matching
based on word-region alignments, using supervision only at the global
image-sentence level. Specifically, we present a novel approach called
Transformer Encoder Reasoning and Alignment Network (TERAN). TERAN enforces a
fine-grained match between the underlying components of images and sentences,
i.e., image regions and words, respectively, in order to preserve the
informative richness of both modalities. TERAN obtains state-of-the-art results
on the image retrieval task on both MS-COCO and Flickr30k datasets. Moreover,
on MS-COCO, it also outperforms current approaches on the sentence retrieval
task.

Focusing on scalable cross-modal information retrieval, TERAN is designed to
keep the visual and textual data pipelines well separated. Cross-attention
links invalidate any chance to separately extract visual and textual features
needed for the online search and the offline indexing steps in large-scale
retrieval systems. In this respect, TERAN merges the information from the two
domains only during the final alignment phase, immediately before the loss
computation. We argue that the fine-grained alignments produced by TERAN pave
the way towards the research for effective and efficient methods for
large-scale cross-modal information retrieval. We compare the effectiveness of
our approach against relevant state-of-the-art methods. On the MS-COCO 1K test
set, we obtain an improvement of 5.7% and 3.5% respectively on the image and
the sentence retrieval tasks on the Recall@1 metric. The code used for the
experiments is publicly available on GitHub at
https://github.com/mesnico/TERAN.
</p>
<a href="http://arxiv.org/abs/2008.05231" target="_blank">arXiv:2008.05231</a> [<a href="http://arxiv.org/pdf/2008.05231" target="_blank">pdf</a>]

<h2>Deep Domain Adaptation for Ordinal Regression of Pain Intensity Estimation Using Weakly-Labelled Videos. (arXiv:2008.06392v2 [cs.CV] UPDATED)</h2>
<h3>Gnana Praveen R, Eric Granger, Patrick Cardinal</h3>
<p>Estimation of pain intensity from facial expressions captured in videos has
an immense potential for health care applications. Given the challenges related
to subjective variations of facial expressions, and operational capture
conditions, the accuracy of state-of-the-art DL models for recognizing facial
expressions may decline. Domain adaptation has been widely explored to
alleviate the problem of domain shifts that typically occur between video data
captured across various source and target domains. Moreover, given the
laborious task of collecting and annotating videos, and subjective bias due to
ambiguity among adjacent intensity levels, weakly-supervised learning is
gaining attention in such applications. State-of-the-art WSL models are
typically formulated as regression problems, and do not leverage the ordinal
relationship among pain intensity levels, nor temporal coherence of multiple
consecutive frames. This paper introduces a new DL model for weakly-supervised
DA with ordinal regression that can be adapted using target domain videos with
coarse labels provided on a periodic basis. The WSDA-OR model enforces ordinal
relationships among intensity levels assigned to target sequences, and
associates multiple relevant frames to sequence-level labels. In particular, it
learns discriminant and domain-invariant feature representations by integrating
multiple instance learning with deep adversarial DA, where soft Gaussian labels
are used to efficiently represent the weak ordinal sequence-level labels from
target domain. The proposed approach was validated using RECOLA video dataset
as fully-labeled source domain data, and UNBC-McMaster shoulder pain video
dataset as weakly-labeled target domain data. We have also validated WSDA-OR on
BIOVID and Fatigue datasets for sequence level estimation.
</p>
<a href="http://arxiv.org/abs/2008.06392" target="_blank">arXiv:2008.06392</a> [<a href="http://arxiv.org/pdf/2008.06392" target="_blank">pdf</a>]

<h2>Goal-Auxiliary Actor-Critic for 6D Robotic Grasping with Point Clouds. (arXiv:2010.00824v3 [cs.RO] UPDATED)</h2>
<h3>Lirui Wang, Yu Xiang, Dieter Fox</h3>
<p>6D robotic grasping beyond top-down bin-picking scenarios is a challenging
task. Previous solutions based on 6D grasp synthesis with robot motion planning
usually operate in an open-loop setting without considering perception
feedback, dynamics, and contacts with objects, which makes them sensitive to
grasp synthesis errors. In this work, we propose a novel method for learning
closed-loop control policies for 6D robotic grasping using point clouds from an
egocentric camera. We combine imitation learning and reinforcement learning in
order to grasp unseen objects and handle the continuous 6D action space, where
expert demonstrations are obtained from a joint motion and grasp planner. We
introduce a goal-auxiliary actor-critic algorithm, which uses grasping goal
prediction as an auxiliary task to facilitate policy learning. The supervision
on grasping goals can be obtained from the expert planner for known objects or
from hindsight goals for unknown objects. Overall, our learned closed-loop
policy achieves over 90% success rates on grasping various ShapeNet objects and
YCB objects in simulation. The policy also transfers well to the real world for
grasping unseen objects in both a tabletop setting and a human-robot handover
setting in our experiments. Our video can be found at
https://sites.google.com/view/gaddpg .
</p>
<a href="http://arxiv.org/abs/2010.00824" target="_blank">arXiv:2010.00824</a> [<a href="http://arxiv.org/pdf/2010.00824" target="_blank">pdf</a>]

<h2>BiPointNet: Binary Neural Network for Point Clouds. (arXiv:2010.05501v3 [cs.CV] UPDATED)</h2>
<h3>Haotong Qin, Zhongang Cai, Mingyuan Zhang, Yifu Ding, Haiyu Zhao, Shuai Yi, Xianglong Liu, Hao Su</h3>
<p>To alleviate the resource constraint for real-time point cloud applications
that run on edge devices, in this paper we present BiPointNet, the first model
binarization approach for efficient deep learning on point clouds. We discover
that the immense performance drop of binarized models for point clouds mainly
stems from two challenges: aggregation-induced feature homogenization that
leads to a degradation of information entropy, and scale distortion that
hinders optimization and invalidates scale-sensitive structures. With
theoretical justifications and in-depth analysis, our BiPointNet introduces
Entropy-Maximizing Aggregation (EMA) to modulate the distribution before
aggregation for the maximum information entropy, and Layer-wise Scale Recovery
(LSR) to efficiently restore feature representation capacity. Extensive
experiments show that BiPointNet outperforms existing binarization methods by
convincing margins, at the level even comparable with the full precision
counterpart. We highlight that our techniques are generic, guaranteeing
significant improvements on various fundamental tasks and mainstream backbones.
Moreover, BiPointNet gives an impressive 14.7x speedup and 18.9x storage saving
on real-world resource-constrained devices.
</p>
<a href="http://arxiv.org/abs/2010.05501" target="_blank">arXiv:2010.05501</a> [<a href="http://arxiv.org/pdf/2010.05501" target="_blank">pdf</a>]

<h2>Object Permanence Through Audio-Visual Representations. (arXiv:2010.09948v2 [cs.RO] UPDATED)</h2>
<h3>Fanjun Bu, Chien-Ming Huang</h3>
<p>As robots perform manipulation tasks and interact with objects, it is
probable that they accidentally drop objects that subsequently bounce out of
their visual fields (e.g., due to an inadequate grasp of an unfamiliar object).
To enable robots to recover from such errors, we draw upon the concept of
object permanence-objects remain in existence even when they are not being
sensed (e.g., seen) directly. In particular, we developed a multimodal neural
network model-using a partial, observed bounce trajectory and the audio
resulting from drop impact as its inputs-to predict the full bounce trajectory
and the end location of a dropped object. We empirically show that: (1) our
multimodal method predicted end locations close in proximity (i.e., within the
visual field of the robot's wrist camera) to the actual locations and (2) the
robot was able to retrieve dropped objects by applying minimal vision-based
pick-up adjustments. Additionally, we show that our method outperformed five
comparison baselines in retrieving dropped objects.
</p>
<a href="http://arxiv.org/abs/2010.09948" target="_blank">arXiv:2010.09948</a> [<a href="http://arxiv.org/pdf/2010.09948" target="_blank">pdf</a>]

<h2>Self-Supervised Learning of Part Mobility from Point Cloud Sequence. (arXiv:2010.11735v2 [cs.CV] UPDATED)</h2>
<h3>Yahao Shi, Xinyu Cao, Bin Zhou</h3>
<p>Part mobility analysis is a significant aspect required to achieve a
functional understanding of 3D objects. It would be natural to obtain part
mobility from the continuous part motion of 3D objects. In this study, we
introduce a self-supervised method for segmenting motion parts and predicting
their motion attributes from a point cloud sequence representing a dynamic
object. To sufficiently utilize spatiotemporal information from the point cloud
sequence, we generate trajectories by using correlations among successive
frames of the sequence instead of directly processing the point clouds. We
propose a novel neural network architecture called PointRNN to learn feature
representations of trajectories along with their part rigid motions. We
evaluate our method on various tasks including motion part segmentation, motion
axis prediction and motion range estimation. The results demonstrate that our
method outperforms previous techniques on both synthetic and real datasets.
Moreover, our method has the ability to generalize to new and unseen objects.
It is important to emphasize that it is not required to know any prior shape
structure, prior shape category information, or shape orientation. To the best
of our knowledge, this is the first study on deep learning to extract part
mobility from point cloud sequence of a dynamic object.
</p>
<a href="http://arxiv.org/abs/2010.11735" target="_blank">arXiv:2010.11735</a> [<a href="http://arxiv.org/pdf/2010.11735" target="_blank">pdf</a>]

<h2>SCFusion: Real-time Incremental Scene Reconstruction with Semantic Completion. (arXiv:2010.13662v2 [cs.CV] UPDATED)</h2>
<h3>Shun-Cheng Wu, Keisuke Tateno, Nassir Navab, Federico Tombari</h3>
<p>Real-time scene reconstruction from depth data inevitably suffers from
occlusion, thus leading to incomplete 3D models. Partial reconstructions, in
turn, limit the performance of algorithms that leverage them for applications
in the context of, e.g., augmented reality, robotic navigation, and 3D mapping.
Most methods address this issue by predicting the missing geometry as an
offline optimization, thus being incompatible with real-time applications. We
propose a framework that ameliorates this issue by performing scene
reconstruction and semantic scene completion jointly in an incremental and
real-time manner, based on an input sequence of depth maps. Our framework
relies on a novel neural architecture designed to process occupancy maps and
leverages voxel states to accurately and efficiently fuse semantic completion
with the 3D global model. We evaluate the proposed approach quantitatively and
qualitatively, demonstrating that our method can obtain accurate 3D semantic
scene completion in real-time.
</p>
<a href="http://arxiv.org/abs/2010.13662" target="_blank">arXiv:2010.13662</a> [<a href="http://arxiv.org/pdf/2010.13662" target="_blank">pdf</a>]

<h2>Learning Stable Normalizing-Flow Control for Robotic Manipulation. (arXiv:2011.00072v2 [cs.RO] UPDATED)</h2>
<h3>Shahbaz Abdul Khader, Hang Yin, Pietro Falco, Danica Kragic</h3>
<p>Reinforcement Learning (RL) of robotic manipulation skills, despite its
impressive successes, stands to benefit from incorporating domain knowledge
from control theory. One of the most important properties that is of interest
is control stability. Ideally, one would like to achieve stability guarantees
while staying within the framework of state-of-the-art deep RL algorithms. Such
a solution does not exist in general, especially one that scales to complex
manipulation tasks. We contribute towards closing this gap by introducing
$\textit{normalizing-flow}$ control structure, that can be deployed in any
latest deep RL algorithms. While stable exploration is not guaranteed, our
method is designed to ultimately produce deterministic controllers with
provable stability. In addition to demonstrating our method on challenging
contact-rich manipulation tasks, we also show that it is possible to achieve
considerable exploration efficiency--reduced state space coverage and actuation
efforts--without losing learning efficiency.
</p>
<a href="http://arxiv.org/abs/2011.00072" target="_blank">arXiv:2011.00072</a> [<a href="http://arxiv.org/pdf/2011.00072" target="_blank">pdf</a>]

<h2>Educational robotics for children and their teachers. (arXiv:2011.08311v2 [cs.RO] UPDATED)</h2>
<h3>Cristina Gena, Claudio Mattutino, Davide Cellie, Enrico Mosca</h3>
<p>This paper describes a Google Educator funded project devoted to the training
of teachers (primary and secondary school) through an e-learning platform that
will introduce them to educational robotics using Wolly, a social, educational
and affective robot.
</p>
<a href="http://arxiv.org/abs/2011.08311" target="_blank">arXiv:2011.08311</a> [<a href="http://arxiv.org/pdf/2011.08311" target="_blank">pdf</a>]

<h2>Reachability-based Trajectory Safeguard (RTS): A Safe and Fast Reinforcement Learning Safety Layer for Continuous Control. (arXiv:2011.08421v3 [cs.RO] UPDATED)</h2>
<h3>Yifei Simon Shao, Chao Chen, Shreyas Kousik, Ram Vasudevan</h3>
<p>Reinforcement Learning (RL) algorithms have achieved remarkable performance
in decision making and control tasks due to their ability to reason about
long-term, cumulative reward using trial and error. However, during RL
training, applying this trial-and-error approach to real-world robots operating
in safety critical environment may lead to collisions. To address this
challenge, this paper proposes a Reachability-based Trajectory Safeguard (RTS),
which leverages reachability analysis to ensure safety during training and
operation. Given a known (but uncertain) model of a robot, RTS precomputes a
Forward Reachable Set of the robot tracking a continuum of parameterized
trajectories. At runtime, the RL agent selects from this continuum in a
receding-horizon way to control the robot; the FRS is used to identify if the
agent's choice is safe or not, and to adjust unsafe choices. The efficacy of
this method is illustrated on three nonlinear robot models, including a 12-D
quadrotor drone, in simulation and in comparison with state-of-the-art safe
motion planning methods.
</p>
<a href="http://arxiv.org/abs/2011.08421" target="_blank">arXiv:2011.08421</a> [<a href="http://arxiv.org/pdf/2011.08421" target="_blank">pdf</a>]

<h2>MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera. (arXiv:2011.11814v2 [cs.CV] UPDATED)</h2>
<h3>Felix Wimbauer, Nan Yang, Lukas von Stumberg, Niclas Zeller, Daniel Cremers</h3>
<p>In this paper, we propose MonoRec, a semi-supervised monocular dense
reconstruction architecture that predicts depth maps from a single moving
camera in dynamic environments. MonoRec is based on a MVS setting which encodes
the information of multiple consecutive images in a cost volume. To deal with
dynamic objects in the scene, we introduce a MaskModule that predicts moving
object masks by leveraging the photometric inconsistencies encoded in the cost
volumes. Unlike other MVS methods, MonoRec is able to predict accurate depths
for both static and moving objects by leveraging the predicted masks.
Furthermore, we present a novel multi-stage training scheme with a
semi-supervised loss formulation that does not require LiDAR depth values. We
carefully evaluate MonoRec on the KITTI dataset and show that it achieves
state-of-the-art performance compared to both multi-view and single-view
methods. With the model trained on KITTI, we further demonstrate that MonoRec
is able to generalize well to both the Oxford RobotCar dataset and the more
challenging TUM-Mono dataset recorded by a handheld camera. Training code and
pre-trained model will be published soon.
</p>
<a href="http://arxiv.org/abs/2011.11814" target="_blank">arXiv:2011.11814</a> [<a href="http://arxiv.org/pdf/2011.11814" target="_blank">pdf</a>]

<h2>Temporal Contrastive Graph for Self-supervised Video Representation Learning. (arXiv:2101.00820v6 [cs.CV] UPDATED)</h2>
<h3>Yang Liu, Keze Wang, Haoyuan Lan, Liang Lin</h3>
<p>Attempt to fully discover the temporal diversity and global-local
chronological characteristics for self-supervised video representation
learning, this work takes advantage of the temporal structure of videos and
further proposes a novel self-supervised method named Temporal Contrastive
Graph (TCG). In contrast to the existing methods that randomly shuffle the
video frames or video snippets within a video, the TCG roots in a hybrid graph
contrastive learning strategy to regard the inter-snippet and intra-snippet
temporal relationships as self-supervision signals for temporal representation
learning. To increase the temporal diversity of features, the TCG integrates
the prior knowledge about the frame and snippet orders into temporal
contrastive graphs, i.e., the intra-/inter- snippet temporal contrastive graph
modules. By randomly removing edges and masking node features of the
intra-snippet graphs or inter-snippet graphs, the TCG can generate different
correlated graph views. Then, specific contrastive losses are designed to
maximize the agreement between node embeddings in different views. To learn the
global context representation and recalibrate the channel-wise features
adaptively, we introduce an adaptive video snippet order prediction module,
which leverages the relational knowledge among video snippets to predict the
actual snippet orders. Experimental results demonstrate the superiority of our
TCG over the state-of-the-art methods on large-scale action recognition and
video retrieval benchmarks.
</p>
<a href="http://arxiv.org/abs/2101.00820" target="_blank">arXiv:2101.00820</a> [<a href="http://arxiv.org/pdf/2101.00820" target="_blank">pdf</a>]

<h2>Local Navigation and Docking of an Autonomous Robot Mower using Reinforcement Learning and Computer Vision. (arXiv:2101.06248v3 [cs.RO] UPDATED)</h2>
<h3>Ali Taghibakhshi, Nathan Ogden, Matthew West</h3>
<p>We demonstrate a successful navigation and docking control system for the
John Deere Tango autonomous mower, using only a single camera as the input.
This vision-only system is of interest because it is inexpensive, simple for
production, and requires no external sensing. This is in contrast to existing
systems that rely on integrated position sensors and global positioning system
(GPS) technologies. To produce our system we combined a state-of-the-art object
detection architecture, You Only Look Once (YOLO), with a reinforcement
learning (RL) architecture, Double Deep QNetworks (Double DQN). The object
detection network identifies features on the mower and passes its output to the
RL network, providing it with a low-dimensional representation that enables
rapid and robust training. Finally, the RL network learns how to navigate the
machine to the desired spot in a custom simulation environment. When tested on
mower hardware, the system is able to dock with centimeter-level accuracy from
arbitrary initial locations and orientations.
</p>
<a href="http://arxiv.org/abs/2101.06248" target="_blank">arXiv:2101.06248</a> [<a href="http://arxiv.org/pdf/2101.06248" target="_blank">pdf</a>]

<h2>EfficientLPS: Efficient LiDAR Panoptic Segmentation. (arXiv:2102.08009v2 [cs.CV] UPDATED)</h2>
<h3>Kshitij Sirohi, Rohit Mohan, Daniel B&#xfc;scher, Wolfram Burgard, Abhinav Valada</h3>
<p>Panoptic segmentation of point clouds is a crucial task that enables
autonomous vehicles to comprehend their vicinity using their highly accurate
and reliable LiDAR sensors. Existing top-down approaches tackle this problem by
either combining independent task-specific networks or translating methods from
the image domain ignoring the intricacies of LiDAR data and thus often
resulting in sub-optimal performance. In this paper, we present the novel
top-down Efficient LiDAR Panoptic Segmentation (EfficientLPS) architecture that
addresses multiple challenges in segmenting LiDAR point clouds including
distance-dependent sparsity, severe occlusions, large scale-variations, and
re-projection errors. EfficientLPS comprises of a novel shared backbone that
encodes with strengthened geometric transformation modeling capacity and
aggregates semantically rich range-aware multi-scale features. It incorporates
new scale-invariant semantic and instance segmentation heads along with the
panoptic fusion module which is supervised by our proposed panoptic periphery
loss function. Additionally, we formulate a regularized pseudo labeling
framework to further improve the performance of EfficientLPS by training on
unlabelled data. We benchmark our proposed model on two large-scale LiDAR
datasets: nuScenes, for which we also provide ground truth annotations, and
SemanticKITTI. Notably, EfficientLPS sets the new state-of-the-art on both
these datasets.
</p>
<a href="http://arxiv.org/abs/2102.08009" target="_blank">arXiv:2102.08009</a> [<a href="http://arxiv.org/pdf/2102.08009" target="_blank">pdf</a>]

<h2>The Catenary Robot: Design and Control of a Cable Propelled by Two Quadrotors. (arXiv:2102.12519v2 [cs.RO] UPDATED)</h2>
<h3>Diego S. D&#x27;antonio, Gustavo A. Cardona, David Salda&#xf1;a</h3>
<p>Transporting objects using aerial robots has been widely studied in the
literature. Still, those approaches always assume that the connection between
the quadrotor and the load is made in a previous stage. However, that previous
stage usually requires human intervention, and autonomous procedures to locate
and attach the object are not considered. Additionally, most of the approaches
assume cables as rigid links, but manipulating cables requires considering the
state when the cables are hanging. In this work, we design and control a
catenary robot. Our robot is able to transport hook-shaped objects in the
environment. The robotic system is composed of two quadrotors attached to the
two ends of a cable. By defining the catenary curve with five degrees of
freedom, position in 3-D, orientation in the z-axis, and span, we can drive the
two quadrotors to track a given trajectory. We validate our approach with
simulations and real robots. We present four different scenarios of
experiments. Our numerical solution is computationally fast and can be executed
in real-time.
</p>
<a href="http://arxiv.org/abs/2102.12519" target="_blank">arXiv:2102.12519</a> [<a href="http://arxiv.org/pdf/2102.12519" target="_blank">pdf</a>]

<h2>Towards Continual, Online, Unsupervised Depth. (arXiv:2103.00369v2 [cs.CV] UPDATED)</h2>
<h3>Muhammad Umar Karim Khan</h3>
<p>Although depth extraction with passive sensors has seen remarkable
improvement with deep learning, these approaches may fail to obtain correct
depth if they are exposed to environments not observed during training. Online
adaptation, where the neural network trains while deployed, with unsupervised
learning provides a convenient solution. However, online adaptation causes a
neural network to forget the past. Thus, past training is wasted and the
network is not able to provide good results if it observes past scenes. This
work deals with practical online-adaptation where the input is online and
temporally-correlated, and training is completely unsupervised. Regularization
and replay-based methods without task boundaries are proposed to avoid
catastrophic forgetting while adapting to online data. Experiments are
performed on different datasets with both structure-from-motion and stereo.
Results of forgetting as well as adaptation are provided, which are superior to
recent methods. The proposed approach is more inline with the artificial
general intelligence paradigm as the neural network learns the scene where it
is deployed without any supervision (target labels and tasks) and without
forgetting about the past. Code is available at github.com/umarKarim/cou_stereo
and github.com/umarKarim/cou_sfm.
</p>
<a href="http://arxiv.org/abs/2103.00369" target="_blank">arXiv:2103.00369</a> [<a href="http://arxiv.org/pdf/2103.00369" target="_blank">pdf</a>]

<h2>Am I a Real or Fake Celebrity? Measuring Commercial Face Recognition Web APIs under Deepfake Impersonation Attack. (arXiv:2103.00847v2 [cs.CV] UPDATED)</h2>
<h3>Shahroz Tariq, Sowon Jeon, Simon S. Woo</h3>
<p>Recently, significant advancements have been made in face recognition
technologies using Deep Neural Networks. As a result, companies such as
Microsoft, Amazon, and Naver offer highly accurate commercial face recognition
web services for diverse applications to meet the end-user needs. Naturally,
however, such technologies are threatened persistently, as virtually any
individual can quickly implement impersonation attacks. In particular, these
attacks can be a significant threat for authentication and identification
services, which heavily rely on their underlying face recognition technologies'
accuracy and robustness. Despite its gravity, the issue regarding deepfake
abuse using commercial web APIs and their robustness has not yet been
thoroughly investigated. This work provides a measurement study on the
robustness of black-box commercial face recognition APIs against Deepfake
Impersonation (DI) attacks using celebrity recognition APIs as an example case
study. We use five deepfake datasets, two of which are created by us and
planned to be released. More specifically, we measure attack performance based
on two scenarios (targeted and non-targeted) and further analyze the differing
system behaviors using fidelity, confidence, and similarity metrics.
Accordingly, we demonstrate how vulnerable face recognition technologies from
popular companies are to DI attack, achieving maximum success rates of 78.0%
and 99.9% for targeted (i.e., precise match) and non-targeted (i.e., match with
any celebrity) attacks, respectively. Moreover, we propose practical defense
strategies to mitigate DI attacks, reducing the attack success rates to as low
as 0% and 0.02% for targeted and non-targeted attacks, respectively.
</p>
<a href="http://arxiv.org/abs/2103.00847" target="_blank">arXiv:2103.00847</a> [<a href="http://arxiv.org/pdf/2103.00847" target="_blank">pdf</a>]

<h2>OMNet: Learning Overlapping Mask for Partial-to-Partial Point Cloud Registration. (arXiv:2103.00937v2 [cs.CV] UPDATED)</h2>
<h3>Hao Xu, Shuaicheng Liu, Guangfu Wang, Guanghui Liu, Bing Zeng</h3>
<p>Point cloud registration is a key task in many computational fields. Previous
correspondence matching based methods require the point clouds to have
distinctive geometric structures to fit a 3D rigid transformation according to
point-wise sparse feature matches. However, the accuracy of transformation
heavily relies on the quality of extracted features, which are prone to errors
with respect partiality and noise of the inputs. In addition, they can not
utilize the geometric knowledge of all regions. On the other hand, previous
global feature based deep learning approaches can utilize the entire point
cloud for the registration, however they ignore the negative effect of
non-overlapping points when aggregating global feature from point-wise
features. In this paper, we present OMNet, a global feature based iterative
network for partial-to-partial point cloud registration. We learn masks in a
coarse-to-fine manner to reject non-overlapping regions, which converting the
partial-to-partial registration to the registration of the same shapes.
Moreover, the data used in previous works are only sampled once from CAD models
for each object, resulting the same point cloud for the source and the
reference. We propose a more practical manner for data generation, where a CAD
model is sampled twice for the source and the reference point clouds, avoiding
over-fitting issues that commonly exist previously. Experimental results show
that our approach achieves state-of-the-art performance compared to traditional
and deep learning methods.
</p>
<a href="http://arxiv.org/abs/2103.00937" target="_blank">arXiv:2103.00937</a> [<a href="http://arxiv.org/pdf/2103.00937" target="_blank">pdf</a>]

<h2>Adversarial Reciprocal Points Learning for Open Set Recognition. (arXiv:2103.00953v2 [cs.CV] UPDATED)</h2>
<h3>Guangyao Chen, Peixi Peng, Xiangqian Wang, Yonghong Tian</h3>
<p>Open set recognition (OSR), aiming to simultaneously classify the seen
classes and identify the unseen classes as 'unknown', is essential for reliable
machine learning.The key challenge of OSR is how to reduce the empirical
classification risk on the labeled known data and the open space risk on the
potential unknown data simultaneously. To handle the challenge, we formulate
the open space risk problem from the perspective of multi-class integration,
and model the unexploited extra-class space with a novel concept Reciprocal
Point. Follow this, a novel learning framework, termed Adversarial Reciprocal
Point Learning (ARPL), is proposed to minimize the overlap of known
distribution and unknown distributions without loss of known classification
accuracy. Specifically, each reciprocal point is learned by the extra-class
space with the corresponding known category, and the confrontation among
multiple known categories are employed to reduce the empirical classification
risk. Then, an adversarial margin constraint is proposed to reduce the open
space risk by limiting the latent open space constructed by reciprocal points.
To further estimate the unknown distribution from open space, an instantiated
adversarial enhancement method is designed to generate diverse and confusing
training samples, based on the adversarial mechanism between the reciprocal
points and known classes. This can effectively enhance the model
distinguishability to the unknown classes. Extensive experimental results on
various benchmark datasets indicate that the proposed method is significantly
superior to other existing approaches and achieves state-of-the-art
performance.
</p>
<a href="http://arxiv.org/abs/2103.00953" target="_blank">arXiv:2103.00953</a> [<a href="http://arxiv.org/pdf/2103.00953" target="_blank">pdf</a>]

<h2>Generative Adversarial Transformers. (arXiv:2103.01209v2 [cs.CV] UPDATED)</h2>
<h3>Drew A. Hudson, C. Lawrence Zitnick</h3>
<p>We introduce the GANsformer, a novel and efficient type of transformer, and
explore it for the task of visual generative modeling. The network employs a
bipartite structure that enables long-range interactions across the image,
while maintaining computation of linearly efficiency, that can readily scale to
high-resolution synthesis. It iteratively propagates information from a set of
latent variables to the evolving visual features and vice versa, to support the
refinement of each in light of the other and encourage the emergence of
compositional representations of objects and scenes. In contrast to the classic
transformer architecture, it utilizes multiplicative integration that allows
flexible region-based modulation, and can thus be seen as a generalization of
the successful StyleGAN network. We demonstrate the model's strength and
robustness through a careful evaluation over a range of datasets, from
simulated multi-object environments to rich real-world indoor and outdoor
scenes, showing it achieves state-of-the-art results in terms of image quality
and diversity, while enjoying fast learning and better data-efficiency. Further
qualitative and quantitative experiments offer us an insight into the model's
inner workings, revealing improved interpretability and stronger
disentanglement, and illustrating the benefits and efficacy of our approach. An
implementation of the model is available at
https://github.com/dorarad/gansformer.
</p>
<a href="http://arxiv.org/abs/2103.01209" target="_blank">arXiv:2103.01209</a> [<a href="http://arxiv.org/pdf/2103.01209" target="_blank">pdf</a>]

<h2>A Deep Genetic Programming based Methodology for Art Media Classification Robust to Adversarial Perturbations. (arXiv:2010.01238v1 [cs.CV] CROSS LISTED)</h2>
<h3>Gustavo Olague, Gerardo Ibarra-Vazquez, Mariana Chan-Ley, Cesar Puente, Carlos Soubervielle-Montalvo, Axel Martinez</h3>
<p>Art Media Classification problem is a current research area that has
attracted attention due to the complex extraction and analysis of features of
high-value art pieces. The perception of the attributes can not be subjective,
as humans sometimes follow a biased interpretation of artworks while ensuring
automated observation's trustworthiness. Machine Learning has outperformed many
areas through its learning process of artificial feature extraction from images
instead of designing handcrafted feature detectors. However, a major concern
related to its reliability has brought attention because, with small
perturbations made intentionally in the input image (adversarial attack), its
prediction can be completely changed. In this manner, we foresee two ways of
approaching the situation: (1) solve the problem of adversarial attacks in
current neural networks methodologies, or (2) propose a different approach that
can challenge deep learning without the effects of adversarial attacks. The
first one has not been solved yet, and adversarial attacks have become even
more complex to defend. Therefore, this work presents a Deep Genetic
Programming method, called Brain Programming, that competes with deep learning
and studies the transferability of adversarial attacks using two artworks
databases made by art experts. The results show that the Brain Programming
method preserves its performance in comparison with AlexNet, making it robust
to these perturbations and competing to the performance of Deep Learning.
</p>
<a href="http://arxiv.org/abs/2010.01238" target="_blank">arXiv:2010.01238</a> [<a href="http://arxiv.org/pdf/2010.01238" target="_blank">pdf</a>]

