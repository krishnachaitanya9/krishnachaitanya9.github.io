---
title: Latest Deep Learning Papers
date: 2021-03-10 21:00:01 +0800
featured-img: DL-logo
categories: [Deep_Learning]
tags: [Deep_Learning]
mathjax: Yes
---

<h1>Your arXiv Feed (106 Articles)</h1>
<h2>Core Challenges of Social Robot Navigation: A Survey. (arXiv:2103.05668v1 [cs.RO])</h2>
<h3>Christoforos Mavrogiannis, Francesca Baldini, Allan Wang, Dapeng Zhao, Aaron Steinfeld, Pete Trautman, Jean Oh</h3>
<p>Robot navigation in crowded public spaces is a complex task that requires
addressing a variety of engineering and human factors challenges. These
challenges have motivated a great amount of research resulting in important
developments for the fields of robotics and human-robot interaction over the
past three decades. Despite the significant progress and the massive recent
interest, we observe a number of significant remaining challenges that prohibit
the seamless deployment of autonomous robots in public pedestrian environments.
In this survey article, we organize existing challenges into a set of
categories related to broader open problems in motion planning, behavior
design, and evaluation methodologies. Within these categories, we review past
work, and offer directions for future research. Our work builds upon and
extends earlier survey efforts by a) taking a critical perspective and
diagnosing fundamental limitations of adopted practices in the field and b)
offering constructive feedback and ideas that we aspire will drive research in
the field over the coming decade.
</p>
<a href="http://arxiv.org/abs/2103.05668" target="_blank">arXiv:2103.05668</a> [<a href="http://arxiv.org/pdf/2103.05668" target="_blank">pdf</a>]

<h2>Self-Supervision by Prediction for Object Discovery in Videos. (arXiv:2103.05669v1 [cs.CV])</h2>
<h3>Beril Besbinar, Pascal Frossard</h3>
<p>Despite their irresistible success, deep learning algorithms still heavily
rely on annotated data. On the other hand, unsupervised settings pose many
challenges, especially about determining the right inductive bias in diverse
scenarios. One scalable solution is to make the model generate the supervision
for itself by leveraging some part of the input data, which is known as
self-supervised learning. In this paper, we use the prediction task as
self-supervision and build a novel object-centric model for image sequence
representation. In addition to disentangling the notion of objects and the
motion dynamics, our compositional structure explicitly handles occlusion and
inpaints inferred objects and background for the composition of the predicted
frame. With the aid of auxiliary loss functions that promote spatially and
temporally consistent object representations, our self-supervised framework can
be trained without the help of any manual annotation or pretrained network.
Initial experiments confirm that the proposed pipeline is a promising step
towards object-centric video prediction.
</p>
<a href="http://arxiv.org/abs/2103.05669" target="_blank">arXiv:2103.05669</a> [<a href="http://arxiv.org/pdf/2103.05669" target="_blank">pdf</a>]

<h2>Entropy-Guided Control Improvisation. (arXiv:2103.05672v1 [cs.RO])</h2>
<h3>Marcell Vazquez-Chanlatte, Sebastian Junges, Daniel J. Fremont, Sanjit Seshia</h3>
<p>High level declarative constraints provide a powerful (and popular) way to
define and construct control policies; however, most synthesis algorithms do
not support specifying the degree of randomness (unpredictability) of the
resulting controller. In many contexts, e.g., patrolling, testing, behavior
prediction, and planning on idealized models, predictable or biased controllers
are undesirable. To address these concerns, we introduce the \emph{Entropic
Reactive Control Improvisation} (ERCI) framework and algorithm that supports
synthesizing control policies for stochastic games that are declaratively
specified by (i) a \emph{hard constraint} specifying what must occur (ii) a
\emph{soft constraint} specifying what typically occurs, and (iii) a
\emph{randomization constraint} specifying the unpredictability and variety of
the controller, as quantified using causal entropy. This framework, which
extends the state-of-the-art by supporting arbitrary combinations of
adversarial and probabilistic uncertainty in the environment, enables a
flexible modeling formalism which we argue, theoretically and empirically,
remains tractable.
</p>
<a href="http://arxiv.org/abs/2103.05672" target="_blank">arXiv:2103.05672</a> [<a href="http://arxiv.org/pdf/2103.05672" target="_blank">pdf</a>]

<h2>Formulating Intuitive Stack-of-Tasks with Visuo-Tactile Perception for Collaborative Human-Robot Fine Manipulation. (arXiv:2103.05676v1 [cs.RO])</h2>
<h3>Sunny Katyara, Fanny Ficuciello, Tao Teng, Fei Chen, Bruno Siciliano, Darwin G. Caldwell</h3>
<p>Enabling robots to work in close proximity with humans necessitates to employ
not only multi-sensory information for coordinated and autonomous interactions
but also a control framework that ensures adaptive and flexible collaborative
behavior. Such a control framework needs to integrate accuracy and
repeatability of robots with cognitive ability and adaptability of humans for
co-manipulation. In this regard, an intuitive stack of tasks (iSOT) formulation
is proposed, that defines the robots actions based on human ergonomics and task
progress. The framework is augmented with visuo-tactile perception for flexible
interaction and autonomous adaption. The visual information using depth
cameras, monitors and estimates the object pose and human arm gesture while the
tactile feedback provides exploration skills for maintaining the desired
contact to avoid slippage. Experiments conducted on robot system with human
partnership for assembly and disassembly tasks confirm the effectiveness and
usability of proposed framework.
</p>
<a href="http://arxiv.org/abs/2103.05676" target="_blank">arXiv:2103.05676</a> [<a href="http://arxiv.org/pdf/2103.05676" target="_blank">pdf</a>]

<h2>SMIL: Multimodal Learning with Severely Missing Modality. (arXiv:2103.05677v1 [cs.CV])</h2>
<h3>Mengmeng Ma, Jian Ren, Long Zhao, Sergey Tulyakov, Cathy Wu, Xi Peng</h3>
<p>A common assumption in multimodal learning is the completeness of training
data, i.e., full modalities are available in all training examples. Although
there exists research endeavor in developing novel methods to tackle the
incompleteness of testing data, e.g., modalities are partially missing in
testing examples, few of them can handle incomplete training modalities. The
problem becomes even more challenging if considering the case of severely
missing, e.g., 90% training examples may have incomplete modalities. For the
first time in the literature, this paper formally studies multimodal learning
with missing modality in terms of flexibility (missing modalities in training,
testing, or both) and efficiency (most training data have incomplete modality).
Technically, we propose a new method named SMIL that leverages Bayesian
meta-learning in uniformly achieving both objectives. To validate our idea, we
conduct a series of experiments on three popular benchmarks: MM-IMDb, CMU-MOSI,
and avMNIST. The results prove the state-of-the-art performance of SMIL over
existing methods and generative baselines including autoencoders and generative
adversarial networks. Our code is available at
https://github.com/mengmenm/SMIL.
</p>
<a href="http://arxiv.org/abs/2103.05677" target="_blank">arXiv:2103.05677</a> [<a href="http://arxiv.org/pdf/2103.05677" target="_blank">pdf</a>]

<h2>Capturing Omni-Range Context for Omnidirectional Segmentation. (arXiv:2103.05687v1 [cs.CV])</h2>
<h3>Kailun Yang, Jiaming Zhang, Simon Rei&#xdf;, Xinxin Hu, Rainer Stiefelhagen</h3>
<p>Convolutional Networks (ConvNets) excel at semantic segmentation and have
become a vital component for perception in autonomous driving. Enabling an
all-encompassing view of street-scenes, omnidirectional cameras present
themselves as a perfect fit in such systems. Most segmentation models for
parsing urban environments operate on common, narrow Field of View (FoV)
images. Transferring these models from the domain they were designed for to
360-degree perception, their performance drops dramatically, e.g., by an
absolute 30.0% (mIoU) on established test-beds. To bridge the gap in terms of
FoV and structural distribution between the imaging domains, we introduce
Efficient Concurrent Attention Networks (ECANets), directly capturing the
inherent long-range dependencies in omnidirectional imagery. In addition to the
learned attention-based contextual priors that can stretch across 360-degree
images, we upgrade model training by leveraging multi-source and
omni-supervised learning, taking advantage of both: Densely labeled and
unlabeled data originating from multiple datasets. To foster progress in
panoramic image segmentation, we put forward and extensively evaluate models on
Wild PAnoramic Semantic Segmentation (WildPASS), a dataset designed to capture
diverse scenes from all around the globe. Our novel model, training regimen and
multi-source prediction fusion elevate the performance (mIoU) to new
state-of-the-art results on the public PASS (60.2%) and the fresh WildPASS
(69.0%) benchmarks.
</p>
<a href="http://arxiv.org/abs/2103.05687" target="_blank">arXiv:2103.05687</a> [<a href="http://arxiv.org/pdf/2103.05687" target="_blank">pdf</a>]

<h2>Multitask 3D CBCT-to-CT Translation and Organs-at-Risk Segmentation Using Physics-Based Data Augmentation. (arXiv:2103.05690v1 [cs.CV])</h2>
<h3>Navdeep Dahiya, Sadegh R Alam, Pengpeng Zhang, Si-Yuan Zhang, Anthony Yezzi, Saad Nadeem</h3>
<p>Purpose: In current clinical practice, noisy and artifact-ridden weekly
cone-beam computed tomography (CBCT) images are only used for patient setup
during radiotherapy. Treatment planning is done once at the beginning of the
treatment using high-quality planning CT (pCT) images and manual contours for
organs-at-risk (OARs) structures. If the quality of the weekly CBCT images can
be improved while simultaneously segmenting OAR structures, this can provide
critical information for adapting radiotherapy mid-treatment as well as for
deriving biomarkers for treatment response. Methods: Using a novel
physics-based data augmentation strategy, we synthesize a large dataset of
perfectly/inherently registered planning CT and synthetic-CBCT pairs for
locally advanced lung cancer patient cohort, which are then used in a multitask
3D deep learning framework to simultaneously segment and translate real weekly
CBCT images to high-quality planning CT-like images. Results: We compared the
synthetic CT and OAR segmentations generated by the model to real planning CT
and manual OAR segmentations and showed promising results. The real week 1
(baseline) CBCT images which had an average MAE of 162.77 HU compared to pCT
images are translated to synthetic CT images that exhibit a drastically
improved average MAE of 29.31 HU and average structural similarity of 92% with
the pCT images. The average DICE scores of the 3D organs-at-risk segmentations
are: lungs 0.96, heart 0.88, spinal cord 0.83 and esophagus 0.66. Conclusions:
We demonstrate an approach to translate artifact-ridden CBCT images to high
quality synthetic CT images while simultaneously generating good quality
segmentation masks for different organs-at-risk. This approach could allow
clinicians to adjust treatment plans using only the routine low-quality CBCT
images, potentially improving patient outcomes.
</p>
<a href="http://arxiv.org/abs/2103.05690" target="_blank">arXiv:2103.05690</a> [<a href="http://arxiv.org/pdf/2103.05690" target="_blank">pdf</a>]

<h2>Simple Flagellated Soft Robot for Locomotion near Air-Fluid Interface. (arXiv:2103.05712v1 [cs.RO])</h2>
<h3>Yayun Du, Andrew Miller, Mohammad Khalid Jawed</h3>
<p>A wide range of microorganisms, e.g. bacteria, propel themselves by rotation
of soft helical tails, also known as flagella. Due to the small size of these
organisms, viscous forces overwhelm inertial effects and the flow is at low
Reynolds number. In this fluid-structure problem, a competition between elastic
forces and hydrodynamic (viscous) forces leads to a net propulsive force
forward. A thorough understanding of this highly coupled fluid-structure
interaction problem can not only help us better understand biological
propulsion but also help us design bio-inspired functional robots with
applications in oil spill cleanup, water quality monitoring, and infrastructure
inspection. Here, we introduce arguably the simplest soft robot with a single
binary control signal, which is capable of moving along an arbitrary 2D
trajectory near air-fluid interface and at the interface between two fluids.
The robot exploits the variation in viscosity to move along the prescribed
trajectory. Our analysis of this newly introduced soft robot consists of three
main components. First, we fabricate this simple robot and use it as an
experimental testbed. Second, a discrete differential geometry-based modeling
framework is used for simulation of the robot. Upon validation of the
simulation tool, the third part of this study employs the simulations to
develop a control scheme with a single binary input to make the robot follow
any prescribed path.
</p>
<a href="http://arxiv.org/abs/2103.05712" target="_blank">arXiv:2103.05712</a> [<a href="http://arxiv.org/pdf/2103.05712" target="_blank">pdf</a>]

<h2>Pixel-wise Distance Regression for Glacier Calving Front Detection and Segmentation. (arXiv:2103.05715v1 [cs.CV])</h2>
<h3>Amirabbas Davari, Christoph Baller, Thorsten Seehaus, Matthias Braun, Andreas Maier, Vincent Christlein</h3>
<p>Glacier calving front position (CFP) is an important glaciological variable.
Traditionally, delineating the CFPs has been carried out manually, which was
subjective, tedious and expensive. Automating this process is crucial for
continuously monitoring the evolution and status of glaciers. Recently, deep
learning approaches have been investigated for this application. However, the
current methods get challenged by a severe class-imbalance problem. In this
work, we propose to mitigate the class-imbalance between the calving front
class and the non-calving front class by reformulating the segmentation problem
into a pixel-wise regression task. A Convolutional Neural Network gets
optimized to predict the distance values to the glacier front for each pixel in
the image. The resulting distance map localizes the CFP and is further
post-processed to extract the calving front line. We propose three
post-processing methods, one method based on statistical thresholding, a second
method based on conditional random fields (CRF), and finally the use of a
second U-Net. The experimental results confirm that our approach significantly
outperforms the state-of-the-art methods and produces accurate delineation. The
Second U-Net obtains the best performance results, resulting in an average
improvement of about 21% dice coefficient enhancement.
</p>
<a href="http://arxiv.org/abs/2103.05715" target="_blank">arXiv:2103.05715</a> [<a href="http://arxiv.org/pdf/2103.05715" target="_blank">pdf</a>]

<h2>A Multi-resolution Approach to Expression Recognition in the Wild. (arXiv:2103.05723v1 [cs.CV])</h2>
<h3>Fabio Valerio Massoli, Donato Cafarelli, Giuseppe Amato, Fabrizio Falchi</h3>
<p>Facial expressions play a fundamental role in human communication. Indeed,
they typically reveal the real emotional status of people beyond the spoken
language. Moreover, the comprehension of human affect based on visual patterns
is a key ingredient for any human-machine interaction system and, for such
reasons, the task of Facial Expression Recognition (FER) draws both scientific
and industrial interest. In the recent years, Deep Learning techniques reached
very high performance on FER by exploiting different architectures and learning
paradigms. In such a context, we propose a multi-resolution approach to solve
the FER task. We ground our intuition on the observation that often faces
images are acquired at different resolutions. Thus, directly considering such
property while training a model can help achieve higher performance on
recognizing facial expressions. To our aim, we use a ResNet-like architecture,
equipped with Squeeze-and-Excitation blocks, trained on the Affect-in-the-Wild
2 dataset. Not being available a test set, we conduct tests and models
selection by employing the validation set only on which we achieve more than
90\% accuracy on classifying the seven expressions that the dataset comprises.
</p>
<a href="http://arxiv.org/abs/2103.05723" target="_blank">arXiv:2103.05723</a> [<a href="http://arxiv.org/pdf/2103.05723" target="_blank">pdf</a>]

<h2>Structural Connectome Atlas Construction in the Space of Riemannian Metrics. (arXiv:2103.05730v1 [cs.CV])</h2>
<h3>Kristen M. Campbell (1), Haocheng Dai (1), Zhe Su (2), Martin Bauer (3), P. Thomas Fletcher (4), Sarang C. Joshi (1 and 5) ((1) Scientific Computing and Imaging Institute, University of Utah, (2) Department of Neurology, University of California Los Angeles, (3) Department of Mathematics, Florida State University, (4) Electrical &amp; Computer Engineering, University of Virginia, (5) Department of Bioengineering, University of Utah)</h3>
<p>The structural connectome is often represented by fiber bundles generated
from various types of tractography. We propose a method of analyzing
connectomes by representing them as a Riemannian metric, thereby viewing them
as points in an infinite-dimensional manifold. After equipping this space with
a natural metric structure, the Ebin metric, we apply object-oriented
statistical analysis to define an atlas as the Fr\'echet mean of a population
of Riemannian metrics. We demonstrate connectome registration and atlas
formation using connectomes derived from diffusion tensors estimated from a
subset of subjects from the Human Connectome Project.
</p>
<a href="http://arxiv.org/abs/2103.05730" target="_blank">arXiv:2103.05730</a> [<a href="http://arxiv.org/pdf/2103.05730" target="_blank">pdf</a>]

<h2>Analyzing Human Models that Adapt Online. (arXiv:2103.05746v1 [cs.RO])</h2>
<h3>Andrea Bajcsy, Anand Siththaranjan, Claire J. Tomlin, Anca D. Dragan</h3>
<p>Predictive human models often need to adapt their parameters online from
human data. This raises previously ignored safety-related questions for robots
relying on these models such as what the model could learn online and how
quickly could it learn it. For instance, when will the robot have a confident
estimate in a nearby human's goal? Or, what parameter initializations guarantee
that the robot can learn the human's preferences in a finite number of
observations? To answer such analysis questions, our key idea is to model the
robot's learning algorithm as a dynamical system where the state is the current
model parameter estimate and the control is the human data the robot observes.
This enables us to leverage tools from reachability analysis and optimal
control to compute the set of hypotheses the robot could learn in finite time,
as well as the worst and best-case time it takes to learn them. We demonstrate
the utility of our analysis tool in four human-robot domains, including
autonomous driving and indoor navigation.
</p>
<a href="http://arxiv.org/abs/2103.05746" target="_blank">arXiv:2103.05746</a> [<a href="http://arxiv.org/pdf/2103.05746" target="_blank">pdf</a>]

<h2>Cybersecurity in Robotics: Challenges, Quantitative Modeling, and Practice. (arXiv:2103.05789v1 [cs.RO])</h2>
<h3>Quanyan Zhu, Stefan Rass, Bernhard Dieber, Victor Mayoral Vilches</h3>
<p>Robotics is becoming more and more ubiquitous, but the pressure to bring
systems to market occasionally goes at the cost of neglecting security
mechanisms during the development, deployment or while in production. As a
result, contemporary robotic systems are vulnerable to diverse attack patterns,
and an a posteriori hardening is at least challenging, if not impossible at
all. This work aims to stipulate the inclusion of security in robotics from the
earliest design phases onward and with a special focus on the cost-bene?t
tradeoff? that can otherwise be an inhibitor for the fast development of
aff?ordable systems. We advocate quantitative methods of security management
and -design, covering vulnerability scoring systems tailored to robotic
systems, and accounting for the highly distributed nature of robots as an
interplay of potentially very many components. A powerful quantitative approach
to model-based security is off?ered by game theory, providing a rich spectrum
of techniques to optimize security against various kinds of attacks. Such a
multiperspective view on security is necessary to address the heterogeneity and
complexity of robotic systems. This book is intended as an accessible starter
for the theoretician and practitioner working in the ?eld.
</p>
<a href="http://arxiv.org/abs/2103.05789" target="_blank">arXiv:2103.05789</a> [<a href="http://arxiv.org/pdf/2103.05789" target="_blank">pdf</a>]

<h2>Robust Collision-free Lightweight Aerial Autonomy for Unknown Area Exploration. (arXiv:2103.05798v1 [cs.RO])</h2>
<h3>Sunggoo Jung, Hanseob Lee, David Hyunchul Shim, Ali-akbar Agha-mohammadi</h3>
<p>Collision-free path planning is an important requirement for autonomous
exploration in unknown environments, especially when operating in confined
spaces or near obstacles. This paper presents an autonomous exploration
technique using a small drone. A local end-point selection method is designed
using LiDAR range measurement and then generates the path from the current
position to the selected end-point. Specifically, the generated path shows the
consistent collision-free path in real-time by adopting the euclidean signed
distance field-based grid search method. Simulations consistently show the
safety, and reliability of the proposed path-planning method. Real-world
experiments are conducted in three different mines, demonstrating successful
autonomous exploration flight in environments with various structural
conditions. All results indicate the high capability of the proposed flight
autonomy framework for lightweight aerial-robot systems. Especially, our drone
was able to perform an autonomous mission during our entry at the Tunnel
Circuit competition (Phase 1) of the DARPA Subterranean Challenge.
</p>
<a href="http://arxiv.org/abs/2103.05798" target="_blank">arXiv:2103.05798</a> [<a href="http://arxiv.org/pdf/2103.05798" target="_blank">pdf</a>]

<h2>Active Exploration and Mapping via Iterative Covariance Regulation over Continuous $SE(3)$ Trajectories. (arXiv:2103.05819v1 [cs.RO])</h2>
<h3>Shumon Koga, Arash Asgharivaskasi, Nikolay Atanasov</h3>
<p>This paper develops \emph{iterative Covariance Regulation} (iCR), a novel
method for active exploration and mapping for a mobile robot equipped with
on-board sensors. The problem is posed as optimal control over the $SE(3)$ pose
kinematics of the robot to minimize the differential entropy of the map
conditioned the potential sensor observations. We introduce a differentiable
field of view formulation, and derive iCR via the gradient descent method to
iteratively update an open-loop control sequence in continuous space so that
the covariance of the map estimate is minimized. We demonstrate autonomous
exploration and uncertainty reduction in simulated occupancy grid environments.
</p>
<a href="http://arxiv.org/abs/2103.05819" target="_blank">arXiv:2103.05819</a> [<a href="http://arxiv.org/pdf/2103.05819" target="_blank">pdf</a>]

<h2>Learning to compose 6-DoF omnidirectional videos using multi-sphere images. (arXiv:2103.05842v1 [cs.CV])</h2>
<h3>Jisheng Li, Yuze He, Yubin Hu, Yuxing Han, Jiangtao Wen</h3>
<p>Omnidirectional video is an essential component of Virtual Reality. Although
various methods have been proposed to generate content that can be viewed with
six degrees of freedom (6-DoF), existing systems usually involve complex depth
estimation, image in-painting or stitching pre-processing. In this paper, we
propose a system that uses a 3D ConvNet to generate a multi-sphere images (MSI)
representation that can be experienced in 6-DoF VR. The system utilizes
conventional omnidirectional VR camera footage directly without the need for a
depth map or segmentation mask, thereby significantly simplifying the overall
complexity of the 6-DoF omnidirectional video composition. By using a newly
designed weighted sphere sweep volume (WSSV) fusing technique, our approach is
compatible with most panoramic VR camera setups. A ground truth generation
approach for high-quality artifact-free 6-DoF contents is proposed and can be
used by the research and development community for 6-DoF content generation.
</p>
<a href="http://arxiv.org/abs/2103.05842" target="_blank">arXiv:2103.05842</a> [<a href="http://arxiv.org/pdf/2103.05842" target="_blank">pdf</a>]

<h2>Incorporating Orientations into End-to-end Driving Model for Steering Control. (arXiv:2103.05846v1 [cs.RO])</h2>
<h3>Peng Wan, Zhenbo Song, Jianfeng Lu</h3>
<p>In this paper, we present a novel end-to-end deep neural network model for
autonomous driving that takes monocular image sequence as input, and directly
generates the steering control angle. Firstly, we model the end-to-end driving
problem as a local path planning process. Inspired by the environmental
representation in the classical planning algorithms(i.e. the beam curvature
method), pixel-wise orientations are fed into the network to learn
direction-aware features. Next, to handle the imbalanced distribution of
steering values in training datasets, we propose an improvement on a
cost-sensitive loss function named SteeringLoss2. Besides, we also present a
new end-to-end driving dataset, which provides corresponding LiDAR and image
sequences, as well as standard driving behaviors. Our dataset includes multiple
driving scenarios, such as urban, country, and off-road. Numerous experiments
are conducted on both public available LiVi-Set and our own dataset, and the
results show that the model using our proposed methods can predict steering
angle accurately.
</p>
<a href="http://arxiv.org/abs/2103.05846" target="_blank">arXiv:2103.05846</a> [<a href="http://arxiv.org/pdf/2103.05846" target="_blank">pdf</a>]

<h2>Novel tile segmentation scheme for omnidirectional video. (arXiv:2103.05858v1 [cs.CV])</h2>
<h3>Jisheng Li, Ziyu Wen, Sihan Li, Yikai Zhao, Bichuan Guo, Jiangtao Wen</h3>
<p>Regular omnidirectional video encoding technics use map projection to flatten
a scene from a spherical shape into one or several 2D shapes. Common projection
methods including equirectangular and cubic projection have varying levels of
interpolation that create a large number of non-information-carrying pixels
that lead to wasted bitrate. In this paper, we propose a tile based
omnidirectional video segmentation scheme which can save up to 28% of pixel
area and 20% of BD-rate averagely compared to the traditional equirectangular
projection based approach.
</p>
<a href="http://arxiv.org/abs/2103.05858" target="_blank">arXiv:2103.05858</a> [<a href="http://arxiv.org/pdf/2103.05858" target="_blank">pdf</a>]

<h2>Manifold Regularized Dynamic Network Pruning. (arXiv:2103.05861v1 [cs.CV])</h2>
<h3>Yehui Tang, Yunhe Wang, Yixing Xu, Yiping Deng, Chao Xu, Dacheng Tao, Chang Xu</h3>
<p>Neural network pruning is an essential approach for reducing the
computational complexity of deep models so that they can be well deployed on
resource-limited devices. Compared with conventional methods, the recently
developed dynamic pruning methods determine redundant filters variant to each
input instance which achieves higher acceleration. Most of the existing methods
discover effective sub-networks for each instance independently and do not
utilize the relationship between different inputs. To maximally excavate
redundancy in the given network architecture, this paper proposes a new
paradigm that dynamically removes redundant filters by embedding the manifold
information of all instances into the space of pruned networks (dubbed as
ManiDP). We first investigate the recognition complexity and feature similarity
between images in the training set. Then, the manifold relationship between
instances and the pruned sub-networks will be aligned in the training
procedure. The effectiveness of the proposed method is verified on several
benchmarks, which shows better performance in terms of both accuracy and
computational cost compared to the state-of-the-art methods. For example, our
method can reduce 55.3% FLOPs of ResNet-34 with only 0.57% top-1 accuracy
degradation on ImageNet.
</p>
<a href="http://arxiv.org/abs/2103.05861" target="_blank">arXiv:2103.05861</a> [<a href="http://arxiv.org/pdf/2103.05861" target="_blank">pdf</a>]

<h2>AutoDO: Robust AutoAugment for Biased Data with Label Noise via Scalable Probabilistic Implicit Differentiation. (arXiv:2103.05863v1 [cs.CV])</h2>
<h3>Denis Gudovskiy, Luca Rigazio, Shun Ishizaka, Kazuki Kozuka, Sotaro Tsukizawa</h3>
<p>AutoAugment has sparked an interest in automated augmentation methods for
deep learning models. These methods estimate image transformation policies for
train data that improve generalization to test data. While recent papers
evolved in the direction of decreasing policy search complexity, we show that
those methods are not robust when applied to biased and noisy data. To overcome
these limitations, we reformulate AutoAugment as a generalized automated
dataset optimization (AutoDO) task that minimizes the distribution shift
between test data and distorted train dataset. In our AutoDO model, we
explicitly estimate a set of per-point hyperparameters to flexibly change
distribution of train data. In particular, we include hyperparameters for
augmentation, loss weights, and soft-labels that are jointly estimated using
implicit differentiation. We develop a theoretical probabilistic interpretation
of this framework using Fisher information and show that its complexity scales
linearly with the dataset size. Our experiments on SVHN, CIFAR-10/100, and
ImageNet classification show up to 9.3% improvement for biased datasets with
label noise compared to prior methods and, importantly, up to 36.6% gain for
underrepresented SVHN classes.
</p>
<a href="http://arxiv.org/abs/2103.05863" target="_blank">arXiv:2103.05863</a> [<a href="http://arxiv.org/pdf/2103.05863" target="_blank">pdf</a>]

<h2>Autonomous Flights in Dynamic Environments with Onboard Vision. (arXiv:2103.05870v1 [cs.RO])</h2>
<h3>Yingjian Wang, Jialin Ji, Qianhao Wang, Chao Xu, Fei Gao</h3>
<p>In this paper, we introduce a complete system for autonomous flight of
quadrotors in dynamic environments with onboard sensing. Extended from existing
work, we develop an occlusion-aware dynamic perception method based on depth
images, which classifies obstacles as dynamic and static. For representing
generic dynamic environment, we model dynamic objects with moving ellipsoids
and fuse static ones into an occupancy grid map. To achieve dynamic avoidance,
we design a planning method composed of modified kinodynamic path searching and
gradient-based optimization. The method leverages manually constructed
gradients without maintaining a signed distance field (SDF), making the
planning procedure finished in milliseconds. We integrate the above methods
into a customized quadrotor system and thoroughly test it in realworld
experiments, verifying its effective collision avoidance in dynamic
environments.
</p>
<a href="http://arxiv.org/abs/2103.05870" target="_blank">arXiv:2103.05870</a> [<a href="http://arxiv.org/pdf/2103.05870" target="_blank">pdf</a>]

<h2>Tuna Nutriment Tracking using Trajectory Mapping in Application to Aquaculture Fish Tank. (arXiv:2103.05886v1 [cs.CV])</h2>
<h3>Hilmil Pradana, Keiichi Horio</h3>
<p>The cost of fish feeding is usually around 40 percent of total production
cost. Estimating a state of fishes in a tank and adjusting an amount of
nutriments play an important role to manage cost of fish feeding system. Our
approach is based on tracking nutriments on videos collected from an active
aquaculture fish farm. Tracking approach is applied to acknowledge movement of
nutriment to understand more about the fish behavior. Recently, there has been
increasing number of researchers focused on developing tracking algorithms to
generate more accurate and faster determination of object. Unfortunately,
recent studies have shown that efficient and robust tracking of multiple
objects with complex relations remain unsolved. Hence, focusing to develop
tracking algorithm in aquaculture is more challenging because tracked object
has a lot of aquatic variant creatures. By following aforementioned problem, we
develop tuna nutriment tracking based on the classical minimum cost problem
which consistently performs well in real environment datasets. In evaluation,
the proposed method achieved 21.32 pixels and 3.08 pixels for average error
distance and standard deviation, respectively. Quantitative evaluation based on
the data generated by human annotators shows that the proposed method is
valuable for aquaculture fish farm and can be widely applied to real
environment datasets.
</p>
<a href="http://arxiv.org/abs/2103.05886" target="_blank">arXiv:2103.05886</a> [<a href="http://arxiv.org/pdf/2103.05886" target="_blank">pdf</a>]

<h2>Evaluating COPY-BLEND Augmentation for Low Level Vision Tasks. (arXiv:2103.05889v1 [cs.CV])</h2>
<h3>Pranjay Shyam, Sandeep Singh Sengar, Kuk-Jin Yoon, Kyung-Soo Kim</h3>
<p>Region modification-based data augmentation techniques have shown to improve
performance for high level vision tasks (object detection, semantic
segmentation, image classification, etc.) by encouraging underlying algorithms
to focus on multiple discriminative features. However, as these techniques
destroy spatial relationship with neighboring regions, performance can be
deteriorated when using them to train algorithms designed for low level vision
tasks (low light image enhancement, image dehazing, deblurring, etc.) where
textural consistency between recovered and its neighboring regions is important
to ensure effective performance. In this paper, we examine the efficacy of a
simple copy-blend data augmentation technique that copies patches from noisy
images and blends onto a clean image and vice versa to ensure that an
underlying algorithm localizes and recovers affected regions resulting in
increased perceptual quality of a recovered image. To assess performance
improvement, we perform extensive experiments alongside different region
modification-based augmentation techniques and report observations such as
improved performance, reduced requirement for training dataset, and early
convergence across tasks such as low light image enhancement, image dehazing
and image deblurring without any modification to baseline algorithm.
</p>
<a href="http://arxiv.org/abs/2103.05889" target="_blank">arXiv:2103.05889</a> [<a href="http://arxiv.org/pdf/2103.05889" target="_blank">pdf</a>]

<h2>Limitations of Post-Hoc Feature Alignment for Robustness. (arXiv:2103.05898v1 [cs.CV])</h2>
<h3>Collin Burns, Jacob Steinhardt</h3>
<p>Feature alignment is an approach to improving robustness to distribution
shift that matches the distribution of feature activations between the training
distribution and test distribution. A particularly simple but effective
approach to feature alignment involves aligning the batch normalization
statistics between the two distributions in a trained neural network. This
technique has received renewed interest lately because of its impressive
performance on robustness benchmarks. However, when and why this method works
is not well understood. We investigate the approach in more detail and identify
several limitations. We show that it only significantly helps with a narrow set
of distribution shifts and we identify several settings in which it even
degrades performance. We also explain why these limitations arise by
pinpointing why this approach can be so effective in the first place. Our
findings call into question the utility of this approach and Unsupervised
Domain Adaptation more broadly for improving robustness in practice.
</p>
<a href="http://arxiv.org/abs/2103.05898" target="_blank">arXiv:2103.05898</a> [<a href="http://arxiv.org/pdf/2103.05898" target="_blank">pdf</a>]

<h2>RL-CSDia: Representation Learning of Computer Science Diagrams. (arXiv:2103.05900v1 [cs.CV])</h2>
<h3>Shaowei Wang, LingLing Zhang, Xuan Luo, Yi Yang, Xin Hu, Jun Liu</h3>
<p>Recent studies on computer vision mainly focus on natural images that express
real-world scenes. They achieve outstanding performance on diverse tasks such
as visual question answering. Diagram is a special form of visual expression
that frequently appears in the education field and is of great significance for
learners to understand multimodal knowledge. Current research on diagrams
preliminarily focuses on natural disciplines such as Biology and Geography,
whose expressions are still similar to natural images. Another type of diagrams
such as from Computer Science is composed of graphics containing complex
topologies and relations, and research on this type of diagrams is still blank.
The main challenges of graphic diagrams understanding are the rarity of data
and the confusion of semantics, which are mainly reflected in the diversity of
expressions. In this paper, we construct a novel dataset of graphic diagrams
named Computer Science Diagrams (CSDia). It contains more than 1,200 diagrams
and exhaustive annotations of objects and relations. Considering the visual
noises caused by the various expressions in diagrams, we introduce the topology
of diagrams to parse topological structure. After that, we propose Diagram
Parsing Net (DPN) to represent the diagram from three branches: topology,
visual feature, and text, and apply the model to the diagram classification
task to evaluate the ability of diagrams understanding. The results show the
effectiveness of the proposed DPN on diagrams understanding.
</p>
<a href="http://arxiv.org/abs/2103.05900" target="_blank">arXiv:2103.05900</a> [<a href="http://arxiv.org/pdf/2103.05900" target="_blank">pdf</a>]

<h2>Learning a Domain-Agnostic Visual Representation for Autonomous Driving via Contrastive Loss. (arXiv:2103.05902v1 [cs.CV])</h2>
<h3>Dongseok Shim, H. Jin Kim</h3>
<p>Deep neural networks have been widely studied in autonomous driving
applications such as semantic segmentation or depth estimation. However,
training a neural network in a supervised manner requires a large amount of
annotated labels which are expensive and time-consuming to collect. Recent
studies leverage synthetic data collected from a virtual environment which are
much easier to acquire and more accurate compared to data from the real world,
but they usually suffer from poor generalization due to the inherent domain
shift problem. In this paper, we propose a Domain-Agnostic Contrastive Learning
(DACL) which is a two-stage unsupervised domain adaptation framework with
cyclic adversarial training and contrastive loss. DACL leads the neural network
to learn domain-agnostic representation to overcome performance degradation
when there exists a difference between training and test data distribution. Our
proposed approach achieves better performance in the monocular depth estimation
task compared to previous state-of-the-art methods and also shows effectiveness
in the semantic segmentation task.
</p>
<a href="http://arxiv.org/abs/2103.05902" target="_blank">arXiv:2103.05902</a> [<a href="http://arxiv.org/pdf/2103.05902" target="_blank">pdf</a>]

<h2>FAST-Dynamic-Vision: Detection and Tracking Dynamic Objects with Event and Depth Sensing. (arXiv:2103.05903v1 [cs.RO])</h2>
<h3>Baotao He, Haojia Li, Siyuan Wu, Dong Wang, Zhiwei Zhang, Qianli Dong, Chao Xu, Fei Gao</h3>
<p>The development of aerial autonomy has enabled aerial robots to fly agilely
in complex environments. However, dodging fast-moving objects in flight remains
a challenge, limiting the further application of unmanned aerial vehicles
(UAVs). The bottleneck of solving this problem is the accurate perception of
rapid dynamic objects. Recently, event cameras have shown great potential in
solving this problem. This paper presents a complete perception system
including ego-motion compensation, object detection, and trajectory prediction
for fast-moving dynamic objects with low latency and high precision. Firstly,
we propose an accurate ego-motion compensation algorithm by considering both
rotational and translational motion for more robust object detection. Then, for
dynamic object detection, an event camera-based efficient regression algorithm
is designed. Finally, we propose an optimizationbased approach that
asynchronously fuses event and depth cameras for trajectory prediction.
Extensive real-world experiments and benchmarks are performed to validate our
framework. Moreover, our code will be released to benefit related researches.
</p>
<a href="http://arxiv.org/abs/2103.05903" target="_blank">arXiv:2103.05903</a> [<a href="http://arxiv.org/pdf/2103.05903" target="_blank">pdf</a>]

<h2>Combining Learning from Demonstration with Learning by Exploration to Facilitate Contact-Rich Tasks. (arXiv:2103.05904v1 [cs.RO])</h2>
<h3>Yunlei Shi, Zhaopeng Chen, Yansong Wu, Dimitri Henkel, Sebastian Riedel, Hongxu Liu, Qian Feng, Jianwei Zhang</h3>
<p>Collaborative robots are expected to be able to work alongside humans and in
some cases directly replace existing human workers, thus effectively responding
to rapid assembly line changes. Current methods for programming contact-rich
tasks, especially in heavily constrained space, tend to be fairly inefficient.
Therefore, faster and more intuitive approaches to robot teaching are urgently
required. This work focuses on combining visual servoing based learning from
demonstration (LfD) and force-based learning by exploration (LbE), to enable
fast and intuitive programming of contact-rich tasks with minimal user effort
required. Two learning approaches were developed and integrated into a
framework, and one relying on human to robot motion mapping (the visual
servoing approach) and one on force-based reinforcement learning. The developed
framework implements the non-contact demonstration teaching method based on
visual servoing approach and optimizes the demonstrated robot target positions
according to the detected contact state. The framework has been compared with
two most commonly used baseline techniques, pendant-based teaching and
hand-guiding teaching. The efficiency and reliability of the framework have
been validated through comparison experiments involving the teaching and
execution of contact-rich tasks. The framework proposed in this paper has
performed the best in terms of teaching time, execution success rate, risk of
damage, and ease of use.
</p>
<a href="http://arxiv.org/abs/2103.05904" target="_blank">arXiv:2103.05904</a> [<a href="http://arxiv.org/pdf/2103.05904" target="_blank">pdf</a>]

<h2>VideoMoCo: Contrastive Video Representation Learning with Temporally Adversarial Examples. (arXiv:2103.05905v1 [cs.CV])</h2>
<h3>Tian Pan, Yibing Song, Tianyu Yang, Wenhao Jiang, Wei Liu</h3>
<p>MoCo is effective for unsupervised image representation learning. In this
paper, we propose VideoMoCo for unsupervised video representation learning.
Given a video sequence as an input sample, we improve the temporal feature
representations of MoCo from two perspectives. First, we introduce a generator
to drop out several frames from this sample temporally. The discriminator is
then learned to encode similar feature representations regardless of frame
removals. By adaptively dropping out different frames during training
iterations of adversarial learning, we augment this input sample to train a
temporally robust encoder. Second, we use temporal decay to model key
attenuation in the memory queue when computing the contrastive loss. As the
momentum encoder updates after keys enqueue, the representation ability of
these keys degrades when we use the current input sample for contrastive
learning. This degradation is reflected via temporal decay to attend the input
sample to recent keys in the queue. As a result, we adapt MoCo to learn video
representations without empirically designing pretext tasks. By empowering the
temporal robustness of the encoder and modeling the temporal decay of the keys,
our VideoMoCo improves MoCo temporally based on contrastive learning.
Experiments on benchmark datasets including UCF101 and HMDB51 show that
VideoMoCo stands as a state-of-the-art video representation learning method.
</p>
<a href="http://arxiv.org/abs/2103.05905" target="_blank">arXiv:2103.05905</a> [<a href="http://arxiv.org/pdf/2103.05905" target="_blank">pdf</a>]

<h2>Robotic Imitation of Human Assembly Skills Using Hybrid Trajectory and Force Learning. (arXiv:2103.05912v1 [cs.RO])</h2>
<h3>Yan Wang, Cristian C. Beltran-Hernandez, Weiwei Wan, Kensuke Harada</h3>
<p>Robotic assembly tasks involve complex and low-clearance insertion
trajectories with varying contact forces at different stages. While the nominal
motion trajectory can be easily obtained from human demonstrations through
kinesthetic teaching, teleoperation, simulation, among other methods, the force
profile is harder to obtain especially when a real robot is unavailable. It is
difficult to obtain a realistic force profile in simulation even with physics
engines. Such simulated force profiles tend to be unsuitable for the actual
robotic assembly due to the reality gap and uncertainty in the assembly
process. To address this problem, we present a combined learning-based
framework to imitate human assembly skills through hybrid trajectory learning
and force learning. The main contribution of this work is the development of a
framework that combines hierarchical imitation learning, to learn the nominal
motion trajectory, with a reinforcement learning-based force control scheme to
learn an optimal force control policy, that can satisfy the nominal trajectory
while adapting to the force requirement of the assembly task. To further
improve the imitation learning part, we develop a hierarchical architecture,
following the idea of goal-conditioned imitation learning, to generate the
trajectory learning policy on the \textit{skill} level offline. Through
experimental validations, we corroborate that the proposed learning-based
framework is robust to uncertainty in the assembly task, can generate
high-quality trajectories, and can find suitable force control policies, which
adapt to the task's force requirements more efficiently.
</p>
<a href="http://arxiv.org/abs/2103.05912" target="_blank">arXiv:2103.05912</a> [<a href="http://arxiv.org/pdf/2103.05912" target="_blank">pdf</a>]

<h2>ES-Net: Erasing Salient Parts to Learn More in Re-Identification. (arXiv:2103.05918v1 [cs.CV])</h2>
<h3>Dong Shen, Shuai Zhao, Jinming Hu, Hao Feng, Deng Cai, Xiaofei He</h3>
<p>As an instance-level recognition problem, re-identification (re-ID) requires
models to capture diverse features. However, with continuous training, re-ID
models pay more and more attention to the salient areas. As a result, the model
may only focus on few small regions with salient representations and ignore
other important information. This phenomenon leads to inferior performance,
especially when models are evaluated on small inter-identity variation data. In
this paper, we propose a novel network, Erasing-Salient Net (ES-Net), to learn
comprehensive features by erasing the salient areas in an image. ES-Net
proposes a novel method to locate the salient areas by the confidence of
objects and erases them efficiently in a training batch. Meanwhile, to mitigate
the over-erasing problem, this paper uses a trainable pooling layer P-pooling
that generalizes global max and global average pooling. Experiments are
conducted on two specific re-identification tasks (i.e., Person re-ID, Vehicle
re-ID). Our ES-Net outperforms state-of-the-art methods on three Person re-ID
benchmarks and two Vehicle re-ID benchmarks. Specifically, mAP / Rank-1 rate:
88.6% / 95.7% on Market1501, 78.8% / 89.2% on DuckMTMC-reID, 57.3% / 80.9% on
MSMT17, 81.9% / 97.0% on Veri-776, respectively. Rank-1 / Rank-5 rate: 83.6% /
96.9% on VehicleID (Small), 79.9% / 93.5% on VehicleID (Medium), 76.9% / 90.7%
on VehicleID (Large), respectively. Moreover, the visualized salient areas show
human-interpretable visual explanations for the ranking results.
</p>
<a href="http://arxiv.org/abs/2103.05918" target="_blank">arXiv:2103.05918</a> [<a href="http://arxiv.org/pdf/2103.05918" target="_blank">pdf</a>]

<h2>An Image-based Approach of Task-driven Driving Scene Categorization. (arXiv:2103.05920v1 [cs.RO])</h2>
<h3>Shaochi Hu, Hanwei Fan, Biao Gao, XijunZhao, Huijing Zhao</h3>
<p>Categorizing driving scenes via visual perception is a key technology for
safe driving and the downstream tasks of autonomous vehicles.

Traditional methods infer scene category by detecting scene-related objects
or using a classifier that is trained on large datasets of fine-labeled scene
images.

Whereas at cluttered dynamic scenes such as campus or park, human activities
are not strongly confined by rules, and the functional attributes of places are
not strongly correlated with objects. So how to define, model and infer scene
categories is crucial to make the technique really helpful in assisting a robot
to pass through the scene.

This paper proposes a method of task-driven driving scene categorization
using weakly supervised data.

Given a front-view video of a driving scene, a set of anchor points is marked
by following the decision making of a human driver, where an anchor point is
not a semantic label but an indicator meaning the semantic attribute of the
scene is different from that of the previous one.

A measure is learned to discriminate the scenes of different semantic
attributes via contrastive learning, and a driving scene profiling and
categorization method is developed based on that measure.

Experiments are conducted on a front-view video that is recorded when a
vehicle passed through the cluttered dynamic campus of Peking University. The
scenes are categorized into straight road, turn road and alerting traffic. The
results of semantic scene similarity learning and driving scene categorization
are extensively studied, and positive result of scene categorization is 97.17
\% on the learning video and 85.44\% on the video of new scenes.
</p>
<a href="http://arxiv.org/abs/2103.05920" target="_blank">arXiv:2103.05920</a> [<a href="http://arxiv.org/pdf/2103.05920" target="_blank">pdf</a>]

<h2>RMP2: A Structured Composable Policy Class for Robot Learning. (arXiv:2103.05922v1 [cs.RO])</h2>
<h3>Anqi Li, Ching-An Cheng, M. Asif Rana, Man Xie, Karl Van Wyk, Nathan Ratliff, Byron Boots</h3>
<p>We consider the problem of learning motion policies for acceleration-based
robotics systems with a structured policy class specified by RMPflow. RMPflow
is a multi-task control framework that has been successfully applied in many
robotics problems. Using RMPflow as a structured policy class in learning has
several benefits, such as sufficient expressiveness, the flexibility to inject
different levels of prior knowledge as well as the ability to transfer policies
between robots. However, implementing a system for end-to-end learning RMPflow
policies faces several computational challenges. In this work, we re-examine
the message passing algorithm of RMPflow and propose a more efficient alternate
algorithm, called RMP2, that uses modern automatic differentiation tools (such
as TensorFlow and PyTorch) to compute RMPflow policies. Our new design retains
the strengths of RMPflow while bringing in advantages from automatic
differentiation, including 1) easy programming interfaces to designing complex
transformations; 2) support of general directed acyclic graph (DAG)
transformation structures; 3) end-to-end differentiability for policy learning;
4) improved computational efficiency. Because of these features, RMP2 can be
treated as a structured policy class for efficient robot learning which is
suitable encoding domain knowledge. Our experiments show that using structured
policy class given by RMP2 can improve policy performance and safety in
reinforcement learning tasks for goal reaching in cluttered space.
</p>
<a href="http://arxiv.org/abs/2103.05922" target="_blank">arXiv:2103.05922</a> [<a href="http://arxiv.org/pdf/2103.05922" target="_blank">pdf</a>]

<h2>Deep Sensing of Urban Waterlogging. (arXiv:2103.05927v1 [cs.CV])</h2>
<h3>Shi-Wei Lo</h3>
<p>In the monsoon season, sudden flood events occur frequently in urban areas,
which hamper the social and economic activities and may threaten the
infrastructure and lives. The use of an efficient large-scale waterlogging
sensing and information system can provide valuable real-time disaster
information to facilitate disaster management and enhance awareness of the
general public to alleviate losses during and after flood disasters. Therefore,
in this study, a visual sensing approach driven by deep neural networks and
information and communication technology was developed to provide an end-to-end
mechanism to realize waterlogging sensing and event-location mapping. The use
of a deep sensing system in the monsoon season in Taiwan was demonstrated, and
waterlogging events were predicted on the island-wide scale. The system could
sense approximately 2379 vision sources through an internet of video things
framework and transmit the event-location information in 5 min. The proposed
approach can sense waterlogging events at a national scale and provide an
efficient and highly scalable alternative to conventional waterlogging sensing
methods.
</p>
<a href="http://arxiv.org/abs/2103.05927" target="_blank">arXiv:2103.05927</a> [<a href="http://arxiv.org/pdf/2103.05927" target="_blank">pdf</a>]

<h2>MapFusion: A General Framework for 3D Object Detection with HDMaps. (arXiv:2103.05929v1 [cs.CV])</h2>
<h3>Jin Fang, Dingfu Zhou, Xibin Song, Liangjun Zhang</h3>
<p>3D object detection is a key perception component in autonomous driving. Most
recent approaches are based on Lidar sensors only or fused with cameras. Maps
(e.g., High Definition Maps), a basic infrastructure for intelligent vehicles,
however, have not been well exploited for boosting object detection tasks. In
this paper, we propose a simple but effective framework - MapFusion to
integrate the map information into modern 3D object detector pipelines. In
particular, we design a FeatureAgg module for HD Map feature extraction and
fusion, and a MapSeg module as an auxiliary segmentation head for the detection
backbone. Our proposed MapFusion is detector independent and can be easily
integrated into different detectors. The experimental results of three
different baselines on large public autonomous driving dataset demonstrate the
superiority of the proposed framework. By fusing the map information, we can
achieve 1.27 to 2.79 points improvements for mean Average Precision (mAP) on
three strong 3d object detection baselines.
</p>
<a href="http://arxiv.org/abs/2103.05929" target="_blank">arXiv:2103.05929</a> [<a href="http://arxiv.org/pdf/2103.05929" target="_blank">pdf</a>]

<h2>AttaNet: Attention-Augmented Network for Fast and Accurate Scene Parsing. (arXiv:2103.05930v1 [cs.CV])</h2>
<h3>Qi Song, Kangfu Mei, Rui Huang</h3>
<p>Two factors have proven to be very important to the performance of semantic
segmentation models: global context and multi-level semantics. However,
generating features that capture both factors always leads to high
computational complexity, which is problematic in real-time scenarios. In this
paper, we propose a new model, called Attention-Augmented Network (AttaNet), to
capture both global context and multilevel semantics while keeping the
efficiency high. AttaNet consists of two primary modules: Strip Attention
Module (SAM) and Attention Fusion Module (AFM). Viewing that in challenging
images with low segmentation accuracy, there are a significantly larger amount
of vertical strip areas than horizontal ones, SAM utilizes a striping operation
to reduce the complexity of encoding global context in the vertical direction
drastically while keeping most of contextual information, compared to the
non-local approaches. Moreover, AFM follows a cross-level aggregation strategy
to limit the computation, and adopts an attention strategy to weight the
importance of different levels of features at each pixel when fusing them,
obtaining an efficient multi-level representation. We have conducted extensive
experiments on two semantic segmentation benchmarks, and our network achieves
different levels of speed/accuracy trade-offs on Cityscapes, e.g., 71 FPS/79.9%
mIoU, 130 FPS/78.5% mIoU, and 180 FPS/70.1% mIoU, and leading performance on
ADE20K as well.
</p>
<a href="http://arxiv.org/abs/2103.05930" target="_blank">arXiv:2103.05930</a> [<a href="http://arxiv.org/pdf/2103.05930" target="_blank">pdf</a>]

<h2>TransMed: Transformers Advance Multi-modal Medical Image Classification. (arXiv:2103.05940v1 [cs.CV])</h2>
<h3>Yin Dai, Yifan Gao</h3>
<p>Over the past decade, convolutional neural networks (CNN) have shown very
competitive performance in medical image analysis tasks, such as disease
classification, tumor segmentation, and lesion detection. CNN has great
advantages in extracting local features of images. However, due to the locality
of convolution operation, it can not deal with long-range relationships well.
Recently, transformers have been applied to computer vision and achieved
remarkable success in large-scale datasets. Compared with natural images,
multi-modal medical images have explicit and important long-range dependencies,
and effective multi-modal fusion strategies can greatly improve the performance
of deep models. This prompts us to study transformer-based structures and apply
them to multi-modal medical images. Existing transformer-based network
architectures require large-scale datasets to achieve better performance.
However, medical imaging datasets are relatively small, which makes it
difficult to apply pure transformers to medical image analysis. Therefore, we
propose TransMed for multi-modal medical image classification. TransMed
combines the advantages of CNN and transformer to efficiently extract low-level
features of images and establish long-range dependencies between modalities. We
evaluated our model for the challenging problem of preoperative diagnosis of
parotid gland tumors, and the experimental results show the advantages of our
proposed method. We argue that the combination of CNN and transformer has
tremendous potential in a large number of medical image analysis tasks. To our
best knowledge, this is the first work to apply transformers to medical image
classification.
</p>
<a href="http://arxiv.org/abs/2103.05940" target="_blank">arXiv:2103.05940</a> [<a href="http://arxiv.org/pdf/2103.05940" target="_blank">pdf</a>]

<h2>Nth Order Analytical Time Derivatives of Inverse Dynamics in Recursive and Closed Forms. (arXiv:2103.05941v1 [cs.RO])</h2>
<h3>Shivesh Kumar, Andreas Mueller</h3>
<p>Derivatives of equations of motion describing the rigid body dynamics are
becoming increasingly relevant for the robotics community and find many
applications in design and control of robotic systems. Controlling robots, and
multibody systems comprising elastic components in particular, not only
requires smooth trajectories but also the time derivatives of the control
forces/torques, hence of the equations of motion (EOM). This paper presents
novel nth order time derivatives of the EOM in both closed and recursive forms.
While the former provides a direct insight into the structure of these
derivatives,the latter leads to their highly efficient implementation for large
degree of freedom robotic system.
</p>
<a href="http://arxiv.org/abs/2103.05941" target="_blank">arXiv:2103.05941</a> [<a href="http://arxiv.org/pdf/2103.05941" target="_blank">pdf</a>]

<h2>Deep Convolutional Sparse Coding Network for Pansharpening with Guidance of Side Information. (arXiv:2103.05946v1 [cs.CV])</h2>
<h3>Shuang Xu, Jiangshe Zhang, Kai Sun, Zixiang Zhao, Lu Huang, Junmin Liu, Chunxia Zhang</h3>
<p>Pansharpening is a fundamental issue in remote sensing field. This paper
proposes a side information partially guided convolutional sparse coding (SCSC)
model for pansharpening. The key idea is to split the low resolution
multispectral image into a panchromatic image related feature map and a
panchromatic image irrelated feature map, where the former one is regularized
by the side information from panchromatic images. With the principle of
algorithm unrolling techniques, the proposed model is generalized as a deep
neural network, called as SCSC pansharpening neural network (SCSC-PNN).
Compared with 13 classic and state-of-the-art methods on three satellites, the
numerical experiments show that SCSC-PNN is superior to others. The codes are
available at https://github.com/xsxjtu/SCSC-PNN.
</p>
<a href="http://arxiv.org/abs/2103.05946" target="_blank">arXiv:2103.05946</a> [<a href="http://arxiv.org/pdf/2103.05946" target="_blank">pdf</a>]

<h2>FSCE: Few-Shot Object Detection via Contrastive Proposal Encoding. (arXiv:2103.05950v1 [cs.CV])</h2>
<h3>Bo Sun, Banghuai Li, Shengcai Cai, Ye Yuan, Chi Zhang</h3>
<p>Emerging interests have been brought to recognize previously unseen objects
given very few training examples, known as few-shot object detection (FSOD).
Recent researches demonstrate that good feature embedding is the key to reach
favorable few-shot learning performance. We observe object proposals with
different Intersection-of-Union (IoU) scores are analogous to the intra-image
augmentation used in contrastive approaches. And we exploit this analogy and
incorporate supervised contrastive learning to achieve more robust objects
representations in FSOD. We present Few-Shot object detection via Contrastive
proposals Encoding (FSCE), a simple yet effective approach to learning
contrastive-aware object proposal encodings that facilitate the classification
of detected objects. We notice the degradation of average precision (AP) for
rare objects mainly comes from misclassifying novel instances as confusable
classes. And we ease the misclassification issues by promoting instance level
intra-class compactness and inter-class variance via our contrastive proposal
encoding loss (CPE loss). Our design outperforms current state-of-the-art works
in any shot and all data splits, with up to +8.8% on standard benchmark PASCAL
VOC and +2.7% on challenging COCO benchmark. Code is available at:
https://github.com/bsun0802/FSCE.git
</p>
<a href="http://arxiv.org/abs/2103.05950" target="_blank">arXiv:2103.05950</a> [<a href="http://arxiv.org/pdf/2103.05950" target="_blank">pdf</a>]

<h2>Spatiotemporal Registration for Event-based Visual Odometry. (arXiv:2103.05955v1 [cs.CV])</h2>
<h3>Daqi Liu Alvaro Parra, Tat-Jun Chin</h3>
<p>A useful application of event sensing is visual odometry, especially in
settings that require high-temporal resolution. The state-of-the-art method of
contrast maximisation recovers the motion from a batch of events by maximising
the contrast of the image of warped events. However, the cost scales with image
resolution and the temporal resolution can be limited by the need for large
batch sizes to yield sufficient structure in the contrast image. In this work,
we propose spatiotemporal registration as a compelling technique for
event-based rotational motion estimation. We theoretcally justify the approach
and establish its fundamental and practical advantages over contrast
maximisation. In particular, spatiotemporal registration also produces feature
tracks as a by-product, which directly supports an efficient visual odometry
pipeline with graph-based optimisation for motion averaging. The simplicity of
our visual odometry pipeline allows it to process more than 1 M events/second.
We also contribute a new event dataset for visual odometry, where motion
sequences with large velocity variations were acquired using a high-precision
robot arm.
</p>
<a href="http://arxiv.org/abs/2103.05955" target="_blank">arXiv:2103.05955</a> [<a href="http://arxiv.org/pdf/2103.05955" target="_blank">pdf</a>]

<h2>Beyond Self-Supervision: A Simple Yet Effective Network Distillation Alternative to Improve Backbones. (arXiv:2103.05959v1 [cs.CV])</h2>
<h3>Cheng Cui, Ruoyu Guo, Yuning Du, Dongliang He, Fu Li, Zewu Wu, Qiwen Liu, Shilei Wen, Jizhou Huang, Xiaoguang Hu, Dianhai Yu, Errui Ding, Yanjun Ma</h3>
<p>Recently, research efforts have been concentrated on revealing how
pre-trained model makes a difference in neural network performance.
Self-supervision and semi-supervised learning technologies have been
extensively explored by the community and are proven to be of great potential
in obtaining a powerful pre-trained model. However, these models require huge
training costs (i.e., hundreds of millions of images or training iterations).
In this paper, we propose to improve existing baseline networks via knowledge
distillation from off-the-shelf pre-trained big powerful models. Different from
existing knowledge distillation frameworks which require student model to be
consistent with both soft-label generated by teacher model and hard-label
annotated by humans, our solution performs distillation by only driving
prediction of the student model consistent with that of the teacher model.
Therefore, our distillation setting can get rid of manually labeled data and
can be trained with extra unlabeled data to fully exploit capability of teacher
model for better learning. We empirically find that such simple distillation
settings perform extremely effective, for example, the top-1 accuracy on
ImageNet-1k validation set of MobileNetV3-large and ResNet50-D can be
significantly improved from 75.2% to 79% and 79.1% to 83%, respectively. We
have also thoroughly analyzed what are dominant factors that affect the
distillation performance and how they make a difference. Extensive downstream
computer vision tasks, including transfer learning, object detection and
semantic segmentation, can significantly benefit from the distilled pretrained
models. All our experiments are implemented based on PaddlePaddle, codes and a
series of improved pretrained models with ssld suffix are available in
PaddleClas.
</p>
<a href="http://arxiv.org/abs/2103.05959" target="_blank">arXiv:2103.05959</a> [<a href="http://arxiv.org/pdf/2103.05959" target="_blank">pdf</a>]

<h2>COLA-Net: Collaborative Attention Network for Image Restoration. (arXiv:2103.05961v1 [cs.CV])</h2>
<h3>Chong Mou, Jian Zhang, Xiaopeng Fan, Hangfan Liu, Ronggang Wang</h3>
<p>Local and non-local attention-based methods have been well studied in various
image restoration tasks while leading to promising performance. However, most
of the existing methods solely focus on one type of attention mechanism (local
or non-local). Furthermore, by exploiting the self-similarity of natural
images, existing pixel-wise non-local attention operations tend to give rise to
deviations in the process of characterizing long-range dependence due to image
degeneration. To overcome these problems, in this paper we propose a novel
collaborative attention network (COLA-Net) for image restoration, as the first
attempt to combine local and non-local attention mechanisms to restore image
content in the areas with complex textures and with highly repetitive details
respectively. In addition, an effective and robust patch-wise non-local
attention model is developed to capture long-range feature correspondences
through 3D patches. Extensive experiments on synthetic image denoising, real
image denoising and compression artifact reduction tasks demonstrate that our
proposed COLA-Net is able to achieve state-of-the-art performance in both peak
signal-to-noise ratio and visual perception, while maintaining an attractive
computational complexity. The source code is available on
https://github.com/MC-E/COLA-Net.
</p>
<a href="http://arxiv.org/abs/2103.05961" target="_blank">arXiv:2103.05961</a> [<a href="http://arxiv.org/pdf/2103.05961" target="_blank">pdf</a>]

<h2>SDD-FIQA: Unsupervised Face Image Quality Assessment with Similarity Distribution Distance. (arXiv:2103.05977v1 [cs.CV])</h2>
<h3>Fu-Zhao Ou, Xingyu Chen, Ruixin Zhang, Yuge Huang, Shaoxin Li, Jilin Li, Yong Li, Liujuan Cao, Yuan-Gen Wang</h3>
<p>In recent years, Face Image Quality Assessment (FIQA) has become an
indispensable part of the face recognition system to guarantee the stability
and reliability of recognition performance in an unconstrained scenario. For
this purpose, the FIQA method should consider both the intrinsic property and
the recognizability of the face image. Most previous works aim to estimate the
sample-wise embedding uncertainty or pair-wise similarity as the quality score,
which only considers the information from partial intra-class. However, these
methods ignore the valuable information from the inter-class, which is for
estimating to the recognizability of face image. In this work, we argue that a
high-quality face image should be similar to its intra-class samples and
dissimilar to its inter-class samples. Thus, we propose a novel unsupervised
FIQA method that incorporates Similarity Distribution Distance for Face Image
Quality Assessment (SDD-FIQA). Our method generates quality pseudo-labels by
calculating the Wasserstein Distance (WD) between the intra-class similarity
distributions and inter-class similarity distributions. With these quality
pseudo-labels, we are capable of training a regression network for quality
prediction. Extensive experiments on benchmark datasets demonstrate that the
proposed SDD-FIQA surpasses the state-of-the-arts by an impressive margin.
Meanwhile, our method shows good generalization across different recognition
systems.
</p>
<a href="http://arxiv.org/abs/2103.05977" target="_blank">arXiv:2103.05977</a> [<a href="http://arxiv.org/pdf/2103.05977" target="_blank">pdf</a>]

<h2>Reformulating HOI Detection as Adaptive Set Prediction. (arXiv:2103.05983v1 [cs.CV])</h2>
<h3>Mingfei Chen, Yue Liao, Si Liu, Zhiyuan Chen, Fei Wang, Chen Qian</h3>
<p>Determining which image regions to concentrate on is critical for
Human-Object Interaction (HOI) detection. Conventional HOI detectors focus on
either detected human and object pairs or pre-defined interaction locations,
which limits learning of the effective features. In this paper, we reformulate
HOI detection as an adaptive set prediction problem, with this novel
formulation, we propose an Adaptive Set-based one-stage framework (AS-Net) with
parallel instance and interaction branches. To attain this, we map a trainable
interaction query set to an interaction prediction set with a transformer. Each
query adaptively aggregates the interaction-relevant features from global
contexts through multi-head co-attention. Besides, the training process is
supervised adaptively by matching each ground-truth with the interaction
prediction. Furthermore, we design an effective instance-aware attention module
to introduce instructive features from the instance branch into the interaction
branch. Our method outperforms previous state-of-the-art methods without any
extra human pose and language features on three challenging HOI detection
datasets. Especially, we achieve over $31\%$ relative improvement on a large
scale HICO-DET dataset. Code is available at
https://github.com/yoyomimi/AS-Net.
</p>
<a href="http://arxiv.org/abs/2103.05983" target="_blank">arXiv:2103.05983</a> [<a href="http://arxiv.org/pdf/2103.05983" target="_blank">pdf</a>]

<h2>Multi-Pretext Attention Network for Few-shot Learning with Self-supervision. (arXiv:2103.05985v1 [cs.CV])</h2>
<h3>Hainan Li, Renshuai Tao, Jun Li, Haotong Qin, Yifu Ding, Shuo Wang, Xianglong Liu</h3>
<p>Few-shot learning is an interesting and challenging study, which enables
machines to learn from few samples like humans. Existing studies rarely exploit
auxiliary information from large amount of unlabeled data. Self-supervised
learning is emerged as an efficient method to utilize unlabeled data. Existing
self-supervised learning methods always rely on the combination of geometric
transformations for the single sample by augmentation, while seriously neglect
the endogenous correlation information among different samples that is the same
important for the task. In this work, we propose a Graph-driven Clustering
(GC), a novel augmentation-free method for self-supervised learning, which does
not rely on any auxiliary sample and utilizes the endogenous correlation
information among input samples. Besides, we propose Multi-pretext Attention
Network (MAN), which exploits a specific attention mechanism to combine the
traditional augmentation-relied methods and our GC, adaptively learning their
optimized weights to improve the performance and enabling the feature extractor
to obtain more universal representations. We evaluate our MAN extensively on
miniImageNet and tieredImageNet datasets and the results demonstrate that the
proposed method outperforms the state-of-the-art (SOTA) relevant methods.
</p>
<a href="http://arxiv.org/abs/2103.05985" target="_blank">arXiv:2103.05985</a> [<a href="http://arxiv.org/pdf/2103.05985" target="_blank">pdf</a>]

<h2>Wide Aspect Ratio Matching for Robust Face Detection. (arXiv:2103.05993v1 [cs.CV])</h2>
<h3>Shi Luo, Xiongfei Li, Xiaoli Zhang</h3>
<p>Recently, anchor-based methods have achieved great progress in face
detection. Once anchor design and anchor matching strategy determined, plenty
of positive anchors will be sampled. However, faces with extreme aspect ratio
always fail to be sampled according to standard anchor matching strategy. In
fact, the max IoUs between anchors and extreme aspect ratio faces are still
lower than fixed sampling threshold. In this paper, we firstly explore the
factors that affect the max IoU of each face in theory. Then, anchor matching
simulation is performed to evaluate the sampling range of face aspect ratio.
Besides, we propose a Wide Aspect Ratio Matching (WARM) strategy to collect
more representative positive anchors from ground-truth faces across a wide
range of aspect ratio. Finally, we present a novel feature enhancement module,
named Receptive Field Diversity (RFD) module, to provide diverse receptive
field corresponding to different aspect ratios. Extensive experiments show that
our method can help detectors better capture extreme aspect ratio faces and
achieve promising detection performance on challenging face detection
benchmarks, including WIDER FACE and FDDB datasets.
</p>
<a href="http://arxiv.org/abs/2103.05993" target="_blank">arXiv:2103.05993</a> [<a href="http://arxiv.org/pdf/2103.05993" target="_blank">pdf</a>]

<h2>Quality-Aware Network for Human Parsing. (arXiv:2103.05997v1 [cs.CV])</h2>
<h3>Lu Yang, Qing Song, Zhihui Wang, Zhiwei Liu, Songcen Xu, Zhihao Li</h3>
<p>How to estimate the quality of the network output is an important issue, and
currently there is no effective solution in the field of human parsing. In
order to solve this problem, this work proposes a statistical method based on
the output probability map to calculate the pixel quality information, which is
called pixel score. In addition, the Quality-Aware Module (QAM) is proposed to
fuse the different quality information, the purpose of which is to estimate the
quality of human parsing results. We combine QAM with a concise and effective
network design to propose Quality-Aware Network (QANet) for human parsing.
Benefiting from the superiority of QAM and QANet, we achieve the best
performance on three multiple and one single human parsing benchmarks,
including CIHP, MHP-v2, Pascal-Person-Part and LIP. Without increasing the
training and inference time, QAM improves the AP$^\text{r}$ criterion by more
than 10 points in the multiple human parsing task. QAM can be extended to other
tasks with good quality estimation, e.g. instance segmentation. Specifically,
QAM improves Mask R-CNN by ~1% mAP on COCO and LVISv1.0 datasets. Based on the
proposed QAM and QANet, our overall system wins 1st place in CVPR2019 COCO
DensePose Challenge, and 1st place in Track 1 &amp; 2 of CVPR2020 LIP Challenge.
Code and models are available at https://github.com/soeaver/QANet.
</p>
<a href="http://arxiv.org/abs/2103.05997" target="_blank">arXiv:2103.05997</a> [<a href="http://arxiv.org/pdf/2103.05997" target="_blank">pdf</a>]

<h2>DSEC: A Stereo Event Camera Dataset for Driving Scenarios. (arXiv:2103.06011v1 [cs.CV])</h2>
<h3>Mathias Gehrig, Willem Aarents, Daniel Gehrig, Davide Scaramuzza</h3>
<p>Once an academic venture, autonomous driving has received unparalleled
corporate funding in the last decade. Still, the operating conditions of
current autonomous cars are mostly restricted to ideal scenarios. This means
that driving in challenging illumination conditions such as night, sunrise, and
sunset remains an open problem. In these cases, standard cameras are being
pushed to their limits in terms of low light and high dynamic range
performance. To address these challenges, we propose, DSEC, a new dataset that
contains such demanding illumination conditions and provides a rich set of
sensory data. DSEC offers data from a wide-baseline stereo setup of two color
frame cameras and two high-resolution monochrome event cameras. In addition, we
collect lidar data and RTK GPS measurements, both hardware synchronized with
all camera data. One of the distinctive features of this dataset is the
inclusion of high-resolution event cameras. Event cameras have received
increasing attention for their high temporal resolution and high dynamic range
performance. However, due to their novelty, event camera datasets in driving
scenarios are rare. This work presents the first high-resolution, large-scale
stereo dataset with event cameras. The dataset contains 53 sequences collected
by driving in a variety of illumination conditions and provides ground truth
disparity for the development and evaluation of event-based stereo algorithms.
</p>
<a href="http://arxiv.org/abs/2103.06011" target="_blank">arXiv:2103.06011</a> [<a href="http://arxiv.org/pdf/2103.06011" target="_blank">pdf</a>]

<h2>Principal component-based image segmentation: a new approach to outline in vitro cell colonies. (arXiv:2103.06022v1 [cs.CV])</h2>
<h3>Delmon Arous, Stefan Schrunner, Ingunn Hanson, Nina F.J. Edin, Eirik Malinen</h3>
<p>The in vitro clonogenic assay is a technique to study the ability of a cell
to form a colony in a culture dish. By optical imaging, dishes with stained
colonies can be scanned and assessed digitally. Identification, segmentation
and counting of stained colonies play a vital part in high-throughput screening
and quantitative assessment of biological assays. Image processing of such
pictured/scanned assays can be affected by image/scan acquisition artifacts
like background noise and spatially varying illumination, and contaminants in
the suspension medium. Although existing approaches tackle these issues, the
segmentation quality requires further improvement, particularly on noisy and
low contrast images. In this work, we present an objective and versatile
machine learning procedure to amend these issues by characterizing, extracting
and segmenting inquired colonies using principal component analysis, k-means
clustering and a modified watershed segmentation algorithm. The intention is to
automatically identify visible colonies through spatial texture assessment and
accordingly discriminate them from background in preparation for successive
segmentation. The proposed segmentation algorithm yielded a similar quality as
manual counting by human observers. High F1 scores (&gt;0.9) and low
root-mean-square errors (around 14%) underlined good agreement with ground
truth data. Moreover, it outperformed a recent state-of-the-art method. The
methodology will be an important tool in future cancer research applications.
</p>
<a href="http://arxiv.org/abs/2103.06022" target="_blank">arXiv:2103.06022</a> [<a href="http://arxiv.org/pdf/2103.06022" target="_blank">pdf</a>]

<h2>Model-free Vehicle Tracking and State Estimation in Point Cloud Sequences. (arXiv:2103.06028v1 [cs.CV])</h2>
<h3>Ziqi Pang, Zhichao Li, Naiyan Wang</h3>
<p>Estimating the states of surrounding traffic participants stays at the core
of autonomous driving. In this paper, we study a novel setting of this problem:
model-free single object tracking (SOT), which takes the object state in the
first frame as input, and jointly solves state estimation and tracking in
subsequent frames. The main purpose for this new setting is to break the strong
limitation of the popular "detection and tracking" scheme in multi-object
tracking. Moreover, we notice that shape completion by overlaying the point
clouds, which is a by-product of our proposed task, not only improves the
performance of state estimation but also has numerous applications. As no
benchmark for this task is available so far, we construct a new dataset
LiDAR-SOT and corresponding evaluation protocols based on the Waymo Open
dataset. We then propose an optimization-based algorithm called SOTracker based
on point cloud registration, vehicle shapes, and motion priors. Our
quantitative and qualitative results prove the effectiveness of our SOTracker
and reveal the challenging cases for SOT in point clouds, including the
sparsity of LiDAR data, abrupt motion variation, etc. Finally, we also explore
how the proposed task and algorithm may benefit other autonomous driving
applications, including simulating LiDAR scans, generating motion data, and
annotating optical flow. The code and protocols for our benchmark and algorithm
are available at https://github.com/TuSimple/LiDAR_SOT/ . A video demonstration
is at https://www.youtube.com/watch?v=BpHixKs91i8 .
</p>
<a href="http://arxiv.org/abs/2103.06028" target="_blank">arXiv:2103.06028</a> [<a href="http://arxiv.org/pdf/2103.06028" target="_blank">pdf</a>]

<h2>FedDG: Federated Domain Generalization on Medical Image Segmentation via Episodic Learning in Continuous Frequency Space. (arXiv:2103.06030v1 [cs.CV])</h2>
<h3>Quande Liu, Cheng Chen, Jing Qin, Qi Dou, Pheng-Ann Heng</h3>
<p>Federated learning allows distributed medical institutions to collaboratively
learn a shared prediction model with privacy protection. While at clinical
deployment, the models trained in federated learning can still suffer from
performance drop when applied to completely unseen hospitals outside the
federation. In this paper, we point out and solve a novel problem setting of
federated domain generalization (FedDG), which aims to learn a federated model
from multiple distributed source domains such that it can directly generalize
to unseen target domains. We present a novel approach, named as Episodic
Learning in Continuous Frequency Space (ELCFS), for this problem by enabling
each client to exploit multi-source data distributions under the challenging
constraint of data decentralization. Our approach transmits the distribution
information across clients in a privacy-protecting way through an effective
continuous frequency space interpolation mechanism. With the transferred
multi-source distributions, we further carefully design a boundary-oriented
episodic learning paradigm to expose the local learning to domain distribution
shifts and particularly meet the challenges of model generalization in medical
image segmentation scenario. The effectiveness of our method is demonstrated
with superior performance over state-of-the-arts and in-depth ablation
experiments on two medical image segmentation tasks. The code is available at
"https://github.com/liuquande/FedDG-ELCFS".
</p>
<a href="http://arxiv.org/abs/2103.06030" target="_blank">arXiv:2103.06030</a> [<a href="http://arxiv.org/pdf/2103.06030" target="_blank">pdf</a>]

<h2>Deep Superpixel Cut for Unsupervised Image Segmentation. (arXiv:2103.06031v1 [cs.CV])</h2>
<h3>Qinghong Lin, Weichan Zhong, Jianglin Lu</h3>
<p>Image segmentation, one of the most critical vision tasks, has been studied
for many years. Most of the early algorithms are unsupervised methods, which
use hand-crafted features to divide the image into many regions. Recently,
owing to the great success of deep learning technology, CNNs based methods show
superior performance in image segmentation. However, these methods rely on a
large number of human annotations, which are expensive to collect. In this
paper, we propose a deep unsupervised method for image segmentation, which
contains the following two stages. First, a Superpixelwise Autoencoder
(SuperAE) is designed to learn the deep embedding and reconstruct a smoothed
image, then the smoothed image is passed to generate superpixels. Second, we
present a novel clustering algorithm called Deep Superpixel Cut (DSC), which
measures the deep similarity between superpixels and formulates image
segmentation as a soft partitioning problem. Via backpropagation, DSC
adaptively partitions the superpixels into perceptual regions. Experimental
results on the BSDS500 dataset demonstrate the effectiveness of the proposed
method.
</p>
<a href="http://arxiv.org/abs/2103.06031" target="_blank">arXiv:2103.06031</a> [<a href="http://arxiv.org/pdf/2103.06031" target="_blank">pdf</a>]

<h2>Cross-modal Image Retrieval with Deep Mutual Information Maximization. (arXiv:2103.06032v1 [cs.CV])</h2>
<h3>Chunbin Gu, Jiajun Bu, Xixi Zhou, Chengwei Yao, Dongfang Ma, Zhi Yu, Xifeng Yan</h3>
<p>In this paper, we study the cross-modal image retrieval, where the inputs
contain a source image plus some text that describes certain modifications to
this image and the desired image. Prior work usually uses a three-stage
strategy to tackle this task: 1) extract the features of the inputs; 2) fuse
the feature of the source image and its modified text to obtain fusion feature;
3) learn a similarity metric between the desired image and the source image +
modified text by using deep metric learning. Since classical image/text
encoders can learn the useful representation and common pair-based loss
functions of distance metric learning are enough for cross-modal retrieval,
people usually improve retrieval accuracy by designing new fusion networks.
However, these methods do not successfully handle the modality gap caused by
the inconsistent distribution and representation of the features of different
modalities, which greatly influences the feature fusion and similarity
learning. To alleviate this problem, we adopt the contrastive self-supervised
learning method Deep InforMax (DIM) to our approach to bridge this gap by
enhancing the dependence between the text, the image, and their fusion.
Specifically, our method narrows the modality gap between the text modality and
the image modality by maximizing mutual information between their not exactly
semantically identical representation. Moreover, we seek an effective common
subspace for the semantically same fusion feature and desired image's feature
by utilizing Deep InforMax between the low-level layer of the image encoder and
the high-level layer of the fusion network. Extensive experiments on three
large-scale benchmark datasets show that we have bridged the modality gap
between different modalities and achieve state-of-the-art retrieval
performance.
</p>
<a href="http://arxiv.org/abs/2103.06032" target="_blank">arXiv:2103.06032</a> [<a href="http://arxiv.org/pdf/2103.06032" target="_blank">pdf</a>]

<h2>Time-Ordered Recent Event (TORE) Volumes for Event Cameras. (arXiv:2103.06108v1 [cs.CV])</h2>
<h3>R. Wes Baldwin, Ruixu Liu, Mohammed Almatrafi, Vijayan Asari, Keigo Hirakawa</h3>
<p>Event cameras are an exciting, new sensor modality enabling high-speed
imaging with extremely low-latency and wide dynamic range. Unfortunately, most
machine learning architectures are not designed to directly handle sparse data,
like that generated from event cameras. Many state-of-the-art algorithms for
event cameras rely on interpolated event representations - obscuring crucial
timing information, increasing the data volume, and limiting overall network
performance. This paper details an event representation called Time-Ordered
Recent Event (TORE) volumes. TORE volumes are designed to compactly store raw
spike timing information with minimal information loss. This bio-inspired
design is memory efficient, computationally fast, avoids time-blocking (i.e.
fixed and predefined frame rates), and contains "local memory" from past data.
The design is evaluated on a wide range of challenging tasks (e.g. event
denoising, image reconstruction, classification, and human pose estimation) and
is shown to dramatically improve state-of-the-art performance. TORE volumes are
an easy-to-implement replacement for any algorithm currently utilizing event
representations.
</p>
<a href="http://arxiv.org/abs/2103.06108" target="_blank">arXiv:2103.06108</a> [<a href="http://arxiv.org/pdf/2103.06108" target="_blank">pdf</a>]

<h2>DLL: Direct LIDAR Localization. A map-based localization approach for aerial robots. (arXiv:2103.06112v1 [cs.RO])</h2>
<h3>Fernando Caballero, Luis Merino</h3>
<p>This paper presents DLL, a direct map-based localization technique using 3D
LIDAR for its application to aerial robots. DLL implements a point cloud to map
registration based on non-linear optimization of the distance of the points and
the map, thus not requiring features, neither point correspondences. Given an
initial pose, the method is able to track the pose of the robot by refining the
predicted pose from odometry. Through benchmarks using real datasets and
simulations, we show how the method performs much better than Monte-Carlo
localization methods and achieves comparable precision to other
optimization-based approaches but running one order of magnitude faster. The
method is also robust under odometric errors. The approach has been implemented
under the Robot Operating System (ROS), and it is publicly available.
</p>
<a href="http://arxiv.org/abs/2103.06112" target="_blank">arXiv:2103.06112</a> [<a href="http://arxiv.org/pdf/2103.06112" target="_blank">pdf</a>]

<h2>GRIT: Verifiable Goal Recognition for Autonomous Driving using Decision Trees. (arXiv:2103.06113v1 [cs.RO])</h2>
<h3>Cillian Brewitt, Balint Gyevnar, Stefano V. Albrecht</h3>
<p>It is useful for autonomous vehicles to have the ability to infer the goals
of other vehicles (goal recognition), in order to safely interact with other
vehicles and predict their future trajectories. Goal recognition methods must
be fast to run in real time and make accurate inferences. As autonomous driving
is safety-critical, it is important to have methods which are human
interpretable and for which safety can be formally verified. Existing goal
recognition methods for autonomous vehicles fail to satisfy all four objectives
of being fast, accurate, interpretable and verifiable. We propose Goal
Recognition with Interpretable Trees (GRIT), a goal recognition system for
autonomous vehicles which achieves these objectives. GRIT makes use of decision
trees trained on vehicle trajectory data. Evaluation on two vehicle trajectory
datasets demonstrates the inference speed and accuracy of GRIT compared to an
ablation and two deep learning baselines. We show that the learned trees are
human interpretable and demonstrate how properties of GRIT can be formally
verified using an SMT solver.
</p>
<a href="http://arxiv.org/abs/2103.06113" target="_blank">arXiv:2103.06113</a> [<a href="http://arxiv.org/pdf/2103.06113" target="_blank">pdf</a>]

<h2>Spatially Consistent Representation Learning. (arXiv:2103.06122v1 [cs.CV])</h2>
<h3>Byungseok Roh, Wuhyun Shin, Ildoo Kim, Sungwoong Kim</h3>
<p>Self-supervised learning has been widely used to obtain transferrable
representations from unlabeled images. Especially, recent contrastive learning
methods have shown impressive performances on downstream image classification
tasks. While these contrastive methods mainly focus on generating invariant
global representations at the image-level under semantic-preserving
transformations, they are prone to overlook spatial consistency of local
representations and therefore have a limitation in pretraining for localization
tasks such as object detection and instance segmentation. Moreover,
aggressively cropped views used in existing contrastive methods can minimize
representation distances between the semantically different regions of a single
image.

In this paper, we propose a spatially consistent representation learning
algorithm (SCRL) for multi-object and location-specific tasks. In particular,
we devise a novel self-supervised objective that tries to produce coherent
spatial representations of a randomly cropped local region according to
geometric translations and zooming operations. On various downstream
localization tasks with benchmark datasets, the proposed SCRL shows significant
performance improvements over the image-level supervised pretraining as well as
the state-of-the-art self-supervised learning methods.
</p>
<a href="http://arxiv.org/abs/2103.06122" target="_blank">arXiv:2103.06122</a> [<a href="http://arxiv.org/pdf/2103.06122" target="_blank">pdf</a>]

<h2>Sim2Real 3D Object Classification using Spherical Kernel Point Convolution and a Deep Center Voting Scheme. (arXiv:2103.06134v1 [cs.CV])</h2>
<h3>Jean-Baptiste Weibel, Timothy Patten, Markus Vincze</h3>
<p>While object semantic understanding is essential for most service robotic
tasks, 3D object classification is still an open problem. Learning from
artificial 3D models alleviates the cost of annotation necessary to approach
this problem, but most methods still struggle with the differences existing
between artificial and real 3D data. We conjecture that the cause of those
issue is the fact that many methods learn directly from point coordinates,
instead of the shape, as the former is hard to center and to scale under
variable occlusions reliably. We introduce spherical kernel point convolutions
that directly exploit the object surface, represented as a graph, and a voting
scheme to limit the impact of poor segmentation on the classification results.
Our proposed approach improves upon state-of-the-art methods by up to 36% when
transferring from artificial objects to real objects.
</p>
<a href="http://arxiv.org/abs/2103.06134" target="_blank">arXiv:2103.06134</a> [<a href="http://arxiv.org/pdf/2103.06134" target="_blank">pdf</a>]

<h2>Non-Holonomic RRT & MPC: Path and Trajectory Planning for an Autonomous Cycle Rickshaw. (arXiv:2103.06141v1 [cs.RO])</h2>
<h3>Damir Bojad&#x17e;i&#x107;, Julian Kunze, Dinko Osmankovi&#x107;, Mohammadhossein Malmir, Alois Knoll</h3>
<p>This paper presents a novel hierarchical motion planning approach based on
Rapidly-Exploring Random Trees (RRT) for global planning and Model Predictive
Control (MPC) for local planning. The approach targets a three-wheeled cycle
rickshaw (trishaw) used for autonomous urban transportation in shared spaces.
Due to the nature of the vehicle, the algorithms had to be adapted in order to
adhere to non-holonomic kinematic constraints using the Kinematic Single-Track
Model.

The vehicle is designed to offer transportation for people and goods in
shared environments such as roads, sidewalks, bicycle lanes but also open
spaces that are often occupied by other traffic participants. Therefore, the
algorithm presented in this paper needs to anticipate and avoid dynamic
obstacles, such as pedestrians or bicycles, but also be fast enough in order to
work in real-time so that it can adapt to changes in the environment. Our
approach uses an RRT variant for global planning that has been modified for
single-track kinematics and improved by exploiting dead-end nodes. This allows
us to compute global paths in unstructured environments very fast. In a second
step, our MPC-based local planner makes use of the global path to compute the
vehicle's trajectory while incorporating dynamic obstacles such as pedestrians
and other road users.

Our approach has shown to work both in simulation as well as first real-life
tests and can be easily extended for more sophisticated behaviors.
</p>
<a href="http://arxiv.org/abs/2103.06141" target="_blank">arXiv:2103.06141</a> [<a href="http://arxiv.org/pdf/2103.06141" target="_blank">pdf</a>]

<h2>Adversarial Regression Learning for Bone Age Estimation. (arXiv:2103.06149v1 [cs.CV])</h2>
<h3>Youshan Zhang, Brian D. Davison</h3>
<p>Estimation of bone age from hand radiographs is essential to determine
skeletal age in diagnosing endocrine disorders and depicting the growth status
of children. However, existing automatic methods only apply their models to
test images without considering the discrepancy between training samples and
test samples, which will lead to a lower generalization ability. In this paper,
we propose an adversarial regression learning network (ARLNet) for bone age
estimation. Specifically, we first extract bone features from a fine-tuned
Inception V3 neural network and propose regression percentage loss for
training. To reduce the discrepancy between training and test data, we then
propose adversarial regression loss and feature reconstruction loss to
guarantee the transition from training data to test data and vice versa,
preserving invariant features from both training and test data. Experimental
results show that the proposed model outperforms state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2103.06149" target="_blank">arXiv:2103.06149</a> [<a href="http://arxiv.org/pdf/2103.06149" target="_blank">pdf</a>]

<h2>Regressive Domain Adaptation for Unsupervised Keypoint Detection. (arXiv:2103.06175v1 [cs.CV])</h2>
<h3>Junguang Jiang, Yifei Ji, Ximei Wang, Yufeng Liu, Jianmin Wang, Mingsheng Long</h3>
<p>Domain adaptation (DA) aims at transferring knowledge from a labeled source
domain to an unlabeled target domain. Though many DA theories and algorithms
have been proposed, most of them are tailored into classification settings and
may fail in regression tasks, especially in the practical keypoint detection
task. To tackle this difficult but significant task, we present a method of
regressive domain adaptation (RegDA) for unsupervised keypoint detection.
Inspired by the latest theoretical work, we first utilize an adversarial
regressor to maximize the disparity on the target domain and train a feature
generator to minimize this disparity. However, due to the high dimension of the
output space, this regressor fails to detect samples that deviate from the
support of the source. To overcome this problem, we propose two important
ideas. First, based on our observation that the probability density of the
output space is sparse, we introduce a spatial probability distribution to
describe this sparsity and then use it to guide the learning of the adversarial
regressor. Second, to alleviate the optimization difficulty in the
high-dimensional space, we innovatively convert the minimax game in the
adversarial training to the minimization of two opposite goals. Extensive
experiments show that our method brings large improvement by 8% to 11% in terms
of PCK on different datasets.
</p>
<a href="http://arxiv.org/abs/2103.06175" target="_blank">arXiv:2103.06175</a> [<a href="http://arxiv.org/pdf/2103.06175" target="_blank">pdf</a>]

<h2>Towards Learning an Unbiased Classifier from Biased Data via Conditional Adversarial Debiasing. (arXiv:2103.06179v1 [cs.CV])</h2>
<h3>Christian Reimers, Paul Bodesheim, Jakob Runge, Joachim Denzler</h3>
<p>Bias in classifiers is a severe issue of modern deep learning methods,
especially for their application in safety- and security-critical areas. Often,
the bias of a classifier is a direct consequence of a bias in the training
dataset, frequently caused by the co-occurrence of relevant features and
irrelevant ones. To mitigate this issue, we require learning algorithms that
prevent the propagation of bias from the dataset into the classifier. We
present a novel adversarial debiasing method, which addresses a feature that is
spuriously connected to the labels of training images but statistically
independent of the labels for test images. Thus, the automatic identification
of relevant features during training is perturbed by irrelevant features. This
is the case in a wide range of bias-related problems for many computer vision
tasks, such as automatic skin cancer detection or driver assistance. We argue
by a mathematical proof that our approach is superior to existing techniques
for the abovementioned bias. Our experiments show that our approach performs
better than state-of-the-art techniques on a well-known benchmark dataset with
real-world images of cats and dogs.
</p>
<a href="http://arxiv.org/abs/2103.06179" target="_blank">arXiv:2103.06179</a> [<a href="http://arxiv.org/pdf/2103.06179" target="_blank">pdf</a>]

<h2>Dynamical Pose Estimation. (arXiv:2103.06182v1 [cs.CV])</h2>
<h3>Heng Yang, Chris Doran, Jean-Jacques Slotine</h3>
<p>We study the problem of aligning two sets of 3D geometric primitives given
known correspondences. Our first contribution is to show that this primitive
alignment framework unifies five perception problems including point cloud
registration, primitive (mesh) registration, category-level 3D registration,
absolution pose estimation (APE), and category-level APE. Our second
contribution is to propose DynAMical Pose estimation (DAMP), the first general
and practical algorithm to solve primitive alignment problem by simulating
rigid body dynamics arising from virtual springs and damping, where the springs
span the shortest distances between corresponding primitives. Our third
contribution is to apply DAMP to the five perception problems in simulated and
real datasets and demonstrate (i) DAMP always converges to the globally optimal
solution in the first three problems with 3D-3D correspondences; (ii) although
DAMP sometimes converges to suboptimal solutions in the last two problems with
2D-3D correspondences, with a simple scheme for escaping local minima, DAMP
almost always succeeds. Our last contribution is to demystify the surprising
empirical performance of DAMP and formally prove a global convergence result in
the case of point cloud registration by charactering local stability of the
equilibrium points of the underlying dynamical system.
</p>
<a href="http://arxiv.org/abs/2103.06182" target="_blank">arXiv:2103.06182</a> [<a href="http://arxiv.org/pdf/2103.06182" target="_blank">pdf</a>]

<h2>A Study of Face Obfuscation in ImageNet. (arXiv:2103.06191v1 [cs.CV])</h2>
<h3>Kaiyu Yang, Jacqueline Yau, Li Fei-Fei, Jia Deng, Olga Russakovsky</h3>
<p>Image obfuscation (blurring, mosaicing, etc.) is widely used for privacy
protection. However, computer vision research often overlooks privacy by
assuming access to original unobfuscated images. In this paper, we explore
image obfuscation in the ImageNet challenge.

Most categories in the ImageNet challenge are not people categories;
nevertheless, many incidental people are in the images, whose privacy is a
concern. We first annotate faces in the dataset. Then we investigate how face
blurring -- a typical obfuscation technique -- impacts classification accuracy.
We benchmark various deep neural networks on face-blurred images and observe a
disparate impact on different categories. Still, the overall accuracy only
drops slightly ($\leq 0.68\%$), demonstrating that we can train privacy-aware
visual classifiers with minimal impact on accuracy. Further, we experiment with
transfer learning to 4 downstream tasks: object recognition, scene recognition,
face attribute classification, and object detection. Results show that features
learned on face-blurred images are equally transferable. Data and code are
available at https://github.com/princetonvisualai/imagenet-face-obfuscation.
</p>
<a href="http://arxiv.org/abs/2103.06191" target="_blank">arXiv:2103.06191</a> [<a href="http://arxiv.org/pdf/2103.06191" target="_blank">pdf</a>]

<h2>Real-Time RGBD Odometry for Fused-State Navigation Systems. (arXiv:2103.06236v1 [cs.RO])</h2>
<h3>Andrew R. Willis, Kevin M. Brink</h3>
<p>This article describes an algorithm that provides visual odometry estimates
from sequential pairs of RGBD images. The key contribution of this article on
RGBD odometry is that it provides both an odometry estimate and a covariance
for the odometry parameters in real-time via a representative covariance
matrix. Accurate, real-time parameter covariance is essential to effectively
fuse odometry measurements into most navigation systems. To date, this topic
has seen little treatment in research which limits the impact existing RGBD
odometry approaches have for localization in these systems. Covariance
estimates are obtained via a statistical perturbation approach motivated by
real-world models of RGBD sensor measurement noise. Results discuss the
accuracy of our RGBD odometry approach with respect to ground truth obtained
from a motion capture system and characterizes the suitability of this approach
for estimating the true RGBD odometry parameter uncertainty.
</p>
<a href="http://arxiv.org/abs/2103.06236" target="_blank">arXiv:2103.06236</a> [<a href="http://arxiv.org/pdf/2103.06236" target="_blank">pdf</a>]

<h2>Grasp Stability Analysis with Passive Reactions. (arXiv:2103.06252v1 [cs.RO])</h2>
<h3>Maximilian Haas-Heger</h3>
<p>[...] We argue that the traditional grasp modeling theory assumes a
complexity that most robotic hands do not possess and is therefore of limited
applicability to the robotic hands commonly used today. We discuss these
limitations of the existing grasp modeling literature and set out to develop
our own tools for the analysis of passive effects in robotic grasping. We show
that problems of this kind are difficult to solve due to the non-convexity of
the Maximum Dissipation Principle (MDP), which is part of the Coulomb friction
law. We show that for planar grasps the MDP can be decomposed into a number of
piecewise convex problems, which can be solved for efficiently. [...] Thus, we
present the first polynomial runtime algorithm for the determination of passive
stability of planar grasps. For the spacial case we [...] describe a convex
relaxation of the Coulomb friction law and a successive hierarchical tightening
approach that allows us to find solutions to the exact problem including the
maximum dissipation principle. [...] The generality of our grasp model allows
us to solve a wide variety of problems such as the computation of optimal
actuator commands. This makes our framework a valuable tool for practical
manipulation applications. Our work is relevant beyond robotic manipulation as
it applies to the stability of any assembly of rigid bodies with frictional
contacts, unilateral constraints and externally applied wrenches. Finally, we
argue that with the advent of data-driven methods as well as the emergence of a
new generation of highly sensorized hands there are opportunities for the
application of the traditional grasp modeling theory to fields such as robotic
in-hand manipulation through model-free reinforcement learning. We present a
method that applies traditional grasp models to maintain quasi-static stability
throughout a nominally model-free reinforcement learning task. [...]
</p>
<a href="http://arxiv.org/abs/2103.06252" target="_blank">arXiv:2103.06252</a> [<a href="http://arxiv.org/pdf/2103.06252" target="_blank">pdf</a>]

<h2>Involution: Inverting the Inherence of Convolution for Visual Recognition. (arXiv:2103.06255v1 [cs.CV])</h2>
<h3>Duo Li, Jie Hu, Changhu Wang, Xiangtai Li, Qi She, Lei Zhu, Tong Zhang, Qifeng Chen</h3>
<p>Convolution has been the core ingredient of modern neural networks,
triggering the surge of deep learning in vision. In this work, we rethink the
inherent principles of standard convolution for vision tasks, specifically
spatial-agnostic and channel-specific. Instead, we present a novel atomic
operation for deep neural networks by inverting the aforementioned design
principles of convolution, coined as involution. We additionally demystify the
recent popular self-attention operator and subsume it into our involution
family as an over-complicated instantiation. The proposed involution operator
could be leveraged as fundamental bricks to build the new generation of neural
networks for visual recognition, powering different deep learning models on
several prevalent benchmarks, including ImageNet classification, COCO detection
and segmentation, together with Cityscapes segmentation. Our involution-based
models improve the performance of convolutional baselines using ResNet-50 by up
to 1.6% top-1 accuracy, 2.5% and 2.4% bounding box AP, and 4.7% mean IoU
absolutely while compressing the computational cost to 66%, 65%, 72%, and 57%
on the above benchmarks, respectively. Code and pre-trained models for all the
tasks are available at https://github.com/d-li14/involution.
</p>
<a href="http://arxiv.org/abs/2103.06255" target="_blank">arXiv:2103.06255</a> [<a href="http://arxiv.org/pdf/2103.06255" target="_blank">pdf</a>]

<h2>MAT-Fly: An Educational Platform for Simulating Unmanned Aerial Vehicles Aimed to Detect and Track Moving Objects. (arXiv:1904.00378v4 [cs.RO] UPDATED)</h2>
<h3>Giuseppe Silano, Luigi Iannelli</h3>
<p>The main motivation of this work is to propose a simulation approach for a
specific task within the Unmanned Aerial Vehicle (UAV) field, i.e., the visual
detection and tracking of arbitrary moving objects. In particular, it is
described MAT-Fly, a numerical simulation platform for multi-rotor aircraft
characterized by the ease of use and control development. The platform is based
on Matlab and the MathWorks Virtual Reality (VR) and Computer Vision System
(CVS) toolboxes that work together to simulate the behavior of a quad-rotor
while tracking a car that moves along a nontrivial path. The VR toolbox has
been chosen due to the familiarity that students have with Matlab and because
it does not require a notable effort by the user for the learning and
development phase thanks to its simple structure. The overall architecture is
quite modular so that each block can be easily replaced with others simplifying
the code reuse and the platform customization. Some simple testbeds are
presented to show the validity of the approach and how the platform works. The
simulator is released as open-source, making it possible to go through any part
of the system, and available for educational purposes.
</p>
<a href="http://arxiv.org/abs/1904.00378" target="_blank">arXiv:1904.00378</a> [<a href="http://arxiv.org/pdf/1904.00378" target="_blank">pdf</a>]

<h2>A Two-step Calibration Method for Unfocused Light Field Camera Based on Projection Model Analysis. (arXiv:2001.03734v2 [cs.CV] UPDATED)</h2>
<h3>Dongyang Jin, Saiping Zhang, Xiao Huo, Wei Zhang, Fuzheng Yang</h3>
<p>Accurately calibrating light field camera is essential to its applications.
Rapid progress has been made in this area in the past decades. In this paper,
detailed analysis was first performed towards the state of the art projection
models for calibration which were further interpreted in three representations,
including the correspondence between rays and pixels, 3D physical points and
pixels and between 3D physical points and 3D signal structure of the captured
light field. Based on the analysis, parameters in the projection model were
grouped into direction parameter set and depth parameter set. A two-step
calibration method was then proposed with each step dealing with each set of
parameters. The proposed method is able to reuse traditional camera calibration
methods for the direction parameter set. A simply raw image-based calibration
of depth parameter set was further proposed. Systematic validations were
conducted to evaluate the performance of the proposed calibration method.
Experimental results show that the accuracy and robustness of the proposed
method outperforms its counterparts under various benchmark criteria.
</p>
<a href="http://arxiv.org/abs/2001.03734" target="_blank">arXiv:2001.03734</a> [<a href="http://arxiv.org/pdf/2001.03734" target="_blank">pdf</a>]

<h2>GraphTCN: Spatio-Temporal Interaction Modeling for Human Trajectory Prediction. (arXiv:2003.07167v6 [cs.CV] UPDATED)</h2>
<h3>Chengxin Wang, Shaofeng Cai, Gary Tan</h3>
<p>Predicting the future paths of an agent's neighbors accurately and in a
timely manner is central to the autonomous applications for collision
avoidance. Conventional approaches, e.g., LSTM-based models, take considerable
computational costs in the prediction, especially for the long sequence
prediction. To support more efficient and accurate trajectory predictions, we
propose a novel CNN-based spatial-temporal graph framework GraphTCN, which
models the spatial interactions as social graphs and captures the
spatio-temporal interactions with a modified temporal convolutional network. In
contrast to conventional models, both the spatial and temporal modeling of our
model are computed within each local time window. Therefore, it can be executed
in parallel for much higher efficiency, and meanwhile with accuracy comparable
to best-performing approaches. Experimental results confirm that our model
achieves better performance in terms of both efficiency and accuracy as
compared with state-of-the-art models on various trajectory prediction
benchmark datasets.
</p>
<a href="http://arxiv.org/abs/2003.07167" target="_blank">arXiv:2003.07167</a> [<a href="http://arxiv.org/pdf/2003.07167" target="_blank">pdf</a>]

<h2>Control and Trajectory Optimization for Soft Aerial Manipulation. (arXiv:2004.04238v2 [cs.RO] UPDATED)</h2>
<h3>Joshua Fishman, Luca Carlone</h3>
<p>Manipulation and grasping with unmanned aerial vehicles (UAVs) currently
require accurate positioning and are often executed at reduced speed to ensure
successful grasps. This is due to the fact that typical UAVs can only
accommodate rigid manipulators with few degrees of freedom, which limits their
capability to compensate for disturbances caused by the vehicle positioning
errors. Moreover, UAVs have to minimize external contact forces in order to
maintain stability. Biological systems, on the other hand, exploit softness to
overcome similar limitations, and leverage compliance to enable aggressive
grasping. This paper investigates control and trajectory optimization for a
soft aerial manipulator, consisting of a quadrotor and a tendon-actuated soft
gripper, in which the advantages of softness can be fully exploited. To the
best of our knowledge, this is the first work at the intersection between soft
manipulation and UAV control. We present a decoupled approach for the quadrotor
and the soft gripper, combining (i) a geometric controller and a minimum-snap
trajectory optimization for the quadrotor (rigid) base, with (ii) a
quasi-static finite element model and control-space interpolation for the soft
gripper. We prove that the geometric controller asymptotically stabilizes the
quadrotor velocity and attitude despite the addition of the soft load. Finally,
we evaluate the proposed system in a realistic soft dynamics simulator, and
show that: (i) the geometric controller is fairly insensitive to the soft
payload, (ii) the platform can reliably grasp unknown objects despite
inaccurate positioning and initial conditions, and (iii) the decoupled
controller is amenable for real-time execution.
</p>
<a href="http://arxiv.org/abs/2004.04238" target="_blank">arXiv:2004.04238</a> [<a href="http://arxiv.org/pdf/2004.04238" target="_blank">pdf</a>]

<h2>Human in Events: A Large-Scale Benchmark for Human-centric Video Analysis in Complex Events. (arXiv:2005.04490v3 [cs.CV] UPDATED)</h2>
<h3>Weiyao Lin, Huabin Liu, Shizhan Liu, Yuxi Li, Guo-Jun Qi, Rui Qian, Tao Wang, Nicu Sebe, Ning Xu, Hongkai Xiong, Mubarak Shah</h3>
<p>Along with the development of modern smart cities, human-centric video
analysis has been encountering the challenge of analyzing diverse and complex
events in real scenes. A complex event relates to dense crowds, anomalous, or
collective behaviors. However, limited by the scale of existing video datasets,
few human analysis approaches have reported their performance on such complex
events. To this end, we present a new large-scale dataset, named
Human-in-Events or HiEve (Human-centric video analysis in complex Events), for
the understanding of human motions, poses, and actions in a variety of
realistic events, especially in crowd and complex events. It contains a record
number of poses (&gt;1M), the largest number of action instances (&gt;56k) under
complex events, as well as one of the largest numbers of trajectories lasting
for longer time (with an average trajectory length of &gt;480 frames). Based on
this dataset, we present an enhanced pose estimation baseline by utilizing the
potential of action information to guide the learning of more powerful 2D pose
features. We demonstrate that the proposed method is able to boost the
performance of existing pose estimation pipelines on our HiEve dataset.
Furthermore, we conduct extensive experiments to benchmark recent video
analysis approaches together with our baseline methods, demonstrating that
HiEve is a challenging dataset for human-centric video analysis. We expect that
the dataset will advance the development of cutting-edge techniques in
human-centric analysis and the understanding of complex events. The dataset is
available at this http URL
</p>
<a href="http://arxiv.org/abs/2005.04490" target="_blank">arXiv:2005.04490</a> [<a href="http://arxiv.org/pdf/2005.04490" target="_blank">pdf</a>]

<h2>TOAN: Target-Oriented Alignment Network for Fine-Grained Image Categorization with Few Labeled Samples. (arXiv:2005.13820v2 [cs.CV] UPDATED)</h2>
<h3>Huaxi Huang, Junjie Zhang, Jian Zhang, Qiang Wu, Chang Xu</h3>
<p>The challenges of high intra-class variance yet low inter-class fluctuations
in fine-grained visual categorization are more severe with few labeled samples,
\textit{i.e.,} Fine-Grained categorization problems under the Few-Shot setting
(FGFS). High-order features are usually developed to uncover subtle differences
between sub-categories in FGFS, but they are less effective in handling the
high intra-class variance. In this paper, we propose a Target-Oriented
Alignment Network (TOAN) to investigate the fine-grained relation between the
target query image and support classes. The feature of each support image is
transformed to match the query ones in the embedding feature space, which
reduces the disparity explicitly within each category. Moreover, different from
existing FGFS approaches devise the high-order features over the global image
with less explicit consideration of discriminative parts, we generate
discriminative fine-grained features by integrating compositional concept
representations to global second-order pooling. Extensive experiments are
conducted on four fine-grained benchmarks to demonstrate the effectiveness of
TOAN compared with the state-of-the-art models.
</p>
<a href="http://arxiv.org/abs/2005.13820" target="_blank">arXiv:2005.13820</a> [<a href="http://arxiv.org/pdf/2005.13820" target="_blank">pdf</a>]

<h2>Ref-NMS: Breaking Proposal Bottlenecks in Two-Stage Referring Expression Grounding. (arXiv:2009.01449v3 [cs.CV] UPDATED)</h2>
<h3>Long Chen, Wenbo Ma, Jun Xiao, Hanwang Zhang, Shih-Fu Chang</h3>
<p>The prevailing framework for solving referring expression grounding is based
on a two-stage process: 1) detecting proposals with an object detector and 2)
grounding the referent to one of the proposals. Existing two-stage solutions
mostly focus on the grounding step, which aims to align the expressions with
the proposals. In this paper, we argue that these methods overlook an obvious
mismatch between the roles of proposals in the two stages: they generate
proposals solely based on the detection confidence (i.e., expression-agnostic),
hoping that the proposals contain all right instances in the expression (i.e.,
expression-aware). Due to this mismatch, current two-stage methods suffer from
a severe performance drop between detected and ground-truth proposals. To this
end, we propose Ref-NMS, which is the first method to yield expression-aware
proposals at the first stage. Ref-NMS regards all nouns in the expression as
critical objects, and introduces a lightweight module to predict a score for
aligning each box with a critical object. These scores can guide the NMS
operation to filter out the boxes irrelevant to the expression, increasing the
recall of critical objects, resulting in a significantly improved grounding
performance. Since Ref- NMS is agnostic to the grounding step, it can be easily
integrated into any state-of-the-art two-stage method. Extensive ablation
studies on several backbones, benchmarks, and tasks consistently demonstrate
the superiority of Ref-NMS. Codes are available at:
https://github.com/ChopinSharp/ref-nms.
</p>
<a href="http://arxiv.org/abs/2009.01449" target="_blank">arXiv:2009.01449</a> [<a href="http://arxiv.org/pdf/2009.01449" target="_blank">pdf</a>]

<h2>A Self-Supervised Gait Encoding Approach with Locality-Awareness for 3D Skeleton Based Person Re-Identification. (arXiv:2009.03671v2 [cs.CV] UPDATED)</h2>
<h3>Haocong Rao, Siqi Wang, Xiping Hu, Mingkui Tan, Yi Guo, Jun Cheng, Bin Hu, Xinwang Liu</h3>
<p>Person re-identification (Re-ID) via gait features within 3D skeleton
sequences is a newly-emerging topic with several advantages. Existing solutions
either rely on hand-crafted descriptors or supervised gait representation
learning. This paper proposes a self-supervised gait encoding approach that can
leverage unlabeled skeleton data to learn gait representations for person
Re-ID. Specifically, we first create self-supervision by learning to
reconstruct unlabeled skeleton sequences reversely, which involves richer
high-level semantics to obtain better gait representations. Other pretext tasks
are also explored to further improve self-supervised learning. Second, inspired
by the fact that motion's continuity endows adjacent skeletons in one skeleton
sequence and temporally consecutive skeleton sequences with higher correlations
(referred as locality in 3D skeleton data), we propose a locality-aware
attention mechanism and a locality-aware contrastive learning scheme, which aim
to preserve locality-awareness on intra-sequence level and inter-sequence level
respectively during self-supervised learning. Last, with context vectors
learned by our locality-aware attention mechanism and contrastive learning
scheme, a novel feature named Constrastive Attention-based Gait Encodings
(CAGEs) is designed to represent gait effectively. Empirical evaluations show
that our approach significantly outperforms skeleton-based counterparts by
15-40% Rank-1 accuracy, and it even achieves superior performance to numerous
multi-modal methods with extra RGB or depth information. Our codes are
available at https://github.com/Kali-Hac/Locality-Awareness-SGE.
</p>
<a href="http://arxiv.org/abs/2009.03671" target="_blank">arXiv:2009.03671</a> [<a href="http://arxiv.org/pdf/2009.03671" target="_blank">pdf</a>]

<h2>UAV Path Planning using Global and Local Map Information with Deep Reinforcement Learning. (arXiv:2010.06917v3 [cs.RO] UPDATED)</h2>
<h3>Mirco Theile, Harald Bayerlein, Richard Nai, David Gesbert, Marco Caccamo</h3>
<p>Path planning methods for autonomous unmanned aerial vehicles (UAVs) are
typically designed for one specific type of mission. In this work, we present a
method for autonomous UAV path planning based on deep reinforcement learning
(DRL) that can be applied to a wide range of mission scenarios. Specifically,
we compare coverage path planning (CPP), where the UAV's goal is to survey an
area of interest to data harvesting (DH), where the UAV collects data from
distributed Internet of Things (IoT) sensor devices. By exploiting structured
map information of the environment, we train double deep Q-networks (DDQNs)
with identical architectures on both distinctly different mission scenarios, to
make movement decisions that balance the respective mission goal with
navigation constraints. By introducing a novel approach exploiting a compressed
global map of the environment combined with a cropped but uncompressed local
map showing the vicinity of the UAV agent, we demonstrate that the proposed
method can efficiently scale to large environments. We also extend previous
results for generalizing control policies that require no retraining when
scenario parameters change and offer a detailed analysis of crucial map
processing parameters' effects on path planning performance.
</p>
<a href="http://arxiv.org/abs/2010.06917" target="_blank">arXiv:2010.06917</a> [<a href="http://arxiv.org/pdf/2010.06917" target="_blank">pdf</a>]

<h2>Face Frontalization Based on Robustly Fitting a Deformable Shape Model to 3D Landmarks. (arXiv:2010.13676v2 [cs.CV] UPDATED)</h2>
<h3>Zhiqi Kang, Mostafa Sadeghi, Radu Horaud</h3>
<p>Face frontalization consists of synthesizing a frontally-viewed face from an
arbitrarily-viewed one. The main contribution of this paper is a robust face
alignment method that enables pixel-to-pixel warping. The method simultaneously
estimates the rigid transformation (scale, rotation, and translation) and the
non-rigid deformation between two 3D point sets: a set of 3D landmarks
extracted from an arbitrary-viewed face, and a set of 3D landmarks
parameterized by a frontally-viewed deformable face model. An important merit
of the proposed method is its ability to deal both with noise (small
perturbations) and with outliers (large errors). We propose to model inliers
and outliers with the generalized Student's t-probability distribution
function, a heavy-tailed distribution that is immune to non-Gaussian errors in
the data. We describe in detail the associated expectation-maximization (EM)
algorithm that alternates between the estimation of (i) the rigid parameters,
(ii) the deformation parameters, and (iii) the Student-t distribution
parameters. We also propose to use the zero-mean normalized cross-correlation,
between a frontalized face and the corresponding ground-truth frontally-viewed
face, to evaluate the performance of frontalization. To this end, we use a
dataset that contains pairs of profile-viewed and frontally-viewed faces. This
evaluation, based on direct image-to-image comparison, stands in contrast with
indirect evaluation, based on analyzing the effect of frontalization on face
recognition.
</p>
<a href="http://arxiv.org/abs/2010.13676" target="_blank">arXiv:2010.13676</a> [<a href="http://arxiv.org/pdf/2010.13676" target="_blank">pdf</a>]

<h2>ORBBuf: A Robust Buffering Method for Remote Visual SLAM. (arXiv:2010.14861v3 [cs.RO] UPDATED)</h2>
<h3>Yu-Ping Wang, Zi-Xin Zou, Cong Wang, Yue-Jiang Dong, Lei Qiao, Dinesh Manocha</h3>
<p>The data loss caused by unreliable network seriously impacts the results of
remote visual SLAM systems. From our experiment, a loss of less than 1 second
of data can cause a visual SLAM algorithm to lose tracking. We present a novel
buffering method, ORBBuf, to reduce the impact of data loss on remote visual
SLAM systems. We model the buffering problem as an optimization problem by
introducing a similarity metric between frames. To solve the buffering problem,
we present an efficient greedy-like algorithm to discard the frames that have
the least impact on the quality of SLAM results. We implement our ORBBuf method
on ROS, a widely used middleware framework. Through an extensive evaluation on
real-world scenarios and tens of gigabytes of datasets, we demonstrate that our
ORBBuf method can be applied to different state-estimation algorithms (DSO and
VINS-Fusion), different sensor data (both monocular images and stereo images),
different scenes (both indoor and outdoor), and different network environments
(both WiFi networks and 4G networks). Our experimental results indicate that
the network losses indeed affect the SLAM results, and our ORBBuf method can
reduce the RMSE up to 50 times comparing with the Drop-Oldest and Random
buffering methods.
</p>
<a href="http://arxiv.org/abs/2010.14861" target="_blank">arXiv:2010.14861</a> [<a href="http://arxiv.org/pdf/2010.14861" target="_blank">pdf</a>]

<h2>Adaptive Force-based Control for Legged Robots. (arXiv:2011.06236v2 [cs.RO] UPDATED)</h2>
<h3>Yiyu Chen, Quan Nguyen</h3>
<p>In this paper, we present a novel methodology to introduce adaptive control
for force-based control systems, with application to legged robots. In our
approach, the reference model is based on the quadratic program force control.
We evaluate our proposed control design on a high-fidelity physical simulation
of LASER, a dynamic quadruped robot. Our proposed method guarantees
input-to-state stability and is successfully validated for the problem of
quadruped robots walking on rough terrain while carrying unknown and
time-varying loads.
</p>
<a href="http://arxiv.org/abs/2011.06236" target="_blank">arXiv:2011.06236</a> [<a href="http://arxiv.org/pdf/2011.06236" target="_blank">pdf</a>]

<h2>Integration of Fully-Actuated Multirotors into Real-World Applications. (arXiv:2011.06666v2 [cs.RO] UPDATED)</h2>
<h3>Azarakhsh Keipour, Mohammadreza Mousaei, Andrew T Ashley, Sebastian Scherer</h3>
<p>The introduction of fully-actuated multirotors has opened the door to new
possibilities and more efficient solutions to many real-world applications.
However, their integration had been slower than expected, partly due to the
need for new tools to take full advantage of these robots.

As far as we know, all the groups currently working on the fully-actuated
multirotors develop new full-pose (6-D) tools and methods to use their robots,
which is inefficient, time-consuming, and requires many resources.

We propose a way of bridging the gap between the tools already available for
underactuated robots and the new fully-actuated vehicles. The approach can
extend the existing underactuated flight controllers to support the
fully-actuated robots, or enhance the existing fully-actuated controllers to
support existing underactuated flight stacks. We introduce attitude strategies
that work with the underactuated controllers, tools, planners and remote
control interfaces, all while allowing taking advantage of the full actuation.
Moreover, new methods are proposed that can properly handle the limited lateral
thrust suffered by many fully-actuated UAV designs. The strategies are
lightweight, simple, and allow rapid integration of the available tools with
these new vehicles for the fast development of new real-world applications.

The real experiments on our robots and simulations on several UAV
architectures with different underlying controller methods show how these
strategies can be utilized to extend existing flight controllers for
fully-actuated applications. We have provided the source code for the PX4
firmware enhanced with our proposed methods to showcase an example flight
controller for underactuated multirotors that can be modified to seamlessly
support fully-actuated vehicles while retaining the rest of the flight stack
unchanged.
</p>
<a href="http://arxiv.org/abs/2011.06666" target="_blank">arXiv:2011.06666</a> [<a href="http://arxiv.org/pdf/2011.06666" target="_blank">pdf</a>]

<h2>RGBT Tracking via Multi-Adapter Network with Hierarchical Divergence Loss. (arXiv:2011.07189v2 [cs.CV] UPDATED)</h2>
<h3>Andong Lu, Chenglong Li, Yuqing Yan, Jin Tang, Bin Luo</h3>
<p>RGBT tracking has attracted increasing attention since RGB and thermal
infrared data have strong complementary advantages, which could make trackers
all-day and all-weather work. However, how to effectively represent RGBT data
for visual tracking remains unstudied well. Existing works usually focus on
extracting modality-shared or modality-specific information, but the potentials
of these two cues are not well explored and exploited in RGBT tracking. In this
paper, we propose a novel multi-adapter network to jointly perform
modality-shared, modality-specific and instance-aware target representation
learning for RGBT tracking. To this end, we design three kinds of adapters
within an end-to-end deep learning framework. In specific, we use the modified
VGG-M as the generality adapter to extract the modality-shared target
representations.To extract the modality-specific features while reducing the
computational complexity, we design a modality adapter, which adds a small
block to the generality adapter in each layer and each modality in a parallel
manner. Such a design could learn multilevel modality-specific representations
with a modest number of parameters as the vast majority of parameters are
shared with the generality adapter. We also design instance adapter to capture
the appearance properties and temporal variations of a certain target.
Moreover, to enhance the shared and specific features, we employ the loss of
multiple kernel maximum mean discrepancy to measure the distribution divergence
of different modal features and integrate it into each layer for more robust
representation learning. Extensive experiments on two RGBT tracking benchmark
datasets demonstrate the outstanding performance of the proposed tracker
against the state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2011.07189" target="_blank">arXiv:2011.07189</a> [<a href="http://arxiv.org/pdf/2011.07189" target="_blank">pdf</a>]

<h2>Learning Navigation Skills for Legged Robots with Learned Robot Embeddings. (arXiv:2011.12255v2 [cs.RO] UPDATED)</h2>
<h3>Joanne Truong, Denis Yarats, Tianyu Li, Franziska Meier, Sonia Chernova, Dhruv Batra, Akshara Rai</h3>
<p>Recent work has shown results on learning navigation policies for idealized
cylinder agents in simulation and transferring them to real wheeled robots.
Deploying such navigation policies on legged robots can be challenging due to
their complex dynamics, and the large dynamical difference between cylinder
agents and legged systems. In this work, we learn hierarchical navigation
policies that account for the low-level dynamics of legged robots, such as
maximum speed, slipping, contacts, and learn to successfully navigate cluttered
indoor environments. To enable transfer of policies learned in simulation to
new legged robots and hardware, we learn dynamics-aware navigation policies
across multiple robots with robot-specific embeddings. The learned embedding is
optimized on new robots, while the rest of the policy is kept fixed, allowing
for quick adaptation. We train our policies across three legged robots in
simulation - 2 quadrupeds (A1, AlienGo) and a hexapod (Daisy). At test time, we
study the performance of our learned policy on two new legged robots in
simulation (Laikago, 4-legged Daisy), and one real-world quadrupedal robot
(A1). Our experiments show that our learned policy can sample-efficiently
generalize to previously unseen robots, and enable sim-to-real transfer of
navigation policies for legged robots.
</p>
<a href="http://arxiv.org/abs/2011.12255" target="_blank">arXiv:2011.12255</a> [<a href="http://arxiv.org/pdf/2011.12255" target="_blank">pdf</a>]

<h2>Human Haptic Gesture Interpretation for Robotic Systems. (arXiv:2012.01959v3 [cs.RO] UPDATED)</h2>
<h3>Elizabeth Bibit Bianchini, Prateek Verma, Kenneth Salisbury</h3>
<p>Physical human-robot interactions (pHRI) are less efficient and communicative
than human-human interactions, and a key reason is a lack of informative sense
of touch in robotic systems. Interpreting human touch gestures is a nuanced,
challenging task with extreme gaps between human and robot capability. Among
prior works that demonstrate human touch recognition capability, differences in
sensors, gesture classes, feature sets, and classification algorithms yield a
conglomerate of non-transferable results and a glaring lack of a standard. To
address this gap, this work presents 1) four proposed touch gesture classes
that cover the majority of the gesture characteristics identified in the
literature, 2) the collection of an extensive force dataset on a common pHRI
robotic arm with only its internal wrist force-torque sensor, and 3) an
exhaustive performance comparison of combinations of feature sets and
classification algorithms on this dataset. We demonstrate high classification
accuracies among our proposed gesture definitions on a test set, emphasizing
that neural network classifiers on the raw data outperform other combinations
of feature sets and algorithms.
</p>
<a href="http://arxiv.org/abs/2012.01959" target="_blank">arXiv:2012.01959</a> [<a href="http://arxiv.org/pdf/2012.01959" target="_blank">pdf</a>]

<h2>Strong but Simple Baseline with Dual-Granularity Triplet Loss for Visible-Thermal Person Re-Identification. (arXiv:2012.05010v2 [cs.CV] UPDATED)</h2>
<h3>Haijun Liu, Yanxia Chai, Xiaoheng Tan, Dong Li, Xichuan Zhou</h3>
<p>In this letter, we propose a conceptually simple and effective
dual-granularity triplet loss for visible-thermal person re-identification
(VT-ReID). In general, ReID models are always trained with the sample-based
triplet loss and identification loss from the fine granularity level. It is
possible when a center-based loss is introduced to encourage the intra-class
compactness and inter-class discrimination from the coarse granularity level.
Our proposed dual-granularity triplet loss well organizes the sample-based
triplet loss and center-based triplet loss in a hierarchical fine to coarse
granularity manner, just with some simple configurations of typical operations,
such as pooling and batch normalization. Experiments on RegDB and SYSU-MM01
datasets show that with only the global features our dual-granularity triplet
loss can improve the VT-ReID performance by a significant margin. It can be a
strong VT-ReID baseline to boost future research with high quality.
</p>
<a href="http://arxiv.org/abs/2012.05010" target="_blank">arXiv:2012.05010</a> [<a href="http://arxiv.org/pdf/2012.05010" target="_blank">pdf</a>]

<h2>Distributed Sensor Networks Deployed Using Soft Growing Robots. (arXiv:2012.07899v2 [cs.RO] UPDATED)</h2>
<h3>Alexander M. Gruebele, Andrew C. Zerbe, Margaret M. Coad, Allison M. Okamura, Mark R. Cutkosky</h3>
<p>Due to their ability to move without sliding relative to their environment,
soft growing robots are attractive for deploying distributed sensor networks in
confined spaces. Sensing of the state of such robots would also add to their
capabilities as human-safe, adaptable manipulators. However, incorporation of
distributed sensors onto soft growing robots is challenging because it requires
an interface between stiff and soft materials, and the sensor network needs to
undergo significant strain. In this work, we present a method for adding
sensors to soft growing robots that uses flexible printed circuit boards with
self-contained units of microcontrollers and sensors encased in a laminate
armor that protects them from unsafe curvatures. We demonstrate the ability of
this system to relay directional temperature and humidity information in
hard-to-access spaces. We also demonstrate and characterize a method for
sensing the growing robot shape using inertial measurement units deployed along
its length, and develop a mathematical model to predict its accuracy. This work
advances the capabilities of soft growing robots, as well as the field of soft
robot sensing.
</p>
<a href="http://arxiv.org/abs/2012.07899" target="_blank">arXiv:2012.07899</a> [<a href="http://arxiv.org/pdf/2012.07899" target="_blank">pdf</a>]

<h2>Shape My Face: Registering 3D Face Scans by Surface-to-Surface Translation. (arXiv:2012.09235v2 [cs.CV] UPDATED)</h2>
<h3>Mehdi Bahri, Eimear O&#x27; Sullivan, Shunwang Gong, Feng Liu, Xiaoming Liu, Michael M. Bronstein, Stefanos Zafeiriou</h3>
<p>Standard registration algorithms need to be independently applied to each
surface to register, following careful pre-processing and hand-tuning.
Recently, learning-based approaches have emerged that reduce the registration
of new scans to running inference with a previously-trained model. In this
paper, we cast the registration task as a surface-to-surface translation
problem, and design a model to reliably capture the latent geometric
information directly from raw 3D face scans. We introduce Shape-My-Face (SMF),
a powerful encoder-decoder architecture based on an improved point cloud
encoder, a novel visual attention mechanism, graph convolutional decoders with
skip connections, and a specialized mouth model that we smoothly integrate with
the mesh convolutions. Compared to the previous state-of-the-art learning
algorithms for non-rigid registration of face scans, SMF only requires the raw
data to be rigidly aligned (with scaling) with a pre-defined face template.
Additionally, our model provides topologically-sound meshes with minimal
supervision, offers faster training time, has orders of magnitude fewer
trainable parameters, is more robust to noise, and can generalize to previously
unseen datasets. We extensively evaluate the quality of our registrations on
diverse data. We demonstrate the robustness and generalizability of our model
with in-the-wild face scans across different modalities, sensor types, and
resolutions. Finally, we show that, by learning to register scans, SMF produces
a hybrid linear and non-linear morphable model. Manipulation of the latent
space of SMF allows for shape generation, and morphing applications such as
expression transfer in-the-wild. We train SMF on a dataset of human faces
comprising 9 large-scale databases on commodity hardware.
</p>
<a href="http://arxiv.org/abs/2012.09235" target="_blank">arXiv:2012.09235</a> [<a href="http://arxiv.org/pdf/2012.09235" target="_blank">pdf</a>]

<h2>Distributed Map Classification using Local Observations. (arXiv:2012.10480v2 [cs.RO] UPDATED)</h2>
<h3>Guangyi Liu, Arash Amini, Martin Tak&#xe1;&#x10d;, H&#xe9;ctor Mu&#xf1;oz-Avila, Nader Motee</h3>
<p>We consider the problem of classifying a map using a team of communicating
robots. It is assumed that all robots have localized visual sensing
capabilities and can exchange their information with neighboring robots. Using
a graph decomposition technique, we proposed an offline learning structure that
makes every robot capable of communicating with and fusing information from its
neighbors to plan its next move towards the most informative parts of the
environment for map classification purposes. The main idea is to decompose a
given undirected graph into a union of directed star graphs and train robots
w.r.t a bounded number of star graphs. This will significantly reduce the
computational cost of offline training and makes learning scalable (independent
of the number of robots). Our approach is particularly useful for fast map
classification in large environments using a large number of communicating
robots. We validate the usefulness of our proposed methodology through
extensive simulations.
</p>
<a href="http://arxiv.org/abs/2012.10480" target="_blank">arXiv:2012.10480</a> [<a href="http://arxiv.org/pdf/2012.10480" target="_blank">pdf</a>]

<h2>Rethinking the Heatmap Regression for Bottom-up Human Pose Estimation. (arXiv:2012.15175v3 [cs.CV] UPDATED)</h2>
<h3>Zhengxiong Luo, Zhicheng Wang, Yan Huang, Tieniu Tan, Erjin Zhou</h3>
<p>Heatmap regression has become the most prevalent choice for nowadays human
pose estimation methods. The ground-truth heatmaps are usually constructed via
covering all skeletal keypoints by 2D gaussian kernels. The standard deviations
of these kernels are fixed. However, for bottom-up methods, which need to
handle a large variance of human scales and labeling ambiguities, the current
practice seems unreasonable. To better cope with these problems, we propose the
scale-adaptive heatmap regression (SAHR) method, which can adaptively adjust
the standard deviation for each keypoint. In this way, SAHR is more tolerant of
various human scales and labeling ambiguities. However, SAHR may aggravate the
imbalance between fore-background samples, which potentially hurts the
improvement of SAHR. Thus, we further introduce the weight-adaptive heatmap
regression (WAHR) to help balance the fore-background samples. Extensive
experiments show that SAHR together with WAHR largely improves the accuracy of
bottom-up human pose estimation. As a result, we finally outperform the
state-of-the-art model by +1.5AP and achieve 72.0AP on COCO test-dev2017, which
is com-arable with the performances of most top-down methods. Source codes are
available at https://github.com/greatlog/SWAHR-HumanPose.
</p>
<a href="http://arxiv.org/abs/2012.15175" target="_blank">arXiv:2012.15175</a> [<a href="http://arxiv.org/pdf/2012.15175" target="_blank">pdf</a>]

<h2>VinVL: Revisiting Visual Representations in Vision-Language Models. (arXiv:2101.00529v2 [cs.CV] UPDATED)</h2>
<h3>Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, Jianfeng Gao</h3>
<p>This paper presents a detailed study of improving visual representations for
vision language (VL) tasks and develops an improved object detection model to
provide object-centric representations of images. Compared to the most widely
used \emph{bottom-up and top-down} model \cite{anderson2018bottom}, the new
model is bigger, better-designed for VL tasks, and pre-trained on much larger
training corpora that combine multiple public annotated object detection
datasets. Therefore, it can generate representations of a richer collection of
visual objects and concepts. While previous VL research focuses mainly on
improving the vision-language fusion model and leaves the object detection
model improvement untouched, we show that visual features matter significantly
in VL models. In our experiments we feed the visual features generated by the
new object detection model into a Transformer-based VL fusion model \oscar
\cite{li2020oscar}, and utilize an improved approach \short\ to pre-train the
VL model and fine-tune it on a wide range of downstream VL tasks. Our results
show that the new visual features significantly improve the performance across
all VL tasks, creating new state-of-the-art results on seven public benchmarks.
We will release the new object detection model to public.
</p>
<a href="http://arxiv.org/abs/2101.00529" target="_blank">arXiv:2101.00529</a> [<a href="http://arxiv.org/pdf/2101.00529" target="_blank">pdf</a>]

<h2>Swarm Herding: A Leader-Follower Framework For Multi-Robot Navigation. (arXiv:2101.07416v2 [cs.RO] UPDATED)</h2>
<h3>Xiaotian Xu, Yancy Diaz-Mercado</h3>
<p>A leader-follower framework is proposed for multi-robot navigation of large
scale teams where the leader agents corral the follower agents. A group of
leaders is modeled as a 2D deformable object where discrete masses (i.e.,
leader robots) are interconnected by springs and dampers. A time-varying domain
is defined by the positions of leaders while the external forces induce
deformations of the domain from its nominal configuration. The team of
followers is performing coverage over the time-varying domain by employing a
perspective transformation that maps between the nominal and deformed
configurations. A decentralized control strategy is proposed where a leader
only takes local sensing information and information about its neighbors
(connected by virtual springs and dampers), and a follower only needs partial
information about leaders and information about its Delaunay neighbors.
</p>
<a href="http://arxiv.org/abs/2101.07416" target="_blank">arXiv:2101.07416</a> [<a href="http://arxiv.org/pdf/2101.07416" target="_blank">pdf</a>]

<h2>Regularization Strategy for Point Cloud via Rigidly Mixed Sample. (arXiv:2102.01929v3 [cs.CV] UPDATED)</h2>
<h3>Dogyoon Lee, Jaeha Lee, Junhyeop Lee, Hyeongmin Lee, Minhyeok Lee, Sungmin Woo, Sangyoun Lee</h3>
<p>Data augmentation is an effective regularization strategy to alleviate the
overfitting, which is an inherent drawback of the deep neural networks.
However, data augmentation is rarely considered for point cloud processing
despite many studies proposing various augmentation methods for image data.
Actually, regularization is essential for point clouds since lack of generality
is more likely to occur in point cloud due to small datasets. This paper
proposes a Rigid Subset Mix (RSMix), a novel data augmentation method for point
clouds that generates a virtual mixed sample by replacing part of the sample
with shape-preserved subsets from another sample. RSMix preserves structural
information of the point cloud sample by extracting subsets from each sample
without deformation using a neighboring function. The neighboring function was
carefully designed considering unique properties of point cloud, unordered
structure and non-grid. Experiments verified that RSMix successfully
regularized the deep neural networks with remarkable improvement for shape
classification. We also analyzed various combinations of data augmentations
including RSMix with single and multi-view evaluations, based on abundant
ablation studies.
</p>
<a href="http://arxiv.org/abs/2102.01929" target="_blank">arXiv:2102.01929</a> [<a href="http://arxiv.org/pdf/2102.01929" target="_blank">pdf</a>]

<h2>Mobile Computational Photography: A Tour. (arXiv:2102.09000v2 [cs.CV] UPDATED)</h2>
<h3>Mauricio Delbracio, Damien Kelly, Michael S. Brown, Peyman Milanfar</h3>
<p>The first mobile camera phone was sold only 20 years ago, when taking
pictures with one's phone was an oddity, and sharing pictures online was
unheard of. Today, the smartphone is more camera than phone. How did this
happen? This transformation was enabled by advances in computational
photography -the science and engineering of making great images from small form
factor, mobile cameras. Modern algorithmic and computing advances, including
machine learning, have changed the rules of photography, bringing to it new
modes of capture, post-processing, storage, and sharing. In this paper, we give
a brief history of mobile computational photography and describe some of the
key technological components, including burst photography, noise reduction, and
super-resolution. At each step, we may draw naive parallels to the human visual
system.
</p>
<a href="http://arxiv.org/abs/2102.09000" target="_blank">arXiv:2102.09000</a> [<a href="http://arxiv.org/pdf/2102.09000" target="_blank">pdf</a>]

<h2>Adversarial Shape Learning for Building Extraction in VHR Remote Sensing Images. (arXiv:2102.11262v3 [cs.CV] UPDATED)</h2>
<h3>Lei Ding, Hao Tang, Yahui Liu, Yilei Shi, Lorenzo Bruzzone</h3>
<p>Building extraction in VHR RSIs remains to be a challenging task due to
occlusion and boundary ambiguity problems. Although conventional convolutional
neural networks (CNNs) based methods are capable of exploiting local texture
and context information, they fail to capture the shape patterns of buildings,
which is a necessary constraint in the human recognition. In this context, we
propose an adversarial shape learning network (ASLNet) to model the building
shape patterns, thus improving the accuracy of building segmentation. In the
proposed ASLNet, we introduce the adversarial learning strategy to explicitly
model the shape constraints, as well as a CNN shape regularizer to strengthen
the embedding of shape features. To assess the geometric accuracy of building
segmentation results, we further introduced several object-based assessment
metrics. Experiments on two open benchmark datasets show that the proposed
ASLNet improves both the pixel-based accuracy and the object-based measurements
by a large margin. The code is available at: https://github.com/ggsDing/ASLNet
</p>
<a href="http://arxiv.org/abs/2102.11262" target="_blank">arXiv:2102.11262</a> [<a href="http://arxiv.org/pdf/2102.11262" target="_blank">pdf</a>]

<h2>SISE-PC: Semi-supervised Image Subsampling for Explainable Pathology. (arXiv:2102.11560v2 [cs.CV] UPDATED)</h2>
<h3>Sohini Roychowdhury, Kwok Sun Tang, Mohith Ashok, Anoop Sanka</h3>
<p>Although automated pathology classification using deep learning (DL) has
proved to be predictively efficient, DL methods are found to be data and
compute cost intensive. In this work, we aim to reduce DL training costs by
pre-training a Resnet feature extractor using SimCLR contrastive loss for
latent encoding of OCT images. We propose a novel active learning framework
that identifies a minimal sub-sampled dataset containing the most uncertain OCT
image samples using label propagation on the SimCLR latent encodings. The
pre-trained Resnet model is then fine-tuned with the labelled minimal
sub-sampled data and the underlying pathological sites are visually explained.
Our framework identifies upto 2% of OCT images to be most uncertain that need
prioritized specialist attention and that can fine-tune a Resnet model to
achieve upto 97% classification accuracy. The proposed method can be extended
to other medical images to minimize prediction costs.
</p>
<a href="http://arxiv.org/abs/2102.11560" target="_blank">arXiv:2102.11560</a> [<a href="http://arxiv.org/pdf/2102.11560" target="_blank">pdf</a>]

<h2>$SE_2(3)$ based Extended Kalman Filtering and Smoothing Framework for Inertial-Integrated Navigation. (arXiv:2102.12897v6 [cs.RO] UPDATED)</h2>
<h3>Yarong Luo, Chi Guo, Shengyong You, Jianlang Hu, Jingnan Liu</h3>
<p>This paper proposes an $SE_2(3)$ based extended Kalman filtering (EKF)
framework for the inertial-integrated state estimation problem. The error
representation using the straight difference of two vectors in the inertial
navigation system may not be reasonable as it does not take the direction
difference into consideration.

Therefore, we choose to use the $SE_2(3)$ matrix Lie group to represent the
state of the inertial-integrated navigation system which consequently leads to
the common frame error representation.

With the new velocity and position error definition, we leverage the group
affine dynamics with the autonomous error properties and derive the error state
differential equation for the inertial-integrated navigation on the
north-east-down (NED) navigation frame and the earth-centered earth-fixed
(ECEF) frame, respectively, the corresponding EKF, terms as $SE_2(3)$ based EKF
has also been derived. It provides a new perspective on the geometric EKF with
a more sophisticated formula for the inertial-integrated navigation system.
Furthermore, we design two new modified error dynamics on the NED frame and the
ECEF frame respectively by introducing new auxiliary vectors. Finally the
equivalence of the left-invariant EKF and left $SE_2(3)$ based EKF have been
shown in navigation frame and ECEF frame.
</p>
<a href="http://arxiv.org/abs/2102.12897" target="_blank">arXiv:2102.12897</a> [<a href="http://arxiv.org/pdf/2102.12897" target="_blank">pdf</a>]

<h2>Careful with That! Observation of Human Movements to Estimate Objects Properties. (arXiv:2103.01555v2 [cs.RO] UPDATED)</h2>
<h3>Linda Lastrico, Alessandro Carf&#xec;, Alessia Vignolo, Alessandra Sciutti, Fulvio Mastrogiovanni, Francesco Rea</h3>
<p>Humans are very effective at interpreting subtle properties of the partner's
movement and use this skill to promote smooth interactions. Therefore, robotic
platforms that support human partners in daily activities should acquire
similar abilities. In this work we focused on the features of human motor
actions that communicate insights on the weight of an object and the
carefulness required in its manipulation. Our final goal is to enable a robot
to autonomously infer the degree of care required in object handling and to
discriminate whether the item is light or heavy, just by observing a human
manipulation. This preliminary study represents a promising step towards the
implementation of those abilities on a robot observing the scene with its
camera. Indeed, we succeeded in demonstrating that it is possible to reliably
deduct if the human operator is careful when handling an object, through
machine learning algorithms relying on the stream of visual acquisition from
either a robot camera or from a motion capture system. On the other hand, we
observed that the same approach is inadequate to discriminate between light and
heavy objects.
</p>
<a href="http://arxiv.org/abs/2103.01555" target="_blank">arXiv:2103.01555</a> [<a href="http://arxiv.org/pdf/2103.01555" target="_blank">pdf</a>]

<h2>Spatial-Phase Shallow Learning: Rethinking Face Forgery Detection in Frequency Domain. (arXiv:2103.01856v3 [cs.CV] UPDATED)</h2>
<h3>Honggu Liu, Xiaodan Li, Wenbo Zhou, Yuefeng Chen, Yuan He, Hui Xue, Weiming Zhang, Nenghai Yu</h3>
<p>The remarkable success in face forgery techniques has received considerable
attention in computer vision due to security concerns. We observe that
up-sampling is a necessary step of most face forgery techniques, and cumulative
up-sampling will result in obvious changes in the frequency domain, especially
in the phase spectrum. According to the property of natural images, the phase
spectrum preserves abundant frequency components that provide extra information
and complement the loss of the amplitude spectrum. To this end, we present a
novel Spatial-Phase Shallow Learning (SPSL) method, which combines spatial
image and phase spectrum to capture the up-sampling artifacts of face forgery
to improve the transferability, for face forgery detection. And we also
theoretically analyze the validity of utilizing the phase spectrum. Moreover,
we notice that local texture information is more crucial than high-level
semantic information for the face forgery detection task. So we reduce the
receptive fields by shallowing the network to suppress high-level features and
focus on the local region. Extensive experiments show that SPSL can achieve the
state-of-the-art performance on cross-datasets evaluation as well as
multi-class classification and obtain comparable results on single dataset
evaluation.
</p>
<a href="http://arxiv.org/abs/2103.01856" target="_blank">arXiv:2103.01856</a> [<a href="http://arxiv.org/pdf/2103.01856" target="_blank">pdf</a>]

<h2>Motion-blurred Video Interpolation and Extrapolation. (arXiv:2103.02984v2 [cs.CV] UPDATED)</h2>
<h3>Dawit Mureja Argaw, Junsik Kim, Francois Rameau, In So Kweon</h3>
<p>Abrupt motion of camera or objects in a scene result in a blurry video, and
therefore recovering high quality video requires two types of enhancements:
visual enhancement and temporal upsampling. A broad range of research attempted
to recover clean frames from blurred image sequences or temporally upsample
frames by interpolation, yet there are very limited studies handling both
problems jointly. In this work, we present a novel framework for deblurring,
interpolating and extrapolating sharp frames from a motion-blurred video in an
end-to-end manner. We design our framework by first learning the pixel-level
motion that caused the blur from the given inputs via optical flow estimation
and then predict multiple clean frames by warping the decoded features with the
estimated flows. To ensure temporal coherence across predicted frames and
address potential temporal ambiguity, we propose a simple, yet effective
flow-based rule. The effectiveness and favorability of our approach are
highlighted through extensive qualitative and quantitative evaluations on
motion-blurred datasets from high speed videos.
</p>
<a href="http://arxiv.org/abs/2103.02984" target="_blank">arXiv:2103.02984</a> [<a href="http://arxiv.org/pdf/2103.02984" target="_blank">pdf</a>]

<h2>Optical Flow Estimation from a Single Motion-blurred Image. (arXiv:2103.02996v2 [cs.CV] UPDATED)</h2>
<h3>Dawit Mureja Argaw, Junsik Kim, Francois Rameau, Jae Won Cho, In So Kweon</h3>
<p>In most of computer vision applications, motion blur is regarded as an
undesirable artifact. However, it has been shown that motion blur in an image
may have practical interests in fundamental computer vision problems. In this
work, we propose a novel framework to estimate optical flow from a single
motion-blurred image in an end-to-end manner. We design our network with
transformer networks to learn globally and locally varying motions from encoded
features of a motion-blurred input, and decode left and right frame features
without explicit frame supervision. A flow estimator network is then used to
estimate optical flow from the decoded features in a coarse-to-fine manner. We
qualitatively and quantitatively evaluate our model through a large set of
experiments on synthetic and real motion-blur datasets. We also provide
in-depth analysis of our model in connection with related approaches to
highlight the effectiveness and favorability of our approach. Furthermore, we
showcase the applicability of the flow estimated by our method on deblurring
and moving object segmentation tasks.
</p>
<a href="http://arxiv.org/abs/2103.02996" target="_blank">arXiv:2103.02996</a> [<a href="http://arxiv.org/pdf/2103.02996" target="_blank">pdf</a>]

<h2>LOHO: Latent Optimization of Hairstyles via Orthogonalization. (arXiv:2103.03891v2 [cs.CV] UPDATED)</h2>
<h3>Rohit Saha, Brendan Duke, Florian Shkurti, Graham W. Taylor, Parham Aarabi</h3>
<p>Hairstyle transfer is challenging due to hair structure differences in the
source and target hair. Therefore, we propose Latent Optimization of Hairstyles
via Orthogonalization (LOHO), an optimization-based approach using GAN
inversion to infill missing hair structure details in latent space during
hairstyle transfer. Our approach decomposes hair into three attributes:
perceptual structure, appearance, and style, and includes tailored losses to
model each of these attributes independently. Furthermore, we propose two-stage
optimization and gradient orthogonalization to enable disentangled latent space
optimization of our hair attributes. Using LOHO for latent space manipulation,
users can synthesize novel photorealistic images by manipulating hair
attributes either individually or jointly, transferring the desired attributes
from reference hairstyles. LOHO achieves a superior FID compared with the
current state-of-the-art (SOTA) for hairstyle transfer. Additionally, LOHO
preserves the subject's identity comparably well according to PSNR and SSIM
when compared to SOTA image embedding pipelines. Code is available at
https://github.com/dukebw/LOHO.
</p>
<a href="http://arxiv.org/abs/2103.03891" target="_blank">arXiv:2103.03891</a> [<a href="http://arxiv.org/pdf/2103.03891" target="_blank">pdf</a>]

<h2>OPANAS: One-Shot Path Aggregation Network Architecture Search for Object. (arXiv:2103.04507v2 [cs.CV] UPDATED)</h2>
<h3>Tingting Liang, Yongtao Wang, Guosheng Hu, Zhi Tang, Haibin Ling</h3>
<p>Recently, neural architecture search (NAS) has been exploited to design
feature pyramid networks (FPNs) and achieved promising results for visual
object detection. Encouraged by the success, we propose a novel One-Shot Path
Aggregation Network Architecture Search (OPANAS) algorithm, which significantly
improves both searching efficiency and detection accuracy. Specifically, we
first introduce six heterogeneous information paths to build our search space,
namely top-down, bottom-up, fusing-splitting, scale-equalizing, skip-connect
and none. Second, we propose a novel search space of FPNs, in which each FPN
candidate is represented by a densely-connected directed acyclic graph (each
node is a feature pyramid and each edge is one of the six heterogeneous
information paths). Third, we propose an efficient one-shot search method to
find the optimal path aggregation architecture, that is, we first train a
super-net and then find the optimal candidate with an evolutionary algorithm.
Experimental results demonstrate the efficacy of the proposed OPANAS for object
detection: (1) OPANAS is more efficient than state-of-the-art methods (e.g.,
NAS-FPN and Auto-FPN), at significantly smaller searching cost (e.g., only 4
GPU days on MS-COCO); (2) the optimal architecture found by OPANAS
significantly improves main-stream detectors including RetinaNet, Faster R-CNN
and Cascade R-CNN, by 2.3-3.2 % mAP comparing to their FPN counterparts; and
(3) a new state-of-the-art accuracy-speed trade-off (52.2 % mAP at 7.6 FPS) at
smaller training costs than comparable state-of-the-arts. Code will be released
at https://github.com/VDIGPKU/OPANAS.
</p>
<a href="http://arxiv.org/abs/2103.04507" target="_blank">arXiv:2103.04507</a> [<a href="http://arxiv.org/pdf/2103.04507" target="_blank">pdf</a>]

<h2>BASAR:Black-box Attack on Skeletal Action Recognition. (arXiv:2103.05266v2 [cs.CV] UPDATED)</h2>
<h3>Yunfeng Diao, Tianjia Shao, Yong-Liang Yang, Kun Zhou, He Wang</h3>
<p>Skeletal motion plays a vital role in human activity recognition as either an
independent data source or a complement. The robustness of skeleton-based
activity recognizers has been questioned recently, which shows that they are
vulnerable to adversarial attacks when the full-knowledge of the recognizer is
accessible to the attacker. However, this white-box requirement is overly
restrictive in most scenarios and the attack is not truly threatening. In this
paper, we show that such threats do exist under black-box settings too. To this
end, we propose the first black-box adversarial attack method BASAR. Through
BASAR, we show that adversarial attack is not only truly a threat but also can
be extremely deceitful, because on-manifold adversarial samples are rather
common in skeletal motions, in contrast to the common belief that adversarial
samples only exist off-manifold. Through exhaustive evaluation and comparison,
we show that BASAR can deliver successful attacks across models, data, and
attack modes. Through harsh perceptual studies, we show that it achieves
effective yet imperceptible attacks. By analyzing the attack on different
activity recognizers, BASAR helps identify the potential causes of their
vulnerability and provides insights on what classifiers are likely to be more
robust against attack.
</p>
<a href="http://arxiv.org/abs/2103.05266" target="_blank">arXiv:2103.05266</a> [<a href="http://arxiv.org/pdf/2103.05266" target="_blank">pdf</a>]

<h2>Probabilistic Modeling of Semantic Ambiguity for Scene Graph Generation. (arXiv:2103.05271v2 [cs.CV] UPDATED)</h2>
<h3>Gengcong Yang, Jingyi Zhang, Yong Zhang, Baoyuan Wu, Yujiu Yang</h3>
<p>To generate "accurate" scene graphs, almost all existing methods predict
pairwise relationships in a deterministic manner. However, we argue that visual
relationships are often semantically ambiguous. Specifically, inspired by
linguistic knowledge, we classify the ambiguity into three types: Synonymy
Ambiguity, Hyponymy Ambiguity, and Multi-view Ambiguity. The ambiguity
naturally leads to the issue of \emph{implicit multi-label}, motivating the
need for diverse predictions. In this work, we propose a novel plug-and-play
Probabilistic Uncertainty Modeling (PUM) module. It models each union region as
a Gaussian distribution, whose variance measures the uncertainty of the
corresponding visual content. Compared to the conventional deterministic
methods, such uncertainty modeling brings stochasticity of feature
representation, which naturally enables diverse predictions. As a byproduct,
PUM also manages to cover more fine-grained relationships and thus alleviates
the issue of bias towards frequent relationships. Extensive experiments on the
large-scale Visual Genome benchmark show that combining PUM with newly proposed
ResCAGCN can achieve state-of-the-art performances, especially under the mean
recall metric. Furthermore, we prove the universal effectiveness of PUM by
plugging it into some existing models and provide insightful analysis of its
ability to generate diverse yet plausible visual relationships.
</p>
<a href="http://arxiv.org/abs/2103.05271" target="_blank">arXiv:2103.05271</a> [<a href="http://arxiv.org/pdf/2103.05271" target="_blank">pdf</a>]

<h2>Deep 6-DoF Tracking of Unknown Objects for Reactive Grasping. (arXiv:2103.05401v2 [cs.RO] UPDATED)</h2>
<h3>Marc Tuscher, Julian H&#xf6;rz, Danny Driess, Marc Toussaint</h3>
<p>Robotic manipulation of unknown objects is an important field of research.
Practical applications occur in many real-world settings where robots need to
interact with an unknown environment. We tackle the problem of reactive
grasping by proposing a method for unknown object tracking, grasp point
sampling and dynamic trajectory planning. Our object tracking method combines
Siamese Networks with an Iterative Closest Point approach for pointcloud
registration into a method for 6-DoF unknown object tracking. The method does
not require further training and is robust to noise and occlusion. We propose a
robotic manipulation system, which is able to grasp a wide variety of formerly
unseen objects and is robust against object perturbations and inferior grasping
points.
</p>
<a href="http://arxiv.org/abs/2103.05401" target="_blank">arXiv:2103.05401</a> [<a href="http://arxiv.org/pdf/2103.05401" target="_blank">pdf</a>]

<h2>Deep Learning based 3D Segmentation: A Survey. (arXiv:2103.05423v2 [cs.CV] UPDATED)</h2>
<h3>Yong He, Hongshan Yu, Xiaoyan Liu, Zhengeng Yang, Wei Sun, Yaonan Wang, Qiang Fu, Yanmei Zou, Ajmal Mian</h3>
<p>3D object segmentation is a fundamental and challenging problem in computer
vision with applications in autonomous driving, robotics, augmented reality and
medical image analysis. It has received significant attention from the computer
vision, graphics and machine learning communities. Traditionally, 3D
segmentation was performed with hand-crafted features and engineered methods
which failed to achieve acceptable accuracy and could not generalize to
large-scale data. Driven by their great success in 2D computer vision, deep
learning techniques have recently become the tool of choice for 3D segmentation
tasks as well. This has led to an influx of a large number of methods in the
literature that have been evaluated on different benchmark datasets. This paper
provides a comprehensive survey of recent progress in deep learning based 3D
segmentation covering over 150 papers. It summarizes the most commonly used
pipelines, discusses their highlights and shortcomings, and analyzes the
competitive results of these segmentation methods. Based on the analysis, it
also provides promising research directions for the future.
</p>
<a href="http://arxiv.org/abs/2103.05423" target="_blank">arXiv:2103.05423</a> [<a href="http://arxiv.org/pdf/2103.05423" target="_blank">pdf</a>]

