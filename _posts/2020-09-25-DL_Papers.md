---
title: Latest Deep Learning Papers
date: 2020-11-20 21:00:01 +0800
featured-img: DL-logo
categories: [Deep_Learning]
tags: [Deep_Learning]
mathjax: Yes
---

<h1>Your arXiv Feed (187 Articles)</h1>
<h2>High-Throughput Approach to Modeling Healthcare Costs Using Electronic Healthcare Records. (arXiv:2011.09497v1 [cs.LG])</h2>
<h3>Alex Taylor, Ross Kleiman, Scott Hebbring, Peggy Peissig, David Page</h3>
<p>Accurate estimation of healthcare costs is crucial for healthcare systems to
plan and effectively negotiate with insurance companies regarding the coverage
of patient-care costs. Greater accuracy in estimating healthcare costs would
provide mutual benefit for both health systems and the insurers that support
these systems by better aligning payment models with patient-care costs. This
study presents the results of a generalizable machine learning approach to
predicting medical events built from 40 years of data from &gt;860,000 patients
pertaining to &gt;6,700 prescription medications, courtesy of Marshfield Clinic in
Wisconsin. It was found that models built using this approach performed well
when compared to similar studies predicting physician prescriptions of
individual medications. In addition to providing a comprehensive predictive
model for all drugs in a large healthcare system, the approach taken in this
research benefits from potential applicability to a wide variety of other
medical events.
</p>
<a href="http://arxiv.org/abs/2011.09497" target="_blank">arXiv:2011.09497</a> [<a href="http://arxiv.org/pdf/2011.09497" target="_blank">pdf</a>]

<h2>GRAPHSPY: Fused Program Semantic-Level Embedding via Graph Neural Networks for Dead Store Detection. (arXiv:2011.09501v1 [cs.LG])</h2>
<h3>Yixin Guo, Pengcheng Li, Yingwei Luo, Xiaolin Wang, Zhenlin Wang</h3>
<p>Production software oftentimes suffers from the issue of performance
inefficiencies caused by inappropriate use of data structures, programming
abstractions, and conservative compiler optimizations. It is desirable to avoid
unnecessary memory operations. However, existing works often use a
whole-program fine-grained monitoring method with incredibly high overhead. To
this end, we propose a learning-aided approach to identify unnecessary memory
operations intelligently with low overhead. By applying several prevalent graph
neural network models to extract program semantics with respect to program
structure, execution order and dynamic states, we present a novel, hybrid
program embedding approach so that to derive unnecessary memory operations
through the embedding. We train our model with tens of thousands of samples
acquired from a set of real-world benchmarks. Results show that our model
achieves 90% of accuracy and incurs only around a half of time overhead of the
state-of-art tool.
</p>
<a href="http://arxiv.org/abs/2011.09501" target="_blank">arXiv:2011.09501</a> [<a href="http://arxiv.org/pdf/2011.09501" target="_blank">pdf</a>]

<h2>Extracting and Learning Fine-Grained Labels from Chest Radiographs. (arXiv:2011.09517v1 [cs.CV])</h2>
<h3>Tanveer Syeda-Mahmood, Ph.D, K.C.L Wong, Ph.D, Joy T. Wu, M.D., M.P.H, Ashutosh Jadhav, Ph.D, Orest Boyko, M.D. Ph.D</h3>
<p>Chest radiographs are the most common diagnostic exam in emergency rooms and
intensive care units today. Recently, a number of researchers have begun
working on large chest X-ray datasets to develop deep learning models for
recognition of a handful of coarse finding classes such as opacities, masses
and nodules. In this paper, we focus on extracting and learning fine-grained
labels for chest X-ray images. Specifically we develop a new method of
extracting fine-grained labels from radiology reports by combining
vocabulary-driven concept extraction with phrasal grouping in dependency parse
trees for association of modifiers with findings. A total of 457 fine-grained
labels depicting the largest spectrum of findings to date were selected and
sufficiently large datasets acquired to train a new deep learning model
designed for fine-grained classification. We show results that indicate a
highly accurate label extraction process and a reliable learning of
fine-grained labels. The resulting network, to our knowledge, is the first to
recognize fine-grained descriptions of findings in images covering over nine
modifiers including laterality, location, severity, size and appearance.
</p>
<a href="http://arxiv.org/abs/2011.09517" target="_blank">arXiv:2011.09517</a> [<a href="http://arxiv.org/pdf/2011.09517" target="_blank">pdf</a>]

<h2>TRAT: Tracking by Attention Using Spatio-Temporal Features. (arXiv:2011.09524v1 [cs.CV])</h2>
<h3>Hasan Saribas, Hakan Cevikalp, Okan K&#xf6;p&#xfc;kl&#xfc;, Bedirhan Uzun</h3>
<p>Robust object tracking requires knowledge of tracked objects' appearance,
motion and their evolution over time. Although motion provides distinctive and
complementary information especially for fast moving objects, most of the
recent tracking architectures primarily focus on the objects' appearance
information. In this paper, we propose a two-stream deep neural network tracker
that uses both spatial and temporal features. Our architecture is developed
over ATOM tracker and contains two backbones: (i) 2D-CNN network to capture
appearance features and (ii) 3D-CNN network to capture motion features. The
features returned by the two networks are then fused with attention based
Feature Aggregation Module (FAM). Since the whole architecture is unified, it
can be trained end-to-end. The experimental results show that the proposed
tracker TRAT (TRacking by ATtention) achieves state-of-the-art performance on
most of the benchmarks and it significantly outperforms the baseline ATOM
tracker.
</p>
<a href="http://arxiv.org/abs/2011.09524" target="_blank">arXiv:2011.09524</a> [<a href="http://arxiv.org/pdf/2011.09524" target="_blank">pdf</a>]

<h2>Contextual Fusion For Adversarial Robustness. (arXiv:2011.09526v1 [cs.CV])</h2>
<h3>Aiswarya Akumalla, Seth Haney, Maksim Bazhenov</h3>
<p>Mammalian brains handle complex reasoning tasks in a gestalt manner by
integrating information from regions of the brain that are specialised to
individual sensory modalities. This allows for improved robustness and better
generalisation ability. In contrast, deep neural networks are usually designed
to process one particular information stream and susceptible to various types
of adversarial perturbations. While many methods exist for detecting and
defending against adversarial attacks, they do not generalise across a range of
attacks and negatively affect performance on clean, unperturbed data. We
developed a fusion model using a combination of background and foreground
features extracted in parallel from Places-CNN and Imagenet-CNN. We tested the
benefits of the fusion approach on preserving adversarial robustness for human
perceivable (e.g., Gaussian blur) and network perceivable (e.g.,
gradient-based) attacks for CIFAR-10 and MS COCO data sets. For gradient based
attacks, our results show that fusion allows for significant improvements in
classification without decreasing performance on unperturbed data and without
need to perform adversarial retraining. Our fused model revealed improvements
for Gaussian blur type perturbations as well. The increase in performance from
fusion approach depended on the variability of the image contexts; larger
increases were seen for classes of images with larger differences in their
contexts. We also demonstrate the effect of regularization to bias the
classifier decision in the presence of a known adversary. We propose that this
biologically inspired approach to integrate information across multiple
modalities provides a new way to improve adversarial robustness that can be
complementary to current state of the art approaches.
</p>
<a href="http://arxiv.org/abs/2011.09526" target="_blank">arXiv:2011.09526</a> [<a href="http://arxiv.org/pdf/2011.09526" target="_blank">pdf</a>]

<h2>Neuro-Symbolic Representations for Video Captioning: A Case for Leveraging Inductive Biases for Vision and Language. (arXiv:2011.09530v1 [cs.CV])</h2>
<h3>Hassan Akbari, Hamid Palangi, Jianwei Yang, Sudha Rao, Asli Celikyilmaz, Roland Fernandez, Paul Smolensky, Jianfeng Gao, Shih-Fu Chang</h3>
<p>Neuro-symbolic representations have proved effective in learning structure
information in vision and language. In this paper, we propose a new model
architecture for learning multi-modal neuro-symbolic representations for video
captioning. Our approach uses a dictionary learning-based method of learning
relations between videos and their paired text descriptions. We refer to these
relations as relative roles and leverage them to make each token role-aware
using attention. This results in a more structured and interpretable
architecture that incorporates modality-specific inductive biases for the
captioning task. Intuitively, the model is able to learn spatial, temporal, and
cross-modal relations in a given pair of video and text. The disentanglement
achieved by our proposal gives the model more capacity to capture multi-modal
structures which result in captions with higher quality for videos. Our
experiments on two established video captioning datasets verifies the
effectiveness of the proposed approach based on automatic metrics. We further
conduct a human evaluation to measure the grounding and relevance of the
generated captions and observe consistent improvement for the proposed model.
The codes and trained models can be found at
https://github.com/hassanhub/R3Transformer
</p>
<a href="http://arxiv.org/abs/2011.09530" target="_blank">arXiv:2011.09530</a> [<a href="http://arxiv.org/pdf/2011.09530" target="_blank">pdf</a>]

<h2>Is Independent Learning All You Need in the StarCraft Multi-Agent Challenge?. (arXiv:2011.09533v1 [cs.AI])</h2>
<h3>Christian Schroeder de Witt, Tarun Gupta, Denys Makoviichuk, Viktor Makoviychuk, Philip H.S. Torr, Mingfei Sun, Shimon Whiteson</h3>
<p>Most recently developed approaches to cooperative multi-agent reinforcement
learning in the \emph{centralized training with decentralized execution}
setting involve estimating a centralized, joint value function. In this paper,
we demonstrate that, despite its various theoretical shortcomings, Independent
PPO (IPPO), a form of independent learning in which each agent simply estimates
its local value function, can perform just as well as or better than
state-of-the-art joint learning approaches on popular multi-agent benchmark
suite SMAC with little hyperparameter tuning. We also compare IPPO to several
variants; the results suggest that IPPO's strong performance may be due to its
robustness to some forms of environment non-stationarity.
</p>
<a href="http://arxiv.org/abs/2011.09533" target="_blank">arXiv:2011.09533</a> [<a href="http://arxiv.org/pdf/2011.09533" target="_blank">pdf</a>]

<h2>StressNet: Detecting Stress in Thermal Videos. (arXiv:2011.09540v1 [cs.CV])</h2>
<h3>Satish Kumar, A S M Iftekhar, Michael Goebel, Tom Bullock, Mary H. MacLean, Michael B. Miller, Tyler Santander, Barry Giesbrecht, Scott T. Grafton, B.S. Manjunath</h3>
<p>Precise measurement of physiological signals is critical for the effective
monitoring of human vital signs. Recent developments in computer vision have
demonstrated that signals such as pulse rate and respiration rate can be
extracted from digital video of humans, increasing the possibility of
contact-less monitoring. This paper presents a novel approach to obtaining
physiological signals and classifying stress states from thermal video. The
proposed network--"StressNet"--features a hybrid emission representation model
that models the direct emission and absorption of heat by the skin and
underlying blood vessels. This results in an information-rich feature
representation of the face, which is used by spatio-temporal network for
reconstructing the ISTI ( Initial Systolic Time Interval: a measure of change
in cardiac sympathetic activity that is considered to be a quantitative index
of stress in humans ). The reconstructed ISTI signal is fed into a
stress-detection model to detect and classify the individual's stress state (
i.e. stress or no stress ). A detailed evaluation demonstrates that StressNet
achieves estimated the ISTI signal with 95% accuracy and detect stress with
average precision of 0.842. The source code is available on Github.
</p>
<a href="http://arxiv.org/abs/2011.09540" target="_blank">arXiv:2011.09540</a> [<a href="http://arxiv.org/pdf/2011.09540" target="_blank">pdf</a>]

<h2>MOFA: Modular Factorial Design for Hyperparameter Optimization. (arXiv:2011.09545v1 [cs.LG])</h2>
<h3>Bo Xiong, Yimin Huang, Steffen Staab, Zhenguo Li</h3>
<p>Automated hyperparameter optimization (HPO) has shown great power in many
machine learning applications. While existing methods suffer from model
selection, parallelism, or sample efficiency, this paper presents a new HPO
method, MOdular FActorial Design (MOFA), to address these issues
simultaneously. The major idea is to use techniques from Experimental Designs
to improve sample efficiency of model-free methods. Particularly, MOFA runs
with four modules in each iteration: (1) an Orthogonal Latin Hypercube
(OLH)-based sampler preserving both univariate projection uniformity and
orthogonality; (2) a highly parallelized evaluator; (3) a transformer to
collapse the OLH performance table into a specified Fractional Factorial
Design--Orthogonal Array (OA); (4) an analyzer including Factorial Performance
Analysis and Factorial Importance Analysis to narrow down the search space. We
theoretically and empirically show that MOFA has great advantages over existing
model-based and model-free methods.
</p>
<a href="http://arxiv.org/abs/2011.09545" target="_blank">arXiv:2011.09545</a> [<a href="http://arxiv.org/pdf/2011.09545" target="_blank">pdf</a>]

<h2>Vector Embeddings with Subvector Permutation Invariance using a Triplet Enhanced Autoencoder. (arXiv:2011.09550v1 [cs.LG])</h2>
<h3>Mark Alan Matties</h3>
<p>The use of deep neural network (DNN) autoencoders (AEs) has recently exploded
due to their wide applicability. However, the embedding representation produced
by a standard DNN AE that is trained to minimize only the reconstruction error
does not always reveal more subtle patterns in the data. Sometimes, the
autoencoder needs further direction in the form of one or more additional loss
functions. In this paper, we use an autoencoder enhanced with triplet loss to
promote the clustering of vectors that are related through permutations of
constituent subvectors. With this approach, we can create an embedding of the
vector that is nearly invariant to such permutations. We can then use these
invariant embeddings as inputs to other problems, like classification and
clustering, and improve detection accuracy in those problems.
</p>
<a href="http://arxiv.org/abs/2011.09550" target="_blank">arXiv:2011.09550</a> [<a href="http://arxiv.org/pdf/2011.09550" target="_blank">pdf</a>]

<h2>Seeing Through your Skin: Recognizing Objects with a Novel Visuotactile Sensor. (arXiv:2011.09552v1 [cs.RO])</h2>
<h3>Francois Robert Hogan, Michael Jenkin, Sahand Rezaei-Shoshtari, Yogesh Girdhar, David Meger, Gregory Dudek</h3>
<p>We introduce a new class of vision-based sensor and associated algorithmic
processes that combine visual imaging with high-resolution tactile sending, all
in a uniform hardware and computational architecture. We demonstrate the
sensor's efficacy for both multi-modal object recognition and metrology. Object
recognition is typically formulated as an unimodal task, but by combining two
sensor modalities we show that we can achieve several significant performance
improvements. This sensor, named the See-Through-your-Skin sensor (STS), is
designed to provide rich multi-modal sensing of contact surfaces. Inspired by
recent developments in optical tactile sensing technology, we address a key
missing feature of these sensors: the ability to capture a visual perspective
of the region beyond the contact surface. Whereas optical tactile sensors are
typically opaque, we present a sensor with a semitransparent skin that has the
dual capabilities of acting as a tactile sensor and/or as a visual camera
depending on its internal lighting conditions. This paper details the design of
the sensor, showcases its dual sensing capabilities, and presents a deep
learning architecture that fuses vision and touch. We validate the ability of
the sensor to classify household objects, recognize fine textures, and infer
their physical properties both through numerical simulations and experiments
with a smart countertop prototype.
</p>
<a href="http://arxiv.org/abs/2011.09552" target="_blank">arXiv:2011.09552</a> [<a href="http://arxiv.org/pdf/2011.09552" target="_blank">pdf</a>]

<h2>A Cognitive Approach based on the Actionable Knowledge Graph for supporting Maintenance Operations. (arXiv:2011.09554v1 [cs.LG])</h2>
<h3>Giuseppe Fenza, Mariacristina Gallo, Vincenzo Loia, Domenico Marino, Francesco Orciuoli</h3>
<p>In the era of Industry 4.0, cognitive computing and its enabling technologies
(Artificial Intelligence, Machine Learning, etc.) allow to define systems able
to support maintenance by providing relevant information, at the right time,
retrieved from structured companies' databases, and unstructured documents,
like technical manuals, intervention reports, and so on. Moreover, contextual
information plays a crucial role in tailoring the support both during the
planning and the execution of interventions. Contextual information can be
detected with the help of sensors, wearable devices, indoor and outdoor
positioning systems, and object recognition capabilities (using fixed or
wearable cameras), all of which can collect historical data for further
analysis. In this work, we propose a cognitive system that learns from past
interventions to generate contextual recommendations for improving maintenance
practices in terms of time, budget, and scope. The system uses formal
conceptual models, incremental learning, and ranking algorithms to accomplish
these objectives.
</p>
<a href="http://arxiv.org/abs/2011.09554" target="_blank">arXiv:2011.09554</a> [<a href="http://arxiv.org/pdf/2011.09554" target="_blank">pdf</a>]

<h2>Visual Diver Face Recognition for Underwater Human-Robot Interaction. (arXiv:2011.09556v1 [cs.RO])</h2>
<h3>Jungseok Hong, Sadman Sakib Enan, Christopher Morse, Junaed Sattar</h3>
<p>This paper presents a deep-learned facial recognition method for underwater
robots to identify scuba divers. Specifically, the proposed method is able to
recognize divers underwater with faces heavily obscured by scuba masks and
breathing apparatus. Our contribution in this research is towards robust facial
identification of individuals under significant occlusion of facial features
and image degradation from underwater optical distortions. With the ability to
correctly recognize divers, autonomous underwater vehicles (AUV) will be able
to engage in collaborative tasks with the correct person in human-robot teams
and ensure that instructions are accepted from only those authorized to command
the robots. We demonstrate that our proposed framework is able to learn
discriminative features from real-world diver faces through different data
augmentation and generation techniques. Experimental evaluations show that this
framework achieves a 3-fold increase in prediction accuracy compared to the
state-of-the-art (SOTA) algorithms and is well-suited for embedded inference on
robotic platforms.
</p>
<a href="http://arxiv.org/abs/2011.09556" target="_blank">arXiv:2011.09556</a> [<a href="http://arxiv.org/pdf/2011.09556" target="_blank">pdf</a>]

<h2>Robustified Domain Adaptation. (arXiv:2011.09563v1 [cs.CV])</h2>
<h3>Jiajin Zhang, Hanqing Chao, Pingkun Yan</h3>
<p>Unsupervised domain adaptation (UDA) is widely used to transfer a model
trained in a labeled source domain to an unlabeled target domain. However, with
extensive studies showing deep learning models being vulnerable under
adversarial attacks, the adversarial robustness of models in domain adaptation
application has largely been overlooked. In this paper, we first conducted an
empirical analysis to show that severe inter-class mismatch is the key barrier
against achieving a robust model with UDA. Then, we propose a novel approach,
Class-consistent Unsupervised Robust Domain Adaptation (CURDA), for robustified
unsupervised domain adaptation. With the introduced contrastive robust training
and source anchored adversarial contrastive loss, our proposed CURDA is able to
effectively conquer the challenge of inter-class mismatch. Experiments on two
public benchmarks show that, compared with vanilla UDA, CURDA can significantly
improve model robustness in target domains for up to 67.4% costing only 0% to
4.4% of accuracy on the clean data samples. This is one of the first works
focusing on the new problem of robustifying unsupervised domain adaptation,
which demonstrates that UDA models can be substantially robustified while
maintaining competitive accuracy.
</p>
<a href="http://arxiv.org/abs/2011.09563" target="_blank">arXiv:2011.09563</a> [<a href="http://arxiv.org/pdf/2011.09563" target="_blank">pdf</a>]

<h2>Learning Recurrent Neural Net Models of Nonlinear Systems. (arXiv:2011.09573v1 [cs.LG])</h2>
<h3>Joshua Hanson, Maxim Raginsky, Eduardo Sontag</h3>
<p>We consider the following learning problem: Given sample pairs of input and
output signals generated by an unknown nonlinear system (which is not assumed
to be causal or time-invariant), we wish to find a continuous-time recurrent
neural net with hyperbolic tangent activation function that approximately
reproduces the underlying i/o behavior with high confidence. Leveraging earlier
work concerned with matching output derivatives up to a given finite order, we
reformulate the learning problem in familiar system-theoretic language and
derive quantitative guarantees on the sup-norm risk of the learned model in
terms of the number of neurons, the sample size, the number of derivatives
being matched, and the regularity properties of the inputs, the outputs, and
the unknown i/o map.
</p>
<a href="http://arxiv.org/abs/2011.09573" target="_blank">arXiv:2011.09573</a> [<a href="http://arxiv.org/pdf/2011.09573" target="_blank">pdf</a>]

<h2>An Efficient and Scalable Deep Learning Approach for Road Damage Detection. (arXiv:2011.09577v1 [cs.CV])</h2>
<h3>Sadra Naddaf-sh, M-Mahdi Naddaf-sh, Amir R. Kashani, Hassan Zargarzadeh</h3>
<p>Pavement condition evaluation is essential to time the preventative or
rehabilitative actions and control distress propagation. Failing to conduct
timely evaluations can lead to severe structural and financial loss of the
infrastructure and complete reconstructions. Automated computer-aided surveying
measures can provide a database of road damage patterns and their locations.
This database can be utilized for timely road repairs to gain the minimum cost
of maintenance and the asphalt's maximum durability. This paper introduces a
deep learning-based surveying scheme to analyze the image-based distress data
in real-time. A database consisting of a diverse population of crack distress
types such as longitudinal, transverse, and alligator cracks, photographed
using mobile-device is used. Then, a family of efficient and scalable models
that are tuned for pavement crack detection is trained. Proposed models,
resulted in F1-scores, ranging from 52% to 56%, and average inference time from
178-10 images per second. Finally, the performance of the object detectors are
examined, and error analysis is reported against various images. The source
code is available at https://github.com/mahdi65/roadDamageDetection2020.
</p>
<a href="http://arxiv.org/abs/2011.09577" target="_blank">arXiv:2011.09577</a> [<a href="http://arxiv.org/pdf/2011.09577" target="_blank">pdf</a>]

<h2>Patient-independent Epileptic Seizure Prediction using Deep Learning Models. (arXiv:2011.09581v1 [cs.CV])</h2>
<h3>Theekshana Dissanayake, Tharindu Fernando, Simon Denman, Sridha Sridharan, Clinton Fookes</h3>
<p>Objective: Epilepsy is one of the most prevalent neurological diseases among
humans and can lead to severe brain injuries, strokes, and brain tumors. Early
detection of seizures can help to mitigate injuries, and can be used to aid the
treatment of patients with epilepsy. The purpose of a seizure prediction system
is to successfully identify the pre-ictal brain stage, which occurs before a
seizure event. Patient-independent seizure prediction models are designed to
offer accurate performance across multiple subjects within a dataset, and have
been identified as a real-world solution to the seizure prediction problem.
However, little attention has been given for designing such models to adapt to
the high inter-subject variability in EEG data. Methods: We propose two
patient-independent deep learning architectures with different learning
strategies that can learn a global function utilizing data from multiple
subjects. Results: Proposed models achieve state-of-the-art performance for
seizure prediction on the CHB-MIT-EEG dataset, demonstrating 88.81% and 91.54%
accuracy respectively. Conclusions: The Siamese model trained on the proposed
learning strategy is able to learn patterns related to patient variations in
data while predicting seizures. Significance: Our models show superior
performance for patient-independent seizure prediction, and the same
architecture can be used as a patient-specific classifier after model
adaptation. We are the first study that employs model interpretation to
understand classifier behavior for the task for seizure prediction, and we also
show that the MFCC feature map utilized by our models contains predictive
biomarkers related to interictal and pre-ictal brain states.
</p>
<a href="http://arxiv.org/abs/2011.09581" target="_blank">arXiv:2011.09581</a> [<a href="http://arxiv.org/pdf/2011.09581" target="_blank">pdf</a>]

<h2>ACRONYM: A Large-Scale Grasp Dataset Based on Simulation. (arXiv:2011.09584v1 [cs.RO])</h2>
<h3>Clemens Eppner, Arsalan Mousavian, Dieter Fox</h3>
<p>We introduce ACRONYM, a dataset for robot grasp planning based on physics
simulation. The dataset contains 17.7M parallel-jaw grasps, spanning 8872
objects from 262 different categories, each labeled with the grasp result
obtained from a physics simulator. We show the value of this large and diverse
dataset by using it to train two state-of-the-art learning-based grasp planning
algorithms. Grasp performance improves significantly when compared to the
original smaller dataset. Data and tools can be accessed at
https://sites.google.com/nvidia.com/graspdataset.
</p>
<a href="http://arxiv.org/abs/2011.09584" target="_blank">arXiv:2011.09584</a> [<a href="http://arxiv.org/pdf/2011.09584" target="_blank">pdf</a>]

<h2>SAFARI: Safe and Active Robot Imitation Learning with Imagination. (arXiv:2011.09586v1 [cs.RO])</h2>
<h3>Norman Di Palo, Edward Johns</h3>
<p>One of the main issues in Imitation Learning is the erroneous behavior of an
agent when facing out-of-distribution situations, not covered by the set of
demonstrations given by the expert. In this work, we tackle this problem by
introducing a novel active learning and control algorithm, SAFARI. During
training, it allows an agent to request further human demonstrations when these
out-of-distribution situations are met. At deployment, it combines model-free
acting using behavioural cloning with model-based planning to reduce
state-distribution shift, using future state reconstruction as a test for state
familiarity. We empirically demonstrate how this method increases the
performance on a set of manipulation tasks with respect to passive Imitation
Learning, by gathering more informative demonstrations and by minimizing
state-distribution shift at test time. We also show how this method enables the
agent to autonomously predict failure rapidly and safely.
</p>
<a href="http://arxiv.org/abs/2011.09586" target="_blank">arXiv:2011.09586</a> [<a href="http://arxiv.org/pdf/2011.09586" target="_blank">pdf</a>]

<h2>Beyond Pinball Loss: Quantile Methods for Calibrated Uncertainty Quantification. (arXiv:2011.09588v1 [cs.LG])</h2>
<h3>Youngseog Chung, Willie Neiswanger, Ian Char, Jeff Schneider</h3>
<p>Among the many ways of quantifying uncertainty in a regression setting,
specifying the full quantile function is attractive, as quantiles are amenable
to interpretation and evaluation. A model that predicts the true conditional
quantiles for each input, at all quantile levels, presents a correct and
efficient representation of the underlying uncertainty. To achieve this, many
current quantile-based methods focus on optimizing the so-called pinball loss.
However, this loss restricts the scope of applicable regression models, limits
the ability to target many desirable properties (e.g. calibration, sharpness,
centered intervals), and may produce poor conditional quantiles. In this work,
we develop new quantile methods that address these shortcomings. In particular,
we propose methods that can apply to any class of regression model, allow for
selecting a Pareto-optimal trade-off between calibration and sharpness,
optimize for calibration of centered intervals, and produce more accurate
conditional quantiles. We provide a thorough experimental evaluation of our
methods, which includes a high dimensional uncertainty quantification task in
nuclear fusion.
</p>
<a href="http://arxiv.org/abs/2011.09588" target="_blank">arXiv:2011.09588</a> [<a href="http://arxiv.org/pdf/2011.09588" target="_blank">pdf</a>]

<h2>Variational Bayes Neural Network: Posterior Consistency, Classification Accuracy and Computational Challenges. (arXiv:2011.09592v1 [stat.ML])</h2>
<h3>Shrijita Bhattacharya, Zihuan Liu, Tapabrata Maiti</h3>
<p>Bayesian neural network models (BNN) have re-surged in recent years due to
the advancement of scalable computations and its utility in solving complex
prediction problems in a wide variety of applications. Despite the popularity
and usefulness of BNN, the conventional Markov Chain Monte Carlo based
implementation suffers from high computational cost, limiting the use of this
powerful technique in large scale studies. The variational Bayes inference has
become a viable alternative to circumvent some of the computational issues.
Although the approach is popular in machine learning, its application in
statistics is somewhat limited. This paper develops a variational Bayesian
neural network estimation methodology and related statistical theory. The
numerical algorithms and their implementational are discussed in detail. The
theory for posterior consistency, a desirable property in nonparametric
Bayesian statistics, is also developed. This theory provides an assessment of
prediction accuracy and guidelines for characterizing the prior distributions
and variational family. The loss of using a variational posterior over the true
posterior has also been quantified. The development is motivated by an
important biomedical engineering application, namely building predictive tools
for the transition from mild cognitive impairment to Alzheimer's disease. The
predictors are multi-modal and may involve complex interactive relations.
</p>
<a href="http://arxiv.org/abs/2011.09592" target="_blank">arXiv:2011.09592</a> [<a href="http://arxiv.org/pdf/2011.09592" target="_blank">pdf</a>]

<h2>Deep Multi-view Depth Estimation with Predicted Uncertainty. (arXiv:2011.09594v1 [cs.CV])</h2>
<h3>Tong Ke, Tien Do, Khiem Vuong, Kourosh Sartipi, Stergios I. Roumeliotis</h3>
<p>In this paper, we address the problem of estimating dense depth from a
sequence of images using deep neural networks. Specifically, we employ a
dense-optical-flow network to compute correspondences and then triangulate the
point cloud to obtain an initial depth map. Parts of the point cloud, however,
may be less accurate than others due to lack of common observations or small
baseline-to-depth ratio. To further increase the triangulation accuracy, we
introduce a depth-refinement network (DRN) that optimizes the initial depth map
based on the image's contextual cues. In particular, the DRN contains an
iterative refinement module (IRM) that improves the depth accuracy over
iterations by refining the deep features. Lastly, the DRN also predicts the
uncertainty in the refined depths, which is desirable in applications such as
measurement selection for scene reconstruction. We show experimentally that our
algorithm outperforms state-of-the-art approaches in terms of depth accuracy,
and verify that our predicted uncertainty is highly correlated to the actual
depth error.
</p>
<a href="http://arxiv.org/abs/2011.09594" target="_blank">arXiv:2011.09594</a> [<a href="http://arxiv.org/pdf/2011.09594" target="_blank">pdf</a>]

<h2>Robustness to Missing Features using Hierarchical Clustering with Split Neural Networks. (arXiv:2011.09596v1 [cs.LG])</h2>
<h3>Rishab Khincha, Utkarsh Sarawgi, Wazeer Zulfikar, Pattie Maes</h3>
<p>The problem of missing data has been persistent for a long time and poses a
major obstacle in machine learning and statistical data analysis. Past works in
this field have tried using various data imputation techniques to fill in the
missing data, or training neural networks (NNs) with the missing data. In this
work, we propose a simple yet effective approach that clusters similar input
features together using hierarchical clustering and then trains proportionately
split neural networks with a joint loss. We evaluate this approach on a series
of benchmark datasets and show promising improvements even with simple
imputation techniques. We attribute this to learning through clusters of
similar features in our model architecture. The source code is available at
https://github.com/usarawgi911/Robustness-to-Missing-Features
</p>
<a href="http://arxiv.org/abs/2011.09596" target="_blank">arXiv:2011.09596</a> [<a href="http://arxiv.org/pdf/2011.09596" target="_blank">pdf</a>]

<h2>Bidirectional RNN-based Few Shot Learning for 3D Medical Image Segmentation. (arXiv:2011.09608v1 [cs.CV])</h2>
<h3>Soopil Kim, Sion An, Philip Chikontwe, Sang Hyun Park</h3>
<p>Segmentation of organs of interest in 3D medical images is necessary for
accurate diagnosis and longitudinal studies. Though recent advances using deep
learning have shown success for many segmentation tasks, large datasets are
required for high performance and the annotation process is both time consuming
and labor intensive. In this paper, we propose a 3D few shot segmentation
framework for accurate organ segmentation using limited training samples of the
target organ annotation. To achieve this, a U-Net like network is designed to
predict segmentation by learning the relationship between 2D slices of support
data and a query image, including a bidirectional gated recurrent unit (GRU)
that learns consistency of encoded features between adjacent slices. Also, we
introduce a transfer learning method to adapt the characteristics of the target
image and organ by updating the model before testing with arbitrary support and
query data sampled from the support data. We evaluate our proposed model using
three 3D CT datasets with annotations of different organs. Our model yielded
significantly improved performance over state-of-the-art few shot segmentation
models and was comparable to a fully supervised model trained with more target
training data.
</p>
<a href="http://arxiv.org/abs/2011.09608" target="_blank">arXiv:2011.09608</a> [<a href="http://arxiv.org/pdf/2011.09608" target="_blank">pdf</a>]

<h2>Abnormal Event Detection in Urban Surveillance Videos Using GAN and Transfer Learning. (arXiv:2011.09619v1 [cs.CV])</h2>
<h3>Ali Atghaei, Soroush Ziaeinejad, Mohammad Rahmati</h3>
<p>Abnormal event detection (AED) in urban surveillance videos has multiple
challenges. Unlike other computer vision problems, the AED is not solely
dependent on the content of frames. It also depends on the appearance of the
objects and their movements in the scene. Various methods have been proposed to
address the AED problem. Among those, deep learning based methods show the best
results. This paper is based on deep learning methods and provides an effective
way to detect and locate abnormal events in videos by handling spatio temporal
data. This paper uses generative adversarial networks (GANs) and performs
transfer learning algorithms on pre trained convolutional neural network (CNN)
which result in an accurate and efficient model. The efficiency of the model is
further improved by processing the optical flow information of the video. This
paper runs experiments on two benchmark datasets for AED problem (UCSD Peds1
and UCSD Peds2) and compares the results with other previous methods. The
comparisons are based on various criteria such as area under curve (AUC) and
true positive rate (TPR). Experimental results show that the proposed method
can effectively detect and locate abnormal events in crowd scenes.
</p>
<a href="http://arxiv.org/abs/2011.09619" target="_blank">arXiv:2011.09619</a> [<a href="http://arxiv.org/pdf/2011.09619" target="_blank">pdf</a>]

<h2>Watch and Learn: Mapping Language and Noisy Real-world Videos with Self-supervision. (arXiv:2011.09634v1 [cs.CV])</h2>
<h3>Yujie Zhong, Linhai Xie, Sen Wang, Lucia Specia, Yishu Miao</h3>
<p>In this paper, we teach machines to understand visuals and natural language
by learning the mapping between sentences and noisy video snippets without
explicit annotations. Firstly, we define a self-supervised learning framework
that captures the cross-modal information. A novel adversarial learning module
is then introduced to explicitly handle the noises in the natural videos, where
the subtitle sentences are not guaranteed to be strongly corresponded to the
video snippets. For training and evaluation, we contribute a new dataset
`ApartmenTour' that contains a large number of online videos and subtitles. We
carry out experiments on the bidirectional retrieval tasks between sentences
and videos, and the results demonstrate that our proposed model achieves the
state-of-the-art performance on both retrieval tasks and exceeds several strong
baselines. The dataset will be released soon.
</p>
<a href="http://arxiv.org/abs/2011.09634" target="_blank">arXiv:2011.09634</a> [<a href="http://arxiv.org/pdf/2011.09634" target="_blank">pdf</a>]

<h2>Node Similarity Preserving Graph Convolutional Networks. (arXiv:2011.09643v1 [cs.LG])</h2>
<h3>Wei Jin, Tyler Derr, Yiqi Wang, Yao Ma, Zitao Liu, Jiliang Tang</h3>
<p>Graph Neural Networks (GNNs) have achieved tremendous success in various
real-world applications due to their strong ability in graph representation
learning. GNNs explore the graph structure and node features by aggregating and
transforming information within node neighborhoods. However, through
theoretical and empirical analysis, we reveal that the aggregation process of
GNNs tends to destroy node similarity in the original feature space. There are
many scenarios where node similarity plays a crucial role. Thus, it has
motivated the proposed framework SimP-GCN that can effectively and efficiently
preserve node similarity while exploiting graph structure. Specifically, to
balance information from graph structure and node features, we propose a
feature similarity preserving aggregation which adaptively integrates graph
structure and node features. Furthermore, we employ self-supervised learning to
explicitly capture the complex feature similarity and dissimilarity relations
between nodes. We validate the effectiveness of SimP-GCN on seven benchmark
datasets including three assortative and four disassorative graphs. The results
demonstrate that SimP-GCN outperforms representative baselines. Further probe
shows various advantages of the proposed framework. The implementation of
SimP-GCN is available at \url{https://github.com/ChandlerBang/SimP-GCN}.
</p>
<a href="http://arxiv.org/abs/2011.09643" target="_blank">arXiv:2011.09643</a> [<a href="http://arxiv.org/pdf/2011.09643" target="_blank">pdf</a>]

<h2>RADAR-X: An Interactive Interface Pairing Contrastive Explanations with Revised Plan Suggestions. (arXiv:2011.09644v1 [cs.AI])</h2>
<h3>Karthik Valmeekam, Sarath Sreedharan, Sailik Sengupta, Subbarao Kambhampati</h3>
<p>Empowering decision support systems with automated planning has received
significant recognition in the planning community. The central idea for such
systems is to augment the capabilities of the human-in-the-loop with automated
planning techniques and provide timely support to enhance the decision-making
experience. In addition to this, an effective decision support system must be
able to provide intuitive explanations based on specific queries on proposed
decisions to its end users. This makes decision-support systems an ideal
test-bed to study the effectiveness of various XAIP techniques being developed
in the community. To this end, we present our decision support system RADAR-X
that extends RADAR (Grover et al. 2020) by allowing the user to participate in
an interactive explanatory dialogue with the system. Specifically, we allow the
user to ask for contrastive explanations, wherein the user can try to
understand why a specific plan was chosen over an alternative (referred to as
the foil). Furthermore, we use the foil raised as evidence for unspecified user
preferences and use it to further refine plan suggestions.
</p>
<a href="http://arxiv.org/abs/2011.09644" target="_blank">arXiv:2011.09644</a> [<a href="http://arxiv.org/pdf/2011.09644" target="_blank">pdf</a>]

<h2>Finding the Homology of Decision Boundaries with Active Learning. (arXiv:2011.09645v1 [cs.LG])</h2>
<h3>Weizhi Li, Gautam Dasarathy, Karthikeyan Natesan Ramamurthy, Visar Berisha</h3>
<p>Accurately and efficiently characterizing the decision boundary of
classifiers is important for problems related to model selection and
meta-learning. Inspired by topological data analysis, the characterization of
decision boundaries using their homology has recently emerged as a general and
powerful tool. In this paper, we propose an active learning algorithm to
recover the homology of decision boundaries. Our algorithm sequentially and
adaptively selects which samples it requires the labels of. We theoretically
analyze the proposed framework and show that the query complexity of our active
learning algorithm depends naturally on the intrinsic complexity of the
underlying manifold. We demonstrate the effectiveness of our framework in
selecting best-performing machine learning models for datasets just using their
respective homological summaries. Experiments on several standard datasets show
the sample complexity improvement in recovering the homology and demonstrate
the practical utility of the framework for model selection. Source code for our
algorithms and experimental results is available at
https://github.com/wayne0908/Active-Learning-Homology.
</p>
<a href="http://arxiv.org/abs/2011.09645" target="_blank">arXiv:2011.09645</a> [<a href="http://arxiv.org/pdf/2011.09645" target="_blank">pdf</a>]

<h2>HMFlow: Hybrid Matching Optical Flow Network for Small and Fast-Moving Objects. (arXiv:2011.09654v1 [cs.CV])</h2>
<h3>Suihanjin Yu, Youmin Zhang, Chen Wang, Xiao Bai, Liang Zhang, Edwin R. Hancock</h3>
<p>In optical flow estimation task, coarse-to-fine (C2F) warping strategy is
widely used to deal with the large displacement problem and provides efficiency
and speed. However, limited by the small search range between the first images
and warped second images, current coarse-to-fine optical flow networks fail to
capture small and fast-moving objects which disappear at coarse resolution
levels. To address this problem, we introduce a lightweight but effective
Global Matching Component (GMC) to grab global matching features. We propose a
new Hybrid Matching Optical Flow Network (HMFlow) by integrating GMC into
existing coarse-to-fine networks seamlessly. Besides keeping in high accuracy
and small model size, our proposed HMFlow can apply global matching features to
guide the network to discover the small and fast-moving objects mismatched by
local matching features. We also build a new dataset, named Small and
Fast-Moving Chairs (SFChairs), for evaluation. The experimental results show
that our proposed network achieves considerable performance, especially at
regions with small and fast-moving objects.
</p>
<a href="http://arxiv.org/abs/2011.09654" target="_blank">arXiv:2011.09654</a> [<a href="http://arxiv.org/pdf/2011.09654" target="_blank">pdf</a>]

<h2>FedEval: A Benchmark System with a Comprehensive Evaluation Model for Federated Learning. (arXiv:2011.09655v1 [cs.LG])</h2>
<h3>Di Chai, Leye Wang, Kai Chen, Qiang Yang</h3>
<p>As an innovative solution for privacy-preserving machine learning (ML),
federated learning (FL) is attracting much attention from research and industry
areas. While new technologies proposed in the past few years do evolve the FL
area, unfortunately, the evaluation results presented in these works fall short
in integrity and are hardly comparable because of the inconsistent evaluation
metrics and the lack of a common platform. In this paper, we propose a
comprehensive evaluation framework for FL systems. Specifically, we first
introduce the ACTPR model, which defines five metrics that cannot be excluded
in FL evaluation, including Accuracy, Communication, Time efficiency, Privacy,
and Robustness. Then we design and implement a benchmarking system called
FedEval, which enables the systematic evaluation and comparison of existing
works under consistent experimental conditions. We then provide an in-depth
benchmarking study between the two most widely-used FL mechanisms, FedSGD and
FedAvg. The benchmarking results show that FedSGD and FedAvg both have
advantages and disadvantages under the ACTPR model. For example, FedSGD is
barely influenced by the none independent and identically distributed (non-IID)
data problem, but FedAvg suffers from a decline in accuracy of up to 9% in our
experiments. On the other hand, FedAvg is more efficient than FedSGD regarding
time consumption and communication. Lastly, we excavate a set of take-away
conclusions, which are very helpful for researchers in the FL area.
</p>
<a href="http://arxiv.org/abs/2011.09655" target="_blank">arXiv:2011.09655</a> [<a href="http://arxiv.org/pdf/2011.09655" target="_blank">pdf</a>]

<h2>Modeling Fashion Influence from Photos. (arXiv:2011.09663v1 [cs.CV])</h2>
<h3>Ziad Al-Halah, Kristen Grauman</h3>
<p>The evolution of clothing styles and their migration across the world is
intriguing, yet difficult to describe quantitatively. We propose to discover
and quantify fashion influences from catalog and social media photos. We
explore fashion influence along two channels: geolocation and fashion brands.
We introduce an approach that detects which of these entities influence which
other entities in terms of propagating their styles. We then leverage the
discovered influence patterns to inform a novel forecasting model that predicts
the future popularity of any given style within any given city or brand. To
demonstrate our idea, we leverage public large-scale datasets of 7.7M Instagram
photos from 44 major world cities (where styles are worn with variable
frequency) as well as 41K Amazon product photos (where styles are purchased
with variable frequency). Our model learns directly from the image data how
styles move between locations and how certain brands affect each other's
designs in a predictable way. The discovered influence relationships reveal how
both cities and brands exert and receive fashion influence for an array of
visual styles inferred from the images. Furthermore, the proposed forecasting
model achieves state-of-the-art results for challenging style forecasting
tasks. Our results indicate the advantage of grounding visual style evolution
both spatially and temporally, and for the first time, they quantify the
propagation of inter-brand and inter-city influences.
</p>
<a href="http://arxiv.org/abs/2011.09663" target="_blank">arXiv:2011.09663</a> [<a href="http://arxiv.org/pdf/2011.09663" target="_blank">pdf</a>]

<h2>Dense Label Encoding for Boundary Discontinuity Free Rotation Detection. (arXiv:2011.09670v1 [cs.CV])</h2>
<h3>Xue Yang, Liping Hou, Yue Zhou, Wentao Wang, Junchi Yan</h3>
<p>Rotation detection serves as a fundamental building block in many visual
applications involving aerial image, scene text, and face etc. Differing from
the dominant regression-based approaches for orientation estimation, this paper
explores a relatively less-studied methodology based on classification. The
hope is to inherently dismiss the boundary discontinuity issue as encountered
by the regression-based detectors. We propose new techniques to push its
frontier in two aspects: i) new encoding mechanism: the design of two Densely
Coded Labels (DCL) for angle classification, to replace the Sparsely Coded
Label (SCL) in existing classification-based detectors, leading to three times
training speed increase as empirically observed across benchmarks, further with
notable improvement in detection accuracy; ii) loss re-weighting: we propose
Angle Distance and Aspect Ratio Sensitive Weighting (ADARSW), which improves
the detection accuracy especially for square-like objects, by making DCL-based
detectors sensitive to angular distance and object's aspect ratio. Extensive
experiments and visual analysis on large-scale public datasets for aerial
images i.e. DOTA, UCAS-AOD, HRSC2016, as well as scene text dataset ICDAR2015
and MLT, show the effectiveness of our approach. The source code is available
at https://github.com/Thinklab-SJTU/DCL_RetinaNet_Tensorflow and is also
integrated in our open source rotation detection benchmark:
https://github.com/yangxue0827/RotationDetection.
</p>
<a href="http://arxiv.org/abs/2011.09670" target="_blank">arXiv:2011.09670</a> [<a href="http://arxiv.org/pdf/2011.09670" target="_blank">pdf</a>]

<h2>Multi-Modal Subjective Context Modelling and Recognition. (arXiv:2011.09671v1 [cs.AI])</h2>
<h3>Qiang Shen, Stefano Teso, Wanyi Zhang, Hao Xu, Fausto Giunchiglia</h3>
<p>Applications like personal assistants need to be aware ofthe user's context,
e.g., where they are, what they are doing, and with whom. Context information
is usually inferred from sensor data, like GPS sensors and accelerometers on
the user's smartphone. This prediction task is known as context recognition. A
well-defined context model is fundamental for successful recognition. Existing
models, however, have two major limitations. First, they focus on few aspects,
like location or activity, meaning that recognition methods based onthem can
only compute and leverage few inter-aspect correlations. Second, existing
models typically assume that context is objective, whereas in most applications
context is best viewed from the user's perspective. Neglecting these factors
limits the usefulness of the context model and hinders recognition. We present
a novel ontological context model that captures five dimensions, namely time,
location, activity, social relations and object. Moreover, our model defines
three levels of description(objective context, machine context and subjective
context) that naturally support subjective annotations and reasoning.An initial
context recognition experiment on real-world data hints at the promise of our
model.
</p>
<a href="http://arxiv.org/abs/2011.09671" target="_blank">arXiv:2011.09671</a> [<a href="http://arxiv.org/pdf/2011.09671" target="_blank">pdf</a>]

<h2>Defocus Blur Detection via Salient Region Detection Prior. (arXiv:2011.09677v1 [cs.CV])</h2>
<h3>Ming Qian, Min Xia, Chunyi Sun, Zhiwei Wang, Liguo Weng</h3>
<p>Defocus blur always occurred in photos when people take photos by Digital
Single Lens Reflex Camera(DSLR), giving salient region and aesthetic pleasure.
Defocus blur Detection aims to separate the out-of-focus and depth-of-field
areas in photos, which is an important work in computer vision. Current works
for defocus blur detection mainly focus on the designing of networks, the
optimizing of the loss function, and the application of multi-stream strategy,
meanwhile, these works do not pay attention to the shortage of training data.
In this work, to address the above data-shortage problem, we turn to rethink
the relationship between two tasks: defocus blur detection and salient region
detection. In an image with bokeh effect, it is obvious that the salient region
and the depth-of-field area overlap in most cases. So we first train our
network on the salient region detection tasks, then transfer the pre-trained
model to the defocus blur detection tasks. Besides, we propose a novel network
for defocus blur detection. Experiments show that our transfer strategy works
well on many current models, and demonstrate the superiority of our network.
</p>
<a href="http://arxiv.org/abs/2011.09677" target="_blank">arXiv:2011.09677</a> [<a href="http://arxiv.org/pdf/2011.09677" target="_blank">pdf</a>]

<h2>Scalable Graph Neural Networks for Heterogeneous Graphs. (arXiv:2011.09679v1 [cs.LG])</h2>
<h3>Lingfan Yu, Jiajun Shen, Jinyang Li, Adam Lerer</h3>
<p>Graph neural networks (GNNs) are a popular class of parametric model for
learning over graph-structured data. Recent work has argued that GNNs primarily
use the graph for feature smoothing, and have shown competitive results on
benchmark tasks by simply operating on graph-smoothed node features, rather
than using end-to-end learned feature hierarchies that are challenging to scale
to large graphs. In this work, we ask whether these results can be extended to
heterogeneous graphs, which encode multiple types of relationship between
different entities. We propose Neighbor Averaging over Relation Subgraphs
(NARS), which trains a classifier on neighbor-averaged features for
randomly-sampled subgraphs of the "metagraph" of relations. We describe
optimizations to allow these sets of node features to be computed in a
memory-efficient way, both at training and inference time. NARS achieves a new
state of the art accuracy on several benchmark datasets, outperforming more
expensive GNN-based methods
</p>
<a href="http://arxiv.org/abs/2011.09679" target="_blank">arXiv:2011.09679</a> [<a href="http://arxiv.org/pdf/2011.09679" target="_blank">pdf</a>]

<h2>Categorical exploratory data analysis on goodness-of-fit issues. (arXiv:2011.09682v1 [stat.ML])</h2>
<h3>Sabrina Enriquez, Fushing Hsieh</h3>
<p>If the aphorism "All models are wrong"- George Box, continues to be true in
data analysis, particularly when analyzing real-world data, then we should
annotate this wisdom with visible and explainable data-driven patterns. Such
annotations can critically shed invaluable light on validity as well as
limitations of statistical modeling as a data analysis approach. In an effort
to avoid holding our real data to potentially unattainable or even unrealistic
theoretical structures, we propose to utilize the data analysis paradigm called
Categorical Exploratory Data Analysis (CEDA). We illustrate the merits of this
proposal with two real-world data sets from the perspective of goodness-of-fit.
In both data sets, the Normal distribution's bell shape seemingly fits rather
well by first glance. We apply CEDA to bring out where and how each data fits
or deviates from the model shape via several important distributional aspects.
We also demonstrate that CEDA affords a version of tree-based p-value, and
compare it with p-values based on traditional statistical approaches. Along our
data analysis, we invest computational efforts in making graphic display to
illuminate the advantages of using CEDA as one primary way of data analysis in
Data Science education.
</p>
<a href="http://arxiv.org/abs/2011.09682" target="_blank">arXiv:2011.09682</a> [<a href="http://arxiv.org/pdf/2011.09682" target="_blank">pdf</a>]

<h2>Learning Deep Video Stabilization without Optical Flow. (arXiv:2011.09697v1 [cs.CV])</h2>
<h3>Muhammad Kashif Ali, Sangjoon Yu, Tae Hyun Kim</h3>
<p>Learning the necessary high-level reasoning for video stabilization without
the help of optical flow has proved to be one of the most challenging tasks in
the field of computer vision. In this work, we present an iterative frame
interpolation strategy to generate a novel dataset that is diverse enough to
formulate video stabilization as a supervised learning problem unassisted by
optical flow. A major benefit of treating video stabilization as a pure RGB
based generative task over the conventional optical flow assisted approaches is
the preservation of content and resolution, which is usually obstructed in the
latter approaches. To do so, we provide a new video stabilization dataset and
train an efficient network that can produce competitive stabilization results
in a fraction of the time taken to do the same with the recent iterative frame
interpolation schema. Our method provides qualitatively and quantitatively
better results than those generated through state-of-the-art video
stabilization methods. To the best of our knowledge, this is the only work that
demonstrates the importance of perspective in formulating video stabilization
as a deep learning problem instead of replacing it with an inter-frame motion
measure
</p>
<a href="http://arxiv.org/abs/2011.09697" target="_blank">arXiv:2011.09697</a> [<a href="http://arxiv.org/pdf/2011.09697" target="_blank">pdf</a>]

<h2>Style Intervention: How to Achieve Spatial Disentanglement with Style-based Generators?. (arXiv:2011.09699v1 [cs.CV])</h2>
<h3>Yunfan Liu, Qi Li, Zhenan Sun, Tieniu Tan</h3>
<p>Generative Adversarial Networks (GANs) with style-based generators (e.g.
StyleGAN) successfully enable semantic control over image synthesis, and recent
studies have also revealed that interpretable image translations could be
obtained by modifying the latent code. However, in terms of the low-level image
content, traveling in the latent space would lead to `spatially entangled
changes' in corresponding images, which is undesirable in many real-world
applications where local editing is required. To solve this problem, we analyze
properties of the 'style space' and explore the possibility of controlling the
local translation with pre-trained style-based generators. Concretely, we
propose 'Style Intervention', a lightweight optimization-based algorithm which
could adapt to arbitrary input images and render natural translation effects
under flexible objectives. We verify the performance of the proposed framework
in facial attribute editing on high-resolution images, where both photo-realism
and consistency are required. Extensive qualitative results demonstrate the
effectiveness of our method, and quantitative measurements also show that the
proposed algorithm outperforms state-of-the-art benchmarks in various aspects.
</p>
<a href="http://arxiv.org/abs/2011.09699" target="_blank">arXiv:2011.09699</a> [<a href="http://arxiv.org/pdf/2011.09699" target="_blank">pdf</a>]

<h2>Latent-Separated Global Prediction for Learned Image Compression. (arXiv:2011.09704v1 [cs.CV])</h2>
<h3>Zongyu Guo, Zhizheng Zhang, Runsen Feng, Simeng Sun, Zhibo Chen</h3>
<p>Over the past several years, we have witnessed the impressive progress of
learned image compression. Recent learned image codecs are based on
auto-encoders, that first encode an image into low-dimensional latent
representations and then decode them for reconstruction. To capture spatial
dependencies in the latent space, prior works exploit hyperprior and spatial
context model to facilitate entropy estimation. However, they are hard to model
effective long-range dependencies of the latents. In this paper, we explore to
further reduce spatial redundancies among the latent variables by utilizing
cross-channel relationships for explicit global prediction in the latent space.
Obviously, it will generate bits overhead to transmit the prediction vectors
that indicate the global correlations between reference point and current
decoding point. Therefore, to avoid the transmission of overhead, we propose a
3-D global context model, which separates the latents into two channel groups.
Once the first group is decoded, the proposed module will leverage the known
group to model spatial correlations that guide the global prediction for the
unknown group and thus achieve more efficient entropy estimation. Besides, we
further adopt split attention module to build more powerful transform networks.
Experimental results demonstrate that our full image compression model
outperforms standard VVC/H.266 codec on Kodak dataset in terms of both PSNR and
MS-SSIM, yielding the state-of-the-art rate-distortion performance.
</p>
<a href="http://arxiv.org/abs/2011.09704" target="_blank">arXiv:2011.09704</a> [<a href="http://arxiv.org/pdf/2011.09704" target="_blank">pdf</a>]

<h2>Iterative Planning with Plan-Space Explanations: A Tool and User Study. (arXiv:2011.09705v1 [cs.AI])</h2>
<h3>Rebecca Eifler, J&#xf6;rg Hoffmann</h3>
<p>In a variety of application settings, the user preference for a planning task
- the precise optimization objective - is difficult to elicit. One possible
remedy is planning as an iterative process, allowing the user to iteratively
refine and modify example plans. A key step to support such a process are
explanations, answering user questions about the current plan. In particular, a
relevant kind of question is "Why does the plan you suggest not satisfy $p$?",
where p is a plan property desirable to the user. Note that such a question
pertains to plan space, i.e., the set of possible alternative plans. Adopting
the recent approach to answer such questions in terms of plan-property
dependencies, here we implement a tool and user interface for human-guided
iterative planning including plan-space explanations. The tool runs in standard
Web browsers, and provides simple user interfaces for both developers and
users. We conduct a first user study, whose outcome indicates the usefulness of
plan-property dependency explanations in iterative planning.
</p>
<a href="http://arxiv.org/abs/2011.09705" target="_blank">arXiv:2011.09705</a> [<a href="http://arxiv.org/pdf/2011.09705" target="_blank">pdf</a>]

<h2>Application of Deep Learning-based Interpolation Methods to Nearshore Bathymetry. (arXiv:2011.09707v1 [stat.ML])</h2>
<h3>Yizhou Qian, Mojtaba Forghani, Jonghyun Harry Lee, Matthew Farthing, Tyler Hesser, Peter Kitanidis, Eric Darve</h3>
<p>Nearshore bathymetry, the topography of the ocean floor in coastal zones, is
vital for predicting the surf zone hydrodynamics and for route planning to
avoid subsurface features. Hence, it is increasingly important for a wide
variety of applications, including shipping operations, coastal management, and
risk assessment. However, direct high resolution surveys of nearshore
bathymetry are rarely performed due to budget constraints and logistical
restrictions. Another option when only sparse observations are available is to
use Gaussian Process regression (GPR), also called Kriging. But GPR has
difficulties recognizing patterns with sharp gradients, like those found around
sand bars and submerged objects, especially when observations are sparse. In
this work, we present several deep learning-based techniques to estimate
nearshore bathymetry with sparse, multi-scale measurements. We propose a Deep
Neural Network (DNN) to compute posterior estimates of the nearshore
bathymetry, as well as a conditional Generative Adversarial Network (cGAN) that
samples from the posterior distribution. We train our neural networks based on
synthetic data generated from nearshore surveys provided by the U.S.\ Army
Corps of Engineer Field Research Facility (FRF) in Duck, North Carolina. We
compare our methods with Kriging on real surveys as well as surveys with
artificially added sharp gradients. Results show that direct estimation by DNN
gives better predictions than Kriging in this application. We use bootstrapping
with DNN for uncertainty quantification. We also propose a method, named
DNN-Kriging, that combines deep learning with Kriging and shows further
improvement of the posterior estimates.
</p>
<a href="http://arxiv.org/abs/2011.09707" target="_blank">arXiv:2011.09707</a> [<a href="http://arxiv.org/pdf/2011.09707" target="_blank">pdf</a>]

<h2>Wasserstein Learning of Determinantal Point Processes. (arXiv:2011.09712v1 [cs.LG])</h2>
<h3>Lucas Anquetil, Mike Gartrell, Alain Rakotomamonjy, Ugo Tanielian, Cl&#xe9;ment Calauz&#xe8;nes</h3>
<p>Determinantal point processes (DPPs) have received significant attention as
an elegant probabilistic model for discrete subset selection. Most prior work
on DPP learning focuses on maximum likelihood estimation (MLE). While efficient
and scalable, MLE approaches do not leverage any subset similarity information
and may fail to recover the true generative distribution of discrete data. In
this work, by deriving a differentiable relaxation of a DPP sampling algorithm,
we present a novel approach for learning DPPs that minimizes the Wasserstein
distance between the model and data composed of observed subsets. Through an
evaluation on a real-world dataset, we show that our Wasserstein learning
approach provides significantly improved predictive performance on a generative
task compared to DPPs trained using MLE.
</p>
<a href="http://arxiv.org/abs/2011.09712" target="_blank">arXiv:2011.09712</a> [<a href="http://arxiv.org/pdf/2011.09712" target="_blank">pdf</a>]

<h2>Adversarial Examples for $k$-Nearest Neighbor Classifiers Based on Higher-Order Voronoi Diagrams. (arXiv:2011.09719v1 [cs.LG])</h2>
<h3>Chawin Sitawarin, Evgenios M. Kornaropoulos, Dawn Song, David Wagner</h3>
<p>Adversarial examples are a widely studied phenomenon in machine learning
models. While most of the attention has been focused on neural networks, other
practical models also suffer from this issue. In this work, we propose an
algorithm for evaluating the adversarial robustness of $k$-nearest neighbor
classification, i.e., finding a minimum-norm adversarial example. Diverging
from previous proposals, we take a geometric approach by performing a search
that expands outwards from a given input point. On a high level, the search
radius expands to the nearby Voronoi cells until we find a cell that classifies
differently from the input point. To scale the algorithm to a large $k$, we
introduce approximation steps that find perturbations with smaller norm,
compared to the baselines, in a variety of datasets. Furthermore, we analyze
the structural properties of a dataset where our approach outperforms the
competition.
</p>
<a href="http://arxiv.org/abs/2011.09719" target="_blank">arXiv:2011.09719</a> [<a href="http://arxiv.org/pdf/2011.09719" target="_blank">pdf</a>]

<h2>Exploring Constraint Handling Techniques in Real-world Problems on MOEA/D with Limited Budget of Evaluations. (arXiv:2011.09722v1 [cs.AI])</h2>
<h3>Felipe Vaz, Yuri Lavinas, Claus Aranha, Marcelo Ladeira</h3>
<p>Finding good solutions for Multi-objective Optimization (MOPs) Problems is
considered a hard problem, especially when considering MOPs with constraints.
Thus, most of the works in the context of MOPs do not explore in-depth how
different constraints affect the performance of MOP solvers. Here, we focus on
exploring the effects of different Constraint Handling Techniques (CHTs) on
MOEA/D, a commonly used MOP solver when solving complex real-world MOPs.
Moreover, we introduce a simple and effective CHT focusing on the exploration
of the decision space, the Three Stage Penalty. We explore each of these CHTs
in MOEA/D on two simulated MOPs and six analytic MOPs (eight in total). The
results of this work indicate that while the best CHT is problem-dependent, our
new proposed Three Stage Penalty achieves competitive results and remarkable
performance in terms of hypervolume values in the hard simulated car design
MOP.
</p>
<a href="http://arxiv.org/abs/2011.09722" target="_blank">arXiv:2011.09722</a> [<a href="http://arxiv.org/pdf/2011.09722" target="_blank">pdf</a>]

<h2>Preparing Weather Data for Real-Time Building Energy Simulation. (arXiv:2011.09733v1 [stat.ML])</h2>
<h3>Maryam MeshkinKiya, Riccardo Paolini</h3>
<p>This study introduces a framework for quality control of measured weather
data, including anomaly detection, and infilling missing values. Weather data
is a fundamental input to building performance simulations, in which anomalous
values defect the results while missing data lead to an unexpected termination
of the simulation process. Traditionally, infilling missing values in weather
data is performed through periodic or linear interpolations. However, when
missing values exceed many consecutive hours, the accuracy of traditional
methods is subject to debate. This study demonstrates how Neural Networks can
increase the accuracy of data imputation when compared to other supervised
learning methods. The framework is validated by predicting missing temperature
and relative humidity data for an observation site, through a network of nearby
weather stations in Milan, Italy. Results show that the proposed method can
facilitate real-time building simulations with accurate and rapid quality
control.
</p>
<a href="http://arxiv.org/abs/2011.09733" target="_blank">arXiv:2011.09733</a> [<a href="http://arxiv.org/pdf/2011.09733" target="_blank">pdf</a>]

<h2>Face Forgery Detection by 3D Decomposition. (arXiv:2011.09737v1 [cs.CV])</h2>
<h3>Xiangyu Zhu, Hao Wang, Hongyan Fei, Zhen Lei, Stan Z. Li</h3>
<p>Detecting digital face manipulation has attracted extensive attention due to
fake media's potential harms to the public. However, recent advances have been
able to reduce the forgery signals to a low magnitude. Decomposition, which
reversibly decomposes an image into several constituent elements, is a
promising way to highlight the hidden forgery details. In this paper, we
consider a face image as the production of the intervention of the underlying
3D geometry and the lighting environment, and decompose it in a computer
graphics view. Specifically, by disentangling the face image into 3D shape,
common texture, identity texture, ambient light, and direct light, we find the
devil lies in the direct light and the identity texture. Based on this
observation, we propose to utilize facial detail, which is the combination of
direct light and identity texture, as the clue to detect the subtle forgery
patterns. Besides, we highlight the manipulated region with a supervised
attention mechanism and introduce a two-stream structure to exploit both face
image and facial detail together as a multi-modality task. Extensive
experiments indicate the effectiveness of the extra features extracted from the
facial detail, and our method achieves the state-of-the-art performance.
</p>
<a href="http://arxiv.org/abs/2011.09737" target="_blank">arXiv:2011.09737</a> [<a href="http://arxiv.org/pdf/2011.09737" target="_blank">pdf</a>]

<h2>SOCAIRE: Forecasting and Monitoring Urban Air Quality in Madrid. (arXiv:2011.09741v1 [cs.LG])</h2>
<h3>Rodrigo de Medrano, V&#xed;ctor de Buen Remiro, Jos&#xe9; L. Aznarte</h3>
<p>Air quality has become one of the main issues in public health and urban
planning management, due to the proven adverse effects of high pollutant
concentrations. Considering the mitigation measures that cities all over the
world are taking in order to face frequent low air quality episodes, the
capability of foreseeing future pollutant concentrations is of great
importance. Through this paper, we present SOCAIRE, an operational tool based
on a Bayesian and spatiotemporal ensemble of neural and statistical nested
models. SOCAIRE integrates endogenous and exogenous information in order to
predict and monitor future distributions of the concentration for several
pollutants in the city of Madrid. It focuses on modeling each and every
available component which might play a role in air quality: past concentrations
of pollutants, human activity, numerical pollution estimation, and numerical
weather predictions. This tool is currently in operation in Madrid, producing
daily air quality predictions for the next 48 hours and anticipating the
probability of the activation of the measures included in the city's official
air quality \no protocols through probabilistic inferences about compound
events.
</p>
<a href="http://arxiv.org/abs/2011.09741" target="_blank">arXiv:2011.09741</a> [<a href="http://arxiv.org/pdf/2011.09741" target="_blank">pdf</a>]

<h2>End-To-End Dilated Variational Autoencoder with Bottleneck Discriminative Loss for Sound Morphing -- A Preliminary Study. (arXiv:2011.09744v1 [cs.LG])</h2>
<h3>Matteo Lionello, Hendrik Purwins</h3>
<p>We present a preliminary study on an end-to-end variational autoencoder (VAE)
for sound morphing. Two VAE variants are compared: VAE with dilation layers
(DC-VAE) and VAE only with regular convolutional layers (CC-VAE). We combine
the following loss functions: 1) the time-domain mean-squared error for
reconstructing the input signal, 2) the Kullback-Leibler divergence to the
standard normal distribution in the bottleneck layer, and 3) the classification
loss calculated from the bottleneck representation. On a database of spoken
digits, we use 1-nearest neighbor classification to show that the sound classes
separate in the bottleneck layer. We introduce the Mel-frequency cepstrum
coefficient dynamic time warping (MFCC-DTW) deviation as a measure of how well
the VAE decoder projects the class center in the latent (bottleneck) layer to
the center of the sounds of that class in the audio domain. In terms of
MFCC-DTW deviation and 1-NN classification, DC-VAE outperforms CC-VAE. These
results for our parametrization and our dataset indicate that DC-VAE is more
suitable for sound morphing than CC-VAE, since the DC-VAE decoder better
preserves the topology when mapping from the audio domain to the latent space.
Examples are given both for morphing spoken digits and drum sounds.
</p>
<a href="http://arxiv.org/abs/2011.09744" target="_blank">arXiv:2011.09744</a> [<a href="http://arxiv.org/pdf/2011.09744" target="_blank">pdf</a>]

<h2>Energy Aware Deep Reinforcement Learning Scheduling for Sensors Correlated in Time and Space. (arXiv:2011.09747v1 [cs.LG])</h2>
<h3>Jernej Hribar, Andrei Marinescu, Alessandro Chiumento, Luiz A. DaSilva</h3>
<p>Millions of battery-powered sensors deployed for monitoring purposes in a
multitude of scenarios, e.g., agriculture, smart cities, industry, etc.,
require energy-efficient solutions to prolong their lifetime. When these
sensors observe a phenomenon distributed in space and evolving in time, it is
expected that collected observations will be correlated in time and space. In
this paper, we propose a Deep Reinforcement Learning (DRL) based scheduling
mechanism capable of taking advantage of correlated information. We design our
solution using the Deep Deterministic Policy Gradient (DDPG) algorithm. The
proposed mechanism is capable of determining the frequency with which sensors
should transmit their updates, to ensure accurate collection of observations,
while simultaneously considering the energy available. To evaluate our
scheduling mechanism, we use multiple datasets containing environmental
observations obtained in multiple real deployments. The real observations
enable us to model the environment with which the mechanism interacts as
realistically as possible. We show that our solution can significantly extend
the sensors' lifetime. We compare our mechanism to an idealized, all-knowing
scheduler to demonstrate that its performance is near-optimal. Additionally, we
highlight the unique feature of our design, energy-awareness, by displaying the
impact of sensors' energy levels on the frequency of updates.
</p>
<a href="http://arxiv.org/abs/2011.09747" target="_blank">arXiv:2011.09747</a> [<a href="http://arxiv.org/pdf/2011.09747" target="_blank">pdf</a>]

<h2>Online Model Selection for Reinforcement Learning with Function Approximation. (arXiv:2011.09750v1 [cs.LG])</h2>
<h3>Jonathan N. Lee, Aldo Pacchiano, Vidya Muthukumar, Weihao Kong, Emma Brunskill</h3>
<p>Deep reinforcement learning has achieved impressive successes yet often
requires a very large amount of interaction data. This result is perhaps
unsurprising, as using complicated function approximation often requires more
data to fit, and early theoretical results on linear Markov decision processes
provide regret bounds that scale with the dimension of the linear
approximation. Ideally, we would like to automatically identify the minimal
dimension of the approximation that is sufficient to encode an optimal policy.
Towards this end, we consider the problem of model selection in RL with
function approximation, given a set of candidate RL algorithms with known
regret guarantees. The learner's goal is to adapt to the complexity of the
optimal algorithm without knowing it \textit{a priori}. We present a
meta-algorithm that successively rejects increasingly complex models using a
simple statistical test. Given at least one candidate that satisfies
realizability, we prove the meta-algorithm adapts to the optimal complexity
with $\tilde{O}(L^{5/6} T^{2/3})$ regret compared to the optimal candidate's
$\tilde{O}(\sqrt T)$ regret, where $T$ is the number of episodes and $L$ is the
number of algorithms. The dimension and horizon dependencies remain optimal
with respect to the best candidate, and our meta-algorithmic approach is
flexible to incorporate multiple candidate algorithms and models. Finally, we
show that the meta-algorithm automatically admits significantly improved
instance-dependent regret bounds that depend on the gaps between the maximal
values attainable by the candidates.
</p>
<a href="http://arxiv.org/abs/2011.09750" target="_blank">arXiv:2011.09750</a> [<a href="http://arxiv.org/pdf/2011.09750" target="_blank">pdf</a>]

<h2>Active Inference and Behavior Trees for Reactive Action Planning and Execution in Robotics. (arXiv:2011.09756v1 [cs.RO])</h2>
<h3>Corrado Pezzato, Carlos Hernandez, Martijn Wisse</h3>
<p>This paper presents how the hybrid combination of behavior trees and the
neuroscientific principle of active inference can be used for action planning
and execution for reactive robot behaviors in dynamic environments. We show how
complex robotic tasks can be formulated as a free-energy minimisation problem,
and how state estimation and symbolic decision making are handled within the
same framework. The general behavior is specified offline through behavior
trees, where the leaf nodes represent desired states, not actions as in
classical behavior trees. The decision of which action to execute to reach a
state is left to the online active inference routine, in order to resolve
unexpected contingencies. This hybrid combination improves the robustness of
plans specified through behavior trees, while allowing to cope with the curse
of dimensionality in active inference. The properties of the proposed algorithm
are analysed in terms of robustness and convergence, and the theoretical
results are validated using a mobile manipulator in a retail environment.
</p>
<a href="http://arxiv.org/abs/2011.09756" target="_blank">arXiv:2011.09756</a> [<a href="http://arxiv.org/pdf/2011.09756" target="_blank">pdf</a>]

<h2>KD3A: Unsupervised Multi-Source Decentralized Domain Adaptation via Knowledge Distillation. (arXiv:2011.09757v1 [cs.LG])</h2>
<h3>Hao-Zhe Feng, Zhaoyang You, Minghao Chen, Tianye Zhang, Minfeng Zhu, Fei Wu, Chao Wu, Wei Chen</h3>
<p>Conventional unsupervised multi-source domain adaptation(UMDA) methods assume
all source domains can be accessed directly. This neglects the
privacy-preserving policy, that is,all the data and computations must be kept
decentralized.There exists three problems in this scenario: (1)Minimizing the
domain distance requires the pairwise calculation of the data from source and
target domains, which is not accessible.(2)The communication cost and privacy
security limit the application of UMDA methods (e.g.,the domain adversarial
training).(3)Since users have no authority to checkthe data quality, the
irrelevant or malicious source domainsare more likely to appear, which causes
negative transfer. In this study, we propose a privacy-preserving UMDA paradigm
named Knowledge Distillation based Decentralized Domain Adaptation(KD3A), which
performs domain adaptation through the knowledge distillation on models from
different source domains. KD3A solves the above problems with three
components:(1)A multi-source knowledge distillation method named Knowledge
Voteto learn high-quality domain consensus knowledge. (2)A dynamic weighting
strategy named Consensus Focusto identify both the malicious and irrelevant
domains.(3)A decentralized optimization strategy for computing domain distance
named BatchNorm MMD.The extensive experiments on DomainNet demonstrate that
KD3A is robust to the negative transfer and brings a 100x reduction of
communication cost compared with other decentralized UMDA methods. Moreover,
our KD3A significantly outperforms state-of-the-art UMDA approaches.
</p>
<a href="http://arxiv.org/abs/2011.09757" target="_blank">arXiv:2011.09757</a> [<a href="http://arxiv.org/pdf/2011.09757" target="_blank">pdf</a>]

<h2>Attention-Based Transformers for Instance Segmentation of Cells in Microstructures. (arXiv:2011.09763v1 [cs.CV])</h2>
<h3>Tim Prangemeier, Christoph Reich, Heinz Koeppl</h3>
<p>Detecting and segmenting object instances is a common task in biomedical
applications. Examples range from detecting lesions on functional magnetic
resonance images, to the detection of tumours in histopathological images and
extracting quantitative single-cell information from microscopy imagery, where
cell segmentation is a major bottleneck. Attention-based transformers are
state-of-the-art in a range of deep learning fields. They have recently been
proposed for segmentation tasks where they are beginning to outperforming other
methods. We present a novel attention-based cell detection transformer
(Cell-DETR) for direct end-to-end instance segmentation. While the segmentation
performance is on par with a state-of-the-art instance segmentation method,
Cell-DETR is simpler and faster. We showcase the method's contribution in a the
typical use case of segmenting yeast in microstructured environments, commonly
employed in systems or synthetic biology. For the specific use case, the
proposed method surpasses the state-of-the-art tools for semantic segmentation
and additionally predicts the individual object instances. The fast and
accurate instance segmentation performance increases the experimental
information yield for a posteriori data processing and makes online monitoring
of experiments and closed-loop optimal experimental design feasible.
</p>
<a href="http://arxiv.org/abs/2011.09763" target="_blank">arXiv:2011.09763</a> [<a href="http://arxiv.org/pdf/2011.09763" target="_blank">pdf</a>]

<h2>Foreground-Aware Relation Network for Geospatial Object Segmentation in High Spatial Resolution Remote Sensing Imagery. (arXiv:2011.09766v1 [cs.CV])</h2>
<h3>Zhuo Zheng, Yanfei Zhong, Junjue Wang, Ailong Ma</h3>
<p>Geospatial object segmentation, as a particular semantic segmentation task,
always faces with larger-scale variation, larger intra-class variance of
background, and foreground-background imbalance in the high spatial resolution
(HSR) remote sensing imagery. However, general semantic segmentation methods
mainly focus on scale variation in the natural scene, with inadequate
consideration of the other two problems that usually happen in the large area
earth observation scene. In this paper, we argue that the problems lie on the
lack of foreground modeling and propose a foreground-aware relation network
(FarSeg) from the perspectives of relation-based and optimization-based
foreground modeling, to alleviate the above two problems. From perspective of
relation, FarSeg enhances the discrimination of foreground features via
foreground-correlated contexts associated by learning foreground-scene
relation. Meanwhile, from perspective of optimization, a foreground-aware
optimization is proposed to focus on foreground examples and hard examples of
background during training for a balanced optimization. The experimental
results obtained using a large scale dataset suggest that the proposed method
is superior to the state-of-the-art general semantic segmentation methods and
achieves a better trade-off between speed and accuracy. Code has been made
available at: \url{https://github.com/Z-Zheng/FarSeg}.
</p>
<a href="http://arxiv.org/abs/2011.09766" target="_blank">arXiv:2011.09766</a> [<a href="http://arxiv.org/pdf/2011.09766" target="_blank">pdf</a>]

<h2>Scene text removal via cascaded text stroke detection and erasing. (arXiv:2011.09768v1 [cs.CV])</h2>
<h3>Xuewei Bian, Chaoqun Wang, Weize Quan, Juntao Ye, Xiaopeng Zhang, Dong-Ming Yan</h3>
<p>Recent learning-based approaches show promising performance improvement for
scene text removal task. However, these methods usually leave some remnants of
text and obtain visually unpleasant results. In this work, we propose a novel
"end-to-end" framework based on accurate text stroke detection. Specifically,
we decouple the text removal problem into text stroke detection and stroke
removal. We design a text stroke detection network and a text removal
generation network to solve these two sub-problems separately. Then, we combine
these two networks as a processing unit, and cascade this unit to obtain the
final model for text removal. Experimental results demonstrate that the
proposed method significantly outperforms the state-of-the-art approaches for
locating and erasing scene text. Since current publicly available datasets are
all synthetic and cannot properly measure the performance of different methods,
we therefore construct a new real-world dataset, which will be released to
facilitate the relevant research.
</p>
<a href="http://arxiv.org/abs/2011.09768" target="_blank">arXiv:2011.09768</a> [<a href="http://arxiv.org/pdf/2011.09768" target="_blank">pdf</a>]

<h2>Solving Footstep Planning as a Feasibility Problem using L1-norm Minimization. (arXiv:2011.09772v1 [cs.RO])</h2>
<h3>Daeun Song, Pierre Fernbach, Thomas Flayols, Andrea Del Prete, Nicolas Mansard, Steve Tonneau, Young J. Kim</h3>
<p>One challenge of legged locomotion on uneven terrains is to deal with both
the discrete problem of selecting a contact surface for each footstep and the
continuous problem of placing each footstep on the selected surface.
Consequently, footstep planning can be addressed with a Mixed Integer Program
(MIP), an elegant but computationally-demanding method, which can make it
unsuitable for online planning. We reformulate the MIP into a cardinality
problem, then approximate it as a computationally efficient l1-norm
minimisation, called SL1M. Moreover, we improve the performance and convergence
of SL1M by combining it with a sampling-based root trajectory planner to prune
irrelevant surface candidates. Our tests on the humanoid Talos in four
representative scenarios show that SL1M always converges faster than MIP. For
scenarios when the combinatorial complexity is small (&lt; 10 surfaces per step),
SL1M converges at least two times faster than MIP with no need for pruning. In
more complex cases, SL1M converges up to 100 times faster than MIP with the
help of pruning. Moreover, pruning can also improve the MIP computation time.
The versatility of the framework is shown with additional tests on the
quadruped robot ANYmal.
</p>
<a href="http://arxiv.org/abs/2011.09772" target="_blank">arXiv:2011.09772</a> [<a href="http://arxiv.org/pdf/2011.09772" target="_blank">pdf</a>]

<h2>A Temporal Convolution Network Approach to State-of-Charge Estimation in Li-ion Batteries. (arXiv:2011.09775v1 [cs.LG])</h2>
<h3>Aniruddh Herle, Janamejaya Channegowda, Dinakar Prabhu</h3>
<p>Electric Vehicle (EV) fleets have dramatically expanded over the past several
years. There has been significant increase in interest to electrify all modes
of transportation. EVs are primarily powered by Energy Storage Systems such as
Lithium-ion Battery packs. Total battery pack capacity translates to the
available range in an EV. State of Charge (SOC) is the ratio of available
battery capacity to total capacity and is expressed in percentages. It is
crucial to accurately estimate SOC to determine the available range in an EV
while it is in use. In this paper, a Temporal Convolution Network (TCN)
approach is taken to estimate SOC. This is the first implementation of TCNs for
the SOC estimation task. Estimation is carried out on various drive cycles such
as HWFET, LA92, UDDS and US06 drive cycles at 1 C and 25 {\deg}Celsius. It was
found that TCN architecture achieved an accuracy of 99.1%.
</p>
<a href="http://arxiv.org/abs/2011.09775" target="_blank">arXiv:2011.09775</a> [<a href="http://arxiv.org/pdf/2011.09775" target="_blank">pdf</a>]

<h2>Improving Bayesian Network Structure Learning in the Presence of Measurement Error. (arXiv:2011.09776v1 [cs.AI])</h2>
<h3>Yang Liu, Anthony C. Constantinou, ZhiGao Guo</h3>
<p>Structure learning algorithms that learn the graph of a Bayesian network from
observational data often do so by assuming the data correctly reflect the true
distribution of the variables. However, this assumption does not hold in the
presence of measurement error, which can lead to spurious edges. This is one of
the reasons why the synthetic performance of these algorithms often
overestimates real-world performance. This paper describes an algorithm that
can be added as an additional learning phase at the end of any structure
learning algorithm, and serves as a correction learning phase that removes
potential false positive edges. The results show that the proposed correction
algorithm successfully improves the graphical score of four well-established
structure learning algorithms spanning different classes of learning in the
presence of measurement error.
</p>
<a href="http://arxiv.org/abs/2011.09776" target="_blank">arXiv:2011.09776</a> [<a href="http://arxiv.org/pdf/2011.09776" target="_blank">pdf</a>]

<h2>Towards Spatio-Temporal Video Scene Text Detection via Temporal Clustering. (arXiv:2011.09781v1 [cs.CV])</h2>
<h3>Yuanqiang Cai, Chang Liu, Weiqiang Wang, Qixiang Ye</h3>
<p>With only bounding-box annotations in the spatial domain, existing video
scene text detection (VSTD) benchmarks lack temporal relation of text instances
among video frames, which hinders the development of video text-related
applications. In this paper, we systematically introduce a new large-scale
benchmark, named as STVText4, a well-designed spatial-temporal detection metric
(STDM), and a novel clustering-based baseline method, referred to as Temporal
Clustering (TC). STVText4 opens a challenging yet promising direction of VSTD,
termed as ST-VSTD, which targets at simultaneously detecting video scene texts
in both spatial and temporal domains. STVText4 contains more than 1.4 million
text instances from 161,347 video frames of 106 videos, where each instance is
annotated with not only spatial bounding box and temporal range but also four
intrinsic attributes, including legibility, density, scale, and lifecycle, to
facilitate the community. With continuous propagation of identical texts in the
video sequence, TC can accurately output the spatial quadrilateral and temporal
range of the texts, which sets a strong baseline for ST-VSTD. Experiments
demonstrate the efficacy of our method and the great academic and practical
value of the STVText4. The dataset and code will be available soon.
</p>
<a href="http://arxiv.org/abs/2011.09781" target="_blank">arXiv:2011.09781</a> [<a href="http://arxiv.org/pdf/2011.09781" target="_blank">pdf</a>]

<h2>DeepMorph: A System for Hiding Bitstrings in Morphable Vector Drawings. (arXiv:2011.09783v1 [cs.CV])</h2>
<h3>S&#xf8;ren Rasmussen, Karsten &#xd8;stergaard Noe, Oliver Gyldenberg Hjermitslev, Henrik Pedersen</h3>
<p>We introduce DeepMorph, an information embedding technique for vector
drawings. Provided a vector drawing, such as a Scalable Vector Graphics (SVG)
file, our method embeds bitstrings in the image by perturbing the drawing
primitives (lines, circles, etc.). This results in a morphed image that can be
decoded to recover the original bitstring. The use-case is similar to that of
the well-known QR code, but our solution provides creatives with artistic
freedom to transfer digital information via drawings of their own design. The
method comprises two neural networks, which are trained jointly: an encoder
network that transforms a bitstring into a perturbation of the drawing
primitives, and a decoder network that recovers the bitstring from an image of
the morphed drawing. To enable end-to-end training via back propagation, we
introduce a soft rasterizer, which is differentiable with respect to
perturbations of the drawing primitives. In order to add robustness towards
real-world image capture conditions, image corruptions are injected between the
soft rasterizer and the decoder. Further, the addition of an object detection
and camera pose estimation system enables decoding of drawings in complex
scenes as well as use of the drawings as markers for use in augmented reality
applications. We demonstrate that our method reliably recovers bitstrings from
real-world photos of printed drawings, thereby providing a novel solution for
creatives to transfer digital information via artistic imagery.
</p>
<a href="http://arxiv.org/abs/2011.09783" target="_blank">arXiv:2011.09783</a> [<a href="http://arxiv.org/pdf/2011.09783" target="_blank">pdf</a>]

<h2>An Experimental Study of Semantic Continuity for Deep Learning Models. (arXiv:2011.09789v1 [cs.LG])</h2>
<h3>Shangxi Wu, Jitao Sang, Xian Zhao, Lizhang Chen</h3>
<p>Deep learning models suffer from the problem of semantic discontinuity: small
perturbations in the input space tend to cause semantic-level interference to
the model output. We argue that the semantic discontinuity results from these
inappropriate training targets and contributes to notorious issues such as
adversarial robustness, interpretability, etc. We first conduct data analysis
to provide evidence of semantic discontinuity in existing deep learning models,
and then design a simple semantic continuity constraint which theoretically
enables models to obtain smooth gradients and learn semantic-oriented features.
Qualitative and quantitative experiments prove that semantically continuous
models successfully reduce the use of non-semantic information, which further
contributes to the improvement in adversarial robustness, interpretability,
model transfer, and machine bias.
</p>
<a href="http://arxiv.org/abs/2011.09789" target="_blank">arXiv:2011.09789</a> [<a href="http://arxiv.org/pdf/2011.09789" target="_blank">pdf</a>]

<h2>The Robot Household Marathon Experiment. (arXiv:2011.09792v1 [cs.RO])</h2>
<h3>Gayane Kazhoyan, Simon Stelter, Franklin Kenghagho Kenfack, Sebastian Koralewski, Michael Beetz</h3>
<p>In this paper, we present an experiment, designed to investigate and evaluate
the scalability and the robustness aspects of mobile manipulation. The
experiment involves performing variations of mobile pick and place actions and
opening/closing environment containers in a human household. The robot is
expected to act completely autonomously for extended periods of time. We
discuss the scientific challenges raised by the experiment as well as present
our robotic system that can address these challenges and successfully perform
all the tasks of the experiment. We present empirical results and the lessons
learned as well as discuss where we hit limitations.
</p>
<a href="http://arxiv.org/abs/2011.09792" target="_blank">arXiv:2011.09792</a> [<a href="http://arxiv.org/pdf/2011.09792" target="_blank">pdf</a>]

<h2>Unifying Instance and Panoptic Segmentation with Dynamic Rank-1 Convolutions. (arXiv:2011.09796v1 [cs.CV])</h2>
<h3>Hao Chen, Chunhua Shen, Zhi Tian</h3>
<p>Recently, fully-convolutional one-stage networks have shown superior
performance comparing to two-stage frameworks for instance segmentation as
typically they can generate higher-quality mask predictions with less
computation. In addition, their simple design opens up new opportunities for
joint multi-task learning. In this paper, we demonstrate that adding a single
classification layer for semantic segmentation, fully-convolutional instance
segmentation networks can achieve state-of-the-art panoptic segmentation
quality. This is made possible by our novel dynamic rank-1 convolution
(DR1Conv), a novel dynamic module that can efficiently merge high-level context
information with low-level detailed features which is beneficial for both
semantic and instance segmentation. Importantly, the proposed new method,
termed DR1Mask, can perform panoptic segmentation by adding a single layer. To
our knowledge, DR1Mask is the first panoptic segmentation framework that
exploits a shared feature map for both instance and semantic segmentation by
considering both efficacy and efficiency. Not only our framework is much more
efficient -- twice as fast as previous best two-branch approaches, but also the
unified framework opens up opportunities for using the same context module to
improve the performance for both tasks. As a byproduct, when performing
instance segmentation alone, DR1Mask is 10% faster and 1 point in mAP more
accurate than previous state-of-the-art instance segmentation network
BlendMask. Code is available at: https://git.io/AdelaiDet
</p>
<a href="http://arxiv.org/abs/2011.09796" target="_blank">arXiv:2011.09796</a> [<a href="http://arxiv.org/pdf/2011.09796" target="_blank">pdf</a>]

<h2>Unmixing Convolutional Features for Crisp Edge Detection. (arXiv:2011.09808v1 [cs.CV])</h2>
<h3>Linxi Huan, Xianwei Zheng, Nan Xue, Wei He, Jianya Gong, Gui-Song Xia</h3>
<p>This paper presents a context-aware tracing strategy (CATS) for crisp edge
detection with deep edge detectors, based on an observation that the
localization ambiguity of deep edge detectors is mainly caused by the mixing
phenomenon of convolutional neural networks: feature mixing in edge
classification and side mixing during fusing side predictions. The CATS
consists of two modules: a novel tracing loss that performs feature unmixing by
tracing boundaries for better side edge learning, and a context-aware fusion
block that tackles the side mixing by aggregating the complementary merits of
learned side edges. Experiments demonstrate that the proposed CATS can be
integrated into modern deep edge detectors to improve localization accuracy.
With the vanilla VGG16 backbone, in terms of BSDS500 dataset, our CATS improves
the F-measure (ODS) of the RCF and BDCN deep edge detectors by 12% and 6%
respectively when evaluating without using the morphological non-maximal
suppression scheme for edge detection.
</p>
<a href="http://arxiv.org/abs/2011.09808" target="_blank">arXiv:2011.09808</a> [<a href="http://arxiv.org/pdf/2011.09808" target="_blank">pdf</a>]

<h2>Lifelong Knowledge Learning in Rule-based Dialogue Systems. (arXiv:2011.09811v1 [cs.AI])</h2>
<h3>Bing Liu, Chuhe Mei</h3>
<p>One of the main weaknesses of current chatbots or dialogue systems is that
they do not learn online during conversations after they are deployed. This is
a major loss of opportunity. Clearly, each human user has a great deal of
knowledge about the world that may be useful to others. If a chatbot can learn
from their users during chatting, it will greatly expand its knowledge base and
serve its users better. This paper proposes to build such a learning capability
in a rule-based chatbot so that it can continuously acquire new knowledge in
its chatting with users. This work is useful because many real-life deployed
chatbots are rule-based.
</p>
<a href="http://arxiv.org/abs/2011.09811" target="_blank">arXiv:2011.09811</a> [<a href="http://arxiv.org/pdf/2011.09811" target="_blank">pdf</a>]

<h2>Effective, Efficient and Robust Neural Architecture Search. (arXiv:2011.09820v1 [cs.LG])</h2>
<h3>Zhixiong Yue, Baijiong Lin, Xiaonan Huang, Yu Zhang</h3>
<p>Recent advances in adversarial attacks show the vulnerability of deep neural
networks searched by Neural Architecture Search (NAS). Although NAS methods can
find network architectures with the state-of-the-art performance, the
adversarial robustness and resource constraint are often ignored in NAS. To
solve this problem, we propose an Effective, Efficient, and Robust Neural
Architecture Search (E2RNAS) method to search a neural network architecture by
taking the performance, robustness, and resource constraint into consideration.
The objective function of the proposed E2RNAS method is formulated as a
bi-level multi-objective optimization problem with the upper-level problem as a
multi-objective optimization problem, which is different from existing NAS
methods. To solve the proposed objective function, we integrate the
multiple-gradient descent algorithm, a widely studied gradient-based
multi-objective optimization algorithm, with the bi-level optimization.
Experiments on benchmark datasets show that the proposed E2RNAS method can find
adversarially robust architectures with optimized model size and comparable
classification accuracy.
</p>
<a href="http://arxiv.org/abs/2011.09820" target="_blank">arXiv:2011.09820</a> [<a href="http://arxiv.org/pdf/2011.09820" target="_blank">pdf</a>]

<h2>Multi-Task Adversarial Attack. (arXiv:2011.09824v1 [cs.LG])</h2>
<h3>Pengxin Guo, Yuancheng Xu, Baijiong Lin, Yu Zhang</h3>
<p>Deep neural networks have achieved impressive performance in various areas,
but they are shown to be vulnerable to adversarial attacks. Previous works on
adversarial attacks mainly focused on the single-task setting. However, in real
applications, it is often desirable to attack several models for different
tasks simultaneously. To this end, we propose Multi-Task adversarial Attack
(MTA), a unified framework that can craft adversarial examples for multiple
tasks efficiently by leveraging shared knowledge among tasks, which helps
enable large-scale applications of adversarial attacks on real-world systems.
More specifically, MTA uses a generator for adversarial perturbations which
consists of a shared encoder for all tasks and multiple task-specific decoders.
Thanks to the shared encoder, MTA reduces the storage cost and speeds up the
inference when attacking multiple tasks simultaneously. Moreover, the proposed
framework can be used to generate per-instance and universal perturbations for
targeted and non-targeted attacks. Experimental results on the Office-31 and
NYUv2 datasets demonstrate that MTA can improve the quality of attacks when
compared with its single-task counterpart.
</p>
<a href="http://arxiv.org/abs/2011.09824" target="_blank">arXiv:2011.09824</a> [<a href="http://arxiv.org/pdf/2011.09824" target="_blank">pdf</a>]

<h2>Differentiable Data Augmentation with Kornia. (arXiv:2011.09832v1 [cs.CV])</h2>
<h3>Jian Shi, Edgar Riba, Dmytro Mishkin, Francesc Moreno, Anguelos Nicolaou</h3>
<p>In this paper we present a review of the Kornia differentiable data
augmentation (DDA) module for both for spatial (2D) and volumetric (3D)
tensors. This module leverages differentiable computer vision solutions from
Kornia, with an aim of integrating data augmentation (DA) pipelines and
strategies to existing PyTorch components (e.g. autograd for differentiability,
optim for optimization). In addition, we provide a benchmark comparing
different DA frameworks and a short review for a number of approaches that make
use of Kornia DDA.
</p>
<a href="http://arxiv.org/abs/2011.09832" target="_blank">arXiv:2011.09832</a> [<a href="http://arxiv.org/pdf/2011.09832" target="_blank">pdf</a>]

<h2>First Order-Rewritability and Containment of Conjunctive Queries in Horn Description Logics. (arXiv:2011.09836v1 [cs.AI])</h2>
<h3>Meghyn Bienvenu, Peter Hansen, Carsten Lutz, Frank Wolter</h3>
<p>We study FO-rewritability of conjunctive queries in the presence of
ontologies formulated in a description logic between EL and Horn-SHIF, along
with related query containment problems. Apart from providing
characterizations, we establish complexity results ranging from ExpTime via
NExpTime to 2ExpTime, pointing out several interesting effects. In particular,
FO-rewriting is more complex for conjunctive queries than for atomic queries
when inverse roles are present, but not otherwise.
</p>
<a href="http://arxiv.org/abs/2011.09836" target="_blank">arXiv:2011.09836</a> [<a href="http://arxiv.org/pdf/2011.09836" target="_blank">pdf</a>]

<h2>Modular Multi Target Tracking Using LSTM Networks. (arXiv:2011.09839v1 [cs.LG])</h2>
<h3>Rishabh Verma, R Rajesh, MS Easwaran</h3>
<p>The process of association and tracking of sensor detections is a key element
in providing situational awareness. When the targets in the scenario are dense
and exhibit high maneuverability, Multi-Target Tracking (MTT) becomes a
challenging task. The conventional techniques to solve such NP-hard
combinatorial optimization problem involves multiple complex models and
requires tedious tuning of parameters, failing to provide an acceptable
performance within the computational constraints. This paper proposes a model
free end-to-end approach for airborne target tracking system using sensor
measurements, integrating all the key elements of multi target tracking --
association, prediction and filtering using deep learning with memory. The
challenging task of association is performed using the Bi-Directional Long
short-term memory (LSTM) whereas filtering and prediction are done using LSTM
models. The proposed modular blocks can be independently trained and used in
multitude of tracking applications including non co-operative (e.g., radar) and
co-operative sensors (e.g., AIS, IFF, ADS-B). Such modular blocks also enhances
the interpretability of the deep learning application. It is shown that
performance of the proposed technique outperforms conventional state of the art
technique Joint Probabilistic Data Association with Interacting Multiple Model
(JPDA-IMM) filter.
</p>
<a href="http://arxiv.org/abs/2011.09839" target="_blank">arXiv:2011.09839</a> [<a href="http://arxiv.org/pdf/2011.09839" target="_blank">pdf</a>]

<h2>Explainable Incipient Fault Detection Systems for Photovoltaic Panels. (arXiv:2011.09843v1 [cs.AI])</h2>
<h3>S. Sairam, Seshadhri Srinivasan, G. Marafioti, B. Subathra, G. Mathisen, Korkut Bekiroglu</h3>
<p>This paper presents an eXplainable Fault Detection and Diagnosis System
(XFDDS) for incipient faults in PV panels. The XFDDS is a hybrid approach that
combines the model-based and data-driven framework. Model-based FDD for PV
panels lacks high fidelity models at low irradiance conditions for detecting
incipient faults. To overcome this, a novel irradiance based three diode model
(IB3DM) is proposed. It is a nine parameter model that provides higher accuracy
even at low irradiance conditions, an important aspect for detecting incipient
faults from noise. To exploit PV data, extreme gradient boosting (XGBoost) is
used due to its ability to detecting incipient faults. Lack of explainability,
feature variability for sample instances, and false alarms are challenges with
data-driven FDD methods. These shortcomings are overcome by hybridization of
XGBoost and IB3DM, and using eXplainable Artificial Intelligence (XAI)
techniques. To combine the XGBoost and IB3DM, a fault-signature metric is
proposed that helps reducing false alarms and also trigger an explanation on
detecting incipient faults. To provide explainability, an eXplainable
Artificial Intelligence (XAI) application is developed. It uses the local
interpretable model-agnostic explanations (LIME) framework and provides
explanations on classifier outputs for data instances. These explanations help
field engineers/technicians for performing troubleshooting and maintenance
operations. The proposed XFDDS is illustrated using experiments on different PV
technologies and our results demonstrate the perceived benefits.
</p>
<a href="http://arxiv.org/abs/2011.09843" target="_blank">arXiv:2011.09843</a> [<a href="http://arxiv.org/pdf/2011.09843" target="_blank">pdf</a>]

<h2>Everybody Sign Now: Translating Spoken Language to Photo Realistic Sign Language Video. (arXiv:2011.09846v1 [cs.CV])</h2>
<h3>Ben Saunders, Necati Cihan Camgoz, Richard Bowden</h3>
<p>To be truly understandable and accepted by Deaf communities, an automatic
Sign Language Production (SLP) system must generate a photo-realistic signer.
Prior approaches based on graphical avatars have proven unpopular, whereas
recent neural SLP works that produce skeleton pose sequences have been shown to
be not understandable to Deaf viewers.

In this paper, we propose SignGAN, the first SLP model to produce
photo-realistic continuous sign language videos directly from spoken language.
We employ a transformer architecture with a Mixture Density Network (MDN)
formulation to handle the translation from spoken language to skeletal pose. A
pose-conditioned human synthesis model is then introduced to generate a
photo-realistic sign language video from the skeletal pose sequence. This
allows the photo-realistic production of sign videos directly translated from
written text.

We further propose a novel keypoint-based loss function, which significantly
improves the quality of synthesized hand images, operating in the keypoint
space to avoid issues caused by motion blur. In addition, we introduce a method
for controllable video generation, enabling training on large, diverse sign
language datasets and providing the ability to control the signer appearance at
inference.

Using a dataset of eight different sign language interpreters extracted from
broadcast footage, we show that SignGAN significantly outperforms all baseline
methods for quantitative metrics and human perceptual studies.
</p>
<a href="http://arxiv.org/abs/2011.09846" target="_blank">arXiv:2011.09846</a> [<a href="http://arxiv.org/pdf/2011.09846" target="_blank">pdf</a>]

<h2>Budgeted Online Selection of Candidate IoT Clients to Participate in Federated Learning. (arXiv:2011.09849v1 [cs.LG])</h2>
<h3>Ihab Mohammed, Shadha Tabatabai, Ala Al-Fuqaha, Faissal El Bouanani, Junaid Qadir, Basheer Qolomany, Mohsen Guizani</h3>
<p>Machine Learning (ML), and Deep Learning (DL) in particular, play a vital
role in providing smart services to the industry. These techniques however
suffer from privacy and security concerns since data is collected from clients
and then stored and processed at a central location. Federated Learning (FL),
an architecture in which model parameters are exchanged instead of client data,
has been proposed as a solution to these concerns. Nevertheless, FL trains a
global model by communicating with clients over communication rounds, which
introduces more traffic on the network and increases the convergence time to
the target accuracy. In this work, we solve the problem of optimizing accuracy
in stateful FL with a budgeted number of candidate clients by selecting the
best candidate clients in terms of test accuracy to participate in the training
process. Next, we propose an online stateful FL heuristic to find the best
candidate clients. Additionally, we propose an IoT client alarm application
that utilizes the proposed heuristic in training a stateful FL global model
based on IoT device type classification to alert clients about unauthorized IoT
devices in their environment. To test the efficiency of the proposed online
heuristic, we conduct several experiments using a real dataset and compare the
results against state-of-the-art algorithms. Our results indicate that the
proposed heuristic outperforms the online random algorithm with up to 27% gain
in accuracy. Additionally, the performance of the proposed online heuristic is
comparable to the performance of the best offline algorithm.
</p>
<a href="http://arxiv.org/abs/2011.09849" target="_blank">arXiv:2011.09849</a> [<a href="http://arxiv.org/pdf/2011.09849" target="_blank">pdf</a>]

<h2>A Theoretical Computer Science Perspective on Consciousness. (arXiv:2011.09850v1 [cs.AI])</h2>
<h3>Manuel Blum, Lenore Blum</h3>
<p>The quest to understand consciousness, once the purview of philosophers and
theologians, is now actively pursued by scientists of many stripes. This paper
studies consciousness from the perspective of theoretical computer science. It
formalizes the Global Workspace Theory (GWT) originated by cognitive
neuroscientist Bernard Baars and further developed by him, Stanislas Dehaene,
and others. Our major contribution lies in the precise formal definition of a
Conscious Turing Machine (CTM), also called a Conscious AI. We define the CTM
in the spirit of Alan Turing's simple yet powerful definition of a computer,
the Turing Machine (TM). We are not looking for a complex model of the brain
nor of cognition but for a simple model of (the admittedly complex concept of)
consciousness. After formally defining CTM, we give a formal definition of
consciousness in CTM. We then suggest why the CTM has the feeling of
consciousness. The reasonableness of the definitions and explanations can be
judged by how well they agree with commonly accepted intuitive concepts of
human consciousness, the breadth of related concepts that the model explains
easily and naturally, and the extent of its agreement with scientific evidence.
</p>
<a href="http://arxiv.org/abs/2011.09850" target="_blank">arXiv:2011.09850</a> [<a href="http://arxiv.org/pdf/2011.09850" target="_blank">pdf</a>]

<h2>Irregularly Tabulated MLP for Fast Point Feature Embedding. (arXiv:2011.09852v1 [cs.LG])</h2>
<h3>Yusuke Sekikawa, Teppei Suzuki</h3>
<p>Aiming at drastic speedup for point-feature embeddings at test time, we
propose a new framework that uses a pair of multi-layer perceptrons (MLP) and a
lookup table (LUT) to transform point-coordinate inputs into high-dimensional
features. When compared with PointNet's feature embedding part realized by MLP
that requires millions of dot products, the proposed framework at test time
requires no such layers of matrix-vector products but requires only looking up
the nearest entities from the tabulated MLP followed by interpolation, defined
over discrete inputs on a 3D lattice that is substantially arranged
irregularly. We call this framework LUTI-MLP: LUT Interpolation ML that
provides a way to train end-to-end irregularly tabulated MLP coupled to a LUT
in a specific manner without the need for any approximation at test time.
LUTI-MLP also provides significant speedup for Jacobian computation of the
embedding function wrt global pose coordinate on Lie algebra $\mathfrak{se}(3)$
at test time, which could be used for point-set registration problems. After
extensive evaluation using the ModelNet40, we confirmed that the LUTI-MLP even
with a small (e.g., $4^3$) lattice yields performance comparable to that of the
MLP while achieving significant speedup: $100\times$ for the embedding,
$12\times$ for the approximate Jacobian, and $860\times$ for the canonical
Jacobian.
</p>
<a href="http://arxiv.org/abs/2011.09852" target="_blank">arXiv:2011.09852</a> [<a href="http://arxiv.org/pdf/2011.09852" target="_blank">pdf</a>]

<h2>A Deep Learning Approach to Predict Hamburg Rutting Curve. (arXiv:2011.09853v1 [cs.LG])</h2>
<h3>Hamed Majidifard, Behnam Jahangiri, Punyaslok Rath, Amir H. Alavi, William G. Buttlar</h3>
<p>Rutting continues to be one of the principal distresses in asphalt pavements
worldwide. This type of distress is caused by permanent deformation and shear
failure of the asphalt mix under the repetition of heavy loads. The Hamburg
wheel tracking test (HWTT) is a widely used testing procedure designed to
accelerate, and to simulate the rutting phenomena in the laboratory. Rut depth,
as one of the outputs of the HWTT, is dependent on a number of parameters
related to mix design and testing conditions. This study introduces a new model
for predicting the rutting depth of asphalt mixtures using a deep learning
technique - the convolution neural network (CNN). A database containing a
comprehensive collection of HWTT results was used to develop a CNN-based
machine learning prediction model. The database includes 10,000 rutting depth
data points measured across a large variety of asphalt mixtures. The model has
been formulated in terms of known influencing mixture variables such as asphalt
binder high temperature performance grade, mixture type, aggregate size,
aggregate gradation, asphalt content, total asphalt binder recycling content,
and testing parameters, including testing temperature and number of wheel
passes. A rigorous validation process was used to assess the accuracy of the
model to predict total rut depth and the HWTT rutting curve. A sensitivity
analysis is presented, which evaluates the effect of the investigated variables
on rutting depth predictions by the CNN model. The model can be used as a tool
to estimate the rut depth in asphalt mixtures when laboratory testing is not
feasible, or for cost saving, pre-design trials.
</p>
<a href="http://arxiv.org/abs/2011.09853" target="_blank">arXiv:2011.09853</a> [<a href="http://arxiv.org/pdf/2011.09853" target="_blank">pdf</a>]

<h2>Generalized Inverse Planning: Learning Lifted non-Markovian Utility for Generalizable Task Representation. (arXiv:2011.09854v1 [cs.LG])</h2>
<h3>Sirui Xie, Feng Gao, Song-Chun Zhu</h3>
<p>In searching for a generalizable representation of temporally extended tasks,
we spot two necessary constituents: the utility needs to be non-Markovian to
transfer temporal relations invariant to a probability shift, the utility also
needs to be lifted to abstract out specific grounding objects. In this work, we
study learning such utility from human demonstrations. While inverse
reinforcement learning (IRL) has been accepted as a general framework of
utility learning, its fundamental formulation is one concrete Markov Decision
Process. Thus the learned reward function does not specify the task
independently of the environment. Going beyond that, we define a domain of
generalization that spans a set of planning problems following a schema. We
hence propose a new quest, Generalized Inverse Planning, for utility learning
in this domain. We further outline a computational framework, Maximum Entropy
Inverse Planning (MEIP), that learns non-Markovian utility and associated
concepts in a generative manner. The learned utility and concepts form a task
representation that generalizes regardless of probability shift or structural
change. Seeing that the proposed generalization problem has not been widely
studied yet, we carefully define an evaluation protocol, with which we
illustrate the effectiveness of MEIP on two proof-of-concept domains and one
challenging task: learning to fold from demonstrations.
</p>
<a href="http://arxiv.org/abs/2011.09854" target="_blank">arXiv:2011.09854</a> [<a href="http://arxiv.org/pdf/2011.09854" target="_blank">pdf</a>]

<h2>On tuning deep learning models: a data mining perspective. (arXiv:2011.09857v1 [cs.LG])</h2>
<h3>M.M. Ozturk</h3>
<p>Deep learning algorithms vary depending on the underlying connection
mechanism of nodes of them. They have various hyperparameters that are either
set via specific algorithms or randomly chosen. Meanwhile, hyperparameters of
deep learning algorithms have the potential to help enhance the performance of
the machine learning tasks. In this paper, a tuning guideline is provided for
researchers who cope with issues originated from hyperparameters of deep
learning models. To that end, four types of deep learning algorithms are
investigated in terms of tuning and data mining perspective. Further, common
search methods of hyperparameters are evaluated on four deep learning
algorithms. Normalization helps increase the performance of classification,
according to the results of this study. The number of features has not
contributed to the decline in the accuracy of deep learning algorithms. Even
though high sparsity results in low accuracy, a uniform distribution is much
more crucial to reach reliable results in terms of data mining.
</p>
<a href="http://arxiv.org/abs/2011.09857" target="_blank">arXiv:2011.09857</a> [<a href="http://arxiv.org/pdf/2011.09857" target="_blank">pdf</a>]

<h2>Conservative Extensions in Horn Description Logics with Inverse Roles. (arXiv:2011.09858v1 [cs.AI])</h2>
<h3>Jean Christoph Jung, Carsten Lutz, Mauricio Martel, Thomas Schneider</h3>
<p>We investigate the decidability and computational complexity of conservative
extensions and the related notions of inseparability and entailment in Horn
description logics (DLs) with inverse roles. We consider both query
conservative extensions, defined by requiring that the answers to all
conjunctive queries are left unchanged, and deductive conservative extensions,
which require that the entailed concept inclusions, role inclusions, and
functionality assertions do not change. Upper bounds for query conservative
extensions are particularly challenging because characterizations in terms of
unbounded homomorphisms between universal models, which are the foundation of
the standard approach to establishing decidability, fail in the presence of
inverse roles. We resort to a characterization that carefully mixes unbounded
and bounded homomorphisms and enables a decision procedure that combines tree
automata and a mosaic technique. Our main results are that query conservative
extensions are 2ExpTime-complete in all DLs between ELI and Horn-ALCHIF and
between Horn-ALC and Horn-ALCHIF, and that deductive conservative extensions
are 2ExpTime-complete in all DLs between ELI and ELHIF_\bot. The same results
hold for inseparability and entailment.
</p>
<a href="http://arxiv.org/abs/2011.09858" target="_blank">arXiv:2011.09858</a> [<a href="http://arxiv.org/pdf/2011.09858" target="_blank">pdf</a>]

<h2>Neural Abstract Reasoner. (arXiv:2011.09860v1 [cs.AI])</h2>
<h3>Victor Kolev, Bogdan Georgiev, Svetlin Penkov</h3>
<p>Abstract reasoning and logic inference are difficult problems for neural
networks, yet essential to their applicability in highly structured domains. In
this work we demonstrate that a well known technique such as spectral
regularization can significantly boost the capabilities of a neural learner. We
introduce the Neural Abstract Reasoner (NAR), a memory augmented architecture
capable of learning and using abstract rules. We show that, when trained with
spectral regularization, NAR achieves $78.8\%$ accuracy on the Abstraction and
Reasoning Corpus, improving performance 4 times over the best known human
hand-crafted symbolic solvers. We provide some intuition for the effects of
spectral regularization in the domain of abstract reasoning based on
theoretical generalization bounds and Solomonoff's theory of inductive
inference.
</p>
<a href="http://arxiv.org/abs/2011.09860" target="_blank">arXiv:2011.09860</a> [<a href="http://arxiv.org/pdf/2011.09860" target="_blank">pdf</a>]

<h2>DCT-Mask: Discrete Cosine Transform Mask Representation for Instance Segmentation. (arXiv:2011.09876v1 [cs.CV])</h2>
<h3>Xing Shen, Jirui Yang, Chunbo Wei, Bing Deng, Jianqiang Huang, Xiansheng Hua, Xiaoliang Cheng, Kewei Liang</h3>
<p>Binary grid mask representation is broadly used in instance segmentation. A
representative instantiation is Mask R-CNN which predicts masks on a $28\times
28$ binary grid. Generally, a low-resolution grid is not sufficient to capture
the details, while a high-resolution grid dramatically increases the training
complexity. In this paper, we propose a new mask representation by applying the
discrete cosine transform(DCT) to encode the high-resolution binary grid mask
into a compact vector. Our method, termed DCT-Mask, could be easily integrated
into most pixel-based instance segmentation methods. Without any bells and
whistles, DCT-Mask yields significant gains on different frameworks, backbones,
datasets, and training schedules. It does not require any pre-processing or
pre-training, and almost no harm to the running speed. Especially, for
higher-quality annotations and more complex backbones, our method has a greater
improvement. Moreover, we analyze the performance of our method from the
perspective of the quality of mask representation. The main reason why DCT-Mask
works well is that it obtains a high-quality mask representation with low
complexity. Code will be made available.
</p>
<a href="http://arxiv.org/abs/2011.09876" target="_blank">arXiv:2011.09876</a> [<a href="http://arxiv.org/pdf/2011.09876" target="_blank">pdf</a>]

<h2>DeepRepair: Style-Guided Repairing for DNNs in the Real-world Operational Environment. (arXiv:2011.09884v1 [cs.LG])</h2>
<h3>Bing Yu, Hua Qi, Qing Guo, Felix Juefei-Xu, Xiaofei Xie, Lei Ma, Jianjun Zhao</h3>
<p>Deep neural networks (DNNs) are being widely applied for various real-world
applications across domains due to their high performance (e.g., high accuracy
on image classification). Nevertheless, a well-trained DNN after deployment
could oftentimes raise errors during practical use in the operational
environment due to the mismatching between distributions of the training
dataset and the potential unknown noise factors in the operational environment,
e.g., weather, blur, noise etc. Hence, it poses a rather important problem for
the DNNs' real-world applications: how to repair the deployed DNNs for
correcting the failure samples (i.e., incorrect prediction) under the deployed
operational environment while not harming their capability of handling normal
or clean data. The number of failure samples we can collect in practice, caused
by the noise factors in the operational environment, is often limited.
Therefore, It is rather challenging how to repair more similar failures based
on the limited failure samples we can collect.

In this paper, we propose a style-guided data augmentation for repairing DNN
in the operational environment. We propose a style transfer method to learn and
introduce the unknown failure patterns within the failure data into the
training data via data augmentation. Moreover, we further propose the
clustering-based failure data generation for much more effective style-guided
data augmentation. We conduct a large-scale evaluation with fifteen degradation
factors that may happen in the real world and compare with four
state-of-the-art data augmentation methods and two DNN repairing methods,
demonstrating that our method can significantly enhance the deployed DNNs on
the corrupted data in the operational environment, and with even better
accuracy on clean datasets.
</p>
<a href="http://arxiv.org/abs/2011.09884" target="_blank">arXiv:2011.09884</a> [<a href="http://arxiv.org/pdf/2011.09884" target="_blank">pdf</a>]

<h2>Similarity-based Distance for Categorical Clustering using Space Structure. (arXiv:2011.09887v1 [cs.LG])</h2>
<h3>Utkarsh Nath, Shikha Asrani, Rahul Katarya</h3>
<p>Clustering is spotting pattern in a group of objects and resultantly grouping
the similar objects together. Objects have attributes which are not always
numerical, sometimes attributes have domain or categories to which they could
belong to. Such data is called categorical data. To group categorical data many
clustering algorithms are used, among which k- modes algorithm has so far given
the most significant results. Nevertheless, there is still a lot which could be
improved. Algorithms like k-means, fuzzy-c-means or hierarchical have given far
better accuracies with numerical data. In this paper, we have proposed a novel
distance metric, similarity-based distance (SBD) to find the distance between
objects of categorical data. Experiments have shown that our proposed distance
(SBD), when used with the SBC (space structure based clustering) type algorithm
significantly outperforms the existing algorithms like k-modes or other SBC
type algorithms when used on categorical datasets.
</p>
<a href="http://arxiv.org/abs/2011.09887" target="_blank">arXiv:2011.09887</a> [<a href="http://arxiv.org/pdf/2011.09887" target="_blank">pdf</a>]

<h2>Fuzzy C-means-based scenario bundling for stochastic service network design. (arXiv:2011.09890v1 [cs.AI])</h2>
<h3>Xiaoping Jiang, Ruibin Bai, Dario Landa-Silva, Uwe Aickelin</h3>
<p>Stochastic service network designs with uncertain demand represented by a set
of scenarios can be modelled as a large-scale two-stage stochastic
mixed-integer program (SMIP). The progressive hedging algorithm (PHA) is a
decomposition method for solving the resulting SMIP. The computational
performance of the PHA can be greatly enhanced by decomposing according to
scenario bundles instead of individual scenarios. At the heart of bundle-based
decomposition is the method for grouping the scenarios into bundles. In this
paper, we present a fuzzy c-means-based scenario bundling method to address
this problem. Rather than full membership of a bundle, which is typically the
case in existing scenario bundling strategies such as k-means, a scenario has
partial membership in each of the bundles and can be assigned to more than one
bundle in our method.
</p>
<a href="http://arxiv.org/abs/2011.09890" target="_blank">arXiv:2011.09890</a> [<a href="http://arxiv.org/pdf/2011.09890" target="_blank">pdf</a>]

<h2>Using simulation to incorporate dynamic criteria into multiple criteria decision-making. (arXiv:2011.09891v1 [cs.AI])</h2>
<h3>Uwe Aickelin, Jenna Marie Reps, Peer-Olaf Siebers, Peng Li</h3>
<p>In this paper, we present a case study demonstrating how dynamic and
uncertain criteria can be incorporated into a multicriteria analysis with the
help of discrete event simulation. The simulation guided multicriteria analysis
can include both monetary and non-monetary criteria that are static or dynamic,
whereas standard multi criteria analysis only deals with static criteria and
cost benefit analysis only deals with static monetary criteria. The dynamic and
uncertain criteria are incorporated by using simulation to explore how the
decision options perform. The results of the simulation are then fed into the
multicriteria analysis. By enabling the incorporation of dynamic and uncertain
criteria, the dynamic multiple criteria analysis was able to take a unique
perspective of the problem. The highest ranked option returned by the dynamic
multicriteria analysis differed from the other decision aid techniques.
</p>
<a href="http://arxiv.org/abs/2011.09891" target="_blank">arXiv:2011.09891</a> [<a href="http://arxiv.org/pdf/2011.09891" target="_blank">pdf</a>]

<h2>Data Representing Ground-Truth Explanations to Evaluate XAI Methods. (arXiv:2011.09892v1 [cs.LG])</h2>
<h3>Shideh Shams Amiri, Rosina O. Weber, Prateek Goel, Owen Brooks, Archer Gandley, Brian Kitchell, Aaron Zehm</h3>
<p>Explainable artificial intelligence (XAI) methods are currently evaluated
with approaches mostly originated in interpretable machine learning (IML)
research that focus on understanding models such as comparison against existing
attribution approaches, sensitivity analyses, gold set of features, axioms, or
through demonstration of images. There are problems with these methods such as
that they do not indicate where current XAI approaches fail to guide
investigations towards consistent progress of the field. They do not measure
accuracy in support of accountable decisions, and it is practically impossible
to determine whether one XAI method is better than the other or what the
weaknesses of existing models are, leaving researchers without guidance on
which research questions will advance the field. Other fields usually utilize
ground-truth data and create benchmarks. Data representing ground-truth
explanations is not typically used in XAI or IML. One reason is that
explanations are subjective, in the sense that an explanation that satisfies
one user may not satisfy another. To overcome these problems, we propose to
represent explanations with canonical equations that can be used to evaluate
the accuracy of XAI methods. The contributions of this paper include a
methodology to create synthetic data representing ground-truth explanations,
three data sets, an evaluation of LIME using these data sets, and a preliminary
analysis of the challenges and potential benefits in using these data to
evaluate existing XAI approaches. Evaluation methods based on human-centric
studies are outside the scope of this paper.
</p>
<a href="http://arxiv.org/abs/2011.09892" target="_blank">arXiv:2011.09892</a> [<a href="http://arxiv.org/pdf/2011.09892" target="_blank">pdf</a>]

<h2>Learning in School: Multi-teacher Knowledge Inversion for Data-Free Quantization. (arXiv:2011.09899v1 [cs.LG])</h2>
<h3>Yuhang Li, Feng Zhu, Ruihao Gong, Mingzhu Shen, Fengwei Yu, Shaoqing Lu, Shi Gu</h3>
<p>User data confidentiality protection is becoming a rising challenge in the
present deep learning research. In that case, data-free quantization has
emerged as a promising method to conduct model compression without the need for
user data. With no access to data, model quantization naturally becomes less
resilient and faces a higher risk of performance degradation. Prior works
propose to distill fake images by matching the activation distribution given a
specific pre-trained model. However, this fake data cannot be applied to other
models easily and is optimized by an invariant objective, resulting in the lack
of generalizability and diversity whereas these properties can be found in the
natural image dataset. To address these problems, we propose Learning in
School~(LIS) algorithm, capable to generate the images suitable for all models
by inverting the knowledge in multiple teachers. We further introduce a
decentralized training strategy by sampling teachers from hierarchical courses
to simultaneously maintain the diversity of generated images. LIS data is
highly diverse, not model-specific and only requires one-time synthesis to
generalize multiple models and applications. Extensive experiments prove that
LIS images resemble natural images with high quality and high fidelity. On
data-free quantization, our LIS method significantly surpasses the existing
model-specific methods. In particular, LIS data is effective in both
post-training quantization and quantization-aware training on the ImageNet
dataset and achieves up to 33\% top-1 accuracy uplift compared with existing
methods.
</p>
<a href="http://arxiv.org/abs/2011.09899" target="_blank">arXiv:2011.09899</a> [<a href="http://arxiv.org/pdf/2011.09899" target="_blank">pdf</a>]

<h2>MG-GCN: Fast and Effective Learning with Mix-grained Aggregators for Training Large Graph Convolutional Networks. (arXiv:2011.09900v1 [cs.LG])</h2>
<h3>Tao Huang, Yihan Zhang, Jiajing Wu, Junyuan Fang, Zibin Zheng</h3>
<p>Graph convolutional networks (GCNs) have been employed as a kind of
significant tool on many graph-based applications recently. Inspired by
convolutional neural networks (CNNs), GCNs generate the embeddings of nodes by
aggregating the information of their neighbors layer by layer. However, the
high computational and memory cost of GCNs due to the recursive neighborhood
expansion across GCN layers makes it infeasible for training on large graphs.
To tackle this issue, several sampling methods during the process of
information aggregation have been proposed to train GCNs in a mini-batch
Stochastic Gradient Descent (SGD) manner. Nevertheless, these sampling
strategies sometimes bring concerns about insufficient information collection,
which may hinder the learning performance in terms of accuracy and convergence.
To tackle the dilemma between accuracy and efficiency, we propose to use
aggregators with different granularities to gather neighborhood information in
different layers. Then, a degree-based sampling strategy, which avoids the
exponential complexity, is constructed for sampling a fixed number of nodes.
Combining the above two mechanisms, the proposed model, named Mix-grained GCN
(MG-GCN) achieves state-of-the-art performance in terms of accuracy, training
speed, convergence speed, and memory cost through a comprehensive set of
experiments on four commonly used benchmark datasets and a new Ethereum
dataset.
</p>
<a href="http://arxiv.org/abs/2011.09900" target="_blank">arXiv:2011.09900</a> [<a href="http://arxiv.org/pdf/2011.09900" target="_blank">pdf</a>]

<h2>Low-latency Federated Learning and Blockchain for Edge Association in Digital Twin empowered 6G Networks. (arXiv:2011.09902v1 [cs.LG])</h2>
<h3>Yunlong Lu, Xiaohong Huang, Ke Zhang, Sabita Maharjan, Yan Zhang</h3>
<p>Emerging technologies such as digital twins and 6th Generation mobile
networks (6G) have accelerated the realization of edge intelligence in
Industrial Internet of Things (IIoT). The integration of digital twin and 6G
bridges the physical system with digital space and enables robust instant
wireless connectivity. With increasing concerns on data privacy, federated
learning has been regarded as a promising solution for deploying distributed
data processing and learning in wireless networks. However, unreliable
communication channels, limited resources, and lack of trust among users,
hinder the effective application of federated learning in IIoT. In this paper,
we introduce the Digital Twin Wireless Networks (DTWN) by incorporating digital
twins into wireless networks, to migrate real-time data processing and
computation to the edge plane. Then, we propose a blockchain empowered
federated learning framework running in the DTWN for collaborative computing,
which improves the reliability and security of the system, and enhances data
privacy. Moreover, to balance the learning accuracy and time cost of the
proposed scheme, we formulate an optimization problem for edge association by
jointly considering digital twin association, training data batch size, and
bandwidth allocation. We exploit multi-agent reinforcement learning to find an
optimal solution to the problem. Numerical results on real-world dataset show
that the proposed scheme yields improved efficiency and reduced cost compared
to benchmark learning method.
</p>
<a href="http://arxiv.org/abs/2011.09902" target="_blank">arXiv:2011.09902</a> [<a href="http://arxiv.org/pdf/2011.09902" target="_blank">pdf</a>]

<h2>Impact of Accuracy on Model Interpretations. (arXiv:2011.09903v1 [cs.LG])</h2>
<h3>Brian Liu, Madeleine Udell</h3>
<p>Model interpretations are often used in practice to extract real world
insights from machine learning models. These interpretations have a wide range
of applications; they can be presented as business recommendations or used to
evaluate model bias. It is vital for a data scientist to choose trustworthy
interpretations to drive real world impact. Doing so requires an understanding
of how the accuracy of a model impacts the quality of standard interpretation
tools. In this paper, we will explore how a model's predictive accuracy affects
interpretation quality. We propose two metrics to quantify the quality of an
interpretation and design an experiment to test how these metrics vary with
model accuracy. We find that for datasets that can be modeled accurately by a
variety of methods, simpler methods yield higher quality interpretations. We
also identify which interpretation method works the best for lower levels of
model accuracy.
</p>
<a href="http://arxiv.org/abs/2011.09903" target="_blank">arXiv:2011.09903</a> [<a href="http://arxiv.org/pdf/2011.09903" target="_blank">pdf</a>]

<h2>LOss-Based SensiTivity rEgulaRization: towards deep sparse neural networks. (arXiv:2011.09905v1 [cs.LG])</h2>
<h3>Enzo Tartaglione, Andrea Bragagnolo, Attilio Fiandrotti, Marco Grangetto</h3>
<p>LOBSTER (LOss-Based SensiTivity rEgulaRization) is a method for training
neural networks having a sparse topology. Let the sensitivity of a network
parameter be the variation of the loss function with respect to the variation
of the parameter. Parameters with low sensitivity, i.e. having little impact on
the loss when perturbed, are shrunk and then pruned to sparsify the network.
Our method allows to train a network from scratch, i.e. without preliminary
learning or rewinding. Experiments on multiple architectures and datasets show
competitive compression ratios with minimal computational overhead.
</p>
<a href="http://arxiv.org/abs/2011.09905" target="_blank">arXiv:2011.09905</a> [<a href="http://arxiv.org/pdf/2011.09905" target="_blank">pdf</a>]

<h2>Towards Learning Controllable Representations of Physical Systems. (arXiv:2011.09906v1 [cs.LG])</h2>
<h3>Kevin Haninger, Raul Vincente Garcia, Joerg Krueger</h3>
<p>Learned representations of dynamical systems reduce dimensionality,
potentially supporting downstream reinforcement learning (RL). However, no
established methods predict a representation's suitability for control and
evaluation is largely done via downstream RL performance, slowing
representation design. Towards a principled evaluation of representations for
control, we consider the relationship between the true state and the
corresponding representations, proposing that ideally each representation
corresponds to a unique true state. This motivates two metrics: temporal
smoothness and high mutual information between true state/representation. These
metrics are related to established representation objectives, and studied on
Lagrangian systems where true state, information requirements, and statistical
properties of the state can be formalized for a broad class of systems. These
metrics are shown to predict reinforcement learning performance in a simulated
peg-in-hole task when comparing variants of autoencoder-based representations.
</p>
<a href="http://arxiv.org/abs/2011.09906" target="_blank">arXiv:2011.09906</a> [<a href="http://arxiv.org/pdf/2011.09906" target="_blank">pdf</a>]

<h2>Multi-objective semi-supervised clustering to identify health service patterns for injured patients. (arXiv:2011.09911v1 [cs.LG])</h2>
<h3>Hadi Akbarzadeh Khorshidi, Uwe Aickelin, Gholamreza Haffari, Behrooz Hassani-Mahmooei</h3>
<p>This study develops a pattern recognition method that identifies patterns
based on their similarity and their association with the outcome of interest.
The practical purpose of developing this pattern recognition method is to group
patients, who are injured in transport accidents, in the early stages
post-injury. This grouping is based on distinctive patterns in health service
use within the first week post-injury. The groups also provide predictive
information towards the total cost of medication process. As a result, the
group of patients who have undesirable outcomes are identified as early as
possible based health service use patterns.
</p>
<a href="http://arxiv.org/abs/2011.09911" target="_blank">arXiv:2011.09911</a> [<a href="http://arxiv.org/pdf/2011.09911" target="_blank">pdf</a>]

<h2>Imputation techniques on missing values in breast cancer treatment and fertility data. (arXiv:2011.09912v1 [cs.LG])</h2>
<h3>Xuetong Wu, Hadi Akbarzadeh Khorshidi, Uwe Aickelin, Zobaida Edib, Michelle Peate</h3>
<p>Clinical decision support using data mining techniques offers more
intelligent way to reduce the decision error in the last few years. However,
clinical datasets often suffer from high missingness, which adversely impacts
the quality of modelling if handled improperly. Imputing missing values
provides an opportunity to resolve the issue. Conventional imputation methods
adopt simple statistical analysis, such as mean imputation or discarding
missing cases, which have many limitations and thus degrade the performance of
learning. This study examines a series of machine learning based imputation
methods and suggests an efficient approach to in preparing a good quality
breast cancer (BC) dataset, to find the relationship between BC treatment and
chemotherapy-related amenorrhoea, where the performance is evaluated with the
accuracy of the prediction.
</p>
<a href="http://arxiv.org/abs/2011.09912" target="_blank">arXiv:2011.09912</a> [<a href="http://arxiv.org/pdf/2011.09912" target="_blank">pdf</a>]

<h2>Challenges in Deploying Machine Learning: a Survey of Case Studies. (arXiv:2011.09926v1 [cs.LG])</h2>
<h3>Andrei Paleyes, Raoul-Gabriel Urma, Neil D. Lawrence</h3>
<p>In recent years, machine learning has received increased interest both as an
academic research field and as a solution for real-world business problems.
However, the deployment of machine learning models in production systems can
present a number of issues and concerns. This survey reviews published reports
of deploying machine learning solutions in a variety of use cases, industries
and applications and extracts practical considerations corresponding to stages
of the machine learning deployment workflow. Our survey shows that
practitioners face challenges at each stage of the deployment. The goal of this
paper is to layout a research agenda to explore approaches addressing these
challenges.
</p>
<a href="http://arxiv.org/abs/2011.09926" target="_blank">arXiv:2011.09926</a> [<a href="http://arxiv.org/pdf/2011.09926" target="_blank">pdf</a>]

<h2>Using Text to Teach Image Retrieval. (arXiv:2011.09928v1 [cs.LG])</h2>
<h3>Haoyu Dong, Ze Wang, Qiang Qiu, Guillermo Sapiro</h3>
<p>Image retrieval relies heavily on the quality of the data modeling and the
distance measurement in the feature space. Building on the concept of image
manifold, we first propose to represent the feature space of images, learned
via neural networks, as a graph. Neighborhoods in the feature space are now
defined by the geodesic distance between images, represented as graph vertices
or manifold samples. When limited images are available, this manifold is
sparsely sampled, making the geodesic computation and the corresponding
retrieval harder. To address this, we augment the manifold samples with
geometrically aligned text, thereby using a plethora of sentences to teach us
about images. In addition to extensive results on standard datasets
illustrating the power of text to help in image retrieval, a new public dataset
based on CLEVR is introduced to quantify the semantic similarity between visual
data and text data. The experimental results show that the joint embedding
manifold is a robust representation, allowing it to be a better basis to
perform image retrieval given only an image and a textual instruction on the
desired modifications over the image
</p>
<a href="http://arxiv.org/abs/2011.09928" target="_blank">arXiv:2011.09928</a> [<a href="http://arxiv.org/pdf/2011.09928" target="_blank">pdf</a>]

<h2>NeVer 2.0: Learning, Verification and Repair of Deep Neural Networks. (arXiv:2011.09933v1 [cs.LG])</h2>
<h3>Dario Guidotti, Luca Pulina, Armando Tacchella</h3>
<p>In this work, we present an early prototype of NeVer 2.0, a new system for
automated synthesis and analysis of deep neural networks.NeVer 2.0borrows its
design philosophy from NeVer, the first package that integrated learning,
automated verification and repair of (shallow) neural networks in a single
tool. The goal of NeVer 2.0 is to provide a similar integration for deep
networks by leveraging a selection of state-of-the-art learning frameworks and
integrating them with verification algorithms to ease the scalability challenge
and make repair of faulty networks possible.
</p>
<a href="http://arxiv.org/abs/2011.09933" target="_blank">arXiv:2011.09933</a> [<a href="http://arxiv.org/pdf/2011.09933" target="_blank">pdf</a>]

<h2>Heterogeneous Contrastive Learning: Encoding Spatial Information for Compact Visual Representations. (arXiv:2011.09941v1 [cs.CV])</h2>
<h3>Xinyue Huo, Lingxi Xie, Longhui Wei, Xiaopeng Zhang, Hao Li, Zijie Yang, Wengang Zhou, Houqiang Li, Qi Tian</h3>
<p>Contrastive learning has achieved great success in self-supervised visual
representation learning, but existing approaches mostly ignored spatial
information which is often crucial for visual representation. This paper
presents heterogeneous contrastive learning (HCL), an effective approach that
adds spatial information to the encoding stage to alleviate the learning
inconsistency between the contrastive objective and strong data augmentation
operations. We demonstrate the effectiveness of HCL by showing that (i) it
achieves higher accuracy in instance discrimination and (ii) it surpasses
existing pre-training methods in a series of downstream tasks while shrinking
the pre-training costs by half. More importantly, we show that our approach
achieves higher efficiency in visual representations, and thus delivers a key
message to inspire the future research of self-supervised visual representation
learning.
</p>
<a href="http://arxiv.org/abs/2011.09941" target="_blank">arXiv:2011.09941</a> [<a href="http://arxiv.org/pdf/2011.09941" target="_blank">pdf</a>]

<h2>Adversarial Threats to DeepFake Detection: A Practical Perspective. (arXiv:2011.09957v1 [cs.CV])</h2>
<h3>Paarth Neekhara, Brian Dolhansky, Joanna Bitton, Cristian Canton Ferrer</h3>
<p>Facially manipulated images and videos or DeepFakes can be used maliciously
to fuel misinformation or defame individuals. Therefore, detecting DeepFakes is
crucial to increase the credibility of social media platforms and other media
sharing web sites. State-of-the art DeepFake detection techniques rely on
neural network based classification models which are known to be vulnerable to
adversarial examples. In this work, we study the vulnerabilities of
state-of-the-art DeepFake detection methods from a practical stand point. We
perform adversarial attacks on DeepFake detectors in a black box setting where
the adversary does not have complete knowledge of the classification models. We
study the extent to which adversarial perturbations transfer across different
models and propose techniques to improve the transferability of adversarial
examples. We also create more accessible attacks using Universal Adversarial
Perturbations which pose a very feasible attack scenario since they can be
easily shared amongst attackers. We perform our evaluations on the winning
entries of the DeepFake Detection Challenge (DFDC) and demonstrate that they
can be easily bypassed in a practical attack scenario by designing transferable
and accessible adversarial attacks.
</p>
<a href="http://arxiv.org/abs/2011.09957" target="_blank">arXiv:2011.09957</a> [<a href="http://arxiv.org/pdf/2011.09957" target="_blank">pdf</a>]

<h2>Electric Vehicle Charging Infrastructure Planning: A Scalable Computational Framework. (arXiv:2011.09967v1 [cs.AI])</h2>
<h3>Wanshi Hong, Cong Zhang, Cy Chan, Bin Wang</h3>
<p>The optimal charging infrastructure planning problem over a large geospatial
area is challenging due to the increasing network sizes of the transportation
system and the electric grid. The coupling between the electric vehicle travel
behaviors and charging events is therefore complex. This paper focuses on the
demonstration of a scalable computational framework for the electric vehicle
charging infrastructure planning over the tightly integrated transportation and
electric grid networks. On the transportation side, a charging profile
generation strategy is proposed leveraging the EV energy consumption model,
trip routing, and charger selection methods. On the grid side, a genetic
algorithm is utilized within the optimal power flow program to solve the
optimal charger placement problem with integer variables by adaptively
evaluating candidate solutions in the current iteration and generating new
solutions for the next iterations.
</p>
<a href="http://arxiv.org/abs/2011.09967" target="_blank">arXiv:2011.09967</a> [<a href="http://arxiv.org/pdf/2011.09967" target="_blank">pdf</a>]

<h2>Learning to Predict the 3D Layout of a Scene. (arXiv:2011.09977v1 [cs.CV])</h2>
<h3>Jihao Andreas Lin, Jakob Br&#xfc;nker, Daniel F&#xe4;hrmann</h3>
<p>While 2D object detection has improved significantly over the past, real
world applications of computer vision often require an understanding of the 3D
layout of a scene. Many recent approaches to 3D detection use LiDAR point
clouds for prediction. We propose a method that only uses a single RGB image,
thus enabling applications in devices or vehicles that do not have LiDAR
sensors. By using an RGB image, we can leverage the maturity and success of
recent 2D object detectors, by extending a 2D detector with a 3D detection
head. In this paper we discuss different approaches and experiments, including
both regression and classification methods, for designing this 3D detection
head. Furthermore, we evaluate how subproblems and implementation details
impact the overall prediction result. We use the KITTI dataset for training,
which consists of street traffic scenes with class labels, 2D bounding boxes
and 3D annotations with seven degrees of freedom. Our final architecture is
based on Faster R-CNN. The outputs of the convolutional backbone are fixed
sized feature maps for every region of interest. Fully connected layers within
the network head then propose an object class and perform 2D bounding box
regression. We extend the network head by a 3D detection head, which predicts
every degree of freedom of a 3D bounding box via classification. We achieve a
mean average precision of 47.3% for moderately difficult data, measured at a 3D
intersection over union threshold of 70%, as required by the official KITTI
benchmark; outperforming previous state-of-the-art single RGB only methods by a
large margin.
</p>
<a href="http://arxiv.org/abs/2011.09977" target="_blank">arXiv:2011.09977</a> [<a href="http://arxiv.org/pdf/2011.09977" target="_blank">pdf</a>]

<h2>Geography-Aware Self-Supervised Learning. (arXiv:2011.09980v1 [cs.CV])</h2>
<h3>Kumar Ayush, Burak Uzkent, Chenlin Meng, Marshall Burke, David Lobell, Stefano Ermon</h3>
<p>Contrastive learning methods have significantly narrowed the gap between
supervised and unsupervised learning on computer vision tasks. In this paper,
we explore their application to remote sensing, where unlabeled data is often
abundant but labeled data is scarce. We first show that due to their different
characteristics, a non-trivial gap persists between contrastive and supervised
learning on standard benchmarks. To close the gap, we propose novel training
methods that exploit the spatiotemporal structure of remote sensing data. We
leverage spatially aligned images over time to construct temporal positive
pairs in contrastive learning and geo-location to design pre-text tasks. Our
experiments show that our proposed method closes the gap between contrastive
and supervised learning on image classification, object detection and semantic
segmentation for remote sensing and other geo-tagged image datasets
</p>
<a href="http://arxiv.org/abs/2011.09980" target="_blank">arXiv:2011.09980</a> [<a href="http://arxiv.org/pdf/2011.09980" target="_blank">pdf</a>]

<h2>Estimation of Shortest Path Covariance Matrices. (arXiv:2011.09986v1 [cs.LG])</h2>
<h3>Raj Kumar Maity, Cameron Musco</h3>
<p>We study the sample complexity of estimating the covariance matrix
$\mathbf{\Sigma} \in \mathbb{R}^{d\times d}$ of a distribution $\mathcal D$
over $\mathbb{R}^d$ given independent samples, under the assumption that
$\mathbf{\Sigma}$ is graph-structured. In particular, we focus on shortest path
covariance matrices, where the covariance between any two measurements is
determined by the shortest path distance in an underlying graph with $d$ nodes.
Such matrices generalize Toeplitz and circulant covariance matrices and are
widely applied in signal processing applications, where the covariance between
two measurements depends on the (shortest path) distance between them in time
or space.

We focus on minimizing both the vector sample complexity: the number of
samples drawn from $\mathcal{D}$ and the entry sample complexity: the number of
entries read in each sample. The entry sample complexity corresponds to
measurement equipment costs in signal processing applications. We give a very
simple algorithm for estimating $\mathbf{\Sigma}$ up to spectral norm error
$\epsilon \left\|\mathbf{\Sigma}\right\|_2$ using just $O(\sqrt{D})$ entry
sample complexity and $\tilde O(r^2/\epsilon^2)$ vector sample complexity,
where $D$ is the diameter of the underlying graph and $r \le d$ is the rank of
$\mathbf{\Sigma}$. Our method is based on extending the widely applied idea of
sparse rulers for Toeplitz covariance estimation to the graph setting.

In the special case when $\mathbf{\Sigma}$ is a low-rank Toeplitz matrix, our
result matches the state-of-the-art, with a far simpler proof. We also give an
information theoretic lower bound matching our upper bound up to a factor $D$
and discuss some directions towards closing this gap.
</p>
<a href="http://arxiv.org/abs/2011.09986" target="_blank">arXiv:2011.09986</a> [<a href="http://arxiv.org/pdf/2011.09986" target="_blank">pdf</a>]

<h2>A Stable High-order Tuner for General Convex Functions. (arXiv:2011.09996v1 [cs.LG])</h2>
<h3>Jos&#xe9; M. Moreu, Anuradha M. Annaswamy</h3>
<p>Iterative gradient-based algorithms have been increasingly applied for the
training of a broad variety of machine learning models including large
neural-nets. In particular, momentum-based methods, with accelerated learning
guarantees, have received a lot of attention due to their provable guarantees
of fast learning in certain classes of problems and multiple algorithms have
been derived. However, properties for these methods hold true only for constant
regressors. When time-varying regressors occur, which is commonplace in dynamic
systems, many of these momentum-based methods cannot guarantee stability.
Recently, a new High-order Tuner (HT) was developed and shown to have 1)
stability and asymptotic convergence for time-varying regressors and 2)
non-asymptotic accelerated learning guarantees for constant regressors. These
results were derived for a linear regression framework producing a quadratic
loss function. In this paper, we extend and discuss the results of this same HT
for general convex loss functions. Through the exploitation of convexity and
smoothness definitions, we establish similar stability and asymptotic
convergence guarantees. Additionally we conjecture that the HT has an
accelerated convergence rate. Finally, we provide numerical simulations
supporting the satisfactory behavior of the HT algorithm as well as the
conjecture of accelerated learning.
</p>
<a href="http://arxiv.org/abs/2011.09996" target="_blank">arXiv:2011.09996</a> [<a href="http://arxiv.org/pdf/2011.09996" target="_blank">pdf</a>]

<h2>Fully Gap-Dependent Bounds for Multinomial Logit Bandit. (arXiv:2011.09998v1 [cs.LG])</h2>
<h3>Jiaqi Yang</h3>
<p>We study the multinomial logit (MNL) bandit problem, where at each time step,
the seller offers an assortment of size at most $K$ from a pool of $N$ items,
and the buyer purchases an item from the assortment according to a MNL choice
model. The objective is to learn the model parameters and maximize the expected
revenue. We present (i) an algorithm that identifies the optimal assortment
$S^*$ within $\widetilde{O}(\sum_{i = 1}^N \Delta_i^{-2})$ time steps with high
probability, and (ii) an algorithm that incurs $O(\sum_{i \notin S^*}
K\Delta_i^{-1} \log T)$ regret in $T$ time steps. To our knowledge, our
algorithms are the first to achieve gap-dependent bounds that fully depends on
the suboptimality gaps of all items. Our technical contributions include an
algorithmic framework that relates the MNL-bandit problem to a variant of the
top-$K$ arm identification problem in multi-armed bandits, a generalized
epoch-based offering procedure, and a layer-based adaptive estimation
procedure.
</p>
<a href="http://arxiv.org/abs/2011.09998" target="_blank">arXiv:2011.09998</a> [<a href="http://arxiv.org/pdf/2011.09998" target="_blank">pdf</a>]

<h2>Inverse Constrained Reinforcement Learning. (arXiv:2011.09999v1 [cs.LG])</h2>
<h3>Usman Anwar, Shehryar Malik, Alireza Aghasi, Ali Ahmed</h3>
<p>Standard reinforcement learning (RL) algorithms train agents to maximize
given reward functions. However, many real-world applications of RL require
agents to also satisfy certain constraints which may, for example, be motivated
by safety concerns. Constrained RL algorithms approach this problem by training
agents to maximize given reward functions while respecting \textit{explicitly}
defined constraints. However, in many cases, manually designing accurate
constraints is a challenging task. In this work, given a reward function and a
set of demonstrations from an expert that maximizes this reward function while
respecting \textit{unknown} constraints, we propose a framework to learn the
most likely constraints that the expert respects. We then train agents to
maximize the given reward function subject to the learned constraints. Previous
works in this regard have either mainly been restricted to tabular settings or
specific types of constraints or assume knowledge of transition dynamics of the
environment. In contrast, we empirically show that our framework is able to
learn arbitrary \textit{Markovian} constraints in high-dimensions in a
model-free setting.
</p>
<a href="http://arxiv.org/abs/2011.09999" target="_blank">arXiv:2011.09999</a> [<a href="http://arxiv.org/pdf/2011.09999" target="_blank">pdf</a>]

<h2>Improved rates for identification of partially observed linear dynamical systems. (arXiv:2011.10006v1 [cs.LG])</h2>
<h3>Holden Lee</h3>
<p>Identification of a linear time-invariant dynamical system from partial
observations is a fundamental problem in control theory. A natural question is
how to do so with non-asymptotic statistical rates depending on the inherent
dimensionality (order) $d$ of the system, rather than on the sufficient rollout
length or on $\frac1{1-\rho(A)}$, where $\rho(A)$ is the spectral radius of the
dynamics matrix. We develop the first algorithm that given a single trajectory
of length $T$ with gaussian observation noise, achieves a near-optimal rate of
$\widetilde O\left(\sqrt\frac{d}{T}\right)$ in $\mathcal{H}_2$ error for the
learned system. We also give bounds under process noise and improved bounds for
learning a realization of the system. Our algorithm is based on low-rank
approximation of Hankel matrices of geometrically increasing sizes.
</p>
<a href="http://arxiv.org/abs/2011.10006" target="_blank">arXiv:2011.10006</a> [<a href="http://arxiv.org/pdf/2011.10006" target="_blank">pdf</a>]

<h2>Multi-Plane Program Induction with 3D Box Priors. (arXiv:2011.10007v1 [cs.CV])</h2>
<h3>Yikai Li, Jiayuan Mao, Xiuming Zhang, Bill Freeman, Josh Tenenbaum, Noah Snavely, Jiajun Wu</h3>
<p>We consider two important aspects in understanding and editing images:
modeling regular, program-like texture or patterns in 2D planes, and 3D posing
of these planes in the scene. Unlike prior work on image-based program
synthesis, which assumes the image contains a single visible 2D plane, we
present Box Program Induction (BPI), which infers a program-like scene
representation that simultaneously models repeated structure on multiple 2D
planes, the 3D position and orientation of the planes, and camera parameters,
all from a single image. Our model assumes a box prior, i.e., that the image
captures either an inner view or an outer view of a box in 3D. It uses neural
networks to infer visual cues such as vanishing points, wireframe lines to
guide a search-based algorithm to find the program that best explains the
image. Such a holistic, structured scene representation enables 3D-aware
interactive image editing operations such as inpainting missing pixels,
changing camera parameters, and extrapolate the image contents.
</p>
<a href="http://arxiv.org/abs/2011.10007" target="_blank">arXiv:2011.10007</a> [<a href="http://arxiv.org/pdf/2011.10007" target="_blank">pdf</a>]

<h2>DiffusionNet: Accelerating the solution of Time-Dependent partial differential equations using deep learning. (arXiv:2011.10015v1 [cs.LG])</h2>
<h3>Mahmoud Asem</h3>
<p>We present our deep learning framework to solve and accelerate the
Time-Dependent partial differential equation's solution of one and two spatial
dimensions. We demonstrate DiffusionNet solver by solving the 2D transient heat
conduction problem with Dirichlet boundary conditions. The model is trained on
solution data calculated using the Alternating direction implicit method. We
show the model's ability to predict the solution from any combination of seven
variables: the starting time step of the solution, initial condition, four
boundary conditions, and a combined variable of the time step size, diffusivity
constant, and grid step size. To improve speed, we exploit our model capability
to predict the solution of the Time-dependent PDE after multiple time steps at
once to improve the speed of solution by dividing the solution into
parallelizable chunks. We try to build a flexible architecture capable of
solving a wide range of partial differential equations with minimal changes. We
demonstrate our model flexibility by applying our model with the same network
architecture used to solve the transient heat conduction to solve the Inviscid
Burgers equation and Steady-state heat conduction, then compare our model
performance against related studies. We show that our model reduces the error
of the solution for the investigated problems.
</p>
<a href="http://arxiv.org/abs/2011.10015" target="_blank">arXiv:2011.10015</a> [<a href="http://arxiv.org/pdf/2011.10015" target="_blank">pdf</a>]

<h2>Parrot: Data-Driven Behavioral Priors for Reinforcement Learning. (arXiv:2011.10024v1 [cs.LG])</h2>
<h3>Avi Singh, Huihan Liu, Gaoyue Zhou, Albert Yu, Nicholas Rhinehart, Sergey Levine</h3>
<p>Reinforcement learning provides a general framework for flexible decision
making and control, but requires extensive data collection for each new task
that an agent needs to learn. In other machine learning fields, such as natural
language processing or computer vision, pre-training on large, previously
collected datasets to bootstrap learning for new tasks has emerged as a
powerful paradigm to reduce data requirements when learning a new task. In this
paper, we ask the following question: how can we enable similarly useful
pre-training for RL agents? We propose a method for pre-training behavioral
priors that can capture complex input-output relationships observed in
successful trials from a wide range of previously seen tasks, and we show how
this learned prior can be used for rapidly learning new tasks without impeding
the RL agent's ability to try out novel behaviors. We demonstrate the
effectiveness of our approach in challenging robotic manipulation domains
involving image observations and sparse reward functions, where our method
outperforms prior works by a substantial margin.
</p>
<a href="http://arxiv.org/abs/2011.10024" target="_blank">arXiv:2011.10024</a> [<a href="http://arxiv.org/pdf/2011.10024" target="_blank">pdf</a>]

<h2>The Cube++ Illumination Estimation Dataset. (arXiv:2011.10028v1 [cs.CV])</h2>
<h3>Egor Ershov, Alex Savchik, Illya Semenkov, Nikola Bani&#x107;, Alexander Belokopytov, Daria Senshina, Karlo Koscevi&#x107;, Marko Suba&#x161;i&#x107;, Sven Lon&#x10d;ari&#x107;</h3>
<p>Computational color constancy has the important task of reducing the
influence of the scene illumination on the object colors. As such, it is an
essential part of the image processing pipelines of most digital cameras. One
of the important parts of the computational color constancy is illumination
estimation, i.e. estimating the illumination color. When an illumination
estimation method is proposed, its accuracy is usually reported by providing
the values of error metrics obtained on the images of publicly available
datasets. However, over time it has been shown that many of these datasets have
problems such as too few images, inappropriate image quality, lack of scene
diversity, absence of version tracking, violation of various assumptions, GDPR
regulation violation, lack of additional shooting procedure info, etc. In this
paper, a new illumination estimation dataset is proposed that aims to alleviate
many of the mentioned problems and to help the illumination estimation
research. It consists of 4890 images with known illumination colors as well as
with additional semantic data that can further make the learning process more
accurate. Due to the usage of the SpyderCube color target, for every image
there are two ground-truth illumination records covering different directions.
Because of that, the dataset can be used for training and testing of methods
that perform single or two-illuminant estimation. This makes it superior to
many similar existing datasets. The datasets, it's smaller version
SimpleCube++, and the accompanying code are available at
https://github.com/Visillect/CubePlusPlus/.
</p>
<a href="http://arxiv.org/abs/2011.10028" target="_blank">arXiv:2011.10028</a> [<a href="http://arxiv.org/pdf/2011.10028" target="_blank">pdf</a>]

<h2>Cylindrical and Asymmetrical 3D Convolution Networks for LiDAR Segmentation. (arXiv:2011.10033v1 [cs.CV])</h2>
<h3>Xinge Zhu, Hui Zhou, Tai Wang, Fangzhou Hong, Yuexin Ma, Wei Li, Hongsheng Li, Dahua Lin</h3>
<p>State-of-the-art methods for large-scale driving-scene LiDAR segmentation
often project the point clouds to 2D space and then process them via 2D
convolution. Although this corporation shows the competitiveness in the point
cloud, it inevitably alters and abandons the 3D topology and geometric
relations. A natural remedy is to utilize the3D voxelization and 3D convolution
network. However, we found that in the outdoor point cloud, the improvement
obtained in this way is quite limited. An important reason is the property of
the outdoor point cloud, namely sparsity and varying density. Motivated by this
investigation, we propose a new framework for the outdoor LiDAR segmentation,
where cylindrical partition and asymmetrical 3D convolution networks are
designed to explore the 3D geometric pat-tern while maintaining these inherent
properties. Moreover, a point-wise refinement module is introduced to alleviate
the interference of lossy voxel-based label encoding. We evaluate the proposed
model on two large-scale datasets, i.e., SemanticKITTI and nuScenes. Our method
achieves the 1st place in the leaderboard of SemanticKITTI and outperforms
existing methods on nuScenes with a noticeable margin, about 4%. Furthermore,
the proposed 3D framework also generalizes well to LiDAR panoptic segmentation
and LiDAR 3D detection.
</p>
<a href="http://arxiv.org/abs/2011.10033" target="_blank">arXiv:2011.10033</a> [<a href="http://arxiv.org/pdf/2011.10033" target="_blank">pdf</a>]

<h2>Decentralized Task and Path Planning for Multi-Robot Systems. (arXiv:2011.10034v1 [cs.RO])</h2>
<h3>Yuxiao Chen, Ugo Rosolia, Aaron D. Ames</h3>
<p>We consider a multi-robot system with a team of collaborative robots and
multiple tasks that emerges over time. We propose a fully decentralized task
and path planning (DTPP) framework consisting of a task allocation module and a
localized path planning module. Each task is modeled as a Markov Decision
Process (MDP) or a Mixed Observed Markov Decision Process (MOMDP) depending on
whether full states or partial states are observable. The task allocation
module then aims at maximizing the expected pure reward (reward minus cost) of
the robotic team. We fuse the Markov model into a factor graph formulation so
that the task allocation can be decentrally solved using the max-sum algorithm.
Each robot agent follows the optimal policy synthesized for the Markov model
and we propose a localized forward dynamic programming scheme that resolves
conflicts between agents and avoids collisions. The proposed framework is
demonstrated with high fidelity ROS simulations and experiments with multiple
ground robots.
</p>
<a href="http://arxiv.org/abs/2011.10034" target="_blank">arXiv:2011.10034</a> [<a href="http://arxiv.org/pdf/2011.10034" target="_blank">pdf</a>]

<h2>On the Dynamics of Training Attention Models. (arXiv:2011.10036v1 [cs.LG])</h2>
<h3>Haoye Lu, Yongyi Mao, Amiya Nayak</h3>
<p>The attention mechanism has been widely used in deep neural networks as a
model component. By now, it has become a critical building block in many
state-of-the-art natural language models. Despite its great success established
empirically, the working mechanism of attention has not been investigated at a
sufficient theoretical depth to date. In this paper, we set up a simple text
classification task and study the dynamics of training a simple attention-based
classification model using gradient descent. In this setting, we show that, for
the discriminative words that the model should attend to, a persisting identity
exists relating its embedding and the inner product of its key and the query.
This allows us to prove that training must converge to attending to the
discriminative words when the attention output is classified by a linear
classifier. Experiments are performed, which validates our theoretical analysis
and provides further insights.
</p>
<a href="http://arxiv.org/abs/2011.10036" target="_blank">arXiv:2011.10036</a> [<a href="http://arxiv.org/pdf/2011.10036" target="_blank">pdf</a>]

<h2>Creative Sketch Generation. (arXiv:2011.10039v1 [cs.CV])</h2>
<h3>Songwei Ge, Vedanuj Goswami, C. Lawrence Zitnick, Devi Parikh</h3>
<p>Sketching or doodling is a popular creative activity that people engage in.
However, most existing work in automatic sketch understanding or generation has
focused on sketches that are quite mundane. In this work, we introduce two
datasets of creative sketches -- Creative Birds and Creative Creatures --
containing 10k sketches each along with part annotations. We propose DoodlerGAN
-- a part-based Generative Adversarial Network (GAN) -- to generate unseen
compositions of novel part appearances. Quantitative evaluations as well as
human studies demonstrate that sketches generated by our approach are more
creative and of higher quality than existing approaches. In fact, in Creative
Birds, subjects prefer sketches generated by DoodlerGAN over those drawn by
humans! Our code can be found at https://github.com/facebookresearch/DoodlerGAN
and a demo can be found at this http URL
</p>
<a href="http://arxiv.org/abs/2011.10039" target="_blank">arXiv:2011.10039</a> [<a href="http://arxiv.org/pdf/2011.10039" target="_blank">pdf</a>]

<h2>Propagate Yourself: Exploring Pixel-Level Consistency for Unsupervised Visual Representation Learning. (arXiv:2011.10043v1 [cs.CV])</h2>
<h3>Zhenda Xie, Yutong Lin, Zheng Zhang, Yue Cao, Stephen Lin, Han Hu</h3>
<p>Contrastive learning methods for unsupervised visual representation learning
have reached remarkable levels of transfer performance. We argue that the power
of contrastive learning has yet to be fully unleashed, as current methods are
trained only on instance-level pretext tasks, leading to representations that
may be sub-optimal for downstream tasks requiring dense pixel predictions. In
this paper, we introduce pixel-level pretext tasks for learning dense feature
representations. The first task directly applies contrastive learning at the
pixel level. We additionally propose a pixel-to-propagation consistency task
that produces better results, even surpassing the state-of-the-art approaches
by a large margin. Specifically, it achieves 60.2 AP, 41.4 / 40.5 mAP and 77.2
mIoU when transferred to Pascal VOC object detection (C4), COCO object
detection (FPN / C4) and Cityscapes semantic segmentation using a ResNet-50
backbone network, which are 2.6 AP, 0.8 / 1.0 mAP and 1.0 mIoU better than the
previous best methods built on instance-level contrastive learning. Moreover,
the pixel-level pretext tasks are found to be effective for pre-training not
only regular backbone networks but also head networks used for dense downstream
tasks, and are complementary to instance-level contrastive methods. These
results demonstrate the strong potential of defining pretext tasks at the pixel
level, and suggest a new path forward in unsupervised visual representation
learning.
</p>
<a href="http://arxiv.org/abs/2011.10043" target="_blank">arXiv:2011.10043</a> [<a href="http://arxiv.org/pdf/2011.10043" target="_blank">pdf</a>]

<h2>Deep Learning Estimation of Absorbed Dose for Nuclear Medicine Diagnostics. (arXiv:1805.09108v6 [stat.ML] UPDATED)</h2>
<h3>Luciano Melodia</h3>
<p>The distribution of energy dose from Lu$^{177}$ radiotherapy can be estimated
by convolving an image of a time-integrated activity distribution with a dose
voxel kernel (DVK) consisting of different types of tissues. This fast and
inacurate approximation is inappropriate for personalized dosimetry as it
neglects tissue heterogenity. The latter can be calculated using different
imaging techniques such as CT and SPECT combined with a time consuming
monte-carlo simulation. The aim of this study is, for the first time, an
estimation of DVKs from CT-derived density kernels (DK) via deep learning in
convolutional neural networks (CNNs). The proposed CNN achieved, on the test
set, a mean intersection over union (IOU) of $= 0.86$ after $308$ epochs and a
corresponding mean squared error (MSE) $= 1.24 \cdot 10^{-4}$. This
generalization ability shows that the trained CNN can indeed learn the
difficult transfer function from DK to DVK. Future work will evaluate DVKs
estimated by CNNs with full monte-carlo simulations of a whole body CT to
predict patient specific voxel dose maps.
</p>
<a href="http://arxiv.org/abs/1805.09108" target="_blank">arXiv:1805.09108</a> [<a href="http://arxiv.org/pdf/1805.09108" target="_blank">pdf</a>]

<h2>Exploring $k$ out of Top $\rho$ Fraction of Arms in Stochastic Bandits. (arXiv:1810.11857v2 [cs.LG] UPDATED)</h2>
<h3>Wenbo Ren, Jia Liu, Ness Shroff</h3>
<p>This paper studies the problem of identifying any $k$ distinct arms among the
top $\rho$ fraction (e.g., top 5\%) of arms from a finite or infinite set with
a probably approximately correct (PAC) tolerance $\epsilon$. We consider two
cases: (i) when the threshold of the top arms' expected rewards is known and
(ii) when it is unknown. We prove lower bounds for the four variants (finite or
infinite arms, and known or unknown threshold), and propose algorithms for
each. Two of these algorithms are shown to be sample complexity optimal (up to
constant factors) and the other two are optimal up to a log factor. Results in
this paper provide up to $\rho n/k$ reductions compared with the
"$k$-exploration" algorithms that focus on finding the (PAC) best $k$ arms out
of $n$ arms. We also numerically show improvements over the state-of-the-art.
</p>
<a href="http://arxiv.org/abs/1810.11857" target="_blank">arXiv:1810.11857</a> [<a href="http://arxiv.org/pdf/1810.11857" target="_blank">pdf</a>]

<h2>Learning Gaussian Graphical Models with Ordered Weighted L1 Regularization. (arXiv:1906.02719v2 [stat.ML] UPDATED)</h2>
<h3>Cody Mazza-Anthony, Bogdan Mazoure, Mark Coates</h3>
<p>We address the task of identifying densely connected subsets of multivariate
Gaussian random variables within a graphical model framework. We propose two
novel estimators based on the Ordered Weighted $\ell_1$ (OWL) norm: 1) The
Graphical OWL (GOWL) is a penalized likelihood method that applies the OWL norm
to the lower triangle components of the precision matrix. 2) The
column-by-column Graphical OWL (ccGOWL) estimates the precision matrix by
performing OWL regularized linear regressions. Both methods can simultaneously
identify highly correlated groups of variables and control the sparsity in the
resulting precision matrix. We formulate GOWL such that it solves a composite
optimization problem and establish that the estimator has a unique global
solution. In addition, we prove sufficient grouping conditions for each column
of the ccGOWL precision matrix estimate. We propose proximal descent algorithms
to find the optimum for both estimators. For synthetic data where group
structure is present, the ccGOWL estimator requires significantly reduced
computation and achieves similar or greater accuracy than state-of-the-art
estimators. Timing comparisons are presented and demonstrates the superior
computational efficiency of the ccGOWL. We illustrate the grouping performance
of the ccGOWL method on a cancer gene expression data set and an equities data
set.
</p>
<a href="http://arxiv.org/abs/1906.02719" target="_blank">arXiv:1906.02719</a> [<a href="http://arxiv.org/pdf/1906.02719" target="_blank">pdf</a>]

<h2>Stochastic Gradient and Langevin Processes. (arXiv:1907.03215v7 [cs.LG] UPDATED)</h2>
<h3>Xiang Cheng, Dong Yin, Peter L. Bartlett, Michael I. Jordan</h3>
<p>We prove quantitative convergence rates at which discrete Langevin-like
processes converge to the invariant distribution of a related stochastic
differential equation. We study the setup where the additive noise can be
non-Gaussian and state-dependent and the potential function can be non-convex.
We show that the key properties of these processes depend on the potential
function and the second moment of the additive noise. We apply our theoretical
findings to studying the convergence of Stochastic Gradient Descent (SGD) for
non-convex problems and corroborate them with experiments using SGD to train
deep neural networks on the CIFAR-10 dataset.
</p>
<a href="http://arxiv.org/abs/1907.03215" target="_blank">arXiv:1907.03215</a> [<a href="http://arxiv.org/pdf/1907.03215" target="_blank">pdf</a>]

<h2>Estimating Mass Distribution of Articulated Objects using Non-prehensile Manipulation. (arXiv:1907.03964v4 [cs.RO] UPDATED)</h2>
<h3>K. Niranjan Kumar, Irfan Essa, Sehoon Ha, C. Karen Liu</h3>
<p>We explore the problem of estimating the mass distribution of an articulated
object by an interactive robotic agent. Our method predicts the mass
distribution of an object by using the limited sensing and actuating
capabilities of a robotic agent that is interacting with the object. We are
inspired by the role of exploratory play in human infants. We take the combined
approach of supervised and reinforcement learning to train an agent that learns
to strategically interact with the object to estimate the object's mass
distribution. Our method consists of two neural networks: (i) the policy
network which decides how to interact with the object, and (ii) the predictor
network that estimates the mass distribution given a history of observations
and interactions. Using our method, we train a robotic arm to estimate the mass
distribution of an object with moving parts (e.g. an articulated rigid body
system) by pushing it on a surface with unknown friction properties. We also
demonstrate how our training from simulations can be transferred to real
hardware using a small amount of real-world data for fine-tuning. We use a UR10
robot to interact with 3D printed articulated chains with varying mass
distributions and show that our method significantly outperforms the baseline
system that uses random pushes to interact with the object.
</p>
<a href="http://arxiv.org/abs/1907.03964" target="_blank">arXiv:1907.03964</a> [<a href="http://arxiv.org/pdf/1907.03964" target="_blank">pdf</a>]

<h2>Toward Metrics for Differentiating Out-of-Distribution Sets. (arXiv:1910.08650v3 [cs.LG] UPDATED)</h2>
<h3>Mahdieh Abbasi, Changjian Shui, Arezoo Rajabi, Christian Gagne, Rakesh Bobba</h3>
<p>Vanilla CNNs, as uncalibrated classifiers, suffer from classifying
out-of-distribution (OOD) samples nearly as confidently as in-distribution
samples. To tackle this challenge, some recent works have demonstrated the
gains of leveraging available OOD sets for training end-to-end calibrated CNNs.
However, a critical question remains unanswered in these works: how to
differentiate OOD sets for selecting the most effective one(s) that induce
training such CNNs with high detection rates on unseen OOD sets? To address
this pivotal question, we provide a criterion based on generalization errors of
Augmented-CNN, a vanilla CNN with an added extra class employed for rejection,
on in-distribution and unseen OOD sets. However, selecting the most effective
OOD set by directly optimizing this criterion incurs a huge computational cost.
Instead, we propose three novel computationally-efficient metrics for
differentiating between OOD sets according to their "protection" level of
in-distribution sub-manifolds. We empirically verify that the most protective
OOD sets -- selected according to our metrics -- lead to A-CNNs with
significantly lower generalization errors than the A-CNNs trained on the least
protective ones. We also empirically show the effectiveness of a protective OOD
set for training well-generalized confidence-calibrated vanilla CNNs. These
results confirm that 1) all OOD sets are not equally effective for training
well-performing end-to-end models (i.e., A-CNNs and calibrated CNNs) for OOD
detection tasks and 2) the protection level of OOD sets is a viable factor for
recognizing the most effective one. Finally, across the image classification
tasks, we exhibit A-CNN trained on the most protective OOD set can also detect
black-box FGS adversarial examples as their distance (measured by our metrics)
is becoming larger from the protected sub-manifolds.
</p>
<a href="http://arxiv.org/abs/1910.08650" target="_blank">arXiv:1910.08650</a> [<a href="http://arxiv.org/pdf/1910.08650" target="_blank">pdf</a>]

<h2>gradSLAM: Automagically differentiable SLAM. (arXiv:1910.10672v3 [cs.RO] UPDATED)</h2>
<h3>Krishna Murthy Jatavallabhula, Soroush Saryazdi, Ganesh Iyer, Liam Paull</h3>
<p>Blending representation learning approaches with simultaneous localization
and mapping (SLAM) systems is an open question, because of their highly modular
and complex nature. Functionally, SLAM is an operation that transforms raw
sensor inputs into a distribution over the state(s) of the robot and the
environment. If this transformation (SLAM) were expressible as a differentiable
function, we could leverage task-based error signals to learn representations
that optimize task performance. However, several components of a typical dense
SLAM system are non-differentiable. In this work, we propose gradSLAM, a
methodology for posing SLAM systems as differentiable computational graphs,
which unifies gradient-based learning and SLAM. We propose differentiable
trust-region optimizers, surface measurement and fusion schemes, and
raycasting, without sacrificing accuracy. This amalgamation of dense SLAM with
computational graphs enables us to backprop all the way from 3D maps to 2D
pixels, opening up new possibilities in gradient-based learning for SLAM.

TL;DR: We leverage the power of automatic differentiation frameworks to make
dense SLAM differentiable.
</p>
<a href="http://arxiv.org/abs/1910.10672" target="_blank">arXiv:1910.10672</a> [<a href="http://arxiv.org/pdf/1910.10672" target="_blank">pdf</a>]

<h2>An Alternative Probabilistic Interpretation of the Huber Loss. (arXiv:1911.02088v3 [stat.ML] UPDATED)</h2>
<h3>Gregory P. Meyer</h3>
<p>The Huber loss is a robust loss function used for a wide range of regression
tasks. To utilize the Huber loss, a parameter that controls the transitions
from a quadratic function to an absolute value function needs to be selected.
We believe the standard probabilistic interpretation that relates the Huber
loss to the Huber density fails to provide adequate intuition for identifying
the transition point. As a result, a hyper-parameter search is often necessary
to determine an appropriate value. In this work, we propose an alternative
probabilistic interpretation of the Huber loss, which relates minimizing the
loss to minimizing an upper-bound on the Kullback-Leibler divergence between
Laplace distributions, where one distribution represents the noise in the
ground-truth and the other represents the noise in the prediction. In addition,
we show that the parameters of the Laplace distributions are directly related
to the transition point of the Huber loss. We demonstrate, through a toy
problem, that the optimal transition point of the Huber loss is closely related
to the distribution of the noise in the ground-truth data. As a result, our
interpretation provides an intuitive way to identify well-suited
hyper-parameters by approximating the amount of noise in the data, which we
demonstrate through a case study and experimentation on the Faster R-CNN and
RetinaNet object detectors.
</p>
<a href="http://arxiv.org/abs/1911.02088" target="_blank">arXiv:1911.02088</a> [<a href="http://arxiv.org/pdf/1911.02088" target="_blank">pdf</a>]

<h2>AdaShare: Learning What To Share For Efficient Deep Multi-Task Learning. (arXiv:1911.12423v2 [cs.CV] UPDATED)</h2>
<h3>Ximeng Sun, Rameswar Panda, Rogerio Feris, Kate Saenko</h3>
<p>Multi-task learning is an open and challenging problem in computer vision.
The typical way of conducting multi-task learning with deep neural networks is
either through handcrafted schemes that share all initial layers and branch out
at an adhoc point, or through separate task-specific networks with an
additional feature sharing/fusion mechanism. Unlike existing methods, we
propose an adaptive sharing approach, called AdaShare, that decides what to
share across which tasks to achieve the best recognition accuracy, while taking
resource efficiency into account. Specifically, our main idea is to learn the
sharing pattern through a task-specific policy that selectively chooses which
layers to execute for a given task in the multi-task network. We efficiently
optimize the task-specific policy jointly with the network weights, using
standard back-propagation. Experiments on several challenging and diverse
benchmark datasets with a variable number of tasks well demonstrate the
efficacy of our approach over state-of-the-art methods. Project page:
https://cs-people.bu.edu/sunxm/AdaShare/project.html.
</p>
<a href="http://arxiv.org/abs/1911.12423" target="_blank">arXiv:1911.12423</a> [<a href="http://arxiv.org/pdf/1911.12423" target="_blank">pdf</a>]

<h2>SAM: Squeeze-and-Mimic Networks for Conditional Visual Driving Policy Learning. (arXiv:1912.02973v2 [cs.CV] UPDATED)</h2>
<h3>Albert Zhao, Tong He, Yitao Liang, Haibin Huang, Guy Van den Broeck, Stefano Soatto</h3>
<p>We describe a policy learning approach to map visual inputs to driving
controls conditioned on turning command that leverages side tasks on semantics
and object affordances via a learned representation trained for driving. To
learn this representation, we train a squeeze network to drive using
annotations for the side task as input. This representation encodes the
driving-relevant information associated with the side task while ideally
throwing out side task-relevant but driving-irrelevant nuisances. We then train
a mimic network to drive using only images as input and use the squeeze
network's latent representation to supervise the mimic network via a mimicking
loss. Notably, we do not aim to achieve the side task nor to learn features for
it; instead, we aim to learn, via the mimicking loss, a representation of the
side task annotations directly useful for driving. We test our approach using
the CARLA simulator. In addition, we introduce a more challenging but realistic
evaluation protocol that considers a run that reaches the destination
successful only if it does not violate common traffic rules. A video
summarizing this work is available at https://youtu.be/ipKAMzmJpMs , and code
is available at https://github.com/twsq/sam-driving .
</p>
<a href="http://arxiv.org/abs/1912.02973" target="_blank">arXiv:1912.02973</a> [<a href="http://arxiv.org/pdf/1912.02973" target="_blank">pdf</a>]

<h2>Static and Dynamic Values of Computation in MCTS. (arXiv:2002.04335v2 [cs.AI] UPDATED)</h2>
<h3>Eren Sezener, Peter Dayan</h3>
<p>Monte-Carlo Tree Search (MCTS) is one of the most-widely used methods for
planning, and has powered many recent advances in artificial intelligence. In
MCTS, one typically performs computations (i.e., simulations) to collect
statistics about the possible future consequences of actions, and then chooses
accordingly. Many popular MCTS methods such as UCT and its variants decide
which computations to perform by trading-off exploration and exploitation. In
this work, we take a more direct approach, and explicitly quantify the value of
a computation based on its expected impact on the quality of the action
eventually chosen. Our approach goes beyond the "myopic" limitations of
existing computation-value-based methods in two senses: (I) we are able to
account for the impact of non-immediate (ie, future) computations (II) on
non-immediate actions. We show that policies that greedily optimize computation
values are optimal under certain assumptions and obtain results that are
competitive with the state-of-the-art.
</p>
<a href="http://arxiv.org/abs/2002.04335" target="_blank">arXiv:2002.04335</a> [<a href="http://arxiv.org/pdf/2002.04335" target="_blank">pdf</a>]

<h2>Multi-Task Incremental Learning for Object Detection. (arXiv:2002.05347v3 [cs.CV] UPDATED)</h2>
<h3>Xialei Liu, Hao Yang, Avinash Ravichandran, Rahul Bhotika, Stefano Soatto</h3>
<p>Multi-task learns multiple tasks, while sharing knowledge and computation
among them. However, it suffers from catastrophic forgetting of previous
knowledge when learned incrementally without access to the old data. Most
existing object detectors are domain-specific and static, while some are
learned incrementally but only within a single domain. Training an object
detector incrementally across various domains has rarely been explored. In this
work, we propose three incremental learning scenarios across various domains
and categories for object detection. To mitigate catastrophic forgetting,
attentive feature distillation is proposed to leverages both bottom-up and
top-down attentions to extract important information for distillation. We then
systematically analyze the proposed distillation method in different scenarios.
We find out that, contrary to common understanding, domain gaps have smaller
negative impact on incremental detection, while category differences are
problematic. For the difficult cases, where the domain gaps and especially
category differences are large, we explore three different exemplar sampling
methods and show the proposed adaptive sampling method is effective to select
diverse and informative samples from entire datasets, to further prevent
forgetting. Experimental results show that we achieve the significant
improvement in three different scenarios across seven object detection
benchmark datasets.
</p>
<a href="http://arxiv.org/abs/2002.05347" target="_blank">arXiv:2002.05347</a> [<a href="http://arxiv.org/pdf/2002.05347" target="_blank">pdf</a>]

<h2>Human Perception of Intrinsically Motivated Autonomy in Human-Robot Interaction. (arXiv:2002.05936v2 [cs.RO] UPDATED)</h2>
<h3>Marcus M. Scheunemann, Christoph Salge, Daniel Polani, Kerstin Dautenhahn</h3>
<p>A challenge in using robots in human-robot interaction (HRI) is to design
behavior that is engaging enough to encourage voluntary, long-term interaction,
yet robust to the perturbations induced by human interaction. Here we evaluate
if a physical robot that generates its behavior based on its intrinsic
motivations could address this challenge. We use an information theoretic
quantity - predictive information maximization - as an intrinsic motivation, as
simulated experiments showed that this leads to playful, exploratory behavior
that is robust to changes to the robot's morphology and to its environment. We
present a game-like study design, which allows us to focus on the interplay
between the robot and the human participant. In contrast to a study design
where participants order or control a robot to do a specific task, the robot
and the human participants in our study design explore their behaviors without
any specific goals. We conducted a within-subjects study (N=24) where
participants interacted with a fully autonomous Sphero BB8 robot with different
behavioral regimes: one realizing an adaptive, intrinsically motivated behavior
and the other being reactive, but not adaptive. A quantitative analysis of
post-interaction questionnaires showed a significantly higher perception of the
dimension "Warmth" compared to the baseline behavior. Warmth is considered a
primary dimension for social attitude formation in human social cognition. A
human perceived as warm (friendly, trustworthy) experiences more positive
social interactions. If future work demonstrates that this transfers to
human-robot social cognition, then the generic methods presented here could be
used to imbue robots with behavior leading to positive perception by and to
positive social interaction with humans.
</p>
<a href="http://arxiv.org/abs/2002.05936" target="_blank">arXiv:2002.05936</a> [<a href="http://arxiv.org/pdf/2002.05936" target="_blank">pdf</a>]

<h2>Adversarial Distributional Training for Robust Deep Learning. (arXiv:2002.05999v2 [cs.LG] UPDATED)</h2>
<h3>Yinpeng Dong, Zhijie Deng, Tianyu Pang, Hang Su, Jun Zhu</h3>
<p>Adversarial training (AT) is among the most effective techniques to improve
model robustness by augmenting training data with adversarial examples.
However, most existing AT methods adopt a specific attack to craft adversarial
examples, leading to the unreliable robustness against other unseen attacks.
Besides, a single attack algorithm could be insufficient to explore the space
of perturbations. In this paper, we introduce adversarial distributional
training (ADT), a novel framework for learning robust models. ADT is formulated
as a minimax optimization problem, where the inner maximization aims to learn
an adversarial distribution to characterize the potential adversarial examples
around a natural one under an entropic regularizer, and the outer minimization
aims to train robust models by minimizing the expected loss over the worst-case
adversarial distributions. Through a theoretical analysis, we develop a general
algorithm for solving ADT, and present three approaches for parameterizing the
adversarial distributions, ranging from the typical Gaussian distributions to
the flexible implicit ones. Empirical results on several benchmarks validate
the effectiveness of ADT compared with the state-of-the-art AT methods.
</p>
<a href="http://arxiv.org/abs/2002.05999" target="_blank">arXiv:2002.05999</a> [<a href="http://arxiv.org/pdf/2002.05999" target="_blank">pdf</a>]

<h2>Double Explore-then-Commit: Asymptotic Optimality and Beyond. (arXiv:2002.09174v2 [cs.LG] UPDATED)</h2>
<h3>Tianyuan Jin, Pan Xu, Xiaokui Xiao, Quanquan Gu</h3>
<p>We study the multi-armed bandit problem with subgaussian rewards. The
explore-then-commit (ETC) strategy, which consists of an exploration phase
followed by an exploitation phase, is one of the most widely used algorithms in
a variety of online decision applications. Nevertheless, it has been shown in
Garivier et al. (2016) that ETC is suboptimal in the asymptotic sense as the
horizon grows, and thus, is worse than fully sequential strategies such as
Upper Confidence Bound (UCB). In this paper, we show that a variant of ETC
algorithm can actually achieve the asymptotic optimality for multi-armed bandit
problems as UCB-type algorithms do and extend it to the batched bandit setting.
Specifically, we propose a double explore-then-commit (DETC) algorithm that has
two exploration and exploitation phases and prove that DETC achieves the
asymptotically optimal regret bound. To our knowledge, DETC is the first
non-fully-sequential algorithm that achieves such asymptotic optimality. In
addition, we extend DETC to batched bandit problems, where (i) the exploration
process is split into a small number of batches and (ii) the round complexity
is of central interest. We prove that a batched version of DETC can achieve the
asymptotic optimality with only a constant round complexity. This is the first
batched bandit algorithm that can attain the optimal asymptotic regret bound
and optimal round complexity simultaneously.
</p>
<a href="http://arxiv.org/abs/2002.09174" target="_blank">arXiv:2002.09174</a> [<a href="http://arxiv.org/pdf/2002.09174" target="_blank">pdf</a>]

<h2>Learning hierarchical relationships for object-goal navigation. (arXiv:2003.06749v2 [cs.RO] UPDATED)</h2>
<h3>Yiding Qiu, Anwesan Pal, Henrik I. Christensen</h3>
<p>Direct search for objects as part of navigation poses a challenge for small
items. Utilizing context in the form of object-object relationships enable
hierarchical search for targets efficiently. Most of the current approaches
tend to directly incorporate sensory input into a reward-based learning
approach, without learning about object relationships in the natural
environment, and thus generalize poorly across domains. We present
Memory-utilized Joint hierarchical Object Learning for Navigation in Indoor
Rooms (MJOLNIR), a target-driven navigation algorithm, which considers the
inherent relationship between target objects, and the more salient contextual
objects occurring in its surrounding. Extensive experiments conducted across
multiple environment settings show an $82.9\%$ and $93.5\%$ gain over existing
state-of-the-art navigation methods in terms of the success rate (SR), and
success weighted by path length (SPL), respectively. We also show that our
model learns to converge much faster than other algorithms, without suffering
from the well-known overfitting problem. Additional details regarding the
supplementary material and code are available at
https://sites.google.com/eng.ucsd.edu/mjolnir.
</p>
<a href="http://arxiv.org/abs/2003.06749" target="_blank">arXiv:2003.06749</a> [<a href="http://arxiv.org/pdf/2003.06749" target="_blank">pdf</a>]

<h2>Shared Cross-Modal Trajectory Prediction for Autonomous Driving. (arXiv:2004.00202v2 [cs.CV] UPDATED)</h2>
<h3>Chiho Choi</h3>
<p>We propose a framework for predicting future trajectories of traffic agents
in highly interactive environments. On the basis of the fact that autonomous
driving vehicles are equipped with various types of sensors (e.g., LiDAR
scanner, RGB camera, etc.), our work aims to get benefit from the use of
multiple input modalities that are complementary to each other. The proposed
approach is composed of two stages. (i) feature encoding where we discover
motion behavior of the target agent with respect to other directly and
indirectly observable influences. We extract such behaviors from multiple
perspectives such as in top-down and frontal view. (ii) cross-modal embedding
where we embed a set of learned behavior representations into a single
cross-modal latent space. We construct a generative model and formulate the
objective functions with an additional regularizer specifically designed for
future prediction. An extensive evaluation is conducted to show the efficacy of
the proposed framework using two benchmark driving datasets.
</p>
<a href="http://arxiv.org/abs/2004.00202" target="_blank">arXiv:2004.00202</a> [<a href="http://arxiv.org/pdf/2004.00202" target="_blank">pdf</a>]

<h2>OptiGAN: Generative Adversarial Networks for Goal Optimized Sequence Generation. (arXiv:2004.07534v9 [cs.LG] UPDATED)</h2>
<h3>Mahmoud Hossam, Trung Le, Viet Huynh, Michael Papasimeon, Dinh Phung</h3>
<p>One of the challenging problems in sequence generation tasks is the optimized
generation of sequences with specific desired goals. Current sequential
generative models mainly generate sequences to closely mimic the training data,
without direct optimization of desired goals or properties specific to the
task. We introduce OptiGAN, a generative model that incorporates both
Generative Adversarial Networks (GAN) and Reinforcement Learning (RL) to
optimize desired goal scores using policy gradients. We apply our model to text
and real-valued sequence generation, where our model is able to achieve higher
desired scores out-performing GAN and RL baselines, while not sacrificing
output sample diversity.
</p>
<a href="http://arxiv.org/abs/2004.07534" target="_blank">arXiv:2004.07534</a> [<a href="http://arxiv.org/pdf/2004.07534" target="_blank">pdf</a>]

<h2>P2ExNet: Patch-based Prototype Explanation Network. (arXiv:2005.02006v2 [cs.AI] UPDATED)</h2>
<h3>Dominique Mercier, Andreas Dengel, Sheraz Ahmed</h3>
<p>Deep learning methods have shown great success in several domains as they
process a large amount of data efficiently, capable of solving complex
classification, forecast, segmentation, and other tasks. However, they come
with the inherent drawback of inexplicability limiting their applicability and
trustworthiness. Although there exists work addressing this perspective, most
of the existing approaches are limited to the image modality due to the
intuitive and prominent concepts. Conversely, the concepts in the time-series
domain are more complex and non-comprehensive but these and an explanation for
the network decision are pivotal in critical domains like medical, financial,
or industry. Addressing the need for an explainable approach, we propose a
novel interpretable network scheme, designed to inherently use an explainable
reasoning process inspired by the human cognition without the need of
additional post-hoc explainability methods. Therefore, class-specific patches
are used as they cover local concepts relevant to the classification to reveal
similarities with samples of the same class. In addition, we introduce a novel
loss concerning interpretability and accuracy that constraints P2ExNet to
provide viable explanations of the data including relevant patches, their
position, class similarities, and comparison methods without compromising
accuracy. Analysis of the results on eight publicly available time-series
datasets reveals that P2ExNet reaches comparable performance when compared to
its counterparts while inherently providing understandable and traceable
decisions.
</p>
<a href="http://arxiv.org/abs/2005.02006" target="_blank">arXiv:2005.02006</a> [<a href="http://arxiv.org/pdf/2005.02006" target="_blank">pdf</a>]

<h2>Synthetic Observational Health Data with GANs: from slow adoption to a boom in medical research and ultimately digital twins?. (arXiv:2005.13510v3 [cs.LG] UPDATED)</h2>
<h3>Jeremy Georges-Filteau, Elisa Cirillo</h3>
<p>After being collected for patient care, Observational Health Data (OHD) can
further benefit patient well-being by sustaining the development of health
informatics and medical research. Vast potential is unexploited because of the
fiercely private nature of patient-related data and regulations to protect it.

Generative Adversarial Networks (GANs) have recently emerged as a
groundbreaking way to learn generative models that produce realistic synthetic
data. They have revolutionized practices in multiple domains such as
self-driving cars, fraud detection, digital twin simulations in industrial
sectors, and medical imaging.

The digital twin concept could readily apply to modelling and quantifying
disease progression. In addition, GANs posses many capabilities relevant to
common problems in healthcare: lack of data, class imbalance, rare diseases,
and preserving privacy. Unlocking open access to privacy-preserving OHD could
be transformative for scientific research. In the midst of COVID-19, the
healthcare system is facing unprecedented challenges, many of which of are data
related for the reasons stated above.

Considering these facts, publications concerning GAN applied to OHD seemed to
be severely lacking. To uncover the reasons for this slow adoption, we broadly
reviewed the published literature on the subject. Our findings show that the
properties of OHD were initially challenging for the existing GAN algorithms
(unlike medical imaging, for which state-of-the-art model were directly
transferable) and the evaluation synthetic data lacked clear metrics.

We find more publications on the subject than expected, starting slowly in
2017, and since then at an increasing rate. The difficulties of OHD remain, and
we discuss issues relating to evaluation, consistency, benchmarking, data
modelling, and reproducibility.
</p>
<a href="http://arxiv.org/abs/2005.13510" target="_blank">arXiv:2005.13510</a> [<a href="http://arxiv.org/pdf/2005.13510" target="_blank">pdf</a>]

<h2>Approximation and convergence of GANs training: an SDE approach. (arXiv:2006.02047v4 [cs.LG] UPDATED)</h2>
<h3>Haoyang Cao, Xin Guo</h3>
<p>Generative adversarial networks (GANs) have enjoyed tremendous empirical
successes, and research interest in the theoretical understanding of GANs
training process is rapidly growing, especially for its evolution and
convergence analysis. This paper establishes approximations, with precise error
bound analysis, for the training of GANs under stochastic gradient algorithms
(SGAs). The approximations are in the form of coupled stochastic differential
equations (SDEs). The analysis of the SDEs and the associated invariant
measures yields conditions for the convergence of GANs training. Further
analysis of the invariant measure for the coupled SDEs gives rise to a
fluctuation-dissipation relations (FDRs) for GANs, revealing the trade-off of
the loss landscape between the generator and the discriminator and providing
guidance for learning rate scheduling.
</p>
<a href="http://arxiv.org/abs/2006.02047" target="_blank">arXiv:2006.02047</a> [<a href="http://arxiv.org/pdf/2006.02047" target="_blank">pdf</a>]

<h2>Sparse learning with CART. (arXiv:2006.04266v2 [stat.ML] UPDATED)</h2>
<h3>Jason M. Klusowski</h3>
<p>Decision trees with binary splits are popularly constructed using
Classification and Regression Trees (CART) methodology. For regression models,
this approach recursively divides the data into two near-homogenous daughter
nodes according to a split point that maximizes the reduction in sum of squares
error (the impurity) along a particular variable. This paper aims to study the
statistical properties of regression trees constructed with CART methodology.
In doing so, we find that the training error is governed by the Pearson
correlation between the optimal decision stump and response data in each node,
which we bound by constructing a prior distribution on the split points and
solving a nonlinear optimization problem. We leverage this connection between
the training error and Pearson correlation to show that CART with
cost-complexity pruning achieves an optimal complexity/goodness-of-fit tradeoff
when the depth scales with the logarithm of the sample size. Data dependent
quantities, which adapt to the dimensionality and latent structure of the
regression model, are seen to govern the rates of convergence of the prediction
error.
</p>
<a href="http://arxiv.org/abs/2006.04266" target="_blank">arXiv:2006.04266</a> [<a href="http://arxiv.org/pdf/2006.04266" target="_blank">pdf</a>]

<h2>SEFR: A Fast Linear-Time Classifier for Ultra-Low Power Devices. (arXiv:2006.04620v2 [cs.LG] UPDATED)</h2>
<h3>Hamidreza Keshavarz, Mohammad Saniee Abadeh, Reza Rawassizadeh</h3>
<p>A fundamental challenge for running machine learning algorithms on
battery-powered devices is the time and energy limitations, as these devices
have constraints on resources. There are resource-efficient classifier
algorithms that can run on these devices, but their accuracy is often
sacrificed for resource efficiency. Here, we propose an ultra-low power
classifier, SEFR, with linear time complexity, both in the training and the
testing phases. SEFR is comparable to state-of-the-art classifiers in terms of
classification accuracy, but it is 63 times faster and 70 times more energy
efficient than the average of state-of-the-art and baseline classifiers on
binary class datasets. The energy and memory consumption of SEFR is very
insignificant, and it can even perform both train and test phases on
microcontrollers. To our knowledge, this is the first multipurpose
classification algorithm specifically designed to perform both training and
testing on ultra-low power devices.
</p>
<a href="http://arxiv.org/abs/2006.04620" target="_blank">arXiv:2006.04620</a> [<a href="http://arxiv.org/pdf/2006.04620" target="_blank">pdf</a>]

<h2>Pruning neural networks without any data by iteratively conserving synaptic flow. (arXiv:2006.05467v3 [cs.LG] UPDATED)</h2>
<h3>Hidenori Tanaka, Daniel Kunin, Daniel L. K. Yamins, Surya Ganguli</h3>
<p>Pruning the parameters of deep neural networks has generated intense interest
due to potential savings in time, memory and energy both during training and at
test time. Recent works have identified, through an expensive sequence of
training and pruning cycles, the existence of winning lottery tickets or sparse
trainable subnetworks at initialization. This raises a foundational question:
can we identify highly sparse trainable subnetworks at initialization, without
ever training, or indeed without ever looking at the data? We provide an
affirmative answer to this question through theory driven algorithm design. We
first mathematically formulate and experimentally verify a conservation law
that explains why existing gradient-based pruning algorithms at initialization
suffer from layer-collapse, the premature pruning of an entire layer rendering
a network untrainable. This theory also elucidates how layer-collapse can be
entirely avoided, motivating a novel pruning algorithm Iterative Synaptic Flow
Pruning (SynFlow). This algorithm can be interpreted as preserving the total
flow of synaptic strengths through the network at initialization subject to a
sparsity constraint. Notably, this algorithm makes no reference to the training
data and consistently competes with or outperforms existing state-of-the-art
pruning algorithms at initialization over a range of models (VGG and ResNet),
datasets (CIFAR-10/100 and Tiny ImageNet), and sparsity constraints (up to
99.99 percent). Thus our data-agnostic pruning algorithm challenges the
existing paradigm that, at initialization, data must be used to quantify which
synapses are important.
</p>
<a href="http://arxiv.org/abs/2006.05467" target="_blank">arXiv:2006.05467</a> [<a href="http://arxiv.org/pdf/2006.05467" target="_blank">pdf</a>]

<h2>Multigrid-in-Channels Architectures for Wide Convolutional Neural Networks. (arXiv:2006.06799v2 [cs.LG] UPDATED)</h2>
<h3>Jonathan Ephrath, Lars Ruthotto, Eran Treister</h3>
<p>We present a multigrid approach that combats the quadratic growth of the
number of parameters with respect to the number of channels in standard
convolutional neural networks (CNNs). It has been shown that there is a
redundancy in standard CNNs, as networks with much sparser convolution
operators can yield similar performance to full networks. The sparsity patterns
that lead to such behavior, however, are typically random, hampering hardware
efficiency. In this work, we present a multigrid-in-channels approach for
building CNN architectures that achieves full coupling of the channels, and
whose number of parameters is linearly proportional to the width of the
network. To this end, we replace each convolution layer in a generic CNN with a
multilevel layer consisting of structured (i.e., grouped) convolutions. Our
examples from supervised image classification show that applying this strategy
to residual networks and MobileNetV2 considerably reduces the number of
parameters without negatively affecting accuracy. Therefore, we can widen
networks without dramatically increasing the number of parameters or
operations.
</p>
<a href="http://arxiv.org/abs/2006.06799" target="_blank">arXiv:2006.06799</a> [<a href="http://arxiv.org/pdf/2006.06799" target="_blank">pdf</a>]

<h2>Two-Sample Testing on Ranked Preference Data and the Role of Modeling Assumptions. (arXiv:2006.11909v2 [stat.ML] UPDATED)</h2>
<h3>Charvi Rastogi, Sivaraman Balakrishnan, Nihar B. Shah, Aarti Singh</h3>
<p>A number of applications require two-sample testing on ranked preference
data. For instance, in crowdsourcing, there is a long-standing question of
whether pairwise comparison data provided by people is distributed similar to
ratings-converted-to-comparisons. Other examples include sports data analysis
and peer grading. In this paper, we design two-sample tests for pairwise
comparison data and ranking data. For our two-sample test for pairwise
comparison data, we establish an upper bound on the sample complexity required
to correctly distinguish between the distributions of the two sets of samples.
Our test requires essentially no assumptions on the distributions. We then
prove complementary lower bounds showing that our results are tight (in the
minimax sense) up to constant factors. We investigate the role of modeling
assumptions by proving lower bounds for a range of pairwise comparison models
(WST, MST,SST, parameter-based such as BTL and Thurstone). We also provide
testing algorithms and associated sample complexity bounds for the problem of
two-sample testing with partial (or total) ranking data.Furthermore, we
empirically evaluate our results via extensive simulations as well as two
real-world datasets consisting of pairwise comparisons. By applying our
two-sample test on real-world pairwise comparison data, we conclude that
ratings and rankings provided by people are indeed distributed differently. On
the other hand, our test recognizes no significant difference in the relative
performance of European football teams across two seasons. Finally, we apply
our two-sample test on a real-world partial and total ranking dataset and find
a statistically significant difference in Sushi preferences across demographic
divisions based on gender, age and region of residence.
</p>
<a href="http://arxiv.org/abs/2006.11909" target="_blank">arXiv:2006.11909</a> [<a href="http://arxiv.org/pdf/2006.11909" target="_blank">pdf</a>]

<h2>Joints in Random Forests. (arXiv:2006.14937v3 [cs.LG] UPDATED)</h2>
<h3>Alvaro H. C. Correia, Robert Peharz, Cassio de Campos</h3>
<p>Decision Trees (DTs) and Random Forests (RFs) are powerful discriminative
learners and tools of central importance to the everyday machine learning
practitioner and data scientist. Due to their discriminative nature, however,
they lack principled methods to process inputs with missing features or to
detect outliers, which requires pairing them with imputation techniques or a
separate generative model. In this paper, we demonstrate that DTs and RFs can
naturally be interpreted as generative models, by drawing a connection to
Probabilistic Circuits, a prominent class of tractable probabilistic models.
This reinterpretation equips them with a full joint distribution over the
feature space and leads to Generative Decision Trees (GeDTs) and Generative
Forests (GeFs), a family of novel hybrid generative-discriminative models. This
family of models retains the overall characteristics of DTs and RFs while
additionally being able to handle missing features by means of marginalisation.
Under certain assumptions, frequently made for Bayes consistency results, we
show that consistency in GeDTs and GeFs extend to any pattern of missing input
features, if missing at random. Empirically, we show that our models often
outperform common routines to treat missing data, such as K-nearest neighbour
imputation, and moreover, that our models can naturally detect outliers by
monitoring the marginal probability of input features.
</p>
<a href="http://arxiv.org/abs/2006.14937" target="_blank">arXiv:2006.14937</a> [<a href="http://arxiv.org/pdf/2006.14937" target="_blank">pdf</a>]

<h2>Geometry-Inspired Top-k Adversarial Perturbations. (arXiv:2006.15669v2 [cs.CV] UPDATED)</h2>
<h3>Nurislam Tursynbek, Aleksandr Petiushko, Ivan Oseledets</h3>
<p>Deep learning models are vulnerable to adversarial examples, which endangers
their usage in real-world applications. The main target of existing adversarial
perturbations is primarily limited to change the correct Top-1 predicted class
by the incorrect one, which does not intend changing the Top-$k$ prediction.
However, in many real-world scenarios, especially dealing with digital images,
Top-$k$ predictions are more important. In this work, we propose a simple yet
effective geometry-inspired method of computing Top-$k$ adversarial examples
for any $k$. We evaluate its effectiveness and efficiency by comparing it with
other adversarial example crafting techniques. Moreover, based on this method,
we propose Top-$k$ Universal Adversarial Perturbations, image-agnostic tiny
perturbations that cause true class to be absent among the Top-$k$ prediction
for most inputs in the dataset. We experimentally show that our approach
outperforms baseline methods and even improves existing techniques of
generating Universal Adversarial Perturbations.
</p>
<a href="http://arxiv.org/abs/2006.15669" target="_blank">arXiv:2006.15669</a> [<a href="http://arxiv.org/pdf/2006.15669" target="_blank">pdf</a>]

<h2>Remix: Rebalanced Mixup. (arXiv:2007.03943v3 [cs.CV] UPDATED)</h2>
<h3>Hsin-Ping Chou, Shih-Chieh Chang, Jia-Yu Pan, Wei Wei, Da-Cheng Juan</h3>
<p>Deep image classifiers often perform poorly when training data are heavily
class-imbalanced. In this work, we propose a new regularization technique,
Remix, that relaxes Mixup's formulation and enables the mixing factors of
features and labels to be disentangled. Specifically, when mixing two samples,
while features are mixed in the same fashion as Mixup, Remix assigns the label
in favor of the minority class by providing a disproportionately higher weight
to the minority class. By doing so, the classifier learns to push the decision
boundaries towards the majority classes and balance the generalization error
between majority and minority classes. We have studied the state-of-the art
regularization techniques such as Mixup, Manifold Mixup and CutMix under
class-imbalanced regime, and shown that the proposed Remix significantly
outperforms these state-of-the-arts and several re-weighting and re-sampling
techniques, on the imbalanced datasets constructed by CIFAR-10, CIFAR-100, and
CINIC-10. We have also evaluated Remix on a real-world large-scale imbalanced
dataset, iNaturalist 2018. The experimental results confirmed that Remix
provides consistent and significant improvements over the previous methods.
</p>
<a href="http://arxiv.org/abs/2007.03943" target="_blank">arXiv:2007.03943</a> [<a href="http://arxiv.org/pdf/2007.03943" target="_blank">pdf</a>]

<h2>Top-Related Meta-Learning Method for Few-Shot Detection. (arXiv:2007.06837v4 [cs.CV] UPDATED)</h2>
<h3>Qian Li, Nan Guo, Xiaochun Ye, Duo Wang, Dongrui Fan, Zhimin Tang</h3>
<p>Many meta-learning methods are proposed for few-shot detection. However,
previous most methods have two main problems, poor detection APs, and strong
bias because of imbalance datasets. Previous works mainly alleviate these
issues by additional datasets, multi-relation attention mechanisms and
sub-modules. However, they require more cost. In this work, for meta-learning,
we find that the main challenges focus on related or irrelevant semantic
features between different categories, and poor distribution of category-based
meta-features. Therefore, we propose a Top-C classification loss (i.e. TCL-C)
for classification task and a category-based grouping mechanism. The TCL
exploits true-label and the most similar class to improve detection performance
on few-shot classes. According to appearance and environment, the
category-based grouping mechanism groups categories into different groupings to
make similar semantic features more compact for different categories,
alleviating the strong bias problem and further improving detection APs. The
whole training consists of the base model and the fine-tuning phase. During
training detection model, the category-related meta-features are regarded as
the weights to convolve dynamically, exploiting the meta-features with a shared
distribution between categories within a group to improve the detection
performance. According to grouping mechanism, we group the meta-features
vectors, so that the distribution difference between groups is obvious, and the
one within each group is less. Extensive experiments on Pascal VOC dataset
demonstrate that ours which combines the TCL with category-based grouping
significantly outperforms previous state-of-the-art methods for few-shot
detection. Compared with previous competitive baseline, ours improves detection
AP by almost 4% for few-shot detection.
</p>
<a href="http://arxiv.org/abs/2007.06837" target="_blank">arXiv:2007.06837</a> [<a href="http://arxiv.org/pdf/2007.06837" target="_blank">pdf</a>]

<h2>AQD: Towards Accurate Quantized Object Detection. (arXiv:2007.06919v3 [cs.CV] UPDATED)</h2>
<h3>Peng Chen, Jing Liu, Bohan Zhuang, Mingkui Tan, Chunhua Shen</h3>
<p>Network quantization allows inference to be conducted using low-precision
arithmetic for improved inference efficiency of deep neural networks on edge
devices. However, designing aggressively low-bit (e.g., 2-bit) quantization
schemes on complex tasks, such as object detection, still remains challenging
in terms of severe performance degradation and unverifiable efficiency on
common hardware. In this paper, we propose an Accurate Quantized object
Detection solution, termed AQD, to fully get rid of floating-point computation.
To this end, we target using fixed-point operations in all kinds of layers,
including the convolutional layers, normalization layers, and skip connections,
allowing the inference to be executed using integer-only arithmetic. To
demonstrate the improved latency-vs-accuracy tradeoff, we apply the proposed
methods on RetinaNet and FCOS. In particular, experimental results on MS-COCO
dataset show that our AQD achieves comparable or even better performance
compared with the full-precision counterpart under extremely low-bit schemes,
which is of great practical value.
</p>
<a href="http://arxiv.org/abs/2007.06919" target="_blank">arXiv:2007.06919</a> [<a href="http://arxiv.org/pdf/2007.06919" target="_blank">pdf</a>]

<h2>MCUNet: Tiny Deep Learning on IoT Devices. (arXiv:2007.10319v2 [cs.CV] UPDATED)</h2>
<h3>Ji Lin, Wei-Ming Chen, Yujun Lin, John Cohn, Chuang Gan, Song Han</h3>
<p>Machine learning on tiny IoT devices based on microcontroller units (MCU) is
appealing but challenging: the memory of microcontrollers is 2-3 orders of
magnitude smaller even than mobile phones. We propose MCUNet, a framework that
jointly designs the efficient neural architecture (TinyNAS) and the lightweight
inference engine (TinyEngine), enabling ImageNet-scale inference on
microcontrollers. TinyNAS adopts a two-stage neural architecture search
approach that first optimizes the search space to fit the resource constraints,
then specializes the network architecture in the optimized search space.
TinyNAS can automatically handle diverse constraints (i.e.device, latency,
energy, memory) under low search costs.TinyNAS is co-designed with TinyEngine,
a memory-efficient inference library to expand the search space and fit a
larger model. TinyEngine adapts the memory scheduling according to the overall
network topology rather than layer-wise optimization, reducing the memory usage
by 4.8x, and accelerating the inference by 1.7-3.3x compared to TF-Lite Micro
and CMSIS-NN. MCUNet is the first to achieves &gt;70% ImageNet top1 accuracy on an
off-the-shelf commercial microcontroller, using 3.5x less SRAM and 5.7x less
Flash compared to quantized MobileNetV2 and ResNet-18. On visual&amp;audio wake
words tasks, MCUNet achieves state-of-the-art accuracy and runs 2.4-3.4x faster
than MobileNetV2 and ProxylessNAS-based solutions with 3.7-4.1x smaller peak
SRAM. Our study suggests that the era of always-on tiny machine learning on IoT
devices has arrived. Code and models can be found here: https://tinyml.mit.edu.
</p>
<a href="http://arxiv.org/abs/2007.10319" target="_blank">arXiv:2007.10319</a> [<a href="http://arxiv.org/pdf/2007.10319" target="_blank">pdf</a>]

<h2>Perceptron Synthesis Network: Rethinking the Action Scale Variances in Videos. (arXiv:2007.11460v2 [cs.CV] UPDATED)</h2>
<h3>Yuan Tian, Guangtao Zhai, Zhiyong Gao</h3>
<p>Video action recognition has been partially addressed by the CNNs stacking of
fixed-size 3D kernels. However, these methods may under-perform for only
capturing rigid spatial-temporal patterns in single-scale spaces, while
neglecting the scale variances across different action primitives. To overcome
this limitation, we propose to learn the optimal-scale kernels from the data.
More specifically, an \textit{action perceptron synthesizer} is proposed to
generate the kernels from a bag of fixed-size kernels that are interacted by
dense routing paths. To guarantee the interaction richness and the information
capacity of the paths, we design the novel \textit{optimized feature fusion
layer}. This layer establishes a principled universal paradigm that suffices to
cover most of the current feature fusion techniques (e.g., channel shuffling,
and channel dropout) for the first time. By inserting the \textit{synthesizer},
our method can easily adapt the traditional 2D CNNs to the video understanding
tasks such as action recognition with marginal additional computation cost. The
proposed method is thoroughly evaluated over several challenging datasets
(i.e., Somehting-to-Somthing, Kinetics and Diving48) that highly require
temporal reasoning or appearance discriminating, achieving new state-of-the-art
results. Particularly, our low-resolution model outperforms the recent strong
baseline methods, i.e., TSM and GST, with less than 30\% of their computation
cost.
</p>
<a href="http://arxiv.org/abs/2007.11460" target="_blank">arXiv:2007.11460</a> [<a href="http://arxiv.org/pdf/2007.11460" target="_blank">pdf</a>]

<h2>A Dual Iterative Refinement Method for Non-rigid Shape Matching. (arXiv:2007.13049v2 [cs.CV] UPDATED)</h2>
<h3>Rui Xiang, Rongjie Lai, Hongkai Zhao</h3>
<p>In this work, a simple and efficient dual iterative refinement (DIR) method
is proposed for dense correspondence between two nearly isometric shapes. The
key idea is to use dual information, such as spatial and spectral, or local and
global features, in a complementary and effective way, and extract more
accurate information from current iteration to use for the next iteration. In
each DIR iteration, starting from current correspondence, a zoom-in process at
each point is used to select well matched anchor pairs by a local mapping
distortion criterion. These selected anchor pairs are then used to align
spectral features (or other appropriate global features) whose dimension
adaptively matches the capacity of the selected anchor pairs. Thanks to the
effective combination of complementary information in a data-adaptive way, DIR
is not only efficient but also robust to render accurate results within a few
iterations. By choosing appropriate dual features, DIR has the flexibility to
handle patch and partial matching as well. Extensive experiments on various
data sets demonstrate the superiority of DIR over other state-of-the-art
methods in terms of both accuracy and efficiency.
</p>
<a href="http://arxiv.org/abs/2007.13049" target="_blank">arXiv:2007.13049</a> [<a href="http://arxiv.org/pdf/2007.13049" target="_blank">pdf</a>]

<h2>PixL2R: Guiding Reinforcement Learning Using Natural Language by Mapping Pixels to Rewards. (arXiv:2007.15543v2 [cs.LG] UPDATED)</h2>
<h3>Prasoon Goyal, Scott Niekum, Raymond J. Mooney</h3>
<p>Reinforcement learning (RL), particularly in sparse reward settings, often
requires prohibitively large numbers of interactions with the environment,
thereby limiting its applicability to complex problems. To address this,
several prior approaches have used natural language to guide the agent's
exploration. However, these approaches typically operate on structured
representations of the environment, and/or assume some structure in the natural
language commands. In this work, we propose a model that directly maps pixels
to rewards, given a free-form natural language description of the task, which
can then be used for policy learning. Our experiments on the Meta-World robot
manipulation domain show that language-based rewards significantly improves the
sample efficiency of policy learning, both in sparse and dense reward settings.
</p>
<a href="http://arxiv.org/abs/2007.15543" target="_blank">arXiv:2007.15543</a> [<a href="http://arxiv.org/pdf/2007.15543" target="_blank">pdf</a>]

<h2>IV-SLAM: Introspective Vision for Simultaneous Localization and Mapping. (arXiv:2008.02760v2 [cs.CV] UPDATED)</h2>
<h3>Sadegh Rabiee, Joydeep Biswas</h3>
<p>Existing solutions to visual simultaneous localization and mapping (V-SLAM)
assume that errors in feature extraction and matching are independent and
identically distributed (i.i.d), but this assumption is known to not be true --
features extracted from low-contrast regions of images exhibit wider error
distributions than features from sharp corners. Furthermore, V-SLAM algorithms
are prone to catastrophic tracking failures when sensed images include
challenging conditions such as specular reflections, lens flare, or shadows of
dynamic objects. To address such failures, previous work has focused on
building more robust visual frontends, to filter out challenging features. In
this paper, we present introspective vision for SLAM (IV-SLAM), a fundamentally
different approach for addressing these challenges. IV-SLAM explicitly models
the noise process of reprojection errors from visual features to be
context-dependent, and hence non-i.i.d. We introduce an autonomously supervised
approach for IV-SLAM to collect training data to learn such a context-aware
noise model. Using this learned noise model, IV-SLAM guides feature extraction
to select more features from parts of the image that are likely to result in
lower noise, and further incorporate the learned noise model into the joint
maximum likelihood estimation, thus making it robust to the aforementioned
types of errors. We present empirical results to demonstrate that IV-SLAM 1) is
able to accurately predict sources of error in input images, 2) reduces
tracking error compared to V-SLAM, and 3) increases the mean distance between
tracking failures by more than 70% on challenging real robot data compared to
V-SLAM.
</p>
<a href="http://arxiv.org/abs/2008.02760" target="_blank">arXiv:2008.02760</a> [<a href="http://arxiv.org/pdf/2008.02760" target="_blank">pdf</a>]

<h2>FATNN: Fast and Accurate Ternary Neural Networks. (arXiv:2008.05101v2 [cs.LG] UPDATED)</h2>
<h3>Peng Chen, Bohan Zhuang, Chunhua Shen</h3>
<p>Ternary Neural Networks (TNNs) have received much attention due to being
potentially orders of magnitude faster in inference, as well as more power
efficient, than full-precision counterparts. However, 2 bits are required to
encode the ternary representation with only 3 quantization levels leveraged. As
a result, conventional TNNs have similar memory consumption and speed compared
with the standard 2-bit models, but have worse representational capability.
Moreover, there is still a significant gap in accuracy between TNNs and
full-precision networks, hampering their deployment to real applications. To
tackle these two challenges, in this work, we first show that, under some mild
constraints, computational complexity of the ternary inner product can be
reduced by a factor of 2. Second, to mitigate the performance gap, we
elaborately design an implementation-dependent ternary quantization algorithm.
The proposed framework is termed Fast and Accurate Ternary Neural Networks
(FATNN). Experiments on image classification demonstrate that our FATNN
surpasses the state-of-the-arts by a significant margin in accuracy. More
importantly, speedup evaluation compared with various precisions is analyzed on
several platforms, which serves as a strong benchmark for further research.
</p>
<a href="http://arxiv.org/abs/2008.05101" target="_blank">arXiv:2008.05101</a> [<a href="http://arxiv.org/pdf/2008.05101" target="_blank">pdf</a>]

<h2>A Note on Rich Incomplete Argumentation Frameworks. (arXiv:2009.04869v3 [cs.AI] UPDATED)</h2>
<h3>Jean-Guy Mailly</h3>
<p>Recently, qualitative uncertainty in abstract argumentation has received much
attention. The first works on this topic introduced uncertainty about the
presence of attacks, then about the presence of arguments, and finally combined
both kinds of uncertainty. This results in the Incomplete Argumentation
Framework (IAFs). But another kind of uncertainty was introduced in the context
of Control Argumentation Frameworks (CAFs): it consists in a conflict relation
with uncertain orientation, i.e. we are sure that there is an attack between
two arguments, but the actual direction of the attack is unknown. Here, we
formally define Rich IAFs, that combine the three different kinds of
uncertainty that were previously introduced in IAFs and CAFs. We show that this
new model, although strictly more expressive than IAFs, does not suffer from a
blow up of computational complexity. Also, the existing computational approach
based on SAT can be easily adapted to the new framework.
</p>
<a href="http://arxiv.org/abs/2009.04869" target="_blank">arXiv:2009.04869</a> [<a href="http://arxiv.org/pdf/2009.04869" target="_blank">pdf</a>]

<h2>Federated Generalized Bayesian Learning via Distributed Stein Variational Gradient Descent. (arXiv:2009.06419v4 [cs.LG] UPDATED)</h2>
<h3>Rahif Kassab, Osvaldo Simeone</h3>
<p>This paper introduces Distributed Stein Variational Gradient Descent (DSVGD),
a non-parametric generalized Bayesian inference framework for federated
learning. DSVGD maintains a number of non-random and interacting particles at a
central server to represent the current iterate of the model global posterior.
The particles are iteratively downloaded and updated by one of the agents with
the end goal of minimizing the global free energy. By varying the number of
particles, DSVGD enables a flexible trade-off between per-iteration
communication load and number of communication rounds. DSVGD is shown to
compare favorably to benchmark frequentist and Bayesian federated learning
strategies, also scheduling a single device per iteration, in terms of accuracy
and scalability with respect to the number of agents, while also providing
well-calibrated, and hence trustworthy, predictions.
</p>
<a href="http://arxiv.org/abs/2009.06419" target="_blank">arXiv:2009.06419</a> [<a href="http://arxiv.org/pdf/2009.06419" target="_blank">pdf</a>]

<h2>Mini-DDSM: Mammography-based Automatic Age Estimation. (arXiv:2010.00494v2 [cs.CV] UPDATED)</h2>
<h3>Charitha Dissanayake Lekamlage, Fabia Afzal, Erik Westerberg, Abbas Cheddad</h3>
<p>Age estimation has attracted attention for its various medical applications.
There are many studies on human age estimation from biomedical images. However,
there is no research done on mammograms for age estimation, as far as we know.
The purpose of this study is to devise an AI-based model for estimating age
from mammogram images. Due to lack of public mammography data sets that have
the age attribute, we resort to using a web crawler to download thumbnail
mammographic images and their age fields from the public data set; the Digital
Database for Screening Mammography. The original images in this data set
unfortunately can only be retrieved by a software which is broken.
Subsequently, we extracted deep learning features from the collected data set,
by which we built a model using Random Forests regressor to estimate the age
automatically. The performance assessment was measured using the mean absolute
error values. The average error value out of 10 tests on random selection of
samples was around 8 years. In this paper, we show the merits of this approach
to fill up missing age values. We ran logistic and linear regression models on
another independent data set to further validate the advantage of our proposed
work. This paper also introduces the free-access Mini-DDSM data set.
</p>
<a href="http://arxiv.org/abs/2010.00494" target="_blank">arXiv:2010.00494</a> [<a href="http://arxiv.org/pdf/2010.00494" target="_blank">pdf</a>]

<h2>$f$-GAIL: Learning $f$-Divergence for Generative Adversarial Imitation Learning. (arXiv:2010.01207v2 [cs.LG] UPDATED)</h2>
<h3>Xin Zhang, Yanhua Li, Ziming Zhang, Zhi-Li Zhang</h3>
<p>Imitation learning (IL) aims to learn a policy from expert demonstrations
that minimizes the discrepancy between the learner and expert behaviors.
Various imitation learning algorithms have been proposed with different
pre-determined divergences to quantify the discrepancy. This naturally gives
rise to the following question: Given a set of expert demonstrations, which
divergence can recover the expert policy more accurately with higher data
efficiency? In this work, we propose $f$-GAIL, a new generative adversarial
imitation learning (GAIL) model, that automatically learns a discrepancy
measure from the $f$-divergence family as well as a policy capable of producing
expert-like behaviors. Compared with IL baselines with various predefined
divergence measures, $f$-GAIL learns better policies with higher data
efficiency in six physics-based control tasks.
</p>
<a href="http://arxiv.org/abs/2010.01207" target="_blank">arXiv:2010.01207</a> [<a href="http://arxiv.org/pdf/2010.01207" target="_blank">pdf</a>]

<h2>Uncertainty-Aware Multi-Modal Ensembling for Severity Prediction of Alzheimer's Dementia. (arXiv:2010.01440v2 [cs.LG] UPDATED)</h2>
<h3>Utkarsh Sarawgi, Wazeer Zulfikar, Rishab Khincha, Pattie Maes</h3>
<p>Reliability in Neural Networks (NNs) is crucial in safety-critical
applications like healthcare, and uncertainty estimation is a widely researched
method to highlight the confidence of NNs in deployment. In this work, we
propose an uncertainty-aware boosting technique for multi-modal ensembling to
predict Alzheimer's Dementia Severity. The propagation of uncertainty across
acoustic, cognitive, and linguistic features produces an ensemble system robust
to heteroscedasticity in the data. Weighing the different modalities based on
the uncertainty estimates, we experiment on the benchmark ADReSS dataset, a
subject-independent and balanced dataset, to show that our method outperforms
the state-of-the-art methods while also reducing the overall entropy of the
system. This work aims to encourage fair and aware models. The source code is
available at https://github.com/wazeerzulfikar/alzheimers-dementia
</p>
<a href="http://arxiv.org/abs/2010.01440" target="_blank">arXiv:2010.01440</a> [<a href="http://arxiv.org/pdf/2010.01440" target="_blank">pdf</a>]

<h2>Generalized Few-Shot Semantic Segmentation. (arXiv:2010.05210v2 [cs.CV] UPDATED)</h2>
<h3>Zhuotao Tian, Xin Lai, Li Jiang, Michelle Shu, Hengshuang Zhao, Jiaya Jia</h3>
<p>Training semantic segmentation models requires a large amount of finely
annotated data, making it hard to quickly adapt to novel classes not satisfying
this condition. Few-Shot Segmentation (FS-Seg) tackles this problem with many
constraints. In this paper, we introduce a new benchmark, called Generalized
Few-Shot Semantic Segmentation (GFS-Seg), to analyze the generalization ability
of segmentation models to simultaneously recognize novel categories with very
few examples as well as base categories with sufficient examples. Previous
state-of-the-art FS-Seg methods fall short in GFS-Seg and the performance
discrepancy mainly comes from the constrained training setting of FS-Seg. To
make GFS-Seg tractable, we set up a GFS-Seg baseline that achieves decent
performance without structural change on the original model. Then, as context
is the key for boosting performance on semantic segmentation, we propose the
Context-Aware Prototype Learning (CAPL) that significantly improves performance
by leveraging the contextual information to update class prototypes with
aligned features. Extensive experiments on Pascal-VOC and COCO manifest the
effectiveness of CAPL, and CAPL also generalizes well to FS-Seg.
</p>
<a href="http://arxiv.org/abs/2010.05210" target="_blank">arXiv:2010.05210</a> [<a href="http://arxiv.org/pdf/2010.05210" target="_blank">pdf</a>]

<h2>Multi-class segmentation under severe class imbalance: A case study in roof damage assessment. (arXiv:2010.07151v2 [cs.CV] UPDATED)</h2>
<h3>Jean-Baptiste Boin, Nat Roth, Jigar Doshi, Pablo Llueca, Nicolas Borensztein</h3>
<p>The task of roof damage classification and segmentation from overhead imagery
presents unique challenges. In this work we choose to address the challenge
posed due to strong class imbalance. We propose four distinct techniques that
aim at mitigating this problem. Through a new scheme that feeds the data to the
network by oversampling the minority classes, and three other network
architectural improvements, we manage to boost the macro-averaged F1-score of a
model by 39.9 percentage points, thus achieving improved segmentation
performance, especially on the minority classes.
</p>
<a href="http://arxiv.org/abs/2010.07151" target="_blank">arXiv:2010.07151</a> [<a href="http://arxiv.org/pdf/2010.07151" target="_blank">pdf</a>]

<h2>Reducing the Teacher-Student Gap via Spherical Knowledge Disitllation. (arXiv:2010.07485v3 [cs.LG] UPDATED)</h2>
<h3>Jia Guo, Minghao Chen, Yao Hu, Chen Zhu, Xiaofei He, Deng Cai</h3>
<p>Knowledge distillation aims at obtaining a compact and effective model by
learning the mapping function from a much larger one. Due to the limited
capacity of the student, the student would underfit the teacher. Therefore,
student performance would unexpectedly drop when distilling from an oversized
teacher, termed the capacity gap problem. We investigate this problem by study
the gap of confidence between teacher and student. We find that the magnitude
of confidence is not necessary for knowledge distillation and could harm the
student performance if the student are forced to learn confidence. We propose
Spherical Knowledge Distillation to eliminate this gap explicitly, which eases
the underfitting problem. We find this novel knowledge representation can
improve compact models with much larger teachers and is robust to temperature.
We conducted experiments on both CIFAR100 and ImageNet, and achieve significant
improvement. Specifically, we train ResNet18 to 73.0 accuracy, which is a
substantial improvement over previous SOTA and is on par with resnet34 almost
twice the student size.
</p>
<a href="http://arxiv.org/abs/2010.07485" target="_blank">arXiv:2010.07485</a> [<a href="http://arxiv.org/pdf/2010.07485" target="_blank">pdf</a>]

<h2>Differentiable Divergences Between Time Series. (arXiv:2010.08354v2 [cs.LG] UPDATED)</h2>
<h3>Mathieu Blondel, Arthur Mensch, Jean-Philippe Vert</h3>
<p>Computing the discrepancy between time series of variable sizes is
notoriously challenging. While dynamic time warping (DTW) is popularly used for
this purpose, it is not differentiable everywhere and is known to lead to bad
local optima when used as a "loss". Soft-DTW addresses these issues, but it is
not a positive definite divergence: due to the bias introduced by entropic
regularization, it can be negative and it is not minimized when the time series
are equal. We propose in this paper a new divergence, dubbed soft-DTW
divergence, which aims to correct these issues. We study its properties; in
particular, under conditions on the ground cost, we show that it is
non-negative and minimized when the time series are equal. We also propose a
new "sharp" variant by further removing entropic bias. We showcase our
divergences on time series averaging and demonstrate significant accuracy
improvements compared to both DTW and soft-DTW on 84 time series classification
datasets.
</p>
<a href="http://arxiv.org/abs/2010.08354" target="_blank">arXiv:2010.08354</a> [<a href="http://arxiv.org/pdf/2010.08354" target="_blank">pdf</a>]

<h2>Tracklets Predicting Based Adaptive Graph Tracking. (arXiv:2010.09015v3 [cs.CV] UPDATED)</h2>
<h3>Chaobing Shan, Chunbo Wei, Bing Deng, Jianqiang Huang, Xian-Sheng Hua, Xiaoliang Cheng, Kewei Liang</h3>
<p>Most of the existing tracking methods link the detected boxes to the
tracklets using a linear combination of feature cosine distances and box
overlap. But the problem of inconsistent features of an object in two different
frames still exists. In addition, when extracting features, only appearance
information is utilized, neither the location relationship nor the information
of the tracklets is considered. We present an accurate and end-to-end learning
framework for multi-object tracking, namely \textbf{TPAGT}. It re-extracts the
features of the tracklets in the current frame based on motion predicting,
which is the key to solve the problem of features inconsistent. The adaptive
graph neural network in TPAGT is adopted to fuse locations, appearance, and
historical information, and plays an important role in distinguishing different
objects. In the training phase, we propose the balanced MSE LOSS to
successfully overcome the unbalanced samples. Experiments show that our method
reaches state-of-the-art performance. It achieves 76.5\% MOTA on the MOT16
challenge and 76.2\% MOTA on the MOT17 challenge.
</p>
<a href="http://arxiv.org/abs/2010.09015" target="_blank">arXiv:2010.09015</a> [<a href="http://arxiv.org/pdf/2010.09015" target="_blank">pdf</a>]

<h2>Robust Constrained Reinforcement Learning for Continuous Control with Model Misspecification. (arXiv:2010.10644v2 [cs.LG] UPDATED)</h2>
<h3>Daniel J. Mankowitz, Dan A. Calian, Rae Jeong, Cosmin Paduraru, Nicolas Heess, Sumanth Dathathri, Martin Riedmiller, Timothy Mann</h3>
<p>Many real-world physical control systems are required to satisfy constraints
upon deployment. Furthermore, real-world systems are often subject to effects
such as non-stationarity, wear-and-tear, uncalibrated sensors and so on. Such
effects effectively perturb the system dynamics and can cause a policy trained
successfully in one domain to perform poorly when deployed to a perturbed
version of the same domain. This can affect a policy's ability to maximize
future rewards as well as the extent to which it satisfies constraints. We
refer to this as constrained model misspecification. We present an algorithm
with theoretical guarantees that mitigates this form of misspecification, and
showcase its performance in multiple Mujoco tasks from the Real World
Reinforcement Learning (RWRL) suite.
</p>
<a href="http://arxiv.org/abs/2010.10644" target="_blank">arXiv:2010.10644</a> [<a href="http://arxiv.org/pdf/2010.10644" target="_blank">pdf</a>]

<h2>Rethinking the competition between detection and ReID in Multi-Object Tracking. (arXiv:2010.12138v2 [cs.CV] UPDATED)</h2>
<h3>Chao Liang, Zhipeng Zhang, Yi Lu, Xue Zhou, Bing Li, Xiyong Ye, Jianxiao Zou</h3>
<p>Due to balanced accuracy and speed, joint learning detection and ReID-based
one-shot models have drawn great attention in multi-object tracking(MOT).
However, the differences between the above two tasks in the one-shot tracking
paradigm are unconsciously overlooked, leading to inferior performance than the
two-stage methods. In this paper, we dissect the reasoning process of the
aforementioned two tasks. Our analysis reveals that the competition of them
inevitably hurts the learning of task-dependent representations, which further
impedes the tracking performance. To remedy this issue, we propose a novel
cross-correlation network that can effectively impel the separate branches to
learn task-dependent representations. Furthermore, we introduce a scale-aware
attention network that learns discriminative embeddings to improve the ReID
capability. We integrate the delicately designed networks into a one-shot
online MOT system, dubbed CSTrack. Without bells and whistles, our model
achieves new state-of-the-art performances on MOT16 and MOT17. Our code is
released at https://github.com/JudasDie/SOTS.
</p>
<a href="http://arxiv.org/abs/2010.12138" target="_blank">arXiv:2010.12138</a> [<a href="http://arxiv.org/pdf/2010.12138" target="_blank">pdf</a>]

<h2>Unsupervised Domain Adaptation without Source Data by Casting a BAIT. (arXiv:2010.12427v3 [cs.CV] UPDATED)</h2>
<h3>Shiqi Yang, Yaxing Wang, Joost van de Weijer, Luis Herranz, Shangling Jui</h3>
<p>Unsupervised domain adaptation (UDA) aims to transfer the knowledge learned
from a labeled source domain to an unlabeled target domain. Existing UDA
methods require access to source data during adaptation, which may not be
feasible in some real-world applications. In this paper, we address the
source-free unsupervised domain adaptation (SFUDA) problem, where only the
source model is available during the adaptation. We propose a method named BAIT
to address SFUDA. Specifically, given only the source model, with the source
classifier head fixed, we introduce a new learnable classifier. When adapting
to the target domain, class prototypes of the new added classifier will act as
a bait. They will first approach the target features which deviate from
prototypes of the source classifier due to domain shift. Then those target
features are pulled towards the corresponding prototypes of the source
classifier, thus achieving feature alignment with the source classifier in the
absence of source data. Experimental results show that the proposed method
achieves state-of-the-art performance on several benchmark datasets compared
with existing UDA and SFUDA methods.
</p>
<a href="http://arxiv.org/abs/2010.12427" target="_blank">arXiv:2010.12427</a> [<a href="http://arxiv.org/pdf/2010.12427" target="_blank">pdf</a>]

<h2>Cross-directional Feature Fusion Network for Building Damage Assessment from Satellite Imagery. (arXiv:2010.14014v2 [cs.CV] UPDATED)</h2>
<h3>Yu Shen, Sijie Zhu, Taojiannan Yang, Chen Chen</h3>
<p>Fast and effective responses are required when a natural disaster (e.g.,
earthquake, hurricane, etc.) strikes. Building damage assessment from satellite
imagery is critical before an effective response is conducted. High-resolution
satellite images provide rich information with pre- and post-disaster scenes
for analysis. However, most existing works simply use pre- and post-disaster
images as input without considering their correlations. In this paper, we
propose a novel cross-directional fusion strategy to better explore the
correlations between pre- and post-disaster images. Moreover, the data
augmentation method CutMix is exploited to tackle the challenge of hard
classes. The proposed method achieves state-of-the-art performance on a
large-scale building damage assessment dataset -- xBD.
</p>
<a href="http://arxiv.org/abs/2010.14014" target="_blank">arXiv:2010.14014</a> [<a href="http://arxiv.org/pdf/2010.14014" target="_blank">pdf</a>]

<h2>ORBBuf: A Robust Buffering Method for Remote Visual SLAM. (arXiv:2010.14861v2 [cs.RO] UPDATED)</h2>
<h3>Yu-Ping Wang, Zi-Xin Zou, Cong Wang, Yue-Jiang Dong, Lei Qiao, Dinesh Manocha</h3>
<p>The data loss caused by unreliable network seriously impacts the results of
remote visual SLAM systems. From our experiment, a loss of less than 1 second
of data can lead to the failure of visual SLAM algorithms. We present a novel
buffering method, ORBBuf, to reduce the impact of data loss on remote visual
SLAM systems. We model the buffering problem as an optimization problem by
introducing a similarity metric between frames, and use an efficient
greedy-like algorithm that drops the frame that results in the least loss to
the quality of SLAM results. We implement our ORBBuf method on ROS, a widely
used middleware framework. Through an extensive evaluation on real-world
scenarios and tens of gigabytes of datasets, we demonstrate that our ORBBuf
method can be applied to different state-estimation algorithms (DSO and
VINSFusion), different sensor data (both monocular images and stereo images),
different scenes (both indoor and outdoor), and different network environments
(both WiFi networks and 4G networks). Our experimental results indicate that
the network losses indeed affect the SLAM results, and our ORBBuf method can
reduce the RMSE up to 50 times comparing with the Drop-Oldest and Random
buffering methods.
</p>
<a href="http://arxiv.org/abs/2010.14861" target="_blank">arXiv:2010.14861</a> [<a href="http://arxiv.org/pdf/2010.14861" target="_blank">pdf</a>]

<h2>LIFI: Towards Linguistically Informed Frame Interpolation. (arXiv:2010.16078v4 [cs.CV] UPDATED)</h2>
<h3>Aradhya Neeraj Mathur, Devansh Batra, Yaman Kumar, Rajiv Ratn Shah, Roger Zimmermann</h3>
<p>In this work, we explore a new problem of frame interpolation for speech
videos. Such content today forms the major form of online communication. We try
to solve this problem by using several deep learning video generation
algorithms to generate the missing frames. We also provide examples where
computer vision models despite showing high performance on conventional
non-linguistic metrics fail to accurately produce faithful interpolation of
speech. With this motivation, we provide a new set of linguistically-informed
metrics specifically targeted to the problem of speech videos interpolation. We
also release several datasets to test computer vision video generation models
of their speech understanding.
</p>
<a href="http://arxiv.org/abs/2010.16078" target="_blank">arXiv:2010.16078</a> [<a href="http://arxiv.org/pdf/2010.16078" target="_blank">pdf</a>]

<h2>Continuous and Diverse Image-to-Image Translation via Signed Attribute Vectors. (arXiv:2011.01215v3 [cs.CV] UPDATED)</h2>
<h3>Qi Mao, Hsin-Ying Lee, Hung-Yu Tseng, Jia-Bin Huang, Siwei Ma, Ming-Hsuan Yang</h3>
<p>Recent image-to-image (I2I) translation algorithms focus on learning the
mapping from a source to a target domain. However, the continuous translation
problem that synthesizes intermediate results between the two domains has not
been well-studied in the literature. Generating a smooth sequence of
intermediate results bridges the gap of two different domains, facilitating the
morphing effect across domains. Existing I2I approaches are limited to either
intra-domain or deterministic inter-domain continuous translation. In this
work, we present an effective signed attribute vector, which enables continuous
translation on diverse mapping paths across various domains. In particular,
utilizing the sign operation to encode the domain information, we introduce a
unified attribute space shared by all domains, thereby allowing the
interpolation on attribute vectors of different domains. To enhance the visual
quality of continuous translation results, we generate a trajectory between two
sign-symmetrical attribute vectors and leverage the domain information of the
interpolated results along the trajectory for adversarial training. We evaluate
the proposed method on a wide range of I2I translation tasks. Both qualitative
and quantitative results demonstrate that the proposed framework generates more
high-quality continuous translation results against the state-of-the-art
methods.
</p>
<a href="http://arxiv.org/abs/2011.01215" target="_blank">arXiv:2011.01215</a> [<a href="http://arxiv.org/pdf/2011.01215" target="_blank">pdf</a>]

<h2>Neural Stochastic Contraction Metrics for Learning-based Robust Control and Estimation. (arXiv:2011.03168v2 [cs.LG] UPDATED)</h2>
<h3>Hiroyasu Tsukamoto, Soon-Jo Chung, Jean-Jacques E. Slotine</h3>
<p>We present Neural Stochastic Contraction Metrics (NSCM), a new design
framework for provably-stable robust control and estimation for a class of
stochastic nonlinear systems. It uses a spectrally-normalized deep neural
network to construct a contraction metric, sampled via simplified convex
optimization in the stochastic setting. Spectral normalization constrains the
state-derivatives of the metric to be Lipschitz continuous, thereby ensuring
exponential boundedness of the mean squared distance of system trajectories
under stochastic disturbances. The NSCM framework allows autonomous agents to
approximate optimal stable control and estimation policies in real-time, and
outperforms existing nonlinear control and estimation techniques including the
state-dependent Riccati equation, iterative LQR, EKF, and the deterministic
neural contraction metric, as illustrated in simulation results.
</p>
<a href="http://arxiv.org/abs/2011.03168" target="_blank">arXiv:2011.03168</a> [<a href="http://arxiv.org/pdf/2011.03168" target="_blank">pdf</a>]

<h2>Learning Neural Event Functions for Ordinary Differential Equations. (arXiv:2011.03902v2 [cs.LG] UPDATED)</h2>
<h3>Ricky T. Q. Chen, Brandon Amos, Maximilian Nickel</h3>
<p>The existing Neural ODE formulation relies on an explicit knowledge of the
termination time. We extend Neural ODEs to implicitly defined termination
criteria modeled by neural event functions, which can be chained together and
differentiated through. Neural Event ODEs are capable of modeling discrete
(instantaneous) changes in a continuous-time system, without prior knowledge of
when these changes should occur or how many such changes should exist. We test
our approach in modeling hybrid discrete- and continuous- systems such as
switching dynamical systems and collision in multi-body systems, and we propose
simulation-based training of point processes with applications in discrete
control.
</p>
<a href="http://arxiv.org/abs/2011.03902" target="_blank">arXiv:2011.03902</a> [<a href="http://arxiv.org/pdf/2011.03902" target="_blank">pdf</a>]

<h2>Neural Spatio-Temporal Point Processes. (arXiv:2011.04583v2 [cs.LG] UPDATED)</h2>
<h3>Ricky T. Q. Chen, Brandon Amos, Maximilian Nickel</h3>
<p>We propose a new class of parameterizations for spatio-temporal point
processes which leverage Neural ODEs as a computational method and enable
flexible, high-fidelity models of discrete events that are localized in
continuous time and space. Central to our approach is a combination of
recurrent continuous-time neural networks with two novel neural architectures,
i.e., Jump and Attentive Continuous-time Normalizing Flows. This approach
allows us to learn complex distributions for both the spatial and temporal
domain and to condition non-trivially on the observed event history. We
validate our models on data sets from a wide variety of contexts such as
seismology, epidemiology, urban mobility, and neuroscience.
</p>
<a href="http://arxiv.org/abs/2011.04583" target="_blank">arXiv:2011.04583</a> [<a href="http://arxiv.org/pdf/2011.04583" target="_blank">pdf</a>]

<h2>Implicit bias of any algorithm: bounding bias via margin. (arXiv:2011.06550v3 [stat.ML] UPDATED)</h2>
<h3>Elvis Dohmatob</h3>
<p>Consider $n$ points $x_1$,\ldots,$x_n$ in finite-dimensional euclidean space,
each having one of two colors. Suppose there exists a separating hyperplane
(identified with its unit normal vector $w)$ for the points, i.e a hyperplane
such that points of same color lie on the same side of the hyperplane. We
measure the quality of such a hyperplane by its margin $\gamma(w)$, defined as
minimum distance between any of the points $x_i$ and the hyperplane. In this
paper, we prove that the margin function $\gamma$ satisfies a nonsmooth
Kurdyka-Lojasiewicz inequality with exponent $1/2$. This result has
far-reaching consequences. For example, let $\gamma^{opt}$ be the maximum
possible margin for the problem and let $w^{opt}$ be the parameter for the
hyperplane which attains this value. Given any other separating hyperplane with
parameter $w$, let $d(w):=\|w-w^{opt}\|$ be the euclidean distance between $w$
and $w^{opt}$, also called the bias of $w$. From the previous KL-inequality, we
deduce that $(\gamma^{opt}-\gamma(w)) / R \le d(w) \le
2\sqrt{(\gamma^{opt}-\gamma(w))/\gamma^{opt}}$, where $R:=\max_i \|x_i\|$ is
the maximum distance of the points $x_i$ from the origin. Consequently, for any
optimization algorithm (gradient-descent or not), the bias of the iterates
converges at least as fast as the square-root of the rate of their convergence
of the margin. Thus, our work provides a generic tool for analyzing the
implicit bias of any algorithm in terms of its margin, in situations where a
specialized analysis might not be available: it is sufficient to establish a
good rate for converge of the margin, a task which is usually much easier.
</p>
<a href="http://arxiv.org/abs/2011.06550" target="_blank">arXiv:2011.06550</a> [<a href="http://arxiv.org/pdf/2011.06550" target="_blank">pdf</a>]

<h2>SHAD3S: A model to Sketch, Shade and Shadow. (arXiv:2011.06822v2 [cs.CV] UPDATED)</h2>
<h3>Raghav B. Venkataramaiyer, Abhishek Joshi, Saisha Narang, Vinay P. Namboodiri</h3>
<p>Hatching is a common method used by artists to accentuate the third dimension
of a sketch, and to illuminate the scene. Our system SHAD3S attempts to compete
with a human at hatching generic three-dimensional (3D) shapes, and also tries
to assist her in a form exploration exercise. The novelty of our approach lies
in the fact that we make no assumptions about the input other than that it
represents a 3D shape, and yet, given a contextual information of illumination
and texture, we synthesise an accurate hatch pattern over the sketch, without
access to 3D or pseudo 3D. In the process, we contribute towards a) a cheap yet
effective method to synthesise a sufficiently large high fidelity dataset,
pertinent to task; b) creating a pipeline with conditional generative
adversarial network (CGAN); and c) creating an interactive utility with GIMP,
that is a tool for artists to engage with automated hatching or a
form-exploration exercise. User evaluation of the tool suggests that the model
performance does generalise satisfactorily over diverse input, both in terms of
style as well as shape. A simple comparison of inception scores suggest that
the generated distribution is as diverse as the ground truth.
</p>
<a href="http://arxiv.org/abs/2011.06822" target="_blank">arXiv:2011.06822</a> [<a href="http://arxiv.org/pdf/2011.06822" target="_blank">pdf</a>]

<h2>Image Animation with Perturbed Masks. (arXiv:2011.06922v2 [cs.CV] UPDATED)</h2>
<h3>Yoav Shalev, Lior Wolf</h3>
<p>We present a novel approach for image-animation of a source image by a
driving video, both depicting the same type of object. We do not assume the
existence of pose models and our method is able to animate arbitrary objects
without knowledge of the object's structure. Furthermore, both the driving
video and the source image are only seen during test-time. Our method is based
on a shared mask generator, which separates the foreground object from its
background, and captures the object's general pose and shape. A mask-refinement
module then replaces, in the mask extracted from the driver image, the identity
of the driver with the identity of the source. Conditioned on the source image,
the transformed mask is then decoded by a multi-scale generator that renders a
realistic image, in which the content of the source frame is animated by the
pose in the driving video. Due to lack of fully supervised data, we train on
the task of reconstructing frames from the same video the source image is taken
from. In order to control {the} source of the identity of the output frame, we
employ during training perturbations that remove the unwanted identity
information. Our method is shown to greatly outperform the state of the art
methods on multiple benchmarks. Our code and samples are available at
https://github.com/itsyoavshalev/Image-Animation-with-Perturbed-Masks
</p>
<a href="http://arxiv.org/abs/2011.06922" target="_blank">arXiv:2011.06922</a> [<a href="http://arxiv.org/pdf/2011.06922" target="_blank">pdf</a>]

<h2>Monitoring and Diagnosability of Perception Systems. (arXiv:2011.07010v3 [cs.RO] UPDATED)</h2>
<h3>Pasquale Antonante, David I. Spivak, Luca Carlone</h3>
<p>Perception is a critical component of high-integrity applications of robotics
and autonomous systems, such as self-driving vehicles. In these applications,
failure of perception systems may put human life at risk, and a broad adoption
of these technologies requires the development of methodologies to guarantee
and monitor safe operation. Despite the paramount importance of perception
systems, currently there is no formal approach for system-level monitoring. In
this work, we propose a mathematical model for runtime monitoring and fault
detection and identification in perception systems. Towards this goal, we draw
connections with the literature on diagnosability in multiprocessor systems,
and generalize it to account for modules with heterogeneous outputs that
interact over time. The resulting temporal diagnostic graphs (i) provide a
framework to reason over the consistency of perception outputs -- across
modules and over time -- thus enabling fault detection, (ii) allow us to
establish formal guarantees on the maximum number of faults that can be
uniquely identified in a given perception system, and (iii) enable the design
of efficient algorithms for fault identification. We demonstrate our monitoring
system, dubbed PerSyS, in realistic simulations using the LGSVL self-driving
simulator and the Apollo Auto autonomy software stack, and show that PerSyS is
able to detect failures in challenging scenarios (including scenarios that have
caused self-driving car accidents in recent years), and is able to correctly
identify faults while entailing a minimal computation overhead (&lt; 5ms on a
single-core CPU).
</p>
<a href="http://arxiv.org/abs/2011.07010" target="_blank">arXiv:2011.07010</a> [<a href="http://arxiv.org/pdf/2011.07010" target="_blank">pdf</a>]

<h2>A New Similarity Space Tailored for Supervised Deep Metric Learning. (arXiv:2011.08325v2 [cs.CV] UPDATED)</h2>
<h3>Pedro H. Barros, Fabiane Queiroz, Flavio Figueredo, Jefersson A. dos Santos, Heitor S. Ramos</h3>
<p>We propose a novel deep metric learning method. Differently from many works
on this area, we defined a novel latent space obtained through an autoencoder.
The new space, namely S-space, is divided into different regions that describe
the positions where pairs of objects are similar/dissimilar. We locate makers
to identify these regions. We estimate the similarities between objects through
a kernel-based t-student distribution to measure the markers' distance and the
new data representation. In our approach, we simultaneously estimate the
markers' position in the S-space and represent the objects in the same space.
Moreover, we propose a new regularization function to avoid similar markers to
collapse altogether. We present evidences that our proposal can represent
complex spaces, for instance, when groups of similar objects are located in
disjoint regions. We compare our proposal to 9 different distance metric
learning approaches (four of them are based on deep-learning) on 28 real-world
heterogeneous datasets. According to the four quantitative metrics used, our
method overcomes all the nine strategies from the literature.
</p>
<a href="http://arxiv.org/abs/2011.08325" target="_blank">arXiv:2011.08325</a> [<a href="http://arxiv.org/pdf/2011.08325" target="_blank">pdf</a>]

<h2>Towards Improved and Interpretable Deep Metric Learning via Attentive Grouping. (arXiv:2011.08877v2 [cs.CV] UPDATED)</h2>
<h3>Xinyi Xu, Zhengyang Wang, Cheng Deng, Hao Yuan, Shuiwang Ji</h3>
<p>Grouping has been commonly used in deep metric learning for computing diverse
features. However, current methods are prone to overfitting and lack
interpretability. In this work, we propose an improved and interpretable
grouping method to be integrated flexibly with any metric learning framework.
Our method is based on the attention mechanism with a learnable query for each
group. The query is fully trainable and can capture group-specific information
when combined with the diversity loss. An appealing property of our method is
that it naturally lends itself interpretability. The attention scores between
the learnable query and each spatial position can be interpreted as the
importance of that position. We formally show that our proposed grouping method
is invariant to spatial permutations of features. When used as a module in
convolutional neural networks, our method leads to translational invariance. We
conduct comprehensive experiments to evaluate our method. Our quantitative
results indicate that the proposed method outperforms prior methods
consistently and significantly across different datasets, evaluation metrics,
base models, and loss functions. For the first time to the best of our
knowledge, our interpretation results clearly demonstrate that the proposed
method enables the learning of distinct and diverse features across groups.
</p>
<a href="http://arxiv.org/abs/2011.08877" target="_blank">arXiv:2011.08877</a> [<a href="http://arxiv.org/pdf/2011.08877" target="_blank">pdf</a>]

<h2>On the Relationship Between KR Approaches for Explainable Planning. (arXiv:2011.09006v2 [cs.AI] UPDATED)</h2>
<h3>Stylianos Loukas Vasileiou, William Yeoh, Tran Cao Son</h3>
<p>In this paper, we build upon notions from knowledge representation and
reasoning (KR) to expand a preliminary logic-based framework that characterizes
the model reconciliation problem for explainable planning. We also provide a
detailed exposition on the relationship between similar KR techniques, such as
abductive explanations and belief change, and their applicability to
explainable planning.
</p>
<a href="http://arxiv.org/abs/2011.09006" target="_blank">arXiv:2011.09006</a> [<a href="http://arxiv.org/pdf/2011.09006" target="_blank">pdf</a>]

<h2>Multigrid-in-Channels Neural Network Architectures. (arXiv:2011.09128v2 [cs.CV] UPDATED)</h2>
<h3>Moshe Eliasof, Jonathan Ephrath, Lars Ruthotto, Eran Treister</h3>
<p>We present a multigrid-in-channels (MGIC) approach that tackles the quadratic
growth of the number of parameters with respect to the number of channels in
standard convolutional neural networks (CNNs). It has been shown that there is
a redundancy in standard CNNs, as networks with light or sparse convolution
operators yield similar performance to full networks. However, the number of
parameters in the former networks also scales quadratically in width, while in
the latter case, the parameters typically have random sparsity patterns,
hampering hardware efficiency. Our approach for building CNN architectures
scales linearly with respect to the network's width while retaining full
coupling of the channels as in standard CNNs. To this end, we replace each
convolution block with its MGIC block utilizing a hierarchy of lightweight
convolutions. Our extensive experiments on image classification, segmentation,
and point cloud classification show that applying this strategy to different
architectures like ResNet and MobileNetV3 considerably reduces the number of
parameters while obtaining similar or better accuracy. For example, we obtain
76.1% top-1 accuracy on ImageNet with a lightweight network with similar
parameters and FLOPs to MobileNetV3.
</p>
<a href="http://arxiv.org/abs/2011.09128" target="_blank">arXiv:2011.09128</a> [<a href="http://arxiv.org/pdf/2011.09128" target="_blank">pdf</a>]

<h2>Semantic Scene Completion using Local Deep Implicit Functions on LiDAR Data. (arXiv:2011.09141v2 [cs.CV] UPDATED)</h2>
<h3>Christoph B. Rist, David Emmerichs, Markus Enzweiler, Dariu M. Gavrila</h3>
<p>Semantic scene completion is the task of jointly estimating 3D geometry and
semantics of objects and surfaces within a given extent. This is a particularly
challenging task on real-world data that is sparse and occluded. We propose a
scene segmentation network based on local Deep Implicit Functions as a novel
learning-based method for scene completion. Unlike previous work on scene
completion, our method produces a continuous scene representation that is not
based on voxelization. We encode raw point clouds into a latent space locally
and at multiple spatial resolutions. A global scene completion function is
subsequently assembled from the localized function patches. We show that this
continuous representation is suitable to encode geometric and semantic
properties of extensive outdoor scenes without the need for spatial
discretization (thus avoiding the trade-off between level of scene detail and
the scene extent that can be covered).

We train and evaluate our method on semantically annotated LiDAR scans from
the Semantic KITTI dataset. Our experiments verify that our method generates a
powerful representation that can be decoded into a dense 3D description of a
given scene. The performance of our method surpasses the state of the art on
the Semantic KITTI Scene Completion Benchmark in terms of both geometric and
semantic completion Intersection-over-Union (IoU).
</p>
<a href="http://arxiv.org/abs/2011.09141" target="_blank">arXiv:2011.09141</a> [<a href="http://arxiv.org/pdf/2011.09141" target="_blank">pdf</a>]

<h2>Inverse Reinforcement Learning via Matching of Optimality Profiles. (arXiv:2011.09264v2 [cs.LG] UPDATED)</h2>
<h3>Luis Haug, Ivan Ovinnikov, Eugene Bykovets</h3>
<p>The goal of inverse reinforcement learning (IRL) is to infer a reward
function that explains the behavior of an agent performing a task. The
assumption that most approaches make is that the demonstrated behavior is
near-optimal. In many real-world scenarios, however, examples of truly optimal
behavior are scarce, and it is desirable to effectively leverage sets of
demonstrations of suboptimal or heterogeneous performance, which are easier to
obtain. We propose an algorithm that learns a reward function from such
demonstrations together with a weak supervision signal in the form of a
distribution over rewards collected during the demonstrations (or, more
generally, a distribution over cumulative discounted future rewards). We view
such distributions, which we also refer to as optimality profiles, as summaries
of the degree of optimality of the demonstrations that may, for example,
reflect the opinion of a human expert. Given an optimality profile and a small
amount of additional supervision, our algorithm fits a reward function, modeled
as a neural network, by essentially minimizing the Wasserstein distance between
the corresponding induced distribution and the optimality profile. We show that
our method is capable of learning reward functions such that policies trained
to optimize them outperform the demonstrations used for fitting the reward
functions.
</p>
<a href="http://arxiv.org/abs/2011.09264" target="_blank">arXiv:2011.09264</a> [<a href="http://arxiv.org/pdf/2011.09264" target="_blank">pdf</a>]

<h2>Self-Gradient Networks. (arXiv:2011.09364v2 [cs.LG] UPDATED)</h2>
<h3>Hossein Aboutalebi, Mohammad Javad Shafiee Alexander Wong</h3>
<p>The incredible effectiveness of adversarial attacks on fooling deep neural
networks poses a tremendous hurdle in the widespread adoption of deep learning
in safety and security-critical domains. While adversarial defense mechanisms
have been proposed since the discovery of the adversarial vulnerability issue
of deep neural networks, there is a long path to fully understand and address
this issue. In this study, we hypothesize that part of the reason for the
incredible effectiveness of adversarial attacks is their ability to implicitly
tap into and exploit the gradient flow of a deep neural network. This innate
ability to exploit gradient flow makes defending against such attacks quite
challenging. Motivated by this hypothesis we argue that if a deep neural
network architecture can explicitly tap into its own gradient flow during the
training, it can boost its defense capability significantly. Inspired by this
fact, we introduce the concept of self-gradient networks, a novel deep neural
network architecture designed to be more robust against adversarial
perturbations. Gradient flow information is leveraged within self-gradient
networks to achieve greater perturbation stability beyond what can be achieved
in the standard training process. We conduct a theoretical analysis to gain
better insights into the behaviour of the proposed self-gradient networks to
illustrate the efficacy of leverage this additional gradient flow information.
The proposed self-gradient network architecture enables much more efficient and
effective adversarial training, leading to faster convergence towards an
adversarially robust solution by at least 10X. Experimental results demonstrate
the effectiveness of self-gradient networks when compared with state-of-the-art
adversarial learning strategies, with 10% improvement on the CIFAR10 dataset
under PGD and CW adversarial perturbations.
</p>
<a href="http://arxiv.org/abs/2011.09364" target="_blank">arXiv:2011.09364</a> [<a href="http://arxiv.org/pdf/2011.09364" target="_blank">pdf</a>]

<h2>Explainable AI for System Failures: Generating Explanations that Improve Human Assistance in Fault Recovery. (arXiv:2011.09407v2 [cs.AI] UPDATED)</h2>
<h3>Devleena Das, Siddhartha Banerjee, Sonia Chernova</h3>
<p>With the growing capabilities of intelligent systems, the integration of
artificial intelligence (AI) and robots in everyday life is increasing.
However, when interacting in such complex human environments, the failure of
intelligent systems, such as robots, can be inevitable, requiring recovery
assistance from users. In this work, we develop automated, natural language
explanations for failures encountered during an AI agents' plan execution.
These explanations are developed with a focus of helping non-expert users
understand different point of failures to better provide recovery assistance.
Specifically, we introduce a context-based information type for explanations
that can both help non-expert users understand the underlying cause of a system
failure, and select proper failure recoveries. Additionally, we extend an
existing sequence-to-sequence methodology to automatically generate our
context-based explanations. By doing so, we are able develop a model that can
generalize context-based explanations over both different failure types and
failure scenarios.
</p>
<a href="http://arxiv.org/abs/2011.09407" target="_blank">arXiv:2011.09407</a> [<a href="http://arxiv.org/pdf/2011.09407" target="_blank">pdf</a>]

<h2>Latent Representation Prediction Networks. (arXiv:2009.09439v1 [cs.LG] CROSS LISTED)</h2>
<h3>Hlynur Dav&#xed;&#xf0; Hlynsson, Merlin Sch&#xfc;ler, Robin Schiewer, Tobias Glasmachers, Laurenz Wiskott</h3>
<p>Deeply-learned planning methods are often based on learning representations
that are optimized for unrelated tasks. For example, they might be trained on
reconstructing the environment. These representations are then combined with
predictor functions for simulating rollouts to navigate the environment. We
find this principle of learning representations unsatisfying and propose to
learn them such that they are directly optimized for the task at hand: to be
maximally predictable for the predictor function. This results in
representations that are by design optimal for the downstream task of planning,
where the learned predictor function is used as a forward model.

To this end, we propose a new way of jointly learning this representation
along with the prediction function, a system we dub Latent Representation
Prediction Network (LARP). The prediction function is used as a forward model
for search on a graph in a viewpoint-matching task and the representation
learned to maximize predictability is found to outperform a pre-trained
representation. Our approach is shown to be more sample-efficient than standard
reinforcement learning methods and our learned representation transfers
successfully to dissimilar objects.
</p>
<a href="http://arxiv.org/abs/2009.09439" target="_blank">arXiv:2009.09439</a> [<a href="http://arxiv.org/pdf/2009.09439" target="_blank">pdf</a>]

