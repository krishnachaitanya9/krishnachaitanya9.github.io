---
title: Latest Deep Learning Papers
date: 2021-03-03 21:00:01 +0800
featured-img: DL-logo
categories: [Deep_Learning]
tags: [Deep_Learning]
mathjax: Yes
---

<h1>Your arXiv Feed (102 Articles)</h1>
<h2>CloudAAE: Learning 6D Object Pose Regression with On-line Data Synthesis on Point Clouds. (arXiv:2103.01977v1 [cs.CV])</h2>
<h3>Ge Gao, Mikko Lauri, Xiaolin Hu, Jianwei Zhang, Simone Frintrop</h3>
<p>It is often desired to train 6D pose estimation systems on synthetic data
because manual annotation is expensive. However, due to the large domain gap
between the synthetic and real images, synthesizing color images is expensive.
In contrast, this domain gap is considerably smaller and easier to fill for
depth information. In this work, we present a system that regresses 6D object
pose from depth information represented by point clouds, and a lightweight data
synthesis pipeline that creates synthetic point cloud segments for training. We
use an augmented autoencoder (AAE) for learning a latent code that encodes 6D
object pose information for pose regression. The data synthesis pipeline only
requires texture-less 3D object models and desired viewpoints, and it is cheap
in terms of both time and hardware storage. Our data synthesis process is up to
three orders of magnitude faster than commonly applied approaches that render
RGB image data. We show the effectiveness of our system on the LineMOD, LineMOD
Occlusion, and YCB Video datasets. The implementation of our system is
available at: https://github.com/GeeeG/CloudAAE.
</p>
<a href="http://arxiv.org/abs/2103.01977" target="_blank">arXiv:2103.01977</a> [<a href="http://arxiv.org/pdf/2103.01977" target="_blank">pdf</a>]

<h2>Self-supervised Pretraining of Visual Features in the Wild. (arXiv:2103.01988v1 [cs.CV])</h2>
<h3>Priya Goyal, Mathilde Caron, Benjamin Lefaudeux, Min Xu, Pengchao Wang, Vivek Pai, Mannat Singh, Vitaliy Liptchinsky, Ishan Misra, Armand Joulin, Piotr Bojanowski</h3>
<p>Recently, self-supervised learning methods like MoCo, SimCLR, BYOL and SwAV
have reduced the gap with supervised methods. These results have been achieved
in a control environment, that is the highly curated ImageNet dataset. However,
the premise of self-supervised learning is that it can learn from any random
image and from any unbounded dataset. In this work, we explore if
self-supervision lives to its expectation by training large models on random,
uncurated images with no supervision. Our final SElf-supERvised (SEER) model, a
RegNetY with 1.3B parameters trained on 1B random images with 512 GPUs achieves
84.2% top-1 accuracy, surpassing the best self-supervised pretrained model by
1% and confirming that self-supervised learning works in a real world setting.
Interestingly, we also observe that self-supervised models are good few-shot
learners achieving 77.9% top-1 with access to only 10% of ImageNet. Code:
https://github.com/facebookresearch/vissl
</p>
<a href="http://arxiv.org/abs/2103.01988" target="_blank">arXiv:2103.01988</a> [<a href="http://arxiv.org/pdf/2103.01988" target="_blank">pdf</a>]

<h2>Sequence-Based Filtering for Visual Route-Based Navigation: Analysing the Benefits, Trade-offs and Design Choices. (arXiv:2103.01994v1 [cs.CV])</h2>
<h3>Mihnea-Alexandru Tomit&#x103;, Mubariz Zaffar, Michael Milford, Klaus McDonald-Maier, Shoaib Ehsan</h3>
<p>Visual Place Recognition (VPR) is the ability to correctly recall a
previously visited place using visual information under environmental,
viewpoint and appearance changes. An emerging trend in VPR is the use of
sequence-based filtering methods on top of single-frame-based place matching
techniques for route-based navigation. The combination leads to varying levels
of potential place matching performance boosts at increased computational
costs. This raises a number of interesting research questions: How does
performance boost (due to sequential filtering) vary along the entire spectrum
of single-frame-based matching methods? How does sequence matching length
affect the performance curve? Which specific combinations provide a good
trade-off between performance and computation? However, there is lack of
previous work looking at these important questions and most of the
sequence-based filtering work to date has been used without a systematic
approach. To bridge this research gap, this paper conducts an in-depth
investigation of the relationship between the performance of single-frame-based
place matching techniques and the use of sequence-based filtering on top of
those methods. It analyzes individual trade-offs, properties and limitations
for different combinations of single-frame-based and sequential techniques. A
number of state-of-the-art VPR methods and widely used public datasets are
utilized to present the findings that contain a number of meaningful insights
for the VPR community.
</p>
<a href="http://arxiv.org/abs/2103.01994" target="_blank">arXiv:2103.01994</a> [<a href="http://arxiv.org/pdf/2103.01994" target="_blank">pdf</a>]

<h2>Material Measurement Units: Foundations Through a Survey. (arXiv:2103.01997v1 [cs.CV])</h2>
<h3>Federico Zocco, Se&#xe1;n McLoone</h3>
<p>Long-term availability of minerals and industrial materials is a necessary
condition for sustainable development as they are the constituents of any
manufacturing product. In particular, technologies with increasing demand such
as GPUs and photovoltaic panels are made of critical raw materials. To enhance
the efficiency of material management, in this paper we make three main
contributions: first, we identify in the literature an emerging
computer-vision-enabled material monitoring technology which we call Material
Measurement Unit (MMU); second, we provide a survey of works relevant to the
development of MMUs; third, we describe a material stock monitoring sensor
network deploying multiple MMUs.
</p>
<a href="http://arxiv.org/abs/2103.01997" target="_blank">arXiv:2103.01997</a> [<a href="http://arxiv.org/pdf/2103.01997" target="_blank">pdf</a>]

<h2>PECNet: A Deep Multi-Label Segmentation Network for Eosinophilic Esophagitis Biopsy Diagnostics. (arXiv:2103.02015v1 [cs.CV])</h2>
<h3>Nati Daniel, Ariel Larey, Eliel Aknin, Garrett A. Osswald, Julie M. Caldwell, Mark Rochman, Margaret H. Collins, Guang-Yu Yang, Nicoleta C. Arva, Kelley E. Capocelli, Marc E. Rothenberg, Yonatan Savir</h3>
<p>Background. Eosinophilic esophagitis (EoE) is an allergic inflammatory
condition of the esophagus associated with elevated numbers of eosinophils.
Disease diagnosis and monitoring requires determining the concentration of
eosinophils in esophageal biopsies, a time-consuming, tedious and somewhat
subjective task currently performed by pathologists. Methods. Herein, we aimed
to use machine learning to identify, quantitate and diagnose EoE. We labeled
more than 100M pixels of 4345 images obtained by scanning whole slides of
H&amp;E-stained sections of esophageal biopsies derived from 23 EoE patients. We
used this dataset to train a multi-label segmentation deep network. To validate
the network, we examined a replication cohort of 1089 whole slide images from
419 patients derived from multiple institutions. Findings. PECNet segmented
both intact and not-intact eosinophils with a mean intersection over union
(mIoU) of 0.93. This segmentation was able to quantitate intact eosinophils
with a mean absolute error of 0.611 eosinophils and classify EoE disease
activity with an accuracy of 98.5%. Using whole slide images from the
validation cohort, PECNet achieved an accuracy of 94.8%, sensitivity of 94.3%,
and specificity of 95.14% in reporting EoE disease activity. Interpretation. We
have developed a deep learning multi-label semantic segmentation network that
successfully addresses two of the main challenges in EoE diagnostics and
digital pathology, the need to detect several types of small features
simultaneously and the ability to analyze whole slides efficiently. Our results
pave the way for an automated diagnosis of EoE and can be utilized for other
conditions with similar challenges.
</p>
<a href="http://arxiv.org/abs/2103.02015" target="_blank">arXiv:2103.02015</a> [<a href="http://arxiv.org/pdf/2103.02015" target="_blank">pdf</a>]

<h2>DeepFake-o-meter: An Open Platform for DeepFake Detection. (arXiv:2103.02018v1 [cs.CV])</h2>
<h3>Yuezun Li, Cong Zhang, Pu Sun, Honggang Qi, Siwei Lyu</h3>
<p>In recent years, the advent of deep learning-based techniques and the
significant reduction in the cost of computation resulted in the feasibility of
creating realistic videos of human faces, commonly known as DeepFakes. The
availability of open-source tools to create DeepFakes poses as a threat to the
trustworthiness of the online media. In this work, we develop an open-source
online platform, known as DeepFake-o-meter, that integrates state-of-the-art
DeepFake detection methods and provide a convenient interface for the users. We
describe the design and function of DeepFake-o-meter in this work.
</p>
<a href="http://arxiv.org/abs/2103.02018" target="_blank">arXiv:2103.02018</a> [<a href="http://arxiv.org/pdf/2103.02018" target="_blank">pdf</a>]

<h2>EnD: Entangling and Disentangling deep representations for bias correction. (arXiv:2103.02023v1 [cs.CV])</h2>
<h3>Enzo Tartaglione, Carlo Alberto Barbano, Marco Grangetto</h3>
<p>Artificial neural networks perform state-of-the-art in an ever-growing number
of tasks, and nowadays they are used to solve an incredibly large variety of
tasks. There are problems, like the presence of biases in the training data,
which question the generalization capability of these models. In this work we
propose EnD, a regularization strategy whose aim is to prevent deep models from
learning unwanted biases. In particular, we insert an "information bottleneck"
at a certain point of the deep neural network, where we disentangle the
information about the bias, still letting the useful information for the
training task forward-propagating in the rest of the model. One big advantage
of EnD is that we do not require additional training complexity (like decoders
or extra layers in the model), since it is a regularizer directly applied on
the trained model. Our experiments show that EnD effectively improves the
generalization on unbiased test sets, and it can be effectively applied on
real-case scenarios, like removing hidden biases in the COVID-19 detection from
radiographic images.
</p>
<a href="http://arxiv.org/abs/2103.02023" target="_blank">arXiv:2103.02023</a> [<a href="http://arxiv.org/pdf/2103.02023" target="_blank">pdf</a>]

<h2>Obstacle Avoidance onboard MAVs using a FMCW RADAR. (arXiv:2103.02050v1 [cs.RO])</h2>
<h3>Nikhil Wessendorp, Raoul Dinaux, Julien Dupeyroux, Guido de Croon</h3>
<p>Micro Air Vehicles (MAVs) are increasingly being used for complex or
hazardous tasks in enclosed and cluttered environments such as surveillance or
search and rescue. With this comes the necessity for sensors that can operate
in poor visibility conditions to facilitate with navigation and avoidance of
objects or people. Radar sensors in particular can provide more robust sensing
of the environment when traditional sensors such as cameras fail in the
presence of dust, fog or smoke. While extensively used in autonomous driving,
miniature FMCW radars on MAVs have been relatively unexplored. This study aims
to investigate to what extent this sensor is of use in these environments by
employing traditional signal processing such as multi-target tracking and
velocity obstacles. The viability of the solution is evaluated with an
implementation on board a MAV by running trial tests in an indoor environment
containing obstacles and by comparison with a human pilot, demonstrating the
potential for the sensor to provide a more robust sense and avoid function in
fully autonomous MAVs.
</p>
<a href="http://arxiv.org/abs/2103.02050" target="_blank">arXiv:2103.02050</a> [<a href="http://arxiv.org/pdf/2103.02050" target="_blank">pdf</a>]

<h2>A 3D Printing Hexacopter: Design and Demonstration. (arXiv:2103.02063v1 [cs.RO])</h2>
<h3>Alexander Nettekoven, Ufuk Topcu</h3>
<p>3D printing using robots has garnered significant interest in manufacturing
and construction in recent years. A robot's versatility paired with the design
freedom of 3D printing offers promising opportunities for how parts and
structures are built in the future. However, 3D printed objects are still
limited in size and location due to a lack of vertical mobility of ground
robots. These limitations severely restrict the potential of the 3D printing
process. To overcome these limitations, we develop a hexacopter testbed that
can print via fused deposition modeling during flight. We discuss the design of
this testbed and develop a simple control strategy for initial print tests. By
successfully performing these initial print tests, we demonstrate the
feasibility of this approach and lay the groundwork for printing 3D parts and
structures with drones.
</p>
<a href="http://arxiv.org/abs/2103.02063" target="_blank">arXiv:2103.02063</a> [<a href="http://arxiv.org/pdf/2103.02063" target="_blank">pdf</a>]

<h2>Sequential Place Learning: Heuristic-Free High-Performance Long-Term Place Recognition. (arXiv:2103.02074v1 [cs.CV])</h2>
<h3>Marvin Chanc&#xe1;n, Michael Milford</h3>
<p>Sequential matching using hand-crafted heuristics has been standard practice
in route-based place recognition for enhancing pairwise similarity results for
nearly a decade. However, precision-recall performance of these algorithms
dramatically degrades when searching on short temporal window (TW) lengths,
while demanding high compute and storage costs on large robotic datasets for
autonomous navigation research. Here, influenced by biological systems that
robustly navigate spacetime scales even without vision, we develop a joint
visual and positional representation learning technique, via a sequential
process, and design a learning-based CNN+LSTM architecture, trainable via
backpropagation through time, for viewpoint- and appearance-invariant place
recognition. Our approach, Sequential Place Learning (SPL), is based on a CNN
function that visually encodes an environment from a single traversal, thus
reducing storage capacity, while an LSTM temporally fuses each visual embedding
with corresponding positional data -- obtained from any source of motion
estimation -- for direct sequential inference. Contrary to classical two-stage
pipelines, e.g., match-then-temporally-filter, our network directly eliminates
false-positive rates while jointly learning sequence matching from a single
monocular image sequence, even using short TWs. Hence, we demonstrate that our
model outperforms 15 classical methods while setting new state-of-the-art
performance standards on 4 challenging benchmark datasets, where one of them
can be considered solved with recall rates of 100% at 100% precision, correctly
matching all places under extreme sunlight-darkness changes. In addition, we
show that SPL can be up to 70x faster to deploy than classical methods on a 729
km route comprising 35,768 consecutive frames. Extensive experiments
demonstrate the... Baseline code available at
https://github.com/mchancan/deepseqslam
</p>
<a href="http://arxiv.org/abs/2103.02074" target="_blank">arXiv:2103.02074</a> [<a href="http://arxiv.org/pdf/2103.02074" target="_blank">pdf</a>]

<h2>Uncertainty guided semi-supervised segmentation of retinal layers in OCT images. (arXiv:2103.02083v1 [cs.CV])</h2>
<h3>Suman Sedai, Bhavna Antony, Ravneet Rai, Katie Jones, Hiroshi Ishikawa, Joel Schuman, Wollstein Gadi, Rahil Garnavi</h3>
<p>Deep convolutional neural networks have shown outstanding performance in
medical image segmentation tasks. The usual problem when training supervised
deep learning methods is the lack of labeled data which is time-consuming and
costly to obtain. In this paper, we propose a novel uncertainty-guided
semi-supervised learning based on a student-teacher approach for training the
segmentation network using limited labeled samples and a large number of
unlabeled images. First, a teacher segmentation model is trained from the
labeled samples using Bayesian deep learning. The trained model is used to
generate soft segmentation labels and uncertainty maps for the unlabeled set.
The student model is then updated using the softly segmented samples and the
corresponding pixel-wise confidence of the segmentation quality estimated from
the uncertainty of the teacher model using a newly designed loss function.
Experimental results on a retinal layer segmentation task show that the
proposed method improves the segmentation performance in comparison to the
fully supervised approach and is on par with the expert annotator. The proposed
semi-supervised segmentation framework is a key contribution and applicable for
biomedical image segmentation across various imaging modalities where access to
annotated medical images is challenging
</p>
<a href="http://arxiv.org/abs/2103.02083" target="_blank">arXiv:2103.02083</a> [<a href="http://arxiv.org/pdf/2103.02083" target="_blank">pdf</a>]

<h2>Pseudo-labeling for Scalable 3D Object Detection. (arXiv:2103.02093v1 [cs.CV])</h2>
<h3>Benjamin Caine, Rebecca Roelofs, Vijay Vasudevan, Jiquan Ngiam, Yuning Chai, Zhifeng Chen, Jonathon Shlens</h3>
<p>To safely deploy autonomous vehicles, onboard perception systems must work
reliably at high accuracy across a diverse set of environments and geographies.
One of the most common techniques to improve the efficacy of such systems in
new domains involves collecting large labeled datasets, but such datasets can
be extremely costly to obtain, especially if each new deployment geography
requires additional data with expensive 3D bounding box annotations. We
demonstrate that pseudo-labeling for 3D object detection is an effective way to
exploit less expensive and more widely available unlabeled data, and can lead
to performance gains across various architectures, data augmentation
strategies, and sizes of the labeled dataset. Overall, we show that better
teacher models lead to better student models, and that we can distill expensive
teachers into efficient, simple students.

Specifically, we demonstrate that pseudo-label-trained student models can
outperform supervised models trained on 3-10 times the amount of labeled
examples. Using PointPillars [24], a two-year-old architecture, as our student
model, we are able to achieve state of the art accuracy simply by leveraging
large quantities of pseudo-labeled data. Lastly, we show that these student
models generalize better than supervised models to a new domain in which we
only have unlabeled data, making pseudo-label training an effective form of
unsupervised domain adaptation.
</p>
<a href="http://arxiv.org/abs/2103.02093" target="_blank">arXiv:2103.02093</a> [<a href="http://arxiv.org/pdf/2103.02093" target="_blank">pdf</a>]

<h2>An Alternative Practice of Tropical Convolution to Traditional Convolutional Neural Networks. (arXiv:2103.02096v1 [cs.CV])</h2>
<h3>Shiqing Fan, Ye Luo</h3>
<p>Convolutional neural networks (CNNs) have been used in many machine learning
fields. In practical applications, the computational cost of convolutional
neural networks is often high with the deepening of the network and the growth
of data volume, mostly due to a large amount of multiplication operations of
floating-point numbers in convolution operations. To reduce the amount of
multiplications, we propose a new type of CNNs called Tropical Convolutional
Neural Networks (TCNNs) which are built on tropical convolutions in which the
multiplications and additions in conventional convolutional layers are replaced
by additions and min/max operations respectively. In addition, since tropical
convolution operators are essentially nonlinear operators, we expect TCNNs to
have higher nonlinear fitting ability than conventional CNNs. In the
experiments, we test and analyze several different architectures of TCNNs for
image classification tasks in comparison with similar-sized conventional CNNs.
The results show that TCNN can achieve higher expressive power than ordinary
convolutional layers on the MNIST and CIFAR10 image data set. In different
noise environments, there are wins and losses in the robustness of TCNN and
ordinary CNNs.
</p>
<a href="http://arxiv.org/abs/2103.02096" target="_blank">arXiv:2103.02096</a> [<a href="http://arxiv.org/pdf/2103.02096" target="_blank">pdf</a>]

<h2>Robust Place Recognition using an Imaging Lidar. (arXiv:2103.02111v1 [cs.CV])</h2>
<h3>Tixiao Shan, Brendan Englot, Fabio Duarte, Carlo Ratti, Daniela Rus</h3>
<p>We propose a methodology for robust, real-time place recognition using an
imaging lidar, which yields image-quality high-resolution 3D point clouds.
Utilizing the intensity readings of an imaging lidar, we project the point
cloud and obtain an intensity image. ORB feature descriptors are extracted from
the image and encoded into a bag-of-words vector. The vector, used to identify
the point cloud, is inserted into a database that is maintained by DBoW for
fast place recognition queries. The returned candidate is further validated by
matching visual feature descriptors. To reject matching outliers, we apply PnP,
which minimizes the reprojection error of visual features' positions in
Euclidean space with their correspondences in 2D image space, using RANSAC.
Combining the advantages from both camera and lidar-based place recognition
approaches, our method is truly rotation-invariant and can tackle reverse
revisiting and upside-down revisiting. The proposed method is evaluated on
datasets gathered from a variety of platforms over different scales and
environments. Our implementation is available at
https://git.io/imaging-lidar-place-recognition
</p>
<a href="http://arxiv.org/abs/2103.02111" target="_blank">arXiv:2103.02111</a> [<a href="http://arxiv.org/pdf/2103.02111" target="_blank">pdf</a>]

<h2>Deblurring Processor for Motion-Blurred Faces Based on Generative Adversarial Networks. (arXiv:2103.02121v1 [cs.CV])</h2>
<h3>Shiqing Fan, Ye Luo</h3>
<p>Low-quality face image restoration is a popular research direction in today's
computer vision field. It can be used as a pre-work for tasks such as face
detection and face recognition. At present, there is a lot of work to solve the
problem of low-quality faces under various environmental conditions. This paper
mainly focuses on the restoration of motion-blurred faces. In increasingly
abundant mobile scenes, the fast recovery of motion-blurred faces can bring
highly effective speed improvements in tasks such as face matching. In order to
achieve this goal, a deblurring method for motion-blurred facial image signals
based on generative adversarial networks(GANs) is proposed. It uses an
end-to-end method to train a sharp image generator, i.e., a processor for
motion-blurred facial images. This paper introduce the processing progress of
motion-blurred images, the development and changes of GANs and some basic
concepts. After that, it give the details of network structure and training
optimization design of the image processor. Then we conducted a motion blur
image generation experiment on some general facial data set, and used the pairs
of blurred and sharp face image data to perform the training and testing
experiments of the processor GAN, and gave some visual displays. Finally, MTCNN
is used to detect the faces of the image generated by the deblurring processor,
and compare it with the result of the blurred image. From the results, the
processing effect of the deblurring processor on the motion-blurred picture has
a significant improvement both in terms of intuition and evaluation indicators
of face detection.
</p>
<a href="http://arxiv.org/abs/2103.02121" target="_blank">arXiv:2103.02121</a> [<a href="http://arxiv.org/pdf/2103.02121" target="_blank">pdf</a>]

<h2>Augmentation Strategies for Learning with Noisy Labels. (arXiv:2103.02130v1 [cs.CV])</h2>
<h3>Kento Nishi, Yi Ding, Alex Rich, Tobias H&#xf6;llerer</h3>
<p>Imperfect labels are ubiquitous in real-world datasets. Several recent
successful methods for training deep neural networks (DNNs) robust to label
noise have used two primary techniques: filtering samples based on loss during
a warm-up phase to curate an initial set of cleanly labeled samples, and using
the output of a network as a pseudo-label for subsequent loss calculations. In
this paper, we evaluate different augmentation strategies for algorithms
tackling the "learning with noisy labels" problem. We propose and examine
multiple augmentation strategies and evaluate them using synthetic datasets
based on CIFAR-10 and CIFAR-100, as well as on the real-world dataset
Clothing1M. Due to several commonalities in these algorithms, we find that
using one set of augmentations for loss modeling tasks and another set for
learning is the most effective, improving results on the state-of-the-art and
other previous methods. Furthermore, we find that applying augmentation during
the warm-up period can negatively impact the loss convergence behavior of
correctly versus incorrectly labeled samples. We introduce this augmentation
strategy to the state-of-the-art technique and demonstrate that we can improve
performance across all evaluated noise levels. In particular, we improve
accuracy on the CIFAR-10 benchmark at 90% symmetric noise by more than 15% in
absolute accuracy and we also improve performance on the real-world dataset
Clothing1M.

(* equal contribution)
</p>
<a href="http://arxiv.org/abs/2103.02130" target="_blank">arXiv:2103.02130</a> [<a href="http://arxiv.org/pdf/2103.02130" target="_blank">pdf</a>]

<h2>Controlling the Sense of Agency in Dyadic Robot Interaction: An Active Inference Approach. (arXiv:2103.02137v1 [cs.RO])</h2>
<h3>Nadine Wirkuttis, Jun Tani</h3>
<p>This study investigated how social interaction among robotic agents changes
dynamically depending on individual sense of agency. In a set of simulation
studies, we examine dyadic imitative interactions of robots using a variational
recurrent neural network model. The model is based on the free energy principle
such that interacting robots find themselves in a loop, attempting to predict
and infer each other's actions using active inference. We examined how
regulating the complexity term to minimize free energy during training
determines the dynamic characteristics of networks and affects dyadic imitative
interactions. Our simulation results show that through softer regulation of the
complexity term, a robot with stronger agency develops and dominates its
counterpart developed with weaker agency through tighter regulation. When two
robots are trained with equally soft regulation, both generate individual
intended behavior patterns, ignoring each other. We argue that primary
intersubjectivity does develop in dyadic robotic interactions.
</p>
<a href="http://arxiv.org/abs/2103.02137" target="_blank">arXiv:2103.02137</a> [<a href="http://arxiv.org/pdf/2103.02137" target="_blank">pdf</a>]

<h2>PML: Progressive Margin Loss for Long-tailed Age Classification. (arXiv:2103.02140v1 [cs.CV])</h2>
<h3>Zongyong Deng, Hao Liu, Yaoxing Wang, Chenyang Wang, Zekuan Yu, Xuehong Sun</h3>
<p>In this paper, we propose a progressive margin loss (PML) approach for
unconstrained facial age classification. Conventional methods make strong
assumption on that each class owns adequate instances to outline its data
distribution, likely leading to bias prediction where the training samples are
sparse across age classes. Instead, our PML aims to adaptively refine the age
label pattern by enforcing a couple of margins, which fully takes in the
in-between discrepancy of the intra-class variance, inter-class variance and
class center. Our PML typically incorporates with the ordinal margin and the
variational margin, simultaneously plugging in the globally-tuned deep neural
network paradigm. More specifically, the ordinal margin learns to exploit the
correlated relationship of the real-world age labels. Accordingly, the
variational margin is leveraged to minimize the influence of head classes that
misleads the prediction of tailed samples. Moreover, our optimization carefully
seeks a series of indicator curricula to achieve robust and efficient model
training. Extensive experimental results on three face aging datasets
demonstrate that our PML achieves compelling performance compared to state of
the arts. Code will be made publicly.
</p>
<a href="http://arxiv.org/abs/2103.02140" target="_blank">arXiv:2103.02140</a> [<a href="http://arxiv.org/pdf/2103.02140" target="_blank">pdf</a>]

<h2>Learning to Fly -- a Gym Environment with PyBullet Physics for Reinforcement Learning of Multi-agent Quadcopter Control. (arXiv:2103.02142v1 [cs.RO])</h2>
<h3>Jacopo Panerati (1 and 2), Hehui Zheng (3), SiQi Zhou (1 and 2), James Xu (1), Amanda Prorok (3), Angela P. Schoellig (1 and 2) ((1) University of Toronto Institute for Aerospace Studies, (2) Vector Institute for Artificial Intelligence, (3) University of Cambridge)</h3>
<p>Robotic simulators are crucial for academic research and education as well as
the development of safety-critical applications. Reinforcement learning
environments -- simple simulations coupled with a problem specification in the
form of a reward function -- are also important to standardize the development
(and benchmarking) of learning algorithms. Yet, full-scale simulators typically
lack portability and parallelizability. Vice versa, many reinforcement learning
environments trade-off realism for high sample throughputs in toy-like
problems. While public data sets have greatly benefited deep learning and
computer vision, we still lack the software tools to simultaneously develop --
and fairly compare -- control theory and reinforcement learning approaches. In
this paper, we propose an open-source OpenAI Gym-like environment for multiple
quadcopters based on the Bullet physics engine. Its multi-agent and vision
based reinforcement learning interfaces, as well as the support of realistic
collisions and aerodynamic effects, make it, to the best of our knowledge, a
first of its kind. We demonstrate its use through several examples, either for
control (trajectory tracking with PID control, multi-robot flight with
downwash, etc.) or reinforcement learning (single and multi-agent stabilization
tasks), hoping to inspire future research that combines control theory and
machine learning.
</p>
<a href="http://arxiv.org/abs/2103.02142" target="_blank">arXiv:2103.02142</a> [<a href="http://arxiv.org/pdf/2103.02142" target="_blank">pdf</a>]

<h2>Group-wise Inhibition based Feature Regularization for Robust Classification. (arXiv:2103.02152v1 [cs.CV])</h2>
<h3>Haozhe Liu, Haoqian Wu, Weicheng Xie, Feng Liu, Linlin Shen</h3>
<p>The vanilla convolutional neural network (CNN) is vulnerable to images with
small variations (e.g. corrupted and adversarial samples). One of the possible
reasons is that CNN pays more attention to the most discriminative regions, but
ignores the auxiliary features, leading to the lack of feature diversity. In
our method , we propose to dynamically suppress significant activation values
of vanilla CNN by group-wise inhibition, but not fix or randomly handle them
when training. Then, the feature maps with different activation distribution
are processed separately due to the independence of features. Vanilla CNN is
finally guided to learn more rich discriminative features hierarchically for
robust classification according to proposed regularization. The proposed method
is able to achieve a significant gain of robustness over 15% comparing with the
state-of-the-art. We also show that the proposed regularization method
complements other defense paradigms, such as adversarial training, to further
improve the robustness.
</p>
<a href="http://arxiv.org/abs/2103.02152" target="_blank">arXiv:2103.02152</a> [<a href="http://arxiv.org/pdf/2103.02152" target="_blank">pdf</a>]

<h2>Sensing population distribution from satellite imagery via deep learning: model selection, neighboring effect, and systematic biases. (arXiv:2103.02155v1 [cs.CV])</h2>
<h3>Xiao Huang, Di Zhu, Fan Zhang, Tao Liu, Xiao Li, Lei Zou</h3>
<p>The rapid development of remote sensing techniques provides rich,
large-coverage, and high-temporal information of the ground, which can be
coupled with the emerging deep learning approaches that enable latent features
and hidden geographical patterns to be extracted. This study marks the first
attempt to cross-compare performances of popular state-of-the-art deep learning
models in estimating population distribution from remote sensing images,
investigate the contribution of neighboring effect, and explore the potential
systematic population estimation biases. We conduct an end-to-end training of
four popular deep learning architectures, i.e., VGG, ResNet, Xception, and
DenseNet, by establishing a mapping between Sentinel-2 image patches and their
corresponding population count from the LandScan population grid. The results
reveal that DenseNet outperforms the other three models, while VGG has the
worst performances in all evaluating metrics under all selected neighboring
scenarios. As for the neighboring effect, contradicting existing studies, our
results suggest that the increase of neighboring sizes leads to reduced
population estimation performance, which is found universal for all four
selected models in all evaluating metrics. In addition, there exists a notable,
universal bias that all selected deep learning models tend to overestimate
sparsely populated image patches and underestimate densely populated image
patches, regardless of neighboring sizes. The methodological, experimental, and
contextual knowledge this study provides is expected to benefit a wide range of
future studies that estimate population distribution via remote sensing
imagery.
</p>
<a href="http://arxiv.org/abs/2103.02155" target="_blank">arXiv:2103.02155</a> [<a href="http://arxiv.org/pdf/2103.02155" target="_blank">pdf</a>]

<h2>Touchless Palmprint Recognition based on 3D Gabor Template and Block Feature Refinement. (arXiv:2103.02167v1 [cs.CV])</h2>
<h3>Zhaoqun Li, Xu Liang, Dandan Fan, Jinxing Li, Wei Jia, David Zhang</h3>
<p>With the growing demand for hand hygiene and convenience of use, palmprint
recognition with touchless manner made a great development recently, providing
an effective solution for person identification. Despite many efforts that have
been devoted to this area, it is still uncertain about the discriminative
ability of the contactless palmprint, especially for large-scale datasets. To
tackle the problem, in this paper, we build a large-scale touchless palmprint
dataset containing 2334 palms from 1167 individuals. To our best knowledge, it
is the largest contactless palmprint image benchmark ever collected with regard
to the number of individuals and palms. Besides, we propose a novel deep
learning framework for touchless palmprint recognition named 3DCPN (3D
Convolution Palmprint recognition Network) which leverages 3D convolution to
dynamically integrate multiple Gabor features. In 3DCPN, a novel variant of
Gabor filter is embedded into the first layer for enhancement of curve feature
extraction. With a well-designed ensemble scheme,low-level 3D features are then
convolved to extract high-level features. Finally on the top, we set a
region-based loss function to strengthen the discriminative ability of both
global and local descriptors. To demonstrate the superiority of our method,
extensive experiments are conducted on our dataset and other popular databases
TongJi and IITD, where the results show the proposed 3DCPN achieves
state-of-the-art or comparable performances.
</p>
<a href="http://arxiv.org/abs/2103.02167" target="_blank">arXiv:2103.02167</a> [<a href="http://arxiv.org/pdf/2103.02167" target="_blank">pdf</a>]

<h2>Towards Fully Intelligent Transportation through Infrastructure-Vehicle Cooperative Autonomous Driving: Challenges and Opportunities. (arXiv:2103.02176v1 [cs.RO])</h2>
<h3>Shaoshan Liu, Bo Yu, Jie Tang, Qi Zhu</h3>
<p>The infrastructure-vehicle cooperative autonomous driving approach depends on
the cooperation between intelligent roads and intelligent vehicles. This
approach is not only safer but also more economical compared to the traditional
on-vehicle-only autonomous driving approach. In this paper, we introduce our
real-world deployment experiences of cooperative autonomous driving, and delve
into the details of new challenges and opportunities. Specifically, based on
our progress towards commercial deployment, we follow a three-stage development
roadmap of the cooperative autonomous driving approach:infrastructure-augmented
autonomous driving (IAAD), infrastructure-guided autonomous driving (IGAD), and
infrastructure-planned autonomous driving (IPAD).
</p>
<a href="http://arxiv.org/abs/2103.02176" target="_blank">arXiv:2103.02176</a> [<a href="http://arxiv.org/pdf/2103.02176" target="_blank">pdf</a>]

<h2>RGB Matters: Learning 7-DoF Grasp Poses on Monocular RGBD Images. (arXiv:2103.02184v1 [cs.RO])</h2>
<h3>Minghao Gou, Hao-Shu Fang, Zhanda Zhu, Sheng Xu, Chenxi Wang, Cewu Lu</h3>
<p>General object grasping is an important yet unsolved problem in the field of
robotics. Most of the current methods either generate grasp poses with few DoF
that fail to cover most of the success grasps, or only take the unstable depth
image or point cloud as input which may lead to poor results in some cases. In
this paper, we propose RGBD-Grasp, a pipeline that solves this problem by
decoupling 7-DoF grasp detection into two sub-tasks where RGB and depth
information are processed separately. In the first stage, an encoder-decoder
like convolutional neural network Angle-View Net(AVN) is proposed to predict
the SO(3) orientation of the gripper at every location of the image.
Consequently, a Fast Analytic Searching(FAS) module calculates the opening
width and the distance of the gripper to the grasp point. By decoupling the
grasp detection problem and introducing the stable RGB modality, our pipeline
alleviates the requirement for the high-quality depth image and is robust to
depth sensor noise. We achieve state-of-the-art results on GraspNet-1Billion
dataset compared with several baselines. Real robot experiments on a UR5 robot
with an Intel Realsense camera and a Robotiq two-finger gripper show high
success rates for both single object scenes and cluttered scenes. Our code and
trained model will be made publicly available.
</p>
<a href="http://arxiv.org/abs/2103.02184" target="_blank">arXiv:2103.02184</a> [<a href="http://arxiv.org/pdf/2103.02184" target="_blank">pdf</a>]

<h2>Task Aligned Generative Meta-learning for Zero-shot Learning. (arXiv:2103.02185v1 [cs.CV])</h2>
<h3>Zhe Liu, Yun Li, Lina Yao, Xianzhi Wang, Guodong Long</h3>
<p>Zero-shot learning (ZSL) refers to the problem of learning to classify
instances from the novel classes (unseen) that are absent in the training set
(seen). Most ZSL methods infer the correlation between visual features and
attributes to train the classifier for unseen classes. However, such models may
have a strong bias towards seen classes during training. Meta-learning has been
introduced to mitigate the basis, but meta-ZSL methods are inapplicable when
tasks used for training are sampled from diverse distributions. In this regard,
we propose a novel Task-aligned Generative Meta-learning model for Zero-shot
learning (TGMZ). TGMZ mitigates the potentially biased training and enables
meta-ZSL to accommodate real-world datasets containing diverse distributions.
TGMZ incorporates an attribute-conditioned task-wise distribution alignment
network that projects tasks into a unified distribution to deliver an unbiased
model. Our comparisons with state-of-the-art algorithms show the improvements
of 2.1%, 3.0%, 2.5%, and 7.6% achieved by TGMZ on AWA1, AWA2, CUB, and aPY
datasets, respectively. TGMZ also outperforms competitors by 3.6% in
generalized zero-shot learning (GZSL) setting and 7.9% in our proposed
fusion-ZSL setting.
</p>
<a href="http://arxiv.org/abs/2103.02185" target="_blank">arXiv:2103.02185</a> [<a href="http://arxiv.org/pdf/2103.02185" target="_blank">pdf</a>]

<h2>Adaptive Consistency Regularization for Semi-Supervised Transfer Learning. (arXiv:2103.02193v1 [cs.CV])</h2>
<h3>Abulikemu Abuduweili, Xingjian Li, Humphrey Shi, Cheng-Zhong Xu, Dejing Dou</h3>
<p>While recent studies on semi-supervised learning have shown remarkable
progress in leveraging both labeled and unlabeled data, most of them presume a
basic setting of the model is randomly initialized. In this work, we consider
semi-supervised learning and transfer learning jointly, leading to a more
practical and competitive paradigm that can utilize both powerful pre-trained
models from source domain as well as labeled/unlabeled data in the target
domain. To better exploit the value of both pre-trained weights and unlabeled
target examples, we introduce adaptive consistency regularization that consists
of two complementary components: Adaptive Knowledge Consistency (AKC) on the
examples between the source and target model, and Adaptive Representation
Consistency (ARC) on the target model between labeled and unlabeled examples.
Examples involved in the consistency regularization are adaptively selected
according to their potential contributions to the target task. We conduct
extensive experiments on several popular benchmarks including CUB-200-2011, MIT
Indoor-67, MURA, by fine-tuning the ImageNet pre-trained ResNet-50 model.
Results show that our proposed adaptive consistency regularization outperforms
state-of-the-art semi-supervised learning techniques such as Pseudo Label, Mean
Teacher, and MixMatch. Moreover, our algorithm is orthogonal to existing
methods and thus able to gain additional improvements on top of MixMatch and
FixMatch. Our code is available at
https://github.com/SHI-Labs/Semi-Supervised-Transfer-Learning.
</p>
<a href="http://arxiv.org/abs/2103.02193" target="_blank">arXiv:2103.02193</a> [<a href="http://arxiv.org/pdf/2103.02193" target="_blank">pdf</a>]

<h2>Bulk Production Augmentation Towards Explainable Melanoma Diagnosis. (arXiv:2103.02198v1 [cs.CV])</h2>
<h3>Kasumi Obi, Quan Huu Cap, Noriko Umegaki-Arao, Masaru Tanaka, Hitoshi Iyatomi</h3>
<p>Although highly accurate automated diagnostic techniques for melanoma have
been reported, the realization of a system capable of providing diagnostic
evidence based on medical indices remains an open issue because of difficulties
in obtaining reliable training data. In this paper, we propose bulk production
augmentation (BPA) to generate high-quality, diverse pseudo-skin tumor images
with the desired structural malignant features for additional training images
from a limited number of labeled images. The proposed BPA acts as an effective
data augmentation in constructing the feature detector for the atypical pigment
network (APN), which is a key structure in melanoma diagnosis. Experiments show
that training with images generated by our BPA largely boosts the APN detection
performance by 20.0 percentage points in the area under the receiver operating
characteristic curve, which is 11.5 to 13.7 points higher than that of
conventional CycleGAN-based augmentations in AUC.
</p>
<a href="http://arxiv.org/abs/2103.02198" target="_blank">arXiv:2103.02198</a> [<a href="http://arxiv.org/pdf/2103.02198" target="_blank">pdf</a>]

<h2>Semantic constraints to represent common sense required in household actions for multi-modal Learning-from-observation robot. (arXiv:2103.02201v1 [cs.RO])</h2>
<h3>Katsushi Ikeuchi, Naoki Wake, Riku Arakawa, Kazuhiro Sasabuchi, Jun Takamatsu</h3>
<p>The paradigm of learning-from-observation (LfO) enables a robot to learn how
to perform actions by observing human-demonstrated actions. Previous research
in LfO have mainly focused on the industrial domain which only consist of the
observable physical constraints between a manipulating tool and the robot's
working environment. In order to extend this paradigm to the household domain
which consists non-observable constraints derived from a human's common sense;
we introduce the idea of semantic constraints. The semantic constraints are
represented similar to the physical constraints by defining a contact with an
imaginary semantic environment. We thoroughly investigate the necessary and
sufficient set of contact state and state transitions to understand the
different types of physical and semantic constraints. We then apply our
constraint representation to analyze various actions in top hit household
YouTube videos and real home cooking recordings. We further categorize the
frequently appearing constraint patterns into physical, semantic, and
multistage task groups and verify that these groups are not only necessary but
a sufficient set for covering standard household actions. Finally, we conduct a
preliminary experiment using textual input to explore the possibilities of
combining verbal and visual input for recognizing the task groups. Our results
provide promising directions for incorporating common sense in the literature
of robot teaching.
</p>
<a href="http://arxiv.org/abs/2103.02201" target="_blank">arXiv:2103.02201</a> [<a href="http://arxiv.org/pdf/2103.02201" target="_blank">pdf</a>]

<h2>EaZy Learning: An Adaptive Variant of Ensemble Learning for Fingerprint Liveness Detection. (arXiv:2103.02207v1 [cs.CV])</h2>
<h3>Shivang Agarwal, C. Ravindranath Chowdary, Vivek Sourabh</h3>
<p>In the field of biometrics, fingerprint recognition systems are vulnerable to
presentation attacks made by artificially generated spoof fingerprints.
Therefore, it is essential to perform liveness detection of a fingerprint
before authenticating it. Fingerprint liveness detection mechanisms perform
well under the within-dataset environment but fail miserably under cross-sensor
(when tested on a fingerprint acquired by a new sensor) and cross-dataset (when
trained on one dataset and tested on another) settings. To enhance the
generalization abilities, robustness and the interoperability of the
fingerprint spoof detectors, the learning models need to be adaptive towards
the data. We propose a generic model, EaZy learning which can be considered as
an adaptive midway between eager and lazy learning. We show the usefulness of
this adaptivity under cross-sensor and cross-dataset environments. EaZy
learning examines the properties intrinsic to the dataset while generating a
pool of hypotheses. EaZy learning is similar to ensemble learning as it
generates an ensemble of base classifiers and integrates them to make a
prediction. Still, it differs in the way it generates the base classifiers.
EaZy learning develops an ensemble of entirely disjoint base classifiers which
has a beneficial influence on the diversity of the underlying ensemble. Also,
it integrates the predictions made by these base classifiers based on their
performance on the validation data. Experiments conducted on the standard high
dimensional datasets LivDet 2011, LivDet 2013 and LivDet 2015 prove the
efficacy of the model under cross-dataset and cross-sensor environments.
</p>
<a href="http://arxiv.org/abs/2103.02207" target="_blank">arXiv:2103.02207</a> [<a href="http://arxiv.org/pdf/2103.02207" target="_blank">pdf</a>]

<h2>K-FACE: A Large-Scale KIST Face Database in Consideration with Unconstrained Environments. (arXiv:2103.02211v1 [cs.CV])</h2>
<h3>Yeji Choi, Hyunjung Park, Gi Pyo Nam, Haksub Kim, Heeseung Choi, Junghyun Cho, Ig-Jae Kim</h3>
<p>In this paper, we introduce a new large-scale face database from KIST,
denoted as K-FACE, and describe a novel capturing device specifically designed
to obtain the data. The K-FACE database contains more than 1 million
high-quality images of 1,000 subjects selected by considering the ratio of
gender and age groups. It includes a variety of attributes, including 27 poses,
35 lighting conditions, three expressions, and occlusions by the combination of
five types of accessories. As the K-FACE database is systematically constructed
through a hemispherical capturing system with elaborate lighting control and
multiple cameras, it is possible to accurately analyze the effects of factors
that cause performance degradation, such as poses, lighting changes, and
accessories. We consider not only the balance of external environmental
factors, such as pose and lighting, but also the balance of personal
characteristics such as gender and age group. The gender ratio is the same,
while the age groups of subjects are uniformly distributed from the 20s to 50s
for both genders. The K-FACE database can be extensively utilized in various
vision tasks, such as face recognition, face frontalization, illumination
normalization, face age estimation, and three-dimensional face model
generation. We expect systematic diversity and uniformity of the K-FACE
database to promote these research fields.
</p>
<a href="http://arxiv.org/abs/2103.02211" target="_blank">arXiv:2103.02211</a> [<a href="http://arxiv.org/pdf/2103.02211" target="_blank">pdf</a>]

<h2>Unsupervised Domain Adaptation Network with Category-Centric Prototype Aligner for Biomedical Image Segmentation. (arXiv:2103.02220v1 [cs.CV])</h2>
<h3>Ping Gong, Wenwen Yu, Qiuwen Sun, Ruohan Zhao, Junfeng Hu</h3>
<p>With the widespread success of deep learning in biomedical image
segmentation, domain shift becomes a critical and challenging problem, as the
gap between two domains can severely affect model performance when deployed to
unseen data with heterogeneous features. To alleviate this problem, we present
a novel unsupervised domain adaptation network, for generalizing models learned
from the labeled source domain to the unlabeled target domain for
cross-modality biomedical image segmentation. Specifically, our approach
consists of two key modules, a conditional domain discriminator~(CDD) and a
category-centric prototype aligner~(CCPA). The CDD, extended from conditional
domain adversarial networks in classifier tasks, is effective and robust in
handling complex cross-modality biomedical images. The CCPA, improved from the
graph-induced prototype alignment mechanism in cross-domain object detection,
can exploit precise instance-level features through an elaborate prototype
representation. In addition, it can address the negative effect of class
imbalance via entropy-based loss. Extensive experiments on a public benchmark
for the cardiac substructure segmentation task demonstrate that our method
significantly improves performance on the target domain.
</p>
<a href="http://arxiv.org/abs/2103.02220" target="_blank">arXiv:2103.02220</a> [<a href="http://arxiv.org/pdf/2103.02220" target="_blank">pdf</a>]

<h2>Energy-Based Learning for Scene Graph Generation. (arXiv:2103.02221v1 [cs.CV])</h2>
<h3>Mohammed Suhail, Abhay Mittal, Behjat Siddiquie, Chris Broaddus, Jayan Eledath, Gerard Medioni, Leonid Sigal</h3>
<p>Traditional scene graph generation methods are trained using cross-entropy
losses that treat objects and relationships as independent entities. Such a
formulation, however, ignores the structure in the output space, in an
inherently structured prediction problem. In this work, we introduce a novel
energy-based learning framework for generating scene graphs. The proposed
formulation allows for efficiently incorporating the structure of scene graphs
in the output space. This additional constraint in the learning framework acts
as an inductive bias and allows models to learn efficiently from a small number
of labels. We use the proposed energy-based framework to train existing
state-of-the-art models and obtain a significant performance improvement, of up
to 21% and 27%, on the Visual Genome and GQA benchmark datasets, respectively.
Furthermore, we showcase the learning efficiency of the proposed framework by
demonstrating superior performance in the zero- and few-shot settings where
data is scarce.
</p>
<a href="http://arxiv.org/abs/2103.02221" target="_blank">arXiv:2103.02221</a> [<a href="http://arxiv.org/pdf/2103.02221" target="_blank">pdf</a>]

<h2>Inertial based Integration with Transformed INS Mechanization in Earth Frame. (arXiv:2103.02229v1 [cs.RO])</h2>
<h3>Lubin Chang, Jingbo Di, Fangjun Qin</h3>
<p>This paper proposes to use a newly-derived transformed inertial navigation
system (INS) mechanization to fuse INS with other complimentary sensors.
Through formulating the attitude, velocity and position as one group state of
group of double direct spatial isometries, the transformed INS mechanization
has proven to be group affine, which opens door to log-linearity and filtering
consistency. In order to make use of the transformed INS mechanization in
inertial based applications, both the right and left error state models are
derived. The INS/GPS and INS/Odometer integration are investigated as two
representatives of inertial based applications. Some application aspects of the
derived error state models in the two applications are presented, which include
how to select the error state model, initialization of the SE2(3) based error
state covariance and feedback correction corresponding to the error state
definitions. Land vehicle experiments are conducted to evaluate the performance
of the derived error state models. It is shown that the most striking
superiority of using the derived error state models is their ability to handle
the large initial attitude misalignments, which is just the result of
log-linearity property of the derived error state models. Therefore, the
derived error state models can be used in the so-called attitude alignment for
the two applications.
</p>
<a href="http://arxiv.org/abs/2103.02229" target="_blank">arXiv:2103.02229</a> [<a href="http://arxiv.org/pdf/2103.02229" target="_blank">pdf</a>]

<h2>FFB6D: A Full Flow Bidirectional Fusion Network for 6D Pose Estimation. (arXiv:2103.02242v1 [cs.CV])</h2>
<h3>Yisheng He, Haibin Huang, Haoqiang Fan, Qifeng Chen, Jian Sun</h3>
<p>In this work, we present FFB6D, a Full Flow Bidirectional fusion network
designed for 6D pose estimation from a single RGBD image. Our key insight is
that appearance information in the RGB image and geometry information from the
depth image are two complementary data sources, and it still remains unknown
how to fully leverage them. Towards this end, we propose FFB6D, which learns to
combine appearance and geometry information for representation learning as well
as output representation selection. Specifically, at the representation
learning stage, we build bidirectional fusion modules in the full flow of the
two networks, where fusion is applied to each encoding and decoding layer. In
this way, the two networks can leverage local and global complementary
information from the other one to obtain better representations. Moreover, at
the output representation stage, we designed a simple but effective 3D
keypoints selection algorithm considering the texture and geometry information
of objects, which simplifies keypoint localization for precise pose estimation.
Experimental results show that our method outperforms the state-of-the-art by
large margins on several benchmarks. Code and video are available at
\url{https://github.com/ethnhe/FFB6D.git}.
</p>
<a href="http://arxiv.org/abs/2103.02242" target="_blank">arXiv:2103.02242</a> [<a href="http://arxiv.org/pdf/2103.02242" target="_blank">pdf</a>]

<h2>MotionRNN: A Flexible Model for Video Prediction with Spacetime-Varying Motions. (arXiv:2103.02243v1 [cs.CV])</h2>
<h3>Haixu Wu, Zhiyu Yao, Mingsheng Long, Jianmin Wan</h3>
<p>This paper tackles video prediction from a new dimension of predicting
spacetime-varying motions that are incessantly changing across both space and
time. Prior methods mainly capture the temporal state transitions but overlook
the complex spatiotemporal variations of the motion itself, making them
difficult to adapt to ever-changing motions. We observe that physical world
motions can be decomposed into transient variation and motion trend, while the
latter can be regarded as the accumulation of previous motions. Thus,
simultaneously capturing the transient variation and the motion trend is the
key to make spacetime-varying motions more predictable. Based on these
observations, we propose the MotionRNN framework, which can capture the complex
variations within motions and adapt to spacetime-varying scenarios. MotionRNN
has two main contributions. The first is that we design the MotionGRU unit,
which can model the transient variation and motion trend in a unified way. The
second is that we apply the MotionGRU to RNN-based predictive models and
indicate a new flexible video prediction architecture with a Motion Highway
that can significantly improve the ability to predict changeable motions and
avoid motion vanishing for stacked multiple-layer predictive models. With high
flexibility, this framework can adapt to a series of models for deterministic
spatiotemporal prediction. Our MotionRNN can yield significant improvements on
three challenging benchmarks for video prediction with spacetime-varying
motions.
</p>
<a href="http://arxiv.org/abs/2103.02243" target="_blank">arXiv:2103.02243</a> [<a href="http://arxiv.org/pdf/2103.02243" target="_blank">pdf</a>]

<h2>Unsupervised Vehicle Re-Identification via Self-supervised Metric Learning using Feature Dictionary. (arXiv:2103.02250v1 [cs.CV])</h2>
<h3>Jongmin Yu, Hyeontaek Oh</h3>
<p>The key challenge of unsupervised vehicle re-identification (Re-ID) is
learning discriminative features from unlabelled vehicle images. Numerous
methods using domain adaptation have achieved outstanding performance, but
those methods still need a labelled dataset as a source domain. This paper
addresses an unsupervised vehicle Re-ID method, which no need any types of a
labelled dataset, through a Self-supervised Metric Learning (SSML) based on a
feature dictionary. Our method initially extracts features from vehicle images
and stores them in a dictionary. Thereafter, based on the dictionary, the
proposed method conducts dictionary-based positive label mining (DPLM) to
search for positive labels. Pair-wise similarity, relative-rank consistency,
and adjacent feature distribution similarity are jointly considered to find
images that may belong to the same vehicle of a given probe image. The results
of DPLM are applied to dictionary-based triplet loss (DTL) to improve the
discriminativeness of learnt features and to refine the quality of the results
of DPLM progressively. The iterative process with DPLM and DTL boosts the
performance of unsupervised vehicle Re-ID. Experimental results demonstrate the
effectiveness of the proposed method by producing promising vehicle Re-ID
performance without a pre-labelled dataset. The source code for this paper is
publicly available on `https://github.com/andreYoo/VeRI_SSML_FD.git'.
</p>
<a href="http://arxiv.org/abs/2103.02250" target="_blank">arXiv:2103.02250</a> [<a href="http://arxiv.org/pdf/2103.02250" target="_blank">pdf</a>]

<h2>LiDAR-based Recurrent 3D Semantic Segmentation with Temporal Memory Alignment. (arXiv:2103.02263v1 [cs.CV])</h2>
<h3>Fabian Duerr, Mario Pfaller, Hendrik Weigel, Juergen Beyerer</h3>
<p>Understanding and interpreting a 3d environment is a key challenge for
autonomous vehicles. Semantic segmentation of 3d point clouds combines 3d
information with semantics and thereby provides a valuable contribution to this
task. In many real-world applications, point clouds are generated by lidar
sensors in a consecutive fashion. Working with a time series instead of single
and independent frames enables the exploitation of temporal information. We
therefore propose a recurrent segmentation architecture (RNN), which takes a
single range image frame as input and exploits recursively aggregated temporal
information. An alignment strategy, which we call Temporal Memory Alignment,
uses ego motion to temporally align the memory between consecutive frames in
feature space. A Residual Network and ConvGRU are investigated for the memory
update. We demonstrate the benefits of the presented approach on two
large-scale datasets and compare it to several stateof-the-art methods. Our
approach ranks first on the SemanticKITTI multiple scan benchmark and achieves
state-of-the-art performance on the single scan benchmark. In addition, the
evaluation shows that the exploitation of temporal information significantly
improves segmentation results compared to a single frame approach.
</p>
<a href="http://arxiv.org/abs/2103.02263" target="_blank">arXiv:2103.02263</a> [<a href="http://arxiv.org/pdf/2103.02263" target="_blank">pdf</a>]

<h2>ID-Unet: Iterative Soft and Hard Deformation for View Synthesis. (arXiv:2103.02264v1 [cs.CV])</h2>
<h3>Mingyu Yin, Li Sun, Qingli Li</h3>
<p>View synthesis is usually done by an autoencoder, in which the encoder maps a
source view image into a latent content code, and the decoder transforms it
into a target view image according to the condition. However, the source
contents are often not well kept in this setting, which leads to unnecessary
changes during the view translation. Although adding skipped connections, like
Unet, alleviates the problem, but it often causes the failure on the view
conformity. This paper proposes a new architecture by performing the
source-to-target deformation in an iterative way. Instead of simply
incorporating the features from multiple layers of the encoder, we design soft
and hard deformation modules, which warp the encoder features to the target
view at different resolutions, and give results to the decoder to complement
the details. Particularly, the current warping flow is not only used to align
the feature of the same resolution, but also as an approximation to coarsely
deform the high resolution feature. Then the residual flow is estimated and
applied in the high resolution, so that the deformation is built up in the
coarse-to-fine fashion. To better constrain the model, we synthesize a rough
target view image based on the intermediate flows and their warped features.
The extensive ablation studies and the final results on two different data sets
show the effectiveness of the proposed model.
</p>
<a href="http://arxiv.org/abs/2103.02264" target="_blank">arXiv:2103.02264</a> [<a href="http://arxiv.org/pdf/2103.02264" target="_blank">pdf</a>]

<h2>Motion Classification and Height Estimation of Pedestrians Using Sparse Radar Data. (arXiv:2103.02278v1 [cs.CV])</h2>
<h3>Markus Horn, Ole Schumann, Markus Hahn, J&#xfc;rgen Dickmann, Klaus Dietmayer</h3>
<p>A complete overview of the surrounding vehicle environment is important for
driver assistance systems and highly autonomous driving. Fusing results of
multiple sensor types like camera, radar and lidar is crucial for increasing
the robustness. The detection and classification of objects like cars, bicycles
or pedestrians has been analyzed in the past for many sensor types. Beyond
that, it is also helpful to refine these classes and distinguish for example
between different pedestrian types or activities. This task is usually
performed on camera data, though recent developments are based on radar
spectrograms. However, for most automotive radar systems, it is only possible
to obtain radar targets instead of the original spectrograms. This work
demonstrates that it is possible to estimate the body height of walking
pedestrians using 2D radar targets. Furthermore, different pedestrian motion
types are classified.
</p>
<a href="http://arxiv.org/abs/2103.02278" target="_blank">arXiv:2103.02278</a> [<a href="http://arxiv.org/pdf/2103.02278" target="_blank">pdf</a>]

<h2>K-means Segmentation Based-on Lab Color Space for Embryo Egg Detection. (arXiv:2103.02288v1 [cs.CV])</h2>
<h3>Shoffan Saifullah</h3>
<p>The hatching process also influences the success of hatching eggs beside the
initial egg factor. So that the results have a large percentage of hatching, it
is necessary to check the development of the embryo at the beginning of the
hatching. This process aims to sort eggs that have embryos to remain hatched
until the end. Maximum checking is done the first week in the hatching period.
This study aims to detect the presence of embryos in eggs. Detection of the
existence of embryos is processed using segmentation. Egg images are segmented
using the K-means algorithm based on Lab color images. The results of the
images acquisition are converted into Lab color space images. The results of
Lab color space images are processed using K-means for each color. The K-means
process uses cluster k=3, where this cluster divided the image into three
parts, namely background, eggs, and yolk eggs. Yolk eggs are part of eggs that
have embryonic characteristics. This study applies the concept of color in the
initial segmentation and grayscale in the final stages. The results of the
initial phase show that the image segmentation results using k-means clustering
based on Lab color space provide a grouping of three parts. At the grayscale
image processing stage, the results of color image segmentation are processed
with grayscaling, image enhancement, and morphology. Thus, it seems clear that
the yolk segmented shows the presence of egg embryos. Based on this process and
results, K-means segmentation based on Lab color space can be used for the
initial stages of the embryo detection process. The evaluation uses MSE and
MSSIM, with values of 0.0486 and 0.9979; this can be used as a reference that
the results obtained can indicate the detection of embryos in egg yolk.
</p>
<a href="http://arxiv.org/abs/2103.02288" target="_blank">arXiv:2103.02288</a> [<a href="http://arxiv.org/pdf/2103.02288" target="_blank">pdf</a>]

<h2>Domain and View-point Agnostic Hand Action Recognition. (arXiv:2103.02303v1 [cs.CV])</h2>
<h3>Alberto Sabater, I&#xf1;igo Alonso, Luis Montesano, Ana C. Murillo</h3>
<p>Hand action recognition is a special case of human action recognition with
applications in human robot interaction, virtual reality or life-logging
systems. Building action classifiers that are useful to recognize such
heterogeneous set of activities is very challenging. There are very subtle
changes across different actions from a given application but also large
variations across domains (e.g. virtual reality vs life-logging). This work
introduces a novel skeleton-based hand motion representation model that tackles
this problem. The framework we propose is agnostic to the application domain or
camera recording view-point. We demonstrate the performance of our proposed
motion representation model both working for a single specific domain
(intra-domain action classification) and working for different unseen domains
(cross-domain action classification). For the intra-domain case, our approach
gets better or similar performance than current state-of-the-art methods on
well-known hand action recognition benchmarks. And when performing cross-domain
hand action recognition (i.e., training our motion representation model in
frontal-view recordings and testing it both for egocentric and third-person
views), our approach achieves comparable results to the state-of-the-art
methods that are trained intra-domain.
</p>
<a href="http://arxiv.org/abs/2103.02303" target="_blank">arXiv:2103.02303</a> [<a href="http://arxiv.org/pdf/2103.02303" target="_blank">pdf</a>]

<h2>Cooking Object's State Identification Without Using Pretrained Model. (arXiv:2103.02305v1 [cs.CV])</h2>
<h3>Md Sadman Sakib</h3>
<p>Recently, Robotic Cooking has been a very promising field. To execute a
recipe, a robot has to recognize different objects and their states. Contrary
to object recognition, state identification has not been explored that much.
But it is very important because different recipe might require different state
of an object. Moreover, robotic grasping depends on the state. Pretrained model
usually perform very well in this type of tests. Our challenge was to handle
this problem without using any pretrained model. In this paper, we have
proposed a CNN and trained it from scratch. The model is trained and tested on
the dataset from cooking state recognition challenge. We have also evaluated
the performance of our network from various perspective. Our model achieves
65.8% accuracy on the unseen test dataset.
</p>
<a href="http://arxiv.org/abs/2103.02305" target="_blank">arXiv:2103.02305</a> [<a href="http://arxiv.org/pdf/2103.02305" target="_blank">pdf</a>]

<h2>Reinforcement Learning Control of a Forestry Crane Manipulator. (arXiv:2103.02315v1 [cs.RO])</h2>
<h3>Jennifer Andersson, Kenneth Bodin, Daniel Lindmark, Martin Servin, Erik Wallin</h3>
<p>Forestry machines are heavy vehicles performing complex manipulation tasks in
unstructured production forest environments. Together with the complex dynamics
of the on-board hydraulically actuated cranes, the rough forest terrains have
posed a particular challenge in forestry automation. In this study, the
feasibility of applying reinforcement learning control to forestry crane
manipulators is investigated in a simulated environment. Our results show that
it is possible to learn successful actuator-space control policies for energy
efficient log grasping by invoking a simple curriculum in a deep reinforcement
learning setup. Given the pose of the selected logs, our best control policy
reaches a grasping success rate of 97%. Including an energy-optimization goal
in the reward function, the energy consumption is significantly reduced
compared to control policies learned without incentive for energy optimization,
while the increase in cycle time is marginal. The energy-optimization effects
can be observed in the overall smoother motion and acceleration profiles during
crane manipulation.
</p>
<a href="http://arxiv.org/abs/2103.02315" target="_blank">arXiv:2103.02315</a> [<a href="http://arxiv.org/pdf/2103.02315" target="_blank">pdf</a>]

<h2>General Instance Distillation for Object Detection. (arXiv:2103.02340v1 [cs.CV])</h2>
<h3>Xing Dai, Zeren Jiang, Zhao Wu, Yiping Bao, Zhicheng Wang, Si Liu, Erjin Zhou</h3>
<p>In recent years, knowledge distillation has been proved to be an effective
solution for model compression. This approach can make lightweight student
models acquire the knowledge extracted from cumbersome teacher models. However,
previous distillation methods of detection have weak generalization for
different detection frameworks and rely heavily on ground truth (GT), ignoring
the valuable relation information between instances. Thus, we propose a novel
distillation method for detection tasks based on discriminative instances
without considering the positive or negative distinguished by GT, which is
called general instance distillation (GID). Our approach contains a general
instance selection module (GISM) to make full use of feature-based,
relation-based and response-based knowledge for distillation. Extensive results
demonstrate that the student model achieves significant AP improvement and even
outperforms the teacher in various detection frameworks. Specifically,
RetinaNet with ResNet-50 achieves 39.1% in mAP with GID on COCO dataset, which
surpasses the baseline 36.2% by 2.9%, and even better than the ResNet-101 based
teacher model with 38.1% AP.
</p>
<a href="http://arxiv.org/abs/2103.02340" target="_blank">arXiv:2103.02340</a> [<a href="http://arxiv.org/pdf/2103.02340" target="_blank">pdf</a>]

<h2>Reinforcement Learning for Orientation Estimation Using Inertial Sensors with Performance Guarantee. (arXiv:2103.02357v1 [cs.RO])</h2>
<h3>Liang Hu, Yujie Tang, Zhipeng Zhou, Wei Pan</h3>
<p>This paper presents a deep reinforcement learning (DRL) algorithm for
orientation estimation using inertial sensors combined with magnetometer. The
Lyapunov method in control theory is employed to prove the convergence of
orientation estimation errors. Based on the theoretical results, the estimator
gains and a Lyapunov function are parametrized by deep neural networks and
learned from samples. The DRL estimator is compared with three well-known
orientation estimation methods on both numerical simulations and real datasets
collected from commercially available sensors. The results show that the
proposed algorithm is superior for arbitrary estimation initialization and can
adapt to very large angular velocities for which other algorithms can be hardly
applicable. To the best of our knowledge, this is the first DRL-based
orientation estimation method with estimation error boundedness guarantee.
</p>
<a href="http://arxiv.org/abs/2103.02357" target="_blank">arXiv:2103.02357</a> [<a href="http://arxiv.org/pdf/2103.02357" target="_blank">pdf</a>]

<h2>FSDR: Frequency Space Domain Randomization for Domain Generalization. (arXiv:2103.02370v1 [cs.CV])</h2>
<h3>Jiaxing Huang, Dayan Guan, Aoran Xiao, Shijian Lu</h3>
<p>Domain generalization aims to learn a generalizable model from a known source
domain for various unknown target domains. It has been studied widely by domain
randomization that transfers source images to different styles in spatial space
for learning domain-agnostic features. However, most existing randomization
uses GANs that often lack of controls and even alter semantic structures of
images undesirably. Inspired by the idea of JPEG that converts spatial images
into multiple frequency components (FCs), we propose Frequency Space Domain
Randomization (FSDR) that randomizes images in frequency space by keeping
domain-invariant FCs (DIFs) and randomizing domain-variant FCs (DVFs) only.
FSDR has two unique features: 1) it decomposes images into DIFs and DVFs which
allows explicit access and manipulation of them and more controllable
randomization; 2) it has minimal effects on semantic structures of images and
domain-invariant features. We examined domain variance and invariance property
of FCs statistically and designed a network that can identify and fuse DIFs and
DVFs dynamically through iterative learning. Extensive experiments over
multiple domain generalizable segmentation tasks show that FSDR achieves
superior segmentation and its performance is even on par with domain adaptation
methods that access target data in training.
</p>
<a href="http://arxiv.org/abs/2103.02370" target="_blank">arXiv:2103.02370</a> [<a href="http://arxiv.org/pdf/2103.02370" target="_blank">pdf</a>]

<h2>Event-based Synthetic Aperture Imaging. (arXiv:2103.02376v1 [cs.CV])</h2>
<h3>Xiang Zhang, Liao Wei, Lei Yu, Wen Yang, Gui-Song Xia</h3>
<p>Synthetic aperture imaging (SAI) is able to achieve the see through effect by
blurring out the off-focus foreground occlusions and reconstructing the
in-focus occluded targets from multi-view images. However, very dense
occlusions and extreme lighting conditions may bring significant disturbances
to SAI based on conventional frame-based cameras, leading to performance
degeneration. To address these problems, we propose a novel SAI system based on
the event camera which can produce asynchronous events with extremely low
latency and high dynamic range. Thus, it can eliminate the interference of
dense occlusions by measuring with almost continuous views, and simultaneously
tackle the over/under exposure problems. To reconstruct the occluded targets,
we propose a hybrid encoder-decoder network composed of spiking neural networks
(SNNs) and convolutional neural networks (CNNs). In the hybrid network, the
spatio-temporal information of the collected events is first encoded by SNN
layers, and then transformed to the visual image of the occluded targets by a
style-transfer CNN decoder. Through experiments, the proposed method shows
remarkable performance in dealing with very dense occlusions and extreme
lighting conditions, and high quality visual images can be reconstructed using
pure event data.
</p>
<a href="http://arxiv.org/abs/2103.02376" target="_blank">arXiv:2103.02376</a> [<a href="http://arxiv.org/pdf/2103.02376" target="_blank">pdf</a>]

<h2>Shape-driven Coordinate Ordering for Star Glyph Sets via Reinforcement Learning. (arXiv:2103.02380v1 [cs.CV])</h2>
<h3>Ruizhen Hu, Bin Chen, Juzhan Xu, Oliver van Kaick, Oliver Deussen, Hui Huang</h3>
<p>We present a neural optimization model trained with reinforcement learning to
solve the coordinate ordering problem for sets of star glyphs. Given a set of
star glyphs associated to multiple class labels, we propose to use shape
context descriptors to measure the perceptual distance between pairs of glyphs,
and use the derived silhouette coefficient to measure the perception of class
separability within the entire set. To find the optimal coordinate order for
the given set, we train a neural network using reinforcement learning to reward
orderings with high silhouette coefficients. The network consists of an encoder
and a decoder with an attention mechanism. The encoder employs a recurrent
neural network (RNN) to encode input shape and class information, while the
decoder together with the attention mechanism employs another RNN to output a
sequence with the new coordinate order. In addition, we introduce a neural
network to efficiently estimate the similarity between shape context
descriptors, which allows to speed up the computation of silhouette
coefficients and thus the training of the axis ordering network. Two user
studies demonstrate that the orders provided by our method are preferred by
users for perceiving class separation. We tested our model on different
settings to show its robustness and generalization abilities and demonstrate
that it allows to order input sets with unseen data size, data dimension, or
number of classes. We also demonstrate that our model can be adapted to
coordinate ordering of other types of plots such as RadViz by replacing the
proposed shape-aware silhouette coefficient with the corresponding quality
metric to guide network training.
</p>
<a href="http://arxiv.org/abs/2103.02380" target="_blank">arXiv:2103.02380</a> [<a href="http://arxiv.org/pdf/2103.02380" target="_blank">pdf</a>]

<h2>Self-Distribution Binary Neural Networks. (arXiv:2103.02394v1 [cs.CV])</h2>
<h3>Ping Xue, Yang Lu, Jingfei Chang, Xing Wei, Zhen Wei</h3>
<p>In this work, we study the binary neural networks (BNNs) of which both the
weights and activations are binary (i.e., 1-bit representation). Feature
representation is critical for deep neural networks, while in BNNs, the
features only differ in signs. Prior work introduces scaling factors into
binary weights and activations to reduce the quantization error and effectively
improves the classification accuracy of BNNs. However, the scaling factors not
only increase the computational complexity of networks, but also make no sense
to the signs of binary features. To this end, Self-Distribution Binary Neural
Network (SD-BNN) is proposed. Firstly, we utilize Activation Self Distribution
(ASD) to adaptively adjust the sign distribution of activations, thereby
improve the sign differences of the outputs of the convolution. Secondly, we
adjust the sign distribution of weights through Weight Self Distribution (WSD)
and then fine-tune the sign distribution of the outputs of the convolution.
Extensive experiments on CIFAR-10 and ImageNet datasets with various network
structures show that the proposed SD-BNN consistently outperforms the
state-of-the-art (SOTA) BNNs (e.g., achieves 92.5% on CIFAR-10 and 66.5% on
ImageNet with ResNet-18) with less computation cost. Code is available at
https://github.com/ pingxue-hfut/SD-BNN.
</p>
<a href="http://arxiv.org/abs/2103.02394" target="_blank">arXiv:2103.02394</a> [<a href="http://arxiv.org/pdf/2103.02394" target="_blank">pdf</a>]

<h2>$S^3$: Learnable Sparse Signal Superdensity for Guided Depth Estimation. (arXiv:2103.02396v1 [cs.CV])</h2>
<h3>Yu-Kai Huang, Yueh-Cheng Liu, Tsung-Han Wu, Hung-Ting Su, Yu-Cheng Chang, Tsung-Lin Tsou, Yu-An Wang, Winston H. Hsu</h3>
<p>Dense Depth estimation plays a key role in multiple applications such as
robotics, 3D reconstruction, and augmented reality. While sparse signal, e.g.,
LiDAR and Radar, has been leveraged as guidance for enhancing dense depth
estimation, the improvement is limited due to its low density and imbalanced
distribution. To maximize the utility from the sparse source, we propose $S^3$
technique, which expands the depth value from sparse cues while estimating the
confidence of expanded region. The proposed $S^3$ can be applied to various
guided depth estimation approaches and trained end-to-end at different stages,
including input, cost volume and output. Extensive experiments demonstrate the
effectiveness, robustness, and flexibility of the $S^3$ technique on LiDAR and
Radar signal.
</p>
<a href="http://arxiv.org/abs/2103.02396" target="_blank">arXiv:2103.02396</a> [<a href="http://arxiv.org/pdf/2103.02396" target="_blank">pdf</a>]

<h2>Multi-attentional Deepfake Detection. (arXiv:2103.02406v1 [cs.CV])</h2>
<h3>Hanqing Zhao, Wenbo Zhou, Dongdong Chen, Tianyi Wei, Weiming Zhang, Nenghai Yu</h3>
<p>Face forgery by deepfake is widely spread over the internet and has raised
severe societal concerns. Recently, how to detect such forgery contents has
become a hot research topic and many deepfake detection methods have been
proposed. Most of them model deepfake detection as a vanilla binary
classification problem, i.e, first use a backbone network to extract a global
feature and then feed it into a binary classifier (real/fake). But since the
difference between the real and fake images in this task is often subtle and
local, we argue this vanilla solution is not optimal. In this paper, we instead
formulate deepfake detection as a fine-grained classification problem and
propose a new multi-attentional deepfake detection network. Specifically, it
consists of three key components: 1) multiple spatial attention heads to make
the network attend to different local parts; 2) textural feature enhancement
block to zoom in the subtle artifacts in shallow features; 3) aggregate the
low-level textural feature and high-level semantic features guided by the
attention maps. Moreover, to address the learning difficulty of this network,
we further introduce a new regional independence loss and an attention guided
data augmentation strategy. Through extensive experiments on different
datasets, we demonstrate the superiority of our method over the vanilla binary
classifier counterparts, and achieve state-of-the-art performance.
</p>
<a href="http://arxiv.org/abs/2103.02406" target="_blank">arXiv:2103.02406</a> [<a href="http://arxiv.org/pdf/2103.02406" target="_blank">pdf</a>]

<h2>Dynamic Fusion Module Evolves Drivable Area and Road Anomaly Detection: A Benchmark and Algorithms. (arXiv:2103.02433v1 [cs.CV])</h2>
<h3>Hengli Wang, Rui Fan, Yuxiang Sun, Ming Liu</h3>
<p>Joint detection of drivable areas and road anomalies is very important for
mobile robots. Recently, many semantic segmentation approaches based on
convolutional neural networks (CNNs) have been proposed for pixel-wise drivable
area and road anomaly detection. In addition, some benchmark datasets, such as
KITTI and Cityscapes, have been widely used. However, the existing benchmarks
are mostly designed for self-driving cars. There lacks a benchmark for ground
mobile robots, such as robotic wheelchairs. Therefore, in this paper, we first
build a drivable area and road anomaly detection benchmark for ground mobile
robots, evaluating the existing state-of-the-art single-modal and data-fusion
semantic segmentation CNNs using six modalities of visual features.
Furthermore, we propose a novel module, referred to as the dynamic fusion
module (DFM), which can be easily deployed in existing data-fusion networks to
fuse different types of visual features effectively and efficiently. The
experimental results show that the transformed disparity image is the most
informative visual feature and the proposed DFM-RTFNet outperforms the
state-of-the-arts. Additionally, our DFM-RTFNet achieves competitive
performance on the KITTI road benchmark. Our benchmark is publicly available at
https://sites.google.com/view/gmrb.
</p>
<a href="http://arxiv.org/abs/2103.02433" target="_blank">arXiv:2103.02433</a> [<a href="http://arxiv.org/pdf/2103.02433" target="_blank">pdf</a>]

<h2>OpenPifPaf: Composite Fields for Semantic Keypoint Detection and Spatio-Temporal Association. (arXiv:2103.02440v1 [cs.CV])</h2>
<h3>Sven Kreiss, Lorenzo Bertoni, Alexandre Alahi</h3>
<p>Many image-based perception tasks can be formulated as detecting, associating
and tracking semantic keypoints, e.g., human body pose estimation and tracking.
In this work, we present a general framework that jointly detects and forms
spatio-temporal keypoint associations in a single stage, making this the first
real-time pose detection and tracking algorithm. We present a generic neural
network architecture that uses Composite Fields to detect and construct a
spatio-temporal pose which is a single, connected graph whose nodes are the
semantic keypoints (e.g., a person's body joints) in multiple frames. For the
temporal associations, we introduce the Temporal Composite Association Field
(TCAF) which requires an extended network architecture and training method
beyond previous Composite Fields. Our experiments show competitive accuracy
while being an order of magnitude faster on multiple publicly available
datasets such as COCO, CrowdPose and the PoseTrack 2017 and 2018 datasets. We
also show that our method generalizes to any class of semantic keypoints such
as car and animal parts to provide a holistic perception framework that is well
suited for urban mobility such as self-driving cars and delivery robots.
</p>
<a href="http://arxiv.org/abs/2103.02440" target="_blank">arXiv:2103.02440</a> [<a href="http://arxiv.org/pdf/2103.02440" target="_blank">pdf</a>]

<h2>Multimodal Scale Consistency and Awareness for Monocular Self-Supervised Depth Estimation. (arXiv:2103.02451v1 [cs.CV])</h2>
<h3>Hemang Chawla, Arnav Varma, Elahe Arani, Bahram Zonooz</h3>
<p>Dense depth estimation is essential to scene-understanding for autonomous
driving. However, recent self-supervised approaches on monocular videos suffer
from scale-inconsistency across long sequences. Utilizing data from the
ubiquitously copresent global positioning systems (GPS), we tackle this
challenge by proposing a dynamically-weighted GPS-to-Scale (g2s) loss to
complement the appearance-based losses. We emphasize that the GPS is needed
only during the multimodal training, and not at inference. The relative
distance between frames captured through the GPS provides a scale signal that
is independent of the camera setup and scene distribution, resulting in richer
learned feature representations. Through extensive evaluation on multiple
datasets, we demonstrate scale-consistent and -aware depth estimation during
inference, improving the performance even when training with low-frequency GPS
data.
</p>
<a href="http://arxiv.org/abs/2103.02451" target="_blank">arXiv:2103.02451</a> [<a href="http://arxiv.org/pdf/2103.02451" target="_blank">pdf</a>]

<h2>Sparsity Aware Normalization for GANs. (arXiv:2103.02458v1 [cs.CV])</h2>
<h3>Idan Kligvasser, Tomer Michaeli</h3>
<p>Generative adversarial networks (GANs) are known to benefit from
regularization or normalization of their critic (discriminator) network during
training. In this paper, we analyze the popular spectral normalization scheme,
find a significant drawback and introduce sparsity aware normalization (SAN), a
new alternative approach for stabilizing GAN training. As opposed to other
normalization methods, our approach explicitly accounts for the sparse nature
of the feature maps in convolutional networks with ReLU activations. We
illustrate the effectiveness of our method through extensive experiments with a
variety of network architectures. As we show, sparsity is particularly dominant
in critics used for image-to-image translation settings. In these cases our
approach improves upon existing methods, in less training epochs and with
smaller capacity networks, while requiring practically no computational
overhead.
</p>
<a href="http://arxiv.org/abs/2103.02458" target="_blank">arXiv:2103.02458</a> [<a href="http://arxiv.org/pdf/2103.02458" target="_blank">pdf</a>]

<h2>Advancing Mixture Models for Least Squares Optimization. (arXiv:2103.02472v1 [cs.RO])</h2>
<h3>Tim Pfeifer, Sven Lange, Peter Protzel</h3>
<p>Gaussian mixtures are a powerful and widely used tool to model non-Gaussian
estimation problems. They are able to describe measurement errors that follow
arbitrary distributions and can represent ambiguity in assignment tasks like
point set registration or tracking. However, using them with common least
squares solvers is still difficult. Existing approaches are either
approximations of the true mixture or prone to convergence issues due to their
strong nonlinearity. We propose a novel least squares representation of a
Gaussian mixture, which is an exact and almost linear model of the
corresponding log-likelihood. Our approach provides an efficient, accurate and
flexible model for many probabilistic estimation problems and can be used as
cost function for least squares solvers. We demonstrate its superior
performance in various Monte Carlo experiments, including different kinds of
point set registration. Our implementation is available as open source code for
the state-of-the-art solvers Ceres and GTSAM.
</p>
<a href="http://arxiv.org/abs/2103.02472" target="_blank">arXiv:2103.02472</a> [<a href="http://arxiv.org/pdf/2103.02472" target="_blank">pdf</a>]

<h2>Cellular Formation Maintenance and Collision Avoidance Using Centroid-Based Point Set Registration in a Swarm of Drones. (arXiv:2103.02480v1 [cs.RO])</h2>
<h3>Jawad N. Yasin, Huma Mahboob, Mohammad-Hashem Haghbayan, Muhammad Mehboob Yasin, Juha Plosila</h3>
<p>This work focuses on low-energy collision avoidance and formation maintenance
in autonomous swarms of drones. Here, the two main problems are: 1) how to
avoid collisions by temporarily breaking the formation, i.e., collision
avoidance reformation, and 2) how do such reformation while minimizing the
deviation resulting in minimization of the overall time and energy consumption
of the drones. To address the first question, we use cellular automata based
technique to find an efficient formation that avoids the obstacle while
minimizing the time and energy. Concerning the second question, a near-optimal
reformation of the swarm after successful collision avoidance is achieved by
applying a temperature function reduction technique, originally used in the
point set registration process. The goal of the reformation process is to
remove the disturbance while minimizing the overall time it takes for the swarm
to reach the destination and consequently reducing the energy consumption
required by this operation. To measure the degree of formation disturbance due
to collision avoidance, deviation of the centroid of the swarm formation is
used, inspired by the concept of the center of mass in classical mechanics.
Experimental results show the efficiency of the proposed technique, in terms of
performance and energy.
</p>
<a href="http://arxiv.org/abs/2103.02480" target="_blank">arXiv:2103.02480</a> [<a href="http://arxiv.org/pdf/2103.02480" target="_blank">pdf</a>]

<h2>DeepFN: Towards Generalizable Facial Action Unit Recognition with Deep Face Normalization. (arXiv:2103.02484v1 [cs.CV])</h2>
<h3>Javier Hernandez, Daniel McDuff, Ognjen (Oggi) Rudovic, Alberto Fung, Mary Czerwinski</h3>
<p>Facial action unit recognition has many applications from market research to
psychotherapy and from image captioning to entertainment. Despite its recent
progress, deployment of these models has been impeded due to their limited
generalization to unseen people and demographics. This work conducts an
in-depth analysis of performance across several dimensions: individuals(40
subjects), genders (male and female), skin types (darker and lighter), and
databases (BP4D and DISFA). To help suppress the variance in data, we use the
notion of self-supervised denoising autoencoders to design a method for deep
face normalization(DeepFN) that transfers facial expressions of different
people onto a common facial template which is then used to train and evaluate
facial action recognition models. We show that person-independent models yield
significantly lower performance (55% average F1 and accuracy across 40
subjects) than person-dependent models (60.3%), leading to a generalization gap
of 5.3%. However, normalizing the data with the newly introduced DeepFN
significantly increased the performance of person-independent models (59.6%),
effectively reducing the gap. Similarly, we observed generalization gaps when
considering gender (2.4%), skin type (5.3%), and dataset (9.4%), which were
significantly reduced with the use of DeepFN. These findings represent an
important step towards the creation of more generalizable facial action unit
recognition systems.
</p>
<a href="http://arxiv.org/abs/2103.02484" target="_blank">arXiv:2103.02484</a> [<a href="http://arxiv.org/pdf/2103.02484" target="_blank">pdf</a>]

<h2>Non-local Channel Aggregation Network for Single Image Rain Removal. (arXiv:2103.02488v1 [cs.CV])</h2>
<h3>Zhipeng Su, Yixiong Zhang, Xiao-Ping Zhang, Feng Qi</h3>
<p>Rain streaks showing in images or videos would severely degrade the
performance of computer vision applications. Thus, it is of vital importance to
remove rain streaks and facilitate our vision systems. While recent
convolutinal neural network based methods have shown promising results in
single image rain removal (SIRR), they fail to effectively capture long-range
location dependencies or aggregate convolutional channel information
simultaneously. However, as SIRR is a highly illposed problem, these spatial
and channel information are very important clues to solve SIRR. First, spatial
information could help our model to understand the image context by gathering
long-range dependency location information hidden in the image. Second,
aggregating channels could help our model to concentrate on channels more
related to image background instead of rain streaks. In this paper, we propose
a non-local channel aggregation network (NCANet) to address the SIRR problem.
NCANet models 2D rainy images as sequences of vectors in three directions,
namely vertical direction, transverse direction and channel direction.
Recurrently aggregating information from all three directions enables our model
to capture the long-range dependencies in both channels and spaitials
locations. Extensive experiments on both heavy and light rain image data sets
demonstrate the effectiveness of the proposed NCANet model.
</p>
<a href="http://arxiv.org/abs/2103.02488" target="_blank">arXiv:2103.02488</a> [<a href="http://arxiv.org/pdf/2103.02488" target="_blank">pdf</a>]

<h2>Vanishing Twin GAN: How training a weak Generative Adversarial Network can improve semi-supervised image classification. (arXiv:2103.02496v1 [cs.CV])</h2>
<h3>Saman Motamed, Farzad Khalvati</h3>
<p>Generative Adversarial Networks can learn the mapping of random noise to
realistic images in a semi-supervised framework. This mapping ability can be
used for semi-supervised image classification to detect images of an unknown
class where there is no training data to be used for supervised classification.
However, if the unknown class shares similar characteristics to the known
class(es), GANs can learn to generalize and generate images that look like both
classes. This generalization ability can hinder the classification performance.
In this work, we propose the Vanishing Twin GAN. By training a weak GAN and
using its generated output image parallel to the regular GAN, the Vanishing
Twin training improves semi-supervised image classification where image
similarity can hurt classification tasks.
</p>
<a href="http://arxiv.org/abs/2103.02496" target="_blank">arXiv:2103.02496</a> [<a href="http://arxiv.org/pdf/2103.02496" target="_blank">pdf</a>]

<h2>EllipsoidNet: Ellipsoid Representation for Point Cloud Classification and Segmentation. (arXiv:2103.02517v1 [cs.CV])</h2>
<h3>Yecheng Lyu, Xinming Huang, Ziming Zhang</h3>
<p>Point cloud patterns are hard to learn because of the implicit local geometry
features among the orderless points. In recent years, point cloud
representation in 2D space has attracted increasing research interest since it
exposes the local geometry features in a 2D space. By projecting those points
to a 2D feature map, the relationship between points is inherited in the
context between pixels, which are further extracted by a 2D convolutional
neural network. However, existing 2D representing methods are either accuracy
limited or time-consuming. In this paper, we propose a novel 2D representation
method that projects a point cloud onto an ellipsoid surface space, where local
patterns are well exposed in ellipsoid-level and point-level. Additionally, a
novel convolutional neural network named EllipsoidNet is proposed to utilize
those features for point cloud classification and segmentation applications.
The proposed methods are evaluated in ModelNet40 and ShapeNet benchmarks, where
the advantages are clearly shown over existing 2D representation methods.
</p>
<a href="http://arxiv.org/abs/2103.02517" target="_blank">arXiv:2103.02517</a> [<a href="http://arxiv.org/pdf/2103.02517" target="_blank">pdf</a>]

<h2>On the role of depth predictions for 3D human pose estimation. (arXiv:2103.02521v1 [cs.CV])</h2>
<h3>Alec Diaz-Arias, Mitchell Messmore, Dmitriy Shin, Stephen Baek</h3>
<p>Following the successful application of deep convolutional neural networks to
2d human pose estimation, the next logical problem to solve is 3d human pose
estimation from monocular images. While previous solutions have shown some
success, they do not fully utilize the depth information from the 2d inputs.
With the goal of addressing this depth ambiguity, we build a system that takes
2d joint locations as input along with their estimated depth value and predicts
their 3d positions in camera coordinates. Given the inherent noise and
inaccuracy from estimating depth maps from monocular images, we perform an
extensive statistical analysis showing that given this noise there is still a
statistically significant correlation between the predicted depth values and
the third coordinate of camera coordinates. We further explain how the
state-of-the-art results we achieve on the H3.6M validation set are due to the
additional input of depth. Notably, our results are produced on neural network
that accepts a low dimensional input and be integrated into a real-time system.
Furthermore, our system can be combined with an off-the-shelf 2d pose detector
and a depth map predictor to perform 3d pose estimation in the wild.
</p>
<a href="http://arxiv.org/abs/2103.02521" target="_blank">arXiv:2103.02521</a> [<a href="http://arxiv.org/pdf/2103.02521" target="_blank">pdf</a>]

<h2>Style-based Point Generator with Adversarial Rendering for Point Cloud Completion. (arXiv:2103.02535v1 [cs.CV])</h2>
<h3>Chulin Xie, Chuxin Wang, Bo Zhang, Hao Yang, Dong Chen, Fang Wen</h3>
<p>In this paper, we proposed a novel Style-based Point Generator with
Adversarial Rendering (SpareNet) for point cloud completion. Firstly, we
present the channel-attentive EdgeConv to fully exploit the local structures as
well as the global shape in point features. Secondly, we observe that the
concatenation manner used by vanilla foldings limits its potential of
generating a complex and faithful shape. Enlightened by the success of
StyleGAN, we regard the shape feature as style code that modulates the
normalization layers during the folding, which considerably enhances its
capability. Thirdly, we realize that existing point supervisions, e.g., Chamfer
Distance or Earth Mover's Distance, cannot faithfully reflect the perceptual
quality of the reconstructed points. To address this, we propose to project the
completed points to depth maps with a differentiable renderer and apply
adversarial training to advocate the perceptual realism under different
viewpoints. Comprehensive experiments on ShapeNet and KITTI prove the
effectiveness of our method, which achieves state-of-the-art quantitative
performance while offering superior visual quality.
</p>
<a href="http://arxiv.org/abs/2103.02535" target="_blank">arXiv:2103.02535</a> [<a href="http://arxiv.org/pdf/2103.02535" target="_blank">pdf</a>]

<h2>Enabling Visual Action Planning for Object Manipulation through Latent Space Roadmap. (arXiv:2103.02554v1 [cs.RO])</h2>
<h3>Martina Lippi, Petra Poklukar, Michael C. Welle, Anastasiia Varava, Hang Yin, Alessandro Marino, Danica Kragic</h3>
<p>We present a framework for visual action planning of complex manipulation
tasks with high-dimensional state spaces, focusing on manipulation of
deformable objects. We propose a Latent Space Roadmap (LSR) for task planning,
a graph-based structure capturing globally the system dynamics in a
low-dimensional latent space. Our framework consists of three parts: (1) a
Mapping Module (MM) that maps observations, given in the form of images, into a
structured latent space extracting the respective states, that generates
observations from the latent states, (2) the LSR which builds and connects
clusters containing similar states in order to find the latent plans between
start and goal states extracted by MM, and (3) the Action Proposal Module that
complements the latent plan found by the LSR with the corresponding actions. We
present a thorough investigation of our framework on two simulated box stacking
tasks and a folding task executed on a real robot.
</p>
<a href="http://arxiv.org/abs/2103.02554" target="_blank">arXiv:2103.02554</a> [<a href="http://arxiv.org/pdf/2103.02554" target="_blank">pdf</a>]

<h2>ICAM-reg: Interpretable Classification and Regression with Feature Attribution for Mapping Neurological Phenotypes in Individual Scans. (arXiv:2103.02561v1 [cs.CV])</h2>
<h3>Cher Bass, Mariana da Silva, Carole Sudre, Logan Z. J. Williams, Petru-Daniel Tudosiu, Fidel Alfaro-Almagro, Sean P. Fitzgibbon, Matthew F. Glasser, Stephen M. Smith, Emma C. Robinson</h3>
<p>An important goal of medical imaging is to be able to precisely detect
patterns of disease specific to individual scans; however, this is challenged
in brain imaging by the degree of heterogeneity of shape and appearance.
Traditional methods, based on image registration to a global template,
historically fail to detect variable features of disease, as they utilise
population-based analyses, suited primarily to studying group-average effects.
In this paper we therefore take advantage of recent developments in generative
deep learning to develop a method for simultaneous classification, or
regression, and feature attribution (FA). Specifically, we explore the use of a
VAE-GAN translation network called ICAM, to explicitly disentangle class
relevant features from background confounds for improved interpretability and
regression of neurological phenotypes. We validate our method on the tasks of
Mini-Mental State Examination (MMSE) cognitive test score prediction for the
Alzheimer's Disease Neuroimaging Initiative (ADNI) cohort, as well as brain age
prediction, for both neurodevelopment and neurodegeneration, using the
developing Human Connectome Project (dHCP) and UK Biobank datasets. We show
that the generated FA maps can be used to explain outlier predictions and
demonstrate that the inclusion of a regression module improves the
disentanglement of the latent space. Our code is freely available on Github
https://github.com/CherBass/ICAM.
</p>
<a href="http://arxiv.org/abs/2103.02561" target="_blank">arXiv:2103.02561</a> [<a href="http://arxiv.org/pdf/2103.02561" target="_blank">pdf</a>]

<h2>House-GAN++: Generative Adversarial Layout Refinement Networks. (arXiv:2103.02574v1 [cs.CV])</h2>
<h3>Nelson Nauata, Sepidehsadat Hosseini, Kai-Hung Chang, Hang Chu, Chin-Yi Cheng, Yasutaka Furukawa</h3>
<p>This paper proposes a novel generative adversarial layout refinement network
for automated floorplan generation. Our architecture is an integration of a
graph-constrained relational GAN and a conditional GAN, where a previously
generated layout becomes the next input constraint, enabling iterative
refinement. A surprising discovery of our research is that a simple
non-iterative training process, dubbed component-wise GT-conditioning, is
effective in learning such a generator. The iterative generator also creates a
new opportunity in further improving a metric of choice via meta-optimization
techniques by controlling when to pass which input constraints during iterative
layout refinement. Our qualitative and quantitative evaluation based on the
three standard metrics demonstrate that the proposed system makes significant
improvements over the current state-of-the-art, even competitive against the
ground-truth floorplans, designed by professional architects.
</p>
<a href="http://arxiv.org/abs/2103.02574" target="_blank">arXiv:2103.02574</a> [<a href="http://arxiv.org/pdf/2103.02574" target="_blank">pdf</a>]

<h2>Simulating time to event prediction with spatiotemporal echocardiography deep learning. (arXiv:2103.02583v1 [cs.CV])</h2>
<h3>Rohan Shad, Nicolas Quach, Robyn Fong, Patpilai Kasinpila, Cayley Bowles, Kate M. Callon, Michelle C. Li, Jeffrey Teuteberg, John P. Cunningham, Curtis P. Langlotz, William Hiesinger</h3>
<p>Integrating methods for time-to-event prediction with diagnostic imaging
modalities is of considerable interest, as accurate estimates of survival
requires accounting for censoring of individuals within the observation period.
New methods for time-to-event prediction have been developed by extending the
cox-proportional hazards model with neural networks. In this paper, to explore
the feasibility of these methods when applied to deep learning with
echocardiography videos, we utilize the Stanford EchoNet-Dynamic dataset with
over 10,000 echocardiograms, and generate simulated survival datasets based on
the expert annotated ejection fraction readings. By training on just the
simulated survival outcomes, we show that spatiotemporal convolutional neural
networks yield accurate survival estimates.
</p>
<a href="http://arxiv.org/abs/2103.02583" target="_blank">arXiv:2103.02583</a> [<a href="http://arxiv.org/pdf/2103.02583" target="_blank">pdf</a>]

<h2>Cross-View Regularization for Domain Adaptive Panoptic Segmentation. (arXiv:2103.02584v1 [cs.CV])</h2>
<h3>Jiaxing Huang, Dayan Guan, Aoran Xiao, Shijian Lu</h3>
<p>Panoptic segmentation unifies semantic segmentation and instance segmentation
which has been attracting increasing attention in recent years. However, most
existing research was conducted under a supervised learning setup whereas
unsupervised domain adaptive panoptic segmentation which is critical in
different tasks and applications is largely neglected. We design a domain
adaptive panoptic segmentation network that exploits inter-style consistency
and inter-task regularization for optimal domain adaptive panoptic
segmentation. The inter-style consistency leverages geometric invariance across
the same image of the different styles which fabricates certain
self-supervisions to guide the network to learn domain-invariant features. The
inter-task regularization exploits the complementary nature of instance
segmentation and semantic segmentation and uses it as a constraint for better
feature alignment across domains. Extensive experiments over multiple domain
adaptive panoptic segmentation tasks (e.g., synthetic-to-real and real-to-real)
show that our proposed network achieves superior segmentation performance as
compared with the state-of-the-art.
</p>
<a href="http://arxiv.org/abs/2103.02584" target="_blank">arXiv:2103.02584</a> [<a href="http://arxiv.org/pdf/2103.02584" target="_blank">pdf</a>]

<h2>Neural 3D Video Synthesis. (arXiv:2103.02597v1 [cs.CV])</h2>
<h3>Tianye Li, Mira Slavcheva, Michael Zollhoefer, Simon Green, Christoph Lassner, Changil Kim, Tanner Schmidt, Steven Lovegrove, Michael Goesele, Zhaoyang Lv</h3>
<p>We propose a novel approach for 3D video synthesis that is able to represent
multi-view video recordings of a dynamic real-world scene in a compact, yet
expressive representation that enables high-quality view synthesis and motion
interpolation. Our approach takes the high quality and compactness of static
neural radiance fields in a new direction: to a model-free, dynamic setting. At
the core of our approach is a novel time-conditioned neural radiance fields
that represents scene dynamics using a set of compact latent codes. To exploit
the fact that changes between adjacent frames of a video are typically small
and locally consistent, we propose two novel strategies for efficient training
of our neural network: 1) An efficient hierarchical training scheme, and 2) an
importance sampling strategy that selects the next rays for training based on
the temporal variation of the input videos. In combination, these two
strategies significantly boost the training speed, lead to fast convergence of
the training process, and enable high quality results. Our learned
representation is highly compact and able to represent a 10 second 30 FPS
multi-view video recording by 18 cameras with a model size of just 28MB. We
demonstrate that our method can render high-fidelity wide-angle novel views at
over 1K resolution, even for highly complex and dynamic scenes. We perform an
extensive qualitative and quantitative evaluation that shows that our approach
outperforms the current state of the art. We include additional video and
information at: https://neural-3d-video.github.io/
</p>
<a href="http://arxiv.org/abs/2103.02597" target="_blank">arXiv:2103.02597</a> [<a href="http://arxiv.org/pdf/2103.02597" target="_blank">pdf</a>]

<h2>Towards Open World Object Detection. (arXiv:2103.02603v1 [cs.CV])</h2>
<h3>K J Joseph, Salman Khan, Fahad Shahbaz Khan, Vineeth N Balasubramanian</h3>
<p>Humans have a natural instinct to identify unknown object instances in their
environments. The intrinsic curiosity about these unknown instances aids in
learning about them, when the corresponding knowledge is eventually available.
This motivates us to propose a novel computer vision problem called: `Open
World Object Detection', where a model is tasked to: 1) identify objects that
have not been introduced to it as `unknown', without explicit supervision to do
so, and 2) incrementally learn these identified unknown categories without
forgetting previously learned classes, when the corresponding labels are
progressively received. We formulate the problem, introduce a strong evaluation
protocol and provide a novel solution, which we call ORE: Open World Object
Detector, based on contrastive clustering and energy based unknown
identification. Our experimental evaluation and ablation studies analyze the
efficacy of ORE in achieving Open World objectives. As an interesting
by-product, we find that identifying and characterizing unknown instances helps
to reduce confusion in an incremental object detection setting, where we
achieve state-of-the-art performance, with no extra methodological effort. We
hope that our work will attract further research into this newly identified,
yet crucial research direction.
</p>
<a href="http://arxiv.org/abs/2103.02603" target="_blank">arXiv:2103.02603</a> [<a href="http://arxiv.org/pdf/2103.02603" target="_blank">pdf</a>]

<h2>Collaborative Global-Local Networks for Memory-Efficient Segmentation of Ultra-High Resolution Images. (arXiv:1905.06368v3 [cs.CV] UPDATED)</h2>
<h3>Wuyang Chen, Ziyu Jiang, Zhangyang Wang, Kexin Cui, Xiaoning Qian</h3>
<p>Segmentation of ultra-high resolution images is increasingly demanded, yet
poses significant challenges for algorithm efficiency, in particular
considering the (GPU) memory limits. Current approaches either downsample an
ultra-high resolution image or crop it into small patches for separate
processing. In either way, the loss of local fine details or global contextual
information results in limited segmentation accuracy. We propose collaborative
Global-Local Networks (GLNet) to effectively preserve both global and local
information in a highly memory-efficient manner. GLNet is composed of a global
branch and a local branch, taking the downsampled entire image and its cropped
local patches as respective inputs. For segmentation, GLNet deeply fuses
feature maps from two branches, capturing both the high-resolution fine
structures from zoomed-in local patches and the contextual dependency from the
downsampled input. To further resolve the potential class imbalance problem
between background and foreground regions, we present a coarse-to-fine variant
of GLNet, also being memory-efficient. Extensive experiments and analyses have
been performed on three real-world ultra-high aerial and medical image datasets
(resolution up to 30 million pixels). With only one single 1080Ti GPU and less
than 2GB memory used, our GLNet yields high-quality segmentation results and
achieves much more competitive accuracy-memory usage trade-offs compared to
state-of-the-arts.
</p>
<a href="http://arxiv.org/abs/1905.06368" target="_blank">arXiv:1905.06368</a> [<a href="http://arxiv.org/pdf/1905.06368" target="_blank">pdf</a>]

<h2>The impact of catastrophic collisions and collision avoidance on a swarming behavior. (arXiv:1910.06412v5 [cs.RO] UPDATED)</h2>
<h3>Chris Taylor, Cameron Nowzari</h3>
<p>Swarms of autonomous agents are useful in many applications due to their
ability to accomplish tasks in a decentralized manner, making them more robust
to failures. Due to the difficulty in running experiments with large numbers of
hardware agents, researchers often make simplifying assumptions and remove
constraints that might be present in a real swarm deployment. While simplifying
away some constraints is tolerable, we feel that two in particular have been
overlooked: one, that agents in a swarm take up physical space, and two, that
agents might be damaged in collisions. Many existing works assume agents have
negligible size or pass through each other with no added penalty. It seems
possible to ignore these constraints using collision avoidance, but we show
using an illustrative example that this is easier said than done. In
particular, we show that collision avoidance can interfere with the intended
swarming behavior and significant parameter tuning is necessary to ensure the
behavior emerges as best as possible while collisions are avoided. We compare
four different collision avoidance algorithms, two of which we consider to be
the best decentralized collision avoidance algorithms available. Despite
putting significant effort into tuning each algorithm to perform at its best,
we believe our results show that further research is necessary to develop
swarming behaviors that can achieve their goal while avoiding collisions with
agents of non-negligible volume.
</p>
<a href="http://arxiv.org/abs/1910.06412" target="_blank">arXiv:1910.06412</a> [<a href="http://arxiv.org/pdf/1910.06412" target="_blank">pdf</a>]

<h2>Class Anchor Clustering: a Loss for Distance-based Open Set Recognition. (arXiv:2004.02434v3 [cs.CV] UPDATED)</h2>
<h3>Dimity Miller, Niko S&#xfc;nderhauf, Michael Milford, Feras Dayoub</h3>
<p>In open set recognition, deep neural networks encounter object classes that
were unknown during training. Existing open set classifiers distinguish between
known and unknown classes by measuring distance in a network's logit space,
assuming that known classes cluster closer to the training data than unknown
classes. However, this approach is applied post-hoc to networks trained with
cross-entropy loss, which does not guarantee this clustering behaviour. To
overcome this limitation, we introduce the Class Anchor Clustering (CAC) loss.
CAC is a distance-based loss that explicitly trains known classes to form tight
clusters around anchored class-dependent centres in the logit space. We show
that training with CAC achieves state-of-the-art performance for distance-based
open set classifiers on all six standard benchmark datasets, with a 15.2% AUROC
increase on the challenging TinyImageNet, without sacrificing classification
accuracy. We also show that our anchored class centres achieve higher open set
performance than learnt class centres, particularly on object-based datasets
and large numbers of training classes.
</p>
<a href="http://arxiv.org/abs/2004.02434" target="_blank">arXiv:2004.02434</a> [<a href="http://arxiv.org/pdf/2004.02434" target="_blank">pdf</a>]

<h2>Peer Collaborative Learning for Online Knowledge Distillation. (arXiv:2006.04147v2 [cs.CV] UPDATED)</h2>
<h3>Guile Wu, Shaogang Gong</h3>
<p>Traditional knowledge distillation uses a two-stage training strategy to
transfer knowledge from a high-capacity teacher model to a compact student
model, which relies heavily on the pre-trained teacher. Recent online knowledge
distillation alleviates this limitation by collaborative learning, mutual
learning and online ensembling, following a one-stage end-to-end training
fashion. However, collaborative learning and mutual learning fail to construct
an online high-capacity teacher, whilst online ensembling ignores the
collaboration among branches and its logit summation impedes the further
optimisation of the ensemble teacher. In this work, we propose a novel Peer
Collaborative Learning method for online knowledge distillation, which
integrates online ensembling and network collaboration into a unified
framework. Specifically, given a target network, we construct a multi-branch
network for training, in which each branch is called a peer. We perform random
augmentation multiple times on the inputs to peers and assemble feature
representations outputted from peers with an additional classifier as the peer
ensemble teacher. This helps to transfer knowledge from a high-capacity teacher
to peers, and in turn further optimises the ensemble teacher. Meanwhile, we
employ the temporal mean model of each peer as the peer mean teacher to
collaboratively transfer knowledge among peers, which helps each peer to learn
richer knowledge and facilitates to optimise a more stable model with better
generalisation. Extensive experiments on CIFAR-10, CIFAR-100 and ImageNet show
that the proposed method significantly improves the generalisation of various
backbone networks and outperforms the state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2006.04147" target="_blank">arXiv:2006.04147</a> [<a href="http://arxiv.org/pdf/2006.04147" target="_blank">pdf</a>]

<h2>RP2K: A Large-Scale Retail Product Dataset for Fine-Grained Image Classification. (arXiv:2006.12634v6 [cs.CV] UPDATED)</h2>
<h3>Jingtian Peng, Chang Xiao, Yifan Li</h3>
<p>We introduce RP2K, a new large-scale retail product dataset for fine-grained
image classification. Unlike previous datasets focusing on relatively few
products, we collect more than 500,000 images of retail products on shelves
belonging to 2000 different products. Our dataset aims to advance the research
in retail object recognition, which has massive applications such as automatic
shelf auditing and image-based product information retrieval. Our dataset
enjoys following properties: (1) It is by far the largest scale dataset in
terms of product categories. (2) All images are captured manually in physical
retail stores with natural lightings, matching the scenario of real
applications. (3) We provide rich annotations to each object, including the
sizes, shapes and flavors/scents. We believe our dataset could benefit both
computer vision research and retail industry. Our dataset is publicly available
at https://www.pinlandata.com/rp2k_dataset.
</p>
<a href="http://arxiv.org/abs/2006.12634" target="_blank">arXiv:2006.12634</a> [<a href="http://arxiv.org/pdf/2006.12634" target="_blank">pdf</a>]

<h2>Dynamic Graph Representation Learning for Video Dialog via Multi-Modal Shuffled Transformers. (arXiv:2007.03848v2 [cs.CV] UPDATED)</h2>
<h3>Shijie Geng, Peng Gao, Moitreya Chatterjee, Chiori Hori, Jonathan Le Roux, Yongfeng Zhang, Hongsheng Li, Anoop Cherian</h3>
<p>Given an input video, its associated audio, and a brief caption, the
audio-visual scene aware dialog (AVSD) task requires an agent to indulge in a
question-answer dialog with a human about the audio-visual content. This task
thus poses a challenging multi-modal representation learning and reasoning
scenario, advancements into which could influence several human-machine
interaction applications. To solve this task, we introduce a
semantics-controlled multi-modal shuffled Transformer reasoning framework,
consisting of a sequence of Transformer modules, each taking a modality as
input and producing representations conditioned on the input question. Our
proposed Transformer variant uses a shuffling scheme on their multi-head
outputs, demonstrating better regularization. To encode fine-grained visual
information, we present a novel dynamic scene graph representation learning
pipeline that consists of an intra-frame reasoning layer producing
spatio-semantic graph representations for every frame, and an inter-frame
aggregation module capturing temporal cues. Our entire pipeline is trained
end-to-end. We present experiments on the benchmark AVSD dataset, both on
answer generation and selection tasks. Our results demonstrate state-of-the-art
performances on all evaluation metrics.
</p>
<a href="http://arxiv.org/abs/2007.03848" target="_blank">arXiv:2007.03848</a> [<a href="http://arxiv.org/pdf/2007.03848" target="_blank">pdf</a>]

<h2>PathGAN: Local Path Planning with Attentive Generative Adversarial Networks. (arXiv:2007.03877v2 [cs.CV] UPDATED)</h2>
<h3>Dooseop Choi, Seung-jun Han, Kyoungwook Min, Jeongdan Choi</h3>
<p>To achieve autonomous driving without high-definition maps, we present a
model capable of generating multiple plausible paths from egocentric images for
autonomous vehicles. Our generative model comprises two neural networks: the
feature extraction network (FEN) and path generation network (PGN). The FEN
extracts meaningful features from an egocentric image, whereas the PGN
generates multiple paths from the features, given a driving intention and
speed. To ensure that the paths generated are plausible and consistent with the
intention, we introduce an attentive discriminator and train it with the PGN
under generative adversarial networks framework. We also devise an interaction
model between the positions in the paths and the intentions hidden in the
positions and design a novel PGN architecture that reflects the interaction
model, resulting in the improvement of the accuracy and diversity of the
generated paths. Finally, we introduce ETRIDriving, a dataset for autonomous
driving in which the recorded sensor data are labeled with discrete high-level
driving actions, and demonstrate the state-of-the-art performance of the
proposed model on ETRIDriving in terms of accuracy and diversity.
</p>
<a href="http://arxiv.org/abs/2007.03877" target="_blank">arXiv:2007.03877</a> [<a href="http://arxiv.org/pdf/2007.03877" target="_blank">pdf</a>]

<h2>Learning RGB-D Feature Embeddings for Unseen Object Instance Segmentation. (arXiv:2007.15157v3 [cs.RO] UPDATED)</h2>
<h3>Yu Xiang, Christopher Xie, Arsalan Mousavian, Dieter Fox</h3>
<p>Segmenting unseen objects in cluttered scenes is an important skill that
robots need to acquire in order to perform tasks in new environments. In this
work, we propose a new method for unseen object instance segmentation by
learning RGB-D feature embeddings from synthetic data. A metric learning loss
function is utilized to learn to produce pixel-wise feature embeddings such
that pixels from the same object are close to each other and pixels from
different objects are separated in the embedding space. With the learned
feature embeddings, a mean shift clustering algorithm can be applied to
discover and segment unseen objects. We further improve the segmentation
accuracy with a new two-stage clustering algorithm. Our method demonstrates
that non-photorealistic synthetic RGB and depth images can be used to learn
feature embeddings that transfer well to real-world images for unseen object
instance segmentation.
</p>
<a href="http://arxiv.org/abs/2007.15157" target="_blank">arXiv:2007.15157</a> [<a href="http://arxiv.org/pdf/2007.15157" target="_blank">pdf</a>]

<h2>Rethinking Recurrent Neural Networks and Other Improvements for Image Classification. (arXiv:2007.15161v2 [cs.CV] UPDATED)</h2>
<h3>Nguyen Huu Phong, Bernardete Ribeiro</h3>
<p>For a long history of Machine Learning which dates back to several decades,
Recurrent Neural Networks (RNNs) have been mainly used for sequential data and
time series or generally 1D information. Even in some rare researches on 2D
images, the networks merely learn and generate data sequentially rather than
for recognition of images. In this research, we propose to integrate RNN as an
additional layer in designing image recognition's models. Moreover, we develop
End-to-End Ensemble Multi-models that are able to learn experts' predictions
from several models. Besides, we extend training strategy and softmax pruning
which overall leads our designs to perform comparably to top models on several
datasets. The source code of the methods provided in this article is available
in https://github.com/leonlha/e2e-3m and this http URL
</p>
<a href="http://arxiv.org/abs/2007.15161" target="_blank">arXiv:2007.15161</a> [<a href="http://arxiv.org/pdf/2007.15161" target="_blank">pdf</a>]

<h2>Forest R-CNN: Large-Vocabulary Long-Tailed Object Detection and Instance Segmentation. (arXiv:2008.05676v2 [cs.CV] UPDATED)</h2>
<h3>Jialian Wu, Liangchen Song, Tiancai Wang, Qian Zhang, Junsong Yuan</h3>
<p>Despite the previous success of object analysis, detecting and segmenting a
large number of object categories with a long-tailed data distribution remains
a challenging problem and is less investigated. For a large-vocabulary
classifier, the chance of obtaining noisy logits is much higher, which can
easily lead to a wrong recognition. In this paper, we exploit prior knowledge
of the relations among object categories to cluster fine-grained classes into
coarser parent classes, and construct a classification tree that is responsible
for parsing an object instance into a fine-grained category via its parent
class. In the classification tree, as the number of parent class nodes are
significantly less, their logits are less noisy and can be utilized to suppress
the wrong/noisy logits existed in the fine-grained class nodes. As the way to
construct the parent class is not unique, we further build multiple trees to
form a classification forest where each tree contributes its vote to the
fine-grained classification. To alleviate the imbalanced learning caused by the
long-tail phenomena, we propose a simple yet effective resampling method, NMS
Resampling, to re-balance the data distribution. Our method, termed as Forest
R-CNN, can serve as a plug-and-play module being applied to most object
recognition models for recognizing more than 1000 categories. Extensive
experiments are performed on the large vocabulary dataset LVIS. Compared with
the Mask R-CNN baseline, the Forest R-CNN significantly boosts the performance
with 11.5% and 3.9% AP improvements on the rare categories and overall
categories, respectively. Moreover, we achieve state-of-the-art results on the
LVIS dataset. Code is available at https://github.com/JialianW/Forest_RCNN.
</p>
<a href="http://arxiv.org/abs/2008.05676" target="_blank">arXiv:2008.05676</a> [<a href="http://arxiv.org/pdf/2008.05676" target="_blank">pdf</a>]

<h2>ScrewNet: Category-Independent Articulation Model Estimation From Depth Images Using Screw Theory. (arXiv:2008.10518v2 [cs.RO] UPDATED)</h2>
<h3>Ajinkya Jain, Rudolf Lioutikov, Caleb Chuck, Scott Niekum</h3>
<p>Robots in human environments will need to interact with a wide variety of
articulated objects such as cabinets, drawers, and dishwashers while assisting
humans in performing day-to-day tasks. Existing methods either require objects
to be textured or need to know the articulation model category a priori for
estimating the model parameters for an articulated object. We propose ScrewNet,
a novel approach that estimates an object's articulation model directly from
depth images without requiring a priori knowledge of the articulation model
category. ScrewNet uses screw theory to unify the representation of different
articulation types and perform category-independent articulation model
estimation. We evaluate our approach on two benchmarking datasets and compare
its performance with a current state-of-the-art method. Results demonstrate
that ScrewNet can successfully estimate the articulation models and their
parameters for novel objects across articulation model categories with better
on average accuracy than the prior state-of-the-art method. Project webpage:
https://pearl-utexas.github.io/ScrewNet/
</p>
<a href="http://arxiv.org/abs/2008.10518" target="_blank">arXiv:2008.10518</a> [<a href="http://arxiv.org/pdf/2008.10518" target="_blank">pdf</a>]

<h2>Removing the Background by Adding the Background: Towards Background Robust Self-supervised Video Representation Learning. (arXiv:2009.05769v3 [cs.CV] UPDATED)</h2>
<h3>Jinpeng Wang, Yuting Gao, Ke Li, Yiqi Lin, Andy J. Ma, Hao Cheng, Pai Peng, Rongrong Ji, Xing Sun</h3>
<p>Self-supervised learning has shown great potentials in improving the video
representation ability of deep neural networks by getting supervision from the
data itself. However, some of the current methods tend to cheat from the
background, i.e., the prediction is highly dependent on the video background
instead of the motion, making the model vulnerable to background changes. To
mitigate the model reliance towards the background, we propose to remove the
background impact by adding the background. That is, given a video, we randomly
select a static frame and add it to every other frames to construct a
distracting video sample. Then we force the model to pull the feature of the
distracting video and the feature of the original video closer, so that the
model is explicitly restricted to resist the background influence, focusing
more on the motion changes. We term our method as \emph{Background Erasing}
(BE). It is worth noting that the implementation of our method is so simple and
neat and can be added to most of the SOTA methods without much efforts.
Specifically, BE brings 16.4% and 19.1% improvements with MoCo on the severely
biased datasets UCF101 and HMDB51, and 14.5% improvement on the less biased
dataset Diving48.
</p>
<a href="http://arxiv.org/abs/2009.05769" target="_blank">arXiv:2009.05769</a> [<a href="http://arxiv.org/pdf/2009.05769" target="_blank">pdf</a>]

<h2>Recognizing Micro-Expression in Video Clip with Adaptive Key-Frame Mining. (arXiv:2009.09179v2 [cs.CV] UPDATED)</h2>
<h3>Min Peng, Chongyang Wang, Yuan Gao, Tao Bi, Tong Chen, Yu Shi, Xiang-Dong Zhou</h3>
<p>As a spontaneous expression of emotion on face, micro-expression is receiving
increasing attention from the affective computing community. Whist better
recognition accuracy is achieved by various deep learning (DL) techniques, one
characteristic of micro-expression has been not fully exploited. That is, such
facial movement is transient and sparsely localized through time. Therefore,
the representation learned from a full video clip is usually redundant. On the
other hand, methods utilizing the single apex frame require manual annotations
and sacrifice the temporal dynamics. To simultaneously localize and recognize
such fleeting facial movements, we propose a novel end-to-end deep learning
architecture, referred to as Adaptive Key-frame Mining Network (AKMNet).
Operating on the raw video clip of micro-expression, AKMNet is able to learn
discriminative spatio-temporal representation by combining spatial features of
self-learned local key frames and their global-temporal dynamics. Empirical and
theoretical evaluations show advantages of the proposed approach with improved
performance.
</p>
<a href="http://arxiv.org/abs/2009.09179" target="_blank">arXiv:2009.09179</a> [<a href="http://arxiv.org/pdf/2009.09179" target="_blank">pdf</a>]

<h2>Robotic Pick-and-Place With Uncertain Object Instance Segmentation and Shape Completion. (arXiv:2010.07892v3 [cs.RO] UPDATED)</h2>
<h3>Marcus Gualtieri, Robert Platt</h3>
<p>We consider robotic pick-and-place of partially visible, novel objects, where
goal placements are non-trivial, e.g., tightly packed into a bin. One approach
is (a) use object instance segmentation and shape completion to model the
objects and (b) use a regrasp planner to decide grasps and places displacing
the models to their goals. However, it is critical for the planner to account
for uncertainty in the perceived models, as object geometries in unobserved
areas are just guesses. We account for perceptual uncertainty by incorporating
it into the regrasp planner's cost function. We compare seven different costs.
One of these, which uses neural networks to estimate probability of grasp and
place stability, consistently outperforms uncertainty-unaware costs and
evaluates faster than Monte Carlo sampling. On a real robot, the proposed cost
results in successfully packing objects tightly into a bin 7.8% more often
versus the commonly used minimum-number-of-grasps cost.
</p>
<a href="http://arxiv.org/abs/2010.07892" target="_blank">arXiv:2010.07892</a> [<a href="http://arxiv.org/pdf/2010.07892" target="_blank">pdf</a>]

<h2>Efficient Robotic Object Search via HIEM: Hierarchical Policy Learning with Intrinsic-Extrinsic Modeling. (arXiv:2010.08596v2 [cs.RO] UPDATED)</h2>
<h3>Xin Ye, Yezhou Yang</h3>
<p>Despite the significant success at enabling robots with autonomous behaviors
makes deep reinforcement learning a promising approach for robotic object
search task, the deep reinforcement learning approach severely suffers from the
nature sparse reward setting of the task. To tackle this challenge, we present
a novel policy learning paradigm for the object search task, based on
hierarchical and interpretable modeling with an intrinsic-extrinsic reward
setting. More specifically, we explore the environment efficiently through a
proxy low-level policy which is driven by the intrinsic rewarding sub-goals. We
further learn our hierarchical policy from the efficient exploration experience
where we optimize both of our high-level and low-level policies towards the
extrinsic rewarding goal to perform the object search task well. Experiments
conducted on the House3D environment validate and show that the robot, trained
with our model, can perform the object search task in a more optimal and
interpretable way.
</p>
<a href="http://arxiv.org/abs/2010.08596" target="_blank">arXiv:2010.08596</a> [<a href="http://arxiv.org/pdf/2010.08596" target="_blank">pdf</a>]

<h2>A Weakly-Supervised Semantic Segmentation Approach based on the Centroid Loss: Application to Quality Control and Inspection. (arXiv:2010.13433v2 [cs.CV] UPDATED)</h2>
<h3>Kai Yao, Alberto Ortiz, Francisco Bonnin-Pascual</h3>
<p>It is generally accepted that one of the critical parts of current vision
algorithms based on deep learning and convolutional neural networks is the
annotation of a sufficient number of images to achieve competitive performance.
This is particularly difficult for semantic segmentation tasks since the
annotation must be ideally generated at the pixel level. Weakly-supervised
semantic segmentation aims at reducing this cost by employing simpler
annotations that, hence, are easier, cheaper and quicker to produce. In this
paper, we propose and assess a new weakly-supervised semantic segmentation
approach making use of a novel loss function whose goal is to counteract the
effects of weak annotations. To this end, this loss function comprises several
terms based on partial cross-entropy losses, being one of them the Centroid
Loss. This term induces a clustering of the image pixels in the object classes
under consideration, whose aim is to improve the training of the segmentation
network by guiding the optimization. The performance of the approach is
evaluated against datasets from two different industry-related case studies:
while one involves the detection of instances of a number of different object
classes in the context of a quality control application, the other stems from
the visual inspection domain and deals with the localization of images areas
whose pixels correspond to scene surface points affected by a specific sort of
defect. The detection results that are reported for both cases show that,
despite the differences among them and the particular challenges, the use of
weak annotations do not prevent from achieving a competitive performance level
for both.
</p>
<a href="http://arxiv.org/abs/2010.13433" target="_blank">arXiv:2010.13433</a> [<a href="http://arxiv.org/pdf/2010.13433" target="_blank">pdf</a>]

<h2>Dense Label Encoding for Boundary Discontinuity Free Rotation Detection. (arXiv:2011.09670v2 [cs.CV] UPDATED)</h2>
<h3>Xue Yang, Liping Hou, Yue Zhou, Wentao Wang, Junchi Yan</h3>
<p>Rotation detection serves as a fundamental building block in many visual
applications involving aerial image, scene text, and face etc. Differing from
the dominant regression-based approaches for orientation estimation, this paper
explores a relatively less-studied methodology based on classification. The
hope is to inherently dismiss the boundary discontinuity issue as encountered
by the regression-based detectors. We propose new techniques to push its
frontier in two aspects: i) new encoding mechanism: the design of two Densely
Coded Labels (DCL) for angle classification, to replace the Sparsely Coded
Label (SCL) in existing classification-based detectors, leading to three times
training speed increase as empirically observed across benchmarks, further with
notable improvement in detection accuracy; ii) loss re-weighting: we propose
Angle Distance and Aspect Ratio Sensitive Weighting (ADARSW), which improves
the detection accuracy especially for square-like objects, by making DCL-based
detectors sensitive to angular distance and object's aspect ratio. Extensive
experiments and visual analysis on large-scale public datasets for aerial
images i.e. DOTA, UCAS-AOD, HRSC2016, as well as scene text dataset ICDAR2015
and MLT, show the effectiveness of our approach. The source code is available
at https://github.com/Thinklab-SJTU/DCL_RetinaNet_Tensorflow and is also
integrated in our open source rotation detection benchmark:
https://github.com/yangxue0827/RotationDetection.
</p>
<a href="http://arxiv.org/abs/2011.09670" target="_blank">arXiv:2011.09670</a> [<a href="http://arxiv.org/pdf/2011.09670" target="_blank">pdf</a>]

<h2>Multi-scale Information Assembly for Image Matting. (arXiv:2101.02391v2 [cs.CV] UPDATED)</h2>
<h3>Yu Qiao, Yuhao Liu, Qiang Zhu, Xin Yang, Yuxin Wang, Qiang Zhang, Xiaopeng Wei</h3>
<p>Image matting is a long-standing problem in computer graphics and vision,
mostly identified as the accurate estimation of the foreground in input images.
We argue that the foreground objects can be represented by different-level
information, including the central bodies, large-grained boundaries, refined
details, etc. Based on this observation, in this paper, we propose a
multi-scale information assembly framework (MSIA-matte) to pull out
high-quality alpha mattes from single RGB images. Technically speaking, given
an input image, we extract advanced semantics as our subject content and retain
initial CNN features to encode different-level foreground expression, then
combine them by our well-designed information assembly strategy. Extensive
experiments can prove the effectiveness of the proposed MSIA-matte, and we can
achieve state-of-the-art performance compared to most existing matting
networks.
</p>
<a href="http://arxiv.org/abs/2101.02391" target="_blank">arXiv:2101.02391</a> [<a href="http://arxiv.org/pdf/2101.02391" target="_blank">pdf</a>]

<h2>From Learning to Relearning: A Framework for Diminishing Bias in Social Robot Navigation. (arXiv:2101.02647v2 [cs.RO] UPDATED)</h2>
<h3>Juana Valeria Hurtado, Laura Londo&#xf1;o, Abhinav Valada</h3>
<p>The exponentially increasing advances in robotics and machine learning are
facilitating the transition of robots from being confined to controlled
industrial spaces to performing novel everyday tasks in domestic and urban
environments. In order to make the presence of robots safe as well as
comfortable for humans, and to facilitate their acceptance in public
environments, they are often equipped with social abilities for navigation and
interaction. Socially compliant robot navigation is increasingly being learned
from human observations or demonstrations. We argue that these techniques that
typically aim to mimic human behavior do not guarantee fair behavior. As a
consequence, social navigation models can replicate, promote, and amplify
societal unfairness such as discrimination and segregation. In this work, we
investigate a framework for diminishing bias in social robot navigation models
so that robots are equipped with the capability to plan as well as adapt their
paths based on both physical and social demands. Our proposed framework
consists of two components: \textit{learning} which incorporates social context
into the learning process to account for safety and comfort, and
\textit{relearning} to detect and correct potentially harmful outcomes before
the onset. We provide both technological and societal analysis using three
diverse case studies in different social scenarios of interaction. Moreover, we
present ethical implications of deploying robots in social environments and
propose potential solutions. Through this study, we highlight the importance
and advocate for fairness in human-robot interactions in order to promote more
equitable social relationships, roles, and dynamics and consequently positively
influence our society.
</p>
<a href="http://arxiv.org/abs/2101.02647" target="_blank">arXiv:2101.02647</a> [<a href="http://arxiv.org/pdf/2101.02647" target="_blank">pdf</a>]

<h2>Transferable Interactiveness Knowledge for Human-Object Interaction Detection. (arXiv:2101.10292v3 [cs.CV] UPDATED)</h2>
<h3>Yong-Lu Li, Xinpeng Liu, Xiaoqian Wu, Xijie Huang, Liang Xu, Cewu Lu</h3>
<p>Human-Object Interaction (HOI) detection is an important problem to
understand how humans interact with objects. In this paper, we explore
interactiveness knowledge which indicates whether a human and an object
interact with each other or not. We found that interactiveness knowledge can be
learned across HOI datasets and bridge the gap between diverse HOI category
settings. Our core idea is to exploit an interactiveness network to learn the
general interactiveness knowledge from multiple HOI datasets and perform
Non-Interaction Suppression (NIS) before HOI classification in inference. On
account of the generalization ability of interactiveness, interactiveness
network is a transferable knowledge learner and can be cooperated with any HOI
detection models to achieve desirable results. We utilize the human instance
and body part features together to learn the interactiveness in hierarchical
paradigm, i.e., instance-level and body part-level interactivenesses.
Thereafter, a consistency task is proposed to guide the learning and extract
deeper interactive visual clues. We extensively evaluate the proposed method on
HICO-DET, V-COCO, and a newly constructed PaStaNet-HOI dataset. With the
learned interactiveness, our method outperforms state-of-the-art HOI detection
methods, verifying its efficacy and flexibility. Code is available at
https://github.com/DirtyHarryLYL/Transferable-Interactiveness-Network.
</p>
<a href="http://arxiv.org/abs/2101.10292" target="_blank">arXiv:2101.10292</a> [<a href="http://arxiv.org/pdf/2101.10292" target="_blank">pdf</a>]

<h2>Gaze-based dual resolution deep imitation learning for high-precision dexterous robot manipulation. (arXiv:2102.01295v2 [cs.RO] UPDATED)</h2>
<h3>Heecheol Kim, Yoshiyuki Ohmura, Yasuo Kuniyoshi</h3>
<p>A high-precision manipulation task, such as needle threading, is challenging.
Physiological studies have proposed connecting low-resolution peripheral vision
and fast movement to transport the hand into the vicinity of an object, and
using high-resolution foveated vision to achieve the accurate homing of the
hand to the object. The results of this study demonstrate that a deep imitation
learning based method, inspired by the gaze-based dual resolution visuomotor
control system in humans, can solve the needle threading task. First, we
recorded the gaze movements of a human operator who was teleoperating a robot.
Then, we used only a high-resolution image around the gaze to precisely control
the thread position when it was close to the target. We used a low-resolution
peripheral image to reach the vicinity of the target. The experimental results
obtained in this study demonstrate that the proposed method enables precise
manipulation tasks using a general-purpose robot manipulator and improves
computational efficiency.
</p>
<a href="http://arxiv.org/abs/2102.01295" target="_blank">arXiv:2102.01295</a> [<a href="http://arxiv.org/pdf/2102.01295" target="_blank">pdf</a>]

<h2>Learning Crisp Boundaries Using Deep Refinement Network and Adaptive Weighting Loss. (arXiv:2102.01301v2 [cs.CV] UPDATED)</h2>
<h3>Yi-Jun Cao, Chuan Lin, Yong-Jie Li</h3>
<p>Significant progress has been made in boundary detection with the help of
convolutional neural networks. Recent boundary detection models not only focus
on real object boundary detection but also "crisp" boundaries (precisely
localized along the object's contour). There are two methods to evaluate crisp
boundary performance. One uses more strict tolerance to measure the distance
between the ground truth and the detected contour. The other focuses on
evaluating the contour map without any postprocessing. In this study, we
analyze both methods and conclude that both methods are two aspects of crisp
contour evaluation. Accordingly, we propose a novel network named deep
refinement network (DRNet) that stacks multiple refinement modules to achieve
richer feature representation and a novel loss function, which combines
cross-entropy and dice loss through effective adaptive fusion. Experimental
results demonstrated that we achieve state-of-the-art performance for several
available datasets.
</p>
<a href="http://arxiv.org/abs/2102.01301" target="_blank">arXiv:2102.01301</a> [<a href="http://arxiv.org/pdf/2102.01301" target="_blank">pdf</a>]

<h2>Data-Driven MPC for Quadrotors. (arXiv:2102.05773v2 [cs.RO] UPDATED)</h2>
<h3>Guillem Torrente, Elia Kaufmann, Philipp Foehn, Davide Scaramuzza</h3>
<p>Aerodynamic forces render accurate high-speed trajectory tracking with
quadrotors extremely challenging. These complex aerodynamic effects become a
significant disturbance at high speeds, introducing large positional tracking
errors, and are extremely difficult to model. To fly at high speeds, feedback
control must be able to account for these aerodynamic effects in real-time.
This necessitates a modelling procedure that is both accurate and efficient to
evaluate. Therefore, we present an approach to model aerodynamic effects using
Gaussian Processes, which we incorporate into a Model Predictive Controller to
achieve efficient and precise real-time feedback control, leading to up to 70%
reduction in trajectory tracking error at high speeds. We verify our method by
extensive comparison to a state-of-the-art linear drag model in synthetic and
real-world experiments at speeds of up to 14m/s and accelerations beyond 4g.
</p>
<a href="http://arxiv.org/abs/2102.05773" target="_blank">arXiv:2102.05773</a> [<a href="http://arxiv.org/pdf/2102.05773" target="_blank">pdf</a>]

<h2>Neural Architecture Search on ImageNet in Four GPU Hours: A Theoretically Inspired Perspective. (arXiv:2102.11535v3 [cs.CV] UPDATED)</h2>
<h3>Wuyang Chen, Xinyu Gong, Zhangyang Wang</h3>
<p>Neural Architecture Search (NAS) has been explosively studied to automate the
discovery of top-performer neural networks. Current works require heavy
training of supernet or intensive architecture evaluations, thus suffering from
heavy resource consumption and often incurring search bias due to truncated
training or approximations. Can we select the best neural architectures without
involving any training and eliminate a drastic portion of the search cost? We
provide an affirmative answer, by proposing a novel framework called
training-free neural architecture search (TE-NAS). TE-NAS ranks architectures
by analyzing the spectrum of the neural tangent kernel (NTK) and the number of
linear regions in the input space. Both are motivated by recent theory advances
in deep networks and can be computed without any training and any label. We
show that: (1) these two measurements imply the trainability and expressivity
of a neural network; (2) they strongly correlate with the network's test
accuracy. Further on, we design a pruning-based NAS mechanism to achieve a more
flexible and superior trade-off between the trainability and expressivity
during the search. In NAS-Bench-201 and DARTS search spaces, TE-NAS completes
high-quality search but only costs 0.5 and 4 GPU hours with one 1080Ti on
CIFAR-10 and ImageNet, respectively. We hope our work inspires more attempts in
bridging the theoretical findings of deep networks and practical impacts in
real NAS applications. Code is available at:
https://github.com/VITA-Group/TENAS.
</p>
<a href="http://arxiv.org/abs/2102.11535" target="_blank">arXiv:2102.11535</a> [<a href="http://arxiv.org/pdf/2102.11535" target="_blank">pdf</a>]

<h2>$SE_2(3)$ based Extended Kalman Filter and Smoothing for Inertial-Integrated Navigation. (arXiv:2102.12897v2 [cs.RO] UPDATED)</h2>
<h3>Yarong Luo, Chi Guo, Shenyong You, Jianlang Hu, Jingnan Liu</h3>
<p>The error representation using the straight difference of two vectors in the
inertial navigation system may not be reasonable as it does not take the
direction difference into consideration. Therefore, we proposed to use the
$SE_2(3)$ matrix Lie group to represent the state of the inertial-integrated
navigation system which consequently leads to the common frame error
representation.

With the new velocity and position error definition, we leverage the group
affine dynamics with the autonomous error properties and derive the error state
differential equation for the inertial-integrated navigation on the
north-east-down local-level navigation frame and the earth-centered earth-fixed
frame, respectively, the corresponding extending Kalman filter (EKF), terms as
$SE_2(3)$-EKF has also been derived. It provides a new perspective on the
geometric EKF with a more sophisticated formula for the inertial-integrated
navigation system. Furthermore, we propose a $SE_2(3)$-based smoothing
algorithm based on the $SE_2(3)$-based EKF.
</p>
<a href="http://arxiv.org/abs/2102.12897" target="_blank">arXiv:2102.12897</a> [<a href="http://arxiv.org/pdf/2102.12897" target="_blank">pdf</a>]

<h2>Exposing Semantic Segmentation Failures via Maximum Discrepancy Competition. (arXiv:2103.00259v2 [cs.CV] UPDATED)</h2>
<h3>Jiebin Yan, Yu Zhong, Yuming Fang, Zhangyang Wang, Kede Ma</h3>
<p>Semantic segmentation is an extensively studied task in computer vision, with
numerous methods proposed every year. Thanks to the advent of deep learning in
semantic segmentation, the performance on existing benchmarks is close to
saturation. A natural question then arises: Does the superior performance on
the closed (and frequently re-used) test sets transfer to the open visual world
with unconstrained variations? In this paper, we take steps toward answering
the question by exposing failures of existing semantic segmentation methods in
the open visual world under the constraint of very limited human labeling
effort. Inspired by previous research on model falsification, we start from an
arbitrarily large image set, and automatically sample a small image set by
MAximizing the Discrepancy (MAD) between two segmentation methods. The selected
images have the greatest potential in falsifying either (or both) of the two
methods. We also explicitly enforce several conditions to diversify the exposed
failures, corresponding to different underlying root causes. A segmentation
method, whose failures are more difficult to be exposed in the MAD competition,
is considered better. We conduct a thorough MAD diagnosis of ten PASCAL VOC
semantic segmentation algorithms. With detailed analysis of experimental
results, we point out strengths and weaknesses of the competing algorithms, as
well as potential research directions for further advancement in semantic
segmentation. The codes are publicly available at
\url{https://github.com/QTJiebin/MAD_Segmentation}.
</p>
<a href="http://arxiv.org/abs/2103.00259" target="_blank">arXiv:2103.00259</a> [<a href="http://arxiv.org/pdf/2103.00259" target="_blank">pdf</a>]

<h2>Unsupervised Depth and Ego-motion Estimation for Monocular Thermal Video using Multi-spectral Consistency Loss. (arXiv:2103.00760v2 [cs.CV] UPDATED)</h2>
<h3>Ukcheol Shin, Kyunghyun Lee, Seokju Lee, In So Kweon</h3>
<p>Most of the deep-learning based depth and ego-motion networks have been
designed for visible cameras. However, visible cameras heavily rely on the
presence of an external light source. Therefore, it is challenging to use them
under low-light conditions such as night scenes, tunnels, and other harsh
conditions. A thermal camera is one solution to compensate for this problem
because it detects Long Wave Infrared Radiation(LWIR) regardless of any
external light sources. However, despite this advantage, both depth and
ego-motion estimation research for the thermal camera are not actively explored
until so far. In this paper, we propose an unsupervised learning method for the
all-day depth and ego-motion estimation. The proposed method exploits
multi-spectral consistency loss to gives complementary supervision for the
networks by reconstructing visible and thermal images with the depth and pose
estimated from thermal images. The networks trained with the proposed method
robustly estimate the depth and pose from monocular thermal video under
low-light and even zero-light conditions. To the best of our knowledge, this is
the first work to simultaneously estimate both depth and ego-motion from the
monocular thermal video in an unsupervised manner.
</p>
<a href="http://arxiv.org/abs/2103.00760" target="_blank">arXiv:2103.00760</a> [<a href="http://arxiv.org/pdf/2103.00760" target="_blank">pdf</a>]

<h2>OMNet: Learning Overlapping Mask for Partial-to-Partial Point Cloud Registration. (arXiv:2103.00937v3 [cs.CV] UPDATED)</h2>
<h3>Hao Xu, Shuaicheng Liu, Guangfu Wang, Guanghui Liu, Bing Zeng</h3>
<p>Point cloud registration is a key task in many computational fields. Previous
correspondence matching based methods require the point clouds to have
distinctive geometric structures to fit a 3D rigid transformation according to
point-wise sparse feature matches. However, the accuracy of transformation
heavily relies on the quality of extracted features, which are prone to errors
with respect partiality and noise of the inputs. In addition, they can not
utilize the geometric knowledge of all regions. On the other hand, previous
global feature based deep learning approaches can utilize the entire point
cloud for the registration, however they ignore the negative effect of
non-overlapping points when aggregating global feature from point-wise
features. In this paper, we present OMNet, a global feature based iterative
network for partial-to-partial point cloud registration. We learn masks in a
coarse-to-fine manner to reject non-overlapping regions, which converting the
partial-to-partial registration to the registration of the same shapes.
Moreover, the data used in previous works are only sampled once from CAD models
for each object, resulting the same point cloud for the source and the
reference. We propose a more practical manner for data generation, where a CAD
model is sampled twice for the source and the reference point clouds, avoiding
over-fitting issues that commonly exist previously. Experimental results show
that our approach achieves state-of-the-art performance compared to traditional
and deep learning methods.
</p>
<a href="http://arxiv.org/abs/2103.00937" target="_blank">arXiv:2103.00937</a> [<a href="http://arxiv.org/pdf/2103.00937" target="_blank">pdf</a>]

<h2>A Deep Emulator for Secondary Motion of 3D Characters. (arXiv:2103.01261v2 [cs.CV] UPDATED)</h2>
<h3>Mianlun Zheng, Yi Zhou, Duygu Ceylan, Jernej Barbic</h3>
<p>Fast and light-weight methods for animating 3D characters are desirable in
various applications such as computer games. We present a learning-based
approach to enhance skinning-based animations of 3D characters with vivid
secondary motion effects. We design a neural network that encodes each local
patch of a character simulation mesh where the edges implicitly encode the
internal forces between the neighboring vertices. The network emulates the
ordinary differential equations of the character dynamics, predicting new
vertex positions from the current accelerations, velocities and positions.
Being a local method, our network is independent of the mesh topology and
generalizes to arbitrarily shaped 3D character meshes at test time. We further
represent per-vertex constraints and material properties such as stiffness,
enabling us to easily adjust the dynamics in different parts of the mesh. We
evaluate our method on various character meshes and complex motion sequences.
Our method can be over 30 times more efficient than ground-truth physically
based simulation, and outperforms alternative solutions that provide fast
approximations.
</p>
<a href="http://arxiv.org/abs/2103.01261" target="_blank">arXiv:2103.01261</a> [<a href="http://arxiv.org/pdf/2103.01261" target="_blank">pdf</a>]

<h2>When Age-Invariant Face Recognition Meets Face Age Synthesis: A Multi-Task Learning Framework. (arXiv:2103.01520v2 [cs.CV] UPDATED)</h2>
<h3>Zhizhong Huang, Junping Zhang, Hongming Shan</h3>
<p>To minimize the effects of age variation in face recognition, previous work
either extracts identity-related discriminative features by minimizing the
correlation between identity- and age-related features, called age-invariant
face recognition (AIFR), or removes age variation by transforming the faces of
different age groups into the same age group, called face age synthesis (FAS);
however, the former lacks visual results for model interpretation while the
latter suffers from artifacts compromising downstream recognition. Therefore,
this paper proposes a unified, multi-task framework to jointly handle these two
tasks, termed MTLFace, which can learn age-invariant identity-related
representation while achieving pleasing face synthesis. Specifically, we first
decompose the mixed face feature into two uncorrelated components -- identity-
and age-related feature -- through an attention mechanism, and then decorrelate
these two components using multi-task training and continuous domain adaption.
In contrast to the conventional one-hot encoding that achieves group-level FAS,
we propose a novel identity conditional module to achieve identity-level FAS,
with a weight-sharing strategy to improve the age smoothness of synthesized
faces. In addition, we collect and release a large cross-age face dataset with
age and gender annotations to advance the development of the AIFR and FAS.
Extensive experiments on five benchmark cross-age datasets demonstrate the
superior performance of our proposed MTLFace over existing state-of-the-art
methods for AIFR and FAS. We further validate MTLFace on two popular general
face recognition datasets, showing competitive performance for face recognition
in the wild. The source code and dataset are available
at~\url{https://github.com/Hzzone/MTLFace}.
</p>
<a href="http://arxiv.org/abs/2103.01520" target="_blank">arXiv:2103.01520</a> [<a href="http://arxiv.org/pdf/2103.01520" target="_blank">pdf</a>]

<h2>A Human-Centered Dynamic Scheduling Architecture for Collaborative Application. (arXiv:2103.01831v2 [cs.RO] UPDATED)</h2>
<h3>Andrea Pupa, Wietse Van Dijk, Cristian Secchi</h3>
<p>In collaborative robotic applications, human and robot have to work together
during a whole shift for executing a sequence of jobs. The performance of the
human robot team can be enhanced by scheduling the right tasks to the human and
the robot. The scheduling should consider the task execution constraints, the
variability in the task execution by the human, and the job quality of the
human. Therefore, it is necessary to dynamically schedule the assigned tasks.
In this paper, we propose a two-layered architecture for task allocation and
scheduling in a collaborative cell. Job quality is explicitly considered during
the allocation of the tasks and over a sequence of jobs. The tasks are
dynamically scheduled based on the real time monitoring of the human's
activities. The effectiveness of the proposed architecture is experimentally
validated.
</p>
<a href="http://arxiv.org/abs/2103.01831" target="_blank">arXiv:2103.01831</a> [<a href="http://arxiv.org/pdf/2103.01831" target="_blank">pdf</a>]

<h2>WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning. (arXiv:2103.01913v2 [cs.CV] UPDATED)</h2>
<h3>Krishna Srinivasan, Karthik Raman, Jiecao Chen, Michael Bendersky, Marc Najork</h3>
<p>The milestone improvements brought about by deep representation learning and
pre-training techniques have led to large performance gains across downstream
NLP, IR and Vision tasks. Multimodal modeling techniques aim to leverage large
high-quality visio-linguistic datasets for learning complementary information
(across image and text modalities). In this paper, we introduce the
Wikipedia-based Image Text (WIT) Dataset
(https://github.com/google-research-datasets/wit) to better facilitate
multimodal, multilingual learning. WIT is composed of a curated set of 37.6
million entity rich image-text examples with 11.5 million unique images across
108 Wikipedia languages. Its size enables WIT to be used as a pretraining
dataset for multimodal models, as we show when applied to downstream tasks such
as image-text retrieval. WIT has four main and unique advantages. First, WIT is
the largest multimodal dataset by the number of image-text examples by 3x (at
the time of writing). Second, WIT is massively multilingual (first of its kind)
with coverage over 100+ languages (each of which has at least 12K examples) and
provides cross-lingual texts for many images. Third, WIT represents a more
diverse set of concepts and real world entities relative to what previous
datasets cover. Lastly, WIT provides a very challenging real-world test set, as
we empirically illustrate using an image-text retrieval task as an example.
</p>
<a href="http://arxiv.org/abs/2103.01913" target="_blank">arXiv:2103.01913</a> [<a href="http://arxiv.org/pdf/2103.01913" target="_blank">pdf</a>]

