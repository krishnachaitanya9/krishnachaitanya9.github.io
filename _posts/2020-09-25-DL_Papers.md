---
title: Latest Deep Learning Papers
date: 2021-01-05 21:00:01 +0800
featured-img: DL-logo
categories: [Deep_Learning]
tags: [Deep_Learning]
mathjax: Yes
---

<h1>Your arXiv Feed (145 Articles)</h1>
<h2>Stochastic Optimization for Vaccine and Testing Kit Allocation for the COVID-19 Pandemic. (arXiv:2101.01204v1 [cs.AI])</h2>
<h3>Lawrence Thul, Warren Powell</h3>
<p>The pandemic caused by the SARS-CoV-2 virus has exposed many flaws in the
decision-making strategies used to distribute resources to combat global health
crises. In this paper, we leverage reinforcement learning and optimization to
improve upon the allocation strategies for various resources. In particular, we
consider a problem where a central controller must decide where to send testing
kits to learn about the uncertain states of the world (active learning); then,
use the new information to construct beliefs about the states and decide where
to allocate resources. We propose a general model coupled with a tunable
lookahead policy for making vaccine allocation decisions without perfect
knowledge about the state of the world. The lookahead policy is compared to a
population-based myopic policy which is more likely to be similar to the
present strategies in practice. Each vaccine allocation policy works in
conjunction with a testing kit allocation policy to perform active learning.
Our simulation results demonstrate that an optimization-based lookahead
decision making strategy will outperform the presented myopic policy.
</p>
<a href="http://arxiv.org/abs/2101.01204" target="_blank">arXiv:2101.01204</a> [<a href="http://arxiv.org/pdf/2101.01204" target="_blank">pdf</a>]

<h2>Semantic Video Segmentation for Intracytoplasmic Sperm Injection Procedures. (arXiv:2101.01207v1 [cs.CV])</h2>
<h3>Peter He, Raksha Jain, J&#xe9;r&#xf4;me Chambost, C&#xe9;line Jacques, Cristina Hickman</h3>
<p>We present the first deep learning model for the analysis of intracytoplasmic
sperm injection (ICSI) procedures. Using a dataset of ICSI procedure videos, we
train a deep neural network to segment key objects in the videos achieving a
mean IoU of 0.962, and to localize the needle tip achieving a mean pixel error
of 3.793 pixels at 14 FPS on a single GPU. We further analyze the variation
between the dataset's human annotators and find the model's performance to be
comparable to human experts.
</p>
<a href="http://arxiv.org/abs/2101.01207" target="_blank">arXiv:2101.01207</a> [<a href="http://arxiv.org/pdf/2101.01207" target="_blank">pdf</a>]

<h2>Learn by Guessing: Multi-Step Pseudo-Label Refinement for Person Re-Identification. (arXiv:2101.01215v1 [cs.CV])</h2>
<h3>Tiago de C. G. Pereira, Teofilo E. de Campos</h3>
<p>Unsupervised Domain Adaptation (UDA) methods for person Re-Identification
(Re-ID) rely on target domain samples to model the marginal distribution of the
data. To deal with the lack of target domain labels, UDA methods leverage
information from labeled source samples and unlabeled target samples. A
promising approach relies on the use of unsupervised learning as part of the
pipeline, such as clustering methods. The quality of the clusters clearly plays
a major role in methods performance, but this point has been overlooked. In
this work, we propose a multi-step pseudo-label refinement method to select the
best possible clusters and keep improving them so that these clusters become
closer to the class divisions without knowledge of the class labels. Our
refinement method includes a cluster selection strategy and a camera-based
normalization method which reduces the within-domain variations caused by the
use of multiple cameras in person Re-ID. This allows our method to reach
state-of-the-art UDA results on DukeMTMC-Market1501 (source-target). We surpass
state-of-the-art for UDA Re-ID by 3.4% on Market1501-DukeMTMC datasets, which
is a more challenging adaptation setup because the target domain (DukeMTMC) has
eight distinct cameras. Furthermore, the camera-based normalization method
causes a significant reduction in the number of iterations required for
training convergence.
</p>
<a href="http://arxiv.org/abs/2101.01215" target="_blank">arXiv:2101.01215</a> [<a href="http://arxiv.org/pdf/2101.01215" target="_blank">pdf</a>]

<h2>A Survey on Embedding Dynamic Graphs. (arXiv:2101.01229v1 [cs.LG])</h2>
<h3>Claudio D. T. Barros (1), Matheus R. F. Mendon&#xe7;a (1), Alex B. Vieira (2), Artur Ziviani (1) ((1) National Laboratory for Scientific Computing (LNCC), Petr&#xf3;polis, RJ, Brazil, (2) Federal University of Juiz de Fora (UFJF), Juiz de Fora, MG, Brazil)</h3>
<p>Embedding static graphs in low-dimensional vector spaces plays a key role in
network analytics and inference, supporting applications like node
classification, link prediction, and graph visualization. However, many
real-world networks present dynamic behavior, including topological evolution,
feature evolution, and diffusion. Therefore, several methods for embedding
dynamic graphs have been proposed to learn network representations over time,
facing novel challenges, such as time-domain modeling, temporal features to be
captured, and the temporal granularity to be embedded. In this survey, we
overview dynamic graph embedding, discussing its fundamentals and the recent
advances developed so far. We introduce the formal definition of dynamic graph
embedding, focusing on the problem setting and introducing a novel taxonomy for
dynamic graph embedding input and output. We further explore different dynamic
behaviors that may be encompassed by embeddings, classifying by topological
evolution, feature evolution, and processes on networks. Afterward, we describe
existing techniques and propose a taxonomy for dynamic graph embedding
techniques based on algorithmic approaches, from matrix and tensor
factorization to deep learning, random walks, and temporal point processes. We
also elucidate main applications, including dynamic link prediction, anomaly
detection, and diffusion prediction, and we further state some promising
research directions in the area.
</p>
<a href="http://arxiv.org/abs/2101.01229" target="_blank">arXiv:2101.01229</a> [<a href="http://arxiv.org/pdf/2101.01229" target="_blank">pdf</a>]

<h2>Robust Maximum Entropy Behavior Cloning. (arXiv:2101.01251v1 [cs.LG])</h2>
<h3>Mostafa Hussein, Brendan Crowe, Marek Petrik, Momotaz Begum</h3>
<p>Imitation learning (IL) algorithms use expert demonstrations to learn a
specific task. Most of the existing approaches assume that all expert
demonstrations are reliable and trustworthy, but what if there exist some
adversarial demonstrations among the given data-set? This may result in poor
decision-making performance. We propose a novel general frame-work to directly
generate a policy from demonstrations that autonomously detect the adversarial
demonstrations and exclude them from the data set. At the same time, it's
sample, time-efficient, and does not require a simulator. To model such
adversarial demonstration we propose a min-max problem that leverages the
entropy of the model to assign weights for each demonstration. This allows us
to learn the behavior using only the correct demonstrations or a mixture of
correct demonstrations.
</p>
<a href="http://arxiv.org/abs/2101.01251" target="_blank">arXiv:2101.01251</a> [<a href="http://arxiv.org/pdf/2101.01251" target="_blank">pdf</a>]

<h2>SpotPatch: Parameter-Efficient Transfer Learning for Mobile Object Detection. (arXiv:2101.01260v1 [cs.CV])</h2>
<h3>Keren Ye, Adriana Kovashka, Mark Sandler, Menglong Zhu, Andrew Howard, Marco Fornoni</h3>
<p>Deep learning based object detectors are commonly deployed on mobile devices
to solve a variety of tasks. For maximum accuracy, each detector is usually
trained to solve one single specific task, and comes with a completely
independent set of parameters. While this guarantees high performance, it is
also highly inefficient, as each model has to be separately downloaded and
stored. In this paper we address the question: can task-specific detectors be
trained and represented as a shared set of weights, plus a very small set of
additional weights for each task? The main contributions of this paper are the
following: 1) we perform the first systematic study of parameter-efficient
transfer learning techniques for object detection problems; 2) we propose a
technique to learn a model patch with a size that is dependent on the
difficulty of the task to be learned, and validate our approach on 10 different
object detection tasks. Our approach achieves similar accuracy as previously
proposed approaches, while being significantly more compact.
</p>
<a href="http://arxiv.org/abs/2101.01260" target="_blank">arXiv:2101.01260</a> [<a href="http://arxiv.org/pdf/2101.01260" target="_blank">pdf</a>]

<h2>Federated Learning-Based Risk-Aware Decision toMitigate Fake Task Impacts on CrowdsensingPlatforms. (arXiv:2101.01266v1 [cs.LG])</h2>
<h3>Zhiyan Chen, Murat Simsek, Burak Kantarci</h3>
<p>Mobile crowdsensing (MCS) leverages distributed and non-dedicated sensing
concepts by utilizing sensors imbedded in a large number of mobile smart
devices. However, the openness and distributed nature of MCS leads to various
vulnerabilities and consequent challenges to address. A malicious user
submitting fake sensing tasks to an MCS platform may be attempting to consume
resources from any number of participants' devices; as well as attempting to
clog the MCS server. In this paper, a novel approach that is based on
horizontal federated learning is proposed to identify fake tasks that contain a
number of independent detection devices and an aggregation entity. Detection
devices are deployed to operate in parallel with each device equipped with a
machine learning (ML) module, and an associated training dataset. Furthermore,
the aggregation module collects the prediction results from individual devices
and determines the final decision with the objective of minimizing the
prediction loss. Loss measurement considers the lost task values with respect
to misclassification, where the final decision utilizes a risk-aware approach
where the risk is formulated as a function of the utility loss. Experimental
results demonstrate that using federated learning-driven illegitimate task
detection with a risk aware aggregation function improves the detection
performance of the traditional centralized framework. Furthermore, the higher
performance of detection and lower loss of utility can be achieved by the
proposed framework. This scheme can even achieve 100%detection accuracy using
small training datasets distributed across devices, while achieving slightly
over an 8% increase in detection improvement over traditional approaches.
</p>
<a href="http://arxiv.org/abs/2101.01266" target="_blank">arXiv:2101.01266</a> [<a href="http://arxiv.org/pdf/2101.01266" target="_blank">pdf</a>]

<h2>Multi-Model Least Squares-Based Recomputation Framework for Large Data Analysis. (arXiv:2101.01271v1 [cs.LG])</h2>
<h3>Wandong Zhang (1 and 2), QM Jonathan Wu (1), Yimin Yang (2 and 3), WG Will Zhao (2 and 4), Hui Zhang (5) ((1) University of Windsor, (2) Lakehead University, (3) Vector Institute for Artificial Intelligence, (4) CEGEP de Ste Foy, (5) Hunan University)</h3>
<p>Most multilayer least squares (LS)-based neural networks are structured with
two separate stages: unsupervised feature encoding and supervised pattern
classification. Once the unsupervised learning is finished, the latent encoding
would be fixed without supervised fine-tuning. However, in complex tasks such
as handling the ImageNet dataset, there are often many more clues that can be
directly encoded, while the unsupervised learning, by definition cannot know
exactly what is useful for a certain task. This serves as the motivation to
retrain the latent space representations to learn some clues that unsupervised
learning has not yet learned. In particular, the error matrix from the output
layer is pulled back to each hidden layer, and the parameters of the hidden
layer are recalculated with Moore-Penrose (MP) inverse for more generalized
representations. In this paper, a recomputation-based multilayer network using
MP inverse (RML-MP) is developed. A sparse RML-MP (SRML-MP) model to boost the
performance of RML-MP is then proposed. The experimental results with varying
training samples (from 3 K to 1.8 M) show that the proposed models provide
better generalization performance than most representation learning algorithms.
</p>
<a href="http://arxiv.org/abs/2101.01271" target="_blank">arXiv:2101.01271</a> [<a href="http://arxiv.org/pdf/2101.01271" target="_blank">pdf</a>]

<h2>GeCo: Quality Counterfactual Explanations in Real Time. (arXiv:2101.01292v1 [cs.LG])</h2>
<h3>Maximilian Schleich, Zixuan Geng, Yihong Zhang, Dan Suciu</h3>
<p>Machine learning is increasingly applied in high-stakes decision making that
directly affect people's lives, and this leads to an increased demand for
systems to explain their decisions. Explanations often take the form of
counterfactuals, which consists of conveying to the end user what she/he needs
to change in order to improve the outcome. Computing counterfactual
explanations is challenging, because of the inherent tension between a rich
semantics of the domain, and the need for real time response. In this paper we
present GeCo, the first system that can compute plausible and feasible
counterfactual explanations in real time. At its core, GeCo relies on a genetic
algorithm, which is customized to favor searching counterfactual explanations
with the smallest number of changes. To achieve real-time performance, we
introduce two novel optimizations: $\Delta$-representation of candidate
counterfactuals, and partial evaluation of the classifier. We compare
empirically GeCo against four other systems described in the literature, and
show that it is the only system that can achieve both high quality explanations
and real time answers.
</p>
<a href="http://arxiv.org/abs/2101.01292" target="_blank">arXiv:2101.01292</a> [<a href="http://arxiv.org/pdf/2101.01292" target="_blank">pdf</a>]

<h2>One vs Previous and Similar Classes Learning -- A Comparative Study. (arXiv:2101.01294v1 [cs.LG])</h2>
<h3>Daniel Cauchi, Adrian Muscat</h3>
<p>When dealing with multi-class classification problems, it is common practice
to build a model consisting of a series of binary classifiers using a learning
paradigm which dictates how the classifiers are built and combined to
discriminate between the individual classes. As new data enters the system and
the model needs updating, these models would often need to be retrained from
scratch. This work proposes three learning paradigms which allow trained models
to be updated without the need of retraining from scratch. A comparative
analysis is performed to evaluate them against a baseline. Results show that
the proposed paradigms are faster than the baseline at updating, with two of
them being faster at training from scratch as well, especially on larger
datasets, while retaining a comparable classification performance.
</p>
<a href="http://arxiv.org/abs/2101.01294" target="_blank">arXiv:2101.01294</a> [<a href="http://arxiv.org/pdf/2101.01294" target="_blank">pdf</a>]

<h2>Composable Geometric Motion Policies using Multi-Task Pullback Bundle Dynamical Systems. (arXiv:2101.01297v1 [cs.RO])</h2>
<h3>Andrew Bylard, Riccardo Bonalli, Marco Pavone</h3>
<p>Despite decades of work in fast reactive planning and control, challenges
remain in developing reactive motion policies on non-Euclidean manifolds and
enforcing constraints while avoiding undesirable potential function local
minima. This work presents a principled method for designing and fusing desired
robot task behaviors into a stable robot motion policy, leveraging the
geometric structure of non-Euclidean manifolds, which are prevalent in robot
configuration and task spaces. Our Pullback Bundle Dynamical Systems (PBDS)
framework drives desired task behaviors and prioritizes tasks using separate
position-dependent and position/velocity-dependent Riemannian metrics,
respectively, thus simplifying individual task design and modular composition
of tasks. For enforcing constraints, we provide a class of metric-based tasks,
eliminating local minima by imposing non-conflicting potential functions only
for goal region attraction. We also provide a geometric optimization problem
for combining tasks inspired by Riemannian Motion Policies (RMPs) that reduces
to a simple least-squares problem, and we show that our approach is
geometrically well-defined. We demonstrate the PBDS framework on the sphere
$\mathbb S^2$ and at 300-500 Hz on a manipulator arm, and we provide task
design guidance and an open-source Julia library implementation. Overall, this
work presents a fast, easy-to-use framework for generating motion policies
without unwanted potential function local minima on general manifolds.
</p>
<a href="http://arxiv.org/abs/2101.01297" target="_blank">arXiv:2101.01297</a> [<a href="http://arxiv.org/pdf/2101.01297" target="_blank">pdf</a>]

<h2>A Linearly Convergent Algorithm for Distributed Principal Component Analysis. (arXiv:2101.01300v1 [cs.LG])</h2>
<h3>Arpita Gang, Waheed U. Bajwa</h3>
<p>Principal Component Analysis (PCA) is the workhorse tool for dimensionality
reduction in this era of big data. While often overlooked, the purpose of PCA
is not only to reduce data dimensionality, but also to yield features that are
uncorrelated. This paper focuses on this dual objective of PCA, namely,
dimensionality reduction and decorrelation of features, which requires
estimating the eigenvectors of a data covariance matrix, as opposed to only
estimating the subspace spanned by the eigenvectors. The ever-increasing volume
of data in the modern world often requires storage of data samples across
multiple machines, which precludes the use of centralized PCA algorithms.
Although a few distributed solutions to the PCA problem have been proposed
recently, convergence guarantees and/or communications overhead of these
solutions remain a concern. With an eye towards communications efficiency, this
paper introduces a feedforward neural network-based one time-scale distributed
PCA algorithm termed Distributed Sanger's Algorithm (DSA) that estimates the
eigenvectors of a data covariance matrix when data are distributed across an
undirected and arbitrarily connected network of machines. Furthermore, the
proposed algorithm is shown to converge linearly to a neighborhood of the true
solution. Numerical results are also shown to demonstrate the efficacy of the
proposed solution.
</p>
<a href="http://arxiv.org/abs/2101.01300" target="_blank">arXiv:2101.01300</a> [<a href="http://arxiv.org/pdf/2101.01300" target="_blank">pdf</a>]

<h2>Adversarial Combinatorial Bandits with General Non-linear Reward Functions. (arXiv:2101.01301v1 [stat.ML])</h2>
<h3>Xi Chen, Yanjun Han, Yining Wang</h3>
<p>In this paper we study the adversarial combinatorial bandit with a known
non-linear reward function, extending existing work on adversarial linear
combinatorial bandit. {The adversarial combinatorial bandit with general
non-linear reward is an important open problem in bandit literature, and it is
still unclear whether there is a significant gap from the case of linear
reward, stochastic bandit, or semi-bandit feedback.} We show that, with $N$
arms and subsets of $K$ arms being chosen at each of $T$ time periods, the
minimax optimal regret is $\widetilde\Theta_{d}(\sqrt{N^d T})$ if the reward
function is a $d$-degree polynomial with $d&lt; K$, and $\Theta_K(\sqrt{N^K T})$
if the reward function is not a low-degree polynomial. {Both bounds are
significantly different from the bound $O(\sqrt{\mathrm{poly}(N,K)T})$ for the
linear case, which suggests that there is a fundamental gap between the linear
and non-linear reward structures.} Our result also finds applications to
adversarial assortment optimization problem in online recommendation. We show
that in the worst-case of adversarial assortment problem, the optimal algorithm
must treat each individual $\binom{N}{K}$ assortment as independent.
</p>
<a href="http://arxiv.org/abs/2101.01301" target="_blank">arXiv:2101.01301</a> [<a href="http://arxiv.org/pdf/2101.01301" target="_blank">pdf</a>]

<h2>CycleSegNet: Object Co-segmentation with Cycle Refinement and Region Correspondence. (arXiv:2101.01308v1 [cs.CV])</h2>
<h3>Guankai Li, Chi Zhang, Guosheng Lin</h3>
<p>Image co-segmentation is an active computer vision task which aims to segment
the common objects in a set of images. Recently, researchers design various
learning-based algorithms to handle the co-segmentation task. The main
difficulty in this task is how to effectively transfer information between
images to infer the common object regions. In this paper, we present
CycleSegNet, a novel framework for the co-segmentation task. Our network design
has two key components: a region correspondence module which is the basic
operation for exchanging information between local image regions, and a cycle
refinement module which utilizes ConvLSTMs to progressively update image
embeddings and exchange information in a cycle manner. Experiment results on
four popular benchmark datasets -- PASCAL VOC dataset, MSRC dataset, Internet
dataset and iCoseg dataset demonstrate that our proposed method significantly
outperforms the existing networks and achieves new state-of-the-art
performance.
</p>
<a href="http://arxiv.org/abs/2101.01308" target="_blank">arXiv:2101.01308</a> [<a href="http://arxiv.org/pdf/2101.01308" target="_blank">pdf</a>]

<h2>Research on Fast Text Recognition Method for Financial Ticket Image. (arXiv:2101.01310v1 [cs.CV])</h2>
<h3>Fukang Tian, Haiyu Wu, Bo Xu</h3>
<p>Currently, deep learning methods have been widely applied in and thus
promoted the development of different fields. In the financial accounting
field, the rapid increase in the number of financial tickets dramatically
increases labor costs; hence, using a deep learning method to relieve the
pressure on accounting is necessary. At present, a few works have applied deep
learning methods to financial ticket recognition. However, first, their
approaches only cover a few types of tickets. In addition, the precision and
speed of their recognition models cannot meet the requirements of practical
financial accounting systems. Moreover, none of the methods provides a detailed
analysis of both the types and content of tickets. Therefore, this paper first
analyzes the different features of 482 kinds of financial tickets, divides all
kinds of financial tickets into three categories and proposes different
recognition patterns for each category. These recognition patterns can meet
almost all types of financial ticket recognition needs. Second, regarding the
fixed format types of financial tickets (accounting for 68.27\% of the total
types of tickets), we propose a simple yet efficient network named the
Financial Ticket Faster Detection network (FTFDNet) based on a Faster RCNN.
Furthermore, according to the characteristics of the financial ticket text, in
order to obtain higher recognition accuracy, the loss function, Region Proposal
Network (RPN), and Non-Maximum Suppression (NMS) are improved to make FTFDNet
focus more on text. Finally, we perform a comparison with the best ticket
recognition model from the ICDAR2019 invoice competition. The experimental
results illustrate that FTFDNet increases the processing speed by 50\% while
maintaining similar precision.
</p>
<a href="http://arxiv.org/abs/2101.01310" target="_blank">arXiv:2101.01310</a> [<a href="http://arxiv.org/pdf/2101.01310" target="_blank">pdf</a>]

<h2>Self-supervised Visual-LiDAR Odometry with Flip Consistency. (arXiv:2101.01322v1 [cs.CV])</h2>
<h3>Bin Li, Mu Hu, Shuling Wang, Lianghao Wang, Xiaojin Gong</h3>
<p>Most learning-based methods estimate ego-motion by utilizing visual sensors,
which suffer from dramatic lighting variations and textureless scenarios. In
this paper, we incorporate sparse but accurate depth measurements obtained from
lidars to overcome the limitation of visual methods. To this end, we design a
self-supervised visual-lidar odometry (Self-VLO) framework. It takes both
monocular images and sparse depth maps projected from 3D lidar points as input,
and produces pose and depth estimations in an end-to-end learning manner,
without using any ground truth labels. To effectively fuse two modalities, we
design a two-pathway encoder to extract features from visual and depth images
and fuse the encoded features with those in decoders at multiple scales by our
fusion module. We also adopt a siamese architecture and design an adaptively
weighted flip consistency loss to facilitate the self-supervised learning of
our VLO. Experiments on the KITTI odometry benchmark show that the proposed
approach outperforms all self-supervised visual or lidar odometries. It also
performs better than fully supervised VOs, demonstrating the power of fusion.
</p>
<a href="http://arxiv.org/abs/2101.01322" target="_blank">arXiv:2101.01322</a> [<a href="http://arxiv.org/pdf/2101.01322" target="_blank">pdf</a>]

<h2>A Trainable Reconciliation Method for Hierarchical Time-Series. (arXiv:2101.01329v1 [cs.LG])</h2>
<h3>Davide Burba, Trista Chen</h3>
<p>In numerous applications, it is required to produce forecasts for multiple
time-series at different hierarchy levels. An obvious example is given by the
supply chain in which demand forecasting may be needed at a store, city, or
country level. The independent forecasts typically do not add up properly
because of the hierarchical constraints, so a reconciliation step is needed. In
this paper, we propose a new general, flexible, and easy-to-implement
reconciliation strategy based on an encoder-decoder neural network. By testing
our method on four real-world datasets, we show that it can consistently reach
or surpass the performance of existing methods in the reconciliation setting.
</p>
<a href="http://arxiv.org/abs/2101.01329" target="_blank">arXiv:2101.01329</a> [<a href="http://arxiv.org/pdf/2101.01329" target="_blank">pdf</a>]

<h2>Equality Saturation for Tensor Graph Superoptimization. (arXiv:2101.01332v1 [cs.AI])</h2>
<h3>Yichen Yang, Phitchaya Mangpo Phothilimtha, Yisu Remy Wang, Max Willsey, Sudip Roy, Jacques Pienaar</h3>
<p>One of the major optimizations employed in deep learning frameworks is graph
rewriting. Production frameworks rely on heuristics to decide if rewrite rules
should be applied and in which order. Prior research has shown that one can
discover more optimal tensor computation graphs if we search for a better
sequence of substitutions instead of relying on heuristics. However, we observe
that existing approaches for tensor graph superoptimization both in production
and research frameworks apply substitutions in a sequential manner. Such
sequential search methods are sensitive to the order in which the substitutions
are applied and often only explore a small fragment of the exponential space of
equivalent graphs. This paper presents a novel technique for tensor graph
superoptimization that employs equality saturation to apply all possible
substitutions at once. We show that our approach can find optimized graphs with
up to 16% speedup over state-of-the-art, while spending on average 48x less
time optimizing.
</p>
<a href="http://arxiv.org/abs/2101.01332" target="_blank">arXiv:2101.01332</a> [<a href="http://arxiv.org/pdf/2101.01332" target="_blank">pdf</a>]

<h2>High Precision Medicine Bottles Vision Online Inspection System and Classification Based on Multi-Features and Ensemble Learning via Independence Test. (arXiv:2101.01362v1 [cs.CV])</h2>
<h3>Le Ma, Xiaoyue Wu, Zhiwei Li, Nan Gao, Jie Liu, Lingfang Sun</h3>
<p>To address the problem of online automatic inspection of drug liquid bottles
in production line, an implantable visual inspection system is designed and the
ensemble learning algorithm for detection is proposed based on multi-features
fusion. A tunnel structure is designed for visual inspection system, which
allows bottles inspection to be automated without changing original
</p>
<a href="http://arxiv.org/abs/2101.01362" target="_blank">arXiv:2101.01362</a> [<a href="http://arxiv.org/pdf/2101.01362" target="_blank">pdf</a>]

<h2>Run-Time Monitoring of Machine Learning for Robotic Perception: A Survey of Emerging Trends. (arXiv:2101.01364v1 [cs.RO])</h2>
<h3>Quazi Marufur Rahman, Peter Corke, Feras Dayoub</h3>
<p>As deep learning continues to dominate all state-of-the-art computer vision
tasks, it is increasingly becoming the essential building blocks for robotic
perception. As a result, the research questions concerning the safety and
reliability of learning-based perception are gaining increased importance.
Although there is an established field that studies safety certification and
convergence guarantee of complex software systems for decision-making during
design-time, the uncertainty in run-time conditions and the unknown future
deployment environments of autonomous systems as well as the complexity of
learning-based perception systems make the generalisation of the verification
results from design-time to run-time problematic. More attention is starting to
shift towards run-time monitoring of performance and reliability of perception
systems with several trends emerging in the literature in the face of such a
challenge. This paper attempts to identify these trends and summarise the
various approaches on the topic.
</p>
<a href="http://arxiv.org/abs/2101.01364" target="_blank">arXiv:2101.01364</a> [<a href="http://arxiv.org/pdf/2101.01364" target="_blank">pdf</a>]

<h2>A Symmetric Loss Perspective of Reliable Machine Learning. (arXiv:2101.01366v1 [stat.ML])</h2>
<h3>Nontawat Charoenphakdee, Jongyeong Lee, Masashi Sugiyama</h3>
<p>When minimizing the empirical risk in binary classification, it is a common
practice to replace the zero-one loss with a surrogate loss to make the
learning objective feasible to optimize. Examples of well-known surrogate
losses for binary classification include the logistic loss, hinge loss, and
sigmoid loss. It is known that the choice of a surrogate loss can highly
influence the performance of the trained classifier and therefore it should be
carefully chosen. Recently, surrogate losses that satisfy a certain symmetric
condition (aka., symmetric losses) have demonstrated their usefulness in
learning from corrupted labels. In this article, we provide an overview of
symmetric losses and their applications. First, we review how a symmetric loss
can yield robust classification from corrupted labels in balanced error rate
(BER) minimization and area under the receiver operating characteristic curve
(AUC) maximization. Then, we demonstrate how the robust AUC maximization method
can benefit natural language processing in the problem where we want to learn
only from relevant keywords and unlabeled documents. Finally, we conclude this
article by discussing future directions, including potential applications of
symmetric losses for reliable machine learning and the design of non-symmetric
losses that can benefit from the symmetric condition.
</p>
<a href="http://arxiv.org/abs/2101.01366" target="_blank">arXiv:2101.01366</a> [<a href="http://arxiv.org/pdf/2101.01366" target="_blank">pdf</a>]

<h2>Similarity Reasoning and Filtration for Image-Text Matching. (arXiv:2101.01368v1 [cs.CV])</h2>
<h3>Haiwen Diao, Ying Zhang, Lin Ma, Huchuan Lu</h3>
<p>Image-text matching plays a critical role in bridging the vision and
language, and great progress has been made by exploiting the global alignment
between image and sentence, or local alignments between regions and words.
However, how to make the most of these alignments to infer more accurate
matching scores is still underexplored. In this paper, we propose a novel
Similarity Graph Reasoning and Attention Filtration (SGRAF) network for
image-text matching. Specifically, the vector-based similarity representations
are firstly learned to characterize the local and global alignments in a more
comprehensive manner, and then the Similarity Graph Reasoning (SGR) module
relying on one graph convolutional neural network is introduced to infer
relation-aware similarities with both the local and global alignments. The
Similarity Attention Filtration (SAF) module is further developed to integrate
these alignments effectively by selectively attending on the significant and
representative alignments and meanwhile casting aside the interferences of
non-meaningful alignments. We demonstrate the superiority of the proposed
method with achieving state-of-the-art performances on the Flickr30K and MSCOCO
datasets, and the good interpretability of SGR and SAF modules with extensive
qualitative experiments and analyses.
</p>
<a href="http://arxiv.org/abs/2101.01368" target="_blank">arXiv:2101.01368</a> [<a href="http://arxiv.org/pdf/2101.01368" target="_blank">pdf</a>]

<h2>An Automatic System to Monitor the Physical Distance and Face Mask Wearing of Construction Workers in COVID-19 Pandemic. (arXiv:2101.01373v1 [cs.CV])</h2>
<h3>Moein Razavi, Hamed Alikhani, Vahid Janfaza, Benyamin Sadeghi, Ehsan Alikhani</h3>
<p>The COVID-19 pandemic has caused many shutdowns in different industries
around the world. Sectors such as infrastructure construction and maintenance
projects have not been suspended due to their significant effect on people's
routine life. In such projects, workers work close together that makes a high
risk of infection. The World Health Organization recommends wearing a face mask
and practicing physical distancing to mitigate the virus's spread. This paper
developed a computer vision system to automatically detect the violation of
face mask wearing and physical distancing among construction workers to assure
their safety on infrastructure projects during the pandemic. For the face mask
detection, the paper collected and annotated 1,000 images, including different
types of face mask wearing, and added them to a pre-existing face mask dataset
to develop a dataset of 1,853 images. Then trained and tested multiple
Tensorflow state-of-the-art object detection models on the face mask dataset
and chose the Faster R-CNN Inception ResNet V2 network that yielded the
accuracy of 99.8%. For physical distance detection, the paper employed the
Faster R-CNN Inception V2 to detect people. A transformation matrix was used to
eliminate the camera angle's effect on the object distances on the image. The
Euclidian distance used the pixels of the transformed image to compute the
actual distance between people. A threshold of six feet was considered to
capture physical distance violation. The paper also used transfer learning for
training the model. The final model was applied on four videos of road
maintenance projects in Houston, TX, that effectively detected the face mask
and physical distance. We recommend that construction owners use the proposed
system to enhance construction workers' safety in the pandemic situation.
</p>
<a href="http://arxiv.org/abs/2101.01373" target="_blank">arXiv:2101.01373</a> [<a href="http://arxiv.org/pdf/2101.01373" target="_blank">pdf</a>]

<h2>Understanding the Ability of Deep Neural Networks to Count Connected Components in Images. (arXiv:2101.01386v1 [cs.CV])</h2>
<h3>Shuyue Guan, Murray Loew</h3>
<p>Humans can count very fast by subitizing, but slow substantially as the
number of objects increases. Previous studies have shown a trained deep neural
network (DNN) detector can count the number of objects in an amount of time
that increases slowly with the number of objects. Such a phenomenon suggests
the subitizing ability of DNNs, and unlike humans, it works equally well for
large numbers. Many existing studies have successfully applied DNNs to object
counting, but few studies have studied the subitizing ability of DNNs and its
interpretation. In this paper, we found DNNs do not have the ability to
generally count connected components. We provided experiments to support our
conclusions and explanations to understand the results and phenomena of these
experiments. We proposed three ML-learnable characteristics to verify learnable
problems for ML models, such as DNNs, and explain why DNNs work for specific
counting problems but cannot generally count connected components.
</p>
<a href="http://arxiv.org/abs/2101.01386" target="_blank">arXiv:2101.01386</a> [<a href="http://arxiv.org/pdf/2101.01386" target="_blank">pdf</a>]

<h2>VersatileGait: A Large-Scale Synthetic Gait Dataset with Fine-GrainedAttributes and Complicated Scenarios. (arXiv:2101.01394v1 [cs.CV])</h2>
<h3>Huanzhang Dou, Wenhu Zhang, Pengyi Zhang, Yuhan Zhao, Songyuan Li, Zequn Qin, Fei Wu, Lin Dong, Xi Li</h3>
<p>With the motivation of practical gait recognition applications, we propose to
automatically create a large-scale synthetic gait dataset (called
VersatileGait) by a game engine, which consists of around one million
silhouette sequences of 11,000 subjects with fine-grained attributes in various
complicated scenarios. Compared with existing real gait datasets with limited
samples and simple scenarios, the proposed VersatileGait dataset possesses
several nice properties, including huge dataset size, high sample diversity,
high-quality annotations, multi-pitch angles, small domain gap with the real
one, etc. Furthermore, we investigate the effectiveness of our dataset (e.g.,
domain transfer after pretraining). Then, we use the fine-grained attributes
from VersatileGait to promote gait recognition in both accuracy and speed, and
meanwhile justify the gait recognition performance under multi-pitch angle
settings. Additionally, we explore a variety of potential applications for
research.Extensive experiments demonstrate the value and effective-ness of the
proposed VersatileGait in gait recognition along with its associated
applications. We will release both VersatileGait and its corresponding data
generation toolkit for further studies.
</p>
<a href="http://arxiv.org/abs/2101.01394" target="_blank">arXiv:2101.01394</a> [<a href="http://arxiv.org/pdf/2101.01394" target="_blank">pdf</a>]

<h2>Relaxed Conditional Image Transfer for Semi-supervised Domain Adaptation. (arXiv:2101.01400v1 [cs.CV])</h2>
<h3>Qijun Luo, Zhili Liu, Lanqing Hong, Chongxuan Li, Kuo Yang, Liyuan Wang, Fengwei Zhou, Guilin Li, Zhenguo Li, Jun Zhu</h3>
<p>Semi-supervised domain adaptation (SSDA), which aims to learn models in a
partially labeled target domain with the assistance of the fully labeled source
domain, attracts increasing attention in recent years. To explicitly leverage
the labeled data in both domains, we naturally introduce a conditional GAN
framework to transfer images without changing the semantics in SSDA. However,
we identify a label-domination problem in such an approach. In fact, the
generator tends to overlook the input source image and only memorizes
prototypes of each class, which results in unsatisfactory adaptation
performance. To this end, we propose a simple yet effective Relaxed conditional
GAN (Relaxed cGAN) framework. Specifically, we feed the image without its label
to our generator. In this way, the generator has to infer the semantic
information of input data. We formally prove that its equilibrium is desirable
and empirically validate its practical convergence and effectiveness in image
transfer. Additionally, we propose several techniques to make use of unlabeled
data in the target domain, enhancing the model in SSDA settings. We validate
our method on the well-adopted datasets: Digits, DomainNet, and Office-Home. We
achieve state-of-the-art performance on DomainNet, Office-Home and most digit
benchmarks in low-resource and high-resource settings.
</p>
<a href="http://arxiv.org/abs/2101.01400" target="_blank">arXiv:2101.01400</a> [<a href="http://arxiv.org/pdf/2101.01400" target="_blank">pdf</a>]

<h2>To do or not to do: cost-sensitive causal decision-making. (arXiv:2101.01407v1 [cs.LG])</h2>
<h3>Diego Olaya, Wouter Verbeke, Jente Van Belle, Marie-Anne Guerry</h3>
<p>Causal classification models are adopted across a variety of operational
business processes to predict the effect of a treatment on a categorical
business outcome of interest depending on the process instance characteristics.
This allows optimizing operational decision-making and selecting the optimal
treatment to apply in each specific instance, with the aim of maximizing the
positive outcome rate. While various powerful approaches have been presented in
the literature for learning causal classification models, no formal framework
has been elaborated for optimal decision-making based on the estimated
individual treatment effects, given the cost of the various treatments and the
benefit of the potential outcomes.

In this article, we therefore extend upon the expected value framework and
formally introduce a cost-sensitive decision boundary for double binary causal
classification, which is a linear function of the estimated individual
treatment effect, the positive outcome probability and the cost and benefit
parameters of the problem setting. The boundary allows causally classifying
instances in the positive and negative treatment class to maximize the expected
causal profit, which is introduced as the objective at hand in cost-sensitive
causal classification. We introduce the expected causal profit ranker which
ranks instances for maximizing the expected causal profit at each possible
threshold for causally classifying instances and differs from the conventional
ranking approach based on the individual treatment effect. The proposed ranking
approach is experimentally evaluated on synthetic and marketing campaign data
sets. The results indicate that the presented ranking method effectively
outperforms the cost-insensitive ranking approach and allows boosting
profitability.
</p>
<a href="http://arxiv.org/abs/2101.01407" target="_blank">arXiv:2101.01407</a> [<a href="http://arxiv.org/pdf/2101.01407" target="_blank">pdf</a>]

<h2>Support Vector Machine and YOLO for a Mobile Food Grading System. (arXiv:2101.01418v1 [cs.CV])</h2>
<h3>Lili Zhu, Petros Spachos</h3>
<p>Food quality and safety are of great concern to society since it is an
essential guarantee not only for human health but also for social development,
and stability. Ensuring food quality and safety is a complex process. All food
processing stages should be considered, from cultivating, harvesting and
storage to preparation and consumption. Grading is one of the essential
processes to control food quality. This paper proposed a mobile visual-based
system to evaluate food grading. Specifically, the proposed system acquires
images of bananas when they are on moving conveyors. A two-layer image
processing system based on machine learning is used to grade bananas, and these
two layers are allocated on edge devices and cloud servers, respectively.
Support Vector Machine (SVM) is the first layer to classify bananas based on an
extracted feature vector composed of color and texture features. Then, the a
You Only Look Once (YOLO) v3 model further locating the peel's defected area
and determining if the inputs belong to the mid-ripened or well-ripened class.
According to experimental results, the first layer's performance achieved an
accuracy of 98.5% while the accuracy of the second layer is 85.7%, and the
overall accuracy is 96.4%.
</p>
<a href="http://arxiv.org/abs/2101.01418" target="_blank">arXiv:2101.01418</a> [<a href="http://arxiv.org/pdf/2101.01418" target="_blank">pdf</a>]

<h2>Data-Driven Copy-Paste Imputation for Energy Time Series. (arXiv:2101.01423v1 [cs.LG])</h2>
<h3>Moritz Weber, Marian Turowski, H&#xfc;seyin K. &#xc7;akmak, Ralf Mikut, Uwe K&#xfc;hnapfel, Veit Hagenmeyer</h3>
<p>A cornerstone of the worldwide transition to smart grids are smart meters.
Smart meters typically collect and provide energy time series that are vital
for various applications, such as grid simulations, fault-detection, load
forecasting, load analysis, and load management. Unfortunately, these time
series are often characterized by missing values that must be handled before
the data can be used. A common approach to handle missing values in time series
is imputation. However, existing imputation methods are designed for power time
series and do not take into account the total energy of gaps, resulting in
jumps or constant shifts when imputing energy time series. In order to overcome
these issues, the present paper introduces the new Copy-Paste Imputation (CPI)
method for energy time series. The CPI method copies data blocks with similar
properties and pastes them into gaps of the time series while preserving the
total energy of each gap. The new method is evaluated on a real-world dataset
that contains six shares of artificially inserted missing values between 1 and
30%. It outperforms by far the three benchmark imputation methods selected for
comparison. The comparison furthermore shows that the CPI method uses matching
patterns and preserves the total energy of each gap while requiring only a
moderate run-time.
</p>
<a href="http://arxiv.org/abs/2101.01423" target="_blank">arXiv:2101.01423</a> [<a href="http://arxiv.org/pdf/2101.01423" target="_blank">pdf</a>]

<h2>Het-node2vec: second order random walk sampling for heterogeneous multigraphs embedding. (arXiv:2101.01425v1 [cs.LG])</h2>
<h3>Giorgio Valentini, Elena Casiraghi, Luca Cappelletti, Vida Ravanmehr, Tommaso Fontana, Justin Reese, Peter Robinson</h3>
<p>We introduce a set of algorithms (Het-node2vec) that extend the original
node2vec node-neighborhood sampling method to heterogeneous multigraphs, i.e.
networks characterized by multiple types of nodes and edges. The resulting
random walk samples capture both the structural characteristics of the graph
and the semantics of the different types of nodes and edges. The proposed
algorithms can focus their attention on specific node or edge types, allowing
accurate representations also for underrepresented types of nodes/edges that
are of interest for the prediction problem under investigation. These rich and
well-focused representations can boost unsupervised and supervised learning on
heterogeneous graphs.
</p>
<a href="http://arxiv.org/abs/2101.01425" target="_blank">arXiv:2101.01425</a> [<a href="http://arxiv.org/pdf/2101.01425" target="_blank">pdf</a>]

<h2>Convergence and finite sample approximations of entropic regularized Wasserstein distances in Gaussian and RKHS settings. (arXiv:2101.01429v1 [stat.ML])</h2>
<h3>Minh Ha Quang</h3>
<p>This work studies the convergence and finite sample approximations of
entropic regularized Wasserstein distances in the Hilbert space setting. Our
first main result is that for Gaussian measures on an infinite-dimensional
Hilbert space, convergence in the 2-Sinkhorn divergence is {\it strictly
weaker} than convergence in the exact 2-Wasserstein distance. Specifically, a
sequence of centered Gaussian measures converges in the 2-Sinkhorn divergence
if the corresponding covariance operators converge in the Hilbert-Schmidt norm.
This is in contrast to the previous known result that a sequence of centered
Gaussian measures converges in the exact 2-Wasserstein distance if and only if
the covariance operators converge in the trace class norm. In the reproducing
kernel Hilbert space (RKHS) setting, the {\it kernel Gaussian-Sinkhorn
divergence}, which is the Sinkhorn divergence between Gaussian measures defined
on an RKHS, defines a semi-metric on the set of Borel probability measures on a
Polish space, given a characteristic kernel on that space. With the
Hilbert-Schmidt norm convergence, we obtain {\it dimension-independent}
convergence rates for finite sample approximations of the kernel
Gaussian-Sinkhorn divergence, with the same order as the Maximum Mean
Discrepancy. These convergence rates apply in particular to Sinkhorn divergence
between Gaussian measures on Euclidean and infinite-dimensional Hilbert spaces.
The sample complexity for the 2-Wasserstein distance between Gaussian measures
on Euclidean space, while dimension-dependent and larger than that of the
Sinkhorn divergence, is exponentially faster than the worst case scenario in
the literature.
</p>
<a href="http://arxiv.org/abs/2101.01429" target="_blank">arXiv:2101.01429</a> [<a href="http://arxiv.org/pdf/2101.01429" target="_blank">pdf</a>]

<h2>Generating Informative CVE Description From ExploitDB Posts by Extractive Summarization. (arXiv:2101.01431v1 [cs.LG])</h2>
<h3>Jiamou Sun, Zhenchang Xing, Hao Guo, Deheng Ye, Xiaohong Li, Xiwei Xu, Liming Zhu</h3>
<p>ExploitDB is one of the important public websites, which contributes a large
number of vulnerabilities to official CVE database. Over 60\% of these
vulnerabilities have high- or critical-security risks. Unfortunately, over 73\%
of exploits appear publicly earlier than the corresponding CVEs, and about 40\%
of exploits do not even have CVEs. To assist in documenting CVEs for the
ExploitDB posts, we propose an open information method to extract 9 key
vulnerability aspects (vulnerable product/version/component, vulnerability
type, vendor, attacker type, root cause, attack vector and impact) from the
verbose and noisy ExploitDB posts. The extracted aspects from an ExploitDB post
are then composed into a CVE description according to the suggested CVE
description templates, which is must-provided information for requesting new
CVEs. Through the evaluation on 13,017 manually labeled sentences and the
statistically sampling of 3,456 extracted aspects, we confirm the high accuracy
of our extraction method. Compared with 27,230 reference CVE descriptions. Our
composed CVE descriptions achieve high ROUGH-L (0.38), a longest common
subsequence based metric for evaluating text summarization methods.
</p>
<a href="http://arxiv.org/abs/2101.01431" target="_blank">arXiv:2101.01431</a> [<a href="http://arxiv.org/pdf/2101.01431" target="_blank">pdf</a>]

<h2>Data Quality Measures and Efficient Evaluation Algorithms for Large-Scale High-Dimensional Data. (arXiv:2101.01441v1 [cs.LG])</h2>
<h3>Hyeongmin Cho, Sangkyun Lee</h3>
<p>Machine learning has been proven to be effective in various application
areas, such as object and speech recognition on mobile systems. Since a
critical key to machine learning success is the availability of large training
data, many datasets are being disclosed and published online. From a data
consumer or manager point of view, measuring data quality is an important first
step in the learning process. We need to determine which datasets to use,
update, and maintain. However, not many practical ways to measure data quality
are available today, especially when it comes to large-scale high-dimensional
data, such as images and videos. This paper proposes two data quality measures
that can compute class separability and in-class variability, the two important
aspects of data quality, for a given dataset. Classical data quality measures
tend to focus only on class separability; however, we suggest that in-class
variability is another important data quality factor. We provide efficient
algorithms to compute our quality measures based on random projections and
bootstrapping with statistical benefits on large-scale high-dimensional data.
In experiments, we show that our measures are compatible with classical
measures on small-scale data and can be computed much more efficiently on
large-scale high-dimensional datasets.
</p>
<a href="http://arxiv.org/abs/2101.01441" target="_blank">arXiv:2101.01441</a> [<a href="http://arxiv.org/pdf/2101.01441" target="_blank">pdf</a>]

<h2>CycleGAN for Interpretable Online EMT Compensation. (arXiv:2101.01444v1 [cs.CV])</h2>
<h3>Henry Krumb, Dhritimaan Das, Romol Chadda, Anirban Mukhopadhyay</h3>
<p>Purpose: Electromagnetic Tracking (EMT) can partially replace X-ray guidance
in minimally invasive procedures, reducing radiation in the OR. However, in
this hybrid setting, EMT is disturbed by metallic distortion caused by the
X-ray device. We plan to make hybrid navigation clinical reality to reduce
radiation exposure for patients and surgeons, by compensating EMT error.

Methods: Our online compensation strategy exploits cycle-consistent
generative adversarial neural networks (CycleGAN). 3D positions are translated
from various bedside environments to their bench equivalents. Domain-translated
points are fine-tuned to reduce error in the bench domain. We evaluate our
compensation approach in a phantom experiment.

Results: Since the domain-translation approach maps distorted points to their
lab equivalents, predictions are consistent among different C-arm environments.
Error is successfully reduced in all evaluation environments. Our qualitative
phantom experiment demonstrates that our approach generalizes well to an unseen
C-arm environment.

Conclusion: Adversarial, cycle-consistent training is an explicable,
consistent and thus interpretable approach for online error compensation.
Qualitative assessment of EMT error compensation gives a glimpse to the
potential of our method for rotational error compensation.
</p>
<a href="http://arxiv.org/abs/2101.01444" target="_blank">arXiv:2101.01444</a> [<a href="http://arxiv.org/pdf/2101.01444" target="_blank">pdf</a>]

<h2>Dataset on Bi- and Multi-Nucleated Tumor Cells in Canine Cutaneous Mast Cell Tumors. (arXiv:2101.01445v1 [cs.CV])</h2>
<h3>Christof A. Bertram, Taryn A. Donovan, Marco Tecilla, Florian Bartenschlager, Marco Fragoso, Frauke Wilm, Christian Marzahl, Katharina Breininger, Andreas Maier, Robert Klopfleisch, Marc Aubreville</h3>
<p>Tumor cells with two nuclei (binucleated cells, BiNC) or more nuclei
(multinucleated cells, MuNC) indicate an increased amount of cellular genetic
material which is thought to facilitate oncogenesis, tumor progression and
treatment resistance. In canine cutaneous mast cell tumors (ccMCT),
binucleation and multinucleation are parameters used in cytologic and
histologic grading schemes (respectively) which correlate with poor patient
outcome. For this study, we created the first open source data-set with 19,983
annotations of BiNC and 1,416 annotations of MuNC in 32 histological whole
slide images of ccMCT. Labels were created by a pathologist and an
algorithmic-aided labeling approach with expert review of each generated
candidate. A state-of-the-art deep learning-based model yielded an $F_1$ score
of 0.675 for BiNC and 0.623 for MuNC on 11 test whole slide images. In regions
of interest ($2.37 mm^2$) extracted from these test images, 6 pathologists had
an object detection performance between 0.270 - 0.526 for BiNC and 0.316 -
0.622 for MuNC, while our model archived an $F_1$ score of 0.667 for BiNC and
0.685 for MuNC. This open dataset can facilitate development of automated image
analysis for this task and may thereby help to promote standardization of this
facet of histologic tumor prognostication.
</p>
<a href="http://arxiv.org/abs/2101.01445" target="_blank">arXiv:2101.01445</a> [<a href="http://arxiv.org/pdf/2101.01445" target="_blank">pdf</a>]

<h2>WildDeepfake: A Challenging Real-World Dataset for Deepfake Detection. (arXiv:2101.01456v1 [cs.CV])</h2>
<h3>Bojia Zi, Minghao Chang, Jingjing Chen, Xingjun Ma, Yu-Gang Jiang</h3>
<p>In recent years, the abuse of a face swap technique called deepfake Deepfake
has raised enormous public concerns. So far, a large number of deepfake videos
(known as "deepfakes") have been crafted and uploaded to the internet, calling
for effective countermeasures. One promising countermeasure against deepfakes
is deepfake detection. Several deepfake datasets have been released to support
the training and testing of deepfake detectors, such as DeepfakeDetection and
FaceForensics++. While this has greatly advanced deepfake detection, most of
the real videos in these datasets are filmed with a few volunteer actors in
limited scenes, and the fake videos are crafted by researchers using a few
popular deepfake softwares. Detectors developed on these datasets may become
less effective against real-world deepfakes on the internet. To better support
detection against real-world deepfakes, in this paper, we introduce a new
dataset WildDeepfake, which consists of 7,314 face sequences extracted from 707
deepfake videos collected completely from the internet. WildDeepfake is a small
dataset that can be used, in addition to existing datasets, to develop and test
the effectiveness of deepfake detectors against real-world deepfakes. We
conduct a systematic evaluation of a set of baseline detection networks on both
existing and our WildDeepfake datasets, and show that WildDeepfake is indeed a
more challenging dataset, where the detection performance can decrease
drastically. We also propose two (eg. 2D and 3D) Attention-based Deepfake
Detection Networks (ADDNets) to leverage the attention masks on real/fake faces
for improved detection. We empirically verify the effectiveness of ADDNets on
both existing datasets and WildDeepfake. The dataset is available
at:https://github.com/deepfakeinthewild/deepfake-in-the-wild.
</p>
<a href="http://arxiv.org/abs/2101.01456" target="_blank">arXiv:2101.01456</a> [<a href="http://arxiv.org/pdf/2101.01456" target="_blank">pdf</a>]

<h2>PointCutMix: Regularization Strategy for Point Cloud Classification. (arXiv:2101.01461v1 [cs.CV])</h2>
<h3>Jinlai Zhang, Lvjie Chen, Bo Ouyang, Binbin Liu, Jihong Zhu, Yujing Chen, Yanmei Meng, Danfeng Wu</h3>
<p>3D point cloud analysis has received increasing attention in recent years,
however, the diversity and availability of point cloud datasets are still
limited. We therefore present PointCutMix, a simple but effective method for
augmentation in point cloud. In our method, after finding the optimal
assignment between two point clouds, we replace some points in one point cloud
by its counterpart point in another point cloud. Our strategy consistently and
significantly improves the performance across various models and datasets.
Surprisingly, when it is used as a defense method, it shows far superior
performance to the SOTA defense algorithm. The code is available
at:https://github.com/cuge1995/PointCutMix
</p>
<a href="http://arxiv.org/abs/2101.01461" target="_blank">arXiv:2101.01461</a> [<a href="http://arxiv.org/pdf/2101.01461" target="_blank">pdf</a>]

<h2>Learning Sign-Constrained Support Vector Machines. (arXiv:2101.01473v1 [cs.LG])</h2>
<h3>Kenya Tajima, Takahiko Henmi, Kohei Tsuchida, Esmeraldo Ronnie R. Zara, Tsuyoshi Kato</h3>
<p>Domain knowledge is useful to improve the generalization performance of
learning machines. Sign constraints are a handy representation to combine
domain knowledge with learning machine. In this paper, we consider constraining
the signs of the weight coefficients in learning the linear support vector
machine, and develop two optimization algorithms for minimizing the empirical
risk under the sign constraints. One of the two algorithms is based on the
projected gradient method, in which each iteration of the projected gradient
method takes $O(nd)$ computational cost and the sublinear convergence of the
objective error is guaranteed. The second algorithm is based on the Frank-Wolfe
method that also converges sublinearly and possesses a clear termination
criterion. We show that each iteration of the Frank-Wolfe also requires $O(nd)$
cost. Furthermore, we derive the explicit expression for the minimal iteration
number to ensure an $\epsilon$-accurate solution by analyzing the curvature of
the objective function. Finally, we empirically demonstrate that the sign
constraints are a promising technique when similarities to the training
examples compose the feature vector.
</p>
<a href="http://arxiv.org/abs/2101.01473" target="_blank">arXiv:2101.01473</a> [<a href="http://arxiv.org/pdf/2101.01473" target="_blank">pdf</a>]

<h2>Scale-Aware Network with Regional and Semantic Attentions for Crowd Counting under Cluttered Background. (arXiv:2101.01479v1 [cs.CV])</h2>
<h3>Qiaosi Yi, Yunxing Liu, Aiwen Jiang, Juncheng Li, Kangfu Mei, Mingwen Wang</h3>
<p>Crowd counting is an important task that shown great application value in
public safety-related fields, which has attracted increasing attention in
recent years. In the current research, the accuracy of counting numbers and
crowd density estimation are the main concerns. Although the emergence of deep
learning has greatly promoted the development of this field, crowd counting
under cluttered background is still a serious challenge. In order to solve this
problem, we propose a ScaleAware Crowd Counting Network (SACCN) with regional
and semantic attentions. The proposed SACCN distinguishes crowd and background
by applying regional and semantic self-attention mechanisms on the shallow
layers and deep layers, respectively. Moreover, the asymmetric multi-scale
module (AMM) is proposed to deal with the problem of scale diversity, and
regional attention based dense connections and skip connections are designed to
alleviate the variations on crowd scales. Extensive experimental results on
multiple public benchmarks demonstrate that our proposed SACCN achieves
satisfied superior performances and outperform most state-of-the-art methods.
All codes and pretrained models will be released soon.
</p>
<a href="http://arxiv.org/abs/2101.01479" target="_blank">arXiv:2101.01479</a> [<a href="http://arxiv.org/pdf/2101.01479" target="_blank">pdf</a>]

<h2>Local Propagation for Few-Shot Learning. (arXiv:2101.01480v1 [cs.CV])</h2>
<h3>Yann Lifchitz, Yannis Avrithis, Sylvaine Picard</h3>
<p>The challenge in few-shot learning is that available data is not enough to
capture the underlying distribution. To mitigate this, two emerging directions
are (a) using local image representations, essentially multiplying the amount
of data by a constant factor, and (b) using more unlabeled data, for instance
by transductive inference, jointly on a number of queries. In this work, we
bring these two ideas together, introducing \emph{local propagation}. We treat
local image features as independent examples, we build a graph on them and we
use it to propagate both the features themselves and the labels, known and
unknown. Interestingly, since there is a number of features per image, even a
single query gives rise to transductive inference. As a result, we provide a
universally safe choice for few-shot inference under both non-transductive and
transductive settings, improving accuracy over corresponding methods. This is
in contrast to existing solutions, where one needs to choose the method
depending on the quantity of available data.
</p>
<a href="http://arxiv.org/abs/2101.01480" target="_blank">arXiv:2101.01480</a> [<a href="http://arxiv.org/pdf/2101.01480" target="_blank">pdf</a>]

<h2>Weight-of-evidence 2.0 with shrinkage and spline-binning. (arXiv:2101.01494v1 [stat.ML])</h2>
<h3>Jakob Raymaekers, Wouter Verbeke, Tim Verdonck</h3>
<p>In many practical applications, such as fraud detection, credit risk modeling
or medical decision making, classification models for assigning instances to a
predefined set of classes are required to be both precise as well as
interpretable. Linear modeling methods such as logistic regression are often
adopted, since they offer an acceptable balance between precision and
interpretability. Linear methods, however, are not well equipped to handle
categorical predictors with high-cardinality or to exploit non-linear relations
in the data. As a solution, data preprocessing methods such as
weight-of-evidence are typically used for transforming the predictors. The
binning procedure that underlies the weight-of-evidence approach, however, has
been little researched and typically relies on ad-hoc or expert driven
procedures. The objective in this paper, therefore, is to propose a formalized,
data-driven and powerful method.

To this end, we explore the discretization of continuous variables through
the binning of spline functions, which allows for capturing non-linear effects
in the predictor variables and yields highly interpretable predictors taking
only a small number of discrete values. Moreover, we extend upon the
weight-of-evidence approach and propose to estimate the proportions using
shrinkage estimators. Together, this offers an improved ability to exploit both
non-linear and categorical predictors for achieving increased classification
precision, while maintaining interpretability of the resulting model and
decreasing the risk of overfitting.

We present the results of a series of experiments in a fraud detection
setting, which illustrate the effectiveness of the presented approach. We
facilitate reproduction of the presented results and adoption of the proposed
approaches by providing both the dataset and the code for implementing the
experiments and the presented approach.
</p>
<a href="http://arxiv.org/abs/2101.01494" target="_blank">arXiv:2101.01494</a> [<a href="http://arxiv.org/pdf/2101.01494" target="_blank">pdf</a>]

<h2>Hierarchical Sampler for Probabilistic Programs via Separation of Control and Data. (arXiv:2101.01502v1 [cs.LG])</h2>
<h3>Ichiro Hasuo, Yuichiro Oyabu, Clovis Eberhart, Kohei Suenaga, Kenta Cho, Shin-ya Katsumata</h3>
<p>We introduce a novel sampling algorithm for Bayesian inference on imperative
probabilistic programs. It features a hierarchical architecture that separates
control flows from data: the top-level samples a control flow, and the bottom
level samples data values along the control flow picked by the top level. This
separation allows us to plug various language-based analysis techniques in
probabilistic program sampling; specifically, we use logical backward
propagation of observations for sampling efficiency. We implemented our
algorithm on top of Anglican. The experimental results demonstrate our
algorithm's efficiency, especially for programs with while loops and rare
observations.
</p>
<a href="http://arxiv.org/abs/2101.01502" target="_blank">arXiv:2101.01502</a> [<a href="http://arxiv.org/pdf/2101.01502" target="_blank">pdf</a>]

<h2>Structured Machine Learning Tools for Modelling Characteristics of Guided Waves. (arXiv:2101.01506v1 [stat.ML])</h2>
<h3>Marcus Haywood-Alexander, Nikolaos Dervilis, Keith Worden, Elizabeth J. Cross, Robin S. Mills, Timothy J. Rogers</h3>
<p>The use of ultrasonic guided waves to probe the materials/structures for
damage continues to increase in popularity for non-destructive evaluation (NDE)
and structural health monitoring (SHM). The use of high-frequency waves such as
these offers an advantage over low-frequency methods from their ability to
detect damage on a smaller scale. However, in order to assess damage in a
structure, and implement any NDE or SHM tool, knowledge of the behaviour of a
guided wave throughout the material/structure is important (especially when
designing sensor placement for SHM systems). Determining this behaviour is
extremely diffcult in complex materials, such as fibre-matrix composites, where
unique phenomena such as continuous mode conversion takes place. This paper
introduces a novel method for modelling the feature-space of guided waves in a
composite material. This technique is based on a data-driven model, where prior
physical knowledge can be used to create structured machine learning tools;
where constraints are applied to provide said structure. The method shown makes
use of Gaussian processes, a full Bayesian analysis tool, and in this paper it
is shown how physical knowledge of the guided waves can be utilised in
modelling using an ML tool. This paper shows that through careful consideration
when applying machine learning techniques, more robust models can be generated
which offer advantages such as extrapolation ability and physical
interpretation.
</p>
<a href="http://arxiv.org/abs/2101.01506" target="_blank">arXiv:2101.01506</a> [<a href="http://arxiv.org/pdf/2101.01506" target="_blank">pdf</a>]

<h2>Modeling Global Semantics for Question Answering over Knowledge Bases. (arXiv:2101.01510v1 [cs.AI])</h2>
<h3>Peiyun Wu, Yunjie Wu, Linjuan Wu, Xiaowang Zhang, Zhiyong Feng</h3>
<p>Semantic parsing, as an important approach to question answering over
knowledge bases (KBQA), transforms a question into the complete query graph for
further generating the correct logical query. Existing semantic parsing
approaches mainly focus on relations matching with paying less attention to the
underlying internal structure of questions (e.g., the dependencies and
relations between all entities in a question) to select the query graph. In
this paper, we present a relational graph convolutional network (RGCN)-based
model gRGCN for semantic parsing in KBQA. gRGCN extracts the global semantics
of questions and their corresponding query graphs, including structure
semantics via RGCN and relational semantics (label representation of relations
between entities) via a hierarchical relation attention mechanism. Experiments
evaluated on benchmarks show that our model outperforms off-the-shelf models.
</p>
<a href="http://arxiv.org/abs/2101.01510" target="_blank">arXiv:2101.01510</a> [<a href="http://arxiv.org/pdf/2101.01510" target="_blank">pdf</a>]

<h2>Deep Class-Specific Affinity-Guided Convolutional Network for Multimodal Unpaired Image Segmentation. (arXiv:2101.01513v1 [cs.CV])</h2>
<h3>Jingkun Chen, Wenqi Li, Hongwei Li, Jianguo Zhang</h3>
<p>Multi-modal medical image segmentation plays an essential role in clinical
diagnosis. It remains challenging as the input modalities are often not
well-aligned spatially. Existing learning-based methods mainly consider sharing
trainable layers across modalities and minimizing visual feature discrepancies.
While the problem is often formulated as joint supervised feature learning,
multiple-scale features and class-specific representation have not yet been
explored. In this paper, we propose an affinity-guided fully convolutional
network for multimodal image segmentation. To learn effective representations,
we design class-specific affinity matrices to encode the knowledge of
hierarchical feature reasoning, together with the shared convolutional layers
to ensure the cross-modality generalization. Our affinity matrix does not
depend on spatial alignments of the visual features and thus allows us to train
with unpaired, multimodal inputs. We extensively evaluated our method on two
public multimodal benchmark datasets and outperform state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2101.01513" target="_blank">arXiv:2101.01513</a> [<a href="http://arxiv.org/pdf/2101.01513" target="_blank">pdf</a>]

<h2>Handling Hard Affine SDP Shape Constraints in RKHSs. (arXiv:2101.01519v1 [stat.ML])</h2>
<h3>Pierre-Cyril Aubin-Frankowski, Zoltan Szabo</h3>
<p>Shape constraints, such as non-negativity, monotonicity, convexity or
supermodularity, play a key role in various applications of machine learning
and statistics. However, incorporating this side information into predictive
models in a hard way (for example at all points of an interval) for rich
function classes is a notoriously challenging problem. We propose a unified and
modular convex optimization framework, relying on second-order cone (SOC)
tightening, to encode hard affine SDP constraints on function derivatives, for
models belonging to vector-valued reproducing kernel Hilbert spaces (vRKHSs).
The modular nature of the proposed approach allows to simultaneously handle
multiple shape constraints, and to tighten an infinite number of constraints
into finitely many. We prove the consistency of the proposed scheme and that of
its adaptive variant, leveraging geometric properties of vRKHSs. The efficiency
of the approach is illustrated in the context of shape optimization,
safety-critical control and econometrics.
</p>
<a href="http://arxiv.org/abs/2101.01519" target="_blank">arXiv:2101.01519</a> [<a href="http://arxiv.org/pdf/2101.01519" target="_blank">pdf</a>]

<h2>On the Control of Attentional Processes in Vision. (arXiv:2101.01533v1 [cs.AI])</h2>
<h3>John K. Tsotsos, Omar Abid, Iuliia Kotseruba, Markus D. Solbach</h3>
<p>The study of attentional processing in vision has a long and deep history.
Recently, several papers have presented insightful perspectives into how the
coordination of multiple attentional functions in the brain might occur. These
begin with experimental observations and the authors propose structures,
processes, and computations that might explain those observations. Here, we
consider a perspective that past works have not, as a complementary approach to
the experimentally-grounded ones. We approach the same problem as past authors
but from the other end of the computational spectrum, from the problem nature,
as Marr's Computational Level would prescribe. What problem must the brain
solve when orchestrating attentional processes in order to successfully
complete one of the myriad possible visuospatial tasks at which we as humans
excel? The hope, of course, is for the approaches to eventually meet and thus
form a complete theory, but this is likely not soon. We make the first steps
towards this by addressing the necessity of attentional control, examining the
breadth and computational difficulty of the visuospatial and attentional tasks
seen in human behavior, and suggesting a sketch of how attentional control
might arise in the brain. The key conclusions of this paper are that an
executive controller is necessary for human attentional function in vision, and
that there is a 'first principles' computational approach to its understanding
that is complementary to the previous approaches that focus on modelling or
learning from experimental observations directly.
</p>
<a href="http://arxiv.org/abs/2101.01533" target="_blank">arXiv:2101.01533</a> [<a href="http://arxiv.org/pdf/2101.01533" target="_blank">pdf</a>]

<h2>Noise Sensitivity-Based Energy Efficient and Robust Adversary Detection in Neural Networks. (arXiv:2101.01543v1 [cs.CV])</h2>
<h3>Rachel Sterneck, Abhishek Moitra, Priyadarshini Panda</h3>
<p>Neural networks have achieved remarkable performance in computer vision,
however they are vulnerable to adversarial examples. Adversarial examples are
inputs that have been carefully perturbed to fool classifier networks, while
appearing unchanged to humans. Based on prior works on detecting adversaries,
we propose a structured methodology of augmenting a deep neural network (DNN)
with a detector subnetwork. We use $\textit{Adversarial Noise Sensitivity}$
(ANS), a novel metric for measuring the adversarial gradient contribution of
different intermediate layers of a network. Based on the ANS value, we append a
detector to the most sensitive layer. In prior works, more complex detectors
were added to a DNN, increasing the inference computational cost of the model.
In contrast, our structured and strategic addition of a detector to a DNN
reduces the complexity of the model while making the overall network
adversarially resilient. Through comprehensive white-box and black-box
experiments on MNIST, CIFAR-10, and CIFAR-100, we show that our method improves
state-of-the-art detector robustness against adversarial examples. Furthermore,
we validate the energy efficiency of our proposed adversarial detection
methodology through an extensive energy analysis on various hardware scalable
CMOS accelerator platforms. We also demonstrate the effects of quantization on
our detector-appended networks.
</p>
<a href="http://arxiv.org/abs/2101.01543" target="_blank">arXiv:2101.01543</a> [<a href="http://arxiv.org/pdf/2101.01543" target="_blank">pdf</a>]

<h2>Adversarially trained LSTMs on reduced order models of urban air pollution simulations. (arXiv:2101.01568v1 [cs.LG])</h2>
<h3>C&#xe9;sar Quilodr&#xe1;n-Casas, Rossella Arcucci, Christopher Pain, Yike Guo</h3>
<p>This paper presents an approach to improve computational fluid dynamics
simulations forecasts of air pollution using deep learning. Our method, which
integrates Principal Components Analysis (PCA) and adversarial training, is a
way to improve the forecast skill of reduced order models obtained from the
original model solution. Once the reduced-order model (ROM) is obtained via
PCA, a Long Short-Term Memory network (LSTM) is adversarially trained on the
ROM to make forecasts. Once trained, the adversarially trained LSTM outperforms
a LSTM trained in a classical way. The study area is in London, including
velocities and a concentration tracer that replicates a busy traffic junction.
This adversarially trained LSTM-based approach is used on the ROM in order to
produce faster forecasts of the air pollution tracer.
</p>
<a href="http://arxiv.org/abs/2101.01568" target="_blank">arXiv:2101.01568</a> [<a href="http://arxiv.org/pdf/2101.01568" target="_blank">pdf</a>]

<h2>Sequential Choice Bandits with Feedback for Personalizing users' experience. (arXiv:2101.01572v1 [stat.ML])</h2>
<h3>Anshuka Rangi, Massimo Franceschetti, Long Tran-Thanh</h3>
<p>In this work, we study sequential choice bandits with feedback. We propose
bandit algorithms for a platform that personalizes users' experience to
maximize its rewards. For each action directed to a given user, the platform is
given a positive reward, which is a non-decreasing function of the action, if
this action is below the user's threshold. Users are equipped with a patience
budget, and actions that are above the threshold decrease the user's patience.
When all patience is lost, the user abandons the platform. The platform
attempts to learn the thresholds of the users in order to maximize its rewards,
based on two different feedback models describing the information pattern
available to the platform at each action. We define a notion of regret by
determining the best action to be taken when the platform knows that the user's
threshold is in a given interval. We then propose bandit algorithms for the two
feedback models and show that upper and lower bounds on the regret are of the
order of $\tilde{O}(N^{2/3})$ and $\tilde\Omega(N^{2/3})$, respectively, where
$N$ is the total number of users. Finally, we show that the waiting time of any
user before receiving a personalized experience is uniform in $N$.
</p>
<a href="http://arxiv.org/abs/2101.01572" target="_blank">arXiv:2101.01572</a> [<a href="http://arxiv.org/pdf/2101.01572" target="_blank">pdf</a>]

<h2>On the price of explainability for some clustering problems. (arXiv:2101.01576v1 [cs.LG])</h2>
<h3>Eduardo Laber, Lucas Murtinho</h3>
<p>Machine learning models and algorithms are used in a number of systems that
affect our daily life. Thus, in some settings, methods that are easy to explain
or interpret may be highly desirable. The price of explainability can be
thought of as the loss in terms of quality that is unavoidable if we restrict
these systems to use explainable methods.

We study the price of explainability, under a theoretical perspective, for
clustering tasks. We provide upper and lower bounds on this price as well as
efficient algorithms to build explainable clustering for the $k$-means,
$k$-medians, $k$-center and the maximum-spacing problems in a natural model in
which explainability is achieved via decision trees.
</p>
<a href="http://arxiv.org/abs/2101.01576" target="_blank">arXiv:2101.01576</a> [<a href="http://arxiv.org/pdf/2101.01576" target="_blank">pdf</a>]

<h2>Learning the Predictability of the Future. (arXiv:2101.01600v1 [cs.CV])</h2>
<h3>D&#xed;dac Sur&#xed;s, Ruoshi Liu, Carl Vondrick</h3>
<p>We introduce a framework for learning from unlabeled video what is
predictable in the future. Instead of committing up front to features to
predict, our approach learns from data which features are predictable. Based on
the observation that hyperbolic geometry naturally and compactly encodes
hierarchical structure, we propose a predictive model in hyperbolic space. When
the model is most confident, it will predict at a concrete level of the
hierarchy, but when the model is not confident, it learns to automatically
select a higher level of abstraction. Experiments on two established datasets
show the key role of hierarchical representations for action prediction.
Although our representation is trained with unlabeled video, visualizations
show that action hierarchies emerge in the representation.
</p>
<a href="http://arxiv.org/abs/2101.01600" target="_blank">arXiv:2101.01600</a> [<a href="http://arxiv.org/pdf/2101.01600" target="_blank">pdf</a>]

<h2>Bilateral Grid Learning for Stereo Matching Network. (arXiv:2101.01601v1 [cs.CV])</h2>
<h3>Bin Xu, Yuhua Xu, Xiaoli Yang, Wei Jia, Yulan Guo</h3>
<p>The real-time performance of the stereo matching network is important for
many applications, such as automatic driving, robot navigation and augmented
reality (AR). Although significant progress has been made in stereo matching
networks in recent years, it is still challenging to balance real-time
performance and accuracy. In this paper, we present a novel edge-preserving
cost volume upsampling module based on the slicing operation in the learned
bilateral grid. The slicing layer is parameter-free, which allows us to obtain
a high quality cost volume of high resolution from a low resolution cost volume
under the guide of the learned guidance map efficiently. The proposed cost
volume upsampling module can be seamlessly embedded into many existing stereo
matching networks, such as GCNet, PSMNet, and GANet. The resulting networks are
accelerated several times while maintaining comparable accuracy. Furthermore,
based on this module we design a real-time network (named BGNet), which
outperforms the existing published real-time deep stereo matching networks, as
well as some complex networks on KITTI stereo datasets. The code of the
proposed method will be available.
</p>
<a href="http://arxiv.org/abs/2101.01601" target="_blank">arXiv:2101.01601</a> [<a href="http://arxiv.org/pdf/2101.01601" target="_blank">pdf</a>]

<h2>STaR: Self-supervised Tracking and Reconstruction of Rigid Objects in Motion with Neural Rendering. (arXiv:2101.01602v1 [cs.CV])</h2>
<h3>Wentao Yuan, Zhaoyang Lv, Tanner Schmidt, Steven Lovegrove</h3>
<p>We present STaR, a novel method that performs Self-supervised Tracking and
Reconstruction of dynamic scenes with rigid motion from multi-view RGB videos
without any manual annotation. Recent work has shown that neural networks are
surprisingly effective at the task of compressing many views of a scene into a
learned function which maps from a viewing ray to an observed radiance value
via volume rendering. Unfortunately, these methods lose all their predictive
power once any object in the scene has moved. In this work, we explicitly model
rigid motion of objects in the context of neural representations of radiance
fields. We show that without any additional human specified supervision, we can
reconstruct a dynamic scene with a single rigid object in motion by
simultaneously decomposing it into its two constituent parts and encoding each
with its own neural representation. We achieve this by jointly optimizing the
parameters of two neural radiance fields and a set of rigid poses which align
the two fields at each frame. On both synthetic and real world datasets, we
demonstrate that our method can render photorealistic novel views, where
novelty is measured on both spatial and temporal axes. Our factored
representation furthermore enables animation of unseen object motion.
</p>
<a href="http://arxiv.org/abs/2101.01602" target="_blank">arXiv:2101.01602</a> [<a href="http://arxiv.org/pdf/2101.01602" target="_blank">pdf</a>]

<h2>Look Twice: A Computational Model of Return Fixations across Tasks and Species. (arXiv:2101.01611v1 [cs.CV])</h2>
<h3>Mengmi Zhang, Will Xiao, Olivia Rose, Katarina Bendtz, Margaret Livingstone, Carlos Ponce, Gabriel Kreiman</h3>
<p>Saccadic eye movements allow animals to bring different parts of an image
into high-resolution. During free viewing, inhibition of return incentivizes
exploration by discouraging previously visited locations. Despite this
inhibition, here we show that subjects make frequent return fixations. We
systematically studied a total of 44,328 return fixations out of 217,440
fixations across different tasks, in monkeys and humans, and in static images
or egocentric videos. The ubiquitous return fixations were consistent across
subjects, tended to occur within short offsets, and were characterized by
longer duration than non-return fixations. The locations of return fixations
corresponded to image areas of higher saliency and higher similarity to the
sought target during visual search tasks. We propose a biologically-inspired
computational model that capitalizes on a deep convolutional neural network for
object recognition to predict a sequence of fixations. Given an input image,
the model computes four maps that constrain the location of the next saccade: a
saliency map, a target similarity map, a saccade size map, and a memory map.
The model exhibits frequent return fixations and approximates the properties of
return fixations across tasks and species. The model provides initial steps
towards capturing the trade-off between exploitation of informative image
locations combined with exploration of novel image locations during scene
viewing.
</p>
<a href="http://arxiv.org/abs/2101.01611" target="_blank">arXiv:2101.01611</a> [<a href="http://arxiv.org/pdf/2101.01611" target="_blank">pdf</a>]

<h2>Auto-Encoding Molecular Conformations. (arXiv:2101.01618v1 [cs.LG])</h2>
<h3>Robin Winter, Frank No&#xe9;, Djork-Arn&#xe9; Clevert</h3>
<p>In this work we introduce an Autoencoder for molecular conformations. Our
proposed model converts the discrete spatial arrangements of atoms in a given
molecular graph (conformation) into and from a continuous fixed-sized latent
representation. We demonstrate that in this latent representation, similar
conformations cluster together while distinct conformations split apart.
Moreover, by training a probabilistic model on a large dataset of molecular
conformations, we demonstrate how our model can be used to generate diverse
sets of energetically favorable conformations for a given molecule. Finally, we
show that the continuous representation allows us to utilize optimization
methods to find molecules that have conformations with favourable spatial
properties.
</p>
<a href="http://arxiv.org/abs/2101.01618" target="_blank">arXiv:2101.01618</a> [<a href="http://arxiv.org/pdf/2101.01618" target="_blank">pdf</a>]

<h2>Novel View Synthesis via Depth-guided Skip Connections. (arXiv:2101.01619v1 [cs.CV])</h2>
<h3>Yuxin Hou, Arno Solin, Juho Kannala</h3>
<p>We introduce a principled approach for synthesizing new views of a scene
given a single source image. Previous methods for novel view synthesis can be
divided into image-based rendering methods (e.g. flow prediction) or pixel
generation methods. Flow predictions enable the target view to re-use pixels
directly, but can easily lead to distorted results. Directly regressing pixels
can produce structurally consistent results but generally suffer from the lack
of low-level details. In this paper, we utilize an encoder-decoder architecture
to regress pixels of a target view. In order to maintain details, we couple the
decoder aligned feature maps with skip connections, where the alignment is
guided by predicted depth map of the target view. Our experimental results show
that our method does not suffer from distortions and successfully preserves
texture details with aligned skip connections.
</p>
<a href="http://arxiv.org/abs/2101.01619" target="_blank">arXiv:2101.01619</a> [<a href="http://arxiv.org/pdf/2101.01619" target="_blank">pdf</a>]

<h2>Explainable AI for Robot Failures: Generating Explanations that Improve User Assistance in Fault Recovery. (arXiv:2101.01625v1 [cs.AI])</h2>
<h3>Devleena Das, Siddhartha Banerjee, Sonia Chernova</h3>
<p>With the growing capabilities of intelligent systems, the integration of
robots in our everyday life is increasing. However, when interacting in such
complex human environments, the occasional failure of robotic systems is
inevitable. The field of explainable AI has sought to make complex-decision
making systems more interpretable but most existing techniques target domain
experts. On the contrary, in many failure cases, robots will require recovery
assistance from non-expert users. In this work, we introduce a new type of
explanation, that explains the cause of an unexpected failure during an agent's
plan execution to non-experts. In order for error explanations to be
meaningful, we investigate what types of information within a set of
hand-scripted explanations are most helpful to non-experts for failure and
solution identification. Additionally, we investigate how such explanations can
be autonomously generated, extending an existing encoder-decoder model, and
generalized across environments. We investigate such questions in the context
of a robot performing a pick-and-place manipulation task in the home
environment. Our results show that explanations capturing the context of a
failure and history of past actions, are the most effective for failure and
solution identification among non-experts. Furthermore, through a second user
evaluation, we verify that our model-generated explanations can generalize to
an unseen office environment, and are just as effective as the hand-scripted
explanations.
</p>
<a href="http://arxiv.org/abs/2101.01625" target="_blank">arXiv:2101.01625</a> [<a href="http://arxiv.org/pdf/2101.01625" target="_blank">pdf</a>]

<h2>Theory-based Habit Modeling for Enhancing Behavior Prediction. (arXiv:2101.01637v1 [cs.AI])</h2>
<h3>Chao Zhang, Joaquin Vanschoren, Arlette van Wissen, Daniel Lakens, Boris de Ruyter, Wijnand A. IJsselsteijn</h3>
<p>Psychological theories of habit posit that when a strong habit is formed
through behavioral repetition, it can trigger behavior automatically in the
same environment. Given the reciprocal relationship between habit and behavior,
changing lifestyle behaviors (e.g., toothbrushing) is largely a task of
breaking old habits and creating new and healthy ones. Thus, representing
users' habit strengths can be very useful for behavior change support systems
(BCSS), for example, to predict behavior or to decide when an intervention
reaches its intended effect. However, habit strength is not directly observable
and existing self-report measures are taxing for users. In this paper, built on
recent computational models of habit formation, we propose a method to enable
intelligent systems to compute habit strength based on observable behavior. The
hypothesized advantage of using computed habit strength for behavior prediction
was tested using data from two intervention studies, where we trained
participants to brush their teeth twice a day for three weeks and monitored
their behaviors using accelerometers. Through hierarchical cross-validation, we
found that for the task of predicting future brushing behavior, computed habit
strength clearly outperformed self-reported habit strength (in both studies)
and was also superior to models based on past behavior frequency (in the larger
second study). Our findings provide initial support for our theory-based
approach of modeling user habits and encourages the use of habit computation to
deliver personalized and adaptive interventions.
</p>
<a href="http://arxiv.org/abs/2101.01637" target="_blank">arXiv:2101.01637</a> [<a href="http://arxiv.org/pdf/2101.01637" target="_blank">pdf</a>]

<h2>Nonlinear Filter for Simultaneous Localization and Mapping on a Matrix Lie Group using IMU and Feature Measurements. (arXiv:2101.01648v1 [cs.RO])</h2>
<h3>Hashim A. Hashim, Abdelrahman E. E. Eltoukhy</h3>
<p>Simultaneous Localization and Mapping (SLAM) is a process of concurrent
estimation of the vehicle's pose and feature locations with respect to a frame
of reference. This paper proposes a computationally cheap geometric nonlinear
SLAM filter algorithm structured to mimic the nonlinear motion dynamics of the
true SLAM problem posed on the matrix Lie group of
$\mathbb{SLAM}_{n}\left(3\right)$. The nonlinear filter on manifold is proposed
in continuous form and it utilizes available measurements obtained from group
velocity vectors, feature measurements and an inertial measurement unit (IMU).
The unknown bias attached to velocity measurements is successfully handled by
the proposed estimator. Simulation results illustrate the robustness of the
proposed filter in discrete form demonstrating its utility for the
six-degrees-of-freedom (6 DoF) pose estimation as well as feature estimation in
three-dimensional (3D) space. In addition, the quaternion representation of the
nonlinear filter for SLAM is provided. Keywords: Simultaneous Localization and
Mapping, Nonlinear observer algorithm for SLAM, inertial measurement unit,
inertial vision system, pose, position, attitude, landmark, estimation, IMU,
SE(3), SO(3), unmanned aerial vehicle, rigid-body, noise, nonlinear observer
for SLAM, Gaussian filter, Kalman filtering, navigation.
</p>
<a href="http://arxiv.org/abs/2101.01648" target="_blank">arXiv:2101.01648</a> [<a href="http://arxiv.org/pdf/2101.01648" target="_blank">pdf</a>]

<h2>Spatial Attention Improves Iterative 6D Object Pose Estimation. (arXiv:2101.01659v1 [cs.CV])</h2>
<h3>Stefan Stevsic, Otmar Hilliges</h3>
<p>The task of estimating the 6D pose of an object from RGB images can be broken
down into two main steps: an initial pose estimation step, followed by a
refinement procedure to correctly register the object and its observation. In
this paper, we propose a new method for 6D pose estimation refinement from RGB
images. To achieve high accuracy of the final estimate, the observation and a
rendered model need to be aligned. Our main insight is that after the initial
pose estimate, it is important to pay attention to distinct spatial features of
the object in order to improve the estimation accuracy during alignment.
Furthermore, parts of the object that are occluded in the image should be given
less weight during the alignment process. Most state-of-the-art refinement
approaches do not allow for this fine-grained reasoning and can not fully
leverage the structure of the problem. In contrast, we propose a novel neural
network architecture built around a spatial attention mechanism that identifies
and leverages information about spatial details during pose refinement. We
experimentally show that this approach learns to attend to salient spatial
features and learns to ignore occluded parts of the object, leading to better
pose estimation across datasets. We conduct experiments on standard benchmark
datasets for 6D pose estimation (LineMOD and Occlusion LineMOD) and outperform
previous state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2101.01659" target="_blank">arXiv:2101.01659</a> [<a href="http://arxiv.org/pdf/2101.01659" target="_blank">pdf</a>]

<h2>Human Activity Recognition using Wearable Sensors: Review, Challenges, Evaluation Benchmark. (arXiv:2101.01665v1 [cs.CV])</h2>
<h3>Reem Abdel-Salam, Rana Mostafa, Mayada Hadhood</h3>
<p>Recognizing human activity plays a significant role in theadvancements of
human-interaction applications in healthcare, personalfitness, and smart
devices. Many papers presented various techniques forhuman activity
representation that resulted in distinguishable progress.In this study, we
conduct an extensive literature review on recent, top-performing techniques in
human activity recognition based on wearablesensors. Due to the lack of
standardized evaluation and to assess andensure a fair comparison between the
state-of-the-art techniques, we ap-plied a standardized evaluation benchmark on
the state-of-the-art tech-niques using six publicly available data-sets:
MHealth, USCHAD, UTD-MHAD, WISDM, WHARF, and OPPORTUNITY. Also, we propose
anexperimental, improved approach that is a hybrid of enhanced hand-crafted
features and a neural network architecture which outperformedtop-performing
techniques with the same standardized evaluation bench-mark applied concerning
MHealth, USCHAD, UTD-MHAD data-sets.
</p>
<a href="http://arxiv.org/abs/2101.01665" target="_blank">arXiv:2101.01665</a> [<a href="http://arxiv.org/pdf/2101.01665" target="_blank">pdf</a>]

<h2>Characterizing Intersectional Group Fairness with Worst-Case Comparisons. (arXiv:2101.01673v1 [cs.LG])</h2>
<h3>Avijit Ghosh, Lea Genuit, Mary Reagan</h3>
<p>Machine Learning or Artificial Intelligence algorithms have gained
considerable scrutiny in recent times owing to their propensity towards
imitating and amplifying existing prejudices in society. This has led to a
niche but growing body of work that identifies and attempts to fix these
biases. A first step towards making these algorithms more fair is designing
metrics that measure unfairness. Most existing work in this field deals with
either a binary view of fairness (protected vs. unprotected groups) or
politically defined categories (race or gender). Such categorization misses the
important nuance of intersectionality - biases can often be amplified in
subgroups that combine membership from different categories, especially if such
a subgroup is particularly underrepresented in historical platforms of
opportunity.

In this paper, we discuss why fairness metrics need to be looked at under the
lens of intersectionality, identify existing work in intersectional fairness,
suggest a simple worst case comparison method to expand the definitions of
existing group fairness metrics to incorporate intersectionality, and finally
conclude with the social, legal and political framework to handle
intersectional fairness in the modern context.
</p>
<a href="http://arxiv.org/abs/2101.01673" target="_blank">arXiv:2101.01673</a> [<a href="http://arxiv.org/pdf/2101.01673" target="_blank">pdf</a>]

<h2>Monocular Depth Estimation for Soft Visuotactile Sensors. (arXiv:2101.01677v1 [cs.RO])</h2>
<h3>Rares Ambrus, Vitor Guizilini, Naveen Kuppuswamy, Andrew Beaulieu, Adrien Gaidon, Alex Alspach</h3>
<p>Fluid-filled soft visuotactile sensors such as the Soft-bubbles alleviate key
challenges for robust manipulation, as they enable reliable grasps along with
the ability to obtain high-resolution sensory feedback on contact geometry and
forces. Although they are simple in construction, their utility has been
limited due to size constraints introduced by enclosed custom IR/depth imaging
sensors to directly measure surface deformations. Towards mitigating this
limitation, we investigate the application of state-of-the-art monocular depth
estimation to infer dense internal (tactile) depth maps directly from the
internal single small IR imaging sensor. Through real-world experiments, we
show that deep networks typically used for long-range depth estimation (1-100m)
can be effectively trained for precise predictions at a much shorter range
(1-100mm) inside a mostly textureless deformable fluid-filled sensor. We
propose a simple supervised learning process to train an object-agnostic
network requiring less than 10 random poses in contact for less than 10 seconds
for a small set of diverse objects (mug, wine glass, box, and fingers in our
experiments). We show that our approach is sample-efficient, accurate, and
generalizes across different objects and sensor configurations unseen at
training time. Finally, we discuss the implications of our approach for the
design of soft visuotactile sensors and grippers.
</p>
<a href="http://arxiv.org/abs/2101.01677" target="_blank">arXiv:2101.01677</a> [<a href="http://arxiv.org/pdf/2101.01677" target="_blank">pdf</a>]

<h2>Label Augmentation via Time-based Knowledge Distillation for Financial Anomaly Detection. (arXiv:2101.01689v1 [cs.LG])</h2>
<h3>Hongda Shen, Eren Kursun</h3>
<p>Detecting anomalies has become increasingly critical to the financial service
industry. Anomalous events are often indicative of illegal activities such as
fraud, identity theft, network intrusion, account takeover, and money
laundering. Financial anomaly detection use cases face serious challenges due
to the dynamic nature of the underlying patterns especially in adversarial
environments such as constantly changing fraud tactics. While retraining the
models with the new patterns is absolutely essential; keeping up with the rapid
changes introduces other challenges as it moves the model away from older
patterns or continuously grows the size of the training data. The resulting
data growth is hard to manage and it reduces the agility of the models'
response to the latest attacks. Due to the data size limitations and the need
to track the latest patterns, older time periods are often dropped in practice,
which in turn, causes vulnerabilities. In this study, we propose a label
augmentation approach to utilize the learning from older models to boost the
latest. Experimental results show that the proposed approach provides a
significant reduction in training time, while providing potential performance
improvement.
</p>
<a href="http://arxiv.org/abs/2101.01689" target="_blank">arXiv:2101.01689</a> [<a href="http://arxiv.org/pdf/2101.01689" target="_blank">pdf</a>]

<h2>Analyzing movies to predict their commercial viability for producers. (arXiv:2101.01697v1 [cs.LG])</h2>
<h3>Devendra Swami, Yash Phogat, Aadiraj Batlaw, Ashwin Goyal</h3>
<p>Upon film premiere, a major form of speculation concerns the relative success
of the film. This relativity is in particular regards to the film's original
budget, as many a time have big-budget blockbusters been met with exceptional
success as met with abject failure. So how does one predict the success of an
upcoming film? In this paper, we explored a vast array of film data in an
attempt to develop a model that could predict the expected return of an
upcoming film. The approach to this development is as follows: First, we began
with the MovieLens dataset having common movie attributes along with genome
tags per each film. Genome tags give insight into what particular
characteristics of the film are most salient. We then included additional
features regarding film content, cast/crew, audience perception, budget, and
earnings from TMDB, IMDB, and Metacritic websites. Next, we performed
exploratory data analysis and engineered a wide range of new features capturing
historical information for the available features. Thereafter, we used singular
value decomposition (SVD) for dimensionality reduction of the high dimensional
features (ex. genome tags). Finally, we built a Random Forest Classifier and
performed hyper-parameter tuning to optimize for model accuracy. A future
application of our model could be seen in the film industry, allowing
production companies to better predict the expected return of their projects
based on their envisioned outline for their production procedure, thereby
allowing them to revise their plan in an attempt to achieve optimal returns.
</p>
<a href="http://arxiv.org/abs/2101.01697" target="_blank">arXiv:2101.01697</a> [<a href="http://arxiv.org/pdf/2101.01697" target="_blank">pdf</a>]

<h2>Learning Accurate Dense Correspondences and When to Trust Them. (arXiv:2101.01710v1 [cs.CV])</h2>
<h3>Prune Truong, Martin Danelljan, Luc Van Gool, Radu Timofte</h3>
<p>Establishing dense correspondences between a pair of images is an important
and general problem. However, dense flow estimation is often inaccurate in the
case of large displacements or homogeneous regions. For most applications and
down-steam tasks, such as pose estimation, image manipulation, or 3D
reconstruction, it is crucial to know when and where to trust the estimated
correspondences.

In this work, we aim to estimate a dense flow field relating two images,
coupled with a robust pixel-wise confidence map indicating the reliability and
accuracy of the prediction. We develop a flexible probabilistic approach that
jointly learns the flow prediction and its uncertainty. In particular, we
parametrize the predictive distribution as a constrained mixture model,
ensuring better modelling of both accurate flow predictions and outliers.
Moreover, we develop an architecture and training strategy tailored for robust
and generalizable uncertainty prediction in the context of self-supervised
training. Our approach obtains state-of-the-art results on multiple challenging
geometric matching and optical flow datasets. We further validate the
usefulness of our probabilistic confidence estimation for the task of pose
estimation. Code and models will be released at github.com/PruneTruong/PDCNet.
</p>
<a href="http://arxiv.org/abs/2101.01710" target="_blank">arXiv:2101.01710</a> [<a href="http://arxiv.org/pdf/2101.01710" target="_blank">pdf</a>]

<h2>Learning from Synthetic Shadows for Shadow Detection and Removal. (arXiv:2101.01713v1 [cs.CV])</h2>
<h3>Naoto Inoue, Toshihiko Yamasaki</h3>
<p>Shadow removal is an essential task in computer vision and computer graphics.
Recent shadow removal approaches all train convolutional neural networks (CNN)
on real paired shadow/shadow-free or shadow/shadow-free/mask image datasets.
However, obtaining a large-scale, diverse, and accurate dataset has been a big
challenge, and it limits the performance of the learned models on shadow images
with unseen shapes/intensities. To overcome this challenge, we present
SynShadow, a novel large-scale synthetic shadow/shadow-free/matte image
triplets dataset and a pipeline to synthesize it. We extend a
physically-grounded shadow illumination model and synthesize a shadow image
given an arbitrary combination of a shadow-free image, a matte image, and
shadow attenuation parameters. Owing to the diversity, quantity, and quality of
SynShadow, we demonstrate that shadow removal models trained on SynShadow
perform well in removing shadows with diverse shapes and intensities on some
challenging benchmarks. Furthermore, we show that merely fine-tuning from a
SynShadow-pre-trained model improves existing shadow detection and removal
models. Codes are publicly available at https://github.com/naoto0804/SynShadow.
</p>
<a href="http://arxiv.org/abs/2101.01713" target="_blank">arXiv:2101.01713</a> [<a href="http://arxiv.org/pdf/2101.01713" target="_blank">pdf</a>]

<h2>Local Memory Attention for Fast Video Semantic Segmentation. (arXiv:2101.01715v1 [cs.CV])</h2>
<h3>Matthieu Paul, Martin Danelljan, Luc Van Gool, Radu Timofte</h3>
<p>We propose a novel neural network module that transforms an existing
single-frame semantic segmentation model into a video semantic segmentation
pipeline. In contrast to prior works, we strive towards a simple and general
module that can be integrated into virtually any single-frame architecture. Our
approach aggregates a rich representation of the semantic information in past
frames into a memory module. Information stored in the memory is then accessed
through an attention mechanism. This provides temporal appearance cues from
prior frames, which are then fused with an encoding of the current frame
through a second attention-based module. The segmentation decoder processes the
fused representation to predict the final semantic segmentation. We integrate
our approach into two popular semantic segmentation networks: ERFNet and
PSPNet. We observe an improvement in segmentation performance on Cityscapes by
1.7% and 2.1% in mIoU respectively, while increasing inference time of ERFNet
by only 1.5ms.
</p>
<a href="http://arxiv.org/abs/2101.01715" target="_blank">arXiv:2101.01715</a> [<a href="http://arxiv.org/pdf/2101.01715" target="_blank">pdf</a>]

<h2>Toward Multi-Diversified Ensemble Clustering of High-Dimensional Data: From Subspaces to Metrics and Beyond. (arXiv:1710.03113v4 [cs.LG] UPDATED)</h2>
<h3>Dong Huang, Chang-Dong Wang, Jian-Huang Lai, Chee-Keong Kwoh</h3>
<p>The rapid emergence of high-dimensional data in various areas has brought new
challenges to current ensemble clustering research. To deal with the curse of
dimensionality, recently considerable efforts in ensemble clustering have been
made by means of different subspace-based techniques. However, besides the
emphasis on subspaces, rather limited attention has been paid to the potential
diversity in similarity/dissimilarity metrics. It remains a surprisingly open
problem in ensemble clustering how to create and aggregate a large population
of diversified metrics, and furthermore, how to jointly investigate the
multi-level diversity in the large populations of metrics, subspaces, and
clusters in a unified framework. To tackle this problem, this paper proposes a
novel multi-diversified ensemble clustering approach. In particular, we create
a large number of diversified metrics by randomizing a scaled exponential
similarity kernel, which are then coupled with random subspaces to form a large
set of metric-subspace pairs. Based on the similarity matrices derived from
these metric-subspace pairs, an ensemble of diversified base clusterings can
thereby be constructed. Further, an entropy-based criterion is utilized to
explore the cluster-wise diversity in ensembles, based on which three specific
ensemble clustering algorithms are presented by incorporating three types of
consensus functions. Extensive experiments are conducted on 30 high-dimensional
datasets, including 18 cancer gene expression datasets and 12 image/speech
datasets, which demonstrate the superiority of our algorithms over the
state-of-the-art. The source code is available at
https://github.com/huangdonghere/MDEC.
</p>
<a href="http://arxiv.org/abs/1710.03113" target="_blank">arXiv:1710.03113</a> [<a href="http://arxiv.org/pdf/1710.03113" target="_blank">pdf</a>]

<h2>B\'ezierGAN: Automatic Generation of Smooth Curves from Interpretable Low-Dimensional Parameters. (arXiv:1808.08871v2 [cs.LG] UPDATED)</h2>
<h3>Wei Chen, Mark Fuge</h3>
<p>Many real-world objects are designed by smooth curves, especially in the
domain of aerospace and ship, where aerodynamic shapes (e.g., airfoils) and
hydrodynamic shapes (e.g., hulls) are designed. To facilitate the design
process of those objects, we propose a deep learning based generative model
that can synthesize smooth curves. The model maps a low-dimensional latent
representation to a sequence of discrete points sampled from a rational
B\'ezier curve. We demonstrate the performance of our method in completing both
synthetic and real-world generative tasks. Results show that our method can
generate diverse and realistic curves, while preserving consistent shape
variation in the latent space, which is favorable for latent space design
optimization or design space exploration.
</p>
<a href="http://arxiv.org/abs/1808.08871" target="_blank">arXiv:1808.08871</a> [<a href="http://arxiv.org/pdf/1808.08871" target="_blank">pdf</a>]

<h2>Application-driven Privacy-preserving Data Publishing with Correlated Attributes. (arXiv:1812.10193v2 [cs.LG] UPDATED)</h2>
<h3>Aria Rezaei, Chaowei Xiao, Jie Gao, Bo Li, Sirajum Munir</h3>
<p>Recent advances in computing have allowed for the possibility to collect
large amounts of data on personal activities and private living spaces. To
address the privacy concerns of users in this environment, we propose a novel
framework called PR-GAN that offers privacy-preserving mechanism using
generative adversarial networks. Given a target application, PR-GAN
automatically modifies the data to hide sensitive attributes -- which may be
hidden and can be inferred by machine learning algorithms -- while preserving
the data utility in the target application. Unlike prior works, the public's
possible knowledge of the correlation between the target application and
sensitive attributes is built into our modeling. We formulate our problem as an
optimization problem, show that an optimal solution exists and use generative
adversarial networks (GAN) to create perturbations. We further show that our
method provides privacy guarantees under the Pufferfish framework, an elegant
generalization of the differential privacy that allows for the modeling of
prior knowledge on data and correlations. Through experiments, we show that our
method outperforms conventional methods in effectively hiding the sensitive
attributes while guaranteeing high performance in the target application, for
both property inference and training purposes. Finally, we demonstrate through
further experiments that once our model learns a privacy-preserving task, such
as hiding subjects' identity, on a group of individuals, it can perform the
same task on a separate group with minimal performance drops.
</p>
<a href="http://arxiv.org/abs/1812.10193" target="_blank">arXiv:1812.10193</a> [<a href="http://arxiv.org/pdf/1812.10193" target="_blank">pdf</a>]

<h2>Soft Autoencoder and Its Wavelet Adaptation Interpretation. (arXiv:1812.11675v4 [cs.LG] UPDATED)</h2>
<h3>Fenglei Fan, Mengzhou Li, Yueyang Teng, Ge Wang</h3>
<p>Recently, deep learning becomes the main focus of machine learning research
and has greatly impacted many important fields. However, deep learning is
criticized for lack of interpretability. As a successful unsupervised model in
deep learning, the autoencoder embraces a wide spectrum of applications, yet it
suffers from the model opaqueness as well. In this paper, we propose a new type
of convolutional autoencoders, termed as Soft Autoencoder (Soft-AE), in which
the activation functions of encoding layers are implemented with adaptable
soft-thresholding units while decoding layers are realized with linear units.
Consequently, Soft-AE can be naturally interpreted as a learned cascaded
wavelet shrinkage system. Our denoising experiments demonstrate that Soft-AE
not only is interpretable but also offers a competitive performance relative to
its counterparts. Furthermore, we propose a generalized linear unit (GenLU) to
make an autoencoder more adaptive in nonlinearly filtering images and data,
such as denoising and deblurring.
</p>
<a href="http://arxiv.org/abs/1812.11675" target="_blank">arXiv:1812.11675</a> [<a href="http://arxiv.org/pdf/1812.11675" target="_blank">pdf</a>]

<h2>Optical Flow Techniques for Facial Expression Analysis -- a Practical Evaluation Study. (arXiv:1904.11592v2 [cs.CV] UPDATED)</h2>
<h3>Benjamin Allaert, Isaac Ronald Ward, Ioan Marius Bilasco, Chaabane Djeraba, Mohammed Bennamoun</h3>
<p>Optical flow techniques are becoming increasingly performant and robust when
estimating motion in a scene, but their performance has yet to be proven in the
area of facial expression recognition. In this work, a variety of optical flow
approaches are evaluated across multiple facial expression datasets, so as to
provide a consistent performance evaluation. The aim of this work is not to
propose a new expression recognition technique, but to understand better the
adequacy of existing state-of-the art optical flow for encoding facial motion
in the context of facial expression recognition. Our evaluations highlight the
fact that motion approximation methods used to overcome motion discontinuities
have a significant impact when optical flows are used to characterize facial
expressions.
</p>
<a href="http://arxiv.org/abs/1904.11592" target="_blank">arXiv:1904.11592</a> [<a href="http://arxiv.org/pdf/1904.11592" target="_blank">pdf</a>]

<h2>Health-Informed Policy Gradients for Multi-Agent Reinforcement Learning. (arXiv:1908.01022v4 [cs.LG] UPDATED)</h2>
<h3>Ross E. Allen, Jayesh K. Gupta, Jaime Pena, Yutai Zhou, Javona White Bear, Mykel J. Kochenderfer</h3>
<p>This paper proposes a definition of system health in the context of multiple
agents optimizing a joint reward function. We use this definition as a credit
assignment term in a policy gradient algorithm to distinguish the contributions
of individual agents to the global reward. The health-informed credit
assignment is then extended to a multi-agent variant of the proximal policy
optimization algorithm and demonstrated on particle and multiwalker robot
environments that have characteristics such as system health, risk-taking,
semi-expendable agents, continuous action spaces, and partial observability. We
show significant improvement in learning performance compared to policy
gradient methods that do not perform multi-agent credit assignment.
</p>
<a href="http://arxiv.org/abs/1908.01022" target="_blank">arXiv:1908.01022</a> [<a href="http://arxiv.org/pdf/1908.01022" target="_blank">pdf</a>]

<h2>Bayesian Incremental Inference Update by Re-using Calculations from Belief Space Planning: A New Paradigm. (arXiv:1908.02002v2 [cs.AI] UPDATED)</h2>
<h3>Elad I. Farhi, Vadim Indelman</h3>
<p>Inference and decision making under uncertainty are key processes in every
autonomous system and numerous robotic problems. In recent years, the
similarities between inference and decision making triggered much work, from
developing unified computational frameworks to pondering about the duality
between the two. In spite of these efforts, inference and control, as well as
inference and belief space planning (BSP) are still treated as two separate
processes. In this paper we propose a paradigm shift, a novel approach which
deviates from conventional Bayesian inference and utilizes the similarities
between inference and BSP. We make the key observation that inference can be
efficiently updated using predictions made during the decision making stage,
even in light of inconsistent data association between the two. We developed a
two staged process that implements our novel approach and updates inference
using calculations from the precursory planning phase. Using autonomous
navigation in an unknown environment along with iSAM2 efficient methodologies
as a test case, we benchmarked our novel approach against standard Bayesian
inference, both with synthetic and real-world data (KITTI dataset). Results
indicate that not only our approach improves running time by at least a factor
of two while providing the same estimation accuracy, but it also alleviates the
computational burden of state dimensionality and loop closures.
</p>
<a href="http://arxiv.org/abs/1908.02002" target="_blank">arXiv:1908.02002</a> [<a href="http://arxiv.org/pdf/1908.02002" target="_blank">pdf</a>]

<h2>Efficient Large-Scale Multi-Drone Delivery Using Transit Networks. (arXiv:1909.11840v5 [cs.RO] UPDATED)</h2>
<h3>Shushman Choudhury, Kiril Solovey, Mykel J. Kochenderfer, Marco Pavone</h3>
<p>We consider the problem of controlling a large fleet of drones to deliver
packages simultaneously across broad urban areas. To conserve energy, drones
hop between public transit vehicles (e.g., buses and trams). We design a
comprehensive algorithmic framework that strives to minimize the maximum time
to complete any delivery. We address the multifaceted complexity of the problem
through a two-layer approach. First, the upper layer assigns drones to package
delivery sequences with a near-optimal polynomial-time task allocation
algorithm. Then, the lower layer executes the allocation by periodically
routing the fleet over the transit network while employing efficient
bounded-suboptimal multi-agent pathfinding techniques tailored to our setting.
Experiments demonstrate the efficiency of our approach on settings with up to
$200$ drones, $5000$ packages, and transit networks with up to $8000$ stops in
San Francisco and Washington DC. Our results show that the framework computes
solutions typically within a few seconds on commodity hardware, and that drones
travel up to $360 \%$ of their flight range with public transit.
</p>
<a href="http://arxiv.org/abs/1909.11840" target="_blank">arXiv:1909.11840</a> [<a href="http://arxiv.org/pdf/1909.11840" target="_blank">pdf</a>]

<h2>Real-World Image Datasets for Federated Learning. (arXiv:1910.11089v3 [cs.CV] UPDATED)</h2>
<h3>Jiahuan Luo, Xueyang Wu, Yun Luo, Anbu Huang, Yunfeng Huang, Yang Liu, Qiang Yang</h3>
<p>Federated learning is a new machine learning paradigm which allows data
parties to build machine learning models collaboratively while keeping their
data secure and private. While research efforts on federated learning have been
growing tremendously in the past two years, most existing works still depend on
pre-existing public datasets and artificial partitions to simulate data
federations due to the lack of high-quality labeled data generated from
real-world edge applications. Consequently, advances on benchmark and model
evaluations for federated learning have been lagging behind. In this paper, we
introduce a real-world image dataset. The dataset contains more than 900 images
generated from 26 street cameras and 7 object categories annotated with
detailed bounding box. The data distribution is non-IID and unbalanced,
reflecting the characteristic real-world federated learning scenarios. Based on
this dataset, we implemented two mainstream object detection algorithms (YOLO
and Faster R-CNN) and provided an extensive benchmark on model performance,
efficiency, and communication in a federated learning setting. Both the dataset
and algorithms are made publicly available.
</p>
<a href="http://arxiv.org/abs/1910.11089" target="_blank">arXiv:1910.11089</a> [<a href="http://arxiv.org/pdf/1910.11089" target="_blank">pdf</a>]

<h2>Matrix Normal PCA for Interpretable Dimension Reduction and Graphical Noise Modeling. (arXiv:1911.10796v2 [cs.LG] UPDATED)</h2>
<h3>Chihao Zhang, Kuo Gai, Shihua Zhang</h3>
<p>Principal component analysis (PCA) is one of the most widely used dimension
reduction and multivariate statistical techniques. From a probabilistic
perspective, PCA seeks a low-dimensional representation of data in the presence
of independent identical Gaussian noise. Probabilistic PCA (PPCA) and its
variants have been extensively studied for decades. Most of them assume the
underlying noise follows a certain independent identical distribution. However,
the noise in the real world is usually complicated and structured. To address
this challenge, some variants of PCA for data with non-IID noise have been
proposed. However, most of the existing methods only assume that the noise is
correlated in the feature space while there may exist two-way structured noise.
To this end, we propose a powerful and intuitive PCA method (MN-PCA) through
modeling the graphical noise by the matrix normal distribution, which enables
us to explore the structure of noise in both the feature space and the sample
space. MN-PCA obtains a low-rank representation of data and the structure of
noise simultaneously. And it can be explained as approximating data over the
generalized Mahalanobis distance. We develop two algorithms to solve this
model: one maximizes the regularized likelihood, the other exploits the
Wasserstein distance, which is more robust. Extensive experiments on various
data demonstrate their effectiveness.
</p>
<a href="http://arxiv.org/abs/1911.10796" target="_blank">arXiv:1911.10796</a> [<a href="http://arxiv.org/pdf/1911.10796" target="_blank">pdf</a>]

<h2>Keyhole Imaging: Non-Line-of-Sight Imaging and Tracking of Moving Objects Along a Single Optical Path. (arXiv:1912.06727v3 [cs.CV] UPDATED)</h2>
<h3>Christopher A. Metzler, David B. Lindell, Gordon Wetzstein</h3>
<p>Non-line-of-sight (NLOS) imaging and tracking is an emerging technology that
allows the shape or position of objects around corners or behind diffusers to
be recovered from transient, time-of-flight measurements. However, existing
NLOS approaches require the imaging system to scan a large area on a visible
surface, where the indirect light paths of hidden objects are sampled. In many
applications, such as robotic vision or autonomous driving, optical access to a
large scanning area may not be available, which severely limits the
practicality of existing NLOS techniques. Here, we propose a new approach,
dubbed keyhole imaging, that captures a sequence of transient measurements
along a single optical path, for example, through a keyhole. Assuming that the
hidden object of interest moves during the acquisition time, we effectively
capture a series of time-resolved projections of the object's shape from
unknown viewpoints. We derive inverse methods based on expectation-maximization
to recover the object's shape and location using these measurements. Then, with
the help of long exposure times and retroreflective tape, we demonstrate
successful experimental results with a prototype keyhole imaging system.
</p>
<a href="http://arxiv.org/abs/1912.06727" target="_blank">arXiv:1912.06727</a> [<a href="http://arxiv.org/pdf/1912.06727" target="_blank">pdf</a>]

<h2>MEDIRL: Predicting the Visual Attention of Drivers via Maximum Entropy Deep Inverse Reinforcement Learning. (arXiv:1912.07773v3 [cs.CV] UPDATED)</h2>
<h3>Sonia Baee, Erfan Pakdamanian, Inki Kim, Lu Feng, Vicente Ordonez, Laura Barnes</h3>
<p>Inspired by human visual attention, we introduce a Maximum Entropy Deep
Inverse Reinforcement Learning (MEDIRL) framework for modeling the visual
attention allocation of drivers in imminent rear-end collisions. MEDIRL is
composed of visual, driving, and attention modules. Given a front-view driving
video and corresponding eye fixations from humans, the visual and driving
modules extract generic and driving-specific visual features, respectively.
Finally, the attention module learns the intrinsic task-sensitive reward
functions induced by eye fixation policies recorded from attentive drivers.
MEDIRL uses the learned policies to predict visual attention allocation of
drivers. We also introduce EyeCar, a new driver visual attention dataset during
accident-prone situations. We conduct comprehensive experiments and show that
MEDIRL outperforms previous state-of-the-art methods on driving task-related
visual attention allocation on the following large-scale driving attention
benchmark datasets: DR(eye)VE, BDD-A, and DADA-2000. The code and dataset are
provided for reproducibility.
</p>
<a href="http://arxiv.org/abs/1912.07773" target="_blank">arXiv:1912.07773</a> [<a href="http://arxiv.org/pdf/1912.07773" target="_blank">pdf</a>]

<h2>Knowledge Graph Embedding for Link Prediction: A Comparative Analysis. (arXiv:2002.00819v3 [cs.LG] UPDATED)</h2>
<h3>Andrea Rossi, Donatella Firmani, Antonio Matinata, Paolo Merialdo, Denilson Barbosa</h3>
<p>Knowledge Graphs (KGs) have found many applications in industry and academic
settings, which in turn, have motivated considerable research efforts towards
large-scale information extraction from a variety of sources. Despite such
efforts, it is well known that even state-of-the-art KGs suffer from
incompleteness. Link Prediction (LP), the task of predicting missing facts
among entities already a KG, is a promising and widely studied task aimed at
addressing KG incompleteness. Among the recent LP techniques, those based on KG
embeddings have achieved very promising performances in some benchmarks.
Despite the fast growing literature in the subject, insufficient attention has
been paid to the effect of the various design choices in those methods.
Moreover, the standard practice in this area is to report accuracy by
aggregating over a large number of test facts in which some entities are
over-represented; this allows LP methods to exhibit good performance by just
attending to structural properties that include such entities, while ignoring
the remaining majority of the KG. This analysis provides a comprehensive
comparison of embedding-based LP methods, extending the dimensions of analysis
beyond what is commonly available in the literature. We experimentally compare
effectiveness and efficiency of 16 state-of-the-art methods, consider a
rule-based baseline, and report detailed analysis over the most popular
benchmarks in the literature.
</p>
<a href="http://arxiv.org/abs/2002.00819" target="_blank">arXiv:2002.00819</a> [<a href="http://arxiv.org/pdf/2002.00819" target="_blank">pdf</a>]

<h2>Cyclic Boosting -- an explainable supervised machine learning algorithm. (arXiv:2002.03425v3 [cs.LG] UPDATED)</h2>
<h3>Felix Wick, Ulrich Kerzel, Michael Feindt</h3>
<p>Supervised machine learning algorithms have seen spectacular advances and
surpassed human level performance in a wide range of specific applications.
However, using complex ensemble or deep learning algorithms typically results
in black box models, where the path leading to individual predictions cannot be
followed in detail. In order to address this issue, we propose the novel
"Cyclic Boosting" machine learning algorithm, which allows to efficiently
perform accurate regression and classification tasks while at the same time
allowing a detailed understanding of how each individual prediction was made.
</p>
<a href="http://arxiv.org/abs/2002.03425" target="_blank">arXiv:2002.03425</a> [<a href="http://arxiv.org/pdf/2002.03425" target="_blank">pdf</a>]

<h2>Randomization matters. How to defend against strong adversarial attacks. (arXiv:2002.11565v4 [cs.LG] UPDATED)</h2>
<h3>Rafael Pinot, Raphael Ettedgui, Geovani Rizk, Yann Chevaleyre, Jamal Atif</h3>
<p>Is there a classifier that ensures optimal robustness against all adversarial
attacks? This paper answers this question by adopting a game-theoretic point of
view. We show that adversarial attacks and defenses form an infinite zero-sum
game where classical results (e.g. Sion theorem) do not apply. We demonstrate
the non-existence of a Nash equilibrium in our game when the classifier and the
Adversary are both deterministic, hence giving a negative answer to the above
question in the deterministic regime. Nonetheless, the question remains open in
the randomized regime. We tackle this problem by showing that, undermild
conditions on the dataset distribution, any deterministic classifier can be
outperformed by a randomized one. This gives arguments for using randomization,
and leads us to a new algorithm for building randomized classifiers that are
robust to strong adversarial attacks. Empirical results validate our
theoretical analysis, and show that our defense method considerably outperforms
Adversarial Training against state-of-the-art attacks.
</p>
<a href="http://arxiv.org/abs/2002.11565" target="_blank">arXiv:2002.11565</a> [<a href="http://arxiv.org/pdf/2002.11565" target="_blank">pdf</a>]

<h2>Curriculum By Smoothing. (arXiv:2003.01367v5 [cs.LG] UPDATED)</h2>
<h3>Samarth Sinha, Animesh Garg, Hugo Larochelle</h3>
<p>Convolutional Neural Networks (CNNs) have shown impressive performance in
computer vision tasks such as image classification, detection, and
segmentation. Moreover, recent work in Generative Adversarial Networks (GANs)
has highlighted the importance of learning by progressively increasing the
difficulty of a learning task [26]. When learning a network from scratch, the
information propagated within the network during the earlier stages of training
can contain distortion artifacts due to noise which can be detrimental to
training. In this paper, we propose an elegant curriculum based scheme that
smoothes the feature embedding of a CNN using anti-aliasing or low-pass
filters. We propose to augment the train-ing of CNNs by controlling the amount
of high frequency information propagated within the CNNs as training
progresses, by convolving the output of a CNN feature map of each layer with a
Gaussian kernel. By decreasing the variance of the Gaussian kernel, we
gradually increase the amount of high-frequency information available within
the network for inference. As the amount of information in the feature maps
increases during training, the network is able to progressively learn better
representations of the data. Our proposed augmented training scheme
significantly improves the performance of CNNs on various vision tasks without
either adding additional trainable parameters or an auxiliary regularization
objective. The generality of our method is demonstrated through empirical
performance gains in CNN architectures across four different tasks: transfer
learning, cross-task transfer learning, and generative models.
</p>
<a href="http://arxiv.org/abs/2003.01367" target="_blank">arXiv:2003.01367</a> [<a href="http://arxiv.org/pdf/2003.01367" target="_blank">pdf</a>]

<h2>Data-efficient Domain Randomization with Bayesian Optimization. (arXiv:2003.02471v4 [cs.LG] UPDATED)</h2>
<h3>Fabio Muratore, Christian Eilers, Michael Gienger, Jan Peters</h3>
<p>When learning policies for robot control, the required real-world data is
typically prohibitively expensive to acquire, so learning in simulation is a
popular strategy. Unfortunately, such polices are often not transferable to the
real world due to a mismatch between the simulation and reality, called
'reality gap'. Domain randomization methods tackle this problem by randomizing
the physics simulator (source domain) during training according to a
distribution over domain parameters in order to obtain more robust policies
that are able to overcome the reality gap. Most domain randomization approaches
sample the domain parameters from a fixed distribution. This solution is
suboptimal in the context of sim-to-real transferability, since it yields
policies that have been trained without explicitly optimizing for the reward on
the real system (target domain). Additionally, a fixed distribution assumes
there is prior knowledge about the uncertainty over the domain parameters. In
this paper, we propose Bayesian Domain Randomization (BayRn), a black-box
sim-to-real algorithm that solves tasks efficiently by adapting the domain
parameter distribution during learning given sparse data from the real-world
target domain. BayRn uses Bayesian optimization to search the space of source
domain distribution parameters such that this leads to a policy which maximizes
the real-word objective, allowing for adaptive distributions during policy
optimization. We experimentally validate the proposed approach in sim-to-sim as
well as in sim-to-real experiments, comparing against three baseline methods on
two robotic tasks. Our results show that BayRn is able to perform sim-to-real
transfer, while significantly reducing the required prior knowledge.
</p>
<a href="http://arxiv.org/abs/2003.02471" target="_blank">arXiv:2003.02471</a> [<a href="http://arxiv.org/pdf/2003.02471" target="_blank">pdf</a>]

<h2>Confronting the Constraints for Optical Character Segmentation from Printed Bangla Text Image. (arXiv:2003.08384v5 [cs.CV] UPDATED)</h2>
<h3>Abu Saleh Md. Abir, Sanjana Rahman, Samia Ellin, Maisha Farzana, Md Hridoy Manik, Chowdhury Rafeed Rahman</h3>
<p>In a world of digitization, optical character recognition holds the
automation to written history. Optical character recognition system basically
converts printed images into editable texts for better storage and usability.
To be completely functional, the system needs to go through some crucial
methods such as pre-processing and segmentation. Pre-processing helps printed
data to be noise free and gets rid of skewness efficiently whereas segmentation
helps the image fragment into line, word and character precisely for better
conversion. These steps hold the door to better accuracy and consistent results
for a printed image to be ready for conversion. Our proposed algorithm is able
to segment characters both from ideal and non-ideal cases of scanned or
captured images giving a sustainable outcome. The implementation of our work is
provided here: https://cutt.ly/rgdfBIa
</p>
<a href="http://arxiv.org/abs/2003.08384" target="_blank">arXiv:2003.08384</a> [<a href="http://arxiv.org/pdf/2003.08384" target="_blank">pdf</a>]

<h2>Active Interaction Force Control for Contact-Based Inspection with a Fully Actuated Aerial Vehicle. (arXiv:2003.09516v3 [cs.RO] UPDATED)</h2>
<h3>Karen Bodie, Maximilian Brunner, Michael Pantic, Stefan Walser, Patrick Pf&#xe4;ndler, Ueli Angst, Roland Siegwart, Juan Nieto</h3>
<p>This paper presents and validates active interaction force control and
planning for fully actuated and omnidirectional aerial manipulation platforms,
with the goal of aerial contact inspection in unstructured environments. We
present a variable axis-selective impedance control which integrates direct
force control for intentional interaction, using feedback from an on-board
force sensor. The control approach aims to reject disturbances in free flight,
while handling unintentional interaction, and actively controlling desired
interaction forces. A fully actuated and omnidirectional tilt-rotor aerial
system is used to show capabilities of the control and planning methods.
Experiments demonstrate disturbance rejection, push-and-slide interaction, and
force controlled interaction in different flight orientations. The system is
validated as a tool for non-destructive testing of concrete infrastructure, and
statistical results of
</p>
<a href="http://arxiv.org/abs/2003.09516" target="_blank">arXiv:2003.09516</a> [<a href="http://arxiv.org/pdf/2003.09516" target="_blank">pdf</a>]

<h2>Convergence of Recursive Stochastic Algorithms using Wasserstein Divergence. (arXiv:2003.11403v2 [cs.LG] UPDATED)</h2>
<h3>Abhishek Gupta, William B. Haskell</h3>
<p>This paper develops a unified framework, based on iterated random operator
theory, to analyze the convergence of constant stepsize recursive stochastic
algorithms (RSAs). RSAs use randomization to efficiently compute expectations,
and so their iterates form a stochastic process. The key idea of our analysis
is to lift the RSA into an appropriate higher-dimensional space and then
express it as an equivalent Markov chain. Instead of determining the
convergence of this Markov chain (which may not converge under constant
stepsize), we study the convergence of the distribution of this Markov chain.
To study this, we define a new notion of Wasserstein divergence. We show that
if the distribution of the iterates in the Markov chain satisfy a contraction
property with respect to the Wasserstein divergence, then the Markov chain
admits an invariant distribution. We show that convergence of a large family of
constant stepsize RSAs can be understood using this framework, and we provide
several detailed examples.
</p>
<a href="http://arxiv.org/abs/2003.11403" target="_blank">arXiv:2003.11403</a> [<a href="http://arxiv.org/pdf/2003.11403" target="_blank">pdf</a>]

<h2>Unpacking Information Bottlenecks: Unifying Information-Theoretic Objectives in Deep Learning. (arXiv:2003.12537v3 [cs.LG] UPDATED)</h2>
<h3>Andreas Kirsch, Clare Lyle, Yarin Gal</h3>
<p>The Information Bottleneck principle offers both a mechanism to explain how
deep neural networks train and generalize, as well as a regularized objective
with which to train models. However, multiple competing objectives are proposed
in the literature, and the information-theoretic quantities used in these
objectives are difficult to compute for large deep neural networks, which in
turn limits their use as a training objective. In this work, we review these
quantities and compare and unify previously proposed objectives, which allows
us to develop surrogate objectives more friendly to optimization without
relying on cumbersome tools such as density estimation. We find that these
surrogate objectives allow us to apply the information bottleneck to modern
neural network architectures. We demonstrate our insights on MNIST, CIFAR-10
and Imagenette with modern DNN architectures (ResNets).
</p>
<a href="http://arxiv.org/abs/2003.12537" target="_blank">arXiv:2003.12537</a> [<a href="http://arxiv.org/pdf/2003.12537" target="_blank">pdf</a>]

<h2>Long Tail Visual Relationship Recognition with Hubless Regularized Relmix. (arXiv:2004.00436v4 [cs.CV] UPDATED)</h2>
<h3>Sherif Abdelkarim, Aniket Agarwal, Panos Achlioptas, Jun Chen, Jiaji Huang, Boyang Li, Kenneth Church, Mohamed Elhoseiny</h3>
<p>Several approaches have been proposed in recent literature to alleviate the
long-tail problem, mostly in the object classification task. We propose to
study the task of Long-Tail Visual Relationship Recognition (LTVRR), which aims
at generalizing on the structured long-tail distribution of visual
relationships (e.g., "rabbit grazing on grass"). In this setup, subject,
relation, and object classes individually follow a long-tail distribution. We
first introduce two large-scale long-tail visual relationship recognition
benchmarks to study this task, dubbed as VG8K-LT (5330 objects, 2000
relationships) and GQA-LT (1703 objects, 310 relations). VG8K-LT and GQA-LT are
built upon the widely used Visual Genome and GQA datasets. In contrast to
existing benchmarks, some classes appear at a very low frequency ($1-14$
examples). We use these benchmarks to study the performance of several
state-of-the-art long-tail models on LTVRR setup. We developed a
visiolinguistic hubless (ViLHub) loss that consistently encourages visual
classifiers to be more predictive of tail classes while being accurate on the
head. We also propose relationship Mixup augmentation, dubbed as RelMix, to
improve performance on the tail on VG8K-LT and GQA-LT benchmarks with the best
performance achieved when combined with ViLHub loss. Benchmarks and code will
be made available.
</p>
<a href="http://arxiv.org/abs/2004.00436" target="_blank">arXiv:2004.00436</a> [<a href="http://arxiv.org/pdf/2004.00436" target="_blank">pdf</a>]

<h2>An End-to-End Learning Approach for Trajectory Prediction in Pedestrian Zones. (arXiv:2004.04787v2 [cs.AI] UPDATED)</h2>
<h3>Ha Q. Ngo, Christoph Henke, Frank Hees</h3>
<p>This paper aims to explore the problem of trajectory prediction in
heterogeneous pedestrian zones, where social dynamics representation is a big
challenge. Proposed is an end-to-end learning framework for prediction accuracy
improvement based on an attention mechanism to learn social interaction from
multi-factor inputs.
</p>
<a href="http://arxiv.org/abs/2004.04787" target="_blank">arXiv:2004.04787</a> [<a href="http://arxiv.org/pdf/2004.04787" target="_blank">pdf</a>]

<h2>Minimizing Energy Use of Mixed-Fleet Public Transit for Fixed-Route Service. (arXiv:2004.05146v3 [cs.AI] UPDATED)</h2>
<h3>Amutheezan Sivagnanam, Afiya Ayman, Michael Wilbur, Philip Pugliese, Abhishek Dubey, Aron Laszka</h3>
<p>Affordable public transit services are crucial for communities since they
enable residents to access employment, education, and other services.
Unfortunately, transit services that provide wide coverage tend to suffer from
relatively low utilization, which results in high fuel usage per passenger per
mile, leading to high operating costs and environmental impact. Electric
vehicles (EVs) can reduce energy costs and environmental impact, but most
public transit agencies have to employ them in combination with conventional,
internal-combustion engine vehicles due to the high upfront costs of EVs. To
make the best use of such a mixed fleet of vehicles, transit agencies need to
optimize route assignments and charging schedules, which presents a challenging
problem for large transit networks. We introduce a novel problem formulation to
minimize fuel and electricity use by assigning vehicles to transit trips and
scheduling them for charging, while serving an existing fixed-route transit
schedule. We present an integer program for optimal assignment and scheduling,
and we propose polynomial-time heuristic and meta-heuristic algorithms for
larger networks. We evaluate our algorithms on the public transit service of
Chattanooga, TN using operational data collected from transit vehicles. Our
results show that the proposed algorithms are scalable and can reduce energy
use and, hence, environmental impact and operational costs. For Chattanooga,
the proposed algorithms can save $145,635 in energy costs and 576.7 metric tons
of CO2 emission annually.
</p>
<a href="http://arxiv.org/abs/2004.05146" target="_blank">arXiv:2004.05146</a> [<a href="http://arxiv.org/pdf/2004.05146" target="_blank">pdf</a>]

<h2>Multi-interactive Siamese Decoder for RGBT Salient Object Detection. (arXiv:2005.02315v2 [cs.CV] UPDATED)</h2>
<h3>Zhengzheng Tu, Zhun Li, Chenglong Li, Yang Lang, Jin Tang</h3>
<p>RGBT salient object detection (SOD) aims to segment the common prominent
regions of visible and thermal infrared images. Existing RGBT SOD methods don't
fully explore and exploit the potentials of complementarity of different
modalities and the global context of image contents, which play a vital role in
achieving accurate results. In this paper, we propose a multi-interactive
Siamese decoder to mine and model the multi-type interactions for accurate RGBT
SOD. In specific, we first encode RGB and thermal image pair into multi-level
multi-modal representation. Then, we design a novel Siamese decoder to
integrate the multi-level interactions of dual modalities and global contexts.
With these interactions, our method works well in diversely challenging
scenarios even in the presence of invalid modality. Moreover, the Siamese
decoder employs label supervision to drive feature learning in each modality
and the modality prejudice is thus suppressed. Finally, we carry out extensive
experiments on several benchmark datasets, and the results show that the
proposed method achieves the outstanding performance against state-of-the-art
algorithms. The source code has released at:
https://github.com/lz118/Multi-interactive-Siamese-Decoder.
</p>
<a href="http://arxiv.org/abs/2005.02315" target="_blank">arXiv:2005.02315</a> [<a href="http://arxiv.org/pdf/2005.02315" target="_blank">pdf</a>]

<h2>CARL: Controllable Agent with Reinforcement Learning for Quadruped Locomotion. (arXiv:2005.03288v3 [cs.LG] UPDATED)</h2>
<h3>Ying-Sheng Luo (1), Jonathan Hans Soeseno (1), Trista Pei-Chun Chen (1), Wei-Chao Chen (1, 2) ((1) Inventec Corp. (2) Skywatch Innovation Inc.)</h3>
<p>Motion synthesis in a dynamic environment has been a long-standing problem
for character animation. Methods using motion capture data tend to scale poorly
in complex environments because of their larger capturing and labeling
requirement. Physics-based controllers are effective in this regard, albeit
less controllable. In this paper, we present CARL, a quadruped agent that can
be controlled with high-level directives and react naturally to dynamic
environments. Starting with an agent that can imitate individual animation
clips, we use Generative Adversarial Networks to adapt high-level controls,
such as speed and heading, to action distributions that correspond to the
original animations. Further fine-tuning through the deep reinforcement
learning enables the agent to recover from unseen external perturbations while
producing smooth transitions. It then becomes straightforward to create
autonomous agents in dynamic environments by adding navigation modules over the
entire process. We evaluate our approach by measuring the agent's ability to
follow user control and provide a visual analysis of the generated motion to
show its effectiveness.
</p>
<a href="http://arxiv.org/abs/2005.03288" target="_blank">arXiv:2005.03288</a> [<a href="http://arxiv.org/pdf/2005.03288" target="_blank">pdf</a>]

<h2>Graph Partitioning and Graph Neural Network based Hierarchical Graph Matching for Graph Similarity Computation. (arXiv:2005.08008v3 [cs.LG] UPDATED)</h2>
<h3>Haoyan Xu, Ziheng Duan, Jie Feng, Runjian Chen, Qianru Zhang, Zhongbin Xu, Yueyang Wang</h3>
<p>Graph similarity computation aims to predict a similarity score between one
pair of graphs to facilitate downstream applications, such as finding the most
similar chemical compounds similar to a query compound or Fewshot 3D Action
Recognition. Recently, some graph similarity computation models based on neural
networks have been proposed, which are either based on graph-level interaction
or node-level comparison. However, when the number of nodes in the graph
increases, it will inevitably bring about reduced representation ability or
high computation cost. Motivated by this observation, we propose a graph
partitioning and graph neural network-based model, called PSimGNN, to
effectively resolve this issue. Specifically, each of the input graphs is
partitioned into a set of subgraphs to extract the local structural features
directly. Next, a novel graph neural network with an attention mechanism is
designed to map each subgraph into an embedding vector. Some of these subgraph
pairs are automatically selected for node-level comparison to supplement the
subgraph-level embedding with fine-grained information. Finally, coarse-grained
interaction information among subgraphs and fine-grained comparison information
among nodes in different subgraphs are integrated to predict the final
similarity score. Experimental results on graph datasets with different graph
sizes demonstrate that PSimGNN outperforms state-of-the-art methods in graph
similarity computation tasks using approximate Graph Edit Distance (GED) as the
graph similarity metric.
</p>
<a href="http://arxiv.org/abs/2005.08008" target="_blank">arXiv:2005.08008</a> [<a href="http://arxiv.org/pdf/2005.08008" target="_blank">pdf</a>]

<h2>Efficient Poverty Mapping using Deep Reinforcement Learning. (arXiv:2006.04224v2 [cs.CV] UPDATED)</h2>
<h3>Kumar Ayush, Burak Uzkent, Kumar Tanmay, Marshall Burke, David Lobell, Stefano Ermon</h3>
<p>The combination of high-resolution satellite imagery and machine learning
have proven useful in many sustainability-related tasks, including poverty
prediction, infrastructure measurement, and forest monitoring. However, the
accuracy afforded by high-resolution imagery comes at a cost, as such imagery
is extremely expensive to purchase at scale. This creates a substantial hurdle
to the efficient scaling and widespread adoption of high-resolution-based
approaches. To reduce acquisition costs while maintaining accuracy, we propose
a reinforcement learning approach in which free low-resolution imagery is used
to dynamically identify where to acquire costly high-resolution images, prior
to performing a deep learning task on the high-resolution images. We apply this
approach to the task of poverty prediction in Uganda, building on an earlier
approach that used object detection to count objects and use these counts to
predict poverty. Our approach exceeds previous performance benchmarks on this
task while using 80% fewer high-resolution images. Our approach could have
application in many sustainability domains that require high-resolution
imagery.
</p>
<a href="http://arxiv.org/abs/2006.04224" target="_blank">arXiv:2006.04224</a> [<a href="http://arxiv.org/pdf/2006.04224" target="_blank">pdf</a>]

<h2>Manifold structure in graph embeddings. (arXiv:2006.05168v3 [stat.ML] UPDATED)</h2>
<h3>Patrick Rubin-Delanchy</h3>
<p>Statistical analysis of a graph often starts with embedding, the process of
representing its nodes as points in space. How to choose the embedding
dimension is a nuanced decision in practice, but in theory a notion of true
dimension is often available. In spectral embedding, this dimension may be very
high. However, this paper shows that existing random graph models, including
graphon and other latent position models, predict the data should live near a
much lower-dimensional set. One may therefore circumvent the curse of
dimensionality by employing methods which exploit hidden manifold structure.
</p>
<a href="http://arxiv.org/abs/2006.05168" target="_blank">arXiv:2006.05168</a> [<a href="http://arxiv.org/pdf/2006.05168" target="_blank">pdf</a>]

<h2>Improving Graph Neural Network Expressivity via Subgraph Isomorphism Counting. (arXiv:2006.09252v2 [cs.LG] UPDATED)</h2>
<h3>Giorgos Bouritsas, Fabrizio Frasca, Stefanos Zafeiriou, Michael M. Bronstein</h3>
<p>While Graph Neural Networks (GNNs) have achieved remarkable results in a
variety of applications, recent studies exposed important shortcomings in their
ability to capture the structure of the underlying graph. It has been shown
that the expressive power of standard GNNs is bounded by the Weisfeiler-Leman
(WL) graph isomorphism test, from which they inherit proven limitations such as
the inability to detect and count graph substructures. On the other hand, there
is significant empirical evidence, e.g. in network science and bioinformatics,
that substructures are often informative for downstream tasks, suggesting that
it is desirable to design GNNs capable of leveraging this important source of
information. To this end, we propose a novel topologically-aware message
passing scheme based on substructure encoding. We show that our architecture
allows incorporating domain-specific inductive biases and that it is strictly
more expressive than the WL test. Importantly, in contrast to recent works on
the expressivity of GNNs, we do not attempt to adhere to the WL hierarchy; this
allows us to retain multiple attractive properties of standard GNNs such as
locality and linear network complexity, while being able to disambiguate even
hard instances of graph isomorphism. We extensively evaluate our method on
graph classification and regression tasks and show state-of-the-art results on
multiple datasets including molecular graphs and social networks.
</p>
<a href="http://arxiv.org/abs/2006.09252" target="_blank">arXiv:2006.09252</a> [<a href="http://arxiv.org/pdf/2006.09252" target="_blank">pdf</a>]

<h2>Neural Ordinary Differential Equation Control of Dynamics on Graphs. (arXiv:2006.09773v3 [cs.LG] UPDATED)</h2>
<h3>Thomas Asikis, Lucas B&#xf6;ttcher, Nino Antulov-Fantulin</h3>
<p>We study the ability of neural networks to steer or control trajectories of
dynamical systems on graphs, which we represent with neural ordinary
differential equations (neural ODEs). To do so, we introduce a neural-ODE
control (NODEC) framework and find that it can learn control signals that drive
graph dynamical systems into desired target states. While we use loss functions
that do not constrain the control energy, our results show that NODEC produces
control signals that are highly correlated with optimal (or minimum energy)
control signals. Finally, we empirically showcase the high performance and
versatility of NODEC for various (non-)linear dynamics and loss functions on
different graphs.
</p>
<a href="http://arxiv.org/abs/2006.09773" target="_blank">arXiv:2006.09773</a> [<a href="http://arxiv.org/pdf/2006.09773" target="_blank">pdf</a>]

<h2>The classification for High-dimension low-sample size data. (arXiv:2006.13018v2 [cs.LG] UPDATED)</h2>
<h3>Liran Shen, Meng Joo Er, Qingbo Yin</h3>
<p>Huge amount of applications in various fields, such as gene expression
analysis or computer vision, undergo data sets with high-dimensional
low-sample-size (HDLSS), which has putted forward great challenges for standard
statistical and modern machine learning methods. In this paper, we propose a
novel classification criterion on HDLSS, tolerance similarity, which emphasizes
the maximization of within-class variance on the premise of class separability.
According to this criterion, a novel linear binary classifier is designed,
denoted by No-separated Data Maximum Dispersion classifier (NPDMD). The
objective of NPDMD is to find a projecting direction w in which all of training
samples scatter in as large an interval as possible. NPDMD has several
characteristics compared to the state-of-the-art classification methods. First,
it works well on HDLSS. Second, it combines the sample statistical information
and local structural information (supporting vectors) into the objective
function to find the solution of projecting direction in the whole feature
spaces. Third, it solves the inverse of high dimensional matrix in low
dimensional space. Fourth, it is relatively simple to be implemented based on
Quadratic Programming. Fifth, it is robust to the model specification for
various real applications. The theoretical properties of NPDMD are deduced. We
conduct a series of evaluations on one simulated and six real-world benchmark
data sets, including face classification and mRNA classification. NPDMD
outperforms those widely used approaches in most cases, or at least obtains
comparable results.
</p>
<a href="http://arxiv.org/abs/2006.13018" target="_blank">arXiv:2006.13018</a> [<a href="http://arxiv.org/pdf/2006.13018" target="_blank">pdf</a>]

<h2>DocVQA: A Dataset for VQA on Document Images. (arXiv:2007.00398v3 [cs.CV] UPDATED)</h2>
<h3>Minesh Mathew, Dimosthenis Karatzas, C.V. Jawahar</h3>
<p>We present a new dataset for Visual Question Answering (VQA) on document
images called DocVQA. The dataset consists of 50,000 questions defined on
12,000+ document images. Detailed analysis of the dataset in comparison with
similar datasets for VQA and reading comprehension is presented. We report
several baseline results by adopting existing VQA and reading comprehension
models. Although the existing models perform reasonably well on certain types
of questions, there is large performance gap compared to human performance
(94.36% accuracy). The models need to improve specifically on questions where
understanding structure of the document is crucial. The dataset, code and
leaderboard are available at docvqa.org
</p>
<a href="http://arxiv.org/abs/2007.00398" target="_blank">arXiv:2007.00398</a> [<a href="http://arxiv.org/pdf/2007.00398" target="_blank">pdf</a>]

<h2>Multiple Instance-Based Video Anomaly Detection using Deep Temporal Encoding-Decoding. (arXiv:2007.01548v2 [cs.CV] UPDATED)</h2>
<h3>Ammar Mansoor Kamoona, Amirali Khodadadian Gosta, Alireza Bab-Hadiashar, Reza Hoseinnezhad</h3>
<p>In this paper, we propose a weakly supervised deep temporal encoding-decoding
solution for anomaly detection in surveillance videos using multiple instance
learning. The proposed approach uses both abnormal and normal video clips
during the training phase which is developed in the multiple instance framework
where we treat video as a bag and video clips as instances in the bag. Our main
contribution lies in the proposed novel approach to consider temporal relations
between video instances. We deal with video instances (clips) as a sequential
visual data rather than independent instances. We employ a deep temporal and
encoder network that is designed to capture spatial-temporal evolution of video
instances over time. We also propose a new loss function that is smoother than
similar loss functions recently presented in the computer vision literature,
and therefore; enjoys faster convergence and improved tolerance to local minima
during the training phase. The proposed temporal encoding-decoding approach
with modified loss is benchmarked against the state-of-the-art in simulation
studies. The results show that the proposed method performs similar to or
better than the state-of-the-art solutions for anomaly detection in video
surveillance applications.
</p>
<a href="http://arxiv.org/abs/2007.01548" target="_blank">arXiv:2007.01548</a> [<a href="http://arxiv.org/pdf/2007.01548" target="_blank">pdf</a>]

<h2>Discovering Reinforcement Learning Algorithms. (arXiv:2007.08794v3 [cs.LG] UPDATED)</h2>
<h3>Junhyuk Oh, Matteo Hessel, Wojciech M. Czarnecki, Zhongwen Xu, Hado van Hasselt, Satinder Singh, David Silver</h3>
<p>Reinforcement learning (RL) algorithms update an agent's parameters according
to one of several possible rules, discovered manually through years of
research. Automating the discovery of update rules from data could lead to more
efficient algorithms, or algorithms that are better adapted to specific
environments. Although there have been prior attempts at addressing this
significant scientific challenge, it remains an open question whether it is
feasible to discover alternatives to fundamental concepts of RL such as value
functions and temporal-difference learning. This paper introduces a new
meta-learning approach that discovers an entire update rule which includes both
'what to predict' (e.g. value functions) and 'how to learn from it' (e.g.
bootstrapping) by interacting with a set of environments. The output of this
method is an RL algorithm that we call Learned Policy Gradient (LPG). Empirical
results show that our method discovers its own alternative to the concept of
value functions. Furthermore it discovers a bootstrapping mechanism to maintain
and use its predictions. Surprisingly, when trained solely on toy environments,
LPG generalises effectively to complex Atari games and achieves non-trivial
performance. This shows the potential to discover general RL algorithms from
data.
</p>
<a href="http://arxiv.org/abs/2007.08794" target="_blank">arXiv:2007.08794</a> [<a href="http://arxiv.org/pdf/2007.08794" target="_blank">pdf</a>]

<h2>The multilayer random dot product graph. (arXiv:2007.10455v2 [stat.ML] UPDATED)</h2>
<h3>Andrew Jones, Patrick Rubin-Delanchy</h3>
<p>We present a significant extension of the latent position network model known
as the generalised random dot product graph to accommodate multiple graphs --
both undirected and directed -- which share a common subset of nodes, and
propose a method for jointly embedding the associated adjacency matrices, or
submatrices thereof, into a suitable latent space. Theoretical results
concerning the asymptotic behaviour of the node representations thus obtained
are established, showing that after the application of a linear transformation
these converge uniformly in the Euclidean norm to the latent positions with
Gaussian error. The flexibility of the model is demonstrated through
application to the tasks of latent position recovery and two-graph hypothesis
testing, in which it performs favourably compared to existing models. Empirical
improvements in link prediction over single graph embeddings are exhibited in a
cyber-security example.
</p>
<a href="http://arxiv.org/abs/2007.10455" target="_blank">arXiv:2007.10455</a> [<a href="http://arxiv.org/pdf/2007.10455" target="_blank">pdf</a>]

<h2>Navigating the Trade-Off between Multi-Task Learning and Learning to Multitask in Deep Neural Networks. (arXiv:2007.10527v2 [cs.LG] UPDATED)</h2>
<h3>Sachin Ravi, Sebastian Musslick, Maia Hamin, Theodore L. Willke, Jonathan D. Cohen</h3>
<p>The terms multi-task learning and multitasking are easily confused.
Multi-task learning refers to a paradigm in machine learning in which a network
is trained on various related tasks to facilitate the acquisition of tasks. In
contrast, multitasking is used to indicate, especially in the cognitive science
literature, the ability to execute multiple tasks simultaneously. While
multi-task learning exploits the discovery of common structure between tasks in
the form of shared representations, multitasking is promoted by separating
representations between tasks to avoid processing interference. Here, we build
on previous work involving shallow networks and simple task settings suggesting
that there is a trade-off between multi-task learning and multitasking,
mediated by the use of shared versus separated representations. We show that
the same tension arises in deep networks and discuss a meta-learning algorithm
for an agent to manage this trade-off in an unfamiliar environment. We display
through different experiments that the agent is able to successfully optimize
its training strategy as a function of the environment.
</p>
<a href="http://arxiv.org/abs/2007.10527" target="_blank">arXiv:2007.10527</a> [<a href="http://arxiv.org/pdf/2007.10527" target="_blank">pdf</a>]

<h2>Storage Fit Learning with Feature Evolvable Streams. (arXiv:2007.11280v2 [cs.LG] UPDATED)</h2>
<h3>Bo-Jian Hou, Yu-Hu Yan, Peng Zhao, Zhi-Hua Zhou</h3>
<p>Feature evolvable learning has been widely studied in recent years where old
features will vanish and new features will emerge when learning with streams.
Conventional methods usually assume that a label will be revealed after
prediction at each time step. However, in practice, this assumption may not
hold whereas no label will be given at most time steps. A good solution is to
leverage the technique of manifold regularization to utilize the previous
similar data to assist the refinement of the online model. Nevertheless, this
approach needs to store all previous data which is impossible in learning with
streams that arrive sequentially in large volume. Thus we need a buffer to
store part of them. Considering that different devices may have different
storage budgets, the learning approaches should be flexible subject to the
storage budget limit. In this paper, we propose a new setting: Storage-Fit
Feature-Evolvable streaming Learning (SF$^2$EL) which incorporates the issue of
rarely-provided labels into feature evolution. Our framework is able to fit its
behavior to different storage budgets when learning with feature evolvable
streams with unlabeled data. Besides, both theoretical and empirical results
validate that our approach can preserve the merit of the original feature
evolvable learning i.e., can always track the best baseline and thus perform
well at any time step.
</p>
<a href="http://arxiv.org/abs/2007.11280" target="_blank">arXiv:2007.11280</a> [<a href="http://arxiv.org/pdf/2007.11280" target="_blank">pdf</a>]

<h2>Multi-Sample Online Learning for Probabilistic Spiking Neural Networks. (arXiv:2007.11894v2 [cs.LG] UPDATED)</h2>
<h3>Hyeryung Jang, Osvaldo Simeone</h3>
<p>Spiking Neural Networks (SNNs) capture some of the efficiency of biological
brains for inference and learning via the dynamic, online, event-driven
processing of binary time series. Most existing learning algorithms for SNNs
are based on deterministic neuronal models, such as leaky integrate-and-fire,
and rely on heuristic approximations of backpropagation through time that
enforce constraints such as locality. In contrast, probabilistic SNN models can
be trained directly via principled online, local, update rules that have proven
to be particularly effective for resource-constrained systems. This paper
investigates another advantage of probabilistic SNNs, namely their capacity to
generate independent outputs when queried over the same input. It is shown that
the multiple generated output samples can be used during inference to robustify
decisions and to quantify uncertainty -- a feature that deterministic SNN
models cannot provide. Furthermore, they can be leveraged for training in order
to obtain more accurate statistical estimates of the log-loss training
criterion, as well as of its gradient. Specifically, this paper introduces an
online learning rule based on generalized expectation-maximization (GEM) that
follows a three-factor form with global learning signals and is referred to as
GEM-SNN. Experimental results on structured output memorization and
classification on a standard neuromorphic data set demonstrate significant
improvements in terms of log-likelihood, accuracy, and calibration when
increasing the number of samples used for inference and training.
</p>
<a href="http://arxiv.org/abs/2007.11894" target="_blank">arXiv:2007.11894</a> [<a href="http://arxiv.org/pdf/2007.11894" target="_blank">pdf</a>]

<h2>The role of explainability in creating trustworthy artificial intelligence for health care: a comprehensive survey of the terminology, design choices, and evaluation strategies. (arXiv:2007.15911v2 [cs.AI] UPDATED)</h2>
<h3>Aniek F. Markus, Jan A. Kors, Peter R. Rijnbeek</h3>
<p>Artificial intelligence (AI) has huge potential to improve the health and
well-being of people, but adoption in clinical practice is still limited. Lack
of transparency is identified as one of the main barriers to implementation, as
clinicians should be confident the AI system can be trusted. Explainable AI has
the potential to overcome this issue and can be a step towards trustworthy AI.
In this paper we review the recent literature to provide guidance to
researchers and practitioners on the design of explainable AI systems for the
health-care domain and contribute to formalization of the field of explainable
AI. We argue the reason to demand explainability determines what should be
explained as this determines the relative importance of the properties of
explainability (i.e. interpretability and fidelity). Based on this, we propose
a framework to guide the choice between classes of explainable AI methods
(explainable modelling versus post-hoc explanation; model-based,
attribution-based, or example-based explanations; global and local
explanations). Furthermore, we find that quantitative evaluation metrics, which
are important for objective standardized evaluation, are still lacking for some
properties (e.g. clarity) and types of explanations (e.g. example-based
methods). We conclude that explainable modelling can contribute to trustworthy
AI, but the benefits of explainability still need to be proven in practice and
complementary measures might be needed to create trustworthy AI in health care
(e.g. reporting data quality, performing extensive (external) validation, and
regulation).
</p>
<a href="http://arxiv.org/abs/2007.15911" target="_blank">arXiv:2007.15911</a> [<a href="http://arxiv.org/pdf/2007.15911" target="_blank">pdf</a>]

<h2>Deep Networks with Fast Retraining. (arXiv:2008.07387v2 [cs.LG] UPDATED)</h2>
<h3>Wandong Zhang (1 and 2), Yimin Yang (2 and 3), Jonathan Wu (1) ((1) University of Windsor, (2) Lakehead University, (3) Vector Institute for Artificial Intelligence)</h3>
<p>Recent work [1] has utilized Moore-Penrose (MP) inverse in deep convolutional
neural network (DCNN) learning, which achieves better generalization
performance over the DCNN with a stochastic gradient descent (SGD) pipeline.
However, Yang's work has not gained much popularity in practice due to its high
sensitivity of hyper-parameters and stringent demands of computational
resources. To enhance its applicability, this paper proposes a novel MP
inverse-based fast retraining strategy. In each training epoch, a random
learning strategy that controls the number of convolutional layers trained in
the backward pass is first utilized. Then, an MP inverse-based batch-by-batch
learning strategy, which enables the network to be implemented without access
to industrial-scale computational resources, is developed to refine the dense
layer parameters. Experimental results empirically demonstrate that fast
retraining is a unified strategy that can be used for all DCNNs. Compared to
other learning strategies, the proposed learning pipeline has robustness
against the hyper-parameters, and the requirement of computational resources is
significantly reduced. [1] Y. Yang, J. Wu, X. Feng, and A. Thangarajah,
"Recomputation of dense layers for the perfor-238mance improvement of dcnn,"
IEEE Trans. Pattern Anal. Mach. Intell., 2019.
</p>
<a href="http://arxiv.org/abs/2008.07387" target="_blank">arXiv:2008.07387</a> [<a href="http://arxiv.org/pdf/2008.07387" target="_blank">pdf</a>]

<h2>MultiVERSE: a multiplex and multiplex-heterogeneous network embedding approach. (arXiv:2008.10085v2 [cs.LG] UPDATED)</h2>
<h3>L&#xe9;o Pio-Lopez, Alberto Valdeolivas, Laurent Tichit, &#xc9;lisabeth Remy, Ana&#xef;s Baudot</h3>
<p>Network embedding approaches are gaining momentum to analyse a large variety
of networks. Indeed, these approaches have demonstrated their efficiency for
tasks such as community detection, node classification, and link prediction.
However, very few network embedding methods have been specifically designed to
handle multiplex networks, i.e. networks composed of different layers sharing
the same set of nodes but having different types of edges. Moreover, to our
knowledge, existing approaches cannot embed multiple nodes from
multiplex-heterogeneous networks, i.e. networks composed of several layers
containing both different types of nodes and edges. In this study, we propose
MultiVERSE, an extension of the VERSE method with Random Walks with Restart on
Multiplex (RWR-M) and Multiplex-Heterogeneous (RWR-MH) networks. MultiVERSE is
a fast and scalable method to learn node embeddings from multiplex and
multiplex-heterogeneous networks. We evaluate MultiVERSE on several biological
and social networks and demonstrate its efficiency. MultiVERSE indeed
outperforms most of the other methods in the tasks of link prediction and
network reconstruction for multiplex network embedding, and is also efficient
in the task of link prediction for multiplex-heterogeneous network embedding.
Finally, we apply MultiVERSE to study rare disease-gene associations using link
prediction and clustering. MultiVERSE is freely available on github at
https://github.com/Lpiol/MultiVERSE.
</p>
<a href="http://arxiv.org/abs/2008.10085" target="_blank">arXiv:2008.10085</a> [<a href="http://arxiv.org/pdf/2008.10085" target="_blank">pdf</a>]

<h2>An Internal Cluster Validity Index Using a Distance-based Separability Measure. (arXiv:2009.01328v2 [cs.LG] UPDATED)</h2>
<h3>Shuyue Guan, Murray Loew</h3>
<p>To evaluate clustering results is a significant part of cluster analysis.
There are no true class labels for clustering in typical unsupervised learning.
Thus, a number of internal evaluations, which use predicted labels and data,
have been created. They are also named internal cluster validity indices
(CVIs). Without true labels, to design an effective CVI is not simple because
it is similar to create a clustering method. And, to have more CVIs is crucial
because there is no universal CVI that can be used to measure all datasets, and
no specific method for selecting a proper CVI for clusters without true labels.
Therefore, to apply more CVIs to evaluate clustering results is necessary. In
this paper, we propose a novel CVI - called Distance-based Separability Index
(DSI), based on a data separability measure. We applied the DSI and eight other
internal CVIs including early studies from Dunn (1974) to most recent studies
CVDD (2019) as comparison. We used an external CVI as ground truth for
clustering results of five clustering algorithms on 12 real and 97 synthetic
datasets. Results show DSI is an effective, unique, and competitive CVI to
other compared CVIs. In addition, we summarized the general process to evaluate
CVIs and created a new method - rank difference - to compare the results of
CVIs.
</p>
<a href="http://arxiv.org/abs/2009.01328" target="_blank">arXiv:2009.01328</a> [<a href="http://arxiv.org/pdf/2009.01328" target="_blank">pdf</a>]

<h2>Implicit Graph Neural Networks. (arXiv:2009.06211v2 [cs.LG] UPDATED)</h2>
<h3>Fangda Gu, Heng Chang, Wenwu Zhu, Somayeh Sojoudi, Laurent El Ghaoui</h3>
<p>Graph Neural Networks (GNNs) are widely used deep learning models that learn
meaningful representations from graph-structured data. Due to the finite nature
of the underlying recurrent structure, current GNN methods may struggle to
capture long-range dependencies in underlying graphs. To overcome this
difficulty, we propose a graph learning framework, called Implicit Graph Neural
Networks (IGNN), where predictions are based on the solution of a fixed-point
equilibrium equation involving implicitly defined "state" vectors. We use the
Perron-Frobenius theory to derive sufficient conditions that ensure
well-posedness of the framework. Leveraging implicit differentiation, we derive
a tractable projected gradient descent method to train the framework.
Experiments on a comprehensive range of tasks show that IGNNs consistently
capture long-range dependencies and outperform the state-of-the-art GNN models.
</p>
<a href="http://arxiv.org/abs/2009.06211" target="_blank">arXiv:2009.06211</a> [<a href="http://arxiv.org/pdf/2009.06211" target="_blank">pdf</a>]

<h2>Decision-based Universal Adversarial Attack. (arXiv:2009.07024v4 [cs.CV] UPDATED)</h2>
<h3>Jing Wu, Mingyi Zhou, Shuaicheng Liu, Yipeng Liu, Ce Zhu</h3>
<p>A single perturbation can pose the most natural images to be misclassified by
classifiers. In black-box setting, current universal adversarial attack methods
utilize substitute models to generate the perturbation, then apply the
perturbation to the attacked model. However, this transfer often produces
inferior results. In this study, we directly work in the black-box setting to
generate the universal adversarial perturbation. Besides, we aim to design an
adversary generating a single perturbation having texture like stripes based on
orthogonal matrix, as the top convolutional layers are sensitive to stripes. To
this end, we propose an efficient Decision-based Universal Attack (DUAttack).
With few data, the proposed adversary computes the perturbation based solely on
the final inferred labels, but good transferability has been realized not only
across models but also span different vision tasks. The effectiveness of
DUAttack is validated through comparisons with other state-of-the-art attacks.
The efficiency of DUAttack is also demonstrated on real world settings
including the Microsoft Azure. In addition, several representative defense
methods are struggling with DUAttack, indicating the practicability of the
proposed method.
</p>
<a href="http://arxiv.org/abs/2009.07024" target="_blank">arXiv:2009.07024</a> [<a href="http://arxiv.org/pdf/2009.07024" target="_blank">pdf</a>]

<h2>BSN++: Complementary Boundary Regressor with Scale-Balanced Relation Modeling for Temporal Action Proposal Generation. (arXiv:2009.07641v4 [cs.CV] UPDATED)</h2>
<h3>Haisheng Su, Weihao Gan, Wei Wu, Yu Qiao, Junjie Yan</h3>
<p>Generating human action proposals in untrimmed videos is an important yet
challenging task with wide applications. Current methods often suffer from the
noisy boundary locations and the inferior quality of confidence scores used for
proposal retrieving. In this paper, we present BSN++, a new framework which
exploits complementary boundary regressor and relation modeling for temporal
proposal generation. First, we propose a novel boundary regressor based on the
complementary characteristics of both starting and ending boundary classifiers.
Specifically, we utilize the U-shaped architecture with nested skip connections
to capture rich contexts and introduce bi-directional boundary matching
mechanism to improve boundary precision. Second, to account for the
proposal-proposal relations ignored in previous methods, we devise a proposal
relation block to which includes two self-attention modules from the aspects of
position and channel. Furthermore, we find that there inevitably exists data
imbalanced problems in the positive/negative proposals and temporal durations,
which harm the model performance on tail distributions. To relieve this issue,
we introduce the scale-balanced re-sampling strategy. Extensive experiments are
conducted on two popular benchmarks: ActivityNet-1.3 and THUMOS14, which
demonstrate that BSN++ achieves the state-of-the-art performance. Not
surprisingly, the proposed BSN++ ranked 1st place in the CVPR19 - ActivityNet
challenge leaderboard on temporal action localization task.
</p>
<a href="http://arxiv.org/abs/2009.07641" target="_blank">arXiv:2009.07641</a> [<a href="http://arxiv.org/pdf/2009.07641" target="_blank">pdf</a>]

<h2>GOCor: Bringing Globally Optimized Correspondence Volumes into Your Neural Network. (arXiv:2009.07823v3 [cs.CV] UPDATED)</h2>
<h3>Prune Truong, Martin Danelljan, Luc Van Gool, Radu Timofte</h3>
<p>The feature correlation layer serves as a key neural network module in
numerous computer vision problems that involve dense correspondences between
image pairs. It predicts a correspondence volume by evaluating dense scalar
products between feature vectors extracted from pairs of locations in two
images. However, this point-to-point feature comparison is insufficient when
disambiguating multiple similar regions in an image, severely affecting the
performance of the end task. We propose GOCor, a fully differentiable dense
matching module, acting as a direct replacement to the feature correlation
layer. The correspondence volume generated by our module is the result of an
internal optimization procedure that explicitly accounts for similar regions in
the scene. Moreover, our approach is capable of effectively learning spatial
matching priors to resolve further matching ambiguities. We analyze our GOCor
module in extensive ablative experiments. When integrated into state-of-the-art
networks, our approach significantly outperforms the feature correlation layer
for the tasks of geometric matching, optical flow, and dense semantic matching.
The code and trained models will be made available at
github.com/PruneTruong/GOCor.
</p>
<a href="http://arxiv.org/abs/2009.07823" target="_blank">arXiv:2009.07823</a> [<a href="http://arxiv.org/pdf/2009.07823" target="_blank">pdf</a>]

<h2>Bongard-LOGO: A New Benchmark for Human-Level Concept Learning and Reasoning. (arXiv:2010.00763v4 [cs.AI] UPDATED)</h2>
<h3>Weili Nie, Zhiding Yu, Lei Mao, Ankit B. Patel, Yuke Zhu, Animashree Anandkumar</h3>
<p>Humans have an inherent ability to learn novel concepts from only a few
samples and generalize these concepts to different situations. Even though
today's machine learning models excel with a plethora of training data on
standard recognition tasks, a considerable gap exists between machine-level
pattern recognition and human-level concept learning. To narrow this gap, the
Bongard problems (BPs) were introduced as an inspirational challenge for visual
cognition in intelligent systems. Despite new advances in representation
learning and learning to learn, BPs remain a daunting challenge for modern AI.
Inspired by the original one hundred BPs, we propose a new benchmark
Bongard-LOGO for human-level concept learning and reasoning. We develop a
program-guided generation technique to produce a large set of
human-interpretable visual cognition problems in action-oriented LOGO language.
Our benchmark captures three core properties of human cognition: 1)
context-dependent perception, in which the same object may have disparate
interpretations given different contexts; 2) analogy-making perception, in
which some meaningful concepts are traded off for other meaningful concepts;
and 3) perception with a few samples but infinite vocabulary. In experiments,
we show that the state-of-the-art deep learning methods perform substantially
worse than human subjects, implying that they fail to capture core human
cognition properties. Finally, we discuss research directions towards a general
architecture for visual reasoning to tackle this benchmark.
</p>
<a href="http://arxiv.org/abs/2010.00763" target="_blank">arXiv:2010.00763</a> [<a href="http://arxiv.org/pdf/2010.00763" target="_blank">pdf</a>]

<h2>Learning Dexterous Manipulation from Suboptimal Experts. (arXiv:2010.08587v2 [cs.RO] UPDATED)</h2>
<h3>Rae Jeong, Jost Tobias Springenberg, Jackie Kay, Daniel Zheng, Yuxiang Zhou, Alexandre Galashov, Nicolas Heess, Francesco Nori</h3>
<p>Learning dexterous manipulation in high-dimensional state-action spaces is an
important open challenge with exploration presenting a major bottleneck.
Although in many cases the learning process could be guided by demonstrations
or other suboptimal experts, current RL algorithms for continuous action spaces
often fail to effectively utilize combinations of highly off-policy expert data
and on-policy exploration data. As a solution, we introduce Relative Entropy
Q-Learning (REQ), a simple policy iteration algorithm that combines ideas from
successful offline and conventional RL algorithms. It represents the optimal
policy via importance sampling from a learned prior and is well-suited to take
advantage of mixed data distributions. We demonstrate experimentally that REQ
outperforms several strong baselines on robotic manipulation tasks for which
suboptimal experts are available. We show how suboptimal experts can be
constructed effectively by composing simple waypoint tracking controllers, and
we also show how learned primitives can be combined with waypoint controllers
to obtain reference behaviors to bootstrap a complex manipulation task on a
simulated bimanual robot with human-like hands. Finally, we show that REQ is
also effective for general off-policy RL, offline RL, and RL from
demonstrations. Videos and further materials are available at
sites.google.com/view/rlfse.
</p>
<a href="http://arxiv.org/abs/2010.08587" target="_blank">arXiv:2010.08587</a> [<a href="http://arxiv.org/pdf/2010.08587" target="_blank">pdf</a>]

<h2>Robot Learning with Crash Constraints. (arXiv:2010.08669v2 [cs.RO] UPDATED)</h2>
<h3>Alonso Marco, Dominik Baumann, Majid Khadiv, Philipp Hennig, Ludovic Righetti, Sebastian Trimpe</h3>
<p>In the past decade, numerous machine learning algorithms have been shown to
successfully learn optimal policies to control real robotic systems. However,
it is common to encounter failing behaviors as the learning loop progresses.
Specifically, in robot applications where failing is undesired but not
catastrophic, many algorithms struggle with leveraging data obtained from
failures. This is usually caused by (i) the failed experiment ending
prematurely, or (ii) the acquired data being scarce or corrupted. Both
complicate the design of proper reward functions to penalize failures. In this
paper, we propose a framework that addresses those issues. We consider failing
behaviors as those that violate a constraint and address the problem of
learning with crash constraints, where no data is obtained upon constraint
violation. The no-data case is addressed by a novel GP model (GPCR) for the
constraint that combines discrete events (failure/success) with continuous
observations (only obtained upon success). We demonstrate the effectiveness of
our framework on simulated benchmarks and on a real jumping quadruped, where
the constraint threshold is unknown a priori. Experimental data is collected,
by means of constrained Bayesian optimization, directly on the real robot. Our
results outperform manual tuning and GPCR proves useful on estimating the
constraint threshold.
</p>
<a href="http://arxiv.org/abs/2010.08669" target="_blank">arXiv:2010.08669</a> [<a href="http://arxiv.org/pdf/2010.08669" target="_blank">pdf</a>]

<h2>Affordance as general value function: A computational model. (arXiv:2010.14289v2 [cs.AI] UPDATED)</h2>
<h3>Daniel Graves, Johannes G&#xfc;nther, Jun Luo</h3>
<p>General value functions (GVFs) in the reinforcement learning (RL) literature
are long-term predictive summaries of the outcomes of agents following specific
policies in the environment. Affordances as perceived action possibilities with
specific valence may be cast into predicted policy-relative goodness and
modelled as GVFs. A systematic explication of this connection shows that GVFs
and especially their deep learning embodiments (1) realize affordance
prediction as a form of direct perception, (2) illuminate the fundamental
connection between action and perception in affordance, and (3) offer a
scalable way to learn affordances using RL methods. Through an extensive review
of existing literature on GVF applications and representative affordance
research in robotics, we demonstrate that GVFs provide the right framework for
learning affordances in real-world applications. In addition, we highlight a
few new avenues of research opened up by the perspective of "affordance as
GVF", including using GVFs for orchestrating complex behaviors.
</p>
<a href="http://arxiv.org/abs/2010.14289" target="_blank">arXiv:2010.14289</a> [<a href="http://arxiv.org/pdf/2010.14289" target="_blank">pdf</a>]

<h2>Bayes-Adaptive Deep Model-Based Policy Optimisation. (arXiv:2010.15948v3 [cs.RO] UPDATED)</h2>
<h3>Tai Hoang, Ngo Anh Vien</h3>
<p>We introduce a Bayesian (deep) model-based reinforcement learning method
(RoMBRL) that can capture model uncertainty to achieve sample-efficient policy
optimisation. We propose to formulate the model-based policy optimisation
problem as a Bayes-adaptive Markov decision process (BAMDP). RoMBRL maintains
model uncertainty via belief distributions through a deep Bayesian neural
network whose samples are generated via stochastic gradient Hamiltonian Monte
Carlo. Uncertainty is propagated through simulations controlled by sampled
models and history-based policies. As beliefs are encoded in visited histories,
we propose a history-based policy network that can be end-to-end trained to
generalise across history space and will be trained using recurrent
Trust-Region Policy Optimisation. We show that RoMBRL outperforms existing
approaches on many challenging control benchmark tasks in terms of sample
complexity and task performance. The source code of this paper is also publicly
available on https://github.com/thobotics/RoMBRL.
</p>
<a href="http://arxiv.org/abs/2010.15948" target="_blank">arXiv:2010.15948</a> [<a href="http://arxiv.org/pdf/2010.15948" target="_blank">pdf</a>]

<h2>General Data Analytics with Applications to Visual Information Analysis: A Provable Backward-Compatible Semisimple Paradigm over T-Algebra. (arXiv:2011.00307v5 [cs.CV] UPDATED)</h2>
<h3>Liang Liao, Stephen John Maybank</h3>
<p>We consider a novel backward-compatible paradigm of general data analytics
over a recently-reported semisimple algebra (called t-algebra). We study the
abstract algebraic framework over the t-algebra by representing the elements of
t-algebra by fix-sized multi-way arrays of complex numbers and the algebraic
structure over the t-algebra by a collection of direct-product constituents.
Over the t-algebra, many algorithms are generalized in a straightforward manner
using this new semisimple paradigm. To demonstrate the new paradigm's
performance and its backward-compatibility, we generalize some canonical
algorithms for visual pattern analysis. Experiments on public datasets show
that the generalized algorithms compare favorably with their canonical
counterparts.
</p>
<a href="http://arxiv.org/abs/2011.00307" target="_blank">arXiv:2011.00307</a> [<a href="http://arxiv.org/pdf/2011.00307" target="_blank">pdf</a>]

<h2>InferBench: Understanding Deep Learning Inference Serving with an Automatic Benchmarking System. (arXiv:2011.02327v3 [cs.LG] UPDATED)</h2>
<h3>Huaizheng Zhang, Yizheng Huang, Yonggang Wen, Jianxiong Yin, Kyle Guan</h3>
<p>Deep learning (DL) models have become core modules for many applications.
However, deploying these models without careful performance benchmarking that
considers both hardware and software's impact often leads to poor service and
costly operational expenditure. To facilitate DL models' deployment, we
implement an automatic and comprehensive benchmark system for DL developers. To
accomplish benchmark-related tasks, the developers only need to prepare a
configuration file consisting of a few lines of code. Our system, deployed to a
leader server in DL clusters, will dispatch users' benchmark jobs to follower
workers. Next, the corresponding requests, workload, and even models can be
generated automatically by the system to conduct DL serving benchmarks.
Finally, developers can leverage many analysis tools and models in our system
to gain insights into the trade-offs of different system configurations. In
addition, a two-tier scheduler is incorporated to avoid unnecessary
interference and improve average job compilation time by up to 1.43x
(equivalent of 30\% reduction). Our system design follows the best practice in
DL clusters operations to expedite day-to-day DL service evaluation efforts by
the developers. We conduct many benchmark experiments to provide in-depth and
comprehensive evaluations. We believe these results are of great values as
guidelines for DL service configuration and resource allocation.
</p>
<a href="http://arxiv.org/abs/2011.02327" target="_blank">arXiv:2011.02327</a> [<a href="http://arxiv.org/pdf/2011.02327" target="_blank">pdf</a>]

<h2>A Theoretical Computer Science Perspective on Consciousness. (arXiv:2011.09850v2 [cs.AI] UPDATED)</h2>
<h3>Manuel Blum, Lenore Blum</h3>
<p>The quest to understand consciousness, once the purview of philosophers and
theologians, is now actively pursued by scientists of many stripes. This paper
studies consciousness from the perspective of theoretical computer science. It
formalizes the Global Workspace Theory (GWT) originated by cognitive
neuroscientist Bernard Baars and further developed by him, Stanislas Dehaene,
and others. Our major contribution lies in the precise formal definition of a
Conscious Turing Machine (CTM), also called a Conscious AI. We define the CTM
in the spirit of Alan Turing's simple yet powerful definition of a computer,
the Turing Machine (TM). We are not looking for a complex model of the brain
nor of cognition but for a simple model of (the admittedly complex concept of)
consciousness. After formally defining CTM, we give a formal definition of
consciousness in CTM. We then suggest why the CTM has the feeling of
consciousness. The reasonableness of the definitions and explanations can be
judged by how well they agree with commonly accepted intuitive concepts of
human consciousness, the breadth of related concepts that the model explains
easily and naturally, and the extent of its agreement with scientific evidence.
</p>
<a href="http://arxiv.org/abs/2011.09850" target="_blank">arXiv:2011.09850</a> [<a href="http://arxiv.org/pdf/2011.09850" target="_blank">pdf</a>]

<h2>CircleGAN: Generative Adversarial Learning across Spherical Circles. (arXiv:2011.12486v2 [cs.CV] UPDATED)</h2>
<h3>Woohyeon Shim, Minsu Cho</h3>
<p>We present a novel discriminator for GANs that improves realness and
diversity of generated samples by learning a structured hypersphere embedding
space using spherical circles. The proposed discriminator learns to populate
realistic samples around the longest spherical circle, i.e., a great circle,
while pushing unrealistic samples toward the poles perpendicular to the great
circle. Since longer circles occupy larger area on the hypersphere, they
encourage more diversity in representation learning, and vice versa.
Discriminating samples based on their corresponding spherical circles can thus
naturally induce diversity to generated samples. We also extend the proposed
method for conditional settings with class labels by creating a hypersphere for
each category and performing class-wise discrimination and update. In
experiments, we validate the effectiveness for both unconditional and
conditional generation on standard benchmarks, achieving the state of the art.
</p>
<a href="http://arxiv.org/abs/2011.12486" target="_blank">arXiv:2011.12486</a> [<a href="http://arxiv.org/pdf/2011.12486" target="_blank">pdf</a>]

<h2>Bayesian Image Reconstruction using Deep Generative Models. (arXiv:2012.04567v2 [cs.CV] UPDATED)</h2>
<h3>Razvan V Marinescu, Daniel Moyer, Polina Golland</h3>
<p>Machine learning models are commonly trained end-to-end and in a supervised
setting, using paired (input, output) data. Classical examples include recent
super-resolution methods that train on pairs of (low-resolution,
high-resolution) images. However, these end-to-end approaches require
re-training every time there is a distribution shift in the inputs (e.g., night
images vs daylight) or relevant latent variables (e.g., camera blur or hand
motion). In this work, we leverage state-of-the-art (SOTA) generative models
(here StyleGAN2) for building powerful image priors, which enable application
of Bayes' theorem for many downstream reconstruction tasks. Our method, called
Bayesian Reconstruction through Generative Models (BRGM), uses a single
pre-trained generator model to solve different image restoration tasks, i.e.,
super-resolution and in-painting, by combining it with different forward
corruption models. We demonstrate BRGM on three large, yet diverse, datasets
that enable us to build powerful priors: (i) 60,000 images from the Flick Faces
High Quality dataset (ii) 240,000 chest X-rays from MIMIC III and (iii) a
combined collection of 5 brain MRI datasets with 7,329 scans. Across all three
datasets and without any dataset-specific hyperparameter tuning, our approach
yields state-of-the-art performance on super-resolution, particularly at
low-resolution levels, as well as inpainting, compared to state-of-the-art
methods that are specific to each reconstruction task. Our source code and all
pre-trained models are available online:
https://razvanmarinescu.github.io/brgm/.
</p>
<a href="http://arxiv.org/abs/2012.04567" target="_blank">arXiv:2012.04567</a> [<a href="http://arxiv.org/pdf/2012.04567" target="_blank">pdf</a>]

<h2>A novel machine learning-based optimization algorithm (ActivO) for accelerating simulation-driven engine design. (arXiv:2012.04649v2 [cs.LG] UPDATED)</h2>
<h3>Opeoluwa Owoyele, Pinaki Pal</h3>
<p>A novel design optimization approach (ActivO) that employs an ensemble of
machine learning algorithms is presented. The proposed approach is a
surrogate-based scheme, where the predictions of a weak leaner and a strong
learner are utilized within an active learning loop. The weak learner is used
to identify promising regions within the design space to explore, while the
strong learner is used to determine the exact location of the optimum within
promising regions. For each design iteration, exploration is done by randomly
selecting evaluation points within regions where the weak learner-predicted
fitness is high. The global optimum obtained by using the strong learner as a
surrogate is also evaluated to enable rapid convergence once the most promising
region has been identified. First, the performance of ActivO was compared
against five other optimizers on a cosine mixture function with 25 local optima
and one global optimum. In the second problem, the objective was to minimize
indicated specific fuel consumption of a compression-ignition internal
combustion (IC) engine while adhering to desired constraints associated with
in-cylinder pressure and emissions. Here, the efficacy of the proposed approach
is compared to that of a genetic algorithm, which is widely used within the
internal combustion engine community for engine optimization, showing that
ActivO reduces the number of function evaluations needed to reach the global
optimum, and thereby time-to-design by 80%. Furthermore, the optimization of
engine design parameters leads to savings of around 1.9% in energy consumption,
while maintaining operability and acceptable pollutant emissions.
</p>
<a href="http://arxiv.org/abs/2012.04649" target="_blank">arXiv:2012.04649</a> [<a href="http://arxiv.org/pdf/2012.04649" target="_blank">pdf</a>]

<h2>Acoustic Leak Detection in Water Networks. (arXiv:2012.06280v2 [cs.LG] UPDATED)</h2>
<h3>Robert M&#xfc;ller, Steffen Illium, Fabian Ritz, Tobias Schr&#xf6;der, Christian Platschek, J&#xf6;rg Ochs, Claudia Linnhoff-Popien</h3>
<p>In this work, we present a general procedure for acoustic leak detection in
water networks that satisfies multiple real-world constraints such as energy
efficiency and ease of deployment. Based on recordings from seven contact
microphones attached to the water supply network of a municipal suburb, we
trained several shallow and deep anomaly detection models. Inspired by how
human experts detect leaks using electronic sounding-sticks, we use these
models to repeatedly listen for leaks over a predefined decision horizon. This
way we avoid constant monitoring of the system. While we found the detection of
leaks in close proximity to be a trivial task for almost all models, neural
network based approaches achieve better results at the detection of distant
leaks.
</p>
<a href="http://arxiv.org/abs/2012.06280" target="_blank">arXiv:2012.06280</a> [<a href="http://arxiv.org/pdf/2012.06280" target="_blank">pdf</a>]

<h2>MVFNet: Multi-View Fusion Network for Efficient Video Recognition. (arXiv:2012.06977v2 [cs.CV] UPDATED)</h2>
<h3>Wenhao Wu, Dongliang He, Tianwei Lin, Fu Li, Chuang Gan, Errui Ding</h3>
<p>Conventionally, spatiotemporal modeling network and its complexity are the
two most concentrated research topics in video action recognition. Existing
state-of-the-art methods have achieved excellent accuracy regardless of the
complexity meanwhile efficient spatiotemporal modeling solutions are slightly
inferior in performance. In this paper, we attempt to acquire both efficiency
and effectiveness simultaneously. First of all, besides traditionally treating
H x W x T video frames as space-time signal (viewing from the Height-Width
spatial plane), we propose to also model video from the other two Height-Time
and Width-Time planes, to capture the dynamics of video thoroughly. Secondly,
our model is designed based on 2D CNN backbones and model complexity is well
kept in mind by design. Specifically, we introduce a novel multi-view fusion
(MVF) module to exploit video dynamics using separable convolution for
efficiency. It is a plug-and-play module and can be inserted into off-the-shelf
2D CNNs to form a simple yet effective model called MVFNet. Moreover, MVFNet
can be thought of as a generalized video modeling framework and it can
specialize to be existing methods such as C2D, SlowOnly, and TSM under
different settings. Extensive experiments are conducted on popular benchmarks
(i.e., Something-Something V1 &amp; V2, Kinetics, UCF-101, and HMDB-51) to show its
superiority. The proposed MVFNet can achieve state-of-the-art performance with
2D CNN's complexity.
</p>
<a href="http://arxiv.org/abs/2012.06977" target="_blank">arXiv:2012.06977</a> [<a href="http://arxiv.org/pdf/2012.06977" target="_blank">pdf</a>]

<h2>Copula-based synthetic data generation for machine learning emulators in weather and climate: application to a simple radiation model. (arXiv:2012.09037v2 [cs.LG] UPDATED)</h2>
<h3>David Meyer, Thomas Nagler, Robin J. Hogan</h3>
<p>Can we improve machine learning (ML) emulators with synthetic data? The use
of real data for training ML models is often the cause of major limitations.
For example, real data may be (a) only representative of a subset of situations
and domains, (b) expensive to source, (c) limited to specific individuals due
to licensing restrictions. Although the use of synthetic data is becoming
increasingly popular in computer vision, the training of ML emulators in
weather and climate still relies on the use of real data datasets. Here we
investigate whether the use of copula-based synthetically-augmented datasets
improves the prediction of ML emulators for estimating the downwelling longwave
radiation. Results show that bulk errors are cut by up to 75 % for the mean
bias error (from 0.08 to -0.02 W m$^{-2}$) and by up to 62 % (from 1.17 to 0.44
W m$^{-2}$) for the mean absolute error, thus showing potential for improving
the generalization of future ML emulators.
</p>
<a href="http://arxiv.org/abs/2012.09037" target="_blank">arXiv:2012.09037</a> [<a href="http://arxiv.org/pdf/2012.09037" target="_blank">pdf</a>]

<h2>Comparison of Classification Algorithms Towards Subject-Specific and Subject-Independent BCI. (arXiv:2012.12473v2 [cs.LG] UPDATED)</h2>
<h3>Parisa Ghane, Narges Zarnaghi Naghsh, Ulisses Braga-Neto</h3>
<p>Motor imagery brain computer interface designs are considered difficult due
to limitations in subject-specific data collection and calibration, as well as
demanding system adaptation requirements. Recently, subject-independent (SI)
designs received attention because of their possible applicability to multiple
users without prior calibration and rigorous system adaptation. SI designs are
challenging and have shown low accuracy in the literature. Two major factors in
system performance are the classification algorithm and the quality of
available data. This paper presents a comparative study of classification
performance for both SS and SI paradigms. Our results show that classification
algorithms for SS models display large variance in performance. Therefore,
distinct classification algorithms per subject may be required. SI models
display lower variance in performance but should only be used if a relatively
large sample size is available. For SI models, LDA and CART had the highest
accuracy for small and moderate sample size, respectively, whereas we
hypothesize that SVM would be superior to the other classifiers if large
training sample-size was available. Additionally, one should choose the design
approach considering the users. While the SS design sound more promising for a
specific subject, an SI approach can be more convenient for mentally or
physically challenged users.
</p>
<a href="http://arxiv.org/abs/2012.12473" target="_blank">arXiv:2012.12473</a> [<a href="http://arxiv.org/pdf/2012.12473" target="_blank">pdf</a>]

<h2>Weighting-Based Treatment Effect Estimation via Distribution Learning. (arXiv:2012.13805v2 [cs.LG] UPDATED)</h2>
<h3>Dongcheng Zhang, Kunpeng Zhang</h3>
<p>Existing weighting methods for treatment effect estimation are often built
upon the idea of propensity scores or covariate balance. They usually impose
strong assumptions on treatment assignment or outcome model to obtain unbiased
estimation, such as linearity or specific functional forms, which easily leads
to the major drawback of model mis-specification. In this paper, we aim to
alleviate these issues by developing a distribution learning-based weighting
method. We first learn the true underlying distribution of covariates
conditioned on treatment assignment, then leverage the ratio of covariates'
density in the treatment group to that of the control group as the weight for
estimating treatment effects. Specifically, we propose to approximate the
distribution of covariates in both treatment and control groups through
invertible transformations via change of variables. To demonstrate the
superiority, robustness, and generalizability of our method, we conduct
extensive experiments using synthetic and real data. From the experiment
results, we find that our method for estimating average treatment effect on
treated (ATT) with observational data outperforms several cutting-edge
weighting-only benchmarking methods, and it maintains its advantage under a
doubly-robust estimation framework that combines weighting with some advanced
outcome modeling methods.
</p>
<a href="http://arxiv.org/abs/2012.13805" target="_blank">arXiv:2012.13805</a> [<a href="http://arxiv.org/pdf/2012.13805" target="_blank">pdf</a>]

<h2>An extension of the angular synchronization problem to the heterogeneous setting. (arXiv:2012.14932v2 [stat.ML] UPDATED)</h2>
<h3>Mihai Cucuringu, Hemant Tyagi</h3>
<p>Given an undirected measurement graph $G = ([n], E)$, the classical angular
synchronization problem consists of recovering unknown angles
$\theta_1,\dots,\theta_n$ from a collection of noisy pairwise measurements of
the form $(\theta_i - \theta_j) \mod 2\pi$, for each $\{i,j\} \in E$. This
problem arises in a variety of applications, including computer vision, time
synchronization of distributed networks, and ranking from preference
relationships. In this paper, we consider a generalization to the setting where
there exist $k$ unknown groups of angles $\theta_{l,1}, \dots,\theta_{l,n}$,
for $l=1,\dots,k$. For each $ \{i,j\} \in E$, we are given noisy pairwise
measurements of the form $\theta_{\ell,i} - \theta_{\ell,j}$ for an unknown
$\ell \in \{1,2,\ldots,k\}$. This can be thought of as a natural extension of
the angular synchronization problem to the heterogeneous setting of multiple
groups of angles, where the measurement graph has an unknown edge-disjoint
decomposition $G = G_1 \cup G_2 \ldots \cup G_k$, where the $G_i$'s denote the
subgraphs of edges corresponding to each group. We propose a probabilistic
generative model for this problem, along with a spectral algorithm for which we
provide a detailed theoretical analysis in terms of robustness against both
sampling sparsity and noise. The theoretical findings are complemented by a
comprehensive set of numerical experiments, showcasing the efficacy of our
algorithm under various parameter regimes. Finally, we consider an application
of bi-synchronization to the graph realization problem, and provide along the
way an iterative graph disentangling procedure that uncovers the subgraphs
$G_i$, $i=1,\ldots,k$ which is of independent interest, as it is shown to
improve the final recovery accuracy across all the experiments considered.
</p>
<a href="http://arxiv.org/abs/2012.14932" target="_blank">arXiv:2012.14932</a> [<a href="http://arxiv.org/pdf/2012.14932" target="_blank">pdf</a>]

<h2>Kaleidoscope: An Efficient, Learnable Representation For All Structured Linear Maps. (arXiv:2012.14966v2 [cs.LG] UPDATED)</h2>
<h3>Tri Dao, Nimit S. Sohoni, Albert Gu, Matthew Eichhorn, Amit Blonder, Megan Leszczynski, Atri Rudra, Christopher R&#xe9;</h3>
<p>Modern neural network architectures use structured linear transformations,
such as low-rank matrices, sparse matrices, permutations, and the Fourier
transform, to improve inference speed and reduce memory usage compared to
general linear maps. However, choosing which of the myriad structured
transformations to use (and its associated parameterization) is a laborious
task that requires trading off speed, space, and accuracy. We consider a
different approach: we introduce a family of matrices called kaleidoscope
matrices (K-matrices) that provably capture any structured matrix with
near-optimal space (parameter) and time (arithmetic operation) complexity. We
empirically validate that K-matrices can be automatically learned within
end-to-end pipelines to replace hand-crafted procedures, in order to improve
model quality. For example, replacing channel shuffles in ShuffleNet improves
classification accuracy on ImageNet by up to 5%. K-matrices can also simplify
hand-engineered pipelines -- we replace filter bank feature computation in
speech data preprocessing with a learnable kaleidoscope layer, resulting in
only 0.4% loss in accuracy on the TIMIT speech recognition task. In addition,
K-matrices can capture latent structure in models: for a challenging permuted
image classification task, a K-matrix based representation of permutations is
able to learn the right latent structure and improves accuracy of a downstream
convolutional model by over 9%. We provide a practically efficient
implementation of our approach, and use K-matrices in a Transformer network to
attain 36% faster end-to-end inference speed on a language translation task.
</p>
<a href="http://arxiv.org/abs/2012.14966" target="_blank">arXiv:2012.14966</a> [<a href="http://arxiv.org/pdf/2012.14966" target="_blank">pdf</a>]

<h2>Joint Air Quality and Weather Prediction Based on Multi-Adversarial Spatiotemporal Networks. (arXiv:2012.15037v2 [cs.LG] UPDATED)</h2>
<h3>Jindong Han, Hao Liu, Hengshu Zhu, Hui Xiong, Dejing Dou</h3>
<p>Accurate and timely air quality and weather predictions are of great
importance to urban governance and human livelihood. Though many efforts have
been made for air quality or weather prediction, most of them simply employ one
another as feature input, which ignores the inner-connection between two
predictive tasks. On the one hand, the accurate prediction of one task can help
improve another task's performance. On the other hand, geospatially distributed
air quality and weather monitoring stations provide additional hints for
city-wide spatiotemporal dependency modeling. Inspired by the above two
insights, in this paper, we propose the Multi-adversarial spatiotemporal
recurrent Graph Neural Networks (MasterGNN) for joint air quality and weather
predictions. Specifically, we first propose a heterogeneous recurrent graph
neural network to model the spatiotemporal autocorrelation among air quality
and weather monitoring stations. Then, we develop a multi-adversarial graph
learning framework to against observation noise propagation introduced by
spatiotemporal modeling. Moreover, we present an adaptive training strategy by
formulating multi-adversarial learning as a multi-task learning problem.
Finally, extensive experiments on two real-world datasets show that MasterGNN
achieves the best performance compared with seven baselines on both air quality
and weather prediction tasks.
</p>
<a href="http://arxiv.org/abs/2012.15037" target="_blank">arXiv:2012.15037</a> [<a href="http://arxiv.org/pdf/2012.15037" target="_blank">pdf</a>]

<h2>Rethinking the Heatmap Regression for Bottom-up Human Pose Estimation. (arXiv:2012.15175v2 [cs.CV] UPDATED)</h2>
<h3>Zhengxiong Luo, Zhicheng Wang, Yan Huang, Tieniu Tan, Erjin Zhou</h3>
<p>Heatmap regression has become the most prevalent choice for nowadays human
pose estimation methods. The ground-truth heatmaps are usually constructed via
covering all skeletal keypoints by 2D gaussian kernels. The standard deviations
of these kernels are fixed. However, for bottom-up methods, which need to
handle a large variance of human scales and labeling ambiguities, the current
practice seems unreasonable. To better cope with these problems, we propose the
scale-adaptive heatmap regression (SAHR) method, which can adaptively adjust
the standard deviation for each keypoint. In this way, SAHR is more tolerant of
various human scales and labeling ambiguities. However, SAHR may aggravate the
imbalance between fore-background samples, which potentially hurts the
improvement of SAHR. Thus, we further introduce the weight-adaptive heatmap
regression (WAHR) to help balance the fore-background samples. Extensive
experiments show that SAHR together with WAHR largely improves the accuracy of
bottom-up human pose estimation. As a result, we finally outperform the
state-of-the-art model by $+1.5AP$ and achieve $72.0 AP$ on COCO test-dev2017,
which is comparable with the performances of most top-down methods.
</p>
<a href="http://arxiv.org/abs/2012.15175" target="_blank">arXiv:2012.15175</a> [<a href="http://arxiv.org/pdf/2012.15175" target="_blank">pdf</a>]

<h2>Gated Ensemble of Spatio-temporal Mixture of Experts for Multi-task Learning in Ride-hailing System. (arXiv:2012.15408v2 [cs.LG] UPDATED)</h2>
<h3>M. H. Rahman, S. M. Rifaat, S. N. Sadeek, M. Abrar, D. Wang</h3>
<p>Designing spatio-temporal forecasting models separately in a task-wise and
city-wise manner pose a burden for the expanding transportation network
companies. Therefore, a multi-task learning architecture is proposed in this
study by developing gated ensemble of spatio-temporal mixture of experts
network (GESME-Net) with convolutional recurrent neural network (CRNN),
convolutional neural network (CNN), and recurrent neural network (RNN) for
simultaneously forecasting spatio-temporal tasks in a city as well as across
different cities. Furthermore, an input agnostic feature weighting layer is
integrated with the architecture for learning joint representation in
multi-task learning and revealing the contribution of the input features
utilized in prediction. The proposed architecture is tested with data from Didi
Chuxing for: (i) simultaneously forecasting demand and supply-demand gap in
Beijing, and (ii) simultaneously forecasting demand across Chengdu and Xian. In
both scenarios, models from our proposed architecture outperformed the
single-task and multi-task deep learning benchmarks and ensemble-based machine
learning algorithms.
</p>
<a href="http://arxiv.org/abs/2012.15408" target="_blank">arXiv:2012.15408</a> [<a href="http://arxiv.org/pdf/2012.15408" target="_blank">pdf</a>]

<h2>Semantic Modeling with SUMO. (arXiv:2012.15835v2 [cs.AI] UPDATED)</h2>
<h3>Robert B. Allen</h3>
<p>We explore using the Suggested Upper Merged Ontology (SUMO) to develop a
semantic simulation. We provide two proof-of-concept demonstrations modeling
transitions in a simulated gasoline engine using a general-purpose programming
language. Rather than focusing on computationally highly intensive techniques,
we explore a less computationally intensive approach related to familiar
software engineering testing procedures.
</p>
<a href="http://arxiv.org/abs/2012.15835" target="_blank">arXiv:2012.15835</a> [<a href="http://arxiv.org/pdf/2012.15835" target="_blank">pdf</a>]

<h2>Long-Term Autonomy in Forest Environment using Self-Corrective SLAM. (arXiv:2101.00043v2 [cs.RO] UPDATED)</h2>
<h3>Paavo Nevalainen, Parisa Movahedi, Jorge Pe&#xf1;a Queralta, Tomi Westerlund, Jukka Heikkonen</h3>
<p>Vehicles with prolonged autonomous missions have to maintain environment
awareness by simultaneous localization and mapping (SLAM). Closed loop
correction is substituted by interpolation in rigid body transformation space
in order to systematically reduce the accumulated error over different scales.
The computation is divided to an edge computed lightweight SLAM and iterative
corrections in the cloud environment. Tree locations in the forest environment
are sent via a potentially limited communication bandwidths. Data from a real
forest site is used in the verification of the proposed algorithm. The
algorithm adds new iterative closest point (ICP) cases to the initial SLAM and
measures the resulting map quality by the mean of the root mean squared error
(RMSE) of individual tree clusters. Adding 4% more match cases yields the mean
RMSE 0.15 m on a large site with 180 m odometric distance.
</p>
<a href="http://arxiv.org/abs/2101.00043" target="_blank">arXiv:2101.00043</a> [<a href="http://arxiv.org/pdf/2101.00043" target="_blank">pdf</a>]

<h2>Refining activation downsampling with SoftPool. (arXiv:2101.00440v2 [cs.CV] UPDATED)</h2>
<h3>Alexandros Stergiou, Ronald Poppe, Grigorios Kalliatakis</h3>
<p>Convolutional Neural Networks (CNNs) use pooling to decrease the size of
activation maps. This process is crucial to locally achieve spatial invariance
and to increase the receptive field of subsequent convolutions. Pooling
operations should minimize the loss of information in the activation maps. At
the same time, the computation and memory overhead should be limited. To meet
these requirements, we propose SoftPool: a fast and efficient method that sums
exponentially weighted activations. Compared to a range of other pooling
methods, SoftPool retains more information in the downsampled activation maps.
More refined downsampling leads to better classification accuracy. On
ImageNet1K, for a range of popular CNN architectures, replacing the original
pooling operations with SoftPool leads to consistent accuracy improvements in
the order of 1-2%. We also test SoftPool on video datasets for action
recognition. Again, replacing only the pooling layers consistently increases
accuracy while computational load and memory remain limited. These favorable
properties make SoftPool an excellent replacement for current pooling
operations, including max-pool and average-pool
</p>
<a href="http://arxiv.org/abs/2101.00440" target="_blank">arXiv:2101.00440</a> [<a href="http://arxiv.org/pdf/2101.00440" target="_blank">pdf</a>]

<h2>Meta Variationally Intrinsic Motivated Reinforcement Learning for Decentralized Traffic Signal Control. (arXiv:2101.00746v2 [cs.LG] UPDATED)</h2>
<h3>Liwen Zhu, Peixi Peng, Zongqing Lu, Xiangqian Wang, Yonghong Tian</h3>
<p>The goal of traffic signal control is to coordinate multiple traffic signals
to improve the traffic efficiency of a district or a city. In this work, we
propose a novel Meta Variationally Intrinsic Motivated (MetaVIM) RL method, and
aim to learn the decentralized polices of each traffic signal only conditioned
on its local observation. MetaVIM makes three novel contributions. Firstly, to
make the model available to new unseen target scenarios, we formulate the
traffic signal control as a meta-learning problem over a set of related tasks.
The train scenario is divided as multiple partially observable Markov decision
process (POMDP) tasks, and each task corresponds to a traffic light. In each
task, the neighbours are regarded as an unobserved part of the state.

Secondly, we assume that the reward, transition and policy functions vary
across different tasks but share a common structure, and a learned latent
variable conditioned on the past trajectories is proposed for each task to
represent the specific information of the current task in these functions, then
is further brought into policy for automatically trade off between exploration
and exploitation to induce the RL agent to choose the reasonable action. In
addition, to make the policy learning stable, four decoders are introduced to
predict the received observations and rewards of the current agent with/without
neighbour agents' policies, and a novel intrinsic reward is designed to
encourage the received observation and reward invariant to the neighbour
agents. Empirically, extensive experiments conducted on CityFlow demonstrate
that the proposed method substantially outperforms existing methods and shows
superior generalizability.
</p>
<a href="http://arxiv.org/abs/2101.00746" target="_blank">arXiv:2101.00746</a> [<a href="http://arxiv.org/pdf/2101.00746" target="_blank">pdf</a>]

<h2>A Framework for Fast Scalable BNN Inference using Googlenet and Transfer Learning. (arXiv:2101.00793v2 [cs.CV] UPDATED)</h2>
<h3>Karthik E</h3>
<p>Efficient and accurate object detection in video and image analysis is one of
the major beneficiaries of the advancement in computer vision systems with the
help of deep learning. With the aid of deep learning, more powerful tools
evolved, which are capable to learn high-level and deeper features and thus can
overcome the existing problems in traditional architectures of object detection
algorithms. The work in this thesis aims to achieve high accuracy in object
detection with good real-time performance.

In the area of computer vision, a lot of research is going into the area of
detection and processing of visual information, by improving the existing
algorithms. The binarized neural network has shown high performance in various
vision tasks such as image classification, object detection, and semantic
segmentation. The Modified National Institute of Standards and Technology
database (MNIST), Canadian Institute for Advanced Research (CIFAR), and Street
View House Numbers (SVHN) datasets are used which is implemented using a
pre-trained convolutional neural network (CNN) that is 22 layers deep.
Supervised learning is used in the work, which classifies the particular
dataset with the proper structure of the model. In still images, to improve
accuracy, Googlenet is used. The final layer of the Googlenet is replaced with
the transfer learning to improve the accuracy of the Googlenet. At the same
time, the accuracy in moving images can be maintained by transfer learning
techniques. Hardware is the main backbone for any model to obtain faster
results with a large number of datasets. Here, Nvidia Jetson Nano is used which
is a graphics processing unit (GPU), that can handle a large number of
computations in the process of object detection. Results show that the accuracy
of objects detected by the transfer learning method is more when compared to
the existing methods.
</p>
<a href="http://arxiv.org/abs/2101.00793" target="_blank">arXiv:2101.00793</a> [<a href="http://arxiv.org/pdf/2101.00793" target="_blank">pdf</a>]

<h2>Personal Privacy Protection via Irrelevant Faces Tracking and Pixelation in Video Live Streaming. (arXiv:2101.01060v2 [cs.CV] UPDATED)</h2>
<h3>Jizhe Zhou, Chi-Man Pun</h3>
<p>To date, the privacy-protection intended pixelation tasks are still
labor-intensive and yet to be studied. With the prevailing of video live
streaming, establishing an online face pixelation mechanism during streaming is
an urgency. In this paper, we develop a new method called Face Pixelation in
Video Live Streaming (FPVLS) to generate automatic personal privacy filtering
during unconstrained streaming activities. Simply applying multi-face trackers
will encounter problems in target drifting, computing efficiency, and
over-pixelation. Therefore, for fast and accurate pixelation of irrelevant
people's faces, FPVLS is organized in a frame-to-video structure of two core
stages. On individual frames, FPVLS utilizes image-based face detection and
embedding networks to yield face vectors. In the raw trajectories generation
stage, the proposed Positioned Incremental Affinity Propagation (PIAP)
clustering algorithm leverages face vectors and positioned information to
quickly associate the same person's faces across frames. Such frame-wise
accumulated raw trajectories are likely to be intermittent and unreliable on
video level. Hence, we further introduce the trajectory refinement stage that
merges a proposal network with the two-sample test based on the Empirical
Likelihood Ratio (ELR) statistic to refine the raw trajectories. A Gaussian
filter is laid on the refined trajectories for final pixelation. On the video
live streaming dataset we collected, FPVLS obtains satisfying accuracy,
real-time efficiency, and contains the over-pixelation problems.
</p>
<a href="http://arxiv.org/abs/2101.01060" target="_blank">arXiv:2101.01060</a> [<a href="http://arxiv.org/pdf/2101.01060" target="_blank">pdf</a>]

<h2>CNN-Driven Quasiconformal Model for Large Deformation Image Registration. (arXiv:2011.00731v2 [cs.CV] CROSS LISTED)</h2>
<h3>Ho Law, Gary P. T. Choi, Ka Chun Lam, Lok Ming Lui</h3>
<p>Image registration has been widely studied over the past several decades,
with numerous applications in science, engineering and medicine. Most of the
conventional mathematical models for large deformation image registration rely
on prescribed landmarks, which usually require tedious manual labeling and are
prone to error. In recent years, there has been a surge of interest in the use
of machine learning for image registration. However, most learning-based
methods cannot ensure the bijectivity of the registration, which makes it
difficult to establish a 1-1 correspondence between the images. In this paper,
we develop a novel method for large deformation image registration by a fusion
of convolutional neural network (CNN) and quasiconformal theory. More
specifically, we propose a new fidelity term for incorporating the CNN features
in our quasiconformal energy minimization model, which enables us to obtain
meaningful registration results without prescribing any landmarks. Moreover,
unlike other learning-based methods, the bijectivity of our method is
guaranteed by quasiconformal theory. Experimental results are presented to
demonstrate the effectiveness of the proposed method. More broadly, our work
sheds light on how rigorous mathematical theories and practical machine
learning approaches can be integrated for developing computational methods with
improved performance.
</p>
<a href="http://arxiv.org/abs/2011.00731" target="_blank">arXiv:2011.00731</a> [<a href="http://arxiv.org/pdf/2011.00731" target="_blank">pdf</a>]

<h2>Minimax Statistical Learning with Wasserstein Distances. (arXiv:1705.07815v2 [cs.LG] CROSS LISTED)</h2>
<h3>Jaeho Lee, Maxim Raginsky</h3>
<p>As opposed to standard empirical risk minimization (ERM), distributionally
robust optimization aims to minimize the worst-case risk over a larger
ambiguity set containing the original empirical distribution of the training
data. In this work, we describe a minimax framework for statistical learning
with ambiguity sets given by balls in Wasserstein space. In particular, we
prove generalization bounds that involve the covering number properties of the
original ERM problem. As an illustrative example, we provide generalization
guarantees for transport-based domain adaptation problems where the Wasserstein
distance between the source and target domain distributions can be reliably
estimated from unlabeled samples.
</p>
<a href="http://arxiv.org/abs/1705.07815" target="_blank">arXiv:1705.07815</a> [<a href="http://arxiv.org/pdf/1705.07815" target="_blank">pdf</a>]

