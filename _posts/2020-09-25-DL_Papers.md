---
title: Latest Deep Learning Papers
date: 2021-03-15 21:00:01 +0800
featured-img: DL-logo
categories: [Deep_Learning]
tags: [Deep_Learning]
mathjax: Yes
---

<h1>Your arXiv Feed (140 Articles)</h1>
<h2>Cross-Domain Similarity Learning for Face Recognition in Unseen Domains. (arXiv:2103.07503v1 [cs.CV])</h2>
<h3>Masoud Faraki, Xiang Yu, Yi-Hsuan Tsai, Yumin Suh, Manmohan Chandraker</h3>
<p>Face recognition models trained under the assumption of identical training
and test distributions often suffer from poor generalization when faced with
unknown variations, such as a novel ethnicity or unpredictable individual
make-ups during test time. In this paper, we introduce a novel cross-domain
metric learning loss, which we dub Cross-Domain Triplet (CDT) loss, to improve
face recognition in unseen domains. The CDT loss encourages learning
semantically meaningful features by enforcing compact feature clusters of
identities from one domain, where the compactness is measured by underlying
similarity metrics that belong to another training domain with different
statistics. Intuitively, it discriminatively correlates explicit metrics
derived from one domain, with triplet samples from another domain in a unified
loss function to be minimized within a network, which leads to better alignment
of the training domains. The network parameters are further enforced to learn
generalized features under domain shift, in a model-agnostic learning pipeline.
Unlike the recent work of Meta Face Recognition, our method does not require
careful hard-pair sample mining and filtering strategy during training.
Extensive experiments on various face recognition benchmarks show the
superiority of our method in handling variations, compared to baseline and the
state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2103.07503" target="_blank">arXiv:2103.07503</a> [<a href="http://arxiv.org/pdf/2103.07503" target="_blank">pdf</a>]

<h2>A Continuous-Time Approach for 3D Radar-to-Camera Extrinsic Calibration. (arXiv:2103.07505v1 [cs.RO])</h2>
<h3>Emmett Wise, Juraj Per&#x161;i&#x107;, Christopher Grebe, Ivan Petrovi&#x107;, Jonathan Kelly</h3>
<p>Reliable operation in inclement weather is essential to the deployment of
safe autonomous vehicles (AVs). Robustness and reliability can be achieved by
fusing data from the standard AV sensor suite (i.e., lidars, cameras) with
weather robust sensors, such as millimetre-wavelength radar. Critically,
accurate sensor data fusion requires knowledge of the rigid-body transform
between sensor pairs, which can be determined through the process of extrinsic
calibration. A number of extrinsic calibration algorithms have been designed
for 2D (planar) radar sensors - however, recently-developed, low-cost 3D
millimetre-wavelength radars are set to displace their 2D counterparts in many
applications. In this paper, we present a continuous-time 3D radar-to-camera
extrinsic calibration algorithm that utilizes radar velocity measurements and,
unlike the majority of existing techniques, does not require specialized radar
retroreflectors to be present in the environment. We derive the observability
properties of our formulation and demonstrate the efficacy of our algorithm
through synthetic and real-world experiments.
</p>
<a href="http://arxiv.org/abs/2103.07505" target="_blank">arXiv:2103.07505</a> [<a href="http://arxiv.org/pdf/2103.07505" target="_blank">pdf</a>]

<h2>Safe Sampling-Based Air-Ground Rendezvous Algorithm for Complex Urban Environments. (arXiv:2103.07519v1 [cs.RO])</h2>
<h3>Gabriel Barsi Haberfeld, Aditya Gahlawat, Naira Hovakimyan</h3>
<p>Demand for fast and economical parcel deliveries in urban environments has
risen considerably in recent years. A framework envisions efficient last-mile
delivery in urban environments by leveraging a network of ride-sharing
vehicles, where Unmanned Aerial Systems (UASs) drop packages on said vehicles,
which then cover the majority of the distance before final aerial delivery.
Notably, we consider the problem of planning a rendezvous path for the UAS to
reach a human driver, who may choose between N possible paths and has uncertain
behavior, while meeting strict safety constraints. The long planning horizon
and safety constraints require robust heuristics that combine learning and
optimal control using Gaussian Process Regression, sampling-based optimization,
and Model Predictive Control. The resulting algorithm is computationally
efficient and shown to be effective in a variety of qualitative scenarios.
</p>
<a href="http://arxiv.org/abs/2103.07519" target="_blank">arXiv:2103.07519</a> [<a href="http://arxiv.org/pdf/2103.07519" target="_blank">pdf</a>]

<h2>Uncertainty-guided Model Generalization to Unseen Domains. (arXiv:2103.07531v1 [cs.CV])</h2>
<h3>Fengchun Qiao, Xi Peng</h3>
<p>We study a worst-case scenario in generalization: Out-of-domain
generalization from a single source. The goal is to learn a robust model from a
single source and expect it to generalize over many unknown distributions. This
challenging problem has been seldom investigated while existing solutions
suffer from various limitations. In this paper, we propose a new solution. The
key idea is to augment the source capacity in both input and label spaces,
while the augmentation is guided by uncertainty assessment. To the best of our
knowledge, this is the first work to (1) access the generalization uncertainty
from a single source and (2) leverage it to guide both input and label
augmentation for robust generalization. The model training and deployment are
effectively organized in a Bayesian meta-learning framework. We conduct
extensive comparisons and ablation study to validate our approach. The results
prove our superior performance in a wide scope of tasks including image
classification, semantic segmentation, text classification, and speech
recognition.
</p>
<a href="http://arxiv.org/abs/2103.07531" target="_blank">arXiv:2103.07531</a> [<a href="http://arxiv.org/pdf/2103.07531" target="_blank">pdf</a>]

<h2>Meta-Modeling of Assembly Contingencies and Planning for Repair. (arXiv:2103.07544v1 [cs.RO])</h2>
<h3>Priyam Parashar, Aayush Naik, Jiaming Hu, Henrik I. Christensen</h3>
<p>The World Robotics Challenge (2018 &amp; 2020) was designed to challenge teams to
design systems that are easy to adapt to new tasks and to ensure robust
operation in a semi-structured environment. We present a layered strategy to
transform missions into tasks and actions and provide a set of strategies to
address simple and complex failures. We propose a model for characterizing
failures using this model and discuss repairs. Simple failures are by far the
most common in our WRC system and we also present how we repaired them.
</p>
<a href="http://arxiv.org/abs/2103.07544" target="_blank">arXiv:2103.07544</a> [<a href="http://arxiv.org/pdf/2103.07544" target="_blank">pdf</a>]

<h2>Towards Learning Food Portion From Monocular Images With Cross-Domain Feature Adaptation. (arXiv:2103.07562v1 [cs.CV])</h2>
<h3>Zeman Shao, Shaobo Fang, Runyu Mao, Jiangpeng He, Janine Wright, Deborah Kerr, Carol Jo Boushey, Fengqing Zhu</h3>
<p>We aim to estimate food portion size, a property that is strongly related to
the presence of food object in 3D space, from single monocular images under
real life setting. Specifically, we are interested in end-to-end estimation of
food portion size, which has great potential in the field of personal health
management. Unlike image segmentation or object recognition where annotation
can be obtained through large scale crowd sourcing, it is much more challenging
to collect datasets for portion size estimation since human cannot accurately
estimate the size of an object in an arbitrary 2D image without expert
knowledge. To address such challenge, we introduce a real life food image
dataset collected from a nutrition study where the groundtruth food energy
(calorie) is provided by registered dietitians, and will be made available to
the research community. We propose a deep regression process for portion size
estimation by combining features estimated from both RGB and learned energy
distribution domains. Our estimates of food energy achieved state-of-the-art
with a MAPE of 11.47%, significantly outperforms non-expert human estimates by
27.56%.
</p>
<a href="http://arxiv.org/abs/2103.07562" target="_blank">arXiv:2103.07562</a> [<a href="http://arxiv.org/pdf/2103.07562" target="_blank">pdf</a>]

<h2>Dilated Fully Convolutional Neural Network for Depth Estimation from a Single Image. (arXiv:2103.07570v1 [cs.CV])</h2>
<h3>Binghan Li, Yindong Hua, Yifeng Liu, Mi Lu</h3>
<p>Depth prediction plays a key role in understanding a 3D scene. Several
techniques have been developed throughout the years, among which Convolutional
Neural Network has recently achieved state-of-the-art performance on estimating
depth from a single image. However, traditional CNNs suffer from the lower
resolution and information loss caused by the pooling layers. And oversized
parameters generated from fully connected layers often lead to a exploded
memory usage problem. In this paper, we present an advanced Dilated Fully
Convolutional Neural Network to address the deficiencies. Taking advantages of
the exponential expansion of the receptive field in dilated convolutions, our
model can minimize the loss of resolution. It also reduces the amount of
parameters significantly by replacing the fully connected layers with the fully
convolutional layers. We show experimentally on NYU Depth V2 datasets that the
depth prediction obtained from our model is considerably closer to ground truth
than that from traditional CNNs techniques.
</p>
<a href="http://arxiv.org/abs/2103.07570" target="_blank">arXiv:2103.07570</a> [<a href="http://arxiv.org/pdf/2103.07570" target="_blank">pdf</a>]

<h2>Revisiting ResNets: Improved Training and Scaling Strategies. (arXiv:2103.07579v1 [cs.CV])</h2>
<h3>Irwan Bello, William Fedus, Xianzhi Du, Ekin D. Cubuk, Aravind Srinivas, Tsung-Yi Lin, Jonathon Shlens, Barret Zoph</h3>
<p>Novel computer vision architectures monopolize the spotlight, but the impact
of the model architecture is often conflated with simultaneous changes to
training methodology and scaling strategies. Our work revisits the canonical
ResNet (He et al., 2015) and studies these three aspects in an effort to
disentangle them. Perhaps surprisingly, we find that training and scaling
strategies may matter more than architectural changes, and further, that the
resulting ResNets match recent state-of-the-art models. We show that the best
performing scaling strategy depends on the training regime and offer two new
scaling strategies: (1) scale model depth in regimes where overfitting can
occur (width scaling is preferable otherwise); (2) increase image resolution
more slowly than previously recommended (Tan &amp; Le, 2019). Using improved
training and scaling strategies, we design a family of ResNet architectures,
ResNet-RS, which are 1.7x - 2.7x faster than EfficientNets on TPUs, while
achieving similar accuracies on ImageNet. In a large-scale semi-supervised
learning setup, ResNet-RS achieves 86.2% top-1 ImageNet accuracy, while being
4.7x faster than EfficientNet NoisyStudent. The training techniques improve
transfer performance on a suite of downstream tasks (rivaling state-of-the-art
self-supervised algorithms) and extend to video classification on Kinetics-400.
We recommend practitioners use these simple revised ResNets as baselines for
future research.
</p>
<a href="http://arxiv.org/abs/2103.07579" target="_blank">arXiv:2103.07579</a> [<a href="http://arxiv.org/pdf/2103.07579" target="_blank">pdf</a>]

<h2>Slip-Based Autonomous ZUPT through Gaussian Process to Improve Planetary Rover Localization. (arXiv:2103.07587v1 [cs.RO])</h2>
<h3>Cagri Kilic, Nicholas Ohi, Yu Gu, Jason N. Gross</h3>
<p>The zero-velocity update (ZUPT) algorithm provides valuable state information
to maintain the inertial navigation system (INS) reliability when stationary
conditions are satisfied. Employing ZUPT along with leveraging non-holonomic
constraints can greatly benefit wheeled mobile robot dead-reckoning
localization accuracy. However, determining how often they should be employed
requires consideration to balance localization accuracy and traversal rate for
planetary rovers. To address this, we investigate when to autonomously initiate
stops to improve wheel-inertial odometry (WIO) localization performance with
ZUPT. To do this, we propose a 3D dead-reckoning approach that predicts wheel
slippage while the rover is in motion and forecasts the appropriate time to
stop without changing any rover hardware or major rover operations. We validate
with field tests that our approach is viable on different terrain types and
achieves a 3D localization accuracy of more than 97% over 650 m drives on rough
terrain.
</p>
<a href="http://arxiv.org/abs/2103.07587" target="_blank">arXiv:2103.07587</a> [<a href="http://arxiv.org/pdf/2103.07587" target="_blank">pdf</a>]

<h2>RLSS: Real-time Multi-Robot Trajectory Replanning using Linear Spatial Separations. (arXiv:2103.07588v1 [cs.RO])</h2>
<h3>Bask&#x131;n &#x15e;enba&#x15f;lar, Wolfgang H&#xf6;nig, Nora Ayanian</h3>
<p>Trajectory replanning is a critical problem for multi-robot teams navigating
dynamic environments. We present RLSS (Replanning using Linear Spatial
Separations): a real-time trajectory replanning algorithm for cooperative
multi-robot teams that uses linear spatial separations to enforce safety. Our
algorithm handles the dynamic limits of the robots explicitly, is completely
distributed, and is robust to environment changes, robot failures, and
trajectory tracking errors. It requires no communication between robots and
relies instead on local relative measurements only. We demonstrate that the
algorithm works in real-time both in simulations and in experiments using
physical robots. We compare our algorithm to a state-of-the-art online
trajectory generation algorithm based on model predictive control, and show
that our algorithm results in significantly fewer collisions in highly
constrained environments, and effectively avoids deadlocks.
</p>
<a href="http://arxiv.org/abs/2103.07588" target="_blank">arXiv:2103.07588</a> [<a href="http://arxiv.org/pdf/2103.07588" target="_blank">pdf</a>]

<h2>An Efficient Multitask Neural Network for Face Alignment, Head Pose Estimation and Face Tracking. (arXiv:2103.07615v1 [cs.CV])</h2>
<h3>Jiahao Xia, Haimin Zhang, Shiping Wen, Shuo Yang, Min Xu</h3>
<p>While convolutional neural networks (CNNs) have significantly boosted the
performance of face related algorithms, maintaining accuracy and efficiency
simultaneously in practical use remains challenging. Recent study shows that
using a cascade of hourglass modules which consist of a number of bottom-up and
top-down convolutional layers can extract facial structural information for
face alignment to improve accuracy. However, previous studies have shown that
features produced by shallow convolutional layers are highly correspond to
edges. These features could be directly used to provide the structural
information without addition cost. Motivated by this intuition, we propose an
efficient multitask face alignment, face tracking and head pose estimation
network (ATPN). Specifically, we introduce a shortcut connection between
shallow-layer features and deep-layer features to provide the structural
information for face alignment and apply the CoordConv to the last few layers
to provide coordinate information. The predicted facial landmarks enable us to
generate a cheap heatmap which contains both geometric and appearance
information for head pose estimation and it also provides attention clues for
face tracking. Moreover, the face tracking task saves us the face detection
procedure for each frame, which is significant to boost performance for
video-based tasks. The proposed framework is evaluated on four benchmark
datasets, WFLW, 300VW, WIDER Face and 300W-LP. The experimental results show
that the ATPN achieves improved performance compared to previous
state-of-the-art methods while having less number of parameters and FLOPS.
</p>
<a href="http://arxiv.org/abs/2103.07615" target="_blank">arXiv:2103.07615</a> [<a href="http://arxiv.org/pdf/2103.07615" target="_blank">pdf</a>]

<h2>Potential Escalator-related Injury Identification and Prevention Based on Multi-module Integrated System for Public Health. (arXiv:2103.07620v1 [cs.CV])</h2>
<h3>Zeyu Jiao, Huan Lei, Hengshan Zong, Yingjie Cai, Zhenyu Zhong</h3>
<p>Escalator-related injuries threaten public health with the widespread use of
escalators. The existing studies tend to focus on after-the-fact statistics,
reflecting on the original design and use of defects to reduce the impact of
escalator-related injuries, but few attention has been paid to ongoing and
impending injuries. In this study, a multi-module escalator safety monitoring
system based on computer vision is designed and proposed to simultaneously
monitor and deal with three major injury triggers, including losing balance,
not holding on to handrails and carrying large items. The escalator
identification module is utilized to determine the escalator region, namely the
region of interest. The passenger monitoring module is leveraged to estimate
the passengers' pose to recognize unsafe behaviors on the escalator. The
dangerous object detection module detects large items that may enter the
escalator and raises alarms. The processing results of the above three modules
are summarized in the safety assessment module as the basis for the intelligent
decision of the system. The experimental results demonstrate that the proposed
system has good performance and great application potential.
</p>
<a href="http://arxiv.org/abs/2103.07620" target="_blank">arXiv:2103.07620</a> [<a href="http://arxiv.org/pdf/2103.07620" target="_blank">pdf</a>]

<h2>AIR4Children: Artificial Intelligence and Robotics for Children. (arXiv:2103.07637v1 [cs.RO])</h2>
<h3>Rocio Montenegro, Elva Corona, Donato Badillo-Perez, Angel Mandujano, Leticia Vazquez, Dago Cruz, Miguel Xochicale</h3>
<p>We introduce AIR4Children, Artificial Intelligence for Children, as a way to
(a) tackle aspects for inclusion, accessibility, transparency, equity, fairness
and participation and (b) to create affordable child-centred materials in AI
and Robotics (AIR). We present current challenges and opportunities for a
child-centred approaches for AIR. Similarly, we touch on open-sourced software
and hardware technologies to make a more inclusive, affordable and fair
participation of children in areas of AIR. Then, we describe the avenues that
AIR4Children can take with the development of open-sourced software and
hardware based on our initial pilots and experiences. Similarly, we propose to
follow the philosophy of Montessori education to help children to not only
develop computational thinking but also to internalise new concepts and
learning skills through activities of movement and repetition. Finally, we
conclude with the opportunities of our work and mainly we pose the future work
of putting in practice what is proposed here to evaluate the potential impact
on AIR to children, instructors, parents and their community.
</p>
<a href="http://arxiv.org/abs/2103.07637" target="_blank">arXiv:2103.07637</a> [<a href="http://arxiv.org/pdf/2103.07637" target="_blank">pdf</a>]

<h2>Generating Unrestricted Adversarial Examples via Three Parameters. (arXiv:2103.07640v1 [cs.CV])</h2>
<h3>Hanieh Naderi, Leili Goli, Shohreh Kasaei</h3>
<p>Deep neural networks have been shown to be vulnerable to adversarial examples
deliberately constructed to misclassify victim models. As most adversarial
examples have restricted their perturbations to $L_{p}$-norm, existing defense
methods have focused on these types of perturbations and less attention has
been paid to unrestricted adversarial examples; which can create more realistic
attacks, able to deceive models without affecting human predictions. To address
this problem, the proposed adversarial attack generates an unrestricted
adversarial example with a limited number of parameters. The attack selects
three points on the input image and based on their locations transforms the
image into an adversarial example. By limiting the range of movement and
location of these three points and using a discriminatory network, the proposed
unrestricted adversarial example preserves the image appearance. Experimental
results show that the proposed adversarial examples obtain an average success
rate of 93.5% in terms of human evaluation on the MNIST and SVHN datasets. It
also reduces the model accuracy by an average of 73% on six datasets MNIST,
FMNIST, SVHN, CIFAR10, CIFAR100, and ImageNet. It should be noted that, in the
case of attacks, lower accuracy in the victim model denotes a more successful
attack. The adversarial train of the attack also improves model robustness
against a randomly transformed image.
</p>
<a href="http://arxiv.org/abs/2103.07640" target="_blank">arXiv:2103.07640</a> [<a href="http://arxiv.org/pdf/2103.07640" target="_blank">pdf</a>]

<h2>PhotoApp: Photorealistic Appearance Editing of Head Portraits. (arXiv:2103.07658v1 [cs.CV])</h2>
<h3>Mallikarjun B R, Ayush Tewari, Abdallah Dib, Tim Weyrich, Bernd Bickel, Hans-Peter Seidel, Hanspeter Pfister, Wojciech Matusik, Louis Chevallier, Mohamed Elgharib, Christian Theobalt</h3>
<p>Photorealistic editing of portraits is a challenging task as humans are very
sensitive to inconsistencies in faces. We present an approach for high-quality
intuitive editing of the camera viewpoint and scene illumination in a portrait
image. This requires our method to capture and control the full reflectance
field of the person in the image. Most editing approaches rely on supervised
learning using training data captured with setups such as light and camera
stages. Such datasets are expensive to acquire, not readily available and do
not capture all the rich variations of in-the-wild portrait images. In
addition, most supervised approaches only focus on relighting, and do not allow
camera viewpoint editing. Thus, they only capture and control a subset of the
reflectance field. Recently, portrait editing has been demonstrated by
operating in the generative model space of StyleGAN. While such approaches do
not require direct supervision, there is a significant loss of quality when
compared to the supervised approaches. In this paper, we present a method which
learns from limited supervised training data. The training images only include
people in a fixed neutral expression with eyes closed, without much hair or
background variations. Each person is captured under 150 one-light-at-a-time
conditions and under 8 camera poses. Instead of training directly in the image
space, we design a supervised problem which learns transformations in the
latent space of StyleGAN. This combines the best of supervised learning and
generative adversarial modeling. We show that the StyleGAN prior allows for
generalisation to different expressions, hairstyles and backgrounds. This
produces high-quality photorealistic results for in-the-wild images and
significantly outperforms existing methods. Our approach can edit the
illumination and pose simultaneously, and runs at interactive rates.
</p>
<a href="http://arxiv.org/abs/2103.07658" target="_blank">arXiv:2103.07658</a> [<a href="http://arxiv.org/pdf/2103.07658" target="_blank">pdf</a>]

<h2>uTHCD: A New Benchmarking for Tamil Handwritten OCR. (arXiv:2103.07676v1 [cs.CV])</h2>
<h3>Noushath Shaffi, Faizal Hajamohideen</h3>
<p>Handwritten character recognition is a challenging research in the field of
document image analysis over many decades due to numerous reasons such as large
writing styles variation, inherent noise in data, expansive applications it
offers, non-availability of benchmark databases etc. There has been
considerable work reported in literature about creation of the database for
several Indic scripts but the Tamil script is still in its infancy as it has
been reported only in one database [5]. In this paper, we present the work done
in the creation of an exhaustive and large unconstrained Tamil Handwritten
Character Database (uTHCD). Database consists of around 91000 samples with
nearly 600 samples in each of 156 classes. The database is a unified collection
of both online and offline samples. Offline samples were collected by asking
volunteers to write samples on a form inside a specified grid. For online
samples, we made the volunteers write in a similar grid using a digital writing
pad. The samples collected encompass a vast variety of writing styles, inherent
distortions arising from offline scanning process viz stroke discontinuity,
variable thickness of stroke, distortion etc. Algorithms which are resilient to
such data can be practically deployed for real time applications. The samples
were generated from around 650 native Tamil volunteers including school going
kids, homemakers, university students and faculty. The isolated character
database will be made publicly available as raw images and Hierarchical Data
File (HDF) compressed file. With this database, we expect to set a new
benchmark in Tamil handwritten character recognition and serve as a launchpad
for many avenues in document image analysis domain. Paper also presents an
ideal experimental set-up using the database on convolutional neural networks
(CNN) with a baseline accuracy of 88% on test data.
</p>
<a href="http://arxiv.org/abs/2103.07676" target="_blank">arXiv:2103.07676</a> [<a href="http://arxiv.org/pdf/2103.07676" target="_blank">pdf</a>]

<h2>NeuralHumanFVV: Real-Time Neural Volumetric Human Performance Rendering using RGB Cameras. (arXiv:2103.07700v1 [cs.CV])</h2>
<h3>Xin Suo, Yuheng Jiang, Pei Lin, Yingliang Zhang, Kaiwen Guo, Minye Wu, Lan Xu</h3>
<p>4D reconstruction and rendering of human activities is critical for immersive
VR/AR experience.Recent advances still fail to recover fine geometry and
texture results with the level of detail present in the input images from
sparse multi-view RGB cameras. In this paper, we propose NeuralHumanFVV, a
real-time neural human performance capture and rendering system to generate
both high-quality geometry and photo-realistic texture of human activities in
arbitrary novel views. We propose a neural geometry generation scheme with a
hierarchical sampling strategy for real-time implicit geometry inference, as
well as a novel neural blending scheme to generate high resolution (e.g., 1k)
and photo-realistic texture results in the novel views. Furthermore, we adopt
neural normal blending to enhance geometry details and formulate our neural
geometry and texture rendering into a multi-task learning framework. Extensive
experiments demonstrate the effectiveness of our approach to achieve
high-quality geometry and photo-realistic free view-point reconstruction for
challenging human performances.
</p>
<a href="http://arxiv.org/abs/2103.07700" target="_blank">arXiv:2103.07700</a> [<a href="http://arxiv.org/pdf/2103.07700" target="_blank">pdf</a>]

<h2>Mean Field Behaviour of Collaborative Multi-Agent Foragers. (arXiv:2103.07714v1 [cs.RO])</h2>
<h3>Daniel Jarne Ornia, Pedro J Zufiria, Manuel Mazo Jr</h3>
<p>Collaborative multi-agent robotic systems where agents coordinate by
modifying a shared environment often result in undesired dynamical couplings
that complicate the analysis and experiments when solving a specific problem or
task. Simultaneously, biologically-inspired robotics rely on simplifying agents
and increasing their number to obtain more efficient solutions to such
problems, drawing similarities with natural processes. In this work we focus on
the problem of a biologically-inspired multi-agent system solving collaborative
foraging. We show how mean field techniques can be used to re-formulate such a
stochastic multi-agent problem into a deterministic au- tonomous system. This
de-couples agent dynamics, enabling the computation of limit behaviours and the
analysis of optimality guarantees. Furthermore, we analyse how having finite
number of agents affects the performance when compared to the mean field limit
and we discuss the implications of such limit approximations in this
multi-agent system, which have impact on more general collaborative stochastic
problems.
</p>
<a href="http://arxiv.org/abs/2103.07714" target="_blank">arXiv:2103.07714</a> [<a href="http://arxiv.org/pdf/2103.07714" target="_blank">pdf</a>]

<h2>Personalized Human-Swarm Interaction through Hand Motion. (arXiv:2103.07731v1 [cs.RO])</h2>
<h3>Matteo Macchini, Ludovic De Matte&#xef;s, Fabrizio Schiano, Dario Floreano</h3>
<p>The control of collective robotic systems, such as drone swarms, is often
delegated to autonomous navigation algorithms due to their high dimensionality.
However, like other robotic entities, drone swarms can still benefit from being
teleoperated by human operators, whose perception and decision-making
capabilities are still out of the reach of autonomous systems. Drone swarm
teleoperation is only at its dawn, and a standard human-swarm interface (HRI)
is missing to date. In this study, we analyzed the spontaneous interaction
strategies of naive users with a swarm of drones. We implemented a
machine-learning algorithm to define a personalized Body-Machine Interface
(BoMI) based only on a short calibration procedure. During this procedure, the
human operator is asked to move spontaneously as if they were in control of a
simulated drone swarm. We assessed that hands are the most commonly adopted
body segment, and thus we chose a LEAP Motion controller to track them to let
the users control the aerial drone swarm. This choice makes our interface
portable since it does not rely on a centralized system for tracking the human
body. We validated our algorithm to define personalized HRIs for a set of
participants in a realistic simulated environment, showing promising results in
performance and user experience. Our method leaves unprecedented freedom to the
user to choose between position and velocity control only based on their body
motion preferences.
</p>
<a href="http://arxiv.org/abs/2103.07731" target="_blank">arXiv:2103.07731</a> [<a href="http://arxiv.org/pdf/2103.07731" target="_blank">pdf</a>]

<h2>Error-Aware Policy Learning: Zero-Shot Generalization in Partially Observable Dynamic Environments. (arXiv:2103.07732v1 [cs.RO])</h2>
<h3>Visak Kumar, Sehoon Ha, C. Karen Liu</h3>
<p>Simulation provides a safe and efficient way to generate useful data for
learning complex robotic tasks. However, matching simulation and real-world
dynamics can be quite challenging, especially for systems that have a large
number of unobserved or unmeasurable parameters, which may lie in the robot
dynamics itself or in the environment with which the robot interacts. We
introduce a novel approach to tackle such a sim-to-real problem by developing
policies capable of adapting to new environments, in a zero-shot manner. Key to
our approach is an error-aware policy (EAP) that is explicitly made aware of
the effect of unobservable factors during training. An EAP takes as input the
predicted future state error in the target environment, which is provided by an
error-prediction function, simultaneously trained with the EAP. We validate our
approach on an assistive walking device trained to help the human user recover
from external pushes. We show that a trained EAP for a hip-torque assistive
device can be transferred to different human agents with unseen biomechanical
characteristics. In addition, we show that our method can be applied to other
standard RL control tasks.
</p>
<a href="http://arxiv.org/abs/2103.07732" target="_blank">arXiv:2103.07732</a> [<a href="http://arxiv.org/pdf/2103.07732" target="_blank">pdf</a>]

<h2>ReDet: A Rotation-equivariant Detector for Aerial Object Detection. (arXiv:2103.07733v1 [cs.CV])</h2>
<h3>Jiaming Han, Jian Ding, Nan Xue, Gui-Song Xia</h3>
<p>Recently, object detection in aerial images has gained much attention in
computer vision. Different from objects in natural images, aerial objects are
often distributed with arbitrary orientation. Therefore, the detector requires
more parameters to encode the orientation information, which are often highly
redundant and inefficient. Moreover, as ordinary CNNs do not explicitly model
the orientation variation, large amounts of rotation augmented data is needed
to train an accurate object detector. In this paper, we propose a
Rotation-equivariant Detector (ReDet) to address these issues, which explicitly
encodes rotation equivariance and rotation invariance. More precisely, we
incorporate rotation-equivariant networks into the detector to extract
rotation-equivariant features, which can accurately predict the orientation and
lead to a huge reduction of model size. Based on the rotation-equivariant
features, we also present Rotation-invariant RoI Align (RiRoI Align), which
adaptively extracts rotation-invariant features from equivariant features
according to the orientation of RoI. Extensive experiments on several
challenging aerial image datasets DOTA-v1.0, DOTA-v1.5 and HRSC2016, show that
our method can achieve state-of-the-art performance on the task of aerial
object detection. Compared with previous best results, our ReDet gains 1.2, 3.5
and 2.6 mAP on DOTA-v1.0, DOTA-v1.5 and HRSC2016 respectively while reducing
the number of parameters by 60\% (313 Mb vs. 121 Mb). The code is available at:
\url{https://github.com/csuhan/ReDet}.
</p>
<a href="http://arxiv.org/abs/2103.07733" target="_blank">arXiv:2103.07733</a> [<a href="http://arxiv.org/pdf/2103.07733" target="_blank">pdf</a>]

<h2>Reconsidering Representation Alignment for Multi-view Clustering. (arXiv:2103.07738v1 [cs.CV])</h2>
<h3>Daniel J. Trosten, Sigurd L&#xf8;kse, Robert Jenssen, Michael Kampffmeyer</h3>
<p>Aligning distributions of view representations is a core component of today's
state of the art models for deep multi-view clustering. However, we identify
several drawbacks with na\"ively aligning representation distributions. We
demonstrate that these drawbacks both lead to less separable clusters in the
representation space, and inhibit the model's ability to prioritize views.
Based on these observations, we develop a simple baseline model for deep
multi-view clustering. Our baseline model avoids representation alignment
altogether, while performing similar to, or better than, the current state of
the art. We also expand our baseline model by adding a contrastive learning
component. This introduces a selective alignment procedure that preserves the
model's ability to prioritize views. Our experiments show that the contrastive
learning component enhances the baseline model, improving on the current state
of the art by a large margin on several datasets.
</p>
<a href="http://arxiv.org/abs/2103.07738" target="_blank">arXiv:2103.07738</a> [<a href="http://arxiv.org/pdf/2103.07738" target="_blank">pdf</a>]

<h2>Unsupervised Image Transformation Learning via Generative Adversarial Networks. (arXiv:2103.07751v1 [cs.CV])</h2>
<h3>Kaiwen Zha, Yujun Shen, Bolei Zhou</h3>
<p>In this work, we study the image transformation problem by learning the
underlying transformations from a collection of images using Generative
Adversarial Networks (GANs). Specifically, we propose an unsupervised learning
framework, termed as TrGAN, to project images onto a transformation space that
is shared by the generator and the discriminator. Any two points in this
projected space define a transformation that can guide the image generation
process, leading to continuous semantic change. By projecting a pair of images
onto the transformation space, we are able to adequately extract the semantic
variation between them and further apply the extracted semantic to facilitating
image editing, including not only transferring image styles (e.g., changing day
to night) but also manipulating image contents (e.g., adding clouds in the
sky). Code and models are available at https://genforce.github.io/trgan.
</p>
<a href="http://arxiv.org/abs/2103.07751" target="_blank">arXiv:2103.07751</a> [<a href="http://arxiv.org/pdf/2103.07751" target="_blank">pdf</a>]

<h2>Image Segmentation Methods for Non-destructive testing Applications. (arXiv:2103.07754v1 [cs.CV])</h2>
<h3>EL-Hachemi Guerrout, Ramdane Mahiou, Randa Boukabene, Assia Ouali</h3>
<p>In this paper, we present new image segmentation methods based on hidden
Markov random fields (HMRFs) and cuckoo search (CS) variants. HMRFs model the
segmentation problem as a minimization of an energy function. CS algorithm is
one of the recent powerful optimization techniques. Therefore, five variants of
the CS algorithm are used to compute a solution. Through tests, we conduct a
study to choose the CS variant with parameters that give good results
(execution time and quality of segmentation). CS variants are evaluated and
compared with non-destructive testing (NDT) images using a misclassification
error (ME) criterion.
</p>
<a href="http://arxiv.org/abs/2103.07754" target="_blank">arXiv:2103.07754</a> [<a href="http://arxiv.org/pdf/2103.07754" target="_blank">pdf</a>]

<h2>Online Learning of Objects through Curiosity-Driven Active Learning. (arXiv:2103.07758v1 [cs.RO])</h2>
<h3>Ali Ayub, Alan R. Wagner</h3>
<p>Children learn continually by asking questions about the concepts they are
most curious about. With robots becoming an integral part of our society, they
must also learn unknown concepts continually by asking humans questions. This
paper presents a novel framework for curiosity-driven online learning of
objects. The paper utilizes a recent state-of-the-art approach for continual
learning and adapts it for online learning of objects. The paper further
develops a self-supervised technique to find most of the uncertain objects in
an environment by utilizing an internal representation of previously learned
classes. We test our approach on a benchmark dataset for continual learning on
robots. Our results show that our curiosity-driven online learning approach
beats random sampling and softmax-based uncertainty sampling in terms of
classification accuracy and the total number of classes learned.
</p>
<a href="http://arxiv.org/abs/2103.07758" target="_blank">arXiv:2103.07758</a> [<a href="http://arxiv.org/pdf/2103.07758" target="_blank">pdf</a>]

<h2>Multi-Object Tracking using Poisson Multi-Bernoulli Mixture Filtering for Autonomous Vehicles. (arXiv:2103.07783v1 [cs.RO])</h2>
<h3>Su Pang, Hayder Radha</h3>
<p>The ability of an autonomous vehicle to perform 3D tracking is essential for
safe planing and navigation in cluttered environments. The main challenges for
multi-object tracking (MOT) in autonomous driving applications reside in the
inherent uncertainties regarding the number of objects, when and where the
objects may appear and disappear, and uncertainties regarding objects' states.
Random finite set (RFS) based approaches can naturally model these
uncertainties accurately and elegantly, and they have been widely used in
radar-based tracking applications. In this work, we developed an RFS-based MOT
framework for 3D LiDAR data. In partiuclar, we propose a Poisson
multi-Bernoulli mixture (PMBM) filter to solve the amodal MOT problem for
autonomous driving applications. To the best of our knowledge, this represents
a first attempt for employing an RFS-based approach in conjunction with 3D
LiDAR data for MOT applications with comprehensive validation using challenging
datasets made available by industry leaders. The superior experimental results
of our PMBM tracker on public Waymo and Argoverse datasets clearly illustrate
that an RFS-based tracker outperforms many state-of-the-art deep learning-based
and Kalman filter-based methods, and consequently, these results indicate a
great potential for further exploration of RFS-based frameworks for 3D MOT
applications.
</p>
<a href="http://arxiv.org/abs/2103.07783" target="_blank">arXiv:2103.07783</a> [<a href="http://arxiv.org/pdf/2103.07783" target="_blank">pdf</a>]

<h2>A Few-Shot Learning Approach for Accelerated MRI via Fusion of Data-Driven and Subject-Driven Priors. (arXiv:2103.07790v1 [cs.CV])</h2>
<h3>Salman Ul Hassan Dar, Mahmut Yurt, Tolga &#xc7;ukur</h3>
<p>Deep neural networks (DNNs) have recently found emerging use in accelerated
MRI reconstruction. DNNs typically learn data-driven priors from large datasets
constituting pairs of undersampled and fully-sampled acquisitions. Acquiring
such large datasets, however, might be impractical. To mitigate this
limitation, we propose a few-shot learning approach for accelerated MRI that
merges subject-driven priors obtained via physical signal models with
data-driven priors obtained from a few training samples. Demonstrations on
brain MR images from the NYU fastMRI dataset indicate that the proposed
approach requires just a few samples to outperform traditional parallel imaging
and DNN algorithms.
</p>
<a href="http://arxiv.org/abs/2103.07790" target="_blank">arXiv:2103.07790</a> [<a href="http://arxiv.org/pdf/2103.07790" target="_blank">pdf</a>]

<h2>ORStereo: Occlusion-Aware Recurrent Stereo Matching for 4K-Resolution Images. (arXiv:2103.07798v1 [cs.CV])</h2>
<h3>Yaoyu Hu, Wenshan Wang, Huai Yu, Weikun Zhen, Sebastian Scherer</h3>
<p>Stereo reconstruction models trained on small images do not generalize well
to high-resolution data. Training a model on high-resolution image size faces
difficulties of data availability and is often infeasible due to limited
computing resources. In this work, we present the Occlusion-aware Recurrent
binocular Stereo matching (ORStereo), which deals with these issues by only
training on available low disparity range stereo images. ORStereo generalizes
to unseen high-resolution images with large disparity ranges by formulating the
task as residual updates and refinements of an initial prediction. ORStereo is
trained on images with disparity ranges limited to 256 pixels, yet it can
operate 4K-resolution input with over 1000 disparities using limited GPU
memory. We test the model's capability on both synthetic and real-world
high-resolution images. Experimental results demonstrate that ORStereo achieves
comparable performance on 4K-resolution images compared to state-of-the-art
methods trained on large disparity ranges. Compared to other methods that are
only trained on low-resolution images, our method is 70% more accurate on
4K-resolution images.
</p>
<a href="http://arxiv.org/abs/2103.07798" target="_blank">arXiv:2103.07798</a> [<a href="http://arxiv.org/pdf/2103.07798" target="_blank">pdf</a>]

<h2>Radar Camera Fusion via Representation Learning in Autonomous Driving. (arXiv:2103.07825v1 [cs.CV])</h2>
<h3>Xu Dong, Binnan Zhuang, Yunxiang Mao, Langechuan Liu</h3>
<p>Radars and cameras are mature, cost-effective, and robust sensors and have
been widely used in the perception stack of mass-produced autonomous driving
systems. Due to their complementary properties, outputs from radar detection
(radar pins) and camera perception (2D bounding boxes) are usually fused to
generate the best perception results. The key to successful radar-camera fusion
is accurate data association. The challenges in radar-camera association can be
attributed to the complexity of driving scenes, the noisy and sparse nature of
radar measurements, and the depth ambiguity from 2D bounding boxes. Traditional
rule-based association methods are susceptible to performance degradation in
challenging scenarios and failure in corner cases. In this study, we propose to
address rad-cam association via deep representation learning, to explore
feature-level interaction and global reasoning. Concretely, we design a loss
sampling mechanism and an innovative ordinal loss to overcome the difficulty of
imperfect labeling and to enforce critical human reasoning. Despite being
trained with noisy labels generated by a rule-based algorithm, our proposed
method achieves a performance of 92.2% F1 score, which is 11.6% higher than the
rule-based teacher. Moreover, this data-driven method also lends itself to
continuous improvement via corner case mining.
</p>
<a href="http://arxiv.org/abs/2103.07825" target="_blank">arXiv:2103.07825</a> [<a href="http://arxiv.org/pdf/2103.07825" target="_blank">pdf</a>]

<h2>Cycle4Completion: Unpaired Point Cloud Completion using Cycle Transformation with Missing Region Coding. (arXiv:2103.07838v1 [cs.CV])</h2>
<h3>Xin Wen, Zhizhong Han, Yan-Pei Cao, Pengfei Wan, Wen Zheng, Yu-Shen Liu</h3>
<p>In this paper, we present a novel unpaired point cloud completion network,
named Cycle4Completion, to infer the complete geometries from a partial 3D
object. Previous unpaired completion methods merely focus on the learning of
geometric correspondence from incomplete shapes to complete shapes, and ignore
the learning in the reverse direction, which makes them suffer from low
completion accuracy due to the limited 3D shape understanding ability. To
address this problem, we propose two simultaneous cycle transformations between
the latent spaces of complete shapes and incomplete ones. The insight of cycle
transformation is to promote networks to understand 3D shapes by learning to
generate complete or incomplete shapes from their complementary ones.
Specifically, the first cycle transforms shapes from incomplete domain to
complete domain, and then projects them back to the incomplete domain. This
process learns the geometric characteristic of complete shapes, and maintains
the shape consistency between the complete prediction and the incomplete input.
Similarly, the inverse cycle transformation starts from complete domain to
incomplete domain, and goes back to complete domain to learn the characteristic
of incomplete shapes. We provide a comprehensive evaluation in experiments,
which shows that our model with the learned bidirectional geometry
correspondence outperforms state-of-the-art unpaired completion methods.
</p>
<a href="http://arxiv.org/abs/2103.07838" target="_blank">arXiv:2103.07838</a> [<a href="http://arxiv.org/pdf/2103.07838" target="_blank">pdf</a>]

<h2>Three Steps to Multimodal Trajectory Prediction: Modality Clustering, Classification and Synthesis. (arXiv:2103.07854v1 [cs.CV])</h2>
<h3>Jianhua Sun, Yuxuan Li, Hao-Shu Fang, Cewu Lu</h3>
<p>Multimodal prediction results are essential for trajectory forecasting task
as there is no single correct answer for the future. Previous frameworks can be
divided into three categories: regression, generation and classification
frameworks. However, these frameworks have weaknesses in different aspects so
that they cannot model the multimodal prediction task comprehensively. In this
paper, we present a novel insight along with a brand-new prediction framework
by formulating multimodal prediction into three steps: modality clustering,
classification and synthesis, and address the shortcomings of earlier
frameworks. Exhaustive experiments on popular benchmarks have demonstrated that
our proposed method surpasses state-of-the-art works even without introducing
social and map information. Specifically, we achieve 19.2% and 20.8%
improvement on ADE and FDE respectively on ETH/UCY dataset. Our code will be
made publicly available.
</p>
<a href="http://arxiv.org/abs/2103.07854" target="_blank">arXiv:2103.07854</a> [<a href="http://arxiv.org/pdf/2103.07854" target="_blank">pdf</a>]

<h2>Learning a Proposal Classifier for Multiple Object Tracking. (arXiv:2103.07889v1 [cs.CV])</h2>
<h3>Peng Dai, Renliang Weng, Wongun Choi, Changshui Zhang, Zhangping He, Wei Ding</h3>
<p>The recent trend in multiple object tracking (MOT) is heading towards
leveraging deep learning to boost the tracking performance. However, it is not
trivial to solve the data-association problem in an end-to-end fashion. In this
paper, we propose a novel proposal-based learnable framework, which models MOT
as a proposal generation, proposal scoring and trajectory inference paradigm on
an affinity graph. This framework is similar to the two-stage object detector
Faster RCNN, and can solve the MOT problem in a data-driven way. For proposal
generation, we propose an iterative graph clustering method to reduce the
computational cost while maintaining the quality of the generated proposals.
For proposal scoring, we deploy a trainable graph-convolutional-network (GCN)
to learn the structural patterns of the generated proposals and rank them
according to the estimated quality scores. For trajectory inference, a simple
deoverlapping strategy is adopted to generate tracking output while complying
with the constraints that no detection can be assigned to more than one track.
We experimentally demonstrate that the proposed method achieves a clear
performance improvement in both MOTA and IDF1 with respect to previous
state-of-the-art on two public benchmarks. Our code is available at
\url{https://github.com/daip13/LPC_MOT.git}.
</p>
<a href="http://arxiv.org/abs/2103.07889" target="_blank">arXiv:2103.07889</a> [<a href="http://arxiv.org/pdf/2103.07889" target="_blank">pdf</a>]

<h2>DivCo: Diverse Conditional Image Synthesis via Contrastive Generative Adversarial Network. (arXiv:2103.07893v1 [cs.CV])</h2>
<h3>Rui Liu, Yixiao Ge, Ching Lam Choi, Xiaogang Wang, Hongsheng Li</h3>
<p>Conditional generative adversarial networks (cGANs) target at synthesizing
diverse images given the input conditions and latent codes, but unfortunately,
they usually suffer from the issue of mode collapse. To solve this issue,
previous works mainly focused on encouraging the correlation between the latent
codes and their generated images, while ignoring the relations between images
generated from various latent codes. The recent MSGAN tried to encourage the
diversity of the generated image but only considers "negative" relations
between the image pairs. In this paper, we propose a novel DivCo framework to
properly constrain both "positive" and "negative" relations between the
generated images specified in the latent space. To the best of our knowledge,
this is the first attempt to use contrastive learning for diverse conditional
image synthesis. A novel latent-augmented contrastive loss is introduced, which
encourages images generated from adjacent latent codes to be similar and those
generated from distinct latent codes to be dissimilar. The proposed
latent-augmented contrastive loss is well compatible with various cGAN
architectures. Extensive experiments demonstrate that the proposed DivCo can
produce more diverse images than state-of-the-art methods without sacrificing
visual quality in multiple unpaired and paired image generation tasks.
</p>
<a href="http://arxiv.org/abs/2103.07893" target="_blank">arXiv:2103.07893</a> [<a href="http://arxiv.org/pdf/2103.07893" target="_blank">pdf</a>]

<h2>Refer-it-in-RGBD: A Bottom-up Approach for 3D Visual Grounding in RGBD Images. (arXiv:2103.07894v1 [cs.CV])</h2>
<h3>Haolin Liu, Anran Lin, Xiaoguang Han, Lei Yang, Yizhou Yu, Shuguang Cui</h3>
<p>Grounding referring expressions in RGBD image has been an emerging field. We
present a novel task of 3D visual grounding in single-view RGBD image where the
referred objects are often only partially scanned due to occlusion. In contrast
to previous works that directly generate object proposals for grounding in the
3D scenes, we propose a bottom-up approach to gradually aggregate context-aware
information, effectively addressing the challenge posed by the partial
geometry. Our approach first fuses the language and the visual features at the
bottom level to generate a heatmap that coarsely localizes the relevant regions
in the RGBD image. Then our approach conducts an adaptive feature learning
based on the heatmap and performs the object-level matching with another
visio-linguistic fusion to finally ground the referred object. We evaluate the
proposed method by comparing to the state-of-the-art methods on both the RGBD
images extracted from the ScanRefer dataset and our newly collected SUNRefer
dataset. Experiments show that our method outperforms the previous methods by a
large margin (by 11.2% and 15.6% Acc@0.5) on both datasets.
</p>
<a href="http://arxiv.org/abs/2103.07894" target="_blank">arXiv:2103.07894</a> [<a href="http://arxiv.org/pdf/2103.07894" target="_blank">pdf</a>]

<h2>Principled Ultrasound Data Augmentation for Classification of Standard Planes. (arXiv:2103.07895v1 [cs.CV])</h2>
<h3>Lok Hin Lee, Yuan Gao, J. Alison Noble</h3>
<p>Deep learning models with large learning capacities often overfit to medical
imaging datasets. This is because training sets are often relatively small due
to the significant time and financial costs incurred in medical data
acquisition and labelling. Data augmentation is therefore often used to expand
the availability of training data and to increase generalization. However,
augmentation strategies are often chosen on an ad-hoc basis without
justification. In this paper, we present an augmentation policy search method
with the goal of improving model classification performance. We include in the
augmentation policy search additional transformations that are often used in
medical image analysis and evaluate their performance. In addition, we extend
the augmentation policy search to include non-linear mixed-example data
augmentation strategies. Using these learned policies, we show that principled
data augmentation for medical image model training can lead to significant
improvements in ultrasound standard plane detection, with an an average
F1-score improvement of 7.0% overall over naive data augmentation strategies in
ultrasound fetal standard plane classification. We find that the learned
representations of ultrasound images are better clustered and defined with
optimized data augmentation.
</p>
<a href="http://arxiv.org/abs/2103.07895" target="_blank">arXiv:2103.07895</a> [<a href="http://arxiv.org/pdf/2103.07895" target="_blank">pdf</a>]

<h2>GVINS: Tightly Coupled GNSS-Visual-Inertial for Smooth and Consistent State Estimation. (arXiv:2103.07899v1 [cs.RO])</h2>
<h3>Shaozu Cao, Xiuyuan Lu, Shaojie Shen</h3>
<p>Visual-Inertial odometry is known to suffer from drifting especially over
long-term runs. In this paper, we present GVINS, a non-linear optimization
based system that tightly fuses GNSS raw measurements with visual and inertial
information for real-time and drift-free state estimation. The proposed system
combines merits from VIO and GNSS system, thus is able to achieve both local
smoothness and global consistency. To associate global measurements with local
states, a coarse-to-fine initialization procedure is proposed to efficiently
online calibrate the transformation and initialize GNSS states from only a
short window of measurements. The GNSS pseudorange and Doppler shift
measurements are modelled and optimized under a factor graph framework along
with visual and inertial constraints. For complex and GNSS-unfriendly area, the
degenerate cases are discussed and carefully handled to ensure robustness. The
engineering challenges involved in the system are also included to facilitate
relevant GNSS fusion researches. Thanks to the tightly-coupled multi-sensor
approach and system design, our estimator is able to recover the position and
orientation in the global Earth frame, even with less than 4 satellites being
tracked. We extensively evaluate the proposed system on simulation and
real-world experiments, and the result demonstrates that our system
substantially eliminates the drift of VIO and preserves the accuracy in spite
of noisy GNSS measurements.
</p>
<a href="http://arxiv.org/abs/2103.07899" target="_blank">arXiv:2103.07899</a> [<a href="http://arxiv.org/pdf/2103.07899" target="_blank">pdf</a>]

<h2>Bangla Handwritten Digit Recognition and Generation. (arXiv:2103.07905v1 [cs.CV])</h2>
<h3>Md Fahim Sikder</h3>
<p>Handwritten digit or numeral recognition is one of the classical issues in
the area of pattern recognition and has seen tremendous advancement because of
the recent wide availability of computing resources. Plentiful works have
already done on English, Arabic, Chinese, Japanese handwritten script. Some
work on Bangla also have been done but there is space for development. From
that angle, in this paper, an architecture has been implemented which achieved
the validation accuracy of 99.44% on BHAND dataset and outperforms Alexnet and
Inception V3 architecture. Beside digit recognition, digit generation is
another field which has recently caught the attention of the researchers though
not many works have been done in this field especially on Bangla. In this
paper, a Semi-Supervised Generative Adversarial Network or SGAN has been
applied to generate Bangla handwritten numerals and it successfully generated
Bangla digits.
</p>
<a href="http://arxiv.org/abs/2103.07905" target="_blank">arXiv:2103.07905</a> [<a href="http://arxiv.org/pdf/2103.07905" target="_blank">pdf</a>]

<h2>A Normal Distribution Transform-Based Radar Odometry Designed For Scanning and Automotive Radars. (arXiv:2103.07908v1 [cs.RO])</h2>
<h3>Pou-Chun Kung, Chieh-Chih Wang, Wen-Chieh Lin</h3>
<p>Existing radar sensors can be classified into automotive and scanning radars.
While most radar odometry (RO) methods are only designed for a specific type of
radar, our RO method adapts to both scanning and automotive radars. Our RO is
simple yet effective, where the pipeline consists of thresholding,
probabilistic submap building, and an NDT-based radar scan matching. The
proposed RO has been tested on two public radar datasets: the Oxford Radar
RobotCar dataset and the nuScenes dataset, which provide scanning and
automotive radar data respectively. The results show that our approach
surpasses state-of-the-art RO using either automotive or scanning radar by
reducing translational error by 51\% and 30\%, respectively, and rotational
error by 17\% and 29\%, respectively. Besides, we show that our RO achieves
centimeter-level accuracy as lidar odometry, and automotive and scanning RO
have similar accuracy.
</p>
<a href="http://arxiv.org/abs/2103.07908" target="_blank">arXiv:2103.07908</a> [<a href="http://arxiv.org/pdf/2103.07908" target="_blank">pdf</a>]

<h2>Towards Generalizable and Robust Face Manipulation Detection via Bag-of-local-feature. (arXiv:2103.07915v1 [cs.CV])</h2>
<h3>Changtao Miao, Qi Chu, Weihai Li, Tao Gong, Wanyi Zhuang, Nenghai Yu</h3>
<p>Over the past several years, in order to solve the problem of malicious abuse
of facial manipulation technology, face manipulation detection technology has
obtained considerable attention and achieved remarkable progress. However, most
existing methods have very impoverished generalization ability and robustness.
In this paper, we propose a novel method for face manipulation detection, which
can improve the generalization ability and robustness by bag-of-local-feature.
Specifically, we extend Transformers using bag-of-feature approach to encode
inter-patch relationships, allowing it to learn local forgery features without
any explicit supervision. Extensive experiments demonstrate that our method can
outperform competing state-of-the-art methods on FaceForensics++, Celeb-DF and
DeeperForensics-1.0 datasets.
</p>
<a href="http://arxiv.org/abs/2103.07915" target="_blank">arXiv:2103.07915</a> [<a href="http://arxiv.org/pdf/2103.07915" target="_blank">pdf</a>]

<h2>SaNet: Scale-aware neural Network for Parsing Multiple Spatial Resolution Aerial Images. (arXiv:2103.07935v1 [cs.CV])</h2>
<h3>Libo Wang (School of Remote Sensing and Information Engineering Wuhan University, China)</h3>
<p>Assigning the geospatial objects of aerial images with categorical
information at the pixel-level is a basic task in urban scene understanding.
However, the huge differencc in remote sensing sensors makes the acqured aerial
images in multiple spatial resolution (MSR), which brings two issues: the
increased scale variation of geospatial objects and informative feature loss as
spatial resolution drops. To address the two issues, we propose a novel
scale-aware neural network (SaNet) for parsing MSR aerial images. For coping
with the imbalanced segmentation quality between larger and smaller objects
caused by the scale variation, the SaNet deploys a densely connected feature
network (DCFPN) module to capture quality multi-scale context with large
receptive fields. To alleviate the informative feature loss, a SFR module is
incorporated into the network to learn scale-invariant features with spatial
relation enhancement. Extensive experimental results on the ISPRS Vaihingen 2D
Dataset and ISPRS Potsdam 2D Dataset demonstrate the outstanding
cross-resolution segmentation ability of the proposed SaNet compared to other
state-of-the-art networks.
</p>
<a href="http://arxiv.org/abs/2103.07935" target="_blank">arXiv:2103.07935</a> [<a href="http://arxiv.org/pdf/2103.07935" target="_blank">pdf</a>]

<h2>Learning needle insertion from sample task executions. (arXiv:2103.07938v1 [cs.RO])</h2>
<h3>Amir Ghalamzan-E</h3>
<p>Automating a robotic task, e.g., robotic suturing can be very complex and
time-consuming. Learning a task model to autonomously perform the task is
invaluable making the technology, robotic surgery, accessible for a wider
community. The data of robotic surgery can be easily logged where the collected
data can be used to learn task models. This will result in reduced time and
cost of robotic surgery in which a surgeon can supervise the robot operation or
give high-level commands instead of low-level control of the tools. We present
a data-set of needle insertion in soft tissue with two arms where Arm 1 inserts
the needle into the tissue and Arm 2 actively manipulate the soft tissue to
ensure the desired and actual exit points are the same. This is important in
real-surgery because suturing without active manipulation of tissue may yield
failure of the suturing as the stitch may not grip enough tissue to resist the
force applied for the suturing. We present a needle insertion dataset including
60 successful trials recorded by 3 pair of stereo cameras. Moreover, we present
Deep-robot Learning from Demonstrations that predicts the desired state of the
robot at the time step after t (which the optimal action taken at t yields) by
looking at the video of the past time steps, i.e. n step time history where N
is the memory time window, of the task execution. The experimental results
illustrate our proposed deep model architecture is outperforming the existing
methods. Although the solution is not yet ready to be deployed on a real robot,
the results indicate the possibility of future development for real robot
deployment.
</p>
<a href="http://arxiv.org/abs/2103.07938" target="_blank">arXiv:2103.07938</a> [<a href="http://arxiv.org/pdf/2103.07938" target="_blank">pdf</a>]

<h2>Semi-Supervised Video Deraining with Dynamic Rain Generator. (arXiv:2103.07939v1 [cs.CV])</h2>
<h3>Zongsheng Yue, Jianwen Xie, Qian Zhao, Deyu Meng</h3>
<p>While deep learning (DL)-based video deraining methods have achieved
significant success recently, they still exist two major drawbacks. Firstly,
most of them do not sufficiently model the characteristics of rain layers of
rainy videos. In fact, the rain layers exhibit strong physical properties
(e.g., direction, scale and thickness) in spatial dimension and natural
continuities in temporal dimension, and thus can be generally modelled by the
spatial-temporal process in statistics. Secondly, current DL-based methods
seriously depend on the labeled synthetic training data, whose rain types are
always deviated from those in unlabeled real data. Such gap between synthetic
and real data sets leads to poor performance when applying them in real
scenarios. Against these issues, this paper proposes a new semi-supervised
video deraining method, in which a dynamic rain generator is employed to fit
the rain layer, expecting to better depict its insightful characteristics.
Specifically, such dynamic generator consists of one emission model and one
transition model to simultaneously encode the spatially physical structure and
temporally continuous changes of rain streaks, respectively, which both are
parameterized as deep neural networks (DNNs). Further more, different prior
formats are designed for the labeled synthetic and unlabeled real data, so as
to fully exploit the common knowledge underlying them. Last but not least, we
also design a Monte Carlo EM algorithm to solve this model. Extensive
experiments are conducted to verify the superiorities of the proposed
semi-supervised deraining model.
</p>
<a href="http://arxiv.org/abs/2103.07939" target="_blank">arXiv:2103.07939</a> [<a href="http://arxiv.org/pdf/2103.07939" target="_blank">pdf</a>]

<h2>Modular Interactive Video Object Segmentation: Interaction-to-Mask, Propagation and Difference-Aware Fusion. (arXiv:2103.07941v1 [cs.CV])</h2>
<h3>Ho Kei Cheng, Yu-Wing Tai, Chi-Keung Tang</h3>
<p>We present Modular interactive VOS (MiVOS) framework which decouples
interaction-to-mask and mask propagation, allowing for higher generalizability
and better performance. Trained separately, the interaction module converts
user interactions to an object mask, which is then temporally propagated by our
propagation module using a novel top-$k$ filtering strategy in reading the
space-time memory. To effectively take the user's intent into account, a novel
difference-aware module is proposed to learn how to properly fuse the masks
before and after each interaction, which are aligned with the target frames by
employing the space-time memory. We evaluate our method both qualitatively and
quantitatively with different forms of user interactions (e.g., scribbles,
clicks) on DAVIS to show that our method outperforms current state-of-the-art
algorithms while requiring fewer frame interactions, with the additional
advantage in generalizing to different types of user interactions. We
contribute a large-scale synthetic VOS dataset with pixel-accurate segmentation
of 4.8M frames to accompany our source codes to facilitate future research.
</p>
<a href="http://arxiv.org/abs/2103.07941" target="_blank">arXiv:2103.07941</a> [<a href="http://arxiv.org/pdf/2103.07941" target="_blank">pdf</a>]

<h2>Monte Carlo Scene Search for 3D Scene Understanding. (arXiv:2103.07969v1 [cs.CV])</h2>
<h3>Shreyas Hampali, Sinisa Stekovic, Sayan Deb Sarkar, Chetan Srinivasa Kumar, Friedrich Fraundorfer, Vincent Lepetit</h3>
<p>We explore how a general AI algorithm can be used for 3D scene understanding
in order to reduce the need for training data. More exactly, we propose a
modification of the Monte Carlo Tree Search (MCTS) algorithm to retrieve
objects and room layouts from noisy RGB-D scans. While MCTS was developed as a
game-playing algorithm, we show it can also be used for complex perception
problems. It has few easy-to-tune hyperparameters and can optimise general
losses. We use it to optimise the posterior probability of objects and room
layout hypotheses given the RGB-D data. This results in an
analysis-by-synthesis approach that explores the solution space by rendering
the current solution and comparing it to the RGB-D observations. To perform
this exploration even more efficiently, we propose simple changes to the
standard MCTS' tree construction and exploration policy. We demonstrate our
approach on the ScanNet dataset. Our method often retrieves configurations that
are better than some manual annotations especially on layouts.
</p>
<a href="http://arxiv.org/abs/2103.07969" target="_blank">arXiv:2103.07969</a> [<a href="http://arxiv.org/pdf/2103.07969" target="_blank">pdf</a>]

<h2>TransFG: A Transformer Architecture for Fine-grained Recognition. (arXiv:2103.07976v1 [cs.CV])</h2>
<h3>Ju He, Jieneng Chen, Shuai Liu, Adam Kortylewski, Cheng Yang, Yutong Bai, Changhu Wang, Alan Yuille</h3>
<p>Fine-grained visual classification (FGVC) which aims at recognizing objects
from subcategories is a very challenging task due to the inherently subtle
inter-class differences. Recent works mainly tackle this problem by focusing on
how to locate the most discriminative image regions and rely on them to improve
the capability of networks to capture subtle variances. Most of these works
achieve this by using an RPN module to propose bounding boxes and re-use the
backbone network to extract features of selected boxes. Recently, vision
transformer (ViT) shows its strong performance in the traditional
classification task. The self-attention mechanism of the transformer links
every patch token to the classification token. The strength of the attention
link can be intuitively considered as an indicator of the importance of tokens.
In this work, we propose a novel transformer-based framework TransFG where we
integrate all raw attention weights of the transformer into an attention map
for guiding the network to effectively and accurately select discriminative
image patches and compute their relations. A duplicate loss is introduced to
encourage multiple attention heads to focus on different regions. In addition,
a contrastive loss is applied to further enlarge the distance between feature
representations of similar sub-classes. We demonstrate the value of TransFG by
conducting experiments on five popular fine-grained benchmarks: CUB-200-2011,
Stanford Cars, Stanford Dogs, NABirds and iNat2017 where we achieve
state-of-the-art performance. Qualitative results are presented for better
understanding of our model.
</p>
<a href="http://arxiv.org/abs/2103.07976" target="_blank">arXiv:2103.07976</a> [<a href="http://arxiv.org/pdf/2103.07976" target="_blank">pdf</a>]

<h2>Deep Tiling: Texture Tile Synthesis Using a Deep Learning Approach. (arXiv:2103.07992v1 [cs.CV])</h2>
<h3>Vasilis Toulatzis, Ioannis Fudos</h3>
<p>Texturing is a fundamental process in computer graphics. Texture is leveraged
to enhance the visualization outcome for a 3D scene. In many cases a texture
image cannot cover a large 3D model surface because of its small resolution.
Conventional techniques like repeating, mirror repeating or clamp to edge do
not yield visually acceptable results. Deep learning based texture synthesis
has proven to be very effective in such cases. All deep texture synthesis
methods trying to create larger resolution textures are limited in terms of GPU
memory resources. In this paper, we propose a novel approach to example-based
texture synthesis by using a robust deep learning process for creating tiles of
arbitrary resolutions that resemble the structural components of an input
texture. In this manner, our method is firstly much less memory limited owing
to the fact that a new texture tile of small size is synthesized and merged
with the original texture and secondly can easily produce missing parts of a
large texture.
</p>
<a href="http://arxiv.org/abs/2103.07992" target="_blank">arXiv:2103.07992</a> [<a href="http://arxiv.org/pdf/2103.07992" target="_blank">pdf</a>]

<h2>Design of a vision based range bearing and heading system for robot swarms. (arXiv:2103.08003v1 [cs.RO])</h2>
<h3>Hamid Majidi Balanji, Emre Yilmaz, Omer Cakmak, Ali Emre Turgut</h3>
<p>An essential problem of swarm robotics is how members of the swarm knows the
positions of other robots. The main aim of this research is to develop a
cost-effective and simple vision-based system to detect the range, bearing, and
heading of the robots inside a swarm using a multi-purpose passive landmark. A
small Zumo robot equipped with Raspberry Pi, PiCamera is utilized for the
implementation of the algorithm, and different kinds of multipurpose passive
landmarks with nonsymmetrical patterns, which give reliable information about
the range, bearing and heading in a single unit, are designed. By comparing the
recorded features obtained from image analysis of the landmark through
systematical experimentation and the actual measurements, correlations are
obtained, and algorithms converting those features into range, bearing and
heading are designed. The reliability and accuracy of algorithms are tested and
errors are found within an acceptable range.
</p>
<a href="http://arxiv.org/abs/2103.08003" target="_blank">arXiv:2103.08003</a> [<a href="http://arxiv.org/pdf/2103.08003" target="_blank">pdf</a>]

<h2>Vision based range and bearing algorithm for robot swarms. (arXiv:2103.08006v1 [cs.RO])</h2>
<h3>Hamid Majidi Balanji, Ali Emre Turgut</h3>
<p>This paper presents a novel computer vision the algorithm proposed for the
on-line range and bearing detection of the robot swarms. Results demonstrated
the reliability of the proposed vision system such that it can be used for the
robot swarms applications.
</p>
<a href="http://arxiv.org/abs/2103.08006" target="_blank">arXiv:2103.08006</a> [<a href="http://arxiv.org/pdf/2103.08006" target="_blank">pdf</a>]

<h2>Meta Preference Learning for Fast User Adaptation in Human-Supervisory Multi-Robot Deployments. (arXiv:2103.08008v1 [cs.RO])</h2>
<h3>Chao Huang, Wenhao Luo, Rui Liu</h3>
<p>As multi-robot systems (MRS) are widely used in various tasks such as natural
disaster response and social security, people enthusiastically expect an MRS to
be ubiquitous that a general user without heavy training can easily operate.
However, humans have various preferences on balancing between task performance
and safety, imposing different requirements onto MRS control. Failing to comply
with preferences makes people feel difficult in operation and decreases human
willingness of using an MRS. Therefore, to improve social acceptance as well as
performance, there is an urgent need to adjust MRS behaviors according to human
preferences before triggering human corrections, which increases cognitive
load. In this paper, a novel Meta Preference Learning (MPL) method was
developed to enable an MRS to fast adapt to user preferences. MPL based on meta
learning mechanism can quickly assess human preferences from limited
instructions; then, a neural network based preference model adjusts MRS
behaviors for preference adaption. To validate method effectiveness, a task
scenario "An MRS searches victims in an earthquake disaster site" was designed;
20 human users were involved to identify preferences as "aggressive", "medium",
"reserved"; based on user guidance and domain knowledge, about 20,000
preferences were simulated to cover different operations related to "task
quality", "task progress", "robot safety". The effectiveness of MPL in
preference adaption was validated by the reduced duration and frequency of
human interventions.
</p>
<a href="http://arxiv.org/abs/2103.08008" target="_blank">arXiv:2103.08008</a> [<a href="http://arxiv.org/pdf/2103.08008" target="_blank">pdf</a>]

<h2>Success Weighted by Completion Time: A Dynamics-Aware Evaluation Criteria for Embodied Navigation. (arXiv:2103.08022v1 [cs.RO])</h2>
<h3>Naoki Yokoyama, Sehoon Ha, Dhruv Batra</h3>
<p>We present Success weighted by Completion Time (SCT), a new metric for
evaluating navigation performance for mobile robots. Several related works on
navigation have used Success weighted by Path Length (SPL) as the primary
method of evaluating the path an agent makes to a goal location, but SPL is
limited in its ability to properly evaluate agents with complex dynamics. In
contrast, SCT explicitly takes the agent's dynamics model into consideration,
and aims to accurately capture how well the agent has approximated the fastest
navigation behavior afforded by its dynamics. While several embodied navigation
works use point-turn dynamics, we focus on unicycle-cart dynamics for our
agent, which better exemplifies the dynamics model of popular mobile robotics
platforms (e.g., LoCoBot, TurtleBot, Fetch, etc.). We also present
RRT*-Unicycle, an algorithm for unicycle dynamics that estimates the fastest
collision-free path and completion time from a starting pose to a goal location
in an environment containing obstacles. We experiment with deep reinforcement
learning and reward shaping to train and compare the navigation performance of
agents with different dynamics models. In evaluating these agents, we show that
in contrast to SPL, SCT is able to capture the advantages in navigation speed a
unicycle model has over a simpler point-turn model of dynamics. Lastly, we show
that we can successfully deploy our trained models and algorithms outside of
simulation in the real world. We embody our agents in an real robot to navigate
an apartment, and show that they can generalize in a zero-shot manner.
</p>
<a href="http://arxiv.org/abs/2103.08022" target="_blank">arXiv:2103.08022</a> [<a href="http://arxiv.org/pdf/2103.08022" target="_blank">pdf</a>]

<h2>Repairing Human Trust by Promptly Correcting Robot Mistakes with An Attention Transfer Model. (arXiv:2103.08025v1 [cs.RO])</h2>
<h3>Ruijiao Luo, Chao Huang, Yuntao Peng, Boyi Song, Rui Liu</h3>
<p>In human-robot collaboration (HRC), human trust in the robot is the human
expectation that a robot executes tasks with desired performance. A
higher-level trust increases the willingness of a human operator to assign
tasks, share plans, and reduce the interruption during robot executions,
thereby facilitating human-robot integration both physically and mentally.
However, due to real-world disturbances, robots inevitably make mistakes,
decreasing human trust and further influencing collaboration. Trust is fragile
and trust loss is triggered easily when robots show incapability of task
executions, making the trust maintenance challenging. To maintain human trust,
in this research, a trust repair framework is developed based on a
human-to-robot attention transfer (H2R-AT) model and a user trust study. The
rationale of this framework is that a prompt mistake correction restores human
trust. With H2R-AT, a robot localizes human verbal concerns and makes prompt
mistake corrections to avoid task failures in an early stage and to finally
improve human trust. User trust study measures trust status before and after
the behavior corrections to quantify the trust loss. Robot experiments were
designed to cover four typical mistakes, wrong action, wrong region, wrong
pose, and wrong spatial relation, validated the accuracy of H2R-AT in robot
behavior corrections; a user trust study with $252$ participants was conducted,
and the changes in trust levels before and after corrections were evaluated.
The effectiveness of the human trust repairing was evaluated by the mistake
correction accuracy and the trust improvement.
</p>
<a href="http://arxiv.org/abs/2103.08025" target="_blank">arXiv:2103.08025</a> [<a href="http://arxiv.org/pdf/2103.08025" target="_blank">pdf</a>]

<h2>Versailles-FP dataset: Wall Detection in Ancient. (arXiv:2103.08064v1 [cs.CV])</h2>
<h3>Wassim Swaileh, Dimitrios Kotzinos, Suman Ghosh, Michel Jordan, Son Vu, Yaguan Qian</h3>
<p>Access to historical monuments' floor plans over a time period is necessary
to understand the architectural evolution and history. Such knowledge bases
also helps to rebuild the history by establishing connection between different
event, person and facts which are once part of the buildings. Since the
two-dimensional plans do not capture the entire space, 3D modeling sheds new
light on the reading of these unique archives and thus opens up great
perspectives for understanding the ancient states of the monument. Since the
first step in the building's or monument's 3D model is the wall detection in
the floor plan, we introduce in this paper the new and unique Versailles FP
dataset of wall groundtruthed images of the Versailles Palace dated between
17th and 18th century. The dataset's wall masks are generated using an
automatic approach based on multi directional steerable filters. The generated
wall masks are then validated and corrected manually. We validate our approach
of wall mask generation in state-of-the-art modern datasets. Finally we propose
a U net based convolutional framework for wall detection. Our method achieves
state of the art result surpassing fully connected network based approach.
</p>
<a href="http://arxiv.org/abs/2103.08064" target="_blank">arXiv:2103.08064</a> [<a href="http://arxiv.org/pdf/2103.08064" target="_blank">pdf</a>]

<h2>Learning robust driving policies without online exploration. (arXiv:2103.08070v1 [cs.RO])</h2>
<h3>Daniel Graves, Nhat M. Nguyen, Kimia Hassanzadeh, Jun Jin, Jun Luo</h3>
<p>We propose a multi-time-scale predictive representation learning method to
efficiently learn robust driving policies in an offline manner that generalize
well to novel road geometries, and damaged and distracting lane conditions
which are not covered in the offline training data. We show that our proposed
representation learning method can be applied easily in an offline (batch)
reinforcement learning setting demonstrating the ability to generalize well and
efficiently under novel conditions compared to standard batch RL methods. Our
proposed method utilizes training data collected entirely offline in the
real-world which removes the need of intensive online explorations that impede
applying deep reinforcement learning on real-world robot training. Various
experiments were conducted in both simulator and real-world scenarios for the
purpose of evaluation and analysis of our proposed claims.
</p>
<a href="http://arxiv.org/abs/2103.08070" target="_blank">arXiv:2103.08070</a> [<a href="http://arxiv.org/pdf/2103.08070" target="_blank">pdf</a>]

<h2>Pushing the Limits of Capsule Networks. (arXiv:2103.08074v1 [cs.CV])</h2>
<h3>Prem Nair, Rohan Doshi, Stefan Keselj</h3>
<p>Convolutional neural networks use pooling and other downscaling operations to
maintain translational invariance for detection of features, but in their
architecture they do not explicitly maintain a representation of the locations
of the features relative to each other. This means they do not represent two
instances of the same object in different orientations the same way, like
humans do, and so training them often requires extensive data augmentation and
exceedingly deep networks. A team at Google Brain recently made news with an
attempt to fix this problem: Capsule Networks. While a normal CNN works with
scalar outputs representing feature presence, a CapsNet works with vector
outputs representing entity presence. We want to stress test CapsNet in various
incremental ways to better understand their performance and expressiveness. In
broad terms, the goals of our investigation are: (1) test CapsNets on datasets
that are like MNIST but harder in a specific way, and (2) explore the internal
embedding space and sources of error for CapsNets.
</p>
<a href="http://arxiv.org/abs/2103.08074" target="_blank">arXiv:2103.08074</a> [<a href="http://arxiv.org/pdf/2103.08074" target="_blank">pdf</a>]

<h2>Exploring Genetic-histologic Relationships in Breast Cancer. (arXiv:2103.08082v1 [cs.CV])</h2>
<h3>Ruchi Chauhan, PK Vinod, CV Jawahar</h3>
<p>The advent of digital pathology presents opportunities for computer vision
for fast, accurate, and objective solutions for histopathological images and
aid in knowledge discovery. This work uses deep learning to predict genomic
biomarkers - TP53 mutation, PIK3CA mutation, ER status, PR status, HER2 status,
and intrinsic subtypes, from breast cancer histopathology images. Furthermore,
we attempt to understand the underlying morphology as to how these genomic
biomarkers manifest in images. Since gene sequencing is expensive, not always
available, or even feasible, predicting these biomarkers from images would help
in diagnosis, prognosis, and effective treatment planning. We outperform the
existing works with a minimum improvement of 0.02 and a maximum of 0.13 AUROC
scores across all tasks. We also gain insights that can serve as hypotheses for
further experimentations, including the presence of lymphocytes and
karyorrhexis. Moreover, our fully automated workflow can be extended to other
tasks across other cancer subtypes.
</p>
<a href="http://arxiv.org/abs/2103.08082" target="_blank">arXiv:2103.08082</a> [<a href="http://arxiv.org/pdf/2103.08082" target="_blank">pdf</a>]

<h2>Classifying Cycling Hazards in Egocentric Data. (arXiv:2103.08102v1 [cs.CV])</h2>
<h3>Jayson Haebich, Christian Sandor, Alvaro Cassinelli</h3>
<p>This proposal is for the creation and annotation of an egocentric video data
set of hazardous cycling situations. The resulting data set will facilitate
projects to improve the safety and experience of cyclists. Since cyclists are
highly sensitive to road surface conditions and hazards they require more
detail about road conditions when navigating their route. Features such as tram
tracks, cobblestones, gratings, and utility access points can pose hazards or
uncomfortable riding conditions for their journeys. Possible uses for the data
set are identifying existing hazards in cycling infrastructure for municipal
authorities, real time hazard and surface condition warnings for cyclists, and
the identification of conditions that cause cyclists to make sudden changes in
their immediate route.
</p>
<a href="http://arxiv.org/abs/2103.08102" target="_blank">arXiv:2103.08102</a> [<a href="http://arxiv.org/pdf/2103.08102" target="_blank">pdf</a>]

<h2>MBAPose: Mask and Bounding-Box Aware Pose Estimation of Surgical Instruments with Photorealistic Domain Randomization. (arXiv:2103.08105v1 [cs.RO])</h2>
<h3>Masakazu Yoshimura, Murilo Marques Marinho, Kanako Harada, Mamoru Mitsuishi</h3>
<p>Surgical robots are controlled using a priori models based on robots'
geometric parameters, which are calibrated before the surgical procedure. One
of the challenges in using robots in real surgical settings is that parameters
change over time, consequently deteriorating control accuracy. In this context,
our group has been investigating online calibration strategies without added
sensors. In one step toward that goal, we have developed an algorithm to
estimate the pose of the instruments' shafts in endoscopic images. In this
study, we build upon that earlier work and propose a new framework to more
precisely estimate the pose of a rigid surgical instrument. Our strategy is
based on a novel pose estimation model called MBAPose and the use of synthetic
training data. Our experiments demonstrated an improvement of 21 % for
translation error and 26 % for orientation error on synthetic test data with
respect to our previous work. Results with real test data provide a baseline
for further research.
</p>
<a href="http://arxiv.org/abs/2103.08105" target="_blank">arXiv:2103.08105</a> [<a href="http://arxiv.org/pdf/2103.08105" target="_blank">pdf</a>]

<h2>Extrinsic Contact Sensing with Relative-Motion Tracking from Distributed Tactile Measurements. (arXiv:2103.08108v1 [cs.RO])</h2>
<h3>Daolin Ma, Siyuan Dong, Alberto Rodriguez</h3>
<p>This paper addresses the localization of contacts of an unknown grasped rigid
object with its environment, i.e., extrinsic to the robot.

We explore the key role that distributed tactile sensing plays in localizing
contacts external to the robot, in contrast to the role that aggregated
force/torque measurements play in localizing contacts on the robot. When in
contact with the environment, an object will move in accordance with the
kinematic and possibly frictional constraints imposed by that contact. Small
motions of the object, which are observable with tactile sensors, indirectly
encode those constraints and the geometry that defines them.

We formulate the extrinsic contact sensing problem as a constraint-based
estimation. The estimation is subject to the kinematic constraints imposed by
the tactile measurements of object motion, as well as the kinematic (e.g.,
non-penetration) and possibly frictional (e.g., sticking) constraints imposed
by rigid-body mechanics.

We validate the approach in simulation and with real experiments on the case
studies of fixed point and line contacts.

This paper discusses the theoretical basis for the value of distributed
tactile sensing in contrast to aggregated force/torque measurements. It also
provides an estimation framework for localizing environmental contacts with
potential impact in contact-rich manipulation scenarios such as assembling or
packing.
</p>
<a href="http://arxiv.org/abs/2103.08108" target="_blank">arXiv:2103.08108</a> [<a href="http://arxiv.org/pdf/2103.08108" target="_blank">pdf</a>]

<h2>Boundary Proposal Network for Two-Stage Natural Language Video Localization. (arXiv:2103.08109v1 [cs.CV])</h2>
<h3>Shaoning Xiao, Long Chen, Songyang Zhang, Wei Ji, Jian Shao, Lu Ye, Jun Xiao</h3>
<p>We aim to address the problem of Natural Language Video Localization
(NLVL)-localizing the video segment corresponding to a natural language
description in a long and untrimmed video. State-of-the-art NLVL methods are
almost in one-stage fashion, which can be typically grouped into two
categories: 1) anchor-based approach: it first pre-defines a series of video
segment candidates (e.g., by sliding window), and then does classification for
each candidate; 2) anchor-free approach: it directly predicts the probabilities
for each video frame as a boundary or intermediate frame inside the positive
segment. However, both kinds of one-stage approaches have inherent drawbacks:
the anchor-based approach is susceptible to the heuristic rules, further
limiting the capability of handling videos with variant length. While the
anchor-free approach fails to exploit the segment-level interaction thus
achieving inferior results. In this paper, we propose a novel Boundary Proposal
Network (BPNet), a universal two-stage framework that gets rid of the issues
mentioned above. Specifically, in the first stage, BPNet utilizes an
anchor-free model to generate a group of high-quality candidate video segments
with their boundaries. In the second stage, a visual-language fusion layer is
proposed to jointly model the multi-modal interaction between the candidate and
the language query, followed by a matching score rating layer that outputs the
alignment score for each candidate. We evaluate our BPNet on three challenging
NLVL benchmarks (i.e., Charades-STA, TACoS and ActivityNet-Captions). Extensive
experiments and ablative studies on these datasets demonstrate that the BPNet
outperforms the state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2103.08109" target="_blank">arXiv:2103.08109</a> [<a href="http://arxiv.org/pdf/2103.08109" target="_blank">pdf</a>]

<h2>Improving Generalization of Transfer Learning Across Domains Using Spatio-Temporal Features in Autonomous Driving. (arXiv:2103.08116v1 [cs.CV])</h2>
<h3>Shivam Akhauri, Laura Zheng, Tom Goldstein, Ming Lin</h3>
<p>Training vision-based autonomous driving in the real world can be inefficient
and impractical. Vehicle simulation can be used to learn in the virtual world,
and the acquired skills can be transferred to handle real-world scenarios more
effectively. Between virtual and real visual domains, common features such as
relative distance to road edges and other vehicles over time are consistent.
These visual elements are intuitively crucial for human decision making during
driving. We hypothesize that these spatio-temporal factors can also be used in
transfer learning to improve generalization across domains. First, we propose a
CNN+LSTM transfer learning framework to extract the spatio-temporal features
representing vehicle dynamics from scenes. Next, we conduct an ablation study
to quantitatively estimate the significance of various features in the
decisions of driving systems. We observe that physically interpretable factors
are highly correlated with network decisions, while representational
differences between scenes are not. Finally, based on the results of our
ablation study, we propose a transfer learning pipeline that uses saliency maps
and physical features extracted from a source model to enhance the performance
of a target model. Training of our network is initialized with the learned
weights from CNN and LSTM latent features (capturing the intrinsic physics of
the moving vehicle w.r.t. its surroundings) transferred from one domain to
another. Our experiments show that this proposed transfer learning framework
better generalizes across unseen domains compared to a baseline CNN model on a
binary classification learning task.
</p>
<a href="http://arxiv.org/abs/2103.08116" target="_blank">arXiv:2103.08116</a> [<a href="http://arxiv.org/pdf/2103.08116" target="_blank">pdf</a>]

<h2>Mobile Teleoperation: Evaluation of Wireless Wearable Sensing of the Operator's Arm Motion. (arXiv:2103.08119v1 [cs.RO])</h2>
<h3>Guanhao Fu, Ehsan Azimi, Peter Kazanzides</h3>
<p>Teleoperation platforms often require the user to be situated at a fixed
location to both visualize and control the movement of the robot and thus do
not provide the operator with much mobility. One example of such systems is in
existing robotic surgery solutions that require the surgeons to be away from
the patient, attached to consoles where their heads must be fixed and their
arms can only move in a limited space. This creates a barrier between
physicians and patients that does not exist in normal surgery. To address this
issue, we propose a mobile telesurgery solution where the surgeons are no
longer mechanically limited to control consoles and are able to teleoperate the
robots from the patient bedside, using their arms equipped with wireless
sensors and viewing the endoscope video via optical see-through HMDs. We
evaluate the feasibility and efficiency of our user interaction method with a
standard surgical robotic manipulator via two tasks with different levels of
required dexterity. The results indicate that with sufficient training our
proposed platform can attain similar efficiency while providing added mobility
for the operator.
</p>
<a href="http://arxiv.org/abs/2103.08119" target="_blank">arXiv:2103.08119</a> [<a href="http://arxiv.org/pdf/2103.08119" target="_blank">pdf</a>]

<h2>Modelling Human Kinetics and Kinematics during Walking using Reinforcement Learning. (arXiv:2103.08125v1 [cs.RO])</h2>
<h3>Visak Kumar</h3>
<p>In this work, we develop an automated method to generate 3D human walking
motion in simulation which is comparable to real-world human motion. At the
core, our work leverages the ability of deep reinforcement learning methods to
learn high-dimensional motor skills while being robust to variations in the
environment dynamics. Our approach iterates between policy learning and
parameter identification to match the real-world bio-mechanical human data. We
present a thorough evaluation of the kinematics, kinetics and ground reaction
forces generated by our learned virtual human agent. We also show that the
method generalizes well across human-subjects with different kinematic
structure and gait-characteristics.
</p>
<a href="http://arxiv.org/abs/2103.08125" target="_blank">arXiv:2103.08125</a> [<a href="http://arxiv.org/pdf/2103.08125" target="_blank">pdf</a>]

<h2>R-PointHop: A Green, Accurate and Unsupervised Point Cloud Registration Method. (arXiv:2103.08129v1 [cs.CV])</h2>
<h3>Pranav Kadam, Min Zhang, Shan Liu, C.-C. Jay Kuo</h3>
<p>Inspired by the recent PointHop classification method, an unsupervised 3D
point cloud registration method, called R-PointHop, is proposed in this work.
R-PointHop first determines a local reference frame (LRF) for every point using
its nearest neighbors and finds its local attributes. Next, R-PointHop obtains
local-to-global hierarchical features by point downsampling, neighborhood
expansion, attribute construction and dimensionality reduction steps. Thus, we
can build the correspondence of points in the hierarchical feature space using
the nearest neighbor rule. Afterwards, a subset of salient points of good
correspondence is selected to estimate the 3D transformation. The use of LRF
allows for hierarchical features of points to be invariant with respect to
rotation and translation, thus making R-PointHop more robust in building point
correspondence even when rotation angles are large. Experiments are conducted
on the ModelNet40 and the Stanford Bunny dataset, which demonstrate the
effectiveness of R-PointHop on the 3D point cloud registration task. R-PointHop
is a green and accurate solution since its model size and training time are
smaller than those of deep learning methods by an order of magnitude while its
registration errors are smaller. Our codes are available on GitHub.
</p>
<a href="http://arxiv.org/abs/2103.08129" target="_blank">arXiv:2103.08129</a> [<a href="http://arxiv.org/pdf/2103.08129" target="_blank">pdf</a>]

<h2>Detection and Localization of Facial Expression Manipulations. (arXiv:2103.08134v1 [cs.CV])</h2>
<h3>Ghazal Mazaheri, Amit K. Roy-Chowdhury</h3>
<p>Concern regarding the wide-spread use of fraudulent images/videos in social
media necessitates precise detection of such fraud. The importance of facial
expressions in communication is widely known, and adversarial attacks often
focus on manipulating the expression related features. Thus, it is important to
develop methods that can detect manipulations in facial expressions, and
localize the manipulated regions. To address this problem, we propose a
framework that is able to detect manipulations in facial expression using a
close combination of facial expression recognition and image manipulation
methods. With the addition of feature maps extracted from the facial expression
recognition framework, our manipulation detector is able to localize the
manipulated region. We show that, on the Face2Face dataset, where there is
abundant expression manipulation, our method achieves over 3% higher accuracy
for both classification and localization of manipulations compared to
state-of-the-art methods. In addition, results on the NeuralTextures dataset
where the facial expressions corresponding to the mouth regions have been
modified, show 2% higher accuracy in both classification and localization of
manipulation. We demonstrate that the method performs at-par with the
state-of-the-art methods in cases where the expression is not manipulated, but
rather the identity is changed, thus ensuring generalizability of the approach.
</p>
<a href="http://arxiv.org/abs/2103.08134" target="_blank">arXiv:2103.08134</a> [<a href="http://arxiv.org/pdf/2103.08134" target="_blank">pdf</a>]

<h2>Cloth Manipulation Planning on Basis of Mesh Representations with Incomplete Domain Knowledge and Voxel-to-Mesh Estimation. (arXiv:2103.08137v1 [cs.RO])</h2>
<h3>Solvi Arnold (1), Daisuke Tanaka (1), Kimitoshi Yamazaki (1) ((1) Shinshu University)</h3>
<p>We consider the problem of open-goal planning for robotic cloth manipulation.
Core of our system is a neural network trained as a forward model of cloth
behaviour under manipulation, with planning performed through backpropagation.
We introduce a neural network-based routine for estimating mesh representations
from voxel input, and perform planning in mesh format internally. We address
the problem of planning with incomplete domain knowledge by means of an
explicit epistemic uncertainty signal. This signal is calculated from
prediction divergence between two instances of the forward model network and
used to avoid epistemic uncertainty during planning. Finally, we introduce
logic for handling restriction of grasp points to a discrete set of candidates,
in order to accommodate graspability constraints imposed by robotic hardware.
We evaluate the system's mesh estimation, prediction, and planning ability on
simulated cloth for sequences of one to three manipulations. Comparative
experiments confirm that planning on basis of estimated meshes improves
accuracy compared to voxel-based planning, and that epistemic uncertainty
avoidance improves performance under conditions of incomplete domain knowledge.
We additionally present qualitative results on robot hardware.
</p>
<a href="http://arxiv.org/abs/2103.08137" target="_blank">arXiv:2103.08137</a> [<a href="http://arxiv.org/pdf/2103.08137" target="_blank">pdf</a>]

<h2>LARNet: Lie Algebra Residual Network for Profile Face Recognition. (arXiv:2103.08147v1 [cs.CV])</h2>
<h3>Xiaolong Yang</h3>
<p>Due to large variations between profile and frontal faces, profile-based face
recognition remains as a tremendous challenge in many practical vision
scenarios. Traditional techniques address this challenge either by synthesizing
frontal faces or by pose-invariants learning. In this paper, we propose a novel
method with Lie algebra theory to explore how face rotation in the 3D space
affects the deep feature generation process of convolutional neural networks
(CNNs). We prove that face rotation in the image space is equivalent to an
additive residual component in the feature space of CNNs, which is determined
solely by the rotation. Based on this theoretical finding, we further design a
Lie algebraic residual network (LARNet) for tackling profile-based face
recognition. Our LARNet consists of a residual subnet for decoding rotation
information from input face images, and a gating subnet to learn rotation
magnitude for controlling the number of residual components contributing to the
feature learning process. Comprehensive experimental evaluations on
frontal-profile face datasets and general face recognition datasets demonstrate
that our method consistently outperforms the state-of-the-arts.
</p>
<a href="http://arxiv.org/abs/2103.08147" target="_blank">arXiv:2103.08147</a> [<a href="http://arxiv.org/pdf/2103.08147" target="_blank">pdf</a>]

<h2>DMN4: Few-shot Learning via Discriminative Mutual Nearest Neighbor Neural Network. (arXiv:2103.08160v1 [cs.CV])</h2>
<h3>Yang Liu, Tu Zheng, Jie Song, Deng Cai, Xiaofei He</h3>
<p>Few-shot learning (FSL) aims to classify images under low-data regimes, where
the conventional pooled global representation is likely to lose useful local
characteristics. Recent work has achieved promising performances by using deep
descriptors. They generally take all deep descriptors from neural networks into
consideration while ignoring that some of them are useless in classification
due to their limited receptive field, e.g., task-irrelevant descriptors could
be misleading and multiple aggregative descriptors from background clutter
could even overwhelm the object's presence. In this paper, we argue that a
Mutual Nearest Neighbor (MNN) relation should be established to explicitly
select the query descriptors that are most relevant to each task and discard
less relevant ones from aggregative clutters in FSL. Specifically, we propose
Discriminative Mutual Nearest Neighbor Neural Network (DMN4) for FSL. Extensive
experiments demonstrate that our method not only qualitatively selects
task-relevant descriptors but also quantitatively outperforms the existing
state-of-the-arts by a large margin of 1.8~4.9% on fine-grained CUB, a
considerable margin of 1.4~2.2% on both supervised and semi-supervised
miniImagenet, and ~1.4% on challenging tieredimagenet.
</p>
<a href="http://arxiv.org/abs/2103.08160" target="_blank">arXiv:2103.08160</a> [<a href="http://arxiv.org/pdf/2103.08160" target="_blank">pdf</a>]

<h2>Gathering of seven autonomous mobile robots on triangular grids. (arXiv:2103.08172v1 [cs.RO])</h2>
<h3>Masahiro Shibata, Masaki Ohyabu, Yuichi Sudo, Junya Nakamura, Yonghwan Kim, Yoshiaki Katayama</h3>
<p>In this paper, we consider the gathering problem of seven autonomous mobile
robots on triangular grids. The gathering problem requires that, starting from
any connected initial configuration where a subgraph induced by all robot nodes
(nodes where a robot exists) constitutes one connected graph, robots reach a
configuration such that the maximum distance between two robots is minimized.
For the case of seven robots, gathering is achieved when one robot has six
adjacent robot nodes (they form a shape like a hexagon). In this paper, we aim
to clarify the relationship between the capability of robots and the
solvability of gathering on a triangular grid. In particular, we focus on
visibility range of robots. To discuss the solvability of the problem in terms
of the visibility range, we consider strong assumptions except for visibility
range. Concretely, we assume that robots are fully synchronous and they agree
on the direction and orientation of the x-axis, and chirality in the triangular
grid. In this setting, we first consider the weakest assumption about
visibility range, i.e., robots with visibility range 1. In this case, we show
that there exists no collision-free algorithm to solve the gathering problem.
Next, we extend the visibility range to 2. In this case, we show that our
algorithm can solve the problem from any connected initial configuration. Thus,
the proposed algorithm is optimal in terms of visibility range.
</p>
<a href="http://arxiv.org/abs/2103.08172" target="_blank">arXiv:2103.08172</a> [<a href="http://arxiv.org/pdf/2103.08172" target="_blank">pdf</a>]

<h2>Shape-induced obstacle attraction and repulsion during dynamic locomotion. (arXiv:2103.08176v1 [cs.RO])</h2>
<h3>Yuanfeng Han, Ratan Othayoth, Yulong Wang, Chun-Cheng Hsu, Rafael de la Tijera Obert, Evains Francois, Chen Li</h3>
<p>Robots still struggle to dynamically traverse complex 3-D terrain with many
large obstacles, an ability required for many critical applications.
Body-obstacle interaction is often inevitable and induces perturbation and
uncertainty in motion that challenges closed-form dynamic modeling. Here,
inspired by recent discovery of a terradynamic streamlined shape, we studied
how two body shapes interacting with obstacles affect turning and pitching
motions of an open-loop multi-legged robot and cockroaches during dynamic
locomotion. With a common cuboidal body, the robot was attracted towards
obstacles, resulting in pitching up and flipping-over. By contrast, with an
elliptical body, the robot was repelled by obstacles and readily traversed. The
animal displayed qualitatively similar turning and pitching motions induced by
these two body shapes. However, unlike the cuboidal robot, the cuboidal animal
was capable of escaping obstacle attraction and subsequent high pitching and
flipping over, which inspired us to develop an empirical pitch-and-turn
strategy for cuboidal robots. Considering the similarity of our self-propelled
body-obstacle interaction with part-feeder interaction in robotic part
manipulation, we developed a quasi-static potential energy landscape model to
explain the dependence of dynamic locomotion on body shape. Our experimental
and modeling results also demonstrated that obstacle attraction or repulsion is
an inherent property of locomotor body shape and insensitive to obstacle
geometry and size. Our study expanded the concept and usefulness of
terradynamic shapes for passive control of robot locomotion to traverse large
obstacles using physical interaction. Our study is also a step in establishing
an energy landscape approach to locomotor transitions.
</p>
<a href="http://arxiv.org/abs/2103.08176" target="_blank">arXiv:2103.08176</a> [<a href="http://arxiv.org/pdf/2103.08176" target="_blank">pdf</a>]

<h2>Geometric Change Detection in Digital Twins using 3D Machine Learning. (arXiv:2103.08201v1 [cs.CV])</h2>
<h3>Tiril Sundby, Julia Maria Graham, Adil Rasheed, Mandar Tabib, Omer San</h3>
<p>Digital twins are meant to bridge the gap between real-world physical systems
and virtual representations. Both stand-alone and descriptive digital twins
incorporate 3D geometric models, which are the physical representations of
objects in the digital replica. Digital twin applications are required to
rapidly update internal parameters with the evolution of their physical
counterpart. Due to an essential need for having high-quality geometric models
for accurate physical representations, the storage and bandwidth requirements
for storing 3D model information can quickly exceed the available storage and
bandwidth capacity. In this work, we demonstrate a novel approach to geometric
change detection in the context of a digital twin. We address the issue through
a combined solution of Dynamic Mode Decomposition (DMD) for motion detection,
YOLOv5 for object detection, and 3D machine learning for pose estimation. DMD
is applied for background subtraction, enabling detection of moving foreground
objects in real-time. The video frames containing detected motion are extracted
and used as input to the change detection network. The object detection
algorithm YOLOv5 is applied to extract the bounding boxes of detected objects
in the video frames. Furthermore, the rotational pose of each object is
estimated in a 3D pose estimation network. A series of convolutional neural
networks conducts feature extraction from images and 3D model shapes. Then, the
network outputs the estimated Euler angles of the camera orientation with
respect to the object in the input image. By only storing data associated with
a detected change in pose, we minimize necessary storage and bandwidth
requirements while still being able to recreate the 3D scene on demand.
</p>
<a href="http://arxiv.org/abs/2103.08201" target="_blank">arXiv:2103.08201</a> [<a href="http://arxiv.org/pdf/2103.08201" target="_blank">pdf</a>]

<h2>3DCaricShop: A Dataset and A Baseline Method for Single-view 3D Caricature Face Reconstruction. (arXiv:2103.08204v1 [cs.CV])</h2>
<h3>Yuda Qiu, Xiaojie Xu, Lingteng Qiu, Yan Pan, Yushuang Wu, Weikai Chen, Xiaoguang Han</h3>
<p>Caricature is an artistic representation that deliberately exaggerates the
distinctive features of a human face to convey humor or sarcasm. However,
reconstructing a 3D caricature from a 2D caricature image remains a challenging
task, mostly due to the lack of data. We propose to fill this gap by
introducing 3DCaricShop, the first large-scale 3D caricature dataset that
contains 2000 high-quality diversified 3D caricatures manually crafted by
professional artists. 3DCaricShop also provides rich annotations including a
paired 2D caricature image, camera parameters and 3D facial landmarks. To
demonstrate the advantage of 3DCaricShop, we present a novel baseline approach
for single-view 3D caricature reconstruction. To ensure a faithful
reconstruction with plausible face deformations, we propose to connect the good
ends of the detailrich implicit functions and the parametric mesh
representations. In particular, we first register a template mesh to the output
of the implicit generator and iteratively project the registration result onto
a pre-trained PCA space to resolve artifacts and self-intersections. To deal
with the large deformation during non-rigid registration, we propose a novel
view-collaborative graph convolution network (VCGCN) to extract key points from
the implicit mesh for accurate alignment. Our method is able to generate
highfidelity 3D caricature in a pre-defined mesh topology that is
animation-ready. Extensive experiments have been conducted on 3DCaricShop to
verify the significance of the database and the effectiveness of the proposed
method.
</p>
<a href="http://arxiv.org/abs/2103.08204" target="_blank">arXiv:2103.08204</a> [<a href="http://arxiv.org/pdf/2103.08204" target="_blank">pdf</a>]

<h2>Cascaded Feature Warping Network for Unsupervised Medical Image Registration. (arXiv:2103.08213v1 [cs.CV])</h2>
<h3>Liutong Zhang, Lei Zhou, Ruiyang Li, Xianyu Wang, Boxuan Han, Hongen Liao</h3>
<p>Deformable image registration is widely utilized in medical image analysis,
but most proposed methods fail in the situation of complex deformations. In
this paper, we pre-sent a cascaded feature warping network to perform the
coarse-to-fine registration. To achieve this, a shared-weights encoder network
is adopted to generate the feature pyramids for the unaligned images. The
feature warping registration module is then used to estimate the deformation
field at each level. The coarse-to-fine manner is implemented by cascading the
module from the bottom level to the top level. Furthermore, the multi-scale
loss is also introduced to boost the registration performance. We employ two
public benchmark datasets and conduct various experiments to evaluate our
method. The results show that our method outperforms the state-of-the-art
methods, which also demonstrates that the cascaded feature warping network can
perform the coarse-to-fine registration effectively and efficiently.
</p>
<a href="http://arxiv.org/abs/2103.08213" target="_blank">arXiv:2103.08213</a> [<a href="http://arxiv.org/pdf/2103.08213" target="_blank">pdf</a>]

<h2>Detecting Human-Object Interaction via Fabricated Compositional Learning. (arXiv:2103.08214v1 [cs.CV])</h2>
<h3>Zhi Hou, Baosheng Yu, Yu Qiao, Xiaojiang Peng, Dacheng Tao</h3>
<p>Human-Object Interaction (HOI) detection, inferring the relationships between
human and objects from images/videos, is a fundamental task for high-level
scene understanding. However, HOI detection usually suffers from the open
long-tailed nature of interactions with objects, while human has extremely
powerful compositional perception ability to cognize rare or unseen HOI
samples. Inspired by this, we devise a novel HOI compositional learning
framework, termed as Fabricated Compositional Learning (FCL), to address the
problem of open long-tailed HOI detection. Specifically, we introduce an object
fabricator to generate effective object representations, and then combine verbs
and fabricated objects to compose new HOI samples. With the proposed object
fabricator, we are able to generate large-scale HOI samples for rare and unseen
categories to alleviate the open long-tailed issues in HOI detection. Extensive
experiments on the most popular HOI detection dataset, HICO-DET, demonstrate
the effectiveness of the proposed method for imbalanced HOI detection and
significantly improve the state-of-the-art performance on rare and unseen HOI
categories. Code is available at https://github.com/zhihou7/FCL.
</p>
<a href="http://arxiv.org/abs/2103.08214" target="_blank">arXiv:2103.08214</a> [<a href="http://arxiv.org/pdf/2103.08214" target="_blank">pdf</a>]

<h2>Adapt Everywhere: Unsupervised Adaptation of Point-Clouds and Entropy Minimisation for Multi-modal Cardiac Image Segmentation. (arXiv:2103.08219v1 [cs.CV])</h2>
<h3>Sulaiman Vesal, Mingxuan Gu, Ronak Kosti, Andreas Maier, Nishant Ravikumar</h3>
<p>Deep learning models are sensitive to domain shift phenomena. A model trained
on images from one domain cannot generalise well when tested on images from a
different domain, despite capturing similar anatomical structures. It is mainly
because the data distribution between the two domains is different. Moreover,
creating annotation for every new modality is a tedious and time-consuming
task, which also suffers from high inter- and intra- observer variability.
Unsupervised domain adaptation (UDA) methods intend to reduce the gap between
source and target domains by leveraging source domain labelled data to generate
labels for the target domain. However, current state-of-the-art (SOTA) UDA
methods demonstrate degraded performance when there is insufficient data in
source and target domains. In this paper, we present a novel UDA method for
multi-modal cardiac image segmentation. The proposed method is based on
adversarial learning and adapts network features between source and target
domain in different spaces. The paper introduces an end-to-end framework that
integrates: a) entropy minimisation, b) output feature space alignment and c) a
novel point-cloud shape adaptation based on the latent features learned by the
segmentation model. We validated our method on two cardiac datasets by adapting
from the annotated source domain, bSSFP-MRI (balanced Steady-State Free
Procession-MRI), to the unannotated target domain, LGE-MRI (Late-gadolinium
enhance-MRI), for the multi-sequence dataset; and from MRI (source) to CT
(target) for the cross-modality dataset. The results highlighted that by
enforcing adversarial learning in different parts of the network, the proposed
method delivered promising performance, compared to other SOTA methods.
</p>
<a href="http://arxiv.org/abs/2103.08219" target="_blank">arXiv:2103.08219</a> [<a href="http://arxiv.org/pdf/2103.08219" target="_blank">pdf</a>]

<h2>Generating Synthetic Handwritten Historical Documents With OCR Constrained GANs. (arXiv:2103.08236v1 [cs.CV])</h2>
<h3>Lars V&#xf6;gtlin, Manuel Drazyk, Vinaychandran Pondenkandath, Michele Alberti, Rolf Ingold</h3>
<p>We present a framework to generate synthetic historical documents with
precise ground truth using nothing more than a collection of unlabeled
historical images. Obtaining large labeled datasets is often the limiting
factor to effectively use supervised deep learning methods for Document Image
Analysis (DIA). Prior approaches towards synthetic data generation either
require expertise or result in poor accuracy in the synthetic documents. To
achieve high precision transformations without requiring expertise, we tackle
the problem in two steps. First, we create template documents with
user-specified content and structure. Second, we transfer the style of a
collection of unlabeled historical images to these template documents while
preserving their text and layout. We evaluate the use of our synthetic
historical documents in a pre-training setting and find that we outperform the
baselines (randomly initialized and pre-trained). Additionally, with visual
examples, we demonstrate a high-quality synthesis that makes it possible to
generate large labeled historical document datasets with precise ground truth.
</p>
<a href="http://arxiv.org/abs/2103.08236" target="_blank">arXiv:2103.08236</a> [<a href="http://arxiv.org/pdf/2103.08236" target="_blank">pdf</a>]

<h2>Boosting ship detection in SAR images with complementary pretraining techniques. (arXiv:2103.08251v1 [cs.CV])</h2>
<h3>Wei Bao, Meiyu Huang, Yaqin Zhang, Yao Xu, Xuejiao Liu, Xueshuang Xiang</h3>
<p>Deep learning methods have made significant progress in ship detection in
synthetic aperture radar (SAR) images. The pretraining technique is usually
adopted to support deep neural networks-based SAR ship detectors due to the
scarce labeled SAR images. However, directly leveraging ImageNet pretraining is
hardly to obtain a good ship detector because of different imaging perspective
and geometry. In this paper, to resolve the problem of inconsistent imaging
perspective between ImageNet and earth observations, we propose an optical ship
detector (OSD) pretraining technique, which transfers the characteristics of
ships in earth observations to SAR images from a large-scale aerial image
dataset. On the other hand, to handle the problem of different imaging geometry
between optical and SAR images, we propose an optical-SAR matching (OSM)
pretraining technique, which transfers plentiful texture features from optical
images to SAR images by common representation learning on the optical-SAR
matching task. Finally, observing that the OSD pretraining based SAR ship
detector has a better recall on sea area while the OSM pretraining based SAR
ship detector can reduce false alarms on land area, we combine the predictions
of the two detectors through weighted boxes fusion to further improve detection
results. Extensive experiments on four SAR ship detection datasets and two
representative CNN-based detection benchmarks are conducted to show the
effectiveness and complementarity of the two proposed detectors, and the
state-of-the-art performance of the combination of the two detectors. The
proposed method won the sixth place of ship detection in SAR images in 2020
Gaofen challenge.
</p>
<a href="http://arxiv.org/abs/2103.08251" target="_blank">arXiv:2103.08251</a> [<a href="http://arxiv.org/pdf/2103.08251" target="_blank">pdf</a>]

<h2>Learning Compositional Representation for 4D Captures with Neural ODE. (arXiv:2103.08271v1 [cs.CV])</h2>
<h3>Boyan Jiang, Yinda Zhang, Xingkui Wei, Xiangyang Xue, Yanwei Fu</h3>
<p>Learning based representation has become the key to the success of many
computer vision systems. While many 3D representations have been proposed, it
is still an unaddressed problem for how to represent a dynamically changing 3D
object. In this paper, we introduce a compositional representation for 4D
captures, i.e. a deforming 3D object over a temporal span, that disentangles
shape, initial state, and motion respectively. Each component is represented by
a latent code via a trained encoder. To model the motion, a neural Ordinary
Differential Equation (ODE) is trained to update the initial state conditioned
on the learned motion code, and a decoder takes the shape code and the updated
pose code to reconstruct 4D captures at each time stamp. To this end, we
propose an Identity Exchange Training (IET) strategy to encourage the network
to learn effectively decoupling each component. Extensive experiments
demonstrate that the proposed method outperforms existing state-of-the-art deep
learning based methods on 4D reconstruction, and significantly improves on
various tasks, including motion transfer and completion.
</p>
<a href="http://arxiv.org/abs/2103.08271" target="_blank">arXiv:2103.08271</a> [<a href="http://arxiv.org/pdf/2103.08271" target="_blank">pdf</a>]

<h2>Refine Myself by Teaching Myself: Feature Refinement via Self-Knowledge Distillation. (arXiv:2103.08273v1 [cs.CV])</h2>
<h3>Mingi Ji, Seungjae Shin, Seunghyun Hwang, Gibeom Park, Il-Chul Moon</h3>
<p>Knowledge distillation is a method of transferring the knowledge from a
pretrained complex teacher model to a student model, so a smaller network can
replace a large teacher network at the deployment stage. To reduce the
necessity of training a large teacher model, the recent literatures introduced
a self-knowledge distillation, which trains a student network progressively to
distill its own knowledge without a pretrained teacher network. While
Self-knowledge distillation is largely divided into a data augmentation based
approach and an auxiliary network based approach, the data augmentation
approach looses its local information in the augmentation process, which
hinders its applicability to diverse vision tasks, such as semantic
segmentation. Moreover, these knowledge distillation approaches do not receive
the refined feature maps, which are prevalent in the object detection and
semantic segmentation community. This paper proposes a novel self-knowledge
distillation method, Feature Refinement via Self-Knowledge Distillation
(FRSKD), which utilizes an auxiliary self-teacher network to transfer a refined
knowledge for the classifier network. Our proposed method, FRSKD, can utilize
both soft label and feature-map distillations for the self-knowledge
distillation. Therefore, FRSKD can be applied to classification, and semantic
segmentation, which emphasize preserving the local information. We demonstrate
the effectiveness of FRSKD by enumerating its performance improvements in
diverse tasks and benchmark datasets. The implemented code is available at
https://github.com/MingiJi/FRSKD.
</p>
<a href="http://arxiv.org/abs/2103.08273" target="_blank">arXiv:2103.08273</a> [<a href="http://arxiv.org/pdf/2103.08273" target="_blank">pdf</a>]

<h2>Trust Your IMU: Consequences of Ignoring the IMU Drift. (arXiv:2103.08286v1 [cs.CV])</h2>
<h3>Marcus Valtonen &#xd6;rnhag, Patrik Persson, M&#xe5;rten Wadenb&#xe4;ck, Kalle &#xc5;str&#xf6;m, Anders Heyden</h3>
<p>In this paper, we argue that modern pre-integration methods for inertial
measurement units (IMUs) are accurate enough to ignore the drift for short time
intervals. This allows us to consider a simplified camera model, which in turn
admits further intrinsic calibration. We develop the first-ever solver to
jointly solve the relative pose problem with unknown and equal focal length and
radial distortion profile while utilizing the IMU data. Furthermore, we show
significant speed-up compared to state-of-the-art algorithms, with small or
negligible loss in accuracy for partially calibrated setups. The proposed
algorithms are tested on both synthetic and real data, where the latter is
focused on navigation using unmanned aerial vehicles (UAVs). We evaluate the
proposed solvers on different commercially available low-cost UAVs, and
demonstrate that the novel assumption on IMU drift is feasible in real-life
applications. The extended intrinsic auto-calibration enables us to use
distorted input images, making tedious calibration processes obsolete, compared
to current state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2103.08286" target="_blank">arXiv:2103.08286</a> [<a href="http://arxiv.org/pdf/2103.08286" target="_blank">pdf</a>]

<h2>DeepOPG: Improving Orthopantomogram Finding Summarization with Weak Supervision. (arXiv:2103.08290v1 [cs.CV])</h2>
<h3>Tzu-Ming Hsu, Yin-Chih Wang</h3>
<p>Finding summaries from an orthopantomogram, or a dental panoramic radiograph,
has significant potential to improve patient communication and to speed up
clinical judgments. While orthopantomogram is a first-line tool for dental
examinations, no existing work has explored the summarization of findings from
it. A finding summary has to not only find teeth in the imaging study but also
label the teeth with several types of treatments. To tackle the problem, we
develop DeepOPG that breaks the summarization process into functional
segmentation and teeth localization, the latter of which is further refined by
a novel dental coherence module. We also leverage weak supervision labels to
improve detection results in a reinforcement learning scenario. Experiments
show high efficacy of DeepOPG on finding summarization, achieving an overall
AUC of 88.2% in detecting six types of findings. The proposed dental coherence
and weak supervision both are shown to improve DeepOPG by adding 5.9% and 0.4%
to AP@IoU=0.5 respectively.
</p>
<a href="http://arxiv.org/abs/2103.08290" target="_blank">arXiv:2103.08290</a> [<a href="http://arxiv.org/pdf/2103.08290" target="_blank">pdf</a>]

<h2>Rotation Coordinate Descent for Fast Globally Optimal Rotation Averaging. (arXiv:2103.08292v1 [cs.CV])</h2>
<h3>&#xc1;lvaro Parra, Shin-Fang Chng, Tat-Jun Chin, Anders Eriksson, Ian Reid</h3>
<p>Under mild conditions on the noise level of the measurements, rotation
averaging satisfies strong duality, which enables global solutions to be
obtained via semidefinite programming (SDP) relaxation. However, generic
solvers for SDP are rather slow in practice, even on rotation averaging
instances of moderate size, thus developing specialised algorithms is vital. In
this paper, we present a fast algorithm that achieves global optimality called
rotation coordinate descent (RCD). Unlike block coordinate descent (BCD) which
solves SDP by updating the semidefinite matrix in a row-by-row fashion, RCD
directly maintains and updates all valid rotations throughout the iterations.
This obviates the need to store a large dense semidefinite matrix. We
mathematically prove the convergence of our algorithm and empirically show its
superior efficiency over state-of-the-art global methods on a variety of
problem configurations. Maintaining valid rotations also facilitates
incorporating local optimisation routines for further speed-ups. Moreover, our
algorithm is simple to implement; see supplementary material for a
demonstration program.
</p>
<a href="http://arxiv.org/abs/2103.08292" target="_blank">arXiv:2103.08292</a> [<a href="http://arxiv.org/pdf/2103.08292" target="_blank">pdf</a>]

<h2>3D-FFS: Faster 3D object detection with Focused Frustum Search in sensor fusion based networks. (arXiv:2103.08294v1 [cs.CV])</h2>
<h3>Aniruddha Ganguly, Tasin Ishmam, Khandker Aftarul Islam, Md Zahidur Rahman, Md. Shamsuzzoha Bayzid</h3>
<p>In this work we propose 3D-FFS, a novel approach to make sensor fusion based
3D object detection networks significantly faster using a class of
computationally inexpensive heuristics. Existing sensor fusion based networks
generate 3D region proposals by leveraging inferences from 2D object detectors.
However, as images have no depth information, these networks rely on extracting
semantic features of points from the entire scene to locate the object. By
leveraging aggregated intrinsic properties (e.g. point density) of the 3D point
cloud data, 3D-FFS can substantially constrain the 3D search space and thereby
significantly reduce training time, inference time and memory consumption
without sacrificing accuracy. To demonstrate the efficacy of 3D-FFS, we have
integrated it with Frustum ConvNet (F-ConvNet), a prominent sensor fusion based
3D object detection model. We assess the performance of 3D-FFS on the KITTI
dataset. Compared to F-ConvNet, we achieve improvements in training and
inference times by up to 62.84% and 56.46%, respectively, while reducing the
memory usage by up to 58.53%. Additionally, we achieve 0.59%, 2.03% and 3.34%
improvements in accuracy for the Car, Pedestrian and Cyclist classes,
respectively. 3D-FFS shows a lot of promise in domains with limited computing
power, such as autonomous vehicles, drones and robotics where LiDAR-Camera
based sensor fusion perception systems are widely used.
</p>
<a href="http://arxiv.org/abs/2103.08294" target="_blank">arXiv:2103.08294</a> [<a href="http://arxiv.org/pdf/2103.08294" target="_blank">pdf</a>]

<h2>OCNet: Object Context Network for Scene Parsing. (arXiv:1809.00916v4 [cs.CV] UPDATED)</h2>
<h3>Yuhui Yuan, Lang Huang, Jianyuan Guo, Chao Zhang, Xilin Chen, Jingdong Wang</h3>
<p>In this paper, we address the semantic segmentation task with a new context
aggregation scheme named \emph{object context}, which focuses on enhancing the
role of object information. Motivated by the fact that the category of each
pixel is inherited from the object it belongs to, we define the object context
for each pixel as the set of pixels that belong to the same category as the
given pixel in the image. We use a binary relation matrix to represent the
relationship between all pixels, where the value one indicates the two selected
pixels belong to the same category and zero otherwise.

We propose to use a dense relation matrix to serve as a surrogate for the
binary relation matrix. The dense relation matrix is capable to emphasize the
contribution of object information as the relation scores tend to be larger on
the object pixels than the other pixels. Considering that the dense relation
matrix estimation requires quadratic computation overhead and memory
consumption w.r.t. the input size, we propose an efficient interlaced sparse
self-attention scheme to model the dense relations between any two of all
pixels via the combination of two sparse relation matrices.

To capture richer context information, we further combine our interlaced
sparse self-attention scheme with the conventional multi-scale context schemes
including pyramid pooling~\citep{zhao2017pyramid} and atrous spatial pyramid
pooling~\citep{chen2018deeplab}. We empirically show the advantages of our
approach with competitive performances on five challenging benchmarks
including: Cityscapes, ADE20K, LIP, PASCAL-Context and COCO-Stuff
</p>
<a href="http://arxiv.org/abs/1809.00916" target="_blank">arXiv:1809.00916</a> [<a href="http://arxiv.org/pdf/1809.00916" target="_blank">pdf</a>]

<h2>Intra-clip Aggregation for Video Person Re-identification. (arXiv:1905.01722v3 [cs.CV] UPDATED)</h2>
<h3>Takashi Isobe, Jian Han, Fang Zhu, Yali Li, Shengjin Wang</h3>
<p>Video-based person re-identification has drawn massive attention in recent
years due to its extensive applications in video surveillance. While deep
learning-based methods have led to significant progress, these methods are
limited by ineffectively using complementary information, which is blamed on
necessary data augmentation in the training process. Data augmentation has been
widely used to mitigate the over-fitting trap and improve the ability of
network representation. However, the previous methods adopt image-based data
augmentation scheme to individually process the input frames, which corrupts
the complementary information between consecutive frames and causes performance
degradation. Extensive experiments on three benchmark datasets demonstrate that
our framework outperforms the most recent state-of-the-art methods. We also
perform cross-dataset validation to prove the generality of our method.
</p>
<a href="http://arxiv.org/abs/1905.01722" target="_blank">arXiv:1905.01722</a> [<a href="http://arxiv.org/pdf/1905.01722" target="_blank">pdf</a>]

<h2>A Novel Approach for Robust Multi Human Action Recognition and Summarization based on 3D Convolutional Neural Networks. (arXiv:1907.11272v4 [cs.CV] UPDATED)</h2>
<h3>Noor Almaadeed, Omar Elharrouss, Somaya Al-Maadeed, Ahmed Bouridane, Azeddine Beghdadi</h3>
<p>Human actions in videos are 3D signals. However, there are a few methods
available for multiple human action recognition. For long videos, it's
difficult to search within a video for a specific action and/or person. For
that, this paper proposes a new technic for multiple human action recognition
and summarization for surveillance videos. The proposed approach proposes a new
representation of the data by extracting the sequence of each person from the
scene. This is followed by an analysis of each sequence to detect and recognize
the corresponding actions using 3D convolutional neural networks (3DCNNs).
Action-based video summarization is performed by saving each person's action at
each time of the video. Results of this work revealed that the proposed method
provides accurate multi human action recognition that easily used for
summarization of any action. Further, for other videos that can be collected
from the internet, which are complex and not built for surveillance
applications, the proposed model was evaluated on some datasets like UCF101 and
YouTube without any preprocessing. For this category of videos, the
summarization is performed on the video sequences by summarizing the actions in
each subsequence. The results obtained demonstrate its efficiency compared to
state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/1907.11272" target="_blank">arXiv:1907.11272</a> [<a href="http://arxiv.org/pdf/1907.11272" target="_blank">pdf</a>]

<h2>DeepUSPS: Deep Robust Unsupervised Saliency Prediction With Self-Supervision. (arXiv:1909.13055v4 [cs.CV] UPDATED)</h2>
<h3>Duc Tam Nguyen, Maximilian Dax, Chaithanya Kumar Mummadi, Thi Phuong Nhung Ngo, Thi Hoai Phuong Nguyen, Zhongyu Lou, Thomas Brox</h3>
<p>Deep neural network (DNN) based salient object detection in images based on
high-quality labels is expensive. Alternative unsupervised approaches rely on
careful selection of multiple handcrafted saliency methods to generate noisy
pseudo-ground-truth labels. In this work, we propose a two-stage mechanism for
robust unsupervised object saliency prediction, where the first stage involves
refinement of the noisy pseudo labels generated from different handcrafted
methods. Each handcrafted method is substituted by a deep network that learns
to generate the pseudo labels. These labels are refined incrementally in
multiple iterations via our proposed self-supervision technique. In the second
stage, the refined labels produced from multiple networks representing multiple
saliency methods are used to train the actual saliency detection network. We
show that this self-learning procedure outperforms all the existing
unsupervised methods over different datasets. Results are even comparable to
those of fully-supervised state-of-the-art approaches. The code is available at
https://tinyurl.com/wtlhgo3 .
</p>
<a href="http://arxiv.org/abs/1909.13055" target="_blank">arXiv:1909.13055</a> [<a href="http://arxiv.org/pdf/1909.13055" target="_blank">pdf</a>]

<h2>Rotation Differential Invariants of Images Generated by Two Fundamental Differential Operators. (arXiv:1911.05327v2 [cs.CV] UPDATED)</h2>
<h3>Hanlin Mo, Hua Li</h3>
<p>In this paper, we design two fundamental differential operators for the
derivation of rotation differential invariants of images. Each differential
invariant obtained by using the new method can be expressed as a homogeneous
polynomial of image partial derivatives, which preserve their values when the
image is rotated by arbitrary angles. We produce all possible instances of
homogeneous invariants up to the given order and degree, and discuss the
independence of them in detail. As far as we know, no previous papers have
published so many explicit forms of high-order rotation differential invariants
of images. In the experimental part, texture classification and image patch
verification are carried out on popular real databases. These rotation
differential invariants are used as image feature vector. We mainly evaluate
the effects of various factors on the performance of them. The experimental
results also validate that they have better performance than some commonly used
image features in some cases.
</p>
<a href="http://arxiv.org/abs/1911.05327" target="_blank">arXiv:1911.05327</a> [<a href="http://arxiv.org/pdf/1911.05327" target="_blank">pdf</a>]

<h2>Interpretable Goal-based Prediction and Planning for Autonomous Driving. (arXiv:2002.02277v3 [cs.RO] UPDATED)</h2>
<h3>Stefano V. Albrecht, Cillian Brewitt, John Wilhelm, Balint Gyevnar, Francisco Eiras, Mihai Dobre, Subramanian Ramamoorthy</h3>
<p>We propose an integrated prediction and planning system for autonomous
driving which uses rational inverse planning to recognise the goals of other
vehicles. Goal recognition informs a Monte Carlo Tree Search (MCTS) algorithm
to plan optimal maneuvers for the ego vehicle. Inverse planning and MCTS
utilise a shared set of defined maneuvers and macro actions to construct plans
which are explainable by means of rationality principles. Evaluation in
simulations of urban driving scenarios demonstrate the system's ability to
robustly recognise the goals of other vehicles, enabling our vehicle to exploit
non-trivial opportunities to significantly reduce driving times. In each
scenario, we extract intuitive explanations for the predictions which justify
the system's decisions.
</p>
<a href="http://arxiv.org/abs/2002.02277" target="_blank">arXiv:2002.02277</a> [<a href="http://arxiv.org/pdf/2002.02277" target="_blank">pdf</a>]

<h2>Proficiency Constrained Multi-Agent Reinforcement Learning for Environment-Adaptive Multi UAV-UGV Teaming. (arXiv:2002.03910v2 [cs.RO] UPDATED)</h2>
<h3>Qifei Yu, Zhexin Shen, Yijiang Pang, Rui Liu</h3>
<p>A mixed aerial and ground robot team, which includes both unmanned ground
vehicles (UGVs) and unmanned aerial vehicles (UAVs), is widely used for
disaster rescue, social security, precision agriculture, and military missions.
However, team capability and corresponding configuration vary since robots have
different motion speeds, perceiving ranges, reaching areas, and resilient
capabilities to the dynamic environment. Due to heterogeneous robots inside a
team and the resilient capabilities of robots, it is challenging to perform a
task with an optimal balance between reasonable task allocations and maximum
utilization of robot capability. To address this challenge for effective mixed
ground and aerial teaming, this paper developed a novel teaming method,
proficiency aware multi-agent deep reinforcement learning (Mix-RL), to guide
ground and aerial cooperation by considering the best alignments between robot
capabilities, task requirements, and environment conditions. Mix-RL largely
exploits robot capabilities while being aware of the adaption of robot
capabilities to task requirements and environment conditions. Mix-RL's
effectiveness in guiding mixed teaming was validated with the task "social
security for criminal vehicle tracking".
</p>
<a href="http://arxiv.org/abs/2002.03910" target="_blank">arXiv:2002.03910</a> [<a href="http://arxiv.org/pdf/2002.03910" target="_blank">pdf</a>]

<h2>Image Restoration for Under-Display Camera. (arXiv:2003.04857v2 [cs.CV] UPDATED)</h2>
<h3>Yuqian Zhou, David Ren, Neil Emerton, Sehoon Lim, Timothy Large</h3>
<p>The new trend of full-screen devices encourages us to position a camera
behind a screen. Removing the bezel and centralizing the camera under the
screen brings larger display-to-body ratio and enhances eye contact in video
chat, but also causes image degradation. In this paper, we focus on a
newly-defined Under-Display Camera (UDC), as a novel real-world single image
restoration problem. First, we take a 4k Transparent OLED (T-OLED) and a phone
Pentile OLED (P-OLED) and analyze their optical systems to understand the
degradation. Second, we design a Monitor-Camera Imaging System (MCIS) for
easier real pair data acquisition, and a model-based data synthesizing pipeline
to generate Point Spread Function (PSF) and UDC data only from display pattern
and camera measurements. Finally, we resolve the complicated degradation using
deconvolution-based pipeline and learning-based methods. Our model demonstrates
a real-time high-quality restoration. The presented methods and results reveal
the promising research values and directions of UDC.
</p>
<a href="http://arxiv.org/abs/2003.04857" target="_blank">arXiv:2003.04857</a> [<a href="http://arxiv.org/pdf/2003.04857" target="_blank">pdf</a>]

<h2>CinemAirSim: A Camera-Realistic Robotics Simulator for Cinematographic Purposes. (arXiv:2003.07664v3 [cs.RO] UPDATED)</h2>
<h3>Pablo Pueyo, Eric Cristofalo, Eduardo Montijano, Mac Schwager</h3>
<p>Drones and Unmanned Aerial Vehicles (UAV's) are becoming increasingly popular
in the film and entertainment industries in part because of their
maneuverability and the dynamic shots and perspectives they enable. While there
exists methods for controlling the position and orientation of the drones for
visibility, other artistic elements of the filming process, such as focal blur
and light control, remain unexplored in the robotics community. The lack of
cinemetographic robotics solutions is partly due to the cost associated with
the cameras and devices used in the filming industry, but also because
state-of-the-art photo-realistic robotics simulators only utilize a full
in-focus pinhole camera model which does incorporate these desired artistic
attributes. To overcome this, the main contribution of this work is to endow
the well-known drone simulator, AirSim, with a cinematic camera as well as
extended its API to control all of its parameters in real time, including
various filming lenses and common cinematographic properties. In this paper, we
detail the implementation of our AirSim modification, CinemAirSim, present
examples that illustrate the potential of the new tool, and highlight the new
research opportunities that the use of cinematic cameras can bring to research
in robotics and control. https://github.com/ppueyor/CinematicAirSim
</p>
<a href="http://arxiv.org/abs/2003.07664" target="_blank">arXiv:2003.07664</a> [<a href="http://arxiv.org/pdf/2003.07664" target="_blank">pdf</a>]

<h2>Dynamic Region-Aware Convolution. (arXiv:2003.12243v3 [cs.CV] UPDATED)</h2>
<h3>Jin Chen, Xijun Wang, Zichao Guo, Xiangyu Zhang, Jian Sun</h3>
<p>We propose a new convolution called Dynamic Region-Aware Convolution
(DRConv), which can automatically assign multiple filters to corresponding
spatial regions where features have similar representation. In this way, DRConv
outperforms standard convolution in modeling semantic variations. Standard
convolutional layer can increase the number of filers to extract more visual
elements but results in high computational cost. More gracefully, our DRConv
transfers the increasing channel-wise filters to spatial dimension with
learnable instructor, which not only improve representation ability of
convolution, but also maintains computational cost and the
translation-invariance as standard convolution dose. DRConv is an effective and
elegant method for handling complex and variable spatial information
distribution. It can substitute standard convolution in any existing networks
for its plug-and-play property, especially to power convolution layers in
efficient networks. We evaluate DRConv on a wide range of models (MobileNet
series, ShuffleNetV2, etc.) and tasks (Classification, Face Recognition,
Detection and Segmentation). On ImageNet classification, DRConv-based
ShuffleNetV2-0.5x achieves state-of-the-art performance of 67.1% at 46M
multiply-adds level with 6.3% relative improvement.
</p>
<a href="http://arxiv.org/abs/2003.12243" target="_blank">arXiv:2003.12243</a> [<a href="http://arxiv.org/pdf/2003.12243" target="_blank">pdf</a>]

<h2>Explainable Goal-Driven Agents and Robots -- A Comprehensive Review. (arXiv:2004.09705v3 [cs.RO] UPDATED)</h2>
<h3>Fatai Sado, Chu Kiong Loo, Wei Shiung Liew, Matthias Kerzel, Stefan Wermter</h3>
<p>Recent applications of autonomous agents and robots, such as self-driving
cars, scenario-based trainers, exploration robots, and service robots have
brought attention to crucial trust-related challenges associated with the
current generation of artificial intelligence (AI) systems. AI systems based on
the connectionist deep learning neural network approach lack capabilities of
explaining their decisions and actions to others, despite their great
successes. Without symbolic interpretation capabilities, they are black boxes,
which renders their decisions or actions opaque, making it difficult to trust
them in safety-critical applications. The recent stance on the explainability
of AI systems has witnessed several approaches on eXplainable Artificial
Intelligence (XAI); however, most of the studies have focused on data-driven
XAI systems applied in computational sciences. Studies addressing the
increasingly pervasive goal-driven agents and robots are still missing. This
paper reviews approaches on explainable goal-driven intelligent agents and
robots, focusing on techniques for explaining and communicating agents
perceptual functions (example, senses, and vision) and cognitive reasoning
(example, beliefs, desires, intention, plans, and goals) with humans in the
loop. The review highlights key strategies that emphasize transparency,
understandability, and continual learning for explainability. Finally, the
paper presents requirements for explainability and suggests a roadmap for the
possible realization of effective goal-driven explainable agents and robots.
</p>
<a href="http://arxiv.org/abs/2004.09705" target="_blank">arXiv:2004.09705</a> [<a href="http://arxiv.org/pdf/2004.09705" target="_blank">pdf</a>]

<h2>OF-VO: Efficient Navigation among Pedestrians Using Commodity Sensors. (arXiv:2004.10976v6 [cs.RO] UPDATED)</h2>
<h3>Jing Liang, Yi-Ling Qiao, Tianrui Guan, Dinesh Manocha</h3>
<p>We present a modified velocity-obstacle (VO) algorithm that uses
probabilistic partial observations of the environment to compute velocities and
navigate a robot to a target. Our system uses commodity visual sensors,
including a mono-camera and a 2D Lidar, to explicitly predict the velocities
and positions of surrounding obstacles through optical flow estimation, object
detection, and sensor fusion. A key aspect of our work is coupling the
perception (OF: optical flow) and planning (VO) components for reliable
navigation. Overall, our OF-VO algorithm using learning-based perception and
model-based planning methods offers better performance than prior algorithms in
terms of navigation time and success rate of collision avoidance. Our method
also provides bounds on the probabilistic collision avoidance algorithm. We
highlight the realtime performance of OF-VO on a Turtlebot navigating among
pedestrians in both simulated and real-world scenes. A demo video is available
at https://gamma.umd.edu/ofvo/
</p>
<a href="http://arxiv.org/abs/2004.10976" target="_blank">arXiv:2004.10976</a> [<a href="http://arxiv.org/pdf/2004.10976" target="_blank">pdf</a>]

<h2>Dynamic Scale Training for Object Detection. (arXiv:2004.12432v2 [cs.CV] UPDATED)</h2>
<h3>Yukang Chen, Peizhen Zhang, Zeming Li, Yanwei Li, Xiangyu Zhang, Lu Qi, Jian Sun, Jiaya Jia</h3>
<p>We propose a Dynamic Scale Training paradigm (abbreviated as DST) to mitigate
scale variation challenge in object detection. Previous strategies like image
pyramid, multi-scale training, and their variants are aiming at preparing
scale-invariant data for model optimization. However, the preparation procedure
is unaware of the following optimization process that restricts their
capability in handling the scale variation. Instead, in our paradigm, we use
feedback information from the optimization process to dynamically guide the
data preparation. The proposed method is surprisingly simple yet obtains
significant gains (2%+ Average Precision on MS COCO dataset), outperforming
previous methods. Experimental results demonstrate the efficacy of our proposed
DST method towards scale variation handling. It could also generalize to
various backbones, benchmarks, and other challenging downstream tasks like
instance segmentation. It does not introduce inference overhead and could serve
as a free lunch for general detection configurations. Besides, it also
facilitates efficient training due to fast convergence. Code and models are
available at github.com/yukang2017/Stitcher.
</p>
<a href="http://arxiv.org/abs/2004.12432" target="_blank">arXiv:2004.12432</a> [<a href="http://arxiv.org/pdf/2004.12432" target="_blank">pdf</a>]

<h2>Self-supervised Keypoint Correspondences for Multi-Person Pose Estimation and Tracking in Videos. (arXiv:2004.12652v3 [cs.CV] UPDATED)</h2>
<h3>Umer Rafi, Andreas Doering, Bastian Leibe, Juergen Gall</h3>
<p>Video annotation is expensive and time consuming. Consequently, datasets for
multi-person pose estimation and tracking are less diverse and have more sparse
annotations compared to large scale image datasets for human pose estimation.
This makes it challenging to learn deep learning based models for associating
keypoints across frames that are robust to nuisance factors such as motion blur
and occlusions for the task of multi-person pose tracking. To address this
issue, we propose an approach that relies on keypoint correspondences for
associating persons in videos. Instead of training the network for estimating
keypoint correspondences on video data, it is trained on a large scale image
datasets for human pose estimation using self-supervision. Combined with a
top-down framework for human pose estimation, we use keypoints correspondences
to (i) recover missed pose detections (ii) associate pose detections across
video frames. Our approach achieves state-of-the-art results for multi-frame
pose estimation and multi-person pose tracking on the PosTrack $2017$ and
PoseTrack $2018$ data sets.
</p>
<a href="http://arxiv.org/abs/2004.12652" target="_blank">arXiv:2004.12652</a> [<a href="http://arxiv.org/pdf/2004.12652" target="_blank">pdf</a>]

<h2>Human in Events: A Large-Scale Benchmark for Human-centric Video Analysis in Complex Events. (arXiv:2005.04490v5 [cs.CV] UPDATED)</h2>
<h3>Weiyao Lin, Huabin Liu, Shizhan Liu, Yuxi Li, Rui Qian, Tao Wang, Ning Xu, Hongkai Xiong, Guo-Jun Qi, Nicu Sebe</h3>
<p>Along with the development of modern smart cities, human-centric video
analysis has been encountering the challenge of analyzing diverse and complex
events in real scenes. A complex event relates to dense crowds, anomalous, or
collective behaviors. However, limited by the scale of existing video datasets,
few human analysis approaches have reported their performance on such complex
events. To this end, we present a new large-scale dataset, named
Human-in-Events or HiEve (Human-centric video analysis in complex Events), for
the understanding of human motions, poses, and actions in a variety of
realistic events, especially in crowd and complex events. It contains a record
number of poses (&gt;1M), the largest number of action instances (&gt;56k) under
complex events, as well as one of the largest numbers of trajectories lasting
for longer time (with an average trajectory length of &gt;480 frames). Based on
this dataset, we present an enhanced pose estimation baseline by utilizing the
potential of action information to guide the learning of more powerful 2D pose
features. We demonstrate that the proposed method is able to boost the
performance of existing pose estimation pipelines on our HiEve dataset.
Furthermore, we conduct extensive experiments to benchmark recent video
analysis approaches together with our baseline methods, demonstrating that
HiEve is a challenging dataset for human-centric video analysis. We expect that
the dataset will advance the development of cutting-edge techniques in
human-centric analysis and the understanding of complex events. The dataset is
available at this http URL
</p>
<a href="http://arxiv.org/abs/2005.04490" target="_blank">arXiv:2005.04490</a> [<a href="http://arxiv.org/pdf/2005.04490" target="_blank">pdf</a>]

<h2>Dance Revolution: Long-Term Dance Generation with Music via Curriculum Learning. (arXiv:2006.06119v7 [cs.CV] UPDATED)</h2>
<h3>Ruozi Huang, Huang Hu, Wei Wu, Kei Sawada, Mi Zhang, Daxin Jiang</h3>
<p>Dancing to music is one of human's innate abilities since ancient times. In
machine learning research, however, synthesizing dance movements from music is
a challenging problem. Recently, researchers synthesize human motion sequences
through autoregressive models like recurrent neural network (RNN). Such an
approach often generates short sequences due to an accumulation of prediction
errors that are fed back into the neural network. This problem becomes even
more severe in the long motion sequence generation. Besides, the consistency
between dance and music in terms of style, rhythm and beat is yet to be taken
into account during modeling. In this paper, we formalize the music-conditioned
dance generation as a sequence-to-sequence learning problem and devise a novel
seq2seq architecture to efficiently process long sequences of music features
and capture the fine-grained correspondence between music and dance.
Furthermore, we propose a novel curriculum learning strategy to alleviate error
accumulation of autoregressive models in long motion sequence generation, which
gently changes the training process from a fully guided teacher-forcing scheme
using the previous ground-truth movements, towards a less guided autoregressive
scheme mostly using the generated movements instead. Extensive experiments show
that our approach significantly outperforms the existing state-of-the-arts on
automatic metrics and human evaluation. We also make a demo video to
demonstrate the superior performance of our proposed approach at
https://www.youtube.com/watch?v=lmE20MEheZ8.
</p>
<a href="http://arxiv.org/abs/2006.06119" target="_blank">arXiv:2006.06119</a> [<a href="http://arxiv.org/pdf/2006.06119" target="_blank">pdf</a>]

<h2>Robot Inner Attention Modeling for Task-Adaptive Teaming of Heterogeneous Multi Robots. (arXiv:2006.15482v2 [cs.RO] UPDATED)</h2>
<h3>Chao Huang, Rui Liu</h3>
<p>Attracted by team scale and function diversity, a heterogeneous multi-robot
system (HMRS), where multiple robots with different functions and numbers are
coordinated to perform tasks, has been widely used for complex and large-scale
scenarios, including disaster search and rescue, site surveillance, and social
security. However, due to the variety of the task requirements, it is
challenging to accurately compose a robot team with appropriate sizes and
functions to dynamically satisfy task needs while limiting the robot resource
cost to a low level. To solve this problem, in this paper, a novel adaptive
cooperation method, inner attention (innerATT), is developed to flexibly team
heterogeneous robots to execute tasks as task types and environment change.
innerATT is designed by integrating a novel attention mechanism into a
multi-agent actor-critic reinforcement learning architecture. With an attention
mechanism, robot capability will be analyzed to flexibly form teams to meet
task requirements. Scenarios with different task variety ("Single Task",
"Double Task", and "Mixed Task") were designed. The effectiveness of the
innerATT was validated by its accuracy in flexible cooperation.
</p>
<a href="http://arxiv.org/abs/2006.15482" target="_blank">arXiv:2006.15482</a> [<a href="http://arxiv.org/pdf/2006.15482" target="_blank">pdf</a>]

<h2>Enhancement of damaged-image prediction through Cahn-Hilliard Image Inpainting. (arXiv:2007.10753v2 [cs.CV] UPDATED)</h2>
<h3>Jos&#xe9; A. Carrillo, Serafim Kalliadasis, Fuyue Liang, Sergio P. Perez</h3>
<p>We assess the benefit of including an image inpainting filter before passing
damaged images into a classification neural network. For this we employ a
modified Cahn-Hilliard equation as an image inpainting filter, which is solved
via a finite volume scheme with reduced computational cost and adequate
properties for energy stability and boundedness. The benchmark dataset employed
here is MNIST, which consists of binary images of handwritten digits and is a
standard dataset to validate image-processing methodologies. We train a neural
network based of dense layers with the training set of MNIST, and subsequently
we contaminate the test set with damage of different types and intensities. We
then compare the prediction accuracy of the neural network with and without
applying the Cahn-Hilliard filter to the damaged images test. Our results
quantify the significant improvement of damaged-image prediction due to
applying the Cahn-Hilliard filter, which for specific damages can increase up
to 50% and is in general advantageous for low to moderate damage.
</p>
<a href="http://arxiv.org/abs/2007.10753" target="_blank">arXiv:2007.10753</a> [<a href="http://arxiv.org/pdf/2007.10753" target="_blank">pdf</a>]

<h2>Unsupervised Learning for Identifying Events in Active Target Experiments. (arXiv:2008.02757v3 [cs.CV] UPDATED)</h2>
<h3>Robert Solli, Daniel Bazin, Michelle P. Kuchera, Ryan R. Strauss, Morten Hjorth-Jensen</h3>
<p>This article presents novel applications of unsupervised machine learning
methods to the problem of event separation in an active target detector, the
Active-Target Time Projection Chamber (AT-TPC). The overarching goal is to
group similar events in the early stages of the data analysis, thereby
improving efficiency by limiting the computationally expensive processing of
unnecessary events. The application of unsupervised clustering algorithms to
the analysis of two-dimensional projections of particle tracks from a resonant
proton scattering experiment on $^{46}$Ar is introduced. We explore the
performance of autoencoder neural networks and a pre-trained VGG16
convolutional neural network. We study clustering performance on both data from
a simulated $^{46}$Ar experiment, and real events from the AT-TPC detector. We
find that a $k$-means algorithm applied to simulated data in the VGG16 latent
space forms almost perfect clusters. Additionally, the VGG16+$k$-means approach
finds high purity clusters of proton events for real experimental data. We also
explore the application of clustering the latent space of autoencoder neural
networks for event separation. While these networks show strong performance,
they suffer from high variability in their results.
</p>
<a href="http://arxiv.org/abs/2008.02757" target="_blank">arXiv:2008.02757</a> [<a href="http://arxiv.org/pdf/2008.02757" target="_blank">pdf</a>]

<h2>Transform Quantization for CNN Compression. (arXiv:2009.01174v2 [cs.CV] UPDATED)</h2>
<h3>Sean I. Young, Wang Zhe, David Taubman, Bernd Girod</h3>
<p>In this paper, we compress convolutional neural network (CNN) weights
post-training via transform quantization. Previous CNN quantization techniques
tend to ignore the joint statistics of weights and activations, producing
sub-optimal CNN performance at a given quantization bit-rate, or consider their
joint statistics during training only and do not facilitate efficient
compression of already trained CNN models. We optimally transform (decorrelate)
and quantize the weights post-training using a rate-distortion framework to
improve compression at any given quantization bit-rate. Transform quantization
unifies quantization and dimensionality reduction (decorrelation) techniques in
a single framework to facilitate low bit-rate compression of CNNs and efficient
inference in the transform domain. We first introduce a theory of rate and
distortion for CNN quantization, and pose optimum quantization as a
rate-distortion optimization problem. We then show that this problem can be
solved using optimal bit-depth allocation following decorrelation by the
optimal End-to-end Learned Transform (ELT) we derive in this paper. Experiments
demonstrate that transform quantization advances the state of the art in CNN
compression in both retrained and non-retrained quantization scenarios. In
particular, we find that transform quantization with retraining is able to
compress CNN models such as AlexNet, ResNet and DenseNet to very low bit-rates
(1-2 bits).
</p>
<a href="http://arxiv.org/abs/2009.01174" target="_blank">arXiv:2009.01174</a> [<a href="http://arxiv.org/pdf/2009.01174" target="_blank">pdf</a>]

<h2>Recognizing Micro-Expression in Video Clip with Adaptive Key-Frame Mining. (arXiv:2009.09179v3 [cs.CV] UPDATED)</h2>
<h3>Min Peng, Chongyang Wang, Yuan Gao, Tao Bi, Tong Chen, Yu Shi, Xiang-Dong Zhou</h3>
<p>As a spontaneous expression of emotion on face, micro-expression reveals the
underlying emotion that cannot be controlled by human. In micro-expression,
facial movement is transient and sparsely localized through time. However, the
existing representation based on various deep learning techniques learned from
a full video clip is usually redundant. In addition, methods utilizing the
single apex frame of each video clip require expert annotations and sacrifice
the temporal dynamics. To simultaneously localize and recognize such fleeting
facial movements, we propose a novel end-to-end deep learning architecture,
referred to as adaptive key-frame mining network (AKMNet). Operating on the
video clip of micro-expression, AKMNet is able to learn discriminative
spatio-temporal representation by combining spatial features of self-learned
local key frames and their global-temporal dynamics. Theoretical analysis and
empirical evaluation show that the proposed approach improved recognition
accuracy in comparison with state-of-the-art methods on multiple benchmark
datasets.
</p>
<a href="http://arxiv.org/abs/2009.09179" target="_blank">arXiv:2009.09179</a> [<a href="http://arxiv.org/pdf/2009.09179" target="_blank">pdf</a>]

<h2>MSR-DARTS: Minimum Stable Rank of Differentiable Architecture Search. (arXiv:2009.09209v2 [cs.CV] UPDATED)</h2>
<h3>Kengo Machida, Kuniaki Uto, Koichi Shinoda, Taiji Suzuki</h3>
<p>In neural architecture search (NAS), differentiable architecture search
(DARTS) has recently attracted much attention due to its high efficiency. It
defines an over-parameterized network with mixed edges, each of which
represents all operator candidates, and jointly optimizes the weights of the
network and its architecture in an alternating manner. However, this method
finds a model with the weights converging faster than the others, and such a
model with fastest convergence often leads to overfitting. Accordingly, the
resulting model cannot always be well-generalized. To overcome this problem, we
propose a method called minimum stable rank DARTS (MSR-DARTS), for finding a
model with the best generalization error by replacing architecture optimization
with the selection process using the minimum stable rank criterion.
Specifically, a convolution operator is represented by a matrix, and MSR-DARTS
selects the one with the smallest stable rank. We evaluated MSR-DARTS on
CIFAR-10 and ImageNet datasets. It achieves an error rate of 2.54% with 4.0M
parameters within 0.3 GPU-days on CIFAR-10, and a top-1 error rate of 23.9% on
ImageNet. The official code is available at
https://github.com/mtaecchhi/msrdarts.git.
</p>
<a href="http://arxiv.org/abs/2009.09209" target="_blank">arXiv:2009.09209</a> [<a href="http://arxiv.org/pdf/2009.09209" target="_blank">pdf</a>]

<h2>EqCo: Equivalent Rules for Self-supervised Contrastive Learning. (arXiv:2010.01929v3 [cs.CV] UPDATED)</h2>
<h3>Benjin Zhu, Junqiang Huang, Zeming Li, Xiangyu Zhang, Jian Sun</h3>
<p>In this paper, we propose a method, named EqCo (Equivalent Rules for
Contrastive Learning), to make self-supervised learning irrelevant to the
number of negative samples in InfoNCE-based contrastive learning frameworks.
Inspired by the InfoMax principle, we point that the margin term in contrastive
loss needs to be adaptively scaled according to the number of negative pairs in
order to keep steady mutual information bound and gradient magnitude. EqCo
bridges the performance gap among a wide range of negative sample sizes, so
that we can use only a few negative pairs (e.g. 16 per query) to perform
self-supervised contrastive training on large-scale vision datasets like
ImageNet, while with almost no accuracy drop. This is quite a contrast to the
widely used large batch training or memory bank mechanism in current practices.
Equipped with EqCo, our simplified MoCo (SiMo) achieves comparable accuracy
with MoCo v2 on ImageNet (linear evaluation protocol) while only involves 4
negative pairs per query instead of 65536, suggesting that large quantities of
negative samples might not be a critical factor in InfoNCE loss.
</p>
<a href="http://arxiv.org/abs/2010.01929" target="_blank">arXiv:2010.01929</a> [<a href="http://arxiv.org/pdf/2010.01929" target="_blank">pdf</a>]

<h2>Robust Behavioral Cloning for Autonomous Vehicles using End-to-End Imitation Learning. (arXiv:2010.04767v3 [cs.RO] UPDATED)</h2>
<h3>Tanmay Vilas Samak, Chinmay Vilas Samak, Sivanathan Kandhasamy</h3>
<p>In this work, we present a lightweight pipeline for robust behavioral cloning
of a human driver using end-to-end imitation learning. The proposed pipeline
was employed to train and deploy three distinct driving behavior models onto a
simulated vehicle. The training phase comprised of data collection, balancing,
augmentation, preprocessing and training a neural network, following which, the
trained model was deployed onto the ego vehicle to predict steering commands
based on the feed from an onboard camera. A novel coupled control law was
formulated to generate longitudinal control commands on-the-go based on the
predicted steering angle and other parameters such as actual speed of the ego
vehicle and the prescribed constraints for speed and steering. We analyzed
computational efficiency of the pipeline and evaluated robustness of the
trained models through exhaustive experimentation during the deployment phase.
We also compared our approach against state-of-the-art implementation in order
to comment on its validity.
</p>
<a href="http://arxiv.org/abs/2010.04767" target="_blank">arXiv:2010.04767</a> [<a href="http://arxiv.org/pdf/2010.04767" target="_blank">pdf</a>]

<h2>Robust Footstep Planning and LQR Control for Dynamic Quadrupedal Locomotion. (arXiv:2010.12326v2 [cs.RO] UPDATED)</h2>
<h3>Guiyang Xin, Songyan Xin, Oguzhan Cebe, Mathew Jose Pollayil, Franco Angelini, Manolo Garabini, Sethu Vijayakumar, Michael Mistry</h3>
<p>In this paper, we aim to improve the robustness of dynamic quadrupedal
locomotion through two aspects: 1) fast model predictive foothold planning, and
2) applying LQR to projected inverse dynamic control for robust motion
tracking. In our proposed planning and control framework, foothold plans are
updated at 400 Hz considering the current robot state and an LQR controller
generates optimal feedback gains for motion tracking. The LQR optimal gain
matrix with non-zero off-diagonal elements leverages the coupling of dynamics
to compensate for system underactuation. Meanwhile, the projected inverse
dynamic control complements the LQR to satisfy inequality constraints. In
addition to these contributions, we show robustness of our control framework to
unmodeled adaptive feet. Experiments on the quadruped ANYmal demonstrate the
effectiveness of the proposed method for robust dynamic locomotion given
external disturbances and environmental uncertainties.
</p>
<a href="http://arxiv.org/abs/2010.12326" target="_blank">arXiv:2010.12326</a> [<a href="http://arxiv.org/pdf/2010.12326" target="_blank">pdf</a>]

<h2>Distributed Multi-Target Tracking in Camera Networks. (arXiv:2010.13701v2 [cs.CV] UPDATED)</h2>
<h3>Sara Casao, Abel Naya, Ana C. Murillo, Eduardo Montijano</h3>
<p>Most recent works on multi-target tracking with multiple cameras focus on
centralized systems. In contrast, this paper presents a multi-target tracking
approach implemented in a distributed camera network. The advantages of
distributed systems lie in lighter communication management, greater robustness
to failures and local decision making. On the other hand, data association and
information fusion are more challenging than in a centralized setup, mostly due
to the lack of global and complete information. The proposed algorithm boosts
the benefits of the Distributed-Consensus Kalman Filter with the support of a
re-identification network and a distributed tracker manager module to
facilitate consistent information. These techniques complement each other and
facilitate the cross-camera data association in a simple and effective manner.
We evaluate the whole system with known public data sets under different
conditions demonstrating the advantages of combining all the modules. In
addition, we compare our algorithm to some existing centralized tracking
methods, outperforming their behavior in terms of accuracy and bandwidth usage.
</p>
<a href="http://arxiv.org/abs/2010.13701" target="_blank">arXiv:2010.13701</a> [<a href="http://arxiv.org/pdf/2010.13701" target="_blank">pdf</a>]

<h2>A comparison of LiDAR-based SLAM systems for Control of Unmanned Aerial Vehicles. (arXiv:2011.02306v2 [cs.RO] UPDATED)</h2>
<h3>Robert Milijas, Lovro Markovic, Antun Ivanovic, Frano Petric, Stjepan Bogdan</h3>
<p>This paper investigates the use of LiDAR SLAM as a pose feedback for
autonomous flight. Cartographer, LOAM and HDL graph SLAM are first introduced
on a conceptual level and later tested for this role. They are first compared
offline on a series of datasets to see if they are capable of producing
high-quality pose estimates in agile and long-range flight scenarios. The
second stage of testing consists of integrating the SLAM algorithms into a
cascade PID UAV control system and comparing the control system performance on
step excitation signals and helical trajectories. The comparison is based on
step response characteristics and several time integral performancecriteria as
well as the RMS error between planned and executed trajectory.
</p>
<a href="http://arxiv.org/abs/2011.02306" target="_blank">arXiv:2011.02306</a> [<a href="http://arxiv.org/pdf/2011.02306" target="_blank">pdf</a>]

<h2>B-GAP: Behavior-Guided Action Prediction and Navigation for Autonomous Driving. (arXiv:2011.03748v3 [cs.RO] UPDATED)</h2>
<h3>Angelos Mavrogiannis, Rohan Chandra, Dinesh Manocha</h3>
<p>We present an algorithm for behaviorally-guided action prediction and local
navigation for autonomous driving in dense traffic scenarios. Our approach
classifies the driver behavior of other vehicles or road-agents (aggressive or
conservative) and considers that information for decision-making and safe
driving. We present a behavior-driven simulator that can generate trajectories
corresponding to different levels of aggressive behaviors, and we use our
simulator to train a reinforcement learning policy using a multilayer
perceptron neural network. We use our reinforcement learning-based navigation
scheme to compute safe trajectories for the ego-vehicle accounting for
aggressive driver maneuvers such as overtaking, over-speeding, weaving, and
sudden lane changes. We have integrated our algorithm with the OpenAI gym-based
``Highway-Env'' simulator and demonstrate the benefits of our navigation
algorithm in terms of reducing collisions by $3.25 - 26.90$% and handling
scenarios with $2.5 \times$ higher traffic density.
</p>
<a href="http://arxiv.org/abs/2011.03748" target="_blank">arXiv:2011.03748</a> [<a href="http://arxiv.org/pdf/2011.03748" target="_blank">pdf</a>]

<h2>EGO-Swarm: A Fully Autonomous and Decentralized Quadrotor Swarm System in Cluttered Environments. (arXiv:2011.04183v2 [cs.RO] UPDATED)</h2>
<h3>Xin Zhou, Xiangyong Wen, Jiangchao Zhu, Hongyu Zhou, Chao Xu, Fei Gao</h3>
<p>This paper presents a decentralized and asynchronous systematic solution for
multi-robot autonomous navigation in unknown obstacle-rich scenes using merely
onboard resources. The planning system is formulated under gradient-based local
planning framework, where collision avoidance is achieved by formulating the
collision risk as a penalty of a nonlinear optimization problem. In order to
improve robustness and escape local minima, we incorporate a lightweight
topological trajectory generation method. Then agents generate safe, smooth,
and dynamically feasible trajectories in only several milliseconds using an
unreliable trajectory sharing network. Relative localization drift among agents
is corrected by using agent detection in depth images. Our method is
demonstrated in both simulation and real-world experiments. The source code is
released for the reference of the community.
</p>
<a href="http://arxiv.org/abs/2011.04183" target="_blank">arXiv:2011.04183</a> [<a href="http://arxiv.org/pdf/2011.04183" target="_blank">pdf</a>]

<h2>RSPNet: Relative Speed Perception for Unsupervised Video Representation Learning. (arXiv:2011.07949v2 [cs.CV] UPDATED)</h2>
<h3>Peihao Chen, Deng Huang, Dongliang He, Xiang Long, Runhao Zeng, Shilei Wen, Mingkui Tan, Chuang Gan</h3>
<p>We study unsupervised video representation learning that seeks to learn both
motion and appearance features from unlabeled video only, which can be reused
for downstream tasks such as action recognition. This task, however, is
extremely challenging due to 1) the highly complex spatial-temporal information
in videos; and 2) the lack of labeled data for training. Unlike the
representation learning for static images, it is difficult to construct a
suitable self-supervised task to well model both motion and appearance
features. More recently, several attempts have been made to learn video
representation through video playback speed prediction. However, it is
non-trivial to obtain precise speed labels for the videos. More critically, the
learnt models may tend to focus on motion pattern and thus may not learn
appearance features well. In this paper, we observe that the relative playback
speed is more consistent with motion pattern, and thus provide more effective
and stable supervision for representation learning. Therefore, we propose a new
way to perceive the playback speed and exploit the relative speed between two
video clips as labels. In this way, we are able to well perceive speed and
learn better motion features. Moreover, to ensure the learning of appearance
features, we further propose an appearance-focused task, where we enforce the
model to perceive the appearance difference between two video clips. We show
that optimizing the two tasks jointly consistently improves the performance on
two downstream tasks, namely action recognition and video retrieval.
Remarkably, for action recognition on UCF101 dataset, we achieve 93.7% accuracy
without the use of labeled data for pre-training, which outperforms the
ImageNet supervised pre-trained model. Code and pre-trained models can be found
at https://github.com/PeihaoChen/RSPNet.
</p>
<a href="http://arxiv.org/abs/2011.07949" target="_blank">arXiv:2011.07949</a> [<a href="http://arxiv.org/pdf/2011.07949" target="_blank">pdf</a>]

<h2>Open-Vocabulary Object Detection Using Captions. (arXiv:2011.10678v2 [cs.CV] UPDATED)</h2>
<h3>Alireza Zareian, Kevin Dela Rosa, Derek Hao Hu, Shih-Fu Chang</h3>
<p>Despite the remarkable accuracy of deep neural networks in object detection,
they are costly to train and scale due to supervision requirements.
Particularly, learning more object categories typically requires proportionally
more bounding box annotations. Weakly supervised and zero-shot learning
techniques have been explored to scale object detectors to more categories with
less supervision, but they have not been as successful and widely adopted as
supervised models. In this paper, we put forth a novel formulation of the
object detection problem, namely open-vocabulary object detection, which is
more general, more practical, and more effective than weakly supervised and
zero-shot approaches. We propose a new method to train object detectors using
bounding box annotations for a limited set of object categories, as well as
image-caption pairs that cover a larger variety of objects at a significantly
lower cost. We show that the proposed method can detect and localize objects
for which no bounding box annotation is provided during training, at a
significantly higher accuracy than zero-shot approaches. Meanwhile, objects
with bounding box annotation can be detected almost as accurately as supervised
methods, which is significantly better than weakly supervised baselines.
Accordingly, we establish a new state of the art for scalable object detection.
</p>
<a href="http://arxiv.org/abs/2011.10678" target="_blank">arXiv:2011.10678</a> [<a href="http://arxiv.org/pdf/2011.10678" target="_blank">pdf</a>]

<h2>Imagination-enabled Robot Perception. (arXiv:2011.11397v4 [cs.RO] UPDATED)</h2>
<h3>Patrick Mania, Franklin Kenghagho Kenfack, Michael Neumann, Michael Beetz</h3>
<p>Many of today's robot perception systems aim at accomplishing perception
tasks that are too simplistic and too hard. They are too simplistic because
they do not require the perception systems to provide all the information
needed to accomplish manipulation tasks. Typically the perception results do
not include information about the part structure of objects, articulation
mechanisms and other attributes needed for adapting manipulation behavior. On
the other hand, the perception problems stated are also too hard because --
unlike humans -- the perception systems cannot leverage the expectations about
what they will see to their full potential. Therefore, we investigate a
variation of robot perception tasks suitable for robots accomplishing everyday
manipulation tasks, such as household robots or a robot in a retail store. In
such settings it is reasonable to assume that robots know most objects and have
detailed models of them.

We propose a perception system that maintains its beliefs about its
environment as a scene graph with physics simulation and visual rendering. When
detecting objects, the perception system retrieves the model of the object and
places it at the corresponding place in a VR-based environment model. The
physics simulation ensures that object detections that are physically not
possible are rejected and scenes can be rendered to generate expectations at
the image level. The result is a perception system that can provide useful
information for manipulation tasks.
</p>
<a href="http://arxiv.org/abs/2011.11397" target="_blank">arXiv:2011.11397</a> [<a href="http://arxiv.org/pdf/2011.11397" target="_blank">pdf</a>]

<h2>AdaGrasp: Learning an Adaptive Gripper-Aware Grasping Policy. (arXiv:2011.14206v3 [cs.RO] UPDATED)</h2>
<h3>Zhenjia Xu, Beichun Qi, Shubham Agrawal, Shuran Song</h3>
<p>This paper aims to improve robots' versatility and adaptability by allowing
them to use a large variety of end-effector tools and quickly adapt to new
tools. We propose AdaGrasp, a method to learn a single grasping policy that
generalizes to novel grippers. By training on a large collection of grippers,
our algorithm is able to acquire generalizable knowledge of how different
grippers should be used in various tasks. Given a visual observation of the
scene and the gripper, AdaGrasp infers the possible grasp poses and their grasp
scores by computing the cross convolution between the shape encodings of the
gripper and scene. Intuitively, this cross convolution operation can be
considered as an efficient way of exhaustively matching the scene geometry with
gripper geometry under different grasp poses (i.e., translations and
orientations), where a good "match" of 3D geometry will lead to a successful
grasp. We validate our methods in both simulation and real-world environments.
Our experiment shows that AdaGrasp significantly outperforms the existing
multi-gripper grasping policy method, especially when handling cluttered
environments and partial observations. Video is available at
https://youtu.be/kknTYTbORfs
</p>
<a href="http://arxiv.org/abs/2011.14206" target="_blank">arXiv:2011.14206</a> [<a href="http://arxiv.org/pdf/2011.14206" target="_blank">pdf</a>]

<h2>RaP-Net: A Region-wise and Point-wise Weighting Network to Extract Robust Features for Indoor Localization. (arXiv:2012.00234v2 [cs.CV] UPDATED)</h2>
<h3>Dongjiang Li, Jinyu Miao, Xuesong Shi, Yuxin Tian, Qiwei Long, Tianyu Cai, Ping Guo, Hongfei Yu, Wei Yang, Haosong Yue, Qi Wei, Fei Qiao</h3>
<p>Feature extraction plays an important role in visual localization. Unreliable
features on dynamic objects or repetitive regions will disturb robust feature
matching and thus, challenge indoor localization greatly. To conquer such an
issue, we propose a novel network, RaP-Net, to simultaneously predict
region-wise invariability and point-wise reliability, and then extract features
by considering both of them. We also introduce a new dataset, named
OpenLORIS-Location, to train proposed network. The dataset contains 1553 indoor
images from 93 indoor locations. Various appearance changes between images of
the same location are included and they can help to learn the invariability in
typical indoor scenes. Experimental results show that the proposed RaP-Net
trained with the OpenLORIS-Location dataset achieves an excellent performance
in the feature matching task and significantly outperforms state-of-the-arts
feature algorithms in indoor localization. The RaP-Net code and dataset are
available at https://github.com/ivipsourcecode/RaP-Net.
</p>
<a href="http://arxiv.org/abs/2012.00234" target="_blank">arXiv:2012.00234</a> [<a href="http://arxiv.org/pdf/2012.00234" target="_blank">pdf</a>]

<h2>Learning Collision-Free Space Detection from Stereo Images: Homography Matrix Brings Better Data Augmentation. (arXiv:2012.07890v3 [cs.RO] UPDATED)</h2>
<h3>Rui Fan, Hengli Wang, Peide Cai, Jin Wu, Mohammud Junaid Bocus, Lei Qiao, Ming Liu</h3>
<p>Collision-free space detection is a critical component of autonomous vehicle
perception. The state-of-the-art algorithms are typically based on supervised
learning. The performance of such approaches is always dependent on the quality
and amount of labeled training data. Additionally, it remains an open challenge
to train deep convolutional neural networks (DCNNs) using only a small quantity
of training samples. Therefore, this paper mainly explores an effective
training data augmentation approach that can be employed to improve the overall
DCNN performance, when additional images captured from different views are
available. Due to the fact that the pixels of the collision-free space
(generally regarded as a planar surface) between two images captured from
different views can be associated by a homography matrix, the scenario of the
target image can be transformed into the reference view. This provides a simple
but effective way of generating training data from additional multi-view
images. Extensive experimental results, conducted with six state-of-the-art
semantic segmentation DCNNs on three datasets, demonstrate the effectiveness of
our proposed training data augmentation algorithm for enhancing collision-free
space detection performance. When validated on the KITTI road benchmark, our
approach provides the best results for stereo vision-based collision-free space
detection.
</p>
<a href="http://arxiv.org/abs/2012.07890" target="_blank">arXiv:2012.07890</a> [<a href="http://arxiv.org/pdf/2012.07890" target="_blank">pdf</a>]

<h2>Multi-FinGAN: Generative Coarse-To-Fine Sampling of Multi-Finger Grasps. (arXiv:2012.09696v2 [cs.RO] UPDATED)</h2>
<h3>Jens Lundell, Enric Corona, Tran Nguyen Le, Francesco Verdoja, Philippe Weinzaepfel, Gregory Rogez, Francesc Moreno-Noguer, Ville Kyrki</h3>
<p>While there exists many methods for manipulating rigid objects with
parallel-jaw grippers, grasping with multi-finger robotic hands remains a quite
unexplored research topic. Reasoning and planning collision-free trajectories
on the additional degrees of freedom of several fingers represents an important
challenge that, so far, involves computationally costly and slow processes. In
this work, we present Multi-FinGAN, a fast generative multi-finger grasp
sampling method that synthesizes high quality grasps directly from RGB-D images
in about a second. We achieve this by training in an end-to-end fashion a
coarse-to-fine model composed of a classification network that distinguishes
grasp types according to a specific taxonomy and a refinement network that
produces refined grasp poses and joint angles. We experimentally validate and
benchmark our method against a standard grasp-sampling method on 790 grasps in
simulation and 20 grasps on a real Franka Emika Panda. All experimental results
using our method show consistent improvements both in terms of grasp quality
metrics and grasp success rate. Remarkably, our approach is up to 20-30 times
faster than the baseline, a significant improvement that opens the door to
feedback-based grasp re-planning and task informative grasping. Code is
available at https://irobotics.aalto.fi/multi-fingan/.
</p>
<a href="http://arxiv.org/abs/2012.09696" target="_blank">arXiv:2012.09696</a> [<a href="http://arxiv.org/pdf/2012.09696" target="_blank">pdf</a>]

<h2>Jacobian-based learning for inverse kinematics of soft robots. (arXiv:2012.13965v2 [cs.RO] UPDATED)</h2>
<h3>Guoxin Fang, Yingjun Tian, Zhi-Xin Yang, Jo M.P. Geraedts, Charlie C.L. Wang</h3>
<p>This paper presents a new method to solve the inverse kinematic (IK) problem
in real-time on soft robots with highly non-linear deformation. The major
challenge of efficiently computing IK for such robots is caused by the lack of
analytical formulation for either forward or inverse kinematics. To tackle this
challenge, we employ neural-networks to learn both the mapping function of
forward kinematics and also the Jacobian of this function. As a result,
Jacobian-based iteration can be applied to solve the IK problem. A sim-to-real
training transfer strategy is conducted to make this approach more practical.
We first generate large amount of samples in a simulation environment for
learning both the kinematic and the Jacobian networks of a soft robot design.
After that, a sim-to-real layer of differentiable neurons is employed to map
the results of simulation to the physical hardware, where this sim-to-real
layer can be learned from very limited number of training samples generated on
the hardware. The effectiveness of our approach has been verified on several
pneumatic-driven soft robots in the tasks of trajectory following and
interactive positioning.
</p>
<a href="http://arxiv.org/abs/2012.13965" target="_blank">arXiv:2012.13965</a> [<a href="http://arxiv.org/pdf/2012.13965" target="_blank">pdf</a>]

<h2>COMPAS: Representation Learning with Compositional Part Sharing for Few-Shot Classification. (arXiv:2101.11878v2 [cs.CV] UPDATED)</h2>
<h3>Ju He, Adam Kortylewski, Alan Yuille</h3>
<p>Few-shot image classification consists of two consecutive learning processes:
1) In the meta-learning stage, the model acquires a knowledge base from a set
of training classes. 2) During meta-testing, the acquired knowledge is used to
recognize unseen classes from very few examples. Inspired by the compositional
representation of objects in humans, we train a neural network architecture
that explicitly represents objects as a set of parts and their spatial
composition. In particular, during meta-learning, we train a knowledge base
that consists of a dictionary of part representations and a dictionary of part
activation maps that encode common spatial activation patterns of parts. The
elements of both dictionaries are shared among the training classes. During
meta-testing, the representation of unseen classes is learned using the part
representations and the part activation maps from the knowledge base. Finally,
an attention mechanism is used to strengthen those parts that are most
important for each category. We demonstrate the value of our compositional
learning framework for a few-shot classification using miniImageNet,
tieredImageNet, CIFAR-FS, and FC100, where we achieve state-of-the-art
performance.
</p>
<a href="http://arxiv.org/abs/2101.11878" target="_blank">arXiv:2101.11878</a> [<a href="http://arxiv.org/pdf/2101.11878" target="_blank">pdf</a>]

<h2>Multi-Scale Cost Volumes Cascade Network for Stereo Matching. (arXiv:2102.01940v2 [cs.CV] UPDATED)</h2>
<h3>Xiaogang Jia, Wei Chen, Zhengfa Liang, Mingfei Wu, Yusong Tan, Libo Huang</h3>
<p>Stereo matching is essential for robot navigation. However, the accuracy of
current widely used traditional methods is low, while methods based on CNN need
expensive computational cost and running time. This is because different cost
volumes play a crucial role in balancing speed and accuracy. Thus we propose
MSCVNet, which combines traditional methods and neural networks to improve the
quality of cost volume. Concretely, our network first generates multiple 3D
cost volumes with different resolutions and then uses 2D convolutions to
construct a novel cascade hourglass network for cost aggregation. Meanwhile, we
design an algorithm to distinguish and calculate the loss for discontinuous
areas of disparity result. According to the KITTI official website, our network
is much faster than most top-performing methods (24 times than CSPN, 44 times
than GANet, etc.). Meanwhile, compared to traditional methods (SPS-St, SGM) and
other real-time stereo matching networks (Fast DS-CS, DispNetC, and RTSNet,
etc.), our network achieves a big improvement in accuracy, demonstrating the
feasibility and capability of the proposed method.
</p>
<a href="http://arxiv.org/abs/2102.01940" target="_blank">arXiv:2102.01940</a> [<a href="http://arxiv.org/pdf/2102.01940" target="_blank">pdf</a>]

<h2>A Collaborative Visual SLAM Framework for Service Robots. (arXiv:2102.03228v2 [cs.RO] UPDATED)</h2>
<h3>Ming Ouyang, Xuesong Shi, Yujie Wang, Yuxin Tian, Yingzhe Shen, Dawei Wang, Peng Wang, Zhiqiang Cao</h3>
<p>With the rapid deployment of service robots, a method should be established
to allow multiple robots to work in the same place to collaborate and share the
spatial information. To this end, we present a collaborative visual
simultaneous localization and mapping (SLAM) framework particularly designed
for service robot scenarios. With an edge server maintaining a map database and
performing global optimization, each robot can register to an existing map,
update the map, or build new maps, all with a unified interface and low
computation and memory cost. To enable real-time information sharing, we design
a simple but effective communication pipeline and a novel landmark retrieval
method to augment each client's local map with nearby landmarks from the
server. The framework is general enough to support both RGB-D and monocular
cameras, as well as robots with multiple cameras, taking the rigid constraints
between cameras into consideration. The proposed framework has been fully
implemented and verified with public datasets and live experiments.
</p>
<a href="http://arxiv.org/abs/2102.03228" target="_blank">arXiv:2102.03228</a> [<a href="http://arxiv.org/pdf/2102.03228" target="_blank">pdf</a>]

<h2>FaSTrack: a Modular Framework for Real-Time Motion Planning and Guaranteed Safe Tracking. (arXiv:2102.07039v2 [cs.RO] UPDATED)</h2>
<h3>Mo Chen, Sylvia L. Herbert, Haimin Hu, Ye Pu, Jaime F. Fisac, Somil Bansal, SooJean Han, Claire J. Tomlin</h3>
<p>Real-time, guaranteed safe trajectory planning is vital for navigation in
unknown environments. However, real-time navigation algorithms typically
sacrifice robustness for computation speed. Alternatively, provably safe
trajectory planning tends to be too computationally intensive for real-time
replanning. We propose FaSTrack, Fast and Safe Tracking, a framework that
achieves both real-time replanning and guaranteed safety. In this framework,
real-time computation is achieved by allowing any trajectory planner to use a
simplified \textit{planning model} of the system. The plan is tracked by the
system, represented by a more realistic, higher-dimensional \textit{tracking
model}. We precompute the tracking error bound (TEB) due to mismatch between
the two models and due to external disturbances. We also obtain the
corresponding tracking controller used to stay within the TEB. The
precomputation does not require prior knowledge of the environment. We
demonstrate FaSTrack using Hamilton-Jacobi reachability for precomputation and
three different real-time trajectory planners with three different
tracking-planning model pairs.
</p>
<a href="http://arxiv.org/abs/2102.07039" target="_blank">arXiv:2102.07039</a> [<a href="http://arxiv.org/pdf/2102.07039" target="_blank">pdf</a>]

<h2>Towards Optimized Distributed Multi-Robot Printing: An Algorithmic Approach. (arXiv:2102.12026v2 [cs.RO] UPDATED)</h2>
<h3>Kedar Karpe, Avinash Sinha, Shreyas Raorane, Ayon Chatterjee, Pranav Srinivas, Lorenzo Sabattini</h3>
<p>This paper presents a distributed multi-robot printing method which utilizes
an optimization approach to decompose and allocate a printing task to a group
of mobile robots. The motivation for this problem is to minimize the printing
time of the robots by using an appropriate task decomposition algorithm. We
present one such algorithm which decomposes an image into rasterized geodesic
cells before allocating them to the robots for printing. In addition to this,
we also present the design of a numerically controlled holonomic robot capable
of spraying ink on smooth surfaces. Further, we use this robot to
experimentally verify the results of this paper.
</p>
<a href="http://arxiv.org/abs/2102.12026" target="_blank">arXiv:2102.12026</a> [<a href="http://arxiv.org/pdf/2102.12026" target="_blank">pdf</a>]

<h2>Efficient Transformer based Method for Remote Sensing Image Change Detection. (arXiv:2103.00208v2 [cs.CV] UPDATED)</h2>
<h3>Hao Chen, Zipeng Qi, Zhenwei Shi</h3>
<p>Modern change detection (CD) has achieved remarkable success by the powerful
discriminative ability of deep convolutions. However, high-resolution remote
sensing CD remains challenging due to the complexity of objects in the scene.
Objects with the same semantic concept may show distinct spectral behaviors at
different times and different spatial locations. Most recent CD pipelines using
pure convolutions are still struggling to relate long-range concepts in
space-time. Non-local self-attention approaches show promising performance via
modeling dense relations among pixels, yet are computationally inefficient.
Here, we propose a bitemporal image transformer (BiT) to efficiently and
effectively model contexts within the spatial-temporal domain. Our intuition is
that the high-level concepts of the change of interest can be represented by a
few visual words, i.e., semantic tokens. To achieve this, we express the
bitemporal image into a few tokens, and use a transformer encoder to model
contexts in the compact token-based space-time. The learned context-rich tokens
are then feedback to the pixel-space for refining the original features via a
transformer decoder. We incorporate BiT in a deep feature differencing-based CD
framework. Extensive experiments on three CD datasets demonstrate the
effectiveness and efficiency of the proposed method. Notably, our BiT-based
model significantly outperforms the purely convolutional baseline using only 3
times lower computational costs and model parameters. Based on a naive backbone
(ResNet18) without sophisticated structures (e.g., FPN, UNet), our model
surpasses several state-of-the-art CD methods, including better than two recent
attention-based methods in terms of efficiency and accuracy. Our code will be
made public.
</p>
<a href="http://arxiv.org/abs/2103.00208" target="_blank">arXiv:2103.00208</a> [<a href="http://arxiv.org/pdf/2103.00208" target="_blank">pdf</a>]

<h2>Scalable Scene Flow from Point Clouds in the Real World. (arXiv:2103.01306v3 [cs.CV] UPDATED)</h2>
<h3>Philipp Jund, Chris Sweeney, Nichola Abdo, Zhifeng Chen, Jonathon Shlens</h3>
<p>Autonomous vehicles operate in highly dynamic environments necessitating an
accurate assessment of which aspects of a scene are moving and where they are
moving to. A popular approach to 3D motion estimation -- termed scene flow --
is to employ 3D point cloud data from consecutive LiDAR scans, although such
approaches have been limited by the small size of real-world, annotated LiDAR
data. In this work, we introduce a new large scale benchmark for scene flow
based on the Waymo Open Dataset. The dataset is $\sim$1,000$\times$ larger than
previous real-world datasets in terms of the number of annotated frames and is
derived from the corresponding tracked 3D objects. We demonstrate how previous
works were bounded based on the amount of real LiDAR data available, suggesting
that larger datasets are required to achieve state-of-the-art predictive
performance. Furthermore, we show how previous heuristics for operating on
point clouds such as artificial down-sampling heavily degrade performance,
motivating a new class of models that are tractable on the full point cloud. To
address this issue, we introduce the model architecture FastFlow3D that
provides real time inference on the full point cloud. Finally, we demonstrate
that this problem is amenable to techniques from semi-supervised learning by
highlighting open problems for generalizing methods for predicting motion on
unlabeled objects. We hope that this dataset may provide new opportunities for
developing real world scene flow systems and motivate a new class of machine
learning problems.
</p>
<a href="http://arxiv.org/abs/2103.01306" target="_blank">arXiv:2103.01306</a> [<a href="http://arxiv.org/pdf/2103.01306" target="_blank">pdf</a>]

<h2>Inertial based Integration with Transformed INS Mechanization in Earth Frame. (arXiv:2103.02229v3 [cs.RO] UPDATED)</h2>
<h3>Lubin Chang, Jingbo Di, Fangjun Qin</h3>
<p>This paper proposes to use a newly-derived transformed inertial navigation
system (INS) mechanization to fuse INS with other complementary navigation
systems. Through formulating the attitude, velocity and position as one group
state of group of double direct spatial isometries SE2(3), the transformed INS
mechanization has proven to be group affine, which means that the corresponding
vector error state model will be trajectory-independent. In order to make use
of the transformed INS mechanization in inertial based integration, both the
right and left vector error state models are derived. The INS/GPS and
INS/Odometer integration are investigated as two representatives of inertial
based integration. Some application aspects of the derived error state models
in the two applications are presented, which include how to select the error
state model, initialization of the SE2(3) based error state covariance and
feedback correction corresponding to the error state definitions. Extensive
Monte Carlo simulations and land vehicle experiments are conducted to evaluate
the performance of the derived error state models. It is shown that the most
striking superiority of using the derived error state models is their ability
to handle the large initial attitude misalignments, which is just the result of
log-linearity property of the derived error state models. Therefore, the
derived error state models can be used in the so-called attitude alignment for
the two applications. Moreover, the derived right error state-space model is
also very preferred for long-endurance INS/Odometer integration due to the
filtering consistency caused by its less dependence on the global state
estimate.
</p>
<a href="http://arxiv.org/abs/2103.02229" target="_blank">arXiv:2103.02229</a> [<a href="http://arxiv.org/pdf/2103.02229" target="_blank">pdf</a>]

<h2>ID-Unet: Iterative Soft and Hard Deformation for View Synthesis. (arXiv:2103.02264v4 [cs.CV] UPDATED)</h2>
<h3>Mingyu Yin, Li Sun, Qingli Li</h3>
<p>View synthesis is usually done by an autoencoder, in which the encoder maps a
source view image into a latent content code, and the decoder transforms it
into a target view image according to the condition. However, the source
contents are often not well kept in this setting, which leads to unnecessary
changes during the view translation. Although adding skipped connections, like
Unet, alleviates the problem, but it often causes the failure on the view
conformity. This paper proposes a new architecture by performing the
source-to-target deformation in an iterative way. Instead of simply
incorporating the features from multiple layers of the encoder, we design soft
and hard deformation modules, which warp the encoder features to the target
view at different resolutions, and give results to the decoder to complement
the details. Particularly, the current warping flow is not only used to align
the feature of the same resolution, but also as an approximation to coarsely
deform the high resolution feature. Then the residual flow is estimated and
applied in the high resolution, so that the deformation is built up in the
coarse-to-fine fashion. To better constrain the model, we synthesize a rough
target view image based on the intermediate flows and their warped features.
The extensive ablation studies and the final results on two different data sets
show the effectiveness of the proposed model.
</p>
<a href="http://arxiv.org/abs/2103.02264" target="_blank">arXiv:2103.02264</a> [<a href="http://arxiv.org/pdf/2103.02264" target="_blank">pdf</a>]

<h2>DeepTag: An Unsupervised Deep Learning Method for Motion Tracking on Cardiac Tagging Magnetic Resonance Images. (arXiv:2103.02772v2 [cs.CV] UPDATED)</h2>
<h3>Meng Ye, Mikael Kanski, Dong Yang, Qi Chang, Zhennan Yan, Qiaoying Huang, Leon Axel, Dimitris Metaxas</h3>
<p>Cardiac tagging magnetic resonance imaging (t-MRI) is the gold standard for
regional myocardium deformation and cardiac strain estimation. However, this
technique has not been widely used in clinical diagnosis, as a result of the
difficulty of motion tracking encountered with t-MRI images. In this paper, we
propose a novel deep learning-based fully unsupervised method for in vivo
motion tracking on t-MRI images. We first estimate the motion field (INF)
between any two consecutive t-MRI frames by a bi-directional generative
diffeomorphic registration neural network. Using this result, we then estimate
the Lagrangian motion field between the reference frame and any other frame
through a differentiable composition layer. By utilizing temporal information
to perform reasonable estimations on spatio-temporal motion fields, this novel
method provides a useful solution for motion tracking and image registration in
dynamic medical imaging. Our method has been validated on a representative
clinical t-MRI dataset; the experimental results show that our method is
superior to conventional motion tracking methods in terms of landmark tracking
accuracy and inference efficiency.
</p>
<a href="http://arxiv.org/abs/2103.02772" target="_blank">arXiv:2103.02772</a> [<a href="http://arxiv.org/pdf/2103.02772" target="_blank">pdf</a>]

<h2>Student-Teacher Feature Pyramid Matching for Unsupervised Anomaly Detection. (arXiv:2103.04257v2 [cs.CV] UPDATED)</h2>
<h3>Guodong Wang, Shumin Han, Errui Ding, Di Huang</h3>
<p>Anomaly detection is a challenging task and usually formulated as an
unsupervised learning problem for the unexpectedness of anomalies. This paper
proposes a simple yet powerful approach to this issue, which is implemented in
the student-teacher framework for its advantages but substantially extends it
in terms of both accuracy and efficiency. Given a strong model pre-trained on
image classification as the teacher, we distill the knowledge into a single
student network with the identical architecture to learn the distribution of
anomaly-free images and this one-step transfer preserves the crucial clues as
much as possible. Moreover, we integrate the multi-scale feature matching
strategy into the framework, and this hierarchical feature alignment enables
the student network to receive a mixture of multi-level knowledge from the
feature pyramid under better supervision, thus allowing to detect anomalies of
various sizes. The difference between feature pyramids generated by the two
networks serves as a scoring function indicating the probability of anomaly
occurring. Due to such operations, our approach achieves accurate and fast
pixel-level anomaly detection. Very competitive results are delivered on three
major benchmarks, significantly superior to the state of the art ones. In
addition, it makes inferences at a very high speed (with 100 FPS for images of
the size at 256x256), at least dozens of times faster than the latest
counterparts.
</p>
<a href="http://arxiv.org/abs/2103.04257" target="_blank">arXiv:2103.04257</a> [<a href="http://arxiv.org/pdf/2103.04257" target="_blank">pdf</a>]

<h2>RFN-Nest: An end-to-end residual fusion network for infrared and visible images. (arXiv:2103.04286v2 [cs.CV] UPDATED)</h2>
<h3>Hui Li, Xiao-Jun Wu, Josef Kittler</h3>
<p>In the image fusion field, the design of deep learning-based fusion methods
is far from routine. It is invariably fusion-task specific and requires a
careful consideration. The most difficult part of the design is to choose an
appropriate strategy to generate the fused image for a specific task in hand.
Thus, devising learnable fusion strategy is a very challenging problem in the
community of image fusion. To address this problem, a novel end-to-end fusion
network architecture (RFN-Nest) is developed for infrared and visible image
fusion. We propose a residual fusion network (RFN) which is based on a residual
architecture to replace the traditional fusion approach. A novel
detail-preserving loss function, and a feature enhancing loss function are
proposed to train RFN. The fusion model learning is accomplished by a novel
two-stage training strategy. In the first stage, we train an auto-encoder based
on an innovative nest connection (Nest) concept. Next, the RFN is trained using
the proposed loss functions. The experimental results on public domain data
sets show that, compared with the existing methods, our end-to-end fusion
network delivers a better performance than the state-of-the-art methods in both
subjective and objective evaluation. The code of our fusion method is available
at https://github.com/hli1221/imagefusion-rfn-nest
</p>
<a href="http://arxiv.org/abs/2103.04286" target="_blank">arXiv:2103.04286</a> [<a href="http://arxiv.org/pdf/2103.04286" target="_blank">pdf</a>]

<h2>FSCE: Few-Shot Object Detection via Contrastive Proposal Encoding. (arXiv:2103.05950v2 [cs.CV] UPDATED)</h2>
<h3>Bo Sun, Banghuai Li, Shengcai Cai, Ye Yuan, Chi Zhang</h3>
<p>Emerging interests have been brought to recognize previously unseen objects
given very few training examples, known as few-shot object detection (FSOD).
Recent researches demonstrate that good feature embedding is the key to reach
favorable few-shot learning performance. We observe object proposals with
different Intersection-of-Union (IoU) scores are analogous to the intra-image
augmentation used in contrastive approaches. And we exploit this analogy and
incorporate supervised contrastive learning to achieve more robust objects
representations in FSOD. We present Few-Shot object detection via Contrastive
proposals Encoding (FSCE), a simple yet effective approach to learning
contrastive-aware object proposal encodings that facilitate the classification
of detected objects. We notice the degradation of average precision (AP) for
rare objects mainly comes from misclassifying novel instances as confusable
classes. And we ease the misclassification issues by promoting instance level
intra-class compactness and inter-class variance via our contrastive proposal
encoding loss (CPE loss). Our design outperforms current state-of-the-art works
in any shot and all data splits, with up to +8.8% on standard benchmark PASCAL
VOC and +2.7% on challenging COCO benchmark. Code is available at: https:
//github.com/MegviiDetection/FSCE
</p>
<a href="http://arxiv.org/abs/2103.05950" target="_blank">arXiv:2103.05950</a> [<a href="http://arxiv.org/pdf/2103.05950" target="_blank">pdf</a>]

<h2>A Study of Face Obfuscation in ImageNet. (arXiv:2103.06191v2 [cs.CV] UPDATED)</h2>
<h3>Kaiyu Yang, Jacqueline Yau, Li Fei-Fei, Jia Deng, Olga Russakovsky</h3>
<p>Face obfuscation (blurring, mosaicing, etc.) has been shown to be effective
for privacy protection; nevertheless, object recognition research typically
assumes access to complete, unobfuscated images. In this paper, we explore the
effects of face obfuscation on the popular ImageNet challenge visual
recognition benchmark. Most categories in the ImageNet challenge are not people
categories; however, many incidental people appear in the images, and their
privacy is a concern. We first annotate faces in the dataset. Then we
demonstrate that face blurring -- a typical obfuscation technique -- has
minimal impact on the accuracy of recognition models. Concretely, we benchmark
multiple deep neural networks on face-blurred images and observe that the
overall recognition accuracy drops only slightly (no more than 0.68%). Further,
we experiment with transfer learning to 4 downstream tasks (object recognition,
scene recognition, face attribute classification, and object detection) and
show that features learned on face-blurred images are equally transferable. Our
work demonstrates the feasibility of privacy-aware visual recognition, improves
the highly-used ImageNet challenge benchmark, and suggests an important path
for future visual datasets. Data and code are available at
https://github.com/princetonvisualai/imagenet-face-obfuscation.
</p>
<a href="http://arxiv.org/abs/2103.06191" target="_blank">arXiv:2103.06191</a> [<a href="http://arxiv.org/pdf/2103.06191" target="_blank">pdf</a>]

<h2>WenLan: Bridging Vision and Language by Large-Scale Multi-Modal Pre-Training. (arXiv:2103.06561v2 [cs.CV] UPDATED)</h2>
<h3>Yuqi Huo, Manli Zhang, Guangzhen Liu, Haoyu Lu, Yizhao Gao, Guoxing Yang, Jingyuan Wen, Heng Zhang, Baogui Xu, Weihao Zheng, Zongzheng Xi, Yueqian Yang, Anwen Hu, Jinming Zhao, Ruichen Li, Yida Zhao, Liang Zhang, Yuqing Song, Xin Hong, Wanqing Cui, Danyang Hou, Yingyan Li, Junyi Li, Peiyu Liu, Zheng Gong, Chuhao Jin, Yuchong Sun, Shizhe Chen, Zhiwu Lu, Zhicheng Dou, Qin Jin, Yanyan Lan, Wayne Xin Zhao, Ruihua Song, Ji-Rong Wen</h3>
<p>Multi-modal pre-training models have been intensively explored to bridge
vision and language in recent years. However, most of them explicitly model the
cross-modal interaction between image-text pairs, by assuming that there exists
strong semantic correlation between the text and image modalities. Since this
strong assumption is often invalid in real-world scenarios, we choose to
implicitly model the cross-modal correlation for large-scale multi-modal
pre-training, which is the focus of the Chinese project `WenLan' led by our
team. Specifically, with the weak correlation assumption over image-text pairs,
we propose a two-tower pre-training model called BriVL within the cross-modal
contrastive learning framework. Unlike OpenAI CLIP that adopts a simple
contrastive learning method, we devise a more advanced algorithm by adapting
the latest method MoCo into the cross-modal scenario. By building a large
queue-based dictionary, our BriVL can incorporate more negative samples in
limited GPU resources. We further construct a large Chinese multi-source
image-text dataset called RUC-CAS-WenLan for pre-training our BriVL model.
Extensive experiments demonstrate that the pre-trained BriVL model outperforms
both UNITER and OpenAI CLIP on various downstream tasks.
</p>
<a href="http://arxiv.org/abs/2103.06561" target="_blank">arXiv:2103.06561</a> [<a href="http://arxiv.org/pdf/2103.06561" target="_blank">pdf</a>]

<h2>MagFace: A Universal Representation for Face Recognition and Quality Assessment. (arXiv:2103.06627v2 [cs.CV] UPDATED)</h2>
<h3>Qiang Meng, Shichao Zhao, Zhida Huang, Feng Zhou</h3>
<p>The performance of face recognition system degrades when the variability of
the acquired faces increases. Prior work alleviates this issue by either
monitoring the face quality in pre-processing or predicting the data
uncertainty along with the face feature. This paper proposes MagFace, a
category of losses that learn a universal feature embedding whose magnitude can
measure the quality of the given face. Under the new loss, it can be proven
that the magnitude of the feature embedding monotonically increases if the
subject is more likely to be recognized. In addition, MagFace introduces an
adaptive mechanism to learn a wellstructured within-class feature distributions
by pulling easy samples to class centers while pushing hard samples away. This
prevents models from overfitting on noisy low-quality samples and improves face
recognition in the wild. Extensive experiments conducted on face recognition,
quality assessments as well as clustering demonstrate its superiority over
state-of-the-arts. The code is available at
https://github.com/IrvingMeng/MagFace.
</p>
<a href="http://arxiv.org/abs/2103.06627" target="_blank">arXiv:2103.06627</a> [<a href="http://arxiv.org/pdf/2103.06627" target="_blank">pdf</a>]

<h2>Deep Graph Matching under Quadratic Constraint. (arXiv:2103.06643v2 [cs.CV] UPDATED)</h2>
<h3>Quankai Gao, Fudong Wang, Nan Xue, Jin-Gang Yu, Gui-Song Xia</h3>
<p>Recently, deep learning based methods have demonstrated promising results on
the graph matching problem, by relying on the descriptive capability of deep
features extracted on graph nodes. However, one main limitation with existing
deep graph matching (DGM) methods lies in their ignorance of explicit
constraint of graph structures, which may lead the model to be trapped into
local minimum in training. In this paper, we propose to explicitly formulate
pairwise graph structures as a \textbf{quadratic constraint} incorporated into
the DGM framework. The quadratic constraint minimizes the pairwise structural
discrepancy between graphs, which can reduce the ambiguities brought by only
using the extracted CNN features.

Moreover, we present a differentiable implementation to the quadratic
constrained-optimization such that it is compatible with the unconstrained deep
learning optimizer. To give more precise and proper supervision, a
well-designed false matching loss against class imbalance is proposed, which
can better penalize the false negatives and false positives with less
overfitting. Exhaustive experiments demonstrate that our method competitive
performance on real-world datasets.
</p>
<a href="http://arxiv.org/abs/2103.06643" target="_blank">arXiv:2103.06643</a> [<a href="http://arxiv.org/pdf/2103.06643" target="_blank">pdf</a>]

<h2>Temporal Action Segmentation from Timestamp Supervision. (arXiv:2103.06669v2 [cs.CV] UPDATED)</h2>
<h3>Zhe Li, Yazan Abu Farha, Juergen Gall</h3>
<p>Temporal action segmentation approaches have been very successful recently.
However, annotating videos with frame-wise labels to train such models is very
expensive and time consuming. While weakly supervised methods trained using
only ordered action lists require much less annotation effort, the performance
is still much worse than fully supervised approaches. In this paper, we
introduce timestamp supervision for the temporal action segmentation task.
Timestamps require a comparable annotation effort to weakly supervised
approaches, and yet provide a more supervisory signal. To demonstrate the
effectiveness of timestamp supervision, we propose an approach to train a
segmentation model using only timestamps annotations. Our approach uses the
model output and the annotated timestamps to generate frame-wise labels by
detecting the action changes. We further introduce a confidence loss that
forces the predicted probabilities to monotonically decrease as the distance to
the timestamps increases. This ensures that all and not only the most
distinctive frames of an action are learned during training. The evaluation on
four datasets shows that models trained with timestamps annotations achieve
comparable performance to the fully supervised approaches.
</p>
<a href="http://arxiv.org/abs/2103.06669" target="_blank">arXiv:2103.06669</a> [<a href="http://arxiv.org/pdf/2103.06669" target="_blank">pdf</a>]

<h2>Efficient Pairwise Neuroimage Analysis using the Soft Jaccard Index and 3D Keypoint Sets. (arXiv:2103.06966v2 [cs.CV] UPDATED)</h2>
<h3>Laurent Chauvin, Kuldeep Kumar, Christian Desrosiers, William Wells III, Matthew Toews</h3>
<p>We propose a novel pairwise distance measure between variable-sized sets of
image keypoints for the purpose of large-scale medical image indexing. Our
measure generalizes the Jaccard index to account for soft set equivalence (SSE)
between set elements, via an adaptive kernel framework accounting for
uncertainty in keypoint appearance and geometry. Novel kernels are proposed to
quantify the variability of keypoint geometry in location and scale. Our
distance measure may be estimated between $N^2$ image pairs in $O(N~\log~N)$
operations via keypoint indexing. Experiments validate our method in predicting
509,545 pairwise relationships from T1-weighted MRI brain volumes of
monozygotic and dizygotic twins, siblings and half-siblings sharing 100%-25% of
their polymorphic genes. Soft set equivalence and keypoint geometry kernels
outperform standard hard set equivalence (HSE) in predicting family
relationships. High accuracy is achieved, with monozygotic twin identification
near 100% and several cases of unknown family labels, due to errors in the
genotyping process, are correctly paired with family members. Software is
provided for efficient fine-grained curation of large, generic image datasets.
</p>
<a href="http://arxiv.org/abs/2103.06966" target="_blank">arXiv:2103.06966</a> [<a href="http://arxiv.org/pdf/2103.06966" target="_blank">pdf</a>]

<h2>Deep Dual Consecutive Network for Human Pose Estimation. (arXiv:2103.07254v2 [cs.CV] UPDATED)</h2>
<h3>Zhenguang Liu, Haoming Chen, Runyang Feng, Shuang Wu, Shouling Ji, Bailin Yang, Xun Wang</h3>
<p>Multi-frame human pose estimation in complicated situations is challenging.
Although state-of-the-art human joints detectors have demonstrated remarkable
results for static images, their performances come short when we apply these
models to video sequences. Prevalent shortcomings include the failure to handle
motion blur, video defocus, or pose occlusions, arising from the inability in
capturing the temporal dependency among video frames. On the other hand,
directly employing conventional recurrent neural networks incurs empirical
difficulties in modeling spatial contexts, especially for dealing with pose
occlusions. In this paper, we propose a novel multi-frame human pose estimation
framework, leveraging abundant temporal cues between video frames to facilitate
keypoint detection. Three modular components are designed in our framework. A
Pose Temporal Merger encodes keypoint spatiotemporal context to generate
effective searching scopes while a Pose Residual Fusion module computes
weighted pose residuals in dual directions. These are then processed via our
Pose Correction Network for efficient refining of pose estimations. Our method
ranks No.1 in the Multi-frame Person Pose Estimation Challenge on the
large-scale benchmark datasets PoseTrack2017 and PoseTrack2018. We have
released our code, hoping to inspire future research.
</p>
<a href="http://arxiv.org/abs/2103.07254" target="_blank">arXiv:2103.07254</a> [<a href="http://arxiv.org/pdf/2103.07254" target="_blank">pdf</a>]

<h2>PrivacyNet: Semi-Adversarial Networks for Multi-attribute Face Privacy. (arXiv:2001.00561v3 [cs.CV] CROSS LISTED)</h2>
<h3>Vahid Mirjalili, Sebastian Raschka, Arun Ross</h3>
<p>Recent research has established the possibility of deducing soft-biometric
attributes such as age, gender and race from an individual's face image with
high accuracy. However, this raises privacy concerns, especially when face
images collected for biometric recognition purposes are used for attribute
analysis without the person's consent. To address this problem, we develop a
technique for imparting soft biometric privacy to face images via an image
perturbation methodology. The image perturbation is undertaken using a
GAN-based Semi-Adversarial Network (SAN) - referred to as PrivacyNet - that
modifies an input face image such that it can be used by a face matcher for
matching purposes but cannot be reliably used by an attribute classifier.
Further, PrivacyNet allows a person to choose specific attributes that have to
be obfuscated in the input face images (e.g., age and race), while allowing for
other types of attributes to be extracted (e.g., gender). Extensive experiments
using multiple face matchers, multiple age/gender/race classifiers, and
multiple face datasets demonstrate the generalizability of the proposed
multi-attribute privacy enhancing method across multiple face and attribute
classifiers.
</p>
<a href="http://arxiv.org/abs/2001.00561" target="_blank">arXiv:2001.00561</a> [<a href="http://arxiv.org/pdf/2001.00561" target="_blank">pdf</a>]

