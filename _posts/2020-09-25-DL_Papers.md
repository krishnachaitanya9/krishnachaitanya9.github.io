---
title: Latest Deep Learning Papers
date: 2021-01-27 21:00:01 +0800
featured-img: DL-logo
categories: [Deep_Learning]
tags: [Deep_Learning]
mathjax: Yes
---

<h1>Your arXiv Feed (133 Articles)</h1>
<h2>New Algorithms for Computing Field of Vision over 2D Grids. (arXiv:2101.11002v1 [cs.CV])</h2>
<h3>Evan R.M. Debenham, Roberto Solis-Oba (The University of Western Ontario, Canada)</h3>
<p>The aim of this paper is to propose new algorithms for Field of Vision (FOV)
computation which improve on existing work at high resolutions. FOV refers to
the set of locations that are visible from a specific position in a scene of a
computer game.

We summarize existing algorithms for FOV computation, describe their
limitations, and present new algorithms which aim to address these limitations.
We first present an algorithm which makes use of spatial data structures in a
way which is new for FOV calculation. We then present a novel technique which
updates a previously calculated FOV, rather than re-calculating an FOV from
scratch.

We compare our algorithms to existing FOV algorithms and show they provide
substantial improvements to running time. Our algorithms provide the largest
improvement over existing FOV algorithms at large grid sizes, thus allowing the
possibility of the design of high resolution FOV-based video games.
</p>
<a href="http://arxiv.org/abs/2101.11002" target="_blank">arXiv:2101.11002</a> [<a href="http://arxiv.org/pdf/2101.11002" target="_blank">pdf</a>]

<h2>On formal concepts of random formal contexts. (arXiv:2101.11023v1 [cs.AI])</h2>
<h3>Taro Sakurai (Chiba University)</h3>
<p>In formal concept analysis, it is well-known that the number of formal
concepts can be exponential in the worst case. To analyze the average case, we
introduce a probabilistic model for random formal contexts and prove that the
average number of formal concepts has a superpolynomial asymptotic lower bound.
</p>
<a href="http://arxiv.org/abs/2101.11023" target="_blank">arXiv:2101.11023</a> [<a href="http://arxiv.org/pdf/2101.11023" target="_blank">pdf</a>]

<h2>Average Localised Proximity: a new data descriptor with good default one-class classification performance. (arXiv:2101.11037v1 [cs.LG])</h2>
<h3>Oliver Urs Lenz, Daniel Peralta, Chris Cornelis</h3>
<p>One-class classification is a challenging subfield of machine learning in
which so-called data descriptors are used to predict membership of a class
based solely on positive examples of that class, and no counter-examples. A
number of data descriptors that have been shown to perform well in previous
studies of one-class classification, like the Support Vector Machine (SVM),
require setting one or more hyperparameters. There has been no systematic
attempt to date to determine optimal default values for these hyperparameters,
which limits their ease of use, especially in comparison with
hyperparameter-free proposals like the Isolation Forest (IF). We address this
issue by determining optimal default hyperparameter values across a collection
of 246 one-class classification problems derived from 50 different real-world
datasets. In addition, we propose a new data descriptor, Average Localised
Proximity (ALP) to address certain issues with existing approaches based on
nearest neighbour distances. Finally, we evaluate classification performance
using a leave-one-dataset-out procedure, and find strong evidence that ALP
outperforms IF and a number of other data descriptors, as well as weak evidence
that it outperforms SVM, making ALP a good default choice.
</p>
<a href="http://arxiv.org/abs/2101.11037" target="_blank">arXiv:2101.11037</a> [<a href="http://arxiv.org/pdf/2101.11037" target="_blank">pdf</a>]

<h2>Generalized Doubly Reparameterized Gradient Estimators. (arXiv:2101.11046v1 [stat.ML])</h2>
<h3>Matthias Bauer, Andriy Mnih</h3>
<p>Efficient low-variance gradient estimation enabled by the reparameterization
trick (RT) has been essential to the success of variational autoencoders.
Doubly-reparameterized gradients (DReGs) improve on the RT for multi-sample
variational bounds by applying reparameterization a second time for an
additional reduction in variance. Here, we develop two generalizations of the
DReGs estimator and show that they can be used to train conditional and
hierarchical VAEs on image modelling tasks more effectively. We first extend
the estimator to hierarchical models with several stochastic layers by showing
how to treat additional score function terms due to the hierarchical
variational posterior. We then generalize DReGs to score functions of arbitrary
distributions instead of just those of the sampling distribution, which makes
the estimator applicable to the parameters of the prior in addition to those of
the posterior.
</p>
<a href="http://arxiv.org/abs/2101.11046" target="_blank">arXiv:2101.11046</a> [<a href="http://arxiv.org/pdf/2101.11046" target="_blank">pdf</a>]

<h2>Revisiting Contrastive Learning for Few-Shot Classification. (arXiv:2101.11058v1 [cs.CV])</h2>
<h3>Orchid Majumder, Avinash Ravichandran, Subhransu Maji, Marzia Polito, Rahul Bhotika, Stefano Soatto</h3>
<p>Instance discrimination based contrastive learning has emerged as a leading
approach for self-supervised learning of visual representations. Yet, its
generalization to novel tasks remains elusive when compared to representations
learned with supervision, especially in the few-shot setting. We demonstrate
how one can incorporate supervision in the instance discrimination based
contrastive self-supervised learning framework to learn representations that
generalize better to novel tasks. We call our approach CIDS (Contrastive
Instance Discrimination with Supervision). CIDS performs favorably compared to
existing algorithms on popular few-shot benchmarks like Mini-ImageNet or
Tiered-ImageNet. We also propose a novel model selection algorithm that can be
used in conjunction with a universal embedding trained using CIDS to outperform
state-of-the-art algorithms on the challenging Meta-Dataset benchmark.
</p>
<a href="http://arxiv.org/abs/2101.11058" target="_blank">arXiv:2101.11058</a> [<a href="http://arxiv.org/pdf/2101.11058" target="_blank">pdf</a>]

<h2>The MineRL 2020 Competition on Sample Efficient Reinforcement Learning using Human Priors. (arXiv:2101.11071v1 [cs.LG])</h2>
<h3>William H. Guss, Mario Ynocente Castro, Sam Devlin, Brandon Houghton, Noboru Sean Kuno, Crissman Loomis, Stephanie Milani, Sharada Mohanty, Keisuke Nakata, Ruslan Salakhutdinov, John Schulman, Shinya Shiroshita, Nicholay Topin, Avinash Ummadisingu, Oriol Vinyals</h3>
<p>Although deep reinforcement learning has led to breakthroughs in many
difficult domains, these successes have required an ever-increasing number of
samples, affording only a shrinking segment of the AI community access to their
development. Resolution of these limitations requires new, sample-efficient
methods. To facilitate research in this direction, we propose this second
iteration of the MineRL Competition. The primary goal of the competition is to
foster the development of algorithms which can efficiently leverage human
demonstrations to drastically reduce the number of samples needed to solve
complex, hierarchical, and sparse environments. To that end, participants
compete under a limited environment sample-complexity budget to develop systems
which solve the MineRL ObtainDiamond task in Minecraft, a sequential decision
making environment requiring long-term planning, hierarchical control, and
efficient exploration methods. The competition is structured into two rounds in
which competitors are provided several paired versions of the dataset and
environment with different game textures and shaders. At the end of each round,
competitors submit containerized versions of their learning algorithms to the
AIcrowd platform where they are trained from scratch on a hold-out
dataset-environment pair for a total of 4-days on a pre-specified hardware
platform. In this follow-up iteration to the NeurIPS 2019 MineRL Competition,
we implement new features to expand the scale and reach of the competition. In
response to the feedback of the previous participants, we introduce a second
minor track focusing on solutions without access to environment interactions of
any kind except during test-time. Further we aim to prompt domain agnostic
submissions by implementing several novel competition mechanics including
action-space randomization and desemantization of observations and actions.
</p>
<a href="http://arxiv.org/abs/2101.11071" target="_blank">arXiv:2101.11071</a> [<a href="http://arxiv.org/pdf/2101.11071" target="_blank">pdf</a>]

<h2>Property Inference From Poisoning. (arXiv:2101.11073v1 [cs.LG])</h2>
<h3>Melissa Chase, Esha Ghosh, Saeed Mahloujifar</h3>
<p>Property inference attacks consider an adversary who has access to the
trained model and tries to extract some global statistics of the training data.
In this work, we study property inference in scenarios where the adversary can
maliciously control part of the training data (poisoning data) with the goal of
increasing the leakage.

Previous work on poisoning attacks focused on trying to decrease the accuracy
of models either on the whole population or on specific sub-populations or
instances. Here, for the first time, we study poisoning attacks where the goal
of the adversary is to increase the information leakage of the model. Our
findings suggest that poisoning attacks can boost the information leakage
significantly and should be considered as a stronger threat model in sensitive
applications where some of the data sources may be malicious.

We describe our \emph{property inference poisoning attack} that allows the
adversary to learn the prevalence in the training data of any property it
chooses. We theoretically prove that our attack can always succeed as long as
the learning algorithm used has good generalization properties.

We then verify the effectiveness of our attack by experimentally evaluating
it on two datasets: a Census dataset and the Enron email dataset. We were able
to achieve above $90\%$ attack accuracy with $9-10\%$ poisoning in all of our
experiments.
</p>
<a href="http://arxiv.org/abs/2101.11073" target="_blank">arXiv:2101.11073</a> [<a href="http://arxiv.org/pdf/2101.11073" target="_blank">pdf</a>]

<h2>Adaptivity without Compromise: A Momentumized, Adaptive, Dual Averaged Gradient Method for Stochastic Optimization. (arXiv:2101.11075v1 [cs.LG])</h2>
<h3>Aaron Defazio, Samy Jelassi</h3>
<p>We introduce MADGRAD, a novel optimization method in the family of AdaGrad
adaptive gradient methods. MADGRAD shows excellent performance on deep learning
optimization problems from multiple fields, including classification and
image-to-image tasks in vision, and recurrent and bidirectionally-masked models
in natural language processing. For each of these tasks, MADGRAD matches or
outperforms both SGD and ADAM in test set performance, even on problems for
which adaptive methods normally perform poorly.
</p>
<a href="http://arxiv.org/abs/2101.11075" target="_blank">arXiv:2101.11075</a> [<a href="http://arxiv.org/pdf/2101.11075" target="_blank">pdf</a>]

<h2>Deep Video Inpainting Detection. (arXiv:2101.11080v1 [cs.CV])</h2>
<h3>Peng Zhou, Ning Yu, Zuxuan Wu, Larry S. Davis, Abhinav Shrivastava, Ser-Nam Lim</h3>
<p>This paper studies video inpainting detection, which localizes an inpainted
region in a video both spatially and temporally. In particular, we introduce
VIDNet, Video Inpainting Detection Network, which contains a two-stream
encoder-decoder architecture with attention module. To reveal artifacts encoded
in compression, VIDNet additionally takes in Error Level Analysis frames to
augment RGB frames, producing multimodal features at different levels with an
encoder. Exploring spatial and temporal relationships, these features are
further decoded by a Convolutional LSTM to predict masks of inpainted regions.
In addition, when detecting whether a pixel is inpainted or not, we present a
quad-directional local attention module that borrows information from its
surrounding pixels from four directions. Extensive experiments are conducted to
validate our approach. We demonstrate, among other things, that VIDNet not only
outperforms by clear margins alternative inpainting detection methods but also
generalizes well on novel videos that are unseen during training.
</p>
<a href="http://arxiv.org/abs/2101.11080" target="_blank">arXiv:2101.11080</a> [<a href="http://arxiv.org/pdf/2101.11080" target="_blank">pdf</a>]

<h2>The Effect of Class Definitions on the Transferability of Adversarial Attacks Against Forensic CNNs. (arXiv:2101.11081v1 [cs.CV])</h2>
<h3>Xinwei Zhao, Matthew C. Stamm</h3>
<p>In recent years, convolutional neural networks (CNNs) have been widely used
by researchers to perform forensic tasks such as image tampering detection. At
the same time, adversarial attacks have been developed that are capable of
fooling CNN-based classifiers. Understanding the transferability of adversarial
attacks, i.e. an attacks ability to attack a different CNN than the one it was
trained against, has important implications for designing CNNs that are
resistant to attacks. While attacks on object recognition CNNs are believed to
be transferrable, recent work by Barni et al. has shown that attacks on
forensic CNNs have difficulty transferring to other CNN architectures or CNNs
trained using different datasets. In this paper, we demonstrate that
adversarial attacks on forensic CNNs are even less transferrable than
previously thought even between virtually identical CNN architectures! We show
that several common adversarial attacks against CNNs trained to identify image
manipulation fail to transfer to CNNs whose only difference is in the class
definitions (i.e. the same CNN architectures trained using the same data). We
note that all formulations of class definitions contain the unaltered class.
This has important implications for the future design of forensic CNNs that are
robust to adversarial and anti-forensic attacks.
</p>
<a href="http://arxiv.org/abs/2101.11081" target="_blank">arXiv:2101.11081</a> [<a href="http://arxiv.org/pdf/2101.11081" target="_blank">pdf</a>]

<h2>EPIC-Survival: End-to-end Part Inferred Clustering for Survival Analysis, Featuring Prognostic Stratification Boosting. (arXiv:2101.11085v1 [cs.CV])</h2>
<h3>Hassan Muhammad, Chensu Xie, Carlie S. Sigel, Michael Doukas, Lindsay Alpert, Thomas J. Fuchs</h3>
<p>Histopathology-based survival modelling has two major hurdles. Firstly, a
well-performing survival model has minimal clinical application if it does not
contribute to the stratification of a cancer patient cohort into different risk
groups, preferably driven by histologic morphologies. In the clinical setting,
individuals are not given specific prognostic predictions, but are rather
predicted to lie within a risk group which has a general survival trend. Thus,
It is imperative that a survival model produces well-stratified risk groups.
Secondly, until now, survival modelling was done in a two-stage approach
(encoding and aggregation). The massive amount of pixels in digitized whole
slide images were never utilized to their fullest extent due to technological
constraints on data processing, forcing decoupled learning. EPIC-Survival
bridges encoding and aggregation into an end-to-end survival modelling
approach, while introducing stratification boosting to encourage the model to
not only optimize ranking, but also to discriminate between risk groups. In
this study we show that EPIC-Survival performs better than other approaches in
modelling intrahepatic cholangiocarcinoma, a historically difficult cancer to
model. Further, we show that stratification boosting improves further improves
model performance, resulting in a concordance-index of 0.880 on a held-out test
set. Finally, we were able to identify specific histologic differences, not
commonly sought out in ICC, between low and high risk groups.
</p>
<a href="http://arxiv.org/abs/2101.11085" target="_blank">arXiv:2101.11085</a> [<a href="http://arxiv.org/pdf/2101.11085" target="_blank">pdf</a>]

<h2>Non-Monotone Energy-Aware Information Gathering for Heterogeneous Robot Teams. (arXiv:2101.11093v1 [cs.RO])</h2>
<h3>Xiaoyi Cai, Brent Schlotfeldt, Kasra Khosoussi, Nikolay Atanasov, George J. Pappas, Jonathan P. How</h3>
<p>This paper considers the problem of planning trajectories for a team of
sensor-equipped robots to reduce uncertainty about a dynamical process.
Optimizing the trade-off between information gain and energy cost (e.g.,
control effort, energy expenditure, distance travelled) is desirable but leads
to a non-monotone objective function in the set of robot trajectories.
Therefore, common multi-robot planning algorithms based on techniques such as
coordinate descent lose their performance guarantees. Methods based on local
search provide performance guarantees for optimizing a non-monotone submodular
function, but require access to all robots' trajectories, making it not
suitable for distributed execution. This work proposes a distributed planning
approach based on local search, and shows how to reduce its computation and
communication requirements without sacrificing algorithm performance. We
demonstrate the efficacy of our proposed method by coordinating robot teams
composed of both ground and aerial vehicles with different sensing and control
profiles, and evaluate the algorithm's performance in two target tracking
scenarios. Our results show up to 60% communication reduction and 80-92%
computation reduction on average when coordinating up to 10 robots, while
outperforming the coordinate descent based algorithm in achieving a desirable
trade-off between sensing and energy expenditure.
</p>
<a href="http://arxiv.org/abs/2101.11093" target="_blank">arXiv:2101.11093</a> [<a href="http://arxiv.org/pdf/2101.11093" target="_blank">pdf</a>]

<h2>Pitfalls of Assessing Extracted Hierarchies for Multi-Class Classification. (arXiv:2101.11095v1 [cs.LG])</h2>
<h3>Pablo del Moral, Slawomir Nowaczyk, Anita Sant&#x27;Anna, Sepideh Pashami</h3>
<p>Using hierarchies of classes is one of the standard methods to solve
multi-class classification problems. In the literature, selecting the right
hierarchy is considered to play a key role in improving classification
performance. Although different methods have been proposed, there is still a
lack of understanding of what makes one method to extract hierarchies perform
better or worse. To this effect, we analyze and compare some of the most
popular approaches to extracting hierarchies. We identify some common pitfalls
that may lead practitioners to make misleading conclusions about their methods.
In addition, to address some of these problems, we demonstrate that using
random hierarchies is an appropriate benchmark to assess how the hierarchy's
quality affects the classification performance. In particular, we show how the
hierarchy's quality can become irrelevant depending on the experimental setup:
when using powerful enough classifiers, the final performance is not affected
by the quality of the hierarchy. We also show how comparing the effect of the
hierarchies against non-hierarchical approaches might incorrectly indicate
their superiority. Our results confirm that datasets with a high number of
classes generally present complex structures in how these classes relate to
each other. In these datasets, the right hierarchy can dramatically improve
classification performance.
</p>
<a href="http://arxiv.org/abs/2101.11095" target="_blank">arXiv:2101.11095</a> [<a href="http://arxiv.org/pdf/2101.11095" target="_blank">pdf</a>]

<h2>Autonomous Off-road Navigation over Extreme Terrains with Perceptually-challenging Conditions. (arXiv:2101.11110v1 [cs.RO])</h2>
<h3>Rohan Thakker, Nikhilesh Alatur, David D. Fan, Jesus Tordesillas, Michael Paton, Kyohei Otsu, Olivier Toupet, Ali-akbar Agha-mohammadi</h3>
<p>We propose a framework for resilient autonomous navigation in perceptually
challenging unknown environments with mobility-stressing elements such as
uneven surfaces with rocks and boulders, steep slopes, negative obstacles like
cliffs and holes, and narrow passages. Environments are GPS-denied and
perceptually-degraded with variable lighting from dark to lit and obscurants
(dust, fog, smoke). Lack of prior maps and degraded communication eliminates
the possibility of prior or off-board computation or operator intervention.
This necessitates real-time on-board computation using noisy sensor data. To
address these challenges, we propose a resilient architecture that exploits
redundancy and heterogeneity in sensing modalities. Further resilience is
achieved by triggering recovery behaviors upon failure. We propose a fast
settling algorithm to generate robust multi-fidelity traversability estimates
in real-time. The proposed approach was deployed on multiple physical systems
including skid-steer and tracked robots, a high-speed RC car and legged robots,
as a part of Team CoSTAR's effort to the DARPA Subterranean Challenge, where
the team won 2nd and 1st place in the Tunnel and Urban Circuits, respectively.
</p>
<a href="http://arxiv.org/abs/2101.11110" target="_blank">arXiv:2101.11110</a> [<a href="http://arxiv.org/pdf/2101.11110" target="_blank">pdf</a>]

<h2>Automatic Comic Generation with Stylistic Multi-page Layouts and Emotion-driven Text Balloon Generation. (arXiv:2101.11111v1 [cs.CV])</h2>
<h3>Xin Yang, Zongliang Ma, Letian Yu, Ying Cao, Baocai Yin, Xiaopeng Wei, Qiang Zhang, Rynson W.H. Lau</h3>
<p>In this paper, we propose a fully automatic system for generating comic books
from videos without any human intervention. Given an input video along with its
subtitles, our approach first extracts informative keyframes by analyzing the
subtitles, and stylizes keyframes into comic-style images. Then, we propose a
novel automatic multi-page layout framework, which can allocate the images
across multiple pages and synthesize visually interesting layouts based on the
rich semantics of the images (e.g., importance and inter-image relation).
Finally, as opposed to using the same type of balloon as in previous works, we
propose an emotion-aware balloon generation method to create different types of
word balloons by analyzing the emotion of subtitles and audios. Our method is
able to vary balloon shapes and word sizes in balloons in response to different
emotions, leading to more enriched reading experience. Once the balloons are
generated, they are placed adjacent to their corresponding speakers via speaker
detection. Our results show that our method, without requiring any user inputs,
can generate high-quality comic pages with visually rich layouts and balloons.
Our user studies also demonstrate that users prefer our generated results over
those by state-of-the-art comic generation systems.
</p>
<a href="http://arxiv.org/abs/2101.11111" target="_blank">arXiv:2101.11111</a> [<a href="http://arxiv.org/pdf/2101.11111" target="_blank">pdf</a>]

<h2>Exact and Approximate Heterogeneous Bayesian Decentralized Data Fusion. (arXiv:2101.11116v1 [cs.RO])</h2>
<h3>Ofer Dagan, Nisar R. Ahmed</h3>
<p>In Bayesian peer-to-peer decentralized data fusion for static and dynamic
systems, the underlying estimated or communicated distributions are frequently
assumed to be homogeneous between agents. This requires each agent to process
and communicate the full global joint distribution, and thus leads to high
computation and communication costs irrespective of relevancy to specific local
objectives. This work considers a family of heterogeneous decentralized fusion
problems, where we consider the set of problems in which either the
communicated or the estimated distributions describe different, but
overlapping, states of interest that are subsets of a larger full global joint
state. We exploit the conditional independence structure of such problems and
provide a rigorous derivation for a family of exact and approximate
heterogeneous conditionally factorized channel filter methods. We further
extend existing methods for approximate conservative filtering and
decentralized fusion in heterogeneous dynamic problems. Numerical examples show
more than 99.5% potential communication reduction for heterogeneous channel
filter fusion, and a multi-target tracking simulation shows that these methods
provide consistent estimates.
</p>
<a href="http://arxiv.org/abs/2101.11116" target="_blank">arXiv:2101.11116</a> [<a href="http://arxiv.org/pdf/2101.11116" target="_blank">pdf</a>]

<h2>Accuracy and Privacy Evaluations of Collaborative Data Analysis. (arXiv:2101.11144v1 [cs.LG])</h2>
<h3>Akira Imakura, Anna Bogdanova, Takaya Yamazoe, Kazumasa Omote, Tetsuya Sakurai</h3>
<p>Distributed data analysis without revealing the individual data has recently
attracted significant attention in several applications. A collaborative data
analysis through sharing dimensionality reduced representations of data has
been proposed as a non-model sharing-type federated learning. This paper
analyzes the accuracy and privacy evaluations of this novel framework. In the
accuracy analysis, we provided sufficient conditions for the equivalence of the
collaborative data analysis and the centralized analysis with dimensionality
reduction. In the privacy analysis, we proved that collaborative users' private
datasets are protected with a double privacy layer against insider and external
attacking scenarios.
</p>
<a href="http://arxiv.org/abs/2101.11144" target="_blank">arXiv:2101.11144</a> [<a href="http://arxiv.org/pdf/2101.11144" target="_blank">pdf</a>]

<h2>On the Importance of Capturing a Sufficient Diversity of Perspective for the Classification of micro-PCBs. (arXiv:2101.11164v1 [cs.CV])</h2>
<h3>Adam Byerly, Tatiana Kalganova, Anthony J. Grichnik</h3>
<p>We present a dataset consisting of high-resolution images of 13 micro-PCBs
captured in various rotations and perspectives relative to the camera, with
each sample labeled for PCB type, rotation category, and perspective
categories. We then present the design and results of experimentation on
combinations of rotations and perspectives used during training and the
resulting impact on test accuracy. We then show when and how well data
augmentation techniques are capable of simulating rotations vs. perspectives
not present in the training data. We perform all experiments using CNNs with
and without homogeneous vector capsules (HVCs) and investigate and show the
capsules' ability to better encode the equivariance of the sub-components of
the micro-PCBs. The results of our experiments lead us to conclude that
training a neural network equipped with HVCs, capable of modeling equivariance
among sub-components, coupled with training on a diversity of perspectives,
achieves the greatest classification accuracy on micro-PCB data.
</p>
<a href="http://arxiv.org/abs/2101.11164" target="_blank">arXiv:2101.11164</a> [<a href="http://arxiv.org/pdf/2101.11164" target="_blank">pdf</a>]

<h2>Graph Neural Network for Traffic Forecasting: A Survey. (arXiv:2101.11174v1 [cs.LG])</h2>
<h3>Weiwei Jiang, Jiayun Luo</h3>
<p>Traffic forecasting is an important factor for the success of intelligent
transportation systems. Deep learning models including convolution neural
networks and recurrent neural networks have been applied in traffic forecasting
problems to model the spatial and temporal dependencies. In recent years, to
model the graph structures in the transportation systems as well as the
contextual information, graph neural networks (GNNs) are introduced as new
tools and have achieved the state-of-the-art performance in a series of traffic
forecasting problems. In this survey, we review the rapidly growing body of
recent research using different GNNs, e.g., graph convolutional and graph
attention networks, in various traffic forecasting problems, e.g., road traffic
flow and speed forecasting, passenger flow forecasting in urban rail transit
systems, demand forecasting in ride-hailing platforms, etc. We also present a
collection of open data and source resources for each problem, as well as
future research directions. To the best of our knowledge, this paper is the
first comprehensive survey that explores the application of graph neural
networks for traffic forecasting problems. We have also created a public Github
repository to update the latest papers, open data and source resources.
</p>
<a href="http://arxiv.org/abs/2101.11174" target="_blank">arXiv:2101.11174</a> [<a href="http://arxiv.org/pdf/2101.11174" target="_blank">pdf</a>]

<h2>DeepOIS: Gyroscope-Guided Deep Optical Image Stabilizer Compensation. (arXiv:2101.11183v1 [cs.CV])</h2>
<h3>Haipeng Li, Shuaicheng Liu, Jue Wang</h3>
<p>Mobile captured images can be aligned using their gyroscope sensors. Optical
image stabilizer (OIS) terminates this possibility by adjusting the images
during the capturing. In this work, we propose a deep network that compensates
the motions caused by the OIS, such that the gyroscopes can be used for image
alignment on the OIS cameras. To achieve this, first, we record both videos and
gyroscopes with an OIS camera as training data. Then, we convert gyroscope
readings into motion fields. Second, we propose a Fundamental Mixtures motion
model for rolling shutter cameras, where an array of rotations within a frame
are extracted as the ground-truth guidance. Third, we train a convolutional
neural network with gyroscope motions as input to compensate for the OIS
motion. Once finished, the compensation network can be applied for other
scenes, where the image alignment is purely based on gyroscopes with no need
for images contents, delivering strong robustness. Experiments show that our
results are comparable with that of non-OIS cameras, and outperform image-based
alignment results with a relatively large margin.
</p>
<a href="http://arxiv.org/abs/2101.11183" target="_blank">arXiv:2101.11183</a> [<a href="http://arxiv.org/pdf/2101.11183" target="_blank">pdf</a>]

<h2>Evolutionary Generative Adversarial Networks with Crossover Based Knowledge Distillation. (arXiv:2101.11186v1 [cs.LG])</h2>
<h3>Junjie Li, Junwei Zhang, Xiaoyu Gong, Shuai L&#xfc;</h3>
<p>Generative Adversarial Networks (GAN) is an adversarial model, and it has
been demonstrated to be effective for various generative tasks. However, GAN
and its variants also suffer from many training problems, such as mode collapse
and gradient vanish. In this paper, we firstly propose a general crossover
operator, which can be widely applied to GANs using evolutionary strategies.
Then we design an evolutionary GAN framework C-GAN based on it. And we combine
the crossover operator with evolutionary generative adversarial networks (EGAN)
to implement the evolutionary generative adversarial networks with crossover
(CE-GAN). Under the premise that a variety of loss functions are used as
mutation operators to generate mutation individuals, we evaluate the generated
samples and allow the mutation individuals to learn experiences from the output
in a knowledge distillation manner, imitating the best output outcome,
resulting in better offspring. Then, we greedily selected the best offspring as
parents for subsequent training using discriminator as evaluator. Experiments
on real datasets demonstrate the effectiveness of CE-GAN and show that our
method is competitive in terms of generated images quality and time efficiency.
</p>
<a href="http://arxiv.org/abs/2101.11186" target="_blank">arXiv:2101.11186</a> [<a href="http://arxiv.org/pdf/2101.11186" target="_blank">pdf</a>]

<h2>Arbitrary-Oriented Ship Detection through Center-Head Point Extraction. (arXiv:2101.11189v1 [cs.CV])</h2>
<h3>Feng Zhang, Xueying Wang, Shilin Zhou, Yingqian Wang</h3>
<p>Ship detection in remote sensing images plays a crucial role in military and
civil applications and has drawn increasing attention in recent years. However,
existing multi-oriented ship detection methods are generally developed on a set
of predefined rotated anchor boxes. These predefined boxes not only lead to
inaccurate angle predictions but also introduce extra hyper-parameters and high
computational cost. Moreover, the prior knowledge of ship size has not been
fully exploited by existing methods, which hinders the improvement of their
detection accuracy. Aiming at solving the above issues, in this paper, we
propose a center-head point extraction based detector (named CHPDet) to achieve
arbitrary-oriented ship detection in remote sensing images. Our CHPDet
formulates arbitrary-oriented ships as rotated boxes with head points which are
used to determine the direction. Key-point estimation is performed to find the
center of ships. Then the size and head points of the ship is regressed.
Finally, we use the target size as prior to finetune the results. Moreover, we
introduce a new dataset for multi-class arbitrary-oriented ship detection in
remote sensing Images at fixed ground sample distance (GSD) which is named
FGSD2021. Experimental results on two ship detection datasets (i.e., FGSD2021
and HRSC2016) demonstrate that our CHPDet achieves state-of-the-art performance
and can well distinguish bow and stern. The code and dataset will be made
publicly available.
</p>
<a href="http://arxiv.org/abs/2101.11189" target="_blank">arXiv:2101.11189</a> [<a href="http://arxiv.org/pdf/2101.11189" target="_blank">pdf</a>]

<h2>Safe Multi-Agent Reinforcement Learning via Shielding. (arXiv:2101.11196v1 [cs.LG])</h2>
<h3>Ingy Elsayed-Aly, Suda Bharadwaj, Christopher Amato, R&#xfc;diger Ehlers, Ufuk Topcu, Lu Feng</h3>
<p>Multi-agent reinforcement learning (MARL) has been increasingly used in a
wide range of safety-critical applications, which require guaranteed safety
(e.g., no unsafe states are ever visited) during the learning
process.Unfortunately, current MARL methods do not have safety guarantees.
Therefore, we present two shielding approaches for safe MARL. In centralized
shielding, we synthesize a single shield to monitor all agents' joint actions
and correct any unsafe action if necessary. In factored shielding, we
synthesize multiple shields based on a factorization of the joint state space
observed by all agents; the set of shields monitors agents concurrently and
each shield is only responsible for a subset of agents at each
step.Experimental results show that both approaches can guarantee the safety of
agents during learning without compromising the quality of learned policies;
moreover, factored shielding is more scalable in the number of agents than
centralized shielding.
</p>
<a href="http://arxiv.org/abs/2101.11196" target="_blank">arXiv:2101.11196</a> [<a href="http://arxiv.org/pdf/2101.11196" target="_blank">pdf</a>]

<h2>Similarity of Classification Tasks. (arXiv:2101.11201v1 [cs.LG])</h2>
<h3>Cuong Nguyen, Thanh-Toan Do, Gustavo Carneiro</h3>
<p>Recent advances in meta-learning has led to remarkable performances on
several few-shot learning benchmarks. However, such success often ignores the
similarity between training and testing tasks, resulting in a potential bias
evaluation. We, therefore, propose a generative approach based on a variant of
Latent Dirichlet Allocation to analyse task similarity to optimise and better
understand the performance of meta-learning. We demonstrate that the proposed
method can provide an insightful evaluation for meta-learning algorithms on two
few-shot classification benchmarks that matches common intuition: the more
similar the higher performance. Based on this similarity measure, we propose a
task-selection strategy for meta-learning and show that it can produce more
accurate classification results than methods that randomly select training
tasks.
</p>
<a href="http://arxiv.org/abs/2101.11201" target="_blank">arXiv:2101.11201</a> [<a href="http://arxiv.org/pdf/2101.11201" target="_blank">pdf</a>]

<h2>Achieving Linear Speedup with Partial Worker Participation in Non-IID Federated Learning. (arXiv:2101.11203v1 [cs.LG])</h2>
<h3>Haibo Yang, Minghong Fang, Jia Liu</h3>
<p>Federated learning (FL) is a distributed machine learning architecture that
leverages a large number of workers to jointly learn a model with decentralized
data. FL has received increasing attention in recent years thanks to its data
privacy protection, communication efficiency and a linear speedup for
convergence in training (i.e., convergence performance increases linearly with
respect to the number of workers). However, existing studies on linear speedup
for convergence are only limited to the assumptions of i.i.d. datasets across
workers and/or full worker participation, both of which rarely hold in
practice. So far, it remains an open question whether or not the linear speedup
for convergence is achievable under non-i.i.d. datasets with partial worker
participation in FL. In this paper, we show that the answer is affirmative.
Specifically, we show that the federated averaging (FedAvg) algorithm (with
two-sided learning rates) on non-i.i.d. datasets in non-convex settings
achieves a convergence rate $\mathcal{O}(\frac{1}{\sqrt{mKT}} + \frac{1}{T})$
for full worker participation and a convergence rate
$\mathcal{O}(\frac{1}{\sqrt{nKT}} + \frac{1}{T})$ for partial worker
participation, where $K$ is the number of local steps, $T$ is the number of
total communication rounds, $m$ is the total worker number and $n$ is the
worker number in one communication round if for partial worker participation.
Our results also reveal that the local steps in FL could help the convergence
and show that the maximum number of local steps can be improved to $T/m$. We
conduct extensive experiments on MNIST and CIFAR-10 to verify our theoretical
results.
</p>
<a href="http://arxiv.org/abs/2101.11203" target="_blank">arXiv:2101.11203</a> [<a href="http://arxiv.org/pdf/2101.11203" target="_blank">pdf</a>]

<h2>Automated Crop Field Surveillance using Computer Vision. (arXiv:2101.11217v1 [cs.CV])</h2>
<h3>Tejas Atul Khare, Anuradha C. Phadke</h3>
<p>Artificial Intelligence is everywhere today. But unfortunately, Agriculture
has not been able to get that much attention from Artificial Intelligence (AI).
A lack of automation persists in the agriculture industry. For over many years,
farmers and crop field owners have been facing a problem of trespassing of wild
animals for which no feasible solution has been provided. Installing a fence or
barrier like structure is neither feasible nor efficient due to the large areas
covered by the fields. Also, if the landowner can afford to build a wall or
barrier, government policies for building walls are often very irksome. The
paper intends to give a simple intelligible solution to the problem with
Automated Crop Field Surveillance using Computer Vision. The solution will
significantly reduce the cost of crops destroyed annually and completely
automate the security of the field.
</p>
<a href="http://arxiv.org/abs/2101.11217" target="_blank">arXiv:2101.11217</a> [<a href="http://arxiv.org/pdf/2101.11217" target="_blank">pdf</a>]

<h2>Learning task-agnostic representation via toddler-inspired learning. (arXiv:2101.11221v1 [cs.AI])</h2>
<h3>Kwanyoung Park, Junseok Park, Hyunseok Oh, Byoung-Tak Zhang, Youngki Lee</h3>
<p>One of the inherent limitations of current AI systems, stemming from the
passive learning mechanisms (e.g., supervised learning), is that they perform
well on labeled datasets but cannot deduce knowledge on their own. To tackle
this problem, we derive inspiration from a highly intentional learning system
via action: the toddler. Inspired by the toddler's learning procedure, we
design an interactive agent that can learn and store task-agnostic visual
representation while exploring and interacting with objects in the virtual
environment. Experimental results show that such obtained representation was
expandable to various vision tasks such as image classification, object
localization, and distance estimation tasks. In specific, the proposed model
achieved 100%, 75.1% accuracy and 1.62% relative error, respectively, which is
noticeably better than autoencoder-based model (99.7%, 66.1%, 1.95%), and also
comparable with those of supervised models (100%, 87.3%, 0.71%).
</p>
<a href="http://arxiv.org/abs/2101.11221" target="_blank">arXiv:2101.11221</a> [<a href="http://arxiv.org/pdf/2101.11221" target="_blank">pdf</a>]

<h2>Automatic image annotation base on Naive Bayes and Decision Tree classifiers using MPEG-7. (arXiv:2101.11222v1 [cs.CV])</h2>
<h3>Jafar Majidpour, Samer Kais Jameel</h3>
<p>Recently it has become essential to search for and retrieve high-resolution
and efficient images easily due to swift development of digital images, many
present annotation algorithms facing a big challenge which is the variance for
represent the image where high level represent image semantic and low level
illustrate the features, this issue is known as semantic gab. This work has
been used MPEG-7 standard to extract the features from the images, where the
color feature was extracted by using Scalable Color Descriptor (SCD) and Color
Layout Descriptor (CLD), whereas the texture feature was extracted by employing
Edge Histogram Descriptor (EHD), the CLD produced high dimensionality feature
vector therefore it is reduced by Principal Component Analysis (PCA). The
features that have extracted by these three descriptors could be passing to the
classifiers (Naive Bayes and Decision Tree) for training. Finally, they
annotated the query image. In this study TUDarmstadt image bank had been used.
The results of tests and comparative performance evaluation indicated better
precision and executing time of Naive Bayes classification in comparison with
Decision Tree classification.
</p>
<a href="http://arxiv.org/abs/2101.11222" target="_blank">arXiv:2101.11222</a> [<a href="http://arxiv.org/pdf/2101.11222" target="_blank">pdf</a>]

<h2>Multi-Hypothesis Pose Networks: Rethinking Top-Down Pose Estimation. (arXiv:2101.11223v1 [cs.CV])</h2>
<h3>Rawal Khirodkar, Visesh Chari, Amit Agrawal, Ambrish Tyagi</h3>
<p>A key assumption of top-down human pose estimation approaches is their
expectation of having a single person present in the input bounding box. This
often leads to failures in crowded scenes with occlusions. We propose a novel
solution to overcome the limitations of this fundamental assumption. Our
Multi-Hypothesis Pose Network (MHPNet) allows for predicting multiple 2D poses
within a given bounding box. We introduce a Multi-Hypothesis Attention Block
(MHAB) that can adaptively modulate channel-wise feature responses for each
hypothesis and is parameter efficient. We demonstrate the efficacy of our
approach by evaluating on COCO, CrowdPose, and OCHuman datasets. Specifically,
we achieve 70.0 AP on CrowdPose and 42.5 AP on OCHuman test sets, a significant
improvement of 2.4 AP and 6.5 AP over the prior art, respectively. When using
ground truth bounding boxes for inference, MHPNet achieves an improvement of
0.7 AP on COCO, 0.9 AP on CrowdPose, and 9.1 AP on OCHuman validation sets
compared to HRNet. Interestingly, when fewer, high confidence bounding boxes
are used, HRNet's performance degrades (by 5 AP) on OCHuman, whereas MHPNet
maintains a relatively stable performance (a drop of 1 AP) for the same inputs.
</p>
<a href="http://arxiv.org/abs/2101.11223" target="_blank">arXiv:2101.11223</a> [<a href="http://arxiv.org/pdf/2101.11223" target="_blank">pdf</a>]

<h2>Reciprocal Landmark Detection and Tracking with Extremely Few Annotations. (arXiv:2101.11224v1 [cs.CV])</h2>
<h3>Jianzhe Lin, Ghazal Sahebzamani, Christina Luong, Fatemeh Taheri Dezaki, Mohammad Jafari, Purang Abolmaesumi, Teresa Tsang</h3>
<p>Localization of anatomical landmarks to perform two-dimensional measurements
in echocardiography is part of routine clinical workflow in cardiac disease
diagnosis. Automatic localization of those landmarks is highly desirable to
improve workflow and reduce interobserver variability. Training a machine
learning framework to perform such localization is hindered given the sparse
nature of gold standard labels; only few percent of cardiac cine series frames
are normally manually labeled for clinical use. In this paper, we propose a new
end-to-end reciprocal detection and tracking model that is specifically
designed to handle the sparse nature of echocardiography labels. The model is
trained using few annotated frames across the entire cardiac cine sequence to
generate consistent detection and tracking of landmarks, and an adversarial
training for the model is proposed to take advantage of these annotated frames.
The superiority of the proposed reciprocal model is demonstrated using a series
of experiments.
</p>
<a href="http://arxiv.org/abs/2101.11224" target="_blank">arXiv:2101.11224</a> [<a href="http://arxiv.org/pdf/2101.11224" target="_blank">pdf</a>]

<h2>GaitGraph: Graph Convolutional Network for Skeleton-Based Gait Recognition. (arXiv:2101.11228v1 [cs.CV])</h2>
<h3>Torben Teepe, Ali Khan, Johannes Gilg, Fabian Herzog, Stefan H&#xf6;rmann, Gerhard Rigoll</h3>
<p>Gait recognition is a promising video-based biometric for identifying
individual walking patterns from a long distance. At present, most gait
recognition methods use silhouette images to represent a person in each frame.
However, silhouette images can lose fine-grained spatial information, and most
papers do not regard how to obtain these silhouettes in complex scenes.
Furthermore, silhouette images contain not only gait features but also other
visual clues that can be recognized. Hence these approaches can not be
considered as strict gait recognition.

We leverage recent advances in human pose estimation to estimate robust
skeleton poses directly from RGB images to bring back model-based gait
recognition with a cleaner representation of gait. Thus, we propose GaitGraph
that combines skeleton poses with Graph Convolutional Network (GCN) to obtain a
modern model-based approach for gait recognition. The main advantages are a
cleaner, more elegant extraction of the gait features and the ability to
incorporate powerful spatio-temporal modeling using GCN. Experiments on the
popular CASIA-B gait dataset show that our method archives state-of-the-art
performance in model-based gait recognition.

The code and models are publicly available.
</p>
<a href="http://arxiv.org/abs/2101.11228" target="_blank">arXiv:2101.11228</a> [<a href="http://arxiv.org/pdf/2101.11228" target="_blank">pdf</a>]

<h2>Im2Mesh GAN: Accurate 3D Hand Mesh Recovery from a Single RGB Image. (arXiv:2101.11239v1 [cs.CV])</h2>
<h3>Akila Pemasiri, Kien Nguyen Thanh, Sridha Sridharan, Clinton Fookes</h3>
<p>This work addresses hand mesh recovery from a single RGB image. In contrast
to most of the existing approaches where the parametric hand models are
employed as the prior, we show that the hand mesh can be learned directly from
the input image. We propose a new type of GAN called Im2Mesh GAN to learn the
mesh through end-to-end adversarial training. By interpreting the mesh as a
graph, our model is able to capture the topological relationship among the mesh
vertices. We also introduce a 3D surface descriptor into the GAN architecture
to further capture the 3D features associated. We experiment two approaches
where one can reap the benefits of coupled groundtruth data availability of
images and the corresponding meshes, while the other combats the more
challenging problem of mesh estimations without the corresponding groundtruth.
Through extensive evaluations we demonstrate that the proposed method
outperforms the state-of-the-art.
</p>
<a href="http://arxiv.org/abs/2101.11239" target="_blank">arXiv:2101.11239</a> [<a href="http://arxiv.org/pdf/2101.11239" target="_blank">pdf</a>]

<h2>Convolutional Neural Network-Based Age Estimation Using B-Mode Ultrasound Tongue Image. (arXiv:2101.11245v1 [cs.CV])</h2>
<h3>Kele Xu, Tamas G&#xe1;bor Csap&#xf3;, Ming Feng</h3>
<p>Ultrasound tongue imaging is widely used for speech production research, and
it has attracted increasing attention as its potential applications seem to be
evident in many different fields, such as the visual biofeedback tool for
second language acquisition and silent speech interface. Unlike previous
studies, here we explore the feasibility of age estimation using the ultrasound
tongue image of the speakers. Motivated by the success of deep learning, this
paper leverages deep learning on this task. We train a deep convolutional
neural network model on the UltraSuite dataset. The deep model achieves mean
absolute error (MAE) of 2.03 for the data from typically developing children,
while MAE is 4.87 for the data from the children with speech sound disorders,
which suggest that age estimation using ultrasound is more challenging for the
children with speech sound disorder. The developed method can be used a tool to
evaluate the performance of speech therapy sessions. It is also worthwhile to
notice that, although we leverage the ultrasound tongue imaging for our study,
the proposed methods may also be extended to other imaging modalities (e.g.
MRI) to assist the studies on speech production.
</p>
<a href="http://arxiv.org/abs/2101.11245" target="_blank">arXiv:2101.11245</a> [<a href="http://arxiv.org/pdf/2101.11245" target="_blank">pdf</a>]

<h2>Efficient Video Summarization Framework using EEG and Eye-tracking Signals. (arXiv:2101.11249v1 [cs.CV])</h2>
<h3>Sai Sukruth Bezugam, Swatilekha Majumdar, Chetan Ralekar, Tapan Kumar Gandhi</h3>
<p>This paper proposes an efficient video summarization framework that will give
a gist of the entire video in a few key-frames or video skims. Existing video
summarization frameworks are based on algorithms that utilize computer vision
low-level feature extraction or high-level domain level extraction. However,
being the ultimate user of the summarized video, humans remain the most
neglected aspect. Therefore, the proposed paper considers human's role in
summarization and introduces human visual attention-based summarization
techniques. To understand human attention behavior, we have designed and
performed experiments with human participants using electroencephalogram (EEG)
and eye-tracking technology. The EEG and eye-tracking data obtained from the
experimentation are processed simultaneously and used to segment frames
containing useful information from a considerable video volume. Thus, the frame
segmentation primarily relies on the cognitive judgments of human beings. Using
our approach, a video is summarized by 96.5% while maintaining higher precision
and high recall factors. The comparison with the state-of-the-art techniques
demonstrates that the proposed approach yields ceiling-level performance with
reduced computational cost in summarising the videos.
</p>
<a href="http://arxiv.org/abs/2101.11249" target="_blank">arXiv:2101.11249</a> [<a href="http://arxiv.org/pdf/2101.11249" target="_blank">pdf</a>]

<h2>e-ACJ: Accurate Junction Extraction For Event Cameras. (arXiv:2101.11251v1 [cs.CV])</h2>
<h3>Zhihao Liu, Yuqian Fu</h3>
<p>Junctions reflect the important geometrical structure information of the
image, and are of primary significance to applications such as image matching
and motion analysis. Previous event-based feature extraction methods are mainly
focused on corners, which mainly find their locations, however, ignoring the
geometrical structure information like orientations and scales of edges. This
paper adapts the frame-based a-contrario junction detector(ACJ) to event data,
proposing the event-based a-contrario junction detector(e-ACJ), which yields
junctions' locations while giving the scales and orientations of their
branches. The proposed method relies on an a-contrario model and can operate on
asynchronous events directly without generating synthesized event frames. We
evaluate the performance on public event datasets. The result shows our method
successfully finds the orientations and scales of branches, while maintaining
high accuracy in junction's location.
</p>
<a href="http://arxiv.org/abs/2101.11251" target="_blank">arXiv:2101.11251</a> [<a href="http://arxiv.org/pdf/2101.11251" target="_blank">pdf</a>]

<h2>Puzzle-CAM: Improved localization via matching partial and full features. (arXiv:2101.11253v1 [cs.CV])</h2>
<h3>Sanhyun Jo, In-Jae Yu</h3>
<p>Weakly-supervised semantic segmentation (WSSS) is introduced to narrow the
gap for semantic segmentation performance from pixel-level supervision to
image-level supervision. Most advanced approaches are based on class activation
maps (CAMs) to generate pseudo-labels to train the segmentation network. The
main limitation of WSSS is that the process of generating pseudo-labels from
CAMs that use an image classifier is mainly focused on the most discriminative
parts of the objects. To address this issue, we propose Puzzle-CAM, a process
that minimizes differences between the features from separate patches and the
whole image. Our method consists of a puzzle module and two regularization
terms to discover the most integrated region in an object. Puzzle-CAM can
activate the overall region of an object using image-level supervision without
requiring extra parameters. % In experiments, Puzzle-CAM outperformed previous
state-of-the-art methods using the same labels for supervision on the PASCAL
VOC 2012 test dataset. In experiments, Puzzle-CAM outperformed previous
state-of-the-art methods using the same labels for supervision on the PASCAL
VOC 2012 dataset. Code associated with our experiments is available at
\url{https://github.com/OFRIN/PuzzleCAM}.
</p>
<a href="http://arxiv.org/abs/2101.11253" target="_blank">arXiv:2101.11253</a> [<a href="http://arxiv.org/pdf/2101.11253" target="_blank">pdf</a>]

<h2>Partition of unity networks: deep hp-approximation. (arXiv:2101.11256v1 [cs.LG])</h2>
<h3>Kookjin Lee, Nathaniel A. Trask, Ravi G. Patel, Mamikon A. Gulian, Eric C. Cyr</h3>
<p>Approximation theorists have established best-in-class optimal approximation
rates of deep neural networks by utilizing their ability to simultaneously
emulate partitions of unity and monomials. Motivated by this, we propose
partition of unity networks (POUnets) which incorporate these elements directly
into the architecture. Classification architectures of the type used to learn
probability measures are used to build a meshfree partition of space, while
polynomial spaces with learnable coefficients are associated to each partition.
The resulting hp-element-like approximation allows use of a fast least-squares
optimizer, and the resulting architecture size need not scale exponentially
with spatial dimension, breaking the curse of dimensionality. An abstract
approximation result establishes desirable properties to guide network design.
Numerical results for two choices of architecture demonstrate that POUnets
yield hp-convergence for smooth functions and consistently outperform MLPs for
piecewise polynomial functions with large numbers of discontinuities.
</p>
<a href="http://arxiv.org/abs/2101.11256" target="_blank">arXiv:2101.11256</a> [<a href="http://arxiv.org/pdf/2101.11256" target="_blank">pdf</a>]

<h2>TorchPRISM: Principal Image Sections Mapping, a novel method for Convolutional Neural Network features visualization. (arXiv:2101.11266v1 [cs.CV])</h2>
<h3>Tomasz Szandala</h3>
<p>In this paper we introduce a tool called Principal Image Sections Mapping -
PRISM, dedicated for PyTorch, but can be easily ported to other deep learning
frameworks. Presented software relies on Principal Component Analysis to
visualize the most significant features recognized by a given Convolutional
Neural Network. Moreover, it allows to display comparative set features between
images processed in the same batch, therefore PRISM can be a method well
synerging with technique Explanation by Example.
</p>
<a href="http://arxiv.org/abs/2101.11266" target="_blank">arXiv:2101.11266</a> [<a href="http://arxiv.org/pdf/2101.11266" target="_blank">pdf</a>]

<h2>Deep Image Retrieval: A Survey. (arXiv:2101.11282v1 [cs.CV])</h2>
<h3>Wei Chen, Yu Liu, Weiping Wang, Erwin Bakker, Theodoros Georgiou, Paul Fieguth, Li Liu, Michael S. Lew</h3>
<p>In recent years a vast amount of visual content has been generated and shared
from various fields, such as social media platforms, medical images, and
robotics. This abundance of content creation and sharing has introduced new
challenges. In particular, searching databases for similar content, i.e.
content based image retrieval (CBIR), is a long-established research area, and
more efficient and accurate methods are needed for real time retrieval.
Artificial intelligence has made progress in CBIR and has significantly
facilitated the process of intelligent search. In this survey we organize and
review recent CBIR works that are developed based on deep learning algorithms
and techniques, including insights and techniques from recent papers. We
identify and present the commonly-used databases, benchmarks, and evaluation
methods used in the field. We collect common challenges and propose promising
future directions. More specifically, we focus on image retrieval with deep
learning and organize the state of the art methods according to the types of
deep network structure, deep features, feature enhancement methods, and network
fine-tuning strategies. Our survey considers a wide variety of recent methods,
aiming to promote a global view of the field of category-based CBIR.
</p>
<a href="http://arxiv.org/abs/2101.11282" target="_blank">arXiv:2101.11282</a> [<a href="http://arxiv.org/pdf/2101.11282" target="_blank">pdf</a>]

<h2>A Note on the Representation Power of GHHs. (arXiv:2101.11286v1 [cs.LG])</h2>
<h3>Zhou Lu</h3>
<p>In this note we prove a sharp lower bound on the necessary number of nestings
of nested absolute-value functions of generalized hinging hyperplanes (GHH) to
represent arbitrary CPWL functions. Previous upper bound states that $n+1$
nestings is sufficient for GHH to achieve universal representation power, but
the corresponding lower bound was unknown. We prove that $n$ nestings is
necessary for universal representation power, which provides an almost tight
lower bound. We also show that one-hidden-layer neural networks don't have
universal approximation power over the whole domain. The analysis is based on a
key lemma showing that any finite sum of periodic functions is either
non-integrable or the zero function, which might be of independent interest.
</p>
<a href="http://arxiv.org/abs/2101.11286" target="_blank">arXiv:2101.11286</a> [<a href="http://arxiv.org/pdf/2101.11286" target="_blank">pdf</a>]

<h2>FedH2L: Federated Learning with Model and Statistical Heterogeneity. (arXiv:2101.11296v1 [cs.LG])</h2>
<h3>Yiying Li, Wei Zhou, Huaimin Wang, Haibo Mi, Timothy M. Hospedales</h3>
<p>Federated learning (FL) enables distributed participants to collectively
learn a strong global model without sacrificing their individual data privacy.
Mainstream FL approaches require each participant to share a common network
architecture and further assume that data are are sampled IID across
participants. However, in real-world deployments participants may require
heterogeneous network architectures; and the data distribution is almost
certainly non-uniform across participants. To address these issues we introduce
FedH2L, which is agnostic to both the model architecture and robust to
different data distributions across participants. In contrast to approaches
sharing parameters or gradients, FedH2L relies on mutual distillation,
exchanging only posteriors on a shared seed set between participants in a
decentralized manner. This makes it extremely bandwidth efficient, model
agnostic, and crucially produces models capable of performing well on the whole
data distribution when learning from heterogeneous silos.
</p>
<a href="http://arxiv.org/abs/2101.11296" target="_blank">arXiv:2101.11296</a> [<a href="http://arxiv.org/pdf/2101.11296" target="_blank">pdf</a>]

<h2>Edge-Labeling based Directed Gated Graph Network for Few-shot Learning. (arXiv:2101.11299v1 [cs.CV])</h2>
<h3>Peixiao Zheng, Xin Guo, Lin Qi</h3>
<p>Existing graph-network-based few-shot learning methods obtain similarity
between nodes through a convolution neural network (CNN). However, the CNN is
designed for image data with spatial information rather than vector form node
feature. In this paper, we proposed an edge-labeling-based directed gated graph
network (DGGN) for few-shot learning, which utilizes gated recurrent units to
implicitly update the similarity between nodes. DGGN is composed of a gated
node aggregation module and an improved gated recurrent unit (GRU) based edge
update module. Specifically, the node update module adopts a gate mechanism
using activation of edge feature, making a learnable node aggregation process.
Besides, improved GRU cells are employed in the edge update procedure to
compute the similarity between nodes. Further, this mechanism is beneficial to
gradient backpropagation through the GRU sequence across layers. Experiment
results conducted on two benchmark datasets show that our DGGN achieves a
comparable performance to the-state-of-art methods.
</p>
<a href="http://arxiv.org/abs/2101.11299" target="_blank">arXiv:2101.11299</a> [<a href="http://arxiv.org/pdf/2101.11299" target="_blank">pdf</a>]

<h2>Learning Non-linear Wavelet Transformation via Normalizing Flow. (arXiv:2101.11306v1 [cs.LG])</h2>
<h3>Shuo-Hui Li</h3>
<p>Wavelet transformation stands as a cornerstone in modern data analysis and
signal processing. Its mathematical essence is an invertible transformation
that discerns slow patterns from fast patterns in the frequency domain, which
repeats at each level. Such an invertible transformation can be learned by a
designed normalizing flow model. With a factor-out scheme resembling the
wavelet downsampling mechanism, a mutually independent prior, and parameter
sharing along the depth of the network, one can train normalizing flow models
to factor-out variables corresponding to fast patterns at different levels,
thus extending linear wavelet transformations to non-linear learnable models.
In this paper, a concrete way of building such flows is given. Then, a
demonstration of the model's ability in lossless compression task, progressive
loading, and super-resolution (upsampling) task. Lastly, an analysis of the
learned model in terms of low-pass/high-pass filters is given.
</p>
<a href="http://arxiv.org/abs/2101.11306" target="_blank">arXiv:2101.11306</a> [<a href="http://arxiv.org/pdf/2101.11306" target="_blank">pdf</a>]

<h2>OffCon$^3$: What is state of the art anyway?. (arXiv:2101.11331v1 [cs.LG])</h2>
<h3>Philip J. Ball, Stephen J. Roberts</h3>
<p>Two popular approaches to model-free continuous control tasks are SAC and
TD3. At first glance these approaches seem rather different; SAC aims to solve
the entropy-augmented MDP by minimising the KL-divergence between a stochastic
proposal policy and a hypotheical energy-basd soft Q-function policy, whereas
TD3 is derived from DPG, which uses a deterministic policy to perform policy
gradient ascent along the value function. In reality, both approaches are
remarkably similar, and belong to a family of approaches we call `Off-Policy
Continuous Generalized Policy Iteration'. This illuminates their similar
performance in most continuous control benchmarks, and indeed when
hyperparameters are matched, their performance can be statistically
indistinguishable. To further remove any difference due to implementation, we
provide OffCon$^3$ (Off-Policy Continuous Control: Consolidated), a code base
featuring state-of-the-art versions of both algorithms.
</p>
<a href="http://arxiv.org/abs/2101.11331" target="_blank">arXiv:2101.11331</a> [<a href="http://arxiv.org/pdf/2101.11331" target="_blank">pdf</a>]

<h2>On the Interpretability of Deep Learning Based Models for Knowledge Tracing. (arXiv:2101.11335v1 [cs.LG])</h2>
<h3>Xinyi Ding, Eric C. Larson</h3>
<p>Knowledge tracing allows Intelligent Tutoring Systems to infer which topics
or skills a student has mastered, thus adjusting curriculum accordingly. Deep
Learning based models like Deep Knowledge Tracing (DKT) and Dynamic Key-Value
Memory Network (DKVMN) have achieved significant improvements compared with
models like Bayesian Knowledge Tracing (BKT) and Performance Factors Analysis
(PFA). However, these deep learning based models are not as interpretable as
other models because the decision-making process learned by deep neural
networks is not wholly understood by the research community. In previous work,
we critically examined the DKT model, visualizing and analyzing the behaviors
of DKT in high dimensional space. In this work, we extend our original analyses
with a much larger dataset and add discussions about the memory states of the
DKVMN model. We discover that Deep Knowledge Tracing has some critical
pitfalls: 1) instead of tracking each skill through time, DKT is more likely to
learn an `ability' model; 2) the recurrent nature of DKT reinforces irrelevant
information that it uses during the tracking task; 3) an untrained recurrent
network can achieve similar results to a trained DKT model, supporting a
conclusion that recurrence relations are not properly learned and, instead,
improvements are simply a benefit of projection into a high dimensional, sparse
vector space. Based on these observations, we propose improvements and future
directions for conducting knowledge tracing research using deep neural network
models.
</p>
<a href="http://arxiv.org/abs/2101.11335" target="_blank">arXiv:2101.11335</a> [<a href="http://arxiv.org/pdf/2101.11335" target="_blank">pdf</a>]

<h2>Towards Improving the Consistency, Efficiency, and Flexibility of Differentiable Neural Architecture Search. (arXiv:2101.11342v1 [cs.CV])</h2>
<h3>Yibo Yang, Shan You, Hongyang Li, Fei Wang, Chen Qian, Zhouchen Lin</h3>
<p>Most differentiable neural architecture search methods construct a super-net
for search and derive a target-net as its sub-graph for evaluation. There
exists a significant gap between the architectures in search and evaluation. As
a result, current methods suffer from an inconsistent, inefficient, and
inflexible search process. In this paper, we introduce EnTranNAS that is
composed of Engine-cells and Transit-cells. The Engine-cell is differentiable
for architecture search, while the Transit-cell only transits a sub-graph by
architecture derivation. Consequently, the gap between the architectures in
search and evaluation is significantly reduced. Our method also spares much
memory and computation cost, which speeds up the search process. A feature
sharing strategy is introduced for more balanced optimization and more
efficient search. Furthermore, we develop an architecture derivation method to
replace the traditional one that is based on a hand-crafted rule. Our method
enables differentiable sparsification, and keeps the derived architecture
equivalent to that of Engine-cell, which further improves the consistency
between search and evaluation. Besides, it supports the search for topology
where a node can be connected to prior nodes with any number of connections, so
that the searched architectures could be more flexible. For experiments on
CIFAR-10, our search on the standard space requires only 0.06 GPU-day. We
further have an error rate of 2.22% with 0.07 GPU-day for the search on an
extended space. We can also directly perform the search on ImageNet with
topology learnable and achieve a top-1 error rate of 23.8% in 2.1 GPU-day.
</p>
<a href="http://arxiv.org/abs/2101.11342" target="_blank">arXiv:2101.11342</a> [<a href="http://arxiv.org/pdf/2101.11342" target="_blank">pdf</a>]

<h2>Decision Machines: Interpreting Decision Tree as a Model Combination Method. (arXiv:2101.11347v1 [cs.LG])</h2>
<h3>Jinxiong Zhang</h3>
<p>Based on decision trees, it is efficient to handle tabular data. Conventional
decision tree growth methods often result in suboptimal trees because of their
greedy nature. Their inherent structure limits the options of hardware to
implement decision trees in parallel. Here is a compact representation of
binary decision trees to overcome these deficiencies. We explicitly formulate
the dependence of prediction on binary tests for binary decision trees and
construct a function to guide the input sample from the root to the appropriate
leaf node. And based on this formulation we introduce a new interpretation of
binary decision trees. Then we approximate this formulation via continuous
functions. Finally, we interpret the decision tree as a model combination
method. And we propose the selection-prediction scheme to unify a few learning
methods.
</p>
<a href="http://arxiv.org/abs/2101.11347" target="_blank">arXiv:2101.11347</a> [<a href="http://arxiv.org/pdf/2101.11347" target="_blank">pdf</a>]

<h2>Bayesian Nested Neural Networks for Uncertainty Calibration and Adaptive Compression. (arXiv:2101.11353v1 [cs.LG])</h2>
<h3>Yufei Cui, Ziquan Liu, Qiao Li, Yu Mao, Antoni B. Chan, Chun Jason Xue</h3>
<p>Nested networks or slimmable networks are neural networks whose architectures
can be adjusted instantly during testing time, e.g., based on computational
constraints. Recent studies have focused on a "nested dropout" layer, which is
able to order the nodes of a layer by importance during training, thus
generating a nested set of sub-networks that are optimal for different
configurations of resources. However, the dropout rate is fixed as a
hyper-parameter over different layers during the whole training process.
Therefore, when nodes are removed, the performance decays in a human-specified
trajectory rather than in a trajectory learned from data. Another drawback is
the generated sub-networks are deterministic networks without well-calibrated
uncertainty. To address these two problems, we develop a Bayesian approach to
nested neural networks. We propose a variational ordering unit that draws
samples for nested dropout at a low cost, from a proposed Downhill
distribution, which provides useful gradients to the parameters of nested
dropout. Based on this approach, we design a Bayesian nested neural network
that learns the order knowledge of the node distributions. In experiments, we
show that the proposed approach outperforms the nested network in terms of
accuracy, calibration, and out-of-domain detection in classification tasks. It
also outperforms the related approach on uncertainty-critical tasks in computer
vision.
</p>
<a href="http://arxiv.org/abs/2101.11353" target="_blank">arXiv:2101.11353</a> [<a href="http://arxiv.org/pdf/2101.11353" target="_blank">pdf</a>]

<h2>Combat Data Shift in Few-shot Learning with Knowledge Graph. (arXiv:2101.11354v1 [cs.LG])</h2>
<h3>Yongchun zhu, Fuzhen Zhuang, Xiangliang Zhang, Zhiyuan Qi, Zhiping Shi, Qing He</h3>
<p>Many few-shot learning approaches have been designed under the meta-learning
framework, which learns from a variety of learning tasks and generalizes to new
tasks. These meta-learning approaches achieve the expected performance in the
scenario where all samples are drawn from the same distributions (i.i.d.
observations). However, in real-world applications, few-shot learning paradigm
often suffers from data shift, i.e., samples in different tasks, even in the
same task, could be drawn from various data distributions. Most existing
few-shot learning approaches are not designed with the consideration of data
shift, and thus show downgraded performance when data distribution shifts.
However, it is non-trivial to address the data shift problem in few-shot
learning, due to the limited number of labeled samples in each task. Targeting
at addressing this problem, we propose a novel metric-based meta-learning
framework to extract task-specific representations and task-shared
representations with the help of knowledge graph. The data shift within/between
tasks can thus be combated by the combination of task-shared and task-specific
representations. The proposed model is evaluated on popular benchmarks and two
constructed new challenging datasets. The evaluation results demonstrate its
remarkable performance.
</p>
<a href="http://arxiv.org/abs/2101.11354" target="_blank">arXiv:2101.11354</a> [<a href="http://arxiv.org/pdf/2101.11354" target="_blank">pdf</a>]

<h2>Detecting discriminatory risk through data annotation based on Bayesian inferences. (arXiv:2101.11358v1 [cs.LG])</h2>
<h3>Elena Beretta, Antonio Vetr&#xf2;, Bruno Lepri, Juan Carlos De Martin</h3>
<p>Thanks to the increasing growth of computational power and data availability,
the research in machine learning has advanced with tremendous rapidity.
Nowadays, the majority of automatic decision making systems are based on data.
However, it is well known that machine learning systems can present problematic
results if they are built on partial or incomplete data. In fact, in recent
years several studies have found a convergence of issues related to the ethics
and transparency of these systems in the process of data collection and how
they are recorded. Although the process of rigorous data collection and
analysis is fundamental in the model design, this step is still largely
overlooked by the machine learning community. For this reason, we propose a
method of data annotation based on Bayesian statistical inference that aims to
warn about the risk of discriminatory results of a given data set. In
particular, our method aims to deepen knowledge and promote awareness about the
sampling practices employed to create the training set, highlighting that the
probability of success or failure conditioned to a minority membership is given
by the structure of the data available. We empirically test our system on three
datasets commonly accessed by the machine learning community and we investigate
the risk of racial discrimination.
</p>
<a href="http://arxiv.org/abs/2101.11358" target="_blank">arXiv:2101.11358</a> [<a href="http://arxiv.org/pdf/2101.11358" target="_blank">pdf</a>]

<h2>An explainable Transformer-based deep learning model for the prediction of incident heart failure. (arXiv:2101.11359v1 [cs.LG])</h2>
<h3>Shishir Rao, Yikuan Li, Rema Ramakrishnan, Abdelaali Hassaine, Dexter Canoy, John Cleland, Thomas Lukasiewicz, Gholamreza Salimi-Khorshidi, Kazem Rahimi</h3>
<p>Predicting the incidence of complex chronic conditions such as heart failure
is challenging. Deep learning models applied to rich electronic health records
may improve prediction but remain unexplainable hampering their wider use in
medical practice. We developed a novel Transformer deep-learning model for more
accurate and yet explainable prediction of incident heart failure involving
100,071 patients from longitudinal linked electronic health records across the
UK. On internal 5-fold cross validation and held-out external validation, our
model achieved 0.93 and 0.93 area under the receiver operator curve and 0.69
and 0.70 area under the precision-recall curve, respectively and outperformed
existing deep learning models. Predictor groups included all community and
hospital diagnoses and medications contextualised within the age and calendar
year for each patient's clinical encounter. The importance of contextualised
medical information was revealed in a number of sensitivity analyses, and our
perturbation method provided a way of identifying factors contributing to risk.
Many of the identified risk factors were consistent with existing knowledge
from clinical and epidemiological research but several new associations were
revealed which had not been considered in expert-driven risk prediction models.
</p>
<a href="http://arxiv.org/abs/2101.11359" target="_blank">arXiv:2101.11359</a> [<a href="http://arxiv.org/pdf/2101.11359" target="_blank">pdf</a>]

<h2>Learning Abstract Representations through Lossy Compression of Multi-Modal Signals. (arXiv:2101.11376v1 [cs.LG])</h2>
<h3>Charles Wilmot, Jochen Triesch</h3>
<p>A key competence for open-ended learning is the formation of increasingly
abstract representations useful for driving complex behavior. Abstract
representations ignore specific details and facilitate generalization. Here we
consider the learning of abstract representations in a multi-modal setting with
two or more input modalities. We treat the problem as a lossy compression
problem and show that generic lossy compression of multimodal sensory input
naturally extracts abstract representations that tend to strip away modalitiy
specific details and preferentially retain information that is shared across
the different modalities. Furthermore, we propose an architecture to learn
abstract representations by identifying and retaining only the information that
is shared across multiple modalities while discarding any modality specific
information.
</p>
<a href="http://arxiv.org/abs/2101.11376" target="_blank">arXiv:2101.11376</a> [<a href="http://arxiv.org/pdf/2101.11376" target="_blank">pdf</a>]

<h2>Self-Calibrating Active Binocular Vision via Active Efficient Coding with Deep Autoencoders. (arXiv:2101.11391v1 [cs.CV])</h2>
<h3>Charles Wilmot, Bertram E. Shi, Jochen Triesch</h3>
<p>We present a model of the self-calibration of active binocular vision
comprising the simultaneous learning of visual representations, vergence, and
pursuit eye movements. The model follows the principle of Active Efficient
Coding (AEC), a recent extension of the classic Efficient Coding Hypothesis to
active perception. In contrast to previous AEC models, the present model uses
deep autoencoders to learn sensory representations. We also propose a new
formulation of the intrinsic motivation signal that guides the learning of
behavior. We demonstrate the performance of the model in simulations.
</p>
<a href="http://arxiv.org/abs/2101.11391" target="_blank">arXiv:2101.11391</a> [<a href="http://arxiv.org/pdf/2101.11391" target="_blank">pdf</a>]

<h2>Reproducing kernel Hilbert C*-module and kernel mean embeddings. (arXiv:2101.11410v1 [stat.ML])</h2>
<h3>Yuka Hashimoto, Isao Ishikawa, Masahiro Ikeda, Fuyuta Komura, Takeshi Katsura, Yoshinobu Kawahara</h3>
<p>Kernel methods have been among the most popular techniques in machine
learning, where learning tasks are solved using the property of reproducing
kernel Hilbert space (RKHS). In this paper, we propose a novel data analysis
framework with reproducing kernel Hilbert $C^*$-module (RKHM) and kernel mean
embedding (KME) in RKHM. Since RKHM contains richer information than RKHS or
vector-valued RKHS (vv RKHS), analysis with RKHM enables us to capture and
extract structural properties in multivariate data, functional data and other
structured data. We show a branch of theories for RKHM to apply to data
analysis, including the representer theorem, and the injectivity and
universality of the proposed KME. We also show RKHM generalizes RKHS and vv
RKHS. Then, we provide concrete procedures for employing RKHM and the proposed
KME to data analysis.
</p>
<a href="http://arxiv.org/abs/2101.11410" target="_blank">arXiv:2101.11410</a> [<a href="http://arxiv.org/pdf/2101.11410" target="_blank">pdf</a>]

<h2>Online Extrinsic Calibration based on Per-Sensor Ego-Motion Using Dual Quaternions. (arXiv:2101.11440v1 [cs.RO])</h2>
<h3>Markus Horn, Thomas Wodtko, Michael Buchholz, Klaus Dietmayer</h3>
<p>In this work, we propose an approach for extrinsic sensor calibration from
per-sensor ego-motion estimates. Our problem formulation is based on dual
quaternions, enabling two different online capable solving approaches. We
provide a certifiable globally optimal and a fast local approach along with a
method to verify the globality of the local approach. Additionally, means for
integrating previous knowledge, for example, a common ground plane for planar
sensor motion, are described. Our algorithms are evaluated on simulated data
and on a publicly available dataset containing RGB-D camera images. Further,
our online calibration approach is tested on the KITTI odometry dataset, which
provides data of a lidar and two stereo camera systems mounted on a vehicle.
Our evaluation confirms the short run time, state-of-the-art accuracy, as well
as online capability of our approach while retaining the global optimality of
the solution at any time.
</p>
<a href="http://arxiv.org/abs/2101.11440" target="_blank">arXiv:2101.11440</a> [<a href="http://arxiv.org/pdf/2101.11440" target="_blank">pdf</a>]

<h2>Adversaries in Online Learning Revisited: with applications in Robust Optimization and Adversarial training. (arXiv:2101.11443v1 [cs.LG])</h2>
<h3>Sebastian Pokutta, Huan Xu</h3>
<p>We revisit the concept of "adversary" in online learning, motivated by
solving robust optimization and adversarial training using online learning
methods. While one of the classical setups in online learning deals with the
"adversarial" setup, it appears that this concept is used less rigorously,
causing confusion in applying results and insights from online learning.
Specifically, there are two fundamentally different types of adversaries,
depending on whether the "adversary" is able to anticipate the exogenous
randomness of the online learning algorithms. This is particularly relevant to
robust optimization and adversarial training because the adversarial sequences
are often anticipative, and many online learning algorithms do not achieve
diminishing regret in such a case.

We then apply this to solving robust optimization problems or (equivalently)
adversarial training problems via online learning and establish a general
approach for a large variety of problem classes using imaginary play. Here two
players play against each other, the primal player playing the decisions and
the dual player playing realizations of uncertain data. When the game
terminates, the primal player has obtained an approximately robust solution.
This meta-game allows for solving a large variety of robust optimization and
multi-objective optimization problems and generalizes the approach of
arXiv:1402.6361.
</p>
<a href="http://arxiv.org/abs/2101.11443" target="_blank">arXiv:2101.11443</a> [<a href="http://arxiv.org/pdf/2101.11443" target="_blank">pdf</a>]

<h2>Controlling by Showing: i-Mimic: A Video-based Method to Control Robotic Arms. (arXiv:2101.11451v1 [cs.CV])</h2>
<h3>Debarati B. Chakraborty, Mukesh Sharma, Bhaskar Vijay</h3>
<p>A novel concept of vision-based intelligent control of robotic arms is
developed here in this work. This work enables the controlling of robotic arms
motion only with visual inputs, that is, controlling by showing the videos of
correct movements. This work can broadly be sub-divided into two segments. The
first part of this work is to develop an unsupervised vision-based method to
control robotic arm in 2-D plane, and the second one is with deep CNN in the
same task in 3-D plane. The first method is unsupervised, where our aim is to
perform mimicking of human arm motion in real-time by a manipulator. We
developed a network, namely the vision-to-motion optical network (DON), where
the input should be a video stream containing hand movements of human, the the
output would be out the velocity and torque information of the hand movements
shown in the videos. The output information of the DON is then fed to the
robotic arm by enabling it to generate motion according to the real hand
videos. The method has been tested with both live-stream video feed as well as
on recorded video obtained from a monocular camera even by intelligently
predicting the trajectory of human hand hand when it gets occluded. This is why
the mimicry of the arm incorporates some intelligence to it and becomes
intelligent mimic (i-mimic). Alongside the unsupervised method another method
has also been developed deploying the deep neural network technique with CNN
(Convolutional Neural Network) to perform the mimicking, where labelled
datasets are used for training. The same dataset, as used in the unsupervised
DON-based method, is used in the deep CNN method, after manual annotations.
Both the proposed methods are validated with off-line as well as with on-line
video datasets in real-time. The entire methodology is validated with real-time
1-link and simulated n-link manipulators alongwith suitable comparisons.
</p>
<a href="http://arxiv.org/abs/2101.11451" target="_blank">arXiv:2101.11451</a> [<a href="http://arxiv.org/pdf/2101.11451" target="_blank">pdf</a>]

<h2>Meta Adversarial Training. (arXiv:2101.11453v1 [cs.LG])</h2>
<h3>Jan Hendrik Metzen, Nicole Finnie, Robin Hutmacher</h3>
<p>Recently demonstrated physical-world adversarial attacks have exposed
vulnerabilities in perception systems that pose severe risks for
safety-critical applications such as autonomous driving. These attacks place
adversarial artifacts in the physical world that indirectly cause the addition
of universal perturbations to inputs of a model that can fool it in a variety
of contexts. Adversarial training is the most effective defense against
image-dependent adversarial attacks. However, tailoring adversarial training to
universal perturbations is computationally expensive since the optimal
universal perturbations depend on the model weights which change during
training. We propose meta adversarial training (MAT), a novel combination of
adversarial training with meta-learning, which overcomes this challenge by
meta-learning universal perturbations along with model training. MAT requires
little extra computation while continuously adapting a large set of
perturbations to the current model. We present results for universal patch and
universal perturbation attacks on image classification and traffic-light
detection. MAT considerably increases robustness against universal patch
attacks compared to prior work.
</p>
<a href="http://arxiv.org/abs/2101.11453" target="_blank">arXiv:2101.11453</a> [<a href="http://arxiv.org/pdf/2101.11453" target="_blank">pdf</a>]

<h2>Machine learning with limited data. (arXiv:2101.11461v1 [cs.CV])</h2>
<h3>Fupin Yao</h3>
<p>Thanks to the availability of powerful computing resources, big data and deep
learning algorithms, we have made great progress on computer vision in the last
few years. Computer vision systems begin to surpass humans in some tasks, such
as object recognition, object detection, face recognition and pose estimation.
Lots of computer vision algorithms have been deployed to real world
applications and started to improve our life quality. However, big data and
labels are not always available. Sometimes we only have very limited labeled
data, such as medical images which requires experts to label them. In this
paper, we study few shot image classification, in which we only have very few
labeled data. Machine learning with little data is a big challenge. To tackle
this challenge, we propose two methods and test their effectiveness thoroughly.
One method is to augment image features by mixing the style of these images.
The second method is applying spatial attention to explore the relations
between patches of images. We also find that domain shift is a critical issue
in few shot learning when the training domain and testing domain are different.
So we propose a more realistic cross-domain few-shot learning with unlabeled
data setting, in which some unlabeled data is available in the target domain.
We propose two methods in this setting. Our first method transfers the style
information of the unlabeled target dataset to the samples in the source
dataset and trains a model with stylized images and original images. Our second
method proposes a unified framework to fully utilize all the data. Both of our
methods surpass the baseline method by a large margin.
</p>
<a href="http://arxiv.org/abs/2101.11461" target="_blank">arXiv:2101.11461</a> [<a href="http://arxiv.org/pdf/2101.11461" target="_blank">pdf</a>]

<h2>Detecting Adversarial Examples by Input Transformations, Defense Perturbations, and Voting. (arXiv:2101.11466v1 [cs.CV])</h2>
<h3>Federico Nesti, Alessandro Biondi, Giorgio Buttazzo</h3>
<p>Over the last few years, convolutional neural networks (CNNs) have proved to
reach super-human performance in visual recognition tasks. However, CNNs can
easily be fooled by adversarial examples, i.e., maliciously-crafted images that
force the networks to predict an incorrect output while being extremely similar
to those for which a correct output is predicted. Regular adversarial examples
are not robust to input image transformations, which can then be used to detect
whether an adversarial example is presented to the network. Nevertheless, it is
still possible to generate adversarial examples that are robust to such
transformations.

This paper extensively explores the detection of adversarial examples via
image transformations and proposes a novel methodology, called \textit{defense
perturbation}, to detect robust adversarial examples with the same input
transformations the adversarial examples are robust to. Such a \textit{defense
perturbation} is shown to be an effective counter-measure to robust adversarial
examples.

Furthermore, multi-network adversarial examples are introduced. This kind of
adversarial examples can be used to simultaneously fool multiple networks,
which is critical in systems that use network redundancy, such as those based
on architectures with majority voting over multiple CNNs. An extensive set of
experiments based on state-of-the-art CNNs trained on the Imagenet dataset is
finally reported.
</p>
<a href="http://arxiv.org/abs/2101.11466" target="_blank">arXiv:2101.11466</a> [<a href="http://arxiv.org/pdf/2101.11466" target="_blank">pdf</a>]

<h2>Spatial-Channel Transformer Network for Trajectory Prediction on the Traffic Scenes. (arXiv:2101.11472v1 [cs.CV])</h2>
<h3>Jingwen Zhao, Xuanpeng Li, Qifan Xue, Weigong Zhang</h3>
<p>Predicting motion of surrounding agents is critical to real-world
applications of tactical path planning for autonomous driving. Due to the
complex temporal dependencies and social interactions of agents, on-line
trajectory prediction is a challenging task. With the development of attention
mechanism in recent years, transformer model has been applied in natural
language sequence processing first and then image processing. In this paper, we
present a Spatial-Channel Transformer Network for trajectory prediction with
attention functions. Instead of RNN models, we employ transformer model to
capture the spatial-temporal features of agents. A channel-wise module is
inserted to measure the social interaction between agents. We find that the
Spatial-Channel Transformer Network achieves promising results on real-world
trajectory prediction datasets on the traffic scenes.
</p>
<a href="http://arxiv.org/abs/2101.11472" target="_blank">arXiv:2101.11472</a> [<a href="http://arxiv.org/pdf/2101.11472" target="_blank">pdf</a>]

<h2>Utilizing Uncertainty Estimation in Deep Learning Segmentation of Fluorescence Microscopy Images with Missing Markers. (arXiv:2101.11476v1 [cs.CV])</h2>
<h3>Alvaro Gomariz, Raphael Egli, Tiziano Portenier, C&#xe9;sar Nombela-Arrieta, Orcun Goksel</h3>
<p>Fluorescence microscopy images contain several channels, each indicating a
marker staining the sample. Since many different marker combinations are
utilized in practice, it has been challenging to apply deep learning based
segmentation models, which expect a predefined channel combination for all
training samples as well as at inference for future application. Recent work
circumvents this problem using a modality attention approach to be effective
across any possible marker combination. However, for combinations that do not
exist in a labeled training dataset, one cannot have any estimation of
potential segmentation quality if that combination is encountered during
inference. Without this, not only one lacks quality assurance but one also does
not know where to put any additional imaging and labeling effort. We herein
propose a method to estimate segmentation quality on unlabeled images by (i)
estimating both aleatoric and epistemic uncertainties of convolutional neural
networks for image segmentation, and (ii) training a Random Forest model for
the interpretation of uncertainty features via regression to their
corresponding segmentation metrics. Additionally, we demonstrate that including
these uncertainty measures during training can provide an improvement on
segmentation performance.
</p>
<a href="http://arxiv.org/abs/2101.11476" target="_blank">arXiv:2101.11476</a> [<a href="http://arxiv.org/pdf/2101.11476" target="_blank">pdf</a>]

<h2>Evolution of artificial intelligence languages, a systematic literature review. (arXiv:2101.11501v1 [cs.AI])</h2>
<h3>Emmanuel Adetiba, Temitope John, Adekunle Akinrinmade, Funmilayo Moninuola, Oladipupo Akintade, Joke Badejo</h3>
<p>The field of Artificial Intelligence (AI) has undoubtedly received
significant attention in recent years. AI is being adopted to provide solutions
to problems in fields such as medicine, engineering, education, government and
several other domains. In order to analyze the state of the art of research in
the field of AI, we present a systematic literature review focusing on the
Evolution of AI programming languages. We followed the systematic literature
review method by searching relevant databases like SCOPUS, IEEE Xplore and
Google Scholar. EndNote reference manager was used to catalog the relevant
extracted papers. Our search returned a total of 6565 documents, whereof 69
studies were retained. Of the 69 retained studies, 15 documents discussed LISP
programming language, another 34 discussed PROLOG programming language, the
remaining 20 documents were spread between Logic and Object Oriented
Programming (LOOP), ARCHLOG, Epistemic Ontology Language with Constraints
(EOLC), Python, C++, ADA and JAVA programming languages. This review provides
information on the year of implementation, development team, capabilities,
limitations and applications of each of the AI programming languages discussed.
The information in this review could guide practitioners and researchers in AI
to make the right choice of languages to implement their novel AI methods.
</p>
<a href="http://arxiv.org/abs/2101.11501" target="_blank">arXiv:2101.11501</a> [<a href="http://arxiv.org/pdf/2101.11501" target="_blank">pdf</a>]

<h2>Effects of Image Size on Deep Learning. (arXiv:2101.11508v1 [cs.CV])</h2>
<h3>Olivier Rukundo</h3>
<p>The question is: what size of the region of interest is likely to lead to
better training outcomes? To answer this: The U-net is used for semantic
segmentation. Image interpolation algorithms are used to double the cropped
image size and create new datasets. Depending on the selected image
interpolation algorithm category, non-original classes are created in the
ground truth images thus a filtering strategy is introduced to remove such
spurious classes. Evaluation results of effects on the myocardium segmentation
and quantification of the myocardial infarction are provided and discussed.
</p>
<a href="http://arxiv.org/abs/2101.11508" target="_blank">arXiv:2101.11508</a> [<a href="http://arxiv.org/pdf/2101.11508" target="_blank">pdf</a>]

<h2>Investigating Bi-Level Optimization for Learning and Vision from a Unified Perspective: A Survey and Beyond. (arXiv:2101.11517v1 [cs.LG])</h2>
<h3>Risheng Liu, Jiaxin Gao, Jin Zhang, Deyu Meng, Zhouchen Lin</h3>
<p>Bi-Level Optimization (BLO) is originated from the area of economic game
theory and then introduced into the optimization community. BLO is able to
handle problems with a hierarchical structure, involving two levels of
optimization tasks, where one task is nested inside the other. In machine
learning and computer vision fields, despite the different motivations and
mechanisms, a lot of complex problems, such as hyper-parameter optimization,
multi-task and meta-learning, neural architecture search, adversarial learning
and deep reinforcement learning, actually all contain a series of closely
related subproblms. In this paper, we first uniformly express these complex
learning and vision problems from the perspective of BLO. Then we construct a
value-function-based single-level reformulation and establish a unified
algorithmic framework to understand and formulate mainstream gradient-based BLO
methodologies, covering aspects ranging from fundamental automatic
differentiation schemes to various accelerations, simplifications, extensions
and their convergence and complexity properties. Last but not least, we discuss
the potentials of our unified BLO framework for designing new algorithms and
point out some promising directions for future research.
</p>
<a href="http://arxiv.org/abs/2101.11517" target="_blank">arXiv:2101.11517</a> [<a href="http://arxiv.org/pdf/2101.11517" target="_blank">pdf</a>]

<h2>Supervised Tree-Wasserstein Distance. (arXiv:2101.11520v1 [cs.LG])</h2>
<h3>Yuki Takezawa, Ryoma Sato, Makoto Yamada</h3>
<p>To measure the similarity of documents, the Wasserstein distance is a
powerful tool, but it requires a high computational cost. Recently, for fast
computation of the Wasserstein distance, methods for approximating the
Wasserstein distance using a tree metric have been proposed. These tree-based
methods allow fast comparisons of a large number of documents; however, they
are unsupervised and do not learn task-specific distances. In this work, we
propose the Supervised Tree-Wasserstein (STW) distance, a fast, supervised
metric learning method based on the tree metric. Specifically, we rewrite the
Wasserstein distance on the tree metric by the parent-child relationships of a
tree, and formulate it as a continuous optimization problem using a contrastive
loss. Experimentally, we show that the STW distance can be computed fast, and
improves the accuracy of document classification tasks. Furthermore, the STW
distance is formulated by matrix multiplications, runs on a GPU, and is
suitable for batch processing. Therefore, we show that the STW distance is
extremely efficient when comparing a large number of documents.
</p>
<a href="http://arxiv.org/abs/2101.11520" target="_blank">arXiv:2101.11520</a> [<a href="http://arxiv.org/pdf/2101.11520" target="_blank">pdf</a>]

<h2>Improving Graph Representation Learning by Contrastive Regularization. (arXiv:2101.11525v1 [cs.LG])</h2>
<h3>Kaili Ma, Haochen Yang, Han Yang, Tatiana Jin, Pengfei Chen, Yongqiang Chen, Barakeel Fanseu Kamhoua, James Cheng</h3>
<p>Graph representation learning is an important task with applications in
various areas such as online social networks, e-commerce networks, WWW, and
semantic webs. For unsupervised graph representation learning, many algorithms
such as Node2Vec and Graph-SAGE make use of "negative sampling" and/or noise
contrastive estimation loss. This bears similar ideas to contrastive learning,
which "contrasts" the node representation similarities of semantically similar
(positive) pairs against those of negative pairs. However, despite the success
of contrastive learning, we found that directly applying this technique to
graph representation learning models (e.g., graph convolutional networks) does
not always work. We theoretically analyze the generalization performance and
propose a light-weight regularization term that avoids the high scales of node
representations' norms and the high variance among them to improve the
generalization performance. Our experimental results further validate that this
regularization term significantly improves the representation quality across
different node similarity definitions and outperforms the state-of-the-art
methods.
</p>
<a href="http://arxiv.org/abs/2101.11525" target="_blank">arXiv:2101.11525</a> [<a href="http://arxiv.org/pdf/2101.11525" target="_blank">pdf</a>]

<h2>NTU60-X: Towards Skeleton-based Recognition of Subtle Human Actions. (arXiv:2101.11529v1 [cs.CV])</h2>
<h3>Anirudh Thatipelli, Neel Trivedi, Ravi Kiran Sarvadevabhatla</h3>
<p>The lack of fine-grained joints such as hand fingers is a fundamental
performance bottleneck for state of the art skeleton action recognition models
trained on the largest action recognition dataset, NTU-RGBD. To address this
bottleneck, we introduce a new skeleton based human action dataset - NTU60-X.
In addition to the 25 body joints for each skeleton as in NTU-RGBD, NTU60-X
dataset includes finger and facial joints, enabling a richer skeleton
representation. We appropriately modify the state of the art approaches to
enable training using the introduced dataset. Our results demonstrate the
effectiveness of NTU60-X in overcoming the aforementioned bottleneck and
improve state of the art performance, overall and on hitherto worst performing
action categories.
</p>
<a href="http://arxiv.org/abs/2101.11529" target="_blank">arXiv:2101.11529</a> [<a href="http://arxiv.org/pdf/2101.11529" target="_blank">pdf</a>]

<h2>Syntactically Guided Generative Embeddings for Zero-Shot Skeleton Action Recognition. (arXiv:2101.11530v1 [cs.CV])</h2>
<h3>Pranay Gupta, Divyanshu Sharma, Ravi Kiran Sarvadevabhatla</h3>
<p>We introduce SynSE, a novel syntactically guided generative approach for
Zero-Shot Learning (ZSL). Our end-to-end approach learns progressively refined
generative embedding spaces constrained within and across the involved
modalities (visual, language). The inter-modal constraints are defined between
action sequence embedding and embeddings of Parts of Speech (PoS) tagged words
in the corresponding action description. We deploy SynSE for the task of
skeleton-based action sequence recognition. Our design choices enable SynSE to
generalize compositionally, i.e., recognize sequences whose action descriptions
contain words not encountered during training. We also extend our approach to
the more challenging Generalized Zero-Shot Learning (GZSL) problem via a
confidence-based gating mechanism. We are the first to present zero-shot
skeleton action recognition results on the large-scale NTU-60 and NTU-120
skeleton action datasets with multiple splits. Our results demonstrate SynSE's
state of the art performance in both ZSL and GZSL settings compared to strong
baselines on the NTU-60 and NTU-120 datasets.
</p>
<a href="http://arxiv.org/abs/2101.11530" target="_blank">arXiv:2101.11530</a> [<a href="http://arxiv.org/pdf/2101.11530" target="_blank">pdf</a>]

<h2>Tropical Support Vector Machines: Evaluations and Extension to Function Spaces. (arXiv:2101.11531v1 [cs.LG])</h2>
<h3>Ruriko Yoshida, Misaki Takamori, Hideyuki Matsumoto, Keiji Miura</h3>
<p>Support Vector Machines (SVMs) are one of the most popular supervised
learning models to classify using a hyperplane in an Euclidean space. Similar
to SVMs, tropical SVMs classify data points using a tropical hyperplane under
the tropical metric with the max-plus algebra. In this paper, first we show
generalization error bounds of tropical SVMs over the tropical projective
space. While the generalization error bounds attained via VC dimensions in a
distribution-free manner still depend on the dimension, we also show
theoretically by extreme value statistics that the tropical SVMs for
classifying data points from two Gaussian distributions as well as empirical
data sets of different neuron types are fairly robust against the curse of
dimensionality. Extreme value statistics also underlie the anomalous scaling
behaviors of the tropical distance between random vectors with additional noise
dimensions. Finally, we define tropical SVMs over a function space with the
tropical metric and discuss the Gaussian function space as an example.
</p>
<a href="http://arxiv.org/abs/2101.11531" target="_blank">arXiv:2101.11531</a> [<a href="http://arxiv.org/pdf/2101.11531" target="_blank">pdf</a>]

<h2>Autoencoder-based Condition Monitoring and Anomaly Detection Method for Rotating Machines. (arXiv:2101.11539v1 [cs.LG])</h2>
<h3>Sabtain Ahmad, Kevin Styp-Rekowski, Sasho Nedelkoski, Odej Kao</h3>
<p>Rotating machines like engines, pumps, or turbines are ubiquitous in modern
day societies. Their mechanical parts such as electrical engines, rotors, or
bearings are the major components and any failure in them may result in their
total shutdown. Anomaly detection in such critical systems is very important to
monitor the system's health. As the requirement to obtain a dataset from
rotating machines where all possible faults are explicitly labeled is difficult
to satisfy, we propose a method that focuses on the normal behavior of the
machine instead. We propose an autoencoder model-based method for condition
monitoring of rotating machines by using an anomaly detection approach. The
method learns the characteristics of a rotating machine using the normal
vibration signals to model the healthy state of the machine. A threshold-based
approach is then applied to the reconstruction error of unseen data, thus
enabling the detection of unseen anomalies. The proposed method can directly
extract the salient features from raw vibration signals and eliminate the need
for manually engineered features. We demonstrate the effectiveness of the
proposed method by employing two rotating machine datasets and the quality of
the automatically learned features is compared with a set of handcrafted
features by training an Isolation Forest model on either of these two sets.
Experimental results on two real-world datasets indicate that our proposed
solution gives promising results, achieving an average F1-score of 99.6%.
</p>
<a href="http://arxiv.org/abs/2101.11539" target="_blank">arXiv:2101.11539</a> [<a href="http://arxiv.org/pdf/2101.11539" target="_blank">pdf</a>]

<h2>Efficient Graph Deep Learning in TensorFlow with tf_geometric. (arXiv:2101.11552v1 [cs.LG])</h2>
<h3>Jun Hu, Shengsheng Qian, Quan Fang, Youze Wang, Quan Zhao, Huaiwen Zhang, Changsheng Xu</h3>
<p>We introduce tf_geometric, an efficient and friendly library for graph deep
learning, which is compatible with both TensorFlow 1.x and 2.x. tf_geometric
provides kernel libraries for building Graph Neural Networks (GNNs) as well as
implementations of popular GNNs. The kernel libraries consist of
infrastructures for building efficient GNNs, including graph data structures,
graph map-reduce framework, graph mini-batch strategy, etc. These
infrastructures enable tf_geometric to support single-graph computation,
multi-graph computation, graph mini-batch, distributed training, etc.;
therefore, tf_geometric can be used for a variety of graph deep learning tasks,
such as transductive node classification, inductive node classification, link
prediction, and graph classification. Based on the kernel libraries,
tf_geometric implements a variety of popular GNN models for different tasks. To
facilitate the implementation of GNNs, tf_geometric also provides some other
libraries for dataset management, graph sampling, etc. Different from existing
popular GNN libraries, tf_geometric provides not only Object-Oriented
Programming (OOP) APIs, but also Functional APIs, which enable tf_geometric to
handle advanced graph deep learning tasks such as graph meta-learning. The APIs
of tf_geometric are friendly, and they are suitable for both beginners and
experts. In this paper, we first present an overview of tf_geometric's
framework. Then, we conduct experiments on some benchmark datasets and report
the performance of several popular GNN models implemented by tf_geometric.
</p>
<a href="http://arxiv.org/abs/2101.11552" target="_blank">arXiv:2101.11552</a> [<a href="http://arxiv.org/pdf/2101.11552" target="_blank">pdf</a>]

<h2>Wisdom of the Contexts: Active Ensemble Learning for Contextual Anomaly Detection. (arXiv:2101.11560v1 [cs.LG])</h2>
<h3>Ece Calikus, Slawomir Nowaczyk, Mohamed-Rafik Bouguelia, Onur Dikmen</h3>
<p>In contextual anomaly detection (CAD), an object is only considered anomalous
within a specific context. Most existing methods for CAD use a single context
based on a set of user-specified contextual features. However, identifying the
right context can be very challenging in practice, especially in datasets, with
a large number of attributes. Furthermore, in real-world systems, there might
be multiple anomalies that occur in different contexts and, therefore, require
a combination of several "useful" contexts to unveil them. In this work, we
leverage active learning and ensembles to effectively detect complex contextual
anomalies in situations where the true contextual and behavioral attributes are
unknown. We propose a novel approach, called WisCon (Wisdom of the Contexts),
that automatically creates contexts from the feature set. Our method constructs
an ensemble of multiple contexts, with varying importance scores, based on the
assumption that not all useful contexts are equally so. Experiments show that
WisCon significantly outperforms existing baselines in different categories
(i.e., active classifiers, unsupervised contextual and non-contextual anomaly
detectors, and supervised classifiers) on seven datasets. Furthermore, the
results support our initial hypothesis that there is no single perfect context
that successfully uncovers all kinds of contextual anomalies, and leveraging
the "wisdom" of multiple contexts is necessary.
</p>
<a href="http://arxiv.org/abs/2101.11560" target="_blank">arXiv:2101.11560</a> [<a href="http://arxiv.org/pdf/2101.11560" target="_blank">pdf</a>]

<h2>Scheduled Sampling in Vision-Language Pretraining with Decoupled Encoder-Decoder Network. (arXiv:2101.11562v1 [cs.CV])</h2>
<h3>Yehao Li, Yingwei Pan, Ting Yao, Jingwen Chen, Tao Mei</h3>
<p>Despite having impressive vision-language (VL) pretraining with BERT-based
encoder for VL understanding, the pretraining of a universal encoder-decoder
for both VL understanding and generation remains challenging. The difficulty
originates from the inherently different peculiarities of the two disciplines,
e.g., VL understanding tasks capitalize on the unrestricted message passing
across modalities, while generation tasks only employ visual-to-textual message
passing. In this paper, we start with a two-stream decoupled design of
encoder-decoder structure, in which two decoupled cross-modal encoder and
decoder are involved to separately perform each type of proxy tasks, for
simultaneous VL understanding and generation pretraining. Moreover, for VL
pretraining, the dominant way is to replace some input visual/word tokens with
mask tokens and enforce the multi-modal encoder/decoder to reconstruct the
original tokens, but no mask token is involved when fine-tuning on downstream
tasks. As an alternative, we propose a primary scheduled sampling strategy that
elegantly mitigates such discrepancy via pretraining encoder-decoder in a
two-pass manner. Extensive experiments demonstrate the compelling
generalizability of our pretrained encoder-decoder by fine-tuning on four VL
understanding and generation downstream tasks. Source code is available at
\url{https://github.com/YehLi/TDEN}.
</p>
<a href="http://arxiv.org/abs/2101.11562" target="_blank">arXiv:2101.11562</a> [<a href="http://arxiv.org/pdf/2101.11562" target="_blank">pdf</a>]

<h2>Detecting Deepfake Videos Using Euler Video Magnification. (arXiv:2101.11563v1 [cs.CV])</h2>
<h3>Rashmiranjan Das, Gaurav Negi, Alan F. Smeaton</h3>
<p>Recent advances in artificial intelligence make it progressively hard to
distinguish between genuine and counterfeit media, especially images and
videos. One recent development is the rise of deepfake videos, based on
manipulating videos using advanced machine learning techniques. This involves
replacing the face of an individual from a source video with the face of a
second person, in the destination video. This idea is becoming progressively
refined as deepfakes are getting progressively seamless and simpler to compute.
Combined with the outreach and speed of social media, deepfakes could easily
fool individuals when depicting someone saying things that never happened and
thus could persuade people in believing fictional scenarios, creating distress,
and spreading fake news. In this paper, we examine a technique for possible
identification of deepfake videos. We use Euler video magnification which
applies spatial decomposition and temporal filtering on video data to highlight
and magnify hidden features like skin pulsation and subtle motions. Our
approach uses features extracted from the Euler technique to train three models
to classify counterfeit and unaltered videos and compare the results with
existing techniques.
</p>
<a href="http://arxiv.org/abs/2101.11563" target="_blank">arXiv:2101.11563</a> [<a href="http://arxiv.org/pdf/2101.11563" target="_blank">pdf</a>]

<h2>An Integrated Localisation, Motion Planning and Obstacle Avoidance Algorithm in Belief Space. (arXiv:2101.11566v1 [cs.RO])</h2>
<h3>Antony Thomas, Fulvio Mastrogiovanni, Marco Baglietto</h3>
<p>As robots are being increasingly used in close proximity to humans and
objects, it is imperative that robots operate safely and efficiently under
real-world conditions. Yet, the environment is seldom known perfectly. Noisy
sensors and actuation errors compound to the errors introduced while estimating
features of the environment. We present a novel approach (1) to incorporate
these uncertainties for robot state estimation and (2) to compute the
probability of collision pertaining to the estimated robot configurations. The
expression for collision probability is obtained as an infinite series and we
prove its convergence. An upper bound for the truncation error is also derived
and the number of terms required is demonstrated by analyzing the convergence
for different robot and obstacle configurations. We evaluate our approach using
two simulation domains which use a roadmap-based strategy to synthesize
trajectories that satisfy collision probability bounds.
</p>
<a href="http://arxiv.org/abs/2101.11566" target="_blank">arXiv:2101.11566</a> [<a href="http://arxiv.org/pdf/2101.11566" target="_blank">pdf</a>]

<h2>Privacy Information Classification: A Hybrid Approach. (arXiv:2101.11574v1 [cs.AI])</h2>
<h3>Jiaqi Wu, Weihua Li, Quan Bai, Takayuki Ito, Ahmed Moustafa</h3>
<p>A large amount of information has been published to online social networks
every day. Individual privacy-related information is also possibly disclosed
unconsciously by the end-users. Identifying privacy-related data and protecting
the online social network users from privacy leakage turn out to be
significant. Under such a motivation, this study aims to propose and develop a
hybrid privacy classification approach to detect and classify privacy
information from OSNs. The proposed hybrid approach employs both deep learning
models and ontology-based models for privacy-related information extraction.
Extensive experiments are conducted to validate the proposed hybrid approach,
and the empirical results demonstrate its superiority in assisting online
social network users against privacy leakage.
</p>
<a href="http://arxiv.org/abs/2101.11574" target="_blank">arXiv:2101.11574</a> [<a href="http://arxiv.org/pdf/2101.11574" target="_blank">pdf</a>]

<h2>Adversarial Attacks on Uncertainty Enable Active Learning for Neural Network Potentials. (arXiv:2101.11588v1 [cs.LG])</h2>
<h3>Daniel Schwalbe-Koda, Aik Rui Tan, Rafael G&#xf3;mez-Bombarelli</h3>
<p>Neural network (NN)-based interatomic potentials provide fast prediction of
potential energy surfaces with the accuracy of electronic structure methods.
However, NN predictions are only reliable within well-learned training domains,
with unknown behavior when extrapolating. Uncertainty quantification through NN
committees identify domains with low prediction confidence, but thoroughly
exploring the configuration space for training NN potentials often requires
slow atomistic simulations. Here, we employ adversarial attacks with a
differentiable uncertainty metric to sample new molecular geometries and
bootstrap NN potentials. In combination with an active learning loop, the
extrapolation power of NN potentials is improved beyond the original training
data with few additional samples. The framework is demonstrated on multiple
examples, leading to better sampling of kinetic barriers and collective
variables without extensive prior data on the relevant geometries. Adversarial
attacks are new ways to simultaneously sample the phase space and bootstrap NN
potentials, increasing their robustness and enabling a faster, accurate
prediction of potential energy landscapes.
</p>
<a href="http://arxiv.org/abs/2101.11588" target="_blank">arXiv:2101.11588</a> [<a href="http://arxiv.org/pdf/2101.11588" target="_blank">pdf</a>]

<h2>Dexterous Manipulation Primitives for the Real Robot Challenge. (arXiv:2101.11597v1 [cs.RO])</h2>
<h3>Claire Chen, Krishnan Srinivasan, Jeffrey Zhang, Junwu Zhang</h3>
<p>This report describes our approach for Phase 3 of the Real Robot Challenge.
To solve cuboid manipulation tasks of varying difficulty, we decompose each
task into the following primitives: moving the fingers to the cuboid to grasp
it, turning it on the table to minimize orientation error, and re-positioning
it to the goal position. We use model-based trajectory optimization and control
to plan and execute these primitives. These grasping, turning, and
re-positioning primitives are sequenced with a state-machine that determines
which primitive to execute given the current object state and goal. Our method
shows robust performance over multiple runs with randomized initial and goal
positions. With this approach, our team placed second in the challenge, under
the anonymous name "sombertortoise" on the leaderboard. Example runs of our
method solving each of the four levels can be seen in this video
(https://www.youtube.com/watch?v=I65Kwu9PGmg&amp;list=PLt9QxrtaftrHGXcp4Oh8-s_OnQnBnLtei&amp;index=1).
</p>
<a href="http://arxiv.org/abs/2101.11597" target="_blank">arXiv:2101.11597</a> [<a href="http://arxiv.org/pdf/2101.11597" target="_blank">pdf</a>]

<h2>Shape or Texture: Understanding Discriminative Features in CNNs. (arXiv:2101.11604v1 [cs.CV])</h2>
<h3>Md Amirul Islam, Matthew Kowal, Patrick Esser, Sen Jia, Bjorn Ommer, Konstantinos G. Derpanis, Neil Bruce</h3>
<p>Contrasting the previous evidence that neurons in the later layers of a
Convolutional Neural Network (CNN) respond to complex object shapes, recent
studies have shown that CNNs actually exhibit a `texture bias': given an image
with both texture and shape cues (e.g., a stylized image), a CNN is biased
towards predicting the category corresponding to the texture. However, these
previous studies conduct experiments on the final classification output of the
network, and fail to robustly evaluate the bias contained (i) in the latent
representations, and (ii) on a per-pixel level. In this paper, we design a
series of experiments that overcome these issues. We do this with the goal of
better understanding what type of shape information contained in the network is
discriminative, where shape information is encoded, as well as when the network
learns about object shape during training. We show that a network learns the
majority of overall shape information at the first few epochs of training and
that this information is largely encoded in the last few layers of a CNN.
Finally, we show that the encoding of shape does not imply the encoding of
localized per-pixel semantic information. The experimental results and findings
provide a more accurate understanding of the behaviour of current CNNs, thus
helping to inform future design choices.
</p>
<a href="http://arxiv.org/abs/2101.11604" target="_blank">arXiv:2101.11604</a> [<a href="http://arxiv.org/pdf/2101.11604" target="_blank">pdf</a>]

<h2>Bottleneck Transformers for Visual Recognition. (arXiv:2101.11605v1 [cs.CV])</h2>
<h3>Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon Shlens, Pieter Abbeel, Ashish Vaswani</h3>
<p>We present BoTNet, a conceptually simple yet powerful backbone architecture
that incorporates self-attention for multiple computer vision tasks including
image classification, object detection and instance segmentation. By just
replacing the spatial convolutions with global self-attention in the final
three bottleneck blocks of a ResNet and no other changes, our approach improves
upon the baselines significantly on instance segmentation and object detection
while also reducing the parameters, with minimal overhead in latency. Through
the design of BoTNet, we also point out how ResNet bottleneck blocks with
self-attention can be viewed as Transformer blocks. Without any bells and
whistles, BoTNet achieves 44.4% Mask AP and 49.7% Box AP on the COCO Instance
Segmentation benchmark using the Mask R-CNN framework; surpassing the previous
best published single model and single scale results of ResNeSt evaluated on
the COCO validation set. Finally, we present a simple adaptation of the BoTNet
design for image classification, resulting in models that achieve a strong
performance of 84.7% top-1 accuracy on the ImageNet benchmark while being up to
2.33x faster in compute time than the popular EfficientNet models on TPU-v3
hardware. We hope our simple and effective approach will serve as a strong
baseline for future research in self-attention models for vision.
</p>
<a href="http://arxiv.org/abs/2101.11605" target="_blank">arXiv:2101.11605</a> [<a href="http://arxiv.org/pdf/2101.11605" target="_blank">pdf</a>]

<h2>Generative Multi-Label Zero-Shot Learning. (arXiv:2101.11606v1 [cs.CV])</h2>
<h3>Akshita Gupta, Sanath Narayan, Salman Khan, Fahad Shahbaz Khan, Ling Shao, Joost van de Weijer</h3>
<p>Multi-label zero-shot learning strives to classify images into multiple
unseen categories for which no data is available during training. The test
samples can additionally contain seen categories in the generalized variant.
Existing approaches rely on learning either shared or label-specific attention
from the seen classes. Nevertheless, computing reliable attention maps for
unseen classes during inference in a multi-label setting is still a challenge.
In contrast, state-of-the-art single-label generative adversarial network (GAN)
based approaches learn to directly synthesize the class-specific visual
features from the corresponding class attribute embeddings. However,
synthesizing multi-label features from GANs is still unexplored in the context
of zero-shot setting. In this work, we introduce different fusion approaches at
the attribute-level, feature-level and cross-level (across attribute and
feature-levels) for synthesizing multi-label features from their corresponding
multi-label class embedding. To the best of our knowledge, our work is the
first to tackle the problem of multi-label feature synthesis in the
(generalized) zero-shot setting. Comprehensive experiments are performed on
three zero-shot image classification benchmarks: NUS-WIDE, Open Images and MS
COCO. Our cross-level fusion-based generative approach outperforms the
state-of-the-art on all three datasets. Furthermore, we show the generalization
capabilities of our fusion approach in the zero-shot detection task on MS COCO,
achieving favorable performance against existing methods. The source code is
available at https://github.com/akshitac8/Generative_MLZSL.
</p>
<a href="http://arxiv.org/abs/2101.11606" target="_blank">arXiv:2101.11606</a> [<a href="http://arxiv.org/pdf/2101.11606" target="_blank">pdf</a>]

<h2>Res2Net: A New Multi-scale Backbone Architecture. (arXiv:1904.01169v3 [cs.CV] UPDATED)</h2>
<h3>Shang-Hua Gao, Ming-Ming Cheng, Kai Zhao, Xin-Yu Zhang, Ming-Hsuan Yang, Philip Torr</h3>
<p>Representing features at multiple scales is of great importance for numerous
vision tasks. Recent advances in backbone convolutional neural networks (CNNs)
continually demonstrate stronger multi-scale representation ability, leading to
consistent performance gains on a wide range of applications. However, most
existing methods represent the multi-scale features in a layer-wise manner. In
this paper, we propose a novel building block for CNNs, namely Res2Net, by
constructing hierarchical residual-like connections within one single residual
block. The Res2Net represents multi-scale features at a granular level and
increases the range of receptive fields for each network layer. The proposed
Res2Net block can be plugged into the state-of-the-art backbone CNN models,
e.g., ResNet, ResNeXt, and DLA. We evaluate the Res2Net block on all these
models and demonstrate consistent performance gains over baseline models on
widely-used datasets, e.g., CIFAR-100 and ImageNet. Further ablation studies
and experimental results on representative computer vision tasks, i.e., object
detection, class activation mapping, and salient object detection, further
verify the superiority of the Res2Net over the state-of-the-art baseline
methods. The source code and trained models are available on
https://mmcheng.net/res2net/.
</p>
<a href="http://arxiv.org/abs/1904.01169" target="_blank">arXiv:1904.01169</a> [<a href="http://arxiv.org/pdf/1904.01169" target="_blank">pdf</a>]

<h2>Tight Regret Bounds for Infinite-armed Linear Contextual Bandits. (arXiv:1905.01435v3 [stat.ML] UPDATED)</h2>
<h3>Yingkai Li, Yining Wang, Xi Chen, Yuan Zhou</h3>
<p>Linear contextual bandit is an important class of sequential decision making
problems with a wide range of applications to recommender systems, online
advertising, healthcare, and many other machine learning related tasks. While
there is a lot of prior research, tight regret bounds of linear contextual
bandit with infinite action sets remain open. In this paper, we address this
open problem by considering the linear contextual bandit with (changing)
infinite action sets. We prove a regret upper bound on the order of
$O(\sqrt{d^2T\log T})\times \text{poly}(\log\log T)$ where $d$ is the domain
dimension and $T$ is the time horizon. Our upper bound matches the previous
lower bound of $\Omega(\sqrt{d^2 T\log T})$ in [Li et al., 2019] up to iterated
logarithmic terms.
</p>
<a href="http://arxiv.org/abs/1905.01435" target="_blank">arXiv:1905.01435</a> [<a href="http://arxiv.org/pdf/1905.01435" target="_blank">pdf</a>]

<h2>Homogeneous Vector Capsules Enable Adaptive Gradient Descent in Convolutional Neural Networks. (arXiv:1906.08676v2 [cs.CV] UPDATED)</h2>
<h3>Adam Byerly, Tatiana Kalganova</h3>
<p>Capsules are the name given by Geoffrey Hinton to vector-valued neurons.
Neural networks traditionally produce a scalar value for an activated neuron.
Capsules, on the other hand, produce a vector of values, which Hinton argues
correspond to a single, composite feature wherein the values of the components
of the vectors indicate properties of the feature such as transformation or
contrast. We present a new way of parameterizing and training capsules that we
refer to as homogeneous vector capsules (HVCs). We demonstrate, experimentally,
that altering a convolutional neural network (CNN) to use HVCs can achieve
superior classification accuracy without increasing the number of parameters or
operations in its architecture as compared to a CNN using a single final fully
connected layer. Additionally, the introduction of HVCs enables the use of
adaptive gradient descent, reducing the dependence a model's achievable
accuracy has on the finely tuned hyperparameters of a non-adaptive optimizer.
We demonstrate our method and results using two neural network architectures.
First, a very simple monolithic CNN that when using HVCs achieved a 63%
improvement in top-1 classification accuracy and a 35% improvement in top-5
classification accuracy over the baseline architecture. Second, with the CNN
architecture referred to as Inception v3 that achieved similar accuracies both
with and without HVCs. Additionally, the simple monolithic CNN when using HVCs
showed no overfitting after more than 300 epochs whereas the baseline showed
overfitting after 30 epochs. We use the ImageNet ILSVRC 2012 classification
challenge dataset with both networks.
</p>
<a href="http://arxiv.org/abs/1906.08676" target="_blank">arXiv:1906.08676</a> [<a href="http://arxiv.org/pdf/1906.08676" target="_blank">pdf</a>]

<h2>A Bayesian Approach to Direct and Inverse Abstract Argumentation Problems. (arXiv:1909.04319v2 [cs.AI] UPDATED)</h2>
<h3>Hiroyuki Kido, Beishui Liao</h3>
<p>This paper studies a fundamental mechanism of how to detect a conflict
between arguments given sentiments regarding acceptability of the arguments. We
introduce a concept of the inverse problem of the abstract argumentation to
tackle the problem. Given noisy sets of acceptable arguments, it aims to find
attack relations explaining the sets well in terms of acceptability semantics.
It is the inverse of the direct problem corresponding to the traditional
problem of the abstract argumentation that focuses on finding sets of
acceptable arguments in terms of the semantics given an attack relation between
the arguments. We give a probabilistic model handling both of the problems in a
way that is faithful to the acceptability semantics. From a theoretical point
of view, we show that a solution to both the direct and inverse problems is a
special case of the probabilistic inference on the model. We discuss that the
model provides a natural extension of the semantics to cope with uncertain
attack relations distributed probabilistically. From en empirical point of
view, we argue that it reasonably predicts individuals sentiments regarding
acceptability of arguments. This paper contributes to lay the foundation for
making acceptability semantics data-driven and to provide a way to tackle the
knowledge acquisition bottleneck.
</p>
<a href="http://arxiv.org/abs/1909.04319" target="_blank">arXiv:1909.04319</a> [<a href="http://arxiv.org/pdf/1909.04319" target="_blank">pdf</a>]

<h2>Cut-and-Paste Dataset Generation for Balancing Domain Gaps in Object Instance Detection. (arXiv:1909.11972v2 [cs.RO] UPDATED)</h2>
<h3>Woo-han Yun, Taewoo Kim, Jaeyeon Lee, Jaehong Kim, Junmo Kim</h3>
<p>Training an object instance detector where only a few training object images
are available is a challenging task. One solution is a cut-and-paste method
that generates a training dataset by cutting object areas out of training
images and pasting them onto other background images. A detector trained on a
dataset generated with a cut-and-paste method suffers from the conventional
domain shift problem, which stems from a discrepancy between the source domain
(generated training dataset) and the target domain (real test dataset). Though
state-of-the-art domain adaptation methods are able to reduce this gap, it is
limited because they do not consider the difference of domain gaps of
foreground and background. In this study, we present that the conventional
domain gap can be divided into two sub-domain gaps for foreground and
background. Then, we show that the original cut-and-paste approach suffers from
a new domain gap problem, an unbalanced domain gaps, because it has two
separate source domains for foreground and background, unlike the conventional
domain shift problem. Then, we introduce an advanced cut-and-paste method to
balance the unbalanced domain gaps by diversifying the foreground with GAN
(generative adversarial network)-generated seed images and simplifying the
background using image processing techniques. Experimental results show that
our method is effective for balancing domain gaps and improving the accuracy of
object instance detection in a cluttered indoor environment using only a few
seed images. Furthermore, we show that balancing domain gaps can improve the
detection accuracy of state-of-the-art domain adaptation methods.
</p>
<a href="http://arxiv.org/abs/1909.11972" target="_blank">arXiv:1909.11972</a> [<a href="http://arxiv.org/pdf/1909.11972" target="_blank">pdf</a>]

<h2>Sequential Training of Neural Networks with Gradient Boosting. (arXiv:1909.12098v2 [cs.LG] UPDATED)</h2>
<h3>Seyedsaman Emami, Gonzalo Mart&#xed;nez-Mu&#xf1;oz</h3>
<p>This paper presents a novel technique based on gradient boosting to train a
shallow neural network (NN). Gradient boosting is an additive expansion
algorithm in which a series of models are trained sequentially to approximate a
given function. A neural network can also be seen as an additive model where
the scalar product of the responses of the last hidden layer and its weights
provide the final output of the network. Instead of training the network as a
whole, the proposed algorithm trains the network sequentially in $T$ steps.
First, the bias term of the network is initialized with a constant
approximation that minimizes the average loss of the data. Then, at each step,
a portion of the network, composed of $J$ neurons, is trained to approximate
the pseudo-residuals on the training data computed from the previous
iterations. Finally, the $T$ partial models and bias are integrated as a single
NN with $T \times J$ neurons in the hidden layer. Extensive experiments in
classification and regression tasks are carried out showing a competitive
generalization performance with respect to neural networks trained with
different standard solvers, such as Adam, L-BFGS and SGD. Furthermore, we show
that the proposed method design permits to switch off a number of hidden units
during test (the units that were last trained) without a significant reduction
of its generalization ability. This permits the adaptation of the model to
different classification speed requirements on the fly.
</p>
<a href="http://arxiv.org/abs/1909.12098" target="_blank">arXiv:1909.12098</a> [<a href="http://arxiv.org/pdf/1909.12098" target="_blank">pdf</a>]

<h2>Randomized Shortest Paths with Net Flows and Capacity Constraints. (arXiv:1910.01849v3 [cs.LG] UPDATED)</h2>
<h3>Sylvain Courtain, Pierre Leleux, Ilkka Kivimaki, Guillaume Guex, Marco Saerens</h3>
<p>This work extends the randomized shortest paths (RSP) model by investigating
the net flow RSP and adding capacity constraints on edge flows. The standard
RSP is a model of movement, or spread, through a network interpolating between
a random-walk and a shortest-path behavior [30, 42, 49]. The framework assumes
a unit flow injected into a source node and collected from a target node with
flows minimizing the expected transportation cost, together with a relative
entropy regularization term. In this context, the present work first develops
the net flow RSP model considering that edge flows in opposite directions
neutralize each other (as in electric networks), and proposes an algorithm for
computing the expected routing costs between all pairs of nodes. This quantity
is called the net flow RSP dissimilarity measure between nodes. Experimental
comparisons on node clustering tasks indicate that the net flow RSP
dissimilarity is competitive with other state-of-the-art dissimilarities. In
the second part of the paper, it is shown how to introduce capacity constraints
on edge flows, and a procedure is developed to solve this constrained problem
by exploiting Lagrangian duality. These two extensions should improve
significantly the scope of applications of the RSP framework.
</p>
<a href="http://arxiv.org/abs/1910.01849" target="_blank">arXiv:1910.01849</a> [<a href="http://arxiv.org/pdf/1910.01849" target="_blank">pdf</a>]

<h2>A Scalable Multilabel Classification to Deploy Deep Learning Architectures For Edge Devices. (arXiv:1911.02098v3 [cs.LG] UPDATED)</h2>
<h3>Tolulope A. Odetola, Ogheneuriri Oderhohwo, Syed Rafay Hasan</h3>
<p>Convolution Neural Networks (CNN) have performed well in many applications
such as object detection, pattern recognition, video surveillance and so on.
CNN carryout feature extraction on labelled data to perform classification.
Multi-label classification assigns more than one label to a particular data
sample in a data set. In multi-label classification, properties of a data point
that are considered to be mutually exclusive are classified. However, existing
multi-label classification requires some form of data pre-processing that
involves image training data cropping or image tiling. The computation and
memory requirement of these multi-label CNN models makes their deployment on
edge devices challenging. In this paper, we propose a methodology that solves
this problem by extending the capability of existing multi-label classification
and provide models with lower latency that requires smaller memory size when
deployed on edge devices. We make use of a single CNN model designed with
multiple loss layers and multiple accuracy layers. This methodology is tested
on state-of-the-art deep learning algorithms such as AlexNet, GoogleNet and
SqueezeNet using the Stanford Cars Dataset and deployed on Raspberry Pi3. From
the results the proposed methodology achieves comparable accuracy with 1.8x
less MACC operation, 0.97x reduction in latency and 0.5x, 0.84x and 0.97x
reduction in size for the generated AlexNet, GoogleNet and SqueezeNet CNN
models respectively when compared to conventional ways of achieving multi-label
classification like hard-coding multi-label instances into single labels. The
methodology also yields CNN models that achieve 50\% less MACC operations, 50%
reduction in latency and size of generated versions of AlexNet, GoogleNet and
SqueezeNet respectively when compared to conventional ways using 2 different
single-labelled models to achieve multi-label classification.
</p>
<a href="http://arxiv.org/abs/1911.02098" target="_blank">arXiv:1911.02098</a> [<a href="http://arxiv.org/pdf/1911.02098" target="_blank">pdf</a>]

<h2>Infrequent adverse event prediction in low carbon energy production using machine learning. (arXiv:2001.06916v2 [cs.LG] UPDATED)</h2>
<h3>Stefano Coniglio, Anthony J. Dunn, Alain B. Zemkoho</h3>
<p>We address the problem of predicting the occurrence of infrequent adverse
events in the context of predictive maintenance. We cast the corresponding
machine learning task as an imbalanced classification problem and propose a
framework for solving it that is capable of leveraging different classifiers in
order to predict the occurrence of an adverse event before it takes place. In
particular, we focus on two applications arising in low-carbon energy
production: foam formation in anaerobic digestion and condenser tube leakage in
the steam turbines of a nuclear power station. The results of an extensive set
of omputational experiments show the effectiveness of the techniques that we
propose.
</p>
<a href="http://arxiv.org/abs/2001.06916" target="_blank">arXiv:2001.06916</a> [<a href="http://arxiv.org/pdf/2001.06916" target="_blank">pdf</a>]

<h2>A Branching and Merging Convolutional Network with Homogeneous Filter Capsules. (arXiv:2001.09136v5 [cs.CV] UPDATED)</h2>
<h3>Adam Byerly, Tatiana Kalganova, Ian Dear</h3>
<p>We present a convolutional neural network design with additional branches
after certain convolutions so that we can extract features with differing
effective receptive fields and levels of abstraction. From each branch, we
transform each of the final filters into a pair of homogeneous vector capsules.
As the capsules are formed from entire filters, we refer to them as filter
capsules. We then compare three methods for merging the branches--merging with
equal weight and merging with learned weights, with two different weight
initialization strategies. This design, in combination with a domain-specific
set of randomly applied augmentation techniques, establishes a new state of the
art for the MNIST dataset with an accuracy of 99.84% for an ensemble of these
models, as well as establishing a new state of the art for a single model
(99.79% accurate). These accuracies were achieved with a 75% reduction in both
the number of parameters and the number of epochs of training relative to the
previously best performing capsule network on MNIST. All training was performed
using the Adam optimizer and experienced no overfitting.
</p>
<a href="http://arxiv.org/abs/2001.09136" target="_blank">arXiv:2001.09136</a> [<a href="http://arxiv.org/pdf/2001.09136" target="_blank">pdf</a>]

<h2>Naive Exploration is Optimal for Online LQR. (arXiv:2001.09576v3 [cs.LG] UPDATED)</h2>
<h3>Max Simchowitz, Dylan J. Foster</h3>
<p>We consider the problem of online adaptive control of the linear quadratic
regulator, where the true system parameters are unknown. We prove new upper and
lower bounds demonstrating that the optimal regret scales as
$\widetilde{\Theta}({\sqrt{d_{\mathbf{u}}^2 d_{\mathbf{x}} T}})$, where $T$ is
the number of time steps, $d_{\mathbf{u}}$ is the dimension of the input space,
and $d_{\mathbf{x}}$ is the dimension of the system state. Notably, our lower
bounds rule out the possibility of a $\mathrm{poly}(\log{}T)$-regret algorithm,
which had been conjectured due to the apparent strong convexity of the problem.
Our upper bound is attained by a simple variant of $\textit{{certainty
equivalent control}}$, where the learner selects control inputs according to
the optimal controller for their estimate of the system while injecting
exploratory random noise. While this approach was shown to achieve
$\sqrt{T}$-regret by (Mania et al. 2019), we show that if the learner
continually refines their estimates of the system matrices, the method attains
optimal dimension dependence as well.

Central to our upper and lower bounds is a new approach for controlling
perturbations of Riccati equations called the $\textit{self-bounding ODE
method}$, which we use to derive suboptimality bounds for the certainty
equivalent controller synthesized from estimated system dynamics. This in turn
enables regret upper bounds which hold for $\textit{any stabilizable instance}$
and scale with natural control-theoretic quantities.
</p>
<a href="http://arxiv.org/abs/2001.09576" target="_blank">arXiv:2001.09576</a> [<a href="http://arxiv.org/pdf/2001.09576" target="_blank">pdf</a>]

<h2>Distributed stochastic gradient MCMC for federated learning. (arXiv:2004.11231v2 [stat.ML] UPDATED)</h2>
<h3>Khaoula El Mekkaoui, Diego Mesquita, Paul Blomstedt, Samuel Kaski</h3>
<p>Stochastic gradient MCMC methods, such as stochastic gradient Langevin
dynamics (SGLD), enable large-scale posterior inference by leveraging noisy but
cheap gradient estimates. However, when federated data are non-IID, the
variance of distributed gradient estimates is amplified compared to its
centralized version, and delayed communication rounds lead chains to diverge
from the target posterior. In this work, we introduce the concept of conducive
gradients, zero-mean stochastic gradients that serve as a mechanism for sharing
probabilistic information between data shards. We propose a novel stochastic
gradient estimator that incorporates the conducive gradients, and we show that
it improves convergence on federated data when compared to distributed SGLD
(DSGLD). We evaluate, conducive gradient DSGLD (CG-DSGLD) on metric learning
and deep MLPs tasks. Experiments show that it outperforms standard DSGLD for
non-IID federated data.
</p>
<a href="http://arxiv.org/abs/2004.11231" target="_blank">arXiv:2004.11231</a> [<a href="http://arxiv.org/pdf/2004.11231" target="_blank">pdf</a>]

<h2>Bayesian Entailment Hypothesis: How Brains Implement Monotonic and Non-monotonic Reasoning. (arXiv:2005.00961v3 [cs.AI] UPDATED)</h2>
<h3>Hiroyuki Kido</h3>
<p>Recent success of Bayesian methods in neuroscience and artificial
intelligence gives rise to the hypothesis that the brain is a Bayesian machine.
Since logic, as the laws of thought, is a product and practice of the human
brain, it leads to another hypothesis that there is a Bayesian algorithm and
data-structure for logical reasoning. In this paper, we give a Bayesian account
of entailment and characterize its abstract inferential properties. The
Bayesian entailment is shown to be a monotonic consequence relation in an
extreme case. In general, it is a sort of non-monotonic consequence relation
without Cautious monotony or Cut. The preferential entailment, which is a
representative non-monotonic consequence relation, is shown to be maximum a
posteriori entailment, which is an approximation of the Bayesian entailment. We
finally discuss merits of our proposals in terms of encoding preferences on
defaults, handling change and contradiction, and modeling human entailment.
</p>
<a href="http://arxiv.org/abs/2005.00961" target="_blank">arXiv:2005.00961</a> [<a href="http://arxiv.org/pdf/2005.00961" target="_blank">pdf</a>]

<h2>Is deeper better? It depends on locality of relevant features. (arXiv:2005.12488v2 [cs.LG] UPDATED)</h2>
<h3>Takashi Mori, Masahito Ueda</h3>
<p>It has been recognized that a heavily overparameterized artificial neural
network exhibits surprisingly good generalization performance in various
machine-learning tasks. Recent theoretical studies have made attempts to unveil
the mystery of the overparameterization. In most of those previous works, the
overparameterization is achieved by increasing the width of the network, while
the effect of increasing the depth has remained less well understood. In this
work, we investigate the effect of increasing the depth within an
overparameterized regime. To gain an insight into the advantage of depth, we
introduce local and global labels as abstract but simple classification rules.
It turns out that the locality of the relevant feature for a given
classification rule plays a key role; our experimental results suggest that
deeper is better for local labels, whereas shallower is better for global
labels. We also compare the results of finite networks with those of the neural
tangent kernel (NTK), which is equivalent to an infinitely wide network with a
proper initialization and an infinitesimal learning rate. It is shown that the
NTK does not correctly capture the depth dependence of the generalization
performance, which indicates the importance of the feature learning rather than
the lazy learning.
</p>
<a href="http://arxiv.org/abs/2005.12488" target="_blank">arXiv:2005.12488</a> [<a href="http://arxiv.org/pdf/2005.12488" target="_blank">pdf</a>]

<h2>Bayesian Sparse Factor Analysis with Kernelized Observations. (arXiv:2006.00968v3 [stat.ML] UPDATED)</h2>
<h3>Carlos Sevilla-Salcedo, Alejandro Guerrero-L&#xf3;pez, Pablo M. Olmos, Vanessa G&#xf3;mez-Verdejo</h3>
<p>Multi-view problems can be faced with latent variable models since they are
able to find low-dimensional projections that fairly capture the correlations
among the multiple views that characterise each datum. On the other hand,
high-dimensionality and non-linear issues are traditionally handled by kernel
methods, inducing a (non)-linear function between the latent projection and the
data itself. However, they usually come with scalability issues and exposition
to overfitting. Here, we propose merging both approaches into single model so
that we can exploit the best features of multi-view latent models and kernel
methods and, moreover, overcome their limitations.

In particular, we combine probabilistic factor analysis with what we refer to
as kernelized observations, in which the model focuses on reconstructing not
the data itself, but its relationship with other data points measured by a
kernel function. This model can combine several types of views (kernelized or
not), and it can handle heterogeneous data and work in semi-supervised
settings. Additionally, by including adequate priors, it can provide compact
solutions for the kernelized observations -- based in a automatic selection of
Bayesian Relevance Vectors (RVs) -- and can include feature selection
capabilities. Using several public databases, we demonstrate the potential of
our approach (and its extensions) w.r.t. common multi-view learning models such
as kernel canonical correlation analysis or manifold relevance determination.
</p>
<a href="http://arxiv.org/abs/2006.00968" target="_blank">arXiv:2006.00968</a> [<a href="http://arxiv.org/pdf/2006.00968" target="_blank">pdf</a>]

<h2>Content and Context Features for Scene Image Representation. (arXiv:2006.03217v2 [cs.CV] UPDATED)</h2>
<h3>Chiranjibi Sitaula, Sunil Aryal, Yong Xiang, Anish Basnet, Xuequan Lu</h3>
<p>Existing research in scene image classification has focused on either content
features (e.g., visual information) or context features (e.g., annotations). As
they capture different information about images which can be complementary and
useful to discriminate images of different classes, we suppose the fusion of
them will improve classification results. In this paper, we propose new
techniques to compute content features and context features, and then fuse them
together. For content features, we design multi-scale deep features based on
background and foreground information in images. For context features, we use
annotations of similar images available in the web to design a filter words
(codebook). Our experiments in three widely used benchmark scene datasets using
support vector machine classifier reveal that our proposed context and content
features produce better results than existing context and content features,
respectively. The fusion of the proposed two types of features significantly
outperform numerous state-of-the-art features.
</p>
<a href="http://arxiv.org/abs/2006.03217" target="_blank">arXiv:2006.03217</a> [<a href="http://arxiv.org/pdf/2006.03217" target="_blank">pdf</a>]

<h2>The foundations of cost-sensitive causal classification. (arXiv:2007.12582v4 [cs.LG] UPDATED)</h2>
<h3>Wouter Verbeke, Diego Olaya, Jeroen Berrevoets, Sebasti&#xe1;n Maldonado</h3>
<p>Classification is a well-studied machine learning task which concerns the
assignment of instances to a set of outcomes. Classification models support the
optimization of managerial decision-making across a variety of operational
business processes. For instance, customer churn prediction models are adopted
to increase the efficiency of retention campaigns by optimizing the selection
of customers that are to be targeted. Cost-sensitive and causal classification
methods have independently been proposed to improve the performance of
classification models. The former considers the benefits and costs of correct
and incorrect classifications, such as the benefit of a retained customer,
whereas the latter estimates the causal effect of an action, such as a
retention campaign, on the outcome of interest. This study integrates
cost-sensitive and causal classification by elaborating a unifying evaluation
framework. The framework encompasses a range of existing and novel performance
measures for evaluating both causal and conventional classification models in a
cost-sensitive as well as a cost-insensitive manner. We proof that conventional
classification is a specific case of causal classification in terms of a range
of performance measures when the number of actions is equal to one. The
framework is shown to instantiate to application-specific cost-sensitive
performance measures that have been recently proposed for evaluating customer
retention and response uplift models, and allows to maximize profitability when
adopting a causal classification model for optimizing decision-making. The
proposed framework paves the way toward the development of cost-sensitive
causal learning methods and opens a range of opportunities for improving
data-driven business decision-making.
</p>
<a href="http://arxiv.org/abs/2007.12582" target="_blank">arXiv:2007.12582</a> [<a href="http://arxiv.org/pdf/2007.12582" target="_blank">pdf</a>]

<h2>SeCo: Exploring Sequence Supervision for Unsupervised Representation Learning. (arXiv:2008.00975v2 [cs.CV] UPDATED)</h2>
<h3>Ting Yao, Yiheng Zhang, Zhaofan Qiu, Yingwei Pan, Tao Mei</h3>
<p>A steady momentum of innovations and breakthroughs has convincingly pushed
the limits of unsupervised image representation learning. Compared to static 2D
images, video has one more dimension (time). The inherent supervision existing
in such sequential structure offers a fertile ground for building unsupervised
learning models. In this paper, we compose a trilogy of exploring the basic and
generic supervision in the sequence from spatial, spatiotemporal and sequential
perspectives. We materialize the supervisory signals through determining
whether a pair of samples is from one frame or from one video, and whether a
triplet of samples is in the correct temporal order. We uniquely regard the
signals as the foundation in contrastive learning and derive a particular form
named Sequence Contrastive Learning (SeCo). SeCo shows superior results under
the linear protocol on action recognition (Kinetics), untrimmed activity
recognition (ActivityNet) and object tracking (OTB-100). More remarkably, SeCo
demonstrates considerable improvements over recent unsupervised pre-training
techniques, and leads the accuracy by 2.96% and 6.47% against fully-supervised
ImageNet pre-training in action recognition task on UCF101 and HMDB51,
respectively. Source code is available at
\url{https://github.com/YihengZhang-CV/SeCo-Sequence-Contrastive-Learning}.
</p>
<a href="http://arxiv.org/abs/2008.00975" target="_blank">arXiv:2008.00975</a> [<a href="http://arxiv.org/pdf/2008.00975" target="_blank">pdf</a>]

<h2>Convolutional Ordinal Regression Forest for Image Ordinal Estimation. (arXiv:2008.03077v2 [cs.CV] UPDATED)</h2>
<h3>Haiping Zhu, Hongming Shan, Yuheng Zhang, Lingfu Che, Xiaoyang Xu, Junping Zhang, Jianbo Shi, Fei-Yue Wang</h3>
<p>Image ordinal estimation is to predict the ordinal label of a given image,
which can be categorized as an ordinal regression problem. Recent methods
formulate an ordinal regression problem as a series of binary classification
problems. Such methods cannot ensure that the global ordinal relationship is
preserved since the relationships among different binary classifiers are
neglected. We propose a novel ordinal regression approach, termed Convolutional
Ordinal Regression Forest or CORF, for image ordinal estimation, which can
integrate ordinal regression and differentiable decision trees with a
convolutional neural network for obtaining precise and stable global ordinal
relationships. The advantages of the proposed CORF are twofold. First, instead
of learning a series of binary classifiers \emph{independently}, the proposed
method aims at learning an ordinal distribution for ordinal regression by
optimizing those binary classifiers \emph{simultaneously}. Second, the
differentiable decision trees in the proposed CORF can be trained together with
the ordinal distribution in an end-to-end manner. The effectiveness of the
proposed CORF is verified on two image ordinal estimation tasks, i.e. facial
age estimation and image aesthetic assessment, showing significant improvements
and better stability over the state-of-the-art ordinal regression methods.
</p>
<a href="http://arxiv.org/abs/2008.03077" target="_blank">arXiv:2008.03077</a> [<a href="http://arxiv.org/pdf/2008.03077" target="_blank">pdf</a>]

<h2>Preferential Bayesian optimisation with Skew Gaussian Processes. (arXiv:2008.06677v2 [cs.LG] UPDATED)</h2>
<h3>Alessio Benavoli, Dario Azzimonti, Dario Piga</h3>
<p>Bayesian optimisation (BO) is a very effective approach for sequential
black-box optimization where direct queries of the objective function are
expensive. However, there are cases where the objective function can only be
accessed via preference judgments, such as "this is better than that" between
two candidate solutions (like in A/B tests or recommender systems). The
state-of-the-art approach to Preferential Bayesian Optimization (PBO) uses a
Gaussian process to model the preference function and a Bernoulli likelihood to
model the observed pairwise comparisons. Laplace's method is then employed to
compute posterior inferences and, in particular, to build an appropriate
acquisition function. In this paper, we prove that the true posterior
distribution of the preference function is a Skew Gaussian Process (SkewGP),
with highly skewed pairwise marginals and, thus, show that Laplace's method
usually provides a very poor approximation. We then derive an efficient method
to compute the exact SkewGP posterior and use it as surrogate model for PBO
employing standard acquisition functions (Upper Credible Bound, etc.). We
illustrate the benefits of our exact PBO-SkewGP in a variety of experiments, by
showing that it consistently outperforms PBO based on Laplace's approximation
both in terms of convergence speed and computational time. We also show that
our framework can be extended to deal with mixed preferential-categorical BO,
typical for instance in smart manufacturing, where binary judgments (valid or
non-valid) together with preference judgments are available.
</p>
<a href="http://arxiv.org/abs/2008.06677" target="_blank">arXiv:2008.06677</a> [<a href="http://arxiv.org/pdf/2008.06677" target="_blank">pdf</a>]

<h2>3D-DEEP: 3-Dimensional Deep-learning based on elevation patterns forroad scene interpretation. (arXiv:2009.00330v2 [cs.CV] UPDATED)</h2>
<h3>A. Hern&#xe1;ndez, S. Woo, H. Corrales, I. Parra, E. Kim, D. F. Llorca, M. A. Sotelo</h3>
<p>Road detection and segmentation is a crucial task in computer vision for safe
autonomous driving. With this in mind, a new net architecture (3D-DEEP) and its
end-to-end training methodology for CNN-based semantic segmentation are
described along this paper for. The method relies on disparity filtered and
LiDAR projected images for three-dimensional information and image feature
extraction through fully convolutional networks architectures. The developed
models were trained and validated over Cityscapes dataset using just fine
annotation examples with 19 different training classes, and over KITTI road
dataset. 72.32% mean intersection over union(mIoU) has been obtained for the 19
Cityscapes training classes using the validation images. On the other hand,
over KITTIdataset the model has achieved an F1 error value of 97.85%
invalidation and 96.02% using the test images.
</p>
<a href="http://arxiv.org/abs/2009.00330" target="_blank">arXiv:2009.00330</a> [<a href="http://arxiv.org/pdf/2009.00330" target="_blank">pdf</a>]

<h2>An Algorithm for Out-Of-Distribution Attack to Neural Network Encoder. (arXiv:2009.08016v4 [cs.CV] UPDATED)</h2>
<h3>Liang Liang, Linhai Ma, Linchen Qian, Jiasong Chen</h3>
<p>Deep neural networks (DNNs), especially convolutional neural networks, have
achieved superior performance on image classification tasks. However, such
performance is only guaranteed if the input to a trained model is similar to
the training samples, i.e., the input follows the probability distribution of
the training set. Out-Of-Distribution (OOD) samples do not follow the
distribution of training set, and therefore the predicted class labels on OOD
samples become meaningless. Classification-based methods have been proposed for
OOD detection; however, in this study we show that this type of method has no
theoretical guarantee and is practically breakable by our OOD Attack algorithm
because of dimensionality reduction in the DNN models. We also show that Glow
likelihood-based OOD detection is breakable as well.
</p>
<a href="http://arxiv.org/abs/2009.08016" target="_blank">arXiv:2009.08016</a> [<a href="http://arxiv.org/pdf/2009.08016" target="_blank">pdf</a>]

<h2>Knowledge Graph Embeddings in Geometric Algebras. (arXiv:2010.00989v3 [cs.LG] UPDATED)</h2>
<h3>Chengjin Xu, Mojtaba Nayyeri, Yung-Yu Chen, Jens Lehmann</h3>
<p>Knowledge graph (KG) embedding aims at embedding entities and relations in a
KG into a lowdimensional latent representation space. Existing KG embedding
approaches model entities andrelations in a KG by utilizing real-valued ,
complex-valued, or hypercomplex-valued (Quaternionor Octonion) representations,
all of which are subsumed into a geometric algebra. In this work,we introduce a
novel geometric algebra-based KG embedding framework, GeomE, which uti-lizes
multivector representations and the geometric product to model entities and
relations. Ourframework subsumes several state-of-the-art KG embedding
approaches and is advantageouswith its ability of modeling various key relation
patterns, including (anti-)symmetry, inversionand composition, rich
expressiveness with higher degree of freedom as well as good general-ization
capacity. Experimental results on multiple benchmark knowledge graphs show that
theproposed approach outperforms existing state-of-the-art models for link
prediction.
</p>
<a href="http://arxiv.org/abs/2010.00989" target="_blank">arXiv:2010.00989</a> [<a href="http://arxiv.org/pdf/2010.00989" target="_blank">pdf</a>]

<h2>MedMNIST Classification Decathlon: A Lightweight AutoML Benchmark for Medical Image Analysis. (arXiv:2010.14925v3 [cs.CV] UPDATED)</h2>
<h3>Jiancheng Yang, Rui Shi, Bingbing Ni</h3>
<p>We present MedMNIST, a collection of 10 pre-processed medical open datasets.
MedMNIST is standardized to perform classification tasks on lightweight 28x28
images, which requires no background knowledge. Covering the primary data
modalities in medical image analysis, it is diverse on data scale (from 100 to
100,000) and tasks (binary/multi-class, ordinal regression and multi-label).
MedMNIST could be used for educational purpose, rapid prototyping, multi-modal
machine learning or AutoML in medical image analysis. Moreover, MedMNIST
Classification Decathlon is designed to benchmark AutoML algorithms on all 10
datasets; We have compared several baseline methods, including open-source or
commercial AutoML tools. The datasets, evaluation code and baseline methods for
MedMNIST are publicly available at https://medmnist.github.io/.
</p>
<a href="http://arxiv.org/abs/2010.14925" target="_blank">arXiv:2010.14925</a> [<a href="http://arxiv.org/pdf/2010.14925" target="_blank">pdf</a>]

<h2>Continuous Conditional Generative Adversarial Networks for Image Generation: Novel Losses and Label Input Mechanisms. (arXiv:2011.07466v4 [cs.CV] UPDATED)</h2>
<h3>Xin Ding, Yongwei Wang, Zuheng Xu, William J. Welch, Z. Jane Wang</h3>
<p>This work proposes the continuous conditional generative adversarial network
(CcGAN), the first generative model for image generation conditional on
continuous, scalar conditions (termed regression labels). Existing conditional
GANs (cGANs) are mainly designed for categorical conditions (eg, class labels);
conditioning on regression labels is mathematically distinct and raises two
fundamental problems:(P1) Since there may be very few (even zero) real images
for some regression labels, minimizing existing empirical versions of cGAN
losses (aka empirical cGAN losses) often fails in practice;(P2) Since
regression labels are scalar and infinitely many, conventional label input
methods are not applicable. The proposed CcGAN solves the above problems,
respectively, by (S1) reformulating existing empirical cGAN losses to be
appropriate for the continuous scenario; and (S2) proposing a naive label input
(NLI) method and an improved label input (ILI) method to incorporate regression
labels into the generator and the discriminator. The reformulation in (S1)
leads to two novel empirical discriminator losses, termed the hard vicinal
discriminator loss (HVDL) and the soft vicinal discriminator loss (SVDL)
respectively, and a novel empirical generator loss. The error bounds of a
discriminator trained with HVDL and SVDL are derived under mild assumptions in
this work. Two new benchmark datasets (RC-49 and Cell-200) and a novel
evaluation metric (Sliding Fr\'echet Inception Distance) are also proposed for
this continuous scenario. Our experiments on the Circular 2-D Gaussians, RC-49,
UTKFace, Cell-200, and Steering Angle datasets show that CcGAN is able to
generate diverse, high-quality samples from the image distribution conditional
on a given regression label. Moreover, in these experiments, CcGAN
substantially outperforms cGAN both visually and quantitatively.
</p>
<a href="http://arxiv.org/abs/2011.07466" target="_blank">arXiv:2011.07466</a> [<a href="http://arxiv.org/pdf/2011.07466" target="_blank">pdf</a>]

<h2>Functional Time Series Forecasting: Functional Singular Spectrum Analysis Approaches. (arXiv:2011.13077v4 [stat.ML] UPDATED)</h2>
<h3>Jordan Trinka, Hossein Haghbin, Mehdi Maadooliat</h3>
<p>In this paper, we propose two nonparametric methods used in the forecasting
of functional time-dependent data, namely functional singular spectrum analysis
recurrent forecasting and vector forecasting. Both algorithms utilize the
results of functional singular spectrum analysis and past observations in order
to predict future data points where recurrent forecasting predicts one function
at a time and the vector forecasting makes predictions using functional
vectors. We compare our forecasting methods to a gold standard algorithm used
in the prediction of functional, time-dependent data by way of simulation and
real data and we find our techniques do better for periodic stochastic
processes.
</p>
<a href="http://arxiv.org/abs/2011.13077" target="_blank">arXiv:2011.13077</a> [<a href="http://arxiv.org/pdf/2011.13077" target="_blank">pdf</a>]

<h2>Target Reaching Behaviour for Unfreezing the Robot in a Semi-Static and Crowded Environment. (arXiv:2012.01206v2 [cs.RO] UPDATED)</h2>
<h3>Arturo Cruz-Maya</h3>
<p>Robot navigation in human semi-static and crowded environments can lead to
the freezing problem, where the robot can not move due to the presence of
humans standing on its path and no other path is available. Classical
approaches of robot navigation do not provide a solution for this problem. In
such situations, the robot could interact with the humans in order to clear its
path instead of considering them as unanimated obstacles. In this work, we
propose a robot behavior for a wheeled humanoid robot that complains with
social norms for clearing its path when the robot is frozen due to the presence
of humans. The behavior consists of two modules: 1) A detection module, which
make use of the Yolo v3 algorithm trained to detect human hands and human arms.
2) A gesture module, which make use of a policy trained in simulation using the
Proximal Policy Optimization algorithm. Orchestration of the two models is done
using the ROS framework.
</p>
<a href="http://arxiv.org/abs/2012.01206" target="_blank">arXiv:2012.01206</a> [<a href="http://arxiv.org/pdf/2012.01206" target="_blank">pdf</a>]

<h2>Accelerating Continuous Normalizing Flow with Trajectory Polynomial Regularization. (arXiv:2012.04228v2 [cs.LG] UPDATED)</h2>
<h3>Han-Hsien Huang, Mi-Yen Yeh</h3>
<p>In this paper, we propose an approach to effectively accelerating the
computation of continuous normalizing flow (CNF), which has been proven to be a
powerful tool for the tasks such as variational inference and density
estimation. The training time cost of CNF can be extremely high because the
required number of function evaluations (NFE) for solving corresponding
ordinary differential equations (ODE) is very large. We think that the high NFE
results from large truncation errors of solving ODEs. To address the problem,
we propose to add a regularization. The regularization penalizes the difference
between the trajectory of the ODE and its fitted polynomial regression. The
trajectory of ODE will approximate a polynomial function, and thus the
truncation error will be smaller. Furthermore, we provide two proofs and claim
that the additional regularization does not harm training quality. Experimental
results show that our proposed method can result in 42.3% to 71.3% reduction of
NFE on the task of density estimation, and 19.3% to 32.1% reduction of NFE on
variational auto-encoder, while the testing losses are not affected.
</p>
<a href="http://arxiv.org/abs/2012.04228" target="_blank">arXiv:2012.04228</a> [<a href="http://arxiv.org/pdf/2012.04228" target="_blank">pdf</a>]

<h2>A unified framework for closed-form nonparametric regression, classification, preference and mixed problems with Skew Gaussian Processes. (arXiv:2012.06846v2 [stat.ML] UPDATED)</h2>
<h3>Alessio Benavoli, Dario Azzimonti, Dario Piga</h3>
<p>Skew-Gaussian processes (SkewGPs) extend the multivariate Unified Skew-Normal
distributions over finite dimensional vectors to distribution over functions.
SkewGPs are more general and flexible than Gaussian processes, as SkewGPs may
also represent asymmetric distributions. In a recent contribution we showed
that SkewGP and probit likelihood are conjugate, which allows us to compute the
exact posterior for non-parametric binary classification and preference
learning. In this paper, we generalize previous results and we prove that
SkewGP is conjugate with both the normal and affine probit likelihood, and more
in general, with their product. This allows us to (i) handle classification,
preference, numeric and ordinal regression, and mixed problems in a unified
framework; (ii) derive closed-form expression for the corresponding posterior
distributions. We show empirically that the proposed framework based on SkewGP
provides better performance than Gaussian processes in active learning and
Bayesian (constrained) optimization. These two tasks are fundamental for design
of experiments and in Data Science.
</p>
<a href="http://arxiv.org/abs/2012.06846" target="_blank">arXiv:2012.06846</a> [<a href="http://arxiv.org/pdf/2012.06846" target="_blank">pdf</a>]

<h2>Bayes Meets Entailment and Prediction: Commonsense Reasoning with Non-monotonicity, Paraconsistency and Predictive Accuracy. (arXiv:2012.08479v3 [cs.AI] UPDATED)</h2>
<h3>Hiroyuki Kido, Keishi Okamoto</h3>
<p>The recent success of Bayesian methods in neuroscience and artificial
intelligence gives rise to the hypothesis that the brain is a Bayesian machine.
Since logic and learning are both practices of the human brain, it leads to
another hypothesis that there is a Bayesian interpretation underlying both
logical reasoning and machine learning. In this paper, we introduce a
generative model of logical consequence relations. It formalises the process of
how the truth value of a sentence is probabilistically generated from the
probability distribution over states of the world. We show that the generative
model characterises a classical consequence relation, paraconsistent
consequence relation and nonmonotonic consequence relation. In particular, the
generative model gives a new consequence relation that outperforms them in
reasoning with inconsistent knowledge. We also show that the generative model
gives a new classification algorithm that outperforms several representative
algorithms in predictive accuracy and complexity on the Kaggle Titanic dataset.
</p>
<a href="http://arxiv.org/abs/2012.08479" target="_blank">arXiv:2012.08479</a> [<a href="http://arxiv.org/pdf/2012.08479" target="_blank">pdf</a>]

<h2>Personalized Federated Learning with First Order Model Optimization. (arXiv:2012.08565v2 [cs.LG] UPDATED)</h2>
<h3>Michael Zhang, Karan Sapra, Sanja Fidler, Serena Yeung, Jose M. Alvarez</h3>
<p>While federated learning traditionally aims to train a single global model
across decentralized local datasets, one model may not always be ideal for all
participating clients. Here we propose an alternative, where each client only
federates with other relevant clients to obtain a stronger model per
client-specific objectives. To achieve this personalization, rather than
computing a single model average with constant weights for the entire
federation as in traditional FL, we efficiently calculate optimal weighted
model combinations for each client, based on figuring out how much a client can
benefit from another's model. We do not assume knowledge of any underlying data
distributions or client similarities, and allow each client to optimize for
arbitrary target distributions of interest, enabling greater flexibility for
personalization. We evaluate and characterize our method on a variety of
federated settings, datasets, and degrees of local data heterogeneity. Our
method outperforms existing alternatives, while also enabling new features for
personalized FL such as transfer outside of local data distributions.
</p>
<a href="http://arxiv.org/abs/2012.08565" target="_blank">arXiv:2012.08565</a> [<a href="http://arxiv.org/pdf/2012.08565" target="_blank">pdf</a>]

<h2>C-Watcher: A Framework for Early Detection of High-Risk Neighborhoods Ahead of COVID-19 Outbreak. (arXiv:2012.12169v2 [cs.LG] UPDATED)</h2>
<h3>Congxi Xiao, Jingbo Zhou, Jizhou Huang, An Zhuo, Ji Liu, Haoyi Xiong, Dejing Dou</h3>
<p>The novel coronavirus disease (COVID-19) has crushed daily routines and is
still rampaging through the world. Existing solution for nonpharmaceutical
interventions usually needs to timely and precisely select a subset of
residential urban areas for containment or even quarantine, where the spatial
distribution of confirmed cases has been considered as a key criterion for the
subset selection. While such containment measure has successfully stopped or
slowed down the spread of COVID-19 in some countries, it is criticized for
being inefficient or ineffective, as the statistics of confirmed cases are
usually time-delayed and coarse-grained. To tackle the issues, we propose
C-Watcher, a novel data-driven framework that aims at screening every
neighborhood in a target city and predicting infection risks, prior to the
spread of COVID-19 from epicenters to the city. In terms of design, C-Watcher
collects large-scale long-term human mobility data from Baidu Maps, then
characterizes every residential neighborhood in the city using a set of
features based on urban mobility patterns. Furthermore, to transfer the
firsthand knowledge (witted in epicenters) to the target city before local
outbreaks, we adopt a novel adversarial encoder framework to learn
"city-invariant" representations from the mobility-related features for precise
early detection of high-risk neighborhoods, even before any confirmed cases
known, in the target city. We carried out extensive experiments on C-Watcher
using the real-data records in the early stage of COVID-19 outbreaks, where the
results demonstrate the efficiency and effectiveness of C-Watcher for early
detection of high-risk neighborhoods from a large number of cities.
</p>
<a href="http://arxiv.org/abs/2012.12169" target="_blank">arXiv:2012.12169</a> [<a href="http://arxiv.org/pdf/2012.12169" target="_blank">pdf</a>]

<h2>Active Screening for Recurrent Diseases: A Reinforcement Learning Approach. (arXiv:2101.02766v2 [cs.LG] UPDATED)</h2>
<h3>Han-Ching Ou, Haipeng Chen, Shahin Jabbari, Milind Tambe</h3>
<p>Active screening is a common approach in controlling the spread of recurring
infectious diseases such as tuberculosis and influenza. In this approach,
health workers periodically select a subset of population for screening.
However, given the limited number of health workers, only a small subset of the
population can be visited in any given time period. Given the recurrent nature
of the disease and rapid spreading, the goal is to minimize the number of
infections over a long time horizon. Active screening can be formalized as a
sequential combinatorial optimization over the network of people and their
connections. The main computational challenges in this formalization arise from
i) the combinatorial nature of the problem, ii) the need of sequential planning
and iii) the uncertainties in the infectiousness states of the population.

Previous works on active screening fail to scale to large time horizon while
fully considering the future effect of current interventions. In this paper, we
propose a novel reinforcement learning (RL) approach based on Deep Q-Networks
(DQN), with several innovative adaptations that are designed to address the
above challenges. First, we use graph convolutional networks (GCNs) to
represent the Q-function that exploit the node correlations of the underlying
contact network. Second, to avoid solving a combinatorial optimization problem
in each time period, we decompose the node set selection as a sub-sequence of
decisions, and further design a two-level RL framework that solves the problem
in a hierarchical way. Finally, to speed-up the slow convergence of RL which
arises from reward sparseness, we incorporate ideas from curriculum learning
into our hierarchical RL approach. We evaluate our RL algorithm on several
real-world networks.
</p>
<a href="http://arxiv.org/abs/2101.02766" target="_blank">arXiv:2101.02766</a> [<a href="http://arxiv.org/pdf/2101.02766" target="_blank">pdf</a>]

<h2>Multi-robot Symmetric Rendezvous Search on the Line. (arXiv:2101.05324v2 [cs.RO] UPDATED)</h2>
<h3>Deniz Ozsoyeller, Pratap Tokekar</h3>
<p>We study the Symmetric Rendezvous Search Problem for a multi-robot system.
There are $n&gt;2$ robots arbitrarily located on a line. Their goal is to meet
somewhere on the line as quickly as possible. The robots do not know the
initial location of any of the other robots or their own positions on the line.
The symmetric version of the problem requires the robots to execute the same
search strategy to achieve rendezvous. Therefore, we solve the problem in an
online fashion with a randomized strategy. In this paper, we present a
symmetric rendezvous algorithm which achieves a constant competitive ratio for
the total distance traveled by the robots. We validate our theoretical results
through simulations.
</p>
<a href="http://arxiv.org/abs/2101.05324" target="_blank">arXiv:2101.05324</a> [<a href="http://arxiv.org/pdf/2101.05324" target="_blank">pdf</a>]

<h2>Quantitative Rates and Fundamental Obstructions to Non-Euclidean Universal Approximation with Deep Narrow Feed-Forward Networks. (arXiv:2101.05390v2 [cs.LG] UPDATED)</h2>
<h3>Anastasis Kratsios, Leonie Papon</h3>
<p>By incorporating structured pairs of non-trainable input and output layers,
the universal approximation property of feed-forward have recently been
extended across a broad range of non-Euclidean input spaces X and output spaces
Y. We quantify the number of narrow layers required for these "deep geometric
feed-forward neural networks" (DGNs) to approximate any continuous function in
$C(X,Y)$, uniformly on compacts. The DGN architecture is then extended to
accommodate complete Riemannian manifolds, where the input and output layers
are only defined locally, and we obtain local analogs of our results. In this
case, we find that both the global and local universal approximation guarantees
can only coincide when approximating null-homotopic functions. Consequently, we
show that if Y is a compact Riemannian manifold, then there exists a function
that cannot be uniformly approximated on large compact subsets of X.
Nevertheless, we obtain lower-bounds of the maximum diameter of any geodesic
ball in X wherein our local universal approximation results hold. Applying our
results, we build universal approximators between spaces of non-degenerate
Gaussian measures. We also obtain a quantitative version of the universal
approximation theorem for classical deep narrow feed-forward networks with
general activation functions.
</p>
<a href="http://arxiv.org/abs/2101.05390" target="_blank">arXiv:2101.05390</a> [<a href="http://arxiv.org/pdf/2101.05390" target="_blank">pdf</a>]

<h2>Neural Attention Distillation: Erasing Backdoor Triggers from Deep Neural Networks. (arXiv:2101.05930v2 [cs.LG] UPDATED)</h2>
<h3>Yige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, Xingjun Ma</h3>
<p>Deep neural networks (DNNs) are known vulnerable to backdoor attacks, a
training time attack that injects a trigger pattern into a small proportion of
training data so as to control the model's prediction at the test time.
Backdoor attacks are notably dangerous since they do not affect the model's
performance on clean examples, yet can fool the model to make incorrect
prediction whenever the trigger pattern appears during testing. In this paper,
we propose a novel defense framework Neural Attention Distillation (NAD) to
erase backdoor triggers from backdoored DNNs. NAD utilizes a teacher network to
guide the finetuning of the backdoored student network on a small clean subset
of data such that the intermediate-layer attention of the student network
aligns with that of the teacher network. The teacher network can be obtained by
an independent finetuning process on the same clean subset. We empirically
show, against 6 state-of-the-art backdoor attacks, NAD can effectively erase
the backdoor triggers using only 5\% clean training data without causing
obvious performance degradation on clean examples. Code is available in
https://github.com/bboylyg/NAD.
</p>
<a href="http://arxiv.org/abs/2101.05930" target="_blank">arXiv:2101.05930</a> [<a href="http://arxiv.org/pdf/2101.05930" target="_blank">pdf</a>]

<h2>Catching Out-of-Context Misinformation with Self-supervised Learning. (arXiv:2101.06278v2 [cs.CV] UPDATED)</h2>
<h3>Shivangi Aneja, Christoph Bregler, Matthias Nie&#xdf;ner</h3>
<p>Despite the recent attention to DeepFakes and other forms of image
manipulations, one of the most prevalent ways to mislead audiences is the use
of unaltered images in a new but false context. To address these challenges and
support fact-checkers, we propose a new method that automatically detects
out-of-context image and text pairs. Our core idea is a self-supervised
training strategy where we only need images with matching (and non-matching)
captions from different sources. At train time, our method learns to
selectively align individual objects in an image with textual claims, without
explicit supervision. At test time, we check for a given text pair if both
texts correspond to same object(s) in the image but semantically convey
different descriptions, which allows us to make fairly accurate out-of-context
predictions. Our method achieves 82% out-of-context detection accuracy. To
facilitate training our method, we created a large-scale dataset of 200K images
which we match with 450K textual captions from a variety of news websites,
blogs, and social media posts; i.e., for each image, we obtained several
captions.
</p>
<a href="http://arxiv.org/abs/2101.06278" target="_blank">arXiv:2101.06278</a> [<a href="http://arxiv.org/pdf/2101.06278" target="_blank">pdf</a>]

<h2>Predicting Hyperkalemia in the ICU and Evaluation of Generalizability and Interpretability. (arXiv:2101.06443v2 [cs.LG] UPDATED)</h2>
<h3>Gloria Hyunjung Kwak, Christina Chen, Lowell Ling, Erina Ghosh, Leo Anthony Celi, Pan Hui</h3>
<p>Hyperkalemia is a potentially life-threatening condition that can lead to
fatal arrhythmias. Early identification of high risk patients can inform
clinical care to mitigate the risk. While hyperkalemia is often a complication
of acute kidney injury (AKI), it also occurs in the absence of AKI. We
developed predictive models to identify intensive care unit (ICU) patients at
risk of developing hyperkalemia by using the Medical Information Mart for
Intensive Care (MIMIC) and the eICU Collaborative Research Database (eICU-CRD).
Our methodology focused on building multiple models, optimizing for
interpretability through model selection, and simulating various clinical
scenarios.

In order to determine if our models perform accurately on patients with and
without AKI, we evaluated the following clinical cases: (i) predicting
hyperkalemia after AKI within 14 days of ICU admission, (ii) predicting
hyperkalemia within 14 days of ICU admission regardless of AKI status, and
compared different lead times for (i) and (ii). Both clinical scenarios were
modeled using logistic regression (LR), random forest (RF), and XGBoost.

Using observations from the first day in the ICU, our models were able to
predict hyperkalemia with an AUC of (i) 0.79, 0.81, 0.81 and (ii) 0.81, 0.85,
0.85 for LR, RF, and XGBoost respectively. We found that 4 out of the top 5
features were consistent across the models. AKI stage was significant in the
models that included all patients with or without AKI, but not in the models
which only included patients with AKI. This suggests that while AKI is
important for hyperkalemia, the specific stage of AKI may not be as important.
Our findings require further investigation and confirmation.
</p>
<a href="http://arxiv.org/abs/2101.06443" target="_blank">arXiv:2101.06443</a> [<a href="http://arxiv.org/pdf/2101.06443" target="_blank">pdf</a>]

<h2>Intelligent Frame Selection as a Privacy-Friendlier Alternative to Face Recognition. (arXiv:2101.07529v2 [cs.CV] UPDATED)</h2>
<h3>Mattijs Baert, Sam Leroux, Pieter Simoens</h3>
<p>The widespread deployment of surveillance cameras for facial recognition
gives rise to many privacy concerns. This study proposes a privacy-friendly
alternative to large scale facial recognition. While there are multiple
techniques to preserve privacy, our work is based on the minimization principle
which implies minimizing the amount of collected personal data. Instead of
running facial recognition software on all video data, we propose to
automatically extract a high quality snapshot of each detected person without
revealing his or her identity. This snapshot is then encrypted and access is
only granted after legal authorization. We introduce a novel unsupervised face
image quality assessment method which is used to select the high quality
snapshots. For this, we train a variational autoencoder on high quality face
images from a publicly available dataset and use the reconstruction probability
as a metric to estimate the quality of each face crop. We experimentally
confirm that the reconstruction probability can be used as biometric quality
predictor. Unlike most previous studies, we do not rely on a manually defined
face quality metric as everything is learned from data. Our face quality
assessment method outperforms supervised, unsupervised and general image
quality assessment methods on the task of improving face verification
performance by rejecting low quality images. The effectiveness of the whole
system is validated qualitatively on still images and videos.
</p>
<a href="http://arxiv.org/abs/2101.07529" target="_blank">arXiv:2101.07529</a> [<a href="http://arxiv.org/pdf/2101.07529" target="_blank">pdf</a>]

<h2>Semantic Disentangling Generalized Zero-Shot Learning. (arXiv:2101.07978v2 [cs.CV] UPDATED)</h2>
<h3>Zhi Chen, Ruihong Qiu, Sen Wang, Zi Huang, Jingjing Li, Zheng Zhang</h3>
<p>Generalized Zero-Shot Learning (GZSL) aims to recognize images from both seen
and unseen categories. Most GZSL methods typically learn to synthesize CNN
visual features for the unseen classes by leveraging entire semantic
information, e.g., tags and attributes, and the visual features of the seen
classes. Within the visual features, we define two types of features that
semantic-consistent and semantic-unrelated to represent the characteristics of
images annotated in attributes and less informative features of images
respectively. Ideally, the semantic-unrelated information is impossible to
transfer by semantic-visual relationship from seen classes to unseen classes,
as the corresponding characteristics are not annotated in the semantic
information. Thus, the foundation of the visual feature synthesis is not always
solid as the features of the seen classes may involve semantic-unrelated
information that could interfere with the alignment between semantic and visual
modalities. To address this issue, in this paper, we propose a novel feature
disentangling approach based on an encoder-decoder architecture to factorize
visual features of images into these two latent feature spaces to extract
corresponding representations. Furthermore, a relation module is incorporated
into this architecture to learn semantic-visual relationship, whilst a total
correlation penalty is applied to encourage the disentanglement of two latent
representations. The proposed model aims to distill quality semantic-consistent
representations that capture intrinsic features of seen images, which are
further taken as the generation target for unseen classes. Extensive
experiments conducted on seven GZSL benchmark datasets have verified the
state-of-the-art performance of the proposal.
</p>
<a href="http://arxiv.org/abs/2101.07978" target="_blank">arXiv:2101.07978</a> [<a href="http://arxiv.org/pdf/2101.07978" target="_blank">pdf</a>]

<h2>Collaborative Teacher-Student Learning via Multiple Knowledge Transfer. (arXiv:2101.08471v2 [cs.LG] UPDATED)</h2>
<h3>Liyuan Sun, Jianping Gou, Baosheng Yu, Lan Du, Dacheng Tao</h3>
<p>Knowledge distillation (KD), as an efficient and effective model compression
technique, has been receiving considerable attention in deep learning. The key
to its success is to transfer knowledge from a large teacher network to a small
student one. However, most of the existing knowledge distillation methods
consider only one type of knowledge learned from either instance features or
instance relations via a specific distillation strategy in teacher-student
learning. There are few works that explore the idea of transferring different
types of knowledge with different distillation strategies in a unified
framework. Moreover, the frequently used offline distillation suffers from a
limited learning capacity due to the fixed teacher-student architecture. In
this paper we propose a collaborative teacher-student learning via multiple
knowledge transfer (CTSL-MKT) that prompts both self-learning and collaborative
learning. It allows multiple students learn knowledge from both individual
instances and instance relations in a collaborative way. While learning from
themselves with self-distillation, they can also guide each other via online
distillation. The experiments and ablation studies on four image datasets
demonstrate that the proposed CTSL-MKT significantly outperforms the
state-of-the-art KD methods.
</p>
<a href="http://arxiv.org/abs/2101.08471" target="_blank">arXiv:2101.08471</a> [<a href="http://arxiv.org/pdf/2101.08471" target="_blank">pdf</a>]

<h2>Characterizing signal propagation to close the performance gap in unnormalized ResNets. (arXiv:2101.08692v2 [cs.LG] UPDATED)</h2>
<h3>Andrew Brock, Soham De, Samuel L. Smith</h3>
<p>Batch Normalization is a key component in almost all state-of-the-art image
classifiers, but it also introduces practical challenges: it breaks the
independence between training examples within a batch, can incur compute and
memory overhead, and often results in unexpected bugs. Building on recent
theoretical analyses of deep ResNets at initialization, we propose a simple set
of analysis tools to characterize signal propagation on the forward pass, and
leverage these tools to design highly performant ResNets without activation
normalization layers. Crucial to our success is an adapted version of the
recently proposed Weight Standardization. Our analysis tools show how this
technique preserves the signal in networks with ReLU or Swish activation
functions by ensuring that the per-channel activation means do not grow with
depth. Across a range of FLOP budgets, our networks attain performance
competitive with the state-of-the-art EfficientNets on ImageNet.
</p>
<a href="http://arxiv.org/abs/2101.08692" target="_blank">arXiv:2101.08692</a> [<a href="http://arxiv.org/pdf/2101.08692" target="_blank">pdf</a>]

<h2>A Closer Look at Temporal Sentence Grounding in Videos: Datasets and Metrics. (arXiv:2101.09028v2 [cs.CV] UPDATED)</h2>
<h3>Yitian Yuan, Xiaohan Lan, Long Chen, Wei Liu, Xin Wang, Wenwu Zhu</h3>
<p>Despite Temporal Sentence Grounding in Videos (TSGV) has realized impressive
progress over the last few years, current TSGV models tend to capture the
moment annotation biases and fail to take full advantage of multi-modal inputs.
Miraculously, some extremely simple TSGV baselines even without training can
also achieve state-of-the-art performance. In this paper, we first take a
closer look at the existing evaluation protocol, and argue that both the
prevailing datasets and metrics are the devils to cause the unreliable
benchmarking. To this end, we propose to re-organize two widely-used TSGV
datasets (Charades-STA and ActivityNet Captions), and deliberately
\textbf{C}hange the moment annotation \textbf{D}istribution of the test split
to make it different from the training split, dubbed as Charades-CD and
ActivityNet-CD, respectively. Meanwhile, we further introduce a new evaluation
metric "dR@$n$,IoU@$m$" to calibrate the basic IoU scores by penalizing more on
the over-long moment predictions and reduce the inflating performance caused by
the moment annotation biases. Under this new evaluation protocol, we conduct
extensive experiments and ablation studies on eight state-of-the-art TSGV
models. All the results demonstrate that the re-organized datasets and new
metric can better monitor the progress in TSGV, which is still far from
satisfactory. The repository of this work is at
\url{https://github.com/yytzsy/grounding_changing_distribution}.
</p>
<a href="http://arxiv.org/abs/2101.09028" target="_blank">arXiv:2101.09028</a> [<a href="http://arxiv.org/pdf/2101.09028" target="_blank">pdf</a>]

<h2>Rethinking Domain Generalization Baselines. (arXiv:2101.09060v2 [cs.CV] UPDATED)</h2>
<h3>Francesco Cappio Borlino, Antonio D&#x27;Innocente, Tatiana Tommasi</h3>
<p>Despite being very powerful in standard learning settings, deep learning
models can be extremely brittle when deployed in scenarios different from those
on which they were trained. Domain generalization methods investigate this
problem and data augmentation strategies have shown to be helpful tools to
increase data variability, supporting model robustness across domains. In our
work we focus on style transfer data augmentation and we present how it can be
implemented with a simple and inexpensive strategy to improve generalization.
Moreover, we analyze the behavior of current state of the art domain
generalization methods when integrated with this augmentation solution: our
thorough experimental evaluation shows that their original effect almost always
disappears with respect to the augmented baseline. This issue open new
scenarios for domain generalization research, highlighting the need of novel
methods properly able to take advantage of the introduced data variability.
</p>
<a href="http://arxiv.org/abs/2101.09060" target="_blank">arXiv:2101.09060</a> [<a href="http://arxiv.org/pdf/2101.09060" target="_blank">pdf</a>]

<h2>Predicting Autism Spectrum Disorder Using Machine Learning Classifiers. (arXiv:2101.09279v2 [cs.LG] UPDATED)</h2>
<h3>Koushik Chowdhury, Mir Ahmad Iraj</h3>
<p>Autism Spectrum Disorder (ASD) is on the rise and constantly growing. Earlier
identify of ASD with the best outcome will allow someone to be safe and healthy
by proper nursing. Humans can hardly estimate the present condition and stage
of ASD by measuring primary symptoms. Therefore, it is being necessary to
develop a method that will provide the best outcome and measurement of ASD.
This paper aims to show several measurements that implemented in several
classifiers. Among them, Support Vector Machine (SVM) provides the best result
and under SVM, there are also some kernels to perform. Among them, the Gaussian
Radial Kernel gives the best result. The proposed classifier achieves 95%
accuracy using the publicly available standard ASD dataset.
</p>
<a href="http://arxiv.org/abs/2101.09279" target="_blank">arXiv:2101.09279</a> [<a href="http://arxiv.org/pdf/2101.09279" target="_blank">pdf</a>]

<h2>Vessel-CAPTCHA: an efficient learning framework for vessel annotation and segmentation. (arXiv:2101.09321v2 [cs.CV] UPDATED)</h2>
<h3>Vien Ngoc Dang, Giuseppe Di Giacomo, Viola Marconetto, Pratek Mathur, Rosa Cortese, Marco Lorenzi, Ferran Prados, Maria A. Zuluaga</h3>
<p>The use of deep learning techniques for 3D brain vessel image segmentation
has not been as widespread as for the segmentation of other organs and tissues.
This can be explained by two factors. First, deep learning techniques tend to
show poor performances at the segmentation of relatively small objects compared
to the size of the full image. Second, due to the complexity of vascular trees
and the small size of vessels, it is challenging to obtain the amount of
annotated training data typically needed by deep learning methods. To address
these problems, we propose a novel annotation-efficient deep learning vessel
segmentation framework. The framework avoids pixel-wise annotations, only
requiring patch-level labels to discriminate between vessel and non-vessel 2D
patches in the training set, in a setup similar to the CAPTCHAs used to
differentiate humans from bots in web applications. The user-provided
annotations are used for two tasks: 1) to automatically generate pixel-wise
labels for vessels and background in each patch, which are used to train a
segmentation network, and 2) to train a classifier network. The classifier
network allows to generate additional weak patch labels, further reducing the
annotation burden, and it acts as a noise filter for poor quality images. We
use this framework for the segmentation of the cerebrovascular tree in
Time-of-Flight angiography (TOF) and Susceptibility-Weighted Images (SWI). The
results show that the framework achieves state-of-the-art accuracy, while
reducing the annotation time by up to 80% with respect to learning-based
segmentation methods using pixel-wise labels for training
</p>
<a href="http://arxiv.org/abs/2101.09321" target="_blank">arXiv:2101.09321</a> [<a href="http://arxiv.org/pdf/2101.09321" target="_blank">pdf</a>]

<h2>BF++: a language for general-purpose neural program synthesis. (arXiv:2101.09571v2 [cs.AI] UPDATED)</h2>
<h3>Vadim Liventsev, Aki H&#xe4;rm&#xe4;, Milan Petkovi&#x107;</h3>
<p>Most state of the art decision systems based on Reinforcement Learning (RL)
are data-driven black-box neural models, where it is often difficult to
incorporate expert knowledge into the models or let experts review and validate
the learned decision mechanisms. Knowledge-insertion and model review are
important requirements in many applications involving human health and safety.
One way to bridge the gap between data- and knowledge-driven systems is program
synthesis: replacing a neural network that outputs decisions with one that
generates decision-making code in some programming language. We propose a new
programming language, BF++, designed specifically for neural program synthesis
in a Partially Observable Markov Decision Process (POMDP) setting and generate
programs for a number of standard OpenAI Gym benchmarks.
</p>
<a href="http://arxiv.org/abs/2101.09571" target="_blank">arXiv:2101.09571</a> [<a href="http://arxiv.org/pdf/2101.09571" target="_blank">pdf</a>]

<h2>Exploitation of Image Statistics with Sparse Coding in the Case of Stereo Vision. (arXiv:2101.09710v2 [cs.CV] UPDATED)</h2>
<h3>Gerrit A. Ecke, Harald M. Papp, Hanspeter A. Mallot</h3>
<p>The sparse coding algorithm has served as a model for early processing in
mammalian vision. It has been assumed that the brain uses sparse coding to
exploit statistical properties of the sensory stream. We hypothesize that
sparse coding discovers patterns from the data set, which can be used to
estimate a set of stimulus parameters by simple readout. In this study, we
chose a model of stereo vision to test our hypothesis. We used the Locally
Competitive Algorithm (LCA), followed by a na\"ive Bayes classifier, to infer
stereo disparity. From the results we report three observations. First,
disparity inference was successful with this naturalistic processing pipeline.
Second, an expanded, highly redundant representation is required to robustly
identify the input patterns. Third, the inference error can be predicted from
the number of active coefficients in the LCA representation. We conclude that
sparse coding can generate a suitable general representation for subsequent
inference tasks. Keywords: Sparse coding; Locally Competitive Algorithm (LCA);
Efficient coding; Compact code; Probabilistic inference; Stereo vision
</p>
<a href="http://arxiv.org/abs/2101.09710" target="_blank">arXiv:2101.09710</a> [<a href="http://arxiv.org/pdf/2101.09710" target="_blank">pdf</a>]

<h2>ADMM-based Adaptive Sampling Strategy for Nonholonomic Mobile Robotic Sensor Networks. (arXiv:2101.10500v2 [cs.RO] UPDATED)</h2>
<h3>Viet-Anh Le, Linh Nguyen, Truong X. Nghiem</h3>
<p>This paper discusses the adaptive sampling problem in a nonholonomic mobile
robotic sensor network for efficiently monitoring a spatial field. It is
proposed to employ Gaussian process to model a spatial phenomenon and predict
it at unmeasured positions, which enables the sampling optimization problem to
be formulated by the use of the log determinant of a predicted covariance
matrix at next sampling locations. The control, movement and nonholonomic
dynamics constraints of the mobile sensors are also considered in the adaptive
sampling optimization problem. In order to tackle the nonlinearity and
nonconvexity of the objective function in the optimization problem we first
exploit the linearized alternating direction method of multipliers (L-ADMM)
method that can effectively simplify the objective function, though it is
computationally expensive since a nonconvex problem needs to be solved exactly
in each iteration. We then propose a novel approach called the successive
convexified ADMM (SC-ADMM) that sequentially convexify the nonlinear dynamic
constraints so that the original optimization problem can be split into convex
subproblems. It is noted that both the L-ADMM algorithm and our SC-ADMM
approach can solve the sampling optimization problem in either a centralized or
a distributed manner. We validated the proposed approaches in 1000 experiments
in a synthetic environment with a real-world dataset, where the obtained
results suggest that both the L-ADMM and SC- ADMM techniques can provide good
accuracy for the monitoring purpose. However, our proposed SC-ADMM approach
computationally outperforms the L-ADMM counterpart, demonstrating its better
practicality.
</p>
<a href="http://arxiv.org/abs/2101.10500" target="_blank">arXiv:2101.10500</a> [<a href="http://arxiv.org/pdf/2101.10500" target="_blank">pdf</a>]

<h2>ResLT: Residual Learning for Long-tailed Recognition. (arXiv:2101.10633v2 [cs.CV] UPDATED)</h2>
<h3>Jiequan Cui, Shu Liu, Zhuotao Tian, Zhisheng Zhong, Jiaya Jia</h3>
<p>Deep learning algorithms face great challenges with long-tailed data
distribution which, however, is quite a common case in real-world scenarios.
Previous methods tackle the problem from either the aspect of input space
(re-sampling classes with different frequencies) or loss space (re-weighting
classes with different weights), suffering from heavy over-fitting to tail
classes or hard optimization during training. To alleviate these issues, we
propose a more fundamental perspective for long-tailed recognition, {i.e., from
the aspect of parameter space, and aims to preserve specific capacity for
classes with low frequencies. From this perspective, the trivial solution
utilizes different branches for the head, medium, tail classes respectively,
and then sums their outputs as the final results is not feasible. Instead, we
design the effective residual fusion mechanism -- with one main branch
optimized to recognize images from all classes, another two residual branches
are gradually fused and optimized to enhance images from medium+tail classes
and tail classes respectively. Then the branches are aggregated into final
results by additive shortcuts. We test our method on several benchmarks, {i.e.,
long-tailed version of CIFAR-10, CIFAR-100, Places, ImageNet, and iNaturalist
2018. Experimental results manifest that our method achieves new
state-of-the-art for long-tailed recognition. Code will be available at
\url{https://github.com/FPNAS/ResLT}.
</p>
<a href="http://arxiv.org/abs/2101.10633" target="_blank">arXiv:2101.10633</a> [<a href="http://arxiv.org/pdf/2101.10633" target="_blank">pdf</a>]

<h2>CPTR: Full Transformer Network for Image Captioning. (arXiv:2101.10804v2 [cs.CV] UPDATED)</h2>
<h3>Wei Liu, Sihan Chen, Longteng Guo, Xinxin Zhu, Jing Liu</h3>
<p>In this paper, we consider the image captioning task from a new
sequence-to-sequence prediction perspective and propose CaPtion TransformeR
(CPTR) which takes the sequentialized raw images as the input to Transformer.
Compared to the "CNN+Transformer" design paradigm, our model can model global
context at every encoder layer from the beginning and is totally
convolution-free. Extensive experiments demonstrate the effectiveness of the
proposed model and we surpass the conventional "CNN+Transformer" methods on the
MSCOCO dataset. Besides, we provide detailed visualizations of the
self-attention between patches in the encoder and the "words-to-patches"
attention in the decoder thanks to the full Transformer architecture.
</p>
<a href="http://arxiv.org/abs/2101.10804" target="_blank">arXiv:2101.10804</a> [<a href="http://arxiv.org/pdf/2101.10804" target="_blank">pdf</a>]

