---
title: Latest Deep Learning Papers
date: 2020-11-25 21:00:01 +0800
featured-img: DL-logo
categories: [Deep_Learning]
tags: [Deep_Learning]
mathjax: Yes
---

<h1>Your arXiv Feed (208 Articles)</h1>
<h2>Generalized Variational Continual Learning. (arXiv:2011.12328v1 [cs.LG])</h2>
<h3>Noel Loo, Siddharth Swaroop, Richard E. Turner</h3>
<p>Continual learning deals with training models on new tasks and datasets in an
online fashion. One strand of research has used probabilistic regularization
for continual learning, with two of the main approaches in this vein being
Online Elastic Weight Consolidation (Online EWC) and Variational Continual
Learning (VCL). VCL employs variational inference, which in other settings has
been improved empirically by applying likelihood-tempering. We show that
applying this modification to VCL recovers Online EWC as a limiting case,
allowing for interpolation between the two approaches. We term the general
algorithm Generalized VCL (GVCL). In order to mitigate the observed overpruning
effect of VI, we take inspiration from a common multi-task architecture, neural
networks with task-specific FiLM layers, and find that this addition leads to
significant performance gains, specifically for variational methods. In the
small-data regime, GVCL strongly outperforms existing baselines. In larger
datasets, GVCL with FiLM layers outperforms or is competitive with existing
baselines in terms of accuracy, whilst also providing significantly better
calibration.
</p>
<a href="http://arxiv.org/abs/2011.12328" target="_blank">arXiv:2011.12328</a> [<a href="http://arxiv.org/pdf/2011.12328" target="_blank">pdf</a>]

<h2>Zero-Shot Visual Slot Filling as Question Answering. (arXiv:2011.12340v1 [cs.AI])</h2>
<h3>Larry Heck, Simon Heck</h3>
<p>This paper presents a new approach to visual zero-shot slot filling. The
approach extends previous approaches by reformulating the slot filling task as
Question Answering. Slot tags are converted to rich natural language questions
that capture the semantics of visual information and lexical text on the GUI
screen. These questions are paired with the user's utterance and slots are
extracted from the utterance using a state-of-the-art ALBERT-based Question
Answering system trained on the Stanford Question Answering dataset (SQuaD2).
An approach to further refine the model with multi-task training is presented.
The multi-task approach facilitates the incorporation of a large number of
successive refinements and transfer learning across similar tasks. A new Visual
Slot dataset and a visual extension of the popular ATIS dataset is introduced
to support research and experimentation on visual slot filling. Results show F1
scores between 0.52 and 0.60 on the Visual Slot and ATIS datasets with no
training data (zero-shot).
</p>
<a href="http://arxiv.org/abs/2011.12340" target="_blank">arXiv:2011.12340</a> [<a href="http://arxiv.org/pdf/2011.12340" target="_blank">pdf</a>]

<h2>Trust but Verify: Assigning Prediction Credibility by Counterfactual Constrained Learning. (arXiv:2011.12344v1 [cs.LG])</h2>
<h3>Luiz F. O. Chamon, Santiago Paternain, Alejandro Ribeiro</h3>
<p>Prediction credibility measures, in the form of confidence intervals or
probability distributions, are fundamental in statistics and machine learning
to characterize model robustness, detect out-of-distribution samples
(outliers), and protect against adversarial attacks. To be effective, these
measures should (i) account for the wide variety of models used in practice,
(ii) be computable for trained models or at least avoid modifying established
training procedures, (iii) forgo the use of data, which can expose them to the
same robustness issues and attacks as the underlying model, and (iv) be
followed by theoretical guarantees. These principles underly the framework
developed in this work, which expresses the credibility as a risk-fit
trade-off, i.e., a compromise between how much can fit be improved by
perturbing the model input and the magnitude of this perturbation (risk). Using
a constrained optimization formulation and duality theory, we analyze this
compromise and show that this balance can be determined counterfactually,
without having to test multiple perturbations. This results in an unsupervised,
a posteriori method of assigning prediction credibility for any (possibly
non-convex) differentiable model, from RKHS-based solutions to any architecture
of (feedforward, convolutional, graph) neural network. Its use is illustrated
in data filtering and defense against adversarial attacks.
</p>
<a href="http://arxiv.org/abs/2011.12344" target="_blank">arXiv:2011.12344</a> [<a href="http://arxiv.org/pdf/2011.12344" target="_blank">pdf</a>]

<h2>Improving Clinical Outcome Predictions Using Convolution overMedical Entities with Multimodal Learning. (arXiv:2011.12349v1 [cs.LG])</h2>
<h3>Batuhan Bardak, Mehmet Tan</h3>
<p>Early prediction of mortality and length of stay(LOS) of a patient is vital
for saving a patient's life and management of hospital resources. Availability
of electronic health records(EHR) makes a huge impact on the healthcare domain
and there has seen several works on predicting clinical problems. However, many
studies did not benefit from the clinical notes because of the sparse, and high
dimensional nature. In this work, we extract medical entities from clinical
notes and use them as additional features besides time-series features to
improve our predictions. We propose a convolution based multimodal
architecture, which not only learns effectively combining medical entities and
time-series ICU signals of patients, but also allows us to compare the effect
of different embedding techniques such as Word2vec, FastText on medical
entities. In the experiments, our proposed method robustly outperforms all
other baseline models including different multimodal architectures for all
clinical tasks. The code for the proposed method is available at
https://github.com/tanlab/ConvolutionMedicalNer.
</p>
<a href="http://arxiv.org/abs/2011.12349" target="_blank">arXiv:2011.12349</a> [<a href="http://arxiv.org/pdf/2011.12349" target="_blank">pdf</a>]

<h2>Hindsight Network Credit Assignment. (arXiv:2011.12351v1 [cs.LG])</h2>
<h3>Kenny Young</h3>
<p>We present Hindsight Network Credit Assignment (HNCA), a novel learning
method for stochastic neural networks, which works by assigning credit to each
neuron's stochastic output based on how it influences the output of its
immediate children in the network. We prove that HNCA provides unbiased
gradient estimates while reducing variance compared to the REINFORCE estimator.
We also experimentally demonstrate the advantage of HNCA over REINFORCE in a
contextual bandit version of MNIST. The computational complexity of HNCA is
similar to that of backpropagation. We believe that HNCA can help stimulate new
ways of thinking about credit assignment in stochastic compute graphs.
</p>
<a href="http://arxiv.org/abs/2011.12351" target="_blank">arXiv:2011.12351</a> [<a href="http://arxiv.org/pdf/2011.12351" target="_blank">pdf</a>]

<h2>FireSRnet: Geoscience-Driven Super-Resolution of Future Fire Risk from Climate Change. (arXiv:2011.12353v1 [cs.LG])</h2>
<h3>Tristan Ballard, Gopal Erinjippurath</h3>
<p>With fires becoming increasingly frequent and severe across the globe in
recent years, understanding climate change's role in fire behavior is critical
for quantifying current and future fire risk. However, global climate models
typically simulate fire behavior at spatial scales too coarse for local risk
assessments. Therefore, we propose a novel approach towards super-resolution
(SR) enhancement of fire risk exposure maps that incorporates not only 2000 to
2020 monthly satellite observations of active fires but also local information
on land cover and temperature. Inspired by SR architectures, we propose an
efficient deep learning model trained for SR on fire risk exposure maps. We
evaluate this model on resolution enhancement and find it outperforms standard
image interpolation techniques at both 4x and 8x enhancement while having
comparable performance at 2x enhancement. We then demonstrate the
generalizability of this SR model over northern California and New South Wales,
Australia. We conclude with a discussion and application of our proposed model
to climate model simulations of fire risk in 2040 and 2100, illustrating the
potential for SR enhancement of fire risk maps from the latest state-of-the-art
climate models.
</p>
<a href="http://arxiv.org/abs/2011.12353" target="_blank">arXiv:2011.12353</a> [<a href="http://arxiv.org/pdf/2011.12353" target="_blank">pdf</a>]

<h2>A reinforcement learning control approach for underwater manipulation under position and torque constraints. (arXiv:2011.12360v1 [cs.RO])</h2>
<h3>Ignacio Carlucho, Mariano De Paula, Gerardo G. Acosta, Corina Barbalata</h3>
<p>In marine operations underwater manipulators play a primordial role. However,
due to uncertainties in the dynamic model and disturbances caused by the
environment, low-level control methods require great capabilities to adapt to
change. Furthermore, under position and torque constraints the requirements for
the control system are greatly increased. Reinforcement learning is a data
driven control technique that can learn complex control policies without the
need of a model. The learning capabilities of these type of agents allow for
great adaptability to changes in the operative conditions. In this article we
present a novel reinforcement learning low-level controller for the position
control of an underwater manipulator under torque and position constraints. The
reinforcement learning agent is based on an actor-critic architecture using
sensor readings as state information. Simulation results using the Reach Alpha
5 underwater manipulator show the advantages of the proposed control strategy.
</p>
<a href="http://arxiv.org/abs/2011.12360" target="_blank">arXiv:2011.12360</a> [<a href="http://arxiv.org/pdf/2011.12360" target="_blank">pdf</a>]

<h2>C-Learning: Horizon-Aware Cumulative Accessibility Estimation. (arXiv:2011.12363v1 [cs.LG])</h2>
<h3>Panteha Naderian, Gabriel Loaiza-Ganem, Harry J. Braviner, Anthony L. Caterini, Jesse C. Cresswell, Tong Li, Animesh Garg</h3>
<p>Multi-goal reaching is an important problem in reinforcement learning needed
to achieve algorithmic generalization. Despite recent advances in this field,
current algorithms suffer from three major challenges: high sample complexity,
learning only a single way of reaching the goals, and difficulties in solving
complex motion planning tasks. In order to address these limitations, we
introduce the concept of cumulative accessibility functions, which measure the
reachability of a goal from a given state within a specified horizon. We show
that these functions obey a recurrence relation, which enables learning from
offline interactions. We also prove that optimal cumulative accessibility
functions are monotonic in the planning horizon. Additionally, our method can
trade off speed and reliability in goal-reaching by suggesting multiple paths
to a single goal depending on the provided horizon. We evaluate our approach on
a set of multi-goal discrete and continuous control tasks. We show that our
method outperforms state-of-the-art goal-reaching algorithms in success rate,
sample complexity, and path optimality. Our code is available at
https://github.com/layer6ai-labs/CAE, and additional visualizations can be
found at https://sites.google.com/view/learning-cae/ .
</p>
<a href="http://arxiv.org/abs/2011.12363" target="_blank">arXiv:2011.12363</a> [<a href="http://arxiv.org/pdf/2011.12363" target="_blank">pdf</a>]

<h2>Play Fair: Frame Attributions in Video Models. (arXiv:2011.12372v1 [cs.CV])</h2>
<h3>Will Price, Dima Damen</h3>
<p>In this paper, we introduce an attribution method for explaining action
recognition models. Such models fuse information from multiple frames within a
video, through score aggregation or relational reasoning. We break down a
model's class score into the sum of contributions from each frame, fairly. Our
method adapts an axiomatic solution to fair reward distribution in cooperative
games, known as the Shapley value, for elements in a variable-length sequence,
which we call the Element Shapley Value (ESV). Critically, we propose a
tractable approximation of ESV that scales linearly with the number of frames
in the sequence. We employ ESV to explain two action recognition models (TRN
and TSN) on the fine-grained dataset Something-Something. We offer detailed
analysis of supporting/distracting frames, and the relationships of ESVs to the
frame's position, class prediction, and sequence length. We compare ESV to
naive baselines and two commonly used feature attribution methods: Grad-CAM and
Integrated-Gradients.
</p>
<a href="http://arxiv.org/abs/2011.12372" target="_blank">arXiv:2011.12372</a> [<a href="http://arxiv.org/pdf/2011.12372" target="_blank">pdf</a>]

<h2>A Non-linear Function-on-Function Model for Regression with Time Series Data. (arXiv:2011.12378v1 [cs.LG])</h2>
<h3>Qiyao Wang, Haiyan Wang, Chetan Gupta, Aniruddha Rajendra Rao, Hamed Khorasgani</h3>
<p>In the last few decades, building regression models for non-scalar variables,
including time series, text, image, and video, has attracted increasing
interests of researchers from the data analytic community. In this paper, we
focus on a multivariate time series regression problem. Specifically, we aim to
learn mathematical mappings from multiple chronologically measured numerical
variables within a certain time interval S to multiple numerical variables of
interest over time interval T. Prior arts, including the multivariate
regression model, the Seq2Seq model, and the functional linear models, suffer
from several limitations. The first two types of models can only handle
regularly observed time series. Besides, the conventional multivariate
regression models tend to be biased and inefficient, as they are incapable of
encoding the temporal dependencies among observations from the same time
series. The sequential learning models explicitly use the same set of
parameters along time, which has negative impacts on accuracy. The
function-on-function linear model in functional data analysis (a branch of
statistics) is insufficient to capture complex correlations among the
considered time series and suffer from underfitting easily. In this paper, we
propose a general functional mapping that embraces the function-on-function
linear model as a special case. We then propose a non-linear
function-on-function model using the fully connected neural network to learn
the mapping from data, which addresses the aforementioned concerns in the
existing approaches. For the proposed model, we describe in detail the
corresponding numerical implementation procedures. The effectiveness of the
proposed model is demonstrated through the application to two real-world
problems.
</p>
<a href="http://arxiv.org/abs/2011.12378" target="_blank">arXiv:2011.12378</a> [<a href="http://arxiv.org/pdf/2011.12378" target="_blank">pdf</a>]

<h2>Invariant Representation Learning for Treatment Effect Estimation. (arXiv:2011.12379v1 [cs.LG])</h2>
<h3>Claudia Shi, Victor Veitch, David Blei</h3>
<p>The defining challenge for causal inference from observational data is the
presence of `confounders', covariates that affect both treatment assignment and
the outcome. To address this challenge, practitioners collect and adjust for
the covariates, hoping that they adequately correct for confounding. However,
including every observed covariate in the adjustment runs the risk of including
`bad controls', variables that \emph{induce} bias when they are conditioned on.
The problem is that we do not always know which variables in the covariate set
are safe to adjust for and which are not. To address this problem, we develop
Nearly Invariant Causal Estimation (NICE). NICE uses invariant risk
minimization (IRM) [Arj19] to learn a representation of the covariates that,
under some assumptions, strips out bad controls but preserves sufficient
information to adjust for confounding. Adjusting for the learned
representation, rather than the covariates themselves, avoids the induced bias
and provides valid causal inferences. NICE is appropriate in the following
setting. i) We observe data from multiple environments that share a common
causal mechanism for the outcome, but that differ in other ways. ii) In each
environment, the collected covariates are a superset of the causal parents of
the outcome, and contain sufficient information for causal identification. iii)
But the covariates also may contain bad controls, and it is unknown which
covariates are safe to adjust for and which ones induce bias. We evaluate NICE
on both synthetic and semi-synthetic data. When the covariates contain unknown
collider variables and other bad controls, NICE performs better than existing
methods that adjust for all the covariates.
</p>
<a href="http://arxiv.org/abs/2011.12379" target="_blank">arXiv:2011.12379</a> [<a href="http://arxiv.org/pdf/2011.12379" target="_blank">pdf</a>]

<h2>A3D: Adaptive 3D Networks for Video Action Recognition. (arXiv:2011.12384v1 [cs.CV])</h2>
<h3>Sijie Zhu, Taojiannan Yang, Matias Mendieta, Chen Chen</h3>
<p>This paper presents A3D, an adaptive 3D network that can infer at a wide
range of computational constraints with one-time training. Instead of training
multiple models in a grid-search manner, it generates good configurations by
trading off between network width and spatio-temporal resolution. Furthermore,
the computation cost can be adapted after the model is deployed to meet
variable constraints, for example, on edge devices. Even under the same
computational constraints, the performance of our adaptive networks can be
significantly boosted over the baseline counterparts by the mutual training
along three dimensions. When a multiple pathway framework, e.g. SlowFast, is
adopted, our adaptive method encourages a better trade-off between pathways
than manual designs. Extensive experiments on the Kinetics dataset show the
effectiveness of the proposed framework. The performance gain is also verified
to transfer well between datasets and tasks. Code will be made available.
</p>
<a href="http://arxiv.org/abs/2011.12384" target="_blank">arXiv:2011.12384</a> [<a href="http://arxiv.org/pdf/2011.12384" target="_blank">pdf</a>]

<h2>Geom-SPIDER-EM: Faster Variance Reduced Stochastic Expectation Maximization for Nonconvex Finite-Sum Optimization. (arXiv:2011.12392v1 [stat.ML])</h2>
<h3>Gersende Fort (IMT), Eric Moulines (X-DEP-MATHAPP), Hoi-To Wai</h3>
<p>The Expectation Maximization (EM) algorithm is a key reference for inference
in latent variable models; unfortunately, its computational cost is prohibitive
in the large scale learning setting. In this paper, we propose an extension of
the Stochastic Path-Integrated Differential EstimatoR EM (SPIDER-EM) and derive
complexity bounds for this novel algorithm, designed to solve smooth nonconvex
finite-sum optimization problems. We show that it reaches the same state of the
art complexity bounds as SPIDER-EM; and provide conditions for a linear rate of
convergence. Numerical results support our findings.
</p>
<a href="http://arxiv.org/abs/2011.12392" target="_blank">arXiv:2011.12392</a> [<a href="http://arxiv.org/pdf/2011.12392" target="_blank">pdf</a>]

<h2>Prediction-Based Reachability for Collision Avoidance in Autonomous Driving. (arXiv:2011.12406v1 [cs.RO])</h2>
<h3>Anjian Li, Liting Sun, Wei Zhan, Masayoshi Tomizuka, Mo Chen</h3>
<p>Safety is an important topic in autonomous driving since any collision may
cause serious damage to people and the environment. Hamilton-Jacobi (HJ)
Reachability is a formal method that verifies safety in multi-agent interaction
and provides a safety controller for collision avoidance. However, due to the
worst-case assumption on the car's future actions, reachability might result in
too much conservatism such that the normal operation of the vehicle is largely
hindered. In this paper, we leverage the power of trajectory prediction, and
propose a prediction-based reachability framework for the safety controller.
Instead of always assuming for the worst-case, we first cluster the car's
behaviors into multiple driving modes, e.g. left turn or right turn. Under each
mode, a reachability-based safety controller is designed based on a less
conservative action set. For online purpose, we first utilize the trajectory
prediction and our proposed mode classifier to predict the possible modes, and
then deploy the corresponding safety controller. Through simulations in a
T-intersection and an 8-way roundabout, we demonstrate that our
prediction-based reachability method largely avoids collision between two
interacting cars and reduces the conservatism that the safety controller brings
to the car's original operations.
</p>
<a href="http://arxiv.org/abs/2011.12406" target="_blank">arXiv:2011.12406</a> [<a href="http://arxiv.org/pdf/2011.12406" target="_blank">pdf</a>]

<h2>Online Domain Adaptation for Continuous Cross-Subject Liver Viability Evaluation Based on Irregular Thermal Data. (arXiv:2011.12408v1 [cs.CV])</h2>
<h3>Sahand Hajifar, Hongyue Sun</h3>
<p>Accurate evaluation of liver viability during its procurement is a
challenging issue and has traditionally been addressed by taking invasive
biopsy on liver. Recently, people have started to investigate on the
non-invasive evaluation of liver viability during its procurement using the
liver surface thermal images. However, existing works include the background
noise in the thermal images and do not consider the cross-subject heterogeneity
of livers, thus the viability evaluation accuracy can be affected. In this
paper, we propose to use the irregular thermal data of the pure liver region,
and the cross-subject liver evaluation information (i.e., the available
viability label information in cross-subject livers), for the real-time
evaluation of a new liver's viability. To achieve this objective, we extract
features of irregular thermal data based on tools from graph signal processing
(GSP), and propose an online domain adaptation (DA) and classification
framework using the GSP features of cross-subject livers. A multiconvex block
coordinate descent based algorithm is designed to jointly learn the
domain-invariant features during online DA and learn the classifier. Our
proposed framework is applied to the liver procurement data, and classifies the
liver viability accurately.
</p>
<a href="http://arxiv.org/abs/2011.12408" target="_blank">arXiv:2011.12408</a> [<a href="http://arxiv.org/pdf/2011.12408" target="_blank">pdf</a>]

<h2>Wide-band butterfly network: stable and efficient inversion via multi-frequency neural networks. (arXiv:2011.12413v1 [cs.LG])</h2>
<h3>Matthew Li, Laurent Demanet, Leonardo Zepeda-N&#xfa;&#xf1;ez</h3>
<p>We introduce an end-to-end deep learning architecture called the wide-band
butterfly network (WideBNet) for approximating the inverse scattering map from
wide-band scattering data. This architecture incorporates tools from
computational harmonic analysis, such as the butterfly factorization, and
traditional multi-scale methods, such as the Cooley-Tukey FFT algorithm, to
drastically reduce the number of trainable parameters to match the inherent
complexity of the problem. As a result WideBNet is efficient: it requires fewer
training points than off-the-shelf architectures, and has stable training
dynamics, thus it can rely on standard weight initialization strategies. The
architecture automatically adapts to the dimensions of the data with only a few
hyper-parameters that the user must specify. WideBNet is able to produce images
that are competitive with optimization-based approaches, but at a fraction of
the cost, and we also demonstrate numerically that it learns to super-resolve
scatterers in the full aperture scattering setup.
</p>
<a href="http://arxiv.org/abs/2011.12413" target="_blank">arXiv:2011.12413</a> [<a href="http://arxiv.org/pdf/2011.12413" target="_blank">pdf</a>]

<h2>Bi-directional Domain Adaptation for Sim2Real Transfer of Embodied Navigation Agents. (arXiv:2011.12421v1 [cs.RO])</h2>
<h3>Joanne Truong, Sonia Chernova, Dhruv Batra</h3>
<p>Deep reinforcement learning models are notoriously data hungry, yet
real-world data is expensive and time consuming to obtain. The solution that
many have turned to is to use simulation for training before deploying the
robot in a real environment. Simulation offers the ability to train large
numbers of robots in parallel, and offers an abundance of data. However, no
simulation is perfect, and robots trained solely in simulation fail to
generalize to the real-world, resulting in a "sim-vs-real gap". How can we
overcome the trade-off between the abundance of less accurate, artificial data
from simulators and the scarcity of reliable, real-world data? In this paper,
we propose Bi-directional Domain Adaptation (BDA), a novel approach to bridge
the sim-vs-real gap in both directions -- real2sim to bridge the visual domain
gap, and sim2real to bridge the dynamics domain gap. We demonstrate the
benefits of BDA on the task of PointGoal Navigation. BDA with only 5k
real-world (state, action, next-state) samples matches the performance of a
policy fine-tuned with ~600k samples, resulting in a speed-up of ~120x.
</p>
<a href="http://arxiv.org/abs/2011.12421" target="_blank">arXiv:2011.12421</a> [<a href="http://arxiv.org/pdf/2011.12421" target="_blank">pdf</a>]

<h2>Stochastic sparse adversarial attacks. (arXiv:2011.12423v1 [cs.LG])</h2>
<h3>Hatem Hajri, Manon C&#xe9;saire, Th&#xe9;o Combey, Sylvain Lamprier, Patrick Gallinari</h3>
<p>Adversarial attacks of neural network classifiers (NNC) and the use of random
noises in these methods have stimulated a large number of works in recent
years. However, despite all the previous investigations, existing approaches
that rely on random noises to fool NNC have fallen far short of
the-state-of-the-art adversarial methods performances. In this paper, we fill
this gap by introducing stochastic sparse adversarial attacks (SSAA), standing
as simple, fast and purely noise-based targeted and untargeted attacks of NNC.
SSAA offer new examples of sparse (or $L_0$) attacks for which only few methods
have been proposed previously. These attacks are devised by exploiting a
small-time expansion idea widely used for Markov processes. Experiments on
small and large datasets (CIFAR-10 and ImageNet) illustrate several advantages
of SSAA in comparison with the-state-of-the-art methods. For instance, in the
untargeted case, our method called voting folded Gaussian attack (VFGA) scales
efficiently to ImageNet and achieves a significantly lower $L_0$ score than
SparseFool (up to $\frac{1}{14}$ lower) while being faster. In the targeted
setting, VFGA achives appealing results on ImageNet and is significantly much
faster than Carlini-Wagner $L_0$ attack.
</p>
<a href="http://arxiv.org/abs/2011.12423" target="_blank">arXiv:2011.12423</a> [<a href="http://arxiv.org/pdf/2011.12423" target="_blank">pdf</a>]

<h2>Interpretable Models in ANNs. (arXiv:2011.12424v1 [cs.LG])</h2>
<h3>Yang Li</h3>
<p>Artificial neural networks are often very complex and too deep for a human to
understand. As a result, they are usually referred to as black boxes. For a lot
of real-world problems, the underlying pattern itself is very complicated, such
that an analytic solution does not exist. However, in some cases, laws of
physics, for example, the pattern can be described by relatively simple
mathematical expressions. In that case, we want to get a readable equation
rather than a black box. In this paper, we try to find a way to explain a
network and extract a human-readable equation that describes the model.
</p>
<a href="http://arxiv.org/abs/2011.12424" target="_blank">arXiv:2011.12424</a> [<a href="http://arxiv.org/pdf/2011.12424" target="_blank">pdf</a>]

<h2>UFPR-Periocular: A Periocular Dataset Collected by Mobile Devices in Unconstrained Scenarios. (arXiv:2011.12427v1 [cs.CV])</h2>
<h3>Luiz A. Zanlorensi, Rayson Laroca, Diego R. Lucio, Lucas R. Santos, Alceu S. Britto Jr., David Menotti</h3>
<p>Recently, ocular biometrics in unconstrained environments using images
obtained at visible wavelength have gained the researchers' attention,
especially with images captured by mobile devices. Periocular recognition has
been demonstrated to be an alternative when the iris trait is not available due
to occlusions or low image resolution. However, the periocular trait does not
have the high uniqueness presented in the iris trait. Thus, the use of datasets
containing many subjects is essential to assess biometric systems' capacity to
extract discriminating information from the periocular region. Also, to address
the within-class variability caused by lighting and attributes in the
periocular region, it is of paramount importance to use datasets with images of
the same subject captured in distinct sessions. As the datasets available in
the literature do not present all these factors, in this work, we present a new
periocular dataset containing samples from 1,122 subjects, acquired in 3
sessions by 196 different mobile devices. The images were captured under
unconstrained environments with just a single instruction to the participants:
to place their eyes on a region of interest. We also performed an extensive
benchmark with several Convolutional Neural Network (CNN) architectures and
models that have been employed in state-of-the-art approaches based on
Multi-class Classification, Multitask Learning, Pairwise Filters Network, and
Siamese Network. The results achieved in the closed- and open-world protocol,
considering the identification and verification tasks, show that this area
still needs research and development.
</p>
<a href="http://arxiv.org/abs/2011.12427" target="_blank">arXiv:2011.12427</a> [<a href="http://arxiv.org/pdf/2011.12427" target="_blank">pdf</a>]

<h2>The dynamics of learning with feedback alignment. (arXiv:2011.12428v1 [stat.ML])</h2>
<h3>Maria Refinetti, St&#xe9;phane d&#x27;Ascoli, Ruben Ohana, Sebastian Goldt</h3>
<p>Direct Feedback Alignment (DFA) is emerging as an efficient and biologically
plausible alternative to the ubiquitous backpropagation algorithm for training
deep neural networks. Despite relying on random feedback weights for the
backward pass, DFA successfully trains state-of-the-art models such as
Transformers. On the other hand, it notoriously fails to train convolutional
networks. An understanding of the inner workings of DFA to explain these
diverging results remains elusive. Here, we propose a theory for the success of
DFA. We first show that learning in shallow networks proceeds in two steps: an
alignment phase, where the model adapts its weights to align the approximate
gradient with the true gradient of the loss function, is followed by a
memorisation phase, where the model focuses on fitting the data. This two-step
process has a degeneracy breaking effect: out of all the low-loss solutions in
the landscape, a network trained with DFA naturally converges to the solution
which maximises gradient alignment. We also identify a key quantity underlying
alignment in deep linear networks: the conditioning of the alignment matrices.
The latter enables a detailed understanding of the impact of data structure on
alignment, and suggests a simple explanation for the well-known failure of DFA
to train convolutional neural networks. Numerical experiments on MNIST and
CIFAR10 clearly demonstrate degeneracy breaking in deep non-linear networks and
show that the align-then-memorize process occurs sequentially from the bottom
layers of the network to the top.
</p>
<a href="http://arxiv.org/abs/2011.12428" target="_blank">arXiv:2011.12428</a> [<a href="http://arxiv.org/pdf/2011.12428" target="_blank">pdf</a>]

<h2>SOE-Net: A Self-Attention and Orientation Encoding Network for Point Cloud based Place Recognition. (arXiv:2011.12430v1 [cs.CV])</h2>
<h3>Yan Xia, Yusheng Xu, Shuang Li, Rui Wang, Juan Du, Daniel Cremers, Uwe Stilla</h3>
<p>We tackle the problem of place recognition from point cloud data and
introduce a self-attention and orientation encoding network (SOE-Net) that
fully explores the relationship between points and incorporates long-range
context into point-wise local descriptors. Local information of each point from
eight orientations is captured in a PointOE module, whereas long-range feature
dependencies among local descriptors are captured with a self-attention unit.
Moreover, we propose a novel loss function called Hard Positive Hard Negative
quadruplet loss (HPHN quadruplet), that achieves better performance than the
commonly used metric learning loss. Experiments on various benchmark datasets
demonstrate promising performance of the proposed network. It significantly
outperforms the current state-of-the-art approaches - the average recall at top
1 retrieval on the Oxford RobotCar dataset is improved by over 16%. Codes and
the trained model will be made publicly available.
</p>
<a href="http://arxiv.org/abs/2011.12430" target="_blank">arXiv:2011.12430</a> [<a href="http://arxiv.org/pdf/2011.12430" target="_blank">pdf</a>]

<h2>Continuous Surface Embeddings. (arXiv:2011.12438v1 [cs.CV])</h2>
<h3>Natalia Neverova, David Novotny, Vasil Khalidov, Marc Szafraniec, Patrick Labatut, Andrea Vedaldi</h3>
<p>In this work, we focus on the task of learning and representing dense
correspondences in deformable object categories. While this problem has been
considered before, solutions so far have been rather ad-hoc for specific object
types (i.e., humans), often with significant manual work involved. However,
scaling the geometry understanding to all objects in nature requires more
automated approaches that can also express correspondences between related, but
geometrically different objects. To this end, we propose a new, learnable
image-based representation of dense correspondences. Our model predicts, for
each pixel in a 2D image, an embedding vector of the corresponding vertex in
the object mesh, therefore establishing dense correspondences between image
pixels and 3D object geometry. We demonstrate that the proposed approach
performs on par or better than the state-of-the-art methods for dense pose
estimation for humans, while being conceptually simpler. We also collect a new
in-the-wild dataset of dense correspondences for animal classes and demonstrate
that our framework scales naturally to the new deformable object categories.
</p>
<a href="http://arxiv.org/abs/2011.12438" target="_blank">arXiv:2011.12438</a> [<a href="http://arxiv.org/pdf/2011.12438" target="_blank">pdf</a>]

<h2>Contract Scheduling With Predictions. (arXiv:2011.12439v1 [cs.AI])</h2>
<h3>Spyros Angelopoulos, Shahin Kamali</h3>
<p>Contract scheduling is a general technique that allows to design a system
with interruptible capabilities, given an algorithm that is not necessarily
interruptible. Previous work on this topic has largely assumed that the
interruption is a worst-case deadline that is unknown to the scheduler. In this
work, we study the setting in which there is a potentially erroneous prediction
concerning the interruption. Specifically, we consider the setting in which the
prediction describes the time that the interruption occurs, as well as the
setting in which the prediction is obtained as a response to a single or
multiple binary queries. For both settings, we investigate tradeoffs between
the robustness (i.e., the worst-case performance assuming adversarial
prediction) and the consistency (i.e, the performance assuming that the
prediction is error-free), both from the side of positive and negative results.
</p>
<a href="http://arxiv.org/abs/2011.12439" target="_blank">arXiv:2011.12439</a> [<a href="http://arxiv.org/pdf/2011.12439" target="_blank">pdf</a>]

<h2>Building 3D Morphable Models from a Single Scan. (arXiv:2011.12440v1 [cs.CV])</h2>
<h3>Skylar Sutherland, Bernhard Egger, Joshua Tenenbaum</h3>
<p>We propose a method for constructing generative models of 3D objects from a
single 3D mesh. Our method produces a 3D morphable model that represents shape
and albedo in terms of Gaussian processes. We define the shape deformations in
physical (3D) space and the albedo deformations as a combination of
physical-space and color-space deformations. Whereas previous approaches have
typically built 3D morphable models from multiple high-quality 3D scans through
principal component analysis, we build 3D morphable models from a single scan
or template. We demonstrate the utility of these models in the domain of face
modeling through inverse rendering and registration tasks. Specifically, we
show that our approach can be used to perform face recognition using only a
single 3D scan (one scan total, not one per person), and further demonstrate
how multiple scans can be incorporated to improve performance without requiring
dense correspondence. Our approach enables the synthesis of 3D morphable models
for 3D object categories where dense correspondence between multiple scans is
unavailable. We demonstrate this by constructing additional 3D morphable models
for fish and birds and use them to perform simple inverse rendering tasks.
</p>
<a href="http://arxiv.org/abs/2011.12440" target="_blank">arXiv:2011.12440</a> [<a href="http://arxiv.org/pdf/2011.12440" target="_blank">pdf</a>]

<h2>The Human Effect Requires Affect: Addressing Social-Psychological Factors of Climate Change with Machine Learning. (arXiv:2011.12443v1 [cs.AI])</h2>
<h3>Kyle Tilbury, Jesse Hoey</h3>
<p>Machine learning has the potential to aid in mitigating the human effects of
climate change. Previous applications of machine learning to tackle the human
effects in climate change include approaches like informing individuals of
their carbon footprint and strategies to reduce it. For these methods to be the
most effective they must consider relevant social-psychological factors for
each individual. Of social-psychological factors at play in climate change,
affect has been previously identified as a key element in perceptions and
willingness to engage in mitigative behaviours. In this work, we propose an
investigation into how affect could be incorporated to enhance machine learning
based interventions for climate change. We propose using affective agent-based
modelling for climate change as well as the use of a simulated climate change
social dilemma to explore the potential benefits of affective machine learning
interventions. Behavioural and informational interventions can be a powerful
tool in helping humans adopt mitigative behaviours. We expect that utilizing
affective ML can make interventions an even more powerful tool and help
mitigative behaviours become widely adopted.
</p>
<a href="http://arxiv.org/abs/2011.12443" target="_blank">arXiv:2011.12443</a> [<a href="http://arxiv.org/pdf/2011.12443" target="_blank">pdf</a>]

<h2>Sparse R-CNN: End-to-End Object Detection with Learnable Proposals. (arXiv:2011.12450v1 [cs.CV])</h2>
<h3>Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chenfeng Xu, Wei Zhan, Masayoshi Tomizuka, Lei Li, Zehuan Yuan, Changhu Wang, Ping Luo</h3>
<p>We present Sparse R-CNN, a purely sparse method for object detection in
images. Existing works on object detection heavily rely on dense object
candidates, such as $k$ anchor boxes pre-defined on all grids of image feature
map of size $H\times W$. In our method, however, a fixed sparse set of learned
object proposals, total length of $N$, are provided to object recognition head
to perform classification and location. By eliminating $HWk$ (up to hundreds of
thousands) hand-designed object candidates to $N$ (e.g. 100) learnable
proposals, Sparse R-CNN completely avoids all efforts related to object
candidates design and many-to-one label assignment. More importantly, final
predictions are directly output without non-maximum suppression post-procedure.
Sparse R-CNN demonstrates accuracy, run-time and training convergence
performance on par with the well-established detector baselines on the
challenging COCO dataset, e.g., achieving 44.5 AP in standard $3\times$
training schedule and running at 22 fps using ResNet-50 FPN model. We hope our
work could inspire re-thinking the convention of dense prior in object
detectors. The code is available at: https://github.com/PeizeSun/SparseR-CNN.
</p>
<a href="http://arxiv.org/abs/2011.12450" target="_blank">arXiv:2011.12450</a> [<a href="http://arxiv.org/pdf/2011.12450" target="_blank">pdf</a>]

<h2>Supercharging Imbalanced Data Learning With Causal Representation Transfer. (arXiv:2011.12454v1 [cs.CV])</h2>
<h3>Junya Chen, Zidi Xiu, Benjamin Goldstein, Ricardo Henao, Lawrence Carin, Chenyang Tao</h3>
<p>Dealing with severe class imbalance poses a major challenge for real-world
applications, especially when the accurate classification and generalization of
minority classes is of primary interest. In computer vision, learning from long
tailed datasets is a recurring theme, especially for natural image datasets.
While existing solutions mostly appeal to sampling or weighting adjustments to
alleviate the pathological imbalance, or imposing inductive bias to prioritize
non-spurious associations, we take novel perspectives to promote sample
efficiency and model generalization based on the invariance principles of
causality. Our proposal posits a meta-distributional scenario, where the data
generating mechanism is invariant across the label-conditional feature
distributions. Such causal assumption enables efficient knowledge transfer from
the dominant classes to their under-represented counterparts, even if the
respective feature distributions show apparent disparities. This allows us to
leverage a causal data inflation procedure to enlarge the representation of
minority classes. Our development is orthogonal to the existing extreme
classification techniques thus can be seamlessly integrated. The utility of our
proposal is validated with an extensive set of synthetic and real-world
computer vision tasks against SOTA solutions.
</p>
<a href="http://arxiv.org/abs/2011.12454" target="_blank">arXiv:2011.12454</a> [<a href="http://arxiv.org/pdf/2011.12454" target="_blank">pdf</a>]

<h2>Redundancy Resolution and Disturbance Rejection via Torque Optimization in Hybrid Cable-Driven Robots. (arXiv:2011.12457v1 [cs.RO])</h2>
<h3>Ronghuai Qi, Amir Khajepour, William W. Melek</h3>
<p>This paper presents redundancy resolution and disturbance rejection via
torque optimization in Hybrid Cable-Driven Robots (HCDRs). To begin with, we
initiate a redundant HCDR for nonlinear whole-body system modeling and model
reduction. Based on the reduced dynamic model, two new methods are proposed to
solve the redundancy resolution problem: joint-space torque optimization for
actuated joints (TOAJ) and joint-space torque optimization for actuated and
unactuated joints (TOAUJ), and they can be extended to other HCDRs. Compared to
the existing approaches, this paper provides the first solution (TOAUJ-based
method) for HCDRs that can solve the redundancy resolution problem as well as
disturbance rejection. Additionally, this paper develops detailed algorithms
targeting TOAJ and TOAUJ implementation. A simple yet effective controller is
designed for generated data analysis and validation. Case studies are conducted
to evaluate the performance of TOAJ and TOAUJ, and the results suggest the
effectiveness of the aforementioned approaches.
</p>
<a href="http://arxiv.org/abs/2011.12457" target="_blank">arXiv:2011.12457</a> [<a href="http://arxiv.org/pdf/2011.12457" target="_blank">pdf</a>]

<h2>Experiments in Autonomous Driving Through Imitation Learning. (arXiv:2011.12460v1 [cs.RO])</h2>
<h3>Michael Muratov, Abdulwasay Mehar, Wan Song Lee, Michael Szpakowicz, Ose Edmond Umolu, Joshua Mazariegos Bobadilla, Ali Kuwajerwala</h3>
<p>This report demonstrates several methods used to make a self-driving vehicle
using a supervised learning algorithm and a forward-facing RGBD camera. The
project originally involved research in creating an adversarial attack on the
vehicle's model, but due to difficulties with the initial training of the car,
the plans were discarded in favor of completing the imitation learning portion
of the project. Many approaches were explored, but due to challenges introduced
by an unbalanced data set, the approaches had limited effectiveness.
</p>
<a href="http://arxiv.org/abs/2011.12460" target="_blank">arXiv:2011.12460</a> [<a href="http://arxiv.org/pdf/2011.12460" target="_blank">pdf</a>]

<h2>Toward Multiple Federated Learning Services Resource Sharing in Mobile Edge Networks. (arXiv:2011.12469v1 [cs.LG])</h2>
<h3>Minh N. H. Nguyen, Nguyen H. Tran, Yan Kyaw Tun, Zhu Han, Choong Seon Hong</h3>
<p>Federated Learning is a new learning scheme for collaborative training a
shared prediction model while keeping data locally on participating devices. In
this paper, we study a new model of multiple federated learning services at the
multi-access edge computing server. Accordingly, the sharing of CPU resources
among learning services at each mobile device for the local training process
and allocating communication resources among mobile devices for exchanging
learning information must be considered. Furthermore, the convergence
performance of different learning services depends on the hyper-learning rate
parameter that needs to be precisely decided. Towards this end, we propose a
joint resource optimization and hyper-learning rate control problem, namely
MS-FEDL, regarding the energy consumption of mobile devices and overall
learning time. We design a centralized algorithm based on the block coordinate
descent method and a decentralized JP-miADMM algorithm for solving the MS-FEDL
problem. Different from the centralized approach, the decentralized approach
requires many iterations to obtain but it allows each learning service to
independently manage the local resource and learning process without revealing
the learning service information. Our simulation results demonstrate the
convergence performance of our proposed algorithms and the superior performance
of our proposed algorithms compared to the heuristic strategy.
</p>
<a href="http://arxiv.org/abs/2011.12469" target="_blank">arXiv:2011.12469</a> [<a href="http://arxiv.org/pdf/2011.12469" target="_blank">pdf</a>]

<h2>Emotional Semantics-Preserved and Feature-Aligned CycleGAN for Visual Emotion Adaptation. (arXiv:2011.12470v1 [cs.CV])</h2>
<h3>Sicheng Zhao, Xuanbai Chen, Xiangyu Yue, Chuang Lin, Pengfei Xu, Ravi Krishna, Jufeng Yang, Guiguang Ding, Alberto L. Sangiovanni-Vincentelli, Kurt Keutzer</h3>
<p>Thanks to large-scale labeled training data, deep neural networks (DNNs) have
obtained remarkable success in many vision and multimedia tasks. However,
because of the presence of domain shift, the learned knowledge of the
well-trained DNNs cannot be well generalized to new domains or datasets that
have few labels. Unsupervised domain adaptation (UDA) studies the problem of
transferring models trained on one labeled source domain to another unlabeled
target domain. In this paper, we focus on UDA in visual emotion analysis for
both emotion distribution learning and dominant emotion classification.
Specifically, we design a novel end-to-end cycle-consistent adversarial model,
termed CycleEmotionGAN++. First, we generate an adapted domain to align the
source and target domains on the pixel-level by improving CycleGAN with a
multi-scale structured cycle-consistency loss. During the image translation, we
propose a dynamic emotional semantic consistency loss to preserve the emotion
labels of the source images. Second, we train a transferable task classifier on
the adapted domain with feature-level alignment between the adapted and target
domains. We conduct extensive UDA experiments on the Flickr-LDL &amp; Twitter-LDL
datasets for distribution learning and ArtPhoto &amp; FI datasets for emotion
classification. The results demonstrate the significant improvements yielded by
the proposed CycleEmotionGAN++ as compared to state-of-the-art UDA approaches.
</p>
<a href="http://arxiv.org/abs/2011.12470" target="_blank">arXiv:2011.12470</a> [<a href="http://arxiv.org/pdf/2011.12470" target="_blank">pdf</a>]

<h2>Minimax Estimation of Distances on a Surface and Minimax Manifold Learning in the Isometric-to-Convex Setting. (arXiv:2011.12478v1 [stat.ML])</h2>
<h3>Ery Arias-Castro, Phong Alain Chau</h3>
<p>We start by considering the problem of estimating intrinsic distances on a
smooth surface. We show that sharper estimates can be obtained via a
reconstruction of the surface, and discuss the use of the tangential Delaunay
complex for that purpose. We further show that the resulting approximation rate
is in fact optimal in an information-theoretic (minimax) sense. We then turn to
manifold learning and argue that a variant of Isomap where the distances are
instead computed on a reconstructed surface is minimax optimal for the problem
of isometric manifold embedding.
</p>
<a href="http://arxiv.org/abs/2011.12478" target="_blank">arXiv:2011.12478</a> [<a href="http://arxiv.org/pdf/2011.12478" target="_blank">pdf</a>]

<h2>Mixed-Integer Linear Programming Models for Multi-Robot Non-Adversarial Search. (arXiv:2011.12480v1 [cs.RO])</h2>
<h3>Beatriz A. Asfora, Jacopo Banfi, Mark Campbell</h3>
<p>In this letter, we consider the Multi-Robot Efficient Search Path Planning
(MESPP) problem, where a team of robots is deployed in a graph-represented
environment to capture a moving target within a given deadline. We prove this
problem to be NP-hard, and present the first set of Mixed-Integer Linear
Programming (MILP) models to tackle the MESPP problem. Our models are the first
to encompass multiple searchers, arbitrary capture ranges, and false negatives
simultaneously. While state-of-the-art algorithms for MESPP are based on simple
path enumeration, the adoption of MILP as a planning paradigm allows to
leverage the powerful techniques of modern solvers, yielding better
computational performance and, as a consequence, longer planning horizons. The
models are designed for computing optimal solutions offline, but can be easily
adapted for a distributed online approach. Our simulations show that it is
possible to achieve 98% decrease in computational time relative to the previous
state-of-the-art. We also show that the distributed approach performs nearly as
well as the centralized, within 6% in the settings studied in this letter, with
the advantage of requiring significant less time - an important consideration
in practical search missions.
</p>
<a href="http://arxiv.org/abs/2011.12480" target="_blank">arXiv:2011.12480</a> [<a href="http://arxiv.org/pdf/2011.12480" target="_blank">pdf</a>]

<h2>CellSegmenter: unsupervised representation learning and instance segmentation of modular images. (arXiv:2011.12482v1 [cs.CV])</h2>
<h3>Luca D&#x27;Alessio, Mehrtash Babadi</h3>
<p>We introduce CellSegmenter, a structured deep generative model and an
amortized inference framework for unsupervised representation learning and
instance segmentation tasks. The proposed inference algorithm is convolutional
and parallelized, without any recurrent mechanisms, and is able to resolve
object-object occlusion while simultaneously treating distant non-occluding
objects independently. This leads to extremely fast training times while
allowing extrapolation to arbitrary number of instances. We further introduce a
transparent posterior regularization strategy that encourages scene
reconstructions with fewest localized objects and a low-complexity background.
We evaluate our method on a challenging synthetic multi-MNIST dataset with a
structured background and achieve nearly perfect accuracy with only a few
hundred training epochs. Finally, we show segmentation results obtained for a
cell nuclei imaging dataset, demonstrating the ability of our method to provide
high-quality segmentations while also handling realistic use cases involving
large number of instances.
</p>
<a href="http://arxiv.org/abs/2011.12482" target="_blank">arXiv:2011.12482</a> [<a href="http://arxiv.org/pdf/2011.12482" target="_blank">pdf</a>]

<h2>CRACT: Cascaded Regression-Align-Classification for Robust Visual Tracking. (arXiv:2011.12483v1 [cs.CV])</h2>
<h3>Heng Fan, Haibin Ling</h3>
<p>High quality object proposals are crucial in visual tracking algorithms that
utilize region proposal network (RPN). Refinement of these proposals, typically
by box regression and classification in parallel, has been popularly adopted to
boost tracking performance. However, it still meets problems when dealing with
complex and dynamic background. Thus motivated, in this paper we introduce an
improved proposal refinement module, Cascaded Regression-Align-Classification
(CRAC), which yields new state-of-the-art performances on many benchmarks.

First, having observed that the offsets from box regression can serve as
guidance for proposal feature refinement, we design CRAC as a cascade of box
regression, feature alignment and box classification. The key is to bridge box
regression and classification via an alignment step, which leads to more
accurate features for proposal classification with improved robustness. To
address the variation in object appearance, we introduce an
identification-discrimination component for box classification, which leverages
offline reliable fine-grained template and online rich background information
to distinguish the target from background. Moreover, we present pyramid
RoIAlign that benefits CRAC by exploiting both the local and global cues of
proposals. During inference, tracking proceeds by ranking all refined proposals
and selecting the best one. In experiments on seven benchmarks including
OTB-2015, UAV123, NfS, VOT-2018, TrackingNet, GOT-10k and LaSOT, our CRACT
exhibits very promising results in comparison with state-of-the-art competitors
and runs in real-time.
</p>
<a href="http://arxiv.org/abs/2011.12483" target="_blank">arXiv:2011.12483</a> [<a href="http://arxiv.org/pdf/2011.12483" target="_blank">pdf</a>]

<h2>CircleGAN: Generative Adversarial Learning across Spherical Circles. (arXiv:2011.12486v1 [cs.CV])</h2>
<h3>Woohyeon Shim, Minsu Cho</h3>
<p>We present a novel discriminator for GANs that improves realness and
diversity of generated samples by learning a structured hypersphere embedding
space using spherical circles. The proposed discriminator learns to populate
realistic samples around the longest spherical circle, i.e., a great circle,
while pushing unrealistic samples toward the poles perpendicular to the great
circle. Since longer circles occupy larger area on the hypersphere, they
encourage more diversity in representation learning, and vice versa.
Discriminating samples based on their corresponding spherical circles can thus
naturally induce diversity to generated samples. We also extend the proposed
method for conditional settings with class labels by creating a hypersphere for
each category and performing class-wise discrimination and update. In
experiments, we validate the effectiveness for both unconditional and
conditional generation on standard benchmarks, achieving the state of the art.
</p>
<a href="http://arxiv.org/abs/2011.12486" target="_blank">arXiv:2011.12486</a> [<a href="http://arxiv.org/pdf/2011.12486" target="_blank">pdf</a>]

<h2>DeRF: Decomposed Radiance Fields. (arXiv:2011.12490v1 [cs.CV])</h2>
<h3>Daniel Rebain, Wei Jiang, Soroosh Yazdani, Ke Li, Kwang Moo Yi, Andrea Tagliasacchi</h3>
<p>With the advent of Neural Radiance Fields (NeRF), neural networks can now
render novel views of a 3D scene with quality that fools the human eye. Yet,
generating these images is very computationally intensive, limiting their
applicability in practical scenarios. In this paper, we propose a technique
based on spatial decomposition capable of mitigating this issue. Our key
observation is that there are diminishing returns in employing larger (deeper
and/or wider) networks. Hence, we propose to spatially decompose a scene and
dedicate smaller networks for each decomposed part. When working together,
these networks can render the whole scene. This allows us near-constant
inference time regardless of the number of decomposed parts. Moreover, we show
that a Voronoi spatial decomposition is preferable for this purpose, as it is
provably compatible with the Painter's Algorithm for efficient and GPU-friendly
rendering. Our experiments show that for real-world scenes, our method provides
up to 3x more efficient inference than NeRF (with the same rendering quality),
or an improvement of up to 1.0~dB in PSNR (for the same inference cost).
</p>
<a href="http://arxiv.org/abs/2011.12490" target="_blank">arXiv:2011.12490</a> [<a href="http://arxiv.org/pdf/2011.12490" target="_blank">pdf</a>]

<h2>World Model as a Graph: Learning Latent Landmarks for Planning. (arXiv:2011.12491v1 [cs.AI])</h2>
<h3>Lunjun Zhang, Ge Yang, Bradly C. Stadie</h3>
<p>Planning - the ability to analyze the structure of a problem in the large and
decompose it into interrelated subproblems - is a hallmark of human
intelligence. While deep reinforcement learning (RL) has shown great promise
for solving relatively straightforward control tasks, it remains an open
problem how to best incorporate planning into existing deep RL paradigms to
handle increasingly complex environments. One prominent framework, Model-Based
RL, learns a world model and plans using step-by-step virtual rollouts. This
type of world model quickly diverges from reality when the planning horizon
increases, thus struggling at long-horizon planning. How can we learn world
models that endow agents with the ability to do temporally extended reasoning?
In this work, we propose to learn graph-structured world models composed of
sparse, multi-step transitions. We devise a novel algorithm to learn latent
landmarks that are scattered (in terms of reachability) across the goal space
as the nodes on the graph. In this same graph, the edges are the reachability
estimates distilled from Q-functions. On a variety of high-dimensional
continuous control tasks ranging from robotic manipulation to navigation, we
demonstrate that our method, named L3P, significantly outperforms prior work,
and is oftentimes the only method capable of leveraging both the robustness of
model-free RL and generalization of graph-search algorithms. We believe our
work is an important step towards scalable planning in reinforcement learning.
</p>
<a href="http://arxiv.org/abs/2011.12491" target="_blank">arXiv:2011.12491</a> [<a href="http://arxiv.org/pdf/2011.12491" target="_blank">pdf</a>]

<h2>Multi-feature driven active contour segmentation model for infrared image with intensity inhomogeneity. (arXiv:2011.12492v1 [cs.CV])</h2>
<h3>Qinyan Huang, Weiwen Zhou, Minjie Wan, Xin Chen, Qian Chen, Guohua Gu</h3>
<p>Infrared (IR) image segmentation is essential in many urban defence
applications, such as pedestrian surveillance, vehicle counting, security
monitoring, etc. Active contour model (ACM) is one of the most widely used
image segmentation tools at present, but the existing methods only utilize the
local or global single feature information of image to minimize the energy
function, which is easy to cause false segmentations in IR images. In this
paper, we propose a multi-feature driven active contour segmentation model to
handle IR images with intensity inhomogeneity. Firstly, an especially-designed
signed pressure force (SPF) function is constructed by combining the global
information calculated by global average gray information and the local
multi-feature information calculated by local entropy, local standard deviation
and gradient information. Then, we draw upon adaptive weight coefficient
calculated by local range to adjust the afore-mentioned global term and local
term. Next, the SPF function is substituted into the level set formulation
(LSF) for further evolution. Finally, the LSF converges after a finite number
of iterations, and the IR image segmentation result is obtained from the
corresponding convergence result. Experimental results demonstrate that the
presented method outperforms the state-of-the-art models in terms of precision
rate and overlapping rate in IR test images.
</p>
<a href="http://arxiv.org/abs/2011.12492" target="_blank">arXiv:2011.12492</a> [<a href="http://arxiv.org/pdf/2011.12492" target="_blank">pdf</a>]

<h2>Humble Teacher and Eager Student: Dual Network Learning for Semi-supervised 2D Human Pose Estimation. (arXiv:2011.12498v1 [cs.CV])</h2>
<h3>Rongchang Xie, Chunyu Wang, Wenjun Zeng, Yizhou Wang</h3>
<p>Semi-supervised learning aims to boost the accuracy of a model by exploring
unlabeled images. The state-of-the-art methods are consistency-based which
learn about unlabeled images by encouraging the model to give consistent
predictions for images under different augmentations. However, when applied to
pose estimation, the methods degenerate and predict every pixel in unlabeled
images as background. This is because contradictory predictions are gradually
pushed to the background class due to highly imbalanced class distribution. But
this is not an issue in supervised learning because it has accurate labels.
This inspires us to stabilize the training by obtaining reliable pseudo labels.
Specifically, we learn two networks to mutually teach each other. In
particular, for each image, we compose an easy-hard pair by applying different
augmentations and feed them to both networks. The more reliable predictions on
easy images in each network are used to teach the other network to learn about
the corresponding hard images. The approach successfully avoids degeneration
and achieves promising results on public datasets. The source code will be
released.
</p>
<a href="http://arxiv.org/abs/2011.12498" target="_blank">arXiv:2011.12498</a> [<a href="http://arxiv.org/pdf/2011.12498" target="_blank">pdf</a>]

<h2>Privacy-preserving Collaborative Learning with Automatic Transformation Search. (arXiv:2011.12505v1 [cs.CV])</h2>
<h3>Wei Gao, Shangwei Guo, Tianwei Zhang, Han Qiu, Yonggang Wen, Yang Liu</h3>
<p>Collaborative learning has gained great popularity due to its benefit of data
privacy protection: participants can jointly train a Deep Learning model
without sharing their training sets. However, recent works discovered that an
adversary can fully recover the sensitive training samples from the shared
gradients. Such reconstruction attacks pose severe threats to collaborative
learning. Hence, effective mitigation solutions are urgently desired.

In this paper, we propose to leverage data augmentation to defeat
reconstruction attacks: by preprocessing sensitive images with
carefully-selected transformation policies, it becomes infeasible for the
adversary to extract any useful information from the corresponding gradients.
We design a novel search method to automatically discover qualified policies.
We adopt two new metrics to quantify the impacts of transformations on data
privacy and model usability, which can significantly accelerate the search
speed. Comprehensive evaluations demonstrate that the policies discovered by
our method can defeat existing reconstruction attacks in collaborative
learning, with high efficiency and negligible impact on the model performance.
</p>
<a href="http://arxiv.org/abs/2011.12505" target="_blank">arXiv:2011.12505</a> [<a href="http://arxiv.org/pdf/2011.12505" target="_blank">pdf</a>]

<h2>Using Radiomics as Prior Knowledge for Abnormality Classification and Localization in Chest X-rays. (arXiv:2011.12506v1 [cs.CV])</h2>
<h3>Yan Han, Chongyan Chen, Liyan Tang, Mingquan Lin, Ajay Jaiswal, Ying Ding, Yifan Peng</h3>
<p>Chest X-rays become one of the most common medical diagnoses due to its
noninvasiveness. The number of chest X-ray images has skyrocketed, but reading
chest X-rays still have been manually performed by radiologists, which creates
huge burnouts and delays. Traditionally, radiomics, as a subfield of radiology
that can extract a large number of quantitative features from medical images,
demonstrates its potential to facilitate medical imaging diagnosis before the
deep learning era. In this paper, we develop an algorithm that can utilize the
radiomics features to improve the abnormality classification performance. Our
algorithm, ChexRadiNet, applies a light-weight but efficient triplet-attention
mechanism for highlighting the meaningful image regions to improve localization
accuracy. We first apply ChexRadiNet to classify the chest X-rays by using only
image features. Then we use the generated heatmaps of chest X-rays to extract
radiomics features. Finally, the extracted radiomics features could be used to
guide our model to learn more robust accurate image features. After a number of
iterations, our model could focus on more accurate image regions and extract
more robust features. The empirical evaluation of our method supports our
intuition and outperforms other state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2011.12506" target="_blank">arXiv:2011.12506</a> [<a href="http://arxiv.org/pdf/2011.12506" target="_blank">pdf</a>]

<h2>Causal inference using deep neural networks. (arXiv:2011.12508v1 [cs.LG])</h2>
<h3>Ye Yuan, Xueying Ding, Ziv Bar-Joseph</h3>
<p>Causal inference from observation data is a core problem in many scientific
fields. Here we present a general supervised deep learning framework that
infers causal interactions by transforming the input vectors to an image-like
representation for every pair of inputs. Given a training dataset we first
construct a normalized empirical probability density distribution (NEPDF)
matrix. We then train a convolutional neural network (CNN) on NEPDFs for
causality predictions. We tested the method on several different simulated and
real world data and compared it to prior methods for causal inference. As we
show, the method is general, can efficiently handle very large datasets and
improves upon prior methods.
</p>
<a href="http://arxiv.org/abs/2011.12508" target="_blank">arXiv:2011.12508</a> [<a href="http://arxiv.org/pdf/2011.12508" target="_blank">pdf</a>]

<h2>MetaGater: Fast Learning of Conditional Channel Gated Networks via Federated Meta-Learning. (arXiv:2011.12511v1 [cs.LG])</h2>
<h3>Sen Lin, Li Yang, Zhezhi He, Deliang Fan, Junshan Zhang</h3>
<p>While deep learning has achieved phenomenal successes in many AI
applications, its enormous model size and intensive computation requirements
pose a formidable challenge to the deployment in resource-limited nodes. There
has recently been an increasing interest in computationally-efficient learning
methods, e.g., quantization, pruning and channel gating. However, most existing
techniques cannot adapt to different tasks quickly. In this work, we advocate a
holistic approach to jointly train the backbone network and the channel gating
which enables dynamical selection of a subset of filters for more efficient
local computation given the data input. Particularly, we develop a federated
meta-learning approach to jointly learn good meta-initializations for both
backbone networks and gating modules, by making use of the model similarity
across learning tasks on different nodes. In this way, the learnt meta-gating
module effectively captures the important filters of a good meta-backbone
network, based on which a task-specific conditional channel gated network can
be quickly adapted, i.e., through one-step gradient descent, from the
meta-initializations in a two-stage procedure using new samples of that task.
The convergence of the proposed federated meta-learning algorithm is
established under mild conditions. Experimental results corroborate the
effectiveness of our method in comparison to related work.
</p>
<a href="http://arxiv.org/abs/2011.12511" target="_blank">arXiv:2011.12511</a> [<a href="http://arxiv.org/pdf/2011.12511" target="_blank">pdf</a>]

<h2>Match Them Up: Visually Explainable Few-shot Image Classification. (arXiv:2011.12527v1 [cs.CV])</h2>
<h3>Bowen Wang, Liangzhi Li, Manisha Verma, Yuta Nakashima, Ryo Kawasaki, Hajime Nagahara</h3>
<p>Few-shot learning (FSL) approaches are usually based on an assumption that
the pre-trained knowledge can be obtained from base (seen) categories and can
be well transferred to novel (unseen) categories. However, there is no
guarantee, especially for the latter part. This issue leads to the unknown
nature of the inference process in most FSL methods, which hampers its
application in some risk-sensitive areas. In this paper, we reveal a new way to
perform FSL for image classification, using visual representations from the
backbone model and weights generated by a newly-emerged explainable classifier.
The weighted representations only include a minimum number of distinguishable
features and the visualized weights can serve as an informative hint for the
FSL process. Finally, a discriminator will compare the representations of each
pair of the images in the support set and the query set. Pairs with the highest
scores will decide the classification results. Experimental results prove that
the proposed method can achieve both good accuracy and satisfactory
explainability on three mainstream datasets.
</p>
<a href="http://arxiv.org/abs/2011.12527" target="_blank">arXiv:2011.12527</a> [<a href="http://arxiv.org/pdf/2011.12527" target="_blank">pdf</a>]

<h2>Reference-Based Video Colorization with Spatiotemporal Correspondence. (arXiv:2011.12528v1 [cs.CV])</h2>
<h3>Naofumi Akimoto, Akio Hayakawa, Andrew Shin, Takuya Narihira</h3>
<p>We propose a novel reference-based video colorization framework with
spatiotemporal correspondence. Reference-based methods colorize grayscale
frames referencing a user input color frame. Existing methods suffer from the
color leakage between objects and the emergence of average colors, derived from
non-local semantic correspondence in space. To address this issue, we warp
colors only from the regions on the reference frame restricted by
correspondence in time. We propagate masks as temporal correspondences, using
two complementary tracking approaches: off-the-shelf instance tracking for high
performance segmentation, and newly proposed dense tracking to track various
types of objects. By restricting temporally-related regions for referencing
colors, our approach propagates faithful colors throughout the video.
Experiments demonstrate that our method outperforms state-of-the-art methods
quantitatively and qualitatively.
</p>
<a href="http://arxiv.org/abs/2011.12528" target="_blank">arXiv:2011.12528</a> [<a href="http://arxiv.org/pdf/2011.12528" target="_blank">pdf</a>]

<h2>Consistency-aware and Inconsistency-aware Graph-based Multi-view Clustering. (arXiv:2011.12532v1 [cs.LG])</h2>
<h3>Mitsuhiko Horie, Hiroyuki Kasai</h3>
<p>Multi-view data analysis has gained increasing popularity because multi-view
data are frequently encountered in machine learning applications. A simple but
promising approach for clustering of multi-view data is multi-view clustering
(MVC), which has been developed extensively to classify given subjects into
some clustered groups by learning latent common features that are shared across
multi-view data. Among existing approaches, graph-based multi-view clustering
(GMVC) achieves state-of-the-art performance by leveraging a shared graph
matrix called the unified matrix. However, existing methods including GMVC do
not explicitly address inconsistent parts of input graph matrices.
Consequently, they are adversely affected by unacceptable clustering
performance. To this end, this paper proposes a new GMVC method that
incorporates consistent and inconsistent parts lying across multiple views.
This proposal is designated as CI-GMVC. Numerical evaluations of real-world
datasets demonstrate the effectiveness of the proposed CI-GMVC.
</p>
<a href="http://arxiv.org/abs/2011.12532" target="_blank">arXiv:2011.12532</a> [<a href="http://arxiv.org/pdf/2011.12532" target="_blank">pdf</a>]

<h2>A Odor Labeling Convolutional Encoder-Decoder for Odor Sensing in Machine Olfaction. (arXiv:2011.12538v1 [cs.AI])</h2>
<h3>Tengteng Wen, Zhuofeng Mo, Jingshan Li, Qi Liu, Liming Wu, Dehan Luo</h3>
<p>Nowadays, machine olfaction has been widely used in many fields. In this
paper, we proposed an odor labeling convolutional encoder-decoder (OLCE) for
odor sensing. In order to demonstrate the performance of the model, a
comparison of several algorithms that had been applied to machine olfaction was
implemented. Experiment results demonstrated that OLCE had a decent performance
using in machine olfaction. The challenge of establishing general odor
characterization was discussed.
</p>
<a href="http://arxiv.org/abs/2011.12538" target="_blank">arXiv:2011.12538</a> [<a href="http://arxiv.org/pdf/2011.12538" target="_blank">pdf</a>]

<h2>Leveraging Predictions in Smoothed Online Convex Optimization via Gradient-based Algorithms. (arXiv:2011.12539v1 [cs.LG])</h2>
<h3>Yingying Li, Na Li</h3>
<p>We consider online convex optimization with time-varying stage costs and
additional switching costs. Since the switching costs introduce coupling across
all stages, multi-step-ahead (long-term) predictions are incorporated to
improve the online performance. However, longer-term predictions tend to suffer
from lower quality. Thus, a critical question is: how to reduce the impact of
long-term prediction errors on the online performance? To address this
question, we introduce a gradient-based online algorithm, Receding Horizon
Inexact Gradient (RHIG), and analyze its performance by dynamic regrets in
terms of the temporal variation of the environment and the prediction errors.
RHIG only considers at most $W$-step-ahead predictions to avoid being misled by
worse predictions in the longer term. The optimal choice of $W$ suggested by
our regret bounds depends on the tradeoff between the variation of the
environment and the prediction accuracy. Additionally, we apply RHIG to a
well-established stochastic prediction error model and provide expected regret
and concentration bounds under correlated prediction errors. Lastly, we
numerically test the performance of RHIG on quadrotor tracking problems.
</p>
<a href="http://arxiv.org/abs/2011.12539" target="_blank">arXiv:2011.12539</a> [<a href="http://arxiv.org/pdf/2011.12539" target="_blank">pdf</a>]

<h2>Wasserstein k-means with sparse simplex projection. (arXiv:2011.12542v1 [cs.LG])</h2>
<h3>Takumi Fukunaga, Hiroyuki Kasai</h3>
<p>This paper presents a proposal of a faster Wasserstein $k$-means algorithm
for histogram data by reducing Wasserstein distance computations and exploiting
sparse simplex projection. We shrink data samples, centroids, and the ground
cost matrix, which leads to considerable reduction of the computations used to
solve optimal transport problems without loss of clustering quality.
Furthermore, we dynamically reduced the computational complexity by removing
lower-valued data samples and harnessing sparse simplex projection while
keeping the degradation of clustering quality lower. We designate this proposed
algorithm as sparse simplex projection based Wasserstein $k$-means, or SSPW
$k$-means. Numerical evaluations conducted with comparison to results obtained
using Wasserstein $k$-means algorithm demonstrate the effectiveness of the
proposed SSPW $k$-means for real-world datasets
</p>
<a href="http://arxiv.org/abs/2011.12542" target="_blank">arXiv:2011.12542</a> [<a href="http://arxiv.org/pdf/2011.12542" target="_blank">pdf</a>]

<h2>Implicit bias of deep linear networks in the large learning rate phase. (arXiv:2011.12547v1 [cs.LG])</h2>
<h3>Wei Huang, Weitao Du, Richard Yi Da Xu, Chunrui Liu</h3>
<p>Correctly choosing a learning rate (scheme) for gradient-based optimization
is vital in deep learning since different learning rates may lead to
considerable differences in optimization and generalization. As found by
Lewkowycz et al. \cite{lewkowycz2020large} recently, there is a learning rate
phase with large stepsize named {\it catapult phase}, where the loss grows at
the early stage of training, and optimization eventually converges to a flatter
minimum with better generalization. While this phenomenon is valid for deep
neural networks with mean squared loss, it is an open question whether logistic
(cross-entropy) loss still has a {\it catapult phase} and enjoys better
generalization ability. This work answeres this question by studying deep
linear networks with logistic loss. We find that the large learning rate phase
is closely related to the separability of data. The non-separable data results
in the {\it catapult phase}, and thus flatter minimum can be achieved in this
learning rate phase. We demonstrate empirically that this interpretation can be
applied to real settings on MNIST and CIFAR10 datasets with the fact that the
optimal performance is often found in this large learning rate phase.
</p>
<a href="http://arxiv.org/abs/2011.12547" target="_blank">arXiv:2011.12547</a> [<a href="http://arxiv.org/pdf/2011.12547" target="_blank">pdf</a>]

<h2>Measuring Happiness Around the World Through Artificial Intelligence. (arXiv:2011.12548v1 [cs.AI])</h2>
<h3>Rustem Ozakar, Rafet Efe Gazanfer, Y. Sinan Hanay</h3>
<p>In this work, we analyze the happiness levels of countries using an unbiased
emotion detector, artificial intelligence (AI). To date, researchers proposed
many factors that may affect happiness such as wealth, health and safety. Even
though these factors all seem relevant, there is no clear consensus between
sociologists on how to interpret these, and the models to estimate the cost of
these utilities include some assumptions. Researchers in social sciences have
been working on determination of the happiness levels in society and
exploration of the factors correlated with it through polls and different
statistical methods. In our work, by using artificial intelligence, we
introduce a different and relatively unbiased approach to this problem. By
using AI, we make no assumption about what makes a person happy, and leave the
decision to AI to detect the emotions from the faces of people collected from
publicly available street footages. We analyzed the happiness levels in eight
different cities around the world through available footage on the Internet and
found out that there is no statistically significant difference between
countries in terms of happiness.
</p>
<a href="http://arxiv.org/abs/2011.12548" target="_blank">arXiv:2011.12548</a> [<a href="http://arxiv.org/pdf/2011.12548" target="_blank">pdf</a>]

<h2>Robust Correlation Tracking via Multi-channel Fused Features and Reliable Response Map. (arXiv:2011.12550v1 [cs.CV])</h2>
<h3>Xizhe Xue, Ying Li, Qiang Shen</h3>
<p>Benefiting from its ability to efficiently learn how an object is changing,
correlation filters have recently demonstrated excellent performance for
rapidly tracking objects. Designing effective features and handling model
drifts are two important aspects for online visual tracking. This paper tackles
these challenges by proposing a robust correlation tracking algorithm (RCT)
based on two ideas: First, we propose a method to fuse features in order to
more naturally describe the gradient and color information of the tracked
object, and introduce the fused features into a background aware correlation
filter to obtain the response map. Second, we present a novel strategy to
significantly reduce noise in the response map and therefore ease the problem
of model drift. Systematic comparative evaluations performed over multiple
tracking benchmarks demonstrate the efficacy of the proposed approach.
</p>
<a href="http://arxiv.org/abs/2011.12550" target="_blank">arXiv:2011.12550</a> [<a href="http://arxiv.org/pdf/2011.12550" target="_blank">pdf</a>]

<h2>Delving Deep into Label Smoothing. (arXiv:2011.12562v1 [cs.CV])</h2>
<h3>Chang-Bin Zhang, Peng-Tao Jiang, Qibin Hou, Yunchao Wei, Qi Han, Zhen Li, Ming-Ming Cheng</h3>
<p>Label smoothing is an effective regularization tool for deep neural networks
(DNNs), which generates soft labels by applying a weighted average between the
uniform distribution and the hard label. It is often used to reduce the
overfitting problem of training DNNs and further improve classification
performance. In this paper, we aim to investigate how to generate more reliable
soft labels. We present an Online Label Smoothing (OLS) strategy, which
generates soft labels based on the statistics of the model prediction for the
target category. The proposed OLS constructs a more reasonable probability
distribution between the target categories and non-target categories to
supervise DNNs. Experiments demonstrate that based on the same classification
models, the proposed approach can effectively improve the classification
performance on CIFAR-100, ImageNet, and fine-grained datasets. Additionally,
the proposed method can significantly improve the robustness of DNN models to
noisy labels compared to current label smoothing approaches. The code will be
made publicly available.
</p>
<a href="http://arxiv.org/abs/2011.12562" target="_blank">arXiv:2011.12562</a> [<a href="http://arxiv.org/pdf/2011.12562" target="_blank">pdf</a>]

<h2>Multi-Domain Adversarial Feature Generalization for Person Re-Identification. (arXiv:2011.12563v1 [cs.CV])</h2>
<h3>Shan Lin, Chang-Tsun Li, Alex C. Kot</h3>
<p>With the assistance of sophisticated training methods applied to single
labeled datasets, the performance of fully-supervised person re-identification
(Person Re-ID) has been improved significantly in recent years. However, these
models trained on a single dataset usually suffer from considerable performance
degradation when applied to videos of a different camera network. To make
Person Re-ID systems more practical and scalable, several cross-dataset domain
adaptation methods have been proposed, which achieve high performance without
the labeled data from the target domain. However, these approaches still
require the unlabeled data of the target domain during the training process,
making them impractical. A practical Person Re-ID system pre-trained on other
datasets should start running immediately after deployment on a new site
without having to wait until sufficient images or videos are collected and the
pre-trained model is tuned. To serve this purpose, in this paper, we
reformulate person re-identification as a multi-dataset domain generalization
problem. We propose a multi-dataset feature generalization network (MMFA-AAE),
which is capable of learning a universal domain-invariant feature
representation from multiple labeled datasets and generalizing it to `unseen'
camera systems. The network is based on an adversarial auto-encoder to learn a
generalized domain-invariant latent feature representation with the Maximum
Mean Discrepancy (MMD) measure to align the distributions across multiple
domains. Extensive experiments demonstrate the effectiveness of the proposed
method. Our MMFA-AAE approach not only outperforms most of the domain
generalization Person Re-ID methods, but also surpasses many state-of-the-art
supervised methods and unsupervised domain adaptation methods by a large
margin.
</p>
<a href="http://arxiv.org/abs/2011.12563" target="_blank">arXiv:2011.12563</a> [<a href="http://arxiv.org/pdf/2011.12563" target="_blank">pdf</a>]

<h2>ColdGAN: Resolving Cold Start User Recommendation by using Generative Adversarial Networks. (arXiv:2011.12566v1 [cs.AI])</h2>
<h3>Po-Lin Lai, Chih-Yun Chen, Liang-Wei Lo, Chien-Chin Chen</h3>
<p>Mitigating the new user cold-start problem has been critical in the
recommendation system for online service providers to influence user experience
in decision making which can ultimately affect the intention of users to use a
particular service. Previous studies leveraged various side information from
users and items; however, it may be impractical due to privacy concerns. In
this paper, we present ColdGAN, an end-to-end GAN based model with no use of
side information to resolve this problem. The main idea of the proposed model
is to train a network that learns the rating distributions of experienced users
given their cold-start distributions. We further design a time-based function
to restore the preferences of users to cold-start states. With extensive
experiments on two real-world datasets, the results show that our proposed
method achieves significantly improved performance compared with the
state-of-the-art recommenders.
</p>
<a href="http://arxiv.org/abs/2011.12566" target="_blank">arXiv:2011.12566</a> [<a href="http://arxiv.org/pdf/2011.12566" target="_blank">pdf</a>]

<h2>Learning Certified Control using Contraction Metric. (arXiv:2011.12569v1 [cs.RO])</h2>
<h3>Dawei Sun, Susmit Jha, Chuchu Fan</h3>
<p>In this paper, we solve the problem of finding a certified control policy
that drives a robot from any given initial state and under any bounded
disturbance to the desired reference trajectory, with guarantees on the
convergence or bounds on the tracking error. Such a controller is crucial in
safe motion planning. We leverage the advanced theory in Control Contraction
Metric and design a learning framework based on neural networks to
co-synthesize the contraction metric and the controller for control-affine
systems. We further provide methods to validate the convergence and bounded
error guarantees. We demonstrate the performance of our method using a suite of
challenging robotic models, including models with learned dynamics as neural
networks. We compare our approach with leading methods using sum-of-squares
programming, reinforcement learning, and model predictive control. Results show
that our methods indeed can handle a broader class of systems with less
tracking error and faster execution speed. Code is available at
https://github.com/sundw2014/C3M.
</p>
<a href="http://arxiv.org/abs/2011.12569" target="_blank">arXiv:2011.12569</a> [<a href="http://arxiv.org/pdf/2011.12569" target="_blank">pdf</a>]

<h2>Enhanced Scene Specificity with Sparse Dynamic Value Estimation. (arXiv:2011.12574v1 [cs.LG])</h2>
<h3>Jaskirat Singh, Liang Zheng</h3>
<p>Multi-scene reinforcement learning involves training the RL agent across
multiple scenes / levels from the same task, and has become essential for many
generalization applications. However, the inclusion of multiple scenes leads to
an increase in sample variance for policy gradient computations, often
resulting in suboptimal performance with the direct application of traditional
methods (e.g. PPO, A3C). One strategy for variance reduction is to consider
each scene as a distinct Markov decision process (MDP) and learn a joint value
function dependent on both state (s) and MDP (M). However, this is non-trivial
as the agent is usually unaware of the underlying level at train / test times
in multi-scene RL. Recently, Singh et al. [1] tried to address this by
proposing a dynamic value estimation approach that models the true joint value
function distribution as a Gaussian mixture model (GMM). In this paper, we
argue that the error between the true scene-specific value function and the
predicted dynamic estimate can be further reduced by progressively enforcing
sparse cluster assignments once the agent has explored most of the state space.
The resulting agents not only show significant improvements in the final reward
score across a range of OpenAI ProcGen environments, but also exhibit increased
navigation efficiency while completing a game level.
</p>
<a href="http://arxiv.org/abs/2011.12574" target="_blank">arXiv:2011.12574</a> [<a href="http://arxiv.org/pdf/2011.12574" target="_blank">pdf</a>]

<h2>Continual learning with direction-constrained optimization. (arXiv:2011.12581v1 [cs.LG])</h2>
<h3>Yunfei Teng, Anna Choromanska, Murray Campbell</h3>
<p>This paper studies a new design of the optimization algorithm for training
deep learning models with a fixed architecture of the classification network in
a continual learning framework, where the training data is non-stationary and
the non-stationarity is imposed by a sequence of distinct tasks. This setting
implies the existence of a manifold of network parameters that correspond to
good performance of the network on all tasks. Our algorithm is derived from the
geometrical properties of this manifold. We first analyze a deep model trained
on only one learning task in isolation and identify a region in network
parameter space, where the model performance is close to the recovered optimum.
We provide empirical evidence that this region resembles a cone that expands
along the convergence direction. We study the principal directions of the
trajectory of the optimizer after convergence and show that traveling along a
few top principal directions can quickly bring the parameters outside the cone
but this is not the case for the remaining directions. We argue that
catastrophic forgetting in a continual learning setting can be alleviated when
the parameters are constrained to stay within the intersection of the plausible
cones of individual tasks that were so far encountered during training.
Enforcing this is equivalent to preventing the parameters from moving along the
top principal directions of convergence corresponding to the past tasks. For
each task we introduce a new linear autoencoder to approximate its
corresponding top forbidden principal directions. They are then incorporated
into the loss function in the form of a regularization term for the purpose of
learning the coming tasks without forgetting. We empirically demonstrate that
our algorithm performs favorably compared to other state-of-art
regularization-based continual learning methods, including EWC and SI.
</p>
<a href="http://arxiv.org/abs/2011.12581" target="_blank">arXiv:2011.12581</a> [<a href="http://arxiv.org/pdf/2011.12581" target="_blank">pdf</a>]

<h2>Supervised Learning Achieves Human-Level Performance in MOBA Games: A Case Study of Honor of Kings. (arXiv:2011.12582v1 [cs.AI])</h2>
<h3>Deheng Ye, Guibin Chen, Peilin Zhao, Fuhao Qiu, Bo Yuan, Wen Zhang, Sheng Chen, Mingfei Sun, Xiaoqian Li, Siqin Li, Jing Liang, Zhenjie Lian, Bei Shi, Liang Wang, Tengfei Shi, Qiang Fu, Wei Yang, Lanxiao Huang</h3>
<p>We present JueWu-SL, the first supervised-learning-based artificial
intelligence (AI) program that achieves human-level performance in playing
multiplayer online battle arena (MOBA) games. Unlike prior attempts, we
integrate the macro-strategy and the micromanagement of MOBA-game-playing into
neural networks in a supervised and end-to-end manner. Tested on Honor of
Kings, the most popular MOBA at present, our AI performs competitively at the
level of High King players in standard 5v5 games.
</p>
<a href="http://arxiv.org/abs/2011.12582" target="_blank">arXiv:2011.12582</a> [<a href="http://arxiv.org/pdf/2011.12582" target="_blank">pdf</a>]

<h2>Combining Semantic Guidance and Deep Reinforcement Learning For Generating Human Level Paintings. (arXiv:2011.12589v1 [cs.CV])</h2>
<h3>Jaskirat Singh, Liang Zheng</h3>
<p>Generation of stroke-based non-photorealistic imagery, is an important
problem in the computer vision community. As an endeavor in this direction,
substantial recent research efforts have been focused on teaching machines "how
to paint", in a manner similar to a human painter. However, the applicability
of previous methods has been limited to datasets with little variation in
position, scale and saliency of the foreground object. As a consequence, we
find that these methods struggle to cover the granularity and diversity
possessed by real world images. To this end, we propose a Semantic Guidance
pipeline with 1) a bi-level painting procedure for learning the distinction
between foreground and background brush strokes at training time. 2) We also
introduce invariance to the position and scale of the foreground object through
a neural alignment model, which combines object localization and spatial
transformer networks in an end to end manner, to zoom into a particular
semantic instance. 3) The distinguishing features of the in-focus object are
then amplified by maximizing a novel guided backpropagation based focus reward.
The proposed agent does not require any supervision on human stroke-data and
successfully handles variations in foreground object attributes, thus,
producing much higher quality canvases for the CUB-200 Birds and Stanford
Cars-196 datasets. Finally, we demonstrate the further efficacy of our method
on complex datasets with multiple foreground object instances by evaluating an
extension of our method on the challenging Virtual-KITTI dataset.
</p>
<a href="http://arxiv.org/abs/2011.12589" target="_blank">arXiv:2011.12589</a> [<a href="http://arxiv.org/pdf/2011.12589" target="_blank">pdf</a>]

<h2>Energy Forecasting in Smart Grid Systems: A Review of the State-of-the-art Techniques. (arXiv:2011.12598v1 [cs.LG])</h2>
<h3>Devinder Kaur, Shama Naz Islam, Md. Apel Mahmud, ZhaoYang Dong</h3>
<p>Energy forecasting has a vital role to play in smart grid (SG) systems
involving various applications such as demand-side management, load shedding,
optimum dispatch, etc. Managing efficient forecasting while ensuring the least
possible prediction error is one of the main challenges posed in the grid
today, considering the uncertainty and granularity in the SG data. This paper
presents a comprehensive and application-oriented review of state-of-the-art
forecasting methods for SG systems considering the different models and
architectures. Traditional statistical and machine learning-based forecasting
methods are extensively investigated in terms of their applicability to energy
forecasting. In addition, the significance of hybrid methods and data
pre-processing techniques for better forecasting accuracy is also highlighted.
A comparative case study using the Victorian electricity consumption benchmark
and American electric power (AEP) datasets is conducted to analyze the
performance of different forecasting methods. The analysis demonstrates higher
accuracy of the recurrent neural network (RNN) and long-short term memory
(LSTM) methods when sample sizes are larger and hyperparameters are
appropriately tuned. Furthermore, hybrid methods such as CNN-LSTM are also
highly effective to deal with long sequences in energy data.
</p>
<a href="http://arxiv.org/abs/2011.12598" target="_blank">arXiv:2011.12598</a> [<a href="http://arxiv.org/pdf/2011.12598" target="_blank">pdf</a>]

<h2>The Landscape of Ontology Reuse Approaches. (arXiv:2011.12599v1 [cs.AI])</h2>
<h3>Valentina Anita Carriero, Marilena Daquino, Aldo Gangemi, Andrea Giovanni Nuzzolese, Silvio Peroni, Valentina Presutti, Francesca Tomasi</h3>
<p>Ontology reuse aims to foster interoperability and facilitate knowledge
reuse. Several approaches are typically evaluated by ontology engineers when
bootstrapping a new project. However, current practices are often motivated by
subjective, case-by-case decisions, which hamper the definition of a
recommended behaviour. In this chapter we argue that to date there are no
effective solutions for supporting developers' decision-making process when
deciding on an ontology reuse strategy. The objective is twofold: (i) to survey
current approaches to ontology reuse, presenting motivations, strategies,
benefits and limits, and (ii) to analyse two representative approaches and
discuss their merits.
</p>
<a href="http://arxiv.org/abs/2011.12599" target="_blank">arXiv:2011.12599</a> [<a href="http://arxiv.org/pdf/2011.12599" target="_blank">pdf</a>]

<h2>Unsupervised Domain Adaptation in Semantic Segmentation via Orthogonal and Clustered Embeddings. (arXiv:2011.12616v1 [cs.CV])</h2>
<h3>Marco Toldo, Umberto Michieli, Pietro Zanuttigh</h3>
<p>Deep learning frameworks allowed for a remarkable advancement in semantic
segmentation, but the data hungry nature of convolutional networks has rapidly
raised the demand for adaptation techniques able to transfer learned knowledge
from label-abundant domains to unlabeled ones. In this paper we propose an
effective Unsupervised Domain Adaptation (UDA) strategy, based on a feature
clustering method that captures the different semantic modes of the feature
distribution and groups features of the same class into tight and
well-separated clusters. Furthermore, we introduce two novel learning
objectives to enhance the discriminative clustering performance: an
orthogonality loss forces spaced out individual representations to be
orthogonal, while a sparsity loss reduces class-wise the number of active
feature channels. The joint effect of these modules is to regularize the
structure of the feature space. Extensive evaluations in the synthetic-to-real
scenario show that we achieve state-of-the-art performance.
</p>
<a href="http://arxiv.org/abs/2011.12616" target="_blank">arXiv:2011.12616</a> [<a href="http://arxiv.org/pdf/2011.12616" target="_blank">pdf</a>]

<h2>ImCLR: Implicit Contrastive Learning for Image Classification. (arXiv:2011.12618v1 [cs.CV])</h2>
<h3>John Chen, Samarth Sinha, Anastasios Kyrillidis</h3>
<p>Contrastive learning is an effective method for learning visual
representations. In most cases, this involves adding an explicit loss function
to encourage similar images to have similar representations, and different
images to have different representations. Inspired by contrastive learning, we
introduce a clever input construction for Implicit Contrastive Learning
(ImCLR), primarily in the supervised setting: there, the network can implicitly
learn to differentiate between similar and dissimilar images. Each input is
presented as a concatenation of two images, and the label is the mean of the
two one-hot labels. Furthermore, this requires almost no change to existing
pipelines, which allows for easy integration and for fair demonstration of
effectiveness on a wide range of well-accepted benchmarks. Namely, there is no
change to loss, no change to hyperparameters, and no change to general network
architecture. We show that ImCLR improves the test error in the supervised
setting across a variety of settings, including 3.24% on Tiny ImageNet, 1.30%
on CIFAR-100, 0.14% on CIFAR-10, and 2.28% on STL-10. We show that this holds
across different number of labeled samples, maintaining approximately a 2% gap
in test accuracy down to using only 5% of the whole dataset. We further show
that gains hold for robustness to common input corruptions and perturbations at
varying severities with a 0.72% improvement on CIFAR-100-C, and in the
semi-supervised setting with a 2.16% improvement with the standard benchmark
$\Pi$-model. We demonstrate that ImCLR is complementary to existing data
augmentation techniques, achieving over 1% improvement on CIFAR-100 and 2%
improvement on Tiny ImageNet by combining ImCLR with CutMix over either
baseline, and 2% by combining ImCLR with AutoAugment over either baseline.
</p>
<a href="http://arxiv.org/abs/2011.12618" target="_blank">arXiv:2011.12618</a> [<a href="http://arxiv.org/pdf/2011.12618" target="_blank">pdf</a>]

<h2>Recent Progress in Appearance-based Action Recognition. (arXiv:2011.12619v1 [cs.CV])</h2>
<h3>Jack Humphreys, Zhe Chen, Dacheng Tao</h3>
<p>Action recognition, which is formulated as a task to identify various human
actions in a video, has attracted increasing interest from computer vision
researchers due to its importance in various applications. Recently,
appearance-based methods have achieved promising progress towards accurate
action recognition. In general, these methods mainly fulfill the task by
applying various schemes to model spatial and temporal visual information
effectively. To better understand the current progress of appearance-based
action recognition, we provide a comprehensive review of recent achievements in
this area. In particular, we summarise and discuss several dozens of related
research papers, which can be roughly divided into four categories according to
different appearance modelling strategies. The obtained categories include 2D
convolutional methods, 3D convolutional methods, motion representation-based
methods, and context representation-based methods. We analyse and discuss
representative methods from each category, comprehensively. Empirical results
are also summarised to better illustrate cutting-edge algorithms. We conclude
by identifying important areas for future research gleaned from our
categorisation.
</p>
<a href="http://arxiv.org/abs/2011.12619" target="_blank">arXiv:2011.12619</a> [<a href="http://arxiv.org/pdf/2011.12619" target="_blank">pdf</a>]

<h2>Improving Augmentation and Evaluation Schemes for Semantic Image Synthesis. (arXiv:2011.12636v1 [cs.CV])</h2>
<h3>Prateek Katiyar, Anna Khoreva</h3>
<p>Despite data augmentation being a de facto technique for boosting the
performance of deep neural networks, little attention has been paid to
developing augmentation strategies for generative adversarial networks (GANs).
To this end, we introduce a novel augmentation scheme designed specifically for
GAN-based semantic image synthesis models. We propose to randomly warp object
shapes in the semantic label maps used as an input to the generator. The local
shape discrepancies between the warped and non-warped label maps and images
enable the GAN to learn better the structural and geometric details of the
scene and thus to improve the quality of generated images. While benchmarking
the augmented GAN models against their vanilla counterparts, we discover that
the quantification metrics reported in the previous semantic image synthesis
studies are strongly biased towards specific semantic classes as they are
derived via an external pre-trained segmentation network. We therefore propose
to improve the established semantic image synthesis evaluation scheme by
analyzing separately the performance of generated images on the biased and
unbiased classes for the given segmentation network. Finally, we show strong
quantitative and qualitative improvements obtained with our augmentation
scheme, on both class splits, using state-of-the-art semantic image synthesis
models across three different datasets. On average across COCO-Stuff, ADE20K
and Cityscapes datasets, the augmented models outperform their vanilla
counterparts by ~3 mIoU and ~10 FID points.
</p>
<a href="http://arxiv.org/abs/2011.12636" target="_blank">arXiv:2011.12636</a> [<a href="http://arxiv.org/pdf/2011.12636" target="_blank">pdf</a>]

<h2>PGL: Prior-Guided Local Self-supervised Learning for 3D Medical Image Segmentation. (arXiv:2011.12640v1 [cs.CV])</h2>
<h3>Yutong Xie, Jianpeng Zhang, Zehui Liao, Yong Xia, Chunhua Shen</h3>
<p>It has been widely recognized that the success of deep learning in image
segmentation relies overwhelmingly on a myriad amount of densely annotated
training data, which, however, are difficult to obtain due to the tremendous
labor and expertise required, particularly for annotating 3D medical images.
Although self-supervised learning (SSL) has shown great potential to address
this issue, most SSL approaches focus only on image-level global consistency,
but ignore the local consistency which plays a pivotal role in capturing
structural information for dense prediction tasks such as segmentation. In this
paper, we propose a PriorGuided Local (PGL) self-supervised model that learns
the region-wise local consistency in the latent feature space. Specifically, we
use the spatial transformations, which produce different augmented views of the
same image, as a prior to deduce the location relation between two views, which
is then used to align the feature maps of the same local region but being
extracted on two views. Next, we construct a local consistency loss to minimize
the voxel-wise discrepancy between the aligned feature maps. Thus, our PGL
model learns the distinctive representations of local regions, and hence is
able to retain structural information. This ability is conducive to downstream
segmentation tasks. We conducted an extensive evaluation on four public
computerized tomography (CT) datasets that cover 11 kinds of major human organs
and two tumors. The results indicate that using pre-trained PGL model to
initialize a downstream network leads to a substantial performance improvement
over both random initialization and the initialization with global
consistency-based models. Code and pre-trained weights will be made available
at: https://git.io/PGL.
</p>
<a href="http://arxiv.org/abs/2011.12640" target="_blank">arXiv:2011.12640</a> [<a href="http://arxiv.org/pdf/2011.12640" target="_blank">pdf</a>]

<h2>Auto Graph Encoder-Decoder for Model Compression and Network Acceleration. (arXiv:2011.12641v1 [cs.CV])</h2>
<h3>Sixing Yu, Arya Mazaheri, Ali Jannesari</h3>
<p>Model compression aims to deploy deep neural networks (DNN) to mobile devices
with limited computing power and storage resource. However, most of the
existing model compression methods rely on manually defined rules, which
requires domain expertise. In this paper, we propose an Auto Graph
encoder-decoder Model Compression (AGMC) method combined with graph neural
networks (GNN) and reinforcement learning (RL) to find the best compression
policy. We model the target DNN as a graph and use GNN to learn the embeddings
of the DNN automatically. In our experiments, we first compared our method with
rule-based DNN embedding methods to show the graph auto encoder-decoder's
effectiveness. Our learning-based DNN embedding achieved better performance and
a higher compression ratio with fewer search steps. Moreover, we evaluated the
AGMC on CIFAR-10 and ILSVRC-2012 datasets and compared handcrafted and
learning-based model compression approaches. Our method outperformed
handcrafted and learning-based methods on ResNet-56 with 3.6% and 1.8% higher
accuracy, respectively. Furthermore, we achieved a higher compression ratio
than state-of-the-art methods on MobileNet-V2 with just 0.93% accuracy loss.
</p>
<a href="http://arxiv.org/abs/2011.12641" target="_blank">arXiv:2011.12641</a> [<a href="http://arxiv.org/pdf/2011.12641" target="_blank">pdf</a>]

<h2>Quantifying Explainers of Graph Neural Networks in Computational Pathology. (arXiv:2011.12646v1 [cs.CV])</h2>
<h3>Guillaume Jaume, Pushpak Pati, Behzad Bozorgtabar, Antonio Foncubierta-Rodr&#xed;guez, Florinda Feroce, Anna Maria Anniciello, Tilman Rau, Jean-Philippe Thiran, Maria Gabrani, Orcun Goksel</h3>
<p>Explainability of deep learning methods is imperative to facilitate their
clinical adoption in digital pathology. However, popular deep learning methods
and explainability techniques (explainers) based on pixel-wise processing
disregard biological entities' notion, thus complicating comprehension by
pathologists. In this work, we address this by adopting biological entity-based
graph processing and graph explainers enabling explanations accessible to
pathologists. In this context, a major challenge becomes to discern meaningful
explainers, particularly in a standardized and quantifiable fashion. To this
end, we propose herein a set of novel quantitative metrics based on statistics
of class separability using pathologically measurable concepts to characterize
graph explainers. We employ the proposed metrics to evaluate three types of
graph explainers, namely the layer-wise relevance propagation, gradient-based
saliency, and graph pruning approaches, to explain Cell-Graph representations
for Breast Cancer Subtyping. The proposed metrics are also applicable in other
domains by using domain-specific intuitive concepts. We validate the
qualitative and quantitative findings on the BRACS dataset, a large cohort of
breast cancer RoIs, by expert pathologists.
</p>
<a href="http://arxiv.org/abs/2011.12646" target="_blank">arXiv:2011.12646</a> [<a href="http://arxiv.org/pdf/2011.12646" target="_blank">pdf</a>]

<h2>Feature space approximation for kernel-based supervised learning. (arXiv:2011.12651v1 [stat.ML])</h2>
<h3>Patrick Gel&#xdf;, Stefan Klus, Ingmar Schuster, Christof Sch&#xfc;tte</h3>
<p>We propose a method for the approximation of high- or even
infinite-dimensional feature vectors, which play an important role in
supervised learning. The goal is to reduce the size of the training data,
resulting in lower storage consumption and computational complexity.
Furthermore, the method can be regarded as a regularization technique, which
improves the generalizability of learned target functions. We demonstrate
significant improvements in comparison to the computation of data-driven
predictions involving the full training data set. The method is applied to
classification and regression problems from different application areas such as
image recognition, system identification, and oceanographic time series
analysis.
</p>
<a href="http://arxiv.org/abs/2011.12651" target="_blank">arXiv:2011.12651</a> [<a href="http://arxiv.org/pdf/2011.12651" target="_blank">pdf</a>]

<h2>Unsupervised learning of disentangled representations in deep restricted kernel machines with orthogonality constraints. (arXiv:2011.12659v1 [cs.LG])</h2>
<h3>Francesco Tonin, Panagiotis Patrinos, Johan A. K. Suykens</h3>
<p>We introduce Constr-DRKM, a deep kernel method for the unsupervised learning
of disentangled data representations. We propose augmenting the original deep
restricted kernel machine formulation for kernel PCA by orthogonality
constraints on the latent variables to promote disentanglement and to make it
possible to carry out optimization without first defining a stabilized
objective. After illustrating an end-to-end training procedure based on a
quadratic penalty optimization algorithm with warm start, we quantitatively
evaluate the proposed method's effectiveness in disentangled feature learning.
We demonstrate on four benchmark datasets that this approach performs similarly
overall to $\beta$-VAE on a number of disentanglement metrics when few training
points are available, while being less sensitive to randomness and
hyperparameter selection than $\beta$-VAE. We also present a deterministic
initialization of Constr-DRKM's training algorithm that significantly improves
the reproducibility of the results. Finally, we empirically evaluate and
discuss the role of the number of layers in the proposed methodology, examining
the influence of each principal component in every layer and showing that
components in lower layers act as local feature detectors capturing the broad
trends of the data distribution, while components in deeper layers use the
representation learned by previous layers and more accurately reproduce
higher-level features.
</p>
<a href="http://arxiv.org/abs/2011.12659" target="_blank">arXiv:2011.12659</a> [<a href="http://arxiv.org/pdf/2011.12659" target="_blank">pdf</a>]

<h2>Temporal Autoencoder with U-Net Style Skip-Connections for Frame Prediction. (arXiv:2011.12661v1 [cs.LG])</h2>
<h3>Jay Santokhi, Pankaj Daga, Joned Sarwar, Anna Jordan, Emil Hewage</h3>
<p>Finding sustainable and novel solutions to predict city-wide mobility
behaviour is an ever-growing problem given increased urban complexity and
growing populations. This paper seeks to address this by describing a traffic
frame prediction approach that uses Convolutional LSTMs to create a Temporal
Autoencoder with U-Net style skip-connections that marry together recurrent and
traditional computer vision techniques to capture spatio-temporal dependencies
at different scales without losing topological details of a given city.
Utilisation of Cyclical Learning Rates is also presented, improving training
efficiency by achieving lower loss scores in fewer epochs than standard
approaches.
</p>
<a href="http://arxiv.org/abs/2011.12661" target="_blank">arXiv:2011.12661</a> [<a href="http://arxiv.org/pdf/2011.12661" target="_blank">pdf</a>]

<h2>Bayesian Triplet Loss: Uncertainty Quantification in Image Retrieval. (arXiv:2011.12663v1 [cs.CV])</h2>
<h3>Frederik Warburg, Martin J&#xf8;rgensen, Javier Civera, S&#xf8;ren Hauberg</h3>
<p>Uncertainty quantification in image retrieval is crucial for downstream
decisions, yet it remains a challenging and largely unexplored problem. Current
methods for estimating uncertainties are poorly calibrated, computationally
expensive, or based on heuristics. We present a new method that views image
embeddings as stochastic features rather than deterministic features. Our two
main contributions are (1) a likelihood that matches the triplet constraint and
that evaluates the probability of an anchor being closer to a positive than a
negative; and (2) a prior over the feature space that justifies the
conventional l2 normalization. To ensure computational efficiency, we derive a
variational approximation of the posterior, called the Bayesian triplet loss,
that produces state-of-the-art uncertainty estimates and matches the predictive
performance of current state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2011.12663" target="_blank">arXiv:2011.12663</a> [<a href="http://arxiv.org/pdf/2011.12663" target="_blank">pdf</a>]

<h2>Batch Normalization Embeddings for Deep Domain Generalization. (arXiv:2011.12672v1 [cs.LG])</h2>
<h3>Mattia Seg&#xf9;, Alessio Tonioni, Federico Tombari</h3>
<p>Domain generalization aims at training machine learning models to perform
robustly across different and unseen domains. Several recent methods use
multiple datasets to train models to extract domain-invariant features, hoping
to generalize to unseen domains. Instead, first we explicitly train
domain-dependant representations by using ad-hoc batch normalization layers to
collect independent domain's statistics. Then, we propose to use these
statistics to map domains in a shared latent space, where membership to a
domain can be measured by means of a distance function. At test time, we
project samples from an unknown domain into the same space and infer properties
of their domain as a linear combination of the known ones. We apply the same
mapping strategy at training and test time, learning both a latent
representation and a powerful but lightweight ensemble model. We show a
significant increase in classification accuracy over current state-of-the-art
techniques on popular domain generalization benchmarks: PACS, Office-31 and
Office-Caltech.
</p>
<a href="http://arxiv.org/abs/2011.12672" target="_blank">arXiv:2011.12672</a> [<a href="http://arxiv.org/pdf/2011.12672" target="_blank">pdf</a>]

<h2>Adversarial Attack on Facial Recognition using Visible Light. (arXiv:2011.12680v1 [cs.CV])</h2>
<h3>Morgan Frearson, Kien Nguyen</h3>
<p>The use of deep learning for human identification and object detection is
becoming ever more prevalent in the surveillance industry. These systems have
been trained to identify human body's or faces with a high degree of accuracy.
However, there have been successful attempts to fool these systems with
different techniques called adversarial attacks. This paper presents a final
report for an adversarial attack using visible light on facial recognition
systems. The relevance of this research is to exploit the physical downfalls of
deep neural networks. This demonstration of weakness within these systems are
in hopes that this research will be used in the future to improve the training
models for object recognition. As results were gathered the project objectives
were adjusted to fit the outcomes. Because of this the following paper
initially explores an adversarial attack using infrared light before
readjusting to a visible light attack. A research outline on infrared light and
facial recognition are presented within. A detailed analyzation of the current
findings and possible future recommendations of the project are presented. The
challenges encountered are evaluated and a final solution is delivered. The
projects final outcome exhibits the ability to effectively fool recognition
systems using light.
</p>
<a href="http://arxiv.org/abs/2011.12680" target="_blank">arXiv:2011.12680</a> [<a href="http://arxiv.org/pdf/2011.12680" target="_blank">pdf</a>]

<h2>Reduced Reference Perceptual Quality Model and Application to Rate Control for 3D Point Cloud Compression. (arXiv:2011.12688v1 [cs.CV])</h2>
<h3>Qi Liu, Hui Yuan, Raouf Hamzaoui, Honglei Su, Junhui Hou, Huan Yang</h3>
<p>In rate-distortion optimization, the encoder settings are determined by
maximizing a reconstruction quality measure subject to a constraint on the bit
rate. One of the main challenges of this approach is to define a quality
measure that can be computed with low computational cost and which correlates
well with perceptual quality. While several quality measures that fulfil these
two criteria have been developed for images and video, no such one exists for
3D point clouds. We address this limitation for the video-based point cloud
compression (V-PCC) standard by proposing a linear perceptual quality model
whose variables are the V-PCC geometry and color quantization parameters and
whose coefficients can easily be computed from two features extracted from the
original 3D point cloud. Subjective quality tests with 400 compressed 3D point
clouds show that the proposed model correlates well with the mean opinion
score, outperforming state-of-the-art full reference objective measures in
terms of Spearman rank-order and Pearsons linear correlation coefficient.
Moreover, we show that for the same target bit rate, ratedistortion
optimization based on the proposed model offers higher perceptual quality than
rate-distortion optimization based on exhaustive search with a point-to-point
objective quality metric.
</p>
<a href="http://arxiv.org/abs/2011.12688" target="_blank">arXiv:2011.12688</a> [<a href="http://arxiv.org/pdf/2011.12688" target="_blank">pdf</a>]

<h2>DeepKoCo: Efficient latent planning with an invariant Koopman representation. (arXiv:2011.12690v1 [cs.LG])</h2>
<h3>Bas van der Heijden, Laura Ferranti, Jens Kober, Robert Babuska</h3>
<p>This paper presents DeepKoCo, a novel model-based agent that learns a latent
Koopman representation from images. This representation allows DeepKoCo to plan
efficiently using linear control methods, such as linear model predictive
control. Compared to traditional agents, DeepKoCo is invariant to
task-irrelevant dynamics, thanks to the use of a tailored lossy autoencoder
network that allows DeepKoCo to learn latent dynamics that reconstruct and
predict only observed costs, rather than all observed dynamics. As our results
show, DeepKoCo achieves a similar final performance as traditional model-free
methods on complex control tasks, while being considerably more robust to
distractor dynamics, making the proposed agent more amenable for real-life
applications.
</p>
<a href="http://arxiv.org/abs/2011.12690" target="_blank">arXiv:2011.12690</a> [<a href="http://arxiv.org/pdf/2011.12690" target="_blank">pdf</a>]

<h2>Towards Playing Full MOBA Games with Deep Reinforcement Learning. (arXiv:2011.12692v1 [cs.AI])</h2>
<h3>Deheng Ye, Guibin Chen, Wen Zhang, Sheng Chen, Bo Yuan, Bo Liu, Jia Chen, Zhao Liu, Fuhao Qiu, Hongsheng Yu, Yinyuting Yin, Bei Shi, Liang Wang, Tengfei Shi, Qiang Fu, Wei Yang, Lanxiao Huang, Wei Liu</h3>
<p>MOBA games, e.g., Honor of Kings, League of Legends, and Dota 2, pose grand
challenges to AI systems such as multi-agent, enormous state-action space,
complex action control, etc. Developing AI for playing MOBA games has raised
much attention accordingly. However, existing work falls short in handling the
raw game complexity caused by the explosion of agent combinations, i.e.,
lineups, when expanding the hero pool in case that OpenAI's Dota AI limits the
play to a pool of only 17 heroes. As a result, full MOBA games without
restrictions are far from being mastered by any existing AI system. In this
paper, we propose a MOBA AI learning paradigm that methodologically enables
playing full MOBA games with deep reinforcement learning. Specifically, we
develop a combination of novel and existing learning techniques, including
off-policy adaption, multi-head value estimation, curriculum self-play
learning, policy distillation, and Monte-Carlo tree-search, in training and
playing a large pool of heroes, meanwhile addressing the scalability issue
skillfully. Tested on Honor of Kings, a popular MOBA game, we show how to build
superhuman AI agents that can defeat top esports players. The superiority of
our AI is demonstrated by the first large-scale performance test of MOBA AI
agent in the literature.
</p>
<a href="http://arxiv.org/abs/2011.12692" target="_blank">arXiv:2011.12692</a> [<a href="http://arxiv.org/pdf/2011.12692" target="_blank">pdf</a>]

<h2>SLARM: Simultaneous Localization and Radio Mapping for Communication-aware Connected Robot. (arXiv:2011.12702v1 [cs.RO])</h2>
<h3>Xinyu Gao, Yuanwei Liu, Xidong Mu</h3>
<p>A novel simultaneous localization and radio mapping (SLARM) framework for
communication-aware connected robots in the unknown indoor environment is
proposed, where the simultaneous localization and mapping (SLAM) algorithm and
the global geographic map recovery (GGMR) algorithm are leveraged to
simultaneously construct a geographic map and a radio map named a channel power
gain map. Specifically, the geographic map contains the information of a
precise layout of obstacles and passable regions, and the radio map
characterizes the position-dependent maximum expected channel power gain
between the access point and the connected robot. Numerical results show that:
1) The pre-defined resolution in the SLAM algorithm and the proposed GGMR
algorithm significantly affect the accuracy of the constructed radio map; and
2) The accuracy of radio map constructed by the SLARM framework is more than
78.78% when the resolution value smaller than 0.15m, and the accuracy reaches
91.95% when the resolution value is pre-defined as 0.05m.
</p>
<a href="http://arxiv.org/abs/2011.12702" target="_blank">arXiv:2011.12702</a> [<a href="http://arxiv.org/pdf/2011.12702" target="_blank">pdf</a>]

<h2>Trajectory and Passive Beamforming Design for IRS-aided Multi-Robot NOMA Indoor Networks. (arXiv:2011.12703v1 [cs.RO])</h2>
<h3>Xinyu Gao, Yuanwei Liu, Xidong Mu</h3>
<p>A novel intelligent reflecting surface (IRS)-aided multi-robot network is
proposed, where multiple mobile wheeled robots are served by an access point
(AP) through non-orthogonal multiple access (NOMA). The goal is to maximize the
sum-rate of all robots by jointly optimizing trajectories and NOMA decoding
orders of robots, reflecting coefficients of the IRS, and the power allocation
of the AP, subject to the quality of service (QoS) of each robot. To tackle
this problem, a dueling double deep Q-network (D^{3}QN) based algorithm is
invoked for jointly determining the phase shift matrix and robots'
trajectories. Specifically, the trajectories for robots contain a set of local
optimal positions, which reveals that robots make the optimal decision at each
step. Numerical results demonstrated that the proposed D^{3}QN algorithm
outperforms the conventional algorithm, while the performance of IRS-NOMA
network is better than the orthogonal multiple access (OMA) network.
</p>
<a href="http://arxiv.org/abs/2011.12703" target="_blank">arXiv:2011.12703</a> [<a href="http://arxiv.org/pdf/2011.12703" target="_blank">pdf</a>]

<h2>Prediction of neonatal mortality in Sub-Saharan African countries using data-level linkage of multiple surveys. (arXiv:2011.12707v1 [cs.LG])</h2>
<h3>Girmaw Abebe Tadesse, Celia Cintas, Skyler Speakman, Komminist Weldemariam</h3>
<p>Existing datasets available to address crucial problems, such as child
mortality and family planning discontinuation in developing countries, are not
ample for data-driven approaches. This is partly due to disjoint data
collection efforts employed across locations, times, and variations of
modalities. On the other hand, state-of-the-art methods for small data problem
are confined to image modalities. In this work, we proposed a data-level
linkage of disjoint surveys across Sub-Saharan African countries to improve
prediction performance of neonatal death and provide cross-domain
explainability.
</p>
<a href="http://arxiv.org/abs/2011.12707" target="_blank">arXiv:2011.12707</a> [<a href="http://arxiv.org/pdf/2011.12707" target="_blank">pdf</a>]

<h2>Resonance: Replacing Software Constants with Context-Aware Models in Real-time Communication. (arXiv:2011.12715v1 [cs.AI])</h2>
<h3>Jayant Gupchup, Ashkan Aazami, Yaran Fan, Senja Filipi, Tom Finley, Scott Inglis, Marcus Asteborg, Luke Caroll, Rajan Chari, Markus Cozowicz, Vishak Gopal, Vinod Prakash, Sasikanth Bendapudi, Jack Gerrits, Eric Lau, Huazhou Liu, Marco Rossi, Dima Slobodianyk, Dmitri Birjukov, Matty Cooper, Nilesh Javar, Dmitriy Perednya, Sriram Srinivasan, John Langford, Ross Cutler, Johannes Gehrke</h3>
<p>Large software systems tune hundreds of 'constants' to optimize their runtime
performance. These values are commonly derived through intuition, lab tests, or
A/B tests. A 'one-size-fits-all' approach is often sub-optimal as the best
value depends on runtime context. In this paper, we provide an experimental
approach to replace constants with learned contextual functions for Skype - a
widely used real-time communication (RTC) application. We present Resonance, a
system based on contextual bandits (CB). We describe experiences from three
real-world experiments: applying it to the audio, video, and transport
components in Skype. We surface a unique and practical challenge of performing
machine learning (ML) inference in large software systems written using
encapsulation principles. Finally, we open-source FeatureBroker, a library to
reduce the friction in adopting ML models in such development environments
</p>
<a href="http://arxiv.org/abs/2011.12715" target="_blank">arXiv:2011.12715</a> [<a href="http://arxiv.org/pdf/2011.12715" target="_blank">pdf</a>]

<h2>Distributed Reinforcement Learning is a Dataflow Problem. (arXiv:2011.12719v1 [cs.LG])</h2>
<h3>Eric Liang, Zhanghao Wu, Michael Luo, Sven Mika, Ion Stoica</h3>
<p>Researchers and practitioners in the field of reinforcement learning (RL)
frequently leverage parallel computation, which has led to a plethora of new
algorithms and systems in the last few years. In this paper, we re-examine the
challenges posed by distributed RL and try to view it through the lens of an
old idea: distributed dataflow. We show that viewing RL as a dataflow problem
leads to highly composable and performant implementations. We propose AnonFlow,
a hybrid actor-dataflow programming model for distributed RL, and validate its
practicality by porting the full suite of algorithms in AnonLib, a
widely-adopted distributed RL library.
</p>
<a href="http://arxiv.org/abs/2011.12719" target="_blank">arXiv:2011.12719</a> [<a href="http://arxiv.org/pdf/2011.12719" target="_blank">pdf</a>]

<h2>Attention Aware Cost Volume Pyramid Based Multi-view Stereo Network for 3D Reconstruction. (arXiv:2011.12722v1 [cs.CV])</h2>
<h3>Anzhu Yu, Wenyue Guo, Bing Liu, Xin Chen, Xin Wang, Xuefeng Cao, Bingchuan Jiang</h3>
<p>We present an efficient multi-view stereo (MVS) network for 3D reconstruction
from multiview images. While previous learning based reconstruction approaches
performed quite well, most of them estimate depth maps at a fixed resolution
using plane sweep volumes with a fixed depth hypothesis at each plane, which
requires densely sampled planes for desired accuracy and therefore is difficult
to achieve high resolution depth maps. In this paper we introduce a
coarseto-fine depth inference strategy to achieve high resolution depth. This
strategy estimates the depth map at coarsest level, while the depth maps at
finer levels are considered as the upsampled depth map from previous level with
pixel-wise depth residual. Thus, we narrow the depth searching range with
priori information from previous level and construct new cost volumes from the
pixel-wise depth residual to perform depth map refinement. Then the final depth
map could be achieved iteratively since all the parameters are shared between
different levels. At each level, the self-attention layer is introduced to the
feature extraction block for capturing the long range dependencies for depth
inference task, and the cost volume is generated using similarity measurement
instead of the variance based methods used in previous work. Experiments were
conducted on both the DTU benchmark dataset and recently released BlendedMVS
dataset. The results demonstrated that our model could outperform most
state-of-the-arts (SOTA) methods. The codebase of this project is at
https://github.com/ArthasMil/AACVP-MVSNet.
</p>
<a href="http://arxiv.org/abs/2011.12722" target="_blank">arXiv:2011.12722</a> [<a href="http://arxiv.org/pdf/2011.12722" target="_blank">pdf</a>]

<h2>On limitations of learning algorithms in competitive environments. (arXiv:2011.12728v1 [cs.AI])</h2>
<h3>Alexander Y Klimenko, Dimitri A Klimenko</h3>
<p>We discuss conceptual limitations of generic learning algorithms acting in a
competitive environment, and demonstrate that they are subject to constraints
that are analogous to the constraints on knowledge imposed by the famous
theorems of G\"odel, Church and Turing.
</p>
<a href="http://arxiv.org/abs/2011.12728" target="_blank">arXiv:2011.12728</a> [<a href="http://arxiv.org/pdf/2011.12728" target="_blank">pdf</a>]

<h2>Simple statistical methods for unsupervised brain anomaly detection on MRI are competitive to deep learning methods. (arXiv:2011.12735v1 [cs.CV])</h2>
<h3>Victor Saase, Holger Wenz, Thomas Ganslandt, Christoph Groden, M&#xe1;t&#xe9; E. Maros</h3>
<p>Statistical analysis of magnetic resonance imaging (MRI) can help
radiologists to detect pathologies that are otherwise likely to be missed. Deep
learning (DL) has shown promise in modeling complex spatial data for brain
anomaly detection. However, DL models have major deficiencies: they need large
amounts of high-quality training data, are difficult to design and train and
are sensitive to subtle changes in scanning protocols and hardware. Here, we
show that also simple statistical methods such as voxel-wise (baseline and
covariance) models and a linear projection method using spatial patterns can
achieve DL-equivalent (3D convolutional autoencoder) performance in
unsupervised pathology detection. All methods were trained (N=395) and compared
(N=44) on a novel, expert-curated multiparametric (8 sequences) head MRI
dataset of healthy and pathological cases, respectively. We show that these
simple methods can be more accurate in detecting small lesions and are
considerably easier to train and comprehend. The methods were quantitatively
compared using AUC and average precision and evaluated qualitatively on
clinical use cases comprising brain atrophy, tumors (small metastases) and
movement artefacts. Our results demonstrate that while DL methods may be
useful, they should show a sufficiently large performance improvement over
simpler methods to justify their usage. Thus, simple statistical methods should
provide the baseline for benchmarks. Source code and trained models are
available on GitHub (https://github.com/vsaase/simpleBAD).
</p>
<a href="http://arxiv.org/abs/2011.12735" target="_blank">arXiv:2011.12735</a> [<a href="http://arxiv.org/pdf/2011.12735" target="_blank">pdf</a>]

<h2>Ranking Deep Learning Generalization using Label Variation in Latent Geometry Graphs. (arXiv:2011.12737v1 [cs.LG])</h2>
<h3>Carlos Lassance, Louis B&#xe9;thune, Myriam Bontonou, Mounia Hamidouche, Vincent Gripon</h3>
<p>Measuring the generalization performance of a Deep Neural Network (DNN)
without relying on a validation set is a difficult task. In this work, we
propose exploiting Latent Geometry Graphs (LGGs) to represent the latent spaces
of trained DNN architectures. Such graphs are obtained by connecting samples
that yield similar latent representations at a given layer of the considered
DNN. We then obtain a generalization score by looking at how strongly connected
are samples of distinct classes in LGGs. This score allowed us to rank 3rd on
the NeurIPS 2020 Predicting Generalization in Deep Learning (PGDL) competition.
</p>
<a href="http://arxiv.org/abs/2011.12737" target="_blank">arXiv:2011.12737</a> [<a href="http://arxiv.org/pdf/2011.12737" target="_blank">pdf</a>]

<h2>Deep Magnification-Arbitrary Upsampling over 3D Point Clouds. (arXiv:2011.12745v1 [cs.CV])</h2>
<h3>Yue Qian, Junhui Hou, Sam Kwong, Ying He</h3>
<p>This paper addresses the problem of generating dense point clouds from given
sparse point clouds to model the underlying geometric structures of
objects/scenes. To tackle this challenging issue, we propose a novel end-to-end
learning based framework, namely MAPU-Net. Specifically, by taking advantage of
the linear approximation theorem, we first formulate the problem explicitly,
which boils down to determining the interpolation weights and high-order
approximation errors. Then, we design a lightweight neural network to
adaptively learn unified and sorted interpolation weights and normal-guided
displacements, by analyzing the local geometry of the input point cloud.
MAPU-Net can be interpreted by the explicit formulation, and thus is more
memory-efficient than existing ones. In sharp contrast to the existing methods
that work only for a pre-defined and fixed upsampling factor, MAPU-Net, a
single neural network with one-time training, can handle an arbitrary
upsampling factor, which is highly desired in real-world applications. In
addition, we propose a simple yet effective training strategy to drive such a
flexible ability. Extensive experiments on both synthetic and real world data
demonstrate the superiority of the proposed MAPU-Net over state-of-the-art
methods both quantitatively and qualitatively. To the best of our knowledge,
this is the first end-to-end learning based method that is capable of achieving
magnification-arbitrary upsampling over 3D point clouds.
</p>
<a href="http://arxiv.org/abs/2011.12745" target="_blank">arXiv:2011.12745</a> [<a href="http://arxiv.org/pdf/2011.12745" target="_blank">pdf</a>]

<h2>Symmetry-Aware Actor-Critic for 3D Molecular Design. (arXiv:2011.12747v1 [stat.ML])</h2>
<h3>Gregor N. C. Simm, Robert Pinsler, G&#xe1;bor Cs&#xe1;nyi, Jos&#xe9; Miguel Hern&#xe1;ndez-Lobato</h3>
<p>Automating molecular design using deep reinforcement learning (RL) has the
potential to greatly accelerate the search for novel materials. Despite recent
progress on leveraging graph representations to design molecules, such methods
are fundamentally limited by the lack of three-dimensional (3D) information. In
light of this, we propose a novel actor-critic architecture for 3D molecular
design that can generate molecular structures unattainable with previous
approaches. This is achieved by exploiting the symmetries of the design process
through a rotationally covariant state-action representation based on a
spherical harmonics series expansion. We demonstrate the benefits of our
approach on several 3D molecular design tasks, where we find that building in
such symmetries significantly improves generalization and the quality of
generated molecules.
</p>
<a href="http://arxiv.org/abs/2011.12747" target="_blank">arXiv:2011.12747</a> [<a href="http://arxiv.org/pdf/2011.12747" target="_blank">pdf</a>]

<h2>Transfer Learning for Aided Target Recognition: Comparing Deep Learning to other Machine Learning Approaches. (arXiv:2011.12762v1 [cs.CV])</h2>
<h3>Samuel Rivera, Olga Mendoza-Schrock, Ashley Diehl</h3>
<p>Aided target recognition (AiTR), the problem of classifying objects from
sensor data, is an important problem with applications across industry and
defense. While classification algorithms continue to improve, they often
require more training data than is available or they do not transfer well to
settings not represented in the training set. These problems are mitigated by
transfer learning (TL), where knowledge gained in a well-understood source
domain is transferred to a target domain of interest. In this context, the
target domain could represents a poorly-labeled dataset, a different sensor, or
an altogether new set of classes to identify.

While TL for classification has been an active area of machine learning (ML)
research for decades, transfer learning within a deep learning framework
remains a relatively new area of research. Although deep learning (DL) provides
exceptional modeling flexibility and accuracy on recent real world problems,
open questions remain regarding how much transfer benefit is gained by using DL
versus other ML architectures. Our goal is to address this shortcoming by
comparing transfer learning within a DL framework to other ML approaches across
transfer tasks and datasets. Our main contributions are: 1) an empirical
analysis of DL and ML algorithms on several transfer tasks and domains
including gene expressions and satellite imagery, and 2) a discussion of the
limitations and assumptions of TL for aided target recognition -- both for DL
and ML in general. We close with a discussion of future directions for DL
transfer.
</p>
<a href="http://arxiv.org/abs/2011.12762" target="_blank">arXiv:2011.12762</a> [<a href="http://arxiv.org/pdf/2011.12762" target="_blank">pdf</a>]

<h2>Regret-optimal measurement-feedback control. (arXiv:2011.12785v1 [cs.LG])</h2>
<h3>Gautam Goel, Babak Hassobi</h3>
<p>We consider measurement-feedback control in linear dynamical systems from the
perspective of regret minimization. Unlike most prior work in this area, we
focus on the problem of designing an online controller which competes with the
optimal dynamic sequence of control actions selected in hindsight, instead of
the best controller in some specific class of controllers. This formulation of
regret is attractive when the environment changes over time and no single
controller achieves good performance over the entire time horizon. We show that
in the measurement-feedback setting, unlike in the full-information setting,
there is no single offline controller which outperforms every other offline
controller on every disturbance, and propose a new $H_2$-optimal offline
controller as a benchmark for the online controller to compete against. We show
that the corresponding regret-optimal online controller can be found via a
novel reduction to the classical Nehari problem from robust control and present
a tight data-dependent bound on its regret.
</p>
<a href="http://arxiv.org/abs/2011.12785" target="_blank">arXiv:2011.12785</a> [<a href="http://arxiv.org/pdf/2011.12785" target="_blank">pdf</a>]

<h2>Face recognition using PCA integrated with Delaunay triangulation. (arXiv:2011.12786v1 [cs.CV])</h2>
<h3>Kavan Adeshara, Vinayak Elangovan</h3>
<p>Face Recognition is most used for biometric user authentication that
identifies a user based on his or her facial features. The system is in high
demand, as it is used by many businesses and employed in many devices such as
smartphones and surveillance cameras. However, one frequent problem that is
still observed in this user-verification method is its accuracy rate. Numerous
approaches and algorithms have been experimented to improve the stated flaw of
the system. This research develops one such algorithm that utilizes a
combination of two different approaches. Using the concepts from Linear Algebra
and computational geometry, the research examines the integration of Principal
Component Analysis with Delaunay Triangulation; the method triangulates a set
of face landmark points and obtains eigenfaces of the provided images. It
compares the algorithm with traditional PCA and discusses the inclusion of
different face landmark points to deliver an effective recognition rate.
</p>
<a href="http://arxiv.org/abs/2011.12786" target="_blank">arXiv:2011.12786</a> [<a href="http://arxiv.org/pdf/2011.12786" target="_blank">pdf</a>]

<h2>Fast Region Proposal Learning for Object Detection for Robotics. (arXiv:2011.12790v1 [cs.CV])</h2>
<h3>Federico Ceola, Elisa Maiettini, Giulia Pasquale, Lorenzo Rosasco, Lorenzo Natale</h3>
<p>Object detection is a fundamental task for robots to operate in unstructured
environments. Today, there are several deep learning algorithms that solve this
task with remarkable performance. Unfortunately, training such systems requires
several hours of GPU time. For robots, to successfully adapt to changes in the
environment or learning new objects, it is also important that object detectors
can be re-trained in a short amount of time. A recent method [1] proposes an
architecture that leverages on the powerful representation of deep learning
descriptors, while permitting fast adaptation time. Leveraging on the natural
decomposition of the task in (i) regions candidate generation, (ii) feature
extraction and (iii) regions classification, this method performs fast
adaptation of the detector, by only re-training the classification layer. This
shortens training time while maintaining state-of-the-art performance. In this
paper, we firstly demonstrate that a further boost in accuracy can be obtained
by adapting, in addition, the regions candidate generation on the task at hand.
Secondly, we extend the object detection system presented in [1] with the
proposed fast learning approach, showing experimental evidence on the
improvement provided in terms of speed and accuracy on two different robotics
datasets. The code to reproduce the experiments is publicly available on
GitHub.
</p>
<a href="http://arxiv.org/abs/2011.12790" target="_blank">arXiv:2011.12790</a> [<a href="http://arxiv.org/pdf/2011.12790" target="_blank">pdf</a>]

<h2>StyleSpace Analysis: Disentangled Controls for StyleGAN Image Generation. (arXiv:2011.12799v1 [cs.CV])</h2>
<h3>Zongze Wu, Dani Lischinski, Eli Shechtman</h3>
<p>We explore and analyze the latent style space of StyleGAN2, a
state-of-the-art architecture for image generation, using models pretrained on
several different datasets. We first show that StyleSpace, the space of
channel-wise style parameters, is significantly more disentangled than the
other intermediate latent spaces explored by previous works. Next, we describe
a method for discovering a large collection of style channels, each of which is
shown to control a distinct visual attribute in a highly localized and
disentangled manner. Third, we propose a simple method for identifying style
channels that control a specific attribute, using a pretrained classifier or a
small number of example images. Manipulation of visual attributes via these
StyleSpace controls is shown to be better disentangled than via those proposed
in previous works. To show this, we make use of a newly proposed Attribute
Dependency metric. Finally, we demonstrate the applicability of StyleSpace
controls to the manipulation of real images. Our findings pave the way to
semantically meaningful and well-disentangled image manipulations via simple
and intuitive interfaces.
</p>
<a href="http://arxiv.org/abs/2011.12799" target="_blank">arXiv:2011.12799</a> [<a href="http://arxiv.org/pdf/2011.12799" target="_blank">pdf</a>]

<h2>Fast Object Segmentation Learning with Kernel-based Methods for Robotics. (arXiv:2011.12805v1 [cs.CV])</h2>
<h3>Federico Ceola, Elisa Maiettini, Giulia Pasquale, Lorenzo Rosasco, Lorenzo Natale</h3>
<p>Object segmentation is a key component in the visual system of a robot that
performs tasks like grasping and object manipulation, especially in presence of
occlusions. Like many other Computer Vision tasks, the adoption of deep
architectures has made available algorithms that perform this task with
remarkable performance. However, adoption of such algorithms in robotics is
hampered by the fact that training requires large amount of computing time and
it cannot be performed on-line. In this work, we propose a novel architecture
for object segmentation, that overcomes this problem and provides comparable
performance in a fraction of the time required by the state-of-the-art methods.
Our approach is based on a pre-trained Mask R-CNN, in which various layers have
been replaced with a set of classifiers and regressors that are retrained for a
new task. We employ an efficient Kernel-based method that allows for fast
training on large scale problems. Our approach is validated on the YCB-Video
dataset which is widely adopted in the Computer Vision and Robotics community,
demonstrating that we can achieve and even surpass performance of the
state-of-the-art, with a significant reduction (${\sim}6\times$) of the
training time. The code will be released upon acceptance.
</p>
<a href="http://arxiv.org/abs/2011.12805" target="_blank">arXiv:2011.12805</a> [<a href="http://arxiv.org/pdf/2011.12805" target="_blank">pdf</a>]

<h2>Interpreting U-Nets via Task-Driven Multiscale Dictionary Learning. (arXiv:2011.12815v1 [cs.CV])</h2>
<h3>Tianlin Liu, Anadi Chaman, David Belius, Ivan Dokmani&#x107;</h3>
<p>U-Nets have been tremendously successful in many imaging inverse problems. In
an effort to understand the source of this success, we show that one can reduce
a U-Net to a tractable, well-understood sparsity-driven dictionary model while
retaining its strong empirical performance. We achieve this by extracting a
certain multiscale convolutional dictionary from the standard U-Net. This
dictionary imitates the structure of the U-Net in its convolution,
scale-separation, and skip connection aspects, while doing away with the
nonlinear parts. We show that this model can be trained in a task-driven
dictionary learning framework and yield comparable results to standard U-Nets
on a number of relevant tasks, including CT and MRI reconstruction. These
results suggest that the success of the U-Net may be explained mainly by its
multiscale architecture and the induced sparse representation.
</p>
<a href="http://arxiv.org/abs/2011.12815" target="_blank">arXiv:2011.12815</a> [<a href="http://arxiv.org/pdf/2011.12815" target="_blank">pdf</a>]

<h2>Attention-Based Learning on Molecular Ensembles. (arXiv:2011.12820v1 [cs.LG])</h2>
<h3>Kangway V. Chuang, Michael J. Keiser</h3>
<p>The three-dimensional shape and conformation of small-molecule ligands are
critical for biomolecular recognition, yet encoding 3D geometry has not
improved ligand-based virtual screening approaches. We describe an end-to-end
deep learning approach that operates directly on small-molecule conformational
ensembles and identifies key conformational poses of small-molecules. Our
networks leverage two levels of representation learning: 1) individual
conformers are first encoded as spatial graphs using a graph neural network,
and 2) sampled conformational ensembles are represented as sets using an
attention mechanism to aggregate over individual instances. We demonstrate the
feasibility of this approach on a simple task based on bidentate coordination
of biaryl ligands, and show how attention-based pooling can elucidate key
conformational poses in tasks based on molecular geometry. This work
illustrates how set-based learning approaches may be further developed for
small molecule-based virtual screening.
</p>
<a href="http://arxiv.org/abs/2011.12820" target="_blank">arXiv:2011.12820</a> [<a href="http://arxiv.org/pdf/2011.12820" target="_blank">pdf</a>]

<h2>All You Need is a Good Functional Prior for Bayesian Deep Learning. (arXiv:2011.12829v1 [stat.ML])</h2>
<h3>Ba-Hien Tran, Simone Rossi, Dimitrios Milios, Maurizio Filippone</h3>
<p>The Bayesian treatment of neural networks dictates that a prior distribution
is specified over their weight and bias parameters. This poses a challenge
because modern neural networks are characterized by a large number of
parameters, and the choice of these priors has an uncontrolled effect on the
induced functional prior, which is the distribution of the functions obtained
by sampling the parameters from their prior distribution. We argue that this is
a hugely limiting aspect of Bayesian deep learning, and this work tackles this
limitation in a practical and effective way. Our proposal is to reason in terms
of functional priors, which are easier to elicit, and to "tune" the priors of
neural network parameters in a way that they reflect such functional priors.
Gaussian processes offer a rigorous framework to define prior distributions
over functions, and we propose a novel and robust framework to match their
prior with the functional prior of neural networks based on the minimization of
their Wasserstein distance. We provide vast experimental evidence that coupling
these priors with scalable Markov chain Monte Carlo sampling offers
systematically large performance improvements over alternative choices of
priors and state-of-the-art approximate Bayesian deep learning approaches. We
consider this work a considerable step in the direction of making the
long-standing challenge of carrying out a fully Bayesian treatment of neural
networks, including convolutional neural networks, a concrete possibility.
</p>
<a href="http://arxiv.org/abs/2011.12829" target="_blank">arXiv:2011.12829</a> [<a href="http://arxiv.org/pdf/2011.12829" target="_blank">pdf</a>]

<h2>Enhanced 3DMM Attribute Control via Synthetic Dataset Creation Pipeline. (arXiv:2011.12833v1 [cs.CV])</h2>
<h3>Wonwoong Cho, Inyeop Lee, David Inouye</h3>
<p>While facial attribute manipulation of 2D images via Generative Adversarial
Networks (GANs) has become common in computer vision and graphics due to its
many practical uses, research on 3D attribute manipulation is relatively
undeveloped. Existing 3D attribute manipulation methods are limited because the
same semantic changes are applied to every 3D face. The key challenge for
developing better 3D attribute control methods is the lack of paired training
data in which one attribute is changed while other attributes are held
fixed---e.g., a pair of 3D faces where one is male and the other is female but
all other attributes, such as race and expression, are the same. To overcome
this challenge, we design a novel pipeline for generating paired 3D faces by
harnessing the power of GANs. On top of this pipeline, we then propose an
enhanced non-linear 3D conditional attribute controller that increases the
precision and diversity of 3D attribute control compared to existing methods.
We demonstrate the validity of our dataset creation pipeline and the superior
performance of our conditional attribute controller via quantitative and
qualitative evaluations.
</p>
<a href="http://arxiv.org/abs/2011.12833" target="_blank">arXiv:2011.12833</a> [<a href="http://arxiv.org/pdf/2011.12833" target="_blank">pdf</a>]

<h2>Image Inpainting with Contextual Reconstruction Loss. (arXiv:2011.12836v1 [cs.CV])</h2>
<h3>Yu Zeng, Zhe Lin, Huchuan Lu, Vishal M. Patel</h3>
<p>Convolutional neural networks (CNNs) have been observed to be inefficient in
propagating information across distant spatial positions in images. Recent
studies in image inpainting attempt to overcome this issue by explicitly
searching reference regions throughout the entire image to fill the features
from reference regions in the missing regions. This operation can be
implemented as contextual attention layer (CA layer) \cite{yu2018generative},
which has been widely used in many deep learning-based methods. However, it
brings significant computational overhead as it computes the pair-wise
similarity of feature patches at every spatial position. Also, it often fails
to find proper reference regions due to the lack of supervision in terms of the
correspondence between missing regions and known regions. We propose a novel
contextual reconstruction loss (CR loss) to solve these problems. First, a
criterion of searching reference region is designed based on minimizing
reconstruction and adversarial losses corresponding to the searched reference
and the ground-truth image. Second, unlike previous approaches which integrate
the computationally heavy patch searching and replacement operation in the
inpainting model, CR loss encourages a vanilla CNN to simulate this behavior
during training, thus no extra computations are required during inference.
Experimental results demonstrate that the proposed inpainting model with the CR
loss compares favourably against the state-of-the-arts in terms of quantitative
and visual performance. Code is available at
\url{https://github.com/zengxianyu/crfill}.
</p>
<a href="http://arxiv.org/abs/2011.12836" target="_blank">arXiv:2011.12836</a> [<a href="http://arxiv.org/pdf/2011.12836" target="_blank">pdf</a>]

<h2>High-Level Description of Robot Architecture. (arXiv:2011.12837v1 [cs.RO])</h2>
<h3>Sabah Al-Fedaghi, Manar AlSaraf</h3>
<p>Architectural Description (AD) is the backbone that facilitates the
implementation and validation of robotic systems. In general, current
high-level ADs reflect great variation and lead to various difficulties,
including mixing ADs with implementation issues. They lack the qualities of
being systematic and coherent, as well as lacking technical-related forms
(e.g., icons of faces, computer screens). Additionally, a variety of languages
exist for eliciting requirements, such as object-oriented analysis methods
susceptible to inconsistency (e.g., those using multiple diagrams in UML and
SysML). In this paper, we orient our research toward a more generic
conceptualization of ADs in robotics. We apply a new modeling methodology,
namely the Thinging Machine (TM), to describe the architecture in robotic
systems. The focus of such an application is on high-level specification, which
is one important aspect for realizing the design and implementation in such
systems. TM modeling can be utilized in documentation and communication and as
the first step in the system s design phase. Accordingly, sample robot
architectures are re-expressed in terms of TM, thus developing (1) a static
model that captures the robot s atemporal aspects, (2) a dynamic model that
identifies states, and (3) a behavioral model that specifies the chronology of
events in the system. This result shows a viable approach in robot modeling
that determines a robot system s behavior through its static description.
</p>
<a href="http://arxiv.org/abs/2011.12837" target="_blank">arXiv:2011.12837</a> [<a href="http://arxiv.org/pdf/2011.12837" target="_blank">pdf</a>]

<h2>Deep-learning coupled with novel classification method to classify the urban environment of the developing world. (arXiv:2011.12847v1 [cs.CV])</h2>
<h3>Qianwei Cheng, AKM Mahbubur Rahman, Anis Sarker, Abu Bakar Siddik Nayem, Ovi Paul, Amin Ahsan Ali, M Ashraful Amin, Ryosuke Shibasaki, Moinul Zaber</h3>
<p>Rapid globalization and the interdependence of humanity that engender
tremendous in-flow of human migration towards the urban spaces. With advent of
high definition satellite images, high resolution data, computational methods
such as deep neural network, capable hardware; urban planning is seeing a
paradigm shift. Legacy data on urban environments are now being complemented
with high-volume, high-frequency data. In this paper we propose a novel
classification method that is readily usable for machine analysis and show
applicability of the methodology on a developing world setting. The
state-of-the-art is mostly dominated by classification of building structures,
building types etc. and largely represents the developed world which are
insufficient for developing countries such as Bangladesh where the surrounding
is crucial for the classification. Moreover, the traditional methods propose
small-scale classifications, which give limited information with poor
scalability and are slow to compute. We categorize the urban area in terms of
informal and formal spaces taking the surroundings into account. 50 km x 50 km
Google Earth image of Dhaka, Bangladesh was visually annotated and categorized
by an expert. The classification is based broadly on two dimensions:
urbanization and the architectural form of urban environment. Consequently, the
urban space is divided into four classes: 1) highly informal; 2) moderately
informal; 3) moderately formal; and 4) highly formal areas. In total 16
sub-classes were identified. For semantic segmentation, Google's DeeplabV3+
model was used which increases the field of view of the filters to incorporate
larger context. Image encompassing 70% of the urban space was used for training
and the remaining 30% was used for testing and validation. The model is able to
segment with 75% accuracy and 60% Mean IoU.
</p>
<a href="http://arxiv.org/abs/2011.12847" target="_blank">arXiv:2011.12847</a> [<a href="http://arxiv.org/pdf/2011.12847" target="_blank">pdf</a>]

<h2>Relation3DMOT: Exploiting Deep Affinity for 3D Multi-Object Tracking from View Aggregation. (arXiv:2011.12850v1 [cs.CV])</h2>
<h3>Can Chen, Luca Zanotti Fragonara, Antonios Tsourdos</h3>
<p>Autonomous systems need to localize and track surrounding objects in 3D space
for safe motion planning. As a result, 3D multi-object tracking (MOT) plays a
vital role in autonomous navigation. Most MOT methods use a
tracking-by-detection pipeline, which includes object detection and data
association processing. However, many approaches detect objects in 2D RGB
sequences for tracking, which is lack of reliability when localizing objects in
3D space. Furthermore, it is still challenging to learn discriminative features
for temporally-consistent detection in different frames, and the affinity
matrix is normally learned from independent object features without considering
the feature interaction between detected objects in the different frames. To
settle these problems, We firstly employ a joint feature extractor to fuse the
2D and 3D appearance features captured from both 2D RGB images and 3D point
clouds respectively, and then propose a novel convolutional operation, named
RelationConv, to better exploit the correlation between each pair of objects in
the adjacent frames, and learn a deep affinity matrix for further data
association. We finally provide extensive evaluation to reveal that our
proposed model achieves state-of-the-art performance on KITTI tracking
benchmark.
</p>
<a href="http://arxiv.org/abs/2011.12850" target="_blank">arXiv:2011.12850</a> [<a href="http://arxiv.org/pdf/2011.12850" target="_blank">pdf</a>]

<h2>Right for the Right Concept: Revising Neuro-Symbolic Concepts by Interacting with their Explanations. (arXiv:2011.12854v1 [cs.LG])</h2>
<h3>Wolfgang Stammer, Patrick Schramowski, Kristian Kersting</h3>
<p>Most explanation methods in deep learning map importance estimates for a
model's prediction back to the original input space. These "visual"
explanations are often insufficient, as the model's actual concept remains
elusive. Moreover, without insights into the model's semantic concept, it is
difficult -- if not impossible -- to intervene on the model's behavior via its
explanations, called Explanatory Interactive Learning. Consequently, we propose
to intervene on a Neuro-Symbolic scene representation, which allows one to
revise the model on the semantic level, e.g. "never focus on the color to make
your decision". We compiled a novel confounded visual scene data set, the
CLEVR-Hans data set, capturing complex compositions of different objects. The
results of our experiments on CLEVR-Hans demonstrate that our semantic
explanations, i.e. compositional explanations at a per-object level, can
identify confounders that are not identifiable using "visual" explanations
only. More importantly, feedback on this semantic level makes it possible to
revise the model from focusing on these confounding factors.
</p>
<a href="http://arxiv.org/abs/2011.12854" target="_blank">arXiv:2011.12854</a> [<a href="http://arxiv.org/pdf/2011.12854" target="_blank">pdf</a>]

<h2>Anytime Prediction as a Model of Human Reaction Time. (arXiv:2011.12859v1 [cs.AI])</h2>
<h3>Omkar Kumbhar, Elena Sizikova, Najib Majaj, Denis G. Pelli</h3>
<p>Neural networks today often recognize objects as well as people do, and thus
might serve as models of the human recognition process. However, most such
networks provide their answer after a fixed computational effort, whereas human
reaction time varies, e.g. from 0.2 to 10 s, depending on the properties of
stimulus and task. To model the effect of difficulty on human reaction time, we
considered a classification network that uses early-exit classifiers to make
anytime predictions. Comparing human and MSDNet accuracy in classifying
CIFAR-10 images in added Gaussian noise, we find that the network equivalent
input noise SD is 15 times higher than human, and that human efficiency is only
0.6\% that of the network. When appropriate amounts of noise are present to
bring the two observers (human and network) into the same accuracy range, they
show very similar dependence on duration or FLOPS, i.e. very similar
speed-accuracy tradeoff. We conclude that Anytime classification (i.e. early
exits) is a promising model for human reaction time in recognition tasks.
</p>
<a href="http://arxiv.org/abs/2011.12859" target="_blank">arXiv:2011.12859</a> [<a href="http://arxiv.org/pdf/2011.12859" target="_blank">pdf</a>]

<h2>Sensorimotor representation learning for an "active self" in robots: A model survey. (arXiv:2011.12860v1 [cs.RO])</h2>
<h3>Phuong D.H. Nguyen, Yasmin Kim Georgie, Ezgi Kayhan, Manfred Eppe, Verena Vanessa Hafner, Stefan Wermter</h3>
<p>Safe human-robot interactions require robots to be able to learn how to
behave appropriately in \sout{humans' world} \rev{spaces populated by people}
and thus to cope with the challenges posed by our dynamic and unstructured
environment, rather than being provided a rigid set of rules for operations. In
humans, these capabilities are thought to be related to our ability to perceive
our body in space, sensing the location of our limbs during movement, being
aware of other objects and agents, and controlling our body parts to interact
with them intentionally. Toward the next generation of robots with bio-inspired
capacities, in this paper, we first review the developmental processes of
underlying mechanisms of these abilities: The sensory representations of body
schema, peripersonal space, and the active self in humans. Second, we provide a
survey of robotics models of these sensory representations and robotics models
of the self; and we compare these models with the human counterparts. Finally,
we analyse what is missing from these robotics models and propose a theoretical
computational framework, which aims to allow the emergence of the sense of self
in artificial agents by developing sensory representations through
self-exploration.
</p>
<a href="http://arxiv.org/abs/2011.12860" target="_blank">arXiv:2011.12860</a> [<a href="http://arxiv.org/pdf/2011.12860" target="_blank">pdf</a>]

<h2>Cable Tree Wiring -- Benchmarking Solvers on a Real-World Scheduling Problem with a Variety of Precedence Constraints. (arXiv:2011.12862v1 [cs.AI])</h2>
<h3>Jana Koehler, Joseph B&#xfc;rgler, Urs Fontana, Etienne Fux, Florian Herzog, Marc Pouly, Sophia Saller, Anastasia Salyaeva, Peter Scheiblechner, Kai Waelti</h3>
<p>Cable trees are used in industrial products to transmit energy and
information between different product parts. To this date, they are mostly
assembled by humans and only few automated manufacturing solutions exist using
complex robotic machines. For these machines, the wiring plan has to be
translated into a wiring sequence of cable plugging operations to be followed
by the machine. In this paper, we study and formalize the problem of deriving
the optimal wiring sequence for a given layout of a cable tree. We summarize
our investigations to model this cable tree wiring Problem (CTW) as a traveling
salesman problem with atomic, soft atomic, and disjunctive precedence
constraints as well as tour-dependent edge costs such that it can be solved by
state-of-the-art constraint programming (CP), Optimization Modulo Theories
(OMT), and mixed-integer programming (MIP) solvers. It is further shown, how
the CTW problem can be viewed as a soft version of the coupled tasks scheduling
problem. We discuss various modeling variants for the problem, prove its
NP-hardness, and empirically compare CP, OMT, and MIP solvers on a benchmark
set of 278 instances. The complete benchmark set with all models and instance
data is available on github and is accepted for inclusion in the MiniZinc
challenge 2020.
</p>
<a href="http://arxiv.org/abs/2011.12862" target="_blank">arXiv:2011.12862</a> [<a href="http://arxiv.org/pdf/2011.12862" target="_blank">pdf</a>]

<h2>European Strategy on AI: Are we truly fostering social good?. (arXiv:2011.12863v1 [cs.AI])</h2>
<h3>Francesca Foffano, Teresa Scantamburlo, Atia Cort&#xe9;s, Chiara Bissolo</h3>
<p>Artificial intelligence (AI) is already part of our daily lives and is
playing a key role in defining the economic and social shape of the future. In
2018, the European Commission introduced its AI strategy able to compete in the
next years with world powers such as China and US, but relying on the respect
of European values and fundamental rights. As a result, most of the Member
States have published their own National Strategy with the aim to work on a
coordinated plan for Europe. In this paper, we present an ongoing study on how
European countries are approaching the field of Artificial Intelligence, with
its promises and risks, through the lens of their national AI strategies. In
particular, we aim to investigate how European countries are investing in AI
and to what extent the stated plans can contribute to the benefit of the whole
society. This paper reports the main findings of a qualitative analysis of the
investment plans reported in 15 European National Strategies
</p>
<a href="http://arxiv.org/abs/2011.12863" target="_blank">arXiv:2011.12863</a> [<a href="http://arxiv.org/pdf/2011.12863" target="_blank">pdf</a>]

<h2>Deep Physics-aware Inference of Cloth Deformation for Monocular Human Performance Capture. (arXiv:2011.12866v1 [cs.CV])</h2>
<h3>Yue Li, Marc Habermann, Bernhard Thomaszewski, Stelian Coros, Thabo Beeler, Christian Theobalt</h3>
<p>Recent monocular human performance capture approaches have shown compelling
dense tracking results of the full body from a single RGB camera. However,
existing methods either do not estimate clothing at all or model cloth
deformation with simple geometric priors instead of taking into account the
underlying physical principles. This leads to noticeable artifacts in their
reconstructions, such as baked-in wrinkles, implausible deformations that
seemingly defy gravity, and intersections between cloth and body. To address
these problems, we propose a person-specific, learning-based method that
integrates a finite element-based simulation layer into the training process to
provide for the first time physics supervision in the context of
weakly-supervised deep monocular human performance capture. We show how
integrating physics into the training process improves the learned cloth
deformations, allows modeling clothing as a separate piece of geometry, and
largely reduces cloth-body intersections. Relying only on weak 2D multi-view
supervision during training, our approach leads to a significant improvement
over current state-of-the-art methods and is thus a clear step towards
realistic monocular capture of the entire deforming surface of a clothed human.
</p>
<a href="http://arxiv.org/abs/2011.12866" target="_blank">arXiv:2011.12866</a> [<a href="http://arxiv.org/pdf/2011.12866" target="_blank">pdf</a>]

<h2>Multimodal Learning for Hateful Memes Detection. (arXiv:2011.12870v1 [cs.CV])</h2>
<h3>Yi Zhou, Zhenhao Chen</h3>
<p>Memes are pixel-based multimedia documents containing images and expressions
that usually raise a funny meaning when mixed. Hateful memes are also spread
hatred through social networks. Automatically detecting the hateful memes would
help reduce their harmful societal influence. The challenge of hateful memes
detection lies in its multimodal information, unlike the conventional
multimodal tasks, where the visual and textual information are semantically
aligned. The multimodal information in the meme is weakly aligned or even
irrelevant, which makes the model not only needs to understand the content in
the memes but also reasoning over the multiple modalities. In this paper, we
propose a novel method that incorporates the image captioning process into the
memes detection process. We conducted extensive experiments on meme datasets
and illustrated the effectiveness of our method. Our model also achieves
promising results on the Hateful memes detection challenge.
</p>
<a href="http://arxiv.org/abs/2011.12870" target="_blank">arXiv:2011.12870</a> [<a href="http://arxiv.org/pdf/2011.12870" target="_blank">pdf</a>]

<h2>Improving Redundancy Availability: Dynamic Subtasks Modulation for Robots with Redundancy Insufficiency. (arXiv:2011.12884v1 [cs.RO])</h2>
<h3>Lu Chen, Lipeng Chen, Xiangchi Chen, Yi Ren, Longfei Zhao, Yue Wang, Rong Xion</h3>
<p>This work presents an approach for robots to suitably carry out complex
applications characterized by the presence of multiple additional constraints
or subtasks (e.g. obstacle and self-collision avoidance) but subject to
redundancy insufficiency. The proposed approach, based on a novel subtask
merging strategy, enforces all subtasks in due course by dynamically modulating
a virtual secondary task, where the task status and soft priority are
incorporated to improve the overall efficiency of redundancy resolution. The
proposed approach greatly improves the redundancy availability by unitizing and
deploying subtasks in a fine-grained and compact manner. We build up our
control framework on the null space projection, which guarantees the execution
of subtasks does not interfere with the primary task. Experimental results on
two case studies are presented to show the performance of our approach.
</p>
<a href="http://arxiv.org/abs/2011.12884" target="_blank">arXiv:2011.12884</a> [<a href="http://arxiv.org/pdf/2011.12884" target="_blank">pdf</a>]

<h2>Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection. (arXiv:2011.12885v1 [cs.CV])</h2>
<h3>Xiang Li, Wenhai Wang, Xiaolin Hu, Jun Li, Jinhui Tang, Jian Yang</h3>
<p>Localization Quality Estimation (LQE) is crucial and popular in the recent
advancement of dense object detectors since it can provide accurate ranking
scores that benefit the Non-Maximum Suppression processing and improve
detection performance. As a common practice, most existing methods predict LQE
scores through vanilla convolutional features shared with object classification
or bounding box regression. In this paper, we explore a completely novel and
different perspective to perform LQE -- based on the learned distributions of
the four parameters of the bounding box. The bounding box distributions are
inspired and introduced as "General Distribution" in GFLV1, which describes the
uncertainty of the predicted bounding boxes well. Such a property makes the
distribution statistics of a bounding box highly correlated to its real
localization quality. Specifically, a bounding box distribution with a sharp
peak usually corresponds to high localization quality, and vice versa. By
leveraging the close correlation between distribution statistics and the real
localization quality, we develop a considerably lightweight Distribution-Guided
Quality Predictor (DGQP) for reliable LQE based on GFLV1, thus producing GFLV2.
To our best knowledge, it is the first attempt in object detection to use a
highly relevant, statistical representation to facilitate LQE. Extensive
experiments demonstrate the effectiveness of our method. Notably, GFLV2
(ResNet-101) achieves 46.2 AP at 14.6 FPS, surpassing the previous
state-of-the-art ATSS baseline (43.6 AP at 14.6 FPS) by absolute 2.6 AP on COCO
{\tt test-dev}, without sacrificing the efficiency both in training and
inference. Code will be available at https://github.com/implus/GFocalV2.
</p>
<a href="http://arxiv.org/abs/2011.12885" target="_blank">arXiv:2011.12885</a> [<a href="http://arxiv.org/pdf/2011.12885" target="_blank">pdf</a>]

<h2>Recalibration of Neural Networks for Point Cloud Analysis. (arXiv:2011.12888v1 [cs.CV])</h2>
<h3>Ignacio Sarasua, Sebastian Poelsterl, Christian Wachinger</h3>
<p>Spatial and channel re-calibration have become powerful concepts in computer
vision. Their ability to capture long-range dependencies is especially useful
for those networks that extract local features, such as CNNs. While
re-calibration has been widely studied for image analysis, it has not yet been
used on shape representations. In this work, we introduce re-calibration
modules on deep neural networks for 3D point clouds. We propose a set of
re-calibration blocks that extend Squeeze and Excitation blocks and that can be
added to any network for 3D point cloud analysis that builds a global
descriptor by hierarchically combining features from multiple local
neighborhoods. We run two sets of experiments to validate our approach. First,
we demonstrate the benefit and versatility of our proposed modules by
incorporating them into three state-of-the-art networks for 3D point cloud
analysis: PointNet++, DGCNN, and RSCNN. We evaluate each network on two tasks:
object classification on ModelNet40, and object part segmentation on ShapeNet.
Our results show an improvement of up to 1% in accuracy for ModelNet40 compared
to the baseline method. In the second set of experiments, we investigate the
benefits of re-calibration blocks on Alzheimer's Disease (AD) diagnosis. Our
results demonstrate that our proposed methods yield a 2% increase in accuracy
for diagnosing AD and a 2.3% increase in concordance index for predicting AD
onset with time-to-event analysis. Concluding, re-calibration improves the
accuracy of point cloud architectures, while only minimally increasing the
number of parameters.
</p>
<a href="http://arxiv.org/abs/2011.12888" target="_blank">arXiv:2011.12888</a> [<a href="http://arxiv.org/pdf/2011.12888" target="_blank">pdf</a>]

<h2>TLeague: A Framework for Competitive Self-Play based Distributed Multi-Agent Reinforcement Learning. (arXiv:2011.12895v1 [cs.LG])</h2>
<h3>Peng Sun, Jiechao Xiong, Lei Han, Xinghai Sun, Shuxing Li, Jiawei Xu, Meng Fang, Zhengyou Zhang</h3>
<p>Competitive Self-Play (CSP) based Multi-Agent Reinforcement Learning (MARL)
has shown phenomenal breakthroughs recently. Strong AIs are achieved for
several benchmarks, including Dota 2, Glory of Kings, Quake III, StarCraft II,
to name a few. Despite the success, the MARL training is extremely data
thirsty, requiring typically billions of (if not trillions of) frames be seen
from the environment during training in order for learning a high performance
agent. This poses non-trivial difficulties for researchers or engineers and
prevents the application of MARL to a broader range of real-world problems. To
address this issue, in this manuscript we describe a framework, referred to as
TLeague, that aims at large-scale training and implements several main-stream
CSP-MARL algorithms. The training can be deployed in either a single machine or
a cluster of hybrid machines (CPUs and GPUs), where the standard Kubernetes is
supported in a cloud native manner. TLeague achieves a high throughput and a
reasonable scale-up when performing distributed training. Thanks to the modular
design, it is also easy to extend for solving other multi-agent problems or
implementing and verifying MARL algorithms. We present experiments over
StarCraft II, ViZDoom and Pommerman to show the efficiency and effectiveness of
TLeague. The code is open-sourced and available at
https://github.com/tencent-ailab/tleague_projpage
</p>
<a href="http://arxiv.org/abs/2011.12895" target="_blank">arXiv:2011.12895</a> [<a href="http://arxiv.org/pdf/2011.12895" target="_blank">pdf</a>]

<h2>Adversarial Evaluation of Multimodal Models under Realistic Gray Box Assumption. (arXiv:2011.12902v1 [cs.CV])</h2>
<h3>Ivan Evtimov, Russel Howes, Brian Dolhansky, Hamed Firooz, Cristian Canton</h3>
<p>This work examines the vulnerability of multimodal (image + text) models to
adversarial threats similar to those discussed in previous literature on
unimodal (image- or text-only) models. We introduce realistic assumptions of
partial model knowledge and access, and discuss how these assumptions differ
from the standard "black-box"/"white-box" dichotomy common in current
literature on adversarial attacks. Working under various levels of these
"gray-box" assumptions, we develop new attack methodologies unique to
multimodal classification and evaluate them on the Hateful Memes Challenge
classification task. We find that attacking multiple modalities yields stronger
attacks than unimodal attacks alone (inducing errors in up to 73% of cases),
and that the unimodal image attacks on multimodal classifiers we explored were
stronger than character-based text augmentation attacks (inducing errors on
average in 45% and 30% of cases, respectively).
</p>
<a href="http://arxiv.org/abs/2011.12902" target="_blank">arXiv:2011.12902</a> [<a href="http://arxiv.org/pdf/2011.12902" target="_blank">pdf</a>]

<h2>Open-World Learning Without Labels. (arXiv:2011.12906v1 [cs.CV])</h2>
<h3>Mohsen Jafarzadeh, Akshay Raj Dhamija, Steve Cruz, Chunchun Li, Touqeer Ahmad, Terrance E. Boult</h3>
<p>Open-world learning is a problem where an autonomous agent detects things
that it does not know and learns them over time from a non-stationary and
never-ending stream of data; in an open-world environment, the training data
and objective criteria are never available at once. The agent should grasp new
knowledge from learning without forgetting acquired prior knowledge.
Researchers proposed a few open-world learning agents for image classification
tasks that operate in complex scenarios. However, all prior work on open-world
learning has all labeled data to learn the new classes from the stream of
images. In scenarios where autonomous agents should respond in near real-time
or work in areas with limited communication infrastructure, human labeling of
data is not possible. Therefore, supervised open-world learning agents are not
scalable solutions for such applications. Herein, we propose a new framework
that enables agents to learn new classes from a stream of unlabeled data in an
unsupervised manner. Also, we study the robustness and learning speed of such
agents with supervised and unsupervised feature representation. We also
introduce a new metric for open-world learning without labels. We anticipate
our theories and method to be a starting point for developing autonomous true
open-world never-ending learning agents.
</p>
<a href="http://arxiv.org/abs/2011.12906" target="_blank">arXiv:2011.12906</a> [<a href="http://arxiv.org/pdf/2011.12906" target="_blank">pdf</a>]

<h2>DRACO: Weakly Supervised Dense Reconstruction And Canonicalization of Objects. (arXiv:2011.12912v1 [cs.CV])</h2>
<h3>Rahul Sajnani, AadilMehdi Sanchawala, Krishna Murthy Jatavallabhula, Srinath Sridhar, K. Madhava Krishna</h3>
<p>We present DRACO, a method for Dense Reconstruction And Canonicalization of
Object shape from one or more RGB images. Canonical shape reconstruction,
estimating 3D object shape in a coordinate space canonicalized for scale,
rotation, and translation parameters, is an emerging paradigm that holds
promise for a multitude of robotic applications. Prior approaches either rely
on painstakingly gathered dense 3D supervision, or produce only sparse
canonical representations, limiting real-world applicability. DRACO performs
dense canonicalization using only weak supervision in the form of camera poses
and semantic keypoints at train time. During inference, DRACO predicts dense
object-centric depth maps in a canonical coordinate-space, solely using one or
more RGB images of an object. Extensive experiments on canonical shape
reconstruction and pose estimation show that DRACO is competitive or superior
to fully-supervised methods.
</p>
<a href="http://arxiv.org/abs/2011.12912" target="_blank">arXiv:2011.12912</a> [<a href="http://arxiv.org/pdf/2011.12912" target="_blank">pdf</a>]

<h2>torchdistill: A Modular, Configuration-Driven Framework for Knowledge Distillation. (arXiv:2011.12913v1 [cs.LG])</h2>
<h3>Yoshitomo Matsubara</h3>
<p>While knowledge distillation (transfer) has been attracting attentions from
the research community, the recent development in the fields has heightened the
need for reproducible studies and highly generalized frameworks to lower
barriers to such high-quality, reproducible deep learning research. Several
researchers voluntarily published frameworks used in their knowledge
distillation studies to help other interested researchers reproduce their
original work. Such frameworks, however, are usually neither well generalized
nor maintained, thus researchers are still required to write a lot of code to
refactor/build on the frameworks for introducing new methods, models, datasets
and designing experiments. In this paper, we present our developed open-source
framework built on PyTorch and dedicated for knowledge distillation studies.
The framework is designed to enable users to design experiments by a
declarative PyYAML configuration file, and helps researchers complete the
recently proposed ML Code Completeness Checklist. Using the developed
framework, we demonstrate its various efficient training strategies, and
implement a variety of knowledge distillation methods. We also reproduce some
of their original experimental results on the ImageNet and COCO datasets
presented at major machine learning conferences such as ICLR, NeurIPS, CVPR and
ECCV, including recent state-of-the-art methods. All the source code,
configurations, log files and the trained model weights are publicly available
at https://github.com/yoshitomo-matsubara/torchdistill .
</p>
<a href="http://arxiv.org/abs/2011.12913" target="_blank">arXiv:2011.12913</a> [<a href="http://arxiv.org/pdf/2011.12913" target="_blank">pdf</a>]

<h2>Equivariant Conditional Neural Processes. (arXiv:2011.12916v1 [cs.LG])</h2>
<h3>Peter Holderrieth, Michael Hutchinson, Yee Whye Teh</h3>
<p>We introduce Equivariant Conditional Neural Processes (EquivCNPs), a new
member of the Neural Process family that models vector-valued data in an
equivariant manner with respect to isometries of $\mathbb{R}^n$. In addition,
we look at multi-dimensional Gaussian Processes (GPs) under the perspective of
equivariance and find the sufficient and necessary constraints to ensure a GP
over $\mathbb{R}^n$ is equivariant. We test EquivCNPs on the inference of
vector fields using Gaussian process samples and real-world weather data. We
observe that our model significantly improves the performance of previous
models. By imposing equivariance as constraints, the parameter and data
efficiency of these models are increased. Moreover, we find that EquivCNPs are
more robust against overfitting to local conditions of the training data.
</p>
<a href="http://arxiv.org/abs/2011.12916" target="_blank">arXiv:2011.12916</a> [<a href="http://arxiv.org/pdf/2011.12916" target="_blank">pdf</a>]

<h2>Analyzing the Machine Learning Conference Review Process. (arXiv:2011.12919v1 [cs.LG])</h2>
<h3>David Tran, Alex Valtchanov, Keshav Ganapathy, Raymond Feng, Eric Slud, Micah Goldblum, Tom Goldstein</h3>
<p>Mainstream machine learning conferences have seen a dramatic increase in the
number of participants, along with a growing range of perspectives, in recent
years. Members of the machine learning community are likely to overhear
allegations ranging from randomness of acceptance decisions to institutional
bias. In this work, we critically analyze the review process through a
comprehensive study of papers submitted to ICLR between 2017 and 2020. We
quantify reproducibility/randomness in review scores and acceptance decisions,
and examine whether scores correlate with paper impact. Our findings suggest
strong institutional bias in accept/reject decisions, even after controlling
for paper quality. Furthermore, we find evidence for a gender gap, with female
authors receiving lower scores, lower acceptance rates, and fewer citations per
paper than their male counterparts. We conclude our work with recommendations
for future conference organizers.
</p>
<a href="http://arxiv.org/abs/2011.12919" target="_blank">arXiv:2011.12919</a> [<a href="http://arxiv.org/pdf/2011.12919" target="_blank">pdf</a>]

<h2>Unsupervised Object Keypoint Learning using Local Spatial Predictability. (arXiv:2011.12930v1 [cs.CV])</h2>
<h3>Anand Gopalakrishnan, Sjoerd van Steenkiste, J&#xfc;rgen Schmidhuber</h3>
<p>We propose PermaKey, a novel approach to representation learning based on
object keypoints. It leverages the predictability of local image regions from
spatial neighborhoods to identify salient regions that correspond to object
parts, which are then converted to keypoints. Unlike prior approaches, it
utilizes predictability to discover object keypoints, an intrinsic property of
objects. This ensures that it does not overly bias keypoints to focus on
characteristics that are not unique to objects, such as movement, shape, colour
etc. We demonstrate the efficacy of PermaKey on Atari where it learns keypoints
corresponding to the most salient object parts and is robust to certain visual
distractors. Further, on downstream RL tasks in the Atari domain we demonstrate
how agents equipped with our keypoints outperform those using competing
alternatives, even on challenging environments with moving backgrounds or
distractor objects.
</p>
<a href="http://arxiv.org/abs/2011.12930" target="_blank">arXiv:2011.12930</a> [<a href="http://arxiv.org/pdf/2011.12930" target="_blank">pdf</a>]

<h2>Multiclass non-Adversarial Image Synthesis, with Application to Classification from Very Small Sample. (arXiv:2011.12942v1 [cs.CV])</h2>
<h3>Itamar Winter, Daphna Weinshall</h3>
<p>The generation of synthetic images is currently being dominated by Generative
Adversarial Networks (GANs). Despite their outstanding success in generating
realistic looking images, they still suffer from major drawbacks, including an
unstable and highly sensitive training procedure, mode-collapse and
mode-mixture, and dependency on large training sets. In this work we present a
novel non-adversarial generative method - Clustered Optimization of LAtent
space (COLA), which overcomes some of the limitations of GANs, and outperforms
GANs when training data is scarce. In the full data regime, our method is
capable of generating diverse multi-class images with no supervision,
surpassing previous non-adversarial methods in terms of image quality and
diversity. In the small-data regime, where only a small sample of labeled
images is available for training with no access to additional unlabeled data,
our results surpass state-of-the-art GAN models trained on the same amount of
data. Finally, when utilizing our model to augment small datasets, we surpass
the state-of-the-art performance in small-sample classification tasks on
challenging datasets, including CIFAR-10, CIFAR-100, STL-10 and Tiny-ImageNet.
A theoretical analysis supporting the essence of the method is presented.
</p>
<a href="http://arxiv.org/abs/2011.12942" target="_blank">arXiv:2011.12942</a> [<a href="http://arxiv.org/pdf/2011.12942" target="_blank">pdf</a>]

<h2>No Subclass Left Behind: Fine-Grained Robustness in Coarse-Grained Classification Problems. (arXiv:2011.12945v1 [cs.LG])</h2>
<h3>Nimit S. Sohoni, Jared A. Dunnmon, Geoffrey Angus, Albert Gu, Christopher R&#xe9;</h3>
<p>In real-world classification tasks, each class often comprises multiple
finer-grained "subclasses." As the subclass labels are frequently unavailable,
models trained using only the coarser-grained class labels often exhibit highly
variable performance across different subclasses. This phenomenon, known as
hidden stratification, has important consequences for models deployed in
safety-critical applications such as medicine. We propose GEORGE, a method to
both measure and mitigate hidden stratification even when subclass labels are
unknown. We first observe that unlabeled subclasses are often separable in the
feature space of deep models, and exploit this fact to estimate subclass labels
for the training data via clustering techniques. We then use these approximate
subclass labels as a form of noisy supervision in a distributionally robust
optimization objective. We theoretically characterize the performance of GEORGE
in terms of the worst-case generalization error across any subclass. We
empirically validate GEORGE on a mix of real-world and benchmark image
classification datasets, and show that our approach boosts worst-case subclass
accuracy by up to 22 percentage points compared to standard training
techniques, without requiring any information about the subclasses.
</p>
<a href="http://arxiv.org/abs/2011.12945" target="_blank">arXiv:2011.12945</a> [<a href="http://arxiv.org/pdf/2011.12945" target="_blank">pdf</a>]

<h2>Deformable Neural Radiance Fields. (arXiv:2011.12948v1 [cs.CV])</h2>
<h3>Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Sofien Bouaziz, Dan B Goldman, Steven M. Seitz, Ricardo-Martin Brualla</h3>
<p>We present the first method capable of photorealistically reconstructing a
non-rigidly deforming scene using photos/videos captured casually from mobile
phones. Our approach -- D-NeRF -- augments neural radiance fields (NeRF) by
optimizing an additional continuous volumetric deformation field that warps
each observed point into a canonical 5D NeRF. We observe that these NeRF-like
deformation fields are prone to local minima, and propose a coarse-to-fine
optimization method for coordinate-based models that allows for more robust
optimization. By adapting principles from geometry processing and physical
simulation to NeRF-like models, we propose an elastic regularization of the
deformation field that further improves robustness. We show that D-NeRF can
turn casually captured selfie photos/videos into deformable NeRF models that
allow for photorealistic renderings of the subject from arbitrary viewpoints,
which we dub "nerfies." We evaluate our method by collecting data using a rig
with two mobile phones that take time-synchronized photos, yielding
train/validation images of the same pose at different viewpoints. We show that
our method faithfully reconstructs non-rigidly deforming scenes and reproduces
unseen views with high fidelity.
</p>
<a href="http://arxiv.org/abs/2011.12948" target="_blank">arXiv:2011.12948</a> [<a href="http://arxiv.org/pdf/2011.12948" target="_blank">pdf</a>]

<h2>Space-time Neural Irradiance Fields for Free-Viewpoint Video. (arXiv:2011.12950v1 [cs.CV])</h2>
<h3>Wenqi Xian, Jia-Bin Huang, Johannes Kopf, Changil Kim</h3>
<p>We present a method that learns a spatiotemporal neural irradiance field for
dynamic scenes from a single video. Our learned representation enables
free-viewpoint rendering of the input video. Our method builds upon recent
advances in implicit representations. Learning a spatiotemporal irradiance
field from a single video poses significant challenges because the video
contains only one observation of the scene at any point in time. The 3D
geometry of a scene can be legitimately represented in numerous ways since
varying geometry (motion) can be explained with varying appearance and vice
versa. We address this ambiguity by constraining the time-varying geometry of
our dynamic scene representation using the scene depth estimated from video
depth estimation methods, aggregating contents from individual frames into a
single global representation. We provide an extensive quantitative evaluation
and demonstrate compelling free-viewpoint rendering results.
</p>
<a href="http://arxiv.org/abs/2011.12950" target="_blank">arXiv:2011.12950</a> [<a href="http://arxiv.org/pdf/2011.12950" target="_blank">pdf</a>]

<h2>Unsupervised Object Detection with LiDAR Clues. (arXiv:2011.12953v1 [cs.CV])</h2>
<h3>Hao Tian, Yuntao Chen, Jifeng Dai, Zhaoxiang Zhang, Xizhou Zhu</h3>
<p>Despite the importance of unsupervised object detection, to the best of our
knowledge, there is no previous work addressing this problem. One main issue,
widely known to the community, is that object boundaries derived only from 2D
image appearance are ambiguous and unreliable. To address this, we exploit
LiDAR clues to aid unsupervised object detection. By exploiting the 3D scene
structure, the issue of localization can be considerably mitigated. We further
identify another major issue, seldom noticed by the community, that the
long-tailed and open-ended (sub-)category distribution should be accommodated.
In this paper, we present the first practical method for unsupervised object
detection with the aid of LiDAR clues. In our approach, candidate object
segments based on 3D point clouds are firstly generated. Then, an iterative
segment labeling process is conducted to assign segment labels and to train a
segment labeling network, which is based on features from both 2D images and 3D
point clouds. The labeling process is carefully designed so as to mitigate the
issue of long-tailed and open-ended distribution. The final segment labels are
set as pseudo annotations for object detection network training. Extensive
experiments on the large-scale Waymo Open dataset suggest that the derived
unsupervised object detection method achieves reasonable accuracy compared with
that of strong supervision within the LiDAR visible range. Code shall be
released.
</p>
<a href="http://arxiv.org/abs/2011.12953" target="_blank">arXiv:2011.12953</a> [<a href="http://arxiv.org/pdf/2011.12953" target="_blank">pdf</a>]

<h2>RELLIS-3D Dataset: Data, Benchmarks and Analysis. (arXiv:2011.12954v1 [cs.CV])</h2>
<h3>Peng Jiang, Philip Osteen, Maggie Wigness, Srikanth Saripalli</h3>
<p>Semantic scene understanding is crucial for robust and safe autonomous
navigation, particularly so in off-road environments. Recent deep learning
advances for 3D semantic segmentation rely heavily on large sets of training
data, however existing autonomy datasets either represent urban environments or
lack multimodal off-road data. We fill this gap with RELLIS-3D, a multimodal
dataset collected in an off-road environment, which contains annotations for
13,556 LiDAR scans and 6,235 images. The data was collected on the Rellis
Campus of Texas A&amp;M University, and presents challenges to existing algorithms
related to class imbalance and environmental topography. Additionally, we
evaluate the current state of the art deep learning semantic segmentation
models on this dataset. Experimental results show that RELLIS-3D presents
challenges for algorithms designed for segmentation in urban environments. This
novel dataset provides the resources needed by researchers to continue to
develop more advanced algorithms and investigate new research directions to
enhance autonomous navigation in off-road environments. RELLIS-3D will be
published at https://github.com/unmannedlab/RELLIS-3D.
</p>
<a href="http://arxiv.org/abs/2011.12954" target="_blank">arXiv:2011.12954</a> [<a href="http://arxiv.org/pdf/2011.12954" target="_blank">pdf</a>]

<h2>Tractable Epistemic Reasoning with Functional Fluents, Static Causal Laws and Postdiction. (arXiv:1403.0034v4 [cs.AI] UPDATED)</h2>
<h3>Manfred Eppe</h3>
<p>We present an epistemic action theory for tractable epistemic reasoning as an
extension to the h-approximation (HPX) theory. In contrast to existing
tractable approaches, the theory supports functional fluents and postdictive
reasoning with static causal laws. We argue that this combination is
particularly synergistic because it allows one not only to perform direct
postdiction about the conditions of actions, but also indirect postdiction
about the conditions of static causal laws. We show that despite the richer
expressiveness, the temporal projection problem remains tractable (polynomial),
and therefore the planning problem remains in NP. We present the operational
semantics of our theory as well as its formulation as Answer Set Programming.
</p>
<a href="http://arxiv.org/abs/1403.0034" target="_blank">arXiv:1403.0034</a> [<a href="http://arxiv.org/pdf/1403.0034" target="_blank">pdf</a>]

<h2>The duality structure gradient descent algorithm: analysis and applications to neural networks. (arXiv:1708.00523v6 [cs.LG] UPDATED)</h2>
<h3>Thomas Flynn</h3>
<p>The training of deep neural networks is typically carried out using some form
of gradient descent, often with great success. However, existing non-asymptotic
analyses of first-order optimization algorithms typically employ a gradient
smoothness assumption that is too strong to be applicable in the case of deep
neural networks. To address this, we propose an algorithm named duality
structure gradient descent (DSGD) that is amenable to non-asymptotic
performance analysis, under mild assumptions on the training set and network
architecture. The algorithm can be viewed as a form of layer-wise coordinate
descent, where at each iteration the algorithm chooses one layer of the network
to update. The decision of what layer to update is done in a greedy fashion,
based on a rigorous lower bound on the improvement of the objective function
for each choice of layer. In the analysis, we bound the time required to reach
approximate stationary points, in both the deterministic and stochastic
settings. The convergence is measured in terms of a parameter-dependent family
of norms that is derived from the network architecture and designed to confirm
a smoothness-like property on the gradient of the training loss function. We
empirically demonstrate the effectiveness of DSGD in several neural network
training scenarios.
</p>
<a href="http://arxiv.org/abs/1708.00523" target="_blank">arXiv:1708.00523</a> [<a href="http://arxiv.org/pdf/1708.00523" target="_blank">pdf</a>]

<h2>Deep Nets: What have they ever done for Vision?. (arXiv:1805.04025v4 [cs.CV] UPDATED)</h2>
<h3>Alan L. Yuille, Chenxi Liu</h3>
<p>This is an opinion paper about the strengths and weaknesses of Deep Nets for
vision. They are at the heart of the enormous recent progress in artificial
intelligence and are of growing importance in cognitive science and
neuroscience. They have had many successes but also have several limitations
and there is limited understanding of their inner workings. At present Deep
Nets perform very well on specific visual tasks with benchmark datasets but
they are much less general purpose, flexible, and adaptive than the human
visual system. We argue that Deep Nets in their current form are unlikely to be
able to overcome the fundamental problem of computer vision, namely how to deal
with the combinatorial explosion, caused by the enormous complexity of natural
images, and obtain the rich understanding of visual scenes that the human
visual achieves. We argue that this combinatorial explosion takes us into a
regime where "big data is not enough" and where we need to rethink our methods
for benchmarking performance and evaluating vision algorithms. We stress that,
as vision algorithms are increasingly used in real world applications, that
performance evaluation is not merely an academic exercise but has important
consequences in the real world. It is impractical to review the entire Deep Net
literature so we restrict ourselves to a limited range of topics and references
which are intended as entry points into the literature. The views expressed in
this paper are our own and do not necessarily represent those of anybody else
in the computer vision community.
</p>
<a href="http://arxiv.org/abs/1805.04025" target="_blank">arXiv:1805.04025</a> [<a href="http://arxiv.org/pdf/1805.04025" target="_blank">pdf</a>]

<h2>PNS: Population-Guided Novelty Search Learning Method for Reinforcement Learning. (arXiv:1811.10264v3 [cs.LG] UPDATED)</h2>
<h3>Qihao Liu, Xiaofeng Liu, Guoping Cai</h3>
<p>Reinforcement Learning (RL) has made remarkable achievements, but it still
suffers from inadequate exploration strategies, sparse reward signals, and
deceptive reward functions. These problems motivate the need for a more
efficient and directed exploration. For solving this, a Population-guided
Novelty Search (PNS) parallel learning method is proposed. In PNS, the
population is divided into multiple sub-populations, each of which has one
chief agent and several exploring agents. The role of the chief agent is to
evaluate the policies learned by exploring agents and to share the optimal
policy with all sub-populations. The role of exploring agents is to learn their
policies in collaboration with the guidance of the optimal policy and,
simultaneously, upload their policies to the chief agent. To balance
exploration and exploitation, the Novelty Search (NS) is employed in chief
agents to encourage policies with high novelty while maximizing per-episode
performance. The introduction of sub-populations and NS mechanisms promote
directed exploration and enables better policy search. In the numerical
experiment section, the proposed scheme is applied to the twin delayed deep
deterministic (TD3) policy gradient algorithm, and the effectiveness of PNS to
promote exploration and improve performance in both continuous control domains
and discrete control domains is demonstrated. Notably, the proposed method
achieves rewards that far exceed the SOTA methods in Delayed MoJoCo
environments.
</p>
<a href="http://arxiv.org/abs/1811.10264" target="_blank">arXiv:1811.10264</a> [<a href="http://arxiv.org/pdf/1811.10264" target="_blank">pdf</a>]

<h2>AdaCoSeg: Adaptive Shape Co-Segmentation with Group Consistency Loss. (arXiv:1903.10297v5 [cs.CV] UPDATED)</h2>
<h3>Chenyang Zhu, Kai Xu, Siddhartha Chaudhuri, Li Yi, Leonidas Guibas, Hao Zhang</h3>
<p>We introduce AdaCoSeg, a deep neural network architecture for adaptive
co-segmentation of a set of 3D shapes represented as point clouds. Differently
from the familiar single-instance segmentation problem, co-segmentation is
intrinsically contextual: how a shape is segmented can vary depending on the
set it is in. Hence, our network features an adaptive learning module to
produce a consistent shape segmentation which adapts to a set. Specifically,
given an input set of unsegmented shapes, we first employ an offline
pre-trained part prior network to propose per-shape parts. Then, the
co-segmentation network iteratively and} jointly optimizes the part labelings
across the set subjected to a novel group consistency loss defined by matrix
ranks. While the part prior network can be trained with noisy and
inconsistently segmented shapes, the final output of AdaCoSeg is a consistent
part labeling for the input set, with each shape segmented into up to (a
user-specified) K parts. Overall, our method is weakly supervised, producing
segmentations tailored to the test set, without consistent ground-truth
segmentations. We show qualitative and quantitative results from AdaCoSeg and
evaluate it via ablation studies and comparisons to state-of-the-art
co-segmentation methods.
</p>
<a href="http://arxiv.org/abs/1903.10297" target="_blank">arXiv:1903.10297</a> [<a href="http://arxiv.org/pdf/1903.10297" target="_blank">pdf</a>]

<h2>Time-Series Event Prediction with Evolutionary State Graph. (arXiv:1905.05006v4 [cs.LG] UPDATED)</h2>
<h3>Wenjie Hu, Yang Yang, Ziqiang Cheng, Carl Yang, Xiang Ren</h3>
<p>The accurate and interpretable prediction of future events in time-series
data often requires the capturing of representative patterns (or referred to as
states) underpinning the observed data. To this end, most existing studies
focus on the representation and recognition of states, but ignore the changing
transitional relations among them. In this paper, we present evolutionary state
graph, a dynamic graph structure designed to systematically represent the
evolving relations (edges) among states (nodes) along time. We conduct analysis
on the dynamic graphs constructed from the time-series data and show that
changes on the graph structures (e.g., edges connecting certain state nodes)
can inform the occurrences of events (i.e., time-series fluctuation). Inspired
by this, we propose a novel graph neural network model, Evolutionary State
Graph Network (EvoNet), to encode the evolutionary state graph for accurate and
interpretable time-series event prediction. Specifically, Evolutionary State
Graph Network models both the node-level (state-to-state) and graph-level
(segment-to-segment) propagation, and captures the node-graph
(state-to-segment) interactions over time. Experimental results based on five
real-world datasets show that our approach not only achieves clear improvements
compared with 11 baselines, but also provides more insights towards explaining
the results of event predictions.
</p>
<a href="http://arxiv.org/abs/1905.05006" target="_blank">arXiv:1905.05006</a> [<a href="http://arxiv.org/pdf/1905.05006" target="_blank">pdf</a>]

<h2>High Dimensional Classification via Regularized and Unregularized Empirical Risk Minimization: Precise Error and Optimal Loss. (arXiv:1905.13742v2 [stat.ML] UPDATED)</h2>
<h3>Xiaoyi Mai, Zhenyu Liao</h3>
<p>This article provides, through theoretical analysis, an in-depth
understanding of the classification performance of the empirical risk
minimization framework, in both ridge-regularized and unregularized cases, when
high dimensional data are considered. Focusing on the fundamental problem of
separating a two-class Gaussian mixture, the proposed analysis allows for a
precise prediction of the classification error for a set of numerous data
vectors $\mathbf{x} \in \mathbb R^p$ of sufficiently large dimension $p$. This
precise error depends on the loss function, the number of training samples, and
the statistics of the mixture data model. It is shown to hold beyond Gaussian
distribution under some additional non-sparsity condition of the data
statistics. Building upon this quantitative error analysis, we identify the
simple square loss as the optimal choice for high dimensional classification in
both ridge-regularized and unregularized cases, regardless of the number of
training samples.
</p>
<a href="http://arxiv.org/abs/1905.13742" target="_blank">arXiv:1905.13742</a> [<a href="http://arxiv.org/pdf/1905.13742" target="_blank">pdf</a>]

<h2>Latent Gaussian process with composite likelihoods and numerical quadrature. (arXiv:1909.01614v3 [stat.ML] UPDATED)</h2>
<h3>Siddharth Ramchandran, Miika Koskinen, Harri L&#xe4;hdesm&#xe4;ki</h3>
<p>Clinical patient records are an example of high-dimensional data that is
typically collected from disparate sources and comprises of multiple
likelihoods with noisy as well as missing values. In this work, we propose an
unsupervised generative model that can learn a low-dimensional representation
among the observations in a latent space, while making use of all available
data in a heterogeneous data setting with missing values. We improve upon the
existing Gaussian process latent variable model (GPLVM) by incorporating
multiple likelihoods and deep neural network parameterised back-constraints to
create a non-linear dimensionality reduction technique for heterogeneous data.
In addition, we develop a variational inference method for our model that uses
numerical quadrature. We establish the effectiveness of our model and compare
against existing GPLVM methods on a standard benchmark dataset as well as on
clinical data of Parkinson's disease patients treated at the HUS Helsinki
University Hospital.
</p>
<a href="http://arxiv.org/abs/1909.01614" target="_blank">arXiv:1909.01614</a> [<a href="http://arxiv.org/pdf/1909.01614" target="_blank">pdf</a>]

<h2>Can $Q$-Learning with Graph Networks Learn a Generalizable Branching Heuristic for a SAT Solver?. (arXiv:1909.11830v2 [cs.LG] UPDATED)</h2>
<h3>Vitaly Kurin, Saad Godil, Shimon Whiteson, Bryan Catanzaro</h3>
<p>We present Graph-$Q$-SAT, a branching heuristic for a Boolean SAT solver
trained with value-based reinforcement learning (RL) using Graph Neural
Networks for function approximation. Solvers using Graph-$Q$-SAT are complete
SAT solvers that either provide a satisfying assignment or proof of
unsatisfiability, which is required for many SAT applications. The branching
heuristics commonly used in SAT solvers make poor decisions during their
warm-up period, whereas Graph-$Q$-SAT is trained to examine the structure of
the particular problem instance to make better decisions early in the search.
Training Graph-$Q$-SAT is data efficient and does not require elaborate dataset
preparation or feature engineering. We train Graph-$Q$-SAT using RL interfacing
with MiniSat solver and show that Graph-$Q$-SAT can reduce the number of
iterations required to solve SAT problems by 2-3X. Furthermore, it generalizes
to unsatisfiable SAT instances, as well as to problems with 5X more variables
than it was trained on. We show that for larger problems, reductions in the
number of iterations lead to wall clock time reductions, the ultimate goal when
designing heuristics. We also show positive zero-shot transfer behavior when
testing Graph-$Q$-SAT on a task family different from that used for training.
While more work is needed to apply Graph-$Q$-SAT to reduce wall clock time in
modern SAT solving settings, it is a compelling proof-of-concept showing that
RL equipped with Graph Neural Networks can learn a generalizable branching
heuristic for SAT search.
</p>
<a href="http://arxiv.org/abs/1909.11830" target="_blank">arXiv:1909.11830</a> [<a href="http://arxiv.org/pdf/1909.11830" target="_blank">pdf</a>]

<h2>Reduced-Order Modeling of Deep Neural Networks. (arXiv:1910.06995v5 [cs.LG] UPDATED)</h2>
<h3>Julia Gusak, Talgat Daulbaev, Evgeny Ponomarev, Andrzej Cichocki, Ivan Oseledets</h3>
<p>We introduce a new method for speeding up the inference of deep neural
networks. It is somewhat inspired by the reduced-order modeling techniques for
dynamical systems.The cornerstone of the proposed method is the maximum volume
algorithm. We demonstrate efficiency on neural networks pre-trained on
different datasets. We show that in many practical cases it is possible to
replace convolutional layers with much smaller fully-connected layers with a
relatively small drop in accuracy.
</p>
<a href="http://arxiv.org/abs/1910.06995" target="_blank">arXiv:1910.06995</a> [<a href="http://arxiv.org/pdf/1910.06995" target="_blank">pdf</a>]

<h2>Bridging the Gap between Sample-based and One-shot Neural Architecture Search with BONAS. (arXiv:1911.09336v4 [cs.LG] UPDATED)</h2>
<h3>Han Shi, Renjie Pi, Hang Xu, Zhenguo Li, James T. Kwok, Tong Zhang</h3>
<p>Neural Architecture Search (NAS) has shown great potentials in finding better
neural network designs. Sample-based NAS is the most reliable approach which
aims at exploring the search space and evaluating the most promising
architectures. However, it is computationally very costly. As a remedy, the
one-shot approach has emerged as a popular technique for accelerating NAS using
weight-sharing. However, due to the weight-sharing of vastly different
networks, the one-shot approach is less reliable than the sample-based
approach. In this work, we propose BONAS (Bayesian Optimized Neural
Architecture Search), a sample-based NAS framework which is accelerated using
weight-sharing to evaluate multiple related architectures simultaneously.
Specifically, we apply Graph Convolutional Network predictor as a surrogate
model for Bayesian Optimization to select multiple related candidate models in
each iteration. We then apply weight-sharing to train multiple candidate models
simultaneously. This approach not only accelerates the traditional sample-based
approach significantly, but also keeps its reliability. This is because
weight-sharing among related architectures are more reliable than those in the
one-shot approach. Extensive experiments are conducted to verify the
effectiveness of our method over many competing algorithms.
</p>
<a href="http://arxiv.org/abs/1911.09336" target="_blank">arXiv:1911.09336</a> [<a href="http://arxiv.org/pdf/1911.09336" target="_blank">pdf</a>]

<h2>Group-Connected Multilayer Perceptron Networks. (arXiv:1912.09600v3 [cs.LG] UPDATED)</h2>
<h3>Mohammad Kachuee, Sajad Darabi, Shayan Fazeli, Majid Sarrafzadeh</h3>
<p>Despite the success of deep learning in domains such as image, voice, and
graphs, there has been little progress in deep representation learning for
domains without a known structure between features. For instance, a tabular
dataset of different demographic and clinical factors where the feature
interactions are not given as a prior. In this paper, we propose
Group-Connected Multilayer Perceptron (GMLP) networks to enable deep
representation learning in these domains. GMLP is based on the idea of learning
expressive feature combinations (groups) and exploiting them to reduce the
network complexity by defining local group-wise operations. During the training
phase, GMLP learns a sparse feature grouping matrix using temperature annealing
softmax with an added entropy loss term to encourage the sparsity. Furthermore,
an architecture is suggested which resembles binary trees, where group-wise
operations are followed by pooling operations to combine information; reducing
the number of groups as the network grows in depth. To evaluate the proposed
method, we conducted experiments on different real-world datasets covering
various application areas. Additionally, we provide visualizations on MNIST and
synthesized data. According to the results, GMLP is able to successfully learn
and exploit expressive feature combinations and achieve state-of-the-art
classification performance on different datasets.
</p>
<a href="http://arxiv.org/abs/1912.09600" target="_blank">arXiv:1912.09600</a> [<a href="http://arxiv.org/pdf/1912.09600" target="_blank">pdf</a>]

<h2>FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence. (arXiv:2001.07685v2 [cs.LG] UPDATED)</h2>
<h3>Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin D. Cubuk, Alex Kurakin, Han Zhang, Colin Raffel</h3>
<p>Semi-supervised learning (SSL) provides an effective means of leveraging
unlabeled data to improve a model's performance. In this paper, we demonstrate
the power of a simple combination of two common SSL methods: consistency
regularization and pseudo-labeling. Our algorithm, FixMatch, first generates
pseudo-labels using the model's predictions on weakly-augmented unlabeled
images. For a given image, the pseudo-label is only retained if the model
produces a high-confidence prediction. The model is then trained to predict the
pseudo-label when fed a strongly-augmented version of the same image. Despite
its simplicity, we show that FixMatch achieves state-of-the-art performance
across a variety of standard semi-supervised learning benchmarks, including
94.93% accuracy on CIFAR-10 with 250 labels and 88.61% accuracy with 40 -- just
4 labels per class. Since FixMatch bears many similarities to existing SSL
methods that achieve worse performance, we carry out an extensive ablation
study to tease apart the experimental factors that are most important to
FixMatch's success. We make our code available at
https://github.com/google-research/fixmatch.
</p>
<a href="http://arxiv.org/abs/2001.07685" target="_blank">arXiv:2001.07685</a> [<a href="http://arxiv.org/pdf/2001.07685" target="_blank">pdf</a>]

<h2>Image Embedded Segmentation: Uniting Supervised and Unsupervised Objectives for Segmenting Histopathological Images. (arXiv:2001.11202v3 [cs.CV] UPDATED)</h2>
<h3>C. T. Sari, C. Sokmensuer, C. Gunduz-Demir</h3>
<p>This paper presents a new regularization method to train a fully
convolutional network for semantic tissue segmentation in histopathological
images. This method relies on the benefit of unsupervised learning, in the form
of image reconstruction, for network training. To this end, it puts forward an
idea of defining a new embedding that allows uniting the main supervised task
of semantic segmentation and an auxiliary unsupervised task of image
reconstruction into a single one and proposes to learn this united task by a
single generative model. This embedding generates an output image by
superimposing an input image on its segmentation map. Then, the method learns
to translate the input image to this embedded output image using a conditional
generative adversarial network, which is known as quite effective for
image-to-image translations. This proposal is different than the existing
approach that uses image reconstruction for the same regularization purpose.
The existing approach considers segmentation and image reconstruction as two
separate tasks in a multi-task network, defines their losses independently, and
combines them in a joint loss function. However, the definition of such a
function requires externally determining right contributions of the supervised
and unsupervised losses that yield balanced learning between the segmentation
and image reconstruction tasks. The proposed approach provides an easier
solution to this problem by uniting these two tasks into a single one, which
intrinsically combines their losses. We test our approach on three datasets of
histopathological images. Our experiments demonstrate that it leads to better
segmentation results in these datasets, compared to its counterparts.
</p>
<a href="http://arxiv.org/abs/2001.11202" target="_blank">arXiv:2001.11202</a> [<a href="http://arxiv.org/pdf/2001.11202" target="_blank">pdf</a>]

<h2>A Survey on String Constraint Solving. (arXiv:2002.02376v6 [cs.AI] UPDATED)</h2>
<h3>Roberto Amadini</h3>
<p>String constraint solving refers to solving combinatorial problems involving
constraints over string variables. String solving approaches have become
popular over the last years given the massive use of strings in different
application domains like formal analysis, automated testing, database query
processing, and cybersecurity. This paper reports a comprehensive survey on
string constraint solving by exploring the large number of approaches that have
been proposed over the last decades to solve string constraints.
</p>
<a href="http://arxiv.org/abs/2002.02376" target="_blank">arXiv:2002.02376</a> [<a href="http://arxiv.org/pdf/2002.02376" target="_blank">pdf</a>]

<h2>Boosting Adversarial Training with Hypersphere Embedding. (arXiv:2002.08619v3 [cs.LG] UPDATED)</h2>
<h3>Tianyu Pang, Xiao Yang, Yinpeng Dong, Kun Xu, Jun Zhu, Hang Su</h3>
<p>Adversarial training (AT) is one of the most effective defenses against
adversarial attacks for deep learning models. In this work, we advocate
incorporating the hypersphere embedding (HE) mechanism into the AT procedure by
regularizing the features onto compact manifolds, which constitutes a
lightweight yet effective module to blend in the strength of representation
learning. Our extensive analyses reveal that AT and HE are well coupled to
benefit the robustness of the adversarially trained models from several
aspects. We validate the effectiveness and adaptability of HE by embedding it
into the popular AT frameworks including PGD-AT, ALP, and TRADES, as well as
the FreeAT and FastAT strategies. In the experiments, we evaluate our methods
under a wide range of adversarial attacks on the CIFAR-10 and ImageNet
datasets, which verifies that integrating HE can consistently enhance the model
robustness for each AT framework with little extra computation.
</p>
<a href="http://arxiv.org/abs/2002.08619" target="_blank">arXiv:2002.08619</a> [<a href="http://arxiv.org/pdf/2002.08619" target="_blank">pdf</a>]

<h2>No Regret Sample Selection with Noisy Labels. (arXiv:2003.03179v4 [cs.LG] UPDATED)</h2>
<h3>H. Song, N. Mitsuo, S. Uchida, D. Suehiro</h3>
<p>Deep neural networks (DNNs) suffer from noisy-labeled data because of the
risk of overfitting. To avoid the risk, in this paper, we propose a novel DNN
training method with sample selection based on adaptive k-set selection, which
selects k (&lt; n) clean sample candidates from the whole n noisy training samples
at each epoch. It has a strong advantage of guaranteeing the performance of the
selection theoretically. Roughly speaking, a regret, which is defined by the
difference between the actual selection and the best selection, of the proposed
method is theoretically bounded, even though the best selection is unknown
until the end of all epochs. The experimental results on multiple noisy-labeled
datasets demonstrate that our sample selection strategy works effectively in
the DNN training; in fact, the proposed method achieved the best or the
second-best performance among state-of-the-art methods, while requiring a
significantly lower computational cost. The code is available at
https://github.com/songheony/TAkS.
</p>
<a href="http://arxiv.org/abs/2003.03179" target="_blank">arXiv:2003.03179</a> [<a href="http://arxiv.org/pdf/2003.03179" target="_blank">pdf</a>]

<h2>Encoder-Decoder Based Convolutional Neural Networks with Multi-Scale-Aware Modules for Crowd Counting. (arXiv:2003.05586v5 [cs.CV] UPDATED)</h2>
<h3>Pongpisit Thanasutives, Ken-ichi Fukui, Masayuki Numao, Boonserm Kijsirikul</h3>
<p>In this paper, we propose two modified neural networks based on dual path
multi-scale fusion networks (SFANet) and SegNet for accurate and efficient
crowd counting. Inspired by SFANet, the first model, which is named M-SFANet,
is attached with atrous spatial pyramid pooling (ASPP) and context-aware module
(CAN). The encoder of M-SFANet is enhanced with ASPP containing parallel atrous
convolutional layers with different sampling rates and hence able to extract
multi-scale features of the target object and incorporate larger context. To
further deal with scale variation throughout an input image, we leverage the
CAN module which adaptively encodes the scales of the contextual information.
The combination yields an effective model for counting in both dense and sparse
crowd scenes. Based on the SFANet decoder structure, M-SFANet's decoder has
dual paths, for density map and attention map generation. The second model is
called M-SegNet, which is produced by replacing the bilinear upsampling in
SFANet with max unpooling that is used in SegNet. This change provides a faster
model while providing competitive counting performance. Designed for high-speed
surveillance applications, M-SegNet has no additional multi-scale-aware module
in order to not increase the complexity. Both models are encoder-decoder based
architectures and are end-to-end trainable. We conduct extensive experiments on
five crowd counting datasets and one vehicle counting dataset to show that
these modifications yield algorithms that could improve state-of-the-art crowd
counting methods. Codes are available at
https://github.com/Pongpisit-Thanasutives/Variations-of-SFANet-for-Crowd-Counting.
</p>
<a href="http://arxiv.org/abs/2003.05586" target="_blank">arXiv:2003.05586</a> [<a href="http://arxiv.org/pdf/2003.05586" target="_blank">pdf</a>]

<h2>Occlusion-Aware Depth Estimation with Adaptive Normal Constraints. (arXiv:2004.00845v3 [cs.CV] UPDATED)</h2>
<h3>Xiaoxiao Long, Lingjie Liu, Christian Theobalt, Wenping Wang</h3>
<p>We present a new learning-based method for multi-frame depth estimation from
a color video, which is a fundamental problem in scene understanding, robot
navigation or handheld 3D reconstruction. While recent learning-based methods
estimate depth at high accuracy, 3D point clouds exported from their depth maps
often fail to preserve important geometric feature (e.g., corners, edges,
planes) of man-made scenes. Widely-used pixel-wise depth errors do not
specifically penalize inconsistency on these features. These inaccuracies are
particularly severe when subsequent depth reconstructions are accumulated in an
attempt to scan a full environment with man-made objects with this kind of
features. Our depth estimation algorithm therefore introduces a Combined Normal
Map (CNM) constraint, which is designed to better preserve high-curvature
features and global planar regions. In order to further improve the depth
estimation accuracy, we introduce a new occlusion-aware strategy that
aggregates initial depth predictions from multiple adjacent views into one
final depth map and one occlusion probability map for the current reference
view. Our method outperforms the state-of-the-art in terms of depth estimation
accuracy, and preserves essential geometric features of man-made indoor scenes
much better than other algorithms.
</p>
<a href="http://arxiv.org/abs/2004.00845" target="_blank">arXiv:2004.00845</a> [<a href="http://arxiv.org/pdf/2004.00845" target="_blank">pdf</a>]

<h2>Combinatorial 3D Shape Generation via Sequential Assembly. (arXiv:2004.07414v2 [cs.CV] UPDATED)</h2>
<h3>Jungtaek Kim, Hyunsoo Chung, Jinhwi Lee, Minsu Cho, Jaesik Park</h3>
<p>Sequential assembly with geometric primitives has drawn attention in robotics
and 3D vision since it yields a practical blueprint to construct a target
shape. However, due to its combinatorial property, a greedy method falls short
of generating a sequence of volumetric primitives. To alleviate this
consequence induced by a huge number of feasible combinations, we propose a
combinatorial 3D shape generation framework. The proposed framework reflects an
important aspect of human generation processes in real life -- we often create
a 3D shape by sequentially assembling unit primitives with geometric
constraints. To find the desired combination regarding combination evaluations,
we adopt Bayesian optimization, which is able to exploit and explore
efficiently the feasible regions constrained by the current primitive
placements. An evaluation function conveys global structure guidance for an
assembly process and stability in terms of gravity and external forces
simultaneously. Experimental results demonstrate that our method successfully
generates combinatorial 3D shapes and simulates more realistic generation
processes. We also introduce a new dataset for combinatorial 3D shape
generation. All the codes are available at
\url{https://github.com/POSTECH-CVLab/Combinatorial-3D-Shape-Generation}.
</p>
<a href="http://arxiv.org/abs/2004.07414" target="_blank">arXiv:2004.07414</a> [<a href="http://arxiv.org/pdf/2004.07414" target="_blank">pdf</a>]

<h2>WoodFisher: Efficient Second-Order Approximation for Neural Network Compression. (arXiv:2004.14340v5 [cs.LG] UPDATED)</h2>
<h3>Sidak Pal Singh, Dan Alistarh</h3>
<p>Second-order information, in the form of Hessian- or Inverse-Hessian-vector
products, is a fundamental tool for solving optimization problems. Recently,
there has been significant interest in utilizing this information in the
context of deep neural networks; however, relatively little is known about the
quality of existing approximations in this context. Our work examines this
question, identifies issues with existing approaches, and proposes a method
called WoodFisher to compute a faithful and efficient estimate of the inverse
Hessian.

Our main application is to neural network compression, where we build on the
classic Optimal Brain Damage/Surgeon framework. We demonstrate that WoodFisher
significantly outperforms popular state-of-the-art methods for one-shot
pruning. Further, even when iterative, gradual pruning is considered, our
method results in a gain in test accuracy over the state-of-the-art approaches,
for pruning popular neural networks (like ResNet-50, MobileNetV1) trained on
standard image classification datasets such as ImageNet ILSVRC. We examine how
our method can be extended to take into account first-order information, as
well as illustrate its ability to automatically set layer-wise pruning
thresholds and perform compression in the limited-data regime. The code is
available at the following link, https://github.com/IST-DASLab/WoodFisher.
</p>
<a href="http://arxiv.org/abs/2004.14340" target="_blank">arXiv:2004.14340</a> [<a href="http://arxiv.org/pdf/2004.14340" target="_blank">pdf</a>]

<h2>Learning programs by learning from failures. (arXiv:2005.02259v3 [cs.AI] UPDATED)</h2>
<h3>Andrew Cropper, Rolf Morel</h3>
<p>We describe an inductive logic programming (ILP) approach called learning
from failures. In this approach, an ILP system (the learner) decomposes the
learning problem into three separate stages: generate, test, and constrain. In
the generate stage, the learner generates a hypothesis (a logic program) that
satisfies a set of hypothesis constraints (constraints on the syntactic form of
hypotheses). In the test stage, the learner tests the hypothesis against
training examples. A hypothesis fails when it does not entail all the positive
examples or entails a negative example. If a hypothesis fails, then, in the
constrain stage, the learner learns constraints from the failed hypothesis to
prune the hypothesis space, i.e. to constrain subsequent hypothesis generation.
For instance, if a hypothesis is too general (entails a negative example), the
constraints prune generalisations of the hypothesis. If a hypothesis is too
specific (does not entail all the positive examples), the constraints prune
specialisations of the hypothesis. This loop repeats until either (i) the
learner finds a hypothesis that entails all the positive and none of the
negative examples, or (ii) there are no more hypotheses to test. We introduce
Popper, an ILP system that implements this approach by combining answer set
programming and Prolog. Popper supports infinite problem domains, reasoning
about lists and numbers, learning textually minimal programs, and learning
recursive programs. Our experimental results on three domains (toy game
problems, robot strategies, and list transformations) show that (i) constraints
drastically improve learning performance, and (ii) Popper can outperform
existing ILP systems, both in terms of predictive accuracies and learning
times.
</p>
<a href="http://arxiv.org/abs/2005.02259" target="_blank">arXiv:2005.02259</a> [<a href="http://arxiv.org/pdf/2005.02259" target="_blank">pdf</a>]

<h2>ADAHESSIAN: An Adaptive Second Order Optimizer for Machine Learning. (arXiv:2006.00719v2 [cs.LG] UPDATED)</h2>
<h3>Zhewei Yao, Amir Gholami, Sheng Shen, Mustafa Mustafa, Kurt Keutzer, Michael W. Mahoney</h3>
<p>We introduce ADAHESSIAN, a second order stochastic optimization algorithm
which dynamically incorporates the curvature of the loss function via ADAptive
estimates of the HESSIAN. Second order algorithms are among the most powerful
optimization algorithms with superior convergence properties as compared to
first order methods such as SGD and Adam. The main disadvantage of traditional
second order methods is their heavier per iteration computation and poor
accuracy as compared to first order methods. To address these, we incorporate
several novel approaches in ADAHESSIAN, including: (i) a fast Hutchinson based
method to approximate the curvature matrix with low computational overhead;
(ii) a root-mean-square exponential moving average to smooth out variations of
the Hessian diagonal across different iterations; and (iii) a block diagonal
averaging to reduce the variance of Hessian diagonal elements. We show that
ADAHESSIAN achieves new state-of-the-art results by a large margin as compared
to other adaptive optimization methods, including variants of Adam. In
particular, we perform extensive tests on CV, NLP, and recommendation system
tasks and find that ADAHESSIAN: (i) achieves 1.80%/1.45% higher accuracy on
ResNets20/32 on Cifar10, and 5.55% higher accuracy on ImageNet as compared to
Adam; (ii) outperforms AdamW for transformers by 0.13/0.33 BLEU score on
IWSLT14/WMT14 and 2.7/1.0 PPL on PTB/Wikitext-103; (iii) outperforms AdamW for
SqueezeBert by 0.41 points on GLUE; and (iv) achieves 0.032% better score than
Adagrad for DLRM on the Criteo Ad Kaggle dataset. Importantly, we show that the
cost per iteration of ADAHESSIAN is comparable to first order methods, and that
it exhibits robustness towards its hyperparameters.
</p>
<a href="http://arxiv.org/abs/2006.00719" target="_blank">arXiv:2006.00719</a> [<a href="http://arxiv.org/pdf/2006.00719" target="_blank">pdf</a>]

<h2>Analysis of Regularized Least Squares in Reproducing Kernel Krein Spaces. (arXiv:2006.01073v2 [stat.ML] UPDATED)</h2>
<h3>Fanghui Liu, Lei Shi, Xiaolin Huang, Jie Yang, Johan A.K. Suykens</h3>
<p>In this paper, we study the asymptotic properties of regularized least
squares with indefinite kernels in reproducing kernel Krein spaces (RKKS). By
introducing a bounded hyper-sphere constraint to such non-convex regularized
risk minimization problem, we theoretically demonstrate that this problem has a
globally optimal solution with a closed form on the sphere, which makes
approximation analysis feasible in RKKS. Regarding to the original regularizer
induced by the indefinite inner product, we modify traditional error
decomposition techniques, prove convergence results for the introduced
hypothesis error based on matrix perturbation theory, and derive learning rates
of such regularized regression problem in RKKS. Under some conditions, the
derived learning rates in RKKS are the same as that in reproducing kernel
Hilbert spaces (RKHS), which is actually the first work on approximation
analysis of regularized learning algorithms in RKKS.
</p>
<a href="http://arxiv.org/abs/2006.01073" target="_blank">arXiv:2006.01073</a> [<a href="http://arxiv.org/pdf/2006.01073" target="_blank">pdf</a>]

<h2>Carath\'eodory Sampling for Stochastic Gradient Descent. (arXiv:2006.01819v2 [cs.LG] UPDATED)</h2>
<h3>Francesco Cosentino, Harald Oberhauser, Alessandro Abate</h3>
<p>Many problems require to optimize empirical risk functions over large data
sets. Gradient descent methods that calculate the full gradient in every
descent step do not scale to such datasets. Various flavours of Stochastic
Gradient Descent (SGD) replace the expensive summation that computes the full
gradient by approximating it with a small sum over a randomly selected
subsample of the data set that in turn suffers from a high variance. We present
a different approach that is inspired by classical results of Tchakaloff and
Carath\'eodory about measure reduction. These results allow to replace an
empirical measure with another, carefully constructed probability measure that
has a much smaller support, but can preserve certain statistics such as the
expected gradient. To turn this into scalable algorithms we firstly, adaptively
select the descent steps where the measure reduction is carried out; secondly,
we combine this with Block Coordinate Descent so that measure reduction can be
done very cheaply. This makes the resulting methods scalable to
high-dimensional spaces. Finally, we provide an experimental validation and
comparison.
</p>
<a href="http://arxiv.org/abs/2006.01819" target="_blank">arXiv:2006.01819</a> [<a href="http://arxiv.org/pdf/2006.01819" target="_blank">pdf</a>]

<h2>A Sparse and Locally Coherent Morphable Face Model for Dense Semantic Correspondence Across Heterogeneous 3D Faces. (arXiv:2006.03840v2 [cs.CV] UPDATED)</h2>
<h3>Claudio Ferrari, Stefano Berretti, Pietro Pala, Alberto Del Bimbo</h3>
<p>The 3D Morphable Model (3DMM) is a powerful statistical tool for representing
3D face shapes. To build a 3DMM, a training set of scans in full point-to-point
correspondence is required, and its modeling capabilities directly depend on
the variability of the training data. Hence, to increase the descriptive power
of a 3DMM, accurately establishing dense correspondence across heterogeneous
scans with sufficient diversity in terms of identities, ethnicities, or
expressions becomes essential. In this manuscript, we present a fully automatic
approach that leverages a 3DMM to establish a dense correspondence across raw
3D faces. We propose a novel formulation to learn a set of sparse deformation
components with local support on the face that, together with an original
non-rigid deformation algorithm, allow the 3DMM to precisely fit unseen faces
and transfer its semantic annotation to arbitrary 3D faces. We experimented our
approach on three large and diverse datasets, showing it can effectively
generalize to very different samples and accurately establish a dense
correspondence even in presence of complex facial expressions. The accuracy of
the dense registration is demonstrated by building a heterogeneous, large-scale
3DMM from more than 9,000 fully registered scans obtained by joining the three
datasets.
</p>
<a href="http://arxiv.org/abs/2006.03840" target="_blank">arXiv:2006.03840</a> [<a href="http://arxiv.org/pdf/2006.03840" target="_blank">pdf</a>]

<h2>DeepGG: a Deep Graph Generator. (arXiv:2006.04159v2 [cs.LG] UPDATED)</h2>
<h3>Julian Stier, Michael Granitzer</h3>
<p>Learning distributions of graphs can be used for automatic drug discovery,
molecular design, complex network analysis, and much more. We present an
improved framework for learning generative models of graphs based on the idea
of deep state machines. To learn state transition decisions we use a set of
graph and node embedding techniques as memory of the state machine.

Our analysis is based on learning the distribution of random graph generators
for which we provide statistical tests to determine which properties can be
learned and how well the original distribution of graphs is represented. We
show that the design of the state machine favors specific distributions. Models
of graphs of size up to 150 vertices are learned. Code and parameters are
publicly available to reproduce our results.
</p>
<a href="http://arxiv.org/abs/2006.04159" target="_blank">arXiv:2006.04159</a> [<a href="http://arxiv.org/pdf/2006.04159" target="_blank">pdf</a>]

<h2>Projection Robust Wasserstein Distance and Riemannian Optimization. (arXiv:2006.07458v5 [cs.LG] UPDATED)</h2>
<h3>Tianyi Lin, Chenyou Fan, Nhat Ho, Marco Cuturi, Michael I. Jordan</h3>
<p>Projection robust Wasserstein (PRW) distance, or Wasserstein projection
pursuit (WPP), is a robust variant of the Wasserstein distance. Recent work
suggests that this quantity is more robust than the standard Wasserstein
distance, in particular when comparing probability measures in high-dimensions.
However, it is ruled out for practical application because the optimization
model is essentially non-convex and non-smooth which makes the computation
intractable. Our contribution in this paper is to revisit the original
motivation behind WPP/PRW, but take the hard route of showing that, despite its
non-convexity and lack of nonsmoothness, and even despite some hardness results
proved by~\citet{Niles-2019-Estimation} in a minimax sense, the original
formulation for PRW/WPP \textit{can} be efficiently computed in practice using
Riemannian optimization, yielding in relevant cases better behavior than its
convex relaxation. More specifically, we provide three simple algorithms with
solid theoretical guarantee on their complexity bound (one in the appendix),
and demonstrate their effectiveness and efficiency by conducing extensive
experiments on synthetic and real data. This paper provides a first step into a
computational theory of the PRW distance and provides the links between optimal
transport and Riemannian optimization.
</p>
<a href="http://arxiv.org/abs/2006.07458" target="_blank">arXiv:2006.07458</a> [<a href="http://arxiv.org/pdf/2006.07458" target="_blank">pdf</a>]

<h2>Longitudinal Variational Autoencoder. (arXiv:2006.09763v2 [stat.ML] UPDATED)</h2>
<h3>Siddharth Ramchandran, Gleb Tikhonov, Kalle Kujanp&#xe4;&#xe4;, Miika Koskinen, Harri L&#xe4;hdesm&#xe4;ki</h3>
<p>Longitudinal datasets measured repeatedly over time from individual subjects,
arise in many biomedical, psychological, social, and other studies. A common
approach to analyse high-dimensional data that contains missing values is to
learn a low-dimensional representation using variational autoencoders (VAEs).
However, standard VAEs assume that the learnt representations are i.i.d., and
fail to capture the correlations between the data samples. We propose the
Longitudinal VAE (L-VAE), that uses a multi-output additive Gaussian process
(GP) prior to extend the VAE's capability to learn structured low-dimensional
representations imposed by auxiliary covariate information, and derive a new KL
divergence upper bound for such GPs. Our approach can simultaneously
accommodate both time-varying shared and random effects, produce structured
low-dimensional representations, disentangle effects of individual covariates
or their interactions, and achieve highly accurate predictive performance. We
compare our model against previous methods on synthetic as well as clinical
datasets, and demonstrate the state-of-the-art performance in data imputation,
reconstruction, and long-term prediction tasks.
</p>
<a href="http://arxiv.org/abs/2006.09763" target="_blank">arXiv:2006.09763</a> [<a href="http://arxiv.org/pdf/2006.09763" target="_blank">pdf</a>]

<h2>An Online Method for Distributionally Deep Robust Optimization. (arXiv:2006.10138v4 [cs.LG] UPDATED)</h2>
<h3>Qi Qi, Zhishuai Guo, Yi Xu, Rong Jin, Tianbao Yang</h3>
<p>In this paper, we propose a practical online method for solving a
distributionally robust optimization (DRO) for deep learning, which has
important applications in machine learning for improving the robustness of
neural networks. In the literature, most methods for solving DRO are based on
stochastic primal-dual methods. However, primal-dual methods for deep DRO
suffer from several drawbacks: (1) manipulating a high-dimensional dual
variable corresponding to the size of data is time expensive; (2) they are not
friendly to online learning where data is coming sequentially. To address these
issues, we transform the min-max formulation into a minimization formulation
and propose a practical duality-free online stochastic method for solving deep
DRO with KL divergence regularization. The proposed online stochastic method
resembles the practical stochastic Nesterovs method in several perspectives
that are widely used for learning deep neural networks. Under a
Polyak-Lojasiewicz (PL) condition, we prove that the proposed method can enjoy
an optimal sample complexity without any requirements on large batch size. Of
independent interest, the proposed method can be also used for solving a family
of stochastic compositional problems.
</p>
<a href="http://arxiv.org/abs/2006.10138" target="_blank">arXiv:2006.10138</a> [<a href="http://arxiv.org/pdf/2006.10138" target="_blank">pdf</a>]

<h2>SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks. (arXiv:2006.10503v3 [cs.LG] UPDATED)</h2>
<h3>Fabian B. Fuchs, Daniel E. Worrall, Volker Fischer, Max Welling</h3>
<p>We introduce the SE(3)-Transformer, a variant of the self-attention module
for 3D point clouds and graphs, which is equivariant under continuous 3D
roto-translations. Equivariance is important to ensure stable and predictable
performance in the presence of nuisance transformations of the data input. A
positive corollary of equivariance is increased weight-tying within the model.
The SE(3)-Transformer leverages the benefits of self-attention to operate on
large point clouds and graphs with varying number of points, while guaranteeing
SE(3)-equivariance for robustness. We evaluate our model on a toy N-body
particle simulation dataset, showcasing the robustness of the predictions under
rotations of the input. We further achieve competitive performance on two
real-world datasets, ScanObjectNN and QM9. In all cases, our model outperforms
a strong, non-equivariant attention baseline and an equivariant model without
attention.
</p>
<a href="http://arxiv.org/abs/2006.10503" target="_blank">arXiv:2006.10503</a> [<a href="http://arxiv.org/pdf/2006.10503" target="_blank">pdf</a>]

<h2>OGAN: Disrupting Deepfakes with an Adversarial Attack that Survives Training. (arXiv:2006.12247v2 [cs.CV] UPDATED)</h2>
<h3>Eran Segalis, Eran Galili</h3>
<p>Recent advances in autoencoders and generative models have given rise to
effective video forgery methods, used for generating so-called "deepfakes".
Mitigation research is mostly focused on post-factum deepfake detection and not
on prevention. We complement these efforts by introducing a novel class of
adversarial attacks---training-resistant attacks---which can disrupt
face-swapping autoencoders whether or not its adversarial images have been
included in the training set of said autoencoders. We propose the Oscillating
GAN (OGAN) attack, a novel attack optimized to be training-resistant, which
introduces spatial-temporal distortions to the output of face-swapping
autoencoders. To implement OGAN, we construct a bilevel optimization problem,
where we train a generator and a face-swapping model instance against each
other. Specifically, we pair each input image with a target distortion, and
feed them into a generator that produces an adversarial image. This image will
exhibit the distortion when a face-swapping autoencoder is applied to it. We
solve the optimization problem by training the generator and the face-swapping
model simultaneously using an iterative process of alternating optimization.
Next, we analyze the previously published Distorting Attack and show it is
training-resistant, though it is outperformed by our suggested OGAN. Finally,
we validate both attacks using a popular implementation of FaceSwap, and show
that they transfer across different target models and target faces, including
faces the adversarial attacks were not trained on. More broadly, these results
demonstrate the existence of training-resistant adversarial attacks,
potentially applicable to a wide range of domains.
</p>
<a href="http://arxiv.org/abs/2006.12247" target="_blank">arXiv:2006.12247</a> [<a href="http://arxiv.org/pdf/2006.12247" target="_blank">pdf</a>]

<h2>Gaussian Process Regression with Local Explanation. (arXiv:2007.01669v2 [cs.LG] UPDATED)</h2>
<h3>Yuya Yoshikawa, Tomoharu Iwata</h3>
<p>Gaussian process regression (GPR) is a fundamental model used in machine
learning. Owing to its accurate prediction with uncertainty and versatility in
handling various data structures via kernels, GPR has been successfully used in
various applications. However, in GPR, how the features of an input contribute
to its prediction cannot be interpreted. Herein, we propose GPR with local
explanation, which reveals the feature contributions to the prediction of each
sample, while maintaining the predictive performance of GPR. In the proposed
model, both the prediction and explanation for each sample are performed using
an easy-to-interpret locally linear model. The weight vector of the locally
linear model is assumed to be generated from multivariate Gaussian process
priors. The hyperparameters of the proposed models are estimated by maximizing
the marginal likelihood. For a new test sample, the proposed model can predict
the values of its target variable and weight vector, as well as their
uncertainties, in a closed form. Experimental results on various benchmark
datasets verify that the proposed model can achieve predictive performance
comparable to those of GPR and superior to that of existing interpretable
models, and can achieve higher interpretability than them, both quantitatively
and qualitatively.
</p>
<a href="http://arxiv.org/abs/2007.01669" target="_blank">arXiv:2007.01669</a> [<a href="http://arxiv.org/pdf/2007.01669" target="_blank">pdf</a>]

<h2>Efficient Learning of Generative Models via Finite-Difference Score Matching. (arXiv:2007.03317v2 [cs.LG] UPDATED)</h2>
<h3>Tianyu Pang, Kun Xu, Chongxuan Li, Yang Song, Stefano Ermon, Jun Zhu</h3>
<p>Several machine learning applications involve the optimization of
higher-order derivatives (e.g., gradients of gradients) during training, which
can be expensive in respect to memory and computation even with automatic
differentiation. As a typical example in generative modeling, score matching
(SM) involves the optimization of the trace of a Hessian. To improve computing
efficiency, we rewrite the SM objective and its variants in terms of
directional derivatives, and present a generic strategy to efficiently
approximate any-order directional derivative with finite difference (FD). Our
approximation only involves function evaluations, which can be executed in
parallel, and no gradient computations. Thus, it reduces the total
computational cost while also improving numerical stability. We provide two
instantiations by reformulating variants of SM objectives into the FD forms.
Empirically, we demonstrate that our methods produce results comparable to the
gradient-based counterparts while being much more computationally efficient.
</p>
<a href="http://arxiv.org/abs/2007.03317" target="_blank">arXiv:2007.03317</a> [<a href="http://arxiv.org/pdf/2007.03317" target="_blank">pdf</a>]

<h2>AutoAssign: Differentiable Label Assignment for Dense Object Detection. (arXiv:2007.03496v3 [cs.CV] UPDATED)</h2>
<h3>Benjin Zhu, Jianfeng Wang, Zhengkai Jiang, Fuhang Zong, Songtao Liu, Zeming Li, Jian Sun</h3>
<p>Determining positive/negative samples for object detection is known as label
assignment. Here we present an anchor-free detector named AutoAssign. It
requires little human knowledge and achieves appearance-aware through a fully
differentiable weighting mechanism. During training, to both satisfy the prior
distribution of data and adapt to category characteristics, we present Center
Weighting to adjust the category-specific prior distributions. To adapt to
object appearances, Confidence Weighting is proposed to adjust the specific
assign strategy of each instance. The two weighting modules are then combined
to generate positive and negative weights to adjust each location's confidence.
Extensive experiments on the MS COCO show that our method steadily surpasses
other best sampling strategies by large margins with various backbones.
Moreover, our best model achieves 52.1% AP, outperforming all existing
one-stage detectors. Besides, experiments on other datasets, e.g., PASCAL VOC,
Objects365, and WiderFace, demonstrate the broad applicability of AutoAssign.
</p>
<a href="http://arxiv.org/abs/2007.03496" target="_blank">arXiv:2007.03496</a> [<a href="http://arxiv.org/pdf/2007.03496" target="_blank">pdf</a>]

<h2>Detection as Regression: Certified Object Detection by Median Smoothing. (arXiv:2007.03730v3 [cs.CV] UPDATED)</h2>
<h3>Ping-yeh Chiang, Michael J. Curry, Ahmed Abdelkader, Aounon Kumar, John Dickerson, Tom Goldstein</h3>
<p>Despite the vulnerability of object detectors to adversarial attacks, very
few defenses are known to date. While adversarial training can improve the
empirical robustness of image classifiers, a direct extension to object
detection is very expensive. This work is motivated by recent progress on
certified classification by randomized smoothing. We start by presenting a
reduction from object detection to a regression problem. Then, to enable
certified regression, where standard mean smoothing fails, we propose median
smoothing, which is of independent interest. We obtain the first
model-agnostic, training-free, and certified defense for object detection
against $\ell_2$-bounded attacks.
</p>
<a href="http://arxiv.org/abs/2007.03730" target="_blank">arXiv:2007.03730</a> [<a href="http://arxiv.org/pdf/2007.03730" target="_blank">pdf</a>]

<h2>A Surgery of the Neural Architecture Evaluators. (arXiv:2008.03064v3 [cs.CV] UPDATED)</h2>
<h3>Xuefei Ning, Wenshuo Li, Zixuan Zhou, Tianchen Zhao, Shuang Liang, Yin Zheng, Huazhong Yang, Yu Wang</h3>
<p>Neural architecture search (NAS) has recently received extensive attention
due to its effectiveness in automatically designing effective neural
architectures. A major challenge in NAS is to conduct a fast and accurate
evaluation (i.e., performance estimation) of neural architectures. Commonly
used fast architecture evaluators include parameter-sharing ones and
predictor-based ones. Despite their high evaluation efficiency, the evaluation
correlation (especially of the well-performing architectures) is still
questionable. In this paper, we conduct an extensive assessment of both the
parameter-sharing and predictor-based evaluators on the NAS-Bench-201 search
space, and break up how and why different configurations and strategies
influence the fitness of the evaluators. Specifically, we develop a set of
NAS-oriented criteria to understand the behavior of fast architecture
evaluators in different training stages. And based on the findings of our
experiments, we give pieces of knowledge and suggestions to guide NAS
application and motivate further research. Codes are available at
https://github.com/walkerning/aw_nas.
</p>
<a href="http://arxiv.org/abs/2008.03064" target="_blank">arXiv:2008.03064</a> [<a href="http://arxiv.org/pdf/2008.03064" target="_blank">pdf</a>]

<h2>Variational Autoencoder for Anti-Cancer Drug Response Prediction. (arXiv:2008.09763v6 [cs.LG] UPDATED)</h2>
<h3>Hongyuan Dong, Jiaqing Xie, Zhi Jing, Dexin Ren</h3>
<p>Cancer is a primary cause of human death, but discovering drugs and tailoring
cancer therapies are expensive and time-consuming. We seek to facilitate the
discovery of new drugs and treatment strategies for cancer using variational
autoencoders (VAEs) and multi-layer perceptrons (MLPs) to predict anti-cancer
drug responses. Our model takes as input gene expression data of cancer cell
lines and anti-cancer drug molecular data and encodes these data with our {\sc
{GeneVae}} model, which is an ordinary VAE model, and a rectified junction tree
variational autoencoder ({\sc JTVae}) model, respectively. A multi-layer
perceptron processes these encoded features to produce a final prediction. Our
tests show our system attains a high average coefficient of determination
($R^{2} = 0.83$) in predicting drug responses for breast cancer cell lines and
an average $R^{2} &gt; 0.84$ for pan-cancer cell lines. Additionally, we show that
our model can generates effective drug compounds not previously used for
specific cancer cell lines.
</p>
<a href="http://arxiv.org/abs/2008.09763" target="_blank">arXiv:2008.09763</a> [<a href="http://arxiv.org/pdf/2008.09763" target="_blank">pdf</a>]

<h2>Semantic Labeling of Large-Area Geographic Regions Using Multi-View and Multi-Date Satellite Images and Noisy OSM Training Labels. (arXiv:2008.10271v2 [cs.CV] UPDATED)</h2>
<h3>Bharath Comandur, Avinash C. Kak</h3>
<p>We present a novel multi-view training framework and CNN architecture for
combining information from multiple overlapping satellite images and noisy
training labels derived from OpenStreetMap (OSM) for semantic labeling of
buildings and roads across large geographic regions (100 km$^2$). Our approach
to multi-view semantic segmentation yields a 4-7% improvement in the per-class
IoU scores compared to the traditional approaches that use the views
independently of one another. A unique (and, perhaps, surprising) property of
our system is that modifications that are added to the tail-end of the CNN for
learning from the multi-view data can be discarded at the time of inference
with a relatively small penalty in the overall performance. This implies that
the benefits of training using multiple views are absorbed by all the layers of
the network. Additionally, our approach only adds a small overhead in terms of
the GPU-memory consumption even when training with as many as 32 views per
scene. The system we present is end-to-end automated, which facilitates
comparing the classifiers trained directly on true orthophotos vis-a-vis first
training them on the off-nadir images and subsequently translating the
predicted labels to geographical coordinates. With no human supervision, our
IoU scores for the buildings and roads classes are 0.8 and 0.64 respectively
which are better than state-of-the-art approaches that use OSM labels and that
are not completely automated.
</p>
<a href="http://arxiv.org/abs/2008.10271" target="_blank">arXiv:2008.10271</a> [<a href="http://arxiv.org/pdf/2008.10271" target="_blank">pdf</a>]

<h2>GIF: Generative Interpretable Faces. (arXiv:2009.00149v2 [cs.CV] UPDATED)</h2>
<h3>Partha Ghosh, Pravir Singh Gupta, Roy Uziel, Anurag Ranjan, Michael Black, Timo Bolkart</h3>
<p>Photo-realistic visualization and animation of expressive human faces have
been a long standing challenge. 3D face modeling methods provide parametric
control but generates unrealistic images, on the other hand, generative 2D
models like GANs (Generative Adversarial Networks) output photo-realistic face
images, but lack explicit control. Recent methods gain partial control, either
by attempting to disentangle different factors in an unsupervised manner, or by
adding control post hoc to a pre-trained model. Unconditional GANs, however,
may entangle factors that are hard to undo later. We condition our generative
model on pre-defined control parameters to encourage disentanglement in the
generation process. Specifically, we condition StyleGAN2 on FLAME, a generative
3D face model. While conditioning on FLAME parameters yields unsatisfactory
results, we find that conditioning on rendered FLAME geometry and photometric
details works well. This gives us a generative 2D face model named GIF
(Generative Interpretable Faces) that offers FLAME's parametric control. Here,
interpretable refers to the semantic meaning of different parameters. Given
FLAME parameters for shape, pose, expressions, parameters for appearance,
lighting, and an additional style vector, GIF outputs photo-realistic face
images. We perform an AMT based perceptual study to quantitatively and
qualitatively evaluate how well GIF follows its conditioning. The code, data,
and trained model are publicly available for research purposes at
this http URL
</p>
<a href="http://arxiv.org/abs/2009.00149" target="_blank">arXiv:2009.00149</a> [<a href="http://arxiv.org/pdf/2009.00149" target="_blank">pdf</a>]

<h2>Towards Semantic Segmentation of Urban-Scale 3D Point Clouds: A Dataset, Benchmarks and Challenges. (arXiv:2009.03137v2 [cs.CV] UPDATED)</h2>
<h3>Qingyong Hu, Bo Yang, Sheikh Khalid, Wen Xiao, Niki Trigoni, Andrew Markham</h3>
<p>An essential prerequisite for unleashing the potential of supervised deep
learning algorithms in the area of 3D scene understanding is the availability
of large-scale and richly annotated datasets. However, publicly available
datasets are either in relative small spatial scales or have limited semantic
annotations due to the expensive cost of data acquisition and data annotation,
which severely limits the development of fine-grained semantic understanding in
the context of 3D point clouds. In this paper, we present an urban-scale
photogrammetric point cloud dataset with nearly three billion richly annotated
points, which is three times the number of labeled points than the existing
largest photogrammetric point cloud dataset. Our dataset consists of large
areas from three UK cities, covering about 7.6 km^2 of the city landscape. In
the dataset, each 3D point is labeled as one of 13 semantic classes. We
extensively evaluate the performance of state-of-the-art algorithms on our
dataset and provide a comprehensive analysis of the results. In particular, we
identify several key challenges towards urban-scale point cloud understanding.
The dataset is available at https://github.com/QingyongHu/SensatUrban.
</p>
<a href="http://arxiv.org/abs/2009.03137" target="_blank">arXiv:2009.03137</a> [<a href="http://arxiv.org/pdf/2009.03137" target="_blank">pdf</a>]

<h2>An Algorithm for Automatically Updating a Forsyth-Edwards Notation String Without an Array Board Representation. (arXiv:2009.03193v2 [cs.AI] UPDATED)</h2>
<h3>Azlan Iqbal</h3>
<p>We present an algorithm that correctly updates the Forsyth-Edwards Notation
(FEN) chessboard character string after any move is made without the need for
an intermediary array representation of the board. In particular, this relates
to software that have to do with chess, certain chess variants and possibly
even similar board games with comparable position representation. Even when
performance may be equal or inferior to using arrays, the algorithm still
provides an accurate and viable alternative to accomplishing the same thing, or
when there may be a need for additional or side processing in conjunction with
arrays. Furthermore, the end result (i.e. an updated FEN string) is immediately
ready for export to any other internal module or external program, unlike with
an intermediary array which needs to be first converted into a FEN string for
export purposes. The algorithm is especially useful when there are no existing
array-based modules to represent a visual board as it can do without them
entirely. We provide examples that demonstrate the correctness of the algorithm
given a variety of positions involving castling, en passant and pawn promotion.
</p>
<a href="http://arxiv.org/abs/2009.03193" target="_blank">arXiv:2009.03193</a> [<a href="http://arxiv.org/pdf/2009.03193" target="_blank">pdf</a>]

<h2>GOCor: Bringing Globally Optimized Correspondence Volumes into Your Neural Network. (arXiv:2009.07823v2 [cs.CV] UPDATED)</h2>
<h3>Prune Truong, Martin Danelljan, Luc Van Gool, Radu Timofte</h3>
<p>The feature correlation layer serves as a key neural network module in
numerous computer vision problems that involve dense correspondences between
image pairs. It predicts a correspondence volume by evaluating dense scalar
products between feature vectors extracted from pairs of locations in two
images. However, this point-to-point feature comparison is insufficient when
disambiguating multiple similar regions in an image, severely affecting the
performance of the end task. We propose GOCor, a fully differentiable dense
matching module, acting as a direct replacement to the feature correlation
layer. The correspondence volume generated by our module is the result of an
internal optimization procedure that explicitly accounts for similar regions in
the scene. Moreover, our approach is capable of effectively learning spatial
matching priors to resolve further matching ambiguities. We analyze our GOCor
module in extensive ablative experiments. When integrated into state-of-the-art
networks, our approach significantly outperforms the feature correlation layer
for the tasks of geometric matching, optical flow, and dense semantic matching.
The code and trained models will be made available at
github.com/PruneTruong/GOCor.
</p>
<a href="http://arxiv.org/abs/2009.07823" target="_blank">arXiv:2009.07823</a> [<a href="http://arxiv.org/pdf/2009.07823" target="_blank">pdf</a>]

<h2>Improving Delay Based Reservoir Computing via Eigenvalue Analysis. (arXiv:2009.07928v2 [cs.LG] UPDATED)</h2>
<h3>Felix K&#xf6;ster, Serhiy Yanchuk, Kathy L&#xfc;dge</h3>
<p>We analyze the reservoir computation capability of the Lang-Kobayashi system
by comparing the numerically computed recall capabilities and the eigenvalue
spectrum. We show that these two quantities are deeply connected, and thus the
reservoir computing performance is predictable by analyzing the eigenvalue
spectrum. Our results suggest that any dynamical system used as a reservoir can
be analyzed in this way as long as the reservoir perturbations are sufficiently
small. Optimal performance is found for a system with the eigenvalues having
real parts close to zero and off-resonant imaginary parts.
</p>
<a href="http://arxiv.org/abs/2009.07928" target="_blank">arXiv:2009.07928</a> [<a href="http://arxiv.org/pdf/2009.07928" target="_blank">pdf</a>]

<h2>Behavioral Repertoires for Soft Tensegrity Robots. (arXiv:2009.10864v2 [cs.RO] UPDATED)</h2>
<h3>Kyle Doney, Aikaterini Petridou, Jacob Karaul, Ali Khan, Geoffrey Liu, John Rieffel</h3>
<p>Mobile soft robots offer compelling applications in fields ranging from urban
search and rescue to planetary exploration. A critical challenge of soft
robotic control is that the nonlinear dynamics imposed by soft materials often
result in complex behaviors that are counterintuitive and hard to model or
predict. As a consequence, most behaviors for mobile soft robots are discovered
through empirical trial and error and hand-tuning. A second challenge is that
soft materials are difficult to simulate with high fidelity -- leading to a
significant reality gap when trying to discover or optimize new behaviors. In
this work we employ a Quality Diversity Algorithm running model-free on a
physical soft tensegrity robot that autonomously generates a behavioral
repertoire with no a priori knowledge of the robot dynamics, and minimal human
intervention. The resulting behavior repertoire displays a diversity of unique
locomotive gaits useful for a variety of tasks. These results help provide a
road map for increasing the behavioral capabilities of mobile soft robots
through real-world automation.
</p>
<a href="http://arxiv.org/abs/2009.10864" target="_blank">arXiv:2009.10864</a> [<a href="http://arxiv.org/pdf/2009.10864" target="_blank">pdf</a>]

<h2>How Neural Networks Extrapolate: From Feedforward to Graph Neural Networks. (arXiv:2009.11848v3 [cs.LG] UPDATED)</h2>
<h3>Keyulu Xu, Mozhi Zhang, Jingling Li, Simon S. Du, Ken-ichi Kawarabayashi, Stefanie Jegelka</h3>
<p>We study how neural networks trained by gradient descent extrapolate, i.e.,
what they learn outside the support of the training distribution. Previous
works report mixed empirical results when extrapolating with neural networks:
while multilayer perceptrons (MLPs) do not extrapolate well in certain simple
tasks, Graph Neural Network (GNN), a structured network with MLP modules, has
shown some success in more complex tasks. Working towards a theoretical
explanation, we identify conditions under which MLPs and GNNs extrapolate well.
First, we quantify the observation that ReLU MLPs quickly converge to linear
functions along any direction from the origin, which implies that ReLU MLPs do
not extrapolate most non-linear functions. But, they provably learn a linear
target function when the training distribution is sufficiently "diverse".
Second, in connection to analyzing successes and limitations of GNNs, these
results suggest a hypothesis for which we provide theoretical and empirical
evidence: the success of GNNs in extrapolating algorithmic tasks to new data
(e.g., larger graphs or edge weights) relies on encoding task-specific
non-linearities in the architecture or features. Our theoretical analysis
builds on a connection of overparameterized networks to the neural tangent
kernel. Empirically, our theory holds across different training settings.
</p>
<a href="http://arxiv.org/abs/2009.11848" target="_blank">arXiv:2009.11848</a> [<a href="http://arxiv.org/pdf/2009.11848" target="_blank">pdf</a>]

<h2>N-BEATS neural network for mid-term electricity load forecasting. (arXiv:2009.11961v2 [cs.LG] UPDATED)</h2>
<h3>Boris N. Oreshkin, Grzegorz Dudek, Pawe&#x142; Pe&#x142;ka, Ekaterina Turkina</h3>
<p>This paper addresses the mid-term electricity load forecasting (MTLF)
problem. This problem is relevant and challenging. On the one hand, MTLF
supports high-level (e.g., country level) decision-making at distant planning
horizons (e.g., month, quarter, year). Therefore, the financial impact of
associated decisions is significant and it is desirable that they be made based
on accurate forecasts. On the other hand, the country level monthly time-series
typically associated with MTLF are very complex and stochastic - including
trends, seasonality and significant random fluctuations. In this paper, we show
that our proposed deep neural network modeling approach based on the N-BEATS
neural architecture is very effective at solving the MTLF problem. N-BEATS has
high expressive power to solve non-linear stochastic forecasting problems. At
the same time, it is simple to implement and train, it does not require signal
preprocessing. We compare our approach against the set of ten baseline methods,
including classical statistical methods, machine learning and hybrid
approaches, on 35 monthly electricity demand time-series for European
countries. We show that in terms of the MAPE error metric, our method provides
statistically significant relative gain of 25% with respect to the classical
statistical methods, 28% with respect to classical machine learning methods and
14% with respect to the advanced state-of-the-art hybrid methods combining
machine learning and statistical approaches.
</p>
<a href="http://arxiv.org/abs/2009.11961" target="_blank">arXiv:2009.11961</a> [<a href="http://arxiv.org/pdf/2009.11961" target="_blank">pdf</a>]

<h2>Statistical control for spatio-temporal MEG/EEG source imaging with desparsified multi-task Lasso. (arXiv:2009.14310v2 [stat.ML] UPDATED)</h2>
<h3>J&#xe9;r&#xf4;me-Alexis Chevalier, Alexandre Gramfort, Joseph Salmon, Bertrand Thirion</h3>
<p>Detecting where and when brain regions activate in a cognitive task or in a
given clinical condition is the promise of non-invasive techniques like
magnetoencephalography (MEG) or electroencephalography (EEG). This problem,
referred to as source localization, or source imaging, poses however a
high-dimensional statistical inference challenge. While sparsity promoting
regularizations have been proposed to address the regression problem, it
remains unclear how to ensure statistical control of false detections.
Moreover, M/EEG source imaging requires to work with spatio-temporal data and
autocorrelated noise. To deal with this, we adapt the desparsified Lasso
estimator -- an estimator tailored for high dimensional linear model that
asymptotically follows a Gaussian distribution under sparsity and moderate
feature correlation assumptions -- to temporal data corrupted with
autocorrelated noise. We call it the desparsified multi-task Lasso (d-MTLasso).
We combine d-MTLasso with spatially constrained clustering to reduce data
dimension and with ensembling to mitigate the arbitrary choice of clustering;
the resulting estimator is called ensemble of clustered desparsified multi-task
Lasso (ecd-MTLasso). With respect to the current procedures, the two advantages
of ecd-MTLasso are that i)it offers statistical guarantees and ii)it allows to
trade spatial specificity for sensitivity, leading to a powerful adaptive
method. Extensive simulations on realistic head geometries, as well as
empirical results on various MEG datasets, demonstrate the high recovery
performance of ecd-MTLasso and its primary practical benefit: offer a
statistically principled way to threshold MEG/EEG source maps.
</p>
<a href="http://arxiv.org/abs/2009.14310" target="_blank">arXiv:2009.14310</a> [<a href="http://arxiv.org/pdf/2009.14310" target="_blank">pdf</a>]

<h2>A law of robustness for two-layers neural networks. (arXiv:2009.14444v2 [cs.LG] UPDATED)</h2>
<h3>S&#xe9;bastien Bubeck, Yuanzhi Li, Dheeraj Nagaraj</h3>
<p>We initiate the study of the inherent tradeoffs between the size of a neural
network and its robustness, as measured by its Lipschitz constant. We make a
precise conjecture that, for any Lipschitz activation function and for most
datasets, any two-layers neural network with $k$ neurons that perfectly fit the
data must have its Lipschitz constant larger (up to a constant) than
$\sqrt{n/k}$ where $n$ is the number of datapoints. In particular, this
conjecture implies that overparametrization is necessary for robustness, since
it means that one needs roughly one neuron per datapoint to ensure a
$O(1)$-Lipschitz network, while mere data fitting of $d$-dimensional data
requires only one neuron per $d$ datapoints. We prove a weaker version of this
conjecture when the Lipschitz constant is replaced by an upper bound on it
based on the spectral norm of the weight matrix. We also prove the conjecture
in the high-dimensional regime $n \approx d$ (which we also refer to as the
undercomplete case, since only $k \leq d$ is relevant here). Finally we prove
the conjecture for polynomial activation functions of degree $p$ when $n
\approx d^p$. We complement these findings with experimental evidence
supporting the conjecture.
</p>
<a href="http://arxiv.org/abs/2009.14444" target="_blank">arXiv:2009.14444</a> [<a href="http://arxiv.org/pdf/2009.14444" target="_blank">pdf</a>]

<h2>PettingZoo: Gym for Multi-Agent Reinforcement Learning. (arXiv:2009.14471v4 [cs.LG] UPDATED)</h2>
<h3>Justin K. Terry, Benjamin Black, Mario Jayakumar, Ananth Hari, Luis Santos, Clemens Dieffendahl, Niall L. Williams, Yashas Lokesh, Ryan Sullivan, Caroline Horsch, Praveen Ravi</h3>
<p>This paper introduces PettingZoo, a library of diverse sets of multi-agent
environments under a single elegant Python API. PettingZoo was developed with
the goal of acceleration research in multi-agent reinforcement learning, by
creating a set of benchmark environments easily accessible to all researchers
and a standardized API for the field. This goal is inspired by what OpenAI's
Gym library did for accelerating research in single-agent reinforcement
learning, and PettingZoo draws heavily from Gym in terms of API and user
experience. PettingZoo is unique from other multi-agent environment libraries
in that it's API is based on the model of Agent Environment Cycle ("AEC")
games, which allows for the sensible representation all species of games under
one API for the first time. While retaining a very simple and Gym-like API,
PettingZoo still allows access to low-level environment properties required by
non-traditional learning methods.
</p>
<a href="http://arxiv.org/abs/2009.14471" target="_blank">arXiv:2009.14471</a> [<a href="http://arxiv.org/pdf/2009.14471" target="_blank">pdf</a>]

<h2>Bag of Tricks for Adversarial Training. (arXiv:2010.00467v2 [cs.LG] UPDATED)</h2>
<h3>Tianyu Pang, Xiao Yang, Yinpeng Dong, Hang Su, Jun Zhu</h3>
<p>Adversarial training (AT) is one of the most effective strategies for
promoting model robustness. However, recent benchmarks show that most of the
proposed improvements on AT are less effective than simply early stopping the
training procedure. This counter-intuitive fact motivates us to investigate the
implementation details of tens of AT methods. Surprisingly, we find that the
basic settings (e.g., weight decay, training schedule, etc.) used in these
methods are highly inconsistent. In this work, we provide comprehensive
evaluations on CIFAR-10, focusing on the effects of mostly overlooked training
tricks and hyperparameters for adversarially trained models. Our empirical
observations suggest that adversarial robustness is much more sensitive to some
basic training settings than we thought. For example, a slightly different
value of weight decay can reduce the model robust accuracy by more than 7%,
which is probable to override the potential promotion induced by the proposed
methods. We conclude a baseline training setting and re-implement previous
defenses to achieve new state-of-the-art results. These facts also appeal to
more concerns on the overlooked confounders when benchmarking defenses.
</p>
<a href="http://arxiv.org/abs/2010.00467" target="_blank">arXiv:2010.00467</a> [<a href="http://arxiv.org/pdf/2010.00467" target="_blank">pdf</a>]

<h2>DASGIL: Domain Adaptation for Semantic and Geometric-aware Image-based Localization. (arXiv:2010.00573v2 [cs.CV] UPDATED)</h2>
<h3>Hanjiang Hu, Zhijian Qiao, Ming Cheng, Zhe Liu, Hesheng Wang</h3>
<p>Long-Term visual localization under changing environments is a challenging
problem in autonomous driving and mobile robotics due to season, illumination
variance, etc. Image retrieval for localization is an efficient and effective
solution to the problem. In this paper, we propose a novel multi-task
architecture to fuse the geometric and semantic information into the
multi-scale latent embedding representation for visual place recognition. To
use the high-quality ground truths without any human effort, the effective
multi-scale feature discriminator is proposed for adversarial training to
achieve the domain adaptation from synthetic virtual KITTI dataset to
real-world KITTI dataset. The proposed approach is validated on the Extended
CMU-Seasons dataset and Oxford RobotCar dataset through a series of crucial
comparison experiments, where our performance outperforms state-of-the-art
baselines for retrieval-based localization and large-scale place recognition
under the challenging environment.
</p>
<a href="http://arxiv.org/abs/2010.00573" target="_blank">arXiv:2010.00573</a> [<a href="http://arxiv.org/pdf/2010.00573" target="_blank">pdf</a>]

<h2>Meta-Learning of Structured Task Distributions in Humans and Machines. (arXiv:2010.02317v2 [cs.LG] UPDATED)</h2>
<h3>Sreejan Kumar, Ishita Dasgupta, Jonathan D. Cohen, Nathaniel D. Daw, Thomas L. Griffiths</h3>
<p>In recent years, meta-learning, in which a model is trained on a family of
tasks (i.e. a task distribution), has emerged as an approach to training neural
networks to perform tasks that were previously assumed to require structured
representations, making strides toward closing the gap between humans and
machines. However, we argue that evaluating meta-learning remains a challenge,
and can miss whether meta-learning actually uses the structure embedded within
the tasks. These meta-learners might therefore still be significantly different
from humans learners. To demonstrate this difference, we first define a new
meta-reinforcement learning task in which a structured task distribution is
generated using a compositional grammar. We then introduce a novel approach to
constructing a "null task distribution" with the same statistical complexity as
this structured task distribution but without the explicit rule-based structure
used to generate the structured task. We train a standard meta-learning agent,
a recurrent network trained with model-free reinforcement learning, and compare
it with human performance across the two task distributions. We find a double
dissociation in which humans do better in the structured task distribution
whereas agents do better in the null task distribution -- despite comparable
statistical complexity. This work highlights that multiple strategies can
achieve reasonable meta-test performance, and that careful construction of
control task distributions is a valuable way to understand which strategies
meta-learners acquire, and how they might differ from humans.
</p>
<a href="http://arxiv.org/abs/2010.02317" target="_blank">arXiv:2010.02317</a> [<a href="http://arxiv.org/pdf/2010.02317" target="_blank">pdf</a>]

<h2>Unfolding the Alternating Optimization for Blind Super Resolution. (arXiv:2010.02631v4 [cs.CV] UPDATED)</h2>
<h3>Zhengxiong Luo, Yan Huang, Shang Li, Liang Wang, Tieniu Tan</h3>
<p>Previous methods decompose blind super resolution (SR) problem into two
sequential steps: \textit{i}) estimating blur kernel from given low-resolution
(LR) image and \textit{ii}) restoring SR image based on estimated kernel. This
two-step solution involves two independently trained models, which may not be
well compatible with each other. Small estimation error of the first step could
cause severe performance drop of the second one. While on the other hand, the
first step can only utilize limited information from LR image, which makes it
difficult to predict highly accurate blur kernel. Towards these issues, instead
of considering these two steps separately, we adopt an alternating optimization
algorithm, which can estimate blur kernel and restore SR image in a single
model. Specifically, we design two convolutional neural modules, namely
\textit{Restorer} and \textit{Estimator}. \textit{Restorer} restores SR image
based on predicted kernel, and \textit{Estimator} estimates blur kernel with
the help of restored SR image. We alternate these two modules repeatedly and
unfold this process to form an end-to-end trainable network. In this way,
\textit{Estimator} utilizes information from both LR and SR images, which makes
the estimation of blur kernel easier. More importantly, \textit{Restorer} is
trained with the kernel estimated by \textit{Estimator}, instead of
ground-truth kernel, thus \textit{Restorer} could be more tolerant to the
estimation error of \textit{Estimator}. Extensive experiments on synthetic
datasets and real-world images show that our model can largely outperform
state-of-the-art methods and produce more visually favorable results at much
higher speed. The source code is available at
https://github.com/greatlog/DAN.git.
</p>
<a href="http://arxiv.org/abs/2010.02631" target="_blank">arXiv:2010.02631</a> [<a href="http://arxiv.org/pdf/2010.02631" target="_blank">pdf</a>]

<h2>Information Theory Measures via Multidimensional Gaussianization. (arXiv:2010.03807v2 [stat.ML] UPDATED)</h2>
<h3>Valero Laparra, J. Emmanuel Johnson, Gustau Camps-Valls, Raul Santos-Rodr&#xed;guez, Jesus Malo</h3>
<p>Information theory is an outstanding framework to measure uncertainty,
dependence and relevance in data and systems. It has several desirable
properties for real world applications: it naturally deals with multivariate
data, it can handle heterogeneous data types, and the measures can be
interpreted in physical units. However, it has not been adopted by a wider
audience because obtaining information from multidimensional data is a
challenging problem due to the curse of dimensionality. Here we propose an
indirect way of computing information based on a multivariate Gaussianization
transform. Our proposal mitigates the difficulty of multivariate density
estimation by reducing it to a composition of tractable (marginal) operations
and simple linear transformations, which can be interpreted as a particular
deep neural network. We introduce specific Gaussianization-based methodologies
to estimate total correlation, entropy, mutual information and Kullback-Leibler
divergence. We compare them to recent estimators showing the accuracy on
synthetic data generated from different multivariate distributions. We made the
tools and datasets publicly available to provide a test-bed to analyze future
methodologies. Results show that our proposal is superior to previous
estimators particularly in high-dimensional scenarios; and that it leads to
interesting insights in neuroscience, geoscience, computer vision, and machine
learning.
</p>
<a href="http://arxiv.org/abs/2010.03807" target="_blank">arXiv:2010.03807</a> [<a href="http://arxiv.org/pdf/2010.03807" target="_blank">pdf</a>]

<h2>Active Tuning. (arXiv:2010.03958v2 [cs.LG] UPDATED)</h2>
<h3>Sebastian Otte, Matthias Karlbauer, Martin V. Butz</h3>
<p>We introduce Active Tuning, a novel paradigm for optimizing the internal
dynamics of recurrent neural networks (RNNs) on the fly. In contrast to the
conventional sequence-to-sequence mapping scheme, Active Tuning decouples the
RNN's recurrent neural activities from the input stream, using the unfolding
temporal gradient signal to tune the internal dynamics into the data stream. As
a consequence, the model output depends only on its internal hidden dynamics
and the closed-loop feedback of its own predictions; its hidden state is
continuously adapted by means of the temporal gradient resulting from
backpropagating the discrepancy between the signal observations and the model
outputs through time. In this way, Active Tuning infers the signal actively but
indirectly based on the originally learned temporal patterns, fitting the most
plausible hidden state sequence into the observations. We demonstrate the
effectiveness of Active Tuning on several time series prediction benchmarks,
including multiple super-imposed sine waves, a chaotic double pendulum, and
spatiotemporal wave dynamics. Active Tuning consistently improves the
robustness, accuracy, and generalization abilities of all evaluated models.
Moreover, networks trained for signal prediction and denoising can be
successfully applied to a much larger range of noise conditions with the help
of Active Tuning. Thus, given a capable time series predictor, Active Tuning
enhances its online signal filtering, denoising, and reconstruction abilities
without the need for additional training.
</p>
<a href="http://arxiv.org/abs/2010.03958" target="_blank">arXiv:2010.03958</a> [<a href="http://arxiv.org/pdf/2010.03958" target="_blank">pdf</a>]

<h2>Parameterized Reinforcement Learning for Optical System Optimization. (arXiv:2010.05769v2 [cs.LG] UPDATED)</h2>
<h3>Heribert Wankerl, Maike L. Stern, Ali Mahdavi, Christoph Eichler, Elmar W. Lang</h3>
<p>Designing a multi-layer optical system with designated optical
characteristics is an inverse design problem in which the resulting design is
determined by several discrete and continuous parameters. In particular, we
consider three design parameters to describe a multi-layer stack: Each layer's
dielectric material and thickness as well as the total number of layers. Such a
combination of both, discrete and continuous parameters is a challenging
optimization problem that often requires a computationally expensive search for
an optimal system design. Hence, most methods merely determine the optimal
thicknesses of the system's layers. To incorporate layer material and the total
number of layers as well, we propose a method that considers the stacking of
consecutive layers as parameterized actions in a Markov decision process. We
propose an exponentially transformed reward signal that eases policy
optimization and adapt a recent variant of Q-learning for inverse design
optimization. We demonstrate that our method outperforms human experts and a
naive reinforcement learning algorithm concerning the achieved optical
characteristics. Moreover, the learned Q-values contain information about the
optical properties of multi-layer optical systems, thereby allowing physical
interpretation or what-if analysis.
</p>
<a href="http://arxiv.org/abs/2010.05769" target="_blank">arXiv:2010.05769</a> [<a href="http://arxiv.org/pdf/2010.05769" target="_blank">pdf</a>]

<h2>Gradient Descent Ascent for Min-Max Problems on Riemannian Manifolds. (arXiv:2010.06097v2 [cs.LG] UPDATED)</h2>
<h3>Feihu Huang, Shangqian Gao, Heng Huang</h3>
<p>In the paper, we study a class of useful non-convex minimax optimization
problems on Riemanian manifolds and propose a class of Riemanian gradient
descent ascent algorithms to solve these minimax problems. Specifically, we
propose a new Riemannian gradient descent ascent (RGDA) algorithm for the
deterministic minimax optimization. Moreover, we prove that the RGDA has a
sample complexity of $O(\kappa^2\epsilon^{-2})$ for finding an
$\epsilon$-stationary point of the nonconvex strongly-concave minimax problems,
where $\kappa$ denotes the condition number. At the same time, we introduce a
Riemannian stochastic gradient descent ascent (RSGDA) algorithm for the
stochastic minimax optimization. In the theoretical analysis, we prove that the
RSGDA can achieve a sample complexity of $O(\kappa^4\epsilon^{-4})$. To further
reduce the sample complexity, we propose a novel momentum variance-reduced
Riemannian stochastic gradient descent ascent (MVR-RSGDA) algorithm based on a
new momentum variance-reduced technique of STORM. We prove that the MVR-RSGDA
algorithm achieves a lower sample complexity of
$\tilde{O}(\kappa^{4}\epsilon^{-3})$ without large batches, which reaches near
the best known sample complexity for its Euclidean counterparts. Extensive
experimental results on the robust deep neural networks training over Stiefel
manifold demonstrate the efficiency of our proposed algorithms.
</p>
<a href="http://arxiv.org/abs/2010.06097" target="_blank">arXiv:2010.06097</a> [<a href="http://arxiv.org/pdf/2010.06097" target="_blank">pdf</a>]

<h2>Continuum-Armed Bandits: A Function Space Perspective. (arXiv:2010.08007v3 [stat.ML] UPDATED)</h2>
<h3>Shashank Singh</h3>
<p>Continuum-armed bandits (a.k.a., black-box or $0^{th}$-order optimization)
involves optimizing an unknown objective function given an oracle that
evaluates the function at a query point, with the goal of using as few query
points as possible. In the most well-studied case, the objective function is
assumed to be Lipschitz continuous and minimax rates of simple and cumulative
regrets are known in both noiseless and noisy settings. This paper studies
continuum-armed bandits under more general smoothness conditions, namely Besov
smoothness conditions, on the objective function. In both noiseless and noisy
conditions, we derive minimax rates under simple and cumulative regrets. Our
results show that minimax rates over objective functions in a Besov space are
identical to minimax rates over objective functions in the smallest H\"older
space into which the Besov space embeds.
</p>
<a href="http://arxiv.org/abs/2010.08007" target="_blank">arXiv:2010.08007</a> [<a href="http://arxiv.org/pdf/2010.08007" target="_blank">pdf</a>]

<h2>Sufficient dimension reduction for classification using principal optimal transport direction. (arXiv:2010.09921v3 [cs.LG] UPDATED)</h2>
<h3>Cheng Meng, Jun Yu, Jingyi Zhang, Ping Ma, Wenxuan Zhong</h3>
<p>Sufficient dimension reduction is used pervasively as a supervised dimension
reduction approach. Most existing sufficient dimension reduction methods are
developed for data with a continuous response and may have an unsatisfactory
performance for the categorical response, especially for the binary-response.
To address this issue, we propose a novel estimation method of sufficient
dimension reduction subspace (SDR subspace) using optimal transport. The
proposed method, named principal optimal transport direction (POTD), estimates
the basis of the SDR subspace using the principal directions of the optimal
transport coupling between the data respecting different response categories.
The proposed method also reveals the relationship among three seemingly
irrelevant topics, i.e., sufficient dimension reduction, support vector
machine, and optimal transport. We study the asymptotic properties of POTD and
show that in the cases when the class labels contain no error, POTD estimates
the SDR subspace exclusively. Empirical studies show POTD outperforms most of
the state-of-the-art linear dimension reduction methods.
</p>
<a href="http://arxiv.org/abs/2010.09921" target="_blank">arXiv:2010.09921</a> [<a href="http://arxiv.org/pdf/2010.09921" target="_blank">pdf</a>]

<h2>High-Dimensional Bayesian Optimization via Nested Riemannian Manifolds. (arXiv:2010.10904v3 [cs.LG] UPDATED)</h2>
<h3>No&#xe9;mie Jaquier, Leonel Rozo</h3>
<p>Despite the recent success of Bayesian optimization (BO) in a variety of
applications where sample efficiency is imperative, its performance may be
seriously compromised in settings characterized by high-dimensional parameter
spaces. A solution to preserve the sample efficiency of BO in such problems is
to introduce domain knowledge into its formulation. In this paper, we propose
to exploit the geometry of non-Euclidean search spaces, which often arise in a
variety of domains, to learn structure-preserving mappings and optimize the
acquisition function of BO in low-dimensional latent spaces. Our approach,
built on Riemannian manifolds theory, features geometry-aware Gaussian
processes that jointly learn a nested-manifold embedding and a representation
of the objective function in the latent space. We test our approach in several
benchmark artificial landscapes and report that it not only outperforms other
high-dimensional BO approaches in several settings, but consistently optimizes
the objective functions, as opposed to geometry-unaware BO methods.
</p>
<a href="http://arxiv.org/abs/2010.10904" target="_blank">arXiv:2010.10904</a> [<a href="http://arxiv.org/pdf/2010.10904" target="_blank">pdf</a>]

<h2>Generative Model-Enhanced Human Motion Prediction. (arXiv:2010.11699v3 [cs.CV] UPDATED)</h2>
<h3>Anthony Bourached, Ryan-Rhys Griffiths, Robert Gray, Ashwani Jha, Parashkev Nachev</h3>
<p>The task of predicting human motion is complicated by the natural
heterogeneity and compositionality of actions, necessitating robustness to
distributional shifts as far as out-of-distribution (OoD). Here we formulate a
new OoD benchmark based on the Human3.6M and CMU motion capture datasets, and
introduce a hybrid framework for hardening discriminative architectures to OoD
failure by augmenting them with a generative model. When applied to current
state-of-the-art discriminative models, we show that the proposed approach
improves OoD robustness without sacrificing in-distribution performance, and
can theoretically facilitate model interpretability. We suggest human motion
predictors ought to be constructed with OoD challenges in mind, and provide an
extensible general framework for hardening diverse discriminative architectures
to extreme distributional shift. The code is available at
https://github.com/bouracha/OoDMotion.
</p>
<a href="http://arxiv.org/abs/2010.11699" target="_blank">arXiv:2010.11699</a> [<a href="http://arxiv.org/pdf/2010.11699" target="_blank">pdf</a>]

<h2>Uncertainty Aware Semi-Supervised Learning on Graph Data. (arXiv:2010.12783v2 [cs.LG] UPDATED)</h2>
<h3>Xujiang Zhao, Feng Chen, Shu Hu, Jin-Hee Cho</h3>
<p>Thanks to graph neural networks (GNNs), semi-supervised node classification
has shown the state-of-the-art performance in graph data. However, GNNs have
not considered different types of uncertainties associated with class
probabilities to minimize risk of increasing misclassification under
uncertainty in real life. In this work, we propose a multi-source uncertainty
framework using a GNN that reflects various types of predictive uncertainties
in both deep learning and belief/evidence theory domains for node
classification predictions. By collecting evidence from the given labels of
training nodes, the Graph-based Kernel Dirichlet distribution Estimation (GKDE)
method is designed for accurately predicting node-level Dirichlet distributions
and detecting out-of-distribution (OOD) nodes. We validated the outperformance
of our proposed model compared to the state-of-the-art counterparts in terms of
misclassification detection and OOD detection based on six real network
datasets. We found that dissonance-based detection yielded the best results on
misclassification detection while vacuity-based detection was the best for OOD
detection. To clarify the reasons behind the results, we provided the
theoretical proof that explains the relationships between different types of
uncertainties considered in this work.
</p>
<a href="http://arxiv.org/abs/2010.12783" target="_blank">arXiv:2010.12783</a> [<a href="http://arxiv.org/pdf/2010.12783" target="_blank">pdf</a>]

<h2>A Novel Machine Learning Method for Preference Identification. (arXiv:2010.13517v2 [cs.AI] UPDATED)</h2>
<h3>Azlan Iqbal</h3>
<p>Human preference or taste within any domain is usually a difficult thing to
identify or predict with high probability. In the domain of chess problem
composition, the same is true. Traditional machine learning approaches tend to
focus on the ability of computers to process massive amounts of data and
continuously adjust 'weights' within an artificial neural network to better
distinguish between say, two groups of objects. Contrasted with chess
compositions, there is no clear distinction between what constitutes one and
what does not; even less so between a good one and a poor one. We propose a
computational method that is able to learn from existing databases of 'liked'
and 'disliked' compositions such that a new and unseen collection can be sorted
with increased probability of matching a solver's preferences. The method uses
a simple 'change factor' relating to the Forsyth-Edwards Notation (FEN) of each
composition's starting position, coupled with repeated statistical analysis of
sample pairs from both databases. Tested using the author's own collections of
computer-generated chess problems, the experimental results showed that the
method was able to sort a new and unseen collection of compositions such that,
on average, over 70% of the preferred compositions were in the top half of the
collection. This saves significant time and energy on the part of solvers as
they are likely to find more of what they like sooner. The method may even be
applicable to other domains such as image processing because it does not rely
on any chess-specific rules but rather just a sufficient and quantifiable
'change' in representation from one object to the next.
</p>
<a href="http://arxiv.org/abs/2010.13517" target="_blank">arXiv:2010.13517</a> [<a href="http://arxiv.org/pdf/2010.13517" target="_blank">pdf</a>]

<h2>Latent Causal Invariant Model. (arXiv:2011.02203v2 [cs.LG] UPDATED)</h2>
<h3>Xinwei Sun, Botong Wu, Chang Liu, Xiangyu Zheng, Wei Chen, Tao Qin, Tie-yan Liu</h3>
<p>Current supervised learning can learn spurious correlation during the
data-fitting process, imposing issues regarding interpretability,
out-of-distribution (OOD) generalization, and robustness. To avoid spurious
correlation, we propose a Latent Causal Invariance Model (LaCIM) which pursues
causal prediction. Specifically, we introduce latent variables that are
separated into (a) output-causative factors and (b) others that are spuriously
correlated to the output via confounders, to model the underlying causal
factors. We further assume the generating mechanisms from latent space to
observed data to be causally invariant. We give the identifiable claim of such
invariance, particularly the disentanglement of output-causative factors from
others, as a theoretical guarantee for precise inference and avoiding spurious
correlation. We propose a Variational-Bayesian-based method for estimation and
to optimize over the latent space for prediction. The utility of our approach
is verified by improved interpretability, prediction power on various OOD
scenarios (including healthcare) and robustness on security.
</p>
<a href="http://arxiv.org/abs/2011.02203" target="_blank">arXiv:2011.02203</a> [<a href="http://arxiv.org/pdf/2011.02203" target="_blank">pdf</a>]

<h2>Underspecification Presents Challenges for Credibility in Modern Machine Learning. (arXiv:2011.03395v2 [cs.LG] UPDATED)</h2>
<h3>Alexander D&#x27;Amour, Katherine Heller, Dan Moldovan, Ben Adlam, Babak Alipanahi, Alex Beutel, Christina Chen, Jonathan Deaton, Jacob Eisenstein, Matthew D. Hoffman, Farhad Hormozdiari, Neil Houlsby, Shaobo Hou, Ghassen Jerfel, Alan Karthikesalingam, Mario Lucic, Yian Ma, Cory McLean, Diana Mincu, Akinori Mitani, Andrea Montanari, Zachary Nado, Vivek Natarajan, Christopher Nielson, Thomas F. Osborne, Rajiv Raman, Kim Ramasamy, Rory Sayres, Jessica Schrouff, Martin Seneviratne, Shannon Sequeira, Harini Suresh, Victor Veitch, Max Vladymyrov, Xuezhi Wang, Kellie Webster, Steve Yadlowsky, Taedong Yun, Xiaohua Zhai, D. Sculley</h3>
<p>ML models often exhibit unexpectedly poor behavior when they are deployed in
real-world domains. We identify underspecification as a key reason for these
failures. An ML pipeline is underspecified when it can return many predictors
with equivalently strong held-out performance in the training domain.
Underspecification is common in modern ML pipelines, such as those based on
deep learning. Predictors returned by underspecified pipelines are often
treated as equivalent based on their training domain performance, but we show
here that such predictors can behave very differently in deployment domains.
This ambiguity can lead to instability and poor model behavior in practice, and
is a distinct failure mode from previously identified issues arising from
structural mismatch between training and deployment domains. We show that this
problem appears in a wide variety of practical ML pipelines, using examples
from computer vision, medical imaging, natural language processing, clinical
risk prediction based on electronic health records, and medical genomics. Our
results show the need to explicitly account for underspecification in modeling
pipelines that are intended for real-world deployment in any domain.
</p>
<a href="http://arxiv.org/abs/2011.03395" target="_blank">arXiv:2011.03395</a> [<a href="http://arxiv.org/pdf/2011.03395" target="_blank">pdf</a>]

<h2>Ecole: A Gym-like Library for Machine Learning in Combinatorial Optimization Solvers. (arXiv:2011.06069v2 [cs.LG] UPDATED)</h2>
<h3>Antoine Prouvost, Justin Dumouchelle, Lara Scavuzzo, Maxime Gasse, Didier Ch&#xe9;telat, Andrea Lodi</h3>
<p>We present Ecole, a new library to simplify machine learning research for
combinatorial optimization. Ecole exposes several key decision tasks arising in
general-purpose combinatorial optimization solvers as control problems over
Markov decision processes. Its interface mimics the popular OpenAI Gym library
and is both extensible and intuitive to use. We aim at making this library a
standardized platform that will lower the bar of entry and accelerate
innovation in the field. Documentation and code can be found at
https://www.ecole.ai.
</p>
<a href="http://arxiv.org/abs/2011.06069" target="_blank">arXiv:2011.06069</a> [<a href="http://arxiv.org/pdf/2011.06069" target="_blank">pdf</a>]

<h2>DSAM: A Distance Shrinking with Angular Marginalizing Loss for High Performance Vehicle Re-identificatio. (arXiv:2011.06228v2 [cs.CV] UPDATED)</h2>
<h3>Jiangtao Kong, Yu Cheng, Benjia Zhou, Kai Li, Junliang Xing</h3>
<p>Vehicle Re-identification (ReID) is an important yet challenging problem in
computer vision. Compared to other visual objects like faces and persons,
vehicles simultaneously exhibit much larger intraclass viewpoint variations and
interclass visual similarities, making most exiting loss functions designed for
face recognition and person ReID unsuitable for vehicle ReID. To obtain a
high-performance vehicle ReID model, we present a novel Distance Shrinking with
Angular Marginalizing (DSAM) loss function to perform hybrid learning in both
the Original Feature Space (OFS) and the Feature Angular Space (FAS) using the
local verification and the global identification information. Specifically, it
shrinks the distance between samples of the same class locally in the Original
Feature Space while keeps samples of different classes far away in the Feature
Angular Space. The shrinking and marginalizing operations are performed during
each iteration of the training process and are suitable for different SoftMax
based loss functions. We evaluate the DSAM loss function on three large vehicle
ReID datasets with detailed analyses and extensive comparisons with many
competing vehicle ReID methods. Experimental results show that our DSAM loss
enhances the SoftMax loss by a large margin on the PKU-VD1-Large dataset:
10.41% for mAP, 5.29% for cmc1, and 4.60% for cmc5. Moreover, the mAP is
increased by 9.34% on the PKU-VehicleID dataset and 8.73% on the VeRi-776
dataset. Source code will be released to facilitate further studies in this
research direction.
</p>
<a href="http://arxiv.org/abs/2011.06228" target="_blank">arXiv:2011.06228</a> [<a href="http://arxiv.org/pdf/2011.06228" target="_blank">pdf</a>]

<h2>Active Reinforcement Learning: Observing Rewards at a Cost. (arXiv:2011.06709v2 [cs.LG] UPDATED)</h2>
<h3>David Krueger, Jan Leike, Owain Evans, John Salvatier</h3>
<p>Active reinforcement learning (ARL) is a variant on reinforcement learning
where the agent does not observe the reward unless it chooses to pay a query
cost c &gt; 0. The central question of ARL is how to quantify the long-term value
of reward information. Even in multi-armed bandits, computing the value of this
information is intractable and we have to rely on heuristics. We propose and
evaluate several heuristic approaches for ARL in multi-armed bandits and
(tabular) Markov decision processes, and discuss and illustrate some
challenging aspects of the ARL problem.
</p>
<a href="http://arxiv.org/abs/2011.06709" target="_blank">arXiv:2011.06709</a> [<a href="http://arxiv.org/pdf/2011.06709" target="_blank">pdf</a>]

<h2>Filter Pre-Pruning for Improved Fine-tuning of Quantized Deep Neural Networks. (arXiv:2011.06751v2 [cs.CV] UPDATED)</h2>
<h3>Jun Nishikawa, Ryoji Ikegaya</h3>
<p>Deep Neural Networks(DNNs) have many parameters and activation data, and
these both are expensive to implement. One method to reduce the size of the DNN
is to quantize the pre-trained model by using a low-bit expression for weights
and activations, using fine-tuning to recover the drop in accuracy. However, it
is generally difficult to train neural networks which use low-bit expressions.
One reason is that the weights in the middle layer of the DNN have a wide
dynamic range and so when quantizing the wide dynamic range into a few bits,
the step size becomes large, which leads to a large quantization error and
finally a large degradation in accuracy. To solve this problem, this paper
makes the following three contributions without using any additional learning
parameters and hyper-parameters. First, we analyze how batch normalization,
which causes the aforementioned problem, disturbs the fine-tuning of the
quantized DNN. Second, based on these results, we propose a new pruning method
called Pruning for Quantization (PfQ) which removes the filters that disturb
the fine-tuning of the DNN while not affecting the inferred result as far as
possible. Third, we propose a workflow of fine-tuning for quantized DNNs using
the proposed pruning method(PfQ). Experiments using well-known models and
datasets confirmed that the proposed method achieves higher performance with a
similar model size than conventional quantization methods including
fine-tuning.
</p>
<a href="http://arxiv.org/abs/2011.06751" target="_blank">arXiv:2011.06751</a> [<a href="http://arxiv.org/pdf/2011.06751" target="_blank">pdf</a>]

<h2>Feature Sharing and Integration for Cooperative Cognition and Perception with Volumetric Sensors. (arXiv:2011.08317v2 [cs.CV] UPDATED)</h2>
<h3>Ehsan Emad Marvasti, Arash Raftari, Yaser P.Fallah, Rui Guo, Hongsheng Lu</h3>
<p>The recent advancement in computational and communication systems has led to
the introduction of high-performing neural networks and high-speed wireless
vehicular communication networks. As a result, new technologies such as
cooperative perception and cognition have emerged, addressing the inherent
limitations of sensory devices by providing solutions for the detection of
partially occluded targets and expanding the sensing range. However, designing
a reliable cooperative cognition or perception system requires addressing the
challenges caused by limited network resources and discrepancies between the
data shared by different sources. In this paper, we examine the requirements,
limitations, and performance of different cooperative perception techniques,
and present an in-depth analysis of the notion of Deep Feature Sharing (DFS).
We explore different cooperative object detection designs and evaluate their
performance in terms of average precision. We use the Volony dataset for our
experimental study. The results confirm that the DFS methods are significantly
less sensitive to the localization error caused by GPS noise. Furthermore, the
results attest that detection gain of DFS methods caused by adding more
cooperative participants in the scenes is comparable to raw information sharing
technique while DFS enables flexibility in design toward satisfying
communication requirements.
</p>
<a href="http://arxiv.org/abs/2011.08317" target="_blank">arXiv:2011.08317</a> [<a href="http://arxiv.org/pdf/2011.08317" target="_blank">pdf</a>]

<h2>An Efficient and Scalable Deep Learning Approach for Road Damage Detection. (arXiv:2011.09577v2 [cs.CV] UPDATED)</h2>
<h3>Sadra Naddaf-Sh, M-Mahdi Naddaf-Sh, Amir R. Kashani, Hassan Zargarzadeh</h3>
<p>Pavement condition evaluation is essential to time the preventative or
rehabilitative actions and control distress propagation. Failing to conduct
timely evaluations can lead to severe structural and financial loss of the
infrastructure and complete reconstructions. Automated computer-aided surveying
measures can provide a database of road damage patterns and their locations.
This database can be utilized for timely road repairs to gain the minimum cost
of maintenance and the asphalt's maximum durability. This paper introduces a
deep learning-based surveying scheme to analyze the image-based distress data
in real-time. A database consisting of a diverse population of crack distress
types such as longitudinal, transverse, and alligator cracks, photographed
using mobile-device is used. Then, a family of efficient and scalable models
that are tuned for pavement crack detection is trained, and various
augmentation policies are explored. Proposed models, resulted in F1-scores,
ranging from 52% to 56%, and average inference time from 178-10 images per
second. Finally, the performance of the object detectors are examined, and
error analysis is reported against various images. The source code is available
at https://github.com/mahdi65/roadDamageDetection2020.
</p>
<a href="http://arxiv.org/abs/2011.09577" target="_blank">arXiv:2011.09577</a> [<a href="http://arxiv.org/pdf/2011.09577" target="_blank">pdf</a>]

<h2>FedEval: A Benchmark System with a Comprehensive Evaluation Model for Federated Learning. (arXiv:2011.09655v2 [cs.LG] UPDATED)</h2>
<h3>Di Chai, Leye Wang, Kai Chen, Qiang Yang</h3>
<p>As an innovative solution for privacy-preserving machine learning (ML),
federated learning (FL) is attracting much attention from research and industry
areas. While new technologies proposed in the past few years do evolve the FL
area, unfortunately, the evaluation results presented in these works fall short
in integrity and are hardly comparable because of the inconsistent evaluation
metrics and the lack of a common platform. In this paper, we propose a
comprehensive evaluation framework for FL systems. Specifically, we first
introduce the ACTPR model, which defines five metrics that cannot be excluded
in FL evaluation, including Accuracy, Communication, Time efficiency, Privacy,
and Robustness. Then we design and implement a benchmarking system called
FedEval, which enables the systematic evaluation and comparison of existing
works under consistent experimental conditions. We then provide an in-depth
benchmarking study between the two most widely-used FL mechanisms, FedSGD and
FedAvg. The benchmarking results show that FedSGD and FedAvg both have
advantages and disadvantages under the ACTPR model. For example, FedSGD is
barely influenced by the none independent and identically distributed (non-IID)
data problem, but FedAvg suffers from a decline in accuracy of up to 9% in our
experiments. On the other hand, FedAvg is more efficient than FedSGD regarding
time consumption and communication. Lastly, we excavate a set of take-away
conclusions, which are very helpful for researchers in the FL area.
</p>
<a href="http://arxiv.org/abs/2011.09655" target="_blank">arXiv:2011.09655</a> [<a href="http://arxiv.org/pdf/2011.09655" target="_blank">pdf</a>]

<h2>Deep Reinforcement Learning for Feedback Control in a Collective Flashing Ratchet. (arXiv:2011.10357v2 [cs.LG] UPDATED)</h2>
<h3>Dong-Kyum Kim, Hawoong Jeong</h3>
<p>A collective flashing ratchet transports Brownian particles using a spatially
periodic, asymmetric, and time-dependent on-off switchable potential. The net
current of the particles in this system can be substantially increased by
feedback control based on the particle positions. Several feedback policies for
maximizing the current have been proposed, but optimal policies have not been
found for a moderate number of particles. Here, we use deep reinforcement
learning (RL) to find optimal policies, with results showing that policies
built with a suitable neural network architecture outperform the previous
policies. Moreover, even in a time-delayed feedback situation where the on-off
switching of the potential is delayed, we demonstrate that the policies
provided by deep RL provide higher currents than the previous strategies.
</p>
<a href="http://arxiv.org/abs/2011.10357" target="_blank">arXiv:2011.10357</a> [<a href="http://arxiv.org/pdf/2011.10357" target="_blank">pdf</a>]

<h2>RobustPointSet: A Dataset for Benchmarking Robustness of Point Cloud Classifiers. (arXiv:2011.11572v3 [cs.CV] UPDATED)</h2>
<h3>Saeid Asgari Taghanaki, Jieliang Luo, Ran Zhang, Ye Wang, Pradeep Kumar Jayaraman, Krishna Murthy Jatavallabhula</h3>
<p>The 3D deep learning community has seen significant strides in pointcloud
processing over the last few years. However, the datasets on which deep models
have been trained have largely remained the same. Most datasets comprise clean,
clutter-free pointclouds canonicalized for pose. Models trained on these
datasets fail in uninterpretible and unintuitive ways when presented with data
that contains transformations "unseen" at train time. While data augmentation
enables models to be robust to "previously seen" input transformations, 1) we
show that this does not work for unseen transformations during inference, and
2) data augmentation makes it difficult to analyze a model's inherent
robustness to transformations. To this end, we create a publicly available
dataset for robustness analysis of point cloud classification models
(independent of data augmentation) to input transformations, called
RobustPointSet. Our experiments indicate that despite all the progress in the
point cloud classification, there is no single architecture that consistently
performs better---several fail drastically---when evaluated on transformed test
sets. We also find that robustness to unseen transformations cannot be brought
about merely by extensive data augmentation. RobustPointSet can be accessed
through https://github.com/AutodeskAILab/RobustPointSet.
</p>
<a href="http://arxiv.org/abs/2011.11572" target="_blank">arXiv:2011.11572</a> [<a href="http://arxiv.org/pdf/2011.11572" target="_blank">pdf</a>]

<h2>Scattering Transform Based Image Clustering using Projection onto Orthogonal Complement. (arXiv:2011.11586v2 [cs.CV] UPDATED)</h2>
<h3>Angel Villar-Corrales, Veniamin I. Morgenshtern</h3>
<p>In the last few years, large improvements in image clustering have been
driven by the recent advances in deep learning. However, due to the
architectural complexity of deep neural networks, there is no mathematical
theory that explains the success of deep clustering techniques. In this work we
introduce Projected-Scattering Spectral Clustering (PSSC), a state-of-the-art,
stable, and fast algorithm for image clustering, which is also mathematically
interpretable. PSSC includes a novel method to exploit the geometric structure
of the scattering transform of small images. This method is inspired by the
observation that, in the scattering transform domain, the subspaces formed by
the eigenvectors corresponding to the few largest eigenvalues of the data
matrices of individual classes are nearly shared among different classes.
Therefore, projecting out those shared subspaces reduces the intra-class
variability, substantially increasing the clustering performance. We call this
method Projection onto Orthogonal Complement (POC). Our experiments demonstrate
that PSSC obtains the best results among all shallow clustering algorithms.
Moreover, it achieves comparable clustering performance to that of recent
state-of-the-art clustering techniques, while reducing the execution time by
more than one order of magnitude. In the spirit of reproducible research, we
publish a high quality code repository along with the paper.
</p>
<a href="http://arxiv.org/abs/2011.11586" target="_blank">arXiv:2011.11586</a> [<a href="http://arxiv.org/pdf/2011.11586" target="_blank">pdf</a>]

<h2>Dissecting Image Crops. (arXiv:2011.11831v2 [cs.CV] UPDATED)</h2>
<h3>Basile Van Hoorick, Carl Vondrick</h3>
<p>The elementary operation of cropping underpins nearly every computer vision
system, ranging from data augmentation and translation invariance to
computational photography and representation learning. This paper investigates
the subtle traces introduced by this operation. For example, despite
refinements to camera optics, lenses will leave behind certain clues, notably
chromatic aberration and vignetting. Photographers also leave behind other
clues relating to image aesthetics and scene composition. We study how to
detect these traces, and investigate the impact that cropping has on the image
distribution. While our aim is to dissect the fundamental impact of spatial
crops, there are also a number of practical implications to our work, such as
detecting image manipulations and equipping neural network researchers with a
better understanding of shortcut learning. Code is available at
https://github.com/basilevh/dissecting-image-crops.
</p>
<a href="http://arxiv.org/abs/2011.11831" target="_blank">arXiv:2011.11831</a> [<a href="http://arxiv.org/pdf/2011.11831" target="_blank">pdf</a>]

<h2>CLAWS: Clustering Assisted Weakly Supervised Learning with Normalcy Suppression for Anomalous Event Detection. (arXiv:2011.12077v2 [cs.CV] UPDATED)</h2>
<h3>Muhammad Zaigham Zaheer, Arif Mahmood, Marcella Astrid, Seung-Ik Lee</h3>
<p>Learning to detect real-world anomalous events through video-level labels is
a challenging task due to the rare occurrence of anomalies as well as noise in
the labels. In this work, we propose a weakly supervised anomaly detection
method which has manifold contributions including1) a random batch based
training procedure to reduce inter-batch correlation, 2) a normalcy suppression
mechanism to minimize anomaly scores of the normal regions of a video by taking
into account the overall information available in one training batch, and 3) a
clustering distance based loss to contribute towards mitigating the label noise
and to produce better anomaly representations by encouraging our model to
generate distinct normal and anomalous clusters. The proposed method
obtains83.03% and 89.67% frame-level AUC performance on the UCF Crime and
ShanghaiTech datasets respectively, demonstrating its superiority over the
existing state-of-the-art algorithms.
</p>
<a href="http://arxiv.org/abs/2011.12077" target="_blank">arXiv:2011.12077</a> [<a href="http://arxiv.org/pdf/2011.12077" target="_blank">pdf</a>]

<h2>Differentially Private Learning Needs Better Features (or Much More Data). (arXiv:2011.11660v1 [cs.LG] CROSS LISTED)</h2>
<h3>Florian Tram&#xe8;r, Dan Boneh</h3>
<p>We demonstrate that differentially private machine learning has not yet
reached its "AlexNet moment" on many canonical vision tasks: linear models
trained on handcrafted features significantly outperform end-to-end deep neural
networks for moderate privacy budgets. To exceed the performance of handcrafted
features, we show that private learning requires either much more private data,
or access to features learned on public data from a similar domain. Our work
introduces simple yet strong baselines for differentially private learning that
can inform the evaluation of future progress in this area.
</p>
<a href="http://arxiv.org/abs/2011.11660" target="_blank">arXiv:2011.11660</a> [<a href="http://arxiv.org/pdf/2011.11660" target="_blank">pdf</a>]

