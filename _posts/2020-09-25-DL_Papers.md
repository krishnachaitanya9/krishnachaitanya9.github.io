---
title: Latest Deep Learning Papers
date: 2020-12-03 21:00:01 +0800
featured-img: DL-logo
categories: [Deep_Learning]
tags: [Deep_Learning]
mathjax: Yes
---

<h1>Your arXiv Feed (240 Articles)</h1>
<h2>Cycloidal Trajectory Realization on Staircase with Optimal Trajectory Tracking Control based on Neural Network Temporal Quantized Lagrange Dynamics (NNTQLD). (arXiv:2012.01417v1 [cs.RO])</h2>
<h3>Gaurav Bhardwaj, Utkarsh A. Mishra, N. Sukavanam, R. Balasubramanian</h3>
<p>In this paper, a novel optimal technique for joint angles trajectory tracking
control of a biped robot with toe foot is proposed. For the task of climbing
stairs by a 9 link biped model, a cycloid trajectory for swing phase is
proposed in such a way that the cycloid variables depend on the staircase
dimensions. Zero Moment Point(ZMP) criteria is taken for satisfying stability
constraint. This paper mainly can be divided into 4 steps: 1) Planning stable
cycloid trajectory for initial step and subsequent step for climbing upstairs.
2) Inverse Kinematics using unsupervised artificial neural network with knot
shifting procedure for jerk minimization. 3) Modeling Dynamics for Toe foot
biped model using Lagrange Dynamics along with contact modeling using spring
damper system , and finally 4) Real time joint angle trajectory tracking
optimization using Temporal Quantized Lagrange Dynamics which takes inverse
kinematics output from neural network as its inputs. Generated patterns have
been simulated in MATLAB.
</p>
<a href="http://arxiv.org/abs/2012.01417" target="_blank">arXiv:2012.01417</a> [<a href="http://arxiv.org/pdf/2012.01417" target="_blank">pdf</a>]

<h2>Neural Deformation Graphs for Globally-consistent Non-rigid Reconstruction. (arXiv:2012.01451v1 [cs.CV])</h2>
<h3>Alja&#x17e; Bo&#x17e;i&#x10d;, Pablo Palafox, Michael Zollh&#xf6;fer, Justus Thies, Angela Dai, Matthias Nie&#xdf;ner</h3>
<p>We introduce Neural Deformation Graphs for globally-consistent deformation
tracking and 3D reconstruction of non-rigid objects. Specifically, we
implicitly model a deformation graph via a deep neural network. This neural
deformation graph does not rely on any object-specific structure and, thus, can
be applied to general non-rigid deformation tracking. Our method globally
optimizes this neural graph on a given sequence of depth camera observations of
a non-rigidly moving object. Based on explicit viewpoint consistency as well as
inter-frame graph and surface consistency constraints, the underlying network
is trained in a self-supervised fashion. We additionally optimize for the
geometry of the object with an implicit deformable multi-MLP shape
representation. Our approach does not assume sequential input data, thus
enabling robust tracking of fast motions or even temporally disconnected
recordings. Our experiments demonstrate that our Neural Deformation Graphs
outperform state-of-the-art non-rigid reconstruction approaches both
qualitatively and quantitatively, with 64% improved reconstruction and 62%
improved deformation tracking performance.
</p>
<a href="http://arxiv.org/abs/2012.01451" target="_blank">arXiv:2012.01451</a> [<a href="http://arxiv.org/pdf/2012.01451" target="_blank">pdf</a>]

<h2>ACE-Net: Fine-Level Face Alignment through Anchors and Contours Estimation. (arXiv:2012.01461v1 [cs.CV])</h2>
<h3>Jihua Huang, Amir Tamrakar</h3>
<p>We propose a novel facial Anchors and Contours Estimation framework, ACE-Net,
for fine-level face alignment tasks. ACE-Net predicts facial anchors and
contours that are richer than traditional facial landmarks and more accurate
than facial boundaries. In addition, it does not suffer from the ambiguities
and inconsistencies in facial-landmarks definitions. We introduce a weakly
supervised loss enabling ACE-Net to learn from existing facial landmarks
datasets without the need for extra annotations. Synthetic data is also used
during training to bridge the density gap between landmarks annotation and true
facial contours. We evaluate ACE-Net on commonly used face alignment datasets
300-W and HELEN, and show that ACE-Net achieves significantly higher fine-level
face alignment accuracy than landmarks based models, without compromising its
performance at the landmarks level. The proposed ACE-Net framework does not
rely on any specific network architecture and thus can be applied on top of
existing face alignment models for finer face alignment representation.
</p>
<a href="http://arxiv.org/abs/2012.01461" target="_blank">arXiv:2012.01461</a> [<a href="http://arxiv.org/pdf/2012.01461" target="_blank">pdf</a>]

<h2>Video Anomaly Detection by Estimating Likelihood of Representations. (arXiv:2012.01468v1 [cs.CV])</h2>
<h3>Yuqi Ouyang, Victor Sanchez</h3>
<p>Video anomaly detection is a challenging task not only because it involves
solving many sub-tasks such as motion representation, object localization and
action recognition, but also because it is commonly considered as an
unsupervised learning problem that involves detecting outliers. Traditionally,
solutions to this task have focused on the mapping between video frames and
their low-dimensional features, while ignoring the spatial connections of those
features. Recent solutions focus on analyzing these spatial connections by
using hard clustering techniques, such as K-Means, or applying neural networks
to map latent features to a general understanding, such as action attributes.
In order to solve video anomaly in the latent feature space, we propose a deep
probabilistic model to transfer this task into a density estimation problem
where latent manifolds are generated by a deep denoising autoencoder and
clustered by expectation maximization. Evaluations on several benchmarks
datasets show the strengths of our model, achieving outstanding performance on
challenging datasets.
</p>
<a href="http://arxiv.org/abs/2012.01468" target="_blank">arXiv:2012.01468</a> [<a href="http://arxiv.org/pdf/2012.01468" target="_blank">pdf</a>]

<h2>Fair Attribute Classification through Latent Space De-biasing. (arXiv:2012.01469v1 [cs.CV])</h2>
<h3>Vikram V. Ramaswamy, Sunnis S. Y. Kim, Olga Russakovsky</h3>
<p>Fairness in visual recognition is becoming a prominent and critical topic of
discussion as recognition systems are deployed at scale in the real world.
Models trained from data in which target labels are correlated with protected
attributes (e.g., gender, race) are known to learn and exploit those
correlations. In this work, we introduce a method for training accurate target
classifiers while mitigating biases that stem from these correlations. We use
GANs to generate realistic-looking images, and perturb these images in the
underlying latent space to generate training data that is balanced for each
protected attribute. We augment the original dataset with this perturbed
generated data, and empirically demonstrate that target classifiers trained on
the augmented dataset exhibit a number of both quantitative and qualitative
benefits. We conduct a thorough evaluation across multiple target labels and
protected attributes in the CelebA dataset, and provide an in-depth analysis
and comparison to existing literature in the space.
</p>
<a href="http://arxiv.org/abs/2012.01469" target="_blank">arXiv:2012.01469</a> [<a href="http://arxiv.org/pdf/2012.01469" target="_blank">pdf</a>]

<h2>Second-Order Guarantees in Federated Learning. (arXiv:2012.01474v1 [cs.LG])</h2>
<h3>Stefan Vlaski, Elsa Rizk, Ali H. Sayed</h3>
<p>Federated learning is a useful framework for centralized learning from
distributed data under practical considerations of heterogeneity, asynchrony,
and privacy. Federated architectures are frequently deployed in deep learning
settings, which generally give rise to non-convex optimization problems.
Nevertheless, most existing analysis are either limited to convex loss
functions, or only establish first-order stationarity, despite the fact that
saddle-points, which are first-order stationary, are known to pose bottlenecks
in deep learning. We draw on recent results on the second-order optimality of
stochastic gradient algorithms in centralized and decentralized settings, and
establish second-order guarantees for a class of federated learning algorithms.
</p>
<a href="http://arxiv.org/abs/2012.01474" target="_blank">arXiv:2012.01474</a> [<a href="http://arxiv.org/pdf/2012.01474" target="_blank">pdf</a>]

<h2>Estimation of Trocar and Tool Interaction Forces on the da Vinci ResearchKit with Two-Step Deep Learning. (arXiv:2012.01479v1 [cs.RO])</h2>
<h3>Jie Ying Wu, Nural Yilmaz, Peter Kazanzides, Ugur Tumerdem</h3>
<p>Measurement of environment interaction forces during robotic
minimally-invasive surgery would enable haptic feedback to the surgeon, thereby
solving one long-standing limitation. Estimating this force from existing
sensor data avoids the challenge of retrofitting systems with force sensors,
but is difficult due to mechanical effects such as friction and compliance in
the robot mechanism. We have previously shown that neural networks can be
trained to estimate the internal robot joint torques, thereby enabling
estimation of external forces. In this work, we extend the method to estimate
external Cartesian forces and torques, and also present a two-step approach to
adapt to the specific surgical setup by compensating for forces due to the
interactions between the instrument shaft and cannula seal and between the
trocar and patient body. Experiments show that this approach provides estimates
of external forces and torques within a mean root-mean-square error (RMSE) of 2
N and 0.08 Nm, respectively. Furthermore, the two-step approach can add as
little as 5 minutes to the surgery setup time, with about 4 minutes to collect
intraoperative training data and 1 minute to train the second-step network.
</p>
<a href="http://arxiv.org/abs/2012.01479" target="_blank">arXiv:2012.01479</a> [<a href="http://arxiv.org/pdf/2012.01479" target="_blank">pdf</a>]

<h2>Contour Transformer Network for One-shot Segmentation of Anatomical Structures. (arXiv:2012.01480v1 [cs.CV])</h2>
<h3>Yuhang Lu, Kang Zheng, Weijian Li, Yirui Wang, Adam P. Harrison, Chihung Lin, Song Wang, Jing Xiao, Le Lu, Chang-Fu Kuo, Shun Miao</h3>
<p>Accurate segmentation of anatomical structures is vital for medical image
analysis. The state-of-the-art accuracy is typically achieved by supervised
learning methods, where gathering the requisite expert-labeled image
annotations in a scalable manner remains a main obstacle. Therefore,
annotation-efficient methods that permit to produce accurate anatomical
structure segmentation are highly desirable. In this work, we present Contour
Transformer Network (CTN), a one-shot anatomy segmentation method with a
naturally built-in human-in-the-loop mechanism. We formulate anatomy
segmentation as a contour evolution process and model the evolution behavior by
graph convolutional networks (GCNs). Training the CTN model requires only one
labeled image exemplar and leverages additional unlabeled data through newly
introduced loss functions that measure the global shape and appearance
consistency of contours. On segmentation tasks of four different anatomies, we
demonstrate that our one-shot learning method significantly outperforms
non-learning-based methods and performs competitively to the state-of-the-art
fully supervised deep learning methods. With minimal human-in-the-loop editing
feedback, the segmentation performance can be further improved to surpass the
fully supervised methods.
</p>
<a href="http://arxiv.org/abs/2012.01480" target="_blank">arXiv:2012.01480</a> [<a href="http://arxiv.org/pdf/2012.01480" target="_blank">pdf</a>]

<h2>Data-driven Analysis of Turbulent Flame Images. (arXiv:2012.01485v1 [cs.CV])</h2>
<h3>Rathziel Roncancio, Jupyoung Kim, Aly El Gamal, Jay P. Gore</h3>
<p>Turbulent premixed flames are important for power generation using gas
turbines. Improvements in characterization and understanding of turbulent
flames continue particularly for transient events like ignition and extinction.
Pockets or islands of unburned material are features of turbulent flames during
these events. These features are directly linked to heat release rates and
hydrocarbons emissions. Unburned material pockets in turbulent CH$_4$/air
premixed flames with CO$_2$ addition were investigated using OH Planar
Laser-Induced Fluorescence images. Convolutional Neural Networks (CNN) were
used to classify images containing unburned pockets for three turbulent flames
with 0%, 5%, and 10% CO$_2$ addition. The CNN model was constructed using three
convolutional layers and two fully connected layers using dropout and weight
decay. The CNN model achieved accuracies of 91.72%, 89.35% and 85.80% for the
three flames, respectively.
</p>
<a href="http://arxiv.org/abs/2012.01485" target="_blank">arXiv:2012.01485</a> [<a href="http://arxiv.org/pdf/2012.01485" target="_blank">pdf</a>]

<h2>Distributed Machine Learning for Wireless Communication Networks: Techniques, Architectures, and Applications. (arXiv:2012.01489v1 [cs.LG])</h2>
<h3>S. Hu, X. Chen, W. Ni, E. Hossain, X. Wang</h3>
<p>Distributed machine learning (DML) techniques, such as federated learning,
partitioned learning, and distributed reinforcement learning, have been
increasingly applied to wireless communications. This is due to improved
capabilities of terminal devices, explosively growing data volume, congestion
in the radio interfaces, and increasing concern of data privacy. The unique
features of wireless systems, such as large scale, geographically dispersed
deployment, user mobility, and massive amount of data, give rise to new
challenges in the design of DML techniques. There is a clear gap in the
existing literature in that the DML techniques are yet to be systematically
reviewed for their applicability to wireless systems. This survey bridges the
gap by providing a contemporary and comprehensive survey of DML techniques with
a focus on wireless networks. Specifically, we review the latest applications
of DML in power control, spectrum management, user association, and edge cloud
computing. The optimality, scalability, convergence rate, computation cost, and
communication overhead of DML are analyzed. We also discuss the potential
adversarial attacks faced by DML applications, and describe state-of-the-art
countermeasures to preserve privacy and security. Last but not least, we point
out a number of key issues yet to be addressed, and collate potentially
interesting and challenging topics for future research.
</p>
<a href="http://arxiv.org/abs/2012.01489" target="_blank">arXiv:2012.01489</a> [<a href="http://arxiv.org/pdf/2012.01489" target="_blank">pdf</a>]

<h2>Sample Complexity of Policy Gradient Finding Second-Order Stationary Points. (arXiv:2012.01491v1 [cs.LG])</h2>
<h3>Long Yang, Qian Zheng, Gang Pan</h3>
<p>The goal of policy-based reinforcement learning (RL) is to search the maximal
point of its objective. However, due to the inherent non-concavity of its
objective, convergence to a first-order stationary point (FOSP) can not
guarantee the policy gradient methods finding a maximal point. A FOSP can be a
minimal or even a saddle point, which is undesirable for RL. Fortunately, if
all the saddle points are \emph{strict}, all the second-order stationary points
(SOSP) are exactly equivalent to local maxima. Instead of FOSP, we consider
SOSP as the convergence criteria to character the sample complexity of policy
gradient. Our result shows that policy gradient converges to an
$(\epsilon,\sqrt{\epsilon\chi})$-SOSP with probability at least
$1-\widetilde{\mathcal{O}}(\delta)$ after the total cost of
$\mathcal{O}\left(\dfrac{\epsilon^{-\frac{9}{2}}}{(1-\gamma)\sqrt\chi}\log\dfrac{1}{\delta}\right)$,
where $\gamma\in(0,1)$. Our result improves the state-of-the-art result
significantly where it requires
$\mathcal{O}\left(\dfrac{\epsilon^{-9}\chi^{\frac{3}{2}}}{\delta}\log\dfrac{1}{\epsilon\chi}\right)$.
Our analysis is based on the key idea that decomposes the parameter space
$\mathbb{R}^p$ into three non-intersected regions: non-stationary point, saddle
point, and local optimal region, then making a local improvement of the
objective of RL in each region. This technique can be potentially generalized
to extensive policy gradient methods.
</p>
<a href="http://arxiv.org/abs/2012.01491" target="_blank">arXiv:2012.01491</a> [<a href="http://arxiv.org/pdf/2012.01491" target="_blank">pdf</a>]

<h2>Braille to Text Translation for Bengali Language: A Geometric Approach. (arXiv:2012.01494v1 [cs.CV])</h2>
<h3>Minhas Kamal, Dr. Amin Ahsan Ali, Dr. Muhammad Asif Hossain Khan, Dr. Mohammad Shoyaib</h3>
<p>Braille is the only system to visually impaired people for reading and
writing. However, general people cannot read Braille. So, teachers and
relatives find it hard to assist them with learning. Almost every major
language has software solutions for this translation purpose. However, in
Bengali there is an absence of this useful tool. Here, we propose Braille to
Text Translator, which takes image of these tactile alphabets, and translates
them to plain text. Image deterioration, scan-time page rotation, and braille
dot deformation are the principal issues in this scheme. All of these
challenges are directly checked using special image processing and geometric
structure analysis. The technique yields 97.25% accuracy in recognizing Braille
characters.
</p>
<a href="http://arxiv.org/abs/2012.01494" target="_blank">arXiv:2012.01494</a> [<a href="http://arxiv.org/pdf/2012.01494" target="_blank">pdf</a>]

<h2>Instance-Sensitive Algorithms for Pure Exploration in Multinomial Logit Bandit. (arXiv:2012.01499v1 [cs.LG])</h2>
<h3>Nikolai Karpov, Qin Zhang</h3>
<p>Motivated by real-world applications such as fast fashion retailing and
online advertising, the Multinomial Logit Bandit (MNL-bandit) is a popular
model in online learning and operations research, and has attracted much
attention in the past decade. However, it is a bit surprising that pure
exploration, a basic problem in bandit theory, has not been well studied in
MNL-bandit so far. In this paper we give efficient algorithms for pure
exploration in MNL-bandit. Our algorithms achieve instance-sensitive pull
complexities. We also complement the upper bounds by an almost matching lower
bound.
</p>
<a href="http://arxiv.org/abs/2012.01499" target="_blank">arXiv:2012.01499</a> [<a href="http://arxiv.org/pdf/2012.01499" target="_blank">pdf</a>]

<h2>Fine-Grained Few-Shot Classification with Feature Map Reconstruction Networks. (arXiv:2012.01506v1 [cs.CV])</h2>
<h3>Davis Wertheimer, Luming Tang, Bharath Hariharan</h3>
<p>In this paper we reformulate few-shot classification as a reconstruction
problem in latent space. The ability of the network to reconstruct a query
feature map from support features of a given class predicts membership of the
query in that class. We introduce a novel mechanism for few-shot classification
by regressing directly from support features to query features in closed form,
without introducing any new modules or large-scale learnable parameters. The
resulting Feature Map Reconstruction Networks are both more performant and
computationally efficient than previous approaches. We demonstrate consistent
and significant accuracy gains on four fine-grained benchmarks with varying
neural architectures. Our model is also competitive on the non-fine-grained
mini-ImageNet benchmark with minimal bells and whistles.
</p>
<a href="http://arxiv.org/abs/2012.01506" target="_blank">arXiv:2012.01506</a> [<a href="http://arxiv.org/pdf/2012.01506" target="_blank">pdf</a>]

<h2>DecisiveNets: Training Deep Associative Memories to Solve Complex Machine Learning Problems. (arXiv:2012.01509v1 [cs.LG])</h2>
<h3>Vincent Gripon, Carlos Lassance, Ghouthi Boukli Hacene</h3>
<p>Learning deep representations to solve complex machine learning tasks has
become the prominent trend in the past few years. Indeed, Deep Neural Networks
are now the golden standard in domains as various as computer vision, natural
language processing or even playing combinatorial games. However, problematic
limitations are hidden behind this surprising universal capability. Among other
things, explainability of the decisions is a major concern, especially since
deep neural networks are made up of a very large number of trainable
parameters. Moreover, computational complexity can quickly become a problem,
especially in contexts constrained by real time or limited resources.
Therefore, understanding how information is stored and the impact this storage
can have on the system remains a major and open issue. In this chapter, we
introduce a method to transform deep neural network models into deep
associative memories, with simpler, more explicable and less expensive
operations. We show through experiments that these transformations can be done
without penalty on predictive performance. The resulting deep associative
memories are excellent candidates for artificial intelligence that is easier to
theorize and manipulate.
</p>
<a href="http://arxiv.org/abs/2012.01509" target="_blank">arXiv:2012.01509</a> [<a href="http://arxiv.org/pdf/2012.01509" target="_blank">pdf</a>]

<h2>Unsupervised Learning on Monocular Videos for 3D Human Pose Estimation. (arXiv:2012.01511v1 [cs.CV])</h2>
<h3>Sina Honari, Victor Constantin, Helge Rhodin, Mathieu Salzmann, Pascal Fua</h3>
<p>In this paper, we introduce an unsupervised feature extraction method that
exploits contrastive self-supervised (CSS) learning to extract rich latent
vectors from single-view videos. Instead of simply treating the latent features
of nearby frames as positive pairs and those of temporally-distant ones as
negative pairs as in other CSS approaches, we explicitly separate each latent
vector into a time-variant component and a time-invariant one. We then show
that applying CSS only to the time-variant features, while also reconstructing
the input and encouraging a gradual transition between nearby and away features
yields a rich latent space, well-suited for human pose estimation. Our approach
outperforms other unsupervised single-view methods and match the performance of
multi-view techniques.
</p>
<a href="http://arxiv.org/abs/2012.01511" target="_blank">arXiv:2012.01511</a> [<a href="http://arxiv.org/pdf/2012.01511" target="_blank">pdf</a>]

<h2>From Goals, Waypoints & Paths To Long Term Human Trajectory Forecasting. (arXiv:2012.01526v1 [cs.CV])</h2>
<h3>Karttikeya Mangalam, Yang An, Harshayu Girase, Jitendra Malik</h3>
<p>Human trajectory forecasting is an inherently multi-modal problem.
Uncertainty in future trajectories stems from two sources: (a) sources that are
known to the agent but unknown to the model, such as long term goals and
(b)sources that are unknown to both the agent &amp; the model, such as intent of
other agents &amp; irreducible randomness indecisions. We propose to factorize this
uncertainty into its epistemic &amp; aleatoric sources. We model the epistemic
un-certainty through multimodality in long term goals and the aleatoric
uncertainty through multimodality in waypoints&amp; paths. To exemplify this
dichotomy, we also propose a novel long term trajectory forecasting setting,
with prediction horizons upto a minute, an order of magnitude longer than prior
works. Finally, we presentY-net, a scene com-pliant trajectory forecasting
network that exploits the pro-posed epistemic &amp; aleatoric structure for diverse
trajectory predictions across long prediction horizons.Y-net significantly
improves previous state-of-the-art performance on both (a) The well studied
short prediction horizon settings on the Stanford Drone &amp; ETH/UCY datasets and
(b) The proposed long prediction horizon setting on the re-purposed Stanford
Drone &amp; Intersection Drone datasets.
</p>
<a href="http://arxiv.org/abs/2012.01526" target="_blank">arXiv:2012.01526</a> [<a href="http://arxiv.org/pdf/2012.01526" target="_blank">pdf</a>]

<h2>Improving KernelSHAP: Practical Shapley Value Estimation via Linear Regression. (arXiv:2012.01536v1 [cs.LG])</h2>
<h3>Ian Covert, Su-In Lee</h3>
<p>The Shapley value solution concept from cooperative game theory has become
popular for interpreting ML models, but efficiently estimating Shapley values
remains challenging, particularly in the model-agnostic setting. We revisit the
idea of estimating Shapley values via linear regression to understand and
improve upon this approach. By analyzing KernelSHAP alongside a newly proposed
unbiased estimator, we develop techniques to detect its convergence and
calculate uncertainty estimates. We also find that that the original version
incurs a negligible increase in bias in exchange for a significant reduction in
variance, and we propose a variance reduction technique that further
accelerates the convergence of both estimators. Finally, we develop a version
of KernelSHAP for stochastic cooperative games that yields fast new estimators
for two global explanation methods.
</p>
<a href="http://arxiv.org/abs/2012.01536" target="_blank">arXiv:2012.01536</a> [<a href="http://arxiv.org/pdf/2012.01536" target="_blank">pdf</a>]

<h2>Differential Morphed Face Detection Using Deep Siamese Networks. (arXiv:2012.01541v1 [cs.CV])</h2>
<h3>Sobhan Soleymani, Baaria Chaudhary, Ali Dabouei, Jeremy Dawson, Nasser M. Nasrabadi</h3>
<p>Although biometric facial recognition systems are fast becoming part of
security applications, these systems are still vulnerable to morphing attacks,
in which a facial reference image can be verified as two or more separate
identities. In border control scenarios, a successful morphing attack allows
two or more people to use the same passport to cross borders. In this paper, we
propose a novel differential morph attack detection framework using a deep
Siamese network. To the best of our knowledge, this is the first research work
that makes use of a Siamese network architecture for morph attack detection. We
compare our model with other classical and deep learning models using two
distinct morph datasets, VISAPP17 and MorGAN. We explore the embedding space
generated by the contrastive loss using three decision making frameworks using
Euclidean distance, feature difference and a support vector machine classifier,
and feature concatenation and a support vector machine classifier.
</p>
<a href="http://arxiv.org/abs/2012.01541" target="_blank">arXiv:2012.01541</a> [<a href="http://arxiv.org/pdf/2012.01541" target="_blank">pdf</a>]

<h2>Mutual Information Maximization on Disentangled Representations for Differential Morph Detection. (arXiv:2012.01542v1 [cs.CV])</h2>
<h3>Sobhan Soleymani, Ali Dabouei, Fariborz Taherkhani, Jeremy Dawson, Nasser M. Nasrabadi</h3>
<p>In this paper, we present a novel differential morph detection framework,
utilizing landmark and appearance disentanglement. In our framework, the face
image is represented in the embedding domain using two disentangled but
complementary representations. The network is trained by triplets of face
images, in which the intermediate image inherits the landmarks from one image
and the appearance from the other image. This initially trained network is
further trained for each dataset using contrastive representations. We
demonstrate that, by employing appearance and landmark disentanglement, the
proposed framework can provide state-of-the-art differential morph detection
performance. This functionality is achieved by the using distances in landmark,
appearance, and ID domains. The performance of the proposed framework is
evaluated using three morph datasets generated with different methodologies.
</p>
<a href="http://arxiv.org/abs/2012.01542" target="_blank">arXiv:2012.01542</a> [<a href="http://arxiv.org/pdf/2012.01542" target="_blank">pdf</a>]

<h2>Machine learning prediction of critical transition and system collapse. (arXiv:2012.01545v1 [cs.LG])</h2>
<h3>Ling-Wei Kong, Hua-Wei Fan, Celso Grebogi, Ying-Cheng Lai</h3>
<p>To predict a critical transition due to parameter drift without relying on
model is an outstanding problem in nonlinear dynamics and applied fields. A
closely related problem is to predict whether the system is already in or if
the system will be in a transient state preceding its collapse. We develop a
model free, machine learning based solution to both problems by exploiting
reservoir computing to incorporate a parameter input channel. We demonstrate
that, when the machine is trained in the normal functioning regime with a
chaotic attractor (i.e., before the critical transition), the transition point
can be predicted accurately. Remarkably, for a parameter drift through the
critical point, the machine with the input parameter channel is able to predict
not only that the system will be in a transient state, but also the average
transient time before the final collapse.
</p>
<a href="http://arxiv.org/abs/2012.01545" target="_blank">arXiv:2012.01545</a> [<a href="http://arxiv.org/pdf/2012.01545" target="_blank">pdf</a>]

<h2>Deep learning collocation method for solid mechanics: Linear elasticity, hyperelasticity, and plasticity as examples. (arXiv:2012.01547v1 [cs.LG])</h2>
<h3>Diab W. Abueidda, Qiyue Lu, Seid Koric</h3>
<p>Deep learning and the collocation method are merged and used to solve partial
differential equations describing structures' deformation. We have considered
different types of materials: linear elasticity, hyperelasticity (neo-Hookean)
with large deformation, and von Mises plasticity with isotropic and kinematic
hardening. The performance of this deep collocation method (DCM) depends on the
architecture of the neural network and the corresponding hyperparameters. The
presented DCM is meshfree and avoids any spatial discretization, which is
usually needed for the finite element method (FEM). We show that the DCM can
capture the response qualitatively and quantitatively, without the need for any
data generation using other numerical methods such as the FEM. Data generation
usually is the main bottleneck in most data-driven models. The deep learning
model is trained to learn the model's parameters yielding accurate approximate
solutions. Once the model is properly trained, solutions can be obtained almost
instantly at any point in the domain, given its spatial coordinates. Therefore,
the deep collocation method is potentially a promising standalone technique to
solve partial differential equations involved in the deformation of materials
and structural systems as well as other physical phenomena.
</p>
<a href="http://arxiv.org/abs/2012.01547" target="_blank">arXiv:2012.01547</a> [<a href="http://arxiv.org/pdf/2012.01547" target="_blank">pdf</a>]

<h2>Value Alignment Verification. (arXiv:2012.01557v1 [cs.LG])</h2>
<h3>Daniel S. Brown, Jordan Schneider, Scott Niekum</h3>
<p>As humans interact with autonomous agents to perform increasingly
complicated, potentially risky tasks, it is important that humans can verify
these agents' trustworthiness and efficiently evaluate their performance and
correctness. In this paper we formalize the problem of value alignment
verification: how to efficiently test whether the goals and behavior of another
agent are aligned with a human's values? We explore several different value
alignment verification settings and provide foundational theory regarding value
alignment verification. We study alignment verification problems with an
idealized human that has an explicit reward function as well as value alignment
verification problems where the human has implicit values. Our theoretical and
empirical results in both a discrete grid navigation domain and a continuous
autonomous driving domain demonstrate that it is possible to synthesize highly
efficient and accurate value alignment verification tests for certifying the
alignment of autonomous agents.
</p>
<a href="http://arxiv.org/abs/2012.01557" target="_blank">arXiv:2012.01557</a> [<a href="http://arxiv.org/pdf/2012.01557" target="_blank">pdf</a>]

<h2>From a Fourier-Domain Perspective on Adversarial Examples to a Wiener Filter Defense for Semantic Segmentation. (arXiv:2012.01558v1 [cs.CV])</h2>
<h3>Nikhil Kapoor, Andreas B&#xe4;r, Serin Varghese, Jan David Schneider, Fabian H&#xfc;ger, Peter Schlicht, Tim Fingscheidt</h3>
<p>Despite recent advancements, deep neural networks are not robust against
adversarial perturbations. Many of the proposed adversarial defense approaches
use computationally expensive training mechanisms that do not scale to complex
real-world tasks such as semantic segmentation, and offer only marginal
improvements. In addition, fundamental questions on the nature of adversarial
perturbations and their relation to the network architecture are largely
understudied. In this work, we study the adversarial problem from a frequency
domain perspective. More specifically, we analyze discrete Fourier transform
(DFT) spectra of several adversarial images and report two major findings:
First, there exists a strong connection between a model architecture and the
nature of adversarial perturbations that can be observed and addressed in the
frequency domain. Second, the observed frequency patterns are largely image-
and attack-type independent, which is important for the practical impact of any
defense making use of such patterns. Motivated by these findings, we
additionally propose an adversarial defense method based on the well-known
Wiener filters that captures and suppresses adversarial frequencies in a
data-driven manner. Our proposed method not only generalizes across unseen
attacks but also beats five existing state-of-the-art methods across two models
in a variety of attack settings.
</p>
<a href="http://arxiv.org/abs/2012.01558" target="_blank">arXiv:2012.01558</a> [<a href="http://arxiv.org/pdf/2012.01558" target="_blank">pdf</a>]

<h2>Regularization via Adaptive Pairwise Label Smoothing. (arXiv:2012.01559v1 [cs.LG])</h2>
<h3>Hongyu Guo</h3>
<p>Label Smoothing (LS) is an effective regularizer to improve the
generalization of state-of-the-art deep models. For each training sample the LS
strategy smooths the one-hot encoded training signal by distributing its
distribution mass over the non ground-truth classes, aiming to penalize the
networks from generating overconfident output distributions. This paper
introduces a novel label smoothing technique called Pairwise Label Smoothing
(PLS). The PLS takes a pair of samples as input. Smoothing with a pair of
ground-truth labels enables the PLS to preserve the relative distance between
the two truth labels while further soften that between the truth labels and the
other targets, resulting in models producing much less confident predictions
than the LS strategy. Also, unlike current LS methods, which typically require
to find a global smoothing distribution mass through cross-validation search,
PLS automatically learns the distribution mass for each input pair during
training. We empirically show that PLS significantly outperforms LS and the
baseline models, achieving up to 30% of relative classification error
reduction. We also visually show that when achieving such accuracy gains the
PLS tends to produce very low winning softmax scores.
</p>
<a href="http://arxiv.org/abs/2012.01559" target="_blank">arXiv:2012.01559</a> [<a href="http://arxiv.org/pdf/2012.01559" target="_blank">pdf</a>]

<h2>Multicriteria Group Decision-Making Under Uncertainty Using Interval Data and Cloud Models. (arXiv:2012.01569v1 [cs.AI])</h2>
<h3>Hadi A. Khorshidi, Uwe Aickelin</h3>
<p>In this study, we propose a multicriteria group decision making (MCGDM)
algorithm under uncertainty where data is collected as intervals. The proposed
MCGDM algorithm aggregates the data, determines the optimal weights for
criteria and ranks alternatives with no further input. The intervals give
flexibility to experts in assessing alternatives against criteria and provide
an opportunity to gain maximum information. We also propose a novel method to
aggregate expert judgements using cloud models. We introduce an experimental
approach to check the validity of the aggregation method. After that, we use
the aggregation method for an MCGDM problem. Here, we find the optimal weights
for each criterion by proposing a bilevel optimisation model. Then, we extend
the technique for order of preference by similarity to ideal solution (TOPSIS)
for data based on cloud models to prioritise alternatives. As a result, the
algorithm can gain information from decision makers with different levels of
uncertainty and examine alternatives with no more information from
decision-makers. The proposed MCGDM algorithm is implemented on a case study of
a cybersecurity problem to illustrate its feasibility and effectiveness. The
results verify the robustness and validity of the proposed MCGDM using
sensitivity analysis and comparison with other existing algorithms.
</p>
<a href="http://arxiv.org/abs/2012.01569" target="_blank">arXiv:2012.01569</a> [<a href="http://arxiv.org/pdf/2012.01569" target="_blank">pdf</a>]

<h2>On Variational Inference for User Modeling in Attribute-Driven Collaborative Filtering. (arXiv:2012.01577v1 [cs.LG])</h2>
<h3>Venugopal Mani, Ramasubramanian Balasubramanian, Sushant Kumar, Abhinav Mathur, Kannan Achan</h3>
<p>Recommender Systems have become an integral part of online e-Commerce
platforms, driving customer engagement and revenue. Most popular recommender
systems attempt to learn from users' past engagement data to understand
behavioral traits of users and use that to predict future behavior. In this
work, we present an approach to use causal inference to learn user-attribute
affinities through temporal contexts. We formulate this objective as a
Probabilistic Machine Learning problem and apply a variational inference based
method to estimate the model parameters. We demonstrate the performance of the
proposed method on the next attribute prediction task on two real world
datasets and show that it outperforms standard baseline methods.
</p>
<a href="http://arxiv.org/abs/2012.01577" target="_blank">arXiv:2012.01577</a> [<a href="http://arxiv.org/pdf/2012.01577" target="_blank">pdf</a>]

<h2>Multimodal Contact Detection using Auditory and Force Features for Reliable Object Placing in Household Environments. (arXiv:2012.01583v1 [cs.RO])</h2>
<h3>Jaime Maldonado, Asil Kaan Bozcuo&#x11f;lu, Christoph Zetzsche</h3>
<p>Typical contact detection is based on the monitoring of a threshold value in
the force and torque signals. The selection of a threshold is challenging for
robots operating in unstructured or highly dynamic environments, such in a
household setting, due to the variability of the characteristics of the objects
that might be encountered. We propose a multimodal contact detection approach
using time and frequency domain features which model the distinctive
characteristics of contact events in the auditory and haptic modalities. In our
approach the monitoring of force and torque thresholds is not necessary as
detection is based on the characteristics of force and torque signals in the
frequency domain together with the impact sound generated by the manipulation
task. We evaluated our approach with a typical glass placing task in a
household setting. Our experimental results show that robust contact detection
(99.94% mean cross-validation accuracy) is possible independent of force/torque
threshold values and suitable of being implemented for operation in highly
dynamic scenarios.
</p>
<a href="http://arxiv.org/abs/2012.01583" target="_blank">arXiv:2012.01583</a> [<a href="http://arxiv.org/pdf/2012.01583" target="_blank">pdf</a>]

<h2>Holistic 3D Human and Scene Mesh Estimation from Single View Images. (arXiv:2012.01591v1 [cs.CV])</h2>
<h3>Zhenzhen Weng, Serena Yeung</h3>
<p>The 3D world limits the human body pose and the human body pose conveys
information about the surrounding objects. Indeed, from a single image of a
person placed in an indoor scene, we as humans are adept at resolving
ambiguities of the human pose and room layout through our knowledge of the
physical laws and prior perception of the plausible object and human poses.
However, few computer vision models fully leverage this fact. In this work, we
propose an end-to-end trainable model that perceives the 3D scene from a single
RGB image, estimates the camera pose and the room layout, and reconstructs both
human body and object meshes. By imposing a set of comprehensive and
sophisticated losses on all aspects of the estimations, we show that our model
outperforms existing human body mesh methods and indoor scene reconstruction
methods. To the best of our knowledge, this is the first model that outputs
both object and human predictions at the mesh level, and performs joint
optimization on the scene and human poses.
</p>
<a href="http://arxiv.org/abs/2012.01591" target="_blank">arXiv:2012.01591</a> [<a href="http://arxiv.org/pdf/2012.01591" target="_blank">pdf</a>]

<h2>Graph Neural Networks for Improved El Ni\~no Forecasting. (arXiv:2012.01598v1 [cs.LG])</h2>
<h3>Salva R&#xfc;hling Cachay, Emma Erickson, Arthur Fender C. Bucker, Ernest Pokropek, Willa Potosnak, Salomey Osei, Bj&#xf6;rn L&#xfc;tjens</h3>
<p>Deep learning-based models have recently outperformed state-of-the-art
seasonal forecasting models, such as for predicting El Ni\~no-Southern
Oscillation (ENSO). However, current deep learning models are based on
convolutional neural networks which are difficult to interpret and can fail to
model large-scale atmospheric patterns called teleconnections. Hence, we
propose the application of spatiotemporal Graph Neural Networks (GNN) to
forecast ENSO at long lead times, finer granularity and improved predictive
skill than current state-of-the-art methods. The explicit modeling of
information flow via edges may also allow for more interpretable forecasts.
Preliminary results are promising and outperform state-of-the art systems for
projections 1 and 3 months ahead.
</p>
<a href="http://arxiv.org/abs/2012.01598" target="_blank">arXiv:2012.01598</a> [<a href="http://arxiv.org/pdf/2012.01598" target="_blank">pdf</a>]

<h2>Margin-Based Transfer Bounds for Meta Learning with Deep Feature Embedding. (arXiv:2012.01602v1 [cs.LG])</h2>
<h3>Jiechao Guan, Zhiwu Lu, Tao Xiang, Timothy Hospedales</h3>
<p>By transferring knowledge learned from seen/previous tasks, meta learning
aims to generalize well to unseen/future tasks. Existing meta-learning
approaches have shown promising empirical performance on various multiclass
classification problems, but few provide theoretical analysis on the
classifiers' generalization ability on future tasks. In this paper, under the
assumption that all classification tasks are sampled from the same
meta-distribution, we leverage margin theory and statistical learning theory to
establish three margin-based transfer bounds for meta-learning based multiclass
classification (MLMC). These bounds reveal that the expected error of a given
classification algorithm for a future task can be estimated with the average
empirical error on a finite number of previous tasks, uniformly over a class of
preprocessing feature maps/deep neural networks (i.e. deep feature embeddings).
To validate these bounds, instead of the commonly-used cross-entropy loss, a
multi-margin loss is employed to train a number of representative MLMC models.
Experiments on three benchmarks show that these margin-based models still
achieve competitive performance, validating the practical value of our
margin-based theoretical analysis.
</p>
<a href="http://arxiv.org/abs/2012.01602" target="_blank">arXiv:2012.01602</a> [<a href="http://arxiv.org/pdf/2012.01602" target="_blank">pdf</a>]

<h2>Reliable Model Compression via Label-Preservation-Aware Loss Functions. (arXiv:2012.01604v1 [cs.CV])</h2>
<h3>Vinu Joseph, Shoaib Ahmed Siddiqui, Aditya Bhaskara, Ganesh Gopalakrishnan, Saurav Muralidharan, Michael Garland, Sheraz Ahmed, Andreas Dengel</h3>
<p>Model compression is a ubiquitous tool that brings the power of modern deep
learning to edge devices with power and latency constraints. The goal of model
compression is to take a large reference neural network and output a smaller
and less expensive compressed network that is functionally equivalent to the
reference. Compression typically involves pruning and/or quantization, followed
by re-training to maintain the reference accuracy. However, it has been
observed that compression can lead to a considerable mismatch in the labels
produced by the reference and the compressed models, resulting in bias and
unreliability. To combat this, we present a framework that uses a
teacher-student learning paradigm to better preserve labels. We investigate the
role of additional terms to the loss function and show how to automatically
tune the associated parameters. We demonstrate the effectiveness of our
approach both quantitatively and qualitatively on multiple compression schemes
and accuracy recovery algorithms using a set of 8 different real-world network
architectures. We obtain a significant reduction of up to 4.1X in the number of
mismatches between the compressed and reference models, and up to 5.7X in cases
where the reference model makes the correct prediction.
</p>
<a href="http://arxiv.org/abs/2012.01604" target="_blank">arXiv:2012.01604</a> [<a href="http://arxiv.org/pdf/2012.01604" target="_blank">pdf</a>]

<h2>Domain Adaptation with Incomplete Target Domains. (arXiv:2012.01606v1 [cs.LG])</h2>
<h3>Zhenpeng Li, Jianan Jiang, Yuhong Guo, Tiantian Tang, Chengxiang Zhuo, Jieping Ye</h3>
<p>Domain adaptation, as a task of reducing the annotation cost in a target
domain by exploiting the existing labeled data in an auxiliary source domain,
has received a lot of attention in the research community. However, the
standard domain adaptation has assumed perfectly observed data in both domains,
while in real world applications the existence of missing data can be
prevalent. In this paper, we tackle a more challenging domain adaptation
scenario where one has an incomplete target domain with partially observed
data. We propose an Incomplete Data Imputation based Adversarial Network
(IDIAN) model to address this new domain adaptation challenge. In the proposed
model, we design a data imputation module to fill the missing feature values
based on the partial observations in the target domain, while aligning the two
domains via deep adversarial adaption. We conduct experiments on both
cross-domain benchmark tasks and a real world adaptation task with imperfect
target domains. The experimental results demonstrate the effectiveness of the
proposed method.
</p>
<a href="http://arxiv.org/abs/2012.01606" target="_blank">arXiv:2012.01606</a> [<a href="http://arxiv.org/pdf/2012.01606" target="_blank">pdf</a>]

<h2>Obstacle Avoidance Using a Monocular Camera. (arXiv:2012.01608v1 [cs.RO])</h2>
<h3>Kyle Hatch, John Mern, Mykel Kochenderfer</h3>
<p>A collision avoidance system based on simple digital cameras would help
enable the safe integration of small UAVs into crowded, low-altitude
environments. In this work, we present an obstacle avoidance system for small
UAVs that uses a monocular camera with a hybrid neural network and path planner
controller. The system is comprised of a vision network for estimating depth
from camera images, a high-level control network, a collision prediction
network, and a contingency policy. This system is evaluated on a simulated UAV
navigating an obstacle course in a constrained flight pattern. Results show the
proposed system achieves low collision rates while maintaining operationally
relevant flight speeds.
</p>
<a href="http://arxiv.org/abs/2012.01608" target="_blank">arXiv:2012.01608</a> [<a href="http://arxiv.org/pdf/2012.01608" target="_blank">pdf</a>]

<h2>Single-shot Path Integrated Panoptic Segmentation. (arXiv:2012.01632v1 [cs.CV])</h2>
<h3>Sukjun Hwang, Seoung Wug Oh, Seon Joo Kim</h3>
<p>Panoptic segmentation, which is a novel task of unifying instance
segmentation and semantic segmentation, has attracted a lot of attention
lately. However, most of the previous methods are composed of multiple pathways
with each pathway specialized to a designated segmentation task. In this paper,
we propose to resolve panoptic segmentation in single-shot by integrating the
execution flows. With the integrated pathway, a unified feature map called
Panoptic-Feature is generated, which includes the information of both things
and stuffs. Panoptic-Feature becomes more sophisticated by auxiliary problems
that guide to cluster pixels that belong to the same instance and differentiate
between objects of different classes. A collection of convolutional filters,
where each filter represents either a thing or stuff, is applied to
Panoptic-Feature at once, materializing the single-shot panoptic segmentation.
Taking the advantages of both top-down and bottom-up approaches, our method,
named SPINet, enjoys high efficiency and accuracy on major panoptic
segmentation benchmarks: COCO and Cityscapes.
</p>
<a href="http://arxiv.org/abs/2012.01632" target="_blank">arXiv:2012.01632</a> [<a href="http://arxiv.org/pdf/2012.01632" target="_blank">pdf</a>]

<h2>What Makes a Star Teacher? A Hierarchical BERT Model for Evaluating Teacher's Performance in Online Education. (arXiv:2012.01633v1 [cs.LG])</h2>
<h3>Wen Wang, Honglei Zhuang, Mi Zhou, Hanyu Liu, Beibei Li</h3>
<p>Education has a significant impact on both society and personal life. With
the development of technology, online education has been growing rapidly over
the past decade. While there are several online education studies on student
behavior analysis, the course concept mining, and course recommendations (Feng,
Tang, and Liu 2019; Pan et al. 2017), there is little research on evaluating
teachers' performance in online education. In this paper, we conduct a
systematic study to understand and effectively predict teachers' performance
using the subtitles of 1,085 online courses. Our model-free analysis shows that
teachers' verbal cues (e.g., question strategy, emotional appealing, and
hedging) and their course structure design are both significantly correlated
with teachers' performance evaluation. Based on these insights, we then propose
a hierarchical course BERT model to predict teachers' performance in online
education. Our proposed model can capture the hierarchical structure within
each course as well as the deep semantic features extracted from the course
content. Experiment results show that our proposed method achieves significant
gain over several state-of-the-art methods. Our study provides a significant
social impact in helping teachers improve their teaching style and enhance
their instructional material design for more effective online teaching in the
future.
</p>
<a href="http://arxiv.org/abs/2012.01633" target="_blank">arXiv:2012.01633</a> [<a href="http://arxiv.org/pdf/2012.01633" target="_blank">pdf</a>]

<h2>Rel3D: A Minimally Contrastive Benchmark for Grounding Spatial Relations in 3D. (arXiv:2012.01634v1 [cs.CV])</h2>
<h3>Ankit Goyal, Kaiyu Yang, Dawei Yang, Jia Deng</h3>
<p>Understanding spatial relations (e.g., "laptop on table") in visual input is
important for both humans and robots. Existing datasets are insufficient as
they lack large-scale, high-quality 3D ground truth information, which is
critical for learning spatial relations. In this paper, we fill this gap by
constructing Rel3D: the first large-scale, human-annotated dataset for
grounding spatial relations in 3D. Rel3D enables quantifying the effectiveness
of 3D information in predicting spatial relations on large-scale human data.
Moreover, we propose minimally contrastive data collection -- a novel
crowdsourcing method for reducing dataset bias. The 3D scenes in our dataset
come in minimally contrastive pairs: two scenes in a pair are almost identical,
but a spatial relation holds in one and fails in the other. We empirically
validate that minimally contrastive examples can diagnose issues with current
relation detection models as well as lead to sample-efficient training. Code
and data are available at https://github.com/princeton-vl/Rel3D.
</p>
<a href="http://arxiv.org/abs/2012.01634" target="_blank">arXiv:2012.01634</a> [<a href="http://arxiv.org/pdf/2012.01634" target="_blank">pdf</a>]

<h2>Meta-Generating Deep Attentive Metric for Few-shot Classification. (arXiv:2012.01641v1 [cs.CV])</h2>
<h3>Lei Zhang, Fei Zhou, Wei Wei, Yanning Zhang</h3>
<p>Learning to generate a task-aware base learner proves a promising direction
to deal with few-shot learning (FSL) problem. Existing methods mainly focus on
generating an embedding model utilized with a fixed metric (eg, cosine
distance) for nearest neighbour classification or directly generating a linear
classier. However, due to the limited discriminative capacity of such a simple
metric or classifier, these methods fail to generalize to challenging cases
appropriately. To mitigate this problem, we present a novel deep metric
meta-generation method that turns to an orthogonal direction, ie, learning to
adaptively generate a specific metric for a new FSL task based on the task
description (eg, a few labelled samples). In this study, we structure the
metric using a three-layer deep attentive network that is flexible enough to
produce a discriminative metric for each task. Moreover, different from
existing methods that utilize an uni-modal weight distribution conditioned on
labelled samples for network generation, the proposed meta-learner establishes
a multi-modal weight distribution conditioned on cross-class sample pairs using
a tailored variational autoencoder, which can separately capture the specific
inter-class discrepancy statistics for each class and jointly embed the
statistics for all classes into metric generation. By doing this, the generated
metric can be appropriately adapted to a new FSL task with pleasing
generalization performance. To demonstrate this, we test the proposed method on
four benchmark FSL datasets and gain surprisingly obvious performance
improvement over state-of-the-art competitors, especially in the challenging
cases, eg, improve the accuracy from 26.14% to 46.69% in the 20-way 1-shot task
on miniImageNet, while improve the accuracy from 45.2% to 68.72% in the 5-way
1-shot task on FC100. Code is available: https://github.com/NWPUZhoufei/DAM.
</p>
<a href="http://arxiv.org/abs/2012.01641" target="_blank">arXiv:2012.01641</a> [<a href="http://arxiv.org/pdf/2012.01641" target="_blank">pdf</a>]

<h2>Learning to Transfer Visual Effects from Videos to Images. (arXiv:2012.01642v1 [cs.CV])</h2>
<h3>Christopher Thomas, Yale Song, Adriana Kovashka</h3>
<p>We study the problem of animating images by transferring spatio-temporal
visual effects (such as melting) from a collection of videos. We tackle two
primary challenges in visual effect transfer: 1) how to capture the effect we
wish to distill; and 2) how to ensure that only the effect, rather than content
or artistic style, is transferred from the source videos to the input image. To
address the first challenge, we evaluate five loss functions; the most
promising one encourages the generated animations to have similar optical flow
and texture motions as the source videos. To address the second challenge, we
only allow our model to move existing image pixels from the previous frame,
rather than predicting unconstrained pixel values. This forces any visual
effects to occur using the input image's pixels, preventing unwanted artistic
style or content from the source video from appearing in the output. We
evaluate our method in objective and subjective settings, and show interesting
qualitative results which demonstrate objects undergoing atypical
transformations, such as making a face melt or a deer bloom.
</p>
<a href="http://arxiv.org/abs/2012.01642" target="_blank">arXiv:2012.01642</a> [<a href="http://arxiv.org/pdf/2012.01642" target="_blank">pdf</a>]

<h2>Learning Hyperbolic Representations for Unsupervised 3D Segmentation. (arXiv:2012.01644v1 [cs.CV])</h2>
<h3>Joy Hsu, Jeffrey Gu, Gong-Her Wu, Wah Chiu, Serena Yeung</h3>
<p>There exists a need for unsupervised 3D segmentation on complex volumetric
data, particularly when annotation ability is limited or discovery of new
categories is desired. Using the observation that much of 3D volumetric data is
innately hierarchical, we propose learning effective representations of 3D
patches for unsupervised segmentation through a variational autoencoder (VAE)
with a hyperbolic latent space and a proposed gyroplane convolutional layer,
which better models the underlying hierarchical structure within a 3D image. We
also introduce a hierarchical triplet loss and multi-scale patch sampling
scheme to embed relationships across varying levels of granularity. We
demonstrate the effectiveness of our hyperbolic representations for
unsupervised 3D segmentation on a hierarchical toy dataset, BraTS whole tumor
dataset, and cryogenic electron microscopy data.
</p>
<a href="http://arxiv.org/abs/2012.01644" target="_blank">arXiv:2012.01644</a> [<a href="http://arxiv.org/pdf/2012.01644" target="_blank">pdf</a>]

<h2>How to Formally Model Human in Collaborative Robotics. (arXiv:2012.01647v1 [cs.RO])</h2>
<h3>Mehrnoosh Askarpour (McMaster University)</h3>
<p>Human-robot collaboration (HRC) is an emerging trend of robotics that
promotes the co-presence and cooperation of humans and robots in common
workspaces. Physical vicinity and interaction between humans and robots,
combined with the uncertainty of human behaviour, could lead to undesired
situations where humans are injured. Thus, safety is a priority for HRC
applications.

Safety analysis via formal modelling and verification techniques could
considerably avoid dangerous consequences, but only if the models of HRC
systems are comprehensive and realistic, which requires reasonably realistic
models of human behaviour. This paper explores state-of-the-art solutions for
modelling human and discusses which ones are suitable for HRC scenarios.
</p>
<a href="http://arxiv.org/abs/2012.01647" target="_blank">arXiv:2012.01647</a> [<a href="http://arxiv.org/pdf/2012.01647" target="_blank">pdf</a>]

<h2>Deep Spectral CNN for Laser Induced Breakdown Spectroscopy. (arXiv:2012.01653v1 [cs.LG])</h2>
<h3>Juan Castorena, Diane Oyen, Ann Ollila, Carey Legget, Nina Lanza</h3>
<p>This work proposes a spectral convolutional neural network (CNN) operating on
laser induced breakdown spectroscopy (LIBS) signals to learn to (1) disentangle
spectral signals from the sources of sensor uncertainty (i.e., pre-process) and
(2) get qualitative and quantitative measures of chemical content of a sample
given a spectral signal (i.e., calibrate). Once the spectral CNN is trained, it
can accomplish either task through a single feed-forward pass, with real-time
benefits and without any additional side information requirements including
dark current, system response, temperature and detector-to-target range. Our
experiments demonstrate that the proposed method outperforms the existing
approaches used by the Mars Science Lab for pre-processing and calibration for
remote sensing observations from the Mars rover, 'Curiosity'.
</p>
<a href="http://arxiv.org/abs/2012.01653" target="_blank">arXiv:2012.01653</a> [<a href="http://arxiv.org/pdf/2012.01653" target="_blank">pdf</a>]

<h2>Towards Defending Multiple Adversarial Perturbations via Gated Batch Normalization. (arXiv:2012.01654v1 [cs.CV])</h2>
<h3>Aishan Liu, Shiyu Tang, Xianglong Liu, Xinyun Chen, Lei Huang, Zhuozhuo Tu, Dawn Song, Dacheng Tao</h3>
<p>There is now extensive evidence demonstrating that deep neural networks are
vulnerable to adversarial examples, motivating the development of defenses
against adversarial attacks. However, existing adversarial defenses typically
improve model robustness against individual specific perturbation types. Some
recent methods improve model robustness against adversarial attacks in multiple
$\ell_p$ balls, but their performance against each perturbation type is still
far from satisfactory. To better understand this phenomenon, we propose the
\emph{multi-domain} hypothesis, stating that different types of adversarial
perturbations are drawn from different domains. Guided by the multi-domain
hypothesis, we propose \emph{Gated Batch Normalization (GBN)}, a novel building
block for deep neural networks that improves robustness against multiple
perturbation types. GBN consists of a gated sub-network and a multi-branch
batch normalization (BN) layer, where the gated sub-network separates different
perturbation types, and each BN branch is in charge of a single perturbation
type and learns domain-specific statistics for input transformation. Then,
features from different branches are aligned as domain-invariant
representations for the subsequent layers. We perform extensive evaluations of
our approach on MNIST, CIFAR-10, and Tiny-ImageNet, and demonstrate that GBN
outperforms previous defense proposals against multiple perturbation types,
i.e, $\ell_1$, $\ell_2$, and $\ell_{\infty}$ perturbations, by large margins of
10-20\%.
</p>
<a href="http://arxiv.org/abs/2012.01654" target="_blank">arXiv:2012.01654</a> [<a href="http://arxiv.org/pdf/2012.01654" target="_blank">pdf</a>]

<h2>Dual-Branch Network with Dual-Sampling Modulated Dice Loss for Hard Exudate Segmentation from Colour Fundus Images. (arXiv:2012.01665v1 [cs.CV])</h2>
<h3>Qing Liu, Haotian Liu, Yixiong Liang</h3>
<p>Automated segmentation of hard exudates in colour fundus images is a
challenge task due to issues of extreme class imbalance and enormous size
variation. This paper aims to tackle these issues and proposes a dual-branch
network with dual-sampling modulated Dice loss. It consists of two branches:
large hard exudate biased learning branch and small hard exudate biased
learning branch. Both of them are responsible for their own duty separately.
Furthermore, we propose a dual-sampling modulated Dice loss for the training
such that our proposed dual-branch network is able to segment hard exudates in
different sizes. In detail, for the first branch, we use a uniform sampler to
sample pixels from predicted segmentation mask for Dice loss calculation, which
leads to this branch naturally be biased in favour of large hard exudates as
Dice loss generates larger cost on misidentification of large hard exudates
than small hard exudates. For the second branch, we use a re-balanced sampler
to oversample hard exudate pixels and undersample background pixels for loss
calculation. In this way, cost on misidentification of small hard exudates is
enlarged, which enforces the parameters in the second branch fit small hard
exudates well. Considering that large hard exudates are much easier to be
correctly identified than small hard exudates, we propose an easy-to-difficult
learning strategy by adaptively modulating the losses of two branches. We
evaluate our proposed method on two public datasets and results demonstrate
that ours achieves state-of-the-art performances.
</p>
<a href="http://arxiv.org/abs/2012.01665" target="_blank">arXiv:2012.01665</a> [<a href="http://arxiv.org/pdf/2012.01665" target="_blank">pdf</a>]

<h2>Online Forgetting Process for Linear Regression Models. (arXiv:2012.01668v1 [stat.ML])</h2>
<h3>Yuantong Li, Chi-hua Wang, Guang Cheng</h3>
<p>Motivated by the EU's "Right To Be Forgotten" regulation, we initiate a study
of statistical data deletion problems where users' data are accessible only for
a limited period of time. This setting is formulated as an online supervised
learning task with \textit{constant memory limit}. We propose a deletion-aware
algorithm \texttt{FIFD-OLS} for the low dimensional case, and witness a
catastrophic rank swinging phenomenon due to the data deletion operation, which
leads to statistical inefficiency. As a remedy, we propose the
\texttt{FIFD-Adaptive Ridge} algorithm with a novel online regularization
scheme, that effectively offsets the uncertainty from deletion. In theory, we
provide the cumulative regret upper bound for both online forgetting
algorithms. In the experiment, we showed \texttt{FIFD-Adaptive Ridge}
outperforms the ridge regression algorithm with fixed regularization level, and
hopefully sheds some light on more complex statistical models.
</p>
<a href="http://arxiv.org/abs/2012.01668" target="_blank">arXiv:2012.01668</a> [<a href="http://arxiv.org/pdf/2012.01668" target="_blank">pdf</a>]

<h2>ResPerfNet: Deep Residual Learning for Regressional Performance Modeling of Deep Neural Networks. (arXiv:2012.01671v1 [cs.LG])</h2>
<h3>Chuan-Chi Wang, Ying-Chiao Liao, Chia-Heng Tu, Ming-Chang Kao, Wen-Yew Liang, Shih-Hao Hung</h3>
<p>The rapid advancements of computing technology facilitate the development of
diverse deep learning applications. Unfortunately, the efficiency of parallel
computing infrastructures varies widely with neural network models, which
hinders the exploration of the design space to find high-performance neural
network architectures on specific computing platforms for a given application.
To address such a challenge, we propose a deep learning-based method,
ResPerfNet, which trains a residual neural network with representative datasets
obtained on the target platform to predict the performance for a deep neural
network. Our experimental results show that ResPerfNet can accurately predict
the execution time of individual neural network layers and full network models
on a variety of platforms. In particular, ResPerfNet achieves 8.4% of mean
absolute percentage error for LeNet, AlexNet and VGG16 on the NVIDIA GTX
1080Ti, which is substantially lower than the previously published works.
</p>
<a href="http://arxiv.org/abs/2012.01671" target="_blank">arXiv:2012.01671</a> [<a href="http://arxiv.org/pdf/2012.01671" target="_blank">pdf</a>]

<h2>Interpretable Graph Capsule Networks for Object Recognition. (arXiv:2012.01674v1 [cs.CV])</h2>
<h3>Jindong Gu, Volker Tresp</h3>
<p>Capsule Networks, as alternatives to Convolutional Neural Networks, have been
proposed to recognize objects from images. The current literature demonstrates
many advantages of CapsNets over CNNs. However, how to create explanations for
individual classifications of CapsNets has not been well explored. The widely
used saliency methods are mainly proposed for explaining CNN-based
classifications; they create saliency map explanations by combining activation
values and the corresponding gradients, e.g., Grad-CAM. These saliency methods
require a specific architecture of the underlying classifiers and cannot be
trivially applied to CapsNets due to the iterative routing mechanism therein.
To overcome the lack of interpretability, we can either propose new post-hoc
interpretation methods for CapsNets or modifying the model to have build-in
explanations. In this work, we explore the latter. Specifically, we propose
interpretable Graph Capsule Networks (GraCapsNets), where we replace the
routing part with a multi-head attention-based Graph Pooling approach. In the
proposed model, individual classification explanations can be created
effectively and efficiently. Our model also demonstrates some unexpected
benefits, even though it replaces the fundamental part of CapsNets. Our
GraCapsNets achieve better classification performance with fewer parameters and
better adversarial robustness, when compared to CapsNets. Besides, GraCapsNets
also keep other advantages of CapsNets, namely, disentangled representations
and affine transformation robustness.
</p>
<a href="http://arxiv.org/abs/2012.01674" target="_blank">arXiv:2012.01674</a> [<a href="http://arxiv.org/pdf/2012.01674" target="_blank">pdf</a>]

<h2>Using Cross-Loss Influence Functions to Explain Deep Network Representations. (arXiv:2012.01685v1 [cs.LG])</h2>
<h3>Andrew Silva, Rohit Chopra, Matthew Gombolay</h3>
<p>As machine learning is increasingly deployed in the real world, it is ever
more vital that we understand the decision-criteria of the models we train.
Recently, researchers have shown that influence functions, a statistical
measure of sample impact, may be extended to approximate the effects of
training samples on classification accuracy for deep neural networks. However,
prior work only applies to supervised learning setups where training and
testing share an objective function. Despite the rise in unsupervised learning,
self-supervised learning, and model pre-training, there are currently no
suitable technologies for estimating influence of deep networks that do not
train and test on the same objective. To overcome this limitation, we provide
the first theoretical and empirical demonstration that influence functions can
be extended to handle mismatched training and testing settings. Our result
enables us to compute the influence of unsupervised and self-supervised
training examples with respect to a supervised test objective. We demonstrate
this technique on a synthetic dataset as well as two Skip-gram language model
examples to examine cluster membership and sources of unwanted bias.
</p>
<a href="http://arxiv.org/abs/2012.01685" target="_blank">arXiv:2012.01685</a> [<a href="http://arxiv.org/pdf/2012.01685" target="_blank">pdf</a>]

<h2>Motion-based Camera Localization System in Colonoscopy Videos. (arXiv:2012.01690v1 [cs.CV])</h2>
<h3>Heming Yao, Ryan W. Stidham, Jonathan Gryak, Kayvan Najarian</h3>
<p>Optical colonoscopy is an essential diagnostic and prognostic tool for many
gastrointestinal diseases including cancer screening and staging, intestinal
bleeding, diarrhea, abdominal symptom evaluation, and inflammatory bowel
disease assessment. However, the evaluation, classification, and quantification
of findings on colonoscopy are subject to inter-observer variation. Automated
assessment of colonoscopy is of interest considering the subjectivity present
in qualitative human interpretations of colonoscopy findings. Localization of
the camera is an essential element to consider when inferring the meaning and
context of findings for diseases evaluated by colonoscopy. In this study, we
proposed a camera localization system to estimate the approximate anatomic
location of the camera and classify the anatomical colon segment the camera is
in. The camera localization system starts with non-informative frame detection
to remove frames without camera motion information. Then a self-training
end-to-end convolutional neural network was built to estimate the camera
motion. With the estimated camera motion, the camera trajectory can be derived,
and the location index can be calculated. Based on the estimated location
index, anatomical colon segment classification was performed by building the
colon template. The algorithm was trained and validated using colonoscopy
videos collected from routine clinical practice. From our results, the average
accuracy of the classification is 0.759, which is substantially higher than the
performance of using the location index built from other methods.
</p>
<a href="http://arxiv.org/abs/2012.01690" target="_blank">arXiv:2012.01690</a> [<a href="http://arxiv.org/pdf/2012.01690" target="_blank">pdf</a>]

<h2>Relational Learning for Skill Preconditions. (arXiv:2012.01693v1 [cs.RO])</h2>
<h3>Mohit Sharma, Oliver Kroemer</h3>
<p>To determine if a skill can be executed in any given environment, a robot
needs to learn the preconditions for the skill. As robots begin to operate in
dynamic and unstructured environments, precondition models will need to
generalize to variable number of objects with different shapes and sizes. In
this work, we focus on learning precondition models for manipulation skills in
unconstrained environments. Our work is motivated by the intuition that many
complex manipulation tasks, with multiple objects, can be simplified by
focusing on less complex pairwise object relations. We propose an
object-relation model that learns continuous representations for these pairwise
object relations. Our object-relation model is trained completely in
simulation, and once learned, is used by a separate precondition model to
predict skill preconditions for real world tasks. We evaluate our precondition
model on $3$ different manipulation tasks: sweeping, cutting, and unstacking.
We show that our approach leads to significant improvements in predicting
preconditions for all 3 tasks, across objects of different shapes and sizes.
</p>
<a href="http://arxiv.org/abs/2012.01693" target="_blank">arXiv:2012.01693</a> [<a href="http://arxiv.org/pdf/2012.01693" target="_blank">pdf</a>]

<h2>FairBatch: Batch Selection for Model Fairness. (arXiv:2012.01696v1 [cs.LG])</h2>
<h3>Yuji Roh, Kangwook Lee, Steven Euijong Whang, Changho Suh</h3>
<p>Training a fair machine learning model is essential to prevent demographic
disparity. Existing techniques for improving model fairness require broad
changes in either data preprocessing or model training, rendering themselves
difficult-to-adopt for potentially already complex machine learning systems. We
address this problem via the lens of bilevel optimization. While keeping the
standard training algorithm as an inner optimizer, we incorporate an outer
optimizer so as to equip the inner problem with an additional functionality:
Adaptively selecting minibatch sizes for the purpose of improving model
fairness. Our batch selection algorithm, which we call FairBatch, implements
this optimization and supports prominent fairness measures: equal opportunity,
equalized odds, and demographic parity. FairBatch comes with a significant
implementation benefit -- it does not require any modification to data
preprocessing or model training. For instance, a single-line change of PyTorch
code for replacing batch selection part of model training suffices to employ
FairBatch. Our experiments conducted both on synthetic and benchmark real data
demonstrate that FairBatch can provide such functionalities while achieving
comparable (or even greater) performances against the state of the arts.
Furthermore, FairBatch can readily improve fairness of any pre-trained model
simply via fine-tuning. It is also compatible with existing batch selection
techniques intended for different purposes, such as faster convergence, thus
gracefully achieving multiple purposes.
</p>
<a href="http://arxiv.org/abs/2012.01696" target="_blank">arXiv:2012.01696</a> [<a href="http://arxiv.org/pdf/2012.01696" target="_blank">pdf</a>]

<h2>Neural Network Approximations of Compositional Functions With Applications to Dynamical Systems. (arXiv:2012.01698v1 [cs.LG])</h2>
<h3>Wei Kang, Qi Gong</h3>
<p>As demonstrated in many areas of real-life applications, neural networks have
the capability of dealing with high dimensional data. In the fields of optimal
control and dynamical systems, the same capability was studied and verified in
many published results in recent years. Towards the goal of revealing the
underlying reason why neural networks are capable of solving some high
dimensional problems, we develop an algebraic framework and an approximation
theory for compositional functions and their neural network approximations. The
theoretical foundation is developed in a way so that it supports the error
analysis for not only functions as input-output relations, but also numerical
algorithms. This capability is critical because it enables the analysis of
approximation errors for problems for which analytic solutions are not
available, such as differential equations and optimal control. We identify a
set of key features of compositional functions and the relationship between the
features and the complexity of neural networks. In addition to function
approximations, we prove several formulae of error upper bounds for neural
networks that approximate the solutions to differential equations,
optimization, and optimal control.
</p>
<a href="http://arxiv.org/abs/2012.01698" target="_blank">arXiv:2012.01698</a> [<a href="http://arxiv.org/pdf/2012.01698" target="_blank">pdf</a>]

<h2>Essential Features: Reducing the Attack Surface of Adversarial Perturbations with Robust Content-Aware Image Preprocessing. (arXiv:2012.01699v1 [cs.CV])</h2>
<h3>Ryan Feng, Wu-chi Feng, Atul Prakash</h3>
<p>Adversaries are capable of adding perturbations to an image to fool machine
learning models into incorrect predictions. One approach to defending against
such perturbations is to apply image preprocessing functions to remove the
effects of the perturbation. Existing approaches tend to be designed
orthogonally to the content of the image and can be beaten by adaptive attacks.
We propose a novel image preprocessing technique called Essential Features that
transforms the image into a robust feature space that preserves the main
content of the image while significantly reducing the effects of the
perturbations. Specifically, an adaptive blurring strategy that preserves the
main edge features of the original object along with a k-means color reduction
approach is employed to simplify the image to its k most representative colors.
This approach significantly limits the attack surface for adversaries by
limiting the ability to adjust colors while preserving pertinent features of
the original image. We additionally design several adaptive attacks and find
that our approach remains more robust than previous baselines. On CIFAR-10 we
achieve 64% robustness and 58.13% robustness on RESISC45, raising robustness by
over 10% versus state-of-the-art adversarial training techniques against
adaptive white-box and black-box attacks. The results suggest that strategies
that retain essential features in images by adaptive processing of the content
hold promise as a complement to adversarial training for boosting robustness
against adversarial inputs.
</p>
<a href="http://arxiv.org/abs/2012.01699" target="_blank">arXiv:2012.01699</a> [<a href="http://arxiv.org/pdf/2012.01699" target="_blank">pdf</a>]

<h2>Robust Federated Learning with Noisy Labels. (arXiv:2012.01700v1 [cs.LG])</h2>
<h3>Seunghan Yang, Hyoungseob Park, Junyoung Byun, Changick Kim</h3>
<p>Federated learning is a paradigm that enables local devices to jointly train
a server model while keeping the data decentralized and private. In federated
learning, since local data are collected by clients, it is hardly guaranteed
that the data are correctly annotated. Although a lot of studies have been
conducted to train the networks robust to these noisy data in a centralized
setting, these algorithms still suffer from noisy labels in federated learning.
Compared to the centralized setting, clients' data can have different noise
distributions due to variations in their labeling systems or background
knowledge of users. As a result, local models form inconsistent decision
boundaries and their weights severely diverge from each other, which are
serious problems in federated learning. To solve these problems, we introduce a
novel federated learning scheme that the server cooperates with local models to
maintain consistent decision boundaries by interchanging class-wise centroids.
These centroids are central features of local data on each device, which are
aligned by the server every communication round. Updating local models with the
aligned centroids helps to form consistent decision boundaries among local
models, although the noise distributions in clients' data are different from
each other. To improve local model performance, we introduce a novel approach
to select confident samples that are used for updating the model with given
labels. Furthermore, we propose a global-guided pseudo-labeling method to
update labels of unconfident samples by exploiting the global model. Our
experimental results on the noisy CIFAR-10 dataset and the Clothing1M dataset
show that our approach is noticeably effective in federated learning with noisy
labels.
</p>
<a href="http://arxiv.org/abs/2012.01700" target="_blank">arXiv:2012.01700</a> [<a href="http://arxiv.org/pdf/2012.01700" target="_blank">pdf</a>]

<h2>FenceBox: A Platform for Defeating Adversarial Examples with Data Augmentation Techniques. (arXiv:2012.01701v1 [cs.LG])</h2>
<h3>Han Qiu, Yi Zeng, Tianwei Zhang, Yong Jiang, Meikang Qiu</h3>
<p>It is extensively studied that Deep Neural Networks (DNNs) are vulnerable to
Adversarial Examples (AEs). With more and more advanced adversarial attack
methods have been developed, a quantity of corresponding defense solutions were
designed to enhance the robustness of DNN models. It has become a popularity to
leverage data augmentation techniques to preprocess input samples before
inference to remove adversarial perturbations. By obfuscating the gradients of
DNN models, these approaches can defeat a considerable number of conventional
attacks. Unfortunately, advanced gradient-based attack techniques (e.g., BPDA
and EOT) were introduced to invalidate these preprocessing effects.

In this paper, we present FenceBox, a comprehensive framework to defeat
various kinds of adversarial attacks. FenceBox is equipped with 15 data
augmentation methods from three different categories. We comprehensively
evaluated that these methods can effectively mitigate various adversarial
attacks. FenceBox also provides APIs for users to easily deploy the defense
over their models in different modes: they can either select an arbitrary
preprocessing method, or a combination of functions for a better robustness
guarantee, even under advanced adversarial attacks. We open-source FenceBox,
and expect it can be used as a standard toolkit to facilitate the research of
adversarial attacks and defenses.
</p>
<a href="http://arxiv.org/abs/2012.01701" target="_blank">arXiv:2012.01701</a> [<a href="http://arxiv.org/pdf/2012.01701" target="_blank">pdf</a>]

<h2>Online learning with dynamics: A minimax perspective. (arXiv:2012.01705v1 [cs.LG])</h2>
<h3>Kush Bhatia, Karthik Sridharan</h3>
<p>We study the problem of online learning with dynamics, where a learner
interacts with a stateful environment over multiple rounds. In each round of
the interaction, the learner selects a policy to deploy and incurs a cost that
depends on both the chosen policy and current state of the world. The
state-evolution dynamics and the costs are allowed to be time-varying, in a
possibly adversarial way. In this setting, we study the problem of minimizing
policy regret and provide non-constructive upper bounds on the minimax rate for
the problem.

Our main results provide sufficient conditions for online learnability for
this setup with corresponding rates. The rates are characterized by 1) a
complexity term capturing the expressiveness of the underlying policy class
under the dynamics of state change, and 2) a dynamics stability term measuring
the deviation of the instantaneous loss from a certain counterfactual loss.
Further, we provide matching lower bounds which show that both the complexity
terms are indeed necessary.

Our approach provides a unifying analysis that recovers regret bounds for
several well studied problems including online learning with memory, online
control of linear quadratic regulators, online Markov decision processes, and
tracking adversarial targets. In addition, we show how our tools help obtain
tight regret bounds for a new problems (with non-linear dynamics and non-convex
losses) for which such bounds were not known prior to our work.
</p>
<a href="http://arxiv.org/abs/2012.01705" target="_blank">arXiv:2012.01705</a> [<a href="http://arxiv.org/pdf/2012.01705" target="_blank">pdf</a>]

<h2>A Study on the Autoregressive and non-Autoregressive Multi-label Learning. (arXiv:2012.01711v1 [cs.LG])</h2>
<h3>Elham J. Barezi, Iacer Calixto, Kyunghyun Cho, Pascale Fung</h3>
<p>Extreme classification tasks are multi-label tasks with an extremely large
number of labels (tags). These tasks are hard because the label space is
usually (i) very large, e.g. thousands or millions of labels, (ii) very sparse,
i.e. very few labels apply to each input document, and (iii) highly correlated,
meaning that the existence of one label changes the likelihood of predicting
all other labels. In this work, we propose a self-attention based variational
encoder-model to extract the label-label and label-feature dependencies jointly
and to predict labels for a given input. In more detail, we propose a
non-autoregressive latent variable model and compare it to a strong
autoregressive baseline that predicts a label based on all previously generated
labels. Our model can therefore be used to predict all labels in parallel while
still including both label-label and label-feature dependencies through latent
variables, and compares favourably to the autoregressive baseline. We apply our
models to four standard extreme classification natural language data sets, and
one news videos dataset for automated label detection from a lexicon of
semantic concepts. Experimental results show that although the autoregressive
models, where use a given order of the labels for chain-order label prediction,
work great for the small scale labels or the prediction of the highly ranked
label, but our non-autoregressive model surpasses them by around 2% to 6% when
we need to predict more labels, or the dataset has a larger number of the
labels.
</p>
<a href="http://arxiv.org/abs/2012.01711" target="_blank">arXiv:2012.01711</a> [<a href="http://arxiv.org/pdf/2012.01711" target="_blank">pdf</a>]

<h2>AutoInt: Automatic Integration for Fast Neural Volume Rendering. (arXiv:2012.01714v1 [cs.CV])</h2>
<h3>David B. Lindell, Julien N. P. Martel, Gordon Wetzstein</h3>
<p>Numerical integration is a foundational technique in scientific computing and
is at the core of many computer vision applications. Among these applications,
implicit neural volume rendering has recently been proposed as a new paradigm
for view synthesis, achieving photorealistic image quality. However, a
fundamental obstacle to making these methods practical is the extreme
computational and memory requirements caused by the required volume
integrations along the rendered rays during training and inference. Millions of
rays, each requiring hundreds of forward passes through a neural network are
needed to approximate those integrations with Monte Carlo sampling. Here, we
propose automatic integration, a new framework for learning efficient,
closed-form solutions to integrals using implicit neural representation
networks. For training, we instantiate the computational graph corresponding to
the derivative of the implicit neural representation. The graph is fitted to
the signal to integrate. After optimization, we reassemble the graph to obtain
a network that represents the antiderivative. By the fundamental theorem of
calculus, this enables the calculation of any definite integral in two
evaluations of the network. Using this approach, we demonstrate a greater than
10x improvement in computation requirements, enabling fast neural volume
rendering.
</p>
<a href="http://arxiv.org/abs/2012.01714" target="_blank">arXiv:2012.01714</a> [<a href="http://arxiv.org/pdf/2012.01714" target="_blank">pdf</a>]

<h2>Parallel Residual Bi-Fusion Feature Pyramid Network for Accurate Single-Shot Object Detection. (arXiv:2012.01724v1 [cs.CV])</h2>
<h3>Ping-Yang Chen, Ming-Ching Chang, Jun-Wei Hsieh, Yong-Sheng Chen</h3>
<p>We propose the Parallel Residual Bi-Fusion Feature Pyramid Network (PRB-FPN)
for fast and accurate single-shot object detection. Feature Pyramid (FP) is
widely used in recent visual detection, however the top-down pathway of FP
cannot preserve accurate localization due to pooling shifting. The advantage of
FP is weaken as deeper backbones with more layers are used. To address this
issue, we propose a new parallel FP structure with bi-directional (top-down and
bottom-up) fusion and associated improvements to retain high-quality features
for accurate localization. Our method is particularly suitable for detecting
small objects. We provide the following design improvements: (1) A parallel
bifusion FP structure with a Bottom-up Fusion Module (BFM) to detect both small
and large objects at once with high accuracy. (2) A COncatenation and
RE-organization (CORE) module provides a bottom-up pathway for feature fusion,
which leads to the bi-directional fusion FP that can recover lost information
from lower-layer feature maps. (3) The CORE feature is further purified to
retain richer contextual information. Such purification is performed with CORE
in a few iterations in both top-down and bottom-up pathways. (4) The adding of
a residual design to CORE leads to a new Re-CORE module that enables easy
training and integration with a wide range of (deeper or lighter) backbones.
The proposed network achieves state-of-the-art performance on UAVDT17 and MS
COCO datasets.
</p>
<a href="http://arxiv.org/abs/2012.01724" target="_blank">arXiv:2012.01724</a> [<a href="http://arxiv.org/pdf/2012.01724" target="_blank">pdf</a>]

<h2>Identification of Prototypical Task Executions Based on Smoothness as Basis of Human-to-Robot Kinematic Skill Transfer. (arXiv:2012.01732v1 [cs.RO])</h2>
<h3>Jaime Maldonado, Christoph Zetzsche</h3>
<p>In this paper we investigate human-to-robot skill transfer based on the
identification of prototypical task executions by clustering a set of examples
performed by human demonstrators, where smoothness and kinematic features
represent skill and task performance, respectively. We exemplify our skill
transfer approach with data from an experimental task in which a tool touches a
support surface with a target velocity. Prototypical task executions are
identified and transferred to a generic robot arm in simulation. The results
illustrate how task models based on skill and performance features can provide
analysis and design criteria for robotic applications.
</p>
<a href="http://arxiv.org/abs/2012.01732" target="_blank">arXiv:2012.01732</a> [<a href="http://arxiv.org/pdf/2012.01732" target="_blank">pdf</a>]

<h2>Dual Refinement Feature Pyramid Networks for Object Detection. (arXiv:2012.01733v1 [cs.CV])</h2>
<h3>Jialiang Ma, Bin Chen</h3>
<p>FPN is a common component used in object detectors, it supplements
multi-scale information by adjacent level features interpolation and summation.
However, due to the existence of nonlinear operations and the convolutional
layers with different output dimensions, the relationship between different
levels is much more complex, the pixel-wise summation is not an efficient
approach. In this paper, we first analyze the design defects from pixel level
and feature map level. Then, we design a novel parameter-free feature pyramid
networks named Dual Refinement Feature Pyramid Networks (DRFPN) for the
problems. Specifically, DRFPN consists of two modules: Spatial Refinement Block
(SRB) and Channel Refinement Block (CRB). SRB learns the location and content
of sampling points based on contextual information between adjacent levels. CRB
learns an adaptive channel merging method based on attention mechanism. Our
proposed DRFPN can be easily plugged into existing FPN-based models. Without
bells and whistles, for two-stage detectors, our model outperforms different
FPN-based counterparts by 1.6 to 2.2 AP on the COCO detection benchmark, and
1.5 to 1.9 AP on the COCO segmentation benchmark. For one-stage detectors,
DRFPN improves anchor-based RetinaNet by 1.9 AP and anchor-free FCOS by 1.3 AP
when using ResNet50 as backbone. Extensive experiments verifies the robustness
and generalization ability of DRFPN. The code will be made publicly available.
</p>
<a href="http://arxiv.org/abs/2012.01733" target="_blank">arXiv:2012.01733</a> [<a href="http://arxiv.org/pdf/2012.01733" target="_blank">pdf</a>]

<h2>Automatic Routability Predictor Development Using Neural Architecture Search. (arXiv:2012.01737v1 [cs.LG])</h2>
<h3>Jingyu Pan, Chen-Chia Chang, Tunhou Zhang, Zhiyao Xie, Jiang Hu, Weiyi Qi, Chung-Wei Lin, Rongjian Liang, Joydeep Mitra, Elias Fallon, Yiran Chen</h3>
<p>The rise of machine learning technology inspires a boom of its applications
in electronic design automation (EDA) and helps improve the degree of
automation in chip designs. However, manually crafted machine learning models
require extensive human expertise and tremendous engineering efforts. In this
work, we leverage neural architecture search (NAS) to automatically develop
high-quality neural architectures for routability prediction, which guides cell
placement toward routable solutions. Experimental results demonstrate that the
automatically generated neural architectures clearly outperform the manual
solutions. Compared to the average case of manually designed models,
NAS-generated models achieve $5.6\%$ higher Kendall's $\tau$ in predicting the
number of nets with DRC violations and $1.95\%$ larger area under ROC curve
(ROC-AUC) in DRC hotspots detection.
</p>
<a href="http://arxiv.org/abs/2012.01737" target="_blank">arXiv:2012.01737</a> [<a href="http://arxiv.org/pdf/2012.01737" target="_blank">pdf</a>]

<h2>Sparse Semi-Supervised Action Recognition with Active Learning. (arXiv:2012.01740v1 [cs.CV])</h2>
<h3>Jingyuan Li, Eli Shlizerman</h3>
<p>Current state-of-the-art methods for skeleton-based action recognition are
supervised and rely on labels. The reliance is limiting the performance due to
the challenges involved in annotation and mislabeled data. Unsupervised methods
have been introduced, however, they organize sequences into clusters and still
require labels to associate clusters with actions. In this paper, we propose a
novel approach for skeleton-based action recognition, called SESAR, that
connects these approaches. SESAR leverages the information from both unlabeled
data and a handful of sequences actively selected for labeling, combining
unsupervised training with sparsely supervised guidance. SESAR is composed of
two main components, where the first component learns a latent representation
for unlabeled action sequences through an Encoder-Decoder RNN which
reconstructs the sequences, and the second component performs active learning
to select sequences to be labeled based on cluster and classification
uncertainty. When the two components are simultaneously trained on
skeleton-based action sequences, they correspond to a robust system for action
recognition with only a handful of labeled samples. We evaluate our system on
common datasets with multiple sequences and actions, such as NW UCLA, NTU RGB+D
60, and UWA3D. Our results outperform standalone skeleton-based supervised,
unsupervised with cluster identification, and active-learning methods for
action recognition when applied to sparse labeled samples, as low as 1% of the
data.
</p>
<a href="http://arxiv.org/abs/2012.01740" target="_blank">arXiv:2012.01740</a> [<a href="http://arxiv.org/pdf/2012.01740" target="_blank">pdf</a>]

<h2>3D-NVS: A 3D Supervision Approach for Next View Selection. (arXiv:2012.01743v1 [cs.CV])</h2>
<h3>Kumar Ashutosh, Saurabh Kumar, Subhasis Chaudhuri</h3>
<p>We present a classification based approach for the next best view selection
and show how we can plausibly obtain a supervisory signal for this task. The
proposed approach is end-to-end trainable and aims to get the best possible 3D
reconstruction quality with a pair of passively acquired 2D views. The proposed
model consists of two stages: a classifier and a reconstructor network trained
jointly via the indirect 3D supervision from ground truth voxels. While
testing, the proposed method assumes no prior knowledge of the underlying 3D
shape for selecting the next best view. We demonstrate the proposed method's
effectiveness via detailed experiments on synthetic and real images and show
how it provides improved reconstruction quality than the existing state of the
art 3D reconstruction and the next best view prediction techniques.
</p>
<a href="http://arxiv.org/abs/2012.01743" target="_blank">arXiv:2012.01743</a> [<a href="http://arxiv.org/pdf/2012.01743" target="_blank">pdf</a>]

<h2>Sample-efficient L0-L2 constrained structure learning of sparse Ising models. (arXiv:2012.01744v1 [stat.ML])</h2>
<h3>Antoine Dedieu, Miguel L&#xe1;zaro-Gredilla, Dileep George</h3>
<p>We consider the problem of learning the underlying graph of a sparse Ising
model with $p$ nodes from $n$ i.i.d. samples. The most recent and best
performing approaches combine an empirical loss (the logistic regression loss
or the interaction screening loss) with a regularizer (an L1 penalty or an L1
constraint). This results in a convex problem that can be solved separately for
each node of the graph. In this work, we leverage the cardinality constraint L0
norm, which is known to properly induce sparsity, and further combine it with
an L2 norm to better model the non-zero coefficients. We show that our proposed
estimators achieve an improved sample complexity, both (a) theoretically -- by
reaching new state-of-the-art upper bounds for recovery guarantees -- and (b)
empirically -- by showing sharper phase transitions between poor and full
recovery for graph topologies studied in the literature -- when compared to
their L1-based counterparts.
</p>
<a href="http://arxiv.org/abs/2012.01744" target="_blank">arXiv:2012.01744</a> [<a href="http://arxiv.org/pdf/2012.01744" target="_blank">pdf</a>]

<h2>Understanding Failures of Deep Networks via Robust Feature Extraction. (arXiv:2012.01750v1 [cs.CV])</h2>
<h3>Sahil Singla, Besmira Nushi, Shital Shah, Ece Kamar, Eric Horvitz</h3>
<p>Traditional evaluation metrics for learned models that report aggregate
scores over a test set are insufficient for surfacing important and informative
patterns of failure over features and instances. We introduce and study a
method aimed at characterizing and explaining failures by identifying visual
attributes whose presence or absence results in poor performance. In
distinction to previous work that relies upon crowdsourced labels for visual
attributes, we leverage the representation of a separate robust model to
extract interpretable features and then harness these features to identify
failure modes. We further propose a visualization method to enable humans to
understand the semantic meaning encoded in such features and test the
comprehensibility of the features. An evaluation of the methods on the ImageNet
dataset demonstrates that: (i) the proposed workflow is effective for
discovering important failure modes, (ii) the visualization techniques help
humans to understand the extracted features, and (iii) the extracted insights
can assist engineers with error analysis and debugging.
</p>
<a href="http://arxiv.org/abs/2012.01750" target="_blank">arXiv:2012.01750</a> [<a href="http://arxiv.org/pdf/2012.01750" target="_blank">pdf</a>]

<h2>Pedestrian Trajectory Prediction using Context-Augmented Transformer Networks. (arXiv:2012.01757v1 [cs.CV])</h2>
<h3>Khaled Saleh</h3>
<p>Forecasting the trajectory of pedestrians in shared urban traffic
environments is still considered one of the challenging problems facing the
development of autonomous vehicles (AVs). In the literature, this problem is
often tackled using recurrent neural networks (RNNs). Despite the powerful
capabilities of RNNs in capturing the temporal dependency in the pedestrians'
motion trajectories, they were argued to be challenged when dealing with longer
sequential data. Thus, in this work, we are introducing a framework based on
the transformer networks that were shown recently to be more efficient and
outperformed RNNs in many sequential-based tasks. We relied on a fusion of the
past positional information, agent interactions information and scene physical
semantics information as an input to our framework in order to provide a robust
trajectory prediction of pedestrians. We have evaluated our framework on two
real-life datasets of pedestrians in shared urban traffic environments and it
has outperformed the compared baseline approaches in both short-term and
long-term prediction horizons.
</p>
<a href="http://arxiv.org/abs/2012.01757" target="_blank">arXiv:2012.01757</a> [<a href="http://arxiv.org/pdf/2012.01757" target="_blank">pdf</a>]

<h2>Folding and Unfolding on Metagraphs. (arXiv:2012.01759v1 [cs.AI])</h2>
<h3>Ben Goertzel</h3>
<p>Typed metagraphs are defined as hypergraphs with types assigned to hyperedges
and their targets, and the potential to have targets of hyperedges connect to
whole links as well as targets. Directed typed metagraphs (DTMGs) are
introduced via partitioning the targets of each edge in a typed metagraph into
input, output and lateral sets; one can then look at "metapaths" in which
edges' output-sets are linked to other edges' input-sets. An initial algebra
approach to DTMGs is presented, including introduction of constructors for
building up DTMGs and laws regarding relationships among multiple ways of using
these constructors. A menagerie of useful morphism types is then defined on
DTMGs (catamorphisms, anamorphisms, histomorphisms, futumorphisms,
hylomorphisms, chronomorphisms, metamorphisms and metachronomorphisms),
providing a general abstract framework for formulating a broad variety of
metagraph operations. Deterministic and stochastic processes on typed
metagraphs are represented in terms of forests of DTMGs defined over a common
TMG, where the various morphisms can be straightforwardly extended to these
forests. The framework outlined can be applied to realistic metagraphs
involving complexities like dependent and probabilistic types, multidimensional
values and dynamic processing including insertion and deletion of edges.
</p>
<a href="http://arxiv.org/abs/2012.01759" target="_blank">arXiv:2012.01759</a> [<a href="http://arxiv.org/pdf/2012.01759" target="_blank">pdf</a>]

<h2>Beyond Cats and Dogs: Semi-supervised Classification of fuzzy labels with overclustering. (arXiv:2012.01768v1 [cs.CV])</h2>
<h3>Lars Schmarje, Johannes Br&#xfc;nger, Monty Santarossa, Simon-Martin Schr&#xf6;der, Rainer Kiko, Reinhard Koch</h3>
<p>A long-standing issue with deep learning is the need for large and
consistently labeled datasets. Although the current research in semi-supervised
learning can decrease the required amount of annotated data by a factor of 10
or even more, this line of research still uses distinct classes like cats and
dogs. However, in the real-world we often encounter problems where different
experts have different opinions, thus producing fuzzy labels. We propose a
novel framework for handling semi-supervised classifications of such fuzzy
labels. Our framework is based on the idea of overclustering to detect
substructures in these fuzzy labels. We propose a novel loss to improve the
overclustering capability of our framework and show on the common image
classification dataset STL-10 that it is faster and has better overclustering
performance than previous work. On a real-world plankton dataset, we illustrate
the benefit of overclustering for fuzzy labels and show that we beat previous
state-of-the-art semisupervised methods. Moreover, we acquire 5 to 10% more
consistent predictions of substructures.
</p>
<a href="http://arxiv.org/abs/2012.01768" target="_blank">arXiv:2012.01768</a> [<a href="http://arxiv.org/pdf/2012.01768" target="_blank">pdf</a>]

<h2>Neural Contextual Bandits with Deep Representation and Shallow Exploration. (arXiv:2012.01780v1 [cs.LG])</h2>
<h3>Pan Xu, Zheng Wen, Handong Zhao, Quanquan Gu</h3>
<p>We study a general class of contextual bandits, where each context-action
pair is associated with a raw feature vector, but the reward generating
function is unknown. We propose a novel learning algorithm that transforms the
raw feature vector using the last hidden layer of a deep ReLU neural network
(deep representation learning), and uses an upper confidence bound (UCB)
approach to explore in the last linear layer (shallow exploration). We prove
that under standard assumptions, our proposed algorithm achieves
$\tilde{O}(\sqrt{T})$ finite-time regret, where $T$ is the learning time
horizon. Compared with existing neural contextual bandit algorithms, our
approach is computationally much more efficient since it only needs to explore
in the last layer of the deep neural network.
</p>
<a href="http://arxiv.org/abs/2012.01780" target="_blank">arXiv:2012.01780</a> [<a href="http://arxiv.org/pdf/2012.01780" target="_blank">pdf</a>]

<h2>Attributes Aware Face Generation with Generative Adversarial Networks. (arXiv:2012.01782v1 [cs.CV])</h2>
<h3>Zheng Yuan, Jie Zhang, Shiguang Shan, Xilin Chen</h3>
<p>Recent studies have shown remarkable success in face image generations.
However, most of the existing methods only generate face images from random
noise, and cannot generate face images according to the specific attributes. In
this paper, we focus on the problem of face synthesis from attributes, which
aims at generating faces with specific characteristics corresponding to the
given attributes. To this end, we propose a novel attributes aware face image
generator method with generative adversarial networks called AFGAN.
Specifically, we firstly propose a two-path embedding layer and self-attention
mechanism to convert binary attribute vector to rich attribute features. Then
three stacked generators generate $64 \times 64$, $128 \times 128$ and $256
\times 256$ resolution face images respectively by taking the attribute
features as input. In addition, an image-attribute matching loss is proposed to
enhance the correlation between the generated images and input attributes.
Extensive experiments on CelebA demonstrate the superiority of our AFGAN in
terms of both qualitative and quantitative evaluations.
</p>
<a href="http://arxiv.org/abs/2012.01782" target="_blank">arXiv:2012.01782</a> [<a href="http://arxiv.org/pdf/2012.01782" target="_blank">pdf</a>]

<h2>SB-MTL: Score-based Meta Transfer-Learning for Cross-Domain Few-Shot Learning. (arXiv:2012.01784v1 [cs.CV])</h2>
<h3>John Cai, Bill Cai, Sheng Mei Shen</h3>
<p>While many deep learning methods have seen significant success in tackling
the problem of domain adaptation and few-shot learning separately, far fewer
methods are able to jointly tackle both problems in Cross-Domain Few-Shot
Learning (CD-FSL). This problem is exacerbated under sharp domain shifts that
typify common computer vision applications. In this paper, we present a novel,
flexible and effective method to address the CD-FSL problem. Our method, called
Score-based Meta Transfer-Learning (SB-MTL), combines transfer-learning and
meta-learning by using a MAML-optimized feature encoder and a score-based Graph
Neural Network. First, we have a feature encoder with specific layers designed
to be fine-tuned. To do so, we apply a first-order MAML algorithm to find good
initializations. Second, instead of directly taking the classification scores
after fine-tuning, we interpret the scores as coordinates by mapping the
pre-softmax classification scores onto a metric space. Subsequently, we apply a
Graph Neural Network to propagate label information from the support set to the
query set in our score-based metric space. We test our model on the Broader
Study of Cross-Domain Few-Shot Learning (BSCD-FSL) benchmark, which includes a
range of target domains with highly varying dissimilarity to the miniImagenet
source domain. We observe significant improvements in accuracy across 5, 20 and
50 shot, and on the four target domains. In terms of average accuracy, our
model outperforms previous transfer-learning methods by 5.93% and previous
meta-learning methods by 14.28%.
</p>
<a href="http://arxiv.org/abs/2012.01784" target="_blank">arXiv:2012.01784</a> [<a href="http://arxiv.org/pdf/2012.01784" target="_blank">pdf</a>]

<h2>Object-Driven Active Mapping for More Accurate Object Pose Estimation and Robotic Grasping. (arXiv:2012.01788v1 [cs.RO])</h2>
<h3>Yanmin Wu, Yunzhou Zhang, Delong Zhu, Xin Chen, Sonya Coleman, Wenkai Sun, Xinggang Hu, Zhiqiang Deng</h3>
<p>This paper presents the first active object mapping framework for complex
robotic grasping tasks. The framework is built on an object SLAM system
integrated with a simultaneous multi-object pose estimation process. Aiming to
reduce the observation uncertainty on target objects and increase their pose
estimation accuracy, we also design an object-driven exploration strategy to
guide the object mapping process. By combining the mapping module and the
exploration strategy, an accurate object map that is compatible with robotic
grasping can be generated. Quantitative evaluations also show that the proposed
framework has a very high mapping accuracy. Manipulation experiments, including
object grasping, object placement, and the augmented reality, significantly
demonstrate the effectiveness and advantages of our proposed framework.
</p>
<a href="http://arxiv.org/abs/2012.01788" target="_blank">arXiv:2012.01788</a> [<a href="http://arxiv.org/pdf/2012.01788" target="_blank">pdf</a>]

<h2>Distributed Thompson Sampling. (arXiv:2012.01789v1 [cs.AI])</h2>
<h3>Jing Dong, Tan Li, Shaolei Ren, Linqi Song</h3>
<p>We study a cooperative multi-agent multi-armed bandits with M agents and K
arms. The goal of the agents is to minimized the cumulative regret. We adapt a
traditional Thompson Sampling algoirthm under the distributed setting. However,
with agent's ability to communicate, we note that communication may further
reduce the upper bound of the regret for a distributed Thompson Sampling
approach. To further improve the performance of distributed Thompson Sampling,
we propose a distributed Elimination based Thompson Sampling algorithm that
allow the agents to learn collaboratively. We analyse the algorithm under
Bernoulli reward and derived a problem dependent upper bound on the cumulative
regret.
</p>
<a href="http://arxiv.org/abs/2012.01789" target="_blank">arXiv:2012.01789</a> [<a href="http://arxiv.org/pdf/2012.01789" target="_blank">pdf</a>]

<h2>FAT: Federated Adversarial Training. (arXiv:2012.01791v1 [cs.LG])</h2>
<h3>Giulio Zizzo, Ambrish Rawat, Mathieu Sinn, Beat Buesser</h3>
<p>Federated learning (FL) is one of the most important paradigms addressing
privacy and data governance issues in machine learning (ML). Adversarial
training has emerged, so far, as the most promising approach against evasion
threats on ML models. In this paper, we take the first known steps towards
federated adversarial training (FAT) combining both methods to reduce the
threat of evasion during inference while preserving the data privacy during
training. We investigate the effectiveness of the FAT protocol for idealised
federated settings using MNIST, Fashion-MNIST, and CIFAR10, and provide first
insights on stabilising the training on the LEAF benchmark dataset which
specifically emulates a federated learning environment. We identify challenges
with this natural extension of adversarial training with regards to achieved
adversarial robustness and further examine the idealised settings in the
presence of clients undermining model convergence. We find that Trimmed Mean
and Bulyan defences can be compromised and we were able to subvert Krum with a
novel distillation based attack which presents an apparently "robust" model to
the defender while in fact the model fails to provide robustness against simple
attack modifications.
</p>
<a href="http://arxiv.org/abs/2012.01791" target="_blank">arXiv:2012.01791</a> [<a href="http://arxiv.org/pdf/2012.01791" target="_blank">pdf</a>]

<h2>Semi-Supervised Learning with Variational Bayesian Inference and Maximum Uncertainty Regularization. (arXiv:2012.01793v1 [cs.LG])</h2>
<h3>Kien Do, Truyen Tran, Svetha Venkatesh</h3>
<p>We propose two generic methods for improving semi-supervised learning (SSL).
The first integrates weight perturbation (WP) into existing "consistency
regularization" (CR) based methods. We implement WP by leveraging variational
Bayesian inference (VBI). The second method proposes a novel consistency loss
called "maximum uncertainty regularization" (MUR). While most consistency
losses act on perturbations in the vicinity of each data point, MUR actively
searches for "virtual" points situated beyond this region that cause the most
uncertain class predictions. This allows MUR to impose smoothness on a wider
area in the input-output manifold. Our experiments show clear improvements in
classification errors of various CR based methods when they are combined with
VBI or MUR or both.
</p>
<a href="http://arxiv.org/abs/2012.01793" target="_blank">arXiv:2012.01793</a> [<a href="http://arxiv.org/pdf/2012.01793" target="_blank">pdf</a>]

<h2>Interpretability and Explainability: A Machine Learning Zoo Mini-tour. (arXiv:2012.01805v1 [cs.LG])</h2>
<h3>Ri&#x10d;ards Marcinkevi&#x10d;s, Julia E. Vogt</h3>
<p>In this review, we examine the problem of designing interpretable and
explainable machine learning models. Interpretability and explainability lie at
the core of many machine learning and statistical applications in medicine,
economics, law, and natural sciences. Although interpretability and
explainability have escaped a clear universal definition, many techniques
motivated by these properties have been developed over the recent 30 years with
the focus currently shifting towards deep learning methods. In this review, we
emphasise the divide between interpretability and explainability and illustrate
these two different research directions with concrete examples of the
state-of-the-art. The review is intended for a general machine learning
audience with interest in exploring the problems of interpretation and
explanation beyond logistic regression or random forest variable importance.
This work is not an exhaustive literature survey, but rather a primer focusing
selectively on certain lines of research which the authors found interesting or
informative.
</p>
<a href="http://arxiv.org/abs/2012.01805" target="_blank">arXiv:2012.01805</a> [<a href="http://arxiv.org/pdf/2012.01805" target="_blank">pdf</a>]

<h2>Attribute-Guided Adversarial Training for Robustness to Natural Perturbations. (arXiv:2012.01806v1 [cs.CV])</h2>
<h3>Tejas Gokhale, Rushil Anirudh, Bhavya Kailkhura, Jayaraman J. Thiagarajan, Chitta Baral, Yezhou Yang</h3>
<p>While existing work in robust deep learning has focused on small pixel-level
$\ell_p$ norm-based perturbations, this may not account for perturbations
encountered in several real world settings. In many such cases although test
data might not be available, broad specifications about the types of
perturbations (such as an unknown degree of rotation) may be known. We consider
a setup where robustness is expected over an unseen test domain that is not
i.i.d. but deviates from the training domain. While this deviation may not be
exactly known, its broad characterization is specified a priori, in terms of
attributes. We propose an adversarial training approach which learns to
generate new samples so as to maximize exposure of the classifier to the
attributes-space, without having access to the data from the test domain. Our
adversarial training solves a min-max optimization problem, with the inner
maximization generating adversarial perturbations, and the outer minimization
finding model parameters by optimizing the loss on adversarial perturbations
generated from the inner maximization. We demonstrate the applicability of our
approach on three types of naturally occurring perturbations -- object-related
shifts, geometric transformations, and common image corruptions. Our approach
enables deep neural networks to be robust against a wide range of naturally
occurring perturbations. We demonstrate the usefulness of the proposed approach
by showing the robustness gains of deep neural networks trained using our
adversarial training on MNIST, CIFAR-10, and a new variant of the CLEVR
dataset.
</p>
<a href="http://arxiv.org/abs/2012.01806" target="_blank">arXiv:2012.01806</a> [<a href="http://arxiv.org/pdf/2012.01806" target="_blank">pdf</a>]

<h2>D-Unet: A Dual-encoder U-Net for Image Splicing Forgery Detection and Localization. (arXiv:2012.01821v1 [cs.CV])</h2>
<h3>Xiuli Bi, Yanbin Liu, Bin Xiao, Weisheng Li, Chi-Man Pun, Guoyin Wang, Xinbo Gao</h3>
<p>Recently, many detection methods based on convolutional neural networks
(CNNs) have been proposed for image splicing forgery detection. Most of these
detection methods focus on the local patches or local objects. In fact, image
splicing forgery detection is a global binary classification task that
distinguishes the tampered and non-tampered regions by image fingerprints.
However, some specific image contents are hardly retained by CNN-based
detection networks, but if included, would improve the detection accuracy of
the networks. To resolve these issues, we propose a novel network called
dual-encoder U-Net (D-Unet) for image splicing forgery detection, which employs
an unfixed encoder and a fixed encoder. The unfixed encoder autonomously learns
the image fingerprints that differentiate between the tampered and non-tampered
regions, whereas the fixed encoder intentionally provides the direction
information that assists the learning and detection of the network. This
dual-encoder is followed by a spatial pyramid global-feature extraction module
that expands the global insight of D-Unet for classifying the tampered and
non-tampered regions more accurately. In an experimental comparison study of
D-Unet and state-of-the-art methods, D-Unet outperformed the other methods in
image-level and pixel-level detection, without requiring pre-training or
training on a large number of forgery images. Moreover, it was stably robust to
different attacks.
</p>
<a href="http://arxiv.org/abs/2012.01821" target="_blank">arXiv:2012.01821</a> [<a href="http://arxiv.org/pdf/2012.01821" target="_blank">pdf</a>]

<h2>Cognitive Capabilities for the CAAI in Cyber-Physical Production Systems. (arXiv:2012.01823v1 [cs.AI])</h2>
<h3>Jan Strohschein, Andreas Fischbach, Andreas Bunte, Heide Faeskorn-Woyke, Natalia Moriz, Thomas Bartz-Beielstein</h3>
<p>This paper presents the cognitive module of the cognitive architecture for
artificial intelligence (CAAI) in cyber-physical production systems (CPPS). The
goal of this architecture is to reduce the implementation effort of artificial
intelligence (AI) algorithms in CPPS. Declarative user goals and the provided
algorithm-knowledge base allow the dynamic pipeline orchestration and
configuration. A big data platform (BDP) instantiates the pipelines and
monitors the CPPS performance for further evaluation through the cognitive
module. Thus, the cognitive module is able to select feasible and robust
configurations for process pipelines in varying use cases. Furthermore, it
automatically adapts the models and algorithms based on model quality and
resource consumption. The cognitive module also instantiates additional
pipelines to test algorithms from different classes. CAAI relies on
well-defined interfaces to enable the integration of additional modules and
reduce implementation effort. Finally, an implementation based on Docker,
Kubernetes, and Kafka for the virtualization and orchestration of the
individual modules and as messaging-technology for module communication is used
to evaluate a real-world use case.
</p>
<a href="http://arxiv.org/abs/2012.01823" target="_blank">arXiv:2012.01823</a> [<a href="http://arxiv.org/pdf/2012.01823" target="_blank">pdf</a>]

<h2>Singularity-free Guiding Vector Field for Robot Navigation. (arXiv:2012.01826v1 [cs.RO])</h2>
<h3>Weijia Yao, Hector Garcia de Marina, Bohuan Lin, Ming Cao</h3>
<p>Most of the existing path-following navigation algorithms cannot guarantee
global convergence to desired paths or enable following self-intersected
desired paths due to the existence of singular points where navigation
algorithms return unreliable or even no solutions. One typical example arises
in vector-field guided path-following (VF-PF) navigation algorithms. These
algorithms are based on a vector field, and the singular points are exactly
where the vector field diminishes. In this paper, we show that it is
mathematically impossible for conventional VF-PF algorithms to achieve global
convergence to desired paths that are self-intersected or even just simple
closed (precisely, homeomorphic to the unit circle). Motivated by this new
impossibility result, we propose a novel method to transform self-intersected
or simple closed desired paths to non-self-intersected and unbounded
(precisely, homeomorphic to the real line) counterparts in a higher-dimensional
space. Corresponding to this new desired path, we construct a singularity-free
guiding vector field on a higher-dimensional space. The integral curves of this
new guiding vector field is thus exploited to enable global convergence to the
higher-dimensional desired path, and therefore the projection of the integral
curves on a lower-dimensional subspace converge to the physical
(lower-dimensional) desired path. Rigorous theoretical analysis is carried out
for the theoretical results using dynamical systems theory. In addition, we
show both by theoretical analysis and numerical simulations that our proposed
method is an extension combining conventional VF-PF algorithms and trajectory
tracking algorithms. Finally, to show the practical value of our proposed
approach for complex engineering systems, we conduct outdoor experiments with a
fixed-wing airplane in windy environment to follow both 2D and 3D desired
paths.
</p>
<a href="http://arxiv.org/abs/2012.01826" target="_blank">arXiv:2012.01826</a> [<a href="http://arxiv.org/pdf/2012.01826" target="_blank">pdf</a>]

<h2>Image inpainting using frequency domain priors. (arXiv:2012.01832v1 [cs.CV])</h2>
<h3>Hiya Roy, Subhajit Chaudhury, Toshihiko Yamasaki, Tatsuaki Hashimoto</h3>
<p>In this paper, we present a novel image inpainting technique using frequency
domain information. Prior works on image inpainting predict the missing pixels
by training neural networks using only the spatial domain information. However,
these methods still struggle to reconstruct high-frequency details for real
complex scenes, leading to a discrepancy in color, boundary artifacts,
distorted patterns, and blurry textures. To alleviate these problems, we
investigate if it is possible to obtain better performance by training the
networks using frequency domain information (Discrete Fourier Transform) along
with the spatial domain information. To this end, we propose a frequency-based
deconvolution module that enables the network to learn the global context while
selectively reconstructing the high-frequency components. We evaluate our
proposed method on the publicly available datasets CelebA, Paris Streetview,
and DTD texture dataset, and show that our method outperforms current
state-of-the-art image inpainting techniques both qualitatively and
quantitatively.
</p>
<a href="http://arxiv.org/abs/2012.01832" target="_blank">arXiv:2012.01832</a> [<a href="http://arxiv.org/pdf/2012.01832" target="_blank">pdf</a>]

<h2>Distributed Training and Optimization Of Neural Networks. (arXiv:2012.01839v1 [cs.LG])</h2>
<h3>Jean-Roch Vlimant, Junqi Yin</h3>
<p>Deep learning models are yielding increasingly better performances thanks to
multiple factors. To be successful, model may have large number of parameters
or complex architectures and be trained on large dataset. This leads to large
requirements on computing resource and turn around time, even more so when
hyper-parameter optimization is done (e.g search over model architectures).
While this is a challenge that goes beyond particle physics, we review the
various ways to do the necessary computations in parallel, and put it in the
context of high energy physics.
</p>
<a href="http://arxiv.org/abs/2012.01839" target="_blank">arXiv:2012.01839</a> [<a href="http://arxiv.org/pdf/2012.01839" target="_blank">pdf</a>]

<h2>Stochastic Adversarial Gradient Embedding for Active Domain Adaptation. (arXiv:2012.01843v1 [cs.LG])</h2>
<h3>Victor Bouvier, Philippe Very, Cl&#xe9;ment Chastagnol, Myriam Tami, C&#xe9;line Hudelot</h3>
<p>Unsupervised Domain Adaptation (UDA) aims to bridge the gap between a source
domain, where labelled data are available, and a target domain only represented
with unlabelled data. If domain invariant representations have dramatically
improved the adaptability of models, to guarantee their good transferability
remains a challenging problem. This paper addresses this problem by using
active learning to annotate a small budget of target data. Although this setup,
called Active Domain Adaptation (ADA), deviates from UDA's standard setup, a
wide range of practical applications are faced with this situation. To this
purpose, we introduce \textit{Stochastic Adversarial Gradient Embedding}
(SAGE), a framework that makes a triple contribution to ADA. First, we select
for annotation target samples that are likely to improve the representations'
transferability by measuring the variation, before and after annotation, of the
transferability loss gradient. Second, we increase sampling diversity by
promoting different gradient directions. Third, we introduce a novel training
procedure for actively incorporating target samples when learning invariant
representations. SAGE is based on solid theoretical ground and validated on
various UDA benchmarks against several baselines. Our empirical investigation
demonstrates that SAGE takes the best of uncertainty \textit{vs} diversity
samplings and improves representations transferability substantially.
</p>
<a href="http://arxiv.org/abs/2012.01843" target="_blank">arXiv:2012.01843</a> [<a href="http://arxiv.org/pdf/2012.01843" target="_blank">pdf</a>]

<h2>Goal-Driven Robotic Pushing Using Tactile and Proprioceptive Feedback. (arXiv:2012.01859v1 [cs.RO])</h2>
<h3>John Lloyd, Nathan F. Lepora</h3>
<p>In robots, nonprehensile manipulation operations such as pushing are a useful
way of moving large, heavy or unwieldy objects, moving multiple objects at
once, or reducing uncertainty in the location or pose of objects. In this
study, we propose a reactive and adaptive method for robotic pushing that uses
rich feedback from a high-resolution optical tactile sensor to control push
movements instead of relying on analytical or data-driven models of push
interactions. Specifically, we use goal-driven tactile exploration to actively
search for stable pushing configurations that cause the object to maintain its
pose relative to the pusher while incrementally moving the pusher and object
towards the target. We evaluate our method by pushing objects across planar and
curved surfaces. For planar surfaces, we show that the method is accurate and
robust to variations in initial contact position/angle, object shape and start
position; for curved surfaces, the performance is degraded slightly. An
immediate consequence of our work is that it shows that explicit models of push
interactions might be sufficient but are not necessary for this type of task.
It also raises the interesting question of which aspects of the system should
be modelled to achieve the best performance and generalization across a wide
range of scenarios. Finally, it highlights the importance of testing on
non-planar surfaces and in other more complex environments when developing new
methods for robotic pushing.
</p>
<a href="http://arxiv.org/abs/2012.01859" target="_blank">arXiv:2012.01859</a> [<a href="http://arxiv.org/pdf/2012.01859" target="_blank">pdf</a>]

<h2>Make One-Shot Video Object Segmentation Efficient Again. (arXiv:2012.01866v1 [cs.CV])</h2>
<h3>Tim Meinhardt, Laura Leal-Taixe</h3>
<p>Video object segmentation (VOS) describes the task of segmenting a set of
objects in each frame of a video. In the semi-supervised setting, the first
mask of each object is provided at test time. Following the one-shot principle,
fine-tuning VOS methods train a segmentation model separately on each given
object mask. However, recently the VOS community has deemed such a test time
optimization and its impact on the test runtime as unfeasible. To mitigate the
inefficiencies of previous fine-tuning approaches, we present efficient
One-Shot Video Object Segmentation (e-OSVOS). In contrast to most VOS
approaches, e-OSVOS decouples the object detection task and predicts only local
segmentation masks by applying a modified version of Mask R-CNN. The one-shot
test runtime and performance are optimized without a laborious and handcrafted
hyperparameter search. To this end, we meta learn the model initialization and
learning rates for the test time optimization. To achieve optimal learning
behavior, we predict individual learning rates at a neuron level. Furthermore,
we apply an online adaptation to address the common performance degradation
throughout a sequence by continuously fine-tuning the model on previous mask
predictions supported by a frame-to-frame bounding box propagation. e-OSVOS
provides state-of-the-art results on DAVIS 2016, DAVIS 2017, and YouTube-VOS
for one-shot fine-tuning methods while reducing the test runtime substantially.

Code is available at https://github.com/dvl-tum/e-osvos.
</p>
<a href="http://arxiv.org/abs/2012.01866" target="_blank">arXiv:2012.01866</a> [<a href="http://arxiv.org/pdf/2012.01866" target="_blank">pdf</a>]

<h2>Model-free Neural Counterfactual Regret Minimization with Bootstrap Learning. (arXiv:2012.01870v1 [cs.LG])</h2>
<h3>Weiming Liu, Bin Li, Julian Togelius</h3>
<p>Counterfactual Regret Minimization (CFR) has achieved many fascinating
results in solving large scale Imperfect Information Games (IIGs). Neural CFR
is one of the promising techniques that can effectively reduce the computation
and memory consumption of CFR by generalizing decision information between
similar states. However, current neural CFR algorithms have to approximate the
cumulative variables in iterations with neural networks, which usually results
in large estimation variance given the huge complexity of IIGs. Moreover,
model-based sampling and inefficient training make current neural CFR
algorithms still computationally expensive. In this paper, a new model-free
neural CFR algorithm with bootstrap learning is proposed, in which, a Recursive
Substitute Value (RSV) network is trained to replace the cumulative variables
in CFR. The RSV is defined recursively and can be estimated independently in
every iteration using bootstrapping. Then there is no need to track or
approximate the cumulative variables any more. Based on the RSV, the new neural
CFR algorithm is model-free and has higher training efficiency. Experimental
results show that the new algorithm can match the state-of-the-art neural CFR
algorithms and with less training cost.
</p>
<a href="http://arxiv.org/abs/2012.01870" target="_blank">arXiv:2012.01870</a> [<a href="http://arxiv.org/pdf/2012.01870" target="_blank">pdf</a>]

<h2>Towards Repairing Neural Networks Correctly. (arXiv:2012.01872v1 [cs.LG])</h2>
<h3>Guoliang Dong, Jun Sun, Jingyi Wang, Xinyu Wang, Ting Dai</h3>
<p>Neural networks are increasingly applied to support decision making in
safety-critical applications (like autonomous cars, unmanned aerial vehicles
and face recognition based authentication). While many impressive static
verification techniques have been proposed to tackle the correctness problem of
neural networks, it is possible that static verification may never be
sufficiently scalable to handle real-world neural networks. In this work, we
propose a runtime verification method to ensure the correctness of neural
networks. Given a neural network and a desirable safety property, we adopt
state-of-the-art static verification techniques to identify strategically
locations to introduce additional gates which "correct" neural network
behaviors at runtime. Experiment results show that our approach effectively
generates neural networks which are guaranteed to satisfy the properties,
whilst being consistent with the original neural network most of the time.
</p>
<a href="http://arxiv.org/abs/2012.01872" target="_blank">arXiv:2012.01872</a> [<a href="http://arxiv.org/pdf/2012.01872" target="_blank">pdf</a>]

<h2>Learning Two-Stream CNN for Multi-Modal Age-related Macular Degeneration Categorization. (arXiv:2012.01879v1 [cs.CV])</h2>
<h3>Weisen Wang, Xirong Li, Zhiyan Xu, Weihong Yu, Jianchun Zhao, Dayong Ding, Youxin Chen</h3>
<p>This paper tackles automated categorization of Age-related Macular
Degeneration (AMD), a common macular disease among people over 50. Previous
research efforts mainly focus on AMD categorization with a single-modal input,
let it be a color fundus image or an OCT image. By contrast, we consider AMD
categorization given a multi-modal input, a direction that is clinically
meaningful yet mostly unexplored. Contrary to the prior art that takes a
traditional approach of feature extraction plus classifier training that cannot
be jointly optimized, we opt for end-to-end multi-modal Convolutional Neural
Networks (MM-CNN). Our MM-CNN is instantiated by a two-stream CNN, with
spatially-invariant fusion to combine information from the fundus and OCT
streams. In order to visually interpret the contribution of the individual
modalities to the final prediction, we extend the class activation mapping
(CAM) technique to the multi-modal scenario. For effective training of MM-CNN,
we develop two data augmentation methods. One is GAN-based fundus / OCT image
synthesis, with our novel use of CAMs as conditional input of a high-resolution
image-to-image translation GAN. The other method is Loose Pairing, which pairs
a fundus image and an OCT image on the basis of their classes instead of eye
identities. Experiments on a clinical dataset consisting of 1,099 color fundus
images and 1,290 OCT images acquired from 1,099 distinct eyes verify the
effectiveness of the proposed solution for multi-modal AMD categorization.
</p>
<a href="http://arxiv.org/abs/2012.01879" target="_blank">arXiv:2012.01879</a> [<a href="http://arxiv.org/pdf/2012.01879" target="_blank">pdf</a>]

<h2>Competition analysis on the over-the-counter credit default swap market. (arXiv:2012.01883v1 [cs.LG])</h2>
<h3>Louis Abraham</h3>
<p>We study two questions related to competition on the OTC CDS market using
data collected as part of the EMIR regulation.

First, we study the competition between central counterparties through
collateral requirements. We present models that successfully estimate the
initial margin requirements. However, our estimations are not precise enough to
use them as input to a predictive model for CCP choice by counterparties in the
OTC market.

Second, we model counterpart choice on the interdealer market using a novel
semi-supervised predictive task. We present our methodology as part of the
literature on model interpretability before arguing for the use of conditional
entropy as the metric of interest to derive knowledge from data through a
model-agnostic approach. In particular, we justify the use of deep neural
networks to measure conditional entropy on real-world datasets. We create the
$\textit{Razor entropy}$ using the framework of algorithmic information theory
and derive an explicit formula that is identical to our semi-supervised
training objective. Finally, we borrow concepts from game theory to define
$\textit{top-k Shapley values}$. This novel method of payoff distribution
satisfies most of the properties of Shapley values, and is of particular
interest when the value function is monotone submodular. Unlike classical
Shapley values, top-k Shapley values can be computed in quadratic time of the
number of features instead of exponential. We implement our methodology and
report the results on our particular task of counterpart choice.

Finally, we present an improvement to the $\textit{node2vec}$ algorithm that
could for example be used to further study intermediation. We show that the
neighbor sampling used in the generation of biased walks can be performed in
logarithmic time with a quasilinear time pre-computation, unlike the current
implementations that do not scale well.
</p>
<a href="http://arxiv.org/abs/2012.01883" target="_blank">arXiv:2012.01883</a> [<a href="http://arxiv.org/pdf/2012.01883" target="_blank">pdf</a>]

<h2>Temporal Pyramid Network for Pedestrian Trajectory Prediction with Multi-Supervision. (arXiv:2012.01884v1 [cs.CV])</h2>
<h3>Rongqin Liang, Yuanman Li, Xia Li, yi tang, Jiantao Zhou, Wenbin Zou</h3>
<p>Predicting human motion behavior in a crowd is important for many
applications, ranging from the natural navigation of autonomous vehicles to
intelligent security systems of video surveillance. All the previous works
model and predict the trajectory with a single resolution, which is rather
inefficient and difficult to simultaneously exploit the long-range information
(e.g., the destination of the trajectory), and the short-range information
(e.g., the walking direction and speed at a certain time) of the motion
behavior. In this paper, we propose a temporal pyramid network for pedestrian
trajectory prediction through a squeeze modulation and a dilation modulation.
Our hierarchical framework builds a feature pyramid with increasingly richer
temporal information from top to bottom, which can better capture the motion
behavior at various tempos. Furthermore, we propose a coarse-to-fine fusion
strategy with multi-supervision. By progressively merging the top coarse
features of global context to the bottom fine features of rich local context,
our method can fully exploit both the long-range and short-range information of
the trajectory. Experimental results on several benchmarks demonstrate the
superiority of our method. Our code and models will be available upon
acceptance.
</p>
<a href="http://arxiv.org/abs/2012.01884" target="_blank">arXiv:2012.01884</a> [<a href="http://arxiv.org/pdf/2012.01884" target="_blank">pdf</a>]

<h2>An Empirical Study of Derivative-Free-Optimization Algorithms for Targeted Black-Box Attacks in Deep Neural Networks. (arXiv:2012.01901v1 [cs.LG])</h2>
<h3>Giuseppe Ughi, Vinayak Abrol, Jared Tanner</h3>
<p>We perform a comprehensive study on the performance of derivative free
optimization (DFO) algorithms for the generation of targeted black-box
adversarial attacks on Deep Neural Network (DNN) classifiers assuming the
perturbation energy is bounded by an $\ell_\infty$ constraint and the number of
queries to the network is limited. This paper considers four pre-existing
state-of-the-art DFO-based algorithms along with the introduction of a new
algorithm built on BOBYQA, a model-based DFO method. We compare these
algorithms in a variety of settings according to the fraction of images that
they successfully misclassify given a maximum number of queries to the DNN.

The experiments disclose how the likelihood of finding an adversarial example
depends on both the algorithm used and the setting of the attack; algorithms
limiting the search of adversarial example to the vertices of the $\ell^\infty$
constraint work particularly well without structural defenses, while the
presented BOBYQA based algorithm works better for especially small perturbation
energies. This variance in performance highlights the importance of new
algorithms being compared to the state-of-the-art in a variety of settings, and
the effectiveness of adversarial defenses being tested using as wide a range of
algorithms as possible.
</p>
<a href="http://arxiv.org/abs/2012.01901" target="_blank">arXiv:2012.01901</a> [<a href="http://arxiv.org/pdf/2012.01901" target="_blank">pdf</a>]

<h2>Patch2Pix: Epipolar-Guided Pixel-Level Correspondences. (arXiv:2012.01909v1 [cs.CV])</h2>
<h3>Qunjie Zhou, Torsten Sattler, Laura Leal-Taixe</h3>
<p>Deep learning has been applied to a classical matching pipeline which
typically involves three steps: (i) local feature detection and description,
(ii) feature matching, and (iii) outlier rejection. Recently emerged
correspondence networks propose to perform those steps inside a single network
but suffer from low matching resolution due to the memory bottleneck. In this
work, we propose a new perspective to estimate correspondences in a
detect-to-refine manner, where we first predict patch-level match proposals and
then refine them. We present a novel refinement network Patch2Pix that refines
match proposals by regressing pixel-level matches from the local regions
defined by those proposals and jointly rejecting outlier matches with
confidence scores, which is weakly supervised to learn correspondences that are
consistent with the epipolar geometry of an input image pair. We show that our
refinement network significantly improves the performance of correspondence
networks on image matching, homography estimation, and localization tasks. In
addition, we show that our learned refinement generalizes to fully-supervised
methods without re-training, which leads us to state-of-the-art localization
performance.
</p>
<a href="http://arxiv.org/abs/2012.01909" target="_blank">arXiv:2012.01909</a> [<a href="http://arxiv.org/pdf/2012.01909" target="_blank">pdf</a>]

<h2>Transfer Learning as an Enabler of the Intelligent Digital Twin. (arXiv:2012.01913v1 [cs.LG])</h2>
<h3>Benjamin Maschler, Dominik Braun, Nasser Jazdi, Michael Weyrich</h3>
<p>Digital Twins have been described as beneficial in many areas, such as
virtual commissioning, fault prediction or reconfiguration planning. Equipping
Digital Twins with artificial intelligence functionalities can greatly expand
those beneficial applications or open up altogether new areas of application,
among them cross-phase industrial transfer learning. In the context of machine
learning, transfer learning represents a set of approaches that enhance
learning new tasks based upon previously acquired knowledge. Here, knowledge is
transferred from one lifecycle phase to another in order to reduce the amount
of data or time needed to train a machine learning algorithm. Looking at common
challenges in developing and deploying industrial machinery with deep learning
functionalities, embracing this concept would offer several advantages: Using
an intelligent Digital Twin, learning algorithms can be designed, configured
and tested in the design phase before the physical system exists and real data
can be collected. Once real data becomes available, the algorithms must merely
be fine-tuned, significantly speeding up commissioning and reducing the
probability of costly modifications. Furthermore, using the Digital Twin's
simulation capabilities virtually injecting rare faults in order to train an
algorithm's response or using reinforcement learning, e.g. to teach a robot,
become practically feasible. This article presents several cross-phase
industrial transfer learning use cases utilizing intelligent Digital Twins. A
real cyber physical production system consisting of an automated welding
machine and an automated guided vehicle equipped with a robot arm is used to
illustrate the respective benefits.
</p>
<a href="http://arxiv.org/abs/2012.01913" target="_blank">arXiv:2012.01913</a> [<a href="http://arxiv.org/pdf/2012.01913" target="_blank">pdf</a>]

<h2>DeepCrawl: Deep Reinforcement Learning for Turn-based Strategy Games. (arXiv:2012.01914v1 [cs.LG])</h2>
<h3>Alessandro Sestini, Alexander Kuhnle, Andrew D. Bagdanov</h3>
<p>In this paper we introduce DeepCrawl, a fully-playable Roguelike prototype
for iOS and Android in which all agents are controlled by policy networks
trained using Deep Reinforcement Learning (DRL). Our aim is to understand
whether recent advances in DRL can be used to develop convincing behavioral
models for non-player characters in videogames. We begin with an analysis of
requirements that such an AI system should satisfy in order to be practically
applicable in video game development, and identify the elements of the DRL
model used in the DeepCrawl prototype. The successes and limitations of
DeepCrawl are documented through a series of playability tests performed on the
final game. We believe that the techniques we propose offer insight into
innovative new avenues for the development of behaviors for non-player
characters in video games, as they offer the potential to overcome critical
issues with
</p>
<a href="http://arxiv.org/abs/2012.01914" target="_blank">arXiv:2012.01914</a> [<a href="http://arxiv.org/pdf/2012.01914" target="_blank">pdf</a>]

<h2>Origin-Aware Next Destination Recommendation with Personalized Preference Attention. (arXiv:2012.01915v1 [cs.AI])</h2>
<h3>Nicholas Lim, Bryan Hooi, See-Kiong Ng, Xueou Wang, Yong Liang Goh, Renrong Weng, Rui Tan</h3>
<p>Next destination recommendation is an important task in the transportation
domain of taxi and ride-hailing services, where users are recommended with
personalized destinations given their current origin location. However, recent
recommendation works do not satisfy this origin-awareness property, and only
consider learning from historical destination locations, without origin
information. Thus, the resulting approaches are unable to learn and predict
origin-aware recommendations based on the user's current location, leading to
sub-optimal performance and poor real-world practicality. Hence, in this work,
we study the origin-aware next destination recommendation task. We propose the
Spatial-Temporal Origin-Destination Personalized Preference Attention
(STOD-PPA) encoder-decoder model to learn origin-origin (OO),
destination-destination (DD), and origin-destination (OD) relationships by
first encoding both origin and destination sequences with spatial and temporal
factors in local and global views, then decoding them through personalized
preference attention to predict the next destination. Experimental results on
seven real-world user trajectory taxi datasets show that our model
significantly outperforms baseline and state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2012.01915" target="_blank">arXiv:2012.01915</a> [<a href="http://arxiv.org/pdf/2012.01915" target="_blank">pdf</a>]

<h2>Mapping Patterns for Virtual Knowledge Graphs. (arXiv:2012.01917v1 [cs.AI])</h2>
<h3>Diego Calvanese, Avigdor Gal, Davide Lanti, Marco Montali, Alessandro Mosca, Roee Shraga</h3>
<p>Virtual Knowledge Graphs (VKG) constitute one of the most promising paradigms
for integrating and accessing legacy data sources. A critical bottleneck in the
integration process involves the definition, validation, and maintenance of
mappings that link data sources to a domain ontology. To support the management
of mappings throughout their entire lifecycle, we propose a comprehensive
catalog of sophisticated mapping patterns that emerge when linking databases to
ontologies. To do so, we build on well-established methodologies and patterns
studied in data management, data analysis, and conceptual modeling. These are
extended and refined through the analysis of concrete VKG benchmarks and
real-world use cases, and considering the inherent impedance mismatch between
data sources and ontologies. We validate our catalog on the considered VKG
scenarios, showing that it covers the vast majority of patterns present
therein.
</p>
<a href="http://arxiv.org/abs/2012.01917" target="_blank">arXiv:2012.01917</a> [<a href="http://arxiv.org/pdf/2012.01917" target="_blank">pdf</a>]

<h2>Multi-mode Core Tensor Factorization based Low-Rankness and Its Applications to Tensor Completion. (arXiv:2012.01918v1 [cs.CV])</h2>
<h3>Haijin Zeng, Sheng Liu, Jifeng Ning</h3>
<p>Low-rank tensor completion has been widely used in computer vision and
machine learning. This paper develops a tensor low-rank decomposition method
together with a tensor low-rankness measure (MCTF) and a better nonconvex
relaxation form of it (NonMCTF). This is the first method that can accurately
restore the clean data of intrinsic low-rank structure based on few known
inputs. This metric encodes low-rank insights for general tensors provided by
Tucker and T-SVD. Furthermore, we studied the MCTF and NonMCTF regularization
minimization problem, and designed an effective BSUM algorithm to solve the
problem. This efficient solver can extend MCTF to various tasks, such as tensor
completion and tensor robust principal component analysis. A series of
experiments, including hyperspectral image (HSI) denoising, video completion
and MRI restoration, confirmed the superior performance of the proposed method
</p>
<a href="http://arxiv.org/abs/2012.01918" target="_blank">arXiv:2012.01918</a> [<a href="http://arxiv.org/pdf/2012.01918" target="_blank">pdf</a>]

<h2>IV-Posterior: Inverse Value Estimation for Interpretable Policy Certificates. (arXiv:2012.01925v1 [cs.LG])</h2>
<h3>Tatiana Lopez-Guevara, Michael Burke, Nicholas K. Taylor, Kartic Subr</h3>
<p>Model-free reinforcement learning (RL) is a powerful tool to learn a broad
range of robot skills and policies. However, a lack of policy interpretability
can inhibit their successful deployment in downstream applications,
particularly when differences in environmental conditions may result in
unpredictable behaviour or generalisation failures. As a result, there has been
a growing emphasis in machine learning around the inclusion of stronger
inductive biases in models to improve generalisation. This paper proposes an
alternative strategy, inverse value estimation for interpretable policy
certificates (IV-Posterior), which seeks to identify the inductive biases or
idealised conditions of operation already held by pre-trained policies, and
then use this information to guide their deployment. IV-Posterior uses
MaskedAutoregressive Flows to fit distributions over the set of conditions or
environmental parameters in which a policy is likely to be effective. This
distribution can then be used as a policy certificate in downstream
applications. We illustrate the use of IV-Posterior across a two environments,
and show that substantial performance gains can be obtained when policy
selection incorporates knowledge of the inductive biases that these policies
hold.
</p>
<a href="http://arxiv.org/abs/2012.01925" target="_blank">arXiv:2012.01925</a> [<a href="http://arxiv.org/pdf/2012.01925" target="_blank">pdf</a>]

<h2>A Stochastic Path-Integrated Differential EstimatoR Expectation Maximization Algorithm. (arXiv:2012.01929v1 [cs.LG])</h2>
<h3>Gersende Fort (IMT), Eric Moulines (X-DEP-MATHAPP), Hoi-To Wai</h3>
<p>The Expectation Maximization (EM) algorithm is of key importance for
inference in latent variable models including mixture of regressors and
experts, missing observations. This paper introduces a novel EM algorithm,
called \texttt{SPIDER-EM}, for inference from a training set of size $n$, $n
\gg 1$. At the core of our algorithm is an estimator of the full conditional
expectation in the {\sf E}-step, adapted from the stochastic path-integrated
differential estimator ({\tt SPIDER}) technique. We derive finite-time
complexity bounds for smooth non-convex likelihood: we show that for
convergence to an $\epsilon$-approximate stationary point, the complexity
scales as $K_{\operatorname{Opt}} (n,\epsilon )={\cal O}(\epsilon^{-1})$ and
$K_{\operatorname{CE}}( n,\epsilon ) = n+ \sqrt{n} {\cal O}(\epsilon^{-1} )$,
where $K_{\operatorname{Opt}}( n,\epsilon )$ and $K_{\operatorname{CE}}(n,
\epsilon )$ are respectively the number of {\sf M}-steps and the number of
per-sample conditional expectations evaluations. This improves over the
state-of-the-art algorithms. Numerical results support our findings.
</p>
<a href="http://arxiv.org/abs/2012.01929" target="_blank">arXiv:2012.01929</a> [<a href="http://arxiv.org/pdf/2012.01929" target="_blank">pdf</a>]

<h2>Learning Explainable Interventions to Mitigate HIV Transmission in Sex Workers Across Five States in India. (arXiv:2012.01930v1 [cs.LG])</h2>
<h3>Raghav Awasthi, Prachi Patel, Vineet Joshi, Shama Karkal, Tavpritesh Sethi</h3>
<p>Female sex workers(FSWs) are one of the most vulnerable and stigmatized
groups in society. As a result, they often suffer from a lack of quality access
to care. Grassroot organizations engaged in improving health services are often
faced with the challenge of improving the effectiveness of interventions due to
complex influences. This work combines structure learning, discriminative
modeling, and grass-root level expertise of designing interventions across five
different Indian states to discover the influence of non-obvious factors for
improving safe-sex practices in FSWs. A bootstrapped, ensemble-averaged
Bayesian Network structure was learned to quantify the factors that could
maximize condom usage as revealed from the model. A discriminative model was
then constructed using XgBoost and random forest in order to predict condom use
behavior The best model achieved 83% sensitivity, 99% specificity, and 99% area
under the precision-recall curve for the prediction. Both generative and
discriminative modeling approaches revealed that financial literacy training
was the primary influence and predictor of condom use in FSWs. These insights
have led to a currently ongoing field trial for assessing the real-world
utility of this approach. Our work highlights the potential of explainable
models for transparent discovery and prioritization of anti-HIV interventions
in female sex workers in a resource-limited setting.
</p>
<a href="http://arxiv.org/abs/2012.01930" target="_blank">arXiv:2012.01930</a> [<a href="http://arxiv.org/pdf/2012.01930" target="_blank">pdf</a>]

<h2>Effect of backdoor attacks over the complexity of the latent space distribution. (arXiv:2012.01931v1 [cs.LG])</h2>
<h3>Henry D. Chacon, Paul Rad</h3>
<p>The input space complexity determines the model's capabilities to extract
their knowledge and translate the space of attributes into a function which is
assumed in general, as a concatenation of non-linear functions between layers.
In the presence of backdoor attacks, the space complexity changes, and induces
similarities between classes that directly affect the model's training. As a
consequence, the model tends to overfit the input set. In this research, we
suggest the D-vine Copula Auto-Encoder (VCAE) as a tool to estimate the latent
space distribution under the presence of backdoor triggers. Since no
assumptions are made on the distribution estimation, like in Variational
Autoencoders (VAE). It is possible to observe the backdoor stamp in
non-attacked categories randomly generated. We exhibit the differences between
a clean model (baseline) and the attacked one (backdoor) in a pairwise
representation of the distribution. The idea is to illustrate the dependency
structure change in the input space induced by backdoor features. Finally, we
quantify the entropy's changes and the Kullback-Leibler divergence between
models. In our results, we found the entropy in the latent space increases by
around 27\% due to the backdoor trigger added to the input
</p>
<a href="http://arxiv.org/abs/2012.01931" target="_blank">arXiv:2012.01931</a> [<a href="http://arxiv.org/pdf/2012.01931" target="_blank">pdf</a>]

<h2>Teaching the Machine to Explain Itself using Domain Knowledge. (arXiv:2012.01932v1 [cs.LG])</h2>
<h3>Vladimir Balayan, Pedro Saleiro, Catarina Bel&#xe9;m, Ludwig Krippahl, Pedro Bizarro</h3>
<p>Machine Learning (ML) has been increasingly used to aid humans to make better
and faster decisions. However, non-technical humans-in-the-loop struggle to
comprehend the rationale behind model predictions, hindering trust in
algorithmic decision-making systems. Considerable research work on AI
explainability attempts to win back trust in AI systems by developing
explanation methods but there is still no major breakthrough. At the same time,
popular explanation methods (e.g., LIME, and SHAP) produce explanations that
are very hard to understand for non-data scientist persona. To address this, we
present JOEL, a neural network-based framework to jointly learn a
decision-making task and associated explanations that convey domain knowledge.
JOEL is tailored to human-in-the-loop domain experts that lack deep technical
ML knowledge, providing high-level insights about the model's predictions that
very much resemble the experts' own reasoning. Moreover, we collect the domain
feedback from a pool of certified experts and use it to ameliorate the model
(human teaching), hence promoting seamless and better suited explanations.
Lastly, we resort to semantic mappings between legacy expert systems and domain
taxonomies to automatically annotate a bootstrap training set, overcoming the
absence of concept-based human annotations. We validate JOEL empirically on a
real-world fraud detection dataset. We show that JOEL can generalize the
explanations from the bootstrap dataset. Furthermore, obtained results indicate
that human teaching can further improve the explanations prediction quality by
approximately $13.57\%$.
</p>
<a href="http://arxiv.org/abs/2012.01932" target="_blank">arXiv:2012.01932</a> [<a href="http://arxiv.org/pdf/2012.01932" target="_blank">pdf</a>]

<h2>Every Corporation Owns Its Structure: Corporate Credit Ratings via Graph Neural Networks. (arXiv:2012.01933v1 [cs.LG])</h2>
<h3>Bojing Feng, Haonan Xu, Wenfang Xue, Bindang Xue</h3>
<p>Credit rating is an analysis of the credit risks associated with a
corporation, which reflects the level of the riskiness and reliability in
investing, and plays a vital role in financial risk. There have emerged many
studies that implement machine learning and deep learning techniques which are
based on vector space to deal with corporate credit rating. Recently,
considering the relations among enterprises such as loan guarantee network,
some graph-based models are applied in this field with the advent of graph
neural networks. But these existing models build networks between corporations
without taking the internal feature interactions into account. In this paper,
to overcome such problems, we propose a novel model, Corporate Credit Rating
via Graph Neural Networks, CCR-GNN for brevity. We firstly construct individual
graphs for each corporation based on self-outer product and then use GNN to
model the feature interaction explicitly, which includes both local and global
information. Extensive experiments conducted on the Chinese public-listed
corporate rating dataset, prove that CCR-GNN outperforms the state-of-the-art
methods consistently.
</p>
<a href="http://arxiv.org/abs/2012.01933" target="_blank">arXiv:2012.01933</a> [<a href="http://arxiv.org/pdf/2012.01933" target="_blank">pdf</a>]

<h2>Adaptable Automation with Modular Deep Reinforcement Learning and Policy Transfer. (arXiv:2012.01934v1 [cs.LG])</h2>
<h3>Zohreh Raziei, Mohsen Moghaddam</h3>
<p>Recent advances in deep Reinforcement Learning (RL) have created
unprecedented opportunities for intelligent automation, where a machine can
autonomously learn an optimal policy for performing a given task. However,
current deep RL algorithms predominantly specialize in a narrow range of tasks,
are sample inefficient, and lack sufficient stability, which in turn hinder
their industrial adoption. This article tackles this limitation by developing
and testing a Hyper-Actor Soft Actor-Critic (HASAC) RL framework based on the
notions of task modularization and transfer learning. The goal of the proposed
HASAC is to enhance the adaptability of an agent to new tasks by transferring
the learned policies of former tasks to the new task via a "hyper-actor". The
HASAC framework is tested on a new virtual robotic manipulation benchmark,
Meta-World. Numerical experiments show superior performance by HASAC over
state-of-the-art deep RL algorithms in terms of reward value, success rate, and
task completion time.
</p>
<a href="http://arxiv.org/abs/2012.01934" target="_blank">arXiv:2012.01934</a> [<a href="http://arxiv.org/pdf/2012.01934" target="_blank">pdf</a>]

<h2>Backpropagation-Free Learning Method for Correlated Fuzzy Neural Networks. (arXiv:2012.01935v1 [cs.LG])</h2>
<h3>Armin Salimi-Badr, Mohammad Mehdi Ebadzadeh</h3>
<p>In this paper, a novel stepwise learning approach based on estimating desired
premise parts' outputs by solving a constrained optimization problem is
proposed. This learning approach does not require backpropagating the output
error to learn the premise parts' parameters. Instead, the near best output
values of the rules premise parts are estimated and their parameters are
changed to reduce the error between current premise parts' outputs and the
estimated desired ones. Therefore, the proposed learning method avoids error
backpropagation, which lead to vanishing gradient and consequently getting
stuck in a local optimum. The proposed method does not need any initialization
method. This learning method is utilized to train a new Takagi-Sugeno-Kang
(TSK) Fuzzy Neural Network with correlated fuzzy rules including many
parameters in both premise and consequent parts, avoiding getting stuck in a
local optimum due to vanishing gradient. To learn the proposed network
parameters, first, a constrained optimization problem is introduced and solved
to estimate the desired values of premise parts' output values. Next, the error
between these values and the current ones is utilized to adapt the premise
parts' parameters based on the gradient-descent (GD) approach. Afterward, the
error between the desired and network's outputs is used to learn consequent
parts' parameters by the GD method. The proposed paradigm is successfully
applied to real-world time-series prediction and regression problems. According
to experimental results, its performance outperforms other methods with a more
parsimonious structure.
</p>
<a href="http://arxiv.org/abs/2012.01935" target="_blank">arXiv:2012.01935</a> [<a href="http://arxiv.org/pdf/2012.01935" target="_blank">pdf</a>]

<h2>Quasi-Newton's method in the class gradient defined high-curvature subspace. (arXiv:2012.01938v1 [cs.LG])</h2>
<h3>Mark Tuddenham, Adam Pr&#xfc;gel-Bennett, Jonathan Hare</h3>
<p>Classification problems using deep learning have been shown to have a
high-curvature subspace in the loss landscape equal in dimension to the number
of classes. Moreover, this subspace corresponds to the subspace spanned by the
logit gradients for each class. An obvious strategy to speed up optimisation
would be to use Newton's method in the high-curvature subspace and stochastic
gradient descent in the co-space. We show that a naive implementation actually
slows down convergence and we speculate why this might be.
</p>
<a href="http://arxiv.org/abs/2012.01938" target="_blank">arXiv:2012.01938</a> [<a href="http://arxiv.org/pdf/2012.01938" target="_blank">pdf</a>]

<h2>Locally Linear Attributes of ReLU Neural Networks. (arXiv:2012.01940v1 [cs.LG])</h2>
<h3>Ben Sattelberg, Renzo Cavalieri, Michael Kirby, Chris Peterson, Ross Beveridge</h3>
<p>A ReLU neural network determines/is a continuous piecewise linear map from an
input space to an output space. The weights in the neural network determine a
decomposition of the input space into convex polytopes and on each of these
polytopes the network can be described by a single affine mapping. The
structure of the decomposition, together with the affine map attached to each
polytope, can be analyzed to investigate the behavior of the associated neural
network.
</p>
<a href="http://arxiv.org/abs/2012.01940" target="_blank">arXiv:2012.01940</a> [<a href="http://arxiv.org/pdf/2012.01940" target="_blank">pdf</a>]

<h2>Multi-Label Contrastive Learning for Abstract Visual Reasoning. (arXiv:2012.01944v1 [cs.AI])</h2>
<h3>Miko&#x142;aj Ma&#x142;ki&#x144;ski, Jacek Ma&#x144;dziuk</h3>
<p>For a long time the ability to solve abstract reasoning tasks was considered
one of the hallmarks of human intelligence. Recent advances in application of
deep learning (DL) methods led, as in many other domains, to surpassing human
abstract reasoning performance, specifically in the most popular type of such
problems - the Raven's Progressive Matrices (RPMs). While the efficacy of DL
systems is indeed impressive, the way they approach the RPMs is very different
from that of humans. State-of-the-art systems solving RPMs rely on massive
pattern-based training and sometimes on exploiting biases in the dataset,
whereas humans concentrate on identification of the rules / concepts underlying
the RPM (or generally a visual reasoning task) to be solved. Motivated by this
cognitive difference, this work aims at combining DL with human way of solving
RPMs and getting the best of both worlds. Specifically, we cast the problem of
solving RPMs into multi-label classification framework where each RPM is viewed
as a multi-label data point, with labels determined by the set of abstract
rules underlying the RPM. For efficient training of the system we introduce a
generalisation of the Noise Contrastive Estimation algorithm to the case of
multi-label samples. Furthermore, we propose a new sparse rule encoding scheme
for RPMs which, besides the new training algorithm, is the key factor
contributing to the state-of-the-art performance. The proposed approach is
evaluated on two most popular benchmark datasets (Balanced-RAVEN and PGM) and
on both of them demonstrates an advantage over the current state-of-the-art
results. Contrary to applications of contrastive learning methods reported in
other domains, the state-of-the-art performance reported in the paper is
achieved with no need for large batch sizes or strong data augmentation.
</p>
<a href="http://arxiv.org/abs/2012.01944" target="_blank">arXiv:2012.01944</a> [<a href="http://arxiv.org/pdf/2012.01944" target="_blank">pdf</a>]

<h2>Co-mining: Self-Supervised Learning for Sparsely Annotated Object Detection. (arXiv:2012.01950v1 [cs.CV])</h2>
<h3>Tiancai Wang, Tong Yang, Jiale Cao, Xiangyu Zhang</h3>
<p>Object detectors usually achieve promising results with the supervision of
complete instance annotations. However, their performance is far from
satisfactory with sparse instance annotations. Most existing methods for
sparsely annotated object detection either re-weight the loss of hard negative
samples or convert the unlabeled instances into ignored regions to reduce the
interference of false negatives. We argue that these strategies are
insufficient since they can at most alleviate the negative effect caused by
missing annotations. In this paper, we propose a simple but effective
mechanism, called Co-mining, for sparsely annotated object detection. In our
Co-mining, two branches of a Siamese network predict the pseudo-label sets for
each other. To enhance multi-view learning and better mine unlabeled instances,
the original image and corresponding augmented image are used as the inputs of
two branches of the Siamese network, respectively. Co-mining can serve as a
general training mechanism applied to most of modern object detectors.
Experiments are performed on MS COCO dataset with three different sparsely
annotated settings using two typical frameworks: anchor-based detector
RetinaNet and anchor-free detector FCOS. Experimental results show that our
Co-mining with RetinaNet achieves 1.4%~2.1% improvements compared with
different baselines and surpasses existing methods under the same sparsely
annotated setting.
</p>
<a href="http://arxiv.org/abs/2012.01950" target="_blank">arXiv:2012.01950</a> [<a href="http://arxiv.org/pdf/2012.01950" target="_blank">pdf</a>]

<h2>IMAGO: A family photo album dataset for a socio-historical analysis of the twentieth century. (arXiv:2012.01955v1 [cs.CV])</h2>
<h3>Lorenzo Stacchio, Alessia Angeli, Giuseppe Lisanti, Daniela Calanca, Gustavo Marfia</h3>
<p>Although one of the most popular practices in photography since the end of
the 19th century, an increase in scholarly interest in family photo albums
dates back to the early 1980s. Such collections of photos may reveal
sociological and historical insights regarding specific cultures and times.
They are, however, in most cases scattered among private homes and only
available on paper or photographic film, thus making their analysis by
academics such as historians, social-cultural anthropologists and cultural
theorists very cumbersome. In this paper, we analyze the IMAGO dataset
including photos belonging to family albums assembled at the University of
Bologna's Rimini campus since 2004. Following a deep learning-based approach,
the IMAGO dataset has offered the opportunity of experimenting with photos
taken between year 1845 and year 2009, with the goals of assessing the dates
and the socio-historical contexts of the images, without use of any other
sources of information. Exceeding our initial expectations, such analysis has
revealed its merit not only in terms of the performance of the approach adopted
in this work, but also in terms of the foreseeable implications and use for the
benefit of socio-historical research. To the best of our knowledge, this is the
first work that moves along this path in literature.
</p>
<a href="http://arxiv.org/abs/2012.01955" target="_blank">arXiv:2012.01955</a> [<a href="http://arxiv.org/pdf/2012.01955" target="_blank">pdf</a>]

<h2>Human Tactile Gesture Interpretation for Robotic Systems. (arXiv:2012.01959v1 [cs.RO])</h2>
<h3>Elizabeth Bibit Bianchini, Kenneth Salisbury, Prateek Verma</h3>
<p>Human-robot interactions are less efficient and communicative than
human-to-human interactions, and a key reason is a lack of informed sense of
touch in robotic systems. Existing literature demonstrates robot success in
executing handovers with humans, albeit with substantial reliance on external
sensing or with primitive signal processing methods, deficient compared to the
rich set of information humans can detect. In contrast, we present models
capable of distinguishing between four classes of human tactile gestures at a
robot's end effector, using only a non-collocated six-axis force sensor at the
wrist. Due to the absence in the literature, this work describes 1) the
collection of an extensive force dataset characterized by human-robot contact
events, and 2) classification models informed by this dataset to determine the
nature of the interaction. We demonstrate high classification accuracies among
our proposed gesture definitions on a test set, emphasizing that neural network
classifiers on the raw data outperform several other combinations of algorithms
and feature sets.
</p>
<a href="http://arxiv.org/abs/2012.01959" target="_blank">arXiv:2012.01959</a> [<a href="http://arxiv.org/pdf/2012.01959" target="_blank">pdf</a>]

<h2>A Systematic Literature Review on Federated Learning: From A Model Quality Perspective. (arXiv:2012.01973v1 [cs.LG])</h2>
<h3>Yi Liu, Li Zhang, Ning Ge, Guanghao Li</h3>
<p>As an emerging technique, Federated Learning (FL) can jointly train a global
model with the data remaining locally, which effectively solves the problem of
data privacy protection through the encryption mechanism. The clients train
their local model, and the server aggregates models until convergence. In this
process, the server uses an incentive mechanism to encourage clients to
contribute high-quality and large-volume data to improve the global model.
Although some works have applied FL to the Internet of Things (IoT), medicine,
manufacturing, etc., the application of FL is still in its infancy, and many
related issues need to be solved. Improving the quality of FL models is one of
the current research hotspots and challenging tasks. This paper systematically
reviews and objectively analyzes the approaches to improving the quality of FL
models. We are also interested in the research and application trends of FL and
the effect comparison between FL and non-FL because the practitioners usually
worry that achieving privacy protection needs compromising learning quality. We
use a systematic review method to analyze 147 latest articles related to FL.
This review provides useful information and insights to both academia and
practitioners from the industry. We investigate research questions about
academic research and industrial application trends of FL, essential factors
affecting the quality of FL models, and compare FL and non-FL algorithms in
terms of learning quality. Based on our review's conclusion, we give some
suggestions for improving the FL model quality. Finally, we propose an FL
application framework for practitioners.
</p>
<a href="http://arxiv.org/abs/2012.01973" target="_blank">arXiv:2012.01973</a> [<a href="http://arxiv.org/pdf/2012.01973" target="_blank">pdf</a>]

<h2>Transfer learning to enhance amenorrhea status prediction in cancer and fertility data with missing values. (arXiv:2012.01974v1 [cs.LG])</h2>
<h3>Xuetong Wu, Hadi Akbarzadeh Khorshidi, Uwe Aickelin, Zobaida Edib, Michelle Peate</h3>
<p>Collecting sufficient labelled training data for health and medical problems
is difficult (Antropova, et al., 2018). Also, missing values are unavoidable in
health and medical datasets and tackling the problem arising from the
inadequate instances and missingness is not straightforward (Snell, et al.
2017, Sterne, et al. 2009). However, machine learning algorithms have achieved
significant success in many real-world healthcare problems, such as regression
and classification and these techniques could possibly be a way to resolve the
issues.
</p>
<a href="http://arxiv.org/abs/2012.01974" target="_blank">arXiv:2012.01974</a> [<a href="http://arxiv.org/pdf/2012.01974" target="_blank">pdf</a>]

<h2>A small note on variation in segmentation annotations. (arXiv:2012.01975v1 [cs.CV])</h2>
<h3>Silas Nyboe &#xd8;rting</h3>
<p>We report on the results of a small crowdsourcing experiment conducted at a
workshop on machine learning for segmentation held at the Danish Bio Imaging
network meeting 2020. During the workshop we asked participants to manually
segment mitochondria in three 2D patches. The aim of the experiment was to
illustrate that manual annotations should not be seen as the ground truth, but
as a reference standard that is subject to substantial variation. In this note
we show how the large variation we observed in the segmentations can be reduced
by removing the annotators with worst pair-wise agreement. Having removed the
annotators with worst performance, we illustrate that the remaining variance is
semantically meaningful and can be exploited to obtain segmentations of cell
boundary and cell interior.
</p>
<a href="http://arxiv.org/abs/2012.01975" target="_blank">arXiv:2012.01975</a> [<a href="http://arxiv.org/pdf/2012.01975" target="_blank">pdf</a>]

<h2>Patient similarity: methods and applications. (arXiv:2012.01976v1 [cs.LG])</h2>
<h3>Leyu Dai, He Zhu, Dianbo Liu</h3>
<p>Patient similarity analysis is important in health care applications. It
takes patient information such as their electronic medical records and genetic
data as input and computes the pairwise similarity between patients. Procedures
of typical a patient similarity study can be divided into several steps
including data integration, similarity measurement, and neighborhood
identification. And according to an analysis of patient similarity, doctors can
easily find the most suitable treatments. There are many methods to analyze the
similarity such as cluster analysis. And during machine learning become more
and more popular, Using neural networks such as CNN is a new hot topic. This
review summarizes representative methods used in each step and discusses
applications of patient similarity networks especially in the context of
precision medicine.
</p>
<a href="http://arxiv.org/abs/2012.01976" target="_blank">arXiv:2012.01976</a> [<a href="http://arxiv.org/pdf/2012.01976" target="_blank">pdf</a>]

<h2>Asymptotic convergence rate of Dropout on shallow linear neural networks. (arXiv:2012.01978v1 [cs.LG])</h2>
<h3>Albert Senen-Cerda, Jaron Sanders</h3>
<p>We analyze the convergence rate of gradient flows on objective functions
induced by Dropout and Dropconnect, when applying them to shallow linear Neural
Networks (NNs) - which can also be viewed as doing matrix factorization using a
particular regularizer. Dropout algorithms such as these are thus
regularization techniques that use 0,1-valued random variables to filter
weights during training in order to avoid coadaptation of features. By
leveraging a recent result on nonconvex optimization and conducting a careful
analysis of the set of minimizers as well as the Hessian of the loss function,
we are able to obtain (i) a local convergence proof of the gradient flow and
(ii) a bound on the convergence rate that depends on the data, the dropout
probability, and the width of the NN. Finally, we compare this theoretical
bound to numerical simulations, which are in qualitative agreement with the
convergence bound and match it when starting sufficiently close to a minimizer.
</p>
<a href="http://arxiv.org/abs/2012.01978" target="_blank">arXiv:2012.01978</a> [<a href="http://arxiv.org/pdf/2012.01978" target="_blank">pdf</a>]

<h2>Tensor Data Scattering and the Impossibility of Slicing Theorem. (arXiv:2012.01982v1 [cs.LG])</h2>
<h3>Wuming Pan</h3>
<p>This paper establishes a broad theoretical framework for tensor data
dissemination methods used in various deep learning frameworks. This paper
gives a theorem that is very important for performance analysis and accelerator
optimization for implementing data scattering. The theorem shows how the
impossibility of slicing happens in tenser data scattering. This paper proposes
an algorithm called ScatterX and its source code is provided.
</p>
<a href="http://arxiv.org/abs/2012.01982" target="_blank">arXiv:2012.01982</a> [<a href="http://arxiv.org/pdf/2012.01982" target="_blank">pdf</a>]

<h2>Multiple Networks are More Efficient than One: Fast and Accurate Models via Ensembles and Cascades. (arXiv:2012.01988v1 [cs.CV])</h2>
<h3>Xiaofang Wang, Dan Kondratyuk, Kris M. Kitani, Yair Movshovitz-Attias, Elad Eban</h3>
<p>Recent work on efficient neural network architectures focuses on discovering
a solitary network that can achieve superior computational efficiency and
accuracy. While this paradigm has yielded impressive results, the search for
novel architectures usually requires significant computational resources. In
this work, we demonstrate a simple complementary paradigm to obtain efficient
and accurate models that requires no architectural tuning. We show that
committee-based models, i.e., ensembles or cascades of models, can easily
obtain higher accuracy with less computation when compared to a single model.
We extensively investigate the benefits of committee-based models on various
vision tasks and architecture families. Our results suggest that in the large
computation regime, model ensembles are a more cost-effective way to improve
accuracy than using a large solitary model. We also find that the computational
cost of an ensemble can be significantly reduced by converting them to
cascades, while often retaining the original accuracy of the full ensemble.
</p>
<a href="http://arxiv.org/abs/2012.01988" target="_blank">arXiv:2012.01988</a> [<a href="http://arxiv.org/pdf/2012.01988" target="_blank">pdf</a>]

<h2>Dynamic RAN Slicing for Service-Oriented Vehicular Networks via Constrained Learning. (arXiv:2012.01991v1 [cs.LG])</h2>
<h3>Wen Wu, Nan Chen, Conghao Zhou, Mushu Li, Xuemin Shen, Weihua Zhuang, Xu Li</h3>
<p>In this paper, we investigate a radio access network (RAN) slicing problem
for Internet of vehicles (IoV) services with different quality of service (QoS)
requirements, in which multiple logically-isolated slices are constructed on a
common roadside network infrastructure. A dynamic RAN slicing framework is
presented to dynamically allocate radio spectrum and computing resource, and
distribute computation workloads for the slices. To obtain an optimal RAN
slicing policy for accommodating the spatial-temporal dynamics of vehicle
traffic density, we first formulate a constrained RAN slicing problem with the
objective to minimize long-term system cost. This problem cannot be directly
solved by traditional reinforcement learning (RL) algorithms due to complicated
coupled constraints among decisions. Therefore, we decouple the problem into a
resource allocation subproblem and a workload distribution subproblem, and
propose a two-layer constrained RL algorithm, named Resource Allocation and
Workload diStribution (RAWS) to solve them. Specifically, an outer layer first
makes the resource allocation decision via an RL algorithm, and then an inner
layer makes the workload distribution decision via an optimization subroutine.
Extensive trace-driven simulations show that the RAWS effectively reduces the
system cost while satisfying QoS requirements with a high probability, as
compared with benchmarks.
</p>
<a href="http://arxiv.org/abs/2012.01991" target="_blank">arXiv:2012.01991</a> [<a href="http://arxiv.org/pdf/2012.01991" target="_blank">pdf</a>]

<h2>Radar Artifact Labeling Framework (RALF): Method for Plausible Radar Detections in Datasets. (arXiv:2012.01993v1 [cs.RO])</h2>
<h3>Simon T. Isele, Marcel P. Schilling, Fabian E. Klein, Sascha Saralajew, J. Marius Zoellner</h3>
<p>Research on localization and perception for Autonomous Driving is mainly
focused on camera and LiDAR datasets, rarely on radar data. Manually labeling
sparse radar point clouds is challenging. For a dataset generation, we propose
the cross sensor Radar Artifact Labeling Framework (RALF). Automatically
generated labels for automotive radar data help to cure radar shortcomings like
artifacts for the application of artificial intelligence. RALF provides
plausibility labels for radar raw detections, distinguishing between artifacts
and targets. The optical evaluation backbone consists of a generalized
monocular depth image estimation of surround view cameras plus LiDAR scans.
Modern car sensor sets of cameras and LiDAR allow to calibrate image-based
relative depth information in overlapping sensing areas. K-Nearest Neighbors
matching relates the optical perception point cloud with raw radar detections.
In parallel, a temporal tracking evaluation part considers the radar
detections' transient behavior. Based on the distance between matches,
respecting both sensor and model uncertainties, we propose a plausibility
rating of every radar detection. We validate the results by evaluating error
metrics on semi-manually labeled ground truth dataset of $3.28\cdot10^6$
points. Besides generating plausible radar detections, the framework enables
further labeled low-level radar signal datasets for applications of perception
and Autonomous Driving learning tasks.
</p>
<a href="http://arxiv.org/abs/2012.01993" target="_blank">arXiv:2012.01993</a> [<a href="http://arxiv.org/pdf/2012.01993" target="_blank">pdf</a>]

<h2>A Novel index-based multidimensional data organization model that enhances the predictability of the machine learning algorithms. (arXiv:2012.02007v1 [cs.LG])</h2>
<h3>Mahbubur Rahman</h3>
<p>Learning from the multidimensional data has been an interesting concept in
the field of machine learning. However, such learning can be difficult,
complex, expensive because of expensive data processing, manipulations as the
number of dimension increases. As a result, we have introduced an ordered
index-based data organization model as the ordered data set provides easy and
efficient access than the unordered one and finally, such organization can
improve the learning. The ordering maps the multidimensional dataset in the
reduced space and ensures that the information associated with the learning can
be retrieved back and forth efficiently. We have found that such
multidimensional data storage can enhance the predictability for both the
unsupervised and supervised machine learning algorithms.
</p>
<a href="http://arxiv.org/abs/2012.02007" target="_blank">arXiv:2012.02007</a> [<a href="http://arxiv.org/pdf/2012.02007" target="_blank">pdf</a>]

<h2>Aerial Imagery Pixel-level Segmentation. (arXiv:2012.02024v1 [cs.CV])</h2>
<h3>Michael R. Heffels, Joaquin Vanschoren</h3>
<p>Aerial imagery can be used for important work on a global scale.
Nevertheless, the analysis of this data using neural network architectures lags
behind the current state-of-the-art on popular datasets such as PASCAL VOC,
CityScapes and Camvid. In this paper we bridge the performance-gap between
these popular datasets and aerial imagery data. Little work is done on aerial
imagery with state-of-the-art neural network architectures in a multi-class
setting. Our experiments concerning data augmentation, normalisation, image
size and loss functions give insight into a high performance setup for aerial
imagery segmentation datasets. Our work, using the state-of-the-art DeepLabv3+
Xception65 architecture, achieves a mean IOU of 70% on the DroneDeploy
validation set. With this result, we clearly outperform the current publicly
available state-of-the-art validation set mIOU (65%) performance with 5%.
Furthermore, to our knowledge, there is no mIOU benchmark for the test set.
Hence, we also propose a new benchmark on the DroneDeploy test set using the
best performing DeepLabv3+ Xception65 architecture, with a mIOU score of 52.5%.
</p>
<a href="http://arxiv.org/abs/2012.02024" target="_blank">arXiv:2012.02024</a> [<a href="http://arxiv.org/pdf/2012.02024" target="_blank">pdf</a>]

<h2>Towards an AI assistant for human grid operators. (arXiv:2012.02026v1 [stat.ML])</h2>
<h3>Antoine Marot, Alexandre Rozier, Matthieu Dussartre, Laure Crochepierre, Benjamin Donnot</h3>
<p>Power systems are becoming more complex to operate in the digital age. As a
result, real-time decision-making is getting more challenging as the human
operator has to deal with more information, more uncertainty, more applications
and more coordination. While supervision has been primarily used to help them
make decisions over the last decades, it cannot reasonably scale up anymore.
There is a great need for rethinking the human-machine interface under more
unified and interactive frameworks. Taking advantage of the latest developments
in Human-machine Interactions and Artificial intelligence, we share the vision
of a new assistant framework relying on an hypervision interface and greater
bidirectional interactions. We review the known principles of decision-making
that drives the assistant design and supporting assistance functions we
present. We finally share some guidelines to make progress towards the
development of such an assistant.
</p>
<a href="http://arxiv.org/abs/2012.02026" target="_blank">arXiv:2012.02026</a> [<a href="http://arxiv.org/pdf/2012.02026" target="_blank">pdf</a>]

<h2>SuperOCR: A Conversion from Optical Character Recognition to Image Captioning. (arXiv:2012.02033v1 [cs.CV])</h2>
<h3>Baohua Sun, Michael Lin, Hao Sha, Lin Yang</h3>
<p>Optical Character Recognition (OCR) has many real world applications. The
existing methods normally detect where the characters are, and then recognize
the character for each detected location. Thus the accuracy of characters
recognition is impacted by the performance of characters detection. In this
paper, we propose a method for recognizing characters without detecting the
location of each character. This is done by converting the OCR task into an
image captioning task. One advantage of the proposed method is that the labeled
bounding boxes for the characters are not needed during training. The
experimental results show the proposed method outperforms the existing methods
on both the license plate recognition and the watermeter character recognition
tasks. The proposed method is also deployed into a low-power (300mW) CNN
accelerator chip connected to a Raspberry Pi 3 for on-device applications.
</p>
<a href="http://arxiv.org/abs/2012.02033" target="_blank">arXiv:2012.02033</a> [<a href="http://arxiv.org/pdf/2012.02033" target="_blank">pdf</a>]

<h2>Integrable Nonparametric Flows. (arXiv:2012.02035v1 [stat.ML])</h2>
<h3>David Pfau, Danilo Rezende</h3>
<p>We introduce a method for reconstructing an infinitesimal normalizing flow
given only an infinitesimal change to a (possibly unnormalized) probability
distribution. This reverses the conventional task of normalizing flows --
rather than being given samples from a unknown target distribution and learning
a flow that approximates the distribution, we are given a perturbation to an
initial distribution and aim to reconstruct a flow that would generate samples
from the known perturbed distribution. While this is an underdetermined
problem, we find that choosing the flow to be an integrable vector field yields
a solution closely related to electrostatics, and a solution can be computed by
the method of Green's functions. Unlike conventional normalizing flows, this
flow can be represented in an entirely nonparametric manner. We validate this
derivation on low-dimensional problems, and discuss potential applications to
problems in quantum Monte Carlo and machine learning.
</p>
<a href="http://arxiv.org/abs/2012.02035" target="_blank">arXiv:2012.02035</a> [<a href="http://arxiv.org/pdf/2012.02035" target="_blank">pdf</a>]

<h2>Recovering Trajectories of Unmarked Joints in 3D Human Actions Using Latent Space Optimization. (arXiv:2012.02043v1 [cs.CV])</h2>
<h3>Suhas Lohit, Rushil Anirudh, Pavan Turaga</h3>
<p>Motion capture (mocap) and time-of-flight based sensing of human actions are
becoming increasingly popular modalities to perform robust activity analysis.
Applications range from action recognition to quantifying movement quality for
health applications. While marker-less motion capture has made great progress,
in critical applications such as healthcare, marker-based systems, especially
active markers, are still considered gold-standard. However, there are several
practical challenges in both modalities such as visibility, tracking errors,
and simply the need to keep marker setup convenient wherein movements are
recorded with a reduced marker-set. This implies that certain joint locations
will not even be marked-up, making downstream analysis of full body movement
challenging. To address this gap, we first pose the problem of reconstructing
the unmarked joint data as an ill-posed linear inverse problem. We recover
missing joints for a given action by projecting it onto the manifold of human
actions, this is achieved by optimizing the latent space representation of a
deep autoencoder. Experiments on both mocap and Kinect datasets clearly
demonstrate that the proposed method performs very well in recovering semantics
of the actions and dynamics of missing joints. We will release all the code and
models publicly.
</p>
<a href="http://arxiv.org/abs/2012.02043" target="_blank">arXiv:2012.02043</a> [<a href="http://arxiv.org/pdf/2012.02043" target="_blank">pdf</a>]

<h2>Blockchain Assisted Decentralized Federated Learning (BLADE-FL) with Lazy Clients. (arXiv:2012.02044v1 [cs.LG])</h2>
<h3>Jun Li, Yumeng Shao, Ming Ding, Chuan Ma, Kang Wei, Zhu Han, H. Vincent Poor</h3>
<p>Federated learning (FL), as a distributed machine learning approach, has
drawn a great amount of attention in recent years. FL shows an inherent
advantage in privacy preservation, since users' raw data are processed locally.
However, it relies on a centralized server to perform model aggregation.
Therefore, FL is vulnerable to server malfunctions and external attacks. In
this paper, we propose a novel framework by integrating blockchain into FL,
namely, blockchain assisted decentralized federated learning (BLADE-FL), to
enhance the security of FL. The proposed BLADE-FL has a good performance in
terms of privacy preservation, tamper resistance, and effective cooperation of
learning. However, it gives rise to a new problem of training deficiency,
caused by lazy clients who plagiarize others' trained models and add artificial
noises to conceal their cheating behaviors. To be specific, we first develop a
convergence bound of the loss function with the presence of lazy clients and
prove that it is convex with respect to the total number of generated blocks
$K$. Then, we solve the convex problem by optimizing $K$ to minimize the loss
function. Furthermore, we discover the relationship between the optimal $K$,
the number of lazy clients, and the power of artificial noises used by lazy
clients. We conduct extensive experiments to evaluate the performance of the
proposed framework using the MNIST and Fashion-MNIST datasets. Our analytical
results are shown to be consistent with the experimental results. In addition,
the derived optimal $K$ achieves the minimum value of loss function, and in
turn the optimal accuracy performance.
</p>
<a href="http://arxiv.org/abs/2012.02044" target="_blank">arXiv:2012.02044</a> [<a href="http://arxiv.org/pdf/2012.02044" target="_blank">pdf</a>]

<h2>Neural Prototype Trees for Interpretable Fine-grained Image Recognition. (arXiv:2012.02046v1 [cs.CV])</h2>
<h3>Meike Nauta, Ron van Bree, Christin Seifert</h3>
<p>Interpretable machine learning addresses the black-box nature of deep neural
networks. Visual prototypes have been suggested for intrinsically interpretable
image recognition, instead of generating post-hoc explanations that approximate
a trained model. However, a large number of prototypes can be overwhelming. To
reduce explanation size and improve interpretability, we propose the Neural
Prototype Tree (ProtoTree), a deep learning method that includes prototypes in
an interpretable decision tree to faithfully visualize the entire model. In
addition to global interpretability, a path in the tree explains a single
prediction. Each node in our binary tree contains a trainable prototypical
part. The presence or absence of this prototype in an image determines the
routing through a node. Decision making is therefore similar to human
reasoning: Does the bird have a red throat? And an elongated beak? Then it's a
hummingbird! We tune the accuracy-interpretability trade-off using ensembling
and pruning. We apply pruning without sacrificing accuracy, resulting in a
small tree with only 8 prototypes along a path to classify a bird from 200
species. An ensemble of 5 ProtoTrees achieves competitive accuracy on the
CUB-200-2011 and Stanford Cars data sets. Code is available at
https://github.com/M-Nauta/ProtoTree
</p>
<a href="http://arxiv.org/abs/2012.02046" target="_blank">arXiv:2012.02046</a> [<a href="http://arxiv.org/pdf/2012.02046" target="_blank">pdf</a>]

<h2>Full-Resolution Correspondence Learning for Image Translation. (arXiv:2012.02047v1 [cs.CV])</h2>
<h3>Xingran Zhou, Bo Zhang, Ting Zhang, Pan Zhang, Jianmin Bao, Dong Chen, Zhongfei Zhang, Fang Wen</h3>
<p>We present the full-resolution correspondence learning for cross-domain
images, which aids image translation. We adopt a hierarchical strategy that
uses the correspondence from coarse level to guide the finer levels. In each
hierarchy, the correspondence can be efficiently computed via PatchMatch that
iteratively leverages the matchings from the neighborhood. Within each
PatchMatch iteration, the ConvGRU module is employed to refine the current
correspondence considering not only the matchings of larger context but also
the historic estimates. The proposed GRU-assisted PatchMatch is fully
differentiable and highly efficient. When jointly trained with image
translation, full-resolution semantic correspondence can be established in an
unsupervised manner, which in turn facilitates the exemplar-based image
translation. Experiments on diverse translation tasks show our approach
performs considerably better than state-of-the-arts on producing
high-resolution images.
</p>
<a href="http://arxiv.org/abs/2012.02047" target="_blank">arXiv:2012.02047</a> [<a href="http://arxiv.org/pdf/2012.02047" target="_blank">pdf</a>]

<h2>Intervention Design for Effective Sim2Real Transfer. (arXiv:2012.02055v1 [cs.RO])</h2>
<h3>Melissa Mozifian, Amy Zhang, Joelle Pineau, David Meger</h3>
<p>The goal of this work is to address the recent success of domain
randomization and data augmentation for the sim2real setting. We explain this
success through the lens of causal inference, positioning domain randomization
and data augmentation as interventions on the environment which encourage
invariance to irrelevant features. Such interventions include visual
perturbations that have no effect on reward and dynamics. This encourages the
learning algorithm to be robust to these types of variations and learn to
attend to the true causal mechanisms for solving the task. This connection
leads to two key findings: (1) perturbations to the environment do not have to
be realistic, but merely show variation along dimensions that also vary in the
real world, and (2) use of an explicit invariance-inducing objective improves
generalization in sim2sim and sim2real transfer settings over just data
augmentation or domain randomization alone. We demonstrate the capability of
our method by performing zero-shot transfer of a robot arm reach task on a 7DoF
Jaco arm learning from pixel observations.
</p>
<a href="http://arxiv.org/abs/2012.02055" target="_blank">arXiv:2012.02055</a> [<a href="http://arxiv.org/pdf/2012.02055" target="_blank">pdf</a>]

<h2>A Multi-task Contextual Atrous Residual Network for Brain Tumor Detection & Segmentation. (arXiv:2012.02073v1 [cs.CV])</h2>
<h3>Ngan Le, Kashu Yamazaki, Dat Truong, Kha Gia Quach, Marios Savvides</h3>
<p>In recent years, deep neural networks have achieved state-of-the-art
performance in a variety of recognition and segmentation tasks in medical
imaging including brain tumor segmentation. We investigate that segmenting a
brain tumor is facing to the imbalanced data problem where the number of pixels
belonging to the background class (non tumor pixel) is much larger than the
number of pixels belonging to the foreground class (tumor pixel). To address
this problem, we propose a multi-task network which is formed as a cascaded
structure. Our model consists of two targets, i.e., (i) effectively
differentiate the brain tumor regions and (ii) estimate the brain tumor mask.
The first objective is performed by our proposed contextual brain tumor
detection network, which plays a role of an attention gate and focuses on the
region around brain tumor only while ignoring the far neighbor background which
is less correlated to the tumor. The second objective is built upon a 3D atrous
residual network and under an encode-decode network in order to effectively
segment both large and small objects (brain tumor). Our 3D atrous residual
network is designed with a skip connection to enables the gradient from the
deep layers to be directly propagated to shallow layers, thus, features of
different depths are preserved and used for refining each other. In order to
incorporate larger contextual information from volume MRI data, our network
utilizes the 3D atrous convolution with various kernel sizes, which enlarges
the receptive field of filters. Our proposed network has been evaluated on
various datasets including BRATS2015, BRATS2017 and BRATS2018 datasets with
both validation set and testing set. Our performance has been benchmarked by
both region-based metrics and surface-based metrics. We also have conducted
comparisons against state-of-the-art approaches.
</p>
<a href="http://arxiv.org/abs/2012.02073" target="_blank">arXiv:2012.02073</a> [<a href="http://arxiv.org/pdf/2012.02073" target="_blank">pdf</a>]

<h2>SSGD: A safe and efficient method of gradient descent. (arXiv:2012.02076v1 [cs.LG])</h2>
<h3>Jinhuan Duan, Xianxian Li, Shiqi Gao, Jinyan Wang, Zili Zhong</h3>
<p>With the vigorous development of artificial intelligence technology, various
engineering technology applications have been implemented one after another.
The gradient descent method plays an important role in solving various
optimization problems, due to its simple structure, good stability and easy
implementation. In multi-node machine learning system, the gradients usually
need to be shared. Data reconstruction attacks can reconstruct training data
simply by knowing the gradient information. In this paper, to prevent gradient
leakage while keeping the accuracy of model, we propose the super stochastic
gradient descent approach to update parameters by concealing the modulus length
of gradient vectors and converting it or them into a unit vector. Furthermore,
we analyze the security of stochastic gradient descent approach. Experiment
results show that our approach is obviously superior to prevalent gradient
descent approaches in terms of accuracy and robustness.
</p>
<a href="http://arxiv.org/abs/2012.02076" target="_blank">arXiv:2012.02076</a> [<a href="http://arxiv.org/pdf/2012.02076" target="_blank">pdf</a>]

<h2>Towards Part-Based Understanding of RGB-D Scans. (arXiv:2012.02094v1 [cs.CV])</h2>
<h3>Alexey Bokhovkin, Vladislav Ishimtsev, Emil Bogomolov, Denis Zorin, Alexey Artemov, Evgeny Burnaev, Angela Dai</h3>
<p>Recent advances in 3D semantic scene understanding have shown impressive
progress in 3D instance segmentation, enabling object-level reasoning about 3D
scenes; however, a finer-grained understanding is required to enable
interactions with objects and their functional understanding. Thus, we propose
the task of part-based scene understanding of real-world 3D environments: from
an RGB-D scan of a scene, we detect objects, and for each object predict its
decomposition into geometric part masks, which composed together form the
complete geometry of the observed object. We leverage an intermediary part
graph representation to enable robust completion as well as building of part
priors, which we use to construct the final part mask predictions. Our
experiments demonstrate that guiding part understanding through part graph to
part prior-based predictions significantly outperforms alternative approaches
to the task of semantic part completion.
</p>
<a href="http://arxiv.org/abs/2012.02094" target="_blank">arXiv:2012.02094</a> [<a href="http://arxiv.org/pdf/2012.02094" target="_blank">pdf</a>]

<h2>Emergent Complexity and Zero-shot Transfer via Unsupervised Environment Design. (arXiv:2012.02096v1 [cs.LG])</h2>
<h3>Michael Dennis, Natasha Jaques, Eugene Vinitsky, Alexandre Bayen, Stuart Russell, Andrew Critch, Sergey Levine</h3>
<p>A wide range of reinforcement learning (RL) problems - including robustness,
transfer learning, unsupervised RL, and emergent complexity - require
specifying a distribution of tasks or environments in which a policy will be
trained. However, creating a useful distribution of environments is error
prone, and takes a significant amount of developer time and effort. We propose
Unsupervised Environment Design (UED) as an alternative paradigm, where
developers provide environments with unknown parameters, and these parameters
are used to automatically produce a distribution over valid, solvable
environments. Existing approaches to automatically generating environments
suffer from common failure modes: domain randomization cannot generate
structure or adapt the difficulty of the environment to the agent's learning
progress, and minimax adversarial training leads to worst-case environments
that are often unsolvable. To generate structured, solvable environments for
our protagonist agent, we introduce a second, antagonist agent that is allied
with the environment-generating adversary. The adversary is motivated to
generate environments which maximize regret, defined as the difference between
the protagonist and antagonist agent's return. We call our technique
Protagonist Antagonist Induced Regret Environment Design (PAIRED). Our
experiments demonstrate that PAIRED produces a natural curriculum of
increasingly complex environments, and PAIRED agents achieve higher zero-shot
transfer performance when tested in highly novel environments.
</p>
<a href="http://arxiv.org/abs/2012.02096" target="_blank">arXiv:2012.02096</a> [<a href="http://arxiv.org/pdf/2012.02096" target="_blank">pdf</a>]

<h2>Recursive Tree Grammar Autoencoders. (arXiv:2012.02097v1 [cs.LG])</h2>
<h3>Benjamin Paassen, Irena Koprinska, Kalina Yacef</h3>
<p>Machine learning on tree data has been mostly focused on trees as input. Much
less research has covered trees as output, like in chemical molecule
optimization or hint generation for intelligent tutoring systems. In this work,
we propose recursive tree grammar autoencoders (RTG-AEs), which encode trees
via a bottom-up parser and decode trees via a tree grammar, both controlled by
a neural network that minimizes the variational autoencoder loss. The resulting
encoding and decoding functions can then be employed in subsequent tasks, such
as optimization and time series prediction. Our key message is that combining
grammar knowledge with recursive processing improves performance compared to
using either grammar knowledge or non-sequential processing, but not both. In
particular, we show experimentally that our model improves autoencoding error,
training time, and optimization score on four benchmark datasets compared to
baselines from the literature.
</p>
<a href="http://arxiv.org/abs/2012.02097" target="_blank">arXiv:2012.02097</a> [<a href="http://arxiv.org/pdf/2012.02097" target="_blank">pdf</a>]

<h2>Robust Instance Segmentation through Reasoning about Multi-Object Occlusion. (arXiv:2012.02107v1 [cs.CV])</h2>
<h3>Xiaoding Yuan, Adam Kortylewski, Yihong Sun, Alan Yuille</h3>
<p>Analyzing complex scenes with Deep Neural Networks is a challenging task,
particularly when images contain multiple objects that partially occlude each
other. Existing approaches to image analysis mostly process objects
independently and do not take into account the relative occlusion of nearby
objects. In this paper, we propose a deep network for multi-object instance
segmentation that is robust to occlusion and can be trained from bounding box
supervision only. Our work builds on Compositional Networks, which learn a
generative model of neural feature activations to locate occluders and to
classify objects based on their non-occluded parts. We extend their generative
model to include multiple objects and introduce a framework for the efficient
inference in challenging occlusion scenarios. In particular, we obtain
feed-forward predictions of the object classes and their instance and occluder
segmentations. We introduce an Occlusion Reasoning Module (ORM) that locates
erroneous segmentations and estimates the occlusion ordering to correct them.
The improved segmentation masks are, in turn, integrated into the network in a
top-down manner to improve the image classification. Our experiments on the
KITTI INStance dataset (KINS) and a synthetic occlusion dataset demonstrate the
effectiveness and robustness of our model at multi-object instance segmentation
under occlusion.
</p>
<a href="http://arxiv.org/abs/2012.02107" target="_blank">arXiv:2012.02107</a> [<a href="http://arxiv.org/pdf/2012.02107" target="_blank">pdf</a>]

<h2>Proceedings of NeurIPS 2020 Workshop on Artificial Intelligence for Humanitarian Assistance and Disaster Response. (arXiv:2012.02108v1 [cs.AI])</h2>
<h3>Ritwik Gupta, Eric T. Heim, Edoardo Nemni</h3>
<p>These are the "proceedings" of the 2nd AI + HADR workshop which was held
virtually on December 12, 2020 as part of the Neural Information Processing
Systems conference. These are non-archival and merely serve as a way to collate
all the papers accepted to the workshop.
</p>
<a href="http://arxiv.org/abs/2012.02108" target="_blank">arXiv:2012.02108</a> [<a href="http://arxiv.org/pdf/2012.02108" target="_blank">pdf</a>]

<h2>SAFCAR: Structured Attention Fusion for Compositional Action Recognition. (arXiv:2012.02109v1 [cs.CV])</h2>
<h3>Tae Soo Kim, Gregory D. Hager</h3>
<p>We present a general framework for compositional action recognition -- i.e.
action recognition where the labels are composed out of simpler components such
as subjects, atomic-actions and objects. The main challenge in compositional
action recognition is that there is a combinatorially large set of possible
actions that can be composed using basic components. However, compositionality
also provides a structure that can be exploited. To do so, we develop and test
a novel Structured Attention Fusion (SAF) self-attention mechanism to combine
information from object detections, which capture the time-series structure of
an action, with visual cues that capture contextual information. We show that
our approach recognizes novel verb-noun compositions more effectively than
current state of the art systems, and it generalizes to unseen action
categories quite efficiently from only a few labeled examples. We validate our
approach on the challenging Something-Else tasks from the
Something-Something-V2 dataset. We further show that our framework is flexible
and can generalize to a new domain by showing competitive results on the
Charades-Fewshot dataset.
</p>
<a href="http://arxiv.org/abs/2012.02109" target="_blank">arXiv:2012.02109</a> [<a href="http://arxiv.org/pdf/2012.02109" target="_blank">pdf</a>]

<h2>Deep Inverse Sensor Models as Priors for evidential Occupancy Mapping. (arXiv:2012.02111v1 [cs.RO])</h2>
<h3>Daniel Bauer, Lars Kuhnert, Lutz Eckstein</h3>
<p>With the recent boost in autonomous driving, increased attention has been
paid on radars as an input for occupancy mapping. Besides their many benefits,
the inference of occupied space based on radar detections is notoriously
difficult because of the data sparsity and the environment dependent noise
(e.g. multipath reflections). Recently, deep learning-based inverse sensor
models, from here on called deep ISMs, have been shown to improve over their
geometric counterparts in retrieving occupancy information. Nevertheless, these
methods perform a data-driven interpolation which has to be verified later on
in the presence of measurements. In this work, we describe a novel approach to
integrate deep ISMs together with geometric ISMs into the evidential occupancy
mapping framework. Our method leverages both the capabilities of the
data-driven approach to initialize cells not yet observable for the geometric
model effectively enhancing the perception field and convergence speed, while
at the same time use the precision of the geometric ISM to converge to sharp
boundaries. We further define a lower limit on the deep ISM estimate's
certainty together with analytical proofs of convergence which we use to
distinguish cells that are solely allocated by the deep ISM from cells already
verified using the geometric approach.
</p>
<a href="http://arxiv.org/abs/2012.02111" target="_blank">arXiv:2012.02111</a> [<a href="http://arxiv.org/pdf/2012.02111" target="_blank">pdf</a>]

<h2>Traffic4cast 2020 -- Graph Ensemble Net and the Importance of Feature And Loss Function Design for Traffic Prediction. (arXiv:2012.02115v1 [cs.LG])</h2>
<h3>Qi Qi, Pak Hay Kwok</h3>
<p>This paper details our solution to Traffic4cast 2020. Similar to Traffic4cast
2019, Traffic4cast 2020 challenged its contestants to develop algorithms that
can predict the future traffic states of big cities. Our team tackled this
challenge on two fronts. We studied the importance of feature and loss function
design, and achieved significant improvement to the best performing U-Net
solution from last year. We also explored the use of Graph Neural Networks and
introduced a novel ensemble GNN architecture which outperformed the GNN
solution from last year. While our GNN was improved, it was still unable to
match the performance of U-Nets and the potential reasons for this shortfall
were discussed. Our final solution, an ensemble of our U-Net and GNN, achieved
the 4th place solution in Traffic4cast 2020.
</p>
<a href="http://arxiv.org/abs/2012.02115" target="_blank">arXiv:2012.02115</a> [<a href="http://arxiv.org/pdf/2012.02115" target="_blank">pdf</a>]

<h2>Fast-reactive probabilistic motion planning for high-dimensional robots. (arXiv:2012.02118v1 [cs.RO])</h2>
<h3>Siyu Dai, Andreas Hofmann, Brian C. Williams</h3>
<p>Many real-world robotic operations that involve high-dimensional humanoid
robots require fast-reaction to plan disturbances and probabilistic guarantees
over collision risks, whereas most probabilistic motion planning approaches
developed for car-like robots can not be directly applied to high-dimensional
robots. In this paper, we present probabilistic Chekov (p-Chekov), a
fast-reactive motion planning system that can provide safety guarantees for
high-dimensional robots suffering from process noises and observation noises.
Leveraging recent advances in machine learning as well as our previous work in
deterministic motion planning that integrated trajectory optimization into a
sparse roadmap framework, p-Chekov demonstrates its superiority in terms of
collision avoidance ability and planning speed in high-dimensional robotic
motion planning tasks in complex environments without the convexification of
obstacles. Comprehensive theoretical and empirical analysis provided in this
paper shows that p-Chekov can effectively satisfy user-specified chance
constraints over collision risk in practical robotic manipulation tasks.
</p>
<a href="http://arxiv.org/abs/2012.02118" target="_blank">arXiv:2012.02118</a> [<a href="http://arxiv.org/pdf/2012.02118" target="_blank">pdf</a>]

<h2>Generalized Object Detection on Fisheye Cameras for Autonomous Driving: Dataset, Representations and Baseline. (arXiv:2012.02124v1 [cs.CV])</h2>
<h3>Hazem Rashed, Eslam Mohamed, Ganesh Sistu, Varun Ravi Kumar, Ciaran Eising, Ahmad El-Sallab, Senthil Yogamani</h3>
<p>Object detection is a comprehensively studied problem in autonomous driving.
However, it has been relatively less explored in the case of fisheye cameras.
The standard bounding box fails in fisheye cameras due to the strong radial
distortion, particularly in the image's periphery. We explore better
representations like oriented bounding box, ellipse, and generic polygon for
object detection in fisheye images in this work. We use the IoU metric to
compare these representations using accurate instance segmentation ground
truth. We design a novel curved bounding box model that has optimal properties
for fisheye distortion models. We also design a curvature adaptive perimeter
sampling method for obtaining polygon vertices, improving relative mAP score by
4.9% compared to uniform sampling. Overall, the proposed polygon model improves
mIoU relative accuracy by 40.3%. It is the first detailed study on object
detection on fisheye cameras for autonomous driving scenarios to the best of
our knowledge. The dataset comprising of 10,000 images along with all the
object representations ground truth will be made public to encourage further
research. We summarize our work in a short video with qualitative results at
https://youtu.be/iLkOzvJpL-A.
</p>
<a href="http://arxiv.org/abs/2012.02124" target="_blank">arXiv:2012.02124</a> [<a href="http://arxiv.org/pdf/2012.02124" target="_blank">pdf</a>]

<h2>A similarity-based Bayesian mixture-of-experts model. (arXiv:2012.02130v1 [stat.ML])</h2>
<h3>Tianfang Zhang, Rasmus Bokrantz, Jimmy Olsson</h3>
<p>We present a new nonparametric mixture-of-experts model for multivariate
regression problems, inspired by the probabilistic $k$-nearest neighbors
algorithm. Using a conditionally specified model, predictions for out-of-sample
inputs are based on similarities to each observed data point, yielding
predictive distributions represented by Gaussian mixtures. Posterior inference
is performed on the parameters of the mixture components as well as the
distance metric using a mean-field variational Bayes algorithm accompanied with
a stochastic gradient-based optimization procedure. The proposed method is
especially advantageous in settings where inputs are of relatively high
dimension in comparison to the data size, where input--output relationships are
complex, and where predictive distributions may be skewed or multimodal.
Computational studies on two synthetic datasets and one dataset comprising dose
statistics of radiation therapy treatment plans show that our
mixture-of-experts method outperforms a Gaussian process benchmark model both
in terms of validation metrics and visual inspection.
</p>
<a href="http://arxiv.org/abs/2012.02130" target="_blank">arXiv:2012.02130</a> [<a href="http://arxiv.org/pdf/2012.02130" target="_blank">pdf</a>]

<h2>Manifold Learning and Deep Clustering with Local Dictionaries. (arXiv:2012.02134v1 [cs.LG])</h2>
<h3>Pranay Tankala, Abiy Tasissa, James M. Murphy, Demba Ba</h3>
<p>We introduce a novel clustering algorithm for data sampled from a union of
nonlinear manifolds. Our algorithm extends a popular manifold clustering
framework, which first computes a sparse similarity graph over the input data
and then uses spectral methods to find clusters in this graph. While previous
manifold learning algorithms directly compute similarity scores between pairs
of data points, our algorithm first augments the data set with a small handful
of representative atoms and then computes similarity scores between data points
and atoms. To measure similarity, we express each data point as sparse convex
combination of nearby atoms. To learn the atoms, we employ algorithm unrolling,
an increasingly popular technique for structured deep learning. Ultimately,
this departure from established manifold learning techniques leads to
improvements in clustering accuracy and scalability.
</p>
<a href="http://arxiv.org/abs/2012.02134" target="_blank">arXiv:2012.02134</a> [<a href="http://arxiv.org/pdf/2012.02134" target="_blank">pdf</a>]

<h2>Graph-SIM: A Graph-based Spatiotemporal Interaction Modelling for Pedestrian Action Prediction. (arXiv:2012.02148v1 [cs.CV])</h2>
<h3>Tiffany Yau, Saber Malekmohammadi, Amir Rasouli, Peter Lakner, Mohsen Rohani, Jun Luo</h3>
<p>One of the most crucial yet challenging tasks for autonomous vehicles in
urban environments is predicting the future behaviour of nearby pedestrians,
especially at points of crossing. Predicting behaviour depends on many social
and environmental factors, particularly interactions between road users.
Capturing such interactions requires a global view of the scene and dynamics of
the road users in three-dimensional space. This information, however, is
missing from the current pedestrian behaviour benchmark datasets. Motivated by
these challenges, we propose 1) a novel graph-based model for predicting
pedestrian crossing action. Our method models pedestrians' interactions with
nearby road users through clustering and relative importance weighting of
interactions using features obtained from the bird's-eye-view. 2) We introduce
a new dataset that provides 3D bounding box and pedestrian behavioural
annotations for the existing nuScenes dataset. On the new data, our approach
achieves state-of-the-art performance by improving on various metrics by more
than 10% in comparison to existing methods. Upon publishing of this paper, our
dataset will be made publicly available.
</p>
<a href="http://arxiv.org/abs/2012.02148" target="_blank">arXiv:2012.02148</a> [<a href="http://arxiv.org/pdf/2012.02148" target="_blank">pdf</a>]

<h2>Approximate kNN Classification for Biomedical Data. (arXiv:2012.02149v1 [cs.LG])</h2>
<h3>Panagiotis Anagnostou, Petros T. Barmbas, Aristidis G. Vrahatis, Sotiris K. Tasoulis</h3>
<p>We are in the era where the Big Data analytics has changed the way of
interpreting the various biomedical phenomena, and as the generated data
increase, the need for new machine learning methods to handle this evolution
grows. An indicative example is the single-cell RNA-seq (scRNA-seq), an
emerging DNA sequencing technology with promising capabilities but significant
computational challenges due to the large-scaled generated data. Regarding the
classification process for scRNA-seq data, an appropriate method is the k
Nearest Neighbor (kNN) classifier since it is usually utilized for large-scale
prediction tasks due to its simplicity, minimal parameterization, and
model-free nature. However, the ultra-high dimensionality that characterizes
scRNA-seq impose a computational bottleneck, while prediction power can be
affected by the "Curse of Dimensionality". In this work, we proposed the
utilization of approximate nearest neighbor search algorithms for the task of
kNN classification in scRNA-seq data focusing on a particular methodology
tailored for high dimensional data. We argue that even relaxed approximate
solutions will not affect the prediction performance significantly. The
experimental results confirm the original assumption by offering the potential
for broader applicability.
</p>
<a href="http://arxiv.org/abs/2012.02149" target="_blank">arXiv:2012.02149</a> [<a href="http://arxiv.org/pdf/2012.02149" target="_blank">pdf</a>]

<h2>Dr-COVID: Graph Neural Networks for SARS-CoV-2 Drug Repurposing. (arXiv:2012.02151v1 [cs.LG])</h2>
<h3>Siddhant Doshi, Sundeep Prabhakar Chepuri</h3>
<p>The 2019 novel coronavirus (SARS-CoV-2) pandemic has resulted in more than a
million deaths, high morbidities, and economic distress worldwide. There is an
urgent need to identify medications that would treat and prevent novel diseases
like the 2019 coronavirus disease (COVID-19). Drug repurposing is a promising
strategy to discover new medical indications of the existing approved drugs due
to several advantages in terms of the costs, safety factors, and quick results
compared to new drug design and discovery. In this work, we explore
computational data-driven methods for drug repurposing and propose a dedicated
graph neural network (GNN) based drug repurposing model, called Dr-COVID.
Although we analyze the predicted drugs in detail for COVID-19, the model is
generic and can be used for any novel diseases. We construct a four-layered
heterogeneous graph to model the complex interactions between drugs, diseases,
genes, and anatomies. We pose drug repurposing as a link prediction problem.
Specifically, we design an encoder based on the scalable inceptive graph neural
network (SIGN) to generate embeddings for all the nodes in the four-layered
graph and propose a quadratic norm scorer as a decoder to predict treatment for
a disease. We provide a detailed analysis of the 150 potential drugs (such as
Dexamethasone, Ivermectin) predicted by Dr-COVID for COVID-19 from different
pharmacological classes (e.g., corticosteroids, antivirals, antiparasitic). Out
of these 150 drugs, 46 drugs are currently in clinical trials. Dr-COVID is
evaluated in terms of its prediction performance and its ability to rank the
known treatment drugs for diseases as high as possible. For a majority of the
diseases, Dr-COVID ranks the actual treatment drug in the top 15.
</p>
<a href="http://arxiv.org/abs/2012.02151" target="_blank">arXiv:2012.02151</a> [<a href="http://arxiv.org/pdf/2012.02151" target="_blank">pdf</a>]

<h2>MakeupBag: Disentangling Makeup Extraction and Application. (arXiv:2012.02157v1 [cs.CV])</h2>
<h3>Dokhyam Hoshen</h3>
<p>This paper introduces MakeupBag, a novel method for automatic makeup style
transfer. Our proposed technique can transfer a new makeup style from a
reference face image to another previously unseen facial photograph. We solve
makeup disentanglement and facial makeup application as separable objectives,
in contrast to other current deep methods that entangle the two tasks.
MakeupBag presents a significant advantage for our approach as it allows
customization and pixel specific modification of the extracted makeup style,
which is not possible using current methods. Extensive experiments, both
qualitative and numerical, are conducted demonstrating the high quality and
accuracy of the images produced by our method. Furthermore, in contrast to most
other current methods, MakeupBag tackles both classical and extreme and costume
makeup transfer. In a comparative analysis, MakeupBag is shown to outperform
current state-of-the-art approaches.
</p>
<a href="http://arxiv.org/abs/2012.02157" target="_blank">arXiv:2012.02157</a> [<a href="http://arxiv.org/pdf/2012.02157" target="_blank">pdf</a>]

<h2>Self-labeled Conditional GANs. (arXiv:2012.02162v1 [cs.CV])</h2>
<h3>Mehdi Noroozi</h3>
<p>This paper introduces a novel and fully unsupervised framework for
conditional GAN training in which labels are automatically obtained from data.
We incorporate a clustering network into the standard conditional GAN framework
that plays against the discriminator. With the generator, it aims to find a
shared structured mapping for associating pseudo-labels with the real and fake
images. Our generator outperforms unconditional GANs in terms of FID with
significant margins on large scale datasets like ImageNet and LSUN. It also
outperforms class conditional GANs trained on human labels on CIFAR10 and
CIFAR100 where fine-grained annotations or a large number of samples per class
are not available. Additionally, our clustering network exceeds the
state-of-the-art on CIFAR100 clustering.
</p>
<a href="http://arxiv.org/abs/2012.02162" target="_blank">arXiv:2012.02162</a> [<a href="http://arxiv.org/pdf/2012.02162" target="_blank">pdf</a>]

<h2>Visualization of Supervised and Self-Supervised Neural Networks via Attribution Guided Factorization. (arXiv:2012.02166v1 [cs.CV])</h2>
<h3>Shir Gur, Ameen Ali, Lior Wolf</h3>
<p>Neural network visualization techniques mark image locations by their
relevancy to the network's classification. Existing methods are effective in
highlighting the regions that affect the resulting classification the most.
However, as we show, these methods are limited in their ability to identify the
support for alternative classifications, an effect we name {\em the saliency
bias} hypothesis. In this work, we integrate two lines of research:
gradient-based methods and attribution-based methods, and develop an algorithm
that provides per-class explainability. The algorithm back-projects the per
pixel local influence, in a manner that is guided by the local attributions,
while correcting for salient features that would otherwise bias the
explanation. In an extensive battery of experiments, we demonstrate the ability
of our methods to class-specific visualization, and not just the predicted
label. Remarkably, the method obtains state of the art results in benchmarks
that are commonly applied to gradient-based methods as well as in those that
are employed mostly for evaluating attribution methods. Using a new
unsupervised procedure, our method is also successful in demonstrating that
self-supervised methods learn semantic information.
</p>
<a href="http://arxiv.org/abs/2012.02166" target="_blank">arXiv:2012.02166</a> [<a href="http://arxiv.org/pdf/2012.02166" target="_blank">pdf</a>]

<h2>Multimodal Spatio-Temporal Deep Learning Approach for Neonatal Postoperative Pain Assessment. (arXiv:2012.02175v1 [cs.CV])</h2>
<h3>Md Sirajus Salekin, Ghada Zamzmi, Dmitry Goldgof, Rangachar Kasturi, Thao Ho, Yu Sun</h3>
<p>The current practice for assessing neonatal postoperative pain relies on
bedside caregivers. This practice is subjective, inconsistent, slow, and
discontinuous. To develop a reliable medical interpretation, several automated
approaches have been proposed to enhance the current practice. These approaches
are unimodal and focus mainly on assessing neonatal procedural (acute) pain. As
pain is a multimodal emotion that is often expressed through multiple
modalities, the multimodal assessment of pain is necessary especially in case
of postoperative (acute prolonged) pain. Additionally, spatio-temporal analysis
is more stable over time and has been proven to be highly effective at
minimizing misclassification errors. In this paper, we present a novel
multimodal spatio-temporal approach that integrates visual and vocal signals
and uses them for assessing neonatal postoperative pain. We conduct
comprehensive experiments to investigate the effectiveness of the proposed
approach. We compare the performance of the multimodal and unimodal
postoperative pain assessment, and measure the impact of temporal information
integration. The experimental results, on a real-world dataset, show that the
proposed multimodal spatio-temporal approach achieves the highest AUC (0.87)
and accuracy (79%), which are on average 6.67% and 6.33% higher than unimodal
approaches. The results also show that the integration of temporal information
markedly improves the performance as compared to the non-temporal approach as
it captures changes in the pain dynamic. These results demonstrate that the
proposed approach can be used as a viable alternative to manual assessment,
which would tread a path toward fully automated pain monitoring in clinical
settings, point-of-care testing, and homes.
</p>
<a href="http://arxiv.org/abs/2012.02175" target="_blank">arXiv:2012.02175</a> [<a href="http://arxiv.org/pdf/2012.02175" target="_blank">pdf</a>]

<h2>Material Recognition via Heat Transfer Given Ambiguous Initial Conditions. (arXiv:2012.02176v1 [cs.RO])</h2>
<h3>Tapomayukh Bhattacharjee, Henry M. Clever, Joshua Wade, Charles C. Kemp</h3>
<p>Humans and robots can recognize materials with distinct thermal effusivities
by making physical contact and observing temperatures during heat transfer.
This works well with room temperature materials and humans and robots at human
body temperatures. Past research has shown that cooling or heating a material
can result in temperatures that are similar to contact with another material.
To thoroughly investigate this perceptual ambiguity, we designed a
psychophysical experiment in which a participant discriminates between two
materials given ambiguous initial conditions. We conducted a study with 32
human participants and a robot. Humans and the robot confused the materials. We
also found that robots can overcome this ambiguity using two temperature
sensors with different temperatures prior to contact. We support this
conclusion based on a mathematical proof using a heat transfer model and
empirical results in which a robot achieved 100% accuracy compared to 5% human
accuracy. Our results also indicate that robots can use subtle cues to
distinguish thermally ambiguous materials with a single temperature sensor.
Overall, our work provides insights into challenging conditions for material
recognition via heat transfer, and suggests methods by which robots can
overcome these challenges to outperform humans.
</p>
<a href="http://arxiv.org/abs/2012.02176" target="_blank">arXiv:2012.02176</a> [<a href="http://arxiv.org/pdf/2012.02176" target="_blank">pdf</a>]

<h2>DeepVideoMVS: Multi-View Stereo on Video with Recurrent Spatio-Temporal Fusion. (arXiv:2012.02177v1 [cs.CV])</h2>
<h3>Arda D&#xfc;z&#xe7;eker, Silvano Galliani, Christoph Vogel, Pablo Speciale, Mihai Dusmanu, Marc Pollefeys</h3>
<p>We propose an online multi-view depth prediction approach on posed video
streams, where the scene geometry information computed in the previous time
steps is propagated to the current time step in an efficient and geometrically
plausible way. The backbone of our approach is a real-time capable, lightweight
encoder-decoder that relies on cost volumes computed from pairs of images. We
extend it by placing a ConvLSTM cell at the bottleneck layer, which compresses
an arbitrary amount of past information in its states. The novelty lies in
propagating the hidden state of the cell by accounting for the viewpoint
changes between time steps. At a given time step, we warp the previous hidden
state into the current camera plane using the previous depth prediction. Our
extension brings only a small overhead of computation time and memory
consumption, while improving the depth predictions significantly. As a result,
we outperform the existing state-of-the-art multi-view stereo methods on most
of the evaluated metrics in hundreds of indoor scenes while maintaining a
real-time performance. Code available:
https://github.com/ardaduz/deep-video-mvs
</p>
<a href="http://arxiv.org/abs/2012.02177" target="_blank">arXiv:2012.02177</a> [<a href="http://arxiv.org/pdf/2012.02177" target="_blank">pdf</a>]

<h2>Verifiable Planning in Expected Reward Multichain MDPs. (arXiv:2012.02178v1 [cs.AI])</h2>
<h3>George K. Atia, Andre Beckus, Ismail Alkhouri, Alvaro Velasquez</h3>
<p>The planning domain has experienced increased interest in the formal
synthesis of decision-making policies. This formal synthesis typically entails
finding a policy which satisfies formal specifications in the form of some
well-defined logic, such as Linear Temporal Logic (LTL) or Computation Tree
Logic (CTL), among others. While such logics are very powerful and expressive
in their capacity to capture desirable agent behavior, their value is limited
when deriving decision-making policies which satisfy certain types of
asymptotic behavior. In particular, we are interested in specifying constraints
on the steady-state behavior of an agent, which captures the proportion of time
an agent spends in each state as it interacts for an indefinite period of time
with its environment. This is sometimes called the average or expected behavior
of the agent. In this paper, we explore the steady-state planning problem of
deriving a decision-making policy for an agent such that constraints on its
steady-state behavior are satisfied. A linear programming solution for the
general case of multichain Markov Decision Processes (MDPs) is proposed and we
prove that optimal solutions to the proposed programs yield stationary policies
with rigorous guarantees of behavior.
</p>
<a href="http://arxiv.org/abs/2012.02178" target="_blank">arXiv:2012.02178</a> [<a href="http://arxiv.org/pdf/2012.02178" target="_blank">pdf</a>]

<h2>BasicVSR: The Search for Essential Components in Video Super-Resolution and Beyond. (arXiv:2012.02181v1 [cs.CV])</h2>
<h3>Kelvin C.K. Chan, Xintao Wang, Ke Yu, Chao Dong, Chen Change Loy</h3>
<p>Video super-resolution (VSR) approaches tend to have more components than the
image counterparts as they need to exploit the additional temporal dimension.
Complex designs are not uncommon. In this study, we wish to untangle the knots
and reconsider some most essential components for VSR guided by four basic
functionalities, i.e., Propagation, Alignment, Aggregation, and Upsampling. By
reusing some existing components added with minimal redesigns, we show a
succinct pipeline, BasicVSR, that achieves appealing improvements in terms of
speed and restoration quality in comparison to many state-of-the-art
algorithms. We conduct systematic analysis to explain how such gain can be
obtained and discuss the pitfalls. We further show the extensibility of
BasicVSR by presenting an information-refill mechanism and a coupled
propagation scheme to facilitate information aggregation. The BasicVSR and its
extension, IconVSR, can serve as strong baselines for future VSR approaches.
</p>
<a href="http://arxiv.org/abs/2012.02181" target="_blank">arXiv:2012.02181</a> [<a href="http://arxiv.org/pdf/2012.02181" target="_blank">pdf</a>]

<h2>Stochastic Gradient Descent with Nonlinear Conjugate Gradient-Style Adaptive Momentum. (arXiv:2012.02188v1 [cs.LG])</h2>
<h3>Bao Wang, Qiang Ye</h3>
<p>Momentum plays a crucial role in stochastic gradient-based optimization
algorithms for accelerating or improving training deep neural networks (DNNs).
In deep learning practice, the momentum is usually weighted by a
well-calibrated constant. However, tuning hyperparameters for momentum can be a
significant computational burden. In this paper, we propose a novel
\emph{adaptive momentum} for improving DNNs training; this adaptive momentum,
with no momentum related hyperparameter required, is motivated by the nonlinear
conjugate gradient (NCG) method. Stochastic gradient descent (SGD) with this
new adaptive momentum eliminates the need for the momentum hyperparameter
calibration, allows a significantly larger learning rate, accelerates DNN
training, and improves final accuracy and robustness of the trained DNNs. For
instance, SGD with this adaptive momentum reduces classification errors for
training ResNet110 for CIFAR10 and CIFAR100 from $5.25\%$ to $4.64\%$ and
$23.75\%$ to $20.03\%$, respectively. Furthermore, SGD with the new adaptive
momentum also benefits adversarial training and improves adversarial robustness
of the trained DNNs.
</p>
<a href="http://arxiv.org/abs/2012.02188" target="_blank">arXiv:2012.02188</a> [<a href="http://arxiv.org/pdf/2012.02188" target="_blank">pdf</a>]

<h2>Learned Initializations for Optimizing Coordinate-Based Neural Representations. (arXiv:2012.02189v1 [cs.CV])</h2>
<h3>Matthew Tancik, Ben Mildenhall, Terrance Wang, Divi Schmidt, Pratul P. Srinivasan, Jonathan T. Barron, Ren Ng</h3>
<p>Coordinate-based neural representations have shown significant promise as an
alternative to discrete, array-based representations for complex low
dimensional signals. However, optimizing a coordinate-based network from
randomly initialized weights for each new signal is inefficient. We propose
applying standard meta-learning algorithms to learn the initial weight
parameters for these fully-connected networks based on the underlying class of
signals being represented (e.g., images of faces or 3D models of chairs).
Despite requiring only a minor change in implementation, using these learned
initial weights enables faster convergence during optimization and can serve as
a strong prior over the signal class being modeled, resulting in better
generalization when only partial observations of a given signal are available.
We explore these benefits across a variety of tasks, including representing 2D
images, reconstructing CT scans, and recovering 3D shapes and scenes from 2D
image observations.
</p>
<a href="http://arxiv.org/abs/2012.02189" target="_blank">arXiv:2012.02189</a> [<a href="http://arxiv.org/pdf/2012.02189" target="_blank">pdf</a>]

<h2>pixelNeRF: Neural Radiance Fields from One or Few Images. (arXiv:2012.02190v1 [cs.CV])</h2>
<h3>Alex Yu, Vickie Ye, Matthew Tancik, Angjoo Kanazawa</h3>
<p>We propose pixelNeRF, a learning framework that predicts a continuous neural
scene representation conditioned on one or few input images. The existing
approach for constructing neural radiance fields involves optimizing the
representation to every scene independently, requiring many calibrated views
and significant compute time. We take a step towards resolving these
shortcomings by introducing an architecture that conditions a NeRF on image
inputs in a fully convolutional manner. This allows the network to be trained
across multiple scenes to learn a scene prior, enabling it to perform novel
view synthesis in a feed-forward manner from a sparse set of views (as few as
one). Leveraging the volume rendering approach of NeRF, our model can be
trained directly from images with no explicit 3D supervision. We conduct
extensive experiments on ShapeNet benchmarks for single image novel view
synthesis tasks with held-out objects as well as entire unseen categories. We
further demonstrate the flexibility of pixelNeRF by demonstrating it on
multi-object ShapeNet scenes and real scenes from the DTU dataset. In all
cases, pixelNeRF outperforms current state-of-the-art baselines for novel view
synthesis and single image 3D reconstruction. For the video and code, please
visit the project website: https://alexyu.net/pixelnerf
</p>
<a href="http://arxiv.org/abs/2012.02190" target="_blank">arXiv:2012.02190</a> [<a href="http://arxiv.org/pdf/2012.02190" target="_blank">pdf</a>]

<h2>The Resistance to Label Noise in K-NN and DNN Depends on its Concentration. (arXiv:1803.11410v3 [cs.LG] UPDATED)</h2>
<h3>Amnon Drory, Oria Ratzon, Shai Avidan, Raja Giryes</h3>
<p>We investigate the classification performance of K-nearest neighbors (K-NN)
and deep neural networks (DNNs) in the presence of label noise. We first show
empirically that a DNN's prediction for a given test example depends on the
labels of the training examples in its local neighborhood. This motivates us to
derive a realizable analytic expression that approximates the multi-class K-NN
classification error in the presence of label noise, which is of independent
importance. We then suggest that the expression for K-NN may serve as a
first-order approximation for the DNN error. Finally, we demonstrate
empirically the proximity of the developed expression to the observed
performance of K-NN and DNN classifiers. Our result may explain the already
observed surprising resistance of DNN to some types of label noise. It also
characterizes an important factor of it showing that the more concentrated the
noise the greater is the degradation in performance.
</p>
<a href="http://arxiv.org/abs/1803.11410" target="_blank">arXiv:1803.11410</a> [<a href="http://arxiv.org/pdf/1803.11410" target="_blank">pdf</a>]

<h2>The Tsetlin Machine -- A Game Theoretic Bandit Driven Approach to Optimal Pattern Recognition with Propositional Logic. (arXiv:1804.01508v14 [cs.AI] UPDATED)</h2>
<h3>Ole-Christoffer Granmo</h3>
<p>Although simple individually, artificial neurons provide state-of-the-art
performance when interconnected in deep networks. Arguably, the Tsetlin
Automaton is an even simpler and more versatile learning mechanism, capable of
solving the multi-armed bandit problem. Merely by means of a single integer as
memory, it learns the optimal action in stochastic environments through
increment and decrement operations. In this paper, we introduce the Tsetlin
Machine, which solves complex pattern recognition problems with propositional
formulas, composed by a collective of Tsetlin Automata. To eliminate the
longstanding problem of vanishing signal-to-noise ratio, the Tsetlin Machine
orchestrates the automata using a novel game. Further, both inputs, patterns,
and outputs are expressed as bits, while recognition and learning rely on bit
manipulation, simplifying computation. Our theoretical analysis establishes
that the Nash equilibria of the game align with the propositional formulas that
provide optimal pattern recognition accuracy. This translates to learning
without local optima, only global ones. In five benchmarks, the Tsetlin Machine
provides competitive accuracy compared with SVMs, Decision Trees, Random
Forests, Naive Bayes Classifier, Logistic Regression, and Neural Networks. We
further demonstrate how the propositional formulas facilitate interpretation.
In conclusion, we believe the combination of high accuracy, interpretability,
and computational simplicity makes the Tsetlin Machine a promising tool for a
wide range of domains.
</p>
<a href="http://arxiv.org/abs/1804.01508" target="_blank">arXiv:1804.01508</a> [<a href="http://arxiv.org/pdf/1804.01508" target="_blank">pdf</a>]

<h2>Fixed-Point Convolutional Neural Network for Real-Time Video Processing in FPGA. (arXiv:1808.09945v2 [cs.CV] UPDATED)</h2>
<h3>Roman Solovyev, Alexander Kustov, Dmitry Telpukhov, Vladimir Rukhlov, Alexandr Kalinin</h3>
<p>Modern mobile neural networks with a reduced number of weights and parameters
do a good job with image classification tasks, but even they may be too complex
to be implemented in an FPGA for video processing tasks. The article proposes
neural network architecture for the practical task of recognizing images from a
camera, which has several advantages in terms of speed. This is achieved by
reducing the number of weights, moving from a floating-point to a fixed-point
arithmetic, and due to a number of hardware-level optimizations associated with
storing weights in blocks, a shift register, and an adjustable number of
convolutional blocks that work in parallel. The article also proposed methods
for adapting the existing data set for solving a different task. As the
experiments showed, the proposed neural network copes well with real-time video
processing even on the cheap FPGAs.
</p>
<a href="http://arxiv.org/abs/1808.09945" target="_blank">arXiv:1808.09945</a> [<a href="http://arxiv.org/pdf/1808.09945" target="_blank">pdf</a>]

<h2>Contrastive Explanation: A Structural-Model Approach. (arXiv:1811.03163v2 [cs.AI] UPDATED)</h2>
<h3>Tim Miller</h3>
<p>This paper presents a model of contrastive explanation using structural
casual models. The topic of causal explanation in artificial intelligence has
gathered interest in recent years as researchers and practitioners aim to
increase trust and understanding of intelligent decision-making. While
different sub-fields of artificial intelligence have looked into this problem
with a sub-field-specific view, there are few models that aim to capture
explanation more generally. One general model is based on structural causal
models. It defines an explanation as a fact that, if found to be true, would
constitute an actual cause of a specific event. However, research in philosophy
and social sciences shows that explanations are contrastive: that is, when
people ask for an explanation of an event -- the fact -- they (sometimes
implicitly) are asking for an explanation relative to some contrast case; that
is, "Why P rather than Q?". In this paper, we extend the structural causal
model approach to define two complementary notions of contrastive explanation,
and demonstrate them on two classical problems in artificial intelligence:
classification and planning. We believe that this model can help researchers in
subfields of artificial intelligence to better understand contrastive
explanation.
</p>
<a href="http://arxiv.org/abs/1811.03163" target="_blank">arXiv:1811.03163</a> [<a href="http://arxiv.org/pdf/1811.03163" target="_blank">pdf</a>]

<h2>Learning Heuristics over Large Graphs via Deep Reinforcement Learning. (arXiv:1903.03332v5 [cs.LG] UPDATED)</h2>
<h3>Sahil Manchanda, Akash Mittal, Anuj Dhawan, Sourav Medya, Sayan Ranu, Ambuj Singh</h3>
<p>There has been an increased interest in discovering heuristics for
combinatorial problems on graphs through machine learning. While existing
techniques have primarily focused on obtaining high-quality solutions,
scalability to billion-sized graphs has not been adequately addressed. In
addition, the impact of budget-constraint, which is necessary for many
practical scenarios, remains to be studied. In this paper, we propose a
framework called GCOMB to bridge these gaps. GCOMB trains a Graph Convolutional
Network (GCN) using a novel probabilistic greedy mechanism to predict the
quality of a node. To further facilitate the combinatorial nature of the
problem, GCOMB utilizes a Q-learning framework, which is made efficient through
importance sampling. We perform extensive experiments on real graphs to
benchmark the efficiency and efficacy of GCOMB. Our results establish that
GCOMB is 100 times faster and marginally better in quality than
state-of-the-art algorithms for learning combinatorial algorithms.
Additionally, a case-study on the practical combinatorial problem of Influence
Maximization (IM) shows GCOMB is 150 times faster than the specialized IM
algorithm IMM with similar quality.
</p>
<a href="http://arxiv.org/abs/1903.03332" target="_blank">arXiv:1903.03332</a> [<a href="http://arxiv.org/pdf/1903.03332" target="_blank">pdf</a>]

<h2>ThumbNet: One Thumbnail Image Contains All You Need for Recognition. (arXiv:1904.05034v3 [cs.CV] UPDATED)</h2>
<h3>Chen Zhao, Bernard Ghanem</h3>
<p>Although deep convolutional neural networks (CNNs) have achieved great
success in computer vision tasks, its real-world application is still impeded
by its voracious demand of computational resources. Current works mostly seek
to compress the network by reducing its parameters or parameter-incurred
computation, neglecting the influence of the input image on the system
complexity. Based on the fact that input images of a CNN contain substantial
redundancy, in this paper, we propose a unified framework, dubbed as ThumbNet,
to simultaneously accelerate and compress CNN models by enabling them to infer
on one thumbnail image. We provide three effective strategies to train
ThumbNet. In doing so, ThumbNet learns an inference network that performs
equally well on small images as the original-input network on large images.
With ThumbNet, not only do we obtain the thumbnail-input inference network that
can drastically reduce computation and memory requirements, but also we obtain
an image downscaler that can generate thumbnail images for generic
classification tasks. Extensive experiments show the effectiveness of ThumbNet,
and demonstrate that the thumbnail-input inference network learned by ThumbNet
can adequately retain the accuracy of the original-input network even when the
input images are downscaled 16 times.
</p>
<a href="http://arxiv.org/abs/1904.05034" target="_blank">arXiv:1904.05034</a> [<a href="http://arxiv.org/pdf/1904.05034" target="_blank">pdf</a>]

<h2>Implicit Pairs for Boosting Unpaired Image-to-Image Translation. (arXiv:1904.06913v4 [cs.CV] UPDATED)</h2>
<h3>Yiftach Ginger, Dov Danon, Hadar Averbuch-Elor, Daniel Cohen-Or</h3>
<p>In image-to-image translation the goal is to learn a mapping from one image
domain to another. In the case of supervised approaches the mapping is learned
from paired samples. However, collecting large sets of image pairs is often
either prohibitively expensive or not possible. As a result, in recent years
more attention has been given to techniques that learn the mapping from
unpaired sets.

In our work, we show that injecting implicit pairs into unpaired sets
strengthens the mapping between the two domains, improves the compatibility of
their distributions, and leads to performance boosting of unsupervised
techniques by over 14% across several measurements.

The competence of the implicit pairs is further displayed with the use of
pseudo-pairs, i.e., paired samples which only approximate a real pair. We
demonstrate the effect of the approximated implicit samples on image-to-image
translation problems, where such pseudo-pairs may be synthesized in one
direction, but not in the other. We further show that pseudo-pairs are
significantly more effective as implicit pairs in an unpaired setting, than
directly using them explicitly in a paired setting.
</p>
<a href="http://arxiv.org/abs/1904.06913" target="_blank">arXiv:1904.06913</a> [<a href="http://arxiv.org/pdf/1904.06913" target="_blank">pdf</a>]

<h2>More Efficient Policy Learning via Optimal Retargeting. (arXiv:1906.08611v2 [stat.ML] UPDATED)</h2>
<h3>Nathan Kallus</h3>
<p>Policy learning can be used to extract individualized treatment regimes from
observational data in healthcare, civics, e-commerce, and beyond. One big
hurdle to policy learning is a commonplace lack of overlap in the data for
different actions, which can lead to unwieldy policy evaluation and poorly
performing learned policies. We study a solution to this problem based on
retargeting, that is, changing the population on which policies are optimized.
We first argue that at the population level, retargeting may induce little to
no bias. We then characterize the optimal reference policy and retargeting
weights in both binary-action and multi-action settings. We do this in terms of
the asymptotic efficient estimation variance of the new learning objective.
Extensive empirical results in a simulation study and a case study of
personalized job counseling demonstrate that retargeting is a fairly easy way
to significantly improve any policy learning procedure applied to observational
data.
</p>
<a href="http://arxiv.org/abs/1906.08611" target="_blank">arXiv:1906.08611</a> [<a href="http://arxiv.org/pdf/1906.08611" target="_blank">pdf</a>]

<h2>Inverse Visual Question Answering with Multi-Level Attentions. (arXiv:1909.07583v2 [cs.CV] UPDATED)</h2>
<h3>Yaser Alwattar, Yuhong Guo</h3>
<p>In this paper, we propose a novel deep multi-level attention model to address
inverse visual question answering. The proposed model generates regional visual
and semantic features at the object level and then enhances them with the
answer cue by using attention mechanisms. Two levels of multiple attentions are
employed in the model, including the dual attention at the partial question
encoding step and the dynamic attention at the next question word generation
step. We evaluate the proposed model on the VQA V1 dataset. It demonstrates
state-of-the-art performance in terms of multiple commonly used metrics.
</p>
<a href="http://arxiv.org/abs/1909.07583" target="_blank">arXiv:1909.07583</a> [<a href="http://arxiv.org/pdf/1909.07583" target="_blank">pdf</a>]

<h2>Modelling the influence of data structure on learning in neural networks: the hidden manifold model. (arXiv:1909.11500v4 [stat.ML] UPDATED)</h2>
<h3>Sebastian Goldt, Marc M&#xe9;zard, Florent Krzakala, Lenka Zdeborov&#xe1;</h3>
<p>Understanding the reasons for the success of deep neural networks trained
using stochastic gradient-based methods is a key open problem for the nascent
theory of deep learning. The types of data where these networks are most
successful, such as images or sequences of speech, are characterised by
intricate correlations. Yet, most theoretical work on neural networks does not
explicitly model training data, or assumes that elements of each data sample
are drawn independently from some factorised probability distribution. These
approaches are thus by construction blind to the correlation structure of
real-world data sets and their impact on learning in neural networks. Here, we
introduce a generative model for structured data sets that we call the hidden
manifold model (HMM). The idea is to construct high-dimensional inputs that lie
on a lower-dimensional manifold, with labels that depend only on their position
within this manifold, akin to a single layer decoder or generator in a
generative adversarial network. We demonstrate that learning of the hidden
manifold model is amenable to an analytical treatment by proving a "Gaussian
Equivalence Property" (GEP), and we use the GEP to show how the dynamics of
two-layer neural networks trained using one-pass stochastic gradient descent is
captured by a set of integro-differential equations that track the performance
of the network at all times. This permits us to analyse in detail how a neural
network learns functions of increasing complexity during training, how its
performance depends on its size and how it is impacted by parameters such as
the learning rate or the dimension of the hidden manifold.
</p>
<a href="http://arxiv.org/abs/1909.11500" target="_blank">arXiv:1909.11500</a> [<a href="http://arxiv.org/pdf/1909.11500" target="_blank">pdf</a>]

<h2>Deep Semantic Parsing of Freehand Sketches with Homogeneous Transformation, Soft-Weighted Loss, and Staged Learning. (arXiv:1910.06023v2 [cs.CV] UPDATED)</h2>
<h3>Ying Zheng, Hongxun Yao, Xiaoshuai Sun</h3>
<p>In this paper, we propose a novel deep framework for part-level semantic
parsing of freehand sketches, which makes three main contributions that are
experimentally shown to have substantial practical merit. First, we propose a
homogeneous transformation method to address the problem of domain adaptation.
For the task of sketch parsing, there is no available data of labeled freehand
sketches that can be directly used for model training. An alternative solution
is to learn from datasets of real image parsing, while the domain adaptation is
an inevitable problem. Unlike existing methods that utilize the edge maps of
real images to approximate freehand sketches, the proposed homogeneous
transformation method transforms the data from domains of real images and
freehand sketches into a homogeneous space to minimize the semantic gap.
Second, we design a soft-weighted loss function as guidance for the training
process, which gives attention to both the ambiguous label boundary and class
imbalance. Third, we present a staged learning strategy to improve the parsing
performance of the trained model, which takes advantage of the shared
information and specific characteristic from different sketch categories.
Extensive experimental results demonstrate the effectiveness of the above three
methods. Specifically, to evaluate the generalization ability of our
homogeneous transformation method, additional experiments for the task of
sketch-based image retrieval are conducted on the QMUL FG-SBIR dataset.
Finally, by integrating the proposed three methods into a unified framework of
deep semantic sketch parsing (DeepSSP), we achieve the state-of-the-art on the
public SketchParse dataset.
</p>
<a href="http://arxiv.org/abs/1910.06023" target="_blank">arXiv:1910.06023</a> [<a href="http://arxiv.org/pdf/1910.06023" target="_blank">pdf</a>]

<h2>Sketch-Specific Data Augmentation for Freehand Sketch Recognition. (arXiv:1910.06038v2 [cs.CV] UPDATED)</h2>
<h3>Ying Zheng, Hongxun Yao, Xiaoshuai Sun, Shengping Zhang, Sicheng Zhao, Fatih Porikli</h3>
<p>Sketch recognition remains a significant challenge due to the limited
training data and the substantial intra-class variance of freehand sketches for
the same object. Conventional methods for this task often rely on the
availability of the temporal order of sketch strokes, additional cues acquired
from different modalities and supervised augmentation of sketch datasets with
real images, which also limit the applicability and feasibility of these
methods in real scenarios.

In this paper, we propose a novel sketch-specific data augmentation (SSDA)
method that leverages the quantity and quality of the sketches automatically.
From the aspect of quantity, we introduce a Bezier pivot based deformation
(BPD) strategy to enrich the training data. Towards quality improvement, we
present a mean stroke reconstruction (MSR) approach to generate a set of novel
types of sketches with smaller intra-class variances. Both of these solutions
are unrestricted from any multi-source data and temporal cues of sketches.
Furthermore, we show that some recent deep convolutional neural network models
that are trained on generic classes of real images can be better choices than
most of the elaborate architectures that are designed explicitly for sketch
recognition. As SSDA can be integrated with any convolutional neural networks,
it has a distinct advantage over the existing methods. Our extensive
experimental evaluations demonstrate that the proposed method achieves the
state-of-the-art results (84.27%) on the TU-Berlin dataset, outperforming the
human performance by a remarkable 11.17% increase. Finally, more experiments
show the practical value of our approach for the task of sketch-based image
retrieval.
</p>
<a href="http://arxiv.org/abs/1910.06038" target="_blank">arXiv:1910.06038</a> [<a href="http://arxiv.org/pdf/1910.06038" target="_blank">pdf</a>]

<h2>Verification and Parameter Synthesis for Stochastic Systems using Optimistic Optimization. (arXiv:1911.01537v2 [cs.LG] UPDATED)</h2>
<h3>Negin Musavi, Dawei Sun, Sayan Mitra, Geir Dullerud, Sanjay Shakkottai</h3>
<p>We present an algorithm for formal verification and parameter synthesis of
continuous state-space Markov chains. This class of problems captures the
design and analysis of a wide variety of autonomous and cyber-physical systems
defined by nonlinear and black-box modules. In order to solve these problems,
one has to maximize certain probabilistic objective functions overall choices
of initial states and parameters. In this paper, we identify the assumptions
that make it possible to view this problem as a multi-armed bandit problem.
Based on this fresh perspective, we propose an algorithm (HOO-MB) for solving
the problem that carefully instantiates an existing bandit algorithm --
Hierarchical Optimistic Optimization -- with appropriate parameters. As a
consequence, we obtain theoretical regret bounds on sample efficiency of our
solution that depends on key problem parameters like smoothness,
near-optimality dimension, and batch size. The batch size parameter enables us
to strike a balance between the sample efficiency and the memory usage of the
algorithm. Our experiments, using the tool HooVer, suggest that the approach
scales to realistic-sized problems and is often more sample-efficient compared
to PlasmaLab -- a leading tool for verification of stochastic systems.
Specifically, HooVer has distinct advantages in analyzing models in which the
objective function has sharp slopes. In addition, HooVer shows promising
behavior in parameter synthesis for a linear quadratic regulator (LQR) example.
</p>
<a href="http://arxiv.org/abs/1911.01537" target="_blank">arXiv:1911.01537</a> [<a href="http://arxiv.org/pdf/1911.01537" target="_blank">pdf</a>]

<h2>Insights into Ordinal Embedding Algorithms: A Systematic Evaluation. (arXiv:1912.01666v6 [cs.LG] UPDATED)</h2>
<h3>Leena Chennuru Vankadara, Siavash Haghiri, Michael Lohaus, Faiz Ul Wahab, Ulrike von Luxburg</h3>
<p>The objective of ordinal embedding is to find a Euclidean representation of a
set of abstract items, using only answers to triplet comparisons of the form
"Is item $i$ closer to the item $j$ or item $k$?". In recent years, numerous
algorithms have been proposed to solve this problem. However, there does not
exist a fair and thorough assessment of these embedding methods and therefore
several key questions remain unanswered: Which algorithms scale better with
increasing sample size or dimension? Which ones perform better when the
embedding dimension is small or few triplet comparisons are available? In our
paper, we address these questions and provide the first comprehensive and
systematic empirical evaluation of existing algorithms as well as a new neural
network approach. In the large triplet regime, we find that simple, relatively
unknown, non-convex methods consistently outperform all other algorithms,
including elaborate approaches based on neural networks or landmark approaches.
This finding can be explained by our insight that many of the non-convex
optimization approaches do not suffer from local optima. In the low triplet
regime, our neural network approach is either competitive or significantly
outperforms all the other methods. Our comprehensive assessment is enabled by
our unified library of popular embedding algorithms that leverages GPU
resources and allows for fast and accurate embeddings of millions of data
points.
</p>
<a href="http://arxiv.org/abs/1912.01666" target="_blank">arXiv:1912.01666</a> [<a href="http://arxiv.org/pdf/1912.01666" target="_blank">pdf</a>]

<h2>Optimal Policies Tend to Seek Power. (arXiv:1912.01683v6 [cs.AI] UPDATED)</h2>
<h3>Alexander Matt Turner, Logan Smith, Rohin Shah, Andrew Critch, Prasad Tadepalli</h3>
<p>Some researchers have speculated that capable reinforcement learning agents
are often incentivized to seek resources and power in pursuit of their
objectives. While seeking power in order to optimize a misspecified objective,
agents might be incentivized to behave in undesirable ways, including
rationally preventing deactivation and correction. Others have voiced
skepticism: human power-seeking instincts seem idiosyncratic, and these urges
need not be present in reinforcement learning agents. We formalize a notion of
power within the context of Markov decision processes. With respect to a class
of neutral reward function distributions, we provide sufficient conditions for
when optimal policies tend to seek power over the environment.
</p>
<a href="http://arxiv.org/abs/1912.01683" target="_blank">arXiv:1912.01683</a> [<a href="http://arxiv.org/pdf/1912.01683" target="_blank">pdf</a>]

<h2>Basis Prediction Networks for Effective Burst Denoising with Large Kernels. (arXiv:1912.04421v2 [cs.CV] UPDATED)</h2>
<h3>Zhihao Xia, Federico Perazzi, Micha&#xeb;l Gharbi, Kalyan Sunkavalli, Ayan Chakrabarti</h3>
<p>Bursts of images exhibit significant self-similarity across both time and
space. This motivates a representation of the kernels as linear combinations of
a small set of basis elements. To this end, we introduce a novel basis
prediction network that, given an input burst, predicts a set of global basis
kernels -- shared within the image -- and the corresponding mixing coefficients
-- which are specific to individual pixels. Compared to state-of-the-art
techniques that output a large tensor of per-pixel spatiotemporal kernels, our
formulation substantially reduces the dimensionality of the network output.
This allows us to effectively exploit comparatively larger denoising kernels,
achieving both significant quality improvements (over 1dB PSNR) and faster
run-times over state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/1912.04421" target="_blank">arXiv:1912.04421</a> [<a href="http://arxiv.org/pdf/1912.04421" target="_blank">pdf</a>]

<h2>The Counterfactual $\chi$-GAN. (arXiv:2001.03115v2 [cs.LG] UPDATED)</h2>
<h3>Amelia J. Averitt, Natnicha Vanitchanant, Rajesh Ranganath, Adler J. Perotte</h3>
<p>Causal inference often relies on the counterfactual framework, which requires
that treatment assignment is independent of the outcome, known as strong
ignorability. Approaches to enforcing strong ignorability in causal analyses of
observational data include weighting and matching methods. Effect estimates,
such as the average treatment effect (ATE), are then estimated as expectations
under the reweighted or matched distribution, P . The choice of P is important
and can impact the interpretation of the effect estimate and the variance of
effect estimates. In this work, instead of specifying P, we learn a
distribution that simultaneously maximizes coverage and minimizes variance of
ATE estimates. In order to learn this distribution, this research proposes a
generative adversarial network (GAN)-based model called the Counterfactual
$\chi$-GAN (cGAN), which also learns feature-balancing weights and supports
unbiased causal estimation in the absence of unobserved confounding. Our model
minimizes the Pearson $\chi^2$ divergence, which we show simultaneously
maximizes coverage and minimizes the variance of importance sampling estimates.
To our knowledge, this is the first such application of the Pearson $\chi^2$
divergence. We demonstrate the effectiveness of cGAN in achieving feature
balance relative to established weighting methods in simulation and with
real-world medical data.
</p>
<a href="http://arxiv.org/abs/2001.03115" target="_blank">arXiv:2001.03115</a> [<a href="http://arxiv.org/pdf/2001.03115" target="_blank">pdf</a>]

<h2>Imperfect ImaGANation: Implications of GANs Exacerbating Biases on Facial Data Augmentation and Snapchat Selfie Lenses. (arXiv:2001.09528v2 [cs.LG] UPDATED)</h2>
<h3>Niharika Jain, Alberto Olmo, Sailik Sengupta, Lydia Manikonda, Subbarao Kambhampati</h3>
<p>The use of synthetic data generated by Generative Adversarial Networks (GANs)
is widely used for a variety of tasks ranging from data augmentation to
stylizing images. While practitioners celebrate this method as an economical
way to obtain synthetic data to train data-hungry machine learning models or
provide new features to users of mobile applications, it is unclear that they
recognize the perils of such techniques when applied to an already-biased
dataset. Although one expects GANs to replicate the distribution of the
original data, in real-world settings with limited data and finite network
capacity, GANs suffer from mode-collapse. In this paper, we show that popular
(conditional and unconditional) GAN variants exacerbate biases along the axes
of gender and skin tone in the generated data. First, we show readily
accessible GAN variants such as DCGANs 'imagine' faces of synthetic engineering
professors that have masculine facial features and fair skin tones. Further,
architectures such as AdaGAN, ProGAN that attempt to address mode collapse
issue cannot completely correct this behavior. Second, we show that a
conditional GAN variant transforms input images of female faces to have more
masculine features when asked to generate faces of engineering professors.
Worse yet, prevalent filters on Snapchat end up consistently lightening the
skin tones in women of color when trying to make face images appear more
feminine. Thus, our study is meant to serve as a cautionary tale for
practitioners and educate them about the side-effect of bias amplification when
applying GAN-based techniques.
</p>
<a href="http://arxiv.org/abs/2001.09528" target="_blank">arXiv:2001.09528</a> [<a href="http://arxiv.org/pdf/2001.09528" target="_blank">pdf</a>]

<h2>Identifying Mislabeled Data using the Area Under the Margin Ranking. (arXiv:2001.10528v3 [cs.LG] UPDATED)</h2>
<h3>Geoff Pleiss, Tianyi Zhang, Ethan R. Elenberg, Kilian Q. Weinberger</h3>
<p>Not all data in a typical training set help with generalization; some samples
can be overly ambiguous or outrightly mislabeled. This paper introduces a new
method to identify such samples and mitigate their impact when training neural
networks. At the heart of our algorithm is the Area Under the Margin (AUM)
statistic, which exploits differences in the training dynamics of clean and
mislabeled samples. A simple procedure - adding an extra class populated with
purposefully mislabeled threshold samples - learns a AUM upper bound that
isolates mislabeled data. This approach consistently improves upon prior work
on synthetic and real-world datasets. On the WebVision50 classification task
our method removes 17% of training data, yielding a 1.6% (absolute) improvement
in test error. On CIFAR100 removing 13% of the data leads to a 1.2% drop in
error.
</p>
<a href="http://arxiv.org/abs/2001.10528" target="_blank">arXiv:2001.10528</a> [<a href="http://arxiv.org/pdf/2001.10528" target="_blank">pdf</a>]

<h2>Tensor denoising and completion based on ordinal observations. (arXiv:2002.06524v2 [stat.ML] UPDATED)</h2>
<h3>Chanwoo Lee, Miaoyan Wang</h3>
<p>Higher-order tensors arise frequently in applications such as neuroimaging,
recommendation system, social network analysis, and psychological studies. We
consider the problem of low-rank tensor estimation from possibly incomplete,
ordinal-valued observations. Two related problems are studied, one on tensor
denoising and another on tensor completion. We propose a multi-linear
cumulative link model, develop a rank-constrained M-estimator, and obtain
theoretical accuracy guarantees. Our mean squared error bound enjoys a faster
convergence rate than previous results, and we show that the proposed estimator
is minimax optimal under the class of low-rank models. Furthermore, the
procedure developed serves as an efficient completion method which guarantees
consistent recovery of an order-$K$ $(d,\ldots,d)$-dimensional low-rank tensor
using only $\tilde{\mathcal{O}}(Kd)$ noisy, quantized observations. We
demonstrate the outperformance of our approach over previous methods on the
tasks of clustering and collaborative filtering.
</p>
<a href="http://arxiv.org/abs/2002.06524" target="_blank">arXiv:2002.06524</a> [<a href="http://arxiv.org/pdf/2002.06524" target="_blank">pdf</a>]

<h2>Object-Centric Image Generation from Layouts. (arXiv:2003.07449v2 [cs.CV] UPDATED)</h2>
<h3>Tristan Sylvain, Pengchuan Zhang, Yoshua Bengio, R Devon Hjelm, Shikhar Sharma</h3>
<p>Despite recent impressive results on single-object and single-domain image
generation, the generation of complex scenes with multiple objects remains
challenging. In this paper, we start with the idea that a model must be able to
understand individual objects and relationships between objects in order to
generate complex scenes well. Our layout-to-image-generation method, which we
call Object-Centric Generative Adversarial Network (or OC-GAN), relies on a
novel Scene-Graph Similarity Module (SGSM). The SGSM learns representations of
the spatial relationships between objects in the scene, which lead to our
model's improved layout-fidelity. We also propose changes to the conditioning
mechanism of the generator that enhance its object instance-awareness. Apart
from improving image quality, our contributions mitigate two failure modes in
previous approaches: (1) spurious objects being generated without corresponding
bounding boxes in the layout, and (2) overlapping bounding boxes in the layout
leading to merged objects in images. Extensive quantitative evaluation and
ablation studies demonstrate the impact of our contributions, with our model
outperforming previous state-of-the-art approaches on both the COCO-Stuff and
Visual Genome datasets. Finally, we address an important limitation of
evaluation metrics used in previous works by introducing SceneFID -- an
object-centric adaptation of the popular Fr{\'e}chet Inception Distance metric,
that is better suited for multi-object images.
</p>
<a href="http://arxiv.org/abs/2003.07449" target="_blank">arXiv:2003.07449</a> [<a href="http://arxiv.org/pdf/2003.07449" target="_blank">pdf</a>]

<h2>VaB-AL: Incorporating Class Imbalance and Difficulty with Variational Bayes for Active Learning. (arXiv:2003.11249v2 [cs.LG] UPDATED)</h2>
<h3>Jongwon Choi, Kwang Moo Yi, Jihoon Kim, Jinho Choo, Byoungjip Kim, Jin-Yeop Chang, Youngjune Gwon, Hyung Jin Chang</h3>
<p>Active Learning for discriminative models has largely been studied with the
focus on individual samples, with less emphasis on how classes are distributed
or which classes are hard to deal with. In this work, we show that this is
harmful. We propose a method based on the Bayes' rule, that can naturally
incorporate class imbalance into the Active Learning framework. We derive that
three terms should be considered together when estimating the probability of a
classifier making a mistake for a given sample; i) probability of mislabelling
a class, ii) likelihood of the data given a predicted class, and iii) the prior
probability on the abundance of a predicted class. Implementing these terms
requires a generative model and an intractable likelihood estimation.
Therefore, we train a Variational Auto Encoder (VAE) for this purpose. To
further tie the VAE with the classifier and facilitate VAE training, we use the
classifiers' deep feature representations as input to the VAE. By considering
all three probabilities, among them especially the data imbalance, we can
substantially improve the potential of existing methods under limited data
budget. We show that our method can be applied to classification tasks on
multiple different datasets -- including one that is a real-world dataset with
heavy data imbalance -- significantly outperforming the state of the art.
</p>
<a href="http://arxiv.org/abs/2003.11249" target="_blank">arXiv:2003.11249</a> [<a href="http://arxiv.org/pdf/2003.11249" target="_blank">pdf</a>]

<h2>Auto-Ensemble: An Adaptive Learning Rate Scheduling based Deep Learning Model Ensembling. (arXiv:2003.11266v2 [cs.LG] UPDATED)</h2>
<h3>Jun Yang, Fei Wang</h3>
<p>Ensembling deep learning models is a shortcut to promote its implementation
in new scenarios, which can avoid tuning neural networks, losses and training
algorithms from scratch. However, it is difficult to collect sufficient
accurate and diverse models through once training. This paper proposes
Auto-Ensemble (AE) to collect checkpoints of deep learning model and ensemble
them automatically by adaptive learning rate scheduling algorithm. The
advantage of this method is to make the model converge to various local optima
by scheduling the learning rate in once training. When the number of lo-cal
optimal solutions tends to be saturated, all the collected checkpoints are used
for ensemble. Our method is universal, it can be applied to various scenarios.
Experiment results on multiple datasets and neural networks demonstrate it is
effective and competitive, especially on few-shot learning. Besides, we
proposed a method to measure the distance among models. Then we can ensure the
accuracy and diversity of collected models.
</p>
<a href="http://arxiv.org/abs/2003.11266" target="_blank">arXiv:2003.11266</a> [<a href="http://arxiv.org/pdf/2003.11266" target="_blank">pdf</a>]

<h2>Adaptive Partial Scanning Transmission Electron Microscopy with Reinforcement Learning. (arXiv:2004.02786v3 [cs.LG] UPDATED)</h2>
<h3>Jeffrey M. Ede</h3>
<p>Compressed sensing can decrease scanning transmission electron microscopy
electron dose and scan time with minimal information loss. Traditionally,
sparse scans used in compressed sensing sample a static set of probing
locations. However, we present a prototype for a contiguous sparse scan system
that piecewise adapts scan paths to specimens as they are scanned. Sampling
directions for scan segments are chosen by a recurrent neural network based on
previously observed scan segments. The recurrent actor is trained by
reinforcement learning to cooperate with a feedforward convolutional neural
network that completes sparse scans. This paper presents our learning policy,
experiments, and example partial scans, and discusses future research
directions. Source code, pretrained models, and training data is openly
accessible at https://github.com/Jeffrey-Ede/adaptive-scans
</p>
<a href="http://arxiv.org/abs/2004.02786" target="_blank">arXiv:2004.02786</a> [<a href="http://arxiv.org/pdf/2004.02786" target="_blank">pdf</a>]

<h2>Power-Constrained Bandits. (arXiv:2004.06230v3 [cs.LG] UPDATED)</h2>
<h3>Jiayu Yao, Emma Brunskill, Weiwei Pan, Susan Murphy, Finale Doshi-Velez</h3>
<p>Contextual bandits often provide simple and effective personalization in
decision making problems, making them popular in many domains including digital
health. However, when bandits are deployed in the context of a scientific
study, the aim is not only to personalize for an individual, but also to
determine, with sufficient statistical power, whether or not the system's
intervention is effective. The two objectives are often deployed under
different model assumptions, making it hard to determine how achieving one goal
affects the other. In this work, we develop general meta-algorithms to modify
existing algorithms such that sufficient power is guaranteed, without
significant decrease in average return. We also demonstrate that our
meta-algorithms are robust to various model mis-specifications.
</p>
<a href="http://arxiv.org/abs/2004.06230" target="_blank">arXiv:2004.06230</a> [<a href="http://arxiv.org/pdf/2004.06230" target="_blank">pdf</a>]

<h2>AutoTune: Automatically Tuning Convolutional Neural Networks for Improved Transfer Learning. (arXiv:2005.02165v2 [cs.CV] UPDATED)</h2>
<h3>S.H.Shabbeer Basha, Sravan Kumar Vinakota, Viswanath Pulabaigari, Snehasis Mukherjee, Shiv Ram Dubey</h3>
<p>Transfer learning enables solving a specific task having limited data by
using the pre-trained deep networks trained on large-scale datasets. Typically,
while transferring the learned knowledge from source task to the target task,
the last few layers are fine-tuned (re-trained) over the target dataset.
However, these layers are originally designed for the source task that might
not be suitable for the target task. In this paper, we introduce a mechanism
for automatically tuning the Convolutional Neural Networks (CNN) for improved
transfer learning. The pre-trained CNN layers are tuned with the knowledge from
target data using Bayesian Optimization. First, we train the final layer of the
base CNN model by replacing the number of neurons in the softmax layer with the
number of classes involved in the target task. Next, the pre-trained CNN is
tuned automatically by observing the classification performance on the
validation data (greedy criteria). To evaluate the performance of the proposed
method, experiments are conducted on three benchmark datasets, e.g.,
CalTech-101, CalTech-256, and Stanford Dogs. The classification results
obtained through the proposed AutoTune method outperforms the standard baseline
transfer learning methods over the three datasets by achieving $95.92\%$,
$86.54\%$, and $84.67\%$ accuracy over CalTech-101, CalTech-256, and Stanford
Dogs, respectively. The experimental results obtained in this study depict that
tuning of the pre-trained CNN layers with the knowledge from the target dataset
confesses better transfer learning ability. The source codes are available at
https://github.com/JekyllAndHyde8999/AutoTune_CNN_TransferLearning.
</p>
<a href="http://arxiv.org/abs/2005.02165" target="_blank">arXiv:2005.02165</a> [<a href="http://arxiv.org/pdf/2005.02165" target="_blank">pdf</a>]

<h2>Text Recognition in the Wild: A Survey. (arXiv:2005.03492v3 [cs.CV] UPDATED)</h2>
<h3>Xiaoxue Chen, Lianwen Jin, Yuanzhi Zhu, Canjie Luo, Tianwei Wang</h3>
<p>The history of text can be traced back over thousands of years. Rich and
precise semantic information carried by text is important in a wide range of
vision-based application scenarios. Therefore, text recognition in natural
scenes has been an active research field in computer vision and pattern
recognition. In recent years, with the rise and development of deep learning,
numerous methods have shown promising in terms of innovation, practicality, and
efficiency. This paper aims to (1) summarize the fundamental problems and the
state-of-the-art associated with scene text recognition; (2) introduce new
insights and ideas; (3) provide a comprehensive review of publicly available
resources; (4) point out directions for future work. In summary, this
literature review attempts to present the entire picture of the field of scene
text recognition. It provides a comprehensive reference for people entering
this field, and could be helpful to inspire future research. Related resources
are available at our Github repository:
https://github.com/HCIILAB/Scene-Text-Recognition.
</p>
<a href="http://arxiv.org/abs/2005.03492" target="_blank">arXiv:2005.03492</a> [<a href="http://arxiv.org/pdf/2005.03492" target="_blank">pdf</a>]

<h2>A Simple Semi-Supervised Learning Framework for Object Detection. (arXiv:2005.04757v2 [cs.CV] UPDATED)</h2>
<h3>Kihyuk Sohn, Zizhao Zhang, Chun-Liang Li, Han Zhang, Chen-Yu Lee, Tomas Pfister</h3>
<p>Semi-supervised learning (SSL) has a potential to improve the predictive
performance of machine learning models using unlabeled data. Although there has
been remarkable recent progress, the scope of demonstration in SSL has mainly
been on image classification tasks. In this paper, we propose STAC, a simple
yet effective SSL framework for visual object detection along with a data
augmentation strategy. STAC deploys highly confident pseudo labels of localized
objects from an unlabeled image and updates the model by enforcing consistency
via strong augmentations. We propose experimental protocols to evaluate the
performance of semi-supervised object detection using MS-COCO and show the
efficacy of STAC on both MS-COCO and VOC07. On VOC07, STAC improves the
AP$^{0.5}$ from $76.30$ to $79.08$; on MS-COCO, STAC demonstrates $2{\times}$
higher data efficiency by achieving 24.38 mAP using only 5\% labeled data than
supervised baseline that marks 23.86\% using 10\% labeled data. The code is
available at https://github.com/google-research/ssl_detection/.
</p>
<a href="http://arxiv.org/abs/2005.04757" target="_blank">arXiv:2005.04757</a> [<a href="http://arxiv.org/pdf/2005.04757" target="_blank">pdf</a>]

<h2>rTop-k: A Statistical Estimation Approach to Distributed SGD. (arXiv:2005.10761v2 [cs.LG] UPDATED)</h2>
<h3>Leighton Pate Barnes, Huseyin A. Inan, Berivan Isik, Ayfer Ozgur</h3>
<p>The large communication cost for exchanging gradients between different nodes
significantly limits the scalability of distributed training for large-scale
learning models. Motivated by this observation, there has been significant
recent interest in techniques that reduce the communication cost of distributed
Stochastic Gradient Descent (SGD), with gradient sparsification techniques such
as top-k and random-k shown to be particularly effective. The same observation
has also motivated a separate line of work in distributed statistical
estimation theory focusing on the impact of communication constraints on the
estimation efficiency of different statistical models. The primary goal of this
paper is to connect these two research lines and demonstrate how statistical
estimation models and their analysis can lead to new insights in the design of
communication-efficient training techniques. We propose a simple statistical
estimation model for the stochastic gradients which captures the sparsity and
skewness of their distribution. The statistically optimal communication scheme
arising from the analysis of this model leads to a new sparsification technique
for SGD, which concatenates random-k and top-k, considered separately in the
prior literature. We show through extensive experiments on both image and
language domains with CIFAR-10, ImageNet, and Penn Treebank datasets that the
concatenated application of these two sparsification methods consistently and
significantly outperforms either method applied alone.
</p>
<a href="http://arxiv.org/abs/2005.10761" target="_blank">arXiv:2005.10761</a> [<a href="http://arxiv.org/pdf/2005.10761" target="_blank">pdf</a>]

<h2>Evolutionary NAS with Gene Expression Programming of Cellular Encoding. (arXiv:2005.13110v2 [cs.CV] UPDATED)</h2>
<h3>Clifford Broni-Bediako, Yuki Murata, Luiz Henrique Mormille, Masayasu Atsumi</h3>
<p>The renaissance of neural architecture search (NAS) has seen classical
methods such as genetic algorithms (GA) and genetic programming (GP) being
exploited for convolutional neural network (CNN) architectures. While recent
work have achieved promising performance on visual perception tasks, the direct
encoding scheme of both GA and GP has functional complexity deficiency and does
not scale well on large architectures like CNN. To address this, we present a
new generative encoding scheme -- $symbolic\ linear\ generative\ encoding$
(SLGE) -- simple, yet powerful scheme which embeds local graph transformations
in chromosomes of linear fixed-length string to develop CNN architectures of
variant shapes and sizes via evolutionary process of gene expression
programming. In experiments, the effectiveness of SLGE is shown in discovering
architectures that improve the performance of the state-of-the-art handcrafted
CNN architectures on CIFAR-10 and CIFAR-100 image classification tasks; and
achieves a competitive classification error rate with the existing NAS methods
using less GPU resources.
</p>
<a href="http://arxiv.org/abs/2005.13110" target="_blank">arXiv:2005.13110</a> [<a href="http://arxiv.org/pdf/2005.13110" target="_blank">pdf</a>]

<h2>OT-Flow: Fast and Accurate Continuous Normalizing Flows via Optimal Transport. (arXiv:2006.00104v3 [cs.LG] UPDATED)</h2>
<h3>Derek Onken, Samy Wu Fung, Xingjian Li, Lars Ruthotto</h3>
<p>A normalizing flow is an invertible mapping between an arbitrary probability
distribution and a standard normal distribution; it can be used for density
estimation and statistical inference. Computing the flow follows the change of
variables formula and thus requires invertibility of the mapping and an
efficient way to compute the determinant of its Jacobian. To satisfy these
requirements, normalizing flows typically consist of carefully chosen
components. Continuous normalizing flows (CNFs) are mappings obtained by
solving a neural ordinary differential equation (ODE). The neural ODE's
dynamics can be chosen almost arbitrarily while ensuring invertibility.
Moreover, the log-determinant of the flow's Jacobian can be obtained by
integrating the trace of the dynamics' Jacobian along the flow. Our proposed
OT-Flow approach tackles two critical computational challenges that limit a
more widespread use of CNFs. First, OT-Flow leverages optimal transport (OT)
theory to regularize the CNF and enforce straight trajectories that are easier
to integrate. Second, OT-Flow features exact trace computation with time
complexity equal to trace estimators used in existing CNFs. On five
high-dimensional density estimation and generative modeling tasks, OT-Flow
performs competitively to state-of-the-art CNFs while on average requiring
one-fourth of the number of weights with an 8x speedup in training time and 24x
speedup in inference.
</p>
<a href="http://arxiv.org/abs/2006.00104" target="_blank">arXiv:2006.00104</a> [<a href="http://arxiv.org/pdf/2006.00104" target="_blank">pdf</a>]

<h2>Query Training: Learning a Worse Model to Infer Better Marginals in Undirected Graphical Models with Hidden Variables. (arXiv:2006.06803v3 [stat.ML] UPDATED)</h2>
<h3>Miguel L&#xe1;zaro-Gredilla, Wolfgang Lehrach, Nishad Gothoskar, Guangyao Zhou, Antoine Dedieu, Dileep George</h3>
<p>Probabilistic graphical models (PGMs) provide a compact representation of
knowledge that can be queried in a flexible way: after learning the parameters
of a graphical model once, new probabilistic queries can be answered at test
time without retraining. However, when using undirected PGMS with hidden
variables, two sources of error typically compound in all but the simplest
models (a) learning error (both computing the partition function and
integrating out the hidden variables is intractable); and (b) prediction error
(exact inference is also intractable). Here we introduce query training (QT), a
mechanism to learn a PGM that is optimized for the approximate inference
algorithm that will be paired with it. The resulting PGM is a worse model of
the data (as measured by the likelihood), but it is tuned to produce better
marginals for a given inference algorithm. Unlike prior works, our approach
preserves the querying flexibility of the original PGM: at test time, we can
estimate the marginal of any variable given any partial evidence. We
demonstrate experimentally that QT can be used to learn a challenging
8-connected grid Markov random field with hidden variables and that it
consistently outperforms the state-of-the-art AdVIL when tested on three
undirected models across multiple datasets.
</p>
<a href="http://arxiv.org/abs/2006.06803" target="_blank">arXiv:2006.06803</a> [<a href="http://arxiv.org/pdf/2006.06803" target="_blank">pdf</a>]

<h2>FLeet: Online Federated Learning via Staleness Awareness and Performance Prediction. (arXiv:2006.07273v2 [cs.LG] UPDATED)</h2>
<h3>Georgios Damaskinos, Rachid Guerraoui, Anne-Marie Kermarrec, Vlad Nitu, Rhicheek Patra, Francois Taiani</h3>
<p>Federated Learning (FL) is very appealing for its privacy benefits:
essentially, a global model is trained with updates computed on mobile devices
while keeping the data of users local. Standard FL infrastructures are however
designed to have no energy or performance impact on mobile devices, and are
therefore not suitable for applications that require frequent (online) model
updates, such as news recommenders.

This paper presents FLeet, the first Online FL system, acting as a middleware
between the Android OS and the machine learning application. FLeet combines the
privacy of Standard FL with the precision of online learning thanks to two core
components: (i) I-Prof, a new lightweight profiler that predicts and controls
the impact of learning tasks on mobile devices, and (ii) AdaSGD, a new adaptive
learning algorithm that is resilient to delayed updates.

Our extensive evaluation shows that Online FL, as implemented by FLeet, can
deliver a 2.3x quality boost compared to Standard FL, while only consuming
0.036% of the battery per day. I-Prof can accurately control the impact of
learning tasks by improving the prediction accuracy up to 3.6x (computation
time) and up to 19x (energy). AdaSGD outperforms alternative FL approaches by
18.4% in terms of convergence speed on heterogeneous data.
</p>
<a href="http://arxiv.org/abs/2006.07273" target="_blank">arXiv:2006.07273</a> [<a href="http://arxiv.org/pdf/2006.07273" target="_blank">pdf</a>]

<h2>Dual T: Reducing Estimation Error for Transition Matrix in Label-noise Learning. (arXiv:2006.07805v2 [cs.LG] UPDATED)</h2>
<h3>Yu Yao, Tongliang Liu, Bo Han, Mingming Gong, Jiankang Deng, Gang Niu, Masashi Sugiyama</h3>
<p>The transition matrix, denoting the transition relationship from clean labels
to noisy labels, is essential to build statistically consistent classifiers in
label-noise learning. Existing methods for estimating the transition matrix
rely heavily on estimating the noisy class posterior. However, the estimation
error for noisy class posterior could be large due to the randomness of label
noise, which would lead the transition matrix to be poorly estimated.
Therefore, in this paper, we aim to solve this problem by exploiting the
divide-and-conquer paradigm. Specifically, we introduce an intermediate class
to avoid directly estimating the noisy class posterior. By this intermediate
class, the original transition matrix can then be factorized into the product
of two easy-to-estimate transition matrices. We term the proposed method the
dual-T estimator. Both theoretical analyses and empirical results illustrate
the effectiveness of the dual-T estimator for estimating transition matrices,
leading to better classification performances.
</p>
<a href="http://arxiv.org/abs/2006.07805" target="_blank">arXiv:2006.07805</a> [<a href="http://arxiv.org/pdf/2006.07805" target="_blank">pdf</a>]

<h2>Part-dependent Label Noise: Towards Instance-dependent Label Noise. (arXiv:2006.07836v2 [cs.LG] UPDATED)</h2>
<h3>Xiaobo Xia, Tongliang Liu, Bo Han, Nannan Wang, Mingming Gong, Haifeng Liu, Gang Niu, Dacheng Tao, Masashi Sugiyama</h3>
<p>Learning with the \textit{instance-dependent} label noise is challenging,
because it is hard to model such real-world noise. Note that there are
psychological and physiological evidences showing that we humans perceive
instances by decomposing them into parts. Annotators are therefore more likely
to annotate instances based on the parts rather than the whole instances, where
a wrong mapping from parts to classes may cause the instance-dependent label
noise. Motivated by this human cognition, in this paper, we approximate the
instance-dependent label noise by exploiting \textit{part-dependent} label
noise. Specifically, since instances can be approximately reconstructed by a
combination of parts, we approximate the instance-dependent \textit{transition
matrix} for an instance by a combination of the transition matrices for the
parts of the instance. The transition matrices for parts can be learned by
exploiting anchor points (i.e., data points that belong to a specific class
almost surely). Empirical evaluations on synthetic and real-world datasets
demonstrate our method is superior to the state-of-the-art approaches for
learning from the instance-dependent label noise.
</p>
<a href="http://arxiv.org/abs/2006.07836" target="_blank">arXiv:2006.07836</a> [<a href="http://arxiv.org/pdf/2006.07836" target="_blank">pdf</a>]

<h2>Layer-wise Learning of Kernel Dependence Networks. (arXiv:2006.08539v4 [stat.ML] UPDATED)</h2>
<h3>Chieh Wu, Aria Masoomi, Arthur Gretton, Jennifer Dy</h3>
<p>Due to recent debate over the biological plausibility of backpropagation
(BP), finding an alternative network optimization strategy has become an active
area of interest. We design a new type of kernel network, that is solved
greedily, to theoretically answer several questions of interest. First, if BP
is difficult to simulate in the brain, are there instead "trivial network
weights" (requiring minimum computation) that allow a greedily trained network
to classify any pattern. Perhaps a simple repetition of some basic rule can
yield a network equally powerful as ones trained by BP with Stochastic Gradient
Descent (SGD). Second, can a greedily trained network converge to a kernel?
What kernel will it converge to? Third, is this trivial solution optimal? How
is the optimal solution related to generalization? Lastly, can we theoretically
identify the network width and depth without a grid search? We prove that the
kernel embedding is the trivial solution that compels the greedy procedure to
converge to a kernel with Universal property. Yet, this trivial solution is not
even optimal. By obtaining the optimal solution spectrally, it provides insight
into the generalization of the network while informing us of the network width
and depth.
</p>
<a href="http://arxiv.org/abs/2006.08539" target="_blank">arXiv:2006.08539</a> [<a href="http://arxiv.org/pdf/2006.08539" target="_blank">pdf</a>]

<h2>Predicting Livelihood Indicators from Crowdsourced Street Level Images. (arXiv:2006.08661v4 [cs.CV] UPDATED)</h2>
<h3>Jihyeon Lee, Dylan Grosz, Burak Uzkent, Sicheng Zeng, Marshall Burke, David Lobell, Stefano Ermon</h3>
<p>Major decisions from governments and other large organizations rely on
measurements of the populace's well-being, but making such measurements at a
broad scale is expensive and thus infrequent in much of the developing world.
We propose an inexpensive, scalable, and interpretable approach to predict key
livelihood indicators from public crowd-sourced street-level imagery. Such
imagery can be cheaply collected and more frequently updated compared to
traditional surveying methods, while containing plausibly relevant information
for a range of livelihood indicators. We propose two approaches to learn from
the street-level imagery: (1) a method that creates multi-household cluster
representations by detecting informative objects and (2) a graph-based approach
that captures the relationships between images. By visualizing what features
are important to a model and how they are used, we can help end-user
organizations understand the models and offer an alternate approach for index
estimation that uses cheaply obtained roadway features. By comparing our
results against ground data collected in nationally-representative household
surveys, we demonstrate the performance of our approach in accurately
predicting indicators of poverty, population, and health and its scalability by
testing in two different countries, India and Kenya.
</p>
<a href="http://arxiv.org/abs/2006.08661" target="_blank">arXiv:2006.08661</a> [<a href="http://arxiv.org/pdf/2006.08661" target="_blank">pdf</a>]

<h2>Learning Incompressible Fluid Dynamics from Scratch -- Towards Fast, Differentiable Fluid Models that Generalize. (arXiv:2006.08762v2 [cs.LG] UPDATED)</h2>
<h3>Nils Wandel, Michael Weinmann, Reinhard Klein</h3>
<p>Fast and stable fluid simulations are an essential prerequisite for
applications ranging from computer-generated imagery to computer-aided design
in research and development. However, solving the partial differential
equations of incompressible fluids is a challenging task and traditional
numerical approximation schemes come at high computational costs. Recent deep
learning based approaches promise vast speed-ups but do not generalize to new
fluid domains, require fluid simulation data for training, or rely on complex
pipelines that outsource major parts of the fluid simulation to traditional
methods. In this work, we propose a novel physics-constrained training approach
that generalizes to new fluid domains, requires no fluid simulation data, and
allows convolutional neural networks to map a fluid state from time-point t to
a subsequent state at time t + dt in a single forward pass. This simplifies the
pipeline to train and evaluate neural fluid models. After training, the
framework yields models that are capable of fast fluid simulations and can
handle various fluid phenomena including the Magnus effect and Karman vortex
streets. We present an interactive real-time demo to show the speed and
generalization capabilities of our trained models. Moreover, the trained neural
networks are efficient differentiable fluid solvers as they offer a
differentiable update step to advance the fluid simulation in time. We exploit
this fact in a proof-of-concept optimal control experiment. Our models
significantly outperform a recent differentiable fluid solver in terms of
computational speed and accuracy.
</p>
<a href="http://arxiv.org/abs/2006.08762" target="_blank">arXiv:2006.08762</a> [<a href="http://arxiv.org/pdf/2006.08762" target="_blank">pdf</a>]

<h2>Backdoor Attacks Against Deep Learning Systems in the Physical World. (arXiv:2006.14580v2 [cs.CV] UPDATED)</h2>
<h3>Emily Wenger, Josephine Passananti, Arjun Bhagoji, Yuanshun Yao, Haitao Zheng, Ben Y. Zhao</h3>
<p>Backdoor attacks embed hidden malicious behaviors into deep learning models,
which only activate and cause misclassifications on model inputs containing a
specific trigger. Existing works on backdoor attacks and defenses, however,
mostly focus on digital attacks that use digitally generated patterns as
triggers. A critical question remains unanswered: can backdoor attacks succeed
using physical objects as triggers, thus making them a credible threat against
deep learning systems in the real world? We conduct a detailed empirical study
to explore this question for facial recognition, a critical deep learning task.
Using seven physical objects as triggers, we collect a custom dataset of 3205
images of ten volunteers and use it to study the feasibility of physical
backdoor attacks under a variety of real-world conditions. Our study reveals
two key findings. First, physical backdoor attacks can be highly successful if
they are carefully configured to overcome the constraints imposed by physical
objects. In particular, the placement of successful triggers is largely
constrained by the target model's dependence on key facial features. Second,
four of today's state-of-the-art defenses against (digital) backdoors are
ineffective against physical backdoors, because the use of physical objects
breaks core assumptions used to construct these defenses. Our study confirms
that (physical) backdoor attacks are not a hypothetical phenomenon but rather
pose a serious real-world threat to critical classification tasks. We need new
and more robust defenses against backdoors in the physical world.
</p>
<a href="http://arxiv.org/abs/2006.14580" target="_blank">arXiv:2006.14580</a> [<a href="http://arxiv.org/pdf/2006.14580" target="_blank">pdf</a>]

<h2>Space-Time Correspondence as a Contrastive Random Walk. (arXiv:2006.14613v2 [cs.CV] UPDATED)</h2>
<h3>Allan Jabri, Andrew Owens, Alexei A. Efros</h3>
<p>This paper proposes a simple self-supervised approach for learning a
representation for visual correspondence from raw video. We cast correspondence
as prediction of links in a space-time graph constructed from video. In this
graph, the nodes are patches sampled from each frame, and nodes adjacent in
time can share a directed edge. We learn a representation in which pairwise
similarity defines transition probability of a random walk, so that long-range
correspondence is computed as a walk along the graph. We optimize the
representation to place high probability along paths of similarity. Targets for
learning are formed without supervision, by cycle-consistency: the objective is
to maximize the likelihood of returning to the initial node when walking along
a graph constructed from a palindrome of frames. Thus, a single path-level
constraint implicitly supervises chains of intermediate comparisons. When used
as a similarity metric without adaptation, the learned representation
outperforms the self-supervised state-of-the-art on label propagation tasks
involving objects, semantic parts, and pose. Moreover, we demonstrate that a
technique we call edge dropout, as well as self-supervised adaptation at
test-time, further improve transfer for object-centric correspondence.
</p>
<a href="http://arxiv.org/abs/2006.14613" target="_blank">arXiv:2006.14613</a> [<a href="http://arxiv.org/pdf/2006.14613" target="_blank">pdf</a>]

<h2>FracBits: Mixed Precision Quantization via Fractional Bit-Widths. (arXiv:2007.02017v2 [cs.CV] UPDATED)</h2>
<h3>Linjie Yang, Qing Jin</h3>
<p>Model quantization helps to reduce model size and latency of deep neural
networks. Mixed precision quantization is favorable with customized hardwares
supporting arithmetic operations at multiple bit-widths to achieve maximum
efficiency. We propose a novel learning-based algorithm to derive mixed
precision models end-to-end under target computation constraints and model
sizes. During the optimization, the bit-width of each layer / kernel in the
model is at a fractional status of two consecutive bit-widths which can be
adjusted gradually. With a differentiable regularization term, the resource
constraints can be met during the quantization-aware training which results in
an optimized mixed precision model. Further, our method can be naturally
combined with channel pruning for better computation cost allocation. Our final
models achieve comparable or better performance than previous quantization
methods with mixed precision on MobilenetV1/V2, ResNet18 under different
resource constraints on ImageNet dataset.
</p>
<a href="http://arxiv.org/abs/2007.02017" target="_blank">arXiv:2007.02017</a> [<a href="http://arxiv.org/pdf/2007.02017" target="_blank">pdf</a>]

<h2>Are We Overfitting to Experimental Setups in Recognition?. (arXiv:2007.02519v3 [cs.CV] UPDATED)</h2>
<h3>Matthew Wallingford, Aditya Kusupati, Keivan Alizadeh-Vahid, Aaron Walsman, Aniruddha Kembhavi, Ali Farhadi</h3>
<p>Enabling robust intelligence in the real-world entails systems that offer
continuous inference while learning from varying amounts of data and
supervision. The machine learning community has organically broken down this
challenging goal into manageable sub-tasks such as supervised, few-shot, and
continual learning. In light of substantial progress on each sub-task, we pose
the question, "How well does this progress translate to more practical
scenarios?" To investigate this question, we construct a new framework, FLUID,
which removes certain assumptions made by current experimental setups while
integrating these sub-tasks via the following design choices -- consuming
sequential data, allowing for flexible training phases, being compute aware,
and working in an open-world setting. Evaluating a broad set of methods on
FLUID leads to new insights including strong evidence that methods are
overfitting to their experimental setup. For example, we find that
representative few-shot methods are substantially worse than simple baselines,
self-supervised representations from MoCo fail to learn new classes when the
downstream task contains a mix of new and old classes, and pretraining largely
mitigates the problem of catastrophic forgetting. Finally, we propose two new
simple methods which outperform all other evaluated methods which further
questions our progress towards robust, real-world systems. Project page:
https://raivn.cs.washington.edu/projects/FLUID/.
</p>
<a href="http://arxiv.org/abs/2007.02519" target="_blank">arXiv:2007.02519</a> [<a href="http://arxiv.org/pdf/2007.02519" target="_blank">pdf</a>]

<h2>The LoCA Regret: A Consistent Metric to Evaluate Model-Based Behavior in Reinforcement Learning. (arXiv:2007.03158v2 [cs.LG] UPDATED)</h2>
<h3>Harm van Seijen, Hadi Nekoei, Evan Racah, Sarath Chandar</h3>
<p>Deep model-based Reinforcement Learning (RL) has the potential to
substantially improve the sample-efficiency of deep RL. While various
challenges have long held it back, a number of papers have recently come out
reporting success with deep model-based methods. This is a great development,
but the lack of a consistent metric to evaluate such methods makes it difficult
to compare various approaches. For example, the common single-task
sample-efficiency metric conflates improvements due to model-based learning
with various other aspects, such as representation learning, making it
difficult to assess true progress on model-based RL. To address this, we
introduce an experimental setup to evaluate model-based behavior of RL methods,
inspired by work from neuroscience on detecting model-based behavior in humans
and animals. Our metric based on this setup, the Local Change Adaptation (LoCA)
regret, measures how quickly an RL method adapts to a local change in the
environment. Our metric can identify model-based behavior, even if the method
uses a poor representation and provides insight in how close a method's
behavior is from optimal model-based behavior. We use our setup to evaluate the
model-based behavior of MuZero on a variation of the classic Mountain Car task.
</p>
<a href="http://arxiv.org/abs/2007.03158" target="_blank">arXiv:2007.03158</a> [<a href="http://arxiv.org/pdf/2007.03158" target="_blank">pdf</a>]

<h2>SSN: Soft Shadow Network for Image Compositing. (arXiv:2007.08211v2 [cs.CV] UPDATED)</h2>
<h3>Yichen Sheng, Jianming Zhang, Bedrich Benes</h3>
<p>We introduce an interactive Soft Shadow Network (SSN) to generates
controllable soft shadows for image compositing. SSN takes a 2D object mask as
input and thus is agnostic to image types such as painting and vector art. An
environment light map is used to control the shadow's characteristics, such as
angle and softness. SSN employs an Ambient Occlusion Prediction module to
predict an intermediate ambient occlusion map, which can be further refined by
the user to provides geometric cues to modulate the shadow generation. To train
our model, we design an efficient pipeline to produce diverse soft shadow
training data using 3D object models. In addition, we propose an inverse shadow
map representation to improve model training. We demonstrate that our model
produces realistic soft shadows in real-time. Our user study shows that the
generated shadows are often indistinguishable from shadows calculated by a
physics-based renderer.
</p>
<a href="http://arxiv.org/abs/2007.08211" target="_blank">arXiv:2007.08211</a> [<a href="http://arxiv.org/pdf/2007.08211" target="_blank">pdf</a>]

<h2>Multi-label Contrastive Predictive Coding. (arXiv:2007.09852v2 [cs.LG] UPDATED)</h2>
<h3>Jiaming Song, Stefano Ermon</h3>
<p>Variational mutual information (MI) estimators are widely used in
unsupervised representation learning methods such as contrastive predictive
coding (CPC). A lower bound on MI can be obtained from a multi-class
classification problem, where a critic attempts to distinguish a positive
sample drawn from the underlying joint distribution from $(m-1)$ negative
samples drawn from a suitable proposal distribution. Using this approach, MI
estimates are bounded above by $\log m$, and could thus severely underestimate
unless $m$ is very large. To overcome this limitation, we introduce a novel
estimator based on a multi-label classification problem, where the critic needs
to jointly identify multiple positive samples at the same time. We show that
using the same amount of negative samples, multi-label CPC is able to exceed
the $\log m$ bound, while still being a valid lower bound of mutual
information. We demonstrate that the proposed approach is able to lead to
better mutual information estimation, gain empirical improvements in
unsupervised representation learning, and beat a current state-of-the-art
knowledge distillation method over 10 out of 13 tasks.
</p>
<a href="http://arxiv.org/abs/2007.09852" target="_blank">arXiv:2007.09852</a> [<a href="http://arxiv.org/pdf/2007.09852" target="_blank">pdf</a>]

<h2>Active Learning for Video Description With Cluster-Regularized Ensemble Ranking. (arXiv:2007.13913v3 [cs.CV] UPDATED)</h2>
<h3>David M. Chan, Sudheendra Vijayanarasimhan, David A. Ross, John Canny</h3>
<p>Automatic video captioning aims to train models to generate text descriptions
for all segments in a video, however, the most effective approaches require
large amounts of manual annotation which is slow and expensive. Active learning
is a promising way to efficiently build a training set for video captioning
tasks while reducing the need to manually label uninformative examples. In this
work we both explore various active learning approaches for automatic video
captioning and show that a cluster-regularized ensemble strategy provides the
best active learning approach to efficiently gather training sets for video
captioning. We evaluate our approaches on the MSR-VTT and LSMDC datasets using
both transformer and LSTM based captioning models and show that our novel
strategy can achieve high performance while using up to 60% fewer training data
than the strong state of the art baselines.
</p>
<a href="http://arxiv.org/abs/2007.13913" target="_blank">arXiv:2007.13913</a> [<a href="http://arxiv.org/pdf/2007.13913" target="_blank">pdf</a>]

<h2>Generalization Guarantees for Imitation Learning. (arXiv:2008.01913v2 [cs.RO] UPDATED)</h2>
<h3>Allen Z. Ren, Sushant Veer, Anirudha Majumdar</h3>
<p>Control policies from imitation learning can often fail to generalize to
novel environments due to imperfect demonstrations or the inability of
imitation learning algorithms to accurately infer the expert's policies. In
this paper, we present rigorous generalization guarantees for imitation
learning by leveraging the Probably Approximately Correct (PAC)-Bayes framework
to provide upper bounds on the expected cost of policies in novel environments.
We propose a two-stage training method where a latent policy distribution is
first embedded with multi-modal expert behavior using a conditional variational
autoencoder, and then "fine-tuned" in new training environments to explicitly
optimize the generalization bound. We demonstrate strong generalization bounds
and their tightness relative to empirical performance in simulation for (i)
grasping diverse mugs, (ii) planar pushing with visual feedback, and (iii)
vision-based indoor navigation, as well as through hardware experiments for the
two manipulation tasks.
</p>
<a href="http://arxiv.org/abs/2008.01913" target="_blank">arXiv:2008.01913</a> [<a href="http://arxiv.org/pdf/2008.01913" target="_blank">pdf</a>]

<h2>Unsupervised Feature Learning by Cross-Level Instance-Group Discrimination. (arXiv:2008.03813v4 [cs.CV] UPDATED)</h2>
<h3>Xudong Wang, Ziwei Liu, Stella X. Yu</h3>
<p>Unsupervised feature learning has made great strides with contrastive
learning based on instance discrimination and invariant mapping, as benchmarked
on curated class-balanced datasets. However, natural data could be highly
correlated and long-tail distributed. Natural between-instance similarity
conflicts with the presumed instance distinction, causing unstable training and
poor performance.

Our idea is to discover and integrate between-instance similarity into
contrastive learning, not directly by instance grouping, but by cross-level
discrimination (CLD) between instances and local instance groups. While
invariant mapping of each instance is imposed by attraction within its
augmented views, between-instance similarity emerges from common repulsion
against instance groups.

Our batch-wise and cross-view comparisons also greatly improve the
positive/negative sample ratio of contrastive learning and achieve better
invariant mapping. To effect both grouping and discrimination objectives, we
impose them on features separately derived from a shared representation. In
addition, we propose normalized projection heads and unsupervised
hyper-parameter tuning for the first time.

Our extensive experimentation demonstrates that CLD is a lean and powerful
add-on to existing methods (e.g., NPID, MoCo, InfoMin, BYOL) on highly
correlated, long-tail, or balanced datasets. It not only achieves new
state-of-the-art on self-supervision, semi-supervision, and transfer learning
benchmarks, but also beats MoCo v2 and SimCLR on every reported performance
attained with a much larger compute. CLD effectively extends unsupervised
learning to natural data and brings it closer to real-world applications.
</p>
<a href="http://arxiv.org/abs/2008.03813" target="_blank">arXiv:2008.03813</a> [<a href="http://arxiv.org/pdf/2008.03813" target="_blank">pdf</a>]

<h2>NASirt: AutoML based learning with instance-level complexity information. (arXiv:2008.11846v2 [cs.LG] UPDATED)</h2>
<h3>Habib Asseiss Neto, Ronnie C. O. Alves, Sergio V. A. Campos</h3>
<p>Designing adequate and precise neural architectures is a challenging task,
often done by highly specialized personnel. AutoML is a machine learning field
that aims to generate good performing models in an automated way. Spectral data
such as those obtained from biological analysis have generally a lot of
important information, and these data are specifically well suited to
Convolutional Neural Networks (CNN) due to their image-like shape. In this work
we present NASirt, an AutoML methodology based on Neural Architecture Search
(NAS) that finds high accuracy CNN architectures for spectral datasets. The
proposed methodology relies on the Item Response Theory (IRT) for obtaining
characteristics from an instance level, such as discrimination and difficulty,
and it is able to define a rank of top performing submodels. Several
experiments are performed in order to demonstrate the methodology's performance
with different spectral datasets. Accuracy results are compared to other
benchmarks methods, such as a high performing, manually crafted CNN and the
Auto-Keras AutoML tool. The results show that our method performs, in most
cases, better than the benchmarks, achieving average accuracy as high as
97.40%.
</p>
<a href="http://arxiv.org/abs/2008.11846" target="_blank">arXiv:2008.11846</a> [<a href="http://arxiv.org/pdf/2008.11846" target="_blank">pdf</a>]

<h2>Aggregating Long-Term Context for Learning Laparoscopic and Robot-Assisted Surgical Workflows. (arXiv:2009.00681v3 [cs.CV] UPDATED)</h2>
<h3>Yutong Ban, Guy Rosman, Thomas Ward, Daniel Hashimoto, Taisei Kondo, Hidekazu Iwaki, Ozanan Meireles, Daniela Rus</h3>
<p>Analyzing surgical workflow is crucial for surgical assistance robots to
understand surgeries. With the understanding of the complete surgical workflow,
the robots are able to assist the surgeons in intra-operative events, such as
by giving a warning when the surgeon is entering specific keys or high-risk
phases. Deep learning techniques have recently been widely applied to
recognizing surgical workflows. Many of the existing temporal neural network
models are limited in their capability to handle long-term dependencies in the
data, instead, relying upon the strong performance of the underlying per-frame
visual models. We propose a new temporal network structure that leverages
task-specific network representation to collect long-term sufficient statistics
that are propagated by a sufficient statistics model (SSM). We implement our
approach within an LSTM backbone for the task of surgical phase recognition and
explore several choices for propagated statistics. We demonstrate superior
results over existing and novel state-of-the-art segmentation techniques on two
laparoscopic cholecystectomy datasets: the publicly available Cholec80 dataset
and MGH100, a novel dataset with more challenging and clinically meaningful
segment labels.
</p>
<a href="http://arxiv.org/abs/2009.00681" target="_blank">arXiv:2009.00681</a> [<a href="http://arxiv.org/pdf/2009.00681" target="_blank">pdf</a>]

<h2>Counterfactual Explanations & Adversarial Examples -- Common Grounds, Essential Differences, and Potential Transfers. (arXiv:2009.05487v2 [cs.AI] UPDATED)</h2>
<h3>Timo Freiesleben</h3>
<p>The same optimization problem underlies counterfactual explanations (CEs) and
adversarial examples (AEs). While this is well known, the relationship between
the two at the conceptual level remains unclear. The present paper provides
exactly the missing conceptual link. We compare CEs and AEs with respect to
their philosophical basis, aims, and modeling techniques. We argue that CEs are
a more general object-class than AEs. In particular, we introduce the
conceptual distinction between feasible and contesting CEs and show that AEs
correspond to the latter.
</p>
<a href="http://arxiv.org/abs/2009.05487" target="_blank">arXiv:2009.05487</a> [<a href="http://arxiv.org/pdf/2009.05487" target="_blank">pdf</a>]

<h2>Measuring Information Transfer in Neural Networks. (arXiv:2009.07624v2 [cs.LG] UPDATED)</h2>
<h3>Xiao Zhang, Xingjian Li, Dejing Dou, Ji Wu</h3>
<p>Quantifying the information content in a neural network model is essentially
estimating the model's Kolmogorov complexity. Recent success of prequential
coding on neural networks points to a promising path of deriving an efficient
description length of a model. We propose a practical measure of the
generalizable information in a neural network model based on prequential
coding, which we term Information Transfer ($L_{IT}$). Theoretically, $L_{IT}$
is an estimation of the generalizable part of a model's information content. In
experiments, we show that $L_{IT}$ is consistently correlated with
generalizable information and can be used as a measure of patterns or
"knowledge" in a model or a dataset. Consequently, $L_{IT}$ can serve as a
useful analysis tool in deep learning. In this paper, we apply $L_{IT}$ to
compare and dissect information in datasets, evaluate representation models in
transfer learning, and analyze catastrophic forgetting and continual learning
algorithms. $L_{IT}$ provides an information perspective which helps us
discover new insights into neural network learning.
</p>
<a href="http://arxiv.org/abs/2009.07624" target="_blank">arXiv:2009.07624</a> [<a href="http://arxiv.org/pdf/2009.07624" target="_blank">pdf</a>]

<h2>FLAME: Differentially Private Federated Learning in the Shuffle Model. (arXiv:2009.08063v2 [cs.LG] UPDATED)</h2>
<h3>Ruixuan Liu, Yang Cao, Hong Chen, Ruoyang Guo, Masatoshi Yoshikawa</h3>
<p>Differentially private federated learning has been intensively studied. The
current works are mainly based on the \textit{curator model} or \textit{local
model} of differential privacy. However, both of them have pros and cons. The
curator model allows greater accuracy but requires a trusted analyzer. In the
local model where users randomize local data before sending them to the
analyzer, a trusted analyzer is not required, but the accuracy is limited. In
this work, by leveraging the \textit{privacy amplification} effect in the
recently proposed shuffle model of differential privacy, we achieve the best of
two worlds, i.e., accuracy in the curator model and strong privacy without
relying on any trusted party. We first propose an FL framework in the shuffle
model and a simple protocol (SS-Simple) extended from existing work. We find
that SS-Simple only provides an insufficient privacy amplification effect in FL
since the dimension of the model parameter is quite large. To solve this
challenge, we propose an enhanced protocol (SS-Double) to increase the privacy
amplification effect by subsampling. Furthermore, for boosting the utility when
the model size is greater than the user population, we propose an advanced
protocol (SS-Topk) with gradient sparsification techniques. We also provide
theoretical analysis and numerical evaluations of the privacy amplification of
the proposed protocols. Experiments on real-world datasets validate that
SS-Topk improves the testing accuracy by 60.7\% than the local model based FL.
We highlight the observation that SS-Topk even can improve by 33.94\% accuracy
than the curator model based FL without any trusted party. Compared with
non-private FL, our protocol SS-Topk only lose 1.48\% accuracy under $(4.696,
10^{-5})$-DP.
</p>
<a href="http://arxiv.org/abs/2009.08063" target="_blank">arXiv:2009.08063</a> [<a href="http://arxiv.org/pdf/2009.08063" target="_blank">pdf</a>]

<h2>Is Each Layer Non-trivial in CNN?. (arXiv:2009.09938v2 [cs.CV] UPDATED)</h2>
<h3>Wei Wang, Yanjie Zhu, Zhuoxu Cui, Dong Liang</h3>
<p>Convolutional neural network (CNN) models have achieved great success in many
fields. With the advent of ResNet, networks used in practice are getting deeper
and wider. However, is each layer non-trivial in networks? To answer this
question, we trained a network on the training set, then we replace the network
convolution kernels with zeros and test the result models on the test set. We
compared experimental results with baseline and showed that we can reach
similar or even the same performances. Although convolution kernels are the
cores of networks, we demonstrate that some of them are trivial and regular in
ResNet.
</p>
<a href="http://arxiv.org/abs/2009.09938" target="_blank">arXiv:2009.09938</a> [<a href="http://arxiv.org/pdf/2009.09938" target="_blank">pdf</a>]

<h2>MQTransformer: Multi-Horizon Forecasts with Context Dependent and Feedback-Aware Attention. (arXiv:2009.14799v3 [cs.LG] UPDATED)</h2>
<h3>Carson Eisenach, Yagna Patel, Dhruv Madeka</h3>
<p>Recent advances in neural forecasting have produced major improvements in
accuracy for probabilistic demand prediction. In this work, we propose novel
improvements to the current state of the art by incorporating changes inspired
by recent advances in Transformer architectures for Natural Language
Processing. We develop a novel decoder-encoder attention for context-alignment,
improving forecasting accuracy by allowing the network to study its own history
based on the context for which it is producing a forecast. We also present a
novel positional encoding that allows the neural network to learn
context-dependent seasonality functions as well as arbitrary holiday distances.
Finally we show that the current state of the art MQ-Forecaster (Wen et al.,
2017) models display excess variability by failing to leverage previous errors
in the forecast to improve accuracy. We propose a novel decoder-self attention
scheme for forecasting that produces significant improvements in the excess
variation of the forecast.
</p>
<a href="http://arxiv.org/abs/2009.14799" target="_blank">arXiv:2009.14799</a> [<a href="http://arxiv.org/pdf/2009.14799" target="_blank">pdf</a>]

<h2>Semi-Supervised Learning for Multi-Task Scene Understanding by Neural Graph Consensus. (arXiv:2010.01086v2 [cs.CV] UPDATED)</h2>
<h3>Marius Leordeanu, Mihai Pirvu, Dragos Costea, Alina Marcu, Emil Slusanschi, Rahul Sukthankar</h3>
<p>We address the challenging problem of semi-supervised learning in the context
of multiple visual interpretations of the world by finding consensus in a graph
of neural networks. Each graph node is a scene interpretation layer, while each
edge is a deep net that transforms one layer at one node into another from a
different node. During the supervised phase edge networks are trained
independently. During the next unsupervised stage edge nets are trained on the
pseudo-ground truth provided by consensus among multiple paths that reach the
nets' start and end nodes. These paths act as ensemble teachers for any given
edge and strong consensus is used for high-confidence supervisory signal. The
unsupervised learning process is repeated over several generations, in which
each edge becomes a "student" and also part of different ensemble "teachers"
for training other students. By optimizing such consensus between different
paths, the graph reaches consistency and robustness over multiple
interpretations and generations, in the face of unknown labels. We give
theoretical justifications of the proposed idea and validate it on a large
dataset. We show how prediction of different representations such as depth,
semantic segmentation, surface normals and pose from RGB input could be
effectively learned through self-supervised consensus in our graph. We also
compare to state-of-the-art methods for multi-task and semi-supervised learning
and show superior performance.
</p>
<a href="http://arxiv.org/abs/2010.01086" target="_blank">arXiv:2010.01086</a> [<a href="http://arxiv.org/pdf/2010.01086" target="_blank">pdf</a>]

<h2>Regularized Inverse Reinforcement Learning. (arXiv:2010.03691v2 [cs.LG] UPDATED)</h2>
<h3>Wonseok Jeon, Chen-Yang Su, Paul Barde, Thang Doan, Derek Nowrouzezahrai, Joelle Pineau</h3>
<p>Inverse Reinforcement Learning (IRL) aims to facilitate a learner's ability
to imitate expert behavior by acquiring reward functions that explain the
expert's decisions. Regularized IRL applies strongly convex regularizers to the
learner's policy in order to avoid the expert's behavior being rationalized by
arbitrary constant rewards, also known as degenerate solutions. We propose
tractable solutions, and practical methods to obtain them, for regularized IRL.
Current methods are restricted to the maximum-entropy IRL framework, limiting
them to Shannon-entropy regularizers, as well as proposing the solutions that
are intractable in practice. We present theoretical backing for our proposed
IRL method's applicability for both discrete and continuous controls,
empirically validating our performance on a variety of tasks.
</p>
<a href="http://arxiv.org/abs/2010.03691" target="_blank">arXiv:2010.03691</a> [<a href="http://arxiv.org/pdf/2010.03691" target="_blank">pdf</a>]

<h2>Impact of Thermal Throttling on Long-Term Visual Inference in a CPU-based Edge Device. (arXiv:2010.06291v2 [cs.CV] UPDATED)</h2>
<h3>Th&#xe9;o Benoit-Cattin, Delia Velasco-Montero, Jorge Fern&#xe1;ndez-Berni</h3>
<p>Many application scenarios of edge visual inference, e.g., robotics or
environmental monitoring, eventually require long periods of continuous
operation. In such periods, the processor temperature plays a critical role to
keep a prescribed frame rate. Particularly, the heavy computational load of
convolutional neural networks (CNNs) may lead to thermal throttling and hence
performance degradation in few seconds. In this paper, we report and analyze
the long-term performance of 80 different cases resulting from running 5 CNN
models on 4 software frameworks and 2 operating systems without and with active
cooling. This comprehensive study was conducted on a low-cost edge platform,
namely Raspberry Pi 4B (RPi4B), under stable indoor conditions. The results
show that hysteresis-based active cooling prevented thermal throttling in all
cases, thereby improving the throughput up to approximately 90% versus no
cooling. Interestingly, the range of fan usage during active cooling varied
from 33% to 65%. Given the impact of the fan on the power consumption of the
system as a whole, these results stress the importance of a suitable selection
of CNN model and software components. To assess the performance in outdoor
applications, we integrated an external temperature sensor with the RPi4B and
conducted a set of experiments with no active cooling in a wide interval of
ambient temperature, ranging from 22 {\deg}C to 36 {\deg}C. Variations up to
27.7% were measured with respect to the maximum throughput achieved in that
interval. This demonstrates that ambient temperature is a critical parameter in
case active cooling cannot be applied.
</p>
<a href="http://arxiv.org/abs/2010.06291" target="_blank">arXiv:2010.06291</a> [<a href="http://arxiv.org/pdf/2010.06291" target="_blank">pdf</a>]

<h2>Boosting One-Point Derivative-Free Online Optimization via Residual Feedback. (arXiv:2010.07378v3 [cs.LG] UPDATED)</h2>
<h3>Yan Zhang, Yi Zhou, Kaiyi Ji, Michael M. Zavlanos</h3>
<p>Zeroth-order optimization (ZO) typically relies on two-point feedback to
estimate the unknown gradient of the objective function. Nevertheless,
two-point feedback can not be used for online optimization of time-varying
objective functions, where only a single query of the function value is
possible at each time step. In this work, we propose a new one-point feedback
method for online optimization that estimates the objective function gradient
using the residual between two feedback points at consecutive time instants.
Moreover, we develop regret bounds for ZO with residual feedback for both
convex and nonconvex online optimization problems. Specifically, for both
deterministic and stochastic problems and for both Lipschitz and smooth
objective functions, we show that using residual feedback can produce gradient
estimates with much smaller variance compared to conventional one-point
feedback methods. As a result, our regret bounds are much tighter compared to
existing regret bounds for ZO with conventional one-point feedback, which
suggests that ZO with residual feedback can better track the optimizer of
online optimization problems. Additionally, our regret bounds rely on weaker
assumptions than those used in conventional one-point feedback methods.
Numerical experiments show that ZO with residual feedback significantly
outperforms existing one-point feedback methods also in practice.
</p>
<a href="http://arxiv.org/abs/2010.07378" target="_blank">arXiv:2010.07378</a> [<a href="http://arxiv.org/pdf/2010.07378" target="_blank">pdf</a>]

<h2>Auto Seg-Loss: Searching Metric Surrogates for Semantic Segmentation. (arXiv:2010.07930v2 [cs.CV] UPDATED)</h2>
<h3>Hao Li, Chenxin Tao, Xizhou Zhu, Xiaogang Wang, Gao Huang, Jifeng Dai</h3>
<p>Designing proper loss functions is essential in training deep networks.
Especially in the field of semantic segmentation, various evaluation metrics
have been proposed for diverse scenarios. Despite the success of the widely
adopted cross-entropy loss and its variants, the mis-alignment between the loss
functions and evaluation metrics degrades the network performance. Meanwhile,
manually designing loss functions for each specific metric requires expertise
and significant manpower. In this paper, we propose to automate the design of
metric-specific loss functions by searching differentiable surrogate losses for
each metric. We substitute the non-differentiable operations in the metrics
with parameterized functions, and conduct parameter search to optimize the
shape of loss surfaces. Two constraints are introduced to regularize the search
space and make the search efficient. Extensive experiments on PASCAL VOC and
Cityscapes demonstrate that the searched surrogate losses outperform the
manually designed loss functions consistently. The searched losses can
generalize well to other datasets and networks. Code shall be released.
</p>
<a href="http://arxiv.org/abs/2010.07930" target="_blank">arXiv:2010.07930</a> [<a href="http://arxiv.org/pdf/2010.07930" target="_blank">pdf</a>]

<h2>A combined full-reference image quality assessment approach based on convolutional activation maps. (arXiv:2010.09361v3 [cs.CV] UPDATED)</h2>
<h3>Domonkos Varga</h3>
<p>The goal of full-reference image quality assessment (FR-IQA) is to predict
the quality of an image as perceived by human observers with using its
pristine, reference counterpart. In this study, we explore a novel, combined
approach which predicts the perceptual quality of a distorted image by
compiling a feature vector from convolutional activation maps. More
specifically, a reference-distorted image pair is run through a pretrained
convolutional neural network and the activation maps are compared with a
traditional image similarity metric. Subsequently, the resulted feature vector
is mapped onto perceptual quality scores with the help of a trained support
vector regressor. A detailed parameter study is also presented in which the
design choices of the proposed method is reasoned. Furthermore, we study the
relationship between the amount of training images and the prediction
performance. Specifically, it is demonstrated that the proposed method can be
trained with few amount of data to reach high prediction performance. Our best
proposal - ActMapFeat - is compared to the state-of-the-art on six publicly
available benchmark IQA databases, such as KADID-10k, TID2013, TID2008, MDID,
CSIQ, and VCL-FER. Specifically, our method is able to significantly outperform
the state-of-the-art on these benchmark databases.
</p>
<a href="http://arxiv.org/abs/2010.09361" target="_blank">arXiv:2010.09361</a> [<a href="http://arxiv.org/pdf/2010.09361" target="_blank">pdf</a>]

<h2>Soft Jig-Driven Assembly Operations. (arXiv:2010.10843v2 [cs.RO] UPDATED)</h2>
<h3>Takuya Kiyokawa, Tatsuya Sakuma, Jun Takamatsu, Tsukasa Ogasawara</h3>
<p>To design a general-purpose assembly robot system that can handle objects of
various shapes, we propose a soft jig capable of deforming according to the
shape of assembly parts. The soft jig is based on a jamming gripper used for
robot manipulation as a general-purpose robotic gripper developed in the field
of soft robotics. The soft jig has a flexible membrane made of silicone, which
has a high friction, elongation, and contraction rate for keeping parts fixed.
The inside of the membrane is filled with glass beads to achieve a jamming
transition. The usability of the soft jig was evaluated from the viewpoint of
the versatility and fixing performance for various shapes and postures of parts
in assembly operations.
</p>
<a href="http://arxiv.org/abs/2010.10843" target="_blank">arXiv:2010.10843</a> [<a href="http://arxiv.org/pdf/2010.10843" target="_blank">pdf</a>]

<h2>Assembly Sequences Based on Multiple Criteria Against Products with Deformable Parts. (arXiv:2010.10846v2 [cs.RO] UPDATED)</h2>
<h3>Takuya Kiyokawa, Jun Takamatsu, Tsukasa Ogasawara</h3>
<p>This study investigates assembly sequence generation by considering two
tradeoff objectives: (1) insertion conditions and (2) degrees of constraints
among assembled parts. A multiobjective genetic algorithm is used to balance
these two objectives for planning robotic assembly. Furthermore, the method of
extracting part relation matrices including interference-free, insertion, and
degree of constraint matrices is extended for application to 3D computer-aided
design (CAD) models, including deformable parts. The interference of deformable
parts with other parts can be easily investigated by scaling models. A
simulation experiment was conducted using the proposed method, and the results
show the possibility of obtaining Pareto-optimal solutions of assembly
sequences for a 3D CAD model with 33 parts including a deformable part. This
approach can potentially be extended to handle various types of deformable
parts and to explore graspable sequences during assembly operations.
</p>
<a href="http://arxiv.org/abs/2010.10846" target="_blank">arXiv:2010.10846</a> [<a href="http://arxiv.org/pdf/2010.10846" target="_blank">pdf</a>]

<h2>Regularised Least-Squares Regression with Infinite-Dimensional Output Space. (arXiv:2010.10973v3 [stat.ML] UPDATED)</h2>
<h3>Junhyunng Park, Krikamol Muandet</h3>
<p>We present some learning theory results on vector-valued reproducing kernel
Hilbert space (RKHS) regression, where the input space is allowed to be
non-compact and the output space is a (possibly infinite-dimensional) Hilbert
space.
</p>
<a href="http://arxiv.org/abs/2010.10973" target="_blank">arXiv:2010.10973</a> [<a href="http://arxiv.org/pdf/2010.10973" target="_blank">pdf</a>]

<h2>A Policy Gradient Algorithm for Learning to Learn in Multiagent Reinforcement Learning. (arXiv:2011.00382v2 [cs.LG] UPDATED)</h2>
<h3>Dong-Ki Kim, Miao Liu, Matthew Riemer, Chuangchuang Sun, Marwa Abdulhai, Golnaz Habibi, Sebastian Lopez-Cot, Gerald Tesauro, Jonathan P. How</h3>
<p>A fundamental challenge in multiagent reinforcement learning is to learn
beneficial behaviors in a shared environment with other agents that are also
simultaneously learning. In particular, each agent perceives the environment as
effectively non-stationary due to the changing policies of other agents.
Moreover, each agent is itself constantly learning, leading to natural
nonstationarity in the distribution of experiences encountered. In this paper,
we propose a novel meta-multiagent policy gradient theorem that directly
accommodates for the non-stationary policy dynamics inherent to these
multiagent settings. This is achieved by modeling our gradient updates to
directly consider both an agent's own non-stationary policy dynamics and the
non-stationary policy dynamics of other agents interacting with it in the
environment. We find that our theoretically grounded approach provides a
general solution to the multiagent learning problem, which inherently combines
key aspects of previous state of the art approaches on this topic. We test our
method on several multiagent benchmarks and demonstrate a more efficient
ability to adapt to new agents as they learn than previous related approaches
across the spectrum of mixed incentive, competitive, and cooperative
environments.
</p>
<a href="http://arxiv.org/abs/2011.00382" target="_blank">arXiv:2011.00382</a> [<a href="http://arxiv.org/pdf/2011.00382" target="_blank">pdf</a>]

<h2>Toward a Generalization Metric for Deep Generative Models. (arXiv:2011.00754v2 [cs.LG] UPDATED)</h2>
<h3>Hoang Thanh-Tung, Truyen Tran</h3>
<p>Measuring the generalization capacity of Deep Generative Models (DGMs) is
difficult because of the curse of dimensionality. Evaluation metrics for DGMs
such as Inception Score, Fr\'echet Inception Distance, Precision-Recall, and
Neural Net Divergence try to estimate the distance between the generated
distribution and the target distribution using a polynomial number of samples.
These metrics are the target of researchers when designing new models. Despite
the claims, it is still unclear how well can they measure the generalization
capacity of a generative model. In this paper, we investigate the capacity of
these metrics in measuring the generalization capacity. We introduce a
framework for comparing the robustness of evaluation metrics. We show that
better scores in these metrics do not imply better generalization. They can be
fooled easily by a generator that memorizes a small subset of the training set.
We propose a fix to the NND metric to make it more robust to noise in the
generated data. Toward building a robust metric for generalization, we propose
to apply the Minimum Description Length principle to the problem of evaluating
DGMs. We develop an efficient method for estimating the complexity of
Generative Latent Variable Models (GLVMs). Experimental results show that our
metric can effectively detect training set memorization and distinguish GLVMs
of different generalization capacities.
</p>
<a href="http://arxiv.org/abs/2011.00754" target="_blank">arXiv:2011.00754</a> [<a href="http://arxiv.org/pdf/2011.00754" target="_blank">pdf</a>]

<h2>Hi-UCD: A Large-scale Dataset for Urban Semantic Change Detection in Remote Sensing Imagery. (arXiv:2011.03247v6 [cs.CV] UPDATED)</h2>
<h3>Shiqi Tian, Ailong Ma, Zhuo Zheng, Yanfei Zhong</h3>
<p>With the acceleration of the urban expansion, urban change detection (UCD),
as a significant and effective approach, can provide the change information
with respect to geospatial objects for dynamical urban analysis. However,
existing datasets suffer from three bottlenecks: (1) lack of high spatial
resolution images; (2) lack of semantic annotation; (3) lack of long-range
multi-temporal images. In this paper, we propose a large scale benchmark
dataset, termed Hi-UCD. This dataset uses aerial images with a spatial
resolution of 0.1 m provided by the Estonia Land Board, including three-time
phases, and semantically annotated with nine classes of land cover to obtain
the direction of ground objects change. It can be used for detecting and
analyzing refined urban changes. We benchmark our dataset using some classic
methods in binary and multi-class change detection. Experimental results show
that Hi-UCD is challenging yet useful. We hope the Hi-UCD can become a strong
benchmark accelerating future research.
</p>
<a href="http://arxiv.org/abs/2011.03247" target="_blank">arXiv:2011.03247</a> [<a href="http://arxiv.org/pdf/2011.03247" target="_blank">pdf</a>]

<h2>A Deep Neural Network for SSVEP-based Brain-Computer Interfaces. (arXiv:2011.08562v2 [cs.LG] UPDATED)</h2>
<h3>Osman Berke Guney, Muhtasham Oblokulov, Huseyin Ozkan</h3>
<p>Target identification in brain-computer interface (BCI) spellers refers to
the electroencephalogram (EEG) classification for predicting the target
character that the subject intends to spell. When the visual stimulus of each
character is tagged with a distinct frequency, the EEG records steady-state
visually evoked potentials (SSVEP) whose spectrum is dominated by the harmonics
of the target frequency. In this setting, we address the target identification
and propose a novel deep neural network (DNN) architecture. The proposed DNN
processes the multi-channel SSVEP with convolutions across the sub-bands of
harmonics, channels, time, and classifies at the fully connected layer. We test
with two publicly available large scale (the benchmark and BETA) datasets
consisting of in total 105 subjects with 40 characters. Our first stage
training learns a global model by exploiting the statistical commonalities
among all subjects, and the second stage fine tunes to each subject separately
by exploiting the individualities. Our DNN strongly outperforms the
state-of-the-art on both datasets, by achieving impressive information transfer
rates 265.23 bits/min and 196.59 bits/min, respectively, with only 0.4 seconds
of stimulation. To our best knowledge, our rates are the highest ever reported
performance results on these datasets. The code is available for
reproducibility at https://github.com/osmanberke/Deep-SSVEP-BCI.
</p>
<a href="http://arxiv.org/abs/2011.08562" target="_blank">arXiv:2011.08562</a> [<a href="http://arxiv.org/pdf/2011.08562" target="_blank">pdf</a>]

<h2>Privileged Knowledge Distillation for Online Action Detection. (arXiv:2011.09158v2 [cs.CV] UPDATED)</h2>
<h3>Peisen Zhao, Lingxi Xie, Ya Zhang, Yanfeng Wang, Qi Tian</h3>
<p>Online Action Detection (OAD) in videos is proposed as a per-frame labeling
task to address the real-time prediction tasks that can only obtain the
previous and current video frames. This paper presents a novel
learning-with-privileged based framework for online action detection where the
future frames only observable at the training stages are considered as a form
of privileged information. Knowledge distillation is employed to transfer the
privileged information from the offline teacher to the online student. We note
that this setting is different from conventional KD because the difference
between the teacher and student models mostly lies in input data rather than
the network architecture. We propose Privileged Knowledge Distillation (PKD)
which (i) schedules a curriculum learning procedure and (ii) inserts auxiliary
nodes to the student model, both for shrinking the information gap and
improving learning performance. Compared to other OAD methods that explicitly
predict future frames, our approach avoids learning unpredictable unnecessary
yet inconsistent visual contents and achieves state-of-the-art accuracy on two
popular OAD benchmarks, TVSeries and THUMOS14.
</p>
<a href="http://arxiv.org/abs/2011.09158" target="_blank">arXiv:2011.09158</a> [<a href="http://arxiv.org/pdf/2011.09158" target="_blank">pdf</a>]

<h2>Geography-Aware Self-Supervised Learning. (arXiv:2011.09980v5 [cs.CV] UPDATED)</h2>
<h3>Kumar Ayush, Burak Uzkent, Chenlin Meng, Kumar Tanmay, Marshall Burke, David Lobell, Stefano Ermon</h3>
<p>Contrastive learning methods have significantly narrowed the gap between
supervised and unsupervised learning on computer vision tasks. In this paper,
we explore their application to remote sensing, where unlabeled data is often
abundant but labeled data is scarce. We first show that due to their different
characteristics, a non-trivial gap persists between contrastive and supervised
learning on standard benchmarks. To close the gap, we propose novel training
methods that exploit the spatiotemporal structure of remote sensing data. We
leverage spatially aligned images over time to construct temporal positive
pairs in contrastive learning and geo-location to design pre-text tasks. Our
experiments show that our proposed method closes the gap between contrastive
and supervised learning on image classification, object detection and semantic
segmentation for remote sensing and other geo-tagged image datasets.
</p>
<a href="http://arxiv.org/abs/2011.09980" target="_blank">arXiv:2011.09980</a> [<a href="http://arxiv.org/pdf/2011.09980" target="_blank">pdf</a>]

<h2>SHOT-VAE: Semi-supervised Deep Generative Models With Label-aware ELBO Approximations. (arXiv:2011.10684v3 [cs.LG] UPDATED)</h2>
<h3>Hao-Zhe Feng, Kezhi Kong, Minghao Chen, Tianye Zhang, Minfeng Zhu, Wei Chen</h3>
<p>Semi-supervised variational autoencoders (VAEs) have obtained strong results,
but have also encountered the challenge that good ELBO values do not always
imply accurate inference results. In this paper, we investigate and propose two
causes of this problem: (1) The ELBO objective cannot utilize the label
information directly. (2) A bottleneck value exists and continuing to optimize
ELBO after this value will not improve inference accuracy. On the basis of the
experiment results, we propose SHOT-VAE to address these problems without
introducing additional prior knowledge. The SHOT-VAE offers two contributions:
(1) A new ELBO approximation named smooth-ELBO that integrates the label
predictive loss into ELBO. (2) An approximation based on optimal interpolation
that breaks the ELBO value bottleneck by reducing the margin between ELBO and
the data likelihood. The SHOT-VAE achieves good performance with a 25.30% error
rate on CIFAR-100 with 10k labels and reduces the error rate to 6.11% on
CIFAR-10 with 4k labels.
</p>
<a href="http://arxiv.org/abs/2011.10684" target="_blank">arXiv:2011.10684</a> [<a href="http://arxiv.org/pdf/2011.10684" target="_blank">pdf</a>]

<h2>Distributed Reinforcement Learning is a Dataflow Problem. (arXiv:2011.12719v2 [cs.LG] UPDATED)</h2>
<h3>Eric Liang, Zhanghao Wu, Michael Luo, Sven Mika, Ion Stoica</h3>
<p>Researchers and practitioners in the field of reinforcement learning (RL)
frequently leverage parallel computation, which has led to a plethora of new
algorithms and systems in the last few years. In this paper, we re-examine the
challenges posed by distributed RL and try to view it through the lens of an
old idea: distributed dataflow. We show that viewing RL as a dataflow problem
leads to highly composable and performant implementations. We propose AnonFlow,
a hybrid actor-dataflow programming model for distributed RL, and validate its
practicality by porting the full suite of algorithms in AnonLib, a
widely-adopted distributed RL library.
</p>
<a href="http://arxiv.org/abs/2011.12719" target="_blank">arXiv:2011.12719</a> [<a href="http://arxiv.org/pdf/2011.12719" target="_blank">pdf</a>]

<h2>StyleSpace Analysis: Disentangled Controls for StyleGAN Image Generation. (arXiv:2011.12799v2 [cs.CV] UPDATED)</h2>
<h3>Zongze Wu, Dani Lischinski, Eli Shechtman</h3>
<p>We explore and analyze the latent style space of StyleGAN2, a
state-of-the-art architecture for image generation, using models pretrained on
several different datasets. We first show that StyleSpace, the space of
channel-wise style parameters, is significantly more disentangled than the
other intermediate latent spaces explored by previous works. Next, we describe
a method for discovering a large collection of style channels, each of which is
shown to control a distinct visual attribute in a highly localized and
disentangled manner. Third, we propose a simple method for identifying style
channels that control a specific attribute, using a pretrained classifier or a
small number of example images. Manipulation of visual attributes via these
StyleSpace controls is shown to be better disentangled than via those proposed
in previous works. To show this, we make use of a newly proposed Attribute
Dependency metric. Finally, we demonstrate the applicability of StyleSpace
controls to the manipulation of real images. Our findings pave the way to
semantically meaningful and well-disentangled image manipulations via simple
and intuitive interfaces.
</p>
<a href="http://arxiv.org/abs/2011.12799" target="_blank">arXiv:2011.12799</a> [<a href="http://arxiv.org/pdf/2011.12799" target="_blank">pdf</a>]

<h2>Right for the Right Concept: Revising Neuro-Symbolic Concepts by Interacting with their Explanations. (arXiv:2011.12854v2 [cs.LG] UPDATED)</h2>
<h3>Wolfgang Stammer, Patrick Schramowski, Kristian Kersting</h3>
<p>Most explanation methods in deep learning map importance estimates for a
model's prediction back to the original input space. These "visual"
explanations are often insufficient, as the model's actual concept remains
elusive. Moreover, without insights into the model's semantic concept, it is
difficult -- if not impossible -- to intervene on the model's behavior via its
explanations, called Explanatory Interactive Learning. Consequently, we propose
to intervene on a Neuro-Symbolic scene representation, which allows one to
revise the model on the semantic level, e.g. "never focus on the color to make
your decision". We compiled a novel confounded visual scene data set, the
CLEVR-Hans data set, capturing complex compositions of different objects. The
results of our experiments on CLEVR-Hans demonstrate that our semantic
explanations, i.e. compositional explanations at a per-object level, can
identify confounders that are not identifiable using "visual" explanations
only. More importantly, feedback on this semantic level makes it possible to
revise the model from focusing on these confounding factors.
</p>
<a href="http://arxiv.org/abs/2011.12854" target="_blank">arXiv:2011.12854</a> [<a href="http://arxiv.org/pdf/2011.12854" target="_blank">pdf</a>]

<h2>RELLIS-3D Dataset: Data, Benchmarks and Analysis. (arXiv:2011.12954v2 [cs.CV] UPDATED)</h2>
<h3>Peng Jiang, Philip Osteen, Maggie Wigness, Srikanth Saripalli</h3>
<p>Semantic scene understanding is crucial for robust and safe autonomous
navigation, particularly so in off-road environments. Recent deep learning
advances for 3D semantic segmentation rely heavily on large sets of training
data, however existing autonomy datasets either represent urban environments or
lack multimodal off-road data. We fill this gap with RELLIS-3D, a multimodal
dataset collected in an off-road environment, which contains annotations for
13,556 LiDAR scans and 6,235 images. The data was collected on the Rellis
Campus of Texas A&amp;M University, and presents challenges to existing algorithms
related to class imbalance and environmental topography. Additionally, we
evaluate the current state of the art deep learning semantic segmentation
models on this dataset. Experimental results show that RELLIS-3D presents
challenges for algorithms designed for segmentation in urban environments. This
novel dataset provides the resources needed by researchers to continue to
develop more advanced algorithms and investigate new research directions to
enhance autonomous navigation in off-road environments. RELLIS-3D will be
published at https://github.com/unmannedlab/RELLIS-3D.
</p>
<a href="http://arxiv.org/abs/2011.12954" target="_blank">arXiv:2011.12954</a> [<a href="http://arxiv.org/pdf/2011.12954" target="_blank">pdf</a>]

<h2>A Sheaf and Topology Approach to Generating Local Branch Numbers in Digital Images. (arXiv:2011.13580v2 [cs.CV] UPDATED)</h2>
<h3>Chuan-Shen Hu, Yu-Min Chung</h3>
<p>This paper concerns a theoretical approach that combines topological data
analysis (TDA) and sheaf theory. Topological data analysis, a rising field in
mathematics and computer science, concerns the shape of the data and has been
proven effective in many scientific disciplines. Sheaf theory, a mathematics
subject in algebraic geometry, provides a framework for describing the local
consistency in geometric objects. Persistent homology (PH) is one of the main
driving forces in TDA, and the idea is to track changes of geometric objects at
different scales. The persistence diagram (PD) summarizes the information of PH
in the form of a multi-set. While PD provides useful information about the
underlying objects, it lacks fine relations about the local consistency of
specific pairs of generators in PD, such as the merging relation between two
connected components in the PH. The sheaf structure provides a novel point of
view for describing the merging relation of local objects in PH. It is the goal
of this paper to establish a theoretic framework that utilizes the sheaf theory
to uncover finer information from the PH. We also show that the proposed theory
can be applied to identify the branch numbers of local objects in digital
images.
</p>
<a href="http://arxiv.org/abs/2011.13580" target="_blank">arXiv:2011.13580</a> [<a href="http://arxiv.org/pdf/2011.13580" target="_blank">pdf</a>]

<h2>Progressively Volumetrized Deep Generative Models for Data-Efficient Contextual Learning of MR Image Recovery. (arXiv:2011.13913v2 [cs.CV] UPDATED)</h2>
<h3>Mahmut Yurt, Muzaffer &#xd6;zbey, Salman Ul Hassan Dar, Berk T&#x131;naz, Kader Karl&#x131; O&#x11f;uz, Tolga &#xc7;ukur</h3>
<p>Magnetic resonance imaging (MRI) offers the flexibility to image a given
anatomic volume under a multitude of tissue contrasts. Yet, scan time
considerations put stringent limits on the quality and diversity of MRI data.
The gold-standard approach to alleviate this limitation is to recover
high-quality images from data undersampled across various dimensions such as
the Fourier domain or contrast sets. A central divide among recovery methods is
whether the anatomy is processed per volume or per cross-section. Volumetric
models offer enhanced capture of global contextual information, but they can
suffer from suboptimal learning due to elevated model complexity.
Cross-sectional models with lower complexity offer improved learning behavior,
yet they ignore contextual information across the longitudinal dimension of the
volume. Here, we introduce a novel data-efficient progressively volumetrized
generative model (ProvoGAN) that decomposes complex volumetric image recovery
tasks into a series of simpler cross-sectional tasks across individual
rectilinear dimensions. ProvoGAN effectively captures global context and
recovers fine-structural details across all dimensions, while maintaining low
model complexity and data-efficiency advantages of cross-sectional models.
Comprehensive demonstrations on mainstream MRI reconstruction and synthesis
tasks show that ProvoGAN yields superior performance to state-of-the-art
volumetric and cross-sectional models.
</p>
<a href="http://arxiv.org/abs/2011.13913" target="_blank">arXiv:2011.13913</a> [<a href="http://arxiv.org/pdf/2011.13913" target="_blank">pdf</a>]

<h2>AdaGrasp: Learning an Adaptive Gripper-Aware Grasping Policy. (arXiv:2011.14206v2 [cs.RO] UPDATED)</h2>
<h3>Zhenjia Xu, Beichun Qi, Shubham Agrawal, Shuran Song</h3>
<p>This paper aims to improve robots' versatility and adaptability by allowing
them to use a large variety of end-effector tools and quickly adapt to new
tools. We propose AdaGrasp, a method to learn a single grasping policy that
generalizes to novel grippers. By training on a large collection of grippers,
our algorithm is able to acquire generalizable knowledge of how different
grippers should be used in various tasks. Given a visual observation of the
scene and the gripper, AdaGrasp infers the possible grasping poses and their
grasp scores by computing the cross convolution between the shape encodings of
the input gripper and scene. Intuitively, this cross convolution operation can
be considered as an efficient way of exhaustively matching the scene geometry
with gripper geometry under different grasp poses (i.e., translations and
orientations), where a good "match" of 3D geometry will lead to a successful
grasp. We validate our methods in both simulation and real-world environment.
Our experiment shows that AdaGrasp significantly outperforms the existing
multi-gripper grasping policy method, especially when handling cluttered
environments and partial observations. Video is available at
https://youtu.be/MUawdWnQDyQ
</p>
<a href="http://arxiv.org/abs/2011.14206" target="_blank">arXiv:2011.14206</a> [<a href="http://arxiv.org/pdf/2011.14206" target="_blank">pdf</a>]

<h2>Using dynamical quantization to perform split attempts in online tree regressors. (arXiv:2012.00083v2 [cs.LG] UPDATED)</h2>
<h3>Saulo Martiello Mastelini, Andre Carlos Ponce de Leon Ferreira de Carvalho</h3>
<p>A central aspect of online decision tree solutions is evaluating the incoming
data and enabling model growth. For such, trees much deal with different kinds
of input features and partition them to learn from the data. Numerical features
are no exception, and they pose additional challenges compared to other kinds
of features, as there is no trivial strategy to choose the best point to make a
split decision. The problem is even more challenging in regression tasks
because both the features and the target are continuous. Typical online
solutions evaluate and store all the points monitored between split attempts,
which goes against the constraints posed in real-time applications. In this
paper, we introduce the Quantization Observer (QO), a simple yet effective
hashing-based algorithm to monitor and evaluate split point candidates in
numerical features for online tree regressors. QO can be easily integrated into
incremental decision trees, such as Hoeffding Trees, and it has a monitoring
cost of $O(1)$ per instance and sub-linear cost to evaluate split candidates.
Previous solutions had a $O(\log n)$ cost per insertion (in the best case) and
a linear cost to evaluate split points. Our extensive experimental setup
highlights QO's effectiveness in providing accurate split point suggestions
while spending much less memory and processing time than its competitors.
</p>
<a href="http://arxiv.org/abs/2012.00083" target="_blank">arXiv:2012.00083</a> [<a href="http://arxiv.org/pdf/2012.00083" target="_blank">pdf</a>]

<h2>Pre-Trained Image Processing Transformer. (arXiv:2012.00364v2 [cs.CV] UPDATED)</h2>
<h3>Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu, Wen Gao</h3>
<p>As the computing power of modern hardware is increasing strongly, pre-trained
deep learning models (e.g., BERT, GPT-3) learned on large-scale datasets have
shown their effectiveness over conventional methods. The big progress is mainly
contributed to the representation ability of transformer and its variant
architectures. In this paper, we study the low-level computer vision task
(e.g., denoising, super-resolution and deraining) and develop a new pre-trained
model, namely, image processing transformer (IPT). To maximally excavate the
capability of transformer, we present to utilize the well-known ImageNet
benchmark for generating a large amount of corrupted image pairs. The IPT model
is trained on these images with multi-heads and multi-tails. In addition, the
contrastive learning is introduced for well adapting to different image
processing tasks. The pre-trained model can therefore efficiently employed on
desired task after fine-tuning. With only one pre-trained model, IPT
outperforms the current state-of-the-art methods on various low-level
benchmarks.
</p>
<a href="http://arxiv.org/abs/2012.00364" target="_blank">arXiv:2012.00364</a> [<a href="http://arxiv.org/pdf/2012.00364" target="_blank">pdf</a>]

<h2>Proceedings of NeurIPS 2019 Workshop on Artificial Intelligence for Humanitarian Assistance and Disaster Response. (arXiv:2012.01022v2 [cs.AI] UPDATED)</h2>
<h3>Ritwik Gupta, Eric T. Heim</h3>
<p>These are the "proceedings" of the 1st AI + HADR workshop which was held in
Vancouver, Canada on December 13, 2019 as part of the Neural Information
Processing Systems conference. These are non-archival and serve solely as a
collation of all the papers accepted to the workshop.
</p>
<a href="http://arxiv.org/abs/2012.01022" target="_blank">arXiv:2012.01022</a> [<a href="http://arxiv.org/pdf/2012.01022" target="_blank">pdf</a>]

