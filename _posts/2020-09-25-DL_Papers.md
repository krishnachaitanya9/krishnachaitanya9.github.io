---
title: Latest Deep Learning Papers
date: 2020-12-12 21:00:01 +0800
featured-img: DL-logo
categories: [Deep_Learning]
tags: [Deep_Learning]
mathjax: Yes
---

<h1>Your arXiv Feed (193 Articles)</h1>
<h2>ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation. (arXiv:2012.05258v1 [cs.CV])</h2>
<h3>Siyuan Qiao, Yukun Zhu, Hartwig Adam, Alan Yuille, Liang-Chieh Chen</h3>
<p>In this paper, we present ViP-DeepLab, a unified model attempting to tackle
the long-standing and challenging inverse projection problem in vision, which
we model as restoring the point clouds from perspective image sequences while
providing each point with instance-level semantic interpretations. Solving this
problem requires the vision models to predict the spatial location, semantic
class, and temporally consistent instance label for each 3D point. ViP-DeepLab
approaches it by jointly performing monocular depth estimation and video
panoptic segmentation. We name this joint task as Depth-aware Video Panoptic
Segmentation, and propose a new evaluation metric along with two derived
datasets for it, which will be made available to the public. On the individual
sub-tasks, ViP-DeepLab also achieves state-of-the-art results, outperforming
previous methods by 5.1% VPQ on Cityscapes-VPS, ranking 1st on the KITTI
monocular depth estimation benchmark, and 1st on KITTI MOTS pedestrian. The
datasets and the evaluation codes are made publicly available.
</p>
<a href="http://arxiv.org/abs/2012.05258" target="_blank">arXiv:2012.05258</a> [<a href="http://arxiv.org/pdf/2012.05258" target="_blank">pdf</a>]

<h2>On the Lattice of Conceptual Measurements. (arXiv:2012.05267v1 [cs.AI])</h2>
<h3>Tom Hanika, Johannes Hirth</h3>
<p>We present a novel approach for data set scaling based on scale-measures from
formal concept analysis, i.e., continuous maps between closure systems, and
derive a canonical representation. Moreover, we prove said scale-measures are
lattice ordered with respect to the closure systems. This enables exploring the
set of scale-measures through by the use of meet and join operations.
Furthermore we show that the lattice of scale-measures is isomorphic to the
lattice of sub-closure systems that arises from the original data. Finally, we
provide another representation of scale-measures using propositional logic in
terms of data set features. Our theoretical findings are discussed by means of
examples.
</p>
<a href="http://arxiv.org/abs/2012.05267" target="_blank">arXiv:2012.05267</a> [<a href="http://arxiv.org/pdf/2012.05267" target="_blank">pdf</a>]

<h2>Hard and Soft EM in Bayesian Network Learning from Incomplete Data. (arXiv:2012.05269v1 [stat.ML])</h2>
<h3>Andrea Ruggieri, Francesco Stranieri, Fabio Stella, Marco Scutari</h3>
<p>Incomplete data are a common feature in many domains, from clinical trials to
industrial applications. Bayesian networks (BNs) are often used in these
domains because of their graphical and causal interpretations. BN parameter
learning from incomplete data is usually implemented with the
Expectation-Maximisation algorithm (EM), which computes the relevant sufficient
statistics ("soft EM") using belief propagation. Similarly, the Structural
Expectation-Maximisation algorithm (Structural EM) learns the network structure
of the BN from those sufficient statistics using algorithms designed for
complete data. However, practical implementations of parameter and structure
learning often impute missing data ("hard EM") to compute sufficient statistics
instead of using belief propagation, for both ease of implementation and
computational speed. In this paper, we investigate the question: what is the
impact of using imputation instead of belief propagation on the quality of the
resulting BNs? From a simulation study using synthetic data and reference BNs,
we find that it is possible to recommend one approach over the other in several
scenarios based on the characteristics of the data. We then use this
information to build a simple decision tree to guide practitioners in choosing
the EM algorithm best suited to their problem.
</p>
<a href="http://arxiv.org/abs/2012.05269" target="_blank">arXiv:2012.05269</a> [<a href="http://arxiv.org/pdf/2012.05269" target="_blank">pdf</a>]

<h2>MLComp: A Methodology for Machine Learning-based Performance Estimation and Adaptive Selection of Pareto-Optimal Compiler Optimization Sequences. (arXiv:2012.05270v1 [cs.LG])</h2>
<h3>Alessio Colucci, D&#xe1;vid Juh&#xe1;sz, Martin Mosbeck, Alberto Marchisio, Semeen Rehman, Manfred Kreutzer, Guenther Nadbath, Axel Jantsch, Muhammad Shafique</h3>
<p>Embedded systems have proliferated in various consumer and industrial
applications with the evolution of Cyber-Physical Systems and the Internet of
Things. These systems are subjected to stringent constraints so that embedded
software must be optimized for multiple objectives simultaneously, namely
reduced energy consumption, execution time, and code size. Compilers offer
optimization phases to improve these metrics. However, proper selection and
ordering of them depends on multiple factors and typically requires expert
knowledge. State-of-the-art optimizers facilitate different platforms and
applications case by case, and they are limited by optimizing one metric at a
time, as well as requiring a time-consuming adaptation for different targets
through dynamic profiling.

To address these problems, we propose the novel MLComp methodology, in which
optimization phases are sequenced by a Reinforcement Learning-based policy.
Training of the policy is supported by Machine Learning-based analytical models
for quick performance estimation, thereby drastically reducing the time spent
for dynamic profiling. In our framework, different Machine Learning models are
automatically tested to choose the best-fitting one. The trained Performance
Estimator model is leveraged to efficiently devise Reinforcement Learning-based
multi-objective policies for creating quasi-optimal phase sequences.

Compared to state-of-the-art estimation models, our Performance Estimator
model achieves lower relative error (&lt;2%) with up to 50x faster training time
over multiple platforms and application domains. Our Phase Selection Policy
improves execution time and energy consumption of a given code by up to 12% and
6%, respectively. The Performance Estimator and the Phase Selection Policy can
be trained efficiently for any target platform and application domain.
</p>
<a href="http://arxiv.org/abs/2012.05270" target="_blank">arXiv:2012.05270</a> [<a href="http://arxiv.org/pdf/2012.05270" target="_blank">pdf</a>]

<h2>MetaInfoNet: Learning Task-Guided Information for Sample Reweighting. (arXiv:2012.05273v1 [cs.LG])</h2>
<h3>Hongxin Wei, Lei Feng, Rundong Wang, Bo An</h3>
<p>Deep neural networks have been shown to easily overfit to biased training
data with label noise or class imbalance. Meta-learning algorithms are commonly
designed to alleviate this issue in the form of sample reweighting, by learning
a meta weighting network that takes training losses as inputs to generate
sample weights. In this paper, we advocate that choosing proper inputs for the
meta weighting network is crucial for desired sample weights in a specific
task, while training loss is not always the correct answer. In view of this, we
propose a novel meta-learning algorithm, MetaInfoNet, which automatically
learns effective representations as inputs for the meta weighting network by
emphasizing task-related information with an information bottleneck strategy.
Extensive experimental results on benchmark datasets with label noise or class
imbalance validate that MetaInfoNet is superior to many state-of-the-art
methods.
</p>
<a href="http://arxiv.org/abs/2012.05273" target="_blank">arXiv:2012.05273</a> [<a href="http://arxiv.org/pdf/2012.05273" target="_blank">pdf</a>]

<h2>Wi-Fi Based Indoor Positioning System For Mobile Robots By Using Particle Filter. (arXiv:2012.05286v1 [cs.RO])</h2>
<h3>Hikmet Yucel, Gulin Elibol, Ugur Yayan</h3>
<p>Mobile robots have the capability to work in real-time autonomously.
Autonomous behavior is strictly dependent on knowing the position of the mobile
robot. The positioning of a mobile robot in an indoor area is a difficult task
for only one sensor information is used. We proposed a system and method to
locate the mobile robot via fusing signals from WIFI and odometer data via
particle filter. In this study, the Particle filter is a well-known filter that
is used for indoor positioning of mobile robots. The proposed system includes
two parts that are RFKON system and evarobot for data collection and
experiments. The Received Signal Strength (RSS) measurements of the WiFi access
points that are located in any environment are used to locate a stationary
mobile robot in one floor area via SIS Particle Filter. RSS measurements from
the RFKON database are used and the average location error is 0.7606 and 0.1495
m for 300 and 1000 particles respectively.
</p>
<a href="http://arxiv.org/abs/2012.05286" target="_blank">arXiv:2012.05286</a> [<a href="http://arxiv.org/pdf/2012.05286" target="_blank">pdf</a>]

<h2>Topological Planning with Transformers for Vision-and-Language Navigation. (arXiv:2012.05292v1 [cs.RO])</h2>
<h3>Kevin Chen, Junshen K. Chen, Jo Chuang, Marynel V&#xe1;zquez, Silvio Savarese</h3>
<p>Conventional approaches to vision-and-language navigation (VLN) are trained
end-to-end but struggle to perform well in freely traversable environments.
Inspired by the robotics community, we propose a modular approach to VLN using
topological maps. Given a natural language instruction and topological map, our
approach leverages attention mechanisms to predict a navigation plan in the
map. The plan is then executed with low-level actions (e.g. forward, rotate)
using a robust controller. Experiments show that our method outperforms
previous end-to-end approaches, generates interpretable navigation plans, and
exhibits intelligent behaviors such as backtracking.
</p>
<a href="http://arxiv.org/abs/2012.05292" target="_blank">arXiv:2012.05292</a> [<a href="http://arxiv.org/pdf/2012.05292" target="_blank">pdf</a>]

<h2>Optimal oracle inequalities for solving projected fixed-point equations. (arXiv:2012.05299v1 [cs.LG])</h2>
<h3>Wenlong Mou, Ashwin Pananjady, Martin J. Wainwright</h3>
<p>Linear fixed point equations in Hilbert spaces arise in a variety of
settings, including reinforcement learning, and computational methods for
solving differential and integral equations. We study methods that use a
collection of random observations to compute approximate solutions by searching
over a known low-dimensional subspace of the Hilbert space. First, we prove an
instance-dependent upper bound on the mean-squared error for a linear
stochastic approximation scheme that exploits Polyak--Ruppert averaging. This
bound consists of two terms: an approximation error term with an
instance-dependent approximation factor, and a statistical error term that
captures the instance-specific complexity of the noise when projected onto the
low-dimensional subspace. Using information theoretic methods, we also
establish lower bounds showing that both of these terms cannot be improved,
again in an instance-dependent sense. A concrete consequence of our
characterization is that the optimal approximation factor in this problem can
be much larger than a universal constant. We show how our results precisely
characterize the error of a class of temporal difference learning methods for
the policy evaluation problem with linear function approximation, establishing
their optimality.
</p>
<a href="http://arxiv.org/abs/2012.05299" target="_blank">arXiv:2012.05299</a> [<a href="http://arxiv.org/pdf/2012.05299" target="_blank">pdf</a>]

<h2>Competitive Simplicity for Multi-Task Learning for Real-Time Foggy Scene Understanding via Domain Adaptation. (arXiv:2012.05304v1 [cs.CV])</h2>
<h3>Naif Alshammari, Samet Akcay, Toby P. Breckon</h3>
<p>Automotive scene understanding under adverse weather conditions raises a
realistic and challenging problem attributable to poor outdoor scene visibility
(e.g. foggy weather). However, because most contemporary scene understanding
approaches are applied under ideal-weather conditions, such approaches may not
provide genuinely optimal performance when compared to established a priori
insights on extreme-weather understanding. In this paper, we propose a complex
but competitive multi-task learning approach capable of performing in real-time
semantic scene understanding and monocular depth estimation under foggy weather
conditions by leveraging both recent advances in adversarial training and
domain adaptation. As an end-to-end pipeline, our model provides a novel
solution to surpass degraded visibility in foggy weather conditions by
transferring scenes from foggy to normal using a GAN-based model. For optimal
performance in semantic segmentation, our model generates depth to be used as
complementary source information with RGB in the segmentation network. We
provide a robust method for foggy scene understanding by training two models
(normal and foggy) simultaneously with shared weights (each model is trained on
each weather condition independently). Our model incorporates RGB colour,
depth, and luminance images via distinct encoders with dense connectivity and
features fusing, and leverages skip connections to produce consistent depth and
segmentation predictions. Using this architectural formulation with light
computational complexity at inference time, we are able to achieve comparable
performance to contemporary approaches at a fraction of the overall model
complexity.
</p>
<a href="http://arxiv.org/abs/2012.05304" target="_blank">arXiv:2012.05304</a> [<a href="http://arxiv.org/pdf/2012.05304" target="_blank">pdf</a>]

<h2>Multi-Model Learning for Real-Time Automotive Semantic Foggy Scene Understanding via Domain Adaptation. (arXiv:2012.05320v1 [cs.CV])</h2>
<h3>Naif Alshammari, Samet Akcay, Toby P. Breckon</h3>
<p>Robust semantic scene segmentation for automotive applications is a
challenging problem in two key aspects: (1) labelling every individual scene
pixel and (2) performing this task under unstable weather and illumination
changes (e.g., foggy weather), which results in poor outdoor scene visibility.
Such visibility limitations lead to non-optimal performance of generalised deep
convolutional neural network-based semantic scene segmentation. In this paper,
we propose an efficient end-to-end automotive semantic scene understanding
approach that is robust to foggy weather conditions. As an end-to-end pipeline,
our proposed approach provides: (1) the transformation of imagery from foggy to
clear weather conditions using a domain transfer approach (correcting for poor
visibility) and (2) semantically segmenting the scene using a competitive
encoder-decoder architecture with low computational complexity (enabling
real-time performance). Our approach incorporates RGB colour, depth and
luminance images via distinct encoders with dense connectivity and features
fusion to effectively exploit information from different inputs, which
contributes to an optimal feature representation within the overall model.
Using this architectural formulation with dense skip connections, our model
achieves comparable performance to contemporary approaches at a fraction of the
overall model complexity.
</p>
<a href="http://arxiv.org/abs/2012.05320" target="_blank">arXiv:2012.05320</a> [<a href="http://arxiv.org/pdf/2012.05320" target="_blank">pdf</a>]

<h2>Securing Deep Spiking Neural Networks against Adversarial Attacks through Inherent Structural Parameters. (arXiv:2012.05321v1 [cs.LG])</h2>
<h3>Rida El-Allami, Alberto Marchisio, Muhammad Shafique, Ihsen Alouani</h3>
<p>Deep Learning (DL) algorithms have gained popularity owing to their practical
problem-solving capacity. However, they suffer from a serious integrity threat,
i.e., their vulnerability to adversarial attacks. In the quest for DL
trustworthiness, recent works claimed the inherent robustness of Spiking Neural
Networks (SNNs) to these attacks, without considering the variability in their
structural spiking parameters. This paper explores the security enhancement of
SNNs through internal structural parameters. Specifically, we investigate the
SNNs robustness to adversarial attacks with different values of the neuron's
firing voltage thresholds and time window boundaries. We thoroughly study SNNs
security under different adversarial attacks in the strong white-box setting,
with different noise budgets and under variable spiking parameters. Our results
show a significant impact of the structural parameters on the SNNs' security,
and promising sweet spots can be reached to design trustworthy SNNs with 85%
higher robustness than a traditional non-spiking DL system. To the best of our
knowledge, this is the first work that investigates the impact of structural
parameters on SNNs robustness to adversarial attacks. The proposed
contributions and the experimental framework is available online to the
community for reproducible research.
</p>
<a href="http://arxiv.org/abs/2012.05321" target="_blank">arXiv:2012.05321</a> [<a href="http://arxiv.org/pdf/2012.05321" target="_blank">pdf</a>]

<h2>Modeling Disease Progression Trajectories from Longitudinal Observational Data. (arXiv:2012.05324v1 [cs.LG])</h2>
<h3>Bum Chul Kwon, Peter Achenbach, Jessica L. Dunne, William Hagopian, Markus Lundgren, Kenney Ng, Riitta Veijola, Brigitte I. Frohnert, Vibha Anand, the T1DI Study Group</h3>
<p>Analyzing disease progression patterns can provide useful insights into the
disease processes of many chronic conditions. These analyses may help inform
recruitment for prevention trials or the development and personalization of
treatments for those affected. We learn disease progression patterns using
Hidden Markov Models (HMM) and distill them into distinct trajectories using
visualization methods. We apply it to the domain of Type 1 Diabetes (T1D) using
large longitudinal observational data from the T1DI study group. Our method
discovers distinct disease progression trajectories that corroborate with
recently published findings. In this paper, we describe the iterative process
of developing the model. These methods may also be applied to other chronic
conditions that evolve over time.
</p>
<a href="http://arxiv.org/abs/2012.05324" target="_blank">arXiv:2012.05324</a> [<a href="http://arxiv.org/pdf/2012.05324" target="_blank">pdf</a>]

<h2>Convolutional Neural Networks for Multispectral Image Cloud Masking. (arXiv:2012.05325v1 [cs.CV])</h2>
<h3>Gonzalo Mateo-Garc&#xed;a, Luis G&#xf3;mez-Chova, Gustau Camps-Valls</h3>
<p>Convolutional neural networks (CNN) have proven to be state of the art
methods for many image classification tasks and their use is rapidly increasing
in remote sensing problems. One of their major strengths is that, when enough
data is available, CNN perform an end-to-end learning without the need of
custom feature extraction methods. In this work, we study the use of different
CNN architectures for cloud masking of Proba-V multispectral images. We compare
such methods with the more classical machine learning approach based on feature
extraction plus supervised classification. Experimental results suggest that
CNN are a promising alternative for solving cloud masking problems.
</p>
<a href="http://arxiv.org/abs/2012.05325" target="_blank">arXiv:2012.05325</a> [<a href="http://arxiv.org/pdf/2012.05325" target="_blank">pdf</a>]

<h2>Privacy Amplification by Decentralization. (arXiv:2012.05326v1 [cs.LG])</h2>
<h3>Edwige Cyffers, Aur&#xe9;lien Bellet</h3>
<p>Analyzing data owned by several parties while achieving a good trade-off
between utility and privacy is a key challenge in federated learning and
analytics. In this work, we introduce a novel relaxation of local differential
privacy (LDP) that naturally arises in fully decentralized protocols, i.e.
participants exchange information by communicating along the edges of a network
graph. This relaxation, that we call network DP, captures the fact that users
have only a local view of the decentralized system. To show the relevance of
network DP, we study a decentralized model of computation where a token
performs a walk on the network graph and is updated sequentially by the party
who receives it. For tasks such as real summation, histogram computation and
gradient descent, we propose simple algorithms and prove privacy amplification
results on ring and complete topologies. The resulting privacy-utility
trade-off significantly improves upon LDP, and in some cases even matches what
can be achieved with approaches based on secure aggregation and secure
shuffling. Our experiments confirm the practical significance of the gains
compared to LDP.
</p>
<a href="http://arxiv.org/abs/2012.05326" target="_blank">arXiv:2012.05326</a> [<a href="http://arxiv.org/pdf/2012.05326" target="_blank">pdf</a>]

<h2>GAN Steerability without optimization. (arXiv:2012.05328v1 [cs.CV])</h2>
<h3>Nurit Spingarn-Eliezer, Ron Banner, Tomer Michaeli</h3>
<p>Recent research has shown remarkable success in revealing "steering"
directions in the latent spaces of pre-trained GANs. These directions
correspond to semantically meaningful image transformations e.g., shift, zoom,
color manipulations), and have similar interpretable effects across all
categories that the GAN can generate. Some methods focus on user-specified
transformations, while others discover transformations in an unsupervised
manner. However, all existing techniques rely on an optimization procedure to
expose those directions, and offer no control over the degree of allowed
interaction between different transformations. In this paper, we show that
"steering" trajectories can be computed in closed form directly from the
generator's weights without any form of training or optimization. This applies
to user-prescribed geometric transformations, as well as to unsupervised
discovery of more complex effects. Our approach allows determining both linear
and nonlinear trajectories, and has many advantages over previous methods. In
particular, we can control whether one transformation is allowed to come on the
expense of another (e.g. zoom-in with or without allowing translation to keep
the object centered). Moreover, we can determine the natural end-point of the
trajectory, which corresponds to the largest extent to which a transformation
can be applied without incurring degradation. Finally, we show how transferring
attributes between images can be achieved without optimization, even across
different categories.
</p>
<a href="http://arxiv.org/abs/2012.05328" target="_blank">arXiv:2012.05328</a> [<a href="http://arxiv.org/pdf/2012.05328" target="_blank">pdf</a>]

<h2>Know Your Limits: Monotonicity & Softmax Make Neural Classifiers Overconfident on OOD Data. (arXiv:2012.05329v1 [cs.LG])</h2>
<h3>Dennis Ulmer, Giovanni Cin&#xe0;</h3>
<p>A crucial requirement for reliable deployment of deep learning models for
safety-critical applications is the ability to identify out-of-distribution
(OOD) data points, samples which differ from the training data and on which a
model might underperform. Previous work has attempted to tackle this problem
using uncertainty estimation techniques. However, there is empirical evidence
that a large family of these techniques do not detect OOD reliably in
classification tasks.

This paper puts forward a theoretical explanation for said experimental
findings. We prove that such techniques are not able to reliably identify OOD
samples in a classification setting, provided the models satisfy weak
assumptions about the monotonicity of feature values and resulting class
probabilities. This result stems from the interplay between the saturating
nature of activation functions like sigmoid or softmax, coupled with the most
widely-used uncertainty metrics.
</p>
<a href="http://arxiv.org/abs/2012.05329" target="_blank">arXiv:2012.05329</a> [<a href="http://arxiv.org/pdf/2012.05329" target="_blank">pdf</a>]

<h2>Contrastive Predictive Coding for Human Activity Recognition. (arXiv:2012.05333v1 [cs.LG])</h2>
<h3>Harish Haresamudram, Irfan Essa, Thomas Ploetz</h3>
<p>Feature extraction is crucial for human activity recognition (HAR) using
body-worn movement sensors. Recently, learned representations have been used
successfully, offering promising alternatives to manually engineered features.
Our work focuses on effective use of small amounts of labeled data and the
opportunistic exploitation of unlabeled data that are straightforward to
collect in mobile and ubiquitous computing scenarios. We hypothesize and
demonstrate that explicitly considering the temporality of sensor data at
representation level plays an important role for effective HAR in challenging
scenarios. We introduce the Contrastive Predictive Coding (CPC) framework to
human activity recognition, which captures the long-term temporal structure of
sensor data streams. Through a range of experimental evaluations on real-life
recognition tasks, we demonstrate its effectiveness for improved HAR. CPC-based
pre-training is self-supervised, and the resulting learned representations can
be integrated into standard activity chains. It leads to significantly improved
recognition performance when only small amounts of labeled training data are
available, thereby demonstrating the practical value of our approach.
</p>
<a href="http://arxiv.org/abs/2012.05333" target="_blank">arXiv:2012.05333</a> [<a href="http://arxiv.org/pdf/2012.05333" target="_blank">pdf</a>]

<h2>Transfer Learning for Efficient Iterative Safety Validation. (arXiv:2012.05336v1 [cs.LG])</h2>
<h3>Anthony Corso, Mykel J. Kochenderfer</h3>
<p>Safety validation is important during the development of safety-critical
autonomous systems but can require significant computational effort. Existing
algorithms often start from scratch each time the system under test changes. We
apply transfer learning to improve the efficiency of reinforcement learning
based safety validation algorithms when applied to related systems. Knowledge
from previous safety validation tasks is encoded through the action value
function and transferred to future tasks with a learned set of attention
weights. Including a learned state and action value transformation for each
source task can improve performance even when systems have substantially
different failure modes. We conduct experiments on safety validation tasks in
gridworld and autonomous driving scenarios. We show that transfer learning can
improve the initial and final performance of validation algorithms and reduce
the number of training steps.
</p>
<a href="http://arxiv.org/abs/2012.05336" target="_blank">arXiv:2012.05336</a> [<a href="http://arxiv.org/pdf/2012.05336" target="_blank">pdf</a>]

<h2>Neural Rate Control for Video Encoding using Imitation Learning. (arXiv:2012.05339v1 [cs.LG])</h2>
<h3>Hongzi Mao, Chenjie Gu, Miaosen Wang, Angie Chen, Nevena Lazic, Nir Levine, Derek Pang, Rene Claus, Marisabel Hechtman, Ching-Han Chiang, Cheng Chen, Jingning Han</h3>
<p>In modern video encoders, rate control is a critical component and has been
heavily engineered. It decides how many bits to spend to encode each frame, in
order to optimize the rate-distortion trade-off over all video frames. This is
a challenging constrained planning problem because of the complex dependency
among decisions for different video frames and the bitrate constraint defined
at the end of the episode.

We formulate the rate control problem as a Partially Observable Markov
Decision Process (POMDP), and apply imitation learning to learn a neural rate
control policy. We demonstrate that by learning from optimal video encoding
trajectories obtained through evolution strategies, our learned policy achieves
better encoding efficiency and has minimal constraint violation. In addition to
imitating the optimal actions, we find that additional auxiliary losses, data
augmentation/refinement and inference-time policy improvements are critical for
learning a good rate control policy. We evaluate the learned policy against the
rate control policy in libvpx, a widely adopted open source VP9 codec library,
in the two-pass variable bitrate (VBR) mode. We show that over a diverse set of
real-world videos, our learned policy achieves 8.5% median bitrate reduction
without sacrificing video quality.
</p>
<a href="http://arxiv.org/abs/2012.05339" target="_blank">arXiv:2012.05339</a> [<a href="http://arxiv.org/pdf/2012.05339" target="_blank">pdf</a>]

<h2>3D attention mechanism for fine-grained classification of table tennis strokes using a Twin Spatio-Temporal Convolutional Neural Networks. (arXiv:2012.05342v1 [cs.CV])</h2>
<h3>Pierre-Etienne Martin (LaBRI, UB), Jenny Benois-Pineau (LaBRI), Renaud P&#xe9;teri, Julien Morlier</h3>
<p>The paper addresses the problem of recognition of actions in video with low
inter-class variability such as Table Tennis strokes. Two stream, "twin"
convolutional neural networks are used with 3D convolutions both on RGB data
and optical flow. Actions are recognized by classification of temporal windows.
We introduce 3D attention modules and examine their impact on classification
efficiency. In the context of the study of sportsmen performances, a corpus of
the particular actions of table tennis strokes is considered. The use of
attention blocks in the network speeds up the training step and improves the
classification scores up to 5% with our twin model. We visualize the impact on
the obtained features and notice correlation between attention and player
movements and position. Score comparison of state-of-the-art action
classification method and proposed approach with attentional blocks is
performed on the corpus. Proposed model with attention blocks outperforms
previous model without them and our baseline.
</p>
<a href="http://arxiv.org/abs/2012.05342" target="_blank">arXiv:2012.05342</a> [<a href="http://arxiv.org/pdf/2012.05342" target="_blank">pdf</a>]

<h2>Vulnerability Analysis of Face Morphing Attacks from Landmarks and Generative Adversarial Networks. (arXiv:2012.05344v1 [cs.CV])</h2>
<h3>Eklavya Sarkar, Pavel Korshunov, Laurent Colbois, S&#xe9;bastien Marcel</h3>
<p>Morphing attacks is a threat to biometric systems where the biometric
reference in an identity document can be altered. This form of attack presents
an important issue in applications relying on identity documents such as border
security or access control. Research in face morphing attack detection is
developing rapidly, however very few datasets with several forms of attacks are
publicly available. This paper bridges this gap by providing a new dataset with
four different types of morphing attacks, based on OpenCV, FaceMorpher,
WebMorph and a generative adversarial network (StyleGAN), generated with
original face images from three public face datasets. We also conduct extensive
experiments to assess the vulnerability of the state-of-the-art face
recognition systems, notably FaceNet, VGG-Face, and ArcFace. The experiments
demonstrate that VGG-Face, while being less accurate face recognition system
compared to FaceNet, is also less vulnerable to morphing attacks. Also, we
observed that na\"ive morphs generated with a StyleGAN do not pose a
significant threat.
</p>
<a href="http://arxiv.org/abs/2012.05344" target="_blank">arXiv:2012.05344</a> [<a href="http://arxiv.org/pdf/2012.05344" target="_blank">pdf</a>]

<h2>Data and its (dis)contents: A survey of dataset development and use in machine learning research. (arXiv:2012.05345v1 [cs.LG])</h2>
<h3>Amandalynne Paullada, Inioluwa Deborah Raji, Emily M. Bender, Emily Denton, Alex Hanna</h3>
<p>Datasets have played a foundational role in the advancement of machine
learning research. They form the basis for the models we design and deploy, as
well as our primary medium for benchmarking and evaluation. Furthermore, the
ways in which we collect, construct and share these datasets inform the kinds
of problems the field pursues and the methods explored in algorithm
development. However, recent work from a breadth of perspectives has revealed
the limitations of predominant practices in dataset collection and use. In this
paper, we survey the many concerns raised about the way we collect and use data
in machine learning and advocate that a more cautious and thorough
understanding of data is necessary to address several of the practical and
ethical issues of the field.
</p>
<a href="http://arxiv.org/abs/2012.05345" target="_blank">arXiv:2012.05345</a> [<a href="http://arxiv.org/pdf/2012.05345" target="_blank">pdf</a>]

<h2>Automatic Diagnosis of Malaria from Thin Blood Smear Images using Deep Convolutional Neural Network with Multi-Resolution Feature Fusion. (arXiv:2012.05350v1 [cs.CV])</h2>
<h3>Tanvir Mahmud, Shaikh Anowarul Fattah</h3>
<p>Malaria, a life-threatening disease, infects millions of people every year
throughout the world demanding faster diagnosis for proper treatment before any
damages occur. In this paper, an end-to-end deep learning-based approach is
proposed for faster diagnosis of malaria from thin blood smear images by making
efficient optimizations of features extracted from diversified receptive
fields. Firstly, an efficient, highly scalable deep neural network, named as
DilationNet, is proposed that incorporates features from a large spectrum by
varying dilation rates of convolutions to extract features from different
receptive areas. Next, the raw images are resampled to various resolutions to
introduce variations in the receptive fields that are used for independently
optimizing different forms of DilationNet scaled for different resolutions of
images. Afterward, a feature fusion scheme is introduced with the proposed
DeepFusionNet architecture for jointly optimizing the feature space of these
individually trained networks operating on different levels of observations.
All the convolutional layers of various forms of DilationNets that are
optimized to extract spatial features from different resolutions of images are
directly transferred to provide a variegated feature space from any image.
Later, joint optimization of these spatial features is carried out in the
DeepFusionNet to extract the most relevant representation of the sample image.
This scheme offers the opportunity to explore the feature space extensively by
varying the observation level to accurately diagnose the abnormality. Intense
experimentations on a publicly available dataset show outstanding performance
with accuracy over 99.5% outperforming other state-of-the-art approaches.
</p>
<a href="http://arxiv.org/abs/2012.05350" target="_blank">arXiv:2012.05350</a> [<a href="http://arxiv.org/pdf/2012.05350" target="_blank">pdf</a>]

<h2>Physics-consistent deep learning for structural topology optimization. (arXiv:2012.05359v1 [cs.LG])</h2>
<h3>Jaydeep Rade, Aditya Balu, Ethan Herron, Jay Pathak, Rishikesh Ranade, Soumik Sarkar, Adarsh Krishnamurthy</h3>
<p>Topology optimization has emerged as a popular approach to refine a
component's design and increasing its performance. However, current
state-of-the-art topology optimization frameworks are compute-intensive, mainly
due to multiple finite element analysis iterations required to evaluate the
component's performance during the optimization process. Recently, machine
learning-based topology optimization methods have been explored by researchers
to alleviate this issue. However, previous approaches have mainly been
demonstrated on simple two-dimensional applications with low-resolution
geometry. Further, current approaches are based on a single machine learning
model for end-to-end prediction, which requires a large dataset for training.
These challenges make it non-trivial to extend the current approaches to higher
resolutions. In this paper, we explore a deep learning-based framework for
performing topology optimization for three-dimensional geometries with a
reasonably fine (high) resolution. We are able to achieve this by training
multiple networks, each trying to learn a different aspect of the overall
topology optimization methodology. We demonstrate the application of our
framework on both 2D and 3D geometries. The results show that our approach
predicts the final optimized design better than current ML-based topology
optimization methods.
</p>
<a href="http://arxiv.org/abs/2012.05359" target="_blank">arXiv:2012.05359</a> [<a href="http://arxiv.org/pdf/2012.05359" target="_blank">pdf</a>]

<h2>MO-LTR: Multiple Object Localization, Tracking, and Reconstruction from Monocular RGB Videos. (arXiv:2012.05360v1 [cs.CV])</h2>
<h3>Kejie Li, Hamid Rezatofighi, Ian Reid</h3>
<p>Semantic aware reconstruction is more advantageous than geometric-only
reconstruction for future robotic and AR/VR applications because it represents
not only where things are, but also what things are. Object-centric mapping is
a task to build an object-level reconstruction where objects are separate and
meaningful entities that convey both geometry and semantic information. In this
paper, we present MO-LTR, a solution to object-centric mapping using only
monocular image sequences and camera poses. It is able to localize, track, and
reconstruct multiple objects in an online fashion when an RGB camera captures a
video of the surrounding. Given a new RGB frame, MO-LTR firstly applies a
monocular 3D detector to localize objects of interest and extract their shape
codes that represent the object shape in a learned embedding space. Detections
are then merged to existing objects in the map after data association. Motion
state (i.e. kinematics and the motion status) of each object is tracked by a
multiple model Bayesian filter and object shape is progressively refined by
fusing multiple shape code. We evaluate localization, tracking, and
reconstruction on benchmarking datasets for indoor and outdoor scenes, and show
superior performance over previous approaches.
</p>
<a href="http://arxiv.org/abs/2012.05360" target="_blank">arXiv:2012.05360</a> [<a href="http://arxiv.org/pdf/2012.05360" target="_blank">pdf</a>]

<h2>Kineverse: A Symbolic Articulation Model Framework for Model-Generic Software for Mobile Manipulation. (arXiv:2012.05362v1 [cs.RO])</h2>
<h3>Adrian R&#xf6;fer, Georg Bartels, Michael Beetz</h3>
<p>Human developers want to program robots using abstract instructions, such as
"fetch the milk from the fridge". To translate such instructions into
actionable plans, the robot's software requires in-depth background knowledge.
With regards to interactions with articulated objects such as doors and
drawers, the robot requires a model that it can use for state estimation and
motion planning. Existing articulation model frameworks take a descriptive
approach to model building, which requires additional background knowledge to
construct mathematical models for computation. In this paper, we introduce the
articulation model framework Kineverse which uses symbolic mathematical
expressions to model articulated objects. We provide a theoretical description
of this framework, and the operations that are supported by its models, and
suggest a software architecture for integrating our framework in a robotics
application. To demonstrate the applicability of our framework to robotics, we
employ it in solving two common robotics problems from state estimation and
manipulation.
</p>
<a href="http://arxiv.org/abs/2012.05362" target="_blank">arXiv:2012.05362</a> [<a href="http://arxiv.org/pdf/2012.05362" target="_blank">pdf</a>]

<h2>Ensemble Squared: A Meta AutoML System. (arXiv:2012.05390v1 [cs.LG])</h2>
<h3>Jason Yoo, Tony Joseph, Dylan Yung, S. Ali Nasseri, Frank Wood</h3>
<p>The continuing rise in the number of problems amenable to machine learning
solutions, coupled with simultaneous growth in both computing power and variety
of machine learning techniques has led to an explosion of interest in automated
machine learning (AutoML). This paper presents Ensemble Squared (Ensemble$^2$),
a "meta" AutoML system that ensembles at the level of AutoML systems.
Ensemble$^2$ exploits the diversity of existing, competing AutoML systems by
ensembling the top-performing models simultaneously generated by a set of them.
Our work shows that diversity in AutoML systems is sufficient to justify
ensembling at the AutoML system level. In demonstrating this, we also establish
a new state of the art AutoML result on the OpenML classification challenge.
</p>
<a href="http://arxiv.org/abs/2012.05390" target="_blank">arXiv:2012.05390</a> [<a href="http://arxiv.org/pdf/2012.05390" target="_blank">pdf</a>]

<h2>Feasibility Assessment of a Cost-Effective Two-Wheel Kian-I Mobile Robot for Autonomous Navigation. (arXiv:2012.05391v1 [cs.RO])</h2>
<h3>Amin Abbasi, Somaiyeh MahmoudZadeh, Amirmehdi Yazdani, Ata Jahangir Moshayedi</h3>
<p>A two-wheeled mobile robot, namely Kian-I, is designed and prototyped in this
research. The Kian-I is comparable with Khepera-IV in terms of dimensional
specifications, mounted sensors, and performance capabilities and can be used
for educational purposes and cost-effective experimental tests. A motion
control architecture is designed for Kian-I in this study to facilitate
accurate navigation for the robot in an immersive environment. The implemented
control structure consists of two main components of the path recommender
system and trajectory tracking controller. Given partial knowledge about the
operation field, the path recommender system adopts B-spline curves and
Particle Swarm Optimization (PSO) algorithm to determine a collision-free path
curve with translational velocity constraint. The provided optimal reference
path feeds into the trajectory tracking controller enabling Kian-I to navigate
autonomously in the operating field. The trajectory tracking module eliminate
the error between the desired path and the followed trajectory through
controlling the wheels' velocity. To assess the feasibility of the proposed
control architecture, the performance of Kian-I robot in autonomous navigation
from any arbitrary initial pose to a target of interest is evaluated through
numerous simulation and experimental studies. The experimental results
demonstrate the functional capacities and performance of the prototyped robot
to be used as a benchmark for investigation and verification of various mobile
robot algorithms in the laboratory environment.
</p>
<a href="http://arxiv.org/abs/2012.05391" target="_blank">arXiv:2012.05391</a> [<a href="http://arxiv.org/pdf/2012.05391" target="_blank">pdf</a>]

<h2>A Free Lunch for Unsupervised Domain Adaptive Object Detection without Source Data. (arXiv:2012.05400v1 [cs.CV])</h2>
<h3>Xianfeng Li, Weijie Chen, Di Xie, Shicai Yang, Peng Yuan, Shiliang Pu, Yueting Zhuang</h3>
<p>Unsupervised domain adaptation (UDA) assumes that source and target domain
data are freely available and usually trained together to reduce the domain
gap. However, considering the data privacy and the inefficiency of data
transmission, it is impractical in real scenarios. Hence, it draws our eyes to
optimize the network in the target domain without accessing labeled source
data. To explore this direction in object detection, for the first time, we
propose a source data-free domain adaptive object detection (SFOD) framework
via modeling it into a problem of learning with noisy labels. Generally, a
straightforward method is to leverage the pre-trained network from the source
domain to generate the pseudo labels for target domain optimization. However,
it is difficult to evaluate the quality of pseudo labels since no labels are
available in target domain. In this paper, self-entropy descent (SED) is a
metric proposed to search an appropriate confidence threshold for reliable
pseudo label generation without using any handcrafted labels. Nonetheless,
completely clean labels are still unattainable. After a thorough experimental
analysis, false negatives are found to dominate in the generated noisy labels.
Undoubtedly, false negatives mining is helpful for performance improvement, and
we ease it to false negatives simulation through data augmentation like Mosaic.
Extensive experiments conducted in four representative adaptation tasks have
demonstrated that the proposed framework can easily achieve state-of-the-art
performance. From another view, it also reminds the UDA community that the
labeled source data are not fully exploited in the existing methods.
</p>
<a href="http://arxiv.org/abs/2012.05400" target="_blank">arXiv:2012.05400</a> [<a href="http://arxiv.org/pdf/2012.05400" target="_blank">pdf</a>]

<h2>Data-Driven Intersection Management Solutions for Mixed Traffic of Human-Driven and Connected and Automated Vehicles. (arXiv:2012.05402v1 [cs.LG])</h2>
<h3>Masoud Bashiri</h3>
<p>This dissertation proposes two solutions for urban traffic control in the
presence of connected and automated vehicles. First a centralized platoon-based
controller is proposed for the cooperative intersection management problem that
takes advantage of the platooning systems and V2I communication to generate
fast and smooth traffic flow at a single intersection.

Second, a data-driven approach is proposed for adaptive signal control in the
presence of connected vehicles. The proposed system relies on a data-driven
method for optimal signal timing and a data-driven heuristic method for
estimating routing decisions. It requires no additional sensors to be installed
at the intersection, reducing the installation costs compared to typical
settings of state-of-the-practice adaptive signal controllers.

The proposed traffic controller contains an optimal signal timing module and
a traffic state estimator. The signal timing module is a neural network model
trained on microscopic simulation data to achieve optimal results according to
a given performance metric such as vehicular delay or average queue length. The
traffic state estimator relies on connected vehicles' information to estimate
the traffic's routing decisions. A heuristic method is proposed to minimize the
estimation error. With sufficient parameter tuning, the estimation error
decreases as the market penetration rate (MPR) of connected vehicles grows.
Estimation error is below 30% for an MPR of 10% and it shrinks below 20% when
MPR grows larger than 30%.

Simulations showed that the proposed traffic controller outperforms Highway
Capacity Manual's methodology and given proper offline parameter tuning, it can
decrease average vehicular delay by up to 25%.
</p>
<a href="http://arxiv.org/abs/2012.05402" target="_blank">arXiv:2012.05402</a> [<a href="http://arxiv.org/pdf/2012.05402" target="_blank">pdf</a>]

<h2>Research Challenges in Designing Differentially Private Text Generation Mechanisms. (arXiv:2012.05403v1 [cs.LG])</h2>
<h3>Oluwaseyi Feyisetan, Abhinav Aggarwal, Zekun Xu, Nathanael Teissier</h3>
<p>Accurately learning from user data while ensuring quantifiable privacy
guarantees provides an opportunity to build better Machine Learning (ML) models
while maintaining user trust. Recent literature has demonstrated the
applicability of a generalized form of Differential Privacy to provide
guarantees over text queries. Such mechanisms add privacy preserving noise to
vectorial representations of text in high dimension and return a text based
projection of the noisy vectors. However, these mechanisms are sub-optimal in
their trade-off between privacy and utility. This is due to factors such as a
fixed global sensitivity which leads to too much noise added in dense spaces
while simultaneously guaranteeing protection for sensitive outliers. In this
proposal paper, we describe some challenges in balancing the tradeoff between
privacy and utility for these differentially private text mechanisms. At a high
level, we provide two proposals: (1) a framework called LAC which defers some
of the noise to a privacy amplification step and (2), an additional suite of
three different techniques for calibrating the noise based on the local region
around a word. Our objective in this paper is not to evaluate a single solution
but to further the conversation on these challenges and chart pathways for
building better mechanisms.
</p>
<a href="http://arxiv.org/abs/2012.05403" target="_blank">arXiv:2012.05403</a> [<a href="http://arxiv.org/pdf/2012.05403" target="_blank">pdf</a>]

<h2>LaSeSOM: A Latent Representation Framework for Semantic Soft Object Manipulation. (arXiv:2012.05412v1 [cs.RO])</h2>
<h3>Peng Zhou, Jihong Zhu, Shengzeng Huo, David Navarro-Alarcon</h3>
<p>Soft object manipulation has recently gained popularity within the robotics
community due to its potential applications in many economically important
areas. Although great progress has been recently achieved in these types of
tasks, most state-of-the-art methods are case-specific; They can only be used
to perform a single deformation task (e.g. bending), as their shape
representation algorithms typically rely on "hard-coded" features. In this
paper, we present LaSeSOM, a new feedback latent representation framework for
semantic soft object manipulation. Our new method introduces internal latent
representation layers between low-level geometric feature extraction and
high-level semantic shape analysis; This allows the identification of each
compressed semantic function and the formation of a valid shape classifier from
different feature extraction levels. The proposed latent framework makes soft
object representation more generic (independent from the object's geometry and
its mechanical properties) and scalable (it can work with 1D/2D/3D tasks). Its
high-level semantic layer enables to perform (quasi) shape planning tasks with
soft objects, a valuable and underexplored capability in many soft manipulation
tasks. To validate this new methodology, we report a detailed experimental
study with robotic manipulators.
</p>
<a href="http://arxiv.org/abs/2012.05412" target="_blank">arXiv:2012.05412</a> [<a href="http://arxiv.org/pdf/2012.05412" target="_blank">pdf</a>]

<h2>An Efficient Asynchronous Method for Integrating Evolutionary and Gradient-based Policy Search. (arXiv:2012.05417v1 [cs.LG])</h2>
<h3>Kyunghyun Lee, Byeong-Uk Lee, Ukcheol Shin, In So Kweon</h3>
<p>Deep reinforcement learning (DRL) algorithms and evolution strategies (ES)
have been applied to various tasks, showing excellent performances. These have
the opposite properties, with DRL having good sample efficiency and poor
stability, while ES being vice versa. Recently, there have been attempts to
combine these algorithms, but these methods fully rely on synchronous update
scheme, making it not ideal to maximize the benefits of the parallelism in ES.
To solve this challenge, asynchronous update scheme was introduced, which is
capable of good time-efficiency and diverse policy exploration. In this paper,
we introduce an Asynchronous Evolution Strategy-Reinforcement Learning (AES-RL)
that maximizes the parallel efficiency of ES and integrates it with policy
gradient methods. Specifically, we propose 1) a novel framework to merge ES and
DRL asynchronously and 2) various asynchronous update methods that can take all
advantages of asynchronism, ES, and DRL, which are exploration and time
efficiency, stability, and sample efficiency, respectively. The proposed
framework and update methods are evaluated in continuous control benchmark
work, showing superior performance as well as time efficiency compared to the
previous methods.
</p>
<a href="http://arxiv.org/abs/2012.05417" target="_blank">arXiv:2012.05417</a> [<a href="http://arxiv.org/pdf/2012.05417" target="_blank">pdf</a>]

<h2>On the emergence of tetrahedral symmetry in the final and penultimate layers of neural network classifiers. (arXiv:2012.05420v1 [cs.LG])</h2>
<h3>Weinan E, Stephan Wojtowytsch</h3>
<p>A recent numerical study observed that neural network classifiers enjoy a
large degree of symmetry in the penultimate layer. Namely, if $h(x) = Af(x) +b$
where $A$ is a linear map and $f$ is the output of the penultimate layer of the
network (after activation), then all data points $x_{i, 1}, \dots, x_{i, N_i}$
in a class $C_i$ are mapped to a single point $y_i$ by $f$ and the points $y_i$
are located at the vertices of a regular $k-1$-dimensional tetrahedron in a
high-dimensional Euclidean space.

We explain this observation analytically in toy models for highly expressive
deep neural networks. In complementary examples, we demonstrate rigorously that
even the final output of the classifier $h$ is not uniform over data samples
from a class $C_i$ if $h$ is a shallow network (or if the deeper layers do not
bring the data samples into a convenient geometric configuration).
</p>
<a href="http://arxiv.org/abs/2012.05420" target="_blank">arXiv:2012.05420</a> [<a href="http://arxiv.org/pdf/2012.05420" target="_blank">pdf</a>]

<h2>Multi-Classifier Interactive Learning for Ambiguous Speech Emotion Recognition. (arXiv:2012.05429v1 [cs.AI])</h2>
<h3>Ying Zhou, Xuefeng Liang, Yu Gu, Yifei Yin, Longshan Yao</h3>
<p>In recent years, speech emotion recognition technology is of great
significance in industrial applications such as call centers, social robots and
health care. The combination of speech recognition and speech emotion
recognition can improve the feedback efficiency and the quality of service.
Thus, the speech emotion recognition has been attracted much attention in both
industry and academic. Since emotions existing in an entire utterance may have
varied probabilities, speech emotion is likely to be ambiguous, which poses
great challenges to recognition tasks. However, previous studies commonly
assigned a single-label or multi-label to each utterance in certain. Therefore,
their algorithms result in low accuracies because of the inappropriate
representation. Inspired by the optimally interacting theory, we address the
ambiguous speech emotions by proposing a novel multi-classifier interactive
learning (MCIL) method. In MCIL, multiple different classifiers first mimic
several individuals, who have inconsistent cognitions of ambiguous emotions,
and construct new ambiguous labels (the emotion probability distribution).
Then, they are retrained with the new labels to interact with their cognitions.
This procedure enables each classifier to learn better representations of
ambiguous data from others, and further improves the recognition ability. The
experiments on three benchmark corpora (MAS, IEMOCAP, and FAU-AIBO) demonstrate
that MCIL does not only improve each classifier's performance, but also raises
their recognition consistency from moderate to substantial.
</p>
<a href="http://arxiv.org/abs/2012.05429" target="_blank">arXiv:2012.05429</a> [<a href="http://arxiv.org/pdf/2012.05429" target="_blank">pdf</a>]

<h2>Communication-Computation Efficient Secure Aggregation for Federated Learning. (arXiv:2012.05433v1 [cs.LG])</h2>
<h3>Beongjun Choi, Jy-yong Sohn, Dong-Jun Han, Jaekyun Moon</h3>
<p>Federated learning has been spotlighted as a way to train neural networks
using data distributed over multiple nodes without the need for the nodes to
share data. Unfortunately, it has also been shown that data privacy could not
be fully guaranteed as adversaries may be able to extract certain information
on local data from the model parameters transmitted during federated learning.
A recent solution based on the secure aggregation primitive enabled
privacy-preserving federated learning, but at the expense of significant extra
communication/computational resources. In this paper, we propose
communication-computation efficient secure aggregation which substantially
reduces the amount of communication/computational resources relative to the
existing secure solution without sacrificing data privacy. The key idea behind
the suggested scheme is to design the topology of the secret-sharing nodes as
sparse random graphs instead of the complete graph corresponding to the
existing solution. We first obtain the necessary and sufficient condition on
the graph to guarantee reliable and private federated learning in the
information-theoretic sense. We then suggest using the Erd\H{o}s-R\'enyi graph
in particular and provide theoretical guarantees on the reliability/privacy of
the proposed scheme. Through extensive real-world experiments, we demonstrate
that our scheme, using only $20 \sim 30\%$ of the resources required in the
conventional scheme, maintains virtually the same levels of reliability and
data privacy in practical federated learning systems.
</p>
<a href="http://arxiv.org/abs/2012.05433" target="_blank">arXiv:2012.05433</a> [<a href="http://arxiv.org/pdf/2012.05433" target="_blank">pdf</a>]

<h2>Learning Optimization-inspired Image Propagation with Control Mechanisms and Architecture Augmentations for Low-level Vision. (arXiv:2012.05435v1 [cs.CV])</h2>
<h3>Risheng Liu, Zhu Liu, Pan Mu, Zhouchen Lin, Xin Fan, Zhongxuan Luo</h3>
<p>In recent years, building deep learning models from optimization perspectives
has becoming a promising direction for solving low-level vision problems. The
main idea of most existing approaches is to straightforwardly combine numerical
iterations with manually designed network architectures to generate image
propagations for specific kinds of optimization models. However, these
heuristic learning models often lack mechanisms to control the propagation and
rely on architecture engineering heavily. To mitigate the above issues, this
paper proposes a unified optimization-inspired deep image propagation framework
to aggregate Generative, Discriminative and Corrective (GDC for short)
principles for a variety of low-level vision tasks. Specifically, we first
formulate low-level vision tasks using a generic optimization objective and
construct our fundamental propagative modules from three different viewpoints,
i.e., the solution could be obtained/learned 1) in generative manner; 2) based
on discriminative metric, and 3) with domain knowledge correction. By designing
control mechanisms to guide image propagations, we then obtain convergence
guarantees of GDC for both fully- and partially-defined optimization
formulations. Furthermore, we introduce two architecture augmentation
strategies (i.e., normalization and automatic search) to respectively enhance
the propagation stability and task/data-adaption ability. Extensive experiments
on different low-level vision applications demonstrate the effectiveness and
flexibility of GDC.
</p>
<a href="http://arxiv.org/abs/2012.05435" target="_blank">arXiv:2012.05435</a> [<a href="http://arxiv.org/pdf/2012.05435" target="_blank">pdf</a>]

<h2>Developing Motion Code Embedding for Action Recognition in Videos. (arXiv:2012.05438v1 [cs.CV])</h2>
<h3>Maxat Alibayev, David Paulius, Yu Sun</h3>
<p>In this work, we propose a motion embedding strategy known as motion codes,
which is a vectorized representation of motions based on a manipulation's
salient mechanical attributes. These motion codes provide a robust motion
representation, and they are obtained using a hierarchy of features called the
motion taxonomy. We developed and trained a deep neural network model that
combines visual and semantic features to identify the features found in our
motion taxonomy to embed or annotate videos with motion codes. To demonstrate
the potential of motion codes as features for machine learning tasks, we
integrated the extracted features from the motion embedding model into the
current state-of-the-art action recognition model. The obtained model achieved
higher accuracy than the baseline model for the verb classification task on
egocentric videos from the EPIC-KITCHENS dataset.
</p>
<a href="http://arxiv.org/abs/2012.05438" target="_blank">arXiv:2012.05438</a> [<a href="http://arxiv.org/pdf/2012.05438" target="_blank">pdf</a>]

<h2>Few-shot Medical Image Segmentation using a Global Correlation Network with Discriminative Embedding. (arXiv:2012.05440v1 [cs.CV])</h2>
<h3>Liyan Sun, Chenxin Li, Xinghao Ding, Yue Huang, Guisheng Wang, Yizhou Yu</h3>
<p>Despite deep convolutional neural networks achieved impressive progress in
medical image computing and analysis, its paradigm of supervised learning
demands a large number of annotations for training to avoid overfitting and
achieving promising results. In clinical practices, massive semantic
annotations are difficult to acquire in some conditions where specialized
biomedical expert knowledge is required, and it is also a common condition
where only few annotated classes are available. In this work, we proposed a
novel method for few-shot medical image segmentation, which enables a
segmentation model to fast generalize to an unseen class with few training
images. We construct our few-shot image segmentor using a deep convolutional
network trained episodically. Motivated by the spatial consistency and
regularity in medical images, we developed an efficient global correlation
module to capture the correlation between a support and query image and
incorporate it into the deep network called global correlation network.
Moreover, we enhance discriminability of deep embedding to encourage clustering
of the feature domains of the same class while keep the feature domains of
different organs far apart. Ablation Study proved the effectiveness of the
proposed global correlation module and discriminative embedding loss. Extensive
experiments on anatomical abdomen images on both CT and MRI modalities are
performed to demonstrate the state-of-the-art performance of our proposed
model.
</p>
<a href="http://arxiv.org/abs/2012.05440" target="_blank">arXiv:2012.05440</a> [<a href="http://arxiv.org/pdf/2012.05440" target="_blank">pdf</a>]

<h2>Visual Perception Generalization for Visual-and-Language Navigation via Meta-Learning. (arXiv:2012.05446v1 [cs.RO])</h2>
<h3>Ting Wang, Zongkai Wu, Donglin Wang</h3>
<p>Vision-and-language navigation (VLN) is a challenging task that requires an
agent to navigate in real-world environments by understanding natural language
instructions and visual information received in real-time. Prior works have
implemented VLN tasks on continuous environments or physical robots, all of
which use a fixed camera configuration due to the limitations of datasets, such
as 1.5 meters height, 90 degrees horizontal field of view (HFOV), etc. However,
real-life robots with different purposes have multiple camera configurations,
and the huge gap in visual information makes it difficult to directly transfer
the learned navigation model between various robots. In this paper, we propose
a visual perception generalization strategy based on meta-learning, which
enables the agent to fast adapt to a new camera configuration with a few shots.
In the training phase, we first locate the generalization problem to the visual
perception module, and then compare two meta-learning algorithms for better
generalization in seen and unseen environments. One of them uses the
Model-Agnostic Meta-Learning (MAML) algorithm that requires a few shot
adaptation, and the other refers to a metric-based meta-learning method with a
feature-wise affine transformation layer. The experiment results show that our
strategy successfully adapts the learned navigation model to a new camera
configuration, and the two algorithms show their advantages in seen and unseen
environments respectively.
</p>
<a href="http://arxiv.org/abs/2012.05446" target="_blank">arXiv:2012.05446</a> [<a href="http://arxiv.org/pdf/2012.05446" target="_blank">pdf</a>]

<h2>The Representation Power of Neural Networks: Breaking the Curse of Dimensionality. (arXiv:2012.05451v1 [cs.LG])</h2>
<h3>Moise Blanchard, M. Amine Bennouna</h3>
<p>In this paper, we analyze the number of neurons and training parameters that
a neural networks needs to approximate multivariate functions of bounded second
mixed derivatives -- Koborov functions. We prove upper bounds on these
quantities for shallow and deep neural networks, breaking the curse of
dimensionality. Our bounds hold for general activation functions, including
ReLU. We further prove that these bounds nearly match the minimal number of
parameters any continuous function approximator needs to approximate Koborov
functions, showing that neural networks are near-optimal function
approximators.
</p>
<a href="http://arxiv.org/abs/2012.05451" target="_blank">arXiv:2012.05451</a> [<a href="http://arxiv.org/pdf/2012.05451" target="_blank">pdf</a>]

<h2>Neural-Swarm2: Planning and Control of Heterogeneous Multirotor Swarms using Learned Interactions. (arXiv:2012.05457v1 [cs.RO])</h2>
<h3>Guanya Shi, Wolfgang H&#xf6;nig, Xichen Shi, Yisong Yue, Soon-Jo Chung</h3>
<p>We present Neural-Swarm2, a learning-based method for motion planning and
control that allows heterogeneous multirotors in a swarm to safely fly in close
proximity. Such operation for drones is challenging due to complex aerodynamic
interaction forces, such as downwash generated by nearby drones and ground
effect. Conventional planning and control methods neglect capturing these
interaction forces, resulting in sparse swarm configuration during flight. Our
approach combines a physics-based nominal dynamics model with learned Deep
Neural Networks (DNNs) with strong Lipschitz properties. We evolve two
techniques to accurately predict the aerodynamic interactions between
heterogeneous multirotors: i) spectral normalization for stability and
generalization guarantees of unseen data and ii) heterogeneous deep sets for
supporting any number of heterogeneous neighbors in a permutation-invariant
manner without reducing expressiveness. The learned residual dynamics benefit
both the proposed interaction-aware multi-robot motion planning and the
nonlinear tracking control designs because the learned interaction forces
reduce the modelling errors. Experimental results demonstrate that
Neural-Swarm2 is able to generalize to larger swarms beyond training cases and
significantly outperforms a baseline nonlinear tracking controller with up to
three times reduction in worst-case tracking errors.
</p>
<a href="http://arxiv.org/abs/2012.05457" target="_blank">arXiv:2012.05457</a> [<a href="http://arxiv.org/pdf/2012.05457" target="_blank">pdf</a>]

<h2>Beyond Class-Conditional Assumption: A Primary Attempt to Combat Instance-Dependent Label Noise. (arXiv:2012.05458v1 [cs.LG])</h2>
<h3>Pengfei Chen, Junjie Ye, Guangyong Chen, Jingwei Zhao, Pheng-Ann Heng</h3>
<p>Supervised learning under label noise has seen numerous advances recently,
while existing theoretical findings and empirical results broadly build up on
the class-conditional noise (CCN) assumption that the noise is independent of
input features given the true label. In this work, we present a theoretical
hypothesis testing and prove that noise in real-world dataset is unlikely to be
CCN, which confirms that label noise should depend on the instance and
justifies the urgent need to go beyond the CCN assumption.The theoretical
results motivate us to study the more general and practical-relevant
instance-dependent noise (IDN). To stimulate the development of theory and
methodology on IDN, we formalize an algorithm to generate controllable IDN and
present both theoretical and empirical evidence to show that IDN is
semantically meaningful and challenging. As a primary attempt to combat IDN, we
present a tiny algorithm termed self-evolution average label (SEAL), which not
only stands out under IDN with various noise fractions, but also improves the
generalization on real-world noise benchmark Clothing1M. Our code is released.
Notably, our theoretical analysis in Section 2 provides rigorous motivations
for studying IDN, which is an important topic that deserves more research
attention in future.
</p>
<a href="http://arxiv.org/abs/2012.05458" target="_blank">arXiv:2012.05458</a> [<a href="http://arxiv.org/pdf/2012.05458" target="_blank">pdf</a>]

<h2>Investigating Bias in Image Classification using Model Explanations. (arXiv:2012.05463v1 [cs.CV])</h2>
<h3>Schrasing Tong (1), Lalana Kagal (1) ((1) Massachusetts Institute of Technology)</h3>
<p>We evaluated whether model explanations could efficiently detect bias in
image classification by highlighting discriminating features, thereby removing
the reliance on sensitive attributes for fairness calculations. To this end, we
formulated important characteristics for bias detection and observed how
explanations change as the degree of bias in models change. The paper
identifies strengths and best practices for detecting bias using explanations,
as well as three main weaknesses: explanations poorly estimate the degree of
bias, could potentially introduce additional bias into the analysis, and are
sometimes inefficient in terms of human effort involved.
</p>
<a href="http://arxiv.org/abs/2012.05463" target="_blank">arXiv:2012.05463</a> [<a href="http://arxiv.org/pdf/2012.05463" target="_blank">pdf</a>]

<h2>Tensor Composition Net for Visual Relationship Prediction. (arXiv:2012.05473v1 [cs.CV])</h2>
<h3>Yuting Qiang, Yongxin Yang, Yanwen Guo, Timothy M. Hospedales</h3>
<p>We present a novel Tensor Composition Network (TCN) to predict visual
relationships in images. Visual Relationships in subject-predicate-object form
provide a more powerful query modality than simple image tags. However Visual
Relationship Prediction (VRP) also provides a more challenging test of image
understanding than conventional image tagging, and is difficult to learn due to
a large label-space and incomplete annotation. The key idea of our TCN is to
exploit the low rank property of the visual relationship tensor, so as to
leverage correlations within and across objects and relationships, and make a
structured prediction of all objects and their relations in an image. To show
the effectiveness of our method, we first empirically compare our model with
multi-label classification alternatives on VRP, and show that our model
outperforms state-of-the-art MLIC methods. We then show that, thanks to our
tensor (de)composition layer, our model can predict visual relationships which
have not been seen in training dataset. We finally show our TCN's image-level
visual relationship prediction provides a simple and efficient mechanism for
relation-based image retrieval.
</p>
<a href="http://arxiv.org/abs/2012.05473" target="_blank">arXiv:2012.05473</a> [<a href="http://arxiv.org/pdf/2012.05473" target="_blank">pdf</a>]

<h2>One for More: Selecting Generalizable Samples for Generalizable ReID Model. (arXiv:2012.05475v1 [cs.CV])</h2>
<h3>Enwei Zhang, Xinyang Jiang, Hao Cheng, Ancong Wu, Fufu Yu, Ke Li, Xiaowei Guo, Feng Zheng, Wei-Shi Zheng, Xing Sun</h3>
<p>Current training objectives of existing person Re-IDentification (ReID)
models only ensure that the loss of the model decreases on selected training
batch, with no regards to the performance on samples outside the batch. It will
inevitably cause the model to over-fit the data in the dominant position (e.g.,
head data in imbalanced class, easy samples or noisy samples). %We call the
sample that updates the model towards generalizing on more data a generalizable
sample. The latest resampling methods address the issue by designing specific
criterion to select specific samples that trains the model generalize more on
certain type of data (e.g., hard samples, tail data), which is not adaptive to
the inconsistent real world ReID data distributions. Therefore, instead of
simply presuming on what samples are generalizable, this paper proposes a
one-for-more training objective that directly takes the generalization ability
of selected samples as a loss function and learn a sampler to automatically
select generalizable samples. More importantly, our proposed one-for-more based
sampler can be seamlessly integrated into the ReID training framework which is
able to simultaneously train ReID models and the sampler in an end-to-end
fashion. The experimental results show that our method can effectively improve
the ReID model training and boost the performance of ReID models.
</p>
<a href="http://arxiv.org/abs/2012.05475" target="_blank">arXiv:2012.05475</a> [<a href="http://arxiv.org/pdf/2012.05475" target="_blank">pdf</a>]

<h2>AI Driven Knowledge Extraction from Clinical Practice Guidelines: Turning Research into Practice. (arXiv:2012.05489v1 [cs.AI])</h2>
<h3>Musarrat Hussain, Jamil Hussain, Taqdir Ali, Fahad Ahmed Satti, Sungyoung Lee</h3>
<p>Background and Objectives: Clinical Practice Guidelines (CPGs) represent the
foremost methodology for sharing state-of-the-art research findings in the
healthcare domain with medical practitioners to limit practice variations,
reduce clinical cost, improve the quality of care, and provide evidence based
treatment. However, extracting relevant knowledge from the plethora of CPGs is
not feasible for already burdened healthcare professionals, leading to large
gaps between clinical findings and real practices. It is therefore imperative
that state-of-the-art Computing research, especially machine learning is used
to provide artificial intelligence based solution for extracting the knowledge
from CPGs and reducing the gap between healthcare research/guidelines and
practice. Methods: This research presents a novel methodology for knowledge
extraction from CPGs to reduce the gap and turn the latest research findings
into clinical practice. First, our system classifies the CPG sentences into
four classes such as condition-action, condition-consequences, action, and
not-applicable based on the information presented in a sentence. We use deep
learning with state-of-the-art word embedding, improved word vectors technique
in classification process. Second, it identifies qualifier terms in the
classified sentences, which assist in recognizing the condition and action
phrases in a sentence. Finally, the condition and action phrase are processed
and transformed into plain rule If Condition(s) Then Action format. Results: We
evaluate the methodology on three different domains guidelines including
Hypertension, Rhinosinusitis, and Asthma. The deep learning model classifies
the CPG sentences with an accuracy of 95%. While rule extraction was validated
by user-centric approach, which achieved a Jaccard coefficient of 0.6, 0.7, and
0.4 with three human experts extracted rules, respectively.
</p>
<a href="http://arxiv.org/abs/2012.05489" target="_blank">arXiv:2012.05489</a> [<a href="http://arxiv.org/pdf/2012.05489" target="_blank">pdf</a>]

<h2>Auto-MVCNN: Neural Architecture Search for Multi-view 3D Shape Recognition. (arXiv:2012.05493v1 [cs.CV])</h2>
<h3>Zhaoqun Li, Hongren Wang, Jinxing Li</h3>
<p>In 3D shape recognition, multi-view based methods leverage human's
perspective to analyze 3D shapes and have achieved significant outcomes. Most
existing research works in deep learning adopt handcrafted networks as
backbones due to their high capacity of feature extraction, and also benefit
from ImageNet pretraining. However, whether these network architectures are
suitable for 3D analysis or not remains unclear. In this paper, we propose a
neural architecture search method named Auto-MVCNN which is particularly
designed for optimizing architecture in multi-view 3D shape recognition.
Auto-MVCNN extends gradient-based frameworks to process multi-view images, by
automatically searching the fusion cell to explore intrinsic correlation among
view features. Moreover, we develop an end-to-end scheme to enhance retrieval
performance through the trade-off parameter search. Extensive experimental
results show that the searched architectures significantly outperform manually
designed counterparts in various aspects, and our method achieves
state-of-the-art performance at the same time.
</p>
<a href="http://arxiv.org/abs/2012.05493" target="_blank">arXiv:2012.05493</a> [<a href="http://arxiv.org/pdf/2012.05493" target="_blank">pdf</a>]

<h2>Spatiotemporal Graph Neural Network based Mask Reconstruction for Video Object Segmentation. (arXiv:2012.05499v1 [cs.CV])</h2>
<h3>Daizong Liu, Shuangjie Xu, Xiao-Yang Liu, Zichuan Xu, Wei Wei, Pan Zhou</h3>
<p>This paper addresses the task of segmenting class-agnostic objects in
semi-supervised setting. Although previous detection based methods achieve
relatively good performance, these approaches extract the best proposal by a
greedy strategy, which may lose the local patch details outside the chosen
candidate. In this paper, we propose a novel spatiotemporal graph neural
network (STG-Net) to reconstruct more accurate masks for video object
segmentation, which captures the local contexts by utilizing all proposals. In
the spatial graph, we treat object proposals of a frame as nodes and represent
their correlations with an edge weight strategy for mask context aggregation.
To capture temporal information from previous frames, we use a memory network
to refine the mask of current frame by retrieving historic masks in a temporal
graph. The joint use of both local patch details and temporal relationships
allow us to better address the challenges such as object occlusion and missing.
Without online learning and fine-tuning, our STG-Net achieves state-of-the-art
performance on four large benchmarks (DAVIS, YouTube-VOS, SegTrack-v2, and
YouTube-Objects), demonstrating the effectiveness of the proposed approach.
</p>
<a href="http://arxiv.org/abs/2012.05499" target="_blank">arXiv:2012.05499</a> [<a href="http://arxiv.org/pdf/2012.05499" target="_blank">pdf</a>]

<h2>On Shapley Credit Allocation for Interpretability. (arXiv:2012.05506v1 [stat.ML])</h2>
<h3>Debraj Basu</h3>
<p>We emphasize the importance of asking the right question when interpreting
the decisions of a learning model. We discuss a natural extension of the
theoretical machinery from Janzing et. al. 2020, which answers the question
"Why did my model predict a person has cancer?" for answering a more involved
question, "What caused my model to predict a person has cancer?" While the
former quantifies the direct effects of variables on the model, the latter also
accounts for indirect effects, thereby providing meaningful insights wherever
human beings can reason in terms of cause and effect. We propose three broad
categories for interpretations: observational, model-specific and causal each
of which are significant in their own right. Furthermore, this paper quantifies
feature relevance by weaving different natures of interpretations together with
different measures as characteristic functions for Shapley symmetrization.
Besides the widely used expected value of the model, we also discuss measures
of statistical uncertainty and dispersion as informative candidates, and their
merits in generating explanations for each data point, some of which are used
in this context for the first time. These measures are not only useful for
studying the influence of variables on the model output, but also on the
predictive performance of the model, and for that we propose relevant
characteristic functions that are also used for the first time.
</p>
<a href="http://arxiv.org/abs/2012.05506" target="_blank">arXiv:2012.05506</a> [<a href="http://arxiv.org/pdf/2012.05506" target="_blank">pdf</a>]

<h2>SE-ECGNet: A Multi-scale Deep Residual Network with Squeeze-and-Excitation Module for ECG Signal Classification. (arXiv:2012.05510v1 [cs.LG])</h2>
<h3>Haozhen Zhang, Wei Zhao, Shuang Liu</h3>
<p>The classification of electrocardiogram (ECG) signals, which takes much time
and suffers from a high rate of misjudgment, is recognized as an extremely
challenging task for cardiologists. The major difficulty of the ECG signals
classification is caused by the long-term sequence dependencies. Most existing
approaches for ECG signal classification use Recurrent Neural Network models,
e.g., LSTM and GRU, which are unable to extract accurate features for such long
sequences. Other approaches utilize 1-Dimensional Convolutional Neural Network
(CNN), such as ResNet or its variant, and they can not make good use of the
multi-lead information from ECG signals.Based on the above observations, we
develop a multi-scale deep residual network for the ECG signal classification
task. We are the first to propose to treat the multi-lead signal as a
2-dimensional matrix and combines multi-scale 2-D convolution blocks with 1-D
convolution blocks for feature extraction. Our proposed model achieves 99.2%
F1-score in the MIT-BIH dataset and 89.4% F1-score in Alibaba dataset and
outperforms the state-of-the-art performance by 2% and 3%, respectively, view
related code and data at https://github.com/Amadeuszhao/SE-ECGNet
</p>
<a href="http://arxiv.org/abs/2012.05510" target="_blank">arXiv:2012.05510</a> [<a href="http://arxiv.org/pdf/2012.05510" target="_blank">pdf</a>]

<h2>Synthesizing Long-Term 3D Human Motion and Interaction in 3D Scenes. (arXiv:2012.05522v1 [cs.CV])</h2>
<h3>Jiashun Wang, Huazhe Xu, Jingwei Xu, Sifei Liu, Xiaolong Wang</h3>
<p>Synthesizing 3D human motion plays an important role in many graphics
applications as well as understanding human activity. While many efforts have
been made on generating realistic and natural human motion, most approaches
neglect the importance of modeling human-scene interactions and affordance. On
the other hand, affordance reasoning (e.g., standing on the floor or sitting on
the chair) has mainly been studied with static human pose and gestures, and it
has rarely been addressed with human motion. In this paper, we propose to
bridge human motion synthesis and scene affordance reasoning. We present a
hierarchical generative framework to synthesize long-term 3D human motion
conditioning on the 3D scene structure. Building on this framework, we further
enforce multiple geometry constraints between the human mesh and scene point
clouds via optimization to improve realistic synthesis. Our experiments show
significant improvements over previous approaches on generating natural and
physically plausible human motion in a scene.
</p>
<a href="http://arxiv.org/abs/2012.05522" target="_blank">arXiv:2012.05522</a> [<a href="http://arxiv.org/pdf/2012.05522" target="_blank">pdf</a>]

<h2>Recurrence of Optimum for Training Weight and Activation Quantized Networks. (arXiv:2012.05529v1 [cs.LG])</h2>
<h3>Ziang Long, Penghang Yin, Jack Xin</h3>
<p>Deep neural networks (DNNs) are quantized for efficient inference on
resource-constrained platforms. However, training deep learning models with
low-precision weights and activations involves a demanding optimization task,
which calls for minimizing a stage-wise loss function subject to a discrete
set-constraint. While numerous training methods have been proposed, existing
studies for full quantization of DNNs are mostly empirical. From a theoretical
point of view, we study practical techniques for overcoming the combinatorial
nature of network quantization. Specifically, we investigate a simple yet
powerful projected gradient-like algorithm for quantizing two-linear-layer
networks, which proceeds by repeatedly moving one step at float weights in the
negation of a heuristic \emph{fake} gradient of the loss function (so-called
coarse gradient) evaluated at quantized weights. For the first time, we prove
that under mild conditions, the sequence of quantized weights recurrently
visits the global optimum of the discrete minimization problem for training
fully quantized network. We also show numerical evidence of the recurrence
phenomenon of weight evolution in training quantized deep networks.
</p>
<a href="http://arxiv.org/abs/2012.05529" target="_blank">arXiv:2012.05529</a> [<a href="http://arxiv.org/pdf/2012.05529" target="_blank">pdf</a>]

<h2>SSD-GAN: Measuring the Realness in the Spatial and Spectral Domains. (arXiv:2012.05535v1 [cs.CV])</h2>
<h3>Yuanqi Chen, Ge Li, Cece Jin, Shan Liu, Thomas Li</h3>
<p>This paper observes that there is an issue of high frequencies missing in the
discriminator of standard GAN, and we reveal it stems from downsampling layers
employed in the network architecture. This issue makes the generator lack the
incentive from the discriminator to learn high-frequency content of data,
resulting in a significant spectrum discrepancy between generated images and
real images. Since the Fourier transform is a bijective mapping, we argue that
reducing this spectrum discrepancy would boost the performance of GANs. To this
end, we introduce SSD-GAN, an enhancement of GANs to alleviate the spectral
information loss in the discriminator. Specifically, we propose to embed a
frequency-aware classifier into the discriminator to measure the realness of
the input in both the spatial and spectral domains. With the enhanced
discriminator, the generator of SSD-GAN is encouraged to learn high-frequency
content of real data and generate exact details. The proposed method is general
and can be easily integrated into most existing GANs framework without
excessive cost. The effectiveness of SSD-GAN is validated on various network
architectures, objective functions, and datasets. Code will be available at
https://github.com/cyq373/SSD-GAN.
</p>
<a href="http://arxiv.org/abs/2012.05535" target="_blank">arXiv:2012.05535</a> [<a href="http://arxiv.org/pdf/2012.05535" target="_blank">pdf</a>]

<h2>Topology-Adaptive Mesh Deformation for Surface Evolution, Morphing, and Multi-View Reconstruction. (arXiv:2012.05536v1 [cs.CV])</h2>
<h3>Andrei Zaharescu, Edmond Boyer, Radu Horaud</h3>
<p>Triangulated meshes have become ubiquitous discrete-surface representations.
In this paper we address the problem of how to maintain the manifold properties
of a surface while it undergoes strong deformations that may cause topological
changes. We introduce a new self-intersection removal algorithm, TransforMesh,
and we propose a mesh evolution framework based on this algorithm. Numerous
shape modelling applications use surface evolution in order to improve shape
properties, such as appearance or accuracy. Both explicit and implicit
representations can be considered for that purpose. However, explicit mesh
representations, while allowing for accurate surface modelling, suffer from the
inherent difficulty of reliably dealing with self-intersections and topological
changes such as merges and splits. As a consequence, a majority of methods rely
on implicit representations of surfaces, e.g. level-sets, that naturally
overcome these issues. Nevertheless, these methods are based on volumetric
discretizations, which introduce an unwanted precision-complexity trade-off.
The method that we propose handles topological changes in a robust manner and
removes self intersections, thus overcoming the traditional limitations of
mesh-based approaches. To illustrate the effectiveness of TransforMesh, we
describe two challenging applications, namely surface morphing and 3-D
reconstruction.
</p>
<a href="http://arxiv.org/abs/2012.05536" target="_blank">arXiv:2012.05536</a> [<a href="http://arxiv.org/pdf/2012.05536" target="_blank">pdf</a>]

<h2>Image Captioning with Context-Aware Auxiliary Guidance. (arXiv:2012.05545v1 [cs.CV])</h2>
<h3>Zeliang Song, Xiaofei Zhou, Zhendong Mao, Jianlong Tan</h3>
<p>Image captioning is a challenging computer vision task, which aims to
generate a natural language description of an image. Most recent researches
follow the encoder-decoder framework which depends heavily on the previous
generated words for the current prediction. Such methods can not effectively
take advantage of the future predicted information to learn complete semantics.
In this paper, we propose Context-Aware Auxiliary Guidance (CAAG) mechanism
that can guide the captioning model to perceive global contexts. Upon the
captioning model, CAAG performs semantic attention that selectively
concentrates on useful information of the global predictions to reproduce the
current generation. To validate the adaptability of the method, we apply CAAG
to three popular captioners and our proposal achieves competitive performance
on the challenging Microsoft COCO image captioning benchmark, e.g. 132.2
CIDEr-D score on Karpathy split and 130.7 CIDEr-D (c40) score on official
online evaluation server.
</p>
<a href="http://arxiv.org/abs/2012.05545" target="_blank">arXiv:2012.05545</a> [<a href="http://arxiv.org/pdf/2012.05545" target="_blank">pdf</a>]

<h2>Categorical Perception: A Groundwork for Deep Learning. (arXiv:2012.05549v1 [cs.LG])</h2>
<h3>Laurent Bonnasse-Gahot, Jean-Pierre Nadal</h3>
<p>Classification is one of the major tasks that deep learning is successfully
tackling. Categorization is also a fundamental cognitive ability. A well-known
perceptual consequence of categorization in humans and other animals, called
categorical perception, is characterized by a within-category compression and a
between-category separation: two items, close in input space, are perceived
closer if they belong to the same category than if they belong to different
categories. Elaborating on experimental and theoretical results in cognitive
science, here we study categorical effects in artificial neural networks. Our
formal and numerical analysis provides insights into the geometry of the neural
representation in deep layers, with expansion of space near category boundaries
and contraction far from category boundaries. We investigate categorical
representation by using two complementary approaches: one mimics experiments in
psychophysics and cognitive neuroscience by means of morphed continua between
stimuli of different categories, while the other introduces a categoricality
index that quantifies the separability of the classes at the population level
(a given layer in the neural network). We show on both shallow and deep neural
networks that category learning automatically induces categorical perception.
We further show that the deeper a layer, the stronger the categorical effects.
An important outcome of our analysis is to provide a coherent and unifying view
of the efficacy of different heuristic practices of the dropout regularization
technique. Our views, which find echoes in the neuroscience literature, insist
on the differential role of noise as a function of the level of representation
and in the course of learning: noise injected in the hidden layers gets
structured according to the organization of the categories, more variability
being allowed within a category than across classes.
</p>
<a href="http://arxiv.org/abs/2012.05549" target="_blank">arXiv:2012.05549</a> [<a href="http://arxiv.org/pdf/2012.05549" target="_blank">pdf</a>]

<h2>DI-Fusion: Online Implicit 3D Reconstruction with Deep Priors. (arXiv:2012.05551v1 [cs.CV])</h2>
<h3>Jiahui Huang, Shi-Sheng Huang, Haoxuan Song, Shi-Min Hu</h3>
<p>Previous online 3D dense reconstruction methods often cost massive memory
storage while achieving unsatisfactory surface quality mainly due to the usage
of stagnant underlying geometry representation, such as TSDF (truncated signed
distance functions) or surfels, without any knowledge of the scene priors. In
this paper, we present DI-Fusion (Deep Implicit Fusion), based on a novel 3D
representation, called Probabilistic Local Implicit Voxels (PLIVoxs), for
online 3D reconstruction using a commodity RGB-D camera. Our PLIVox encodes
scene priors considering both the local geometry and uncertainty parameterized
by a deep neural network. With such deep priors, we demonstrate by extensive
experiments that we are able to perform online implicit 3D reconstruction
achieving state-of-the-art mapping quality and camera trajectory estimation
accuracy, while taking much less storage compared with previous online 3D
reconstruction approaches.
</p>
<a href="http://arxiv.org/abs/2012.05551" target="_blank">arXiv:2012.05551</a> [<a href="http://arxiv.org/pdf/2012.05551" target="_blank">pdf</a>]

<h2>Debiased-CAM for bias-agnostic faithful visual explanations of deep convolutional networks. (arXiv:2012.05567v1 [cs.CV])</h2>
<h3>Wencan Zhang, Mariella Dimiccoli, Brian Y. Lim</h3>
<p>Class activation maps (CAMs) explain convolutional neural network predictions
by identifying salient pixels, but they become misaligned and misleading when
explaining predictions on images under bias, such as images blurred
accidentally or deliberately for privacy protection, or images with improper
white balance. Despite model fine-tuning to improve prediction performance on
these biased images, we demonstrate that CAM explanations become more deviated
and unfaithful with increased image bias. We present Debiased-CAM to recover
explanation faithfulness across various bias types and levels by training a
multi-input, multi-task model with auxiliary tasks for CAM and bias level
predictions. With CAM as a prediction task, explanations are made tunable by
retraining the main model layers and made faithful by self-supervised learning
from CAMs of unbiased images. The model provides representative, bias-agnostic
CAM explanations about the predictions on biased images as if generated from
their unbiased form. In four simulation studies with different biases and
prediction tasks, Debiased-CAM improved both CAM faithfulness and task
performance. We further conducted two controlled user studies to validate its
truthfulness and helpfulness, respectively. Quantitative and qualitative
analyses of participant responses confirmed Debiased-CAM as more truthful and
helpful. Debiased-CAM thus provides a basis to generate more faithful and
relevant explanations for a wide range of real-world applications with various
sources of bias.
</p>
<a href="http://arxiv.org/abs/2012.05567" target="_blank">arXiv:2012.05567</a> [<a href="http://arxiv.org/pdf/2012.05567" target="_blank">pdf</a>]

<h2>Direct Depth Learning Network for Stereo Matching. (arXiv:2012.05570v1 [cs.CV])</h2>
<h3>Hong Zhang, Haojie Li, Shenglun Chen, Tiantian Yan, Zhihui Wang, Guo Lu, Wanli Ouyang</h3>
<p>Being a crucial task of autonomous driving, Stereo matching has made great
progress in recent years. Existing stereo matching methods estimate disparity
instead of depth. They treat the disparity errors as the evaluation metric of
the depth estimation errors, since the depth can be calculated from the
disparity according to the triangulation principle. However, we find that the
error of the depth depends not only on the error of the disparity but also on
the depth range of the points. Therefore, even if the disparity error is low,
the depth error is still large, especially for the distant points. In this
paper, a novel Direct Depth Learning Network (DDL-Net) is designed for stereo
matching. DDL-Net consists of two stages: the Coarse Depth Estimation stage and
the Adaptive-Grained Depth Refinement stage, which are all supervised by depth
instead of disparity. Specifically, Coarse Depth Estimation stage uniformly
samples the matching candidates according to depth range to construct cost
volume and output coarse depth. Adaptive-Grained Depth Refinement stage
performs further matching near the coarse depth to correct the imprecise
matching and wrong matching. To make the Adaptive-Grained Depth Refinement
stage robust to the coarse depth and adaptive to the depth range of the points,
the Granularity Uncertainty is introduced to Adaptive-Grained Depth Refinement
stage. Granularity Uncertainty adjusts the matching range and selects the
candidates' features according to coarse prediction confidence and depth range.
We verify the performance of DDL-Net on SceneFlow dataset and DrivingStereo
dataset by different depth metrics. Results show that DDL-Net achieves an
average improvement of 25% on the SceneFlow dataset and $12\%$ on the
DrivingStereo dataset comparing the classical methods. More importantly, we
achieve state-of-the-art accuracy at a large distance.
</p>
<a href="http://arxiv.org/abs/2012.05570" target="_blank">arXiv:2012.05570</a> [<a href="http://arxiv.org/pdf/2012.05570" target="_blank">pdf</a>]

<h2>Large-Scale Generative Data-Free Distillation. (arXiv:2012.05578v1 [cs.LG])</h2>
<h3>Liangchen Luo, Mark Sandler, Zi Lin, Andrey Zhmoginov, Andrew Howard</h3>
<p>Knowledge distillation is one of the most popular and effective techniques
for knowledge transfer, model compression and semi-supervised learning. Most
existing distillation approaches require the access to original or augmented
training samples. But this can be problematic in practice due to privacy,
proprietary and availability concerns. Recent work has put forward some methods
to tackle this problem, but they are either highly time-consuming or unable to
scale to large datasets. To this end, we propose a new method to train a
generative image model by leveraging the intrinsic normalization layers'
statistics of the trained teacher network. This enables us to build an ensemble
of generators without training data that can efficiently produce substitute
inputs for subsequent distillation. The proposed method pushes forward the
data-free distillation performance on CIFAR-10 and CIFAR-100 to 95.02% and
77.02% respectively. Furthermore, we are able to scale it to ImageNet dataset,
which to the best of our knowledge, has never been done using generative models
in a data-free setting.
</p>
<a href="http://arxiv.org/abs/2012.05578" target="_blank">arXiv:2012.05578</a> [<a href="http://arxiv.org/pdf/2012.05578" target="_blank">pdf</a>]

<h2>Image Matching with Scale Adjustment. (arXiv:2012.05582v1 [cs.CV])</h2>
<h3>Yves Dufournaud, Cordelia Schmid, Radu Horaud</h3>
<p>In this paper we address the problem of matching two images with two
different resolutions: a high-resolution image and a low-resolution one. The
difference in resolution between the two images is not known and without loss
of generality one of the images is assumed to be the high-resolution one. On
the premise that changes in resolution act as a smoothing equivalent to changes
in scale, a scale-space representation of the high-resolution image is
produced. Hence the one-to-one classical image matching paradigm becomes
one-to-many because the low-resolution image is compared with all the
scale-space representations of the high-resolution one. Key to the success of
such a process is the proper representation of the features to be matched in
scale-space. We show how to represent and extract interest points at variable
scales and we devise a method allowing the comparison of two images at two
different resolutions. The method comprises the use of photometric- and
rotation-invariant descriptors, a geometric model mapping the high-resolution
image onto a low-resolution image region, and an image matching strategy based
on local constraints and on the robust estimation of this geometric model.
Extensive experiments show that our matching method can be used for scale
changes up to a factor of 6.
</p>
<a href="http://arxiv.org/abs/2012.05582" target="_blank">arXiv:2012.05582</a> [<a href="http://arxiv.org/pdf/2012.05582" target="_blank">pdf</a>]

<h2>Full Matching on Low Resolution for Disparity Estimation. (arXiv:2012.05586v1 [cs.CV])</h2>
<h3>Hong Zhang, Shenglun Chen, Zhihui Wang, Haojie Li, Wanli Ouyang</h3>
<p>A Multistage Full Matching disparity estimation scheme (MFM) is proposed in
this work. We demonstrate that decouple all similarity scores directly from the
low-resolution 4D volume step by step instead of estimating low-resolution 3D
cost volume through focusing on optimizing the low-resolution 4D volume
iteratively leads to more accurate disparity. To this end, we first propose to
decompose the full matching task into multiple stages of the cost aggregation
module. Specifically, we decompose the high-resolution predicted results into
multiple groups, and every stage of the newly designed cost aggregation module
learns only to estimate the results for a group of points. This alleviates the
problem of feature internal competitive when learning similarity scores of all
candidates from one low-resolution 4D volume output from one stage. Then, we
propose the strategy of \emph{Stages Mutual Aid}, which takes advantage of the
relationship of multiple stages to boost similarity scores estimation of each
stage, to solve the unbalanced prediction of multiple stages caused by serial
multistage framework. Experiment results demonstrate that the proposed method
achieves more accurate disparity estimation results and outperforms
state-of-the-art methods on Scene Flow, KITTI 2012 and KITTI 2015 datasets.
</p>
<a href="http://arxiv.org/abs/2012.05586" target="_blank">arXiv:2012.05586</a> [<a href="http://arxiv.org/pdf/2012.05586" target="_blank">pdf</a>]

<h2>An Asynchronous Kalman Filter for Hybrid Event Cameras. (arXiv:2012.05590v1 [cs.CV])</h2>
<h3>Ziwei Wang, Yonhon Ng, Cedric Scheerlinck, Robert Mahony</h3>
<p>We present an Asynchronous Kalman Filter (AKF) to reconstruct High Dynamic
Range (HDR) videos by fusing low-dynamic range images with event data. Event
cameras are ideally suited to capture HDR visual information without blur but
perform poorly on static or slowly changing scenes. Conversely, conventional
image sensors measure absolute intensity of slowly changing scenes effectively
but do poorly on quickly changing scenes with high dynamic range. The proposed
approach exploits advantages of hybrid sensors under a unifying uncertainty
model for both conventional frames and events. We present a novel dataset
targeting challenging HDR and fast motion scenes captured on two separate
sensors: an RGB frame-based camera and an event camera. Our video
reconstruction outperforms the state-of-the-art algorithms on existing datasets
and our targeted HDR dataset.
</p>
<a href="http://arxiv.org/abs/2012.05590" target="_blank">arXiv:2012.05590</a> [<a href="http://arxiv.org/pdf/2012.05590" target="_blank">pdf</a>]

<h2>Amodal Segmentation Based on Visible Region Segmentation and Shape Prior. (arXiv:2012.05598v1 [cs.CV])</h2>
<h3>Yuting Xiao, Yanyu Xu, Ziming Zhong, Weixin Luo, Jiawei Li, Shenghua Gao</h3>
<p>Almost all existing amodal segmentation methods make the inferences of
occluded regions by using features corresponding to the whole image.
</p>
<a href="http://arxiv.org/abs/2012.05598" target="_blank">arXiv:2012.05598</a> [<a href="http://arxiv.org/pdf/2012.05598" target="_blank">pdf</a>]

<h2>Equivalent Causal Models. (arXiv:2012.05603v1 [cs.AI])</h2>
<h3>Sander Beckers</h3>
<p>The aim of this paper is to offer the first systematic exploration and
definition of equivalent causal models in the context where both models are not
made up of the same variables. The idea is that two models are equivalent when
they agree on all "essential" causal information that can be expressed using
their common variables. I do so by focussing on the two main features of causal
models, namely their structural relations and their functional relations. In
particular, I define several relations of causal ancestry and several relations
of causal sufficiency, and require that the most general of these relations are
preserved across equivalent models.
</p>
<a href="http://arxiv.org/abs/2012.05603" target="_blank">arXiv:2012.05603</a> [<a href="http://arxiv.org/pdf/2012.05603" target="_blank">pdf</a>]

<h2>Exploiting Diverse Characteristics and Adversarial Ambivalence for Domain Adaptive Segmentation. (arXiv:2012.05608v1 [cs.CV])</h2>
<h3>Bowen Cai, Huan Fu, Rongfei Jia, Binqiang Zhao, Hua Li, Yinghui Xu</h3>
<p>Adapting semantic segmentation models to new domains is an important but
challenging problem. Recently enlightening progress has been made, but the
performance of existing methods are unsatisfactory on real datasets where the
new target domain comprises of heterogeneous sub-domains (e.g., diverse weather
characteristics). We point out that carefully reasoning about the multiple
modalities in the target domain can improve the robustness of adaptation
models. To this end, we propose a condition-guided adaptation framework that is
empowered by a special attentive progressive adversarial training (APAT)
mechanism and a novel self-training policy. The APAT strategy progressively
performs condition-specific alignment and attentive global feature matching.
The new self-training scheme exploits the adversarial ambivalences of easy and
hard adaptation regions and the correlations among target sub-domains
effectively. We evaluate our method (DCAA) on various adaptation scenarios
where the target images vary in weather conditions. The comparisons against
baselines and the state-of-the-art approaches demonstrate the superiority of
DCAA over the competitors.
</p>
<a href="http://arxiv.org/abs/2012.05608" target="_blank">arXiv:2012.05608</a> [<a href="http://arxiv.org/pdf/2012.05608" target="_blank">pdf</a>]

<h2>Retinex-inspired Unrolling with Cooperative Prior Architecture Search for Low-light Image Enhancement. (arXiv:2012.05609v1 [cs.CV])</h2>
<h3>Risheng Liu, Long Ma, Jiaao Zhang, Xin Fan, Zhongxuan Luo</h3>
<p>Low-light image enhancement plays very important roles in low-level vision
field. Recent works have built a large variety of deep learning models to
address this task. However, these approaches mostly rely on significant
architecture engineering and suffer from high computational burden. In this
paper, we propose a new method, named Retinex-inspired Unrolling with
Architecture Search (RUAS), to construct lightweight yet effective enhancement
network for low-light images in real-world scenario. Specifically, building
upon Retinex rule, RUAS first establishes models to characterize the intrinsic
underexposed structure of low-light images and unroll their optimization
processes to construct our holistic propagation structure. Then by designing a
cooperative reference-free learning strategy to discover low-light prior
architectures from a compact search space, RUAS is able to obtain a
top-performing image enhancement network, which is with fast speed and requires
few computational resources. Extensive experiments verify the superiority of
our RUAS framework against recently proposed state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2012.05609" target="_blank">arXiv:2012.05609</a> [<a href="http://arxiv.org/pdf/2012.05609" target="_blank">pdf</a>]

<h2>Enhancing Human Pose Estimation in Ancient Vase Paintings via Perceptually-grounded Style Transfer Learning. (arXiv:2012.05616v1 [cs.CV])</h2>
<h3>Prathmesh Madhu, Angel Villar-Corrales, Ronak Kosti, Torsten Bendschus, Corinna Reinhardt, Peter Bell, Andreas Maier, Vincent Christlein</h3>
<p>Human pose estimation (HPE) is a central part of understanding the visual
narration and body movements of characters depicted in artwork collections,
such as Greek vase paintings. Unfortunately, existing HPE methods do not
generalise well across domains resulting in poorly recognized poses. Therefore,
we propose a two step approach: (1) adapting a dataset of natural images of
known person and pose annotations to the style of Greek vase paintings by means
of image style-transfer. We introduce a perceptually-grounded style transfer
training to enforce perceptual consistency. Then, we fine-tune the base model
with this newly created dataset. We show that using style-transfer learning
significantly improves the SOTA performance on unlabelled data by more than 6%
mean average precision (mAP) as well as mean average recall (mAR). (2) To
improve the already strong results further, we created a small dataset
(ClassArch) consisting of ancient Greek vase paintings from the 6-5th century
BCE with person and pose annotations. We show that fine-tuning on this data
with a style-transferred model improves the performance further. In a thorough
ablation study, we give a targeted analysis of the influence of style
intensities, revealing that the model learns generic domain styles.
Additionally, we provide a pose-based image retrieval to demonstrate the
effectiveness of our method.
</p>
<a href="http://arxiv.org/abs/2012.05616" target="_blank">arXiv:2012.05616</a> [<a href="http://arxiv.org/pdf/2012.05616" target="_blank">pdf</a>]

<h2>Analysis and Optimal Edge Assignment For Hierarchical Federated Learning on Non-IID Data. (arXiv:2012.05622v1 [cs.LG])</h2>
<h3>Naram Mhaisen, Alaa Awad, Amr Mohamed, Aiman Erbad, Mohsen Guizani</h3>
<p>Learning-based applications have demonstrated practical use cases in
ubiquitous environments and amplified interest in exploiting the data stored on
users' mobile devices. Distributed learning algorithms aim to leverage such
distributed and diverse data to learn a global phenomena by performing training
amongst participating devices and repeatedly aggregating their local models'
parameters into a global model. Federated learning is a promising paradigm that
allows for extending local training among the participant devices before
aggregating the parameters, offering better communication efficiency. However,
in the cases where the participants' data are strongly skewed (i.e., non-IID),
the model accuracy can significantly drop. To face this challenge, we leverage
the edge computing paradigm to design a hierarchical learning system that
performs Federated Gradient Descent on the user-edge layer and Federated
Averaging on the edge-cloud layer. In this hierarchical architecture, the users
are assigned to different edges, such that edge-level data distributions turn
to be close to IID. We formalize and optimize this user-edge assignment problem
to minimize classes' distribution distance between edge nodes, which enhances
the Federated Averaging performance. Our experiments on multiple real-world
datasets show that the proposed optimized assignment is tractable and leads to
faster convergence of models towards a better accuracy value.
</p>
<a href="http://arxiv.org/abs/2012.05622" target="_blank">arXiv:2012.05622</a> [<a href="http://arxiv.org/pdf/2012.05622" target="_blank">pdf</a>]

<h2>DONE: Distributed Newton-type Method for Federated Edge Learning. (arXiv:2012.05625v1 [cs.LG])</h2>
<h3>Canh T. Dinh, Nguyen H. Tran, Tuan Dung Nguyen, Wei Bao, Amir Rezaei Balef</h3>
<p>There is growing interest in applying distributed machine learning to edge
computing, forming \emph{federated edge learning}. Compared with conventional
distributed machine learning in a datacenter, federated edge learning faces
non-independent and identically distributed (non-i.i.d.) and heterogeneous
data, and the communications between edge workers, possibly through distant
locations with unstable wireless networks, are more costly than their local
computational overhead. In this work, we propose a distributed Newton-type
algorithm (DONE) with fast convergence rate for communication-efficient
federated edge learning. First, with strongly convex and smooth loss functions,
we show that DONE can produce the Newton direction approximately in a
distributed manner by using the classical Richardson iteration on each edge
worker. Second, we prove that DONE has linear-quadratic convergence and analyze
its computation and communication complexities. Finally, the experimental
results with non-i.i.d. and heterogeneous data show that DONE attains the same
performance as the Newton's method. Notably, DONE requires considerably fewer
communication iterations compared to the distributed gradient descent algorithm
and outperforms DANE, a state-of-the-art, in the case of non-quadratic loss
functions.
</p>
<a href="http://arxiv.org/abs/2012.05625" target="_blank">arXiv:2012.05625</a> [<a href="http://arxiv.org/pdf/2012.05625" target="_blank">pdf</a>]

<h2>Denoising-based Turbo Message Passing for Compressed Video Background Subtraction. (arXiv:2012.05626v1 [cs.LG])</h2>
<h3>Zhipeng Xue, Xiaojun Yuan, Yang Yang</h3>
<p>In this paper, we consider the compressed video background subtraction
problem that separates the background and foreground of a video from its
compressed measurements. The background of a video usually lies in a low
dimensional space and the foreground is usually sparse. More importantly, each
video frame is a natural image that has textural patterns. By exploiting these
properties, we develop a message passing algorithm termed offline
denoising-based turbo message passing (DTMP). We show that these structural
properties can be efficiently handled by the existing denoising techniques
under the turbo message passing framework. We further extend the DTMP algorithm
to the online scenario where the video data is collected in an online manner.
The extension is based on the similarity/continuity between adjacent video
frames. We adopt the optical flow method to refine the estimation of the
foreground. We also adopt the sliding window based background estimation to
reduce complexity. By exploiting the Gaussianity of messages, we develop the
state evolution to characterize the per-iteration performance of offline and
online DTMP. Comparing to the existing algorithms, DTMP can work at much lower
compression rates, and can subtract the background successfully with a lower
mean squared error and better visual quality for both offline and online
compressed video background subtraction.
</p>
<a href="http://arxiv.org/abs/2012.05626" target="_blank">arXiv:2012.05626</a> [<a href="http://arxiv.org/pdf/2012.05626" target="_blank">pdf</a>]

<h2>Can we detect harmony in artistic compositions? A machine learning approach. (arXiv:2012.05633v1 [cs.CV])</h2>
<h3>Adam Vandor, Marie van Vollenhoven, Gerhard Weiss, Gerasimos Spanakis</h3>
<p>Harmony in visual compositions is a concept that cannot be defined or easily
expressed mathematically, even by humans. The goal of the research described in
this paper was to find a numerical representation of artistic compositions with
different levels of harmony. We ask humans to rate a collection of grayscale
images based on the harmony they convey. To represent the images, a set of
special features were designed and extracted. By doing so, it became possible
to assign objective measures to subjectively judged compositions. Given the
ratings and the extracted features, we utilized machine learning algorithms to
evaluate the efficiency of such representations in a harmony classification
problem. The best performing model (SVM) achieved 80% accuracy in
distinguishing between harmonic and disharmonic images, which reinforces the
assumption that concept of harmony can be expressed in a mathematical way that
can be assessed by humans.
</p>
<a href="http://arxiv.org/abs/2012.05633" target="_blank">arXiv:2012.05633</a> [<a href="http://arxiv.org/pdf/2012.05633" target="_blank">pdf</a>]

<h2>Asymptotic study of stochastic adaptive algorithm in non-convex landscape. (arXiv:2012.05640v1 [stat.ML])</h2>
<h3>S&#xe9;bastien Gadat, Ioana Gavra</h3>
<p>This paper studies some asymptotic properties of adaptive algorithms widely
used in optimization and machine learning, and among them Adagrad and Rmsprop,
which are involved in most of the blackbox deep learning algorithms. Our setup
is the non-convex landscape optimization point of view, we consider a one time
scale parametrization and we consider the situation where these algorithms may
be used or not with mini-batches. We adopt the point of view of stochastic
algorithms and establish the almost sure convergence of these methods when
using a decreasing step-size point of view towards the set of critical points
of the target function. With a mild extra assumption on the noise, we also
obtain the convergence towards the set of minimizer of the function. Along our
study, we also obtain a "convergence rate" of the methods, in the vein of the
works of \cite{GhadimiLan}.
</p>
<a href="http://arxiv.org/abs/2012.05640" target="_blank">arXiv:2012.05640</a> [<a href="http://arxiv.org/pdf/2012.05640" target="_blank">pdf</a>]

<h2>Weakly Supervised Arrhythmia Detection Based on Deep Convolutional Neural Network. (arXiv:2012.05641v1 [cs.LG])</h2>
<h3>Yang Liu, Kuanquan Wang, Qince Li, Runnan He, Yongfeng Yuan, Henggui Zhang</h3>
<p>Supervised deep learning has been widely used in the studies of automatic ECG
classification, which largely benefits from sufficient annotation of large
datasets. However, most of the existing large ECG datasets are roughly
annotated, so the classification model trained on them can only detect the
existence of abnormalities in a whole recording, but cannot determine their
exact occurrence time. In addition, it may take huge time and economic cost to
construct a fine-annotated ECG dataset. Therefore, this study proposes weakly
supervised deep learning models for detecting abnormal ECG events and their
occurrence time. The available supervision information for the models is
limited to the event types in an ECG record, excluding the specific occurring
time of each event. By leverage of feature locality of deep convolution neural
network, the models first make predictions based on the local features, and
then aggregate the local predictions to infer the existence of each event
during the whole record. Through training, the local predictions are expected
to reflect the specific occurring time of each event. To test their potentials,
we apply the models for detecting cardiac rhythmic and morphological
arrhythmias by using the AFDB and MITDB datasets, respectively. The results
show that the models achieve beat-level accuracies of 99.09% in detecting
atrial fibrillation, and 99.13% in detecting morphological arrhythmias, which
are comparable to that of fully supervised learning models, demonstrating their
effectiveness. The local prediction maps revealed by this method are also
helpful to analyze and diagnose the decision logic of record-level
classification models.
</p>
<a href="http://arxiv.org/abs/2012.05641" target="_blank">arXiv:2012.05641</a> [<a href="http://arxiv.org/pdf/2012.05641" target="_blank">pdf</a>]

<h2>Learning Graphons via Structured Gromov-Wasserstein Barycenters. (arXiv:2012.05644v1 [cs.LG])</h2>
<h3>Hongteng Xu, Dixin Luo, Lawrence Carin, Hongyuan Zha</h3>
<p>We propose a novel and principled method to learn a nonparametric graph model
called graphon, which is defined in an infinite-dimensional space and
represents arbitrary-size graphs. Based on the weak regularity lemma from the
theory of graphons, we leverage a step function to approximate a graphon. We
show that the cut distance of graphons can be relaxed to the Gromov-Wasserstein
distance of their step functions. Accordingly, given a set of graphs generated
by an underlying graphon, we learn the corresponding step function as the
Gromov-Wasserstein barycenter of the given graphs. Furthermore, we develop
several enhancements and extensions of the basic algorithm, $e.g.$, the
smoothed Gromov-Wasserstein barycenter for guaranteeing the continuity of the
learned graphons and the mixed Gromov-Wasserstein barycenters for learning
multiple structured graphons. The proposed approach overcomes drawbacks of
prior state-of-the-art methods, and outperforms them on both synthetic and
real-world data. The code is available at
https://github.com/HongtengXu/SGWB-Graphon.
</p>
<a href="http://arxiv.org/abs/2012.05644" target="_blank">arXiv:2012.05644</a> [<a href="http://arxiv.org/pdf/2012.05644" target="_blank">pdf</a>]

<h2>HpGAN: Sequence Search with Generative Adversarial Networks. (arXiv:2012.05645v1 [cs.LG])</h2>
<h3>Mingxing Zhang, Zhengchun Zhou, Lanping Li, Zilong Liu, Meng Yang, Yanghe Feng</h3>
<p>Sequences play an important role in many engineering applications and
systems. Searching sequences with desired properties has long been an
interesting but also challenging research topic. This article proposes a novel
method, called HpGAN, to search desired sequences algorithmically using
generative adversarial networks (GAN). HpGAN is based on the idea of zero-sum
game to train a generative model, which can generate sequences with
characteristics similar to the training sequences. In HpGAN, we design the
Hopfield network as an encoder to avoid the limitations of GAN in generating
discrete data. Compared with traditional sequence construction by algebraic
tools, HpGAN is particularly suitable for intractable problems with complex
objectives which prevent mathematical analysis. We demonstrate the search
capabilities of HpGAN in two applications: 1) HpGAN successfully found many
different mutually orthogonal complementary code sets (MOCCS) and optimal
odd-length Z-complementary pairs (OB-ZCPs) which are not part of the training
set. In the literature, both MOCSSs and OB-ZCPs have found wide applications in
wireless communications. 2) HpGAN found new sequences which achieve four-times
increase of signal-to-interference ratio--benchmarked against the well-known
Legendre sequence--of a mismatched filter (MMF) estimator in pulse compression
radar systems. These sequences outperform those found by AlphaSeq.
</p>
<a href="http://arxiv.org/abs/2012.05645" target="_blank">arXiv:2012.05645</a> [<a href="http://arxiv.org/pdf/2012.05645" target="_blank">pdf</a>]

<h2>Concept Generalization in Visual Representation Learning. (arXiv:2012.05649v1 [cs.CV])</h2>
<h3>Mert Bulent Sariyildiz, Yannis Kalantidis, Diane Larlus, Karteek Alahari</h3>
<p>Measuring concept generalization, i.e., the extent to which models trained on
a set of (seen) visual concepts can be used to recognize a new set of (unseen)
concepts, is a popular way of evaluating visual representations, especially
when they are learned with self-supervised learning. Nonetheless, the choice of
which unseen concepts to use is usually made arbitrarily, and independently
from the seen concepts used to train representations, thus ignoring any
semantic relationships between the two. In this paper, we argue that semantic
relationships between seen and unseen concepts affect generalization
performance and propose ImageNet-CoG, a novel benchmark on the ImageNet dataset
that enables measuring concept generalization in a principled way. Our
benchmark leverages expert knowledge that comes from WordNet in order to define
a sequence of unseen ImageNet concept sets that are semantically more and more
distant from the ImageNet-1K subset, a ubiquitous training set. This allows us
to benchmark visual representations learned on ImageNet-1K out-of-the box: we
analyse a number of such models from supervised, semi-supervised and
self-supervised approaches under the prism of concept generalization, and show
how our benchmark is able to uncover a number of interesting insights. We will
provide resources for the benchmark at
https://europe.naverlabs.com/cog-benchmark.
</p>
<a href="http://arxiv.org/abs/2012.05649" target="_blank">arXiv:2012.05649</a> [<a href="http://arxiv.org/pdf/2012.05649" target="_blank">pdf</a>]

<h2>Geometric Adversarial Attacks and Defenses on 3D Point Clouds. (arXiv:2012.05657v1 [cs.CV])</h2>
<h3>Itai Lang, Uriel Kotlicki, Shai Avidan</h3>
<p>Deep neural networks are prone to adversarial examples that maliciously alter
the network's outcome. Due to the increasing popularity of 3D sensors in
safety-critical systems and the vast deployment of deep learning models for 3D
point sets, there is a growing interest in adversarial attacks and defenses for
such models. So far, the research has focused on the semantic level, namely,
deep point cloud classifiers. However, point clouds are also widely used in a
geometric-related form that includes encoding and reconstructing the geometry.
In this work, we explore adversarial examples at a geometric level. That is, a
small change to a clean source point cloud leads, after passing through an
autoencoder model, to a shape from a different target class. On the defense
side, we show that remnants of the attack's target shape are still present at
the reconstructed output after applying the defense to the adversarial input.
Our code is publicly available at https://github.com/itailang/geometric_adv.
</p>
<a href="http://arxiv.org/abs/2012.05657" target="_blank">arXiv:2012.05657</a> [<a href="http://arxiv.org/pdf/2012.05657" target="_blank">pdf</a>]

<h2>Slimmable Generative Adversarial Networks. (arXiv:2012.05660v1 [cs.LG])</h2>
<h3>Liang Hou, Zehuan Yuan, Lei Huang, Huawei Shen, Xueqi Cheng, Changhu Wang</h3>
<p>Generative adversarial networks (GANs) have achieved remarkable progress in
recent years, but the continuously growing scale of models makes them
challenging to deploy widely in practical applications. In particular, for
real-time tasks, different devices require models of different sizes due to
varying computing power. In this paper, we introduce slimmable GANs (SlimGANs),
which can flexibly switch the width (channels of layers) of the generator to
accommodate various quality-efficiency trade-offs at runtime. Specifically, we
leverage multiple partial parameter-shared discriminators to train the
slimmable generator. To facilitate the \textit{consistency} between generators
of different widths, we present a stepwise inplace distillation technique that
encourages narrow generators to learn from wide ones. As for class-conditional
generation, we propose a sliceable conditional batch normalization that
incorporates the label information into different widths. Our methods are
validated, both quantitatively and qualitatively, by extensive experiments and
a detailed ablation study.
</p>
<a href="http://arxiv.org/abs/2012.05660" target="_blank">arXiv:2012.05660</a> [<a href="http://arxiv.org/pdf/2012.05660" target="_blank">pdf</a>]

<h2>Factor Graph Molecule Network for Structure Elucidation. (arXiv:2012.05665v1 [cs.LG])</h2>
<h3>Hieu Le Trung, Yiqing Xu, Wee Sun Lee</h3>
<p>Designing a network to learn a molecule structure given its physical/chemical
properties is a hard problem, but is useful for drug discovery tasks. In this
paper, we incorporate higher-order relational learning of Factor Graphs with
strong approximation power of Neural Networks to create a molecule-structure
learning network that has strong generalization power and can enforce
higher-order relationship and valence constraints. We further propose methods
to tackle problems such as the efficient design of factor nodes, conditional
parameter sharing among factors, and symmetry problems in molecule structure
prediction. Our experiment evaluation shows that the factor learning is
effective and outperforms related methods.
</p>
<a href="http://arxiv.org/abs/2012.05665" target="_blank">arXiv:2012.05665</a> [<a href="http://arxiv.org/pdf/2012.05665" target="_blank">pdf</a>]

<h2>Imitating Interactive Intelligence. (arXiv:2012.05672v1 [cs.LG])</h2>
<h3>Josh Abramson, Arun Ahuja, Arthur Brussee, Federico Carnevale, Mary Cassin, Stephen Clark, Andrew Dudzik, Petko Georgiev, Aurelia Guy, Tim Harley, Felix Hill, Alden Hung, Zachary Kenton, Jessica Landon, Timothy Lillicrap, Kory Mathewson, Alistair Muldal, Adam Santoro, Nikolay Savinov, Vikrant Varma, Greg Wayne, Nathaniel Wong, Chen Yan, Rui Zhu</h3>
<p>A common vision from science fiction is that robots will one day inhabit our
physical spaces, sense the world as we do, assist our physical labours, and
communicate with us through natural language. Here we study how to design
artificial agents that can interact naturally with humans using the
simplification of a virtual environment. This setting nevertheless integrates a
number of the central challenges of artificial intelligence (AI) research:
complex visual perception and goal-directed physical control, grounded language
comprehension and production, and multi-agent social interaction. To build
agents that can robustly interact with humans, we would ideally train them
while they interact with humans. However, this is presently impractical.
Therefore, we approximate the role of the human with another learned agent, and
use ideas from inverse reinforcement learning to reduce the disparities between
human-human and agent-agent interactive behaviour. Rigorously evaluating our
agents poses a great challenge, so we develop a variety of behavioural tests,
including evaluation by humans who watch videos of agents or interact directly
with them. These evaluations convincingly demonstrate that interactive training
and auxiliary losses improve agent behaviour beyond what is achieved by
supervised learning of actions alone. Further, we demonstrate that agent
capabilities generalise beyond literal experiences in the dataset. Finally, we
train evaluation models whose ratings of agents agree well with human
judgement, thus permitting the evaluation of new agent models without
additional effort. Taken together, our results in this virtual environment
provide evidence that large-scale human behavioural imitation is a promising
tool to create intelligent, interactive agents, and the challenge of reliably
evaluating such agents is possible to surmount.
</p>
<a href="http://arxiv.org/abs/2012.05672" target="_blank">arXiv:2012.05672</a> [<a href="http://arxiv.org/pdf/2012.05672" target="_blank">pdf</a>]

<h2>Recurrent Point Review Models. (arXiv:2012.05684v1 [cs.LG])</h2>
<h3>Kostadin Cvejoski, Ramses J. Sanchez, Bogdan Georgiev, Christian Bauckhage, Cesar Ojeda</h3>
<p>Deep neural network models represent the state-of-the-art methodologies for
natural language processing. Here we build on top of these methodologies to
incorporate temporal information and model how to review data changes with
time. Specifically, we use the dynamic representations of recurrent point
process models, which encode the history of how business or service reviews are
received in time, to generate instantaneous language models with improved
prediction capabilities. Simultaneously, our methodologies enhance the
predictive power of our point process models by incorporating summarized review
content representations. We provide recurrent network and temporal convolution
solutions for modeling the review content. We deploy our methodologies in the
context of recommender systems, effectively characterizing the change in
preference and taste of users as time evolves. Source code is available at [1].
</p>
<a href="http://arxiv.org/abs/2012.05684" target="_blank">arXiv:2012.05684</a> [<a href="http://arxiv.org/pdf/2012.05684" target="_blank">pdf</a>]

<h2>Generative Deep Learning Techniques for Password Generation. (arXiv:2012.05685v1 [cs.LG])</h2>
<h3>David Biesner, Kostadin Cvejoski, Bogdan Georgiev, Rafet Sifa, Erik Krupicka</h3>
<p>Password guessing approaches via deep learning have recently been
investigated with significant breakthroughs in their ability to generate novel,
realistic password candidates. In the present work we study a broad collection
of deep learning and probabilistic based models in the light of password
guessing: attention-based deep neural networks, autoencoding mechanisms and
generative adversarial networks. We provide novel generative deep-learning
models in terms of variational autoencoders exhibiting state-of-art sampling
performance, yielding additional latent-space features such as interpolations
and targeted sampling. Lastly, we perform a thorough empirical analysis in a
unified controlled framework over well-known datasets (RockYou, LinkedIn,
Youku, Zomato, Pwnd). Our results not only identify the most promising schemes
driven by deep neural networks, but also illustrate the strengths of each
approach in terms of generation variability and sample uniqueness.
</p>
<a href="http://arxiv.org/abs/2012.05685" target="_blank">arXiv:2012.05685</a> [<a href="http://arxiv.org/pdf/2012.05685" target="_blank">pdf</a>]

<h2>DA-HGT: Domain Adaptive Heterogeneous Graph Transformer. (arXiv:2012.05688v1 [cs.LG])</h2>
<h3>Tiancheng Huang, Ke Xu, Donglin Wang</h3>
<p>Domain adaptation using graph networks is to learn label-discriminative and
network-invariant node embeddings by sharing graph parameters. Most existing
works focus on domain adaptation of homogeneous networks, and just a few works
begin to study heterogeneous cases that only consider the shared node types but
ignore the private node types in individual networks. However, for a given
source and target heterogeneous networks, they generally contain shared and
private node types, where private types bring an extra challenge for graph
domain adaptation. In this paper, we investigate Heterogeneous Information
Networks (HINs) with partial shared node types and propose a novel domain
adaptive heterogeneous graph transformer (DA-HGT) to handle the domain shift
between them. DA-HGT can not only align the distributions of identical-type
nodes and edges in two HINs but also make full use of different-type nodes and
edges to improve the performance of knowledge transfer. Extensive experiments
on several datasets demonstrate that DA-HGT can outperform state-of-the-art
methods in various domain adaptation tasks across heterogeneous networks.
</p>
<a href="http://arxiv.org/abs/2012.05688" target="_blank">arXiv:2012.05688</a> [<a href="http://arxiv.org/pdf/2012.05688" target="_blank">pdf</a>]

<h2>Interactive Fusion of Multi-level Features for Compositional Activity Recognition. (arXiv:2012.05689v1 [cs.CV])</h2>
<h3>Rui Yan, Lingxi Xie, Xiangbo Shu, Jinhui Tang</h3>
<p>To understand a complex action, multiple sources of information, including
appearance, positional, and semantic features, need to be integrated. However,
these features are difficult to be fused since they often differ significantly
in modality and dimensionality. In this paper, we present a novel framework
that accomplishes this goal by interactive fusion, namely, projecting features
across different spaces and guiding it using an auxiliary prediction task.
Specifically, we implement the framework in three steps, namely,
positional-to-appearance feature extraction, semantic feature interaction, and
semantic-to-positional prediction. We evaluate our approach on two action
recognition datasets, Something-Something and Charades. Interactive fusion
achieves consistent accuracy gain beyond off-the-shelf action recognition
algorithms. In particular, on Something-Else, the compositional setting of
Something-Something, interactive fusion reports a remarkable gain of 2.9% in
terms of top-1 accuracy.
</p>
<a href="http://arxiv.org/abs/2012.05689" target="_blank">arXiv:2012.05689</a> [<a href="http://arxiv.org/pdf/2012.05689" target="_blank">pdf</a>]

<h2>Lookahead optimizer improves the performance of Convolutional Autoencoders for reconstruction of natural images. (arXiv:2012.05694v1 [cs.CV])</h2>
<h3>Sayan Nag</h3>
<p>Autoencoders are a class of artificial neural networks which have gained a
lot of attention in the recent past. Using the encoder block of an autoencoder
the input image can be compressed into a meaningful representation. Then a
decoder is employed to reconstruct the compressed representation back to a
version which looks like the input image. It has plenty of applications in the
field of data compression and denoising. Another version of Autoencoders (AE)
exist, called Variational AE (VAE) which acts as a generative model like GAN.
Recently, an optimizer was introduced which is known as lookahead optimizer
which significantly enhances the performances of Adam as well as SGD. In this
paper, we implement Convolutional Autoencoders (CAE) and Convolutional
Variational Autoencoders (CVAE) with lookahead optimizer (with Adam) and
compare them with the Adam (only) optimizer counterparts. For this purpose, we
have used a movie dataset comprising of natural images for the former case and
CIFAR100 for the latter case. We show that lookahead optimizer (with Adam)
improves the performance of CAEs for reconstruction of natural images.
</p>
<a href="http://arxiv.org/abs/2012.05694" target="_blank">arXiv:2012.05694</a> [<a href="http://arxiv.org/pdf/2012.05694" target="_blank">pdf</a>]

<h2>Increased performance in DDM analysis by calculating structure functions through Fourier transform in time. (arXiv:2012.05695v1 [cs.CV])</h2>
<h3>M. Norouzisadeh, G. Cerchiari, F. Croccolo</h3>
<p>Differential Dynamic Microscopy (DDM) is the combination of optical
microscopy to statistical analysis to obtain information about the dynamical
behaviour of a variety of samples spanning from soft matter physics to biology.
In DDM, the dynamical evolution of the samples is investigated separately at
different length scales and extracted from a set of images recorded at
different times. A specific result of interest is the structure function that
can be computed via spatial Fourier transforms and differences of signals. In
this work, we present an algorithm to efficiently process a set of images
according to the DDM analysis scheme. We bench-marked the new approach against
the state-of-the-art algorithm reported in previous work. The new
implementation computes the DDM analysis faster, thanks to an additional
Fourier transform in time instead of performing differences of signals. This
allows obtaining very fast analysis also in CPU based machine. In order to test
the new code, we performed the DDM analysis over sets of more than 1000 images
with and without the help of GPU hardware acceleration. As an example, for
images of $512 \times 512$ pixels, the new algorithm is 10 times faster than
the previous GPU code. Without GPU hardware acceleration and for the same set
of images, we found that the new algorithm is 300 faster than the old one both
running only on the CPU.
</p>
<a href="http://arxiv.org/abs/2012.05695" target="_blank">arXiv:2012.05695</a> [<a href="http://arxiv.org/pdf/2012.05695" target="_blank">pdf</a>]

<h2>Independent Sign Language Recognition with 3D Body, Hands, and Face Reconstruction. (arXiv:2012.05698v1 [cs.CV])</h2>
<h3>Agelos Kratimenos, Georgios Pavlakos, Petros Maragos</h3>
<p>Independent Sign Language Recognition is a complex visual recognition problem
that combines several challenging tasks of Computer Vision due to the necessity
to exploit and fuse information from hand gestures, body features and facial
expressions. While many state-of-the-art works have managed to deeply elaborate
on these features independently, to the best of our knowledge, no work has
adequately combined all three information channels to efficiently recognize
Sign Language. In this work, we employ SMPL-X, a contemporary parametric model
that enables joint extraction of 3D body shape, face and hands information from
a single image. We use this holistic 3D reconstruction for SLR, demonstrating
that it leads to higher accuracy than recognition from raw RGB images and their
optical flow fed into the state-of-the-art I3D-type network for 3D action
recognition and from 2D Openpose skeletons fed into a Recurrent Neural Network.
Finally, a set of experiments on the body, face and hand features showed that
neglecting any of these, significantly reduces the classification accuracy,
proving the importance of jointly modeling body shape, facial expression and
hand pose for Sign Language Recognition.
</p>
<a href="http://arxiv.org/abs/2012.05698" target="_blank">arXiv:2012.05698</a> [<a href="http://arxiv.org/pdf/2012.05698" target="_blank">pdf</a>]

<h2>An Analysis of Deep Object Detectors For Diver Detection. (arXiv:2012.05701v1 [cs.CV])</h2>
<h3>Karin de Langis, Michael Fulton, Junaed Sattar</h3>
<p>With the end goal of selecting and using diver detection models to support
human-robot collaboration capabilities such as diver following, we thoroughly
analyze a large set of deep neural networks for diver detection. We begin by
producing a dataset of approximately 105,000 annotated images of divers sourced
from videos -- one of the largest and most varied diver detection datasets ever
created. Using this dataset, we train a variety of state-of-the-art deep neural
networks for object detection, including SSD with Mobilenet, Faster R-CNN, and
YOLO. Along with these single-frame detectors, we also train networks designed
for detection of objects in a video stream, using temporal information as well
as single-frame image information. We evaluate these networks on typical
accuracy and efficiency metrics, as well as on the temporal stability of their
detections. Finally, we analyze the failures of these detectors, pointing out
the most common scenarios of failure. Based on our results, we recommend SSDs
or Tiny-YOLOv4 for real-time applications on robots and recommend further
investigation of video object detection methods.
</p>
<a href="http://arxiv.org/abs/2012.05701" target="_blank">arXiv:2012.05701</a> [<a href="http://arxiv.org/pdf/2012.05701" target="_blank">pdf</a>]

<h2>TFPnP: Tuning-free Plug-and-Play Proximal Algorithm with Applications to Inverse Imaging Problems. (arXiv:2012.05703v1 [cs.CV])</h2>
<h3>Kaixuan Wei, Angelica Aviles-Rivero, Jingwei Liang, Ying Fu, Hua Huang, Carola-Bibiane Sch&#xf6;nlieb</h3>
<p>Plug-and-Play (PnP) is a non-convex framework that combines proximal
algorithms, for example alternating direction method of multipliers (ADMM),
with advanced denoiser priors. Over the past few years, great empirical success
has been obtained by PnP algorithms, especially for the ones integrated with
deep learning-based denoisers. However, a crucial issue of PnP approaches is
the need of manual parameter tweaking. As it is essential to obtain
high-quality results across the high discrepancy in terms of imaging conditions
and varying scene content. In this work, we present a tuning-free PnP proximal
algorithm, which can automatically determine the internal parameters including
the penalty parameter, the denoising strength and the termination time. A core
part of our approach is to develop a policy network for automatic search of
parameters, which can be effectively learned via mixed model-free and
model-based deep reinforcement learning. We demonstrate, through a set of
numerical and visual experiments, that the learned policy can customize
different parameters for different states, and often more efficient and
effective than existing handcrafted criteria. Moreover, we discuss the
practical considerations of the plugged denoisers, which together with our
learned policy yield to state-of-the-art results. This is prevalent on both
linear and nonlinear exemplary inverse imaging problems, and in particular, we
show promising results on compressed sensing MRI, sparse-view CT and phase
retrieval.
</p>
<a href="http://arxiv.org/abs/2012.05703" target="_blank">arXiv:2012.05703</a> [<a href="http://arxiv.org/pdf/2012.05703" target="_blank">pdf</a>]

<h2>Automatic Micro-sleep Detection under Car-driving Simulation Environment using Night-sleep EEG. (arXiv:2012.05705v1 [cs.LG])</h2>
<h3>Young-Seok Kweon, Gi-Hwan Shin, Heon-Gyu Kwak, Minji Lee</h3>
<p>A micro-sleep is a short sleep that lasts from 1 to 30 secs. Its detection
during driving is crucial to prevent accidents that could claim a lot of
people's lives. Electroencephalogram (EEG) is suitable to detect micro-sleep
because EEG was associated with consciousness and sleep. Deep learning showed
great performance in recognizing brain states, but sufficient data should be
needed. However, collecting micro-sleep data during driving is inefficient and
has a high risk of obtaining poor data quality due to noisy driving situations.
Night-sleep data at home is easier to collect than micro-sleep data during
driving. Therefore, we proposed a deep learning approach using night-sleep EEG
to improve the performance of micro-sleep detection. We pre-trained the U-Net
to classify the 5-class sleep stages using night-sleep EEG and used the sleep
stages estimated by the U-Net to detect micro-sleep during driving. This
improved micro-sleep detection performance by about 30\% compared to the
traditional approach. Our approach was based on the hypothesis that micro-sleep
corresponds to the early stage of non-rapid eye movement (NREM) sleep. We
analyzed EEG distribution during night-sleep and micro-sleep and found that
micro-sleep has a similar distribution to NREM sleep. Our results provide the
possibility of similarity between micro-sleep and the early stage of NREM sleep
and help prevent micro-sleep during driving.
</p>
<a href="http://arxiv.org/abs/2012.05705" target="_blank">arXiv:2012.05705</a> [<a href="http://arxiv.org/pdf/2012.05705" target="_blank">pdf</a>]

<h2>Look Before you Speak: Visually Contextualized Utterances. (arXiv:2012.05710v1 [cs.CV])</h2>
<h3>Paul Hongsuck Seo, Arsha Nagrani, Cordelia Schmid</h3>
<p>While most conversational AI systems focus on textual dialogue only,
conditioning utterances on visual context (when it's available) can lead to
more realistic conversations. Unfortunately, a major challenge for
incorporating visual context into conversational dialogue is the lack of
large-scale labeled datasets. We provide a solution in the form of a new
visually conditioned Future Utterance Prediction task. Our task involves
predicting the next utterance in a video, using both visual frames and
transcribed speech as context. By exploiting the large number of instructional
videos online, we train a model to solve this task at scale, without the need
for manual annotations. Leveraging recent advances in multimodal learning, our
model consists of a novel co-attentional multimodal video transformer, and when
trained on both textual and visual context, outperforms baselines that use
textual inputs alone. Further, we demonstrate that our model trained for this
task on unlabelled videos achieves state-of-the-art performance on a number of
downstream VideoQA benchmarks such as MSRVTT-QA, MSVD-QA, ActivityNet-QA and
How2QA.
</p>
<a href="http://arxiv.org/abs/2012.05710" target="_blank">arXiv:2012.05710</a> [<a href="http://arxiv.org/pdf/2012.05710" target="_blank">pdf</a>]

<h2>Using Differentiable Programming for Flexible Statistical Modeling. (arXiv:2012.05722v1 [cs.LG])</h2>
<h3>Maren Hackenberg, Marlon Grodd, Clemens Kreutz, Martina Fischer, Janina Esins, Linus Grabenhenrich, Christian Karagiannidis, Harald Binder</h3>
<p>Differentiable programming has recently received much interest as a paradigm
that facilitates taking gradients of computer programs. While the corresponding
flexible gradient-based optimization approaches so far have been used
predominantly for deep learning or enriching the latter with modeling
components, we want to demonstrate that they can also be useful for statistical
modeling per se, e.g., for quick prototyping when classical maximum likelihood
approaches are challenging or not feasible. In an application from a COVID-19
setting, we utilize differentiable programming to quickly build and optimize a
flexible prediction model adapted to the data quality challenges at hand.
Specifically, we develop a regression model, inspired by delay differential
equations, that can bridge temporal gaps of observations in the central German
registry of COVID-19 intensive care cases for predicting future demand. With
this exemplary modeling challenge, we illustrate how differentiable programming
can enable simple gradient-based optimization of the model by automatic
differentiation. This allowed us to quickly prototype a model under time
pressure that outperforms simpler benchmark models. We thus exemplify the
potential of differentiable programming also outside deep learning
applications, to provide more options for flexible applied statistical
modeling.
</p>
<a href="http://arxiv.org/abs/2012.05722" target="_blank">arXiv:2012.05722</a> [<a href="http://arxiv.org/pdf/2012.05722" target="_blank">pdf</a>]

<h2>Improving healthcare access management by predicting patient no-show behaviour. (arXiv:2012.05724v1 [cs.LG])</h2>
<h3>David Barrera Ferro, Sally Brailsford, Cristi&#xe1;n Bravo, Honora Smith</h3>
<p>Low attendance levels in medical appointments have been associated with poor
health outcomes and efficiency problems for service providers. To address this
problem, healthcare managers could aim at improving attendance levels or
minimizing the operational impact of no-shows by adapting resource allocation
policies. However, given the uncertainty of patient behaviour, generating
relevant information regarding no-show probabilities could support the
decision-making process for both approaches. In this context many researchers
have used multiple regression models to identify patient and appointment
characteristics than can be used as good predictors for no-show probabilities.
This work develops a Decision Support System (DSS) to support the
implementation of strategies to encourage attendance, for a preventive care
program targeted at underserved communities in Bogot\'a, Colombia. Our
contribution to literature is threefold. Firstly, we assess the effectiveness
of different machine learning approaches to improve the accuracy of regression
models. In particular, Random Forest and Neural Networks are used to model the
problem accounting for non-linearity and variable interactions. Secondly, we
propose a novel use of Layer-wise Relevance Propagation in order to improve the
explainability of neural network predictions and obtain insights from the
modelling step. Thirdly, we identify variables explaining no-show probabilities
in a developing context and study its policy implications and potential for
improving healthcare access. In addition to quantifying relationships reported
in previous studies, we find that income and neighbourhood crime statistics
affect no-show probabilities. Our results will support patient prioritization
in a pilot behavioural intervention and will inform appointment planning
decisions.
</p>
<a href="http://arxiv.org/abs/2012.05724" target="_blank">arXiv:2012.05724</a> [<a href="http://arxiv.org/pdf/2012.05724" target="_blank">pdf</a>]

<h2>HRCenterNet: An Anchorless Approach to Chinese Character Segmentation in Historical Documents. (arXiv:2012.05739v1 [cs.CV])</h2>
<h3>Chia-Wei Tang, Chao-Lin Liu, Po-Sen Chiu</h3>
<p>The information provided by historical documents has always been
indispensable in the transmission of human civilization, but it has also made
these books susceptible to damage due to various factors. Thanks to recent
technology, the automatic digitization of these documents are one of the
quickest and most effective means of preservation. The main steps of automatic
text digitization can be divided into two stages, mainly: character
segmentation and character recognition, where the recognition results depend
largely on the accuracy of segmentation. Therefore, in this study, we will only
focus on the character segmentation of historical Chinese documents. In this
research, we propose a model named HRCenterNet, which is combined with an
anchorless object detection method and parallelized architecture. The MTHv2
dataset consists of over 3000 Chinese historical document images and over 1
million individual Chinese characters; with these enormous data, the
segmentation capability of our model achieves IoU 0.81 on average with the best
speed-accuracy trade-off compared to the others. Our source code is available
at https://github.com/Tverous/HRCenterNet.
</p>
<a href="http://arxiv.org/abs/2012.05739" target="_blank">arXiv:2012.05739</a> [<a href="http://arxiv.org/pdf/2012.05739" target="_blank">pdf</a>]

<h2>R-AGNO-RPN: A LIDAR-Camera Region Deep Network for Resolution-Agnostic Detection. (arXiv:2012.05740v1 [cs.CV])</h2>
<h3>Ruddy Th&#xe9;odose, Dieumet Denis, Thierry Chateau, Vincent Fr&#xe9;mont, Paul Checchin</h3>
<p>Current neural networks-based object detection approaches processing LiDAR
point clouds are generally trained from one kind of LiDAR sensors. However,
their performances decrease when they are tested with data coming from a
different LiDAR sensor than the one used for training, i.e., with a different
point cloud resolution. In this paper, R-AGNO-RPN, a region proposal network
built on fusion of 3D point clouds and RGB images is proposed for 3D object
detection regardless of point cloud resolution. As our approach is designed to
be also applied on low point cloud resolutions, the proposed method focuses on
object localization instead of estimating refined boxes on reduced data. The
resilience to low-resolution point cloud is obtained through image features
accurately mapped to Bird's Eye View and a specific data augmentation procedure
that improves the contribution of the RGB images. To show the proposed
network's ability to deal with different point clouds resolutions, experiments
are conducted on both data coming from the KITTI 3D Object Detection and the
nuScenes datasets. In addition, to assess its performances, our method is
compared to PointPillars, a well-known 3D detection network. Experimental
results show that even on point cloud data reduced by $80\%$ of its original
points, our method is still able to deliver relevant proposals localization.
</p>
<a href="http://arxiv.org/abs/2012.05740" target="_blank">arXiv:2012.05740</a> [<a href="http://arxiv.org/pdf/2012.05740" target="_blank">pdf</a>]

<h2>Scalable and interpretable rule-based link prediction for large heterogeneous knowledge graphs. (arXiv:2012.05750v1 [cs.LG])</h2>
<h3>Simon Ott, Laura Graf, Asan Agibetov, Christian Meilicke, Matthias Samwald</h3>
<p>Neural embedding-based machine learning models have shown promise for
predicting novel links in biomedical knowledge graphs. Unfortunately, their
practical utility is diminished by their lack of interpretability. Recently,
the fully interpretable, rule-based algorithm AnyBURL yielded highly
competitive results on many general-purpose link prediction benchmarks.
However, its applicability to large-scale prediction tasks on complex
biomedical knowledge bases is limited by long inference times and difficulties
with aggregating predictions made by multiple rules. We improve upon AnyBURL by
introducing the SAFRAN rule application framework which aggregates rules
through a scalable clustering algorithm. SAFRAN yields new state-of-the-art
results for fully interpretable link prediction on the established
general-purpose benchmark FB15K-237 and the large-scale biomedical benchmark
OpenBioLink. Furthermore, it exceeds the results of multiple established
embedding-based algorithms on FB15K-237 and narrows the gap between rule-based
and embedding-based algorithms on OpenBioLink. We also show that SAFRAN
increases inference speeds by up to two orders of magnitude.
</p>
<a href="http://arxiv.org/abs/2012.05750" target="_blank">arXiv:2012.05750</a> [<a href="http://arxiv.org/pdf/2012.05750" target="_blank">pdf</a>]

<h2>Thompson Sampling for CVaR Bandits. (arXiv:2012.05754v1 [cs.LG])</h2>
<h3>Dorian Baudry, Romain Gautron, Emilie Kaufmann, Odalric-Ambryn Maillard</h3>
<p>Risk awareness is an important feature to formulate a variety of real world
problems. In this paper we study a multi-arm bandit problem in which the
quality of each arm is measured by the Conditional Value at Risk (CVaR) at some
level {\alpha} of the reward distribution. While existing works in this setting
mainly focus on Upper Confidence Bound algorithms, we introduce the first
Thompson Sampling approaches for CVaR bandits. Building on a recent work by
Riou and Honda (2020), we propose {\alpha}-NPTS for bounded rewards and
{\alpha}-Multinomial-TS for multinomial distributions. We provide a novel lower
bound on the CVaR regret which extends the concept of asymptotic optimality to
CVaR bandits and prove that {\alpha}-Multinomial-TS is the first algorithm to
achieve this lower bound. Finally, we demonstrate empirically the benefit of
Thompson Sampling approaches over their UCB counterparts.
</p>
<a href="http://arxiv.org/abs/2012.05754" target="_blank">arXiv:2012.05754</a> [<a href="http://arxiv.org/pdf/2012.05754" target="_blank">pdf</a>]

<h2>Adversarial Linear Contextual Bandits with Graph-Structured Side Observations. (arXiv:2012.05756v1 [cs.LG])</h2>
<h3>Lingda Wang, Bingcong Li, Huozhi Zhou, Georgios B. Giannakis, Lav R. Varshney, Zhizhen Zhao</h3>
<p>This paper studies the adversarial graphical contextual bandits, a variant of
adversarial multi-armed bandits that leverage two categories of the most common
side information: \emph{contexts} and \emph{side observations}. In this
setting, a learning agent repeatedly chooses from a set of $K$ actions after
being presented with a $d$-dimensional context vector. The agent not only
incurs and observes the loss of the chosen action, but also observes the losses
of its neighboring actions in the observation structures, which are encoded as
a series of feedback graphs. This setting models a variety of applications in
social networks, where both contexts and graph-structured side observations are
available. Two efficient algorithms are developed based on \texttt{EXP3}. Under
mild conditions, our analysis shows that for undirected feedback graphs the
first algorithm, \texttt{EXP3-LGC-U}, achieves the regret of order
$\mathcal{O}(\sqrt{(K+\alpha(G)d)T\log{K}})$ over the time horizon $T$, where
$\alpha(G)$ is the average \emph{independence number} of the feedback graphs. A
slightly weaker result is presented for the directed graph setting as well. The
second algorithm, \texttt{EXP3-LGC-IX}, is developed for a special class of
problems, for which the regret is reduced to
$\mathcal{O}(\sqrt{\alpha(G)dT\log{K}\log(KT)})$ for both directed as well as
undirected feedback graphs. Numerical tests corroborate the efficiency of
proposed algorithms.
</p>
<a href="http://arxiv.org/abs/2012.05756" target="_blank">arXiv:2012.05756</a> [<a href="http://arxiv.org/pdf/2012.05756" target="_blank">pdf</a>]

<h2>Large Non-Stationary Noisy Covariance Matrices: A Cross-Validation Approach. (arXiv:2012.05757v1 [stat.ML])</h2>
<h3>Vincent W. C. Tan, Stefan Zohren</h3>
<p>We introduce a novel covariance estimator that exploits the heteroscedastic
nature of financial time series by employing exponential weighted moving
averages and shrinking the in-sample eigenvalues through cross-validation. Our
estimator is model-agnostic in that we make no assumptions on the distribution
of the random entries of the matrix or structure of the covariance matrix.
Additionally, we show how Random Matrix Theory can provide guidance for
automatic tuning of the hyperparameter which characterizes the time scale for
the dynamics of the estimator. By attenuating the noise from both the
cross-sectional and time-series dimensions, we empirically demonstrate the
superiority of our estimator over competing estimators that are based on
exponentially-weighted and uniformly-weighted covariance matrices.
</p>
<a href="http://arxiv.org/abs/2012.05757" target="_blank">arXiv:2012.05757</a> [<a href="http://arxiv.org/pdf/2012.05757" target="_blank">pdf</a>]

<h2>Notes on Deep Learning Theory. (arXiv:2012.05760v1 [cs.LG])</h2>
<h3>Eugene A. Golikov</h3>
<p>These are the notes for the lectures that I was giving during Fall 2020 at
the Moscow Institute of Physics and Technology (MIPT) and at the Yandex School
of Data Analysis (YSDA). The notes cover some aspects of initialization, loss
landscape, generalization, and a neural tangent kernel theory. While many other
topics (e.g. expressivity, a mean-field theory, a double descent phenomenon)
are missing in the current version, we plan to add them in future revisions.
</p>
<a href="http://arxiv.org/abs/2012.05760" target="_blank">arXiv:2012.05760</a> [<a href="http://arxiv.org/pdf/2012.05760" target="_blank">pdf</a>]

<h2>Deep-CR MTLR: a Multi-Modal Approach for Cancer Survival Prediction with Competing Risks. (arXiv:2012.05765v1 [cs.LG])</h2>
<h3>Sejin Kim, Michal Kazmierski, Benjamin Haibe-Kains</h3>
<p>Accurate survival prediction is crucial for development of precision cancer
medicine, creating the need for new sources of prognostic information.
Recently, there has been significant interest in exploiting routinely collected
clinical and medical imaging data to discover new prognostic markers in
multiple cancer types. However, most of the previous studies focus on
individual data modalities alone and do not make use of recent advances in
machine learning for survival prediction. We present Deep-CR MTLR -- a novel
machine learning approach for accurate cancer survival prediction from
multi-modal clinical and imaging data in the presence of competing risks based
on neural networks and an extension of the multi-task logistic regression
framework. We demonstrate improved prognostic performance of the multi-modal
approach over single modality predictors in a cohort of 2552 head and neck
cancer patients, particularly for cancer specific survival, where our approach
achieves 2-year AUROC of 0.774 and $C$-index of 0.788.
</p>
<a href="http://arxiv.org/abs/2012.05765" target="_blank">arXiv:2012.05765</a> [<a href="http://arxiv.org/pdf/2012.05765" target="_blank">pdf</a>]

<h2>DAX: Deep Argumentative eXplanation for Neural Networks. (arXiv:2012.05766v1 [cs.AI])</h2>
<h3>Emanuele Albini, Piyawat Lertvittayakumjorn, Antonio Rago, Francesca Toni</h3>
<p>Despite the rapid growth in attention on eXplainable AI (XAI) of late,
explanations in the literature provide little insight into the actual
functioning of Neural Networks (NNs), significantly limiting their
transparency. We propose a methodology for explaining NNs, providing
transparency about their inner workings, by utilising computational
argumentation (a form of symbolic AI offering reasoning abstractions for a
variety of settings where opinions matter) as the scaffolding underpinning Deep
Argumentative eXplanations (DAXs). We define three DAX instantiations (for
various neural architectures and tasks) and evaluate them empirically in terms
of stability, computational cost, and importance of depth. We also conduct
human experiments with DAXs for text classification models, indicating that
they are comprehensible to humans and align with their judgement, while also
being competitive, in terms of user acceptance, with existing approaches to XAI
that also have an argumentative spirit.
</p>
<a href="http://arxiv.org/abs/2012.05766" target="_blank">arXiv:2012.05766</a> [<a href="http://arxiv.org/pdf/2012.05766" target="_blank">pdf</a>]

<h2>Influence-Driven Explanations for Bayesian Network Classifiers. (arXiv:2012.05773v1 [cs.AI])</h2>
<h3>Antonio Rago, Emanuele Albini, Pietro Baroni, Francesca Toni</h3>
<p>One of the most pressing issues in AI in recent years has been the need to
address the lack of explainability of many of its models. We focus on
explanations for discrete Bayesian network classifiers (BCs), targeting greater
transparency of their inner workings by including intermediate variables in
explanations, rather than just the input and output variables as is standard
practice. The proposed influence-driven explanations (IDXs) for BCs are
systematically generated using the causal relationships between variables
within the BC, called influences, which are then categorised by logical
requirements, called relation properties, according to their behaviour. These
relation properties both provide guarantees beyond heuristic explanation
methods and allow the information underpinning an explanation to be tailored to
a particular context's and user's requirements, e.g., IDXs may be dialectical
or counterfactual. We demonstrate IDXs' capability to explain various forms of
BCs, e.g., naive or multi-label, binary or categorical, and also integrate
recent approaches to explanations for BCs from the literature. We evaluate IDXs
with theoretical and empirical analyses, demonstrating their considerable
advantages when compared with existing explanation methods.
</p>
<a href="http://arxiv.org/abs/2012.05773" target="_blank">arXiv:2012.05773</a> [<a href="http://arxiv.org/pdf/2012.05773" target="_blank">pdf</a>]

<h2>OneNet: Towards End-to-End One-Stage Object Detection. (arXiv:2012.05780v1 [cs.CV])</h2>
<h3>Peize Sun, Yi Jiang, Enze Xie, Zehuan Yuan, Changhu Wang, Ping Luo</h3>
<p>End-to-end one-stage object detection trailed thus far. This paper discovers
that the lack of classification cost between sample and ground-truth in label
assignment is the main obstacle for one-stage detectors to remove Non-maximum
Suppression(NMS) and reach end-to-end. Existing one-stage object detectors
assign labels by only location cost, e.g. box IoU or point distance. Without
classification cost, sole location cost leads to redundant boxes of high
confidence scores in inference, making NMS necessary post-processing. To design
an end-to-end one-stage object detector, we propose Minimum Cost Assignment.
The cost is the summation of classification cost and location cost between
sample and ground-truth. For each object ground-truth, only one sample of
minimum cost is assigned as the positive sample; others are all negative
samples. To evaluate the effectiveness of our method, we design an extremely
simple one-stage detector named OneNet. Our results show that when trained with
Minimum Cost Assignment, OneNet avoids producing duplicated boxes and achieves
to end-to-end detector. On COCO dataset, OneNet achieves 35.0 AP/80 FPS and
37.7 AP/50 FPS with image size of 512 pixels. We hope OneNet could serve as an
effective baseline for end-to-end one-stage object detection. The code is
available at: \url{https://github.com/PeizeSun/OneNet}.
</p>
<a href="http://arxiv.org/abs/2012.05780" target="_blank">arXiv:2012.05780</a> [<a href="http://arxiv.org/pdf/2012.05780" target="_blank">pdf</a>]

<h2>A Study of Condition Numbers for First-Order Optimization. (arXiv:2012.05782v1 [cs.LG])</h2>
<h3>Charles Guille-Escuret, Baptiste Goujaud, Manuela Girotti, Ioannis Mitliagkas</h3>
<p>The study of first-order optimization algorithms (FOA) typically starts with
assumptions on the objective functions, most commonly smoothness and strong
convexity. These metrics are used to tune the hyperparameters of FOA. We
introduce a class of perturbations quantified via a new norm, called *-norm. We
show that adding a small perturbation to the objective function has an
equivalently small impact on the behavior of any FOA, which suggests that it
should have a minor impact on the tuning of the algorithm. However, we show
that smoothness and strong convexity can be heavily impacted by arbitrarily
small perturbations, leading to excessively conservative tunings and
convergence issues. In view of these observations, we propose a notion of
continuity of the metrics, which is essential for a robust tuning strategy.
Since smoothness and strong convexity are not continuous, we propose a
comprehensive study of existing alternative metrics which we prove to be
continuous. We describe their mutual relations and provide their guaranteed
convergence rates for the Gradient Descent algorithm accordingly tuned. Finally
we discuss how our work impacts the theoretical understanding of FOA and their
performances.
</p>
<a href="http://arxiv.org/abs/2012.05782" target="_blank">arXiv:2012.05782</a> [<a href="http://arxiv.org/pdf/2012.05782" target="_blank">pdf</a>]

<h2>Stochastic Damped L-BFGS with Controlled Norm of the Hessian Approximation. (arXiv:2012.05783v1 [cs.LG])</h2>
<h3>Sanae Lotfi, Tiphaine Bonniot de Ruisselet, Dominique Orban, Andrea Lodi</h3>
<p>We propose a new stochastic variance-reduced damped L-BFGS algorithm, where
we leverage estimates of bounds on the largest and smallest eigenvalues of the
Hessian approximation to balance its quality and conditioning. Our algorithm,
VARCHEN, draws from previous work that proposed a novel stochastic damped
L-BFGS algorithm called SdLBFGS. We establish almost sure convergence to a
stationary point and a complexity bound. We empirically demonstrate that
VARCHEN is more robust than SdLBFGS-VR and SVRG on a modified DavidNet problem
-- a highly nonconvex and ill-conditioned problem that arises in the context of
deep learning, and their performance is comparable on a logistic regression
problem and a nonconvex support-vector machine problem.
</p>
<a href="http://arxiv.org/abs/2012.05783" target="_blank">arXiv:2012.05783</a> [<a href="http://arxiv.org/pdf/2012.05783" target="_blank">pdf</a>]

<h2>Appliance-Level Monitoring with Micro-Moment Smart Plugs. (arXiv:2012.05787v1 [cs.LG])</h2>
<h3>Abdullah Alsalemi, Yassine Himeur, Faycal Bensaali, Abbes Amira</h3>
<p>Human population are striving against energy-related issues that not only
affects society and the development of the world, but also causes global
warming. A variety of broad approaches have been developed by both industry and
the research community. However, there is an ever increasing need for
comprehensive, end-to-end solutions aimed at transforming human behavior rather
than device metrics and benchmarks. In this paper, a micro-moment-based smart
plug system is proposed as part of a larger multi-appliance energy efficiency
program. The smart plug, which includes two sub-units: the power consumption
unit and environmental monitoring unit collect energy consumption of appliances
along with contextual information, such as temperature, humidity, luminosity
and room occupancy respectively. The plug also allows home automation
capability. With the accompanying mobile application, end-users can visualize
energy consumption data along with ambient environmental information. Current
implementation results show that the proposed system delivers cost-effective
deployment while maintaining adequate computation and wireless performance.
</p>
<a href="http://arxiv.org/abs/2012.05787" target="_blank">arXiv:2012.05787</a> [<a href="http://arxiv.org/pdf/2012.05787" target="_blank">pdf</a>]

<h2>Machine Learning Information Fusion in Earth Observation: A Comprehensive Review of Methods, Applications and Data Sources. (arXiv:2012.05795v1 [cs.CV])</h2>
<h3>S. Salcedo-Sanz, P. Ghamisi, M. Piles, M. Werner, L. Cuadra, A. Moreno-Mart&#xed;nez, E. Izquierdo-Verdiguier, J. Mu&#xf1;oz-Mar&#xed;, Amirhosein Mosavi, G. Camps-Valls</h3>
<p>This paper reviews the most important information fusion data-driven
algorithms based on Machine Learning (ML) techniques for problems in Earth
observation. Nowadays we observe and model the Earth with a wealth of
observations, from a plethora of different sensors, measuring states, fluxes,
processes and variables, at unprecedented spatial and temporal resolutions.
Earth observation is well equipped with remote sensing systems, mounted on
satellites and airborne platforms, but it also involves in-situ observations,
numerical models and social media data streams, among other data sources.
Data-driven approaches, and ML techniques in particular, are the natural choice
to extract significant information from this data deluge. This paper produces a
thorough review of the latest work on information fusion for Earth observation,
with a practical intention, not only focusing on describing the most relevant
previous works in the field, but also the most important Earth observation
applications where ML information fusion has obtained significant results. We
also review some of the most currently used data sets, models and sources for
Earth observation problems, describing their importance and how to obtain the
data when needed. Finally, we illustrate the application of ML data fusion with
a representative set of case studies, as well as we discuss and outlook the
near future of the field.
</p>
<a href="http://arxiv.org/abs/2012.05795" target="_blank">arXiv:2012.05795</a> [<a href="http://arxiv.org/pdf/2012.05795" target="_blank">pdf</a>]

<h2>Demystifying Pseudo-LiDAR for Monocular 3D Object Detection. (arXiv:2012.05796v1 [cs.CV])</h2>
<h3>Andrea Simonelli, Samuel Rota Bul&#xf2;, Lorenzo Porzi, Peter Kontschieder, Elisa Ricci</h3>
<p>Pseudo-LiDAR-based methods for monocular 3D object detection have generated
large attention in the community due to performance gains showed on the KITTI3D
benchmark dataset, in particular on the commonly reported validation split.
This generated a distorted impression about the superiority of Pseudo-LiDAR
approaches against methods working with RGB-images only. Our first contribution
consists in rectifying this view by analysing and showing experimentally that
the validation results published by Pseudo-LiDAR-based methods are
substantially biased. The source of the bias resides in an overlap between the
KITTI3D object detection validation set and the training/validation sets used
to train depth predictors feeding Pseudo-LiDAR-based methods. Surprisingly, the
bias remains also after geographically removing the overlap, revealing the
presence of a more structured contamination. This leaves the test set as the
only reliable mean of comparison, where published Pseudo-LiDAR-based methods do
not excel. Our second contribution brings Pseudo-LiDAR-based methods back up in
the ranking with the introduction of a 3D confidence prediction module. Thanks
to the proposed architectural changes, our modified Pseudo-LiDAR-based methods
exhibit extraordinary gains on the test scores (up to +8% 3D AP).
</p>
<a href="http://arxiv.org/abs/2012.05796" target="_blank">arXiv:2012.05796</a> [<a href="http://arxiv.org/pdf/2012.05796" target="_blank">pdf</a>]

<h2>Efficient Nonlinear RX Anomaly Detectors. (arXiv:2012.05799v1 [cs.CV])</h2>
<h3>Jos&#xe9; A. Padr&#xf3;n Hidalgo, Adri&#xe1;n P&#xe9;rez-Suay, Fatih Nar, Gustau Camps-Valls</h3>
<p>Current anomaly detection algorithms are typically challenged by either
accuracy or efficiency. More accurate nonlinear detectors are typically slow
and not scalable. In this letter, we propose two families of techniques to
improve the efficiency of the standard kernel Reed-Xiaoli (RX) method for
anomaly detection by approximating the kernel function with either {\em
data-independent} random Fourier features or {\em data-dependent} basis with
the Nystr\"om approach. We compare all methods for both real multi- and
hyperspectral images. We show that the proposed efficient methods have a lower
computational cost and they perform similar (or outperform) the standard kernel
RX algorithm thanks to their implicit regularization effect. Last but not
least, the Nystr\"om approach has an improved power of detection.
</p>
<a href="http://arxiv.org/abs/2012.05799" target="_blank">arXiv:2012.05799</a> [<a href="http://arxiv.org/pdf/2012.05799" target="_blank">pdf</a>]

<h2>Sylvester Matrix Based Similarity Estimation Method for Automation of Defect Detection in Textile Fabrics. (arXiv:2012.05800v1 [cs.CV])</h2>
<h3>R.M.L.N. Kumari, G.A.C.T. Bandara, Maheshi B. Dissanayake</h3>
<p>Fabric defect detection is a crucial quality control step in the textile
manufacturing industry. In this article, machine vision system based on the
Sylvester Matrix Based Similarity Method (SMBSM) is proposed to automate the
defect detection process. The algorithm involves six phases, namely resolution
matching, image enhancement using Histogram Specification and Median-Mean Based
Sub-Image-Clipped Histogram Equalization, image registration through alignment
and hysteresis process, image subtraction, edge detection, and fault detection
by means of the rank of the Sylvester matrix. The experimental results
demonstrate that the proposed method is robust and yields an accuracy of 93.4%,
precision of 95.8%, with 2275 ms computational speed.
</p>
<a href="http://arxiv.org/abs/2012.05800" target="_blank">arXiv:2012.05800</a> [<a href="http://arxiv.org/pdf/2012.05800" target="_blank">pdf</a>]

<h2>Multi-expert learning of adaptive legged locomotion. (arXiv:2012.05810v1 [cs.RO])</h2>
<h3>Chuanyu Yang, Kai Yuan, Qiuguo Zhu, Wanming Yu, Zhibin Li</h3>
<p>Achieving versatile robot locomotion requires motor skills which can adapt to
previously unseen situations. We propose a Multi-Expert Learning Architecture
(MELA) that learns to generate adaptive skills from a group of representative
expert skills. During training, MELA is first initialised by a distinct set of
pre-trained experts, each in a separate deep neural network (DNN). Then by
learning the combination of these DNNs using a Gating Neural Network (GNN),
MELA can acquire more specialised experts and transitional skills across
various locomotion modes. During runtime, MELA constantly blends multiple DNNs
and dynamically synthesises a new DNN to produce adaptive behaviours in
response to changing situations. This approach leverages the advantages of
trained expert skills and the fast online synthesis of adaptive policies to
generate responsive motor skills during the changing tasks. Using a unified
MELA framework, we demonstrated successful multi-skill locomotion on a real
quadruped robot that performed coherent trotting, steering, and fall recovery
autonomously, and showed the merit of multi-expert learning generating
behaviours which can adapt to unseen scenarios.
</p>
<a href="http://arxiv.org/abs/2012.05810" target="_blank">arXiv:2012.05810</a> [<a href="http://arxiv.org/pdf/2012.05810" target="_blank">pdf</a>]

<h2>Learn what you can't learn: Regularized Ensembles for Transductive Out-of-distribution Detection. (arXiv:2012.05825v1 [cs.LG])</h2>
<h3>Alexandru &#x162;ifrea, Eric Stavarache, Fanny Yang</h3>
<p>Machine learning models are often used in practice if they achieve good
generalization results on in-distribution (ID) holdout data. When employed in
the wild, they should also be able to detect samples they cannot predict well.
We show that current out-of-distribution (OOD) detection algorithms for neural
networks produce unsatisfactory results in a variety of OOD detection
scenarios, e.g. when OOD data consists of unseen classes or corrupted
measurements. This paper studies how such "hard" OOD scenarios can benefit from
adjusting the detection method after observing a batch of the test data. This
transductive setting is relevant when the advantage of even a slightly delayed
OOD detection outweighs the financial cost for additional tuning. We propose a
novel method that uses an artificial labeling scheme for the test data and
regularization to obtain ensembles of models that produce contradictory
predictions only on the OOD samples in a test batch. We show via comprehensive
experiments that our approach is indeed able to significantly outperform both
inductive and transductive baselines on difficult OOD detection scenarios, such
as unseen classes on CIFAR-10/CIFAR-100, severe corruptions(CIFAR-C), and
strong covariate shift (ImageNet vs ObjectNet).
</p>
<a href="http://arxiv.org/abs/2012.05825" target="_blank">arXiv:2012.05825</a> [<a href="http://arxiv.org/pdf/2012.05825" target="_blank">pdf</a>]

<h2>Full-Glow: Fully conditional Glow for more realistic image generation. (arXiv:2012.05846v1 [cs.CV])</h2>
<h3>Moein Sorkhei, Gustav Eje Henter, Hedvig Kjellstr&#xf6;m</h3>
<p>Autonomous agents, such as driverless cars, require large amounts of labeled
visual data for their training. A viable approach for acquiring such data is
training a generative model with collected real data, and then augmenting the
collected real dataset with synthetic images from the model, generated with
control of the scene layout and ground truth labeling. In this paper we propose
Full-Glow, a fully conditional Glow-based architecture for generating plausible
and realistic images of novel street scenes given a semantic segmentation map
indicating the scene layout. Benchmark comparisons show our model to outperform
recent works in terms of the semantic segmentation performance of a pretrained
PSPNet. This indicates that images from our model are, to a higher degree than
from other models, similar to real images of the same kinds of scenes and
objects, making them suitable as training data for a visual semantic
segmentation or object recognition system.
</p>
<a href="http://arxiv.org/abs/2012.05846" target="_blank">arXiv:2012.05846</a> [<a href="http://arxiv.org/pdf/2012.05846" target="_blank">pdf</a>]

<h2>Adapting the Human: Leveraging Wearable Technology in HRI. (arXiv:2012.05854v1 [cs.RO])</h2>
<h3>David Puljiz, Bj&#xf6;rn Hein</h3>
<p>Adhering to current HRI paradigms, all of the sensors, visualisation and
legibility of actions and motions are borne by the robot or its working cell.
This necessarily makes robots more complex or confines them into specialised,
structured environments. We propose leveraging the state of the art of wearable
technologies, such as augmented reality head mounted displays, smart watches,
sensor tags and radio-frequency ranging, to "adapt" the human and reduce the
requirements and complexity of robots.
</p>
<a href="http://arxiv.org/abs/2012.05854" target="_blank">arXiv:2012.05854</a> [<a href="http://arxiv.org/pdf/2012.05854" target="_blank">pdf</a>]

<h2>SPAA: Stealthy Projector-based Adversarial Attacks on Deep Image Classifiers. (arXiv:2012.05858v1 [cs.CV])</h2>
<h3>Bingyao Huang, Haibin Ling</h3>
<p>Light-based adversarial attacks aim to fool deep learning-based image
classifiers by altering the physical light condition using a controllable light
source, e.g., a projector. Compared with physical attacks that place carefully
designed stickers or printed adversarial objects, projector-based ones obviate
modifying the physical entities. Moreover, projector-based attacks can be
performed transiently and dynamically by altering the projection pattern.
However, existing approaches focus on projecting adversarial patterns that
result in clearly perceptible camera-captured perturbations, while the more
interesting yet challenging goal, stealthy projector-based attack, remains an
open problem. In this paper, for the first time, we formulate this problem as
an end-to-end differentiable process and propose Stealthy Projector-based
Adversarial Attack (SPAA). In SPAA, we approximate the real project-and-capture
operation using a deep neural network named PCNet, then we include PCNet in the
optimization of projector-based attacks such that the generated adversarial
projection is physically plausible. Finally, to generate robust and stealthy
adversarial projections, we propose an optimization algorithm that uses minimum
perturbation and adversarial confidence thresholds to alternate between the
adversarial loss and stealthiness loss optimization. Our experimental
evaluations show that the proposed SPAA clearly outperforms other methods by
achieving higher attack success rates and meanwhile being stealthier.
</p>
<a href="http://arxiv.org/abs/2012.05858" target="_blank">arXiv:2012.05858</a> [<a href="http://arxiv.org/pdf/2012.05858" target="_blank">pdf</a>]

<h2>GNN-XML: Graph Neural Networks for Extreme Multi-label Text Classification. (arXiv:2012.05860v1 [cs.AI])</h2>
<h3>Daoming Zong, Shiliang Sun</h3>
<p>Extreme multi-label text classification (XMTC) aims to tag a text instance
with the most relevant subset of labels from an extremely large label set. XMTC
has attracted much recent attention due to massive label sets yielded by modern
applications, such as news annotation and product recommendation. The main
challenges of XMTC are the data scalability and sparsity, thereby leading to
two issues: i) the intractability to scale to the extreme label setting, ii)
the presence of long-tailed label distribution, implying that a large fraction
of labels have few positive training instances. To overcome these problems, we
propose GNN-XML, a scalable graph neural network framework tailored for XMTC
problems. Specifically, we exploit label correlations via mining their
co-occurrence patterns and build a label graph based on the correlation matrix.
We then conduct the attributed graph clustering by performing graph convolution
with a low-pass graph filter to jointly model label dependencies and label
features, which induces semantic label clusters. We further propose a
bilateral-branch graph isomorphism network to decouple representation learning
and classifier learning for better modeling tail labels. Experimental results
on multiple benchmark datasets show that GNN-XML significantly outperforms
state-of-the-art methods while maintaining comparable prediction efficiency and
model size.
</p>
<a href="http://arxiv.org/abs/2012.05860" target="_blank">arXiv:2012.05860</a> [<a href="http://arxiv.org/pdf/2012.05860" target="_blank">pdf</a>]

<h2>Understanding Learned Reward Functions. (arXiv:2012.05862v1 [cs.LG])</h2>
<h3>Eric J. Michaud, Adam Gleave, Stuart Russell</h3>
<p>In many real-world tasks, it is not possible to procedurally specify an RL
agent's reward function. In such cases, a reward function must instead be
learned from interacting with and observing humans. However, current techniques
for reward learning may fail to produce reward functions which accurately
reflect user preferences. Absent significant advances in reward learning, it is
thus important to be able to audit learned reward functions to verify whether
they truly capture user preferences. In this paper, we investigate techniques
for interpreting learned reward functions. In particular, we apply saliency
methods to identify failure modes and predict the robustness of reward
functions. We find that learned reward functions often implement surprising
algorithms that rely on contingent aspects of the environment. We also discover
that existing interpretability techniques often attend to irrelevant changes in
reward output, suggesting that reward interpretability may need significantly
different methods from policy interpretability.
</p>
<a href="http://arxiv.org/abs/2012.05862" target="_blank">arXiv:2012.05862</a> [<a href="http://arxiv.org/pdf/2012.05862" target="_blank">pdf</a>]

<h2>Neurosymbolic AI: The 3rd Wave. (arXiv:2012.05876v1 [cs.AI])</h2>
<h3>Artur d&#x27;Avila Garcez, Luis C. Lamb</h3>
<p>Current advances in Artificial Intelligence (AI) and Machine Learning (ML)
have achieved unprecedented impact across research communities and industry.
Nevertheless, concerns about trust, safety, interpretability and accountability
of AI were raised by influential thinkers. Many have identified the need for
well-founded knowledge representation and reasoning to be integrated with deep
learning and for sound explainability. Neural-symbolic computing has been an
active area of research for many years seeking to bring together robust
learning in neural networks with reasoning and explainability via symbolic
representations for network models. In this paper, we relate recent and early
research results in neurosymbolic AI with the objective of identifying the key
ingredients of the next wave of AI systems. We focus on research that
integrates in a principled way neural network-based learning with symbolic
knowledge representation and logical reasoning. The insights provided by 20
years of neural-symbolic computing are shown to shed new light onto the
increasingly prominent role of trust, safety, interpretability and
accountability of AI. We also identify promising directions and challenges for
the next decade of AI research from the perspective of neural-symbolic systems.
</p>
<a href="http://arxiv.org/abs/2012.05876" target="_blank">arXiv:2012.05876</a> [<a href="http://arxiv.org/pdf/2012.05876" target="_blank">pdf</a>]

<h2>iNeRF: Inverting Neural Radiance Fields for Pose Estimation. (arXiv:2012.05877v1 [cs.CV])</h2>
<h3>Lin Yen-Chen, Pete Florence, Jonathan T. Barron, Alberto Rodriguez, Phillip Isola, Tsung-Yi Lin</h3>
<p>We present iNeRF, a framework that performs pose estimation by "inverting" a
trained Neural Radiance Field (NeRF). NeRFs have been shown to be remarkably
effective for the task of view synthesis - synthesizing photorealistic novel
views of real-world scenes or objects. In this work, we investigate whether we
can apply analysis-by-synthesis with NeRF for 6DoF pose estimation - given an
image, find the translation and rotation of a camera relative to a 3D model.
Starting from an initial pose estimate, we use gradient descent to minimize the
residual between pixels rendered from an already-trained NeRF and pixels in an
observed image. In our experiments, we first study 1) how to sample rays during
pose refinement for iNeRF to collect informative gradients and 2) how different
batch sizes of rays affect iNeRF on a synthetic dataset. We then show that for
complex real-world scenes from the LLFF dataset, iNeRF can improve NeRF by
estimating the camera poses of novel images and using these images as
additional training data for NeRF. Finally, we show iNeRF can be combined with
feature-based pose initialization. The approach outperforms all other RGB-based
methods relying on synthetic data on LineMOD.
</p>
<a href="http://arxiv.org/abs/2012.05877" target="_blank">arXiv:2012.05877</a> [<a href="http://arxiv.org/pdf/2012.05877" target="_blank">pdf</a>]

<h2>Flatland-RL : Multi-Agent Reinforcement Learning on Trains. (arXiv:2012.05893v1 [cs.AI])</h2>
<h3>Sharada Mohanty, Erik Nygren, Florian Laurent, Manuel Schneider, Christian Scheller, Nilabha Bhattacharya, Jeremy Watson, Adrian Egli, Christian Eichenberger, Christian Baumberger, Gereon Vienken, Irene Sturm, Guillaume Sartoretti, Giacomo Spigler</h3>
<p>Efficient automated scheduling of trains remains a major challenge for modern
railway systems. The underlying vehicle rescheduling problem (VRSP) has been a
major focus of Operations Research (OR) since decades. Traditional approaches
use complex simulators to study VRSP, where experimenting with a broad range of
novel ideas is time consuming and has a huge computational overhead. In this
paper, we introduce a two-dimensional simplified grid environment called
"Flatland" that allows for faster experimentation. Flatland does not only
reduce the complexity of the full physical simulation, but also provides an
easy-to-use interface to test novel approaches for the VRSP, such as
Reinforcement Learning (RL) and Imitation Learning (IL). In order to probe the
potential of Machine Learning (ML) research on Flatland, we (1) ran a first
series of RL and IL experiments and (2) design and executed a public Benchmark
at NeurIPS 2020 to engage a large community of researchers to work on this
problem. Our own experimental results, on the one hand, demonstrate that ML has
potential in solving the VRSP on Flatland. On the other hand, we identify key
topics that need further research. Overall, the Flatland environment has proven
to be a robust and valuable framework to investigate the VRSP for railway
networks. Our experiments provide a good starting point for further research
and for the participants of the NeurIPS 2020 Flatland Benchmark. All of these
efforts together have the potential to have a substantial impact on shaping the
mobility of the future.
</p>
<a href="http://arxiv.org/abs/2012.05893" target="_blank">arXiv:2012.05893</a> [<a href="http://arxiv.org/pdf/2012.05893" target="_blank">pdf</a>]

<h2>AutoSelect: Automatic and Dynamic Detection Selection for 3D Multi-Object Tracking. (arXiv:2012.05894v1 [cs.CV])</h2>
<h3>Xinshuo Weng, Kris Kitani</h3>
<p>3D multi-object tracking is an important component in robotic perception
systems such as self-driving vehicles. Recent work follows a
tracking-by-detection pipeline, which aims to match past tracklets with
detections in the current frame. To avoid matching with false positive
detections, prior work filters out detections with low confidence scores via a
threshold. However, finding a proper threshold is non-trivial, which requires
extensive manual search via ablation study. Also, this threshold is sensitive
to many factors such as target object category so we need to re-search the
threshold if these factors change. To ease this process, we propose to
automatically select high-quality detections and remove the efforts needed for
manual threshold search. Also, prior work often uses a single threshold per
data sequence, which is sub-optimal in particular frames or for certain
objects. Instead, we dynamically search threshold per frame or per object to
further boost performance. Through experiments on KITTI and nuScenes, our
method can filter out $45.7\%$ false positives while maintaining the recall,
achieving new S.O.T.A. performance and removing the need for manually threshold
tuning.
</p>
<a href="http://arxiv.org/abs/2012.05894" target="_blank">arXiv:2012.05894</a> [<a href="http://arxiv.org/pdf/2012.05894" target="_blank">pdf</a>]

<h2>Flexible Few-Shot Learning with Contextual Similarity. (arXiv:2012.05895v1 [cs.LG])</h2>
<h3>Mengye Ren, Eleni Triantafillou, Kuan-Chieh Wang, James Lucas, Jake Snell, Xaq Pitkow, Andreas S. Tolias, Richard Zemel</h3>
<p>Existing approaches to few-shot learning deal with tasks that have
persistent, rigid notions of classes. Typically, the learner observes data only
from a fixed number of classes at training time and is asked to generalize to a
new set of classes at test time. Two examples from the same class would always
be assigned the same labels in any episode. In this work, we consider a
realistic setting where the similarities between examples can change from
episode to episode depending on the task context, which is not given to the
learner. We define new benchmark datasets for this flexible few-shot scenario,
where the tasks are based on images of faces (Celeb-A), shoes (Zappos50K), and
general objects (ImageNet-with-Attributes). While classification baselines and
episodic approaches learn representations that work well for standard few-shot
learning, they suffer in our flexible tasks as novel similarity definitions
arise during testing. We propose to build upon recent contrastive unsupervised
learning techniques and use a combination of instance and class invariance
learning, aiming to obtain general and flexible features. We find that our
approach performs strongly on our new flexible few-shot learning benchmarks,
demonstrating that unsupervised learning obtains more generalizable
representations.
</p>
<a href="http://arxiv.org/abs/2012.05895" target="_blank">arXiv:2012.05895</a> [<a href="http://arxiv.org/pdf/2012.05895" target="_blank">pdf</a>]

<h2>Self-Supervised Learning of Lidar Segmentation for Autonomous Indoor Navigation. (arXiv:2012.05897v1 [cs.RO])</h2>
<h3>Hugues Thomas, Ben Agro, Mona Gridseth, Jian Zhang, Timothy D. Barfoot</h3>
<p>We present a self-supervised learning approach for the semantic segmentation
of lidar frames. Our method is used to train a deep point cloud segmentation
architecture without any human annotation. The annotation process is automated
with the combination of simultaneous localization and mapping (SLAM) and
ray-tracing algorithms. By performing multiple navigation sessions in the same
environment, we are able to identify permanent structures, such as walls, and
disentangle short-term and long-term movable objects, such as people and
tables, respectively. New sessions can then be performed using a network
trained to predict these semantic labels. We demonstrate the ability of our
approach to improve itself over time, from one session to the next. With
semantically filtered point clouds, our robot can navigate through more complex
scenarios, which, when added to the training pool, help to improve our network
predictions. We provide insights into our network predictions and show that our
approach can also improve the performances of common localization techniques.
</p>
<a href="http://arxiv.org/abs/2012.05897" target="_blank">arXiv:2012.05897</a> [<a href="http://arxiv.org/pdf/2012.05897" target="_blank">pdf</a>]

<h2>Are Fewer Labels Possible for Few-shot Learning?. (arXiv:2012.05899v1 [cs.CV])</h2>
<h3>Suichan Li, Dongdong Chen, Yinpeng Chen, Lu Yuan, Lei Zhang, Qi Chu, Nenghai Yu</h3>
<p>Few-shot learning is challenging due to its very limited data and labels.
Recent studies in big transfer (BiT) show that few-shot learning can greatly
benefit from pretraining on large scale labeled dataset in a different domain.
This paper asks a more challenging question: "can we use as few as possible
labels for few-shot learning in both pretraining (with no labels) and
fine-tuning (with fewer labels)?".

Our key insight is that the clustering of target samples in the feature space
is all we need for few-shot finetuning. It explains why the vanilla
unsupervised pretraining (poor clustering) is worse than the supervised one. In
this paper, we propose transductive unsupervised pretraining that achieves a
better clustering by involving target data even though its amount is very
limited. The improved clustering result is of great value for identifying the
most representative samples ("eigen-samples") for users to label, and in
return, continued finetuning with the labeled eigen-samples further improves
the clustering. Thus, we propose eigen-finetuning to enable fewer shot learning
by leveraging the co-evolution of clustering and eigen-samples in the
finetuning. We conduct experiments on 10 different few-shot target datasets,
and our average few-shot performance outperforms both vanilla inductive
unsupervised transfer and supervised transfer by a large margin. For instance,
when each target category only has 10 labeled samples, the mean accuracy gain
over the above two baselines is 9.2% and 3.42 respectively.
</p>
<a href="http://arxiv.org/abs/2012.05899" target="_blank">arXiv:2012.05899</a> [<a href="http://arxiv.org/pdf/2012.05899" target="_blank">pdf</a>]

<h2>Robust Consistent Video Depth Estimation. (arXiv:2012.05901v1 [cs.CV])</h2>
<h3>Johannes Kopf, Xuejian Rong, Jia-Bin Huang</h3>
<p>We present an algorithm for estimating consistent dense depth maps and camera
poses from a monocular video. We integrate a learning-based depth prior, in the
form of a convolutional neural network trained for single-image depth
estimation, with geometric optimization, to estimate a smooth camera trajectory
as well as detailed and stable depth reconstruction. Our algorithm combines two
complementary techniques: (1) flexible deformation-splines for low-frequency
large-scale alignment and (2) geometry-aware depth filtering for high-frequency
alignment of fine depth details. In contrast to prior approaches, our method
does not require camera poses as input and achieves robust reconstruction for
challenging hand-held cell phone captures containing a significant amount of
noise, shake, motion blur, and rolling shutter deformations. Our method
quantitatively outperforms state-of-the-arts on the Sintel benchmark for both
depth and pose estimations and attains favorable qualitative results across
diverse wild datasets.
</p>
<a href="http://arxiv.org/abs/2012.05901" target="_blank">arXiv:2012.05901</a> [<a href="http://arxiv.org/pdf/2012.05901" target="_blank">pdf</a>]

<h2>Portrait Neural Radiance Fields from a Single Image. (arXiv:2012.05903v1 [cs.CV])</h2>
<h3>Chen Gao, Yichang Shih, Wei-Sheng Lai, Chia-Kai Liang, Jia-Bin Huang</h3>
<p>We present a method for estimating Neural Radiance Fields (NeRF) from a
single headshot portrait. While NeRF has demonstrated high-quality view
synthesis, it requires multiple images of static scenes and thus impractical
for casual captures and moving subjects. In this work, we propose to pretrain
the weights of a multilayer perceptron (MLP), which implicitly models the
volumetric density and colors, with a meta-learning framework using a light
stage portrait dataset. To improve the generalization to unseen faces, we train
the MLP in the canonical coordinate space approximated by 3D face morphable
models. We quantitatively evaluate the method using controlled captures and
demonstrate the generalization to real portrait images, showing favorable
results against state-of-the-arts.
</p>
<a href="http://arxiv.org/abs/2012.05903" target="_blank">arXiv:2012.05903</a> [<a href="http://arxiv.org/pdf/2012.05903" target="_blank">pdf</a>]

<h2>Model-based Catheter Segmentation in MRI-images. (arXiv:1705.06712v2 [cs.CV] UPDATED)</h2>
<h3>Andre Mastmeyer, Guillaume Pernelle, Lauren Barber, Steve Pieper, Dirk Fortmeier, Sandy Wells, Heinz Handels, Tina Kapur</h3>
<p>Accurate and reliable segmentation of catheters in MR-guided interventions
remains a challenge, and a step of critical importance in clinical workflows.
In this work, under reasonable assumptions, mechanical model based heuristics
guide the segmentation process allows correct catheter identification rates
greater than 98% (error 2.88 mm), and reduction in outliers to one-fourth
compared to the state of the art. Given distal tips, searching towards the
proximal ends of the catheters is guided by mechanical models that are
estimated on a per-catheter basis. Their bending characteristics are used to
constrain the image feature based candidate points. The final catheter
trajectories are hybrid sequences of individual points, each derived from model
and image features. We evaluate the method on a database of 10 patient MRI
scans including 101 manually segmented catheters. The mean errors were 1.40 mm
and the median errors were 1.05 mm. The number of outliers deviating more than
2 mm from the gold standard is 7, and the number of outliers deviating more
than 3 mm from the gold standard is just 2.
</p>
<a href="http://arxiv.org/abs/1705.06712" target="_blank">arXiv:1705.06712</a> [<a href="http://arxiv.org/pdf/1705.06712" target="_blank">pdf</a>]

<h2>A New 3D Method to Segment the Lumbar Vertebral Bodies and to Determine Bone Mineral Density and Geometry. (arXiv:1705.07146v2 [cs.CV] UPDATED)</h2>
<h3>Andre Mastmeyer, Klaus Engelke, Sebastian Meller, Willi Kalender</h3>
<p>In this paper we present a new 3D segmentation approach for the vertebrae of
the lower thoracic and the lumbar spine in spiral computed tomography datasets.
We implemented a multi-step procedure. Its main components are deformable
models, volume growing, and morphological operations. The performance analysis
that included an evaluation of accuracy using the European Spine Phantom, and
of intra-operator precision using clinical CT datasets from 10 patients
highlight the potential for clinical use. The intra-operator precision of the
segmentation procedure was better than 1% for Bone Mineral Density (BMD) and
better than 1.8% for volume. The long-term goal of this work is to enable
better fracture prediction and improved patient monitoring in the field of
osteoporosis. A true 3D segmentation also enables an accurate measurement of
geometrical parameters that can augment the classical measurement of BMD.
</p>
<a href="http://arxiv.org/abs/1705.07146" target="_blank">arXiv:1705.07146</a> [<a href="http://arxiv.org/pdf/1705.07146" target="_blank">pdf</a>]

<h2>Low Rank Regularization: A Review. (arXiv:1808.04521v3 [cs.CV] UPDATED)</h2>
<h3>Zhanxuan Hu, Feiping Nie, Rong Wang, Xuelong Li</h3>
<p>Low rank regularization, in essence, involves introducing a low rank or
approximately low rank assumption for matrix we aim to learn, which has
achieved great success in many fields including machine learning, data mining
and computer version. Over the last decade, much progress has been made in
theories and practical applications. Nevertheless, the intersection between
them is very slight. In order to construct a bridge between practical
applications and theoretical research, in this paper we provide a comprehensive
survey for low rank regularization. We first review several traditional machine
learning models using low rank regularization, and then show their (or their
variants) applications in solving practical issues, such as non-rigid structure
from motion and image denoising. Subsequently, we summarize the regularizers
and optimization methods that achieve great success in traditional machine
learning tasks but are rarely seen in solving practical issues. Finally, we
provide a discussion and comparison for some representative regularizers
including convex and non-convex relaxations. Extensive experimental results
demonstrate that non-convex regularizers can provide a large advantage over the
nuclear norm, the regularizer widely used in solving practical issues.
</p>
<a href="http://arxiv.org/abs/1808.04521" target="_blank">arXiv:1808.04521</a> [<a href="http://arxiv.org/pdf/1808.04521" target="_blank">pdf</a>]

<h2>Learning Counterfactual Representations for Estimating Individual Dose-Response Curves. (arXiv:1902.00981v3 [cs.LG] UPDATED)</h2>
<h3>Patrick Schwab, Lorenz Linhardt, Stefan Bauer, Joachim M. Buhmann, Walter Karlen</h3>
<p>Estimating what would be an individual's potential response to varying levels
of exposure to a treatment is of high practical relevance for several important
fields, such as healthcare, economics and public policy. However, existing
methods for learning to estimate counterfactual outcomes from observational
data are either focused on estimating average dose-response curves, or limited
to settings with only two treatments that do not have an associated dosage
parameter. Here, we present a novel machine-learning approach towards learning
counterfactual representations for estimating individual dose-response curves
for any number of treatments with continuous dosage parameters with neural
networks. Building on the established potential outcomes framework, we
introduce performance metrics, model selection criteria, model architectures,
and open benchmarks for estimating individual dose-response curves. Our
experiments show that the methods developed in this work set a new
state-of-the-art in estimating individual dose-response.
</p>
<a href="http://arxiv.org/abs/1902.00981" target="_blank">arXiv:1902.00981</a> [<a href="http://arxiv.org/pdf/1902.00981" target="_blank">pdf</a>]

<h2>Wasserstein-2 Generative Networks. (arXiv:1909.13082v4 [cs.LG] UPDATED)</h2>
<h3>Alexander Korotin, Vage Egiazarian, Arip Asadulaev, Alexander Safin, Evgeny Burnaev</h3>
<p>We propose a novel end-to-end non-minimax algorithm for training optimal
transport mappings for the quadratic cost (Wasserstein-2 distance). The
algorithm uses input convex neural networks and a cycle-consistency
regularization to approximate Wasserstein-2 distance. In contrast to popular
entropic and quadratic regularizers, cycle-consistency does not introduce bias
and scales well to high dimensions. From the theoretical side, we estimate the
properties of the generative mapping fitted by our algorithm. From the
practical side, we evaluate our algorithm on a wide range of tasks:
image-to-image color transfer, latent space optimal transport, image-to-image
style transfer, and domain adaptation.
</p>
<a href="http://arxiv.org/abs/1909.13082" target="_blank">arXiv:1909.13082</a> [<a href="http://arxiv.org/pdf/1909.13082" target="_blank">pdf</a>]

<h2>MULTIPOLAR: Multi-Source Policy Aggregation for Transfer Reinforcement Learning between Diverse Environmental Dynamics. (arXiv:1909.13111v2 [cs.LG] UPDATED)</h2>
<h3>Mohammadamin Barekatain, Ryo Yonetani, Masashi Hamaya</h3>
<p>Transfer reinforcement learning (RL) aims at improving the learning
efficiency of an agent by exploiting knowledge from other source agents trained
on relevant tasks. However, it remains challenging to transfer knowledge
between different environmental dynamics without having access to the source
environments. In this work, we explore a new challenge in transfer RL, where
only a set of source policies collected under diverse unknown dynamics is
available for learning a target task efficiently. To address this problem, the
proposed approach, MULTI-source POLicy AggRegation (MULTIPOLAR), comprises two
key techniques. We learn to aggregate the actions provided by the source
policies adaptively to maximize the target task performance. Meanwhile, we
learn an auxiliary network that predicts residuals around the aggregated
actions, which ensures the target policy's expressiveness even when some of the
source policies perform poorly. We demonstrated the effectiveness of MULTIPOLAR
through an extensive experimental evaluation across six simulated environments
ranging from classic control problems to challenging robotics simulations,
under both continuous and discrete action spaces. The demo videos and code are
available on the project webpage: https://omron-sinicx.github.io/multipolar/.
</p>
<a href="http://arxiv.org/abs/1909.13111" target="_blank">arXiv:1909.13111</a> [<a href="http://arxiv.org/pdf/1909.13111" target="_blank">pdf</a>]

<h2>Learning event representations for temporal segmentation of image sequences by dynamic graph embedding. (arXiv:1910.03483v3 [cs.LG] UPDATED)</h2>
<h3>Mariella Dimiccoli, Herwig Wendt</h3>
<p>Recently, self-supervised learning has proved to be effective to learn
representations of events suitable for temporal segmentation in image
sequences, where events are understood as sets of temporally adjacent images
that are semantically perceived as a whole. However, although this approach
does not require expensive manual annotations, it is data hungry and suffers
from domain adaptation problems. As an alternative, in this work, we propose a
novel approach for learning event representations named Dynamic Graph Embedding
(DGE). The assumption underlying our model is that a sequence of images can be
represented by a graph that encodes both semantic and temporal similarity. The
key novelty of DGE is to learn jointly the graph and its graph embedding. At
its core, DGE works by iterating over two steps: 1) updating the graph
representing the semantic and temporal similarity of the data based on the
current data representation, and 2) updating the data representation to take
into account the current data graph structure. The main advantage of DGE over
state-of-the-art self-supervised approaches is that it does not require any
training set, but instead learns iteratively from the data itself a
low-dimensional embedding that reflects their temporal and semantic similarity.
Experimental results on two benchmark datasets of real image sequences captured
at regular time intervals demonstrate that the proposed DGE leads to event
representations effective for temporal segmentation. In particular, it achieves
robust temporal segmentation on the EDUBSeg and EDUBSeg-Desc benchmark
datasets, outperforming the state of the art. Additional experiments on two
Human Motion Segmentation benchmark datasets demonstrate the generalization
capabilities of the proposed DGE.
</p>
<a href="http://arxiv.org/abs/1910.03483" target="_blank">arXiv:1910.03483</a> [<a href="http://arxiv.org/pdf/1910.03483" target="_blank">pdf</a>]

<h2>HIGhER : Improving instruction following with Hindsight Generation for Experience Replay. (arXiv:1910.09451v3 [cs.LG] UPDATED)</h2>
<h3>Geoffrey Cideron, Mathieu Seurin, Florian Strub, Olivier Pietquin</h3>
<p>Language creates a compact representation of the world and allows the
description of unlimited situations and objectives through compositionality.
While these characterizations may foster instructing, conditioning or
structuring interactive agent behavior, it remains an open-problem to correctly
relate language understanding and reinforcement learning in even simple
instruction following scenarios. This joint learning problem is alleviated
through expert demonstrations, auxiliary losses, or neural inductive biases. In
this paper, we propose an orthogonal approach called Hindsight Generation for
Experience Replay (HIGhER) that extends the Hindsight Experience Replay (HER)
approach to the language-conditioned policy setting. Whenever the agent does
not fulfill its instruction, HIGhER learns to output a new directive that
matches the agent trajectory, and it relabels the episode with a positive
reward. To do so, HIGhER learns to map a state into an instruction by using
past successful trajectories, which removes the need to have external expert
interventions to relabel episodes as in vanilla HER. We show the efficiency of
our approach in the BabyAI environment, and demonstrate how it complements
other instruction following methods.
</p>
<a href="http://arxiv.org/abs/1910.09451" target="_blank">arXiv:1910.09451</a> [<a href="http://arxiv.org/pdf/1910.09451" target="_blank">pdf</a>]

<h2>Causal bootstrapping. (arXiv:1910.09648v3 [cs.LG] UPDATED)</h2>
<h3>Max A. Little, Reham Badawy</h3>
<p>To draw scientifically meaningful conclusions and build reliable models of
quantitative phenomena, cause and effect must be taken into consideration
(either implicitly or explicitly). This is particularly challenging when the
measurements are not from controlled experimental (interventional) settings,
since cause and effect can be obscured by spurious, indirect influences. Modern
predictive techniques from machine learning are capable of capturing
high-dimensional, nonlinear relationships between variables while relying on
few parametric or probabilistic model assumptions. However, since these
techniques are associational, applied to observational data they are prone to
picking up spurious influences from non-experimental (observational) data,
making their predictions unreliable. Techniques from causal inference, such as
probabilistic causal diagrams and do-calculus, provide powerful (nonparametric)
tools for drawing causal inferences from such observational data. However,
these techniques are often incompatible with modern, nonparametric machine
learning algorithms since they typically require explicit probabilistic models.
Here, we develop causal bootstrapping for augmenting classical nonparametric
bootstrap resampling with information on the causal relationship between
variables. This makes it possible to resample observational data such that, if
it is possible to identify an interventional relationship from that data, new
data representing that relationship can be simulated from the original
observational data. In this way, we can use modern machine learning algorithms
unaltered to make statistically powerful, yet causally-robust, predictions. We
develop several causal bootstrapping algorithms for drawing interventional
inferences from observational data, for classification and regression problems,
and demonstrate, using synthetic and real-world examples, the value of this
approach.
</p>
<a href="http://arxiv.org/abs/1910.09648" target="_blank">arXiv:1910.09648</a> [<a href="http://arxiv.org/pdf/1910.09648" target="_blank">pdf</a>]

<h2>Modality to Modality Translation: An Adversarial Representation Learning and Graph Fusion Network for Multimodal Fusion. (arXiv:1911.07848v4 [cs.CV] UPDATED)</h2>
<h3>Sijie Mai, Haifeng Hu, Songlong Xing</h3>
<p>Learning joint embedding space for various modalities is of vital importance
for multimodal fusion. Mainstream modality fusion approaches fail to achieve
this goal, leaving a modality gap which heavily affects cross-modal fusion. In
this paper, we propose a novel adversarial encoder-decoder-classifier framework
to learn a modality-invariant embedding space. Since the distributions of
various modalities vary in nature, to reduce the modality gap, we translate the
distributions of source modalities into that of target modality via their
respective encoders using adversarial training. Furthermore, we exert
additional constraints on embedding space by introducing reconstruction loss
and classification loss. Then we fuse the encoded representations using
hierarchical graph neural network which explicitly explores unimodal, bimodal
and trimodal interactions in multi-stage. Our method achieves state-of-the-art
performance on multiple datasets. Visualization of the learned embeddings
suggests that the joint embedding space learned by our method is
discriminative. code is available at:
\url{https://github.com/TmacMai/ARGF_multimodal_fusion}
</p>
<a href="http://arxiv.org/abs/1911.07848" target="_blank">arXiv:1911.07848</a> [<a href="http://arxiv.org/pdf/1911.07848" target="_blank">pdf</a>]

<h2>Risk-Averse Action Selection Using Extreme Value Theory Estimates of the CVaR. (arXiv:1912.01718v2 [stat.ML] UPDATED)</h2>
<h3>Dylan Troop, Fr&#xe9;d&#xe9;ric Godin, Jia Yuan Yu</h3>
<p>In a wide variety of sequential decision making problems, it can be important
to estimate the impact of rare events in order to minimize risk exposure. A
popular risk measure is the conditional value-at-risk (CVaR), which is commonly
estimated by averaging observations that occur beyond a quantile at a given
confidence level. When this confidence level is very high, this estimation
method can exhibit high variance due to the limited number of samples above the
corresponding quantile. To mitigate this problem, extreme value theory can be
used to derive an estimator for the CVaR that uses extrapolation beyond
available samples. This estimator requires the selection of a threshold
parameter to work well, which is a difficult challenge that has been widely
studied in the extreme value theory literature. In this paper, we present an
estimation procedure for the CVaR that combines extreme value theory and a
recently introduced method of automated threshold selection by
\cite{bader2018automated}. Under appropriate conditions, we estimate the tail
risk using a generalized Pareto distribution. We compare empirically this
estimation procedure with the commonly used method of sample averaging, and
show an improvement in performance for some distributions. We finally show how
the estimation procedure can be used in reinforcement learning by applying our
method to the multi-arm bandit problem where the goal is to avoid catastrophic
risk.
</p>
<a href="http://arxiv.org/abs/1912.01718" target="_blank">arXiv:1912.01718</a> [<a href="http://arxiv.org/pdf/1912.01718" target="_blank">pdf</a>]

<h2>Differentially Private Synthetic Mixed-Type Data Generation For Unsupervised Learning. (arXiv:1912.03250v2 [cs.LG] UPDATED)</h2>
<h3>Uthaipon Tantipongpipat, Chris Waites, Digvijay Boob, Amaresh Ankit Siva, Rachel Cummings</h3>
<p>We introduce the DP-auto-GAN framework for synthetic data generation, which
combines the low dimensional representation of autoencoders with the
flexibility of Generative Adversarial Networks (GANs). This framework can be
used to take in raw sensitive data and privately train a model for generating
synthetic data that will satisfy similar statistical properties as the original
data. This learned model can generate an arbitrary amount of synthetic data,
which can then be freely shared due to the post-processing guarantee of
differential privacy. Our framework is applicable to unlabeled mixed-type data,
that may include binary, categorical, and real-valued data. We implement this
framework on both binary data (MIMIC-III) and mixed-type data (ADULT), and
compare its performance with existing private algorithms on metrics in
unsupervised settings. We also introduce a new quantitative metric able to
detect diversity, or lack thereof, of synthetic data.
</p>
<a href="http://arxiv.org/abs/1912.03250" target="_blank">arXiv:1912.03250</a> [<a href="http://arxiv.org/pdf/1912.03250" target="_blank">pdf</a>]

<h2>WICA: nonlinear weighted ICA. (arXiv:2001.04147v2 [cs.LG] UPDATED)</h2>
<h3>Andrzej Bedychaj, Przemys&#x142;aw Spurek, Aleksandra Nowak, Jacek Tabor</h3>
<p>Independent Component Analysis (ICA) aims to find a coordinate system in
which the components of the data are independent. In this paper we construct a
new nonlinear ICA model, called WICA, which obtains better and more stable
results than other algorithms. A crucial tool is given by a new efficient
method of verifying nonlinear dependence with the use of computation of
correlation coefficients for normally weighted data. In addition, authors
propose a new baseline nonlinear mixing to perform comparable experiments, and
a~reliable measure which allows fair comparison of nonlinear models. Our code
for WICA is available on Github https://github.com/gmum/wica.
</p>
<a href="http://arxiv.org/abs/2001.04147" target="_blank">arXiv:2001.04147</a> [<a href="http://arxiv.org/pdf/2001.04147" target="_blank">pdf</a>]

<h2>Curriculum Labeling: Revisiting Pseudo-Labeling for Semi-Supervised Learning. (arXiv:2001.06001v2 [cs.LG] UPDATED)</h2>
<h3>Paola Cascante-Bonilla, Fuwen Tan, Yanjun Qi, Vicente Ordonez</h3>
<p>In this paper we revisit the idea of pseudo-labeling in the context of
semi-supervised learning where a learning algorithm has access to a small set
of labeled samples and a large set of unlabeled samples. Pseudo-labeling works
by applying pseudo-labels to samples in the unlabeled set by using a model
trained on the combination of the labeled samples and any previously
pseudo-labeled samples, and iteratively repeating this process in a
self-training cycle. Current methods seem to have abandoned this approach in
favor of consistency regularization methods that train models under a
combination of different styles of self-supervised losses on the unlabeled
samples and standard supervised losses on the labeled samples. We empirically
demonstrate that pseudo-labeling can in fact be competitive with the
state-of-the-art, while being more resilient to out-of-distribution samples in
the unlabeled set. We identify two key factors that allow pseudo-labeling to
achieve such remarkable results (1) applying curriculum learning principles and
(2) avoiding concept drift by restarting model parameters before each
self-training cycle. We obtain 94.91% accuracy on CIFAR-10 using only 4,000
labeled samples, and 68.87% top-1 accuracy on Imagenet-ILSVRC using only 10% of
the labeled samples. The code is available at
https://github.com/uvavision/Curriculum-Labeling
</p>
<a href="http://arxiv.org/abs/2001.06001" target="_blank">arXiv:2001.06001</a> [<a href="http://arxiv.org/pdf/2001.06001" target="_blank">pdf</a>]

<h2>Harvesting Ambient RF for Presence Detection Through Deep Learning. (arXiv:2002.05770v3 [cs.LG] UPDATED)</h2>
<h3>Yang Liu, Tiexing Wang, Yuexin Jiang, Biao Chen</h3>
<p>This paper explores the use of ambient radio frequency (RF) signals for human
presence detection through deep learning. Using WiFi signal as an example, we
demonstrate that the channel state information (CSI) obtained at the receiver
contains rich information about the propagation environment. Through judicious
pre-processing of the estimated CSI followed by deep learning, reliable
presence detection can be achieved. Several challenges in passive RF sensing
are addressed. With presence detection, how to collect training data with human
presence can have a significant impact on the performance. This is in contrast
to activity detection when a specific motion pattern is of interest. A second
challenge is that RF signals are complex-valued. Handling complex-valued input
in deep learning requires careful data representation and network architecture
design. Finally, human presence affects CSI variation along multiple
dimensions; such variation, however, is often masked by system impediments such
as timing or frequency offset. Addressing these challenges, the proposed
learning system uses pre-processing to preserve human motion induced channel
variation while insulating against other impairments. A convolutional neural
network (CNN) properly trained with both magnitude and phase information is
then designed to achieve reliable presence detection. Extensive experiments are
conducted. Using off-the-shelf WiFi devices, the proposed deep learning based
RF sensing achieves near perfect presence detection during multiple extended
periods of test and exhibits superior performance compared with leading edge
passive infrared sensors. Comparison with existing RF based human presence
detection also demonstrates its robustness in performance, especially when
deployed in a completely new environment.
</p>
<a href="http://arxiv.org/abs/2002.05770" target="_blank">arXiv:2002.05770</a> [<a href="http://arxiv.org/pdf/2002.05770" target="_blank">pdf</a>]

<h2>Randomization matters. How to defend against strong adversarial attacks. (arXiv:2002.11565v3 [cs.LG] UPDATED)</h2>
<h3>Rafael Pinot, Raphael Ettedgui, Geovani Rizk, Yann Chevaleyre, Jamal Atif</h3>
<p>Is there a classifier that ensures optimal robustness against all adversarial
attacks? This paper answers this question by adopting a game-theoretic point of
view. We show that adversarial attacks and defenses form an infinite zero-sum
game where classical results (e.g. Sion theorem) do not apply. We demonstrate
the non-existence of a Nash equilibrium in our game when the classifier and the
Adversary are both deterministic, hence giving a negative answer to the above
question in the deterministic regime. Nonetheless, the question remains open in
the randomized regime. We tackle this problem by showing that, undermild
conditions on the dataset distribution, any deterministic classifier can be
outperformed by a randomized one. This gives arguments for using randomization,
and leads us to a new algorithm for building randomized classifiers that are
robust to strong adversarial attacks. Empirical results validate our
theoretical analysis, and show that our defense method considerably outperforms
Adversarial Training against state-of-the-art attacks.
</p>
<a href="http://arxiv.org/abs/2002.11565" target="_blank">arXiv:2002.11565</a> [<a href="http://arxiv.org/pdf/2002.11565" target="_blank">pdf</a>]

<h2>Global Convergence and Geometric Characterization of Slow to Fast Weight Evolution in Neural Network Training for Classifying Linearly Non-Separable Data. (arXiv:2002.12563v3 [cs.LG] UPDATED)</h2>
<h3>Ziang Long, Penghang Yin, Jack Xin</h3>
<p>In this paper, we study the dynamics of gradient descent in learning neural
networks for classification problems. Unlike in existing works, we consider the
linearly non-separable case where the training data of different classes lie in
orthogonal subspaces. We show that when the network has sufficient (but not
exceedingly large) number of neurons, (1) the corresponding minimization
problem has a desirable landscape where all critical points are global minima
with perfect classification; (2) gradient descent is guaranteed to converge to
the global minima. Moreover, we discovered a geometric condition on the network
weights so that when it is satisfied, the weight evolution transitions from a
slow phase of weight direction spreading to a fast phase of weight convergence.
The geometric condition says that the convex hull of the weights projected on
the unit sphere contains the origin.
</p>
<a href="http://arxiv.org/abs/2002.12563" target="_blank">arXiv:2002.12563</a> [<a href="http://arxiv.org/pdf/2002.12563" target="_blank">pdf</a>]

<h2>Noise Estimation Using Density Estimation for Self-Supervised Multimodal Learning. (arXiv:2003.03186v3 [cs.CV] UPDATED)</h2>
<h3>Elad Amrani, Rami Ben-Ari, Daniel Rotman, Alex Bronstein</h3>
<p>One of the key factors of enabling machine learning models to comprehend and
solve real-world tasks is to leverage multimodal data. Unfortunately,
annotation of multimodal data is challenging and expensive. Recently,
self-supervised multimodal methods that combine vision and language were
proposed to learn multimodal representations without annotation. However, these
methods often choose to ignore the presence of high levels of noise and thus
yield sub-optimal results. In this work, we show that the problem of noise
estimation for multimodal data can be reduced to a multimodal density
estimation task. Using multimodal density estimation, we propose a noise
estimation building block for multimodal representation learning that is based
strictly on the inherent correlation between different modalities. We
demonstrate how our noise estimation can be broadly integrated and achieves
comparable results to state-of-the-art performance on five different benchmark
datasets for two challenging multimodal tasks: Video Question Answering and
Text-To-Video Retrieval. Furthermore, we provide a theoretical probabilistic
error bound substantiating our empirical results and analyze failure cases.
Code: https://github.com/elad-amrani/ssml.
</p>
<a href="http://arxiv.org/abs/2003.03186" target="_blank">arXiv:2003.03186</a> [<a href="http://arxiv.org/pdf/2003.03186" target="_blank">pdf</a>]

<h2>DeepPurpose: a Deep Learning Library for Drug-Target Interaction Prediction. (arXiv:2004.08919v3 [cs.LG] UPDATED)</h2>
<h3>Kexin Huang, Tianfan Fu, Lucas Glass, Marinka Zitnik, Cao Xiao, Jimeng Sun</h3>
<p>Accurate prediction of drug-target interactions (DTI) is crucial for drug
discovery. Recently, deep learning (DL) models for show promising performance
for DTI prediction. However, these models can be difficult to use for both
computer scientists entering the biomedical field and bioinformaticians with
limited DL experience. We present DeepPurpose, a comprehensive and easy-to-use
deep learning library for DTI prediction. DeepPurpose supports training of
customized DTI prediction models by implementing 15 compound and protein
encoders and over 50 neural architectures, along with providing many other
useful features. We demonstrate state-of-the-art performance of DeepPurpose on
several benchmark datasets.
</p>
<a href="http://arxiv.org/abs/2004.08919" target="_blank">arXiv:2004.08919</a> [<a href="http://arxiv.org/pdf/2004.08919" target="_blank">pdf</a>]

<h2>Transformer-Based Anomaly Segmentation. (arXiv:2005.02357v2 [cs.CV] UPDATED)</h2>
<h3>Niv Cohen, Yedid Hoshen</h3>
<p>The recent improvement in anomaly detection methods has prompted research
into anomaly segmentation i.e. finding the pixels of the image that contain
anomalies. In this paper, we investigate novel methods for unleashing the full
power of pretrained features for anomaly segmentation. We first present a
simple baseline that uses a pyramid of deep convolutional features and show
that it significantly improves over the state-of-the-art methods, which are
much more complex. One issue with the baseline approach is that it is unable to
use the global context of the image effectively. We show that global
attention-based methods are better able to utilize the global context.
Specifically, we present an approach based on a multi-scale transformer
architecture and show that it further improves performance. By analysing the
attention maps, we find that they often detect anomalous image regions in a
zero-shot fashion, providing some insight into the result. A qualitative
evaluation of our method shows significant gains.
</p>
<a href="http://arxiv.org/abs/2005.02357" target="_blank">arXiv:2005.02357</a> [<a href="http://arxiv.org/pdf/2005.02357" target="_blank">pdf</a>]

<h2>Fast and Effective Robustness Certification for Recurrent Neural Networks. (arXiv:2005.13300v2 [cs.LG] UPDATED)</h2>
<h3>Wonryong Ryou, Jiayu Chen, Mislav Balunovic, Gagandeep Singh, Andrei Dan, Martin Vechev</h3>
<p>We present a precise and scalable verifier for recurrent neural networks,
called R2. The verifier is based on two key ideas: (i) a method to compute
tight linear convex relaxations of a recurrent update function via sampling and
optimization, and (ii) a technique to optimize convex combinations of multiple
bounds for each neuron instead of a single bound as previously done. Using R2,
we present the first study of certifying a non-trivial use case of recurrent
neural networks, namely speech classification. This required us to also develop
custom convex relaxations for the general operations that make up speech
preprocessing. Our evaluation across a number of recurrent architectures in
computer vision and speech domains shows that these networks are out of reach
for existing methods as these are an order of magnitude slower than R2, while
R2 successfully verified robustness in many cases.
</p>
<a href="http://arxiv.org/abs/2005.13300" target="_blank">arXiv:2005.13300</a> [<a href="http://arxiv.org/pdf/2005.13300" target="_blank">pdf</a>]

<h2>Random Hyperboxes. (arXiv:2006.00695v2 [cs.LG] UPDATED)</h2>
<h3>Thanh Tung Khuat, Bogdan Gabrys</h3>
<p>This paper proposes a simple yet powerful ensemble classifier, called Random
Hyperboxes, constructed from individual hyperbox-based classifiers trained on
the random subsets of sample and feature spaces of the training set. We also
show a generalization error bound of the proposed classifier based on the
strength of the individual hyperbox-based classifiers as well as the
correlation among them. The effectiveness of the proposed classifier is
analyzed using a carefully selected illustrative example and compared
empirically with other popular single and ensemble classifiers via 20 datasets
using statistical testing methods. The experimental results confirmed that our
proposed method outperformed other fuzzy min-max neural networks, popular
learning algorithms, and is competitive with other ensemble methods. Finally,
we identify the existing issues related to the generalization error bounds of
the real datasets and inform the potential research directions.
</p>
<a href="http://arxiv.org/abs/2006.00695" target="_blank">arXiv:2006.00695</a> [<a href="http://arxiv.org/pdf/2006.00695" target="_blank">pdf</a>]

<h2>Learning disconnected manifolds: a no GANs land. (arXiv:2006.04596v3 [stat.ML] UPDATED)</h2>
<h3>Ugo Tanielian, Thibaut Issenhuth, Elvis Dohmatob, Jeremie Mary</h3>
<p>Typical architectures of Generative AdversarialNetworks make use of a
unimodal latent distribution transformed by a continuous generator.
Consequently, the modeled distribution always has connected support which is
cumbersome when learning a disconnected set of manifolds. We formalize this
problem by establishing a no free lunch theorem for the disconnected manifold
learning stating an upper bound on the precision of the targeted distribution.
This is done by building on the necessary existence of a low-quality region
where the generator continuously samples data between two disconnected modes.
Finally, we derive a rejection sampling method based on the norm of generators
Jacobian and show its efficiency on several generators including BigGAN.
</p>
<a href="http://arxiv.org/abs/2006.04596" target="_blank">arXiv:2006.04596</a> [<a href="http://arxiv.org/pdf/2006.04596" target="_blank">pdf</a>]

<h2>MeshWalker: Deep Mesh Understanding by Random Walks. (arXiv:2006.05353v3 [cs.CV] UPDATED)</h2>
<h3>Alon Lahav, Ayellet Tal</h3>
<p>Most attempts to represent 3D shapes for deep learning have focused on
volumetric grids, multi-view images and point clouds. In this paper we look at
the most popular representation of 3D shapes in computer graphics - a
triangular mesh - and ask how it can be utilized within deep learning. The few
attempts to answer this question propose to adapt convolutions &amp; pooling to
suit Convolutional Neural Networks (CNNs). This paper proposes a very different
approach, termed MeshWalker, to learn the shape directly from a given mesh. The
key idea is to represent the mesh by random walks along the surface, which
"explore" the mesh's geometry and topology. Each walk is organized as a list of
vertices, which in some manner imposes regularity on the mesh. The walk is fed
into a Recurrent Neural Network (RNN) that "remembers" the history of the walk.
We show that our approach achieves state-of-the-art results for two fundamental
shape analysis tasks: shape classification and semantic segmentation.
Furthermore, even a very small number of examples suffices for learning. This
is highly important, since large datasets of meshes are difficult to acquire.
</p>
<a href="http://arxiv.org/abs/2006.05353" target="_blank">arXiv:2006.05353</a> [<a href="http://arxiv.org/pdf/2006.05353" target="_blank">pdf</a>]

<h2>Low Distortion Block-Resampling with Spatially Stochastic Networks. (arXiv:2006.05394v2 [stat.ML] UPDATED)</h2>
<h3>Sarah Jane Hong, Martin Arjovsky, Darryl Barnhart, Ian Thompson</h3>
<p>We formalize and attack the problem of generating new images from old ones
that are as diverse as possible, only allowing them to change without
restrictions in certain parts of the image while remaining globally consistent.
This encompasses the typical situation found in generative modelling, where we
are happy with parts of the generated data, but would like to resample others
("I like this generated castle overall, but this tower looks unrealistic, I
would like a new one"). In order to attack this problem we build from the best
conditional and unconditional generative models to introduce a new network
architecture, training procedure, and algorithm for resampling parts of the
image as desired.
</p>
<a href="http://arxiv.org/abs/2006.05394" target="_blank">arXiv:2006.05394</a> [<a href="http://arxiv.org/pdf/2006.05394" target="_blank">pdf</a>]

<h2>Exploring the Vulnerability of Deep Neural Networks: A Study of Parameter Corruption. (arXiv:2006.05620v2 [cs.LG] UPDATED)</h2>
<h3>Xu Sun, Zhiyuan Zhang, Xuancheng Ren, Ruixuan Luo, Liangyou Li</h3>
<p>We argue that the vulnerability of model parameters is of crucial value to
the study of model robustness and generalization but little research has been
devoted to understanding this matter. In this work, we propose an indicator to
measure the robustness of neural network parameters by exploiting their
vulnerability via parameter corruption. The proposed indicator describes the
maximum loss variation in the non-trivial worst-case scenario under parameter
corruption. For practical purposes, we give a gradient-based estimation, which
is far more effective than random corruption trials that can hardly induce the
worst accuracy degradation. Equipped with theoretical support and empirical
validation, we are able to systematically investigate the robustness of
different model parameters and reveal vulnerability of deep neural networks
that has been rarely paid attention to before. Moreover, we can enhance the
models accordingly with the proposed adversarial corruption-resistant training,
which not only improves the parameter robustness but also translates into
accuracy elevation.
</p>
<a href="http://arxiv.org/abs/2006.05620" target="_blank">arXiv:2006.05620</a> [<a href="http://arxiv.org/pdf/2006.05620" target="_blank">pdf</a>]

<h2>Exploration by Maximizing R\'enyi Entropy for Reward-Free RL Framework. (arXiv:2006.06193v3 [cs.LG] UPDATED)</h2>
<h3>Chuheng Zhang, Yuanying Cai, Longbo Huang, Jian Li</h3>
<p>Exploration is essential for reinforcement learning (RL). To face the
challenges of exploration, we consider a reward-free RL framework that
completely separates exploration from exploitation and brings new challenges
for exploration algorithms. In the exploration phase, the agent learns an
exploratory policy by interacting with a reward-free environment and collects a
dataset of transitions by executing the policy. In the planning phase, the
agent computes a good policy for any reward function based on the dataset
without further interacting with the environment. This framework is suitable
for the meta RL setting where there are many reward functions of interest. In
the exploration phase, we propose to maximize the Renyi entropy over the
state-action space and justify this objective theoretically. The success of
using Renyi entropy as the objective results from its encouragement to explore
the hard-to-reach state-actions. We further deduce a policy gradient
formulation for this objective and design a practical exploration algorithm
that can deal with complex environments. In the planning phase, we solve for
good policies given arbitrary reward functions using a batch RL algorithm.
Empirically, we show that our exploration algorithm is effective and sample
efficient, and results in superior policies for arbitrary reward functions in
the planning phase.
</p>
<a href="http://arxiv.org/abs/2006.06193" target="_blank">arXiv:2006.06193</a> [<a href="http://arxiv.org/pdf/2006.06193" target="_blank">pdf</a>]

<h2>Untangling tradeoffs between recurrence and self-attention in neural networks. (arXiv:2006.09471v2 [cs.LG] UPDATED)</h2>
<h3>Giancarlo Kerg, Bhargav Kanuparthi, Anirudh Goyal, Kyle Goyette, Yoshua Bengio, Guillaume Lajoie</h3>
<p>Attention and self-attention mechanisms, are now central to state-of-the-art
deep learning on sequential tasks. However, most recent progress hinges on
heuristic approaches with limited understanding of attention's role in model
optimization and computation, and rely on considerable memory and computational
resources that scale poorly. In this work, we present a formal analysis of how
self-attention affects gradient propagation in recurrent networks, and prove
that it mitigates the problem of vanishing gradients when trying to capture
long-term dependencies by establishing concrete bounds for gradient norms.
Building on these results, we propose a relevancy screening mechanism, inspired
by the cognitive process of memory consolidation, that allows for a scalable
use of sparse self-attention with recurrence. While providing guarantees to
avoid vanishing gradients, we use simple numerical experiments to demonstrate
the tradeoffs in performance and computational resources by efficiently
balancing attention and recurrence. Based on our results, we propose a concrete
direction of research to improve scalability of attentive networks.
</p>
<a href="http://arxiv.org/abs/2006.09471" target="_blank">arXiv:2006.09471</a> [<a href="http://arxiv.org/pdf/2006.09471" target="_blank">pdf</a>]

<h2>The Recurrent Neural Tangent Kernel. (arXiv:2006.10246v3 [cs.LG] UPDATED)</h2>
<h3>Sina Alemohammad, Zichao Wang, Randall Balestriero, Richard Baraniuk</h3>
<p>The study of deep neural networks (DNNs) in the infinite-width limit, via the
so-called neural tangent kernel (NTK) approach, has provided new insights into
the dynamics of learning, generalization, and the impact of initialization. One
key DNN architecture remains to be kernelized, namely, the recurrent neural
network (RNN). In this paper we introduce and study the Recurrent Neural
Tangent Kernel (RNTK), which provides new insights into the behavior of
overparametrized RNNs, including how different time steps are weighted by the
RNTK to form the output under different initialization parameters and
nonlinearity choices, and how inputs of different lengths are treated. The
ability to compare inputs of different length is a property of RNTK that should
greatly benefit practitioners. We demonstrate via a synthetic and 56 real-world
data experiments that the RNTK offers significant performance gains over other
kernels, including standard NTKs, across a wide array of data sets.
</p>
<a href="http://arxiv.org/abs/2006.10246" target="_blank">arXiv:2006.10246</a> [<a href="http://arxiv.org/pdf/2006.10246" target="_blank">pdf</a>]

<h2>Adversarial Soft Advantage Fitting: Imitation Learning without Policy Optimization. (arXiv:2006.13258v5 [cs.LG] UPDATED)</h2>
<h3>Paul Barde, Julien Roy, Wonseok Jeon, Joelle Pineau, Christopher Pal, Derek Nowrouzezahrai</h3>
<p>Adversarial Imitation Learning alternates between learning a discriminator --
which tells apart expert's demonstrations from generated ones -- and a
generator's policy to produce trajectories that can fool this discriminator.
This alternated optimization is known to be delicate in practice since it
compounds unstable adversarial training with brittle and sample-inefficient
reinforcement learning. We propose to remove the burden of the policy
optimization steps by leveraging a novel discriminator formulation.
Specifically, our discriminator is explicitly conditioned on two policies: the
one from the previous generator's iteration and a learnable policy. When
optimized, this discriminator directly learns the optimal generator's policy.
Consequently, our discriminator's update solves the generator's optimization
problem for free: learning a policy that imitates the expert does not require
an additional optimization loop. This formulation effectively cuts by half the
implementation and computational burden of Adversarial Imitation Learning
algorithms by removing the Reinforcement Learning phase altogether. We show on
a variety of tasks that our simpler approach is competitive to prevalent
Imitation Learning methods.
</p>
<a href="http://arxiv.org/abs/2006.13258" target="_blank">arXiv:2006.13258</a> [<a href="http://arxiv.org/pdf/2006.13258" target="_blank">pdf</a>]

<h2>SCE: Scalable Network Embedding from Sparsest Cut. (arXiv:2006.16499v4 [cs.LG] UPDATED)</h2>
<h3>Shengzhong Zhang, Zengfeng Huang, Haicang Zhou, Ziang Zhou</h3>
<p>Large-scale network embedding is to learn a latent representation for each
node in an unsupervised manner, which captures inherent properties and
structural information of the underlying graph. In this field, many popular
approaches are influenced by the skip-gram model from natural language
processing. Most of them use a contrastive objective to train an encoder which
forces the embeddings of similar pairs to be close and embeddings of negative
samples to be far. A key of success to such contrastive learning methods is how
to draw positive and negative samples. While negative samples that are
generated by straightforward random sampling are often satisfying, methods for
drawing positive examples remains a hot topic.

In this paper, we propose SCE for unsupervised network embedding only using
negative samples for training. Our method is based on a new contrastive
objective inspired by the well-known sparsest cut problem. To solve the
underlying optimization problem, we introduce a Laplacian smoothing trick,
which uses graph convolutional operators as low-pass filters for smoothing node
representations. The resulting model consists of a GCN-type structure as the
encoder and a simple loss function. Notably, our model does not use positive
samples but only negative samples for training, which not only makes the
implementation and tuning much easier, but also reduces the training time
significantly.

Finally, extensive experimental studies on real world data sets are
conducted. The results clearly demonstrate the advantages of our new model in
both accuracy and scalability compared to strong baselines such as GraphSAGE,
G2G and DGI.
</p>
<a href="http://arxiv.org/abs/2006.16499" target="_blank">arXiv:2006.16499</a> [<a href="http://arxiv.org/pdf/2006.16499" target="_blank">pdf</a>]

<h2>Multi-Fidelity Bayesian Optimization via Deep Neural Networks. (arXiv:2007.03117v4 [cs.LG] UPDATED)</h2>
<h3>Shibo Li, Wei Xing, Mike Kirby, Shandian Zhe</h3>
<p>Bayesian optimization (BO) is a popular framework to optimize black-box
functions. In many applications, the objective function can be evaluated at
multiple fidelities to enable a trade-off between the cost and accuracy. To
reduce the optimization cost, many multi-fidelity BO methods have been
proposed. Despite their success, these methods either ignore or over-simplify
the strong, complex correlations across the fidelities, and hence can be
inefficient in estimating the objective function. To address this issue, we
propose Deep Neural Network Multi-Fidelity Bayesian Optimization (DNN-MFBO)
that can flexibly capture all kinds of complicated relationships between the
fidelities to improve the objective function estimation and hence the
optimization performance. We use sequential, fidelity-wise Gauss-Hermite
quadrature and moment-matching to fulfill a mutual information-based
acquisition function, which is computationally tractable and efficient. We show
the advantages of our method in both synthetic benchmark datasets and
real-world applications in engineering design.
</p>
<a href="http://arxiv.org/abs/2007.03117" target="_blank">arXiv:2007.03117</a> [<a href="http://arxiv.org/pdf/2007.03117" target="_blank">pdf</a>]

<h2>Probabilistic Jacobian-based Saliency Maps Attacks. (arXiv:2007.06032v4 [cs.CV] UPDATED)</h2>
<h3>Th&#xe9;o Combey, Ant&#xf3;nio Loison, Maxime Faucher, Hatem Hajri</h3>
<p>Neural network classifiers (NNCs) are known to be vulnerable to malicious
adversarial perturbations of inputs including those modifying a small fraction
of the input features named sparse or $L_0$ attacks. Effective and fast $L_0$
attacks, such as the widely used Jacobian-based Saliency Map Attack (JSMA) are
practical to fool NNCs but also to improve their robustness. In this paper, we
show that penalising saliency maps of JSMA by the output probabilities and the
input features of the NNC allows to obtain more powerful attack algorithms that
better take into account each input's characteristics. This leads us to
introduce improved versions of JSMA, named Weighted JSMA (WJSMA) and Taylor
JSMA (TJSMA), and demonstrate through a variety of white-box and black-box
experiments on three different datasets (MNIST, CIFAR-10 and GTSRB), that they
are both significantly faster and more efficient than the original targeted and
non-targeted versions of JSMA. Experiments also demonstrate, in some cases,
very competitive results of our attacks in comparison with the Carlini-Wagner
(CW) $L_0$ attack, while remaining, like JSMA, significantly faster (WJSMA and
TJSMA are more than 50 times faster than CW $L_0$ on CIFAR-10). Therefore, our
new attacks provide good trade-offs between JSMA and CW for $L_0$ real-time
adversarial testing on datasets such as the ones previously cited. Codes are
publicly available through the link
https://github.com/probabilistic-jsmas/probabilistic-jsmas.
</p>
<a href="http://arxiv.org/abs/2007.06032" target="_blank">arXiv:2007.06032</a> [<a href="http://arxiv.org/pdf/2007.06032" target="_blank">pdf</a>]

<h2>When stakes are high: balancing accuracy and transparency with Model-Agnostic Interpretable Data-driven suRRogates. (arXiv:2007.06894v2 [stat.ML] UPDATED)</h2>
<h3>Roel Henckaerts, Katrien Antonio, Marie-Pier C&#xf4;t&#xe9;</h3>
<p>Highly regulated industries, like banking and insurance, ask for transparent
decision-making algorithms. At the same time, competitive markets are pushing
for the use of complex black box models. We therefore present a procedure to
develop a Model-Agnostic Interpretable Data-driven suRRogate (maidrr) suited
for structured tabular data. Knowledge is extracted from a black box via
partial dependence effects. These are used to perform smart feature engineering
by grouping variable values. This results in a segmentation of the feature
space with automatic variable selection. A transparent generalized linear model
(GLM) is fit to the features in categorical format and their relevant
interactions. We demonstrate our R package maidrr with a case study on general
insurance claim frequency modeling for six publicly available datasets. Our
maidrr GLM closely approximates a gradient boosting machine (GBM) black box and
outperforms both a linear and tree surrogate as benchmarks.
</p>
<a href="http://arxiv.org/abs/2007.06894" target="_blank">arXiv:2007.06894</a> [<a href="http://arxiv.org/pdf/2007.06894" target="_blank">pdf</a>]

<h2>Cloud Transformers. (arXiv:2007.11679v2 [cs.CV] UPDATED)</h2>
<h3>Kirill Mazur, Victor Lempitsky</h3>
<p>We present a new versatile building block for deep point cloud processing
architectures. This building block combines the ideas of spatial transformers
and multi-view CNNs with the efficiency of standard convolutional layers in two
and three-dimensional dense grids. The new block operates via multiple parallel
heads, whereas each head differentiably rasterizes feature representations of
individual points into a low-dimensional space, and then uses dense convolution
to propagate information across points. The results of the processing of
individual heads are then combined together resulting in the update of point
features. Using the new block, we build architectures for both discriminative
(point cloud segmentation, point cloud classification) and generative (point
cloud inpainting and image-based point cloud reconstruction) tasks. The
resulting architectures invariably achieve state-of-the-art performance for
these tasks, demonstrating the versatility and universality of the new block
for point cloud processing.
</p>
<a href="http://arxiv.org/abs/2007.11679" target="_blank">arXiv:2007.11679</a> [<a href="http://arxiv.org/pdf/2007.11679" target="_blank">pdf</a>]

<h2>The Tractability of SHAP-Score-Based Explanations over Deterministic and Decomposable Boolean Circuits. (arXiv:2007.14045v2 [cs.AI] UPDATED)</h2>
<h3>Marcelo Arenas, Pablo Barcel&#xf3; Leopoldo Bertossi, Mika&#xeb;l Monet</h3>
<p>Scores based on Shapley values are widely used for providing explanations to
classification results over machine learning models. A prime example of this is
the influential SHAP-score, a version of the Shapley value that can help
explain the result of a learned model on a specific entity by assigning a score
to every feature. While in general computing Shapley values is a
computationally intractable problem, it has recently been claimed that the
SHAP-score can be computed in polynomial time over the class of decision trees.
In this paper, we provide a proof of a stronger result over Boolean models: the
SHAP-score can be computed in polynomial time over deterministic and
decomposable Boolean circuits. Such circuits, also known as tractable Boolean
circuits, generalize a wide range of Boolean circuits and binary decision
diagrams classes, including binary decision trees, Ordered Binary Decision
Diagrams (OBDDs) and Free Binary Decision Diagrams (FBDDs). We also establish
the computational limits of the notion of SHAP-score by observing that, under a
mild condition, computing it over a class of Boolean models is always
polynomially as hard as the model counting problem for that class. This implies
that both determinism and decomposability are essential properties for the
circuits that we consider, as removing one or the other renders the problem of
computing the SHAP-score intractable (namely, #P-hard).
</p>
<a href="http://arxiv.org/abs/2007.14045" target="_blank">arXiv:2007.14045</a> [<a href="http://arxiv.org/pdf/2007.14045" target="_blank">pdf</a>]

<h2>Future Trends for Human-AI Collaboration: A Comprehensive Taxonomy of AI/AGI Using Multiple Intelligences and Learning Styles. (arXiv:2008.04793v3 [cs.AI] UPDATED)</h2>
<h3>Andrzej Cichocki, Alexander P. Kuleshov</h3>
<p>This article discusses some trends and concepts in developing new generation
of future Artificial General Intelligence (AGI) systems which relate to complex
facets and different types of human intelligence, especially social, emotional,
attentional and ethical intelligence. We describe various aspects of multiple
human intelligences and learning styles, which may impact on a variety of AI
problem domains. Using the concept of 'multiple intelligences' rather than a
single type of intelligence, we categorize and provide working definitions of
various AGI depending on their cognitive skills or capacities. Future AI
systems will be able not only to communicate with human users and each other,
but also to efficiently exchange knowledge and wisdom with abilities of
cooperation, collaboration and even co-creating something new and valuable and
have meta-learning capacities. Multi-agent systems such as these can be used to
solve problems that would be difficult to solve by any individual intelligent
agent.

Key words: Artificial General Intelligence (AGI), multiple intelligences,
learning styles, physical intelligence, emotional intelligence, social
intelligence, attentional intelligence, moral-ethical intelligence, responsible
decision making, creative-innovative intelligence, cognitive functions,
meta-learning of AI systems.
</p>
<a href="http://arxiv.org/abs/2008.04793" target="_blank">arXiv:2008.04793</a> [<a href="http://arxiv.org/pdf/2008.04793" target="_blank">pdf</a>]

<h2>Overcoming Model Bias for Robust Offline Deep Reinforcement Learning. (arXiv:2008.05533v3 [cs.LG] UPDATED)</h2>
<h3>Phillip Swazinna, Steffen Udluft, Thomas Runkler</h3>
<p>State-of-the-art reinforcement learning algorithms mostly rely on being
allowed to directly interact with their environment to collect millions of
observations. This makes it hard to transfer their success to industrial
control problems, where simulations are often very costly or do not exist, and
exploring in the real environment can potentially lead to catastrophic events.
Recently developed, model-free, offline RL algorithms, can learn from a single
dataset (containing limited exploration) by mitigating extrapolation error in
value functions. However, the robustness of the training process is still
comparatively low, a problem known from methods using value functions. To
improve robustness and stability of the learning process, we use dynamics
models to assess policy performance instead of value functions, resulting in
MOOSE (MOdel-based Offline policy Search with Ensembles), an algorithm which
ensures low model bias by keeping the policy within the support of the data. We
compare MOOSE with state-of-the-art model-free, offline RL algorithms BEAR and
BCQ on the Industrial Benchmark and MuJoCo continuous control tasks in terms of
robust performance, and find that MOOSE outperforms its model-free counterparts
in almost all considered cases, often even by far.
</p>
<a href="http://arxiv.org/abs/2008.05533" target="_blank">arXiv:2008.05533</a> [<a href="http://arxiv.org/pdf/2008.05533" target="_blank">pdf</a>]

<h2>Kullback-Leibler divergence between quantum distributions, and its upper-bound. (arXiv:2008.05932v3 [cs.LG] UPDATED)</h2>
<h3>Vincenzo Bonnici</h3>
<p>This work presents an upper-bound to value that the Kullback-Leibler (KL)
divergence can reach for a class of probability distributions called quantum
distributions (QD). The aim is to find a distribution $U$ which maximizes the
KL divergence from a given distribution $P$ under the assumption that $P$ and
$U$ have been generated by distributing a given discrete quantity, a quantum.
Quantum distributions naturally represent a wide range of probability
distributions that are used in practical applications. Moreover, such a class
of distributions can be obtained as an approximation of any probability
distribution. The retrieving of an upper-bound for the entropic divergence is
here shown to be possible under the condition that the compared distributions
are quantum distributions over the same quantum value, thus they become
comparable. Thus, entropic divergence acquires a more powerful meaning when it
is applied to comparable distributions. This aspect should be taken into
account in future developments of divergences. The theoretical findings are
used for proposing a notion of normalized KL divergence that is empirically
shown to behave differently from already known measures.
</p>
<a href="http://arxiv.org/abs/2008.05932" target="_blank">arXiv:2008.05932</a> [<a href="http://arxiv.org/pdf/2008.05932" target="_blank">pdf</a>]

<h2>XCM: An Explainable Convolutional Neural Network for Multivariate Time Series Classification. (arXiv:2009.04796v2 [cs.LG] UPDATED)</h2>
<h3>Kevin Fauvel, Tao Lin, V&#xe9;ronique Masson, &#xc9;lisa Fromont, Alexandre Termier</h3>
<p>We present XCM, an eXplainable Convolutional neural network for Multivariate
time series classification. XCM is a new compact convolutional neural network
which extracts information relative to the observed variables and time directly
from the input data. Thus, XCM architecture enables a good generalization
ability on both small and large datasets, while allowing the full exploitation
of a faithful post-hoc model-specific explainability method (Gradient-weighted
Class Activation Mapping) by precisely identifying the observed variables and
timestamps of the input data that are important for predictions. Our evaluation
firstly shows that XCM outperforms the state-of-the-art multivariate time
series classifiers on both the large and small public UEA datasets.
Furthermore, following the illustration of the performance and explainability
of XCM on a synthetic dataset, we present how XCM can outperform the current
most accurate state-of-the-art algorithm on a real-world application while
enhancing explainability by providing faithful and more informative
explanations.
</p>
<a href="http://arxiv.org/abs/2009.04796" target="_blank">arXiv:2009.04796</a> [<a href="http://arxiv.org/pdf/2009.04796" target="_blank">pdf</a>]

<h2>BSN++: Complementary Boundary Regressor with Scale-Balanced Relation Modeling for Temporal Action Proposal Generation. (arXiv:2009.07641v3 [cs.CV] UPDATED)</h2>
<h3>Haisheng Su, Weihao Gan, Wei Wu, Junjie Yan, Yu Qiao</h3>
<p>Generating human action proposals in untrimmed videos is an important yet
challenging task with wide applications. Current methods often suffer from the
noisy boundary locations and the inferior quality of confidence scores used for
proposal retrieving. In this paper, we present BSN++, a new framework which
exploits complementary boundary regressor and relation modeling for temporal
proposal generation. First, we propose a novel boundary regressor based on the
complementary characteristics of both starting and ending boundary classifiers.
Specifically, we utilize the U-shaped architecture with nested skip connections
to capture rich contexts and introduce bi-directional boundary matching
mechanism to improve boundary precision. Second, to account for the
proposal-proposal relations ignored in previous methods, we devise a proposal
relation block to which includes two self-attention modules from the aspects of
position and channel. Furthermore, we find that there inevitably exists data
imbalanced problems in the positive/negative proposals and temporal durations,
which harm the model performance on tail distributions. To relieve this issue,
we introduce the scale-balanced re-sampling strategy. Extensive experiments are
conducted on two popular benchmarks: ActivityNet-1.3 and THUMOS14, which
demonstrate that BSN++ achieves the state-of-the-art performance. Not
surprisingly, the proposed BSN++ ranked 1st place in the CVPR19 - ActivityNet
challenge leaderboard on temporal action localization task.
</p>
<a href="http://arxiv.org/abs/2009.07641" target="_blank">arXiv:2009.07641</a> [<a href="http://arxiv.org/pdf/2009.07641" target="_blank">pdf</a>]

<h2>DoubleEnsemble: A New Ensemble Method Based on Sample Reweighting and Feature Selection for Financial Data Analysis. (arXiv:2010.01265v2 [cs.LG] UPDATED)</h2>
<h3>Chuheng Zhang, Yuanqi Li, Xi Chen, Yifei Jin, Pingzhong Tang, Jian Li</h3>
<p>Modern machine learning models (such as deep neural networks and boosting
decision tree models) have become increasingly popular in financial market
prediction, due to their superior capacity to extract complex non-linear
patterns. However, since financial datasets have very low signal-to-noise ratio
and are non-stationary, complex models are often very prone to overfitting and
suffer from instability issues. Moreover, as various machine learning and data
mining tools become more widely used in quantitative trading, many trading
firms have been producing an increasing number of features (aka factors).
Therefore, how to automatically select effective features becomes an imminent
problem. To address these issues, we propose DoubleEnsemble, an ensemble
framework leveraging learning trajectory based sample reweighting and shuffling
based feature selection. Specifically, we identify the key samples based on the
training dynamics on each sample and elicit key features based on the ablation
impact of each feature via shuffling. Our model is applicable to a wide range
of base models, capable of extracting complex patterns, while mitigating the
overfitting and instability issues for financial market prediction. We conduct
extensive experiments, including price prediction for cryptocurrencies and
stock trading, using both DNN and gradient boosting decision tree as base
models. Our experiment results demonstrate that DoubleEnsemble achieves a
superior performance compared with several baseline methods.
</p>
<a href="http://arxiv.org/abs/2010.01265" target="_blank">arXiv:2010.01265</a> [<a href="http://arxiv.org/pdf/2010.01265" target="_blank">pdf</a>]

<h2>Learning Manifold Implicitly via Explicit Heat-Kernel Learning. (arXiv:2010.01761v2 [cs.LG] UPDATED)</h2>
<h3>Yufan Zhou, Changyou Chen, Jinhui Xu</h3>
<p>Manifold learning is a fundamental problem in machine learning with numerous
applications. Most of the existing methods directly learn the low-dimensional
embedding of the data in some high-dimensional space, and usually lack the
flexibility of being directly applicable to down-stream applications. In this
paper, we propose the concept of implicit manifold learning, where manifold
information is implicitly obtained by learning the associated heat kernel. A
heat kernel is the solution of the corresponding heat equation, which describes
how "heat" transfers on the manifold, thus containing ample geometric
information of the manifold. We provide both practical algorithm and
theoretical analysis of our framework. The learned heat kernel can be applied
to various kernel-based machine learning models, including deep generative
models (DGM) for data generation and Stein Variational Gradient Descent for
Bayesian inference. Extensive experiments show that our framework can achieve
state-of-the-art results compared to existing methods for the two tasks.
</p>
<a href="http://arxiv.org/abs/2010.01761" target="_blank">arXiv:2010.01761</a> [<a href="http://arxiv.org/pdf/2010.01761" target="_blank">pdf</a>]

<h2>From Language Games to Drawing Games. (arXiv:2010.02820v2 [cs.AI] UPDATED)</h2>
<h3>Chrisantha Fernando, Daria Zenkova, Stanislav Nikolov, Simon Osindero</h3>
<p>We attempt to automate various artistic processes by inventing a set of
drawing games, analogous to the approach taken by emergent language research in
inventing communication games. A critical difference is that drawing games
demand much less effort from the receiver than do language games. Artists must
work with pre-trained viewers who spend little time learning artist specific
representational conventions, but who instead have a pre-trained visual system
optimized for behaviour in the world by understanding to varying extents the
environment's visual affordances. After considering various kinds of drawing
game we present some preliminary experiments which have generated images by
closing the generative-critical loop.
</p>
<a href="http://arxiv.org/abs/2010.02820" target="_blank">arXiv:2010.02820</a> [<a href="http://arxiv.org/pdf/2010.02820" target="_blank">pdf</a>]

<h2>Directional Graph Networks. (arXiv:2010.02863v2 [cs.LG] UPDATED)</h2>
<h3>Dominique Beaini, Saro Passaro, Vincent L&#xe9;tourneau, William L. Hamilton, Gabriele Corso, Pietro Li&#xf2;</h3>
<p>In order to overcome the expressive limitations of graph neural networks
(GNNs), we propose the first method that exploits vector flows over graphs to
develop globally consistent directional and asymmetric aggregation functions.
We show that our directional graph networks (DGNs) generalize convolutional
neural networks (CNNs) when applied on a grid. Whereas recent theoretical works
focus on understanding local neighbourhoods, local structures and local
isomorphism with no global information flow, our novel theoretical framework
allows directional convolutional kernels in any graph. First, by defining a
vector field in the graph, we develop a method of applying directional
derivatives and smoothing by projecting node-specific messages into the field.
Then we propose the use of the Laplacian eigenvectors as such vector field, and
we show that the method generalizes CNNs on an n-dimensional grid, and is
provably more discriminative than standard GNNs regarding the Weisfeiler-Lehman
1-WL test. Finally, we bring the power of CNN data augmentation to graphs by
providing a means of doing reflection, rotation and distortion on the
underlying directional field. We evaluate our method on different standard
benchmarks and see a relative error reduction of 8\% on the CIFAR10 graph
dataset and 11% to 32% on the molecular ZINC dataset. An important outcome of
this work is that it enables to translate any physical or biological problems
with intrinsic directional axes into a graph network formalism with an embedded
directional field.
</p>
<a href="http://arxiv.org/abs/2010.02863" target="_blank">arXiv:2010.02863</a> [<a href="http://arxiv.org/pdf/2010.02863" target="_blank">pdf</a>]

<h2>Why Does MAML Outperform ERM? An Optimization Perspective. (arXiv:2010.14672v2 [cs.LG] UPDATED)</h2>
<h3>Liam Collins, Aryan Mokhtari, Sanjay Shakkottai</h3>
<p>Model-Agnostic Meta-Learning (MAML) has demonstrated widespread success in
training models that can quickly adapt to new tasks via one or few stochastic
gradient descent steps. However, the MAML objective is significantly more
difficult to optimize compared to standard Empirical Risk Minimization (ERM),
and little is understood about how much MAML improves over ERM in terms of the
fast adaptability of their solutions in various scenarios. We analytically
address this issue in a linear regression setting consisting of a mixture of
easy and hard tasks, where hardness is determined by the number of gradient
steps required to solve the task. Specifically, we prove that for
$\Omega(d_{\text{eff}})$ labelled test samples (for gradient-based fine-tuning)
where $d_{\text{eff}}$ is the effective dimension of the problem, in order for
MAML to achieve substantial gain over ERM, the optimal solutions of the hard
tasks must be closely packed together with the center far from the center of
the easy task optimal solutions. We show that these insights also apply in a
low-dimensional feature space when both MAML and ERM learn a representation of
the tasks, which reduces the effective problem dimension. Further, our few-shot
image classification experiments suggest that our results generalize beyond
linear regression.
</p>
<a href="http://arxiv.org/abs/2010.14672" target="_blank">arXiv:2010.14672</a> [<a href="http://arxiv.org/pdf/2010.14672" target="_blank">pdf</a>]

<h2>Developing High Quality Training Samples for Deep Learning Based Local Climate Zone Classification in Korea. (arXiv:2011.01436v2 [cs.CV] UPDATED)</h2>
<h3>Minho Kim, Doyoung Jeong, Hyoungwoo Choi, Yongil Kim</h3>
<p>Two out of three people will be living in urban areas by 2050, as projected
by the United Nations, emphasizing the need for sustainable urban development
and monitoring. Common urban footprint data provide high-resolution city
extents but lack essential information on the distribution, pattern, and
characteristics. The Local Climate Zone (LCZ) offers an efficient and
standardized framework that can delineate the internal structure and
characteristics of urban areas. Global-scale LCZ mapping has been explored, but
are limited by low accuracy, variable labeling quality, or domain adaptation
challenges. Instead, this study developed a custom LCZ data to map key Korean
cities using a multi-scale convolutional neural network. Results demonstrated
that using a novel, custom LCZ data with deep learning can generate more
accurate LCZ map results compared to conventional community-based LCZ mapping
with machine learning as well as transfer learning of the global So2Sat
dataset.
</p>
<a href="http://arxiv.org/abs/2011.01436" target="_blank">arXiv:2011.01436</a> [<a href="http://arxiv.org/pdf/2011.01436" target="_blank">pdf</a>]

<h2>Learning 3D Dynamic Scene Representations for Robot Manipulation. (arXiv:2011.01968v2 [cs.RO] UPDATED)</h2>
<h3>Zhenjia Xu, Zhanpeng He, Jiajun Wu, Shuran Song</h3>
<p>3D scene representation for robot manipulation should capture three key
object properties: permanency -- objects that become occluded over time
continue to exist; amodal completeness -- objects have 3D occupancy, even if
only partial observations are available; spatiotemporal continuity -- the
movement of each object is continuous over space and time. In this paper, we
introduce 3D Dynamic Scene Representation (DSR), a 3D volumetric scene
representation that simultaneously discovers, tracks, reconstructs objects, and
predicts their dynamics while capturing all three properties. We further
propose DSR-Net, which learns to aggregate visual observations over multiple
interactions to gradually build and refine DSR. Our model achieves
state-of-the-art performance in modeling 3D scene dynamics with DSR on both
simulated and real data. Combined with model predictive control, DSR-Net
enables accurate planning in downstream robotic manipulation tasks such as
planar pushing. Video is available at https://youtu.be/GQjYG3nQJ80.
</p>
<a href="http://arxiv.org/abs/2011.01968" target="_blank">arXiv:2011.01968</a> [<a href="http://arxiv.org/pdf/2011.01968" target="_blank">pdf</a>]

<h2>Neural Stochastic Contraction Metrics for Learning-based Control and Estimation. (arXiv:2011.03168v3 [cs.LG] UPDATED)</h2>
<h3>Hiroyasu Tsukamoto, Soon-Jo Chung, Jean-Jacques E. Slotine</h3>
<p>We present Neural Stochastic Contraction Metrics (NSCM), a new design
framework for provably-stable robust control and estimation for a class of
stochastic nonlinear systems. It uses a spectrally-normalized deep neural
network to construct a contraction metric, sampled via simplified convex
optimization in the stochastic setting. Spectral normalization constrains the
state-derivatives of the metric to be Lipschitz continuous, thereby ensuring
exponential boundedness of the mean squared distance of system trajectories
under stochastic disturbances. The NSCM framework allows autonomous agents to
approximate optimal stable control and estimation policies in real-time, and
outperforms existing nonlinear control and estimation techniques including the
state-dependent Riccati equation, iterative LQR, EKF, and the deterministic
neural contraction metric, as illustrated in simulation results.
</p>
<a href="http://arxiv.org/abs/2011.03168" target="_blank">arXiv:2011.03168</a> [<a href="http://arxiv.org/pdf/2011.03168" target="_blank">pdf</a>]

<h2>Augmented Equivariant Attention Networks for Electron Microscopy Image Super-Resolution. (arXiv:2011.03633v2 [cs.CV] UPDATED)</h2>
<h3>Yaochen Xie, Yu Ding, Shuiwang Ji</h3>
<p>Taking electron microscopy (EM) images in high-resolution is time-consuming
and expensive and could be detrimental to the integrity of the samples under
observation. Advances in deep learning enable us to perform super-resolution
computationally, so as to obtain high-resolution images from low-resolution
ones. When training super-resolution models on pairs of experimentally acquired
EM images, prior models suffer from performance loss while using the
pooled-training strategy due to their inability to capture inter-image
dependencies and common features shared among images. Although there exist
methods that take advantage of shared features among input instances in image
classification tasks, they in the current form cannot be applied to
super-resolution tasks because they fail to preserve an essential property in
image-to-image transformation problems, which is the equivariance property to
spatial permutations. To address these limitations, we propose the augmented
equivariant attention networks (AEANets) with better capability to capture
inter-image dependencies and shared features, while preserving the equivariance
to spatial permutations. The proposed AEANets captures inter-image dependencies
and common features shared among images via two augmentations on the attention
mechanism; namely, the shared references and the batch-aware attention during
training. We theoretically show the equivariance property of the proposed
augmented attention model and experimentally show that AEANets consistently
outperforms the baselines in both quantitative and visual results.
</p>
<a href="http://arxiv.org/abs/2011.03633" target="_blank">arXiv:2011.03633</a> [<a href="http://arxiv.org/pdf/2011.03633" target="_blank">pdf</a>]

<h2>Supervised PCA: A Multiobjective Approach. (arXiv:2011.05309v2 [stat.ML] UPDATED)</h2>
<h3>Alexander Ritchie, Laura Balzano, Clayton Scott</h3>
<p>Methods for supervised principal component analysis (SPCA) aim to incorporate
label information into principal component analysis (PCA), so that the
extracted features are more useful for a prediction task of interest. Prior
work on SPCA has focused primarily on optimizing prediction error, and has
neglected the value of maximizing variance explained by the extracted features.
We propose a new method for SPCA that addresses both of these objectives
jointly, and demonstrate empirically that our approach dominates existing
approaches, i.e., outperforms them with respect to both prediction error and
variation explained. Our approach accommodates arbitrary supervised learning
losses and, through a statistical reformulation, provides a novel low-rank
extension of generalized linear models.
</p>
<a href="http://arxiv.org/abs/2011.05309" target="_blank">arXiv:2011.05309</a> [<a href="http://arxiv.org/pdf/2011.05309" target="_blank">pdf</a>]

<h2>Circus ANYmal: A Quadruped Learning Dexterous Manipulation with Its Limbs. (arXiv:2011.08811v2 [cs.RO] UPDATED)</h2>
<h3>Fan Shi, Timon Homberger, Joonho Lee, Takahiro Miki, Moju Zhao, Farbod Farshidian, Kei Okada, Masayuki Inaba, Marco Hutter</h3>
<p>Quadrupedal robots are skillful at locomotion tasks while lacking
manipulation skills, not to mention dexterous manipulation abilities. Inspired
by the animal behavior and the duality between multi-legged locomotion and
multi-fingered manipulation, we showcase a circus ball challenge on a
quadrupedal robot, ANYmal. We employ a model-free reinforcement learning
approach to train a deep policy that enables the robot to balance and
manipulate a light-weight ball robustly using its limbs without any contact
measurement sensor. The policy is trained in the simulation, in which we
randomize many physical properties with additive noise and inject random
disturbance force during manipulation, and achieves zero-shot deployment on the
real robot without any adjustment. In the hardware experiments, dynamic
performance is achieved with a maximum rotation speed of 15 deg/s, and robust
recovery is showcased under external poking. To our best knowledge, it is the
first work that demonstrates the dexterous dynamic manipulation on a real
quadrupedal robot.
</p>
<a href="http://arxiv.org/abs/2011.08811" target="_blank">arXiv:2011.08811</a> [<a href="http://arxiv.org/pdf/2011.08811" target="_blank">pdf</a>]

<h2>Efficient Conditional Pre-training for Transfer Learning. (arXiv:2011.10231v2 [cs.CV] UPDATED)</h2>
<h3>Shuvam Chakraborty, Burak Uzkent, Kumar Ayush, Kumar Tanmay, Evan Sheehan, Stefano Ermon</h3>
<p>Almost all the state-of-the-art neural networks for computer vision tasks are
trained by (1) Pre-training on a large scale dataset and (2) finetuning on the
target dataset. This strategy helps reduce the dependency on the target dataset
and improves convergence rate and generalization on the target task. Although
pre-training on large scale datasets is very useful, its foremost disadvantage
is high training cost. To address this, we propose efficient target dataset
conditioned filtering methods to remove less relevant samples from the
pre-training dataset. Unlike prior work, we focus on efficiency, adaptability,
and flexibility in addition to performance. Additionally, we discover that
lowering image resolutions in the pre-training step offers a great trade-off
between cost and performance. We validate our techniques by pre-training on
ImageNet in both the unsupervised and supervised settings and finetuning on a
diverse collection of target datasets and tasks. Our proposed methods
drastically reduce pre-training cost and provide strong performance boosts.
</p>
<a href="http://arxiv.org/abs/2011.10231" target="_blank">arXiv:2011.10231</a> [<a href="http://arxiv.org/pdf/2011.10231" target="_blank">pdf</a>]

<h2>FakeSafe: Human Level Data Protection by Disinformation Mapping using Cycle-consistent Adversarial Network. (arXiv:2011.11278v2 [cs.AI] UPDATED)</h2>
<h3>He Zhu, Dianbo Liu</h3>
<p>The concept of disinformation is to use fake messages to confuse people in
order to protect the real information. This strategy can be adapted into data
science to protect valuable private and sensitive data. Huge amount of private
data are being generated from personal devices such as smart phone and wearable
in recent years. Being able to utilize these personal data will bring big
opportunities to design personalized products, conduct precision healthcare and
many other tasks that were impossible in the past. However, due to privacy,
safety and regulation reasons, it is often difficult to transfer or store data
in its original form while keeping them safe. Building a secure data transfer
and storage infrastructure to preserving privacy is costly in most cases and
there is always a concern of data security due to human errors. In this study,
we propose a method, named FakeSafe, to provide human level data protection
using generative adversarial network with cycle consistency and conducted
experiments using both benchmark and real world data sets to illustrate
potential applications of FakeSafe.
</p>
<a href="http://arxiv.org/abs/2011.11278" target="_blank">arXiv:2011.11278</a> [<a href="http://arxiv.org/pdf/2011.11278" target="_blank">pdf</a>]

<h2>Improving Redundancy Availability: Dynamic Subtasks Modulation for Robots with Redundancy Insufficiency. (arXiv:2011.12884v2 [cs.RO] UPDATED)</h2>
<h3>Lu Chen, Lipeng Chen, Xiangchi Chen, Yi Ren, Longfei Zhao, Yue Wang, Rong Xiong</h3>
<p>This work presents an approach for robots to suitably carry out complex
applications characterized by the presence of multiple additional constraints
or subtasks (e.g. obstacle and self-collision avoidance) but subject to
redundancy insufficiency. The proposed approach, based on a novel subtask
merging strategy, enforces all subtasks in due course by dynamically modulating
a virtual secondary task, where the task status and soft priority are
incorporated to improve the overall efficiency of redundancy resolution. The
proposed approach greatly improves the redundancy availability by unitizing and
deploying subtasks in a fine-grained and compact manner. We build up our
control framework on the null space projection, which guarantees the execution
of subtasks does not interfere with the primary task. Experimental results on
two case studies are presented to show the performance of our approach.
</p>
<a href="http://arxiv.org/abs/2011.12884" target="_blank">arXiv:2011.12884</a> [<a href="http://arxiv.org/pdf/2011.12884" target="_blank">pdf</a>]

<h2>Towards Imperceptible Adversarial Image Patches Based on Network Explanations. (arXiv:2012.00909v2 [cs.CV] UPDATED)</h2>
<h3>Yaguan Qian, Jiamin Wang, Bin Wang, Zhaoquan Gu, Xiang Ling, Chunming Wu</h3>
<p>The vulnerability of deep neural networks (DNNs) for adversarial examples
have attracted more attention. Many algorithms are proposed to craft powerful
adversarial examples. However, these algorithms modifying the global or local
region of pixels without taking into account network explanations. Hence, the
perturbations are redundancy and easily detected by human eyes. In this paper,
we propose a novel method to generate local region perturbations. The main idea
is to find the contributing feature regions (CFRs) of images based on network
explanations for perturbations. Due to the network explanations, the
perturbations added to the CFRs are more effective than other regions. In our
method, a soft mask matrix is designed to represent the CFRs for finely
characterizing the contributions of each pixel. Based on this soft mask, we
develop a new objective function with inverse temperature to search for optimal
perturbations in CFRs. Extensive experiments are conducted on CIFAR-10 and
ILSVRC2012, which demonstrate the effectiveness, including attack success rate,
imperceptibility,and transferability.
</p>
<a href="http://arxiv.org/abs/2012.00909" target="_blank">arXiv:2012.00909</a> [<a href="http://arxiv.org/pdf/2012.00909" target="_blank">pdf</a>]

<h2>Probabilistic Tracklet Scoring and Inpainting for Multiple Object Tracking. (arXiv:2012.02337v2 [cs.CV] UPDATED)</h2>
<h3>Fatemeh Saleh, Sadegh Aliakbarian, Hamid Rezatofighi, Mathieu Salzmann, Stephen Gould</h3>
<p>Despite the recent advances in multiple object tracking (MOT), achieved by
joint detection and tracking, dealing with long occlusions remains a challenge.
This is due to the fact that such techniques tend to ignore the long-term
motion information. In this paper, we introduce a probabilistic autoregressive
motion model to score tracklet proposals by directly measuring their
likelihood. This is achieved by training our model to learn the underlying
distribution of natural tracklets. As such, our model allows us not only to
assign new detections to existing tracklets, but also to inpaint a tracklet
when an object has been lost for a long time, e.g., due to occlusion, by
sampling tracklets so as to fill the gap caused by misdetections. Our
experiments demonstrate the superiority of our approach at tracking objects in
challenging sequences; it outperforms the state of the art in most standard MOT
metrics on multiple MOT benchmark datasets, including MOT16, MOT17, and MOT20.
</p>
<a href="http://arxiv.org/abs/2012.02337" target="_blank">arXiv:2012.02337</a> [<a href="http://arxiv.org/pdf/2012.02337" target="_blank">pdf</a>]

<h2>A Variant of Gradient Descent Algorithm Based on Gradient Averaging. (arXiv:2012.02387v2 [cs.LG] UPDATED)</h2>
<h3>Saugata Purkayastha, Sukannya Purkayastha</h3>
<p>In this work, we study an optimizer, Grad-Avg to optimize error functions. We
establish the convergence of the sequence of iterates of Grad-Avg
mathematically to a minimizer (under boundedness assumption). We apply Grad-Avg
along with some of the popular optimizers on regression as well as
classification tasks. In regression tasks, it is observed that the behaviour of
Grad-Avg is almost identical with Stochastic Gradient Descent (SGD). We present
a mathematical justification of this fact. In case of classification tasks, it
is observed that the performance of Grad-Avg can be enhanced by suitably
scaling the parameters. Experimental results demonstrate that Grad-Avg
converges faster than the other state-of-the-art optimizers for the
classification task on two benchmark datasets.
</p>
<a href="http://arxiv.org/abs/2012.02387" target="_blank">arXiv:2012.02387</a> [<a href="http://arxiv.org/pdf/2012.02387" target="_blank">pdf</a>]

<h2>Risk-based Adaptive Deep Learning for Entity Resolution. (arXiv:2012.03513v2 [cs.LG] UPDATED)</h2>
<h3>Qun Chen, Zhaoqiang Chen, Youcef Nafa, Tianyi Duan, Zhanhuai Li</h3>
<p>The state-of-the-art performance on entity resolution (ER) has been achieved
by deep learning. However, deep models are usually trained on large quantities
of accurately labeled training data, and can not be easily tuned towards a
target workload. Unfortunately, in real scenarios, there may not be sufficient
labeled training data, and even worse, their distribution is usually more or
less different from the target workload even when they come from the same
domain.

To alleviate the said limitations, this paper proposes a novel risk-based
approach to tune a deep model towards a target workload by its particular
characteristics. Built on the recent advances on risk analysis for ER, the
proposed approach first trains a deep model on labeled training data, and then
fine-tunes it by minimizing its estimated misprediction risk on unlabeled
target data. Our theoretical analysis shows that risk-based adaptive training
can correct the label status of a mispredicted instance with a fairly good
chance. We have also empirically validated the efficacy of the proposed
approach on real benchmark data by a comparative study. Our extensive
experiments show that it can considerably improve the performance of deep
models. Furthermore, in the scenario of distribution misalignment, it can
similarly outperform the state-of-the-art alternative of transfer learning by
considerable margins. Using ER as a test case, we demonstrate that risk-based
adaptive training is a promising approach potentially applicable to various
challenging classification tasks.
</p>
<a href="http://arxiv.org/abs/2012.03513" target="_blank">arXiv:2012.03513</a> [<a href="http://arxiv.org/pdf/2012.03513" target="_blank">pdf</a>]

<h2>Learned Block Iterative Shrinkage Thresholding Algorithm for Photothermal Super Resolution Imaging. (arXiv:2012.03547v2 [cs.CV] UPDATED)</h2>
<h3>Samim Ahmadi, Jan Christian Hauffen, Linh K&#xe4;stner, Peter Jung, Giuseppe Caire, Mathias Ziegler</h3>
<p>Block-sparse regularization is already well-known in active thermal imaging
and is used for multiple measurement based inverse problems. The main
bottleneck of this method is the choice of regularization parameters which
differs for each experiment. To avoid time-consuming manually selected
regularization parameter, we propose a learned block-sparse optimization
approach using an iterative algorithm unfolded into a deep neural network. More
precisely, we show the benefits of using a learned block iterative shrinkage
thresholding algorithm that is able to learn the choice of regularization
parameters. In addition, this algorithm enables the determination of a suitable
weight matrix to solve the underlying inverse problem. Therefore, in this paper
we present the algorithm and compare it with state of the art block iterative
shrinkage thresholding using synthetically generated test data and experimental
test data from active thermography for defect reconstruction. Our results show
that the use of the learned block-sparse optimization approach provides smaller
normalized mean square errors for a small fixed number of iterations than
without learning. Thus, this new approach allows to improve the convergence
speed and only needs a few iterations to generate accurate defect
reconstruction in photothermal super resolution imaging.
</p>
<a href="http://arxiv.org/abs/2012.03547" target="_blank">arXiv:2012.03547</a> [<a href="http://arxiv.org/pdf/2012.03547" target="_blank">pdf</a>]

<h2>Scene Text Detection with Scribble Lines. (arXiv:2012.05030v2 [cs.CV] UPDATED)</h2>
<h3>Wenqing Zhang, Yang Qiu, Minghui Liao, Rui Zhang, Xiaolin Wei, Xiang Bai</h3>
<p>Scene text detection, which is one of the most popular topics in both
academia and industry, can achieve remarkable performance with sufficient
training data. However, the annotation costs of scene text detection are huge
with traditional labeling methods due to the various shapes of texts. Thus, it
is practical and insightful to study simpler labeling methods without harming
the detection performance. In this paper, we propose to annotate the texts by
scribble lines instead of polygons for text detection. It is a general labeling
method for texts with various shapes and requires low labeling costs.
Furthermore, a weakly-supervised scene text detection framework is proposed to
use the scribble lines for text detection. The experiments on several
benchmarks show that the proposed method bridges the performance gap between
the weakly labeling method and the original polygon-based labeling methods,
with even better performance. We will release the weak annotations of the
benchmarks in our experiments and hope it will benefit the field of scene text
detection to achieve better performance with simpler annotations.
</p>
<a href="http://arxiv.org/abs/2012.05030" target="_blank">arXiv:2012.05030</a> [<a href="http://arxiv.org/pdf/2012.05030" target="_blank">pdf</a>]

<h2>E3D: Event-Based 3D Shape Reconstruction. (arXiv:2012.05214v2 [cs.CV] UPDATED)</h2>
<h3>Alexis Baudron, Zihao W. Wang, Oliver Cossairt, Aggelos K. Katsaggelos</h3>
<p>3D shape reconstruction is a primary component of augmented/virtual reality.
Despite being highly advanced, existing solutions based on RGB, RGB-D and Lidar
sensors are power and data intensive, which introduces challenges for
deployment in edge devices. We approach 3D reconstruction with an event camera,
a sensor with significantly lower power, latency and data expense while
enabling high dynamic range. While previous event-based 3D reconstruction
methods are primarily based on stereo vision, we cast the problem as multi-view
shape from silhouette using a monocular event camera. The output from a moving
event camera is a sparse point set of space-time gradients, largely sketching
scene/object edges and contours. We first introduce an event-to-silhouette
(E2S) neural network module to transform a stack of event frames to the
corresponding silhouettes, with additional neural branches for camera pose
regression. Second, we introduce E3D, which employs a 3D differentiable
renderer (PyTorch3D) to enforce cross-view 3D mesh consistency and fine-tune
the E2S and pose network. Lastly, we introduce a 3D-to-events simulation
pipeline and apply it to publicly available object datasets and generate
synthetic event/silhouette training pairs for supervised learning.
</p>
<a href="http://arxiv.org/abs/2012.05214" target="_blank">arXiv:2012.05214</a> [<a href="http://arxiv.org/pdf/2012.05214" target="_blank">pdf</a>]

<h2>MorphGAN: One-Shot Face Synthesis GAN for Detecting Recognition Bias. (arXiv:2012.05225v2 [cs.CV] UPDATED)</h2>
<h3>Nataniel Ruiz, Barry-John Theobald, Anurag Ranjan, Ahmed Hussein Abdelaziz, Nicholas Apostoloff</h3>
<p>To detect bias in face recognition networks, it can be useful to probe a
network under test using samples in which only specific attributes vary in some
controlled way. However, capturing a sufficiently large dataset with specific
control over the attributes of interest is difficult. In this work, we describe
a simulator that applies specific head pose and facial expression adjustments
to images of previously unseen people. The simulator first fits a 3D morphable
model to a provided image, applies the desired head pose and facial expression
controls, then renders the model into an image. Next, a conditional Generative
Adversarial Network (GAN) conditioned on the original image and the rendered
morphable model is used to produce the image of the original person with the
new facial expression and head pose. We call this conditional GAN -- MorphGAN.
Images generated using MorphGAN conserve the identity of the person in the
original image, and the provided control over head pose and facial expression
allows test sets to be created to identify robustness issues of a facial
recognition deep network with respect to pose and expression. Images generated
by MorphGAN can also serve as data augmentation when training data are scarce.
We show that by augmenting small datasets of faces with new poses and
expressions improves the recognition performance by up to 9% depending on the
augmentation and data scarcity.
</p>
<a href="http://arxiv.org/abs/2012.05225" target="_blank">arXiv:2012.05225</a> [<a href="http://arxiv.org/pdf/2012.05225" target="_blank">pdf</a>]

<h2>TediGAN: Text-Guided Diverse Image Generation and Manipulation. (arXiv:2012.03308v1 [cs.CV] CROSS LISTED)</h2>
<h3>Weihao Xia, Yujiu Yang, Jing-Hao Xue, Baoyuan Wu</h3>
<p>In this work, we propose TediGAN, a novel framework for multi-modal image
generation and manipulation with textual descriptions. The proposed method
consists of three components: StyleGAN inversion module, visual-linguistic
similarity learning, and instance-level optimization. The inversion module is
to train an image encoder to map real images to the latent space of a
well-trained StyleGAN. The visual-linguistic similarity is to learn the
text-image matching by mapping the image and text into a common embedding
space. The instance-level optimization is for identity preservation in
manipulation. Our model can provide the lowest effect guarantee, and produce
diverse and high-quality images with an unprecedented resolution at 1024. Using
a control mechanism based on style-mixing, our TediGAN inherently supports
image synthesis with multi-modal inputs, such as sketches or semantic labels
with or without instance (text or real image) guidance. To facilitate
text-guided multi-modal synthesis, we propose the Multi-Modal CelebA-HQ, a
large-scale dataset consisting of real face images and corresponding semantic
segmentation map, sketch, and textual descriptions. Extensive experiments on
the introduced dataset demonstrate the superior performance of our proposed
method. Code and data are available at https://github.com/weihaox/TediGAN.
</p>
<a href="http://arxiv.org/abs/2012.03308" target="_blank">arXiv:2012.03308</a> [<a href="http://arxiv.org/pdf/2012.03308" target="_blank">pdf</a>]

