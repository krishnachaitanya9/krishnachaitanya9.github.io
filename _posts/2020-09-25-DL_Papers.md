---
title: Latest Deep Learning Papers
date: 2021-01-13 21:00:01 +0800
featured-img: DL-logo
categories: [Deep_Learning]
tags: [Deep_Learning]
mathjax: Yes
---

<h1>Your arXiv Feed (154 Articles)</h1>
<h2>Convolutional Neural Network Simplification with Progressive Retraining. (arXiv:2101.04699v1 [cs.LG])</h2>
<h3>D. Osaku, J.F. Gomes, A.X. Falc&#xe3;o</h3>
<p>Kernel pruning methods have been proposed to speed up, simplify, and improve
explanation of convolutional neural network (CNN) models. However, the
effectiveness of a simplified model is often below the original one. In this
letter, we present new methods based on objective and subjective relevance
criteria for kernel elimination in a layer-by-layer fashion. During the
process, a CNN model is retrained only when the current layer is entirely
simplified, by adjusting the weights from the next layer to the first one and
preserving weights of subsequent layers not involved in the process. We call
this strategy \emph{progressive retraining}, differently from kernel pruning
methods that usually retrain the entire model after each simplification action
-- e.g., the elimination of one or a few kernels. Our subjective relevance
criterion exploits the ability of humans in recognizing visual patterns and
improves the designer's understanding of the simplification process. The
combination of suitable relevance criteria and progressive retraining shows
that our methods can increase effectiveness with considerable model
simplification. We also demonstrate that our methods can provide better results
than two popular ones and another one from the state-of-the-art using four
challenging image datasets.
</p>
<a href="http://arxiv.org/abs/2101.04699" target="_blank">arXiv:2101.04699</a> [<a href="http://arxiv.org/pdf/2101.04699" target="_blank">pdf</a>]

<h2>Cross-Modal Contrastive Learning for Text-to-Image Generation. (arXiv:2101.04702v1 [cs.CV])</h2>
<h3>Han Zhang, Jing Yu Koh, Jason Baldridge, Honglak Lee, Yinfei Yang</h3>
<p>The output of text-to-image synthesis systems should be coherent, clear,
photo-realistic scenes with high semantic fidelity to their conditioned text
descriptions. Our Cross-Modal Contrastive Generative Adversarial Network
(XMC-GAN) addresses this challenge by maximizing the mutual information between
image and text. It does this via multiple contrastive losses which capture
inter-modality and intra-modality correspondences. XMC-GAN uses an attentional
self-modulation generator, which enforces strong text-image correspondence, and
a contrastive discriminator, which acts as a critic as well as a feature
encoder for contrastive learning. The quality of XMC-GAN's output is a major
step up from previous models, as we show on three challenging datasets. On
MS-COCO, not only does XMC-GAN improve state-of-the-art FID from 24.70 to 9.33,
but--more importantly--people prefer XMC-GAN by 77.3 for image quality and 74.1
for image-text alignment, compared to three other recent models. XMC-GAN also
generalizes to the challenging Localized Narratives dataset (which has longer,
more detailed descriptions), improving state-of-the-art FID from 48.70 to
14.12. Lastly, we train and evaluate XMC-GAN on the challenging Open Images
data, establishing a strong benchmark FID score of 26.91.
</p>
<a href="http://arxiv.org/abs/2101.04702" target="_blank">arXiv:2101.04702</a> [<a href="http://arxiv.org/pdf/2101.04702" target="_blank">pdf</a>]

<h2>Boundary-Aware Segmentation Network for Mobile and Web Applications. (arXiv:2101.04704v1 [cs.CV])</h2>
<h3>Xuebin Qin, Deng-Ping Fan, Chenyang Huang, Cyril Diagne, Zichen Zhang, Adri&#xe0; Cabeza Sant&#x27;Anna, Albert Su&#xe0;rez, Martin Jagersand, Ling Shao</h3>
<p>Although deep models have greatly improved the accuracy and robustness of
image segmentation, obtaining segmentation results with highly accurate
boundaries and fine structures is still a challenging problem. In this paper,
we propose a simple yet powerful Boundary-Aware Segmentation Network (BASNet),
which comprises a predict-refine architecture and a hybrid loss, for highly
accurate image segmentation. The predict-refine architecture consists of a
densely supervised encoder-decoder network and a residual refinement module,
which are respectively used to predict and refine a segmentation probability
map. The hybrid loss is a combination of the binary cross entropy, structural
similarity and intersection-over-union losses, which guide the network to learn
three-level (ie, pixel-, patch- and map- level) hierarchy representations. We
evaluate our BASNet on two reverse tasks including salient object segmentation,
camouflaged object segmentation, showing that it achieves very competitive
performance with sharp segmentation boundaries. Importantly, BASNet runs at
over 70 fps on a single GPU which benefits many potential real applications.
Based on BASNet, we further developed two (close to) commercial applications:
AR COPY &amp; PASTE, in which BASNet is integrated with augmented reality for
"COPYING" and "PASTING" real-world objects, and OBJECT CUT, which is a
web-based tool for automatic object background removal. Both applications have
already drawn huge amount of attention and have important real-world impacts.
The code and two applications will be publicly available at:
https://github.com/NathanUA/BASNet.
</p>
<a href="http://arxiv.org/abs/2101.04704" target="_blank">arXiv:2101.04704</a> [<a href="http://arxiv.org/pdf/2101.04704" target="_blank">pdf</a>]

<h2>Explicit homography estimation improves contrastive self-supervised learning. (arXiv:2101.04713v1 [cs.CV])</h2>
<h3>David Torpey, Richard Klein</h3>
<p>The typical contrastive self-supervised algorithm uses a similarity measure
in latent space as the supervision signal by contrasting positive and negative
images directly or indirectly. Although the utility of self-supervised
algorithms has improved recently, there are still bottlenecks hindering their
widespread use, such as the compute needed. In this paper, we propose a module
that serves as an additional objective in the self-supervised contrastive
learning paradigm. We show how the inclusion of this module to regress the
parameters of an affine transformation or homography, in addition to the
original contrastive objective, improves both performance and learning speed.
Importantly, we ensure that this module does not enforce invariance to the
various components of the affine transform, as this is not always ideal. We
demonstrate the effectiveness of the additional objective on two recent,
popular self-supervised algorithms. We perform an extensive experimental
analysis of the proposed method and show an improvement in performance for all
considered datasets. Further, we find that although both the general homography
and affine transformation are sufficient to improve performance and
convergence, the affine transformation performs better in all cases.
</p>
<a href="http://arxiv.org/abs/2101.04713" target="_blank">arXiv:2101.04713</a> [<a href="http://arxiv.org/pdf/2101.04713" target="_blank">pdf</a>]

<h2>SEED: Self-supervised Distillation For Visual Representation. (arXiv:2101.04731v1 [cs.CV])</h2>
<h3>Zhiyuan Fang, Jianfeng Wang, Lijuan Wang, Lei Zhang, Yezhou Yang, Zicheng Liu</h3>
<p>This paper is concerned with self-supervised learning for small models. The
problem is motivated by our empirical studies that while the widely used
contrastive self-supervised learning method has shown great progress on large
model training, it does not work well for small models. To address this
problem, we propose a new learning paradigm, named SElf-SupErvised Distillation
(SEED), where we leverage a larger network (as Teacher) to transfer its
representational knowledge into a smaller architecture (as Student) in a
self-supervised fashion. Instead of directly learning from unlabeled data, we
train a student encoder to mimic the similarity score distribution inferred by
a teacher over a set of instances. We show that SEED dramatically boosts the
performance of small networks on downstream tasks. Compared with
self-supervised baselines, SEED improves the top-1 accuracy from 42.2% to 67.6%
on EfficientNet-B0 and from 36.3% to 68.2% on MobileNet-v3-Large on the
ImageNet-1k dataset.
</p>
<a href="http://arxiv.org/abs/2101.04731" target="_blank">arXiv:2101.04731</a> [<a href="http://arxiv.org/pdf/2101.04731" target="_blank">pdf</a>]

<h2>Bootstrapping Motor Skill Learning with Motion Planning. (arXiv:2101.04736v1 [cs.RO])</h2>
<h3>Ben Abbatematteo, Eric Rosen, Stefanie Tellex, George Konidaris</h3>
<p>Learning a robot motor skill from scratch is impractically slow; so much so
that in practice, learning must be bootstrapped using a good skill policy
obtained from human demonstration. However, relying on human demonstration
necessarily degrades the autonomy of robots that must learn a wide variety of
skills over their operational lifetimes. We propose using kinematic motion
planning as a completely autonomous, sample efficient way to bootstrap motor
skill learning for object manipulation. We demonstrate the use of motion
planners to bootstrap motor skills in two complex object manipulation scenarios
with different policy representations: opening a drawer with a dynamic movement
primitive representation, and closing a microwave door with a deep neural
network policy. We also show how our method can bootstrap a motor skill for the
challenging dynamic task of learning to hit a ball off a tee, where a kinematic
plan based on treating the scene as static is insufficient to solve the task,
but sufficient to bootstrap a more dynamic policy. In all three cases, our
method is competitive with human-demonstrated initialization, and significantly
outperforms starting with a random policy. This approach enables robots to to
efficiently and autonomously learn motor policies for dynamic tasks without
human demonstration.
</p>
<a href="http://arxiv.org/abs/2101.04736" target="_blank">arXiv:2101.04736</a> [<a href="http://arxiv.org/pdf/2101.04736" target="_blank">pdf</a>]

<h2>CityFlow-NL: Tracking and Retrieval of Vehicles at City Scaleby Natural Language Descriptions. (arXiv:2101.04741v1 [cs.CV])</h2>
<h3>Qi Feng, Vitaly Ablavsky, Stan Sclaroff</h3>
<p>Natural Language (NL) descriptions can be the most convenient or the only way
to interact with systems built to understand and detect city scale traffic
patterns and vehicle-related events. In this paper, we extend the widely
adopted CityFlow Benchmark with natural language descriptions for vehicle
targets and introduce the CityFlow-NL Benchmark. The CityFlow-NL contains more
than 5,000 unique and precise NL descriptions of vehicle targets, making it the
largest-scale tracking with NL descriptions dataset to our knowledge. Moreover,
the dataset facilitates research at the intersection of multi-object tracking,
retrieval by NL descriptions, and temporal localization of events.
</p>
<a href="http://arxiv.org/abs/2101.04741" target="_blank">arXiv:2101.04741</a> [<a href="http://arxiv.org/pdf/2101.04741" target="_blank">pdf</a>]

<h2>Linear Representation Meta-Reinforcement Learning for Instant Adaptation. (arXiv:2101.04750v1 [cs.LG])</h2>
<h3>Matt Peng, Banghua Zhu, Jiantao Jiao</h3>
<p>This paper introduces Fast Linearized Adaptive Policy (FLAP), a new
meta-reinforcement learning (meta-RL) method that is able to extrapolate well
to out-of-distribution tasks without the need to reuse data from training, and
adapt almost instantaneously with the need of only a few samples during
testing. FLAP builds upon the idea of learning a shared linear representation
of the policy so that when adapting to a new task, it suffices to predict a set
of linear weights. A separate adapter network is trained simultaneously with
the policy such that during adaptation, we can directly use the adapter network
to predict these linear weights instead of updating a meta-policy via gradient
descent, such as in prior meta-RL methods like MAML, to obtain the new policy.
The application of the separate feed-forward network not only speeds up the
adaptation run-time significantly, but also generalizes extremely well to very
different tasks that prior Meta-RL methods fail to generalize to. Experiments
on standard continuous-control meta-RL benchmarks show FLAP presenting
significantly stronger performance on out-of-distribution tasks with up to
double the average return and up to 8X faster adaptation run-time speeds when
compared to prior methods.
</p>
<a href="http://arxiv.org/abs/2101.04750" target="_blank">arXiv:2101.04750</a> [<a href="http://arxiv.org/pdf/2101.04750" target="_blank">pdf</a>]

<h2>A Compact Deep Learning Model for Face Spoofing Detection. (arXiv:2101.04756v1 [cs.CV])</h2>
<h3>Seyedkooshan Hashemifard, Mohammad Akbari</h3>
<p>In recent years, face biometric security systems are rapidly increasing,
therefore, the presentation attack detection (PAD) has received significant
attention from research communities and has become a major field of research.
Researchers have tackled the problem with various methods, from exploiting
conventional texture feature extraction such as LBP, BSIF, and LPQ to using
deep neural networks with different architectures. Despite the results each of
these techniques has achieved for a certain attack scenario or dataset, most of
them still failed to generalized the problem for unseen conditions, as the
efficiency of each is limited to certain type of presentation attacks and
instruments (PAI). In this paper, instead of completely extracting hand-crafted
texture features or relying only on deep neural networks, we address the
problem via fusing both wide and deep features in a unified neural
architecture. The main idea is to take advantage of the strength of both
methods to derive well-generalized solution for the problem. We also evaluated
the effectiveness of our method by comparing the results with each of the
mentioned techniques separately. The procedure is done on different spoofing
datasets such as ROSE-Youtu, SiW and NUAA Imposter datasets.

In particular, we simultanously learn a low dimensional latent space
empowered with data-driven features learnt via Convolutional Neural Network
designes for spoofing detection task (i.e., deep channel) as well as leverages
spoofing detection feature already popular for spoofing in frequency and
temporal dimensions ( i.e., via wide channel).
</p>
<a href="http://arxiv.org/abs/2101.04756" target="_blank">arXiv:2101.04756</a> [<a href="http://arxiv.org/pdf/2101.04756" target="_blank">pdf</a>]

<h2>Joint aggregation of cardinal and ordinal evaluations with an application to a student paper competition. (arXiv:2101.04765v1 [cs.AI])</h2>
<h3>Dorit S. Hochbaum, Erick Moreno-Centeno</h3>
<p>An important problem in decision theory concerns the aggregation of
individual rankings/ratings into a collective evaluation. We illustrate a new
aggregation method in the context of the 2007 MSOM's student paper competition.
The aggregation problem in this competition poses two challenges. Firstly, each
paper was reviewed only by a very small fraction of the judges; thus the
aggregate evaluation is highly sensitive to the subjective scales chosen by the
judges. Secondly, the judges provided both cardinal and ordinal evaluations
(ratings and rankings) of the papers they reviewed. The contribution here is a
new robust methodology that jointly aggregates ordinal and cardinal evaluations
into a collective evaluation. This methodology is particularly suitable in
cases of incomplete evaluations -- i.e., when the individuals evaluate only a
strict subset of the objects. This approach is potentially useful in managerial
decision making problems by a committee selecting projects from a large set or
capital budgeting involving multiple priorities.
</p>
<a href="http://arxiv.org/abs/2101.04765" target="_blank">arXiv:2101.04765</a> [<a href="http://arxiv.org/pdf/2101.04765" target="_blank">pdf</a>]

<h2>Forecasting glycaemia in Type 1 Diabetes Mellitus with univariate ML algorithms. (arXiv:2101.04770v1 [cs.LG])</h2>
<h3>Ignacio Rodriguez</h3>
<p>AI procedures joined with wearable gadgets can convey exact transient blood
glucose level forecast models. Also, such models can learn customized
glucose-insulin elements dependent on the sensor information gathered by
observing a few parts of the physiological condition and every day movement of
a person. Up to this point, the predominant methodology for creating
information driven forecast models was to gather "however much information as
could be expected" to help doctors and patients ideally change treatment. The
goal of this work was to examine the base information assortment, volume, and
speed needed to accomplish exact individual driven diminutive term expectation
models. We built up a progression of these models utilizing distinctive AI time
arrangement guaging strategies that are appropriate for execution inside a
wearable processor. We completed a broad aloof patient checking concentrate in
genuine conditions to fabricate a strong informational collection. The
examination included a subset of type-1 diabetic subjects wearing a glimmer
glucose checking framework. We directed a relative quantitative assessment of
the presentation of the created information driven expectation models and
comparing AI methods. Our outcomes show that precise momentary forecast can be
accomplished by just checking interstitial glucose information over a brief
timeframe and utilizing a low examining recurrence. The models created can
anticipate glucose levels inside a 15-minute skyline with a normal mistake as
low as 15.43 mg/dL utilizing just 24 memorable qualities gathered inside a time
of 6 hours, and by expanding the inspecting recurrence to incorporate 72
qualities, the normal blunder is limited to 10.15 mg/dL. Our forecast models
are reasonable for execution inside a wearable gadget, requiring the base
equipment necessities while simultaneously accomplishing high expectation
precision.
</p>
<a href="http://arxiv.org/abs/2101.04770" target="_blank">arXiv:2101.04770</a> [<a href="http://arxiv.org/pdf/2101.04770" target="_blank">pdf</a>]

<h2>DuctTake: Spatiotemporal Video Compositing. (arXiv:2101.04772v1 [cs.CV])</h2>
<h3>Jan Rueegg, Oliver Wang, Aljoscha Smolic, Markus Gross</h3>
<p>DuctTake is a system designed to enable practical compositing of multiple
takes of a scene into a single video. Current industry solutions are based
around object segmentation, a hard problem that requires extensive manual input
and cleanup, making compositing an expensive part of the film-making process.
Our method instead composites shots together by finding optimal spatiotemporal
seams using motion-compensated 3D graph cuts through the video volume. We
describe in detail the required components, decisions, and new techniques that
together make a usable, interactive tool for compositing HD video, paying
special attention to running time and performance of each section. We validate
our approach by presenting a wide variety of examples and by comparing result
quality and creation time to composites made by professional artists using
current state-of-the-art tools.
</p>
<a href="http://arxiv.org/abs/2101.04772" target="_blank">arXiv:2101.04772</a> [<a href="http://arxiv.org/pdf/2101.04772" target="_blank">pdf</a>]

<h2>Towards Faster and Stabilized GAN Training for High-fidelity Few-shot Image Synthesis. (arXiv:2101.04775v1 [cs.CV])</h2>
<h3>Bingchen Liu, Yizhe Zhu, Kunpeng Song, Ahmed Elgammal</h3>
<p>Training Generative Adversarial Networks (GAN) on high-fidelity images
usually requires large-scale GPU-clusters and a vast number of training images.
In this paper, we study the few-shot image synthesis task for GAN with minimum
computing cost. We propose a light-weight GAN structure that gains superior
quality on 1024*1024 resolution. Notably, the model converges from scratch with
just a few hours of training on a single RTX-2080 GPU, and has a consistent
performance, even with less than 100 training samples. Two technique designs
constitute our work, a skip-layer channel-wise excitation module and a
self-supervised discriminator trained as a feature-encoder. With thirteen
datasets covering a wide variety of image domains (The datasets and code are
available at: https://github.com/odegeasslbc/FastGAN-pytorch), we show our
model's superior performance compared to the state-of-the-art StyleGAN2, when
data and computing budget are limited.
</p>
<a href="http://arxiv.org/abs/2101.04775" target="_blank">arXiv:2101.04775</a> [<a href="http://arxiv.org/pdf/2101.04775" target="_blank">pdf</a>]

<h2>Binary TTC: A Temporal Geofence for Autonomous Navigation. (arXiv:2101.04777v1 [cs.CV])</h2>
<h3>Abhishek Badki, Orazio Gallo, Jan Kautz, Pradeep Sen</h3>
<p>Time-to-contact (TTC), the time for an object to collide with the observer's
plane, is a powerful tool for path planning: it is potentially more informative
than the depth, velocity, and acceleration of objects in the scene -- even for
humans. TTC presents several advantages, including requiring only a monocular,
uncalibrated camera. However, regressing TTC for each pixel is not
straightforward, and most existing methods make over-simplifying assumptions
about the scene. We address this challenge by estimating TTC via a series of
simpler, binary classifications. We predict with low latency whether the
observer will collide with an obstacle within a certain time, which is often
more critical than knowing exact, per-pixel TTC. For such scenarios, our method
offers a temporal geofence in 6.4 ms -- over 25x faster than existing methods.
Our approach can also estimate per-pixel TTC with arbitrarily fine quantization
(including continuous values), when the computational budget allows for it. To
the best of our knowledge, our method is the first to offer TTC information
(binary or coarsely quantized) at sufficiently high frame-rates for practical
use.
</p>
<a href="http://arxiv.org/abs/2101.04777" target="_blank">arXiv:2101.04777</a> [<a href="http://arxiv.org/pdf/2101.04777" target="_blank">pdf</a>]

<h2>Transferring Experience from Simulation to the Real World for Precise Pick-And-Place Tasks in Highly Cluttered Scenes. (arXiv:2101.04781v1 [cs.RO])</h2>
<h3>Kilian Kleeberger, Markus V&#xf6;lk, Marius Moosmann, Erik Thiessenhusen, Florian Roth, Richard Bormann, Marco F. Huber</h3>
<p>In this paper, we introduce a novel learning-based approach for grasping
known rigid objects in highly cluttered scenes and precisely placing them based
on depth images. Our Placement Quality Network (PQ-Net) estimates the object
pose and the quality for each automatically generated grasp pose for multiple
objects simultaneously at 92 fps in a single forward pass of a neural network.
All grasping and placement trials are executed in a physics simulation and the
gained experience is transferred to the real world using domain randomization.
We demonstrate that our policy successfully transfers to the real world. PQ-Net
outperforms other model-free approaches in terms of grasping success rate and
automatically scales to new objects of arbitrary symmetry without any human
intervention.
</p>
<a href="http://arxiv.org/abs/2101.04781" target="_blank">arXiv:2101.04781</a> [<a href="http://arxiv.org/pdf/2101.04781" target="_blank">pdf</a>]

<h2>Scalable Anytime Planning for Multi-Agent MDPs. (arXiv:2101.04788v1 [cs.AI])</h2>
<h3>Shushman Choudhury, Jayesh K. Gupta, Peter Morales, Mykel J. Kochenderfer</h3>
<p>We present a scalable tree search planning algorithm for large multi-agent
sequential decision problems that require dynamic collaboration. Teams of
agents need to coordinate decisions in many domains, but naive approaches fail
due to the exponential growth of the joint action space with the number of
agents. We circumvent this complexity through an anytime approach that allows
us to trade computation for approximation quality and also dynamically
coordinate actions. Our algorithm comprises three elements: online planning
with Monte Carlo Tree Search (MCTS), factored representations of local agent
interactions with coordination graphs, and the iterative Max-Plus method for
joint action selection. We evaluate our approach on the benchmark SysAdmin
domain with static coordination graphs and achieve comparable performance with
much lower computation cost than our MCTS baselines. We also introduce a
multi-drone delivery domain with dynamic, i.e., state-dependent coordination
graphs, and demonstrate how our approach scales to large problems on this
domain that are intractable for other MCTS methods. We provide an open-source
implementation of our algorithm at
https://github.com/JuliaPOMDP/FactoredValueMCTS.jl.
</p>
<a href="http://arxiv.org/abs/2101.04788" target="_blank">arXiv:2101.04788</a> [<a href="http://arxiv.org/pdf/2101.04788" target="_blank">pdf</a>]

<h2>Graph Filtering for Improving the Accuracy of Classification problems. (arXiv:2101.04789v1 [stat.ML])</h2>
<h3>Mounia Hamidouche, Carlos Lassance, Yuqing Hu, Lucas Drumetz, Bastien Pasdeloup, Vincent Gripon</h3>
<p>In machine learning, classifiers are typically susceptible to noise in the
training data. In this work, we aim at reducing intra-class noise with the help
of graph filtering to improve the classification performance. Considered graphs
are obtained by connecting samples of the training set that belong to a same
class depending on the similarity of their representation in a latent space. As
a matter of fact, by looking at the features in latent representations of
samples as graph signals, it is possible to filter them in order to remove high
frequencies, thus improving the signal-to-noise ratio. A consequence is that
intra-class variance gets smaller, while mean remains the same, as shown
theoretically in this article. We support this analysis through experimental
evaluation of the graph filtering impact on the accuracy of multiple standard
benchmarks of the field. While our approach applies to all classification
problems in general, it is particularly useful in few-shot settings, where
intra-class noise has a huge impact due to initial sample selection.
</p>
<a href="http://arxiv.org/abs/2101.04789" target="_blank">arXiv:2101.04789</a> [<a href="http://arxiv.org/pdf/2101.04789" target="_blank">pdf</a>]

<h2>Personalized Federated Deep Learning for Pain Estimation From Face Images. (arXiv:2101.04800v1 [cs.LG])</h2>
<h3>Ognjen Rudovic, Nicolas Tobis, Sebastian Kaltwang, Bj&#xf6;rn Schuller, Daniel Rueckert, Jeffrey F. Cohn, Rosalind W. Picard</h3>
<p>Standard machine learning approaches require centralizing the users' data in
one computer or a shared database, which raises data privacy and
confidentiality concerns. Therefore, limiting central access is important,
especially in healthcare settings, where data regulations are strict. A
potential approach to tackling this is Federated Learning (FL), which enables
multiple parties to collaboratively learn a shared prediction model by using
parameters of locally trained models while keeping raw training data locally.
In the context of AI-assisted pain-monitoring, we wish to enable
confidentiality-preserving and unobtrusive pain estimation for long-term
pain-monitoring and reduce the burden on the nursing staff who perform frequent
routine check-ups. To this end, we propose a novel Personalized Federated Deep
Learning (PFDL) approach for pain estimation from face images. PFDL performs
collaborative training of a deep model, implemented using a lightweight CNN
architecture, across different clients (i.e., subjects) without sharing their
face images. Instead of sharing all parameters of the model, as in standard FL,
PFDL retains the last layer locally (used to personalize the pain estimates).
This (i) adds another layer of data confidentiality, making it difficult for an
adversary to infer pain levels of the target subject, while (ii) personalizing
the pain estimation to each subject through local parameter tuning. We show
using a publicly available dataset of face videos of pain (UNBC-McMaster
Shoulder Pain Database), that PFDL performs comparably or better than the
standard centralized and FL algorithms, while further enhancing data privacy.
This, has the potential to improve traditional pain monitoring by making it
more secure, computationally efficient, and scalable to a large number of
individuals (e.g., for in-home pain monitoring), providing timely and
unobtrusive pain measurement.
</p>
<a href="http://arxiv.org/abs/2101.04800" target="_blank">arXiv:2101.04800</a> [<a href="http://arxiv.org/pdf/2101.04800" target="_blank">pdf</a>]

<h2>Embedded Computer Vision System Applied to a Four-Legged Line Follower Robot. (arXiv:2101.04804v1 [cs.RO])</h2>
<h3>Beatriz Arruda Asfora</h3>
<p>Robotics can be defined as the connection of perception to action. Taking
this further, this project aims to drive a robot using an automated computer
vision embedded system, connecting the robot's vision to its behavior. In order
to implement a color recognition system on the robot, open source tools are
chosen, such as Processing language, Android system, Arduino platform and Pixy
camera. The constraints are clear: simplicity, replicability and financial
viability. In order to integrate Robotics, Computer Vision and Image
Processing, the robot is applied on a typical mobile robot's issue: line
following. The problem of distinguishing the path from the background is
analyzed through different approaches: the popular Otsu's Method, thresholding
based on color combinations through experimentation and color tracking via hue
and saturation. Decision making of where to move next is based on the line
center of the path and is fully automated. Using a four-legged robot as
platform and a camera as its only sensor, the robot is capable of successfully
follow a line. From capturing the image to moving the robot, it's evident how
integrative Robotics can be. The issue of this paper alone involves knowledge
of Mechanical Engineering, Electronics, Control Systems and Programming.
Everything related to this work was documented and made available on an open
source online page, so it can be useful in learning and experimenting with
robotics.
</p>
<a href="http://arxiv.org/abs/2101.04804" target="_blank">arXiv:2101.04804</a> [<a href="http://arxiv.org/pdf/2101.04804" target="_blank">pdf</a>]

<h2>Digital Elevation Model enhancement using Deep Learning. (arXiv:2101.04812v1 [cs.CV])</h2>
<h3>Casey Handmer</h3>
<p>We demonstrate high fidelity enhancement of planetary digital elevation
models (DEMs) using optical images and deep learning with convolutional neural
networks. Enhancement can be applied recursively to the limit of available
optical data, representing a 90x resolution improvement in global Mars DEMs.
Deep learning-based photoclinometry robustly recovers features obscured by
non-ideal lighting conditions. Method can be automated at global scale.
Analysis shows enhanced DEM slope errors are comparable with high resolution
maps using conventional, labor intensive methods.
</p>
<a href="http://arxiv.org/abs/2101.04812" target="_blank">arXiv:2101.04812</a> [<a href="http://arxiv.org/pdf/2101.04812" target="_blank">pdf</a>]

<h2>Energy-Efficient Distributed Learning Algorithms for Coarsely Quantized Signals. (arXiv:2101.04824v1 [cs.LG])</h2>
<h3>A. Danaee, R. C. de Lamare, V. H. Nascimento</h3>
<p>In this work, we present an energy-efficient distributed learning framework
using low-resolution ADCs and coarsely quantized signals for Internet of Things
(IoT) networks. In particular, we develop a distributed quantization-aware
least-mean square (DQA-LMS) algorithm that can learn parameters in an
energy-efficient fashion using signals quantized with few bits while requiring
a low computational cost. We also carry out a statistical analysis of the
proposed DQA-LMS algorithm that includes a stability condition. Simulations
assess the DQA-LMS algorithm against existing techniques for a distributed
parameter estimation task where IoT devices operate in a peer-to-peer mode and
demonstrate the effectiveness of the DQA-LMS algorithm.
</p>
<a href="http://arxiv.org/abs/2101.04824" target="_blank">arXiv:2101.04824</a> [<a href="http://arxiv.org/pdf/2101.04824" target="_blank">pdf</a>]

<h2>Urban land-use analysis using proximate sensing imagery: a survey. (arXiv:2101.04827v1 [cs.CV])</h2>
<h3>Zhinan Qiao, Xiaohui Yuan</h3>
<p>Urban regions are complicated functional systems that are closely associated
with and reshaped by human activities. The propagation of online geographic
information-sharing platforms and mobile devices equipped with Global
Positioning System (GPS) greatly proliferates proximate sensing images taken
near or on the ground at a close distance to urban targets. Studies leveraging
proximate sensing imagery have demonstrated great potential to address the need
for local data in urban land-use analysis. This paper reviews and summarizes
the state-of-the-art methods and publicly available datasets from proximate
sensing to support land-use analysis. We identify several research problems in
the perspective of examples to support training of models and means of
integrating diverse data sets. Our discussions highlight the challenges,
strategies, and opportunities faced by the existing methods using proximate
sensing imagery in urban land-use studies.
</p>
<a href="http://arxiv.org/abs/2101.04827" target="_blank">arXiv:2101.04827</a> [<a href="http://arxiv.org/pdf/2101.04827" target="_blank">pdf</a>]

<h2>Robust GPS-Vision Localization via Integrity-Driven Landmark Attention. (arXiv:2101.04836v1 [cs.RO])</h2>
<h3>Sriramya Bhamidipati, Grace Xingxin Gao</h3>
<p>For robust GPS-vision navigation in urban areas, we propose an
Integrity-driven Landmark Attention (ILA) technique via stochastic
reachability. Inspired by cognitive attention in humans, we perform convex
optimization to select a subset of landmarks from GPS and vision measurements
that maximizes integrity-driven performance. Given known measurement error
bounds in non-faulty conditions, our ILA follows a unified approach to address
both GPS and vision faults and is compatible with any off-the-shelf estimator.
We analyze measurement deviation to estimate the stochastic reachable set of
expected position for each landmark, which is parameterized via probabilistic
zonotope (p-Zonotope). We apply set union to formulate a p-Zonotopic cost that
represents the size of position bounds based on landmark inclusion/exclusion.
We jointly minimize the p-Zonotopic cost and maximize the number of landmarks
via convex relaxation. For an urban dataset, we demonstrate improved
localization accuracy and robust predicted availability for a pre-defined alert
limit.
</p>
<a href="http://arxiv.org/abs/2101.04836" target="_blank">arXiv:2101.04836</a> [<a href="http://arxiv.org/pdf/2101.04836" target="_blank">pdf</a>]

<h2>Feature refinement: An expression-specific feature learning and fusion method for micro-expression recognition. (arXiv:2101.04838v1 [cs.CV])</h2>
<h3>Ling Zhou, Qirong Mao, Xiaohua Huang, Feifei Zhang, Zhihong Zhang</h3>
<p>Micro-Expression Recognition has become challenging, as it is extremely
difficult to extract the subtle facial changes of micro-expressions. Recently,
several approaches proposed several expression-shared features algorithms for
micro-expression recognition. However, they do not reveal the specific
discriminative characteristics, which lead to sub-optimal performance. This
paper proposes a novel Feature Refinement ({FR}) with expression-specific
feature learning and fusion for micro-expression recognition. It aims to obtain
salient and discriminative features for specific expressions and also predict
expression by fusing the expression-specific features. FR consists of an
expression proposal module with attention mechanism and a classification
branch. First, an inception module is designed based on optical flow to obtain
expression-shared features. Second, in order to extract salient and
discriminative features for specific expression, expression-shared features are
fed into an expression proposal module with attention factors and proposal
loss. Last, in the classification branch, labels of categories are predicted by
a fusion of the expression-specific features. Experiments on three publicly
available databases validate the effectiveness of FR under different protocol.
Results on public benchmarks demonstrate that our FR provides salient and
discriminative information for micro-expression recognition. The results also
show our FR achieves better or competitive performance with the existing
state-of-the-art methods on micro-expression recognition.
</p>
<a href="http://arxiv.org/abs/2101.04838" target="_blank">arXiv:2101.04838</a> [<a href="http://arxiv.org/pdf/2101.04838" target="_blank">pdf</a>]

<h2>Reproducing Activation Function for Deep Learning. (arXiv:2101.04844v1 [cs.LG])</h2>
<h3>Senwei Liang, Liyao Lyu, Chunmei Wang, Haizhao Yang</h3>
<p>In this paper, we propose the reproducing activation function to improve deep
learning accuracy for various applications ranging from computer vision
problems to scientific computing problems. The idea of reproducing activation
functions is to employ several basic functions and their learnable linear
combination to construct neuron-wise data-driven activation functions for each
neuron. Armed with such activation functions, deep neural networks can
reproduce traditional approximation tools and, therefore, approximate target
functions with a smaller number of parameters than traditional neural networks.
In terms of training dynamics of deep learning, reproducing activation
functions can generate neural tangent kernels with a better condition number
than traditional activation functions lessening the spectral bias of deep
learning. As demonstrated by extensive numerical tests, the proposed activation
function can facilitate the convergence of deep learning optimization for a
solution with higher accuracy than existing deep learning solvers for
audio/image/video reconstruction, PDEs, and eigenvalue problems.
</p>
<a href="http://arxiv.org/abs/2101.04844" target="_blank">arXiv:2101.04844</a> [<a href="http://arxiv.org/pdf/2101.04844" target="_blank">pdf</a>]

<h2>Adversarial Sample Enhanced Domain Adaptation: A Case Study on Predictive Modeling with Electronic Health Records. (arXiv:2101.04853v1 [cs.LG])</h2>
<h3>Yiqin Yu, Pin-Yu Chen, Yuan Zhou, Jing Mei</h3>
<p>With the successful adoption of machine learning on electronic health records
(EHRs), numerous computational models have been deployed to address a variety
of clinical problems. However, due to the heterogeneity of EHRs, models trained
on different patient groups suffer from poor generalizability. How to mitigate
domain shifts between the source patient group where the model is built upon
and the target one where the model will be deployed becomes a critical issue.
In this paper, we propose a data augmentation method to facilitate domain
adaptation, which leverages knowledge from the source patient group when
training model on the target one. Specifically, adversarially generated samples
are used during domain adaptation to fill the generalization gap between the
two patient groups. The proposed method is evaluated by a case study on
different predictive modeling tasks on MIMIC-III EHR dataset. Results confirm
the effectiveness of our method and the generality on different tasks.
</p>
<a href="http://arxiv.org/abs/2101.04853" target="_blank">arXiv:2101.04853</a> [<a href="http://arxiv.org/pdf/2101.04853" target="_blank">pdf</a>]

<h2>A Recurrent Neural Network Approach to Roll Estimation for Needle Steering. (arXiv:2101.04856v1 [cs.RO])</h2>
<h3>Maxwell Emerson, James M. Ferguson, Tayfun Efe Ertop, Margaret Rox, Josephine Granna, Michael Lester, Fabien Maldonado, Erin A. Gillaspie, Ron Alterovitz, Robert J. Webster III., Alan Kuntz</h3>
<p>Steerable needles are a promising technology for delivering targeted
therapies in the body in a minimally-invasive fashion, as they can curve around
anatomical obstacles and hone in on anatomical targets. In order to accurately
steer them, controllers must have full knowledge of the needle tip's
orientation. However, current sensors either do not provide full orientation
information or interfere with the needle's ability to deliver therapy. Further,
torsional dynamics can vary and depend on many parameters making steerable
needles difficult to accurately model, limiting the effectiveness of
traditional observer methods. To overcome these limitations, we propose a
model-free, learned-method that leverages LSTM neural networks to estimate the
needle tip's orientation online. We validate our method by integrating it into
a sliding-mode controller and steering the needle to targets in gelatin and ex
vivo ovine brain tissue. We compare our method's performance against an
Extended Kalman Filter, a model-based observer, achieving significantly lower
targeting errors.
</p>
<a href="http://arxiv.org/abs/2101.04856" target="_blank">arXiv:2101.04856</a> [<a href="http://arxiv.org/pdf/2101.04856" target="_blank">pdf</a>]

<h2>A*HAR: A New Benchmark towards Semi-supervised learning for Class-imbalanced Human Activity Recognition. (arXiv:2101.04859v1 [cs.LG])</h2>
<h3>Govind Narasimman, Kangkang Lu, Arun Raja, Chuan Sheng Foo, Mohamed Sabry Aly, Jie Lin, Vijay Chandrasekhar</h3>
<p>Despite the vast literature on Human Activity Recognition (HAR) with wearable
inertial sensor data, it is perhaps surprising that there are few studies
investigating semisupervised learning for HAR, particularly in a challenging
scenario with class imbalance problem. In this work, we present a new
benchmark, called A*HAR, towards semisupervised learning for class-imbalanced
HAR. We evaluate state-of-the-art semi-supervised learning method on A*HAR, by
combining Mean Teacher and Convolutional Neural Network. Interestingly, we find
that Mean Teacher boosts the overall performance when training the classifier
with fewer labelled samples and a large amount of unlabeled samples, but the
classifier falls short in handling unbalanced activities. These findings lead
to an interesting open problem, i.e., development of semi-supervised HAR
algorithms that are class-imbalance aware without any prior knowledge on the
class distribution for unlabeled samples. The dataset and benchmark evaluation
are released at https://github.com/I2RDL2/ASTAR-HAR for future research.
</p>
<a href="http://arxiv.org/abs/2101.04859" target="_blank">arXiv:2101.04859</a> [<a href="http://arxiv.org/pdf/2101.04859" target="_blank">pdf</a>]

<h2>Towards Energy Efficient Federated Learning over 5G+ Mobile Devices. (arXiv:2101.04866v1 [cs.LG])</h2>
<h3>Dian Shi, Liang Li, Rui Chen, Pavana Prakash, Miao Pan, Yuguang Fang</h3>
<p>The continuous convergence of machine learning algorithms, 5G and beyond
(5G+) wireless communications, and artificial intelligence (AI) hardware
implementation hastens the birth of federated learning (FL) over 5G+ mobile
devices, which pushes AI functions to mobile devices and initiates a new era of
on-device AI applications. Despite the remarkable progress made in FL, huge
energy consumption is one of the most significant obstacles restricting the
development of FL over battery-constrained 5G+ mobile devices. To address this
issue, in this paper, we investigate how to develop energy efficient FL over
5G+ mobile devices by making a trade-off between energy consumption for
"working" (i.e., local computing) and that for "talking" (i.e., wireless
communications) in order to boost the overall energy efficiency. Specifically,
we first examine energy consumption models for graphics processing unit (GPU)
computation and wireless transmissions. Then, we overview the state of the art
of integrating FL procedure with energy-efficient learning techniques (e.g.,
gradient sparsification, weight quantization, pruning, etc.). Finally, we
present several potential future research directions for FL over 5G+ mobile
devices from the perspective of energy efficiency.
</p>
<a href="http://arxiv.org/abs/2101.04866" target="_blank">arXiv:2101.04866</a> [<a href="http://arxiv.org/pdf/2101.04866" target="_blank">pdf</a>]

<h2>Convolutional Neural Nets: Foundations, Computations, and New Applications. (arXiv:2101.04869v1 [cs.LG])</h2>
<h3>Shengli Jiang, Victor M. Zavala</h3>
<p>We review mathematical foundations of convolutional neural nets (CNNs) with
the goals of: i) highlighting connections with techniques from statistics,
signal processing, linear algebra, differential equations, and optimization,
ii) demystifying underlying computations, and iii) identifying new types of
applications. CNNs are powerful machine learning models that highlight features
from grid data to make predictions (regression and classification). The grid
data object can be represented as vectors (in 1D), matrices (in 2D), or tensors
(in 3D or higher dimensions) and can incorporate multiple channels (thus
providing high flexibility in the input data representation). For example, an
image can be represented as a 2D grid data object that contains red, green, and
blue (RBG) channels (each channel is a 2D matrix). Similarly, a video can be
represented as a 3D grid data object (two spatial dimensions plus time) with
RGB channels (each channel is a 3D tensor). CNNs highlight features from the
grid data by performing convolution operations with different types of
operators. The operators highlight different types of features (e.g., patterns,
gradients, geometrical features) and are learned by using optimization
techniques. In other words, CNNs seek to identify optimal operators that best
map the input data to the output data. A common misconception is that CNNs are
only capable of processing image or video data but their application scope is
much wider; specifically, datasets encountered in diverse applications can be
expressed as grid data. Here, we show how to apply CNNs to new types of
applications such as optimal control, flow cytometry, multivariate process
monitoring, and molecular simulations.
</p>
<a href="http://arxiv.org/abs/2101.04869" target="_blank">arXiv:2101.04869</a> [<a href="http://arxiv.org/pdf/2101.04869" target="_blank">pdf</a>]

<h2>Asymmetric self-play for automatic goal discovery in robotic manipulation. (arXiv:2101.04882v1 [cs.LG])</h2>
<h3>OpenAI OpenAI, Matthias Plappert, Raul Sampedro, Tao Xu, Ilge Akkaya, Vineet Kosaraju, Peter Welinder, Ruben D&#x27;Sa, Arthur Petron, Henrique Ponde de Oliveira Pinto, Alex Paino, Hyeonwoo Noh, Lilian Weng, Qiming Yuan, Casey Chu, Wojciech Zaremba</h3>
<p>We train a single, goal-conditioned policy that can solve many robotic
manipulation tasks, including tasks with previously unseen goals and objects.
We rely on asymmetric self-play for goal discovery, where two agents, Alice and
Bob, play a game. Alice is asked to propose challenging goals and Bob aims to
solve them. We show that this method can discover highly diverse and complex
goals without any human priors. Bob can be trained with only sparse rewards,
because the interaction between Alice and Bob results in a natural curriculum
and Bob can learn from Alice's trajectory when relabeled as a goal-conditioned
demonstration. Finally, our method scales, resulting in a single policy that
can generalize to many unseen tasks such as setting a table, stacking blocks,
and solving simple puzzles. Videos of a learned policy is available at
https://robotics-self-play.github.io.
</p>
<a href="http://arxiv.org/abs/2101.04882" target="_blank">arXiv:2101.04882</a> [<a href="http://arxiv.org/pdf/2101.04882" target="_blank">pdf</a>]

<h2>Piano Skills Assessment. (arXiv:2101.04884v1 [cs.CV])</h2>
<h3>Paritosh Parmar, Jaiden Reddy, Brendan Morris</h3>
<p>Can a computer determine a piano player's skill level? Is it preferable to
base this assessment on visual analysis of the player's performance or should
we trust our ears over our eyes? Since current CNNs have difficulty processing
long video videos, how can shorter clips be sampled to best reflect the players
skill level? In this work, we collect and release a first-of-its-kind dataset
for multimodal skill assessment focusing on assessing piano player's skill
level, answer the asked questions, initiate work in automated evaluation of
piano playing skills and provide baselines for future work.
</p>
<a href="http://arxiv.org/abs/2101.04884" target="_blank">arXiv:2101.04884</a> [<a href="http://arxiv.org/pdf/2101.04884" target="_blank">pdf</a>]

<h2>Towards Interpretable Ensemble Learning for Image-based Malware Detection. (arXiv:2101.04889v1 [cs.LG])</h2>
<h3>Yuzhou Lin, Xiaolin Chang</h3>
<p>Deep learning (DL) models for image-based malware detection have exhibited
their capability in producing high prediction accuracy. But model
interpretability is posing challenges to their widespread application in
security and safety-critical application domains. This paper aims for designing
an Interpretable Ensemble learning approach for image-based Malware Detection
(IEMD). We first propose a Selective Deep Ensemble Learning-based (SDEL)
detector and then design an Ensemble Deep Taylor Decomposition (EDTD) approach,
which can give the pixel-level explanation to SDEL detector outputs.
Furthermore, we develop formulas for calculating fidelity, robustness and
expressiveness on pixel-level heatmaps in order to assess the quality of EDTD
explanation. With EDTD explanation, we develop a novel Interpretable Dropout
approach (IDrop), which establishes IEMD by training SDEL detector. Experiment
results exhibit the better explanation of our EDTD than the previous
explanation methods for image-based malware detection. Besides, experiment
results indicate that IEMD achieves a higher detection accuracy up to 99.87%
while exhibiting interpretability with high quality of prediction results.
Moreover, experiment results indicate that IEMD interpretability increases with
the increasing detection accuracy during the construction of IEMD. This
consistency suggests that IDrop can mitigate the tradeoff between model
interpretability and detection accuracy.
</p>
<a href="http://arxiv.org/abs/2101.04889" target="_blank">arXiv:2101.04889</a> [<a href="http://arxiv.org/pdf/2101.04889" target="_blank">pdf</a>]

<h2>Singularity-free Aerial Deformation by Two-dimensional Multilinked Aerial Robot with 1-DoF Vectorable Propeller. (arXiv:2101.04892v1 [cs.RO])</h2>
<h3>Moju Zhao, Tomoki Anzai, Kei Okada, Masayuki Inaba</h3>
<p>Two-dimensional multilinked structures can benefit aerial robots in both
maneuvering and manipulation because of their deformation ability. However,
certain types of singular forms must be avoided during deformation. Hence, an
additional 1 Degrees-of-Freedom (DoF) vectorable propeller is employed in this
work to overcome singular forms by properly changing the thrust direction. In
this paper, we first extend modeling and control methods from our previous
works for an under-actuated model whose thrust forces are not unidirectional.
We then propose a planning method for the vectoring angles to solve the
singularity by maximizing the controllability under arbitrary robot forms.
Finally, we demonstrate the feasibility of the proposed methods by experiments
where a quad-type model is used to perform trajectory tracking under
challenging forms, such as a line-shape form, and the deformation passing these
challenging forms.
</p>
<a href="http://arxiv.org/abs/2101.04892" target="_blank">arXiv:2101.04892</a> [<a href="http://arxiv.org/pdf/2101.04892" target="_blank">pdf</a>]

<h2>Unlearnable Examples: Making Personal Data Unexploitable. (arXiv:2101.04898v1 [cs.LG])</h2>
<h3>Hanxun Huang, Xingjun Ma, Sarah Monazam Erfani, James Bailey, Yisen Wang</h3>
<p>The volume of "free" data on the internet has been key to the current success
of deep learning. However, it also raises privacy concerns about the
unauthorized exploitation of personal data for training commercial models. It
is thus crucial to develop methods to prevent unauthorized data exploitation.
This paper raises the question: \emph{can data be made unlearnable for deep
learning models?} We present a type of \emph{error-minimizing} noise that can
indeed make training examples unlearnable. Error-minimizing noise is
intentionally generated to reduce the error of one or more of the training
example(s) close to zero, which can trick the model into believing there is
"nothing" to learn from these example(s). The noise is restricted to be
imperceptible to human eyes, and thus does not affect normal data utility. We
empirically verify the effectiveness of error-minimizing noise in both
sample-wise and class-wise forms. We also demonstrate its flexibility under
extensive experimental settings and practicability in a case study of face
recognition. Our work establishes an important first step towards making
personal data unexploitable to deep learning models.
</p>
<a href="http://arxiv.org/abs/2101.04898" target="_blank">arXiv:2101.04898</a> [<a href="http://arxiv.org/pdf/2101.04898" target="_blank">pdf</a>]

<h2>EEC: Learning to Encode and Regenerate Images for Continual Learning. (arXiv:2101.04904v1 [cs.CV])</h2>
<h3>Ali Ayub, Alan R. Wagner</h3>
<p>The two main impediments to continual learning are catastrophic forgetting
and memory limitations on the storage of data. To cope with these challenges,
we propose a novel, cognitively-inspired approach which trains autoencoders
with Neural Style Transfer to encode and store images. During training on a new
task, reconstructed images from encoded episodes are replayed in order to avoid
catastrophic forgetting. The loss function for the reconstructed images is
weighted to reduce its effect during classifier training to cope with image
degradation. When the system runs out of memory the encoded episodes are
converted into centroids and covariance matrices, which are used to generate
pseudo-images during classifier training, keeping classifier performance stable
while using less memory. Our approach increases classification accuracy by
13-17% over state-of-the-art methods on benchmark datasets, while requiring 78%
less storage space.
</p>
<a href="http://arxiv.org/abs/2101.04904" target="_blank">arXiv:2101.04904</a> [<a href="http://arxiv.org/pdf/2101.04904" target="_blank">pdf</a>]

<h2>COVID-19 Deterioration Prediction via Self-Supervised Representation Learning and Multi-Image Prediction. (arXiv:2101.04909v1 [cs.CV])</h2>
<h3>Anuroop Sriram, Matthew Muckley, Koustuv Sinha, Farah Shamout, Joelle Pineau, Krzysztof J. Geras, Lea Azour, Yindalon Aphinyanaphongs, Nafissa Yakubova, William Moore</h3>
<p>The rapid spread of COVID-19 cases in recent months has strained hospital
resources, making rapid and accurate triage of patients presenting to emergency
departments a necessity. Machine learning techniques using clinical data such
as chest X-rays have been used to predict which patients are most at risk of
deterioration. We consider the task of predicting two types of patient
deterioration based on chest X-rays: adverse event deterioration (i.e.,
transfer to the intensive care unit, intubation, or mortality) and increased
oxygen requirements beyond 6 L per day. Due to the relative scarcity of
COVID-19 patient data, existing solutions leverage supervised pretraining on
related non-COVID images, but this is limited by the differences between the
pretraining data and the target COVID-19 patient data. In this paper, we use
self-supervised learning based on the momentum contrast (MoCo) method in the
pretraining phase to learn more general image representations to use for
downstream tasks. We present three results. The first is deterioration
prediction from a single image, where our model achieves an area under receiver
operating characteristic curve (AUC) of 0.742 for predicting an adverse event
within 96 hours (compared to 0.703 with supervised pretraining) and an AUC of
0.765 for predicting oxygen requirements greater than 6 L a day at 24 hours
(compared to 0.749 with supervised pretraining). We then propose a new
transformer-based architecture that can process sequences of multiple images
for prediction and show that this model can achieve an improved AUC of 0.786
for predicting an adverse event at 96 hours and an AUC of 0.848 for predicting
mortalities at 96 hours. A small pilot clinical study suggested that the
prediction accuracy of our model is comparable to that of experienced
radiologists analyzing the same information.
</p>
<a href="http://arxiv.org/abs/2101.04909" target="_blank">arXiv:2101.04909</a> [<a href="http://arxiv.org/pdf/2101.04909" target="_blank">pdf</a>]

<h2>Neural Sequence-to-grid Module for Learning Symbolic Rules. (arXiv:2101.04921v1 [cs.LG])</h2>
<h3>Segwang Kim, Hyoungwook Nam, Joonyoung Kim, Kyomin Jung</h3>
<p>Logical reasoning tasks over symbols, such as learning arithmetic operations
and computer program evaluations, have become challenges to deep learning. In
particular, even state-of-the-art neural networks fail to achieve
\textit{out-of-distribution} (OOD) generalization of symbolic reasoning tasks,
whereas humans can easily extend learned symbolic rules. To resolve this
difficulty, we propose a neural sequence-to-grid (seq2grid) module, an input
preprocessor that automatically segments and aligns an input sequence into a
grid. As our module outputs a grid via a novel differentiable mapping, any
neural network structure taking a grid input, such as ResNet or TextCNN, can be
jointly trained with our module in an end-to-end fashion. Extensive experiments
show that neural networks having our module as an input preprocessor achieve
OOD generalization on various arithmetic and algorithmic problems including
number sequence prediction problems, algebraic word problems, and computer
program evaluation problems while other state-of-the-art sequence transduction
models cannot. Moreover, we verify that our module enhances TextCNN to solve
the bAbI QA tasks without external memory.
</p>
<a href="http://arxiv.org/abs/2101.04921" target="_blank">arXiv:2101.04921</a> [<a href="http://arxiv.org/pdf/2101.04921" target="_blank">pdf</a>]

<h2>Learning to Anticipate Egocentric Actions by Imagination. (arXiv:2101.04924v1 [cs.CV])</h2>
<h3>Yu Wu, Linchao Zhu, Xiaohan Wang, Yi Yang, Fei Wu</h3>
<p>Anticipating actions before they are executed is crucial for a wide range of
practical applications, including autonomous driving and robotics. In this
paper, we study the egocentric action anticipation task, which predicts future
action seconds before it is performed for egocentric videos. Previous
approaches focus on summarizing the observed content and directly predicting
future action based on past observations. We believe it would benefit the
action anticipation if we could mine some cues to compensate for the missing
information of the unobserved frames. We then propose to decompose the action
anticipation into a series of future feature predictions. We imagine how the
visual feature changes in the near future and then predicts future action
labels based on these imagined representations. Differently, our ImagineRNN is
optimized in a contrastive learning way instead of feature regression. We
utilize a proxy task to train the ImagineRNN, i.e., selecting the correct
future states from distractors. We further improve ImagineRNN by residual
anticipation, i.e., changing its target to predicting the feature difference of
adjacent frames instead of the frame content. This promotes the network to
focus on our target, i.e., the future action, as the difference between
adjacent frame features is more important for forecasting the future. Extensive
experiments on two large-scale egocentric action datasets validate the
effectiveness of our method. Our method significantly outperforms previous
methods on both the seen test set and the unseen test set of the EPIC Kitchens
Action Anticipation Challenge.
</p>
<a href="http://arxiv.org/abs/2101.04924" target="_blank">arXiv:2101.04924</a> [<a href="http://arxiv.org/pdf/2101.04924" target="_blank">pdf</a>]

<h2>Road images augmentation with synthetic traffic signs using neural networks. (arXiv:2101.04927v1 [cs.CV])</h2>
<h3>Anton Konushin, Boris Faizov, Vlad Shakhuro</h3>
<p>Traffic sign recognition is a well-researched problem in computer vision.
However, the state of the art methods works only for frequent sign classes,
which are well represented in training datasets. We consider the task of rare
traffic sign detection and classification. We aim to solve that problem by
using synthetic training data. Such training data is obtained by embedding
synthetic images of signs in the real photos. We propose three methods for
making synthetic signs consistent with a scene in appearance. These methods are
based on modern generative adversarial network (GAN) architectures. Our
proposed methods allow realistic embedding of rare traffic sign classes that
are absent in the training set. We adapt a variational autoencoder for sampling
plausible locations of new traffic signs in images. We demonstrate that using a
mixture of our synthetic data with real data improves the accuracy of both
classifier and detector.
</p>
<a href="http://arxiv.org/abs/2101.04927" target="_blank">arXiv:2101.04927</a> [<a href="http://arxiv.org/pdf/2101.04927" target="_blank">pdf</a>]

<h2>Supervised deep learning of elastic SRV distances on the shape space of curves. (arXiv:2101.04929v1 [cs.CV])</h2>
<h3>Emmanuel Hartman, Yashil Sukurdeep, Nicolas Charon, Eric Klassen, Martin Bauer</h3>
<p>Motivated by applications from computer vision to bioinformatics, the field
of shape analysis deals with problems where one wants to analyze geometric
objects, such as curves, while ignoring actions that preserve their shape, such
as translations, rotations, or reparametrizations. Mathematical tools have been
developed to define notions of distances, averages, and optimal deformations
for geometric objects. One such framework, which has proven to be successful in
many applications, is based on the square root velocity (SRV) transform, which
allows one to define a computable distance between spatial curves regardless of
how they are parametrized. This paper introduces a supervised deep learning
framework for the direct computation of SRV distances between curves, which
usually requires an optimization over the group of reparametrizations that act
on the curves. The benefits of our approach in terms of computational speed and
accuracy are illustrated via several numerical experiments.
</p>
<a href="http://arxiv.org/abs/2101.04929" target="_blank">arXiv:2101.04929</a> [<a href="http://arxiv.org/pdf/2101.04929" target="_blank">pdf</a>]

<h2>A Non-Parametric Subspace Analysis Approach with Application to Anomaly Detection Ensembles. (arXiv:2101.04932v1 [cs.LG])</h2>
<h3>Marcelo Bacher, Irad Ben-Gal, Erez Shmueli</h3>
<p>Identifying anomalies in multi-dimensional datasets is an important task in
many real-world applications. A special case arises when anomalies are occluded
in a small set of attributes, typically referred to as a subspace, and not
necessarily over the entire data space. In this paper, we propose a new
subspace analysis approach named Agglomerative Attribute Grouping (AAG) that
aims to address this challenge by searching for subspaces that are comprised of
highly correlative attributes. Such correlations among attributes represent a
systematic interaction among the attributes that can better reflect the
behavior of normal observations and hence can be used to improve the
identification of two particularly interesting types of abnormal data samples:
anomalies that are occluded in relatively small subsets of the attributes and
anomalies that represent a new data class. AAG relies on a novel
multi-attribute measure, which is derived from information theory measures of
partitions, for evaluating the "information distance" between groups of data
attributes. To determine the set of subspaces to use, AAG applies a variation
of the well-known agglomerative clustering algorithm with the proposed
multi-attribute measure as the underlying distance function. Finally, the set
of subspaces is used in an ensemble for anomaly detection. Extensive evaluation
demonstrates that, in the vast majority of cases, the proposed AAG method (i)
outperforms classical and state-of-the-art subspace analysis methods when used
in anomaly detection ensembles, and (ii) generates fewer subspaces with a fewer
number of attributes each (on average), thus resulting in a faster training
time for the anomaly detection ensemble. Furthermore, in contrast to existing
methods, the proposed AAG method does not require any tuning of parameters.
</p>
<a href="http://arxiv.org/abs/2101.04932" target="_blank">arXiv:2101.04932</a> [<a href="http://arxiv.org/pdf/2101.04932" target="_blank">pdf</a>]

<h2>ABS: Automatic Bit Sharing for Model Compression. (arXiv:2101.04935v1 [cs.CV])</h2>
<h3>Jing Liu, Bohan Zhuang, Peng Chen, Yong Guo, Chunhua Shen, Jianfei Cai, Mingkui Tan</h3>
<p>We present Automatic Bit Sharing (ABS) to automatically search for optimal
model compression configurations (e.g., pruning ratio and bitwidth). Unlike
previous works that consider model pruning and quantization separately, we seek
to optimize them jointly. To deal with the resultant large designing space, we
propose a novel super-bit model, a single-path method, to encode all candidate
compression configurations, rather than maintaining separate paths for each
configuration. Specifically, we first propose a novel decomposition of
quantization that encapsulates all the candidate bitwidths in the search space.
Starting from a low bitwidth, we sequentially consider higher bitwidths by
recursively adding re-assignment offsets. We then introduce learnable binary
gates to encode the choice of bitwidth, including filter-wise 0-bit for
pruning. By jointly training the binary gates in conjunction with network
parameters, the compression configurations of each layer can be automatically
determined. Our ABS brings two benefits for model compression: 1) It avoids the
combinatorially large design space, with a reduced number of trainable
parameters and search costs. 2) It also averts directly fitting an extremely
low bit quantizer to the data, hence greatly reducing the optimization
difficulty due to the non-differentiable quantization. Experiments on CIFAR-100
and ImageNet show that our methods achieve significant computational cost
reduction while preserving promising performance.
</p>
<a href="http://arxiv.org/abs/2101.04935" target="_blank">arXiv:2101.04935</a> [<a href="http://arxiv.org/pdf/2101.04935" target="_blank">pdf</a>]

<h2>Behavioral Model Inference of Black-box Software using Deep Neural Networks. (arXiv:2101.04948v1 [cs.LG])</h2>
<h3>Mohammad Jafar Mashhadi, Foozhan Ataiefard, Hadi Hemmati, Niel Walkinshaw</h3>
<p>Many software engineering tasks, such as testing, and anomaly detection can
benefit from the ability to infer a behavioral model of the software.Most
existing inference approaches assume access to code to collect execution
sequences. In this paper, we investigate a black-box scenario, where the system
under analysis cannot be instrumented, in this granular fashion.This scenario
is particularly prevalent with control systems' log analysis in the form of
continuous signals. In this situation, an execution trace amounts to a
multivariate time-series of input and output signals, where different states of
the system correspond to different `phases` in the time-series. The main
challenge is to detect when these phase changes take place. Unfortunately, most
existing solutions are either univariate, make assumptions on the data
distribution, or have limited learning power.Therefore, we propose a hybrid
deep neural network that accepts as input a multivariate time series and
applies a set of convolutional and recurrent layers to learn the non-linear
correlations between signals and the patterns over time.We show how this
approach can be used to accurately detect state changes, and how the inferred
models can be successfully applied to transfer-learning scenarios, to
accurately process traces from different products with similar execution
characteristics. Our experimental results on two UAV autopilot case studies
indicate that our approach is highly accurate (over 90% F1 score for state
classification) and significantly improves baselines (by up to 102% for change
point detection).Using transfer learning we also show that up to 90% of the
maximum achievable F1 scores in the open-source case study can be achieved by
reusing the trained models from the industrial case and only fine tuning them
using as low as 5 labeled samples, which reduces the manual labeling effort by
98%.
</p>
<a href="http://arxiv.org/abs/2101.04948" target="_blank">arXiv:2101.04948</a> [<a href="http://arxiv.org/pdf/2101.04948" target="_blank">pdf</a>]

<h2>Modeling and Analysis of Unmanned Remote Guided Vehicle on Rough and Loose Snow Terrain. (arXiv:2101.04952v1 [cs.RO])</h2>
<h3>Abhishek D. Patange, Sharad S. Mulik, R. Jegadeeshwaran, Dhananjay R. Jadhav, Prateek J. Ghatage, Gaurav R. Doshi, Rushikesh V Raykar</h3>
<p>Survival in remote snow bounded areas is unsafe and risky for mankind. Many
problems like arthritis, frostbite, asthma, starvation can caused and lead to
death. Indian Military provides transportation vehicles which are heavily built
and needs manpower for monitoring. Hence it necessitates facilitating compact
transportation to fulfill all requirements. This research aimed at design and
analysis of mobile unmanned vehicle for transportation &amp; providing medical
help, food and other essential things necessary for surviving in such areas.
This can also be used for military services to save the life of solider with
less risk. It is typical medium weight, high speed vehicle which carries up to
35 kg load and can negotiate through loose snow, rough terrain with use of
caterpillar track. The noteworthy feature of the vehicle is that it constitutes
of spiral blades and V shape snowplow to make its way through snow. Hence it
will repel the snow in outward direction for self-extraction. It also
incorporates skis and hubs for changing the direction and smooth suspension. 3D
model of the vehicle is drafted in CATIA and structural analysis is carried out
in ANSYS. Control system design and mechatronics integration is proposed to
develop the prototype by assembling various components.
</p>
<a href="http://arxiv.org/abs/2101.04952" target="_blank">arXiv:2101.04952</a> [<a href="http://arxiv.org/pdf/2101.04952" target="_blank">pdf</a>]

<h2>Learning with Gradient Descent and Weakly Convex Losses. (arXiv:2101.04968v1 [stat.ML])</h2>
<h3>Dominic Richards, Mike Rabbat</h3>
<p>We study the learning performance of gradient descent when the empirical risk
is weakly convex, namely, the smallest negative eigenvalue of the empirical
risk's Hessian is bounded in magnitude. By showing that this eigenvalue can
control the stability of gradient descent, generalisation error bounds are
proven that hold under a wider range of step sizes compared to previous work.
Out of sample guarantees are then achieved by decomposing the test error into
generalisation, optimisation and approximation errors, each of which can be
bounded and traded off with respect to algorithmic parameters, sample size and
magnitude of this eigenvalue. In the case of a two layer neural network, we
demonstrate that the empirical risk can satisfy a notion of local weak
convexity, specifically, the Hessian's smallest eigenvalue during training can
be controlled by the normalisation of the layers, i.e., network scaling. This
allows test error guarantees to then be achieved when the population risk
minimiser satisfies a complexity assumption. By trading off the network
complexity and scaling, insights are gained into the implicit bias of neural
network scaling, which are further supported by experimental findings.
</p>
<a href="http://arxiv.org/abs/2101.04968" target="_blank">arXiv:2101.04968</a> [<a href="http://arxiv.org/pdf/2101.04968" target="_blank">pdf</a>]

<h2>Finite-time disturbance reconstruction and robust fractional-order controller design for hybrid port-Hamiltonian dynamics of biped robots. (arXiv:2101.04974v1 [cs.RO])</h2>
<h3>Yousef Farid, Fabio Ruggiero</h3>
<p>In this paper, disturbance reconstruction and robust trajectory tracking
control of biped robots with hybrid dynamics in the port-Hamiltonian form is
investigated. A new type of Hamiltonian function is introduced, which ensures
the finite-time stability of the closed-loop system. The proposed control
system consists of two loops: an inner and an outer loop. A fractional
proportional-integral-derivative filter is used to achieve finite-time
convergence for position tracking errors at the outer loop. A fractional-order
sliding mode controller acts as a centralized controller at the inner-loop,
ensuring the finite-time stability of the velocity tracking error. In this
loop, the undesired effects of unknown external disturbance and parameter
uncertainties are compensated using estimators. Two disturbance estimators are
envisioned. The former is designed using fractional calculus. The latter is an
adaptive estimator, and it is constructed using the general dynamic of biped
robots. Stability analysis shows that the closed-loop system is finite-time
stable in both contact-less and impact phases. Simulation studies on two types
of biped robots (i.e., two-link walker and RABBIT biped robot) demonstrate the
proposed controller's tracking performance and disturbance rejection
capability.
</p>
<a href="http://arxiv.org/abs/2101.04974" target="_blank">arXiv:2101.04974</a> [<a href="http://arxiv.org/pdf/2101.04974" target="_blank">pdf</a>]

<h2>Large scale deduplication based on fingerprints. (arXiv:2101.04976v1 [cs.CV])</h2>
<h3>Jean Aymar Biyiha Nlend, Ibrahim Moukouop Nguena, Thomas Bouetou Bouetou</h3>
<p>In fingerprint-based systems, the size of databases increases considerably
with population growth. In developing countries, because of the difficulty in
using a central system when enlisting voters, it often happens that several
regional voter databases are created and then merged to form a central
database. A process is used to remove duplicates and ensure uniqueness by
voter. Until now, companies specializing in biometrics use several costly
computing servers with algorithms to perform large-scale deduplication based on
fingerprints. These algorithms take a considerable time because of their
complexity in O (n2), where n is the size of the database. This article
presents an algorithm that can perform this operation in O (2n), with just a
computer. It is based on the development of an index obtained using a 5 * 5
matrix performed on each fingerprint. This index makes it possible to build
clusters of O (1) in size in order to compare fingerprints. This approach has
been evaluated using close to 11 4000 fingerprints, and the results obtained
show that this approach allows a penetration rate of less than 1%, an almost O
(1) identification, and an O (n) deduplication. A base of 10 000 000
fingerprints can be deduplicated with a just computer in less than two hours,
contrary to several days and servers for the usual tools.

Keywords: fingerprint, cluster, index, deduplication.
</p>
<a href="http://arxiv.org/abs/2101.04976" target="_blank">arXiv:2101.04976</a> [<a href="http://arxiv.org/pdf/2101.04976" target="_blank">pdf</a>]

<h2>Multi-Source Anomaly Detection in Distributed IT Systems. (arXiv:2101.04977v1 [cs.LG])</h2>
<h3>Jasmin Bogatinovski, Sasho Nedelkoski</h3>
<p>The multi-source data generated by distributed systems, provide a holistic
description of the system. Harnessing the joint distribution of the different
modalities by a learning model can be beneficial for critical applications for
maintenance of the distributed systems. One such important task is the task of
anomaly detection where we are interested in detecting the deviation of the
current behaviour of the system from the theoretically expected. In this work,
we utilize the joint representation from the distributed traces and system log
data for the task of anomaly detection in distributed systems. We demonstrate
that the joint utilization of traces and logs produced better results compared
to the single modality anomaly detection methods. Furthermore, we formalize a
learning task - next template prediction NTP, that is used as a generalization
for anomaly detection for both logs and distributed trace. Finally, we
demonstrate that this formalization allows for the learning of template
embedding for both the traces and logs. The joint embeddings can be reused in
other applications as good initialization for spans and logs.
</p>
<a href="http://arxiv.org/abs/2101.04977" target="_blank">arXiv:2101.04977</a> [<a href="http://arxiv.org/pdf/2101.04977" target="_blank">pdf</a>]

<h2>Machine learning approach for biopsy-based identification of eosinophilic esophagitis reveals importance of global features. (arXiv:2101.04989v1 [cs.CV])</h2>
<h3>Tomer Czyzewski, Nati Daniel, Mark Rochman, Julie M. Caldwell, Garrett A. Osswald, Margaret H. Collins, Marc E. Rothenberg, Yonatan Savir</h3>
<p>Goal: Eosinophilic esophagitis (EoE) is an allergic inflammatory condition
characterized by eosinophil accumulation in the esophageal mucosa. EoE
diagnosis includes a manual assessment of eosinophil levels in mucosal biopsies
- a time-consuming, laborious task that is difficult to standardize. One of the
main challenges in automating this process, like many other biopsy-based
diagnostics, is detecting features that are small relative to the size of the
biopsy. Results: In this work, we utilized hematoxylin- and eosin-stained
slides from esophageal biopsies from patients with active EoE and control
subjects to develop a platform based on a deep convolutional neural network
(DCNN) that can classify esophageal biopsies with an accuracy of 85%,
sensitivity of 82.5%, and specificity of 87%. Moreover, by combining several
downscaling and cropping strategies, we show that some of the features
contributing to the correct classification are global rather than specific,
local features. Conclusions: We report the ability of artificial intelligence
to identify EoE using computer vision analysis of esophageal biopsy slides.
Further, the DCNN features associated with EoE are based on not only local
eosinophils but also global histologic changes. Our approach can be used for
other conditions that rely on biopsy-based histologic diagnostics.
</p>
<a href="http://arxiv.org/abs/2101.04989" target="_blank">arXiv:2101.04989</a> [<a href="http://arxiv.org/pdf/2101.04989" target="_blank">pdf</a>]

<h2>Flatness Based Control of an Industrial Robot Joint Using Secondary Encoders. (arXiv:2101.04992v1 [cs.RO])</h2>
<h3>Jonas Weigand, Nigora Gafur, Martin Ruskowski</h3>
<p>Due to their compliant structure, industrial robots without
precision-enhancing measures are only to a limited extent suitable for
machining applications. Apart from structural, thermal and bearing
deformations, the main cause for compliant structure is backlash of
transmission drives. This paper proposes a method to improve trajectory
tracking accuracy by using secondary encoders and applying a feedback and a
flatness based feed forward control strategy. For this purpose, a novel
nonlinear, continuously differentiable dynamical model of a flexible robot
joint is presented. The robot joint is modeled as a two-mass oscillator with
pose-dependent inertia, nonlinear friction and nonlinear stiffness, including
backlash. A flatness based feed forward control is designed to improve the
guiding behaviour and a feedback controller, based on secondary encoders, is
implemented for disturbance compensation. Using Automatic Differentiation, the
nonlinear feed forward controller can be computed in a few microseconds online.
Finally, the proposed algorithms are evaluated in simulations and
experimentally on a real KUKA Quantec KR300 Ultra SE.
</p>
<a href="http://arxiv.org/abs/2101.04992" target="_blank">arXiv:2101.04992</a> [<a href="http://arxiv.org/pdf/2101.04992" target="_blank">pdf</a>]

<h2>Joint Learning of Hyperbolic Label Embeddings for Hierarchical Multi-label Classification. (arXiv:2101.04997v1 [cs.LG])</h2>
<h3>Soumya Chatterjee, Ayush Maheshwari, Ganesh Ramakrishnan, Saketha Nath Jagaralpudi</h3>
<p>We consider the problem of multi-label classification where the labels lie in
a hierarchy. However, unlike most existing works in hierarchical multi-label
classification, we do not assume that the label-hierarchy is known. Encouraged
by the recent success of hyperbolic embeddings in capturing hierarchical
relations, we propose to jointly learn the classifier parameters as well as the
label embeddings. Such a joint learning is expected to provide a twofold
advantage: i) the classifier generalizes better as it leverages the prior
knowledge of existence of a hierarchy over the labels, and ii) in addition to
the label co-occurrence information, the label-embedding may benefit from the
manifold structure of the input datapoints, leading to embeddings that are more
faithful to the label hierarchy. We propose a novel formulation for the joint
learning and empirically evaluate its efficacy. The results show that the joint
learning improves over the baseline that employs label co-occurrence based
pre-trained hyperbolic embeddings. Moreover, the proposed classifiers achieve
state-of-the-art generalization on standard benchmarks. We also present
evaluation of the hyperbolic embeddings obtained by joint learning and show
that they represent the hierarchy more accurately than the other alternatives.
</p>
<a href="http://arxiv.org/abs/2101.04997" target="_blank">arXiv:2101.04997</a> [<a href="http://arxiv.org/pdf/2101.04997" target="_blank">pdf</a>]

<h2>Sequential IoT Data Augmentation using Generative Adversarial Networks. (arXiv:2101.05003v1 [cs.LG])</h2>
<h3>Maximilian Ernst Tschuchnig, Cornelia Ferner, Stefan Wegenkittl</h3>
<p>Sequential data in industrial applications can be used to train and evaluate
machine learning models (e.g. classifiers). Since gathering representative
amounts of data is difficult and time consuming, there is an incentive to
generate it from a small ground truth. Data augmentation is a common method to
generate more data through a priori knowledge with one specific method, so
called generative adversarial networks (GANs), enabling data generation from
noise. This paper investigates the possibility of using GANs in order to
augment sequential Internet of Things (IoT) data, with an example
implementation that generates household energy consumption data with and
without swimming pools. The results of the example implementation seem
subjectively similar to the original data. Additionally to this subjective
evaluation, the paper also introduces a quantitative evaluation technique for
GANs if labels are provided. The positive results from the evaluation support
the initial assumption that generating sequential data from a small ground
truth is possible. This means that tedious data acquisition of sequential data
can be shortened. In the future, the results of this paper may be included as a
tool in machine learning, tackling the small data challenge.
</p>
<a href="http://arxiv.org/abs/2101.05003" target="_blank">arXiv:2101.05003</a> [<a href="http://arxiv.org/pdf/2101.05003" target="_blank">pdf</a>]

<h2>Learning to Focus: Cascaded Feature Matching Network for Few-shot Image Recognition. (arXiv:2101.05018v1 [cs.CV])</h2>
<h3>Mengting Chen, Xinggang Wang, Heng Luo, Yifeng Geng, Wenyu Liu</h3>
<p>Deep networks can learn to accurately recognize objects of a category by
training on a large number of annotated images. However, a meta-learning
challenge known as a low-shot image recognition task comes when only a few
images with annotations are available for learning a recognition model for one
category. The objects in testing/query and training/support images are likely
to be different in size, location, style, and so on. Our method, called
Cascaded Feature Matching Network (CFMN), is proposed to solve this problem. We
train the meta-learner to learn a more fine-grained and adaptive deep distance
metric by focusing more on the features that have high correlations between
compared images by the feature matching block which can align associated
features together and naturally ignore those non-discriminative features. By
applying the proposed feature matching block in different layers of the
few-shot recognition network, multi-scale information among the compared images
can be incorporated into the final cascaded matching feature, which boosts the
recognition performance further and generalizes better by learning on
relationships. The experiments for few-shot learning on two standard datasets,
\emph{mini}ImageNet and Omniglot, have confirmed the effectiveness of our
method. Besides, the multi-label few-shot task is first studied on a new data
split of COCO which further shows the superiority of the proposed feature
matching network when performing few-shot learning in complex images. The code
will be made publicly available.
</p>
<a href="http://arxiv.org/abs/2101.05018" target="_blank">arXiv:2101.05018</a> [<a href="http://arxiv.org/pdf/2101.05018" target="_blank">pdf</a>]

<h2>Re-labeling ImageNet: from Single to Multi-Labels, from Global to Localized Labels. (arXiv:2101.05022v1 [cs.CV])</h2>
<h3>Sangdoo Yun, Seong Joon Oh, Byeongho Heo, Dongyoon Han, Junsuk Choe, Sanghyuk Chun</h3>
<p>ImageNet has been arguably the most popular image classification benchmark,
but it is also the one with a significant level of label noise. Recent studies
have shown that many samples contain multiple classes, despite being assumed to
be a single-label benchmark. They have thus proposed to turn ImageNet
evaluation into a multi-label task, with exhaustive multi-label annotations per
image. However, they have not fixed the training set, presumably because of a
formidable annotation cost. We argue that the mismatch between single-label
annotations and effectively multi-label images is equally, if not more,
problematic in the training setup, where random crops are applied. With the
single-label annotations, a random crop of an image may contain an entirely
different object from the ground truth, introducing noisy or even incorrect
supervision during training. We thus re-label the ImageNet training set with
multi-labels. We address the annotation cost barrier by letting a strong image
classifier, trained on an extra source of data, generate the multi-labels. We
utilize the pixel-wise multi-label predictions before the final pooling layer,
in order to exploit the additional location-specific supervision signals.
Training on the re-labeled samples results in improved model performances
across the board. ResNet-50 attains the top-1 classification accuracy of 78.9%
on ImageNet with our localized multi-labels, which can be further boosted to
80.2% with the CutMix regularization. We show that the models trained with
localized multi-labels also outperforms the baselines on transfer learning to
object detection and instance segmentation tasks, and various robustness
benchmarks. The re-labeled ImageNet training set, pre-trained weights, and the
source code are available at {https://github.com/naver-ai/relabel_imagenet}.
</p>
<a href="http://arxiv.org/abs/2101.05022" target="_blank">arXiv:2101.05022</a> [<a href="http://arxiv.org/pdf/2101.05022" target="_blank">pdf</a>]

<h2>I Can See it in Your Eyes: Gaze towards a Robot as an Implicit Cue of Uncanniness and Task Performance in Long-term Interactions. (arXiv:2101.05028v1 [cs.RO])</h2>
<h3>Giulia Perugia, Maike Paetzel-Pr&#xfc;smann, Madelene Alanenp&#xe4;&#xe4;, Ginevra Castellano</h3>
<p>Over the past years, extensive research has been dedicated to developing
robust platforms and data-driven dialogue models to support long-term
human-robot interactions. However, little is known about how people's
perception of robots and engagement with them develop over time and how these
can be accurately assessed through implicit and continuous measurement
techniques. In this paper, we investigate this by involving participants in
three interaction sessions with multiple days of zero exposure in between. Each
session consists of a joint task with a robot as well as two short social chats
with it before and after the task. We measure participants' gaze patterns with
a wearable eye-tracker and gauge their perception of the robot and engagement
with it and the joint task using questionnaires. Results disclose that aversion
of gaze in a social chat is an indicator of a robot's uncanniness and that the
more people gaze at the robot in a joint task, the worse they perform. In
contrast with most HRI literature, our results show that gaze towards an object
of shared attention, rather than gaze towards a robotic partner, is the most
meaningful predictor of engagement in a joint task. Furthermore, the analyses
of long-term gaze patterns disclose that people's mutual gaze in a social chat
develops congruently with their perceptions of the robot over time. These are
key findings for the HRI community as they entail that gaze behavior can be
used as an implicit measure of people's perception of robots in a social chat
and of their engagement and task performance in a joint task.
</p>
<a href="http://arxiv.org/abs/2101.05028" target="_blank">arXiv:2101.05028</a> [<a href="http://arxiv.org/pdf/2101.05028" target="_blank">pdf</a>]

<h2>Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors. (arXiv:2101.05036v1 [cs.CV])</h2>
<h3>Ali Harakeh, Steven L. Waslander</h3>
<p>Predictive uncertainty estimation is an essential next step for the reliable
deployment of deep object detectors in safety-critical tasks. In this work, we
focus on estimating predictive distributions for bounding box regression output
with variance networks. We show that in the context of object detection,
training variance networks with negative log likelihood (NLL) can lead to high
entropy predictive distributions regardless of the correctness of the output
mean. We propose to use the energy score as a non-local proper scoring rule and
find that when used for training, the energy score leads to better calibrated
and lower entropy predictive distributions than NLL. We also address the
widespread use of non-proper scoring metrics for evaluating predictive
distributions from deep object detectors by proposing an alternate evaluation
approach founded on proper scoring rules. Using the proposed evaluation tools,
we show that although variance networks can be used to produce high quality
predictive distributions, ad-hoc approaches used by seminal object detectors
for choosing regression targets during training do not provide wide enough data
support for reliable variance learning. We hope that our work helps shift
evaluation in probabilistic object detection to better align with predictive
uncertainty evaluation in other machine learning domains. Code for all models,
evaluation, and datasets is available at:
https://github.com/asharakeh/probdet.git.
</p>
<a href="http://arxiv.org/abs/2101.05036" target="_blank">arXiv:2101.05036</a> [<a href="http://arxiv.org/pdf/2101.05036" target="_blank">pdf</a>]

<h2>Video action recognition for lane-change classification and prediction of surrounding vehicles. (arXiv:2101.05043v1 [cs.CV])</h2>
<h3>Mahdi Biparva, David Fern&#xe1;ndez-Llorca, Rub&#xe9;n Izquierdo-Gonzalo, John K. Tsotsos</h3>
<p>In highway scenarios, an alert human driver will typically anticipate early
cut-in/cut-out maneuvers of surrounding vehicles using visual cues mainly.
Autonomous vehicles must anticipate these situations at an early stage too, to
increase their safety and efficiency. In this work, lane-change recognition and
prediction tasks are posed as video action recognition problems. Up to four
different two-stream-based approaches, that have been successfully applied to
address human action recognition, are adapted here by stacking visual cues from
forward-looking video cameras to recognize and anticipate lane-changes of
target vehicles. We study the influence of context and observation horizons on
performance, and different prediction horizons are analyzed. The different
models are trained and evaluated using the PREVENTION dataset. The obtained
results clearly demonstrate the potential of these methodologies to serve as
robust predictors of future lane-changes of surrounding vehicles proving an
accuracy higher than 90% in time horizons of between 1-2 seconds.
</p>
<a href="http://arxiv.org/abs/2101.05043" target="_blank">arXiv:2101.05043</a> [<a href="http://arxiv.org/pdf/2101.05043" target="_blank">pdf</a>]

<h2>Top Program Construction and Reduction for polynomial time Meta-Interpretive Learning. (arXiv:2101.05050v1 [cs.AI])</h2>
<h3>Stassa Patsantzis, Stephen H. Muggleton</h3>
<p>Meta-Interpretive Learners, like most ILP systems, learn by searching for a
correct hypothesis in the hypothesis space, the powerset of all constructible
clauses. We show how this exponentially-growing search can be replaced by the
construction of a Top program: the set of clauses in all correct hypotheses
that is itself a correct hypothesis. We give an algorithm for Top program
construction and show that it constructs a correct Top program in polynomial
time and from a finite number of examples. We implement our algorithm in Prolog
as the basis of a new MIL system, Louise, that constructs a Top program and
then reduces it by removing redundant clauses. We compare Louise to the
state-of-the-art search-based MIL system Metagol in experiments on grid world
navigation, graph connectedness and grammar learning datasets and find that
Louise improves on Metagol's predictive accuracy when the hypothesis space and
the target theory are both large, or when the hypothesis space does not include
a correct hypothesis because of "classification noise" in the form of
mislabelled examples. When the hypothesis space or the target theory are small,
Louise and Metagol perform equally well.
</p>
<a href="http://arxiv.org/abs/2101.05050" target="_blank">arXiv:2101.05050</a> [<a href="http://arxiv.org/pdf/2101.05050" target="_blank">pdf</a>]

<h2>Understanding Action Sequences based on Video Captioning for Learning-from-Observation. (arXiv:2101.05061v1 [cs.CV])</h2>
<h3>Iori Yanokura, Naoki Wake, Kazuhiro Sasabuchi, Katsushi Ikeuchi, Masayuki Inaba</h3>
<p>Learning actions from human demonstration video is promising for intelligent
robotic systems. Extracting the exact section and re-observing the extracted
video section in detail is important for imitating complex skills because human
motions give valuable hints for robots. However, the general video
understanding methods focus more on the understanding of the full frame,lacking
consideration on extracting accurate sections and aligning them with the
human's intent. We propose a Learning-from-Observation framework that splits
and understands a video of a human demonstration with verbal instructions to
extract accurate action sequences. The splitting is done based on local minimum
points of the hand velocity, which align human daily-life actions with
object-centered face contact transitions required for generating robot motion.
Then, we extract a motion description on the split videos using video
captioning techniques that are trained from our new daily-life action video
dataset. Finally, we match the motion descriptions with the verbal instructions
to understand the correct human intent and ignore the unintended actions inside
the video. We evaluate the validity of hand velocity-based video splitting and
demonstrate that it is effective. The experimental results on our new video
captioning dataset focusing on daily-life human actions demonstrate the
effectiveness of the proposed method. The source code, trained models, and the
dataset will be made available.
</p>
<a href="http://arxiv.org/abs/2101.05061" target="_blank">arXiv:2101.05061</a> [<a href="http://arxiv.org/pdf/2101.05061" target="_blank">pdf</a>]

<h2>Laser Data Based Automatic Generation of Lane-Level Road Map for Intelligent Vehicles. (arXiv:2101.05066v1 [cs.CV])</h2>
<h3>Zehai Yu, Hui Zhu, Linglong Lin, Huawei Liang, Biao Yu, Weixin Huang</h3>
<p>With the development of intelligent vehicle systems, a high-precision road
map is increasingly needed in many aspects. The automatic lane lines extraction
and modeling are the most essential steps for the generation of a precise
lane-level road map. In this paper, an automatic lane-level road map generation
system is proposed. To extract the road markings on the ground, the
multi-region Otsu thresholding method is applied, which calculates the
intensity value of laser data that maximizes the variance between background
and road markings. The extracted road marking points are then projected to the
raster image and clustered using a two-stage clustering algorithm. Lane lines
are subsequently recognized from these clusters by the shape features of their
minimum bounding rectangle. To ensure the storage efficiency of the map, the
lane lines are approximated to cubic polynomial curves using a Bayesian
estimation approach. The proposed lane-level road map generation system has
been tested on urban and expressway conditions in Hefei, China. The
experimental results on the datasets show that our method can achieve excellent
extraction and clustering effect, and the fitted lines can reach a high
position accuracy with an error of less than 10 cm
</p>
<a href="http://arxiv.org/abs/2101.05066" target="_blank">arXiv:2101.05066</a> [<a href="http://arxiv.org/pdf/2101.05066" target="_blank">pdf</a>]

<h2>Probabilistic Embeddings for Cross-Modal Retrieval. (arXiv:2101.05068v1 [cs.CV])</h2>
<h3>Sanghyuk Chun, Seong Joon Oh, Rafael Sampaio de Rezende, Yannis Kalantidis, Diane Larlus</h3>
<p>Cross-modal retrieval methods build a common representation space for samples
from multiple modalities, typically from the vision and the language domains.
For images and their captions, the multiplicity of the correspondences makes
the task particularly challenging. Given an image (respectively a caption),
there are multiple captions (respectively images) that equally make sense. In
this paper, we argue that deterministic functions are not sufficiently powerful
to capture such one-to-many correspondences. Instead, we propose to use
Probabilistic Cross-Modal Embedding (PCME), where samples from the different
modalities are represented as probabilistic distributions in the common
embedding space. Since common benchmarks such as COCO suffer from
non-exhaustive annotations for cross-modal matches, we propose to additionally
evaluate retrieval on the CUB dataset, a smaller yet clean database where all
possible image-caption pairs are annotated. We extensively ablate PCME and
demonstrate that it not only improves the retrieval performance over its
deterministic counterpart, but also provides uncertainty estimates that render
the embeddings more interpretable.
</p>
<a href="http://arxiv.org/abs/2101.05068" target="_blank">arXiv:2101.05068</a> [<a href="http://arxiv.org/pdf/2101.05068" target="_blank">pdf</a>]

<h2>Formatting the Landscape: Spatial conditional GAN for varying population in satellite imagery. (arXiv:2101.05069v1 [cs.CV])</h2>
<h3>Tomas Langer, Natalia Fedorova, Ron Hagensieker</h3>
<p>Climate change is expected to reshuffle the settlement landscape: forcing
people in affected areas to migrate, to change their lifeways, and continuing
to affect demographic change throughout the world. Changes to the geographic
distribution of population will have dramatic impacts on land use and land
cover and thus constitute one of the major challenges of planning for climate
change scenarios. In this paper, we explore a generative model framework for
generating satellite imagery conditional on gridded population distributions.
We make additions to the existing ALAE architecture, creating a spatially
conditional version: SCALAE. This method allows us to explicitly disentangle
population from the model's latent space and thus input custom population
forecasts into the generated imagery. We postulate that such imagery could then
be directly used for land cover and land use change estimation using existing
frameworks, as well as for realistic visualisation of expected local change. We
evaluate the model by comparing pixel and semantic reconstructions, as well as
calculate the standard FID metric. The results suggest the model captures
population distributions accurately and delivers a controllable method to
generate realistic satellite imagery.
</p>
<a href="http://arxiv.org/abs/2101.05069" target="_blank">arXiv:2101.05069</a> [<a href="http://arxiv.org/pdf/2101.05069" target="_blank">pdf</a>]

<h2>Restyling Images with the Bangladeshi Paintings Using Neural Style Transfer: A Comprehensive Experiment, Evaluation, and Human Perspective. (arXiv:2101.05077v1 [cs.CV])</h2>
<h3>Manal, Ali Hasan Md. Linkon, Md. Mahir Labib, Marium-E-Jannat, Md Saiful Islam</h3>
<p>In today's world, Neural Style Transfer (NST) has become a trendsetting term.
NST combines two pictures, a content picture and a reference image in style
(such as the work of a renowned painter) in a way that makes the output image
look like an image of the material, but rendered with the form of a reference
picture. However, there is no study using the artwork or painting of
Bangladeshi painters. Bangladeshi painting has a long history of more than two
thousand years and is still being practiced by Bangladeshi painters. This study
generates NST stylized image on Bangladeshi paintings and analyzes the human
point of view regarding the aesthetic preference of NST on Bangladeshi
paintings. To assure our study's acceptance, we performed qualitative human
evaluations on generated stylized images by 60 individual humans of different
age and gender groups. We have explained how NST works for Bangladeshi
paintings and assess NST algorithms, both qualitatively \&amp; quantitatively. Our
study acts as a pre-requisite for the impact of NST stylized image using
Bangladeshi paintings on mobile UI/GUI and material translation from the human
perspective. We hope that this study will encourage new collaborations to
create more NST related studies and expand the use of Bangladeshi artworks.
</p>
<a href="http://arxiv.org/abs/2101.05077" target="_blank">arXiv:2101.05077</a> [<a href="http://arxiv.org/pdf/2101.05077" target="_blank">pdf</a>]

<h2>Deep Learning Approach Combining Lightweight CNN Architecture with Transfer Learning: An Automatic Approach for the Detection and Recognition of Bangladeshi Banknotes. (arXiv:2101.05081v1 [cs.CV])</h2>
<h3>Ali Hasan Md. Linkon, Md. Mahir Labib, Faisal Haque Bappy, Soumik Sarker, Marium-E-Jannat, Md Saiful Islam</h3>
<p>Automatic detection and recognition of banknotes can be a very useful
technology for people with visual difficulties and also for the banks itself by
providing efficient management for handling different paper currencies.
Lightweight models can easily be integrated into any handy IoT based
gadgets/devices. This article presents our experiments on several
state-of-the-art deep learning methods based on Lightweight Convolutional
Neural Network architectures combining with transfer learning. ResNet152v2,
MobileNet, and NASNetMobile were used as the base models with two different
datasets containing Bangladeshi banknote images. The Bangla Currency dataset
has 8000 Bangladeshi banknote images where the Bangla Money dataset consists of
1970 images. The performances of the models were measured using both the
datasets and the combination of the two datasets. In order to achieve maximum
efficiency, we used various augmentations, hyperparameter tuning, and
optimizations techniques. We have achieved maximum test accuracy of 98.88\% on
8000 images dataset using MobileNet, 100\% on the 1970 images dataset using
NASNetMobile, and 97.77\% on the combined dataset (9970 images) using
MobileNet.
</p>
<a href="http://arxiv.org/abs/2101.05081" target="_blank">arXiv:2101.05081</a> [<a href="http://arxiv.org/pdf/2101.05081" target="_blank">pdf</a>]

<h2>This Face Does Not Exist ... But It Might Be Yours! Identity Leakage in Generative Models. (arXiv:2101.05084v1 [cs.CV])</h2>
<h3>Patrick Tinsley, Adam Czajka, Patrick Flynn</h3>
<p>Generative adversarial networks (GANs) are able to generate high resolution
photo-realistic images of objects that "do not exist." These synthetic images
are rather difficult to detect as fake. However, the manner in which these
generative models are trained hints at a potential for information leakage from
the supplied training data, especially in the context of synthetic faces. This
paper presents experiments suggesting that identity information in face images
can flow from the training corpus into synthetic samples without any
adversarial actions when building or using the existing model. This raises
privacy-related questions, but also stimulates discussions of (a) the face
manifold's characteristics in the feature space and (b) how to create
generative models that do not inadvertently reveal identity information of real
subjects whose images were used for training. We used five different face
matchers (face_recognition, FaceNet, ArcFace, SphereFace and Neurotechnology
MegaMatcher) and the StyleGAN2 synthesis model, and show that this identity
leakage does exist for some, but not all methods. So, can we say that these
synthetically generated faces truly do not exist? Databases of real and
synthetically generated faces are made available with this paper to allow full
replicability of the results discussed in this work.
</p>
<a href="http://arxiv.org/abs/2101.05084" target="_blank">arXiv:2101.05084</a> [<a href="http://arxiv.org/pdf/2101.05084" target="_blank">pdf</a>]

<h2>Relatively Lazy: Indoor-Outdoor Navigation Using Vision and GNSS. (arXiv:2101.05107v1 [cs.RO])</h2>
<h3>Benjamin Congram, Timothy D. Barfoot</h3>
<p>Visual Teach and Repeat (VT&amp;R) has shown relative navigation is a robust and
efficient solution for autonomous vision-based path following in difficult
environments. Adding additional absolute sensors such as Global Navigation
Satellite Systems (GNSS) has the potential to expand the domain of VT&amp;R to
environments where the ability to visually localize is not guaranteed. Our
method of lazy mapping and delaying estimation until a path-tracking error is
needed avoids the need to estimate absolute states. As a result, map
optimization is not required and paths can be driven immediately after being
taught. We validate our approach on a real robot through an experiment in a
joint indoor-outdoor environment comprising 3.5km of autonomous route repeating
across a variety of lighting conditions. We achieve smooth error signals
throughout the runs despite large sections of dropout for each sensor.
</p>
<a href="http://arxiv.org/abs/2101.05107" target="_blank">arXiv:2101.05107</a> [<a href="http://arxiv.org/pdf/2101.05107" target="_blank">pdf</a>]

<h2>Fast convolutional neural networks on FPGAs with hls4ml. (arXiv:2101.05108v1 [cs.LG])</h2>
<h3>Thea Aarrestad, Vladimir Loncar, Maurizio Pierini, Sioni Summers, Jennifer Ngadiuba, Christoffer Petersson, Hampus Linander, Yutaro Iiyama, Giuseppe Di Guglielmo, Javier Duarte, Philip Harris, Dylan Rankin, Sergo Jindariani, Kevin Pedro, Nhan Tran, Mia Liu, Edward Kreinar, Zhenbin Wu, Duc Hoang</h3>
<p>We introduce an automated tool for deploying ultra low-latency, low-power
deep neural networks with large convolutional layers on FPGAs. By extending the
hls4ml library, we demonstrate how to achieve inference latency of $5\,\mu$s
using convolutional architectures, while preserving state-of-the-art model
performance. Considering benchmark models trained on the Street View House
Numbers Dataset, we demonstrate various methods for model compression in order
to fit the computational constraints of a typical FPGA device. In particular,
we discuss pruning and quantization-aware training, and demonstrate how
resource utilization can be reduced by over 90% while maintaining the original
model accuracy.
</p>
<a href="http://arxiv.org/abs/2101.05108" target="_blank">arXiv:2101.05108</a> [<a href="http://arxiv.org/pdf/2101.05108" target="_blank">pdf</a>]

<h2>Multiscale regression on unknown manifolds. (arXiv:2101.05119v1 [stat.ML])</h2>
<h3>Wenjing Liao, Mauro Maggioni, Stefano Vigogna</h3>
<p>We consider the regression problem of estimating functions on $\mathbb{R}^D$
but supported on a $d$-dimensional manifold $ \mathcal{M} \subset \mathbb{R}^D
$ with $ d \ll D $. Drawing ideas from multi-resolution analysis and nonlinear
approximation, we construct low-dimensional coordinates on $\mathcal{M}$ at
multiple scales, and perform multiscale regression by local polynomial fitting.
We propose a data-driven wavelet thresholding scheme that automatically adapts
to the unknown regularity of the function, allowing for efficient estimation of
functions exhibiting nonuniform regularity at different locations and scales.
We analyze the generalization error of our method by proving finite sample
bounds in high probability on rich classes of priors. Our estimator attains
optimal learning rates (up to logarithmic factors) as if the function was
defined on a known Euclidean domain of dimension $d$, instead of an unknown
manifold embedded in $\mathbb{R}^D$. The implemented algorithm has quasilinear
complexity in the sample size, with constants linear in $D$ and exponential in
$d$. Our work therefore establishes a new framework for regression on
low-dimensional sets embedded in high dimensions, with fast implementation and
strong theoretical guarantees.
</p>
<a href="http://arxiv.org/abs/2101.05119" target="_blank">arXiv:2101.05119</a> [<a href="http://arxiv.org/pdf/2101.05119" target="_blank">pdf</a>]

<h2>Formalising Concepts as Grounded Abstractions. (arXiv:2101.05125v1 [cs.AI])</h2>
<h3>Stephen Clark, Alexander Lerchner, Tamara von Glehn, Olivier Tieleman, Richard Tanburn, Misha Dashevskiy, Matko Bosnjak</h3>
<p>The notion of concept has been studied for centuries, by philosophers,
linguists, cognitive scientists, and researchers in artificial intelligence
(Margolis &amp; Laurence, 1999). There is a large literature on formal,
mathematical models of concepts, including a whole sub-field of AI -- Formal
Concept Analysis -- devoted to this topic (Ganter &amp; Obiedkov, 2016). Recently,
researchers in machine learning have begun to investigate how methods from
representation learning can be used to induce concepts from raw perceptual data
(Higgins, Sonnerat, et al., 2018). The goal of this report is to provide a
formal account of concepts which is compatible with this latest work in deep
learning.

The main technical goal of this report is to show how techniques from
representation learning can be married with a lattice-theoretic formulation of
conceptual spaces. The mathematics of partial orders and lattices is a standard
tool for modelling conceptual spaces (Ch.2, Mitchell (1997), Ganter and
Obiedkov (2016)); however, there is no formal work that we are aware of which
defines a conceptual lattice on top of a representation that is induced using
unsupervised deep learning (Goodfellow et al., 2016). The advantages of
partially-ordered lattice structures are that these provide natural mechanisms
for use in concept discovery algorithms, through the meets and joins of the
lattice.
</p>
<a href="http://arxiv.org/abs/2101.05125" target="_blank">arXiv:2101.05125</a> [<a href="http://arxiv.org/pdf/2101.05125" target="_blank">pdf</a>]

<h2>CobBO: Coordinate Backoff Bayesian Optimization. (arXiv:2101.05147v1 [cs.LG])</h2>
<h3>Jian Tan, Niv Nayman, Mengchang Wang, Rong Jin</h3>
<p>Bayesian optimization is a popular method for optimizing expensive black-box
functions. The objective functions of hard real world problems are oftentimes
characterized by a fluctuated landscape of many local optima. Bayesian
optimization risks in over-exploiting such traps, remaining with insufficient
query budget for exploring the global landscape. We introduce Coordinate
Backoff Bayesian optimization (CobBO) to alleviate those challenges. CobBO
captures a smooth approximation of the global landscape by interpolating the
values of queried points projected to randomly selected promising coordinate
subspaces. Thus also a smaller query budget is required for the Gaussian
process regressions applied over the lower dimensional subspaces. This approach
can be viewed as a variant of coordinate ascent, tailored for Bayesian
optimization, using a stopping rule for backing off from a certain subspace and
switching to another coordinate subset. Additionally, adaptive trust regions
are dynamically formed to expedite the convergence, and stagnant local optima
are escaped by switching trust regions. Further smoothness and acceleration are
achieved by filtering out clustered queried points. Through comprehensive
evaluations over a wide spectrum of benchmarks, CobBO is shown to consistently
find comparable or better solutions, with a reduced trial complexity compared
to the state-of-the-art methods in both low and high dimensions.
</p>
<a href="http://arxiv.org/abs/2101.05147" target="_blank">arXiv:2101.05147</a> [<a href="http://arxiv.org/pdf/2101.05147" target="_blank">pdf</a>]

<h2>Temporal Knowledge Graph Forecasting with Neural ODE. (arXiv:2101.05151v1 [cs.LG])</h2>
<h3>Zifeng Ding, Zhen Han, Yunpu Ma, Volker Tresp</h3>
<p>Learning node representation on dynamically-evolving, multi-relational graph
data has gained great research interest. However, most of the existing models
for temporal knowledge graph forecasting use Recurrent Neural Network (RNN)
with discrete depth to capture temporal information, while time is a continuous
variable. Inspired by Neural Ordinary Differential Equation (NODE), we extend
the idea of continuum-depth models to time-evolving multi-relational graph
data, and propose a novel Temporal Knowledge Graph Forecasting model with NODE.
Our model captures temporal information through NODE and structural information
through a Graph Neural Network (GNN). Thus, our graph ODE model achieves a
continuous model in time and efficiently learns node representation for future
prediction. We evaluate our model on six temporal knowledge graph datasets by
performing link forecasting. Experiment results show the superiority of our
model.
</p>
<a href="http://arxiv.org/abs/2101.05151" target="_blank">arXiv:2101.05151</a> [<a href="http://arxiv.org/pdf/2101.05151" target="_blank">pdf</a>]

<h2>Comparative Analysis of Agent-Oriented Task Assignment and Path Planning Algorithms Applied to Drone Swarms. (arXiv:2101.05161v1 [cs.RO])</h2>
<h3>Rohith Gandhi Ganesan, Samantha Kappagoda, Giuseppe Loianno, David K. A. Mordecai</h3>
<p>Autonomous drone swarms are a burgeoning technology with significant
applications in the field of mapping, inspection, transportation and
monitoring. To complete a task, each drone has to accomplish a sub-goal within
the context of the overall task at hand and navigate through the environment by
avoiding collision with obstacles and with other agents in the environment. In
this work, we choose the task of optimal coverage of an environment with drone
swarms where the global knowledge of the goal states and its positions are
known but not of the obstacles. The drones have to choose the Points of
Interest (PoI) present in the environment to visit, along with the order to be
visited to ensure fast coverage. We model this task in a simulation and use an
agent-oriented approach to solve the problem. We evaluate different policy
networks trained with reinforcement learning algorithms based on their
effectiveness, i.e. time taken to map the area and efficiency, i.e.
computational requirements. We couple the task assignment with path planning in
an unique way for performing collision avoidance during navigation and compare
a grid-based global planning algorithm, i.e. Wavefront and a gradient-based
local planning algorithm, i.e. Potential Field. We also evaluate the Potential
Field planning algorithm with different cost functions, propose a method to
adaptively modify the velocity of the drone when using the Huber loss function
to perform collision avoidance and observe its effect on the trajectory of the
drones. We demonstrate our experiments in 2D and 3D simulations.
</p>
<a href="http://arxiv.org/abs/2101.05161" target="_blank">arXiv:2101.05161</a> [<a href="http://arxiv.org/pdf/2101.05161" target="_blank">pdf</a>]

<h2>Memory-Augmented Reinforcement Learning for Image-Goal Navigation. (arXiv:2101.05181v1 [cs.CV])</h2>
<h3>Lina Mezghani, Sainbayar Sukhbaatar, Thibaut Lavril, Oleksandr Maksymets, Dhruv Batra, Piotr Bojanowski, Karteek Alahari</h3>
<p>In this work, we address the problem of image-goal navigation in the context
of visually-realistic 3D environments. This task involves navigating to a
location indicated by a target image in a previously unseen environment.
Earlier attempts, including RL-based and SLAM-based approaches, have either
shown poor generalization performance, or are heavily-reliant on pose/depth
sensors. We present a novel method that leverages a cross-episode memory to
learn to navigate. We first train a state-embedding network in a
self-supervised fashion, and then use it to embed previously-visited states
into a memory. In order to avoid overfitting, we propose to use data
augmentation on the RGB input during training. We validate our approach through
extensive evaluations, showing that our data-augmented memory-based model
establishes a new state of the art on the image-goal navigation task in the
challenging Gibson dataset. We obtain this competitive performance from RGB
input only, without access to additional sensors such as position or depth.
</p>
<a href="http://arxiv.org/abs/2101.05181" target="_blank">arXiv:2101.05181</a> [<a href="http://arxiv.org/pdf/2101.05181" target="_blank">pdf</a>]

<h2>MC-LSTM: Mass-Conserving LSTM. (arXiv:2101.05186v1 [cs.LG])</h2>
<h3>Pieter-Jan Hoedt, Frederik Kratzert, Daniel Klotz, Christina Halmich, Markus Holzleitner, Grey Nearing, Sepp Hochreiter, G&#xfc;nter Klambauer</h3>
<p>The success of Convolutional Neural Networks (CNNs) in computer vision is
mainly driven by their strong inductive bias, which is strong enough to allow
CNNs to solve vision-related tasks with random weights, meaning without
learning. Similarly, Long Short-Term Memory (LSTM) has a strong inductive bias
towards storing information over time. However, many real-world systems are
governed by conservation laws, which lead to the redistribution of particular
quantities -- e.g. in physical and economical systems. Our novel
Mass-Conserving LSTM (MC-LSTM) adheres to these conservation laws by extending
the inductive bias of LSTM to model the redistribution of those stored
quantities. MC-LSTMs set a new state-of-the-art for neural arithmetic units at
learning arithmetic operations, such as addition tasks, which have a strong
conservation law, as the sum is constant over time. Further, MC-LSTM is applied
to traffic forecasting, modelling a pendulum, and a large benchmark dataset in
hydrology, where it sets a new state-of-the-art for predicting peak flows. In
the hydrology example, we show that MC-LSTM states correlate with real-world
processes and are therefore interpretable.
</p>
<a href="http://arxiv.org/abs/2101.05186" target="_blank">arXiv:2101.05186</a> [<a href="http://arxiv.org/pdf/2101.05186" target="_blank">pdf</a>]

<h2>OpenHPS: An Open Source Hybrid Positioning System. (arXiv:2101.05198v1 [cs.CV])</h2>
<h3>Maxim Van de Wynckel, Beat Signer</h3>
<p>Positioning systems and frameworks use various techniques to determine the
position of an object. Some of the existing solutions combine different sensory
data at the time of positioning in order to compute more accurate positions by
reducing the error introduced by the used individual positioning techniques. We
present OpenHPS, a generic hybrid positioning system implemented in TypeScript,
that can not only reduce the error during tracking by fusing different sensory
data based on different algorithms, but also also make use of combined tracking
techniques when calibrating or training the system. In addition to a detailed
discussion of the architecture, features and implementation of the extensible
open source OpenHPS framework, we illustrate the use of our solution in a
demonstrator application fusing different positioning techniques. While OpenHPS
offers a number of positioning techniques, future extensions might integrate
new positioning methods or algorithms and support additional levels of
abstraction including symbolic locations.
</p>
<a href="http://arxiv.org/abs/2101.05198" target="_blank">arXiv:2101.05198</a> [<a href="http://arxiv.org/pdf/2101.05198" target="_blank">pdf</a>]

<h2>MHT-X: Offline Multiple Hypothesis Tracking with Algorithm X. (arXiv:2101.05202v1 [cs.CV])</h2>
<h3>Peteris Zvejnieks, Mihails Birjukovs, Martins Klevs, Megumi Akashi, Sven Eckert, Andris Jakovics</h3>
<p>An efficient and versatile implementation of offline multiple hypothesis
tracking with Algorithm X for optimal association search was developed using
Python. The code is intended for scientific applications that do not require
online processing. Directed graph framework is used and multiple scans with
progressively increasing time window width are used for edge construction for
maximum likelihood trajectories. The current version of the code was developed
for applications in multiphase hydrodynamics, e.g. bubble and particle
tracking, and is capable of resolving object motion, merges and splits.
Feasible object associations and trajectory graph edge likelihoods are
determined using weak mass and momentum conservation laws translated to
statistical functions for object properties. The code is compatible with
n-dimensional motion with arbitrarily many tracked object properties. This
framework is easily extendable beyond the present application by replacing the
currently used heuristics with ones more appropriate for the problem at hand.
The code is open-source and will be continuously developed further.
</p>
<a href="http://arxiv.org/abs/2101.05202" target="_blank">arXiv:2101.05202</a> [<a href="http://arxiv.org/pdf/2101.05202" target="_blank">pdf</a>]

<h2>Neural Volume Rendering: NeRF And Beyond. (arXiv:2101.05204v1 [cs.CV])</h2>
<h3>Frank Dellaert, Lin Yen-Chen</h3>
<p>Besides the COVID-19 pandemic and political upheaval in the US, 2020 was also
the year in which neural volume rendering exploded onto the scene, triggered by
the impressive NeRF paper by Mildenhall et al. (2020). Both of us have tried to
capture this excitement, Frank on a blog post (Dellaert, 2020) and Yen-Chen in
a Github collection (Yen-Chen, 2020). This note is an annotated bibliography of
the relevant papers, and we posted the associated bibtex file on the
repository.
</p>
<a href="http://arxiv.org/abs/2101.05204" target="_blank">arXiv:2101.05204</a> [<a href="http://arxiv.org/pdf/2101.05204" target="_blank">pdf</a>]

<h2>Automated 3D cephalometric landmark identification using computerized tomography. (arXiv:2101.05205v1 [cs.CV])</h2>
<h3>Hye Sun Yun, Chang Min Hyun, Seong Hyeon Baek, Sang-Hwy Lee, Jin Keun Seo</h3>
<p>Identification of 3D cephalometric landmarks that serve as proxy to the shape
of human skull is the fundamental step in cephalometric analysis. Since manual
landmarking from 3D computed tomography (CT) images is a cumbersome task even
for the trained experts, automatic 3D landmark detection system is in a great
need. Recently, automatic landmarking of 2D cephalograms using deep learning
(DL) has achieved great success, but 3D landmarking for more than 80 landmarks
has not yet reached a satisfactory level, because of the factors hindering
machine learning such as the high dimensionality of the input data and limited
amount of training data due to ethical restrictions on the use of medical data.
This paper presents a semi-supervised DL method for 3D landmarking that takes
advantage of anonymized landmark dataset with paired CT data being removed. The
proposed method first detects a small number of easy-to-find reference
landmarks, then uses them to provide a rough estimation of the entire landmarks
by utilizing the low dimensional representation learned by variational
autoencoder (VAE). Anonymized landmark dataset is used for training the VAE.
Finally, coarse-to-fine detection is applied to the small bounding box provided
by rough estimation, using separate strategies suitable for mandible and
cranium. For mandibular landmarks, patch-based 3D CNN is applied to the
segmented image of the mandible (separated from the maxilla), in order to
capture 3D morphological features of mandible associated with the landmarks. We
detect 6 landmarks around the condyle all at once, instead of one by one,
because they are closely related to each other. For cranial landmarks, we again
use VAE-based latent representation for more accurate annotation. In our
experiment, the proposed method achieved an averaged 3D point-to-point error of
2.91 mm for 90 landmarks only with 15 paired training data.
</p>
<a href="http://arxiv.org/abs/2101.05205" target="_blank">arXiv:2101.05205</a> [<a href="http://arxiv.org/pdf/2101.05205" target="_blank">pdf</a>]

<h2>Efficient Object-Level Visual Context Modeling for Multimodal Machine Translation: Masking Irrelevant Objects Helps Grounding. (arXiv:2101.05208v1 [cs.CV])</h2>
<h3>Dexin Wang, Deyi Xiong</h3>
<p>Visual context provides grounding information for multimodal machine
translation (MMT). However, previous MMT models and probing studies on visual
features suggest that visual information is less explored in MMT as it is often
redundant to textual information. In this paper, we propose an object-level
visual context modeling framework (OVC) to efficiently capture and explore
visual information for multimodal machine translation. With detected objects,
the proposed OVC encourages MMT to ground translation on desirable visual
objects by masking irrelevant objects in the visual modality. We equip the
proposed with an additional object-masking loss to achieve this goal. The
object-masking loss is estimated according to the similarity between masked
objects and the source texts so as to encourage masking source-irrelevant
objects. Additionally, in order to generate vision-consistent target words, we
further propose a vision-weighted translation loss for OVC. Experiments on MMT
datasets demonstrate that the proposed OVC model outperforms state-of-the-art
MMT models and analyses show that masking irrelevant objects helps grounding in
MMT.
</p>
<a href="http://arxiv.org/abs/2101.05208" target="_blank">arXiv:2101.05208</a> [<a href="http://arxiv.org/pdf/2101.05208" target="_blank">pdf</a>]

<h2>Image Steganography based on Iteratively Adversarial Samples of A Synchronized-directions Sub-image. (arXiv:2101.05209v1 [cs.CV])</h2>
<h3>Xinghong Qin, Shunquan Tan, Bin Li, Weixuan Tang, Jiwu Huang</h3>
<p>Nowadays a steganography has to face challenges of both feature based
staganalysis and convolutional neural network (CNN) based steganalysis. In this
paper, we present a novel steganography scheme denoted as ITE-SYN (based on
ITEratively adversarial perturbations onto a SYNchronized-directions
sub-image), by which security data is embedded with synchronizing modification
directions to enhance security and then iteratively increased perturbations are
added onto a sub-image to reduce loss with cover class label of the target CNN
classifier. Firstly an exist steganographic function is employed to compute
initial costs. Then the cover image is decomposed into some non-overlapped
sub-images. After each sub-image is embedded, costs will be adjusted following
clustering modification directions profile. And then the next sub-image will be
embedded with adjusted costs until all secret data has been embedded. If the
target CNN classifier does not discriminate the stego image as a cover image,
based on adjusted costs, we change costs with adversarial manners according to
signs of gradients back-propagated from the CNN classifier. And then a
sub-image is chosen to be re-embedded with changed costs. Adversarial intensity
will be iteratively increased until the adversarial stego image can fool the
target CNN classifier. Experiments demonstrate that the proposed method
effectively enhances security to counter both conventional feature-based
classifiers and CNN classifiers, even other non-target CNN classifiers.
</p>
<a href="http://arxiv.org/abs/2101.05209" target="_blank">arXiv:2101.05209</a> [<a href="http://arxiv.org/pdf/2101.05209" target="_blank">pdf</a>]

<h2>Ellipse Regression with Predicted Uncertainties for Accurate Multi-View 3D Object Estimation. (arXiv:2101.05212v1 [cs.CV])</h2>
<h3>Wenbo Dong, Volkan Isler</h3>
<p>Convolutional neural network (CNN) based architectures, such as Mask R-CNN,
constitute the state of the art in object detection and segmentation. Recently,
these methods have been extended for model-based segmentation where the network
outputs the parameters of a geometric model (e.g. an ellipse) directly. This
work considers objects whose three-dimensional models can be represented as
ellipsoids. We present a variant of Mask R-CNN for estimating the parameters of
ellipsoidal objects by segmenting each object and accurately regressing the
parameters of projection ellipses. We show that model regression is sensitive
to the underlying occlusion scenario and that prediction quality for each
object needs to be characterized individually for accurate 3D object
estimation. We present a novel ellipse regression loss which can learn the
offset parameters with their uncertainties and quantify the overall geometric
quality of detection for each ellipse. These values, in turn, allow us to fuse
multi-view detections to obtain 3D ellipsoid parameters in a principled
fashion. The experiments on both synthetic and real datasets quantitatively
demonstrate the high accuracy of our proposed method in estimating 3D objects
under heavy occlusions compared to previous state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2101.05212" target="_blank">arXiv:2101.05212</a> [<a href="http://arxiv.org/pdf/2101.05212" target="_blank">pdf</a>]

<h2>Indonesian ID Card Extractor Using Optical Character Recognition and Natural Language Post-Processing. (arXiv:2101.05214v1 [cs.CV])</h2>
<h3>Firhan Maulana Rusli, Kevin Akbar Adhiguna, Hendy Irawan</h3>
<p>The development of Information Technology has been increasingly changing the
means of information exchange leading to the need of digitizing print
documents. In the present era, there is a lot of fraud that often occur. To
avoid account fraud there was verification using ID card extraction using OCR
and NLP. Optical Character Recognition (OCR) is technology that used to
generate text from image. With OCR we can extract Indonesian ID card or kartu
tanda penduduk (KTP) into text too. This is using to make easier service
operator to do data entry. To improve the accuracy we made text correction
using Natural language Processing (NLP) method to fixing the text. With 50
Indonesian ID card image we got 0.78 F-score, and we need 4510 milliseconds to
extract per ID card.
</p>
<a href="http://arxiv.org/abs/2101.05214" target="_blank">arXiv:2101.05214</a> [<a href="http://arxiv.org/pdf/2101.05214" target="_blank">pdf</a>]

<h2>AttentionLite: Towards Efficient Self-Attention Models for Vision. (arXiv:2101.05216v1 [cs.CV])</h2>
<h3>Souvik Kundu, Sairam Sundaresan</h3>
<p>We propose a novel framework for producing a class of parameter and compute
efficient models called AttentionLitesuitable for resource-constrained
applications. Prior work has primarily focused on optimizing models either via
knowledge distillation or pruning. In addition to fusing these two mechanisms,
our joint optimization framework also leverages recent advances in
self-attention as a substitute for convolutions. We can simultaneously distill
knowledge from a compute-heavy teacher while also pruning the student model in
a single pass of training thereby reducing training and fine-tuning times
considerably. We evaluate the merits of our proposed approach on the CIFAR-10,
CIFAR-100, and Tiny-ImageNet datasets. Not only do our AttentionLite models
significantly outperform their unoptimized counterparts in accuracy, we find
that in some cases, that they perform almost as well as their compute-heavy
teachers while consuming only a fraction of the parameters and FLOPs.
Concretely, AttentionLite models can achieve upto30x parameter efficiency and
2x computation efficiency with no significant accuracy drop compared to their
teacher.
</p>
<a href="http://arxiv.org/abs/2101.05216" target="_blank">arXiv:2101.05216</a> [<a href="http://arxiv.org/pdf/2101.05216" target="_blank">pdf</a>]

<h2>On the human-recognizability phenomenon of adversarially trained deep image classifiers. (arXiv:2101.05219v1 [cs.CV])</h2>
<h3>Jonathan Helland, Nathan VanHoudnos</h3>
<p>In this work, we investigate the phenomenon that robust image classifiers
have human-recognizable features -- often referred to as interpretability -- as
revealed through the input gradients of their score functions and their
subsequent adversarial perturbations. In particular, we demonstrate that
state-of-the-art methods for adversarial training incorporate two terms -- one
that orients the decision boundary via minimizing the expected loss, and
another that induces smoothness of the classifier's decision surface by
penalizing the local Lipschitz constant. Through this demonstration, we provide
a unified discussion of gradient and Jacobian-based regularizers that have been
used to encourage adversarial robustness in prior works. Following this
discussion, we give qualitative evidence that the coupling of smoothness and
orientation of the decision boundary is sufficient to induce the aforementioned
human-recognizability phenomenon.
</p>
<a href="http://arxiv.org/abs/2101.05219" target="_blank">arXiv:2101.05219</a> [<a href="http://arxiv.org/pdf/2101.05219" target="_blank">pdf</a>]

<h2>Robust CUR Decomposition: Theory and Imaging Applications. (arXiv:2101.05231v1 [cs.CV])</h2>
<h3>HanQin Cai, Keaton Hamm, Longxiu Huang, Deanna Needell</h3>
<p>This paper considers the use of Robust PCA in a CUR decomposition framework
and applications thereof. Our main algorithms produce a robust version of
column-row factorizations of matrices $\mathbf{D}=\mathbf{L}+\mathbf{S}$ where
$\mathbf{L}$ is low-rank and $\mathbf{S}$ contains sparse outliers. These
methods yield interpretable factorizations at low computational cost, and
provide new CUR decompositions that are robust to sparse outliers, in contrast
to previous methods. We consider two key imaging applications of Robust PCA:
video foreground-background separation and face modeling. This paper examines
the qualitative behavior of our Robust CUR decompositions on the benchmark
videos and face datasets, and find that our method works as well as standard
Robust PCA while being significantly faster. Additionally, we consider hybrid
randomized and deterministic sampling methods which produce a compact CUR
decomposition of a given matrix, and apply this to video sequences to produce
canonical frames thereof.
</p>
<a href="http://arxiv.org/abs/2101.05231" target="_blank">arXiv:2101.05231</a> [<a href="http://arxiv.org/pdf/2101.05231" target="_blank">pdf</a>]

<h2>On Misspecification in Prediction Problems and Robustness via Improper Learning. (arXiv:2101.05234v1 [stat.ML])</h2>
<h3>John Duchi, Annie Marsden, Gregory Valiant</h3>
<p>We study probabilistic prediction games when the underlying model is
misspecified, investigating the consequences of predicting using an incorrect
parametric model. We show that for a broad class of loss functions and
parametric families of distributions, the regret of playing a "proper"
predictor -- one from the putative model class -- relative to the best
predictor in the same model class has lower bound scaling at least as
$\sqrt{\gamma n}$, where $\gamma$ is a measure of the model misspecification to
the true distribution in terms of total variation distance. In contrast, using
an aggregation-based (improper) learner, one can obtain regret $d \log n$ for
any underlying generating distribution, where $d$ is the dimension of the
parameter; we exhibit instances in which this is unimprovable even over the
family of all learners that may play distributions in the convex hull of the
parametric family. These results suggest that simple strategies for aggregating
multiple learners together should be more robust, and several experiments
conform to this hypothesis.
</p>
<a href="http://arxiv.org/abs/2101.05234" target="_blank">arXiv:2101.05234</a> [<a href="http://arxiv.org/pdf/2101.05234" target="_blank">pdf</a>]

<h2>Denoising Score Matching with Random Fourier Features. (arXiv:2101.05239v1 [cs.LG])</h2>
<h3>Tsimboy Olga, Yermek Kapushev, Evgeny Burnaev, Ivan Oseledets</h3>
<p>The density estimation is one of the core problems in statistics. Despite
this, existing techniques like maximum likelihood estimation are
computationally inefficient due to the intractability of the normalizing
constant. For this reason an interest to score matching has increased being
independent on the normalizing constant. However, such estimator is consistent
only for distributions with the full space support. One of the approaches to
make it consistent is to add noise to the input data which is called Denoising
Score Matching. In this work we derive analytical expression for the Denoising
Score matching using the Kernel Exponential Family as a model distribution. The
usage of the kernel exponential family is motivated by the richness of this
class of densities. To tackle the computational complexity we use Random
Fourier Features based approximation of the kernel function. The analytical
expression allows to drop additional regularization terms based on the
higher-order derivatives as they are already implicitly included. Moreover, the
obtained expression explicitly depends on the noise variance, so the validation
loss can be straightforwardly used to tune the noise level. Along with
benchmark experiments, the model was tested on various synthetic distributions
to study the behaviour of the model in different cases. The empirical study
shows comparable quality to the competing approaches, while the proposed method
being computationally faster. The latter one enables scaling up to complex
high-dimensional data.
</p>
<a href="http://arxiv.org/abs/2101.05239" target="_blank">arXiv:2101.05239</a> [<a href="http://arxiv.org/pdf/2101.05239" target="_blank">pdf</a>]

<h2>Hand-Based Person Identification using Global and Part-Aware Deep Feature Representation Learning. (arXiv:2101.05260v1 [cs.CV])</h2>
<h3>Nathanael L. Baisa, Zheheng Jiang, Ritesh Vyas, Bryan Williams, Hossein Rahmani, Plamen Angelov, Sue Black</h3>
<p>In cases of serious crime, including sexual abuse, often the only available
information with demonstrated potential for identification is images of the
hands. Since this evidence is captured in uncontrolled situations, it is
difficult to analyse. As global approaches to feature comparison are limited in
this case, it is important to extend to consider local information. In this
work, we propose hand-based person identification by learning both global and
local deep feature representation. Our proposed method, Global and Part-Aware
Network (GPA-Net), creates global and local branches on the conv-layer for
learning robust discriminative global and part-level features. For learning the
local (part-level) features, we perform uniform partitioning on the conv-layer
in both horizontal and vertical directions. We retrieve the parts by conducting
a soft partition without explicitly partitioning the images or requiring
external cues such as pose estimation. We make extensive evaluations on two
large multi-ethnic and publicly available hand datasets, demonstrating that our
proposed method significantly outperforms competing approaches.
</p>
<a href="http://arxiv.org/abs/2101.05260" target="_blank">arXiv:2101.05260</a> [<a href="http://arxiv.org/pdf/2101.05260" target="_blank">pdf</a>]

<h2>Contrastive Behavioral Similarity Embeddings for Generalization in Reinforcement Learning. (arXiv:2101.05265v1 [cs.LG])</h2>
<h3>Rishabh Agarwal, Marlos C. Machado, Pablo Samuel Castro, Marc G. Bellemare</h3>
<p>Reinforcement learning methods trained on few environments rarely learn
policies that generalize to unseen environments. To improve generalization, we
incorporate the inherent sequential structure in reinforcement learning into
the representation learning process. This approach is orthogonal to recent
approaches, which rarely exploit this structure explicitly. Specifically, we
introduce a theoretically motivated policy similarity metric (PSM) for
measuring behavioral similarity between states. PSM assigns high similarity to
states for which the optimal policies in those states as well as in future
states are similar. We also present a contrastive representation learning
procedure to embed any state similarity metric, which we instantiate with PSM
to obtain policy similarity embeddings (PSEs). We demonstrate that PSEs improve
generalization on diverse benchmarks, including LQR with spurious correlations,
a jumping task from pixels, and Distracting DM Control Suite.
</p>
<a href="http://arxiv.org/abs/2101.05265" target="_blank">arXiv:2101.05265</a> [<a href="http://arxiv.org/pdf/2101.05265" target="_blank">pdf</a>]

<h2>Towards Understanding Acceleration Tradeoff between Momentum and Asynchrony in Nonconvex Stochastic Optimization. (arXiv:1806.01660v6 [cs.LG] UPDATED)</h2>
<h3>Tianyi Liu, Shiyang Li, Jianping Shi, Enlu Zhou, Tuo Zhao</h3>
<p>Asynchronous momentum stochastic gradient descent algorithms (Async-MSGD) is
one of the most popular algorithms in distributed machine learning. However,
its convergence properties for these complicated nonconvex problems is still
largely unknown, because of the current technical limit. Therefore, in this
paper, we propose to analyze the algorithm through a simpler but nontrivial
nonconvex problem - streaming PCA, which helps us to understand Aync-MSGD
better even for more general problems. Specifically, we establish the
asymptotic rate of convergence of Async-MSGD for streaming PCA by diffusion
approximation. Our results indicate a fundamental tradeoff between asynchrony
and momentum: To ensure convergence and acceleration through asynchrony, we
have to reduce the momentum (compared with Sync-MSGD). To the best of our
knowledge, this is the first theoretical attempt on understanding Async-MSGD
for distributed nonconvex stochastic optimization. Numerical experiments on
both streaming PCA and training deep neural networks are provided to support
our findings for Async-MSGD.
</p>
<a href="http://arxiv.org/abs/1806.01660" target="_blank">arXiv:1806.01660</a> [<a href="http://arxiv.org/pdf/1806.01660" target="_blank">pdf</a>]

<h2>Deep learning to achieve clinically applicable segmentation of head and neck anatomy for radiotherapy. (arXiv:1809.04430v3 [cs.CV] UPDATED)</h2>
<h3>Stanislav Nikolov, Sam Blackwell, Alexei Zverovitch, Ruheena Mendes, Michelle Livne, Jeffrey De Fauw, Yojan Patel, Clemens Meyer, Harry Askham, Bernardino Romera-Paredes, Christopher Kelly, Alan Karthikesalingam, Carlton Chu, Dawn Carnell, Cheng Boon, Derek D&#x27;Souza, Syed Ali Moinuddin, Bethany Garie, Yasmin McQuinlan, Sarah Ireland, Kiarna Hampton, Krystle Fuller, Hugh Montgomery, Geraint Rees, Mustafa Suleyman, Trevor Back, C&#xed;an Hughes, Joseph R. Ledsam, Olaf Ronneberger</h3>
<p>Over half a million individuals are diagnosed with head and neck cancer each
year worldwide. Radiotherapy is an important curative treatment for this
disease, but it requires manual time consuming delineation of radio-sensitive
organs at risk (OARs). This planning process can delay treatment, while also
introducing inter-operator variability with resulting downstream radiation dose
differences. While auto-segmentation algorithms offer a potentially time-saving
solution, the challenges in defining, quantifying and achieving expert
performance remain. Adopting a deep learning approach, we demonstrate a 3D
U-Net architecture that achieves expert-level performance in delineating 21
distinct head and neck OARs commonly segmented in clinical practice. The model
was trained on a dataset of 663 deidentified computed tomography (CT) scans
acquired in routine clinical practice and with both segmentations taken from
clinical practice and segmentations created by experienced radiographers as
part of this research, all in accordance with consensus OAR definitions. We
demonstrate the model's clinical applicability by assessing its performance on
a test set of 21 CT scans from clinical practice, each with the 21 OARs
segmented by two independent experts. We also introduce surface Dice similarity
coefficient (surface DSC), a new metric for the comparison of organ
delineation, to quantify deviation between OAR surface contours rather than
volumes, better reflecting the clinical task of correcting errors in the
automated organ segmentations. The model's generalisability is then
demonstrated on two distinct open source datasets, reflecting different centres
and countries to model training. With appropriate validation studies and
regulatory approvals, this system could improve the efficiency, consistency,
and safety of radiotherapy pathways.
</p>
<a href="http://arxiv.org/abs/1809.04430" target="_blank">arXiv:1809.04430</a> [<a href="http://arxiv.org/pdf/1809.04430" target="_blank">pdf</a>]

<h2>Extending Stan for Deep Probabilistic Programming. (arXiv:1810.00873v4 [cs.LG] UPDATED)</h2>
<h3>Guillaume Baudart, Javier Burroni, Martin Hirzel, Louis Mandel, Avraham Shinnar</h3>
<p>Stan is a probabilistic programming language that is popular in the
statistics community, with a high-level syntax for expressing probabilistic
models. Stan differs by nature from generative probabilistic programming
languages like Church, Anglican, or Pyro. This paper presents a comprehensive
compilation scheme to compile any Stan model to a generative language and
proves its correctness. We use our compilation scheme to build two new backends
for the Stanc3 compiler targeting Pyro and NumPyro. Experimental results show
that the NumPyro backend yields a 4.1x speedup compared to Stan in geometric
mean over 25 benchmarks. Building on Pyro we extend Stan with support for
explicit variational inference guides and deep probabilistic models. That way,
users familiar with Stan get access to new features without having to learn a
fundamentally new language.
</p>
<a href="http://arxiv.org/abs/1810.00873" target="_blank">arXiv:1810.00873</a> [<a href="http://arxiv.org/pdf/1810.00873" target="_blank">pdf</a>]

<h2>A First Look at Deep Learning Apps on Smartphones. (arXiv:1812.05448v4 [cs.LG] UPDATED)</h2>
<h3>Mengwei Xu, Jiawei Liu, Yuanqiang Liu, Felix Xiaozhu Lin, Yunxin Liu, Xuanzhe Liu</h3>
<p>We are in the dawn of deep learning explosion for smartphones. To bridge the
gap between research and practice, we present the first empirical study on
16,500 the most popular Android apps, demystifying how smartphone apps exploit
deep learning in the wild. To this end, we build a new static tool that
dissects apps and analyzes their deep learning functions. Our study answers
threefold questions: what are the early adopter apps of deep learning, what do
they use deep learning for, and how do their deep learning models look like.
Our study has strong implications for app developers, smartphone vendors, and
deep learning R\&amp;D. On one hand, our findings paint a promising picture of deep
learning for smartphones, showing the prosperity of mobile deep learning
frameworks as well as the prosperity of apps building their cores atop deep
learning. On the other hand, our findings urge optimizations on deep learning
models deployed on smartphones, the protection of these models, and validation
of research ideas on these models.
</p>
<a href="http://arxiv.org/abs/1812.05448" target="_blank">arXiv:1812.05448</a> [<a href="http://arxiv.org/pdf/1812.05448" target="_blank">pdf</a>]

<h2>General Probabilistic Surface Optimization and Log Density Estimation. (arXiv:1903.10567v8 [cs.LG] UPDATED)</h2>
<h3>Dmitry Kopitkov, Vadim Indelman</h3>
<p>In this paper we contribute a novel algorithm family, which generalizes many
unsupervised techniques including unnormalized and energy models, and allows us
to infer different statistical modalities (e.g. data likelihood and ratio
between densities) from data samples. The proposed unsupervised technique,
named Probabilistic Surface Optimization (PSO), views a model as a flexible
surface which can be pushed according to loss-specific virtual stochastic
forces, where a dynamical equilibrium is achieved when the pointwise forces on
the surface become equal. Concretely, the surface is pushed up and down at
points sampled from two different distributions. The averaged up and down
forces become functions of these two distribution densities and of force
magnitudes defined by the loss of a particular PSO instance. Upon convergence,
the force equilibrium imposes an optimized model to be equal to various
statistical functions depending on the used magnitude functions. Furthermore,
this dynamical-statistical equilibrium is extremely intuitive and useful,
providing many implications and possible usages in probabilistic inference. We
connect PSO to numerous existing statistical works which are also PSO
instances, and derive new PSO-based inference methods as demonstration of PSO
exceptional usability. Likewise, based on the insights coming from the
virtual-force perspective we analyze PSO stability and propose new ways to
improve it. Finally, we present new instances of PSO, termed PSO-LDE, for data
log-density estimation and also provide a new NN block-diagonal architecture
for increased surface flexibility, which significantly improves estimation
accuracy. Both PSO-LDE and the new architecture are combined together as a new
density estimation technique. In our experiments we demonstrate this technique
to be superior over state-of-the-art baselines in density estimation task for
multimodal 20D data.
</p>
<a href="http://arxiv.org/abs/1903.10567" target="_blank">arXiv:1903.10567</a> [<a href="http://arxiv.org/pdf/1903.10567" target="_blank">pdf</a>]

<h2>Interpreting a Recurrent Neural Network's Predictions of ICU Mortality Risk. (arXiv:1905.09865v4 [cs.LG] UPDATED)</h2>
<h3>Long V. Ho, Melissa D. Aczon, David Ledbetter, Randall Wetzel</h3>
<p>Deep learning has demonstrated success in many applications; however, their
use in healthcare has been limited due to the lack of transparency into how
they generate predictions. Algorithms such as Recurrent Neural Networks (RNNs)
when applied to Electronic Medical Records (EMR) introduce additional barriers
to transparency because of the sequential processing of the RNN and the
multi-modal nature of EMR data. This work seeks to improve transparency by: 1)
introducing Learned Binary Masks (LBM) as a method for identifying which EMR
variables contributed to an RNN model's risk of mortality (ROM) predictions for
critically ill children; and 2) applying KernelSHAP for the same purpose. Given
an individual patient, LBM and KernelSHAP both generate an attribution matrix
that shows the contribution of each input feature to the RNN's sequence of
predictions for that patient. Attribution matrices can be aggregated in many
ways to facilitate different levels of analysis of the RNN model and its
predictions. Presented are three methods of aggregations and analyses: 1) over
volatile time periods within individual patient predictions, 2) over
populations of ICU patients sharing specific diagnoses, and 3) across the
general population of critically ill children.
</p>
<a href="http://arxiv.org/abs/1905.09865" target="_blank">arXiv:1905.09865</a> [<a href="http://arxiv.org/pdf/1905.09865" target="_blank">pdf</a>]

<h2>Improving VAEs' Robustness to Adversarial Attack. (arXiv:1906.00230v5 [stat.ML] UPDATED)</h2>
<h3>Matthew Willetts, Alexander Camuto, Tom Rainforth, Stephen Roberts, Chris Holmes</h3>
<p>Variational autoencoders (VAEs) have recently been shown to be vulnerable to
adversarial attacks, wherein they are fooled into reconstructing a chosen
target image. However, how to defend against such attacks remains an open
problem. We make significant advances in addressing this issue by introducing
methods for producing adversarially robust VAEs. Namely, we first demonstrate
that methods proposed to obtain disentangled latent representations produce
VAEs that are more robust to these attacks. However, this robustness comes at
the cost of reducing the quality of the reconstructions. We ameliorate this by
applying disentangling methods to hierarchical VAEs. The resulting models
produce high-fidelity autoencoders that are also adversarially robust. We
confirm their capabilities on several different datasets and with current
state-of-the-art VAE adversarial attacks, and also show that they increase the
robustness of downstream tasks to attack.
</p>
<a href="http://arxiv.org/abs/1906.00230" target="_blank">arXiv:1906.00230</a> [<a href="http://arxiv.org/pdf/1906.00230" target="_blank">pdf</a>]

<h2>A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection. (arXiv:1907.09693v5 [cs.LG] UPDATED)</h2>
<h3>Qinbin Li, Zeyi Wen, Zhaomin Wu, Sixu Hu, Naibo Wang, Yuan Li, Xu Liu, Bingsheng He</h3>
<p>Federated learning has been a hot research topic in enabling the
collaborative training of machine learning models among different organizations
under the privacy restrictions. As researchers try to support more machine
learning models with different privacy-preserving approaches, there is a
requirement in developing systems and infrastructures to ease the development
of various federated learning algorithms. Similar to deep learning systems such
as PyTorch and TensorFlow that boost the development of deep learning,
federated learning systems (FLSs) are equivalently important, and face
challenges from various aspects such as effectiveness, efficiency, and privacy.
In this survey, we conduct a comprehensive review on federated learning
systems. To achieve smooth flow and guide future research, we introduce the
definition of federated learning systems and analyze the system components.
Moreover, we provide a thorough categorization for federated learning systems
according to six different aspects, including data distribution, machine
learning model, privacy mechanism, communication architecture, scale of
federation and motivation of federation. The categorization can help the design
of federated learning systems as shown in our case studies. By systematically
summarizing the existing federated learning systems, we present the design
factors, case studies, and future research opportunities.
</p>
<a href="http://arxiv.org/abs/1907.09693" target="_blank">arXiv:1907.09693</a> [<a href="http://arxiv.org/pdf/1907.09693" target="_blank">pdf</a>]

<h2>Curriculum Self-Paced Learning for Cross-Domain Object Detection. (arXiv:1911.06849v3 [cs.CV] UPDATED)</h2>
<h3>Petru Soviany, Radu Tudor Ionescu, Paolo Rota, Nicu Sebe</h3>
<p>Training (source) domain bias affects state-of-the-art object detectors, such
as Faster R-CNN, when applied to new (target) domains. To alleviate this
problem, researchers proposed various domain adaptation methods to improve
object detection results in the cross-domain setting, e.g. by translating
images with ground-truth labels from the source domain to the target domain
using Cycle-GAN. On top of combining Cycle-GAN transformations and self-paced
learning in a smart and efficient way, in this paper, we propose a novel
self-paced algorithm that learns from easy to hard. Our method is simple and
effective, without any overhead during inference. It uses only pseudo-labels
for samples taken from the target domain, i.e. the domain adaptation is
unsupervised. We conduct experiments on four cross-domain benchmarks, showing
better results than the state of the art. We also perform an ablation study
demonstrating the utility of each component in our framework. Additionally, we
study the applicability of our framework to other object detectors.
Furthermore, we compare our difficulty measure with other measures from the
related literature, proving that it yields superior results and that it
correlates well with the performance metric.
</p>
<a href="http://arxiv.org/abs/1911.06849" target="_blank">arXiv:1911.06849</a> [<a href="http://arxiv.org/pdf/1911.06849" target="_blank">pdf</a>]

<h2>Improve CAM with Auto-adapted Segmentation and Co-supervised Augmentation. (arXiv:1911.07160v5 [cs.CV] UPDATED)</h2>
<h3>Ziyi Kou, Guofeng Cui, Shaojie Wang, Wentian Zhao, Chenliang Xu</h3>
<p>Weakly Supervised Object Localization (WSOL) methods generate both
classification and localization results by learning from only image category
labels. Previous methods usually utilize class activation map (CAM) to obtain
target object regions. However, most of them only focus on improving foreground
object parts in CAM, but ignore the important effect of its background
contents. In this paper, we propose a confidence segmentation (ConfSeg) module
that builds confidence score for each pixel in CAM without introducing
additional hyper-parameters. The generated sample-specific confidence mask is
able to indicate the extent of determination for each pixel in CAM, and further
supervises additional CAM extended from internal feature maps. Besides, we
introduce Co-supervised Augmentation (CoAug) module to capture feature-level
representation for foreground and background parts in CAM separately. Then a
metric loss is applied at batch sample level to augment distinguish ability of
our model, which helps a lot to localize more related object parts. Our final
model, CSoA, combines the two modules and achieves superior performance, e.g.
$37.69\%$ and $48.81\%$ Top-1 localization error on CUB-200 and ILSVRC
datasets, respectively, which outperforms all previous methods and becomes the
new state-of-the-art.
</p>
<a href="http://arxiv.org/abs/1911.07160" target="_blank">arXiv:1911.07160</a> [<a href="http://arxiv.org/pdf/1911.07160" target="_blank">pdf</a>]

<h2>Universal Adversarial Robustness of Texture and Shape-Biased Models. (arXiv:1911.10364v3 [cs.CV] UPDATED)</h2>
<h3>Kenneth T. Co, Luis Mu&#xf1;oz-Gonz&#xe1;lez, Leslie Kanthan, Ben Glocker, Emil C. Lupu</h3>
<p>Increasing shape-bias in deep neural networks has been shown to improve
robustness to common corruptions and noise. In this paper we analyze the
adversarial robustness of texture and shape-biased models to Universal
Adversarial Perturbations (UAPs). We use UAPs to evaluate the robustness of DNN
models with varying degrees of shape-based training. We find that shape-biased
models do not markedly improve adversarial robustness, and we show that
ensembles of texture and shape-biased models can improve universal adversarial
robustness while maintaining strong performance.
</p>
<a href="http://arxiv.org/abs/1911.10364" target="_blank">arXiv:1911.10364</a> [<a href="http://arxiv.org/pdf/1911.10364" target="_blank">pdf</a>]

<h2>Haptic communication optimises joint decisions and affords implicit confidence sharing. (arXiv:1912.12712v2 [cs.RO] UPDATED)</h2>
<h3>Giovanni Pezzulo, Lucas Roche, Ludovic Saint-Bauzel</h3>
<p>Group decisions can outperform the choices of the best individual group
members. Previous research suggested that optimal group decisions require
individuals to communicate explicitly (e.g., verbally) their confidence levels.
Our study addresses the untested hypothesis that implicit communication using a
sensorimotor channel -- haptic coupling -- may afford optimal group decisions,
too. We report that haptically coupled dyads solve a perceptual discrimination
task more accurately than their best individual members; and five times faster
than dyads using explicit communication. Furthermore, our computational
analyses indicate that the haptic channel affords implicit confidence sharing.
We found that dyads take leadership over the choice and communicate their
confidence in it by modulating both the timing and the force of their
movements. Our findings may pave the way to negotiation technologies using fast
sensorimotor communication to solve problems in groups.
</p>
<a href="http://arxiv.org/abs/1912.12712" target="_blank">arXiv:1912.12712</a> [<a href="http://arxiv.org/pdf/1912.12712" target="_blank">pdf</a>]

<h2>Teach Me What You Want to Play: Learning Variants of Connect Four through Human-Robot Interaction. (arXiv:2001.01004v4 [cs.RO] UPDATED)</h2>
<h3>Ali Ayub, Alan R. Wagner</h3>
<p>This paper investigates the use of game theoretic representations to
represent and learn how to play interactive games such as Connect Four. We
combine aspects of learning by demonstration, active learning, and game theory
allowing a robot to leverage its developing representation of the game to
conduct question/answer sessions with a person, thus filling in gaps in its
knowledge. The paper demonstrates a method for teaching a robot the win
conditions of the game Connect Four and its variants using a single
demonstration and a few trial examples with a question and answer session led
by the robot. Our results show that the robot can learn arbitrary win
conditions for the game with little prior knowledge of the win conditions and
then play the game with a human utilizing the learned win conditions. Our
experiments also show that some questions are more important for learning the
game's win conditions. We believe that this method could be broadly applied to
a variety of interactive learning scenarios.
</p>
<a href="http://arxiv.org/abs/2001.01004" target="_blank">arXiv:2001.01004</a> [<a href="http://arxiv.org/pdf/2001.01004" target="_blank">pdf</a>]

<h2>Trajectron++: Dynamically-Feasible Trajectory Forecasting With Heterogeneous Data. (arXiv:2001.03093v5 [cs.RO] UPDATED)</h2>
<h3>Tim Salzmann, Boris Ivanovic, Punarjay Chakravarty, Marco Pavone</h3>
<p>Reasoning about human motion is an important prerequisite to safe and
socially-aware robotic navigation. As a result, multi-agent behavior prediction
has become a core component of modern human-robot interactive systems, such as
self-driving cars. While there exist many methods for trajectory forecasting,
most do not enforce dynamic constraints and do not account for environmental
information (e.g., maps). Towards this end, we present Trajectron++, a modular,
graph-structured recurrent model that forecasts the trajectories of a general
number of diverse agents while incorporating agent dynamics and heterogeneous
data (e.g., semantic maps). Trajectron++ is designed to be tightly integrated
with robotic planning and control frameworks; for example, it can produce
predictions that are optionally conditioned on ego-agent motion plans. We
demonstrate its performance on several challenging real-world trajectory
forecasting datasets, outperforming a wide array of state-of-the-art
deterministic and generative methods.
</p>
<a href="http://arxiv.org/abs/2001.03093" target="_blank">arXiv:2001.03093</a> [<a href="http://arxiv.org/pdf/2001.03093" target="_blank">pdf</a>]

<h2>Stabilizing Differentiable Architecture Search via Perturbation-based Regularization. (arXiv:2002.05283v3 [cs.LG] UPDATED)</h2>
<h3>Xiangning Chen, Cho-Jui Hsieh</h3>
<p>Differentiable architecture search (DARTS) is a prevailing NAS solution to
identify architectures. Based on the continuous relaxation of the architecture
space, DARTS learns a differentiable architecture weight and largely reduces
the search cost. However, its stability has been challenged for yielding
deteriorating architectures as the search proceeds. We find that the
precipitous validation loss landscape, which leads to a dramatic performance
drop when distilling the final architecture, is an essential factor that causes
instability. Based on this observation, we propose a perturbation-based
regularization - SmoothDARTS (SDARTS), to smooth the loss landscape and improve
the generalizability of DARTS-based methods. In particular, our new
formulations stabilize DARTS-based methods by either random smoothing or
adversarial attack. The search trajectory on NAS-Bench-1Shot1 demonstrates the
effectiveness of our approach and due to the improved stability, we achieve
performance gain across various search spaces on 4 datasets. Furthermore, we
mathematically show that SDARTS implicitly regularizes the Hessian norm of the
validation loss, which accounts for a smoother loss landscape and improved
performance.
</p>
<a href="http://arxiv.org/abs/2002.05283" target="_blank">arXiv:2002.05283</a> [<a href="http://arxiv.org/pdf/2002.05283" target="_blank">pdf</a>]

<h2>Statistically Efficient, Polynomial Time Algorithms for Combinatorial Semi Bandits. (arXiv:2002.07258v2 [stat.ML] UPDATED)</h2>
<h3>Thibaut Cuvelier, Richard Combes, Eric Gourdin</h3>
<p>We consider combinatorial semi-bandits over a set of arms ${\cal X} \subset
\{0,1\}^d$ where rewards are uncorrelated across items. For this problem, the
algorithm ESCB yields the smallest known regret bound $R(T) = {\cal O}\Big( {d
(\ln m)^2 (\ln T) \over \Delta_{\min} }\Big)$, but it has computational
complexity ${\cal O}(|{\cal X}|)$ which is typically exponential in $d$, and
cannot be used in large dimensions. We propose the first algorithm which is
both computationally and statistically efficient for this problem with regret
$R(T) = {\cal O} \Big({d (\ln m)^2 (\ln T)\over \Delta_{\min} }\Big)$ and
computational complexity ${\cal O}(T {\bf poly}(d))$. Our approach involves
carefully designing an approximate version of ESCB with the same regret
guarantees, showing that this approximate algorithm can be implemented in time
${\cal O}(T {\bf poly}(d))$ by repeatedly maximizing a linear function over
${\cal X}$ subject to a linear budget constraint, and showing how to solve this
maximization problems efficiently.
</p>
<a href="http://arxiv.org/abs/2002.07258" target="_blank">arXiv:2002.07258</a> [<a href="http://arxiv.org/pdf/2002.07258" target="_blank">pdf</a>]

<h2>Pose Augmentation: Class-agnostic Object Pose Transformation for Object Recognition. (arXiv:2003.08526v3 [cs.CV] UPDATED)</h2>
<h3>Yunhao Ge, Jiaping Zhao, Laurent Itti</h3>
<p>Object pose increases intraclass object variance which makes object
recognition from 2D images harder. To render a classifier robust to pose
variations, most deep neural networks try to eliminate the influence of pose by
using large datasets with many poses for each class. Here, we propose a
different approach: a class-agnostic object pose transformation network
(OPT-Net) can transform an image along 3D yaw and pitch axes to synthesize
additional poses continuously. Synthesized images lead to better training of an
object classifier. We design a novel eliminate-add structure to explicitly
disentangle pose from object identity: first eliminate pose information of the
input image and then add target pose information (regularized as continuous
variables) to synthesize any target pose. We trained OPT-Net on images of toy
vehicles shot on a turntable from the iLab-20M dataset. After training on
unbalanced discrete poses (5 classes with 6 poses per object instance, plus 5
classes with only 2 poses), we show that OPT-Net can synthesize balanced
continuous new poses along yaw and pitch axes with high quality. Training a
ResNet-18 classifier with original plus synthesized poses improves mAP accuracy
by 9% overtraining on original poses only. Further, the pre-trained OPT-Net can
generalize to new object classes, which we demonstrate on both iLab-20M and
RGB-D. We also show that the learned features can generalize to ImageNet.
</p>
<a href="http://arxiv.org/abs/2003.08526" target="_blank">arXiv:2003.08526</a> [<a href="http://arxiv.org/pdf/2003.08526" target="_blank">pdf</a>]

<h2>Learn to Forget: Memorization Elimination for Neural Networks. (arXiv:2003.10933v2 [cs.LG] UPDATED)</h2>
<h3>Yang Liu, Zhuo Ma, Ximeng Liu, Jian Liu, Zhongyuan Jiang, Jianfeng Ma, Philip Yu, Kui Ren</h3>
<p>Nowadays, with the rapidly exploding amount of user data, more companies
choose to train various business-specified machine learning models, especially
neural networks, to improve service quality. Nevertheless, the current machine
learning application is still a one-way trip for the user data. As long as
users contribute their data, there is no way to retreat the contribution. Such
an irreversible setting has two potential risks: 1) from a legislative point of
view, many national regulations emphasize that users should have the right to
remove their personal data; 2) from a security point of view, the unintended
memorization of a neural network increases the possibility of an adversary to
extract user's sensitive information. To this end, memorization elimination for
machine learning models becomes a popular research topic.

Considering that there is no uniform indicator to evaluate a memorization
elimination method, we explore the concept of membership inference and define a
novel indicator, called forgetting rate. It well describes the transformation
rate of the eliminated data from "memorized" to "unknown" after conducting
memorization elimination. Furthermore, we propose Forsaken, a method that
allows users to eliminate the unintended memorization of their private data
from a trained neural network. The unintended memorization here is formed by
the out-of-distribution but sensitive data inadvertently uploaded by users.
Compared to prior work, our method avoids retraining, achieves higher
forgetting rate, and causes less accuracy loss through a trainable dummy
gradient generator.
</p>
<a href="http://arxiv.org/abs/2003.10933" target="_blank">arXiv:2003.10933</a> [<a href="http://arxiv.org/pdf/2003.10933" target="_blank">pdf</a>]

<h2>Optimal Behavior Planning for Autonomous Driving: A Generic Mixed-Integer Formulation. (arXiv:2003.13312v4 [cs.RO] UPDATED)</h2>
<h3>Klemens Esterle, Tobias Kessler, Alois Knoll</h3>
<p>Mixed-Integer Quadratic Programming (MIQP) has been identified as a suitable
approach for finding an optimal solution to the behavior planning problem with
low runtimes. Logical constraints and continuous equations are optimized
alongside. However, it has only been formulated for a straight road, omitting
common situations such as taking turns at intersections. This has prevented the
model from being used in reality so far. Based on a triple integrator model
formulation, we compute the orientation of the vehicle and model it in a
disjunctive manner. That allows us to formulate linear constraints to account
for the non-holonomy and collision avoidance. These constraints are
approximations, for which we introduce the theory. We show the applicability in
two benchmark scenarios and prove the feasibility by solving the same models
using nonlinear optimization. This new model will allow researchers to leverage
the benefits of MIQP, such as logical constraints, or global optimality.
</p>
<a href="http://arxiv.org/abs/2003.13312" target="_blank">arXiv:2003.13312</a> [<a href="http://arxiv.org/pdf/2003.13312" target="_blank">pdf</a>]

<h2>Data Augmentation using Generative Adversarial Networks (GANs) for GAN-based Detection of Pneumonia and COVID-19 in Chest X-ray Images. (arXiv:2006.03622v2 [cs.CV] UPDATED)</h2>
<h3>Saman Motamed, Patrik Rogalla, Farzad Khalvati</h3>
<p>Successful training of convolutional neural networks (CNNs) requires a
substantial amount of data. With small datasets networks generalize poorly.
Data Augmentation techniques improve the generalizability of neural networks by
using existing training data more effectively. Standard data augmentation
methods, however, produce limited plausible alternative data. Generative
Adversarial Networks (GANs) have been utilized to generate new data and improve
the performance of CNNs. Nevertheless, data augmentation techniques for
training GANs are under-explored compared to CNNs. In this work, we propose a
new GAN architecture for augmentation of chest X-rays for semi-supervised
detection of pneumonia and COVID-19 using generative models. We show that the
proposed GAN can be used to effectively augment data and improve classification
accuracy of disease in chest X-rays for pneumonia and COVID-19. We compare our
augmentation GAN model with Deep Convolutional GAN and traditional augmentation
methods (rotate, zoom, etc) on two different X-ray datasets and show our
GAN-based augmentation method surpasses other augmentation methods for training
a GAN in detecting anomalies in X-ray images.
</p>
<a href="http://arxiv.org/abs/2006.03622" target="_blank">arXiv:2006.03622</a> [<a href="http://arxiv.org/pdf/2006.03622" target="_blank">pdf</a>]

<h2>Adaptation Strategies for Automated Machine Learning on Evolving Data. (arXiv:2006.06480v2 [cs.LG] UPDATED)</h2>
<h3>Bilge Celik, Joaquin Vanschoren</h3>
<p>Automated Machine Learning (AutoML) systems have been shown to efficiently
build good models for new datasets. However, it is often not clear how well
they can adapt when the data evolves over time. The main goal of this study is
to understand the effect of data stream challenges such as concept drift on the
performance of AutoML methods, and which adaptation strategies can be employed
to make them more robust. To that end, we propose 6 concept drift adaptation
strategies and evaluate their effectiveness on different AutoML approaches. We
do this for a variety of AutoML approaches for building machine learning
pipelines, including those that leverage Bayesian optimization, genetic
programming, and random search with automated stacking. These are evaluated
empirically on real-world and synthetic data streams with different types of
concept drift. Based on this analysis, we propose ways to develop more
sophisticated and robust AutoML techniques.
</p>
<a href="http://arxiv.org/abs/2006.06480" target="_blank">arXiv:2006.06480</a> [<a href="http://arxiv.org/pdf/2006.06480" target="_blank">pdf</a>]

<h2>Anti-Transfer Learning for Task Invariance in Convolutional Neural Networks for Speech Processing. (arXiv:2006.06494v2 [cs.LG] UPDATED)</h2>
<h3>Eric Guizzo, Tillman Weyde, Giacomo Tarroni</h3>
<p>We introduce the novel concept of anti-transfer learning for speech
processing with convolutional neural networks. While transfer learning assumes
that the learning process for a target task will benefit from re-using
representations learned for another task, anti-transfer avoids the learning of
representations that have been learned for an orthogonal task, i.e., one that
is not relevant and potentially misleading for the target task, such as speaker
identity for speech recognition or speech content for emotion recognition. In
anti-transfer learning, we penalize similarity between activations of a network
being trained and another one previously trained on an orthogonal task, which
yields more suitable representations. This leads to better generalization and
provides a degree of control over correlations that are spurious or
undesirable, e.g. to avoid social bias. We have implemented anti-transfer for
convolutional neural networks in different configurations with several
similarity metrics and aggregation functions, which we evaluate and analyze
with several speech and audio tasks and settings, using six datasets. We show
that anti-transfer actually leads to the intended invariance to the orthogonal
task and to more appropriate features for the target task at hand.
Anti-transfer learning consistently improves classification accuracy in all
test cases. While anti-transfer creates computation and memory cost at training
time, there is relatively little computation cost when using pre-trained models
for orthogonal tasks. Anti-transfer is widely applicable and particularly
useful where a specific invariance is desirable or where trained models are
available and labeled data for orthogonal tasks are difficult to obtain.
</p>
<a href="http://arxiv.org/abs/2006.06494" target="_blank">arXiv:2006.06494</a> [<a href="http://arxiv.org/pdf/2006.06494" target="_blank">pdf</a>]

<h2>Heterogeneity-Aware Federated Learning. (arXiv:2006.06983v2 [cs.LG] UPDATED)</h2>
<h3>Chengxu Yang, QiPeng Wang, Mengwei Xu, Shangguang Wang, Kaigui Bian, Xuanzhe Liu</h3>
<p>Federated learning (FL) is an emerging distributed machine learning paradigm
that stands out with its inherent privacy-preserving advantages. Heterogeneity
is one of the core challenges in FL, which resides in the diverse user
behaviors and hardware capacity across devices who participate in the training.
Heterogeneity inherently exerts a huge influence on the FL training process,
e.g., causing device unavailability. However, existing FL literature usually
ignores the impacts of heterogeneity. To fill in the knowledge gap, we build
FLASH, the first heterogeneity-aware FL platform. Based on FLASH and a
large-scale user trace from 136k real-world users, we demonstrate the
usefulness of FLASH in anatomizing the impacts of heterogeneity in FL by
exploring three previously unaddressed research questions: whether and how can
heterogeneity affect FL performance; how to configure a heterogeneity-aware FL
system; and what are heterogeneity's impacts on existing FL optimizations. It
shows that heterogeneity causes nontrivial performance degradation in FL from
various aspects, and even invalidates some typical FL optimizations.
</p>
<a href="http://arxiv.org/abs/2006.06983" target="_blank">arXiv:2006.06983</a> [<a href="http://arxiv.org/pdf/2006.06983" target="_blank">pdf</a>]

<h2>AMENet: Attentive Maps Encoder Network for Trajectory Prediction. (arXiv:2006.08264v2 [cs.CV] UPDATED)</h2>
<h3>Hao Cheng, Wentong Liao, Michael Ying Yang, Bodo Rosenhahn, Monika Sester</h3>
<p>Trajectory prediction is critical for applications of planning safe future
movements and remains challenging even for the next few seconds in urban mixed
traffic. How an agent moves is affected by the various behaviors of its
neighboring agents in different environments. To predict movements, we propose
an end-to-end generative model named Attentive Maps Encoder Network (AMENet)
that encodes the agent's motion and interaction information for accurate and
realistic multi-path trajectory prediction. A conditional variational
auto-encoder module is trained to learn the latent space of possible future
paths based on attentive dynamic maps for interaction modeling and then is used
to predict multiple plausible future trajectories conditioned on the observed
past trajectories. The efficacy of AMENet is validated using two public
trajectory prediction benchmarks Trajnet and InD.
</p>
<a href="http://arxiv.org/abs/2006.08264" target="_blank">arXiv:2006.08264</a> [<a href="http://arxiv.org/pdf/2006.08264" target="_blank">pdf</a>]

<h2>Show me the Way: Intrinsic Motivation from Demonstrations. (arXiv:2006.12917v2 [cs.LG] UPDATED)</h2>
<h3>L&#xe9;onard Hussenot, Robert Dadashi, Matthieu Geist, Olivier Pietquin</h3>
<p>The study of exploration in the domain of decision making has a long history
but remains actively debated. From the vast literature that addressed this
topic for decades under various points of view (e.g., developmental psychology,
experimental design, artificial intelligence), intrinsic motivation emerged as
a concept that can practically be transferred to artificial agents. Especially,
in the recent field of Deep Reinforcement Learning (RL), agents implement such
a concept (mainly using a novelty argument) in the shape of an exploration
bonus, added to the task reward, that encourages visiting the whole
environment. This approach is supported by the large amount of theory on RL for
which convergence to optimality assumes exhaustive exploration. Yet, Human
Beings and mammals do not exhaustively explore the world and their motivation
is not only based on novelty but also on various other factors (e.g.,
curiosity, fun, style, pleasure, safety, competition, etc.). They optimize for
life-long learning and train to learn transferable skills in playgrounds
without obvious goals. They also apply innate or learned priors to save time
and stay safe. For these reasons, we propose to learn an exploration bonus from
demonstrations that could transfer these motivations to an artificial agent
with little assumptions about their rationale. Using an inverse RL approach, we
show that complex exploration behaviors, reflecting different motivations, can
be learnt and efficiently used by RL agents to solve tasks for which exhaustive
exploration is prohibitive.
</p>
<a href="http://arxiv.org/abs/2006.12917" target="_blank">arXiv:2006.12917</a> [<a href="http://arxiv.org/pdf/2006.12917" target="_blank">pdf</a>]

<h2>Does the Whole Exceed its Parts? The Effect of AI Explanations on Complementary Team Performance. (arXiv:2006.14779v3 [cs.AI] UPDATED)</h2>
<h3>Gagan Bansal, Tongshuang Wu, Joyce Zhou, Raymond Fok, Besmira Nushi, Ece Kamar, Marco Tulio Ribeiro, Daniel S. Weld</h3>
<p>Many researchers motivate explainable AI with studies showing that human-AI
team performance on decision-making tasks improves when the AI explains its
recommendations. However, prior studies observed improvements from explanations
only when the AI, alone, outperformed both the human and the best team. Can
explanations help lead to complementary performance, where team accuracy is
higher than either the human or the AI working solo? We conduct mixed-method
user studies on three datasets, where an AI with accuracy comparable to humans
helps participants solve a task (explaining itself in some conditions). While
we observed complementary improvements from AI augmentation, they were not
increased by explanations. Rather, explanations increased the chance that
humans will accept the AI's recommendation, regardless of its correctness. Our
result poses new challenges for human-centered AI: Can we develop explanatory
approaches that encourage appropriate trust in AI, and therefore help generate
(or improve) complementary performance?
</p>
<a href="http://arxiv.org/abs/2006.14779" target="_blank">arXiv:2006.14779</a> [<a href="http://arxiv.org/pdf/2006.14779" target="_blank">pdf</a>]

<h2>ReMOTS: Self-Supervised Refining Multi-Object Tracking and Segmentation. (arXiv:2007.03200v3 [cs.CV] UPDATED)</h2>
<h3>Fan Yang, Xin Chang, Chenyu Dang, Ziqiang Zheng, Sakriani Sakti, Satoshi Nakamura, Yang Wu</h3>
<p>We aim to improve the performance of Multiple Object Tracking and
Segmentation (MOTS) by refinement. However, it remains challenging for refining
MOTS results, which could be attributed to that appearance features are not
adapted to target videos and it is also difficult to find proper thresholds to
discriminate them. To tackle this issue, we propose a self-supervised refining
MOTS (i.e., ReMOTS) framework. ReMOTS mainly takes four steps to refine MOTS
results from the data association perspective. (1) Training the appearance
encoder using predicted masks. (2) Associating observations across adjacent
frames to form short-term tracklets. (3) Training the appearance encoder using
short-term tracklets as reliable pseudo labels. (4) Merging short-term
tracklets to long-term tracklets utilizing adopted appearance features and
thresholds that are automatically obtained from statistical information. Using
ReMOTS, we reached the $1^{st}$ place on CVPR 2020 MOTS Challenge 1, with an
sMOTSA score of $69.9$.
</p>
<a href="http://arxiv.org/abs/2007.03200" target="_blank">arXiv:2007.03200</a> [<a href="http://arxiv.org/pdf/2007.03200" target="_blank">pdf</a>]

<h2>AnchorFace: An Anchor-based Facial Landmark Detector Across Large Poses. (arXiv:2007.03221v2 [cs.CV] UPDATED)</h2>
<h3>Zixuan Xu, Banghuai Li, Miao Geng, Ye Yuan</h3>
<p>Facial landmark localization aims to detect the predefined points of human
faces, and the topic has been rapidly improved with the recent development of
neural network based methods. However, it remains a challenging task when
dealing with faces in unconstrained scenarios, especially with large pose
variations. In this paper, we target the problem of facial landmark
localization across large poses and address this task based on a
split-and-aggregate strategy. To split the search space, we propose a set of
anchor templates as references for regression, which well addresses the large
variations of face poses. Based on the prediction of each anchor template, we
propose to aggregate the results, which can reduce the landmark uncertainty due
to the large poses. Overall, our proposed approach, named AnchorFace, obtains
state-of-the-art results with extremely efficient inference speed on four
challenging benchmarks, i.e. AFLW, 300W, Menpo, and WFLW dataset. Code will be
released for reproduction.
</p>
<a href="http://arxiv.org/abs/2007.03221" target="_blank">arXiv:2007.03221</a> [<a href="http://arxiv.org/pdf/2007.03221" target="_blank">pdf</a>]

<h2>Boundary thickness and robustness in learning models. (arXiv:2007.05086v2 [cs.LG] UPDATED)</h2>
<h3>Yaoqing Yang, Rajiv Khanna, Yaodong Yu, Amir Gholami, Kurt Keutzer, Joseph E. Gonzalez, Kannan Ramchandran, Michael W. Mahoney</h3>
<p>Robustness of machine learning models to various adversarial and
non-adversarial corruptions continues to be of interest. In this paper, we
introduce the notion of the boundary thickness of a classifier, and we describe
its connection with and usefulness for model robustness. Thick decision
boundaries lead to improved performance, while thin decision boundaries lead to
overfitting (e.g., measured by the robust generalization gap between training
and testing) and lower robustness. We show that a thicker boundary helps
improve robustness against adversarial examples (e.g., improving the robust
test accuracy of adversarial training) as well as so-called out-of-distribution
(OOD) transforms, and we show that many commonly-used regularization and data
augmentation procedures can increase boundary thickness. On the theoretical
side, we establish that maximizing boundary thickness during training is akin
to the so-called mixup training. Using these observations, we show that
noise-augmentation on mixup training further increases boundary thickness,
thereby combating vulnerability to various forms of adversarial attacks and OOD
transforms. We can also show that the performance improvement in several lines
of recent work happens in conjunction with a thicker boundary.
</p>
<a href="http://arxiv.org/abs/2007.05086" target="_blank">arXiv:2007.05086</a> [<a href="http://arxiv.org/pdf/2007.05086" target="_blank">pdf</a>]

<h2>Transformations between deep neural networks. (arXiv:2007.05646v2 [cs.LG] UPDATED)</h2>
<h3>Tom Bertalan, Felix Dietrich, Ioannis G. Kevrekidis</h3>
<p>We propose to test, and when possible establish, an equivalence between two
different artificial neural networks by attempting to construct a data-driven
transformation between them, using manifold-learning techniques. In particular,
we employ diffusion maps with a Mahalanobis-like metric. If the construction
succeeds, the two networks can be thought of as belonging to the same
equivalence class.

We first discuss transformation functions between only the outputs of the two
networks; we then also consider transformations that take into account outputs
(activations) of a number of internal neurons from each network. In general,
Whitney's theorem dictates the number of measurements from one of the networks
required to reconstruct each and every feature of the second network. The
construction of the transformation function relies on a consistent, intrinsic
representation of the network input space.

We illustrate our algorithm by matching neural network pairs trained to learn
(a) observations of scalar functions; (b) observations of two-dimensional
vector fields; and (c) representations of images of a moving three-dimensional
object (a rotating horse). The construction of such equivalence classes across
different network instantiations clearly relates to transfer learning. We also
expect that it will be valuable in establishing equivalence between different
Machine Learning-based models of the same phenomenon observed through different
instruments and by different research groups.
</p>
<a href="http://arxiv.org/abs/2007.05646" target="_blank">arXiv:2007.05646</a> [<a href="http://arxiv.org/pdf/2007.05646" target="_blank">pdf</a>]

<h2>Adversarial Attacks against Neural Networks in Audio Domain: Exploiting Principal Components. (arXiv:2007.07001v3 [cs.LG] UPDATED)</h2>
<h3>Ken Alparslan, Yigit Alparslan, Matthew Burlick</h3>
<p>Adversarial attacks are inputs that are similar to original inputs but
altered on purpose. Speech-to-text neural networks that are widely used today
are prone to misclassify adversarial attacks. In this study, first, we
investigate the presence of targeted adversarial attacks by altering wave forms
from Common Voice data set. We craft adversarial wave forms via Connectionist
Temporal Classification Loss Function, and attack DeepSpeech, a speech-to-text
neural network implemented by Mozilla. We achieve 100% adversarial success rate
(zero successful classification by DeepSpeech) on all 25 adversarial wave forms
that we crafted. Second, we investigate the use of PCA as a defense mechanism
against adversarial attacks. We reduce dimensionality by applying PCA to these
25 attacks that we created and test them against DeepSpeech. We observe zero
successful classification by DeepSpeech, which suggests PCA is not a good
defense mechanism in audio domain. Finally, instead of using PCA as a defense
mechanism, we use PCA this time to craft adversarial inputs under a black-box
setting with minimal adversarial knowledge. With no knowledge regarding the
model, parameters, or weights, we craft adversarial attacks by applying PCA to
samples from Common Voice data set and achieve 100% adversarial success under
black-box setting again when tested against DeepSpeech. We also experiment with
different percentage of components necessary to result in a classification
during attacking process. In all cases, adversary becomes successful.
</p>
<a href="http://arxiv.org/abs/2007.07001" target="_blank">arXiv:2007.07001</a> [<a href="http://arxiv.org/pdf/2007.07001" target="_blank">pdf</a>]

<h2>Explicit Regularisation in Gaussian Noise Injections. (arXiv:2007.07368v4 [stat.ML] UPDATED)</h2>
<h3>Alexander Camuto, Matthew Willetts, Umut &#x15e;im&#x15f;ekli, Stephen Roberts, Chris Holmes</h3>
<p>We study the regularisation induced in neural networks by Gaussian noise
injections (GNIs). Though such injections have been extensively studied when
applied to data, there have been few studies on understanding the regularising
effect they induce when applied to network activations. Here we derive the
explicit regulariser of GNIs, obtained by marginalising out the injected noise,
and show that it penalises functions with high-frequency components in the
Fourier domain; particularly in layers closer to a neural network's output. We
show analytically and empirically that such regularisation produces calibrated
classifiers with large classification margins.
</p>
<a href="http://arxiv.org/abs/2007.07368" target="_blank">arXiv:2007.07368</a> [<a href="http://arxiv.org/pdf/2007.07368" target="_blank">pdf</a>]

<h2>Towards Evaluating Driver Fatigue with Robust Deep Learning Models. (arXiv:2007.08453v3 [cs.CV] UPDATED)</h2>
<h3>Ken Alparslan, Yigit Alparslan, Matthew Burlick</h3>
<p>In this paper, we explore different deep learning based approaches to detect
driver fatigue. Drowsy driving results in approximately 72,000 crashes and
44,000 injuries every year in the US and detecting drowsiness and alerting the
driver can save many lives. There have been many approaches to detect fatigue,
of which eye closedness detection is one. We propose a framework to detect eye
closedness in a captured camera frame as a gateway for detecting drowsiness. We
explore two different datasets to detect eye closedness. We develop an eye
model by using new Eye-blink dataset and a face model by using the Closed Eyes
in the Wild (CEW). We also explore different techniques to make the models more
robust by adding noise. We achieve 95.84% accuracy on our eye model that
detects eye blinking and 80.01% accuracy on our face model that detects eye
blinking. We also see that we can improve our accuracy on the face model by 6%
when we add noise to our training data and apply data augmentation. We hope
that our work will be useful to the field of driver fatigue detection to avoid
potential vehicle accidents related to drowsy driving.
</p>
<a href="http://arxiv.org/abs/2007.08453" target="_blank">arXiv:2007.08453</a> [<a href="http://arxiv.org/pdf/2007.08453" target="_blank">pdf</a>]

<h2>DeepCLR: Correspondence-Less Architecture for Deep End-to-End Point Cloud Registration. (arXiv:2007.11255v2 [cs.CV] UPDATED)</h2>
<h3>Markus Horn, Nico Engel, Vasileios Belagiannis, Michael Buchholz, Klaus Dietmayer</h3>
<p>This work addresses the problem of point cloud registration using deep neural
networks. We propose an approach to predict the alignment between two point
clouds with overlapping data content, but displaced origins. Such point clouds
originate, for example, from consecutive measurements of a LiDAR mounted on a
moving platform. The main difficulty in deep registration of raw point clouds
is the fusion of template and source point cloud. Our proposed architecture
applies flow embedding to tackle this problem, which generates features that
describe the motion of each template point. These features are then used to
predict the alignment in an end-to-end fashion without extracting explicit
point correspondences between both input clouds. We rely on the KITTI odometry
and ModelNet40 datasets for evaluating our method on various point
distributions. Our approach achieves state-of-the-art accuracy and the lowest
run-time of the compared methods.
</p>
<a href="http://arxiv.org/abs/2007.11255" target="_blank">arXiv:2007.11255</a> [<a href="http://arxiv.org/pdf/2007.11255" target="_blank">pdf</a>]

<h2>Debiasing Concept Bottleneck Models with a Causal Analysis Technique. (arXiv:2007.11500v3 [cs.LG] UPDATED)</h2>
<h3>Mohammad Taha Bahadori, David E. Heckerman</h3>
<p>Concept-based explanation approach is a popular model interpertability tool
because it expresses the reasons for a model's predictions in terms of concepts
that are meaningful for the domain experts. In this work, we study the problem
of the concepts being correlated with confounding information in the features.
We propose a new causal prior graph for modeling the impacts of unobserved
variables and a method to remove the impact of confounding information and
noise using the instrumental variable techniques. We also model the
completeness of the concepts set and show that our debiasing method works when
the concepts are not complete. Our synthetic and real-world experiments
demonstrate the success of our method in removing biases and improving the
ranking of the concepts in terms of their contribution to the explanation of
the predictions.
</p>
<a href="http://arxiv.org/abs/2007.11500" target="_blank">arXiv:2007.11500</a> [<a href="http://arxiv.org/pdf/2007.11500" target="_blank">pdf</a>]

<h2>Interpretable Sequence Learning for COVID-19 Forecasting. (arXiv:2008.00646v2 [cs.LG] UPDATED)</h2>
<h3>Sercan O. Arik, Chun-Liang Li, Jinsung Yoon, Rajarishi Sinha, Arkady Epshteyn, Long T. Le, Vikas Menon, Shashank Singh, Leyou Zhang, Nate Yoder, Martin Nikoltchev, Yash Sonthalia, Hootan Nakhost, Elli Kanal, Tomas Pfister</h3>
<p>We propose a novel approach that integrates machine learning into
compartmental disease modeling to predict the progression of COVID-19. Our
model is explainable by design as it explicitly shows how different
compartments evolve and it uses interpretable encoders to incorporate
covariates and improve performance. Explainability is valuable to ensure that
the model's forecasts are credible to epidemiologists and to instill confidence
in end-users such as policy makers and healthcare institutions. Our model can
be applied at different geographic resolutions, and here we demonstrate it for
states and counties in the United States. We show that our model provides more
accurate forecasts, in metrics averaged across the entire US, than
state-of-the-art alternatives, and that it provides qualitatively meaningful
explanatory insights. Lastly, we analyze the performance of our model for
different subgroups based on the subgroup distributions within the counties.
</p>
<a href="http://arxiv.org/abs/2008.00646" target="_blank">arXiv:2008.00646</a> [<a href="http://arxiv.org/pdf/2008.00646" target="_blank">pdf</a>]

<h2>Line-Circle-Square (LCS): A Multilayered Geometric Filter for Edge-Based Detection. (arXiv:2008.09315v3 [cs.RO] UPDATED)</h2>
<h3>Seyed Amir Tafrishi, Xiaotian Dai, Vahid Esmaeilzadeh Kandjani</h3>
<p>This paper presents a state-of-the-art filter that reduces the complexity in
object detection, tracking and mapping applications. Existing edge detection
and tracking methods are proposed to create suitable autonomy for mobile
robots, however, many of them face overconfidence and large computations at the
entrance to scenarios with an immense number of landmarks. The method in this
work, the Line-Circle-Square (LCS) filter, claims that mobile robots without a
large database for object recognition and highly advanced prediction methods
can deal with incoming objects that the camera captures in real-time. The
proposed filter applies detection, tracking and learning to each defined expert
to extract higher level information for judging scenes without
over-calculation. The interactive learning feed between each expert increases
the consistency of detected landmarks that works against overwhelming detected
features in crowded scenes. Our experts are dependent on trust factors'
covariance under the geometric definitions to ignore, emerge and compare
detected landmarks. The experiment validates the effectiveness of the proposed
filter in terms of detection precision and resource usage in both experimental
and real-world scenarios.
</p>
<a href="http://arxiv.org/abs/2008.09315" target="_blank">arXiv:2008.09315</a> [<a href="http://arxiv.org/pdf/2008.09315" target="_blank">pdf</a>]

<h2>Deep Ice Layer Tracking and Thickness Estimation using Fully Convolutional Networks. (arXiv:2009.00191v3 [cs.CV] UPDATED)</h2>
<h3>Debvrat Varshney, Maryam Rahnemoonfar, Masoud Yari, John Paden</h3>
<p>Global warming is rapidly reducing glaciers and ice sheets across the world.
Real time assessment of this reduction is required so as to monitor its global
climatic impact. In this paper, we introduce a novel way of estimating the
thickness of each internal ice layer using Snow Radar images and Fully
Convolutional Networks. The estimated thickness can be used to understand snow
accumulation each year. To understand the depth and structure of each internal
ice layer, we perform multi-class semantic segmentation on radar images, which
hasn't been performed before. As the radar images lack good training labels, we
carry out a pre-processing technique to get a clean set of labels. After
detecting each ice layer uniquely, we calculate its thickness and compare it
with the processed ground truth. This is the first time that each ice layer is
detected separately and its thickness calculated through automated techniques.
Through this procedure we were able to estimate the ice-layer thicknesses
within a Mean Absolute Error of approximately 3.6 pixels. Such a Deep Learning
based method can be used with ever-increasing datasets to make accurate
assessments for cryospheric studies.
</p>
<a href="http://arxiv.org/abs/2009.00191" target="_blank">arXiv:2009.00191</a> [<a href="http://arxiv.org/pdf/2009.00191" target="_blank">pdf</a>]

<h2>What am I allowed to do here?: Online Learning of Context-Specific Norms by Pepper. (arXiv:2009.05105v2 [cs.CV] UPDATED)</h2>
<h3>Ali Ayub, Alan R. Wagner</h3>
<p>Social norms support coordination and cooperation in society. With social
robots becoming increasingly involved in our society, they also need to follow
the social norms of the society. This paper presents a computational framework
for learning contexts and the social norms present in a context in an online
manner on a robot. The paper utilizes a recent state-of-the-art approach for
incremental learning and adapts it for online learning of scenes (contexts).
The paper further utilizes Dempster-Schafer theory to model context-specific
norms. After learning the scenes (contexts), we use active learning to learn
related norms. We test our approach on the Pepper robot by taking it through
different scene locations. Our results show that Pepper can learn different
scenes and related norms simply by communicating with a human partner in an
online manner.
</p>
<a href="http://arxiv.org/abs/2009.05105" target="_blank">arXiv:2009.05105</a> [<a href="http://arxiv.org/pdf/2009.05105" target="_blank">pdf</a>]

<h2>Asymptotically Optimal Sampling-Based Motion Planning Methods. (arXiv:2009.10484v2 [cs.RO] UPDATED)</h2>
<h3>Jonathan D. Gammell, Marlin P. Strub</h3>
<p>Motion planning is a fundamental problem in autonomous robotics that requires
finding a path to a specified goal that avoids obstacles and takes into account
a robot's limitations and constraints. It is often desirable for this path to
also optimize a cost function, such as path length.

Formal path-quality guarantees for continuously valued search spaces are an
active area of research interest. Recent results have proven that some
sampling-based planning methods probabilistically converge toward the optimal
solution as computational effort approaches infinity. This survey summarizes
the assumptions behind these popular asymptotically optimal techniques and
provides an introduction to the significant ongoing research on this topic.
</p>
<a href="http://arxiv.org/abs/2009.10484" target="_blank">arXiv:2009.10484</a> [<a href="http://arxiv.org/pdf/2009.10484" target="_blank">pdf</a>]

<h2>A Large Multi-Target Dataset of Common Bengali Handwritten Graphemes. (arXiv:2010.00170v3 [cs.CV] UPDATED)</h2>
<h3>Samiul Alam, Tahsin Reasat, Asif Shahriyar Sushmit, Sadi Mohammad Siddiquee, Fuad Rahman, Mahady Hasan, Ahmed Imtiaz Humayun</h3>
<p>Latin has historically led the state-of-the-art in handwritten optical
character recognition (OCR) research. Adapting existing systems from Latin to
alpha-syllabary languages is particularly challenging due to a sharp contrast
between their orthographies. The segmentation of graphical constituents
corresponding to characters becomes significantly hard due to a cursive writing
system and frequent use of diacritics in the alpha-syllabary family of
languages. We propose a labeling scheme based on graphemes (linguistic segments
of word formation) that makes segmentation in-side alpha-syllabary words linear
and present the first dataset of Bengali handwritten graphemes that are
commonly used in an everyday context. The dataset contains 411k curated samples
of 1295 unique commonly used Bengali graphemes. Additionally, the test set
contains 900 uncommon Bengali graphemes for out of dictionary performance
evaluation. The dataset is open-sourced as a part of a public Handwritten
Grapheme Classification Challenge on Kaggle to benchmark vision algorithms for
multi-target grapheme classification. The unique graphemes present in this
dataset are selected based on commonality in the Google Bengali ASR corpus.
From competition proceedings, we see that deep-learning methods can generalize
to a large span of out of dictionary graphemes which are absent during
training. Dataset and starter codes at www.kaggle.com/c/bengaliai-cv19.
</p>
<a href="http://arxiv.org/abs/2010.00170" target="_blank">arXiv:2010.00170</a> [<a href="http://arxiv.org/pdf/2010.00170" target="_blank">pdf</a>]

<h2>Goal-Auxiliary Actor-Critic for 6D Robotic Grasping with Point Clouds. (arXiv:2010.00824v2 [cs.RO] UPDATED)</h2>
<h3>Lirui Wang, Yu Xiang, Dieter Fox</h3>
<p>6D robotic grasping beyond top-down bin-picking scenarios is a challenging
task. Previous solutions based on 6D grasp synthesis with robot motion planning
usually operate in an open-loop setting without considering perception feedback
and dynamics and contacts of objects, which makes them sensitive to grasp
synthesis errors. In this work, we propose a novel method for learning
closed-loop control policies for 6D robotic grasping using point clouds from an
egocentric camera. We combine imitation learning and reinforcement learning in
order to grasp unseen objects and handle the continuous 6D action space, where
expert demonstrations are obtained from a joint motion and grasp planner. We
introduce a goal-auxiliary actor-critic algorithm, which uses grasping goal
prediction as an auxiliary task to facilitate policy learning. The supervision
on grasping goals can be obtained from the expert planner for known objects or
from hindsight goals for unknown objects. Overall, our learned closed-loop
policy achieves over $90\%$ success rates on grasping various ShapeNet objects
and YCB objects in simulation. The policy also transfers well to the real world
with only one failure among grasping of ten different unseen objects in the
presence of perception noises. Our video can be found at
https://sites.google.com/view/gaddpg .
</p>
<a href="http://arxiv.org/abs/2010.00824" target="_blank">arXiv:2010.00824</a> [<a href="http://arxiv.org/pdf/2010.00824" target="_blank">pdf</a>]

<h2>Optimization Landscapes of Wide Deep Neural Networks Are Benign. (arXiv:2010.00885v2 [cs.LG] UPDATED)</h2>
<h3>Johannes Lederer</h3>
<p>We analyze the optimization landscapes of deep learning with wide networks.
We highlight the importance of constraints for such networks and show that
constraint -- as well as unconstraint -- empirical-risk minimization over such
networks has no confined points, that is, suboptimal parameters that are
difficult to escape from. Hence, our theories substantiate the common belief
that wide neural networks are not only highly expressive but also comparably
easy to optimize.
</p>
<a href="http://arxiv.org/abs/2010.00885" target="_blank">arXiv:2010.00885</a> [<a href="http://arxiv.org/pdf/2010.00885" target="_blank">pdf</a>]

<h2>Autonomous UAV Exploration of Dynamic Environments via Incremental Sampling and Probabilistic Roadmap. (arXiv:2010.07429v2 [cs.RO] UPDATED)</h2>
<h3>Zhefan Xu, Di Deng, Kenji Shimada</h3>
<p>Autonomous exploration requires robots to generate informative trajectories
iteratively. Although sampling-based methods are highly efficient in unmanned
aerial vehicle exploration, many of these methods do not effectively utilize
the sampled information from the previous planning iterations, leading to
redundant computation and longer exploration time. Also, few have explicitly
shown their exploration ability in dynamic environments even though they can
run real-time. To overcome these limitations, we propose a novel dynamic
exploration planner (DEP) for exploring unknown environments using incremental
sampling and Probabilistic Roadmap (PRM). In our sampling strategy, nodes are
added incrementally and distributed evenly in the explored region, yielding the
best viewpoints. To further shortening exploration time and ensuring safety,
our planner optimizes paths locally and refine them based on the Euclidean
Signed Distance Function (ESDF) map. Meanwhile, as the multi-query planner, PRM
allows the proposed planner to quickly search alternative paths to avoid
dynamic obstacles for safe exploration. Simulation experiments show that our
method safely explores dynamic environments and outperforms the benchmark
planners in terms of exploration time, path length, and computational time.
</p>
<a href="http://arxiv.org/abs/2010.07429" target="_blank">arXiv:2010.07429</a> [<a href="http://arxiv.org/pdf/2010.07429" target="_blank">pdf</a>]

<h2>FAST-LIO: A Fast, Robust LiDAR-inertial Odometry Package by Tightly-Coupled Iterated Kalman Filter. (arXiv:2010.08196v2 [cs.RO] UPDATED)</h2>
<h3>Wei Xu, Fu Zhang</h3>
<p>This paper presents a computationally efficient and robust LiDAR-inertial
odometry framework. We fuse LiDAR feature points with IMU data using a
tightly-coupled iterated extended Kalman filter to allow robust navigation in
fast-motion, noisy or cluttered environments where degeneration occurs. To
lower the computation load in the presence of large number of measurements, we
present a new formula to compute the Kalman gain. The new formula has
computation load depending on the state dimension instead of the measurement
dimension. The proposed method and its implementation are tested in various
indoor and outdoor environments. In all tests, our method produces reliable
navigation results in real-time: running on a quadrotor onboard computer, it
fuses more than 1,200 effective feature points in a scan and completes all
iterations of an iEKF step within 25 ms. Our codes are open-sourced on Github.
</p>
<a href="http://arxiv.org/abs/2010.08196" target="_blank">arXiv:2010.08196</a> [<a href="http://arxiv.org/pdf/2010.08196" target="_blank">pdf</a>]

<h2>BALM: Bundle Adjustment for Lidar Mapping. (arXiv:2010.08215v2 [cs.RO] UPDATED)</h2>
<h3>Zheng Liu, Fu Zhang</h3>
<p>A local Bundle Adjustment (BA) on a sliding window of keyframes has been
widely used in visual SLAM and proved to be very effective in lowering the
drift. But in lidar SLAM, BA method is hardly used because the sparse feature
points (e.g., edge and plane) make the exact point matching impossible. In this
paper, we formulate the lidar BA as minimizing the distance from a feature
point to its matched edge or plane. Unlike the visual SLAM (and prior plane
adjustment method in lidar SLAM) where the feature has to be co-determined
along with the pose, we show that the feature can be analytically solved and
removed from the BA, the resultant BA is only dependent on the scan poses. This
greatly reduces the optimization scale and allows large-scale dense plane and
edge features to be used. To speedup the optimization, we derive the analytical
derivatives of the cost function, up to second order, in closed form. Moreover,
we propose a novel adaptive voxelization method to search feature
correspondence efficiently. The proposed formulations are incorporated into a
LOAM back-end for map refinement. Results show that, although as a back-end,
the local BA can be solved very efficiently, even in real-time at 10Hz when
optimizing 20 scans of point-cloud. The local BA also considerably lowers the
LOAM drift. Our implementation of the BA optimization and LOAM are open-sourced
to benefit the community.
</p>
<a href="http://arxiv.org/abs/2010.08215" target="_blank">arXiv:2010.08215</a> [<a href="http://arxiv.org/pdf/2010.08215" target="_blank">pdf</a>]

<h2>A Framework to Learn with Interpretation. (arXiv:2010.09345v2 [cs.LG] UPDATED)</h2>
<h3>Jayneel Parekh, Pavlo Mozharovskyi, Florence d&#x27;Alch&#xe9;-Buc</h3>
<p>With increasingly widespread use of deep neural networks in critical
decision-making applications, interpretability of these models is becoming
imperative. We consider the problem of jointly learning a predictive model and
its associated interpretation model. The task of the interpreter is to provide
both local and global interpretability about the predictive model in terms of
human-understandable high level attribute functions, without any loss of
accuracy. This is achieved by a dedicated architecture and well chosen
regularization penalties. We seek for a small-size dictionary of attribute
functions that take as inputs the outputs of selected hidden layers and whose
outputs feed a linear classifier. We impose a high level of conciseness by
constraining the activation of a very few attributes for a given input with a
real-entropy-based criterion while enforcing fidelity to both inputs and
outputs of the predictive model. A major advantage of simultaneous learning is
that the predictive neural network benefits from the interpretability
constraint as well. We also develop a more detailed pipeline based on some
common and novel simple tools to develop understanding about the learnt
features. We show on two datasets, MNIST and QuickDraw, their relevance for
both global and local interpretability.
</p>
<a href="http://arxiv.org/abs/2010.09345" target="_blank">arXiv:2010.09345</a> [<a href="http://arxiv.org/pdf/2010.09345" target="_blank">pdf</a>]

<h2>DiSCO: Differentiable Scan Context with Orientation. (arXiv:2010.10949v2 [cs.RO] UPDATED)</h2>
<h3>Xuecheng Xu, Huan Yin, Zexi Chen, Yue Wang, Rong Xiong</h3>
<p>Global localization is essential for robot navigation, of which the first
step is to retrieve a query from the map database. This problem is called place
recognition. In recent years, LiDAR scan based place recognition has drawn
attention as it is robust against the appearance change. In this paper, we
propose a LiDAR-based place recognition method, named Differentiable Scan
Context with Orientation (DiSCO), which simultaneously finds the scan at a
similar place and estimates their relative orientation. The orientation can
further be used as the initial value for the down-stream local optimal metric
pose estimation, improving the pose estimation especially when a large
orientation between the current scan and retrieved scan exists. Our key idea is
to transform the feature into the frequency domain. We utilize the magnitude of
the spectrum as the place signature, which is theoretically rotation-invariant.
In addition, based on the differentiable phase correlation, we can efficiently
estimate the global optimal relative orientation using the spectrum. With such
structural constraints, the network can be learned in an end-to-end manner, and
the backbone is fully shared by the two tasks, achieving interpretability and
light weight. Finally, DiSCO is validated on three datasets with long-term
outdoor conditions, showing better performance than the compared methods.
</p>
<a href="http://arxiv.org/abs/2010.10949" target="_blank">arXiv:2010.10949</a> [<a href="http://arxiv.org/pdf/2010.10949" target="_blank">pdf</a>]

<h2>Data Segmentation via t-SNE, DBSCAN, and Random Forest. (arXiv:2010.13682v2 [cs.LG] UPDATED)</h2>
<h3>Timothy DeLise</h3>
<p>This research proposes a data segmentation algorithm which combines t-SNE,
DBSCAN, and Random Forest classifier to form an end-to-end pipeline that
separates data into natural clusters and produces a characteristic profile of
each cluster based on the most important features. Out-of-sample cluster labels
can be inferred, and the technique generalizes well on real data sets. We
describe the algorithm and provide case studies using the Iris and MNIST data
sets, as well as real social media site data from Instagram. This is a proof of
concept and sets the stage for further in-depth theoretical analysis.
</p>
<a href="http://arxiv.org/abs/2010.13682" target="_blank">arXiv:2010.13682</a> [<a href="http://arxiv.org/pdf/2010.13682" target="_blank">pdf</a>]

<h2>Autonomous Extraction of Gleason Patterns for Grading Prostate Cancer using Multi-Gigapixel Whole Slide Images. (arXiv:2011.00527v2 [cs.CV] UPDATED)</h2>
<h3>Taimur Hassan, Ayman El-Baz, Naoufel Werghi</h3>
<p>Prostate cancer (PCa) is the second deadliest form of cancer in males. PCa
severity can be clinically graded by examining the structural representations
of Gleason tissues. This paper presents an asymmetric encoder-decoder that
integrates a novel hierarchical decomposition block to exploit the feature
representations pooled across various scales and then fuses them together to
extract the Gleason tissue patterns from the patched whole slide images.
Furthermore, the proposed network is penalized through the three-tiered loss
function, which ensures that it accurately recognizes the cluttered regions of
the cancerous tissues (according to their severity grade), despite having
similar contextual and textural characteristics, leading towards robust grading
of PCa progression. The proposed framework has been extensively evaluated on a
large-scale PCa dataset containing 10,516 whole slide scans (with around 71.7M
patches), where it outperforms state-of-the-art schemes in several metrics for
extracting the Gleason tissues and grading the progression of PCa.
</p>
<a href="http://arxiv.org/abs/2011.00527" target="_blank">arXiv:2011.00527</a> [<a href="http://arxiv.org/pdf/2011.00527" target="_blank">pdf</a>]

<h2>Steady State Analysis of Episodic Reinforcement Learning. (arXiv:2011.06631v2 [cs.LG] UPDATED)</h2>
<h3>Huang Bojun</h3>
<p>This paper proves that the episodic learning environment of every
finite-horizon decision task has a unique steady state under any behavior
policy, and that the marginal distribution of the agent's input indeed
converges to the steady-state distribution in essentially all episodic learning
processes. This observation supports an interestingly reversed mindset against
conventional wisdom: While the existence of unique steady states was often
presumed in continual learning but considered less relevant in episodic
learning, it turns out their existence is guaranteed for the latter. Based on
this insight, the paper unifies episodic and continual RL around several
important concepts that have been separately treated in these two RL
formalisms. Practically, the existence of unique and approachable steady state
enables a general way to collect data in episodic RL tasks, which the paper
applies to policy gradient algorithms as a demonstration, based on a new
steady-state policy gradient theorem. Finally, the paper also proposes and
experimentally validates a perturbation method that facilitates rapid
steady-state convergence in real-world RL tasks.
</p>
<a href="http://arxiv.org/abs/2011.06631" target="_blank">arXiv:2011.06631</a> [<a href="http://arxiv.org/pdf/2011.06631" target="_blank">pdf</a>]

<h2>Mixed-Integer Linear Programming Models for Multi-Robot Non-Adversarial Search. (arXiv:2011.12480v2 [cs.RO] UPDATED)</h2>
<h3>Beatriz A. Asfora, Jacopo Banfi, Mark Campbell</h3>
<p>In this letter, we consider the Multi-Robot Efficient Search Path Planning
(MESPP) problem, where a team of robots is deployed in a graph-represented
environment to capture a moving target within a given deadline. We prove this
problem to be NP-hard, and present the first set of Mixed-Integer Linear
Programming (MILP) models to tackle the MESPP problem. Our models are the first
to encompass multiple searchers, arbitrary capture ranges, and false negatives
simultaneously. While state-of-the-art algorithms for MESPP are based on simple
path enumeration, the adoption of MILP as a planning paradigm allows to
leverage the powerful techniques of modern solvers, yielding better
computational performance and, as a consequence, longer planning horizons. The
models are designed for computing optimal solutions offline, but can be easily
adapted for a distributed online approach. Our simulations show that it is
possible to achieve 98% decrease in computational time relative to the previous
state-of-the-art. We also show that the distributed approach performs nearly as
well as the centralized, within 6% in the settings studied in this letter, with
the advantage of requiring significant less time - an important consideration
in practical search missions.
</p>
<a href="http://arxiv.org/abs/2011.12480" target="_blank">arXiv:2011.12480</a> [<a href="http://arxiv.org/pdf/2011.12480" target="_blank">pdf</a>]

<h2>A Quality Diversity Approach to Automatically Generating Human-Robot Interaction Scenarios in Shared Autonomy. (arXiv:2012.04283v4 [cs.RO] UPDATED)</h2>
<h3>Matthew Fontaine, Stefanos Nikolaidis</h3>
<p>The growth of scale and complexity of interactions between humans and robots
highlights the need for new computational methods to automatically evaluate
novel algorithms and applications. Exploring the diverse scenarios of
interaction between humans and robots in simulation can improve understanding
of the system and avoid potentially costly failures in real-world settings. We
formulate this as a quality diversity (QD) problem, where the goal is to
discover diverse failure scenarios by simultaneously exploring both
environments and human actions. We focus on the shared autonomy domain, where
the robot attempts to infer the goal of a human operator, and adopt the QD
algorithm MAP-Elites to generate scenarios for two published algorithms in this
domain: shared autonomy via hindsight optimization and linear policy blending.
Some of the generated scenarios confirm previous theoretical findings, while
others are surprising and bring about a new understanding of state-of-the-art
implementations. Our experiments show that MAP-Elites outperforms Monte-Carlo
simulation and optimization based methods in effectively searching the scenario
space, highlighting its promise for automatic evaluation of algorithms in
shared autonomy.
</p>
<a href="http://arxiv.org/abs/2012.04283" target="_blank">arXiv:2012.04283</a> [<a href="http://arxiv.org/pdf/2012.04283" target="_blank">pdf</a>]

<h2>ShineOn: Illuminating Design Choices for Practical Video-based Virtual Clothing Try-on. (arXiv:2012.10495v2 [cs.CV] UPDATED)</h2>
<h3>Gaurav Kuppa, Andrew Jong, Vera Liu, Ziwei Liu, Teng-Sheng Moh</h3>
<p>Virtual try-on has garnered interest as a neural rendering benchmark task to
evaluate complex object transfer and scene composition. Recent works in virtual
clothing try-on feature a plethora of possible architectural and data
representation choices. However, they present little clarity on quantifying the
isolated visual effect of each choice, nor do they specify the hyperparameter
details that are key to experimental reproduction. Our work, ShineOn,
approaches the try-on task from a bottom-up approach and aims to shine light on
the visual and quantitative effects of each experiment. We build a series of
scientific experiments to isolate effective design choices in video synthesis
for virtual clothing try-on. Specifically, we investigate the effect of
different pose annotations, self-attention layer placement, and activation
functions on the quantitative and qualitative performance of video virtual
try-on. We find that DensePose annotations not only enhance face details but
also decrease memory usage and training time. Next, we find that attention
layers improve face and neck quality. Finally, we show that GELU and ReLU
activation functions are the most effective in our experiments despite the
appeal of newer activations such as Swish and Sine. We will release a
well-organized code base, hyperparameters, and model checkpoints to support the
reproducibility of our results. We expect our extensive experiments and code to
greatly inform future design choices in video virtual try-on. Our code may be
accessed at https://github.com/andrewjong/ShineOn-Virtual-Tryon.
</p>
<a href="http://arxiv.org/abs/2012.10495" target="_blank">arXiv:2012.10495</a> [<a href="http://arxiv.org/pdf/2012.10495" target="_blank">pdf</a>]

<h2>Characterizing Fairness Over the Set of Good Models Under Selective Labels. (arXiv:2101.00352v2 [cs.LG] UPDATED)</h2>
<h3>Amanda Coston, Ashesh Rambachan, Alexandra Chouldechova</h3>
<p>Algorithmic risk assessments are increasingly used to make and inform
decisions in a wide variety of high-stakes settings. In practice, there is
often a multitude of predictive models that deliver similar overall
performance, an empirical phenomenon commonly known as the "Rashomon Effect."
While many competing models may perform similarly overall, they may have
different properties over various subgroups, and therefore have drastically
different predictive fairness properties. In this paper, we develop a framework
for characterizing predictive fairness properties over the set of models that
deliver similar overall performance, or "the set of good models." We provide
tractable algorithms to compute the range of attainable group-level predictive
disparities and the disparity minimizing model over the set of good models. We
extend our framework to address the empirically relevant challenge of
selectively labelled data in the setting where the selection decision and
outcome are unconfounded given the observed data features. We illustrate our
methods in two empirical applications. In a real world credit-scoring task, we
build a model with lower predictive disparities than the benchmark model, and
demonstrate the benefits of properly accounting for the selective labels
problem. In a recidivism risk prediction task, we audit an existing risk score,
and find that it generates larger predictive disparities than any model in the
set of good models.
</p>
<a href="http://arxiv.org/abs/2101.00352" target="_blank">arXiv:2101.00352</a> [<a href="http://arxiv.org/pdf/2101.00352" target="_blank">pdf</a>]

<h2>A unifying approach on bias and variance analysis for classification. (arXiv:2101.01765v2 [cs.LG] UPDATED)</h2>
<h3>Cemre Zor, Terry Windeatt</h3>
<p>Standard bias and variance (B&amp;V) terminologies were originally defined for
the regression setting and their extensions to classification have led to
several different models / definitions in the literature. In this paper, we aim
to provide the link between the commonly used frameworks of Tumer &amp; Ghosh (T&amp;G)
and James. By unifying the two approaches, we relate the B&amp;V defined for the
0/1 loss to the standard B&amp;V of the boundary distributions given for the
squared error loss. The closed form relationships provide a deeper
understanding of classification performance, and their use is demonstrated in
two case studies.
</p>
<a href="http://arxiv.org/abs/2101.01765" target="_blank">arXiv:2101.01765</a> [<a href="http://arxiv.org/pdf/2101.01765" target="_blank">pdf</a>]

<h2>Opportunities of Federated Learning in Connected, Cooperative and Automated Industrial Systems. (arXiv:2101.03367v2 [cs.LG] UPDATED)</h2>
<h3>Stefano Savazzi, Monica Nicoli, Mehdi Bennis, Sanaz Kianoush, Luca Barbieri</h3>
<p>Next-generation autonomous and networked industrial systems (i.e., robots,
vehicles, drones) have driven advances in ultra-reliable, low latency
communications (URLLC) and computing. These networked multi-agent systems
require fast, communication-efficient and distributed machine learning (ML) to
provide mission critical control functionalities. Distributed ML techniques,
including federated learning (FL), represent a mushrooming multidisciplinary
research area weaving in sensing, communication and learning. FL enables
continual model training in distributed wireless systems: rather than fusing
raw data samples at a centralized server, FL leverages a cooperative fusion
approach where networked agents, connected via URLLC, act as distributed
learners that periodically exchange their locally trained model parameters.
This article explores emerging opportunities of FL for the next-generation
networked industrial systems. Open problems are discussed, focusing on
cooperative driving in connected automated vehicles and collaborative robotics
in smart manufacturing.
</p>
<a href="http://arxiv.org/abs/2101.03367" target="_blank">arXiv:2101.03367</a> [<a href="http://arxiv.org/pdf/2101.03367" target="_blank">pdf</a>]

<h2>Deep Adversarial Inconsistent Cognitive Sampling for Multi-view Progressive Subspace Clustering. (arXiv:2101.03783v2 [cs.CV] UPDATED)</h2>
<h3>Renhao Sun, Yang Wang, Zhao Zhang, Richang Hong, Meng Wang</h3>
<p>Deep multi-view clustering methods have achieved remarkable performance.
However, all of them failed to consider the difficulty labels (uncertainty of
ground-truth for training samples) over multi-view samples, which may result
into a nonideal clustering network for getting stuck into poor local optima
during training process; worse still, the difficulty labels from multi-view
samples are always inconsistent, such fact makes it even more challenging to
handle. In this paper, we propose a novel Deep Adversarial Inconsistent
Cognitive Sampling (DAICS) method for multi-view progressive subspace
clustering. A multiview binary classification (easy or difficult) loss and a
feature similarity loss are proposed to jointly learn a binary classifier and a
deep consistent feature embedding network, throughout an adversarial minimax
game over difficulty labels of multiview consistent samples. We develop a
multi-view cognitive sampling strategy to select the input samples from easy to
difficult for multi-view clustering network training. However, the
distributions of easy and difficult samples are mixed together, hence not
trivial to achieve the goal. To resolve it, we define a sampling probability
with theoretical guarantee. Based on that, a golden section mechanism is
further designed to generate a sample set boundary to progressively select the
samples with varied difficulty labels via a gate unit, which is utilized to
jointly learn a multi-view common progressive subspace and clustering network
for more efficient clustering. Experimental results on four real-world datasets
demonstrate the superiority of DAICS over the state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2101.03783" target="_blank">arXiv:2101.03783</a> [<a href="http://arxiv.org/pdf/2101.03783" target="_blank">pdf</a>]

<h2>The Vulnerability of Semantic Segmentation Networks to Adversarial Attacks in Autonomous Driving: Enhancing Extensive Environment Sensing. (arXiv:2101.03924v2 [cs.CV] UPDATED)</h2>
<h3>Andreas B&#xe4;r, Jonas L&#xf6;hdefink, Nikhil Kapoor, Serin J. Varghese, Fabian H&#xfc;ger, Peter Schlicht, Tim Fingscheidt</h3>
<p>Enabling autonomous driving (AD) can be considered one of the biggest
challenges in today's technology. AD is a complex task accomplished by several
functionalities, with environment perception being one of its core functions.
Environment perception is usually performed by combining the semantic
information captured by several sensors, i.e., lidar or camera. The semantic
information from the respective sensor can be extracted by using convolutional
neural networks (CNNs) for dense prediction. In the past, CNNs constantly
showed state-of-the-art performance on several vision-related tasks, such as
semantic segmentation of traffic scenes using nothing but the red-green-blue
(RGB) images provided by a camera. Although CNNs obtain state-of-the-art
performance on clean images, almost imperceptible changes to the input,
referred to as adversarial perturbations, may lead to fatal deception. The goal
of this article is to illuminate the vulnerability aspects of CNNs used for
semantic segmentation with respect to adversarial attacks, and share insights
into some of the existing known adversarial defense strategies. We aim to
clarify the advantages and disadvantages associated with applying CNNs for
environment perception in AD to serve as a motivation for future research in
this field.
</p>
<a href="http://arxiv.org/abs/2101.03924" target="_blank">arXiv:2101.03924</a> [<a href="http://arxiv.org/pdf/2101.03924" target="_blank">pdf</a>]

<h2>Evaluating Deep Learning Approaches for Covid19 Fake News Detection. (arXiv:2101.04012v2 [cs.LG] UPDATED)</h2>
<h3>Apurva Wani, Isha Joshi, Snehal Khandve, Vedangi Wagh, Raviraj Joshi</h3>
<p>Social media platforms like Facebook, Twitter, and Instagram have enabled
connection and communication on a large scale. It has revolutionized the rate
at which information is shared and enhanced its reach. However, another side of
the coin dictates an alarming story. These platforms have led to an increase in
the creation and spread of fake news. The fake news has not only influenced
people in the wrong direction but also claimed human lives. During these
critical times of the Covid19 pandemic, it is easy to mislead people and make
them believe in fatal information. Therefore it is important to curb fake news
at source and prevent it from spreading to a larger audience. We look at
automated techniques for fake news detection from a data mining perspective. We
evaluate different supervised text classification algorithms on Contraint@AAAI
2021 Covid-19 Fake news detection dataset. The classification algorithms are
based on Convolutional Neural Networks (CNN), Long Short Term Memory (LSTM),
and Bidirectional Encoder Representations from Transformers (BERT). We also
evaluate the importance of unsupervised learning in the form of language model
pre-training and distributed word representations using unlabelled covid tweets
corpus. We report the best accuracy of 98.41\% on the Covid-19 Fake news
detection dataset.
</p>
<a href="http://arxiv.org/abs/2101.04012" target="_blank">arXiv:2101.04012</a> [<a href="http://arxiv.org/pdf/2101.04012" target="_blank">pdf</a>]

<h2>Deeplite Neutrino: An End-to-End Framework for Constrained Deep Learning Model Optimization. (arXiv:2101.04073v2 [cs.LG] UPDATED)</h2>
<h3>Anush Sankaran, Olivier Mastropietro, Ehsan Saboori, Yasser Idris, Davis Sawyer, MohammadHossein AskariHemmat, Ghouthi Boukli Hacene</h3>
<p>Designing deep learning-based solutions is becoming a race for training
deeper models with a greater number of layers. While a large-size deeper model
could provide competitive accuracy, it creates a lot of logistical challenges
and unreasonable resource requirements during development and deployment. This
has been one of the key reasons for deep learning models not being excessively
used in various production environments, especially in edge devices. There is
an immediate requirement for optimizing and compressing these deep learning
models, to enable on-device intelligence. In this research, we introduce a
black-box framework, Deeplite Neutrino for production-ready optimization of
deep learning models. The framework provides an easy mechanism for the
end-users to provide constraints such as a tolerable drop in accuracy or target
size of the optimized models, to guide the whole optimization process. The
framework is easy to include in an existing production pipeline and is
available as a Python Package, supporting PyTorch and Tensorflow libraries. The
optimization performance of the framework is shown across multiple benchmark
datasets and popular deep learning models. Further, the framework is currently
used in production and the results and testimonials from several clients are
summarized.
</p>
<a href="http://arxiv.org/abs/2101.04073" target="_blank">arXiv:2101.04073</a> [<a href="http://arxiv.org/pdf/2101.04073" target="_blank">pdf</a>]

<h2>FaceX-Zoo: A PyTorch Toolbox for Face Recognition. (arXiv:2101.04407v2 [cs.CV] UPDATED)</h2>
<h3>Jun Wang, Yinglu Liu, Yibo Hu, Hailin Shi, Tao Mei</h3>
<p>Deep learning based face recognition has achieved significant progress in
recent years. Yet, the practical model production and further research of deep
face recognition are in great need of corresponding public support. For
example, the production of face representation network desires a modular
training scheme to consider the proper choice from various candidates of
state-of-the-art backbone and training supervision subject to the real-world
face recognition demand; for performance analysis and comparison, the standard
and automatic evaluation with a bunch of models on multiple benchmarks will be
a desired tool as well; besides, a public groundwork is welcomed for deploying
the face recognition in the shape of holistic pipeline. Furthermore, there are
some newly-emerged challenges, such as the masked face recognition caused by
the recent world-wide COVID-19 pandemic, which draws increasing attention in
practical applications. A feasible and elegant solution is to build an
easy-to-use unified framework to meet the above demands. To this end, we
introduce a novel open-source framework, named FaceX-Zoo, which is oriented to
the research-development community of face recognition. Resorting to the highly
modular and scalable design, FaceX-Zoo provides a training module with various
supervisory heads and backbones towards state-of-the-art face recognition, as
well as a standardized evaluation module which enables to evaluate the models
in most of the popular benchmarks just by editing a simple configuration. Also,
a simple yet fully functional face SDK is provided for the validation and
primary application of the trained models. Rather than including as many as
possible of the prior techniques, we enable FaceX-Zoo to easily upgrade and
extend along with the development of face related domains. The source code and
models are available at https://github.com/JDAI-CV/FaceX-Zoo.
</p>
<a href="http://arxiv.org/abs/2101.04407" target="_blank">arXiv:2101.04407</a> [<a href="http://arxiv.org/pdf/2101.04407" target="_blank">pdf</a>]

<h2>Developing an OpenAI Gym-compatible framework and simulation environment for testing Deep Reinforcement Learning agents solving the Ambulance Location Problem. (arXiv:2101.04434v2 [cs.LG] UPDATED)</h2>
<h3>Michael Allen, Kerry Pearn, Tom Monks</h3>
<p>Background and motivation: Deep Reinforcement Learning (Deep RL) is a rapidly
developing field. Historically most application has been made to games (such as
chess, Atari games, and go). Deep RL is now reaching the stage where it may
offer value in real world problems, including optimisation of healthcare
systems. One such problem is where to locate ambulances between calls in order
to minimise time from emergency call to ambulance on-scene. This is known as
the Ambulance Location problem.

Aim: To develop an OpenAI Gym-compatible framework and simulation environment
for testing Deep RL agents.

Methods: A custom ambulance dispatch simulation environment was developed
using OpenAI Gym and SimPy. Deep RL agents were built using PyTorch. The
environment is a simplification of the real world, but allows control over the
number of clusters of incident locations, number of possible dispatch
locations, number of hospitals, and creating incidents that occur at different
locations throughout each day.

Results: A range of Deep RL agents based on Deep Q networks were tested in
this custom environment. All reduced time to respond to emergency calls
compared with random allocation to dispatch points. Bagging Noisy Duelling Deep
Q networks gave the most consistence performance. All methods had a tendency to
lose performance if trained for too long, and so agents were saved at their
optimal performance (and tested on independent simulation runs).

Conclusions: Deep RL agents, developed using simulated environments, have the
potential to offer a novel approach to optimise the Ambulance Location problem.
Creating open simulation environments should allow more rapid progress in this
field.
</p>
<a href="http://arxiv.org/abs/2101.04434" target="_blank">arXiv:2101.04434</a> [<a href="http://arxiv.org/pdf/2101.04434" target="_blank">pdf</a>]

<h2>Robustness of on-device Models: Adversarial Attack to Deep Learning Models on Android Apps. (arXiv:2101.04401v1 [cs.LG] CROSS LISTED)</h2>
<h3>Yujin Huang, Han Hu, Chunyang Chen</h3>
<p>Deep learning has shown its power in many applications, including object
detection in images, natural-language understanding, and speech recognition. To
make it more accessible to end users, many deep learning models are now
embedded in mobile apps. Compared to offloading deep learning from smartphones
to the cloud, performing machine learning on-device can help improve latency,
connectivity, and power consumption. However, most deep learning models within
Android apps can easily be obtained via mature reverse engineering, while the
models' exposure may invite adversarial attacks. In this study, we propose a
simple but effective approach to hacking deep learning models using adversarial
attacks by identifying highly similar pre-trained models from TensorFlow Hub.
All 10 real-world Android apps in the experiment are successfully attacked by
our approach. Apart from the feasibility of the model attack, we also carry out
an empirical study that investigates the characteristics of deep learning
models used by hundreds of Android apps on Google Play. The results show that
many of them are similar to each other and widely use fine-tuning techniques to
pre-trained models on the Internet.
</p>
<a href="http://arxiv.org/abs/2101.04401" target="_blank">arXiv:2101.04401</a> [<a href="http://arxiv.org/pdf/2101.04401" target="_blank">pdf</a>]

