---
title: Latest Deep Learning Papers
date: 2020-12-18 21:00:01 +0800
featured-img: DL-logo
categories: [Deep_Learning]
tags: [Deep_Learning]
mathjax: Yes
---

<h1>Your arXiv Feed (208 Articles)</h1>
<h2>Relational Boosted Bandits. (arXiv:2012.09220v1 [cs.LG])</h2>
<h3>Ashutosh Kakadiya, Sriraam Natarajan, Balaraman Ravindran</h3>
<p>Contextual bandits algorithms have become essential in real-world user
interaction problems in recent years. However, these algorithms rely on context
as attribute value representation, which makes them unfeasible for real-world
domains like social networks are inherently relational. We propose Relational
Boosted Bandits(RB2), acontextual bandits algorithm for relational domains
based on (relational) boosted trees. RB2 enables us to learn interpretable and
explainable models due to the more descriptive nature of the relational
representation. We empirically demonstrate the effectiveness and
interpretability of RB2 on tasks such as link prediction, relational
classification, and recommendations.
</p>
<a href="http://arxiv.org/abs/2012.09220" target="_blank">arXiv:2012.09220</a> [<a href="http://arxiv.org/pdf/2012.09220" target="_blank">pdf</a>]

<h2>Optimal transport for vector Gaussian mixture models. (arXiv:2012.09226v1 [stat.ML])</h2>
<h3>Jiening Zhu, Kaiming Xu, Allen Tannenbaum</h3>
<p>Vector Gaussian mixture models form an important special subset of
vector-valued distributions. Any physical entity that can mutate or transit
among alternative manifestations distributed in a given space falls into this
category. A key example is color imagery. In this note, we vectorize the
Gaussian mixture model and study different optimal mass transport related
problems for such models. The benefits of using vector Gaussian mixture for
optimal mass transport include computational efficiency and the ability to
preserve structure.
</p>
<a href="http://arxiv.org/abs/2012.09226" target="_blank">arXiv:2012.09226</a> [<a href="http://arxiv.org/pdf/2012.09226" target="_blank">pdf</a>]

<h2>Shape My Face: Registering 3D Face Scans by Surface-to-Surface Translation. (arXiv:2012.09235v1 [cs.CV])</h2>
<h3>Mehdi Bahri, Eimear O&#x27; Sullivan, Shunwang Gong, Feng Liu, Xiaoming Liu, Michael M. Bronstein, Stefanos Zafeiriou</h3>
<p>Existing surface registration methods focus on fitting in-sample data with
little to no generalization ability and require both heavy pre-processing and
careful hand-tuning. In this paper, we cast the registration task as a
surface-to-surface translation problem, and design a model to reliably capture
the latent geometric information directly from raw 3D face scans. We introduce
Shape-My-Face (SMF), a powerful encoder-decoder architecture based on an
improved point cloud encoder, a novel visual attention mechanism, graph
convolutional decoders with skip connections, and a specialized mouth model
that we smoothly integrate with the mesh convolutions. Compared to the previous
state-of-the-art learning algorithms for non-rigid registration of face scans,
SMF only requires the raw data to be rigidly aligned (with scaling) with a
pre-defined face template. Additionally, our model provides topologically-sound
meshes with minimal supervision, offers faster training time, has orders of
magnitude fewer trainable parameters, is more robust to noise, and can
generalize to previously unseen datasets. We extensively evaluate the quality
of our registrations on diverse data. We demonstrate the robustness and
generalizability of our model with in-the-wild face scans across different
modalities, sensor types, and resolutions. Finally, we show that, by learning
to register scans, SMF produces a hybrid linear and non-linear morphable model
that can be used for generation, shape morphing, and expression transfer
through manipulation of the latent space, including in-the-wild. We train SMF
on a dataset of human faces comprising 9 large-scale databases on commodity
hardware.
</p>
<a href="http://arxiv.org/abs/2012.09235" target="_blank">arXiv:2012.09235</a> [<a href="http://arxiv.org/pdf/2012.09235" target="_blank">pdf</a>]

<h2>uBAM: Unsupervised Behavior Analysis and Magnification using Deep Learning. (arXiv:2012.09237v1 [cs.CV])</h2>
<h3>Biagio Brattoli, Uta Buechler, Michael Dorkenwald, Philipp Reiser, Linard Filli, Fritjof Helmchen, Anna-Sophia Wahl, Bjoern Ommer</h3>
<p>Motor behavior analysis is essential to biomedical research and clinical
diagnostics as it provides a non-invasive strategy for identifying motor
impairment and its change caused by interventions. State-of-the-art
instrumented movement analysis is time- and cost-intensive, since it requires
placing physical or virtual markers. Besides the effort required for marking
keypoints or annotations necessary for training or finetuning a detector, users
need to know the interesting behavior beforehand to provide meaningful
keypoints. We introduce uBAM, a novel, automatic deep learning algorithm for
behavior analysis by discovering and magnifying deviations. We propose an
unsupervised learning of posture and behavior representations that enable an
objective behavior comparison across subjects. A generative model with novel
disentanglement of appearance and behavior magnifies subtle behavior
differences across subjects directly in a video without requiring a detour via
keypoints or annotations. Evaluations on rodents and human patients with
neurological diseases demonstrate the wide applicability of our approach.
</p>
<a href="http://arxiv.org/abs/2012.09237" target="_blank">arXiv:2012.09237</a> [<a href="http://arxiv.org/pdf/2012.09237" target="_blank">pdf</a>]

<h2>S3CNet: A Sparse Semantic Scene Completion Network for LiDAR Point Clouds. (arXiv:2012.09242v1 [cs.CV])</h2>
<h3>Ran Cheng, Christopher Agia, Yuan Ren, Xinhai Li, Liu Bingbing</h3>
<p>With the increasing reliance of self-driving and similar robotic systems on
robust 3D vision, the processing of LiDAR scans with deep convolutional neural
networks has become a trend in academia and industry alike. Prior attempts on
the challenging Semantic Scene Completion task - which entails the inference of
dense 3D structure and associated semantic labels from "sparse" representations
- have been, to a degree, successful in small indoor scenes when provided with
dense point clouds or dense depth maps often fused with semantic segmentation
maps from RGB images. However, the performance of these systems drop
drastically when applied to large outdoor scenes characterized by dynamic and
exponentially sparser conditions. Likewise, processing of the entire sparse
volume becomes infeasible due to memory limitations and workarounds introduce
computational inefficiency as practitioners are forced to divide the overall
volume into multiple equal segments and infer on each individually, rendering
real-time performance impossible. In this work, we formulate a method that
subsumes the sparsity of large-scale environments and present S3CNet, a sparse
convolution based neural network that predicts the semantically completed scene
from a single, unified LiDAR point cloud. We show that our proposed method
outperforms all counterparts on the 3D task, achieving state-of-the art results
on the SemanticKITTI benchmark. Furthermore, we propose a 2D variant of S3CNet
with a multi-view fusion strategy to complement our 3D network, providing
robustness to occlusions and extreme sparsity in distant regions. We conduct
experiments for the 2D semantic scene completion task and compare the results
of our sparse 2D network against several leading LiDAR segmentation models
adapted for bird's eye view segmentation on two open-source datasets.
</p>
<a href="http://arxiv.org/abs/2012.09242" target="_blank">arXiv:2012.09242</a> [<a href="http://arxiv.org/pdf/2012.09242" target="_blank">pdf</a>]

<h2>Neural Pruning via Growing Regularization. (arXiv:2012.09243v1 [cs.CV])</h2>
<h3>Huan Wang, Can Qin, Yulun Zhang, Yun Fu</h3>
<p>Regularization has long been utilized to learn sparsity in deep neural
network pruning. However, its role is mainly explored in the small penalty
strength regime. In this work, we extend its application to a new scenario
where the regularization grows large gradually to tackle two central problems
of pruning: pruning schedule and weight importance scoring. (1) The former
topic is newly brought up in this work, which we find critical to the pruning
performance while receives little research attention. Specifically, we propose
an L2 regularization variant with rising penalty factors and show it can bring
significant accuracy gains compared with its one-shot counterpart, even when
the same weights are removed. (2) The growing penalty scheme also brings us an
approach to exploit the Hessian information for more accurate pruning without
knowing their specific values, thus not bothered by the common Hessian
approximation problems. Empirically, the proposed algorithms are easy to
implement and scalable to large datasets and networks in both structured and
unstructured pruning. Their effectiveness is demonstrated with modern deep
neural networks on the CIFAR and ImageNet datasets, achieving competitive
results compared to many state-of-the-art algorithms. Our code and trained
models are publicly available at
https://github.com/mingsuntse/regularization-pruning.
</p>
<a href="http://arxiv.org/abs/2012.09243" target="_blank">arXiv:2012.09243</a> [<a href="http://arxiv.org/pdf/2012.09243" target="_blank">pdf</a>]

<h2>ISD: Self-Supervised Learning by Iterative Similarity Distillation. (arXiv:2012.09259v1 [cs.CV])</h2>
<h3>Ajinkya Tejankar, Soroush Abbasi Koohpayegani, Vipin Pillai, Paolo Favaro, Hamed Pirsiavash</h3>
<p>Recently, contrastive learning has achieved great results in self-supervised
learning, where the main idea is to push two augmentations of an image
(positive pairs) closer compared to other random images (negative pairs). We
argue that not all random images are equal. Hence, we introduce a self
supervised learning algorithm where we use a soft similarity for the negative
images rather than a binary distinction between positive and negative pairs. We
iteratively distill a slowly evolving teacher model to the student model by
capturing the similarity of a query image to some random images and
transferring that knowledge to the student. We argue that our method is less
constrained compared to recent contrastive learning methods, so it can learn
better features. Specifically, our method should handle unbalanced and
unlabeled data better than existing contrastive learning methods, because the
randomly chosen negative set might include many samples that are semantically
similar to the query image. In this case, our method labels them as highly
similar while standard contrastive methods label them as negative pairs. Our
method achieves better results compared to state-of-the-art models like BYOL
and MoCo on transfer learning settings. We also show that our method performs
better in the settings where the unlabeled data is unbalanced. Our code is
available here: https://github.com/UMBCvision/ISD.
</p>
<a href="http://arxiv.org/abs/2012.09259" target="_blank">arXiv:2012.09259</a> [<a href="http://arxiv.org/pdf/2012.09259" target="_blank">pdf</a>]

<h2>MSL-RAPTOR: A 6DoF Relative Pose Tracker for Onboard Robotic Perception. (arXiv:2012.09264v1 [cs.RO])</h2>
<h3>Benjamin Ramtoula, Adam Caccavale, Giovanni Beltrame, Mac Schwager</h3>
<p>Determining the relative position and orientation of objects in an
environment is a fundamental building block for a wide range of robotics
applications. To accomplish this task efficiently in practical settings, a
method must be fast, use common sensors, and generalize easily to new objects
and environments. We present MSL-RAPTOR, a two-stage algorithm for tracking a
rigid body with a monocular camera. The image is first processed by an
efficient neural network-based front-end to detect new objects and track 2D
bounding boxes between frames. The class label and bounding box is passed to
the back-end that updates the object's pose using an unscented Kalman filter
(UKF). The measurement posterior is fed back to the 2D tracker to improve
robustness. The object's class is identified so a class-specific UKF can be
used if custom dynamics and constraints are known. Adapting to track the pose
of new classes only requires providing a trained 2D object detector or labeled
2D bounding box data, as well as the approximate size of the objects. The
performance of MSL-RAPTOR is first verified on the NOCS-REAL275 dataset,
achieving results comparable to RGB-D approaches despite not using depth
measurements. When tracking a flying drone from onboard another drone, it
outperforms the fastest comparable method in speed by a factor of 3, while
giving lower translation and rotation median errors by 66% and 23%
respectively.
</p>
<a href="http://arxiv.org/abs/2012.09264" target="_blank">arXiv:2012.09264</a> [<a href="http://arxiv.org/pdf/2012.09264" target="_blank">pdf</a>]

<h2>Data optimization for large batch distributed training of deep neural networks. (arXiv:2012.09272v1 [cs.LG])</h2>
<h3>Shubhankar Gahlot, Junqi Yin, Mallikarjun (Arjun) Shankar</h3>
<p>Distributed training in deep learning (DL) is common practice as data and
models grow. The current practice for distributed training of deep neural
networks faces the challenges of communication bottlenecks when operating at
scale, and model accuracy deterioration with an increase in global batch size.
Present solutions focus on improving message exchange efficiency as well as
implementing techniques to tweak batch sizes and models in the training
process. The loss of training accuracy typically happens because the loss
function gets trapped in a local minima. We observe that the loss landscape
minimization is shaped by both the model and training data and propose a data
optimization approach that utilizes machine learning to implicitly smooth out
the loss landscape resulting in fewer local minima. Our approach filters out
data points which are less important to feature learning, enabling us to speed
up the training of models on larger batch sizes to improved accuracy.
</p>
<a href="http://arxiv.org/abs/2012.09272" target="_blank">arXiv:2012.09272</a> [<a href="http://arxiv.org/pdf/2012.09272" target="_blank">pdf</a>]

<h2>On Exploiting Hitting Sets for Model Reconciliation. (arXiv:2012.09274v1 [cs.AI])</h2>
<h3>Stylianos Loukas Vasileiou, Alessandro Previti, William Yeoh</h3>
<p>In human-aware planning, a planning agent may need to provide an explanation
to a human user on why its plan is optimal. A popular approach to do this is
called model reconciliation, where the agent tries to reconcile the differences
in its model and the human's model such that the plan is also optimal in the
human's model. In this paper, we present a logic-based framework for model
reconciliation that extends beyond the realm of planning. More specifically,
given a knowledge base $KB_1$ entailing a formula $\varphi$ and a second
knowledge base $KB_2$ not entailing it, model reconciliation seeks an
explanation, in the form of a cardinality-minimal subset of $KB_1$, whose
integration into $KB_2$ makes the entailment possible. Our approach, based on
ideas originating in the context of analysis of inconsistencies, exploits the
existing hitting set duality between minimal correction sets (MCSes) and
minimal unsatisfiable sets (MUSes) in order to identify an appropriate
explanation. However, differently from those works targeting inconsistent
formulas, which assume a single knowledge base, MCSes and MUSes are computed
over two distinct knowledge bases. We conclude our paper with an empirical
evaluation of the newly introduced approach on planning instances, where we
show how it outperforms an existing state-of-the-art solver, and generic
non-planning instances from recent SAT competitions, for which no other solver
exists.
</p>
<a href="http://arxiv.org/abs/2012.09274" target="_blank">arXiv:2012.09274</a> [<a href="http://arxiv.org/pdf/2012.09274" target="_blank">pdf</a>]

<h2>Measuring Disentanglement: A Review of Metrics. (arXiv:2012.09276v1 [cs.LG])</h2>
<h3>Julian Zaidi, Jonathan Boilard, Ghyslain Gagnon, Marc-Andr&#xe9; Carbonneau</h3>
<p>Learning to disentangle and represent factors of variation in data is an
important problem in AI. While many advances are made to learn these
representations, it is still unclear how to quantify disentanglement. Several
metrics exist, however little is known on their implicit assumptions, what they
truly measure and their limits. As a result, it is difficult to interpret
results when comparing different representations. In this work, we survey
supervised disentanglement metrics and thoroughly analyze them. We propose a
new taxonomy in which all metrics fall into one of three families:
intervention-based, predictor-based and information-based. We conduct extensive
experiments, where we isolate representation properties to compare all metrics
on many aspects. From experiment results and analysis, we provide insights on
relations between disentangled representation properties. Finally, we provide
guidelines on how to measure disentanglement and report the results.
</p>
<a href="http://arxiv.org/abs/2012.09276" target="_blank">arXiv:2012.09276</a> [<a href="http://arxiv.org/pdf/2012.09276" target="_blank">pdf</a>]

<h2>Sparse Signal Models for Data Augmentation in Deep Learning ATR. (arXiv:2012.09284v1 [cs.CV])</h2>
<h3>Tushar Agarwal, Nithin Sugavanam, Emre Ertin</h3>
<p>Automatic Target Recognition (ATR) algorithms classify a given Synthetic
Aperture Radar (SAR) image into one of the known target classes using a set of
training images available for each class. Recently, learning methods have shown
to achieve state-of-the-art classification accuracy if abundant training data
is available, sampled uniformly over the classes, and their poses. In this
paper, we consider the task of ATR with a limited set of training images. We
propose a data augmentation approach to incorporate domain knowledge and
improve the generalization power of a data-intensive learning algorithm, such
as a Convolutional neural network (CNN). The proposed data augmentation method
employs a limited persistence sparse modeling approach, capitalizing on
commonly observed characteristics of wide-angle synthetic aperture radar (SAR)
imagery. Specifically, we exploit the sparsity of the scattering centers in the
spatial domain and the smoothly-varying structure of the scattering
coefficients in the azimuthal domain to solve the ill-posed problem of
over-parametrized model fitting. Using this estimated model, we synthesize new
images at poses and sub-pixel translations not available in the given data to
augment CNN's training data. The experimental results show that for the
training data starved region, the proposed method provides a significant gain
in the resulting ATR algorithm's generalization performance.
</p>
<a href="http://arxiv.org/abs/2012.09284" target="_blank">arXiv:2012.09284</a> [<a href="http://arxiv.org/pdf/2012.09284" target="_blank">pdf</a>]

<h2>Projected Distribution Loss for Image Enhancement. (arXiv:2012.09289v1 [cs.CV])</h2>
<h3>Mauricio Delbracio, Hossein Talebi, Peyman Milanfar</h3>
<p>Features obtained from object recognition CNNs have been widely used for
measuring perceptual similarities between images. Such differentiable metrics
can be used as perceptual learning losses to train image enhancement models.
However, the choice of the distance function between input and target features
may have a consequential impact on the performance of the trained model. While
using the norm of the difference between extracted features leads to limited
hallucination of details, measuring the distance between distributions of
features may generate more textures; yet also more unrealistic details and
artifacts. In this paper, we demonstrate that aggregating 1D-Wasserstein
distances between CNN activations is more reliable than the existing
approaches, and it can significantly improve the perceptual performance of
enhancement models. More explicitly, we show that in imaging applications such
as denoising, super-resolution, demosaicing, deblurring and JPEG artifact
removal, the proposed learning loss outperforms the current state-of-the-art on
reference-based perceptual losses. This means that the proposed learning loss
can be plugged into different imaging frameworks and produce perceptually
realistic results.
</p>
<a href="http://arxiv.org/abs/2012.09289" target="_blank">arXiv:2012.09289</a> [<a href="http://arxiv.org/pdf/2012.09289" target="_blank">pdf</a>]

<h2>Self-Supervised Sketch-to-Image Synthesis. (arXiv:2012.09290v1 [cs.CV])</h2>
<h3>Bingchen Liu, Yizhe Zhu, Kunpeng Song, Ahmed Elgammal</h3>
<p>Imagining a colored realistic image from an arbitrarily drawn sketch is one
of the human capabilities that we eager machines to mimic. Unlike previous
methods that either requires the sketch-image pairs or utilize low-quantity
detected edges as sketches, we study the exemplar-based sketch-to-image (s2i)
synthesis task in a self-supervised learning manner, eliminating the necessity
of the paired sketch data. To this end, we first propose an unsupervised method
to efficiently synthesize line-sketches for general RGB-only datasets. With the
synthetic paired-data, we then present a self-supervised Auto-Encoder (AE) to
decouple the content/style features from sketches and RGB-images, and
synthesize images that are both content-faithful to the sketches and
style-consistent to the RGB-images. While prior works employ either the
cycle-consistence loss or dedicated attentional modules to enforce the
content/style fidelity, we show AE's superior performance with pure
self-supervisions. To further improve the synthesis quality in high resolution,
we also leverage an adversarial network to refine the details of synthetic
images. Extensive experiments on 1024*1024 resolution demonstrate a new
state-of-art-art performance of the proposed model on CelebA-HQ and Wiki-Art
datasets. Moreover, with the proposed sketch generator, the model shows a
promising performance on style mixing and style transfer, which require
synthesized images to be both style-consistent and semantically meaningful. Our
code is available on
https://github.com/odegeasslbc/Self-Supervised-Sketch-to-Image-Synthesis-PyTorch,
and please visit https://create.playform.io/my-projects?mode=sketch for an
online demo of our model.
</p>
<a href="http://arxiv.org/abs/2012.09290" target="_blank">arXiv:2012.09290</a> [<a href="http://arxiv.org/pdf/2012.09290" target="_blank">pdf</a>]

<h2>Latent-CF: A Simple Baseline for Reverse Counterfactual Explanations. (arXiv:2012.09301v1 [cs.LG])</h2>
<h3>Rachana Balasubramanian, Samuel Sharpe, Brian Barr, Jason Wittenbach, C. Bayan Bruss</h3>
<p>In the environment of fair lending laws and the General Data Protection
Regulation (GDPR), the ability to explain a model's prediction is of paramount
importance. High quality explanations are the first step in assessing fairness.
Counterfactuals are valuable tools for explainability. They provide actionable,
comprehensible explanations for the individual who is subject to decisions made
from the prediction. It is important to find a baseline for producing them. We
propose a simple method for generating counterfactuals by using gradient
descent to search in the latent space of an autoencoder and benchmark our
method against approaches that search for counterfactuals in feature space.
Additionally, we implement metrics to concretely evaluate the quality of the
counterfactuals. We show that latent space counterfactual generation strikes a
balance between the speed of basic feature gradient descent methods and the
sparseness and authenticity of counterfactuals generated by more complex
feature space oriented techniques.
</p>
<a href="http://arxiv.org/abs/2012.09301" target="_blank">arXiv:2012.09301</a> [<a href="http://arxiv.org/pdf/2012.09301" target="_blank">pdf</a>]

<h2>TROJANZOO: Everything you ever wanted to know about neural backdoors (but were afraid to ask). (arXiv:2012.09302v1 [cs.LG])</h2>
<h3>Ren Pang, Zheng Zhang, Xiangshan Gao, Zhaohan Xi, Shouling Ji, Peng Cheng, Ting Wang</h3>
<p>Neural backdoors represent one primary threat to the security of deep
learning systems. The intensive research on this subject has produced a
plethora of attacks/defenses, resulting in a constant arms race. However, due
to the lack of evaluation benchmarks, many critical questions remain largely
unexplored: (i) How effective, evasive, or transferable are different attacks?
(ii) How robust, utility-preserving, or generic are different defenses? (iii)
How do various factors (e.g., model architectures) impact their performance?
(iv) What are the best practices (e.g., optimization strategies) to operate
such attacks/defenses? (v) How can the existing attacks/defenses be further
improved?

To bridge the gap, we design and implement TROJANZOO, the first open-source
platform for evaluating neural backdoor attacks/defenses in a unified,
holistic, and practical manner. Thus, it has incorporated 12 representative
attacks, 15 state-of-the-art defenses, 6 attack performance metrics, 10 defense
utility metrics, as well as rich tools for in-depth analysis of attack-defense
interactions. Leveraging TROJANZOO, we conduct a systematic study of existing
attacks/defenses, leading to a number of interesting findings: (i) different
attacks manifest various trade-offs among multiple desiderata (e.g.,
effectiveness, evasiveness, and transferability); (ii) one-pixel triggers often
suffice; (iii) optimizing trigger patterns and trojan models jointly improves
both attack effectiveness and evasiveness; (iv) sanitizing trojan models often
introduces new vulnerabilities; (v) most defenses are ineffective against
adaptive attacks, but integrating complementary ones significantly enhances
defense robustness. We envision that such findings will help users select the
right defense solutions and facilitate future research on neural backdoors.
</p>
<a href="http://arxiv.org/abs/2012.09302" target="_blank">arXiv:2012.09302</a> [<a href="http://arxiv.org/pdf/2012.09302" target="_blank">pdf</a>]

<h2>Robotics Enabling the Workforce. (arXiv:2012.09309v1 [cs.RO])</h2>
<h3>Henrik Christensen, Maria Gini, Odest Chadwicke Jenkins, Holly Yanco</h3>
<p>Robotics has the potential to magnify the skilled workforce of the nation by
complementing our workforce with automation: teams of people and robots will be
able to do more than either could alone. The economic engine of the U.S. runs
on the productivity of our people. The rise of automation offers new
opportunities to enhance the work of our citizens and drive the innovation and
prosperity of our industries. Most critically, we need research to understand
how future robot technologies can best complement our workforce to get the best
of both human and automated labor in a collaborative team. Investments made in
robotics research and workforce development will lead to increased GDP, an
increased export-import ratio, a growing middle class of skilled workers, and a
U.S.-based supply chain that can withstand global pandemics and other
disruptions. In order to make the United States a leader in robotics, we need
to invest in basic research, technology development, K-16 education, and
lifelong learning.
</p>
<a href="http://arxiv.org/abs/2012.09309" target="_blank">arXiv:2012.09309</a> [<a href="http://arxiv.org/pdf/2012.09309" target="_blank">pdf</a>]

<h2>Learning to Recognize Patch-Wise Consistency for Deepfake Detection. (arXiv:2012.09311v1 [cs.CV])</h2>
<h3>Tianchen Zhao, Xiang Xu, Mingze Xu, Hui Ding, Yuanjun Xiong, Wei Xia</h3>
<p>We propose to detect Deepfake generated by face manipulation based on one of
their fundamental features: images are blended by patches from multiple
sources, carrying distinct and persistent source features. In particular, we
propose a novel representation learning approach for this task, called
patch-wise consistency learning (PCL). It learns by measuring the consistency
of image source features, resulting to representation with good
interpretability and robustness to multiple forgery methods. We develop an
inconsistency image generator (I2G) to generate training data for PCL and boost
its robustness. We evaluate our approach on seven popular Deepfake detection
datasets. Our model achieves superior detection accuracy and generalizes well
to unseen generation methods. On average, our model outperforms the
state-of-the-art in terms of AUC by 2% and 8% in the in- and cross-dataset
evaluation, respectively.
</p>
<a href="http://arxiv.org/abs/2012.09311" target="_blank">arXiv:2012.09311</a> [<a href="http://arxiv.org/pdf/2012.09311" target="_blank">pdf</a>]

<h2>Generate and Verify: Semantically Meaningful Formal Analysis of Neural Network Perception Systems. (arXiv:2012.09313v1 [cs.LG])</h2>
<h3>Chris R. Serrano, Pape M. Sylla, Michael A. Warren</h3>
<p>Testing remains the primary method to evaluate the accuracy of neural network
perception systems. Prior work on the formal verification of neural network
perception models has been limited to notions of local adversarial robustness
for classification with respect to individual image inputs. In this work, we
propose a notion of global correctness for neural network perception models
performing regression with respect to a generative neural network with a
semantically meaningful latent space. That is, against an infinite set of
images produced by a generative model over an interval of its latent space, we
employ neural network verification to prove that the model will always produce
estimates within some error bound of the ground truth. Where the perception
model fails, we obtain semantically meaningful counter-examples which carry
information on concrete states of the system of interest that can be used
programmatically without human inspection of corresponding generated images.
Our approach, Generate and Verify, provides a new technique to gather insight
into the failure cases of neural network perception systems and provide
meaningful guarantees of correct behavior in safety critical applications.
</p>
<a href="http://arxiv.org/abs/2012.09313" target="_blank">arXiv:2012.09313</a> [<a href="http://arxiv.org/pdf/2012.09313" target="_blank">pdf</a>]

<h2>Applying Deutsch's concept of good explanations to artificial intelligence and neuroscience -- an initial exploration. (arXiv:2012.09318v1 [cs.AI])</h2>
<h3>Daniel C. Elton</h3>
<p>Artificial intelligence has made great strides since the deep learning
revolution, but AI systems still struggle to extrapolate outside of their
training data and adapt to new situations. For inspiration we look to the
domain of science, where scientists have been able to develop theories which
show remarkable ability to extrapolate and sometimes predict the existence of
phenomena which have never been observed before. According to David Deutsch,
this type of extrapolation, which he calls "reach", is due to scientific
theories being hard to vary. In this work we investigate Deutsch's hard-to-vary
principle and how it relates to more formalized principles in deep learning
such as the bias-variance trade-off and Occam's razor. We distinguish internal
variability, how much a model/theory can be varied internally while still
yielding the same predictions, with external variability, which is how much a
model must be varied to accurately predict new, out-of-distribution data. We
discuss how to measure internal variability using the size of the Rashomon set
and how to measure external variability using Kolmogorov complexity. We explore
what role hard-to-vary explanations play in intelligence by looking at the
human brain and distinguish two learning systems in the brain. The first system
operates similar to deep learning and likely underlies most of perception and
motor control while the second is a more creative system capable of generating
hard-to-vary explanations of the world. We argue that figuring out how
replicate this second system, which is capable of generating hard-to-vary
explanations, is a key challenge which needs to be solved in order to realize
artificial general intelligence. We make contact with the framework of
Popperian epistemology which rejects induction and asserts that knowledge
generation is an evolutionary process which proceeds through conjecture and
refutation.
</p>
<a href="http://arxiv.org/abs/2012.09318" target="_blank">arXiv:2012.09318</a> [<a href="http://arxiv.org/pdf/2012.09318" target="_blank">pdf</a>]

<h2>Polyblur: Removing mild blur by polynomial reblurring. (arXiv:2012.09322v1 [cs.CV])</h2>
<h3>Mauricio Delbracio, Ignacio Garcia-Dorado, Sungjoon Choi, Damien Kelly, Peyman Milanfar</h3>
<p>We present a highly efficient blind restoration method to remove mild blur in
natural images. Contrary to the mainstream, we focus on removing slight blur
that is often present, damaging image quality and commonly generated by small
out-of-focus, lens blur, or slight camera motion. The proposed algorithm first
estimates image blur and then compensates for it by combining multiple
applications of the estimated blur in a principled way. To estimate blur we
introduce a simple yet robust algorithm based on empirical observations about
the distribution of the gradient in sharp natural images. Our experiments show
that, in the context of mild blur, the proposed method outperforms traditional
and modern blind deblurring methods and runs in a fraction of the time. Our
method can be used to blindly correct blur before applying off-the-shelf deep
super-resolution methods leading to superior results than other highly complex
and computationally demanding techniques. The proposed method estimates and
removes mild blur from a 12MP image on a modern mobile phone in a fraction of a
second.
</p>
<a href="http://arxiv.org/abs/2012.09322" target="_blank">arXiv:2012.09322</a> [<a href="http://arxiv.org/pdf/2012.09322" target="_blank">pdf</a>]

<h2>Series Saliency: Temporal Interpretation for Multivariate Time Series Forecasting. (arXiv:2012.09324v1 [cs.LG])</h2>
<h3>Qingyi Pan, Wenbo Hu, Jun Zhu</h3>
<p>Time series forecasting is an important yet challenging task. Though deep
learning methods have recently been developed to give superior forecasting
results, it is crucial to improve the interpretability of time series models.
Previous interpretation methods, including the methods for general neural
networks and attention-based methods, mainly consider the interpretation in the
feature dimension while ignoring the crucial temporal dimension. In this paper,
we present the series saliency framework for temporal interpretation for
multivariate time series forecasting, which considers the forecasting
interpretation in both feature and temporal dimensions. By extracting the
"series images" from the sliding windows of the time series, we apply the
saliency map segmentation following the smallest destroying region principle.
The series saliency framework can be employed to any well-defined deep learning
models and works as a data augmentation to get more accurate forecasts.
Experimental results on several real datasets demonstrate that our framework
generates temporal interpretations for the time series forecasting task while
produces accurate time series forecast.
</p>
<a href="http://arxiv.org/abs/2012.09324" target="_blank">arXiv:2012.09324</a> [<a href="http://arxiv.org/pdf/2012.09324" target="_blank">pdf</a>]

<h2>Simultaneous View and Feature Selection for Collaborative Multi-Robot Recognition. (arXiv:2012.09328v1 [cs.RO])</h2>
<h3>Brian Reily, Hao Zhang</h3>
<p>Collaborative multi-robot perception provides multiple views of an
environment, offering varying perspectives to collaboratively understand the
environment even when individual robots have poor points of view or when
occlusions are caused by obstacles. These multiple observations must be
intelligently fused for accurate recognition, and relevant observations need to
be selected in order to allow unnecessary robots to continue on to observe
other targets. This research problem has not been well studied in the
literature yet. In this paper, we propose a novel approach to collaborative
multi-robot perception that simultaneously integrates view selection, feature
selection, and object recognition into a unified regularized optimization
formulation, which uses sparsity-inducing norms to identify the robots with the
most representative views and the modalities with the most discriminative
features. As our optimization formulation is hard to solve due to the
introduced non-smooth norms, we implement a new iterative optimization
algorithm, which is guaranteed to converge to the optimal solution. We evaluate
our approach on multi-view benchmark datasets, a case-study in simulation, and
on a physical multi-robot system. Experimental results demonstrate that our
approach enables accurate object recognition and effective view selection as
defined by mutual information.
</p>
<a href="http://arxiv.org/abs/2012.09328" target="_blank">arXiv:2012.09328</a> [<a href="http://arxiv.org/pdf/2012.09328" target="_blank">pdf</a>]

<h2>Team Assignment for Heterogeneous Multi-Robot Sensor Coverage through Graph Representation Learning. (arXiv:2012.09331v1 [cs.RO])</h2>
<h3>Brian Reily, Hao Zhang</h3>
<p>Sensor coverage is the critical multi-robot problem of maximizing the
detection of events in an environment through the deployment of multiple
robots. Large multi-robot systems are often composed of simple robots that are
typically not equipped with a complete set of sensors, so teams with
comprehensive sensing abilities are required to properly cover an area. Robots
also exhibit multiple forms of relationships (e.g., communication connections
or spatial distribution) that need to be considered when assigning robot teams
for sensor coverage. To address this problem, in this paper we introduce a
novel formulation of sensor coverage by multi-robot systems with heterogeneous
relationships as a graph representation learning problem. We propose a
principled approach based on the mathematical framework of regularized
optimization to learn a unified representation of the multi-robot system from
the graphs describing the heterogeneous relationships and to identify the
learned representation's underlying structure in order to assign the robots to
teams. To evaluate the proposed approach, we conduct extensive experiments on
simulated multi-robot systems and a physical multi-robot system as a case
study, demonstrating that our approach is able to effectively assign teams for
heterogeneous multi-robot sensor coverage.
</p>
<a href="http://arxiv.org/abs/2012.09331" target="_blank">arXiv:2012.09331</a> [<a href="http://arxiv.org/pdf/2012.09331" target="_blank">pdf</a>]

<h2>Unsupervised Learning of Local Discriminative Representation for Medical Images. (arXiv:2012.09333v1 [cs.CV])</h2>
<h3>Huai Chen, Jieyu Li, Renzhen Wang, Yijie Huang, Fanrui Meng, Deyu Meng, Qing Peng, Lisheng Wang</h3>
<p>Local discriminative representation is needed in many medical image analysis
tasks such as identifying sub-types of lesion or segmenting detailed components
of anatomical structures by measuring similarity of local image regions.
However, the commonly applied supervised representation learning methods
require a large amount of annotated data, and unsupervised discriminative
representation learning distinguishes different images by learning a global
feature. In order to avoid the limitations of these two methods and be suitable
for localized medical image analysis tasks, we introduce local discrimination
into unsupervised representation learning in this work. The model contains two
branches: one is an embedding branch which learns an embedding function to
disperse dissimilar pixels over a low-dimensional hypersphere; and the other is
a clustering branch which learns a clustering function to classify similar
pixels into the same cluster. These two branches are trained simultaneously in
a mutually beneficial pattern, and the learnt local discriminative
representations are able to well measure the similarity of local image regions.
These representations can be transferred to enhance various downstream tasks.
Meanwhile, they can also be applied to cluster anatomical structures from
unlabeled medical images under the guidance of topological priors from
simulation or other structures with similar topological characteristics. The
effectiveness and usefulness of the proposed method are demonstrated by
enhancing various downstream tasks and clustering anatomical structures in
retinal images and chest X-ray images. The corresponding code is available at
https://github.com/HuaiChen-1994/LDLearning.
</p>
<a href="http://arxiv.org/abs/2012.09333" target="_blank">arXiv:2012.09333</a> [<a href="http://arxiv.org/pdf/2012.09333" target="_blank">pdf</a>]

<h2>Adaptation to Team Composition Changes for Heterogeneous Multi-Robot Sensor Coverage. (arXiv:2012.09334v1 [cs.RO])</h2>
<h3>Brian Reily, Terran Mott, Hao Zhang</h3>
<p>We consider the problem of multi-robot sensor coverage, which deals with
deploying a multi-robot team in an environment and optimizing the sensing
quality of the overall environment. As real-world environments involve a
variety of sensory information, and individual robots are limited in their
available number of sensors, successful multi-robot sensor coverage requires
the deployment of robots in such a way that each individual team member's
sensing quality is maximized. Additionally, because individual robots have
varying complements of sensors and both robots and sensors can fail, robots
must be able to adapt and adjust how they value each sensing capability in
order to obtain the most complete view of the environment, even through changes
in team composition. We introduce a novel formulation for sensor coverage by
multi-robot teams with heterogeneous sensing capabilities that maximizes each
robot's sensing quality, balancing the varying sensing capabilities of
individual robots based on the overall team composition. We propose a solution
based on regularized optimization that uses sparsity-inducing terms to ensure a
robot team focuses on all possible event types, and which we show is proven to
converge to the optimal solution. Through extensive simulation, we show that
our approach is able to effectively deploy a multi-robot team to maximize the
sensing quality of an environment, responding to failures in the multi-robot
team more robustly than non-adaptive approaches.
</p>
<a href="http://arxiv.org/abs/2012.09334" target="_blank">arXiv:2012.09334</a> [<a href="http://arxiv.org/pdf/2012.09334" target="_blank">pdf</a>]

<h2>Game Theoretic Decentralized and Communication-Free Multi-Robot Navigation. (arXiv:2012.09335v1 [cs.RO])</h2>
<h3>Brian Reily, Terran Mott, Hao Zhang</h3>
<p>Effective multi-robot teams require the ability to move to goals in complex
environments in order to address real-world applications such as search and
rescue. Multi-robot teams should be able to operate in a completely
decentralized manner, with individual robot team members being capable of
acting without explicit communication between neighbors. In this paper, we
propose a novel game theoretic model that enables decentralized and
communication-free navigation to a goal position. Robots estimate the behavior
of their local teammates in order to identify behaviors that move them in the
direction of the goal, while also avoiding obstacles and maintaining team
cohesion without collisions. We prove theoretically that generated actions
approach a Nash equilibrium, which also corresponds to an optimal strategy
identified for each robot. We show through simulations that our approach
enables decentralized and communication-free navigation by a multi-robot system
to a goal position, avoiding obstacles and collisions, while also maintaining
connectivity.
</p>
<a href="http://arxiv.org/abs/2012.09335" target="_blank">arXiv:2012.09335</a> [<a href="http://arxiv.org/pdf/2012.09335" target="_blank">pdf</a>]

<h2>Roof-GAN: Learning to Generate Roof Geometry and Relations for Residential Houses. (arXiv:2012.09340v1 [cs.CV])</h2>
<h3>Yiming Qian, Hao Zhang, Yasutaka Furukawa</h3>
<p>This paper presents Roof-GAN, a novel generative adversarial network that
generates structured geometry of residential roof structures as a set of roof
primitives and their relationships. Given the number of primitives, the
generator produces a structured roof model as a graph, which consists of 1)
primitive geometry as raster images at each node, encoding facet segmentation
and angles; 2) inter-primitive colinear/coplanar relationships at each edge;
and 3) primitive geometry in a vector format at each node, generated by a novel
differentiable vectorizer while enforcing the relationships. The discriminator
is trained to assess the primitive raster geometry, the primitive
relationships, and the primitive vector geometry in a fully end-to-end
architecture. Qualitative and quantitative evaluations demonstrate the
effectiveness of our approach in generating diverse and realistic roof models
over the competing methods with a novel metric proposed in this paper for the
task of structured geometry generation. We will share our code and data.
</p>
<a href="http://arxiv.org/abs/2012.09340" target="_blank">arXiv:2012.09340</a> [<a href="http://arxiv.org/pdf/2012.09340" target="_blank">pdf</a>]

<h2>Muscle-inspired flexible mechanical logic architecture for colloidal robotics. (arXiv:2012.09345v1 [cs.RO])</h2>
<h3>Mayank Agrawal, Sharon C. Glotzer</h3>
<p>Materials that respond to external stimuli by expanding or contracting
provide a transduction route that integrates sensing and actuation powered
directly by the stimuli. This motivates us to build colloidal scale robots
using these materials that can morph into arbitrary configurations. For
intelligent use of global stimuli in robotic systems, computation ability needs
to be incorporated within them. The challenge is to design an architecture that
is compact, material agnostic, stable under stochastic forces and can employ
stimuli-responsive materials. We present an architecture that computes
combinatorial logic using mechanical gates that use muscle-like response -
expansion and contraction - as circuit signal with additional benefits of logic
circuitry being physically flexible and able to be retrofit to arbitrary robot
bodies. We mathematically analyze gate geometry and discuss tuning it for the
given requirements of signal dimension and magnitude. We validate the function
and stability of the design at the colloidal scale using Brownian dynamics
simulations. We also demonstrate the gate design using a 3D printed model.
Finally, we simulate a complete robot that folds into Tetris shapes.
</p>
<a href="http://arxiv.org/abs/2012.09345" target="_blank">arXiv:2012.09345</a> [<a href="http://arxiv.org/pdf/2012.09345" target="_blank">pdf</a>]

<h2>Towards Scalable and Privacy-Preserving Deep Neural Network via Algorithmic-Cryptographic Co-design. (arXiv:2012.09364v1 [cs.LG])</h2>
<h3>Chaochao Chen, Jun Zhou, Longfei Zheng, Yan Wang, Xiaolin Zheng, Bingzhe Wu, Cen Chen, Li Wang, Jianwei Yin</h3>
<p>Deep Neural Networks (DNNs) have achieved remarkable progress in various
real-world applications, especially when abundant training data are provided.
However, data isolation has become a serious problem currently. Existing works
build privacy preserving DNN models from either algorithmic perspective or
cryptographic perspective. The former mainly splits the DNN computation graph
between data holders or between data holders and server, which demonstrates
good scalability but suffers from accuracy loss and potential privacy risks. In
contrast, the latter leverages time-consuming cryptographic techniques, which
has strong privacy guarantee but poor scalability. In this paper, we propose
SPNN - a Scalable and Privacy-preserving deep Neural Network learning
framework, from algorithmic-cryptographic co-perspective. From algorithmic
perspective, we split the computation graph of DNN models into two parts, i.e.,
the private data related computations that are performed by data holders and
the rest heavy computations that are delegated to a server with high
computation ability. From cryptographic perspective, we propose using two types
of cryptographic techniques, i.e., secret sharing and homomorphic encryption,
for the isolated data holders to conduct private data related computations
privately and cooperatively. Furthermore, we implement SPNN in a decentralized
setting and introduce user-friendly APIs. Experimental results conducted on
real-world datasets demonstrate the superiority of SPNN.
</p>
<a href="http://arxiv.org/abs/2012.09364" target="_blank">arXiv:2012.09364</a> [<a href="http://arxiv.org/pdf/2012.09364" target="_blank">pdf</a>]

<h2>Learning to Recover 3D Scene Shape from a Single Image. (arXiv:2012.09365v1 [cs.CV])</h2>
<h3>Wei Yin, Jianming Zhang, Oliver Wang, Simon Niklaus, Long Mai, Simon Chen, Chunhua Shen</h3>
<p>Despite significant progress in monocular depth estimation in the wild,
recent state-of-the-art methods cannot be used to recover accurate 3D scene
shape due to an unknown depth shift induced by shift-invariant reconstruction
losses used in mixed-data depth prediction training, and possible unknown
camera focal length. We investigate this problem in detail, and propose a
two-stage framework that first predicts depth up to an unknown scale and shift
from a single monocular image, and then use 3D point cloud encoders to predict
the missing depth shift and focal length that allow us to recover a realistic
3D scene shape. In addition, we propose an image-level normalized regression
loss and a normal-based geometry loss to enhance depth prediction models
trained on mixed datasets. We test our depth model on nine unseen datasets and
achieve state-of-the-art performance on zero-shot dataset generalization. Code
is available at: https://git.io/Depth
</p>
<a href="http://arxiv.org/abs/2012.09365" target="_blank">arXiv:2012.09365</a> [<a href="http://arxiv.org/pdf/2012.09365" target="_blank">pdf</a>]

<h2>Extending the Range of Drone-based Delivery Services by Exploration. (arXiv:2012.09367v1 [cs.RO])</h2>
<h3>Tsz-Chiu Au</h3>
<p>Drones have a fairly short range due to their limited battery life. We
propose an adaptive exploration techniques to extend the range of drones by
taking advantage of physical structures such as tall buildings and trees in
urban environments. Our goal is to extend the coverage of a drone delivery
service by generating paths for a drone to reach its destination while learning
about the energy consumption on each edge on its path in order to optimize its
range for future missions. We evaluated the performance of our exploration
strategy in finding the set of all reachable destinations in a city, and found
that exploring locations near the boundary of the reachable sets according to
the current energy model can speed up the learning process.
</p>
<a href="http://arxiv.org/abs/2012.09367" target="_blank">arXiv:2012.09367</a> [<a href="http://arxiv.org/pdf/2012.09367" target="_blank">pdf</a>]

<h2>Semi-Global Shape-aware Network. (arXiv:2012.09372v1 [cs.CV])</h2>
<h3>Pengju Zhang, Yihong Wu, Jiagang Zhu</h3>
<p>Non-local operations are usually used to capture long-range dependencies via
aggregating global context to each position recently. However, most of the
methods cannot preserve object shapes since they only focus on feature
similarity but ignore proximity between central and other positions for
capturing long-range dependencies, while shape-awareness is beneficial to many
computer vision tasks. In this paper, we propose a Semi-Global Shape-aware
Network (SGSNet) considering both feature similarity and proximity for
preserving object shapes when modeling long-range dependencies. A hierarchical
way is taken to aggregate global context. In the first level, each position in
the whole feature map only aggregates contextual information in vertical and
horizontal directions according to both similarity and proximity. And then the
result is input into the second level to do the same operations. By this
hierarchical way, each central position gains supports from all other
positions, and the combination of similarity and proximity makes each position
gain supports mostly from the same semantic object. Moreover, we also propose a
linear time algorithm for the aggregation of contextual information, where each
of rows and columns in the feature map is treated as a binary tree to reduce
similarity computation cost. Experiments on semantic segmentation and image
retrieval show that adding SGSNet to existing networks gains solid improvements
on both accuracy and efficiency.
</p>
<a href="http://arxiv.org/abs/2012.09372" target="_blank">arXiv:2012.09372</a> [<a href="http://arxiv.org/pdf/2012.09372" target="_blank">pdf</a>]

<h2>Unlabeled Data Guided Semi-supervised Histopathology Image Segmentation. (arXiv:2012.09373v1 [cs.CV])</h2>
<h3>Hongxiao Wang, Hao Zheng, Jianxu Chen, Lin Yang, Yizhe Zhang, Danny Z. Chen</h3>
<p>Automatic histopathology image segmentation is crucial to disease analysis.
Limited available labeled data hinders the generalizability of trained models
under the fully supervised setting. Semi-supervised learning (SSL) based on
generative methods has been proven to be effective in utilizing diverse image
characteristics. However, it has not been well explored what kinds of generated
images would be more useful for model training and how to use such images. In
this paper, we propose a new data guided generative method for histopathology
image segmentation by leveraging the unlabeled data distributions. First, we
design an image generation module. Image content and style are disentangled and
embedded in a clustering-friendly space to utilize their distributions. New
images are synthesized by sampling and cross-combining contents and styles.
Second, we devise an effective data selection policy for judiciously sampling
the generated images: (1) to make the generated training set better cover the
dataset, the clusters that are underrepresented in the original training set
are covered more; (2) to make the training process more effective, we identify
and oversample the images of "hard cases" in the data for which annotated
training data may be scarce. Our method is evaluated on glands and nuclei
datasets. We show that under both the inductive and transductive settings, our
SSL method consistently boosts the performance of common segmentation models
and attains state-of-the-art results.
</p>
<a href="http://arxiv.org/abs/2012.09373" target="_blank">arXiv:2012.09373</a> [<a href="http://arxiv.org/pdf/2012.09373" target="_blank">pdf</a>]

<h2>Event Camera Calibration of Per-pixel Biased Contrast Threshold. (arXiv:2012.09378v1 [cs.CV])</h2>
<h3>Ziwei Wang, Yonhon Ng, Pieter van Goor, Robert Mahony</h3>
<p>Event cameras output asynchronous events to represent intensity changes with
a high temporal resolution, even under extreme lighting conditions. Currently,
most of the existing works use a single contrast threshold to estimate the
intensity change of all pixels. However, complex circuit bias and manufacturing
imperfections cause biased pixels and mismatch contrast threshold among pixels,
which may lead to undesirable outputs. In this paper, we propose a new event
camera model and two calibration approaches which cover event-only cameras and
hybrid image-event cameras. When intensity images are simultaneously provided
along with events, we also propose an efficient online method to calibrate
event cameras that adapts to time-varying event rates. We demonstrate the
advantages of our proposed methods compared to the state-of-the-art on several
different event camera datasets.
</p>
<a href="http://arxiv.org/abs/2012.09378" target="_blank">arXiv:2012.09378</a> [<a href="http://arxiv.org/pdf/2012.09378" target="_blank">pdf</a>]

<h2>DecAug: Out-of-Distribution Generalization via Decomposed Feature Representation and Semantic Augmentation. (arXiv:2012.09382v1 [cs.LG])</h2>
<h3>Haoyue Bai, Rui Sun, Lanqing Hong, Fengwei Zhou, Nanyang Ye, Han-Jia Ye, S.-H. Gary Chan, Zhenguo Li</h3>
<p>While deep learning demonstrates its strong ability to handle independent and
identically distributed (IID) data, it often suffers from out-of-distribution
(OoD) generalization, where the test data come from another distribution
(w.r.t. the training one). Designing a general OoD generalization framework to
a wide range of applications is challenging, mainly due to possible correlation
shift and diversity shift in the real world. Most of the previous approaches
can only solve one specific distribution shift, such as shift across domains or
the extrapolation of correlation. To address that, we propose DecAug, a novel
decomposed feature representation and semantic augmentation approach for OoD
generalization. DecAug disentangles the category-related and context-related
features. Category-related features contain causal information of the target
object, while context-related features describe the attributes, styles,
backgrounds, or scenes, causing distribution shifts between training and test
data. The decomposition is achieved by orthogonalizing the two gradients
(w.r.t. intermediate features) of losses for predicting category and context
labels. Furthermore, we perform gradient-based augmentation on context-related
features to improve the robustness of the learned representations. Experimental
results show that DecAug outperforms other state-of-the-art methods on various
OoD datasets, which is among the very few methods that can deal with different
types of OoD generalization challenges.
</p>
<a href="http://arxiv.org/abs/2012.09382" target="_blank">arXiv:2012.09382</a> [<a href="http://arxiv.org/pdf/2012.09382" target="_blank">pdf</a>]

<h2>On the Limitations of Denoising Strategies as Adversarial Defenses. (arXiv:2012.09384v1 [cs.LG])</h2>
<h3>Zhonghan Niu, Zhaoxi Chen, Linyi Li, Yubin Yang, Bo Li, Jinfeng Yi</h3>
<p>As adversarial attacks against machine learning models have raised increasing
concerns, many denoising-based defense approaches have been proposed. In this
paper, we summarize and analyze the defense strategies in the form of symmetric
transformation via data denoising and reconstruction (denoted as $F+$ inverse
$F$, $F-IF$ Framework). In particular, we categorize these denoising strategies
from three aspects (i.e. denoising in the spatial domain, frequency domain, and
latent space, respectively). Typically, defense is performed on the entire
adversarial example, both image and perturbation are modified, making it
difficult to tell how it defends against the perturbations. To evaluate the
robustness of these denoising strategies intuitively, we directly apply them to
defend against adversarial noise itself (assuming we have obtained all of it),
which saving us from sacrificing benign accuracy. Surprisingly, our
experimental results show that even if most of the perturbations in each
dimension is eliminated, it is still difficult to obtain satisfactory
robustness. Based on the above findings and analyses, we propose the adaptive
compression strategy for different frequency bands in the feature domain to
improve the robustness. Our experiment results show that the adaptive
compression strategies enable the model to better suppress adversarial
perturbations, and improve robustness compared with existing denoising
strategies.
</p>
<a href="http://arxiv.org/abs/2012.09384" target="_blank">arXiv:2012.09384</a> [<a href="http://arxiv.org/pdf/2012.09384" target="_blank">pdf</a>]

<h2>Balancing Geometry and Density: Path Distances on High-Dimensional Data. (arXiv:2012.09385v1 [stat.ML])</h2>
<h3>Anna Little, Daniel McKenzie, James Murphy</h3>
<p>New geometric and computational analyses of power-weighted shortest-path
distances (PWSPDs) are presented. By illuminating the way these metrics balance
density and geometry in the underlying data, we clarify their key parameters
and discuss how they may be chosen in practice. Comparisons are made with
related data-driven metrics, which illustrate the broader role of density in
kernel-based unsupervised and semi-supervised machine learning.
Computationally, we relate PWSPDs on complete weighted graphs to their
analogues on weighted nearest neighbor graphs, providing high probability
guarantees on their equivalence that are near-optimal. Connections with
percolation theory are developed to establish estimates on the bias and
variance of PWSPDs in the finite sample setting. The theoretical results are
bolstered by illustrative experiments, demonstrating the versatility of PWSPDs
for a wide range of data settings. Throughout the paper, our results require
only that the underlying data is sampled from a low-dimensional manifold, and
depend crucially on the intrinsic dimension of this manifold, rather than its
ambient dimension.
</p>
<a href="http://arxiv.org/abs/2012.09385" target="_blank">arXiv:2012.09385</a> [<a href="http://arxiv.org/pdf/2012.09385" target="_blank">pdf</a>]

<h2>A Contrast Synthesized Thalamic Nuclei Segmentation Scheme using Convolutional Neural Networks. (arXiv:2012.09386v1 [cs.LG])</h2>
<h3>Lavanya Umapathy, Mahesh Bharath Keerthivasan, Natalie M. Zahr, Ali Bilgin, Manojkumar Saranathan</h3>
<p>Thalamic nuclei have been implicated in several neurological diseases.
WMn-MPRAGE images have been shown to provide better intra-thalamic nuclear
contrast compared to conventional MPRAGE images but the additional acquisition
results in increased examination times. In this work, we investigated 3D
Convolutional Neural Network (CNN) based techniques for thalamic nuclei
parcellation from conventional MPRAGE images. Two 3D CNNs were developed and
compared for thalamic nuclei parcellation using MPRAGE images: a) a native
contrast segmentation (NCS) and b) a synthesized contrast segmentation (SCS)
using WMn-MPRAGE images synthesized from MPRAGE images. We trained the two
segmentation frameworks using MPRAGE images (n=35) and thalamic nuclei labels
generated on WMn-MPRAGE images using a multi-atlas based parcellation
technique. The segmentation accuracy and clinical utility were evaluated on a
cohort comprising of healthy subjects and patients with alcohol use disorder
(AUD) (n=45). The SCS network yielded higher Dice scores in the Medial
geniculate nucleus (P=.003) and Centromedian nucleus (P=.01) with lower volume
differences for Ventral anterior (P=.001) and Ventral posterior lateral (P=.01)
nuclei when compared to the NCS network. A Bland-Altman analysis revealed
tighter limits of agreement with lower coefficient of variation between true
volumes and those predicted by the SCS network. The SCS network demonstrated a
significant atrophy in Ventral lateral posterior nucleus in AUD patients
compared to healthy age-matched controls (P=0.01), agreeing with previous
studies on thalamic atrophy in alcoholism, whereas the NCS network showed
spurious atrophy of the Ventral posterior lateral nucleus. CNN-based contrast
synthesis prior to segmentation can provide fast and accurate thalamic nuclei
segmentation from conventional MPRAGE images.
</p>
<a href="http://arxiv.org/abs/2012.09386" target="_blank">arXiv:2012.09386</a> [<a href="http://arxiv.org/pdf/2012.09386" target="_blank">pdf</a>]

<h2>Classifying Sequences of Extreme Length with Constant Memory Applied to Malware Detection. (arXiv:2012.09390v1 [stat.ML])</h2>
<h3>Edward Raff, William Fleshman, Richard Zak, Hyrum S. Anderson, Bobby Filar, Mark McLean</h3>
<p>Recent works within machine learning have been tackling inputs of
ever-increasing size, with cybersecurity presenting sequence classification
problems of particularly extreme lengths. In the case of Windows executable
malware detection, inputs may exceed $100$ MB, which corresponds to a time
series with $T=100,000,000$ steps. To date, the closest approach to handling
such a task is MalConv, a convolutional neural network capable of processing up
to $T=2,000,000$ steps. The $\mathcal{O}(T)$ memory of CNNs has prevented
further application of CNNs to malware. In this work, we develop a new approach
to temporal max pooling that makes the required memory invariant to the
sequence length $T$. This makes MalConv $116\times$ more memory efficient, and
up to $25.8\times$ faster to train on its original dataset, while removing the
input length restrictions to MalConv. We re-invest these gains into improving
the MalConv architecture by developing a new Global Channel Gating design,
giving us an attention mechanism capable of learning feature interactions
across 100 million time steps in an efficient manner, a capability lacked by
the original MalConv CNN. Our implementation can be found at
https://github.com/NeuromorphicComputationResearchProgram/MalConv2
</p>
<a href="http://arxiv.org/abs/2012.09390" target="_blank">arXiv:2012.09390</a> [<a href="http://arxiv.org/pdf/2012.09390" target="_blank">pdf</a>]

<h2>MASKER: Masked Keyword Regularization for Reliable Text Classification. (arXiv:2012.09392v1 [cs.LG])</h2>
<h3>Seung Jun Moon, Sangwoo Mo, Kimin Lee, Jaeho Lee, Jinwoo Shin</h3>
<p>Pre-trained language models have achieved state-of-the-art accuracies on
various text classification tasks, e.g., sentiment analysis, natural language
inference, and semantic textual similarity. However, the reliability of the
fine-tuned text classifiers is an often underlooked performance criterion. For
instance, one may desire a model that can detect out-of-distribution (OOD)
samples (drawn far from training distribution) or be robust against domain
shifts. We claim that one central obstacle to the reliability is the
over-reliance of the model on a limited number of keywords, instead of looking
at the whole context. In particular, we find that (a) OOD samples often contain
in-distribution keywords, while (b) cross-domain samples may not always contain
keywords; over-relying on the keywords can be problematic for both cases. In
light of this observation, we propose a simple yet effective fine-tuning
method, coined masked keyword regularization (MASKER), that facilitates
context-based prediction. MASKER regularizes the model to reconstruct the
keywords from the rest of the words and make low-confidence predictions without
enough context. When applied to various pre-trained language models (e.g.,
BERT, RoBERTa, and ALBERT), we demonstrate that MASKER improves OOD detection
and cross-domain generalization without degrading classification accuracy. Code
is available at https://github.com/alinlab/MASKER.
</p>
<a href="http://arxiv.org/abs/2012.09392" target="_blank">arXiv:2012.09392</a> [<a href="http://arxiv.org/pdf/2012.09392" target="_blank">pdf</a>]

<h2>Efficient Golf Ball Detection and Tracking Based on Convolutional Neural Networks and Kalman Filter. (arXiv:2012.09393v1 [cs.CV])</h2>
<h3>Tianxiao Zhang, Xiaohan Zhang, Yiju Yang, Zongbo Wang, Guanghui Wang</h3>
<p>This paper focuses on the problem of online golf ball detection and tracking
from image sequences. An efficient real-time approach is proposed by exploiting
convolutional neural networks (CNN) based object detection and a Kalman filter
based prediction. Five classical deep learning-based object detection networks
are implemented and evaluated for ball detection, including YOLO v3 and its
tiny version, YOLO v4, Faster R-CNN, SSD, and RefineDet. The detection is
performed on small image patches instead of the entire image to increase the
performance of small ball detection. At the tracking stage, a discrete Kalman
filter is employed to predict the location of the ball and a small image patch
is cropped based on the prediction. Then, the object detector is utilized to
refine the location of the ball and update the parameters of Kalman filter. In
order to train the detection models and test the tracking algorithm, a
collection of golf ball dataset is created and annotated. Extensive comparative
experiments are performed to demonstrate the effectiveness and superior
tracking performance of the proposed scheme.
</p>
<a href="http://arxiv.org/abs/2012.09393" target="_blank">arXiv:2012.09393</a> [<a href="http://arxiv.org/pdf/2012.09393" target="_blank">pdf</a>]

<h2>Metrical Task Systems with Online Machine Learned Advice. (arXiv:2012.09394v1 [cs.LG])</h2>
<h3>Kevin Rao</h3>
<p>Machine learning algorithms are designed to make accurate predictions of the
future based on existing data, while online algorithms seek to bound some
performance measure (typically the competitive ratio) without knowledge of the
future. Lykouris and Vassilvitskii demonstrated that augmenting online
algorithms with a machine learned predictor can provably decrease the
competitive ratio under as long as the predictor is suitably accurate.

In this work we apply this idea to the Online Metrical Task System problem,
which was put forth by Borodin, Linial, and Saks as a general model for dynamic
systems processing tasks in an online fashion. We focus on the specific class
of uniform task systems on $n$ tasks, for which the best deterministic
algorithm is $O(n)$ competitive and the best randomized algorithm is $O(\log
n)$ competitive.

By giving an online algorithms access to a machine learned oracle with
absolute predictive error bounded above by $\eta_0$, we construct a
$\Theta(\min(\sqrt{\eta_0}, \log n))$ competitive algorithm for the uniform
case of the metrical task systems problem. We also give a $\Theta(\log
\sqrt{\eta_0})$ lower bound on the competitive ratio of any randomized
algorithm.
</p>
<a href="http://arxiv.org/abs/2012.09394" target="_blank">arXiv:2012.09394</a> [<a href="http://arxiv.org/pdf/2012.09394" target="_blank">pdf</a>]

<h2>Invariant Teacher and Equivariant Student for Unsupervised 3D Human Pose Estimation. (arXiv:2012.09398v1 [cs.CV])</h2>
<h3>Chenxin Xu, Siheng Chen, Maosen Li, Ya Zhang</h3>
<p>We propose a novel method based on teacher-student learning framework for 3D
human pose estimation without any 3D annotation or side information. To solve
this unsupervised-learning problem, the teacher network adopts
pose-dictionary-based modeling for regularization to estimate a physically
plausible 3D pose. To handle the decomposition ambiguity in the teacher
network, we propose a cycle-consistent architecture promoting a 3D
rotation-invariant property to train the teacher network. To further improve
the estimation accuracy, the student network adopts a novel graph convolution
network for flexibility to directly estimate the 3D coordinates. Another
cycle-consistent architecture promoting 3D rotation-equivariant property is
adopted to exploit geometry consistency, together with knowledge distillation
from the teacher network to improve the pose estimation performance. We conduct
extensive experiments on Human3.6M and MPI-INF-3DHP. Our method reduces the 3D
joint prediction error by 11.4% compared to state-of-the-art unsupervised
methods and also outperforms many weakly-supervised methods that use side
information on Human3.6M. Code will be available at
https://github.com/sjtuxcx/ITES.
</p>
<a href="http://arxiv.org/abs/2012.09398" target="_blank">arXiv:2012.09398</a> [<a href="http://arxiv.org/pdf/2012.09398" target="_blank">pdf</a>]

<h2>Stochastic Compositional Gradient Descent under Compositional constraints. (arXiv:2012.09400v1 [cs.LG])</h2>
<h3>Srujan Teja Thomdapu, Harshvardhan, Ketan Rajawat</h3>
<p>This work studies constrained stochastic optimization problems where the
objective and constraint functions are convex and expressed as compositions of
stochastic functions. The problem arises in the context of fair classification,
fair regression, and the design of queuing systems. Of particular interest is
the large-scale setting where an oracle provides the stochastic gradients of
the constituent functions, and the goal is to solve the problem with a minimal
number of calls to the oracle. The problem arises in fair
classification/regression and in the design of queuing systems. Owing to the
compositional form, the stochastic gradients provided by the oracle do not
yield unbiased estimates of the objective or constraint gradients. Instead, we
construct approximate gradients by tracking the inner function evaluations,
resulting in a quasi-gradient saddle point algorithm. We prove that the
proposed algorithm is guaranteed to find the optimal and feasible solution
almost surely. We further establish that the proposed algorithm requires
$\mathcal{O}(1/\epsilon^4)$ data samples in order to obtain an
$\epsilon$-approximate optimal point while also ensuring zero constraint
violation. The result matches the sample complexity of the stochastic
compositional gradient descent method for unconstrained problems and improves
upon the best-known sample complexity results for the constrained settings. The
efficacy of the proposed algorithm is tested on both fair classification and
fair regression problems. The numerical results show that the proposed
algorithm outperforms the state-of-the-art algorithms in terms of the
convergence rate.
</p>
<a href="http://arxiv.org/abs/2012.09400" target="_blank">arXiv:2012.09400</a> [<a href="http://arxiv.org/pdf/2012.09400" target="_blank">pdf</a>]

<h2>Zoom-to-Inpaint: Image Inpainting with High Frequency Details. (arXiv:2012.09401v1 [cs.CV])</h2>
<h3>Soo Ye Kim, Kfir Aberman, Nori Kanazawa, Rahul Garg, Neal Wadhwa, Huiwen Chang, Nikhil Karnad, Munchurl Kim, Orly Liba</h3>
<p>Although deep learning has enabled a huge leap forward in image inpainting,
current methods are often unable to synthesize realistic high-frequency
details. In this paper, we propose applying super resolution to coarsely
reconstructed outputs, refining them at high resolution, and then downscaling
the output to the original resolution. By introducing high-resolution images to
the refinement network, our framework is able to reconstruct finer details that
are usually smoothed out due to spectral bias - the tendency of neural networks
to reconstruct low frequencies better than high frequencies. To assist training
the refinement network on large upscaled holes, we propose a progressive
learning technique in which the size of the missing regions increases as
training progresses. Our zoom-in, refine and zoom-out strategy, combined with
high-resolution supervision and progressive learning, constitutes a
framework-agnostic approach for enhancing high-frequency details that can be
applied to other inpainting methods. We provide qualitative and quantitative
evaluations along with an ablation analysis to show the effectiveness of our
approach, which outperforms state-of-the-art inpainting methods.
</p>
<a href="http://arxiv.org/abs/2012.09401" target="_blank">arXiv:2012.09401</a> [<a href="http://arxiv.org/pdf/2012.09401" target="_blank">pdf</a>]

<h2>LIGHTEN: Learning Interactions with Graph and Hierarchical TEmporal Networks for HOI in videos. (arXiv:2012.09402v1 [cs.CV])</h2>
<h3>Sai Praneeth Reddy Sunkesula, Rishabh Dabral, Ganesh Ramakrishnan</h3>
<p>Analyzing the interactions between humans and objects from a video includes
identification of the relationships between humans and the objects present in
the video. It can be thought of as a specialized version of Visual Relationship
Detection, wherein one of the objects must be a human. While traditional
methods formulate the problem as inference on a sequence of video segments, we
present a hierarchical approach, LIGHTEN, to learn visual features to
effectively capture spatio-temporal cues at multiple granularities in a video.
Unlike current approaches, LIGHTEN avoids using ground truth data like depth
maps or 3D human pose, thus increasing generalization across non-RGBD datasets
as well. Furthermore, we achieve the same using only the visual features,
instead of the commonly used hand-crafted spatial features. We achieve
state-of-the-art results in human-object interaction detection (88.9% and
92.6%) and anticipation tasks of CAD-120 and competitive results on image based
HOI detection in V-COCO dataset, setting a new benchmark for visual features
based approaches. Code for LIGHTEN is available at
https://github.com/praneeth11009/LIGHTEN-Learning-Interactions-with-Graphs-and-Hierarchical-TEmporal-Networks-for-HOI
</p>
<a href="http://arxiv.org/abs/2012.09402" target="_blank">arXiv:2012.09402</a> [<a href="http://arxiv.org/pdf/2012.09402" target="_blank">pdf</a>]

<h2>Joint Search of Data Augmentation Policies and Network Architectures. (arXiv:2012.09407v1 [cs.LG])</h2>
<h3>Taiga Kashima, Yoshihiro Yamada, Shunta Saito</h3>
<p>The common pipeline of training deep neural networks consists of several
building blocks such as data augmentation and network architecture selection.
AutoML is a research field that aims at automatically designing those parts,
but most methods explore each part independently because it is more challenging
to simultaneously search all the parts. In this paper, we propose a joint
optimization method for data augmentation policies and network architectures to
bring more automation to the design of training pipeline. The core idea of our
approach is to make the whole part differentiable. The proposed method combines
differentiable methods for augmentation policy search and network architecture
search to jointly optimize them in the end-to-end manner. The experimental
results show our method achieves competitive or superior performance to the
independently searched results.
</p>
<a href="http://arxiv.org/abs/2012.09407" target="_blank">arXiv:2012.09407</a> [<a href="http://arxiv.org/pdf/2012.09407" target="_blank">pdf</a>]

<h2>Temporal LiDAR Frame Prediction for Autonomous Driving. (arXiv:2012.09409v1 [cs.CV])</h2>
<h3>David Deng, Avideh Zakhor</h3>
<p>Anticipating the future in a dynamic scene is critical for many fields such
as autonomous driving and robotics. In this paper we propose a class of novel
neural network architectures to predict future LiDAR frames given previous
ones. Since the ground truth in this application is simply the next frame in
the sequence, we can train our models in a self-supervised fashion. Our
proposed architectures are based on FlowNet3D and Dynamic Graph CNN. We use
Chamfer Distance (CD) and Earth Mover's Distance (EMD) as loss functions and
evaluation metrics. We train and evaluate our models using the newly released
nuScenes dataset, and characterize their performance and complexity with
several baselines. Compared to directly using FlowNet3D, our proposed
architectures achieve CD and EMD nearly an order of magnitude lower. In
addition, we show that our predictions generate reasonable scene flow
approximations without using any labelled supervision.
</p>
<a href="http://arxiv.org/abs/2012.09409" target="_blank">arXiv:2012.09409</a> [<a href="http://arxiv.org/pdf/2012.09409" target="_blank">pdf</a>]

<h2>Computation-Efficient Knowledge Distillation via Uncertainty-Aware Mixup. (arXiv:2012.09413v1 [cs.CV])</h2>
<h3>Guodong Xu, Ziwei Liu, Chen Change Loy</h3>
<p>Knowledge distillation, which involves extracting the "dark knowledge" from a
teacher network to guide the learning of a student network, has emerged as an
essential technique for model compression and transfer learning. Unlike
previous works that focus on the accuracy of student network, here we study a
little-explored but important question, i.e., knowledge distillation
efficiency. Our goal is to achieve a performance comparable to conventional
knowledge distillation with a lower computation cost during training. We show
that the UNcertainty-aware mIXup (UNIX) can serve as a clean yet effective
solution. The uncertainty sampling strategy is used to evaluate the
informativeness of each training sample. Adaptive mixup is applied to uncertain
samples to compact knowledge. We further show that the redundancy of
conventional knowledge distillation lies in the excessive learning of easy
samples. By combining uncertainty and mixup, our approach reduces the
redundancy and makes better use of each query to the teacher network. We
validate our approach on CIFAR100 and ImageNet. Notably, with only 79%
computation cost, we outperform conventional knowledge distillation on CIFAR100
and achieve a comparable result on ImageNet.
</p>
<a href="http://arxiv.org/abs/2012.09413" target="_blank">arXiv:2012.09413</a> [<a href="http://arxiv.org/pdf/2012.09413" target="_blank">pdf</a>]

<h2>PanoNet3D: Combining Semantic and Geometric Understanding for LiDARPoint Cloud Detection. (arXiv:2012.09418v1 [cs.CV])</h2>
<h3>Xia Chen, Jianren Wang, David Held, Martial Hebert</h3>
<p>Visual data in autonomous driving perception, such as camera image and LiDAR
point cloud, can be interpreted as a mixture of two aspects: semantic feature
and geometric structure. Semantics come from the appearance and context of
objects to the sensor, while geometric structure is the actual 3D shape of
point clouds. Most detectors on LiDAR point clouds focus only on analyzing the
geometric structure of objects in real 3D space. Unlike previous works, we
propose to learn both semantic feature and geometric structure via a unified
multi-view framework. Our method exploits the nature of LiDAR scans -- 2D range
images, and applies well-studied 2D convolutions to extract semantic features.
By fusing semantic and geometric features, our method outperforms
state-of-the-art approaches in all categories by a large margin. The
methodology of combining semantic and geometric features provides a unique
perspective of looking at the problems in real-world 3D point cloud detection.
</p>
<a href="http://arxiv.org/abs/2012.09418" target="_blank">arXiv:2012.09418</a> [<a href="http://arxiv.org/pdf/2012.09418" target="_blank">pdf</a>]

<h2>Learning Fair Policies in Decentralized Cooperative Multi-Agent Reinforcement Learning. (arXiv:2012.09421v1 [cs.LG])</h2>
<h3>Matthieu Zimmer, Umer Siddique, Paul Weng</h3>
<p>We consider the problem of learning fair policies in (deep) cooperative
multi-agent reinforcement learning (MARL). We formalize it in a principled way
as the problem of optimizing a welfare function that explicitly encodes two
important aspects of fairness: efficiency and equity. As a solution method, we
propose a novel neural network architecture, which is composed of two
sub-networks specifically designed for taking into account the two aspects of
fairness. In experiments, we demonstrate the importance of the two sub-networks
for fair optimization. Our overall approach is general as it can accommodate
any (sub)differentiable welfare function. Therefore, it is compatible with
various notions of fairness that have been proposed in the literature (e.g.,
lexicographic maximin, generalized Gini social welfare function, proportional
fairness). Our solution method is generic and can be implemented in various
MARL settings: centralized training and decentralized execution, or fully
decentralized. Finally, we experimentally validate our approach in various
domains and show that it can perform much better than previous methods.
</p>
<a href="http://arxiv.org/abs/2012.09421" target="_blank">arXiv:2012.09421</a> [<a href="http://arxiv.org/pdf/2012.09421" target="_blank">pdf</a>]

<h2>The Variational Method of Moments. (arXiv:2012.09422v1 [cs.LG])</h2>
<h3>Andrew Bennett, Nathan Kallus</h3>
<p>The conditional moment problem is a powerful formulation for describing
structural causal parameters in terms of observables, a prominent example being
instrumental variable regression. A standard approach is to reduce the problem
to a finite set of marginal moment conditions and apply the optimally weighted
generalized method of moments (OWGMM), but this requires we know a finite set
of identifying moments, can still be inefficient even if identifying, or can be
unwieldy and impractical if we use a growing sieve of moments. Motivated by a
variational minimax reformulation of OWGMM, we define a very general class of
estimators for the conditional moment problem, which we term the variational
method of moments (VMM) and which naturally enables controlling infinitely-many
moments. We provide a detailed theoretical analysis of multiple VMM estimators,
including based on kernel methods and neural networks, and provide appropriate
conditions under which these estimators are consistent, asymptotically normal,
and semiparametrically efficient in the full conditional moment model. This is
in contrast to other recently proposed methods for solving conditional moment
problems based on adversarial machine learning, which do not incorporate
optimal weighting, do not establish asymptotic normality, and are not
semiparametrically efficient.
</p>
<a href="http://arxiv.org/abs/2012.09422" target="_blank">arXiv:2012.09422</a> [<a href="http://arxiv.org/pdf/2012.09422" target="_blank">pdf</a>]

<h2>Predicting Events In MOBA Games: Dataset, Attribution, and Evaluation. (arXiv:2012.09424v1 [cs.AI])</h2>
<h3>Zelong Yang, Yan Wang, Piji Li, Shaobin Lin, Shuming Shi, Shao-Lun Huang</h3>
<p>The multiplayer online battle arena (MOBA) games are becoming increasingly
popular in recent years. Consequently, many efforts have been devoted to
providing pre-game or in-game predictions for MOBA games. However, these works
are limited in the following two aspects: 1) the lack of sufficient in-game
features; 2) the absence of interpretability in the prediction results. These
two limitations greatly restrict their practical performances and industrial
applications. In this work, we collect and release a large-scale dataset
containing rich in-game features for the popular MOBA game Honor of Kings. We
then propose to predict four types of important events in an interpretable way
by attributing the predictions to the input features using two gradient-based
attribution methods: Integrated Gradients and SmoothGrad. To evaluate the
explanatory power of different models and attribution methods, a fidelity-based
evaluation metric is further proposed. Finally, we evaluate the accuracy and
Fidelity of several competitive methods on the collected dataset to assess how
well do machines predict the events in MOBA games.
</p>
<a href="http://arxiv.org/abs/2012.09424" target="_blank">arXiv:2012.09424</a> [<a href="http://arxiv.org/pdf/2012.09424" target="_blank">pdf</a>]

<h2>Characterizing the Evasion Attackability of Multi-label Classifiers. (arXiv:2012.09427v1 [cs.LG])</h2>
<h3>Zhuo Yang, Yufei Han, Xiangliang Zhang</h3>
<p>Evasion attack in multi-label learning systems is an interesting, widely
witnessed, yet rarely explored research topic. Characterizing the crucial
factors determining the attackability of the multi-label adversarial threat is
the key to interpret the origin of the adversarial vulnerability and to
understand how to mitigate it. Our study is inspired by the theory of
adversarial risk bound. We associate the attackability of a targeted
multi-label classifier with the regularity of the classifier and the training
data distribution. Beyond the theoretical attackability analysis, we further
propose an efficient empirical attackability estimator via greedy label space
exploration. It provides provably computational efficiency and approximation
accuracy. Substantial experimental results on real-world datasets validate the
unveiled attackability factors and the effectiveness of the proposed empirical
attackability indicator
</p>
<a href="http://arxiv.org/abs/2012.09427" target="_blank">arXiv:2012.09427</a> [<a href="http://arxiv.org/pdf/2012.09427" target="_blank">pdf</a>]

<h2>A Fast Algorithm for Heart Disease Prediction using Bayesian Network Model. (arXiv:2012.09429v1 [cs.LG])</h2>
<h3>Mistura Muibideen, Rajesh Prasad (Department of Computer Science African University of Science and Technology, Abuja, Nigeria)</h3>
<p>Cardiovascular disease is the number one cause of death all over the world.
Data mining can help to retrieve valuable knowledge from available data from
the health sector. It helps to train a model to predict patients' health which
will be faster as compared to clinical experimentation. Various implementation
of machine learning algorithms such as Logistic Regression, K-Nearest Neighbor,
Naive Bayes (NB), Support Vector Machine, etc. have been applied on Cleveland
heart datasets but there has been a limit to modeling using Bayesian Network
(BN). This research applied BN modeling to discover the relationship between 14
relevant attributes of the Cleveland heart data collected from The UCI
repository. The aim is to check how the dependency between attributes affects
the performance of the classifier. The BN produces a reliable and transparent
graphical representation between the attributes with the ability to predict new
scenarios. The model has an accuracy of 85%. It was concluded that the model
outperformed the NB classifier which has an accuracy of 80%.
</p>
<a href="http://arxiv.org/abs/2012.09429" target="_blank">arXiv:2012.09429</a> [<a href="http://arxiv.org/pdf/2012.09429" target="_blank">pdf</a>]

<h2>Helping Reduce Environmental Impact of Aviation with Machine Learning. (arXiv:2012.09433v1 [cs.AI])</h2>
<h3>Ashish Kapoor</h3>
<p>Commercial aviation is one of the biggest contributors towards climate
change. We propose to reduce environmental impact of aviation by considering
solutions that would reduce the flight time. Specifically, we first consider
improving winds aloft forecast so that flight planners could use better
information to find routes that are efficient. Secondly, we propose an aircraft
routing method that seeks to find the fastest route to the destination by
considering uncertainty in the wind forecasts and then optimally trading-off
between exploration and exploitation.
</p>
<a href="http://arxiv.org/abs/2012.09433" target="_blank">arXiv:2012.09433</a> [<a href="http://arxiv.org/pdf/2012.09433" target="_blank">pdf</a>]

<h2>Multi-shot Temporal Event Localization: a Benchmark. (arXiv:2012.09434v1 [cs.CV])</h2>
<h3>Xiaolong Liu (1), Yao Hu (2), Song Bai (2,3), Fei Ding (2), Xiang Bai (1), Philip H.S. Torr (3) ((1) Huazhong University of Science and Technology, (2) Alibaba Group, (3) University of Oxford)</h3>
<p>Current developments in temporal event or action localization usually target
actions captured by a single camera. However, extensive events or actions in
the wild may be captured as a sequence of shots by multiple cameras at
different positions.

In this paper, we propose a new and challenging task called multi-shot
temporal event localization, and accordingly, collect a large scale dataset
called MUlti-Shot EventS (MUSES). MUSES has 31,477 event instances for a total
of 716 video hours. The core nature of MUSES is the frequent shot cuts, for an
average of 19 shots per instance and 176 shots per video, which induces large
intrainstance variations. Our comprehensive evaluations show that the
state-of-the-art method in temporal action localization only achieves an mAP of
13.1% at IoU=0.5. As a minor contribution, we present a simple baseline
approach for handling the intra-instance variations, which reports an mAP of
18.9% on MUSES and 56.9% on THUMOS14 at IoU=0.5. To facilitate research in this
direction, we release the dataset and the project code at
https://songbai.site/muses.
</p>
<a href="http://arxiv.org/abs/2012.09434" target="_blank">arXiv:2012.09434</a> [<a href="http://arxiv.org/pdf/2012.09434" target="_blank">pdf</a>]

<h2>FG-Net: Fast Large-Scale LiDAR Point CloudsUnderstanding Network Leveraging CorrelatedFeature Mining and Geometric-Aware Modelling. (arXiv:2012.09439v1 [cs.CV])</h2>
<h3>Kangcheng Liu, Zhi Gao, Feng Lin, Ben M. Chen</h3>
<p>This work presents FG-Net, a general deep learning framework for large-scale
point clouds understanding without voxelizations, which achieves accurate and
real-time performance with a single NVIDIA GTX 1080 GPU. First, a novel noise
and outlier filtering method is designed to facilitate subsequent high-level
tasks. For effective understanding purpose, we propose a deep convolutional
neural network leveraging correlated feature mining and deformable convolution
based geometric-aware modelling, in which the local feature relationships and
geometric patterns can be fully exploited. For the efficiency issue, we put
forward an inverse density sampling operation and a feature pyramid based
residual learning strategy to save the computational cost and memory
consumption respectively. Extensive experiments on real-world challenging
datasets demonstrated that our approaches outperform state-of-the-art
approaches in terms of accuracy and efficiency. Moreover, weakly supervised
transfer learning is also conducted to demonstrate the generalization capacity
of our method.
</p>
<a href="http://arxiv.org/abs/2012.09439" target="_blank">arXiv:2012.09439</a> [<a href="http://arxiv.org/pdf/2012.09439" target="_blank">pdf</a>]

<h2>Learning to Share: A Multitasking Genetic Programming Approach to Image Feature Learning. (arXiv:2012.09444v1 [cs.CV])</h2>
<h3>Ying Bi, Bing Xue, Mengjie Zhang</h3>
<p>Evolutionary multitasking is a promising approach to simultaneously solving
multiple tasks with knowledge sharing. Image feature learning can be solved as
a multitasking problem because different tasks may have a similar feature
space. Genetic programming (GP) has been successfully applied to image feature
learning for classification. However, most of the existing GP methods solve one
task, independently, using sufficient training data. No multitasking GP method
has been developed for image feature learning. Therefore, this paper develops a
multitasking GP approach to image feature learning for classification with
limited training data. Owing to the flexible representation of GP, a new
knowledge sharing mechanism based on a new individual representation is
developed to allow GP to automatically learn what to share across two tasks.
The shared knowledge is encoded as a common tree, which can represent the
common/general features of two tasks. With the new individual representation,
each task is solved using the features extracted from a common tree and a
task-specific tree representing task-specific features. To learn the best
common and task-specific trees, a new evolutionary process and new fitness
functions are developed. The performance of the proposed approach is examined
on six multitasking problems of 12 image classification datasets with limited
training data and compared with three GP and 14 non-GP-based competitive
methods. Experimental results show that the new approach outperforms these
compared methods in almost all the comparisons. Further analysis reveals that
the new approach learns simple yet effective common trees with high
effectiveness and transferability.
</p>
<a href="http://arxiv.org/abs/2012.09444" target="_blank">arXiv:2012.09444</a> [<a href="http://arxiv.org/pdf/2012.09444" target="_blank">pdf</a>]

<h2>Stabilizing Q Learning Via Soft Mellowmax Operator. (arXiv:2012.09456v1 [cs.LG])</h2>
<h3>Yaozhong Gan, Zhe Zhang, Xiaoyang Tan</h3>
<p>Learning complicated value functions in high dimensional state space by
function approximation is a challenging task, partially due to that the
max-operator used in temporal difference updates can theoretically cause
instability for most linear or non-linear approximation schemes. Mellowmax is a
recently proposed differentiable and non-expansion softmax operator that allows
a convergent behavior in learning and planning. Unfortunately, the performance
bound for the fixed point it converges to remains unclear, and in practice, its
parameter is sensitive to various domains and has to be tuned case by case.
Finally, the Mellowmax operator may suffer from oversmoothing as it ignores the
probability being taken for each action when aggregating them. In this paper,
we address all the above issues with an enhanced Mellowmax operator, named SM2
(Soft Mellowmax). Particularly, the proposed operator is reliable, easy to
implement, and has provable performance guarantee, while preserving all the
advantages of Mellowmax. Furthermore, we show that our SM2 operator can be
applied to the challenging multi-agent reinforcement learning scenarios,
leading to stable value function approximation and state of the art
performance.
</p>
<a href="http://arxiv.org/abs/2012.09456" target="_blank">arXiv:2012.09456</a> [<a href="http://arxiv.org/pdf/2012.09456" target="_blank">pdf</a>]

<h2>Computational principles of intelligence: learning and reasoning with neural networks. (arXiv:2012.09477v1 [cs.AI])</h2>
<h3>Abel Torres Montoya</h3>
<p>Despite significant achievements and current interest in machine learning and
artificial intelligence, the quest for a theory of intelligence, allowing
general and efficient problem solving, has done little progress. This work
tries to contribute in this direction by proposing a novel framework of
intelligence based on three principles. First, the generative and mirroring
nature of learned representations of inputs. Second, a grounded, intrinsically
motivated and iterative process for learning, problem solving and imagination.
Third, an ad hoc tuning of the reasoning mechanism over causal compositional
representations using inhibition rules. Together, those principles create a
systems approach offering interpretability, continuous learning, common sense
and more. This framework is being developed from the following perspectives: as
a general problem solving method, as a human oriented tool and finally, as
model of information processing in the brain.
</p>
<a href="http://arxiv.org/abs/2012.09477" target="_blank">arXiv:2012.09477</a> [<a href="http://arxiv.org/pdf/2012.09477" target="_blank">pdf</a>]

<h2>CT Film Recovery via Disentangling Geometric Deformation and Illumination Variation: Simulated Datasets and Deep Models. (arXiv:2012.09491v1 [cs.CV])</h2>
<h3>Quan Quan, Qiyuan Wang, Liu Li, Yuanqi Du, S. Kevin Zhou</h3>
<p>While medical images such as computed tomography (CT) are stored in DICOM
format in hospital PACS, it is still quite routine in many countries to print a
film as a transferable medium for the purposes of self-storage and secondary
consultation. Also, with the ubiquitousness of mobile phone cameras, it is
quite common to take pictures of the CT films, which unfortunately suffer from
geometric deformation and illumination variation. In this work, we study the
problem of recovering a CT film, which marks the first attempt in the
literature, to the best of our knowledge. We start with building a large-scale
head CT film database CTFilm20K, consisting of approximately 20,000 pictures,
using the widely used computer graphics software Blender. We also record all
accompanying information related to the geometric deformation (such as 3D
coordinate, depth, normal, and UV maps) and illumination variation (such as
albedo map). Then we propose a deep framework to disentangle geometric
deformation and illumination variation using the multiple maps extracted from
the CT films to collaboratively guide the recovery process. Extensive
experiments on simulated and real images demonstrate the superiority of our
approach over the previous approaches. We plan to open source the simulated
images and deep models for promoting the research on CT film recovery
(https://anonymous.4open.science/r/e6b1f6e3-9b36-423f-a225-55b7d0b55523/).
</p>
<a href="http://arxiv.org/abs/2012.09491" target="_blank">arXiv:2012.09491</a> [<a href="http://arxiv.org/pdf/2012.09491" target="_blank">pdf</a>]

<h2>Exploiting Learnable Joint Groups for Hand Pose Estimation. (arXiv:2012.09496v1 [cs.CV])</h2>
<h3>Moran Li, Yuan Gao, Nong Sang</h3>
<p>In this paper, we propose to estimate 3D hand pose by recovering the 3D
coordinates of joints in a group-wise manner, where less-related joints are
automatically categorized into different groups and exhibit different features.
This is different from the previous methods where all the joints are considered
holistically and share the same feature. The benefits of our method are
illustrated by the principle of multi-task learning (MTL), i.e., by separating
less-related joints into different groups (as different tasks), our method
learns different features for each of them, therefore efficiently avoids the
negative transfer (among less related tasks/groups of joints). The key of our
method is a novel binary selector that automatically selects related joints
into the same group. We implement such a selector with binary values
stochastically sampled from a Concrete distribution, which is constructed using
Gumbel softmax on trainable parameters. This enables us to preserve the
differentiable property of the whole network. We further exploit features from
those less-related groups by carrying out an additional feature fusing scheme
among them, to learn more discriminative features. This is realized by
implementing multiple 1x1 convolutions on the concatenated features, where each
joint group contains a unique 1x1 convolution for feature fusion. The detailed
ablation analysis and the extensive experiments on several benchmark datasets
demonstrate the promising performance of the proposed method over the
state-of-the-art (SOTA) methods. Besides, our method achieves top-1 among all
the methods that do not exploit the dense 3D shape labels on the most recently
released FreiHAND competition at the submission date. The source code and
models are available at https://github.com/ moranli-aca/LearnableGroups-Hand.
</p>
<a href="http://arxiv.org/abs/2012.09496" target="_blank">arXiv:2012.09496</a> [<a href="http://arxiv.org/pdf/2012.09496" target="_blank">pdf</a>]

<h2>A Hierarchical Feature Constraint to Camouflage Medical Adversarial Attacks. (arXiv:2012.09501v1 [cs.CV])</h2>
<h3>Qingsong Yao, Zecheng He, Yi Lin, Kai Ma, Yefeng Zheng, S. Kevin Zhou</h3>
<p>Deep neural networks (DNNs) for medical images are extremely vulnerable to
adversarial examples (AEs), which poses security concerns on clinical decision
making. Luckily, medical AEs are also easy to detect in hierarchical feature
space per our study herein. To better understand this phenomenon, we thoroughly
investigate the intrinsic characteristic of medical AEs in feature space,
providing both empirical evidence and theoretical explanations for the
question: why are medical adversarial attacks easy to detect? We first perform
a stress test to reveal the vulnerability of deep representations of medical
images, in contrast to natural images. We then theoretically prove that typical
adversarial attacks to binary disease diagnosis network manipulate the
prediction by continuously optimizing the vulnerable representations in a fixed
direction, resulting in outlier features that make medical AEs easy to detect.
However, this vulnerability can also be exploited to hide the AEs in the
feature space. We propose a novel hierarchical feature constraint (HFC) as an
add-on to existing adversarial attacks, which encourages the hiding of the
adversarial representation within the normal feature distribution. We evaluate
the proposed method on two public medical image datasets, namely {Fundoscopy}
and {Chest X-Ray}. Experimental results demonstrate the superiority of our
adversarial attack method as it bypasses an array of state-of-the-art
adversarial detectors more easily than competing attack methods, supporting
that the great vulnerability of medical features allows an attacker more room
to manipulate the adversarial representations.
</p>
<a href="http://arxiv.org/abs/2012.09501" target="_blank">arXiv:2012.09501</a> [<a href="http://arxiv.org/pdf/2012.09501" target="_blank">pdf</a>]

<h2>Embodied Visual Active Learning for Semantic Segmentation. (arXiv:2012.09503v1 [cs.CV])</h2>
<h3>David Nilsson, Aleksis Pirinen, Erik G&#xe4;rtner, Cristian Sminchisescu</h3>
<p>We study the task of embodied visual active learning, where an agent is set
to explore a 3d environment with the goal to acquire visual scene understanding
by actively selecting views for which to request annotation. While accurate on
some benchmarks, today's deep visual recognition pipelines tend to not
generalize well in certain real-world scenarios, or for unusual viewpoints.
Robotic perception, in turn, requires the capability to refine the recognition
capabilities for the conditions where the mobile system operates, including
cluttered indoor environments or poor illumination. This motivates the proposed
task, where an agent is placed in a novel environment with the objective of
improving its visual recognition capability. To study embodied visual active
learning, we develop a battery of agents - both learnt and pre-specified - and
with different levels of knowledge of the environment. The agents are equipped
with a semantic segmentation network and seek to acquire informative views,
move and explore in order to propagate annotations in the neighbourhood of
those views, then refine the underlying segmentation network by online
retraining. The trainable method uses deep reinforcement learning with a reward
function that balances two competing objectives: task performance, represented
as visual recognition accuracy, which requires exploring the environment, and
the necessary amount of annotated data requested during active exploration. We
extensively evaluate the proposed models using the photorealistic Matterport3D
simulator and show that a fully learnt method outperforms comparable
pre-specified counterparts, even when requesting fewer annotations.
</p>
<a href="http://arxiv.org/abs/2012.09503" target="_blank">arXiv:2012.09503</a> [<a href="http://arxiv.org/pdf/2012.09503" target="_blank">pdf</a>]

<h2>Experts with Lower-Bounded Loss Feedback: A Unifying Framework. (arXiv:2012.09537v1 [cs.LG])</h2>
<h3>Eyal Gofer, Guy Gilboa</h3>
<p>The most prominent feedback models for the best expert problem are the full
information and bandit models. In this work we consider a simple feedback model
that generalizes both, where on every round, in addition to a bandit feedback,
the adversary provides a lower bound on the loss of each expert. Such lower
bounds may be obtained in various scenarios, for instance, in stock trading or
in assessing errors of certain measurement devices. For this model we prove
optimal regret bounds (up to logarithmic factors) for modified versions of
Exp3, generalizing algorithms and bounds both for the bandit and the
full-information settings. Our second-order unified regret analysis simulates a
two-step loss update and highlights three Hessian or Hessian-like expressions,
which map to the full-information regret, bandit regret, and a hybrid of both.
Our results intersect with those for bandits with graph-structured feedback, in
that both settings can accommodate feedback from an arbitrary subset of experts
on each round. However, our model also accommodates partial feedback at the
single-expert level, by allowing non-trivial lower bounds on each loss.
</p>
<a href="http://arxiv.org/abs/2012.09537" target="_blank">arXiv:2012.09537</a> [<a href="http://arxiv.org/pdf/2012.09537" target="_blank">pdf</a>]

<h2>Weakly-Supervised Action Localization and Action Recognition using Global-Local Attention of 3D CNN. (arXiv:2012.09542v1 [cs.CV])</h2>
<h3>Novanto Yudistira, Muthu Subash Kavitha, Takio Kurita</h3>
<p>3D Convolutional Neural Network (3D CNN) captures spatial and temporal
information on 3D data such as video sequences. However, due to the convolution
and pooling mechanism, the information loss seems unavoidable. To improve the
visual explanations and classification in 3D CNN, we propose two approaches; i)
aggregate layer-wise global to local (global-local) discrete gradients using
trained 3DResNext network, and ii) implement attention gating network to
improve the accuracy of the action recognition. The proposed approach intends
to show the usefulness of every layer termed as global-local attention in 3D
CNN via visual attribution, weakly-supervised action localization, and action
recognition. Firstly, the 3DResNext is trained and applied for action
classification using backpropagation concerning the maximum predicted class.
The gradients and activations of every layer are then up-sampled. Later,
aggregation is used to produce more nuanced attention, which points out the
most critical part of the predicted class's input videos. We use contour
thresholding of final attention for final localization. We evaluate spatial and
temporal action localization in trimmed videos using fine-grained visual
explanation via 3DCam. Experimental results show that the proposed approach
produces informative visual explanations and discriminative attention.
Furthermore, the action recognition via attention gating on each layer produces
better classification results than the baseline model.
</p>
<a href="http://arxiv.org/abs/2012.09542" target="_blank">arXiv:2012.09542</a> [<a href="http://arxiv.org/pdf/2012.09542" target="_blank">pdf</a>]

<h2>Few-shot Sequence Learning with Transformers. (arXiv:2012.09543v1 [cs.LG])</h2>
<h3>Lajanugen Logeswaran, Ann Lee, Myle Ott, Honglak Lee, Marc&#x27;Aurelio Ranzato, Arthur Szlam</h3>
<p>Few-shot algorithms aim at learning new tasks provided only a handful of
training examples. In this work we investigate few-shot learning in the setting
where the data points are sequences of tokens and propose an efficient learning
algorithm based on Transformers. In the simplest setting, we append a token to
an input sequence which represents the particular task to be undertaken, and
show that the embedding of this token can be optimized on the fly given few
labeled examples. Our approach does not require complicated changes to the
model architecture such as adapter layers nor computing second order
derivatives as is currently popular in the meta-learning and few-shot learning
literature. We demonstrate our approach on a variety of tasks, and analyze the
generalization properties of several model variants and baseline approaches. In
particular, we show that compositional task descriptors can improve
performance. Experiments show that our approach works at least as well as other
methods, while being more computationally efficient.
</p>
<a href="http://arxiv.org/abs/2012.09543" target="_blank">arXiv:2012.09543</a> [<a href="http://arxiv.org/pdf/2012.09543" target="_blank">pdf</a>]

<h2>Estimating mixed-memberships using the Symmetric Laplacian Inverse Matrix. (arXiv:2012.09561v1 [stat.ML])</h2>
<h3>Huan Qing, Jingli Wang</h3>
<p>Community detection has been well studied in network analysis, and one
popular technique is spectral clustering which is fast and statistically
analyzable for detect-ing clusters for given networks. But the more realistic
case of mixed membership community detection remains a challenge. In this
paper, we propose a new spectral clustering method Mixed-SLIM for mixed
membership community detection. Mixed-SLIM is designed based on the symmetrized
Laplacian inverse matrix (SLIM) (Jing et al. 2021) under the degree-corrected
mixed membership (DCMM) model. We show that this algorithm and its regularized
version Mixed-SLIM {\tau} are asymptotically consistent under mild conditions.
Meanwhile, we provide Mixed-SLIM appro and its regularized version Mixed-SLIM
{\tau}appro by approximating the SLIM matrix when dealing with large networks
in practice. These four Mixed-SLIM methods outperform state-of-art methods in
simulations and substantial empirical datasets for both community detection and
mixed membership community detection problems.
</p>
<a href="http://arxiv.org/abs/2012.09561" target="_blank">arXiv:2012.09561</a> [<a href="http://arxiv.org/pdf/2012.09561" target="_blank">pdf</a>]

<h2>Incremental Learning from Low-labelled Stream Data in Open-Set Video Face Recognition. (arXiv:2012.09571v1 [cs.CV])</h2>
<h3>Eric Lopez-Lopez, Carlos V. Regueiro, Xose M. Pardo</h3>
<p>Deep Learning approaches have brought solutions, with impressive performance,
to general classification problems where wealthy of annotated data are provided
for training. In contrast, less progress has been made in continual learning of
a set of non-stationary classes, mainly when applied to unsupervised problems
with streaming data.

Here, we propose a novel incremental learning approach which combines a deep
features encoder with an Open-Set Dynamic Ensembles of SVM, to tackle the
problem of identifying individuals of interest (IoI) from streaming face data.
From a simple weak classifier trained on a few video-frames, our method can use
unsupervised operational data to enhance recognition. Our approach adapts to
new patterns avoiding catastrophic forgetting and partially heals itself from
miss-adaptation. Besides, to better comply with real world conditions, the
system was designed to operate in an open-set setting. Results show a benefit
of up to 15% F1-score increase respect to non-adaptive state-of-the-art
methods.
</p>
<a href="http://arxiv.org/abs/2012.09571" target="_blank">arXiv:2012.09571</a> [<a href="http://arxiv.org/pdf/2012.09571" target="_blank">pdf</a>]

<h2>Trajectory saliency detection using consistency-oriented latent codes from a recurrent auto-encoder. (arXiv:2012.09573v1 [cs.CV])</h2>
<h3>L. Maczyta, P. Bouthemy, O. Le Meur</h3>
<p>In this paper, we are concerned with the detection of progressive dynamic
saliency from video sequences. More precisely, we are interested in saliency
related to motion and likely to appear progressively over time. It can be
relevant to trigger alarms, to dedicate additional processing or to detect
specific events. Trajectories represent the best way to support progressive
dynamic saliency detection. Accordingly, we will talk about trajectory
saliency. A trajectory will be qualified as salient if it deviates from normal
trajectories that share a common motion pattern related to a given context.
First, we need a compact while discriminative representation of trajectories.
We adopt a (nearly) unsupervised learning-based approach. The latent code
estimated by a recurrent auto-encoder provides the desired representation. In
addition, we enforce consistency for normal (similar) trajectories through the
auto-encoder loss function. The distance of the trajectory code to a prototype
code accounting for normality is the means to detect salient trajectories. We
validate our trajectory saliency detection method on synthetic and real
trajectory datasets, and highlight the contributions of its different
components. We show that our method outperforms existing methods on several
scenarios drawn from the publicly available dataset of pedestrian trajectories
acquired in a railway station (Alahi 2014).
</p>
<a href="http://arxiv.org/abs/2012.09573" target="_blank">arXiv:2012.09573</a> [<a href="http://arxiv.org/pdf/2012.09573" target="_blank">pdf</a>]

<h2>Task Uncertainty Loss Reduce Negative Transfer in Asymmetric Multi-task Feature Learning. (arXiv:2012.09575v1 [cs.LG])</h2>
<h3>Rafael Peres da Silva, Chayaporn Suphavilai, Niranjan Nagarajan</h3>
<p>Multi-task learning (MTL) is frequently used in settings where a target task
has to be learnt based on limited training data, but knowledge can be leveraged
from related auxiliary tasks. While MTL can improve task performance overall
relative to single-task learning (STL), these improvements can hide negative
transfer (NT), where STL may deliver better performance for many individual
tasks. Asymmetric multitask feature learning (AMTFL) is an approach that tries
to address this by allowing tasks with higher loss values to have smaller
influence on feature representations for learning other tasks. Task loss values
do not necessarily indicate reliability of models for a specific task. We
present examples of NT in two orthogonal datasets (image recognition and
pharmacogenomics) and tackle this challenge by using aleatoric homoscedastic
uncertainty to capture the relative confidence between tasks, and set weights
for task loss. Our results show that this approach reduces NT providing a new
approach to enable robust MTL.
</p>
<a href="http://arxiv.org/abs/2012.09575" target="_blank">arXiv:2012.09575</a> [<a href="http://arxiv.org/pdf/2012.09575" target="_blank">pdf</a>]

<h2>Sensitive Data Detection with High-Throughput Neural Network Models for Financial Institutions. (arXiv:2012.09597v1 [cs.LG])</h2>
<h3>Anh Truong, Austin Walters, Jeremy Goodsitt</h3>
<p>Named Entity Recognition has been extensively investigated in many fields.
However, the application of sensitive entity detection for production systems
in financial institutions has not been well explored due to the lack of
publicly available, labeled datasets. In this paper, we use internal and
synthetic datasets to evaluate various methods of detecting NPI (Nonpublic
Personally Identifiable) information commonly found within financial
institutions, in both unstructured and structured data formats. Character-level
neural network models including CNN, LSTM, BiLSTM-CRF, and CNN-CRF are
investigated on two prediction tasks: (i) entity detection on multiple data
formats, and (ii) column-wise entity prediction on tabular datasets. We compare
these models with other standard approaches on both real and synthetic data,
with respect to F1-score, precision, recall, and throughput. The real datasets
include internal structured data and public email data with manually tagged
labels. Our experimental results show that the CNN model is simple yet
effective with respect to accuracy and throughput and thus, is the most
suitable candidate model to be deployed in the production environment(s).
Finally, we provide several lessons learned on data limitations, data labelling
and the intrinsic overlap of data entities.
</p>
<a href="http://arxiv.org/abs/2012.09597" target="_blank">arXiv:2012.09597</a> [<a href="http://arxiv.org/pdf/2012.09597" target="_blank">pdf</a>]

<h2>Deep Fusion Clustering Network. (arXiv:2012.09600v1 [cs.LG])</h2>
<h3>Wenxuan Tu, Sihang Zhou, Xinwang Liu, Xifeng Guo, Zhiping Cai, En zhu, Jieren Cheng</h3>
<p>Deep clustering is a fundamental yet challenging task for data analysis.
Recently we witness a strong tendency of combining autoencoder and graph neural
networks to exploit structure information for clustering performance
enhancement. However, we observe that existing literature 1) lacks a dynamic
fusion mechanism to selectively integrate and refine the information of graph
structure and node attributes for consensus representation learning; 2) fails
to extract information from both sides for robust target distribution (i.e.,
"groundtruth" soft labels) generation. To tackle the above issues, we propose a
Deep Fusion Clustering Network (DFCN). Specifically, in our network, an
interdependency learning-based Structure and Attribute Information Fusion
(SAIF) module is proposed to explicitly merge the representations learned by an
autoencoder and a graph autoencoder for consensus representation learning.
Also, a reliable target distribution generation measure and a triplet
self-supervision strategy, which facilitate cross-modality information
exploitation, are designed for network training. Extensive experiments on six
benchmark datasets have demonstrated that the proposed DFCN consistently
outperforms the state-of-the-art deep clustering methods.
</p>
<a href="http://arxiv.org/abs/2012.09600" target="_blank">arXiv:2012.09600</a> [<a href="http://arxiv.org/pdf/2012.09600" target="_blank">pdf</a>]

<h2>Application of the Neural Network Dependability Kit in Real-World Environments. (arXiv:2012.09602v1 [cs.LG])</h2>
<h3>Amit Sahu, Noelia V&#xe1;llez, Rosana Rodr&#xed;guez-Bobada, Mohamad Alhaddad, Omar Moured, Georg Neugschwandtner</h3>
<p>In this paper, we provide a guideline for using the Neural Network
Dependability Kit (NNDK) during the development process of NN models, and show
how the algorithm is applied in two image classification use cases. The case
studies demonstrate the usage of the dependability kit to obtain insights about
the NN model and how they informed the development process of the neural
network model. After interpreting neural networks via the different metrics
available in the NNDK, the developers were able to increase the NNs' accuracy,
trust the developed networks, and make them more robust. In addition, we
obtained a novel application-oriented technique to provide supporting evidence
for an NN's classification result to the user. In the medical image
classification use case, it was used to retrieve case images from the training
dataset that were similar to the current patient's image and could therefore
act as a support for the NN model's decision and aid doctors in interpreting
the results.
</p>
<a href="http://arxiv.org/abs/2012.09602" target="_blank">arXiv:2012.09602</a> [<a href="http://arxiv.org/pdf/2012.09602" target="_blank">pdf</a>]

<h2>Sparsifying networks by traversing Geodesics. (arXiv:2012.09605v1 [cs.LG])</h2>
<h3>Guruprasad Raghavan, Matt Thomson</h3>
<p>The geometry of weight spaces and functional manifolds of neural networks
play an important role towards 'understanding' the intricacies of ML. In this
paper, we attempt to solve certain open questions in ML, by viewing them
through the lens of geometry, ultimately relating it to the discovery of points
or paths of equivalent function in these spaces. We propose a mathematical
framework to evaluate geodesics in the functional space, to find
high-performance paths from a dense network to its sparser counterpart. Our
results are obtained on VGG-11 trained on CIFAR-10 and MLP's trained on MNIST.
Broadly, we demonstrate that the framework is general, and can be applied to a
wide variety of problems, ranging from sparsification to alleviating
catastrophic forgetting.
</p>
<a href="http://arxiv.org/abs/2012.09605" target="_blank">arXiv:2012.09605</a> [<a href="http://arxiv.org/pdf/2012.09605" target="_blank">pdf</a>]

<h2>Kernelized Classification in Deep Networks. (arXiv:2012.09607v1 [cs.LG])</h2>
<h3>Sadeep Jayasumana, Srikumar Ramalingam, Sanjiv Kumar</h3>
<p>In this paper, we propose a kernelized classification layer for deep
networks. Although conventional deep networks introduce an abundance of
nonlinearity for representation (feature) learning, they almost universally use
a linear classifier on the learned feature vectors. We introduce a nonlinear
classification layer by using the kernel trick on the softmax cross-entropy
loss function during training and the scorer function during testing.
Furthermore, we study the choice of kernel functions one could use with this
framework and show that the optimal kernel function for a given problem can be
learned automatically within the deep network itself using the usual
backpropagation and gradient descent methods. To this end, we exploit a classic
mathematical result on the positive definite kernels on the unit n-sphere
embedded in the (n+1)-dimensional Euclidean space. We show the usefulness of
the proposed nonlinear classification layer on several vision datasets and
tasks.
</p>
<a href="http://arxiv.org/abs/2012.09607" target="_blank">arXiv:2012.09607</a> [<a href="http://arxiv.org/pdf/2012.09607" target="_blank">pdf</a>]

<h2>Cost-sensitive Hierarchical Clustering for Dynamic Classifier Selection. (arXiv:2012.09608v1 [cs.LG])</h2>
<h3>Meinolf Sellman, Tapan Shah</h3>
<p>We consider the dynamic classifier selection (DCS) problem: Given an ensemble
of classifiers, we are to choose which classifier to use depending on the
particular input vector that we get to classify. The problem is a special case
of the general algorithm selection problem where we have multiple different
algorithms we can employ to process a given input. We investigate if a method
developed for general algorithm selection named cost-sensitive hierarchical
clustering (CSHC) is suited for DCS. We introduce some additions to the
original CSHC method for the special case of choosing a classification
algorithm and evaluate their impact on performance. We then compare with a
number of state-of-the-art dynamic classifier selection methods. Our
experimental results show that our modified CSHC algorithm compares favorably
</p>
<a href="http://arxiv.org/abs/2012.09608" target="_blank">arXiv:2012.09608</a> [<a href="http://arxiv.org/pdf/2012.09608" target="_blank">pdf</a>]

<h2>Draw your Neural Networks. (arXiv:2012.09609v1 [cs.LG])</h2>
<h3>Jatin Sharma, Shobha Lata</h3>
<p>Deep Neural Networks are the basic building blocks of modern Artificial
Intelligence. They are increasingly replacing or augmenting existing software
systems due to their ability to learn directly from the data and superior
accuracy on variety of tasks. Existing Software Development Life Cycle (SDLC)
methodologies fall short on representing the unique capabilities and
requirements of AI Development and must be replaced with Artificial
Intelligence Development Life Cycle (AIDLC) methodologies. In this paper, we
discuss an alternative and more natural approach to develop neural networks
that involves intuitive GUI elements such as blocks and lines to draw them
instead of complex computer programming. We present Sketch framework, that uses
this GUI-based approach to design and modify the neural networks and provides
interoperability with traditional frameworks. The system provides popular
layers and operations out-of-the-box and could import any supported pre-trained
model making it a faster method to design and train complex neural networks and
ultimately democratizing the AI by removing the learning curve.
</p>
<a href="http://arxiv.org/abs/2012.09609" target="_blank">arXiv:2012.09609</a> [<a href="http://arxiv.org/pdf/2012.09609" target="_blank">pdf</a>]

<h2>Validate and Enable Machine Learning in Industrial AI. (arXiv:2012.09610v1 [cs.LG])</h2>
<h3>Hongbo Zou, Guangjing Chen, Pengtao Xie, Sean Chen, Yongtian He, Hochih Huang, Zheng Nie, Hongbao Zhang, Tristan Bala, Kazi Tulip, Yuqi Wang, Shenlin Qin, Eric P. Xing</h3>
<p>Industrial Artificial Intelligence (Industrial AI) is an emerging concept
which refers to the application of artificial intelligence to industry.
Industrial AI promises more efficient future industrial control systems.
However, manufacturers and solution partners need to understand how to
implement and integrate an AI model into the existing industrial control
system. A well-trained machine learning (ML) model provides many benefits and
opportunities for industrial control optimization; however, an inferior
Industrial AI design and integration limits the capability of ML models. To
better understand how to develop and integrate trained ML models into the
traditional industrial control system, test the deployed AI control system, and
ultimately outperform traditional systems, manufacturers and their AI solution
partners need to address a number of challenges. Six top challenges, which were
real problems we ran into when deploying Industrial AI, are explored in the
paper. The Petuum Optimum system is used as an example to showcase the
challenges in making and testing AI models, and more importantly, how to
address such challenges in an Industrial AI system.
</p>
<a href="http://arxiv.org/abs/2012.09610" target="_blank">arXiv:2012.09610</a> [<a href="http://arxiv.org/pdf/2012.09610" target="_blank">pdf</a>]

<h2>Efficient Exploration for Model-based Reinforcement Learning with Continuous States and Actions. (arXiv:2012.09613v1 [cs.LG])</h2>
<h3>Ying Fan, Yifei Ming</h3>
<p>Balancing exploration and exploitation is crucial in reinforcement learning
(RL). In this paper, we study the model-based posterior sampling algorithm in
continuous state-action spaces theoretically and empirically. First, we improve
the regret bound: with the assumption that reward and transition functions can
be modeled as Gaussian Processes with linear kernels, we develop a Bayesian
regret bound of $\tilde{O}(H^{3/2}d\sqrt{T})$, where $H$ is the episode length,
$d$ is the dimension of the state-action space, and $T$ indicates the total
time steps. Our bound can be extended to nonlinear cases as well: using linear
kernels on the feature representation $\phi$, the Bayesian regret bound becomes
$\tilde{O}(H^{3/2}d_{\phi}\sqrt{T})$, where $d_\phi$ is the dimension of the
representation space. Moreover, we present MPC-PSRL, a model-based posterior
sampling algorithm with model predictive control for action selection. To
capture the uncertainty in models and realize posterior sampling, we use
Bayesian linear regression on the penultimate layer (the feature representation
layer $\phi$) of neural networks. Empirical results show that our algorithm
achieves the best sample efficiency in benchmark control tasks compared to
prior model-based algorithms, and matches the asymptotic performance of
model-free algorithms.
</p>
<a href="http://arxiv.org/abs/2012.09613" target="_blank">arXiv:2012.09613</a> [<a href="http://arxiv.org/pdf/2012.09613" target="_blank">pdf</a>]

<h2>Who is more ready to get back in shape?. (arXiv:2012.09617v1 [cs.LG])</h2>
<h3>Rajius Idzalika</h3>
<p>This empirical study estimates resilience (adaptive capacity) around the
periods of the 2013 heavy flood in Cambodia. We use nearly 1.2 million
microfinance institution (MFI) customer data and implement the unsupervised
learning method. Our results highlight the opportunity to develop resilience by
having a better understanding of which areas are likely to be more or less
resilient based on the characteristics of the MFI customers, and the individual
choices or situations that support stronger adaptiveness. We also discuss the
limitation of this approach.
</p>
<a href="http://arxiv.org/abs/2012.09617" target="_blank">arXiv:2012.09617</a> [<a href="http://arxiv.org/pdf/2012.09617" target="_blank">pdf</a>]

<h2>Learning to Solve AC Optimal Power Flow by Differentiating through Holomorphic Embeddings. (arXiv:2012.09622v1 [cs.LG])</h2>
<h3>Henning Lange, Bingqing Chen, Mario Berges, Soummya Kar</h3>
<p>Alternating current optimal power flow (AC-OPF) is one of the fundamental
problems in power systems operation. AC-OPF is traditionally cast as a
constrained optimization problem that seeks optimal generation set points
whilst fulfilling a set of non-linear equality constraints -- the power flow
equations. With increasing penetration of renewable generation, grid operators
need to solve larger problems at shorter intervals. This motivates the research
interest in learning OPF solutions with neural networks, which have fast
inference time and is potentially scalable to large networks. The main
difficulty in solving the AC-OPF problem lies in dealing with this equality
constraint that has spurious roots, i.e. there are assignments of voltages that
fulfill the power flow equations that however are not physically realizable.
This property renders any method relying on projected-gradients brittle because
these non-physical roots can act as attractors. In this paper, we show
efficient strategies that circumvent this problem by differentiating through
the operations of a power flow solver that embeds the power flow equations into
a holomorphic function. The resulting learning-based approach is validated
experimentally on a 200-bus system and we show that, after training, the
learned agent produces optimized power flow solutions reliably and fast.
Specifically, we report a 12x increase in speed and a 40% increase in
robustness compared to a traditional solver. To the best of our knowledge, this
approach constitutes the first learning-based approach that successfully
respects the full non-linear AC-OPF equations.
</p>
<a href="http://arxiv.org/abs/2012.09622" target="_blank">arXiv:2012.09622</a> [<a href="http://arxiv.org/pdf/2012.09622" target="_blank">pdf</a>]

<h2>Predictive K-means with local models. (arXiv:2012.09630v1 [cs.LG])</h2>
<h3>Vincent Lemaire, Oumaima Alaoui Ismaili, Antoine Cornu&#xe9;jols, Dominique Gay</h3>
<p>Supervised classification can be effective for prediction but sometimes weak
on interpretability or explainability (XAI). Clustering, on the other hand,
tends to isolate categories or profiles that can be meaningful but there is no
guarantee that they are useful for labels prediction. Predictive clustering
seeks to obtain the best of the two worlds. Starting from labeled data, it
looks for clusters that are as pure as possible with regards to the class
labels. One technique consists in tweaking a clustering algorithm so that data
points sharing the same label tend to aggregate together. With distance-based
algorithms, such as k-means, a solution is to modify the distance used by the
algorithm so that it incorporates information about the labels of the data
points. In this paper, we propose another method which relies on a change of
representation guided by class densities and then carries out clustering in
this new representation space. We present two new algorithms using this
technique and show on a variety of data sets that they are competitive for
prediction performance with pure supervised classifiers while offering
interpretability of the clusters discovered.
</p>
<a href="http://arxiv.org/abs/2012.09630" target="_blank">arXiv:2012.09630</a> [<a href="http://arxiv.org/pdf/2012.09630" target="_blank">pdf</a>]

<h2>Learning active learning at the crossroads? evaluation and discussion. (arXiv:2012.09631v1 [cs.LG])</h2>
<h3>Louis Desreumaux, Vincent Lemaire</h3>
<p>Active learning aims to reduce annotation cost by predicting which samples
are useful for a human expert to label. Although this field is quite old,
several important challenges to using active learning in real-world settings
still remain unsolved. In particular, most selection strategies are
hand-designed, and it has become clear that there is no best active learning
strategy that consistently outperforms all others in all applications. This has
motivated research into meta-learning algorithms for "learning how to actively
learn". In this paper, we compare this kind of approach with the association of
a Random Forest with the margin sampling strategy, reported in recent
comparative studies as a very competitive heuristic. To this end, we present
the results of a benchmark performed on 20 datasets that compares a strategy
learned using a recent meta-learning algorithm with margin sampling. We also
present some lessons learned and open future perspectives.
</p>
<a href="http://arxiv.org/abs/2012.09631" target="_blank">arXiv:2012.09631</a> [<a href="http://arxiv.org/pdf/2012.09631" target="_blank">pdf</a>]

<h2>From Weakly Supervised Learning to Biquality Learning, a brief introduction. (arXiv:2012.09632v1 [cs.LG])</h2>
<h3>Pierre Nodet, Vincent Lemaire, Alexis Bondu, Antoine Cornu&#xe9;jols, Adam Ouorou</h3>
<p>The field of Weakly Supervised Learning (WSL) has recently seen a surge of
popularity, with numerous papers addressing different types of "supervision
deficiencies". In WSL use cases, a variety of situations exists where the
collected "information" is imperfect. The paradigm of WSL attempts to list and
cover these problems with associated solutions. In this paper, we review the
research progress on WSL with the aim to make it as a brief introduction to
this field. We present the three axis of WSL cube and an overview of most of
all the elements of their facets. We propose three measurable quantities that
acts as coordinates in the previously defined cube namely: Quality,
Adaptability and Quantity of information. Thus we suggest that Biquality
Learning framework can be defined as a plan of the WSL cube and propose to
re-discover previously unrelated patches in WSL literature as a unified
Biquality Learning literature.
</p>
<a href="http://arxiv.org/abs/2012.09632" target="_blank">arXiv:2012.09632</a> [<a href="http://arxiv.org/pdf/2012.09632" target="_blank">pdf</a>]

<h2>XAI-P-T: A Brief Review of Explainable Artificial Intelligence from Practice to Theory. (arXiv:2012.09636v1 [cs.AI])</h2>
<h3>Nazanin Fouladgar, Kary Fr&#xe4;mling</h3>
<p>In this work, we report the practical and theoretical aspects of Explainable
AI (XAI) identified in some fundamental literature. Although there is a vast
body of work on representing the XAI backgrounds, most of the corpuses pinpoint
a discrete direction of thoughts. Providing insights into literature in
practice and theory concurrently is still a gap in this field. This is
important as such connection facilitates a learning process for the early stage
XAI researchers and give a bright stand for the experienced XAI scholars.
Respectively, we first focus on the categories of black-box explanation and
give a practical example. Later, we discuss how theoretically explanation has
been grounded in the body of multidisciplinary fields. Finally, some directions
of future works are presented.
</p>
<a href="http://arxiv.org/abs/2012.09636" target="_blank">arXiv:2012.09636</a> [<a href="http://arxiv.org/pdf/2012.09636" target="_blank">pdf</a>]

<h2>Spatial-Temporal Fusion Graph Neural Networks for Traffic Flow Forecasting. (arXiv:2012.09641v1 [cs.LG])</h2>
<h3>Li Mengzhang, Zhu Zhanxing</h3>
<p>Spatial-temporal data forecasting of traffic flow is a challenging task
because of complicated spatial dependencies and dynamical trends of temporal
pattern between different roads. Existing frameworks typically utilize given
spatial adjacency graph and sophisticated mechanisms for modeling spatial and
temporal correlations. However, limited representations of given spatial graph
structure with incomplete adjacent connections may restrict effective
spatial-temporal dependencies learning of those models. To overcome those
limitations, our paper proposes Spatial-Temporal Fusion Graph Neural Networks
(STFGNN) for traffic flow forecasting. SFTGNN could effectively learn hidden
spatial-temporal dependencies by a novel fusion operation of various spatial
and temporal graphs, which is generated by a data-driven method. Meanwhile, by
integrating this fusion graph module and a novel gated convolution module into
a unified layer, SFTGNN could handle long sequences. Experimental results on
several public traffic datasets demonstrate that our method achieves
state-of-the-art performance consistently than other baselines.
</p>
<a href="http://arxiv.org/abs/2012.09641" target="_blank">arXiv:2012.09641</a> [<a href="http://arxiv.org/pdf/2012.09641" target="_blank">pdf</a>]

<h2>On the Importance of Diversity in Re-Sampling for Imbalanced Data and Rare Events in Mortality Risk Models. (arXiv:2012.09645v1 [cs.LG])</h2>
<h3>Yuxuan (Diana) Yang, Hadi Akbarzadeh Khorshidi, Uwe Aickelin, Aditi Nevgi, Elif Ekinci</h3>
<p>Surgical risk increases significantly when patients present with comorbid
conditions. This has resulted in the creation of numerous risk stratification
tools with the objective of formulating associated surgical risk to assist both
surgeons and patients in decision-making. The Surgical Outcome Risk Tool (SORT)
is one of the tools developed to predict mortality risk throughout the entire
perioperative period for major elective in-patient surgeries in the UK. In this
study, we enhance the original SORT prediction model (UK SORT) by addressing
the class imbalance within the dataset. Our proposed method investigates the
application of diversity-based selection on top of common re-sampling
techniques to enhance the classifier's capability in detecting minority
(mortality) events. Diversity amongst training datasets is an essential factor
in ensuring re-sampled data keeps an accurate depiction of the
minority/majority class region, thereby solving the generalization problem of
mainstream sampling approaches. We incorporate the use of the Solow-Polasky
measure as a drop-in functionality to evaluate diversity, with the addition of
greedy algorithms to identify and discard subsets that share the most
similarity. Additionally, through empirical experiments, we prove that the
performance of the classifier trained over diversity-based dataset outperforms
the original classifier over ten external datasets. Our diversity-based
re-sampling method elevates the performance of the UK SORT algorithm by 1.4$.
</p>
<a href="http://arxiv.org/abs/2012.09645" target="_blank">arXiv:2012.09645</a> [<a href="http://arxiv.org/pdf/2012.09645" target="_blank">pdf</a>]

<h2>Detection and Prediction of Nutrient Deficiency Stress using Longitudinal Aerial Imagery. (arXiv:2012.09654v1 [cs.CV])</h2>
<h3>Saba Dadsetan, Gisele Rose, Naira Hovakimyan, Jennifer Hobbs</h3>
<p>Early, precise detection of nutrient deficiency stress (NDS) has key economic
as well as environmental impact; precision application of chemicals in place of
blanket application reduces operational costs for the growers while reducing
the amount of chemicals which may enter the environment unnecessarily.
Furthermore, earlier treatment reduces the amount of loss and therefore boosts
crop production during a given season. With this in mind, we collect sequences
of high-resolution aerial imagery and construct semantic segmentation models to
detect and predict NDS across the field. Our work sits at the intersection of
agriculture, remote sensing, and modern computer vision and deep learning.
First, we establish a baseline for full-field detection of NDS and quantify the
impact of pretraining, backbone architecture, input representation, and
sampling strategy. We then quantify the amount of information available at
different points in the season by building a single-timestamp model based on a
UNet. Next, we construct our proposed spatiotemporal architecture, which
combines a UNet with a convolutional LSTM layer, to accurately detect regions
of the field showing NDS; this approach has an impressive IOU score of 0.53.
Finally, we show that this architecture can be trained to predict regions of
the field which are expected to show NDS in a later flight -- potentially more
than three weeks in the future -- maintaining an IOU score of 0.47-0.51
depending on how far in advance the prediction is made. We will also release a
dataset which we believe will benefit the computer vision, remote sensing, as
well as agriculture fields. This work contributes to the recent developments in
deep learning for remote sensing and agriculture, while addressing a key social
challenge with implications for economics and sustainability.
</p>
<a href="http://arxiv.org/abs/2012.09654" target="_blank">arXiv:2012.09654</a> [<a href="http://arxiv.org/pdf/2012.09654" target="_blank">pdf</a>]

<h2>Firearm Detection via Convolutional Neural Networks: Comparing a Semantic Segmentation Model Against End-to-End Solutions. (arXiv:2012.09662v1 [cs.CV])</h2>
<h3>Alexander Egiazarov, Fabio Massimo Zennaro, Vasileios Mavroeidis</h3>
<p>Threat detection of weapons and aggressive behavior from live video can be
used for rapid detection and prevention of potentially deadly incidents such as
terrorism, general criminal offences, or even domestic violence. One way for
achieving this is through the use of artificial intelligence and, in
particular, machine learning for image analysis. In this paper we conduct a
comparison between a traditional monolithic end-to-end deep learning model and
a previously proposed model based on an ensemble of simpler neural networks
detecting fire-weapons via semantic segmentation. We evaluated both models from
different points of view, including accuracy, computational and data
complexity, flexibility and reliability. Our results show that a semantic
segmentation model provides considerable amount of flexibility and resilience
in the low data environment compared to classical deep model models, although
its configuration and tuning presents a challenge in achieving the same levels
of accuracy as an end-to-end model.
</p>
<a href="http://arxiv.org/abs/2012.09662" target="_blank">arXiv:2012.09662</a> [<a href="http://arxiv.org/pdf/2012.09662" target="_blank">pdf</a>]

<h2>A fully pipelined FPGA accelerator for scale invariant feature transform keypoint descriptor matching,. (arXiv:2012.09666v1 [cs.CV])</h2>
<h3>Luka Daoud, Muhammad Kamran Latif, H S. Jacinto, Nader Rafla</h3>
<p>The scale invariant feature transform (SIFT) algorithm is considered a
classical feature extraction algorithm within the field of computer vision.
SIFT keypoint descriptor matching is a computationally intensive process due to
the amount of data consumed. In this work, we designed a novel fully pipelined
hardware accelerator architecture for SIFT keypoint descriptor matching. The
accelerator core was implemented and tested on a field programmable gate array
(FPGA). The proposed hardware architecture is able to properly handle the
memory bandwidth necessary for a fully-pipelined implementation and hits the
roofline performance model, achieving the potential maximum throughput. The
fully pipelined matching architecture was designed based on the consine angle
distance method. Our architecture was optimized for 16-bit fixed-point
operations and implemented on hardware using a Xilinx Zynq-based FPGA
development board. Our proposed architecture shows a noticeable reduction of
area resources compared with its counterparts in literature, while maintaining
high throughput by alleviating memory bandwidth restrictions. The results show
a reduction in consumed device resources of up to 91 percent in LUTs and 79
percent of BRAMs. Our hardware implementation is 15.7 times faster than the
comparable software approach.
</p>
<a href="http://arxiv.org/abs/2012.09666" target="_blank">arXiv:2012.09666</a> [<a href="http://arxiv.org/pdf/2012.09666" target="_blank">pdf</a>]

<h2>Multi-Modal Depth Estimation Using Convolutional Neural Networks. (arXiv:2012.09667v1 [cs.CV])</h2>
<h3>Sadique Adnan Siddiqui, Axel Vierling, Karsten Berns</h3>
<p>This paper addresses the problem of dense depth predictions from sparse
distance sensor data and a single camera image on challenging weather
conditions. This work explores the significance of different sensor modalities
such as camera, Radar, and Lidar for estimating depth by applying Deep Learning
approaches. Although Lidar has higher depth-sensing abilities than Radar and
has been integrated with camera images in lots of previous works, depth
estimation using CNN's on the fusion of robust Radar distance data and camera
images has not been explored much. In this work, a deep regression network is
proposed utilizing a transfer learning approach consisting of an encoder where
a high performing pre-trained model has been used to initialize it for
extracting dense features and a decoder for upsampling and predicting desired
depth. The results are demonstrated on Nuscenes, KITTI, and a Synthetic dataset
which was created using the CARLA simulator. Also, top-view zoom-camera images
captured from the crane on a construction site are evaluated to estimate the
distance of the crane boom carrying heavy loads from the ground to show the
usability in safety-critical applications.
</p>
<a href="http://arxiv.org/abs/2012.09667" target="_blank">arXiv:2012.09667</a> [<a href="http://arxiv.org/pdf/2012.09667" target="_blank">pdf</a>]

<h2>RainBench: Towards Global Precipitation Forecasting from Satellite Imagery. (arXiv:2012.09670v1 [cs.LG])</h2>
<h3>Christian Schroeder de Witt, Catherine Tong, Valentina Zantedeschi, Daniele De Martini, Freddie Kalaitzis, Matthew Chantry, Duncan Watson-Parris, Piotr Bilinski</h3>
<p>Extreme precipitation events, such as violent rainfall and hail storms,
routinely ravage economies and livelihoods around the developing world. Climate
change further aggravates this issue. Data-driven deep learning approaches
could widen the access to accurate multi-day forecasts, to mitigate against
such events. However, there is currently no benchmark dataset dedicated to the
study of global precipitation forecasts. In this paper, we introduce
\textbf{RainBench}, a new multi-modal benchmark dataset for data-driven
precipitation forecasting. It includes simulated satellite data, a selection of
relevant meteorological data from the ERA5 reanalysis product, and IMERG
precipitation data. We also release \textbf{PyRain}, a library to process large
precipitation datasets efficiently. We present an extensive analysis of our
novel dataset and establish baseline results for two benchmark medium-range
precipitation forecasting tasks. Finally, we discuss existing data-driven
weather forecasting methodologies and suggest future research avenues.
</p>
<a href="http://arxiv.org/abs/2012.09670" target="_blank">arXiv:2012.09670</a> [<a href="http://arxiv.org/pdf/2012.09670" target="_blank">pdf</a>]

<h2>Combating Mode Collapse in GAN training: An Empirical Analysis using Hessian Eigenvalues. (arXiv:2012.09673v1 [cs.LG])</h2>
<h3>Ricard Durall, Avraam Chatzimichailidis, Peter Labus, Janis Keuper</h3>
<p>Generative adversarial networks (GANs) provide state-of-the-art results in
image generation. However, despite being so powerful, they still remain very
challenging to train. This is in particular caused by their highly non-convex
optimization space leading to a number of instabilities. Among them, mode
collapse stands out as one of the most daunting ones. This undesirable event
occurs when the model can only fit a few modes of the data distribution, while
ignoring the majority of them. In this work, we combat mode collapse using
second-order gradient information. To do so, we analyse the loss surface
through its Hessian eigenvalues, and show that mode collapse is related to the
convergence towards sharp minima. In particular, we observe how the eigenvalues
of the $G$ are directly correlated with the occurrence of mode collapse.
Finally, motivated by these findings, we design a new optimization algorithm
called nudged-Adam (NuGAN) that uses spectral information to overcome mode
collapse, leading to empirically more stable convergence properties.
</p>
<a href="http://arxiv.org/abs/2012.09673" target="_blank">arXiv:2012.09673</a> [<a href="http://arxiv.org/pdf/2012.09673" target="_blank">pdf</a>]

<h2>Polynomial-Time Algorithms for Counting and Sampling Markov Equivalent DAGs. (arXiv:2012.09679v1 [cs.LG])</h2>
<h3>Marcel Wien&#xf6;bst, Max Bannach, Maciej Li&#x15b;kiewicz</h3>
<p>Counting and uniform sampling of directed acyclic graphs (DAGs) from a Markov
equivalence class are fundamental tasks in graphical causal analysis. In this
paper, we show that these tasks can be performed in polynomial time, solving a
long-standing open problem in this area. Our algorithms are effective and
easily implementable. Experimental results show that the algorithms
significantly outperform state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2012.09679" target="_blank">arXiv:2012.09679</a> [<a href="http://arxiv.org/pdf/2012.09679" target="_blank">pdf</a>]

<h2>PCT: Point Cloud Transformer. (arXiv:2012.09688v1 [cs.CV])</h2>
<h3>Meng-Hao Guo, Jun-Xiong Cai, Zheng-Ning Liu, Tai-Jiang Mu, Ralph R. Martin, Shi-Min Hu</h3>
<p>The irregular domain and lack of ordering make it challenging to design deep
neural networks for point cloud processing. This paper presents a novel
framework named Point Cloud Transformer(PCT) for point cloud learning. PCT is
based on Transformer, which achieves huge success in natural language
processing and displays great potential in image processing. It is inherently
permutation invariant for processing a sequence of points, making it
well-suited for point cloud learning. To better capture local context within
the point cloud, we enhance input embedding with the support of farthest point
sampling and nearest neighbor search. Extensive experiments demonstrate that
the PCT achieves the state-of-the-art performance on shape classification, part
segmentation and normal estimation tasks.
</p>
<a href="http://arxiv.org/abs/2012.09688" target="_blank">arXiv:2012.09688</a> [<a href="http://arxiv.org/pdf/2012.09688" target="_blank">pdf</a>]

<h2>Trajectory Planning Under Stochastic and Bounded Sensing Uncertainties Using Reachability Analysis. (arXiv:2012.09689v1 [cs.RO])</h2>
<h3>Akshay Shetty, Grace Xingxin Gao</h3>
<p>Trajectory planning under uncertainty is an active research topic. Previous
works predict state and state estimation uncertainties along trajectories to
check for collision safety. They assume either stochastic or bounded sensing
uncertainties. However, GNSS pseudoranges are typically modeled to contain
stochastic uncertainties with additional biases in urban environments. Thus,
given bounds for the bias, the planner needs to account for both stochastic and
bounded sensing uncertainties. In our prior work we presented a reachability
analysis to predict state and state estimation uncertainties under stochastic
and bounded uncertainties. However, we ignored the correlation between these
uncertainties, leading to an imperfect approximation of the state uncertainty.
In this paper we improve our reachability analysis by predicting state
uncertainty as a function of independent quantities. We design a metric for the
predicted uncertainty to compare candidate trajectories during planning.
Finally, we validate the planner for GNSS-based urban navigation of fixed-wing
UAS.
</p>
<a href="http://arxiv.org/abs/2012.09689" target="_blank">arXiv:2012.09689</a> [<a href="http://arxiv.org/pdf/2012.09689" target="_blank">pdf</a>]

<h2>Multi-FinGAN: Generative Coarse-To-Fine Sampling of Multi-Finger Grasps. (arXiv:2012.09696v1 [cs.RO])</h2>
<h3>Jens Lundell, Enric Corona, Tran Nguyen Le, Francesco Verdoja, Philippe Weinzaepfel, Gregory Rogez, Francesc Moreno-Noguer, Ville Kyrki</h3>
<p>While there exists a large number of methods for manipulating rigid objects
with parallel-jaw grippers, grasping with multi-finger robotic hands remains a
quite unexplored research topic. Reasoning and planning collision-free
trajectories on the additional degrees of freedom of several fingers represents
an important challenge that, so far, involves computationally costly and slow
processes. In this work, we present Multi-FinGAN, a fast generative
multi-finger grasp sampling method that synthesizes high quality grasps
directly from RGB-D images in about a second. We achieve this by training in an
end-to-end fashion a coarse-to-fine model composed of a classification network
that distinguishes grasp types according to a specific taxonomy and a
refinement network that produces refined grasp poses and joint angles. We
experimentally validate and benchmark our method against standard
grasp-sampling methods on 790 grasps in simulation and 20 grasps on a real
Franka Emika Panda. All experimental results using our method show consistent
improvements both in terms of grasp quality metrics and grasp success rate.
Remarkably, our approach is up to 20-30 times faster than the baseline, a
significant improvement that opens the door to feedback-based grasp re-planning
and task informative grasping.
</p>
<a href="http://arxiv.org/abs/2012.09696" target="_blank">arXiv:2012.09696</a> [<a href="http://arxiv.org/pdf/2012.09696" target="_blank">pdf</a>]

<h2>A Generalization of Transformer Networks to Graphs. (arXiv:2012.09699v1 [cs.LG])</h2>
<h3>Vijay Prakash Dwivedi, Xavier Bresson</h3>
<p>We propose a generalization of transformer neural network architecture for
arbitrary graphs. The original transformer was designed for Natural Language
Processing (NLP), which operates on fully connected graphs representing all
connections between the words in a sequence. Such architecture does not
leverage the graph connectivity inductive bias, and can perform poorly when the
graph topology is important and has not been encoded into the node features. We
introduce a graph transformer with four new properties compared to the standard
model. First, the attention mechanism is a function of the neighborhood
connectivity for each node in the graph. Second, the positional encoding is
represented by the Laplacian eigenvectors, which naturally generalize the
sinusoidal positional encodings often used in NLP. Third, the layer
normalization is replaced by a batch normalization layer, which provides faster
training and better generalization performance. Finally, the architecture is
extended to edge feature representation, which can be critical to tasks s.a.
chemistry (bond type) or link prediction (entity relationship in knowledge
graphs). Numerical experiments on a graph benchmark demonstrate the performance
of the proposed graph transformer architecture. This work closes the gap
between the original transformer, which was designed for the limited case of
line graphs, and graph neural networks, that can work with arbitrary graphs. As
our architecture is simple and generic, we believe it can be used as a black
box for future applications that wish to consider transformer and graphs.
</p>
<a href="http://arxiv.org/abs/2012.09699" target="_blank">arXiv:2012.09699</a> [<a href="http://arxiv.org/pdf/2012.09699" target="_blank">pdf</a>]

<h2>RainNet: A Large-Scale Dataset for Spatial Precipitation Downscaling. (arXiv:2012.09700v1 [cs.CV])</h2>
<h3>Xuanhong Chen, Kairui Feng, Naiyuan Liu, Naiyuan Liu, Zhengyan Tong, Bingbing Ni, Ziang Liu, Ning Lin</h3>
<p>Spatial Precipitation Downscaling is one of the most important problems in
the geo-science community. However, it still remains an unaddressed issue. Deep
learning is a promising potential solution for downscaling. In order to
facilitate the research on precipitation downscaling for deep learning, we
present the first \textbf{REAL} (non-simulated) Large-Scale Spatial
Precipitation Downscaling Dataset, \textbf{RainNet}, which contains
\textbf{62,424} pairs of low-resolution and high-resolution precipitation maps
for 17 years. Contrary to simulated data, this real dataset covers various
types of real meteorological phenomena (e.g., Hurricane, Squall, etc.), and
shows the physical characters - \textbf{Temporal Misalignment},
\textbf{Temporal Sparse} and \textbf{Fluid Properties} - that challenge the
downscaling algorithms. In order to fully explore potential downscaling
solutions, we propose an implicit physical estimation framework to learn the
above characteristics. Eight metrics specifically considering the physical
property of the data set are raised, while fourteen models are evaluated on the
proposed dataset. Finally, we analyze the effectiveness and feasibility of
these models on precipitation downscaling task. The Dataset and Code will be
available at \url{https://neuralchen.github.io/RainNet/}.
</p>
<a href="http://arxiv.org/abs/2012.09700" target="_blank">arXiv:2012.09700</a> [<a href="http://arxiv.org/pdf/2012.09700" target="_blank">pdf</a>]

<h2>Efficient CNN-LSTM based Image Captioning using Neural Network Compression. (arXiv:2012.09708v1 [cs.CV])</h2>
<h3>Harshit Rampal, Aman Mohanty</h3>
<p>Modern Neural Networks are eminent in achieving state of the art performance
on tasks under Computer Vision, Natural Language Processing and related
verticals. However, they are notorious for their voracious memory and compute
appetite which further obstructs their deployment on resource limited edge
devices. In order to achieve edge deployment, researchers have developed
pruning and quantization algorithms to compress such networks without
compromising their efficacy. Such compression algorithms are broadly
experimented on standalone CNN and RNN architectures while in this work, we
present an unconventional end to end compression pipeline of a CNN-LSTM based
Image Captioning model. The model is trained using VGG16 or ResNet50 as an
encoder and an LSTM decoder on the flickr8k dataset. We then examine the
effects of different compression architectures on the model and design a
compression architecture that achieves a 73.1% reduction in model size, 71.3%
reduction in inference time and a 7.7% increase in BLEU score as compared to
its uncompressed counterpart.
</p>
<a href="http://arxiv.org/abs/2012.09708" target="_blank">arXiv:2012.09708</a> [<a href="http://arxiv.org/pdf/2012.09708" target="_blank">pdf</a>]

<h2>Deep Molecular Dreaming: Inverse machine learning for de-novo molecular design and interpretability with surjective representations. (arXiv:2012.09712v1 [cs.LG])</h2>
<h3>Cynthia Shen, Mario Krenn, Sagi Eppel, Alan Aspuru-Guzik</h3>
<p>Computer-based de-novo design of functional molecules is one of the most
prominent challenges in cheminformatics today. As a result, generative and
evolutionary inverse designs from the field of artificial intelligence have
emerged at a rapid pace, with aims to optimize molecules for a particular
chemical property. These models 'indirectly' explore the chemical space; by
learning latent spaces, policies, distributions or by applying mutations on
populations of molecules. However, the recent development of the SELFIES string
representation of molecules, a surjective alternative to SMILES, have made
possible other potential techniques. Based on SELFIES, we therefore propose
PASITHEA, a direct gradient-based molecule optimization that applies
inceptionism techniques from computer vision. PASITHEA exploits the use of
gradients by directly reversing the learning process of a neural network, which
is trained to predict real-valued chemical properties. Effectively, this forms
an inverse regression model, which is capable of generating molecular variants
optimized for a certain property. Although our results are preliminary, we
observe a shift in distribution of a chosen property during inverse-training, a
clear indication of PASITHEA's viability. A striking property of inceptionism
is that we can directly probe the model's understanding of the chemical space
it was trained on. We expect that extending PASITHEA to larger datasets,
molecules and more complex properties will lead to advances in the design of
new functional molecules as well as the interpretation and explanation of
machine learning models.
</p>
<a href="http://arxiv.org/abs/2012.09712" target="_blank">arXiv:2012.09712</a> [<a href="http://arxiv.org/pdf/2012.09712" target="_blank">pdf</a>]

<h2>Hardness of Learning Halfspaces with Massart Noise. (arXiv:2012.09720v1 [cs.LG])</h2>
<h3>Ilias Diakonikolas, Daniel M. Kane</h3>
<p>We study the complexity of PAC learning halfspaces in the presence of Massart
(bounded) noise. Specifically, given labeled examples $(x, y)$ from a
distribution $D$ on $\mathbb{R}^{n} \times \{ \pm 1\}$ such that the marginal
distribution on $x$ is arbitrary and the labels are generated by an unknown
halfspace corrupted with Massart noise at rate $\eta&lt;1/2$, we want to compute a
hypothesis with small misclassification error. Characterizing the efficient
learnability of halfspaces in the Massart model has remained a longstanding
open problem in learning theory.

Recent work gave a polynomial-time learning algorithm for this problem with
error $\eta+\epsilon$. This error upper bound can be far from the
information-theoretically optimal bound of $\mathrm{OPT}+\epsilon$. More recent
work showed that {\em exact learning}, i.e., achieving error
$\mathrm{OPT}+\epsilon$, is hard in the Statistical Query (SQ) model. In this
work, we show that there is an exponential gap between the
information-theoretically optimal error and the best error that can be achieved
by a polynomial-time SQ algorithm. In particular, our lower bound implies that
no efficient SQ algorithm can approximate the optimal error within any
polynomial factor.
</p>
<a href="http://arxiv.org/abs/2012.09720" target="_blank">arXiv:2012.09720</a> [<a href="http://arxiv.org/pdf/2012.09720" target="_blank">pdf</a>]

<h2>A Note on Optimizing the Ratio of Monotone Supermodular Functions. (arXiv:2012.09725v1 [cs.LG])</h2>
<h3>Wenxin Li</h3>
<p>We show that for the problem of minimizing (or maximizing) the ratio of two
supermodular functions, no bounded approximation ratio can be achieved via
polynomial number of queries, if the two supermodular functions are both
monotone non-decreasing or non-increasing.
</p>
<a href="http://arxiv.org/abs/2012.09725" target="_blank">arXiv:2012.09725</a> [<a href="http://arxiv.org/pdf/2012.09725" target="_blank">pdf</a>]

<h2>Robust Image Captioning. (arXiv:2012.09732v1 [cs.CV])</h2>
<h3>Daniel Yarnell, Xian Wang</h3>
<p>Automated captioning of photos is a mission that incorporates the
difficulties of photo analysis and text generation. One essential feature of
captioning is the concept of attention: how to determine what to specify and in
which sequence. In this study, we leverage the Object Relation using
adversarial robust cut algorithm, that grows upon this method by specifically
embedding knowledge about the spatial association between input data through
graph representation. Our experimental study represent the promising
performance of our proposed method for image captioning.
</p>
<a href="http://arxiv.org/abs/2012.09732" target="_blank">arXiv:2012.09732</a> [<a href="http://arxiv.org/pdf/2012.09732" target="_blank">pdf</a>]

<h2>Model-free and Bayesian Ensembling Model-based Deep Reinforcement Learning for Particle Accelerator Control Demonstrated on the FERMI FEL. (arXiv:2012.09737v1 [cs.LG])</h2>
<h3>Simon Hirlaender, Niky Bruchon</h3>
<p>Reinforcement learning holds tremendous promise in accelerator controls. The
primary goal of this paper is to show how this approach can be utilised on an
operational level on accelerator physics problems. Despite the success of
model-free reinforcement learning in several domains, sample-efficiency still
is a bottle-neck, which might be encompassed by model-based methods. We compare
well-suited purely model-based to model-free reinforcement learning applied to
the intensity optimisation on the FERMI FEL system. We find that the
model-based approach demonstrates higher representational power and
sample-efficiency, while the asymptotic performance of the model-free method is
slightly superior. The model-based algorithm is implemented in a DYNA-style
using an uncertainty aware model, and the model-free algorithm is based on
tailored deep Q-learning. In both cases, the algorithms were implemented in a
way, which presents increased noise robustness as omnipresent in accelerator
control problems. Code is released in
https://github.com/MathPhysSim/FERMI_RL_Paper.
</p>
<a href="http://arxiv.org/abs/2012.09737" target="_blank">arXiv:2012.09737</a> [<a href="http://arxiv.org/pdf/2012.09737" target="_blank">pdf</a>]

<h2>Understanding the Behaviour of Contrastive Loss. (arXiv:2012.09740v1 [cs.LG])</h2>
<h3>Feng Wang, Huaping Liu</h3>
<p>Unsupervised contrastive learning has achieved outstanding success, while the
mechanism of contrastive loss has been less studied. In this paper, we
concentrate on the understanding of the behaviours of unsupervised contrastive
loss. We will show that the contrastive loss is a hardness-aware loss function,
and the temperature $\tau$ controls the strength of penalties on hard negative
samples. The previous study has shown that uniformity is a key property of
contrastive learning. We build relations between the uniformity and the
temperature $\tau$. We will show that uniformity helps the contrastive learning
to learn separable features, however excessive pursuit to the uniformity makes
the contrastive loss not tolerant to semantically similar samples, which may
break the underlying semantic structure and be harmful to the formation of
features useful for downstream tasks. This is caused by the inherent defect of
the instance discrimination objective. Specifically, instance discrimination
objective tries to push all different instances apart, ignoring the underlying
relations between samples. Pushing semantically consistent samples apart has no
positive effect for acquiring a prior informative to general downstream tasks.
A well-designed contrastive loss should have some extents of tolerance to the
closeness of semantically similar samples. Therefore, we find that the
contrastive loss meets a uniformity-tolerance dilemma, and a good choice of
temperature can compromise these two properties properly to both learn
separable features and tolerant to semantically similar samples, improving the
feature qualities and the downstream performances.
</p>
<a href="http://arxiv.org/abs/2012.09740" target="_blank">arXiv:2012.09740</a> [<a href="http://arxiv.org/pdf/2012.09740" target="_blank">pdf</a>]

<h2>AutoCaption: Image Captioning with Neural Architecture Search. (arXiv:2012.09742v1 [cs.CV])</h2>
<h3>Xinxin Zhu, Weining Wang, Longteng Guo, Jing Liu</h3>
<p>Image captioning transforms complex visual information into abstract natural
language for representation, which can help computers understanding the world
quickly. However, due to the complexity of the real environment, it needs to
identify key objects and realize their connections, and further generate
natural language. The whole process involves a visual understanding module and
a language generation module, which brings more challenges to the design of
deep neural networks than other tasks. Neural Architecture Search (NAS) has
shown its important role in a variety of image recognition tasks. Besides, RNN
plays an essential role in the image captioning task. We introduce a
AutoCaption method to better design the decoder module of the image captioning
where we use the NAS to design the decoder module called AutoRNN automatically.
We use the reinforcement learning method based on shared parameters for
automatic design the AutoRNN efficiently. The search space of the AutoCaption
includes connections between the layers and the operations in layers both, and
it can make AutoRNN express more architectures. In particular, RNN is
equivalent to a subset of our search space. Experiments on the MSCOCO datasets
show that our AutoCaption model can achieve better performance than traditional
hand-design methods. Our AutoCaption obtains the best published CIDEr
performance of 135.8% on COCO Karpathy test split. When further using ensemble
technology, CIDEr is boosted up to 139.5%.
</p>
<a href="http://arxiv.org/abs/2012.09742" target="_blank">arXiv:2012.09742</a> [<a href="http://arxiv.org/pdf/2012.09742" target="_blank">pdf</a>]

<h2>Interpretable Image Clustering via Diffeomorphism-Aware K-Means. (arXiv:2012.09743v1 [cs.CV])</h2>
<h3>Romain Cosentino, Randall Balestriero, Yanis Bahroun, Anirvan Sengupta, Richard Baraniuk, Behnaam Aazhang</h3>
<p>We design an interpretable clustering algorithm aware of the nonlinear
structure of image manifolds. Our approach leverages the interpretability of
$K$-means applied in the image space while addressing its clustering
performance issues. Specifically, we develop a measure of similarity between
images and centroids that encompasses a general class of deformations:
diffeomorphisms, rendering the clustering invariant to them. Our work leverages
the Thin-Plate Spline interpolation technique to efficiently learn
diffeomorphisms best characterizing the image manifolds. Extensive numerical
simulations show that our approach competes with state-of-the-art methods on
various datasets.
</p>
<a href="http://arxiv.org/abs/2012.09743" target="_blank">arXiv:2012.09743</a> [<a href="http://arxiv.org/pdf/2012.09743" target="_blank">pdf</a>]

<h2>End-to-End Human Pose and Mesh Reconstruction with Transformers. (arXiv:2012.09760v1 [cs.CV])</h2>
<h3>Kevin Lin, Lijuan Wang, Zicheng Liu</h3>
<p>We present a new method, called MEsh TRansfOrmer (METRO), to reconstruct 3D
human pose and mesh vertices from a single image. Our method uses a transformer
encoder to jointly model vertex-vertex and vertex-joint interactions, and
outputs 3D joint coordinates and mesh vertices simultaneously. Compared to
existing techniques that regress pose and shape parameters, METRO does not rely
on any parametric mesh models like SMPL, thus it can be easily extended to
other objects such as hands. We further relax the mesh topology and allow the
transformer self-attention mechanism to freely attend between any two vertices,
making it possible to learn non-local relationships among mesh vertices and
joints. With the proposed masked vertex modeling, our method is more robust and
effective in handling challenging situations like partial occlusions. METRO
generates new state-of-the-art results for human mesh reconstruction on the
public Human3.6M and 3DPW datasets. Moreover, we demonstrate the
generalizability of METRO to 3D hand reconstruction in the wild, outperforming
existing state-of-the-art methods on FreiHAND dataset.
</p>
<a href="http://arxiv.org/abs/2012.09760" target="_blank">arXiv:2012.09760</a> [<a href="http://arxiv.org/pdf/2012.09760" target="_blank">pdf</a>]

<h2>MAGNet: Multi-agent Graph Network for Deep Multi-agent Reinforcement Learning. (arXiv:2012.09762v1 [cs.LG])</h2>
<h3>Aleksandra Malysheva, Daniel Kudenko, Aleksei Shpilman</h3>
<p>Over recent years, deep reinforcement learning has shown strong successes in
complex single-agent tasks, and more recently this approach has also been
applied to multi-agent domains. In this paper, we propose a novel approach,
called MAGNet, to multi-agent reinforcement learning that utilizes a relevance
graph representation of the environment obtained by a self-attention mechanism,
and a message-generation technique. We applied our MAGnet approach to the
synthetic predator-prey multi-agent environment and the Pommerman game and the
results show that it significantly outperforms state-of-the-art MARL solutions,
including Multi-agent Deep Q-Networks (MADQN), Multi-agent Deep Deterministic
Policy Gradient (MADDPG), and QMIX
</p>
<a href="http://arxiv.org/abs/2012.09762" target="_blank">arXiv:2012.09762</a> [<a href="http://arxiv.org/pdf/2012.09762" target="_blank">pdf</a>]

<h2>Rank-One Measurements of Low-Rank PSD Matrices Have Small Feasible Sets. (arXiv:2012.09768v1 [stat.ML])</h2>
<h3>T. Mitchell Roddenberry, Santiago Segarra, Anastasios Kyrillidis</h3>
<p>We study the role of the constraint set in determining the solution to
low-rank, positive semidefinite (PSD) matrix sensing problems. The setting we
consider involves rank-one sensing matrices: In particular, given a set of
rank-one projections of an approximately low-rank PSD matrix, we characterize
the radius of the set of PSD matrices that satisfy the measurements. This
result yields a sampling rate to guarantee singleton solution sets when the
true matrix is exactly low-rank, such that the choice of the objective function
or the algorithm to be used is inconsequential in its recovery. We discuss
applications of this contribution and compare it to recent literature regarding
implicit regularization for similar problems. We demonstrate practical
implications of this result by applying conic projection methods for PSD matrix
recovery without incorporating low-rank regularization.
</p>
<a href="http://arxiv.org/abs/2012.09768" target="_blank">arXiv:2012.09768</a> [<a href="http://arxiv.org/pdf/2012.09768" target="_blank">pdf</a>]

<h2>End-to-end Deep Object Tracking with Circular Loss Function for Rotated Bounding Box. (arXiv:2012.09771v1 [cs.CV])</h2>
<h3>Vladislav Belyaev, Aleksandra Malysheva, Aleksei Shpilman</h3>
<p>The task object tracking is vital in numerous applications such as autonomous
driving, intelligent surveillance, robotics, etc. This task entails the
assigning of a bounding box to an object in a video stream, given only the
bounding box for that object on the first frame. In 2015, a new type of video
object tracking (VOT) dataset was created that introduced rotated bounding
boxes as an extension of axis-aligned ones. In this work, we introduce a novel
end-to-end deep learning method based on the Transformer Multi-Head Attention
architecture. We also present a new type of loss function, which takes into
account the bounding box overlap and orientation.

Our Deep Object Tracking model with Circular Loss Function (DOTCL) shows an
considerable improvement in terms of robustness over current state-of-the-art
end-to-end deep learning models. It also outperforms state-of-the-art object
tracking methods on VOT2018 dataset in terms of expected average overlap (EAO)
metric.
</p>
<a href="http://arxiv.org/abs/2012.09771" target="_blank">arXiv:2012.09771</a> [<a href="http://arxiv.org/pdf/2012.09771" target="_blank">pdf</a>]

<h2>DenseHMM: Learning Hidden Markov Models by Learning Dense Representations. (arXiv:2012.09783v1 [cs.LG])</h2>
<h3>Joachim Sicking, Maximilian Pintz, Maram Akila, Tim Wirtz</h3>
<p>We propose DenseHMM - a modification of Hidden Markov Models (HMMs) that
allows to learn dense representations of both the hidden states and the
observables. Compared to the standard HMM, transition probabilities are not
atomic but composed of these representations via kernelization. Our approach
enables constraint-free and gradient-based optimization. We propose two
optimization schemes that make use of this: a modification of the Baum-Welch
algorithm and a direct co-occurrence optimization. The latter one is highly
scalable and comes empirically without loss of performance compared to standard
HMMs. We show that the non-linearity of the kernelization is crucial for the
expressiveness of the representations. The properties of the DenseHMM like
learned co-occurrences and log-likelihoods are studied empirically on synthetic
and biomedical datasets.
</p>
<a href="http://arxiv.org/abs/2012.09783" target="_blank">arXiv:2012.09783</a> [<a href="http://arxiv.org/pdf/2012.09783" target="_blank">pdf</a>]

<h2>Use of Bayesian Nonparametric methods for Estimating the Measurements in High Clutter. (arXiv:2012.09785v1 [cs.LG])</h2>
<h3>Bahman Moraffah, Christ Richmond, Raha Moraffah, Antonia Papandreou-Suppappola</h3>
<p>Robust tracking of a target in a clutter environment is an important and
challenging task. In recent years, the nearest neighbor methods and
probabilistic data association filters were proposed. However, the performance
of these methods diminishes as the number of measurements increases. In this
paper, we propose a robust generative approach to effectively model multiple
sensor measurements for tracking a moving target in an environment with high
clutter. We assume a time-dependent number of measurements that include sensor
observations with unknown origin, some of which may only contain clutter with
no additional information. We robustly and accurately estimate the trajectory
of the moving target in a high clutter environment with an unknown number of
clutters by employing Bayesian nonparametric modeling. In particular, we employ
a class of joint Bayesian nonparametric models to construct the joint prior
distribution of target and clutter measurements such that the conditional
distributions follow a Dirichlet process. The marginalized Dirichlet process
prior of the target measurements is then used in a Bayesian tracker to estimate
the dynamically-varying target state. We show through experiments that the
tracking performance and effectiveness of our proposed framework are increased
by suppressing high clutter measurements. In addition, we show that our
proposed method outperforms existing methods such as nearest neighbor and
probability data association filters.
</p>
<a href="http://arxiv.org/abs/2012.09785" target="_blank">arXiv:2012.09785</a> [<a href="http://arxiv.org/pdf/2012.09785" target="_blank">pdf</a>]

<h2>Neural Radiance Flow for 4D View Synthesis and Video Processing. (arXiv:2012.09790v1 [cs.CV])</h2>
<h3>Yilun Du, Yinan Zhang, Hong-Xing Yu, Joshua B. Tenenbaum, Jiajun Wu</h3>
<p>We present a method, Neural Radiance Flow (NeRFlow),to learn a 4D
spatial-temporal representation of a dynamic scene from a set of RGB images.
Key to our approach is the use of a neural implicit representation that learns
to capture the 3D occupancy, radiance, and dynamics of the scene. By enforcing
consistency across different modalities, our representation enables multi-view
rendering in diverse dynamic scenes, including water pouring, robotic
interaction, and real images, outperforming state-of-the-art methods for
spatial-temporal view synthesis. Our approach works even when inputs images are
captured with only one camera. We further demonstrate that the learned
representation can serve as an implicit scene prior, enabling video processing
tasks such as image super-resolution and de-noising without any additional
supervision.
</p>
<a href="http://arxiv.org/abs/2012.09790" target="_blank">arXiv:2012.09790</a> [<a href="http://arxiv.org/pdf/2012.09790" target="_blank">pdf</a>]

<h2>SceneFormer: Indoor Scene Generation with Transformers. (arXiv:2012.09793v1 [cs.CV])</h2>
<h3>Xinpeng Wang, Chandan Yeshwanth, Matthias Nie&#xdf;ner</h3>
<p>The task of indoor scene generation is to generate a sequence of objects,
their locations and orientations conditioned on the shape and size of a room.
Large scale indoor scene datasets allow us to extract patterns from
user-designed indoor scenes and then generate new scenes based on these
patterns. Existing methods rely on the 2D or 3D appearance of these scenes in
addition to object positions, and make assumptions about the possible relations
between objects. In contrast, we do not use any appearance information, and
learn relations between objects using the self attention mechanism of
transformers. We show that this leads to faster scene generation compared to
existing methods with the same or better levels of realism. We build simple and
effective generative models conditioned on the room shape, and on text
descriptions of the room using only the cross-attention mechanism of
transformers. We carried out a user study showing that our generated scenes are
preferred over DeepSynth scenes 57.7% of the time for bedroom scenes, and 63.3%
for living room scenes. In addition, we generate a scene in 1.48 seconds on
average, 20% faster than the state of the art method Fast &amp; Flexible, allowing
interactive scene generation.
</p>
<a href="http://arxiv.org/abs/2012.09793" target="_blank">arXiv:2012.09793</a> [<a href="http://arxiv.org/pdf/2012.09793" target="_blank">pdf</a>]

<h2>Connectivity Maintenance for Multi-Robot Systems Under Motion and Sensing Uncertainties Using Distributed ADMM-based Trajectory Planning. (arXiv:2012.09808v1 [cs.RO])</h2>
<h3>Akshay Shetty, Derek Knowles, Grace Xingxin Gao</h3>
<p>Inter-robot communication enables multi-robot systems to coordinate and
execute complex missions efficiently. Thus, maintaining connectivity of the
communication network between robots is essential for many multi-robot systems.
In this paper, we present a trajectory planner for connectivity maintenance of
a multi-robot system. Unlike previous connectivity maintenance works, we
account for motion and sensing uncertainties inherent in practical robots.
These uncertainties result in uncertain robot positions which directly affects
the connectivity of the system. We first define a metric to quantify the
connectivity of a system with uncertain robot positions. This metric is used to
design our trajectory planner based on a distributed alternating direction
method of multipliers (ADMM) framework. Next, we derive an approximation for
the Hessian matrices required within the ADMM optimization step to reduce the
computational load. Finally, simulation results are presented to statistically
validate the connectivity maintenance of our trajectory planner.
</p>
<a href="http://arxiv.org/abs/2012.09808" target="_blank">arXiv:2012.09808</a> [<a href="http://arxiv.org/pdf/2012.09808" target="_blank">pdf</a>]

<h2>Learning Cross-Domain Correspondence for Control with Dynamics Cycle-Consistency. (arXiv:2012.09811v1 [cs.RO])</h2>
<h3>Qiang Zhang, Tete Xiao, Alexei A. Efros, Lerrel Pinto, Xiaolong Wang</h3>
<p>At the heart of many robotics problems is the challenge of learning
correspondences across domains. For instance, imitation learning requires
obtaining correspondence between humans and robots; sim-to-real requires
correspondence between physics simulators and the real world; transfer learning
requires correspondences between different robotics environments. This paper
aims to learn correspondence across domains differing in representation (vision
vs. internal state), physics parameters (mass and friction), and morphology
(number of limbs). Importantly, correspondences are learned using unpaired and
randomly collected data from the two domains. We propose \textit{dynamics
cycles} that align dynamic robot behavior across two domains using a
cycle-consistency constraint. Once this correspondence is found, we can
directly transfer the policy trained on one domain to the other, without
needing any additional fine-tuning on the second domain. We perform experiments
across a variety of problem domains, both in simulation and on real robot. Our
framework is able to align uncalibrated monocular video of a real robot arm to
dynamic state-action trajectories of a simulated arm without paired data. Video
demonstrations of our results are available at:
https://sjtuzq.github.io/cycle_dynamics.html .
</p>
<a href="http://arxiv.org/abs/2012.09811" target="_blank">arXiv:2012.09811</a> [<a href="http://arxiv.org/pdf/2012.09811" target="_blank">pdf</a>]

<h2>ViNG: Learning Open-World Navigation with Visual Goals. (arXiv:2012.09812v1 [cs.RO])</h2>
<h3>Dhruv Shah, Benjamin Eysenbach, Gregory Kahn, Nicholas Rhinehart, Sergey Levine</h3>
<p>We propose a learning-based navigation system for reaching visually indicated
goals and demonstrate this system on a real mobile robot platform. Learning
provides an appealing alternative to conventional methods for robotic
navigation: instead of reasoning about environments in terms of geometry and
maps, learning can enable a robot to learn about navigational affordances,
understand what types of obstacles are traversable (e.g., tall grass) or not
(e.g., walls), and generalize over patterns in the environment. However, unlike
conventional planning algorithms, it is harder to change the goal for a learned
policy during deployment. We propose a method for learning to navigate towards
a goal image of the desired destination. By combining a learned policy with a
topological graph constructed out of previously observed data, our system can
determine how to reach this visually indicated goal even in the presence of
variable appearance and lighting. Three key insights, waypoint proposal, graph
pruning and negative mining, enable our method to learn to navigate in
real-world environments using only offline data, a setting where prior methods
struggle. We instantiate our method on a real outdoor ground robot and show
that our system, which we call ViNG, outperforms previously-proposed methods
for goal-conditioned reinforcement learning, including other methods that
incorporate reinforcement learning and search. We also study how ViNG
generalizes to unseen environments and evaluate its ability to adapt to such an
environment with growing experience. Finally, we demonstrate ViNG on a number
of real-world applications, such as last-mile delivery and warehouse
inspection. We encourage the reader to check out the videos of our experiments
and demonstrations at our project website
https://sites.google.com/view/ving-robot
</p>
<a href="http://arxiv.org/abs/2012.09812" target="_blank">arXiv:2012.09812</a> [<a href="http://arxiv.org/pdf/2012.09812" target="_blank">pdf</a>]

<h2>Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning. (arXiv:2012.09816v1 [cs.LG])</h2>
<h3>Zeyuan Allen-Zhu, Yuanzhi Li</h3>
<p>We formally study how Ensemble of deep learning models can improve test
accuracy, and how the superior performance of ensemble can be distilled into a
single model using Knowledge Distillation. We consider the challenging case
where the ensemble is simply an average of the outputs of a few independently
trained neural networks with the SAME architecture, trained using the SAME
algorithm on the SAME data set, and they only differ by the random seeds used
in the initialization. We empirically show that ensemble/knowledge distillation
in deep learning works very differently from traditional learning theory,
especially differently from ensemble of random feature mappings or the
neural-tangent-kernel feature mappings, and is potentially out of the scope of
existing theorems. Thus, to properly understand ensemble and knowledge
distillation in deep learning, we develop a theory showing that when data has a
structure we refer to as "multi-view", then ensemble of independently trained
neural networks can provably improve test accuracy, and such superior test
accuracy can also be provably distilled into a single model by training a
single model to match the output of the ensemble instead of the true label. Our
result sheds light on how ensemble works in deep learning in a way that is
completely different from traditional theorems, and how the "dark knowledge" is
hidden in the outputs of the ensemble -- that can be used in knowledge
distillation -- comparing to the true data labels. In the end, we prove that
self-distillation can also be viewed as implicitly combining ensemble and
knowledge distillation to improve test accuracy.
</p>
<a href="http://arxiv.org/abs/2012.09816" target="_blank">arXiv:2012.09816</a> [<a href="http://arxiv.org/pdf/2012.09816" target="_blank">pdf</a>]

<h2>Intrinsically Motivated Goal-Conditioned Reinforcement Learning: a Short Survey. (arXiv:2012.09830v1 [cs.LG])</h2>
<h3>C&#xe9;dric Colas, Tristan Karch, Olivier Sigaud, Pierre-Yves Oudeyer</h3>
<p>Building autonomous machines that can explore open-ended environments,
discover possible interactions and autonomously build repertoires of skills is
a general objective of artificial intelligence. Developmental approaches argue
that this can only be achieved by autonomous and intrinsically motivated
learning agents that can generate, select and learn to solve their own
problems. In recent years, we have seen a convergence of developmental
approaches, and developmental robotics in particular, with deep reinforcement
learning (RL) methods, forming the new domain of developmental machine
learning. Within this new domain, we review here a set of methods where deep RL
algorithms are trained to tackle the developmental robotics problem of the
autonomous acquisition of open-ended repertoires of skills. Intrinsically
motivated goal-conditioned RL algorithms train agents to learn to represent,
generate and pursue their own goals. The self-generation of goals requires the
learning of compact goal encodings as well as their associated goal-achievement
functions, which results in new challenges compared to traditional RL
algorithms designed to tackle pre-defined sets of goals using external reward
signals. This paper proposes a typology of these methods at the intersection of
deep RL and developmental approaches, surveys recent approaches and discusses
future avenues.
</p>
<a href="http://arxiv.org/abs/2012.09830" target="_blank">arXiv:2012.09830</a> [<a href="http://arxiv.org/pdf/2012.09830" target="_blank">pdf</a>]

<h2>On Episodes, Prototypical Networks, and Few-shot Learning. (arXiv:2012.09831v1 [cs.LG])</h2>
<h3>Steinar Laenen, Luca Bertinetto</h3>
<p>Episodic learning is a popular practice among researchers and practitioners
interested in few-shot learning. It consists of organising training in a series
of learning problems, each relying on small "support" and "query" sets to mimic
the few-shot circumstances encountered during evaluation. In this paper, we
investigate the usefulness of episodic learning in Prototypical Networks and
Matching Networks, two of the most popular algorithms making use of this
practice. Surprisingly, in our experiments we found that, for Prototypical and
Matching Networks, it is detrimental to use the episodic learning strategy of
separating training samples between support and query set, as it is a
data-inefficient way to exploit training batches. These "non-episodic"
variants, which are closely related to the classic Neighbourhood Component
Analysis, reliably improve over their episodic counterparts in multiple
datasets, achieving an accuracy that (in the case of Prototypical Networks) is
competitive with the state-of-the-art, despite being extremely simple.
</p>
<a href="http://arxiv.org/abs/2012.09831" target="_blank">arXiv:2012.09831</a> [<a href="http://arxiv.org/pdf/2012.09831" target="_blank">pdf</a>]

<h2>Transformer Interpretability Beyond Attention Visualization. (arXiv:2012.09838v1 [cs.CV])</h2>
<h3>Hila Chefer, Shir Gur, Lior Wolf</h3>
<p>Self-attention techniques, and specifically Transformers, are dominating the
field of text processing and are becoming increasingly popular in computer
vision classification tasks. In order to visualize the parts of the image that
led to a certain classification, existing methods either rely on the obtained
attention maps, or employ heuristic propagation along the attention graph. In
this work, we propose a novel way to compute relevancy for Transformer
networks. The method assigns local relevance based on the deep Taylor
decomposition principle and then propagates these relevancy scores through the
layers. This propagation involves attention layers and skip connections, which
challenge existing methods. Our solution is based on a specific formulation
that is shown to maintain the total relevancy across layers. We benchmark our
method on very recent visual Transformer networks, as well as on a text
classification problem, and demonstrate a clear advantage over the existing
explainability methods.
</p>
<a href="http://arxiv.org/abs/2012.09838" target="_blank">arXiv:2012.09838</a> [<a href="http://arxiv.org/pdf/2012.09838" target="_blank">pdf</a>]

<h2>Towards Resolving the Implicit Bias of Gradient Descent for Matrix Factorization: Greedy Low-Rank Learning. (arXiv:2012.09839v1 [cs.LG])</h2>
<h3>Zhiyuan Li, Yuping Luo, Kaifeng Lyu</h3>
<p>Matrix factorization is a simple and natural test-bed to investigate the
implicit regularization of gradient descent. Gunasekar et al. (2018)
conjectured that Gradient Flow with infinitesimal initialization converges to
the solution that minimizes the nuclear norm, but a series of recent papers
argued that the language of norm minimization is not sufficient to give a full
characterization for the implicit regularization. In this work, we provide
theoretical and empirical evidence that for depth-2 matrix factorization,
gradient flow with infinitesimal initialization is mathematically equivalent to
a simple heuristic rank minimization algorithm, Greedy Low-Rank Learning, under
some reasonable assumptions. This generalizes the rank minimization view from
previous works to a much broader setting and enables us to construct
counter-examples to refute the conjecture from Gunasekar et al. (2018). We also
extend the results to the case where depth $\ge 3$, and we show that the
benefit of being deeper is that the above convergence has a much weaker
dependence over initialization magnitude so that this rank minimization is more
likely to take effect for initialization with practical scale.
</p>
<a href="http://arxiv.org/abs/2012.09839" target="_blank">arXiv:2012.09839</a> [<a href="http://arxiv.org/pdf/2012.09839" target="_blank">pdf</a>]

<h2>Taming Transformers for High-Resolution Image Synthesis. (arXiv:2012.09841v1 [cs.CV])</h2>
<h3>Patrick Esser, Robin Rombach, Bj&#xf6;rn Ommer</h3>
<p>Designed to learn long-range interactions on sequential data, transformers
continue to show state-of-the-art results on a wide variety of tasks. In
contrast to CNNs, they contain no inductive bias that prioritizes local
interactions. This makes them expressive, but also computationally infeasible
for long sequences, such as high-resolution images. We demonstrate how
combining the effectiveness of the inductive bias of CNNs with the expressivity
of transformers enables them to model and thereby synthesize high-resolution
images. We show how to (i) use CNNs to learn a context-rich vocabulary of image
constituents, and in turn (ii) utilize transformers to efficiently model their
composition within high-resolution images. Our approach is readily applied to
conditional synthesis tasks, where both non-spatial information, such as object
classes, and spatial information, such as segmentations, can control the
generated image. In particular, we present the first results on
semantically-guided synthesis of megapixel images with transformers. Project
page at https://compvis.github.io/taming-transformers/ .
</p>
<a href="http://arxiv.org/abs/2012.09841" target="_blank">arXiv:2012.09841</a> [<a href="http://arxiv.org/pdf/2012.09841" target="_blank">pdf</a>]

<h2>$\mathbb{X}$Resolution Correspondence Networks. (arXiv:2012.09842v1 [cs.CV])</h2>
<h3>Georgi Tinchev, Shuda Li, Kai Han, David Mitchell, Rigas Kouskouridas</h3>
<p>In this paper, we aim at establishing accurate dense correspondences between
a pair of images with overlapping field of view under challenging illumination
variation, viewpoint changes, and style differences. Through an extensive
ablation study of the state-of-the-art correspondence networks, we surprisingly
discovered that the widely adopted 4D correlation tensor and its related
learning and processing modules could be de-parameterised and removed from
training with merely a minor impact over the final matching accuracy. Disabling
some of the most memory consuming and computational expensive modules
dramatically speeds up the training procedure and allows to use 4x bigger batch
size, which in turn compensates for the accuracy drop. Together with a
multi-GPU inference stage, our method facilitates the systematic investigation
of the relationship between matching accuracy and up-sampling resolution of the
native testing images from 720p to 4K. This leads to finding an optimal
resolution $\mathbb X$ that produces accurate matching performance surpassing
the state-of-the-art methods particularly over the lower error band for the
proposed network and evaluation datasets.
</p>
<a href="http://arxiv.org/abs/2012.09842" target="_blank">arXiv:2012.09842</a> [<a href="http://arxiv.org/pdf/2012.09842" target="_blank">pdf</a>]

<h2>Human Mesh Recovery from Multiple Shots. (arXiv:2012.09843v1 [cs.CV])</h2>
<h3>Georgios Pavlakos, Jitendra Malik, Angjoo Kanazawa</h3>
<p>Videos from edited media like movies are a useful, yet under-explored source
of information. The rich variety of appearance and interactions between humans
depicted over a large temporal context in these films could be a valuable
source of data. However, the richness of data comes at the expense of
fundamental challenges such as abrupt shot changes and close up shots of actors
with heavy truncation, which limits the applicability of existing human 3D
understanding methods. In this paper, we address these limitations with an
insight that while shot changes of the same scene incur a discontinuity between
frames, the 3D structure of the scene still changes smoothly. This allows us to
handle frames before and after the shot change as multi-view signal that
provide strong cues to recover the 3D state of the actors. We propose a
multi-shot optimization framework, which leads to improved 3D reconstruction
and mining of long sequences with pseudo ground truth 3D human mesh. We show
that the resulting data is beneficial in the training of various human mesh
recovery models: for single image, we achieve improved robustness; for video we
propose a pure transformer-based temporal encoder, which can naturally handle
missing observations due to shot changes in the input frames. We demonstrate
the importance of the insight and proposed models through extensive
experiments. The tools we develop open the door to processing and analyzing in
3D content from a large library of edited media, which could be helpful for
many downstream applications. Project page:
https://geopavlakos.github.io/multishot
</p>
<a href="http://arxiv.org/abs/2012.09843" target="_blank">arXiv:2012.09843</a> [<a href="http://arxiv.org/pdf/2012.09843" target="_blank">pdf</a>]

<h2>High-Throughput Synchronous Deep RL. (arXiv:2012.09849v1 [cs.LG])</h2>
<h3>Iou-Jen Liu, Raymond A. Yeh, Alexander G. Schwing</h3>
<p>Deep reinforcement learning (RL) is computationally demanding and requires
processing of many data points. Synchronous methods enjoy training stability
while having lower data throughput. In contrast, asynchronous methods achieve
high throughput but suffer from stability issues and lower sample efficiency
due to `stale policies.' To combine the advantages of both methods we propose
High-Throughput Synchronous Deep Reinforcement Learning (HTS-RL). In HTS-RL, we
perform learning and rollouts concurrently, devise a system design which avoids
`stale policies' and ensure that actors interact with environment replicas in
an asynchronous manner while maintaining full determinism. We evaluate our
approach on Atari games and the Google Research Football environment. Compared
to synchronous baselines, HTS-RL is 2-6$\times$ faster. Compared to
state-of-the-art asynchronous methods, HTS-RL has competitive throughput and
consistently achieves higher average episode rewards.
</p>
<a href="http://arxiv.org/abs/2012.09849" target="_blank">arXiv:2012.09849</a> [<a href="http://arxiv.org/pdf/2012.09849" target="_blank">pdf</a>]

<h2>Worldsheet: Wrapping the World in a 3D Sheet for View Synthesis from a Single Image. (arXiv:2012.09854v1 [cs.CV])</h2>
<h3>Ronghang Hu, Deepak Pathak</h3>
<p>We present Worldsheet, a method for novel view synthesis using just a single
RGB image as input. This is a challenging problem as it requires an
understanding of the 3D geometry of the scene as well as texture mapping to
generate both visible and occluded regions from new view-points. Our main
insight is that simply shrink-wrapping a planar mesh sheet onto the input
image, consistent with the learned intermediate depth, captures underlying
geometry sufficient enough to generate photorealistic unseen views with
arbitrarily large view-point changes. To operationalize this, we propose a
novel differentiable texture sampler that allows our wrapped mesh sheet to be
textured; which is then transformed into a target image via differentiable
rendering. Our approach is category-agnostic, end-to-end trainable without
using any 3D supervision and requires a single image at test time. Worldsheet
consistently outperforms prior state-of-the-art methods on single-image view
synthesis across several datasets. Furthermore, this simple idea captures novel
views surprisingly well on a wide range of high resolution in-the-wild images
in converting them into a navigable 3D pop-up. Video results and code at
https://worldsheet.github.io
</p>
<a href="http://arxiv.org/abs/2012.09854" target="_blank">arXiv:2012.09854</a> [<a href="http://arxiv.org/pdf/2012.09854" target="_blank">pdf</a>]

<h2>Infinite Nature: Perpetual View Generation of Natural Scenes from a Single Image. (arXiv:2012.09855v1 [cs.CV])</h2>
<h3>Andrew Liu, Richard Tucker, Varun Jampani, Ameesh Makadia, Noah Snavely, Angjoo Kanazawa</h3>
<p>We introduce the problem of perpetual view generation -- long-range
generation of novel views corresponding to an arbitrarily long camera
trajectory given a single image. This is a challenging problem that goes far
beyond the capabilities of current view synthesis methods, which work for a
limited range of viewpoints and quickly degenerate when presented with a large
camera motion. Methods designed for video generation also have limited ability
to produce long video sequences and are often agnostic to scene geometry. We
take a hybrid approach that integrates both geometry and image synthesis in an
iterative render, refine, and repeat framework, allowing for long-range
generation that cover large distances after hundreds of frames. Our approach
can be trained from a set of monocular video sequences without any manual
annotation. We propose a dataset of aerial footage of natural coastal scenes,
and compare our method with recent view synthesis and conditional video
generation baselines, showing that it can generate plausible scenes for much
longer time horizons over large camera trajectories compared to existing
methods.
</p>
<a href="http://arxiv.org/abs/2012.09855" target="_blank">arXiv:2012.09855</a> [<a href="http://arxiv.org/pdf/2012.09855" target="_blank">pdf</a>]

<h2>Reconstructing Hand-Object Interactions in the Wild. (arXiv:2012.09856v1 [cs.CV])</h2>
<h3>Zhe Cao, Ilija Radosavovic, Angjoo Kanazawa, Jitendra Malik</h3>
<p>In this work we explore reconstructing hand-object interactions in the wild.
The core challenge of this problem is the lack of appropriate 3D labeled data.
To overcome this issue, we propose an optimization-based procedure which does
not require direct 3D supervision. The general strategy we adopt is to exploit
all available related data (2D bounding boxes, 2D hand keypoints, 2D instance
masks, 3D object models, 3D in-the-lab MoCap) to provide constraints for the 3D
reconstruction. Rather than optimizing the hand and object individually, we
optimize them jointly which allows us to impose additional constraints based on
hand-object contact, collision, and occlusion. Our method produces compelling
reconstructions on the challenging in-the-wild data from the EPIC Kitchens and
the 100 Days of Hands datasets, across a range of object categories.
Quantitatively, we demonstrate that our approach compares favorably to existing
approaches in the lab settings where ground truth 3D annotations are available.
</p>
<a href="http://arxiv.org/abs/2012.09856" target="_blank">arXiv:2012.09856</a> [<a href="http://arxiv.org/pdf/2012.09856" target="_blank">pdf</a>]

<h2>Multi-Kernel Regression with Sparsity Constraint. (arXiv:1811.00836v4 [cs.LG] UPDATED)</h2>
<h3>Shayan Aziznejad, Michael Unser</h3>
<p>In this paper, we provide a Banach-space formulation of supervised learning
with generalized total-variation (gTV) regularization. We identify the class of
kernel functions that are admissible in this framework. Then, we propose a
variation of supervised learning in a continuous-domain hybrid search space
with gTV regularization. We show that the solution admits a multi-kernel
expansion with adaptive positions. In this representation, the number of active
kernels is upper-bounded by the number of data points while the gTV
regularization imposes an $\ell_1$ penalty on the kernel coefficients. Finally,
we illustrate numerically the outcome of our theory.
</p>
<a href="http://arxiv.org/abs/1811.00836" target="_blank">arXiv:1811.00836</a> [<a href="http://arxiv.org/pdf/1811.00836" target="_blank">pdf</a>]

<h2>Pseudo-Rehearsal: Achieving Deep Reinforcement Learning without Catastrophic Forgetting. (arXiv:1812.02464v6 [cs.LG] UPDATED)</h2>
<h3>Craig Atkinson, Brendan McCane, Lech Szymanski, Anthony Robins</h3>
<p>Neural networks can achieve excellent results in a wide variety of
applications. However, when they attempt to sequentially learn, they tend to
learn the new task while catastrophically forgetting previous ones. We propose
a model that overcomes catastrophic forgetting in sequential reinforcement
learning by combining ideas from continual learning in both the image
classification domain and the reinforcement learning domain. This model
features a dual memory system which separates continual learning from
reinforcement learning and a pseudo-rehearsal system that "recalls" items
representative of previous tasks via a deep generative network. Our model
sequentially learns Atari 2600 games without demonstrating catastrophic
forgetting and continues to perform above human level on all three games. This
result is achieved without: demanding additional storage requirements as the
number of tasks increases, storing raw data or revisiting past tasks. In
comparison, previous state-of-the-art solutions are substantially more
vulnerable to forgetting on these complex deep reinforcement learning tasks.
</p>
<a href="http://arxiv.org/abs/1812.02464" target="_blank">arXiv:1812.02464</a> [<a href="http://arxiv.org/pdf/1812.02464" target="_blank">pdf</a>]

<h2>Stabilizing GANs with Soft Octave Convolutions. (arXiv:1905.12534v3 [cs.LG] UPDATED)</h2>
<h3>Ricard Durall, Franz-Josef Pfreundt, Janis Keuper</h3>
<p>Motivated by recently published methods using frequency decompositions of
convolutions (e.g. Octave Convolutions), we propose a novel convolution scheme
to stabilize the training and reduce the likelihood of a mode collapse. The
basic idea of our approach is to split convolutional filters into additive high
and low frequency parts, while shifting weight updates from low to high during
the training. Intuitively, this method forces GANs to learn low frequency
coarse image structures before descending into fine (high frequency) details.
We also show, that the use of the proposed soft octave convolutions reduces
common artifacts in the frequency domain of generated images. Our approach is
orthogonal and complementary to existing stabilization methods and can simply
be plugged into any CNN based GAN architecture. Experiments on the CelebA
dataset show the effectiveness of the proposed method.
</p>
<a href="http://arxiv.org/abs/1905.12534" target="_blank">arXiv:1905.12534</a> [<a href="http://arxiv.org/pdf/1905.12534" target="_blank">pdf</a>]

<h2>Generative Restricted Kernel Machines: A Framework for Multi-view Generation and Disentangled Feature Learning. (arXiv:1906.08144v7 [cs.LG] UPDATED)</h2>
<h3>Arun Pandey, Joachim Schreurs, Johan A. K. Suykens</h3>
<p>This paper introduces a novel framework for generative models based on
Restricted Kernel Machines (RKMs) with joint multi-view generation and
uncorrelated feature learning, called Gen-RKM. To enable joint multi-view
generation, this mechanism uses a shared representation of data from various
views. Furthermore, the model has a primal and dual formulation to incorporate
both kernel-based and (deep convolutional) neural network based models within
the same setting. When using neural networks as explicit feature-maps, a novel
training procedure is proposed, which jointly learns the features and shared
subspace representation. The latent variables are given by the
eigen-decomposition of the kernel matrix, where the mutual orthogonality of
eigenvectors represents the learned uncorrelated features. Experiments
demonstrate the potential of the framework through qualitative and quantitative
evaluation of generated samples on various standard datasets.
</p>
<a href="http://arxiv.org/abs/1906.08144" target="_blank">arXiv:1906.08144</a> [<a href="http://arxiv.org/pdf/1906.08144" target="_blank">pdf</a>]

<h2>DA-RefineNet:A Dual Input Whole Slide Image Segmentation Algorithm Based on Attention. (arXiv:1907.06358v3 [cs.CV] UPDATED)</h2>
<h3>Ziqiang Li, Rentuo Tao, Qianrun Wu, Bin Li</h3>
<p>Automatic medical image segmentation has wide applications for disease
diagnosing. However, it is much more challenging than natural optical image
segmentation due to the high-resolution of medical images and the corresponding
huge computation cost. The sliding window is a commonly used technique for
whole slide image (WSI) segmentation, however, for these methods based on the
sliding window, the main drawback is lacking global contextual information for
supervision. In this paper, we propose a dual-inputs attention network (denoted
as DA-RefineNet) for WSI segmentation, where both local fine-grained
information and global coarse information can be efficiently utilized.
Sufficient comparative experiments are conducted to evaluate the effectiveness
of the proposed method, the results prove that the proposed method can achieve
better performance on WSI segmentation compared to methods relying on
single-input.
</p>
<a href="http://arxiv.org/abs/1907.06358" target="_blank">arXiv:1907.06358</a> [<a href="http://arxiv.org/pdf/1907.06358" target="_blank">pdf</a>]

<h2>Generating adversarial examples in the harsh conditions. (arXiv:1908.11332v3 [cs.LG] UPDATED)</h2>
<h3>Junde Wu</h3>
<p>Deep Neural Networks have been found vulnerable re-cently. A kind of
well-designed inputs, which called adver-sarial examples, can lead the networks
to make incorrectpredictions. Depending on the different scenarios, goalsand
capabilities, the difficulties of the attacks are different.For example, a
targeted attack is more difficult than a non-targeted attack, a universal
attack is more difficult than anon-universal attack, a transferable attack is
more difficultthan a nontransferable one. The question is: Is there existan
attack that can meet all these requirements? In this pa-per, we answer this
question by producing a kind of attacksunder these conditions. We learn a
universal mapping tomap the sources to the adversarial examples. These
exam-ples can fool classification networks to classify all of theminto one
targeted class, and also have strong transferability.Our code is released at:
xxxxx.
</p>
<a href="http://arxiv.org/abs/1908.11332" target="_blank">arXiv:1908.11332</a> [<a href="http://arxiv.org/pdf/1908.11332" target="_blank">pdf</a>]

<h2>Adversarial Defense via Local Flatness Regularization. (arXiv:1910.12165v4 [cs.CV] UPDATED)</h2>
<h3>Jia Xu, Yiming Li, Yong Jiang, Shu-Tao Xia</h3>
<p>Adversarial defense is a popular and important research area. Due to its
intrinsic mechanism, one of the most straightforward and effective ways of
defending attacks is to analyze the property of loss surface in the input
space. In this paper, we define the local flatness of the loss surface as the
maximum value of the chosen norm of the gradient regarding to the input within
a neighborhood centered on the benign sample, and discuss the relationship
between the local flatness and adversarial vulnerability. Based on the
analysis, we propose a novel defense approach via regularizing the local
flatness, dubbed local flatness regularization (LFR). We also demonstrate the
effectiveness of the proposed method from other perspectives, such as human
visual mechanism, and analyze the relationship between LFR and other related
methods theoretically. Experiments are conducted to verify our theory and
demonstrate the superiority of the proposed method.
</p>
<a href="http://arxiv.org/abs/1910.12165" target="_blank">arXiv:1910.12165</a> [<a href="http://arxiv.org/pdf/1910.12165" target="_blank">pdf</a>]

<h2>Prototypical Networks for Multi-Label Learning. (arXiv:1911.07203v2 [cs.LG] UPDATED)</h2>
<h3>Zhuo Yang, Yufei Han, Guoxian Yu, Qiang Yang, Xiangliang Zhang</h3>
<p>We propose to formulate multi-label learning as a estimation of class
distribution in a non-linear embedding space, where for each label, its
positive data embeddings and negative data embeddings distribute compactly to
form a positive component and negative component respectively, while the
positive component and negative component are pushed away from each other. Duo
to the shared embedding space for all labels, the distribution of embeddings
preserves instances' label membership and feature matrix, thus encodes the
feature-label relation and nonlinear label dependency. Labels of a given
instance are inferred in the embedding space by measuring the probabilities of
its belongingness to the positive or negative components of each label.
Specially, the probabilities are modeled as the distance from the given
instance to representative positive or negative prototypes. Extensive
experiments validate that the proposed solution can provide distinctively more
accurate multi-label classification than other state-of-the-art algorithms.
</p>
<a href="http://arxiv.org/abs/1911.07203" target="_blank">arXiv:1911.07203</a> [<a href="http://arxiv.org/pdf/1911.07203" target="_blank">pdf</a>]

<h2>VerSe: A Vertebrae Labelling and Segmentation Benchmark for Multi-detector CT Images. (arXiv:2001.09193v3 [cs.CV] UPDATED)</h2>
<h3>Anjany Sekuboyina, Amirhossein Bayat, Malek E. Husseini, Maximilian L&#xf6;ffler, Hongwei Li, Giles Tetteh, Jan Kuka&#x10d;ka, Christian Payer, Darko &#x160;tern, Martin Urschler, Maodong Chen, Dalong Cheng, Nikolas Lessmann, Yujin Hu, Tianfu Wang, Dong Yang, Daguang Xu, Felix Ambellan, Tamaz Amiranashvili, Moritz Ehlke, Hans Lamecker, Sebastian Lehnert, Marilia Lirio, Nicol&#xe1;s P&#xe9;rez de Olaguer, Heiko Ramm, Manish Sahu, Alexander Tack, Stefan Zachow, Tao Jiang, Xinjun Ma, Christoph Angerman, Xin Wang, Qingyue Wei, Kevin Brown, Matthias Wolf, Alexandre Kirszenberg, &#xc9;lodie Puybareauq, Alexander Valentinitsch, Markus Rempfler, Bj&#xf6;rn H. Menze, Jan S. Kirschke</h3>
<p>Reliable automated processing of spinal images is expected to benefit
decision-support systems for diagnosis, surgery planning, and population-based
analysis on spine and bone health. Vertebral labelling and segmentation are two
fundamental tasks in such an automated pipeline. Centred around these tasks,
the Large Scale Vertebrae Segmentation Challenge (VerSe) was organised in
conjunction with the International Conference on Medical Image Computing and
Computer Assisted Intervention (MICCAI) 2019. This work is a technical report
summarising the challenge's findings. A total of 160 multi-detector CT scans
closely resembling a typical spine-centred clinical setting were prepared and
annotated at voxel-level by a human-machine hybrid algorithm. Both the
annotation protocol and the algorithm that aided the medical experts in this
annotation process are presented. Eleven fully automated algorithms of the
participating teams were benchmarked on the VerSe data. A detailed performance
comparison of these algorithms along with insights into their design are
presented. The best-performing algorithm achieved a vertebrae identification
rate of 95\% and a Dice coefficient of 90% on a hidden test set. As an
open-call challenge, VerSe'19's annotated image data and its evaluation tools
will continue to be publicly accessible through its online portal.
</p>
<a href="http://arxiv.org/abs/2001.09193" target="_blank">arXiv:2001.09193</a> [<a href="http://arxiv.org/pdf/2001.09193" target="_blank">pdf</a>]

<h2>Temporal Sparse Adversarial Attack on Sequence-based Gait Recognition. (arXiv:2002.09674v2 [cs.CV] UPDATED)</h2>
<h3>Ziwen He, Wei Wang, Jing Dong, Tieniu Tan</h3>
<p>Gait recognition is widely used in social security applications due to its
advantages in long-distance human identification. Recently, sequence-based
methods have achieved high accuracy by learning abundant temporal and spatial
information. However, their robustness under adversarial attacks has not been
clearly explored. In this paper, we demonstrate that the state-of-the-art gait
recognition model is vulnerable to such attacks. To this end, we propose a
novel temporal sparse adversarial attack method. Different from previous
additive noise models which add perturbations on original samples, we employ a
generative adversarial network based architecture to semantically generate
adversarial high-quality gait silhouettes or video frames. Moreover, by
sparsely substituting or inserting a few adversarial gait silhouettes, the
proposed method ensures its imperceptibility and achieves a high attack success
rate. The experimental results show that if only one-fortieth of the frames are
attacked, the accuracy of the target model drops dramatically.
</p>
<a href="http://arxiv.org/abs/2002.09674" target="_blank">arXiv:2002.09674</a> [<a href="http://arxiv.org/pdf/2002.09674" target="_blank">pdf</a>]

<h2>CAKES: Channel-wise Automatic KErnel Shrinking for Efficient 3D Networks. (arXiv:2003.12798v3 [cs.CV] UPDATED)</h2>
<h3>Qihang Yu, Yingwei Li, Jieru Mei, Yuyin Zhou, Alan L. Yuille</h3>
<p>3D Convolution Neural Networks (CNNs) have been widely applied to 3D scene
understanding, such as video analysis and volumetric image recognition.
However, 3D networks can easily lead to over-parameterization which incurs
expensive computation cost. In this paper, we propose Channel-wise Automatic
KErnel Shrinking (CAKES), to enable efficient 3D learning by shrinking standard
3D convolutions into a set of economic operations e.g., 1D, 2D convolutions.
Unlike previous methods, CAKES performs channel-wise kernel shrinkage, which
enjoys the following benefits: 1) enabling operations deployed in every layer
to be heterogeneous, so that they can extract diverse and complementary
information to benefit the learning process; and 2) allowing for an efficient
and flexible replacement design, which can be generalized to both
spatial-temporal and volumetric data. Further, we propose a new search space
based on CAKES, so that the replacement configuration can be determined
automatically for simplifying 3D networks. CAKES shows superior performance to
other methods with similar model size, and it also achieves comparable
performance to state-of-the-art with much fewer parameters and computational
costs on tasks including 3D medical imaging segmentation and video action
recognition. Codes and models are available at
https://github.com/yucornetto/CAKES
</p>
<a href="http://arxiv.org/abs/2003.12798" target="_blank">arXiv:2003.12798</a> [<a href="http://arxiv.org/pdf/2003.12798" target="_blank">pdf</a>]

<h2>Compositional Visual Generation and Inference with Energy Based Models. (arXiv:2004.06030v3 [cs.CV] UPDATED)</h2>
<h3>Yilun Du, Shuang Li, Igor Mordatch</h3>
<p>A vital aspect of human intelligence is the ability to compose increasingly
complex concepts out of simpler ideas, enabling both rapid learning and
adaptation of knowledge. In this paper we show that energy-based models can
exhibit this ability by directly combining probability distributions. Samples
from the combined distribution correspond to compositions of concepts. For
example, given a distribution for smiling faces, and another for male faces, we
can combine them to generate smiling male faces. This allows us to generate
natural images that simultaneously satisfy conjunctions, disjunctions, and
negations of concepts. We evaluate compositional generation abilities of our
model on the CelebA dataset of natural faces and synthetic 3D scene images. We
also demonstrate other unique advantages of our model, such as the ability to
continually learn and incorporate new concepts, or infer compositions of
concept properties underlying an image.
</p>
<a href="http://arxiv.org/abs/2004.06030" target="_blank">arXiv:2004.06030</a> [<a href="http://arxiv.org/pdf/2004.06030" target="_blank">pdf</a>]

<h2>Models Genesis. (arXiv:2004.07882v4 [cs.CV] UPDATED)</h2>
<h3>Zongwei Zhou, Vatsal Sodha, Jiaxuan Pang, Michael B. Gotway, Jianming Liang</h3>
<p>Transfer learning from natural images to medical images has been established
as one of the most practical paradigms in deep learning for medical image
analysis. To fit this paradigm, however, 3D imaging tasks in the most prominent
imaging modalities (e.g., CT and MRI) have to be reformulated and solved in 2D,
losing rich 3D anatomical information, thereby inevitably compromising its
performance. To overcome this limitation, we have built a set of models, called
Generic Autodidactic Models, nicknamed Models Genesis, because they are created
ex nihilo (with no manual labeling), self-taught (learnt by self-supervision),
and generic (served as source models for generating application-specific target
models). Our extensive experiments demonstrate that our Models Genesis
significantly outperform learning from scratch and existing pre-trained 3D
models in all five target 3D applications covering both segmentation and
classification. More importantly, learning a model from scratch simply in 3D
may not necessarily yield performance better than transfer learning from
ImageNet in 2D, but our Models Genesis consistently top any 2D/2.5D approaches
including fine-tuning the models pre-trained from ImageNet as well as
fine-tuning the 2D versions of our Models Genesis, confirming the importance of
3D anatomical information and significance of Models Genesis for 3D medical
imaging. This performance is attributed to our unified self-supervised learning
framework, built on a simple yet powerful observation: the sophisticated and
recurrent anatomy in medical images can serve as strong yet free supervision
signals for deep models to learn common anatomical representation automatically
via self-supervision. As open science, all codes and pre-trained Models Genesis
are available at https://github.com/MrGiovanni/ModelsGenesis.
</p>
<a href="http://arxiv.org/abs/2004.07882" target="_blank">arXiv:2004.07882</a> [<a href="http://arxiv.org/pdf/2004.07882" target="_blank">pdf</a>]

<h2>MangaGAN: Unpaired Photo-to-Manga Translation Based on The Methodology of Manga Drawing. (arXiv:2004.10634v2 [cs.CV] UPDATED)</h2>
<h3>Hao Su, Jianwei Niu, Xuefeng Liu, Qingfeng Li, Jiahe Cui, Ji Wan</h3>
<p>Manga is a world popular comic form originated in Japan, which typically
employs black-and-white stroke lines and geometric exaggeration to describe
humans' appearances, poses, and actions. In this paper, we propose MangaGAN,
the first method based on Generative Adversarial Network (GAN) for unpaired
photo-to-manga translation. Inspired by how experienced manga artists draw
manga, MangaGAN generates the geometric features of manga face by a designed
GAN model and delicately translates each facial region into the manga domain by
a tailored multi-GANs architecture. For training MangaGAN, we construct a new
dataset collected from a popular manga work, containing manga facial features,
landmarks, bodies, and so on. Moreover, to produce high-quality manga faces, we
further propose a structural smoothing loss to smooth stroke-lines and avoid
noisy pixels, and a similarity preserving module to improve the similarity
between domains of photo and manga. Extensive experiments show that MangaGAN
can produce high-quality manga faces which preserve both the facial similarity
and a popular manga style, and outperforms other related state-of-the-art
methods.
</p>
<a href="http://arxiv.org/abs/2004.10634" target="_blank">arXiv:2004.10634</a> [<a href="http://arxiv.org/pdf/2004.10634" target="_blank">pdf</a>]

<h2>Improving the Interpretability of fMRI Decoding using Deep Neural Networks and Adversarial Robustness. (arXiv:2004.11114v3 [cs.LG] UPDATED)</h2>
<h3>Patrick McClure, Dustin Moraczewski, Ka Chun Lam, Adam Thomas, Francisco Pereira</h3>
<p>Deep neural networks (DNNs) are being increasingly used to make predictions
from functional magnetic resonance imaging (fMRI) data. However, they are
widely seen as uninterpretable "black boxes", as it can be difficult to
discover what input information is used by the DNN in the process, something
important in both cognitive neuroscience and clinical applications. A saliency
map is a common approach for producing interpretable visualizations of the
relative importance of input features for a prediction. However, methods for
creating maps often fail due to DNNs being sensitive to input noise, or by
focusing too much on the input and too little on the model. It is also
challenging to evaluate how well saliency maps correspond to the truly relevant
input information, as ground truth is not always available. In this paper, we
review a variety of methods for producing gradient-based saliency maps, and
present a new adversarial training method we developed to make DNNs robust to
input noise, with the goal of improving interpretability. We introduce two
quantitative evaluation procedures for saliency map methods in fMRI, applicable
whenever a DNN or linear model is being trained to decode some information from
imaging data. We evaluate the procedures using a synthetic dataset where the
complex activation structure is known, and on saliency maps produced for DNN
and linear models for task decoding in the Human Connectome Project (HCP)
dataset. Our key finding is that saliency maps produced with different methods
vary widely in interpretability, in both in synthetic and HCP fMRI data.
Strikingly, even when DNN and linear models decode at comparable levels of
performance, DNN saliency maps score higher on interpretability than linear
model saliency maps (derived via weights or gradient). Finally, saliency maps
produced with our adversarial training method outperform those from other
methods.
</p>
<a href="http://arxiv.org/abs/2004.11114" target="_blank">arXiv:2004.11114</a> [<a href="http://arxiv.org/pdf/2004.11114" target="_blank">pdf</a>]

<h2>Semi-Lexical Languages -- A Formal Basis for Unifying Machine Learning and Symbolic Reasoning in Computer Vision. (arXiv:2004.12152v2 [cs.AI] UPDATED)</h2>
<h3>Briti Gangopadhyay, Somnath Hazra, Pallab Dasgupta</h3>
<p>Human vision is able to compensate imperfections in sensory inputs from the
real world by reasoning based on prior knowledge about the world. Machine
learning has had a significant impact on computer vision due to its inherent
ability in handling imprecision, but the absence of a reasoning framework based
on domain knowledge limits its ability to interpret complex scenarios. We
propose semi-lexical languages as a formal basis for dealing with imperfect
tokens provided by the real world. The power of machine learning is used to map
the imperfect tokens into the alphabet of the language and symbolic reasoning
is used to determine the membership of input in the language. Semi-lexical
languages also have bindings that prevent the variations in which a
semi-lexical token is interpreted in different parts of the input, thereby
leaning on deduction to enhance the quality of recognition of individual
tokens. We present case studies that demonstrate the advantage of using such a
framework over pure machine learning and pure symbolic methods.
</p>
<a href="http://arxiv.org/abs/2004.12152" target="_blank">arXiv:2004.12152</a> [<a href="http://arxiv.org/pdf/2004.12152" target="_blank">pdf</a>]

<h2>Style-transfer GANs for bridging the domain gap in synthetic pose estimator training. (arXiv:2004.13681v2 [cs.CV] UPDATED)</h2>
<h3>Pavel Rojtberg, Thomas P&#xf6;llabauer, Arjan Kuijper</h3>
<p>Given the dependency of current CNN architectures on a large training set,
the possibility of using synthetic data is alluring as it allows generating a
virtually infinite amount of labeled training data. However, producing such
data is a non-trivial task as current CNN architectures are sensitive to the
domain gap between real and synthetic data. We propose to adopt general-purpose
GAN models for pixel-level image translation, allowing to formulate the domain
gap itself as a learning problem. The obtained models are then used either
during training or inference to bridge the domain gap. Here, we focus on
training the single-stage YOLO6D object pose estimator on synthetic CAD
geometry only, where not even approximate surface information is available.
When employing paired GAN models, we use an edge-based intermediate domain and
introduce different mappings to represent the unknown surface properties. Our
evaluation shows a considerable improvement in model performance when compared
to a model trained with the same degree of domain randomization, while
requiring only very little additional effort.
</p>
<a href="http://arxiv.org/abs/2004.13681" target="_blank">arXiv:2004.13681</a> [<a href="http://arxiv.org/pdf/2004.13681" target="_blank">pdf</a>]

<h2>XEM: An Explainable Ensemble Method for Multivariate Time Series Classification. (arXiv:2005.03645v4 [cs.LG] UPDATED)</h2>
<h3>Kevin Fauvel, &#xc9;lisa Fromont, V&#xe9;ronique Masson, Philippe Faverdin, Alexandre Termier</h3>
<p>We present XEM, an eXplainable Ensemble method for Multivariate time series
classification. XEM relies on a new hybrid ensemble method that combines an
explicit boosting-bagging approach to handle the bias-variance trade-off faced
by machine learning models and an implicit divide-and-conquer approach to
individualize classifier errors on different parts of the training data. Our
evaluation shows that XEM outperforms the state-of-the-art MTS classifiers on
the UEA datasets. Furthermore, XEM provides faithful explainability by design
and manifests robust performance when faced with challenges arising from
continuous data collection (different MTS length, missing data and noise).
</p>
<a href="http://arxiv.org/abs/2005.03645" target="_blank">arXiv:2005.03645</a> [<a href="http://arxiv.org/pdf/2005.03645" target="_blank">pdf</a>]

<h2>Fake face detection via adaptive manipulation traces extraction network. (arXiv:2005.04945v2 [cs.CV] UPDATED)</h2>
<h3>Zhiqing Guo, Gaobo Yang, Jiyou Chen, Xingming Sun</h3>
<p>With the proliferation of face image manipulation (FIM) techniques such as
Face2Face and Deepfake, more fake face images are spreading over the internet,
which brings serious challenges to public confidence. Face image forgery
detection has made considerable progresses in exposing specific FIM, but it is
still in scarcity of a robust fake face detector to expose face image forgeries
under complex scenarios such as with further compression, blurring, scaling,
etc. Due to the relatively fixed structure, convolutional neural network (CNN)
tends to learn image content representations. However, CNN should learn subtle
manipulation traces for image forensics tasks. Thus, we propose an adaptive
manipulation traces extraction network (AMTEN), which serves as pre-processing
to suppress image content and highlight manipulation traces. AMTEN exploits an
adaptive convolution layer to predict manipulation traces in the image, which
are reused in subsequent layers to maximize manipulation artifacts by updating
weights during the back-propagation pass. A fake face detector, namely
AMTENnet, is constructed by integrating AMTEN with CNN. Experimental results
prove that the proposed AMTEN achieves desirable pre-processing. When detecting
fake face images generated by various FIM techniques, AMTENnet achieves an
average accuracy up to 98.52%, which outperforms the state-of-the-art works.
When detecting face images with unknown post-processing operations, the
detector also achieves an average accuracy of 95.17%.
</p>
<a href="http://arxiv.org/abs/2005.04945" target="_blank">arXiv:2005.04945</a> [<a href="http://arxiv.org/pdf/2005.04945" target="_blank">pdf</a>]

<h2>Multi-View Collaborative Network Embedding. (arXiv:2005.08189v2 [cs.LG] UPDATED)</h2>
<h3>Sezin Kircali Ata, Yuan Fang, Min Wu, Jiaqi Shi, Chee Keong Kwoh, Xiaoli Li</h3>
<p>Real-world networks often exist with multiple views, where each view
describes one type of interaction among a common set of nodes. For example, on
a video-sharing network, while two user nodes are linked if they have common
favorite videos in one view, they can also be linked in another view if they
share common subscribers. Unlike traditional single-view networks, multiple
views maintain different semantics to complement each other. In this paper, we
propose MANE, a multi-view network embedding approach to learn low-dimensional
representations. Similar to existing studies, MANE hinges on diversity and
collaboration - while diversity enables views to maintain their individual
semantics, collaboration enables views to work together. However, we also
discover a novel form of second-order collaboration that has not been explored
previously, and further unify it into our framework to attain superior node
representations. Furthermore, as each view often has varying importance w.r.t.
different nodes, we propose MANE+, an attention-based extension of MANE to
model node-wise view importance. Finally, we conduct comprehensive experiments
on three public, real-world multi-view networks, and the results demonstrate
that our models consistently outperform state-of-the-art approaches.
</p>
<a href="http://arxiv.org/abs/2005.08189" target="_blank">arXiv:2005.08189</a> [<a href="http://arxiv.org/pdf/2005.08189" target="_blank">pdf</a>]

<h2>Parsimonious neural networks learn interpretable physical laws. (arXiv:2005.11144v3 [cs.LG] UPDATED)</h2>
<h3>Saaketh Desai, Alejandro Strachan</h3>
<p>Machine learning is playing an increasing role in the physical sciences and
significant progress has been made towards embedding domain knowledge into
models. Less explored is its use to discover interpretable physical laws from
data. We propose parsimonious neural networks (PNNs) that combine neural
networks with evolutionary optimization to find models that balance accuracy
with parsimony. The power and versatility of the approach is demonstrated by
developing models for classical mechanics and to predict the melting
temperature of materials from fundamental properties. In the first example, the
resulting PNNs are easily interpretable as Newton's second law, expressed as a
non-trivial time integrator that exhibits time-reversibility and conserves
energy, where the parsimony is critical to extract underlying symmetries from
the data. In the second case, the PNNs not only find the celebrated Lindemann
melting law, but also new relationships that outperform it in the pareto sense
of parsimony vs. accuracy.
</p>
<a href="http://arxiv.org/abs/2005.11144" target="_blank">arXiv:2005.11144</a> [<a href="http://arxiv.org/pdf/2005.11144" target="_blank">pdf</a>]

<h2>A Performance-Explainability Framework to Benchmark Machine Learning Methods: Application to Multivariate Time Series Classifiers. (arXiv:2005.14501v4 [cs.LG] UPDATED)</h2>
<h3>Kevin Fauvel, V&#xe9;ronique Masson, &#xc9;lisa Fromont</h3>
<p>Our research aims to propose a new performance-explainability analytical
framework to assess and benchmark machine learning methods. The framework
details a set of characteristics that systematize the
performance-explainability assessment of existing machine learning methods. In
order to illustrate the use of the framework, we apply it to benchmark the
current state-of-the-art multivariate time series classifiers.
</p>
<a href="http://arxiv.org/abs/2005.14501" target="_blank">arXiv:2005.14501</a> [<a href="http://arxiv.org/pdf/2005.14501" target="_blank">pdf</a>]

<h2>Neural Power Units. (arXiv:2006.01681v4 [cs.LG] UPDATED)</h2>
<h3>Niklas Heim, Tom&#xe1;&#x161; Pevn&#xfd;, V&#xe1;clav &#x160;m&#xed;dl</h3>
<p>Conventional Neural Networks can approximate simple arithmetic operations,
but fail to generalize beyond the range of numbers that were seen during
training. Neural Arithmetic Units aim to overcome this difficulty, but current
arithmetic units are either limited to operate on positive numbers or can only
represent a subset of arithmetic operations. We introduce the Neural Power Unit
(NPU) that operates on the full domain of real numbers and is capable of
learning arbitrary power functions in a single layer. The NPU thus fixes the
shortcomings of existing arithmetic units and extends their expressivity. We
achieve this by using complex arithmetic without requiring a conversion of the
network to complex numbers. A simplification of the unit to the RealNPU yields
a highly transparent model. We show that the NPUs outperform their competitors
in terms of accuracy and sparsity on artificial arithmetic datasets, and that
the RealNPU can discover the governing equations of a dynamical system only
from data.
</p>
<a href="http://arxiv.org/abs/2006.01681" target="_blank">arXiv:2006.01681</a> [<a href="http://arxiv.org/pdf/2006.01681" target="_blank">pdf</a>]

<h2>Weakly-supervised Temporal Action Localization by Uncertainty Modeling. (arXiv:2006.07006v3 [cs.CV] UPDATED)</h2>
<h3>Pilhyeon Lee, Jinglu Wang, Yan Lu, Hyeran Byun</h3>
<p>Weakly-supervised temporal action localization aims to learn detecting
temporal intervals of action classes with only video-level labels. To this end,
it is crucial to separate frames of action classes from the background frames
(i.e., frames not belonging to any action classes). In this paper, we present a
new perspective on background frames where they are modeled as
out-of-distribution samples regarding their inconsistency. Then, background
frames can be detected by estimating the probability of each frame being
out-of-distribution, known as uncertainty, but it is infeasible to directly
learn uncertainty without frame-level labels. To realize the uncertainty
learning in the weakly-supervised setting, we leverage the multiple instance
learning formulation. Moreover, we further introduce a background entropy loss
to better discriminate background frames by encouraging their in-distribution
(action) probabilities to be uniformly distributed over all action classes.
Experimental results show that our uncertainty modeling is effective at
alleviating the interference of background frames and brings a large
performance gain without bells and whistles. We demonstrate that our model
significantly outperforms state-of-the-art methods on the benchmarks, THUMOS'14
and ActivityNet (1.2 &amp; 1.3). Our code is available at
https://github.com/Pilhyeon/WTAL-Uncertainty-Modeling.
</p>
<a href="http://arxiv.org/abs/2006.07006" target="_blank">arXiv:2006.07006</a> [<a href="http://arxiv.org/pdf/2006.07006" target="_blank">pdf</a>]

<h2>Denoising Diffusion Probabilistic Models. (arXiv:2006.11239v2 [cs.LG] UPDATED)</h2>
<h3>Jonathan Ho, Ajay Jain, Pieter Abbeel</h3>
<p>We present high quality image synthesis results using diffusion probabilistic
models, a class of latent variable models inspired by considerations from
nonequilibrium thermodynamics. Our best results are obtained by training on a
weighted variational bound designed according to a novel connection between
diffusion probabilistic models and denoising score matching with Langevin
dynamics, and our models naturally admit a progressive lossy decompression
scheme that can be interpreted as a generalization of autoregressive decoding.
On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and
a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality
similar to ProgressiveGAN. Our implementation is available at
https://github.com/hojonathanho/diffusion
</p>
<a href="http://arxiv.org/abs/2006.11239" target="_blank">arXiv:2006.11239</a> [<a href="http://arxiv.org/pdf/2006.11239" target="_blank">pdf</a>]

<h2>A General Class of Transfer Learning Regression without Implementation Cost. (arXiv:2006.13228v2 [stat.ML] UPDATED)</h2>
<h3>Shunya Minami, Song Liu, Stephen Wu, Kenji Fukumizu, Ryo Yoshida</h3>
<p>We propose a novel framework that unifies and extends existing methods of
transfer learning (TL) for regression. To bridge a pretrained source model to
the model on a target task, we introduce a density-ratio reweighting function,
which is estimated through the Bayesian framework with a specific prior
distribution. By changing two intrinsic hyperparameters and the choice of the
density-ratio model, the proposed method can integrate three popular methods of
TL: TL based on cross-domain similarity regularization, a probabilistic TL
using the density-ratio estimation, and fine-tuning of pretrained neural
networks. Moreover, the proposed method can benefit from its simple
implementation without any additional cost; the regression model can be fully
trained using off-the-shelf libraries for supervised learning in which the
original output variable is simply transformed to a new output variable. We
demonstrate its simplicity, generality, and applicability using various real
data applications.
</p>
<a href="http://arxiv.org/abs/2006.13228" target="_blank">arXiv:2006.13228</a> [<a href="http://arxiv.org/pdf/2006.13228" target="_blank">pdf</a>]

<h2>Efficient and Parallel Separable Dictionary Learning. (arXiv:2007.03800v3 [cs.LG] UPDATED)</h2>
<h3>Cristian Rusu, Paul Irofti</h3>
<p>Separable, or Kronecker product, dictionaries provide natural decompositions
for 2D signals, such as images. In this paper, we describe a highly
parallelizable algorithm to learn such dictionaries which reaches sparse
representations competitive with the previous state of the art dictionary
learning algorithms from the literature but at a lower computational cost. We
highlight the performance of the proposed method to sparsely represent image
and hyperspectral data, and for image denoising.
</p>
<a href="http://arxiv.org/abs/2007.03800" target="_blank">arXiv:2007.03800</a> [<a href="http://arxiv.org/pdf/2007.03800" target="_blank">pdf</a>]

<h2>Deep Contextual Clinical Prediction with Reverse Distillation. (arXiv:2007.05611v2 [cs.LG] UPDATED)</h2>
<h3>Rohan S. Kodialam, Rebecca Boiarsky, Justin Lim, Neil Dixit, Aditya Sai, David Sontag</h3>
<p>Healthcare providers are increasingly using machine learning to predict
patient outcomes to make meaningful interventions. However, despite innovations
in this area, deep learning models often struggle to match performance of
shallow linear models in predicting these outcomes, making it difficult to
leverage such techniques in practice. In this work, motivated by the task of
clinical prediction from insurance claims, we present a new technique called
Reverse Distillation which pretrains deep models by using high-performing
linear models for initialization. We make use of the longitudinal structure of
insurance claims datasets to develop Self Attention with Reverse Distillation,
or SARD, an architecture that utilizes a combination of contextual embedding,
temporal embedding and self-attention mechanisms and most critically is trained
via reverse distillation. SARD outperforms state-of-the-art methods on multiple
clinical prediction outcomes, with ablation studies revealing that reverse
distillation is a primary driver of these improvements. Code is available at
https://github.com/clinicalml/omop-learn.
</p>
<a href="http://arxiv.org/abs/2007.05611" target="_blank">arXiv:2007.05611</a> [<a href="http://arxiv.org/pdf/2007.05611" target="_blank">pdf</a>]

<h2>Universal Approximation Power of Deep Residual Neural Networks via Nonlinear Control Theory. (arXiv:2007.06007v3 [cs.LG] UPDATED)</h2>
<h3>Paulo Tabuada, Bahman Gharesifard</h3>
<p>In this paper, we explain the universal approximation capabilities of deep
residual neural networks through geometric nonlinear control. Inspired by
recent work establishing links between residual networks and control systems,
we provide a general sufficient condition for a residual network to have the
power of universal approximation by asking the activation function, or one of
its derivatives, to satisfy a quadratic differential equation. Many activation
functions used in practice satisfy this assumption, exactly or approximately,
and we show this property to be sufficient for an adequately deep neural
network with $n+1$ neurons per layer to approximate arbitrarily well, on a
compact set and with respect to the supremum norm, any continuous function from
$\mathbb{R}^n$ to $\mathbb{R}^n$. We further show this result to hold for very
simple architectures for which the weights only need to assume two values. The
first key technical contribution consists of relating the universal
approximation problem to controllability of an ensemble of control systems
corresponding to a residual network and to leverage classical Lie algebraic
techniques to characterize controllability. The second technical contribution
is to identify monotonicity as the bridge between controllability of finite
ensembles and uniform approximability on compact sets.
</p>
<a href="http://arxiv.org/abs/2007.06007" target="_blank">arXiv:2007.06007</a> [<a href="http://arxiv.org/pdf/2007.06007" target="_blank">pdf</a>]

<h2>CrossTransformers: spatially-aware few-shot transfer. (arXiv:2007.11498v4 [cs.CV] UPDATED)</h2>
<h3>Carl Doersch, Ankush Gupta, Andrew Zisserman</h3>
<p>Given new tasks with very little data$-$such as new classes in a
classification problem or a domain shift in the input$-$performance of modern
vision systems degrades remarkably quickly. In this work, we illustrate how the
neural network representations which underpin modern vision systems are subject
to supervision collapse, whereby they lose any information that is not
necessary for performing the training task, including information that may be
necessary for transfer to new tasks or domains. We then propose two methods to
mitigate this problem. First, we employ self-supervised learning to encourage
general-purpose features that transfer better. Second, we propose a novel
Transformer based neural network architecture called CrossTransformers, which
can take a small number of labeled images and an unlabeled query, find coarse
spatial correspondence between the query and the labeled images, and then infer
class membership by computing distances between spatially-corresponding
features. The result is a classifier that is more robust to task and domain
shift, which we demonstrate via state-of-the-art performance on Meta-Dataset, a
recent dataset for evaluating transfer from ImageNet to many other vision
datasets.
</p>
<a href="http://arxiv.org/abs/2007.11498" target="_blank">arXiv:2007.11498</a> [<a href="http://arxiv.org/pdf/2007.11498" target="_blank">pdf</a>]

<h2>Receding Horizon Control Based Online Motion Planning with Partially Infeasible LTL Specifications. (arXiv:2007.12123v4 [cs.RO] UPDATED)</h2>
<h3>Mingyu Cai, Hao Peng, Zhijun Li, Hongbo Gao, Zhen Kan</h3>
<p>This work considers online optimal motion planning of an autonomous agent
subject to linear temporal logic (LTL) constraints. The environment is dynamic
in the sense of containing mobile obstacles and time-varying areas of interest
(i.e., time-varying reward and workspace properties) to be visited by the
agent. Since user-specified tasks may not be fully realized (i.e., partially
infeasible), this work considers hard and soft LTL constraints, where hard
constraints enforce safety requirement (e.g. avoid obstacles) while soft
constraints represent tasks that can be relaxed to not strictly follow user
specifications. The motion planning of the agent is to generate policies, in
decreasing order of priority, to 1) formally guarantee the satisfaction of
safety constraints; 2) mostly satisfy soft constraints (i.e., minimize the
violation cost if desired tasks are partially infeasible); and 3) optimize the
objective of rewards collection (i.e., visiting dynamic areas of more
interests). To achieve these objectives, a relaxed product automaton, which
allows the agent to not strictly follow the desired LTL constraints, is
constructed. A utility function is developed to quantify the differences
between the revised and the desired motion plan, and the accumulated rewards
are designed to bias the motion plan towards those areas of more interests.
Receding horizon control is synthesized with an LTL formula to maximize the
accumulated utilities over a finite horizon, while ensuring that safety
constraints are fully satisfied and soft constraints are mostly satisfied.
Simulation and experiment results are provided to demonstrate the effectiveness
of the developed motion strategy.
</p>
<a href="http://arxiv.org/abs/2007.12123" target="_blank">arXiv:2007.12123</a> [<a href="http://arxiv.org/pdf/2007.12123" target="_blank">pdf</a>]

<h2>Machine-learned Regularization and Polygonization of Building Segmentation Masks. (arXiv:2007.12587v3 [cs.CV] UPDATED)</h2>
<h3>Stefano Zorzi, Ksenia Bittner, Friedrich Fraundorfer</h3>
<p>We propose a machine learning based approach for automatic regularization and
polygonization of building segmentation masks. Taking an image as input, we
first predict building segmentation maps exploiting generic fully convolutional
network (FCN). A generative adversarial network (GAN) is then involved to
perform a regularization of building boundaries to make them more realistic,
i.e., having more rectilinear outlines which construct right angles if
required. This is achieved through the interplay between the discriminator
which gives a probability of input image being true and generator that learns
from discriminator's response to create more realistic images. Finally, we
train the backbone convolutional neural network (CNN) which is adapted to
predict sparse outcomes corresponding to building corners out of regularized
building segmentation results. Experiments on three building segmentation
datasets demonstrate that the proposed method is not only capable of obtaining
accurate results, but also of producing visually pleasing building outlines
parameterized as polygons.
</p>
<a href="http://arxiv.org/abs/2007.12587" target="_blank">arXiv:2007.12587</a> [<a href="http://arxiv.org/pdf/2007.12587" target="_blank">pdf</a>]

<h2>Second Order PAC-Bayesian Bounds for the Weighted Majority Vote. (arXiv:2007.13532v2 [cs.LG] UPDATED)</h2>
<h3>Andr&#xe9;s R. Masegosa, Stephan S. Lorenzen, Christian Igel, Yevgeny Seldin</h3>
<p>We present a novel analysis of the expected risk of weighted majority vote in
multiclass classification. The analysis takes correlation of predictions by
ensemble members into account and provides a bound that is amenable to
efficient minimization, which yields improved weighting for the majority vote.
We also provide a specialized version of our bound for binary classification,
which allows to exploit additional unlabeled data for tighter risk estimation.
In experiments, we apply the bound to improve weighting of trees in random
forests and show that, in contrast to the commonly used first order bound,
minimization of the new bound typically does not lead to degradation of the
test error of the ensemble.
</p>
<a href="http://arxiv.org/abs/2007.13532" target="_blank">arXiv:2007.13532</a> [<a href="http://arxiv.org/pdf/2007.13532" target="_blank">pdf</a>]

<h2>UNIPoint: Universally Approximating Point Processes Intensities. (arXiv:2007.14082v3 [cs.LG] UPDATED)</h2>
<h3>Alexander Soen, Alexander Mathews, Daniel Grixti-Cheng, Lexing Xie</h3>
<p>Point processes are a useful mathematical tool for describing events over
time, and there are many recent approaches for representing and learning them.
One notable open question is how to precisely describe the flexibility of the
various models, and whether there exists a general model that can represent all
point processes. Our work bridges this gap. Focusing on the widely used event
intensity function representation of point processes, we provide a proof that a
class of learnable functions can universally approximate any valid intensity
function. The proof connects the well known Stone-Weierstrass Theorem for
function approximation, the uniform density of non-negative continuous
functions using a transfer functions, the formulation of the parameters of a
piece-wise continuous functions as a dynamical system, and recurrent neural
networks for capturing the dynamics. Using these insights, we design and
implement UNIPoint, a novel neural point process model, using recurrent neural
networks to parameterise sums of basis function upon each event. Evaluations on
synthetic and real world datasets show that this simpler representation
performs better than Hawkes process variants and more complex neural
network-based approaches. We expect this result will provide a basis for
practically selecting and tuning models, as well as furthering theoretical work
on fine-grained characterisation of representational complexity versus
expressiveness.
</p>
<a href="http://arxiv.org/abs/2007.14082" target="_blank">arXiv:2007.14082</a> [<a href="http://arxiv.org/pdf/2007.14082" target="_blank">pdf</a>]

<h2>Regularization and Normalization For Generative Adversarial Networks: A Survey. (arXiv:2008.08930v3 [cs.LG] UPDATED)</h2>
<h3>Ziqiang Li, Rentuo Tao, Pengfei Xia, Huanhuan Chen, Bin Li</h3>
<p>Generative Adversarial Networks (GANs), a popular generative model, have been
widely applied in different scenarios thanks to the development of deep neural
networks. The proposal of standard GAN is based upon the non-parametric
assumption of the infinite capacity of networks. It is still unknown whether
GANs can generate realistic samples without any prior. Due to excessive
assumptions, many issues need to be addressed in GANs training, such as
non-convergence, mode collapses, gradient disappearance, and the sensitivity of
hyperparameters. As acknowledged, regularization and normalization are common
methods of introducing prior information and can be used for stability training
as well. At present, many regularization and normalization methods are proposed
in GANs.In order to explain these methods in a systematic manner, this paper
summarizes regularization and normalization methods used in GANs and classifies
them into seven groups: Gradient penalty, Norm normalization and
regularization, Jacobian regularization, Layer normalization, Consistency
regularization, Data Augmentation, and Self-supervision. This paper presents
the analysis of these methods and highlights the possible future studies in
this area.
</p>
<a href="http://arxiv.org/abs/2008.08930" target="_blank">arXiv:2008.08930</a> [<a href="http://arxiv.org/pdf/2008.08930" target="_blank">pdf</a>]

<h2>Automating the assessment of biofouling in images using expert agreement as a gold standard. (arXiv:2008.09289v2 [cs.CV] UPDATED)</h2>
<h3>Nathaniel J. Bloomfield, Susan Wei, Bartholomew Woodham, Peter Wilkinson, Andrew Robinson</h3>
<p>Biofouling is the accumulation of organisms on surfaces immersed in water. It
is of particular concern to the international shipping industry because it
increases fuel costs and presents a biosecurity risk by providing a pathway for
non-indigenous marine species to establish in new areas. There is growing
interest within jurisdictions to strengthen biofouling risk-management
regulations, but it is expensive to conduct in-water inspections and assess the
collected data to determine the biofouling state of vessel hulls. Machine
learning is well suited to tackle the latter challenge, and here we apply deep
learning to automate the classification of images from in-water inspections to
identify the presence and severity of fouling. We combined several datasets to
obtain over 10,000 images collected from in-water surveys which were annotated
by a group biofouling experts. We compared the annotations from three experts
on a 120-sample subset of these images, and found that they showed 89%
agreement (95% CI: 87-92%). Subsequent labelling of the whole dataset by one of
these experts achieved similar levels of agreement with this group of experts,
which we defined as performing at most 5% worse (p=0.009-0.054). Using these
expert labels, we were able to train a deep learning model that also agreed
similarly with the group of experts (p=0.001-0.014), demonstrating that
automated analysis of biofouling in images is feasible and effective using this
method.
</p>
<a href="http://arxiv.org/abs/2008.09289" target="_blank">arXiv:2008.09289</a> [<a href="http://arxiv.org/pdf/2008.09289" target="_blank">pdf</a>]

<h2>Probabilistic Deep Learning for Instance Segmentation. (arXiv:2008.10678v2 [cs.CV] UPDATED)</h2>
<h3>Josef Lorenz Rumberger, Lisa Mais, Dagmar Kainmueller</h3>
<p>Probabilistic convolutional neural networks, which predict distributions of
predictions instead of point estimates, led to recent advances in many areas of
computer vision, from image reconstruction to semantic segmentation. Besides
state of the art benchmark results, these networks made it possible to quantify
local uncertainties in the predictions. These were used in active learning
frameworks to target the labeling efforts of specialist annotators or to assess
the quality of a prediction in a safety-critical environment. However, for
instance segmentation problems these methods are not frequently used so far. We
seek to close this gap by proposing a generic method to obtain model-inherent
uncertainty estimates within proposal-free instance segmentation models.
Furthermore, we analyze the quality of the uncertainty estimates with a metric
adapted from semantic segmentation. We evaluate our method on the BBBC010 C.\
elegans dataset, where it yields competitive performance while also predicting
uncertainty estimates that carry information about object-level inaccuracies
like false splits and false merges. We perform a simulation to show the
potential use of such uncertainty estimates in guided proofreading.
</p>
<a href="http://arxiv.org/abs/2008.10678" target="_blank">arXiv:2008.10678</a> [<a href="http://arxiv.org/pdf/2008.10678" target="_blank">pdf</a>]

<h2>Revisiting Co-Occurring Directions: Sharper Analysis and Efficient Algorithm for Sparse Matrices. (arXiv:2009.02553v2 [cs.LG] UPDATED)</h2>
<h3>Luo Luo, Cheng Chen, Guangzeng Xie, Haishan Ye</h3>
<p>We study the streaming model for approximate matrix multiplication (AMM). We
are interested in the scenario that the algorithm can only take one pass over
the data with limited memory. The state-of-the-art deterministic sketching
algorithm for streaming AMM is the co-occurring directions (COD), which has
much smaller approximation errors than randomized algorithms and outperforms
other deterministic sketching methods empirically. In this paper, we provide a
tighter error bound for COD whose leading term considers the potential
approximate low-rank structure and the correlation of input matrices. We prove
COD is space optimal with respect to our improved error bound. We also propose
a variant of COD for sparse matrices with theoretical guarantees. The
experiments on real-world sparse datasets show that the proposed algorithm is
more efficient than baseline methods.
</p>
<a href="http://arxiv.org/abs/2009.02553" target="_blank">arXiv:2009.02553</a> [<a href="http://arxiv.org/pdf/2009.02553" target="_blank">pdf</a>]

<h2>Pay Attention when Required. (arXiv:2009.04534v2 [cs.LG] UPDATED)</h2>
<h3>Swetha Mandava, Szymon Migacz, Alex Fit Florea</h3>
<p>Transformer-based models consist of interleaved feed-forward blocks - that
capture content meaning, and relatively more expensive self-attention blocks -
that capture context meaning. In this paper, we explored trade-offs and
ordering of the blocks to improve upon the current Transformer architecture and
proposed PAR Transformer. It needs 35% lower compute time than Transformer-XL
achieved by replacing ~63% of the self-attention blocks with feed-forward
blocks, and retains the perplexity on WikiText-103 language modelling
benchmark. We further validated our results on text8 and enwiki8 datasets, as
well as on the BERT model.
</p>
<a href="http://arxiv.org/abs/2009.04534" target="_blank">arXiv:2009.04534</a> [<a href="http://arxiv.org/pdf/2009.04534" target="_blank">pdf</a>]

<h2>Large Norms of CNN Layers Do Not Hurt Adversarial Robustness. (arXiv:2009.08435v3 [cs.LG] UPDATED)</h2>
<h3>Youwei Liang, Dong Huang</h3>
<p>Since the Lipschitz properties of convolutional neural networks (CNNs) are
widely considered to be related to adversarial robustness, we theoretically
characterize the $\ell_1$ norm and $\ell_\infty$ norm of 2D multi-channel
convolutional layers and provide efficient methods to compute the exact
$\ell_1$ norm and $\ell_\infty$ norm. Based on our theorem, we propose a novel
regularization method termed norm decay, which can effectively reduce the norms
of convolutional layers and fully-connected layers. Experiments show that
norm-regularization methods, including norm decay, weight decay, and singular
value clipping, can improve generalization of CNNs. However, they can slightly
hurt adversarial robustness. Observing this unexpected phenomenon, we compute
the norms of layers in the CNNs trained with three different adversarial
training frameworks and suprisingly find that adversarially robust CNNs have
comparable or even larger layer norms than their non-adversarially robust
counterparts. Furthermore, we prove that under a mild assumption, adversarially
robust classifiers can be achieved using neural networks, and an adversarially
robust neural network can have an arbitrarily large Lipschitz constant. For
this reason, enforcing small norms on CNN layers may be neither effective nor
necessary in achieving adversarial robustness. The code is available at
https://github.com/youweiliang/norm_robustness.
</p>
<a href="http://arxiv.org/abs/2009.08435" target="_blank">arXiv:2009.08435</a> [<a href="http://arxiv.org/pdf/2009.08435" target="_blank">pdf</a>]

<h2>Variational Disentanglement for Rare Event Modeling. (arXiv:2009.08541v4 [stat.ML] UPDATED)</h2>
<h3>Zidi Xiu, Chenyang Tao, Michael Gao, Connor Davis, Benjamin A. Goldstein, Ricardo Henao</h3>
<p>Combining the increasing availability and abundance of healthcare data and
the current advances in machine learning methods have created renewed
opportunities to improve clinical decision support systems. However, in
healthcare risk prediction applications, the proportion of cases with the
condition (label) of interest is often very low relative to the available
sample size. Though very prevalent in healthcare, such imbalanced
classification settings are also common and challenging in many other
scenarios. So motivated, we propose a variational disentanglement approach to
semi-parametrically learn from rare events in heavily imbalanced classification
problems. Specifically, we leverage the imposed extreme-distribution behavior
on a latent space to extract information from low-prevalence events, and
develop a robust prediction arm that joins the merits of the generalized
additive model and isotonic neural nets. Results on synthetic studies and
diverse real-world datasets, including mortality prediction on a COVID-19
cohort, demonstrate that the proposed approach outperforms existing
alternatives.
</p>
<a href="http://arxiv.org/abs/2009.08541" target="_blank">arXiv:2009.08541</a> [<a href="http://arxiv.org/pdf/2009.08541" target="_blank">pdf</a>]

<h2>Group Fairness by Probabilistic Modeling with Latent Fair Decisions. (arXiv:2009.09031v2 [cs.LG] UPDATED)</h2>
<h3>YooJung Choi, Meihua Dang, Guy Van den Broeck</h3>
<p>Machine learning systems are increasingly being used to make impactful
decisions such as loan applications and criminal justice risk assessments, and
as such, ensuring fairness of these systems is critical. This is often
challenging as the labels in the data are biased. This paper studies learning
fair probability distributions from biased data by explicitly modeling a latent
variable that represents a hidden, unbiased label. In particular, we aim to
achieve demographic parity by enforcing certain independencies in the learned
model. We also show that group fairness guarantees are meaningful only if the
distribution used to provide those guarantees indeed captures the real-world
data. In order to closely model the data distribution, we employ probabilistic
circuits, an expressive and tractable probabilistic model, and propose an
algorithm to learn them from incomplete data. We evaluate our approach on a
synthetic dataset in which observed labels indeed come from fair labels but
with added bias, and demonstrate that the fair labels are successfully
retrieved. Moreover, we show on real-world datasets that our approach not only
is a better model than existing methods of how the data was generated but also
achieves competitive accuracy.
</p>
<a href="http://arxiv.org/abs/2009.09031" target="_blank">arXiv:2009.09031</a> [<a href="http://arxiv.org/pdf/2009.09031" target="_blank">pdf</a>]

<h2>Solution Concepts in Hierarchical Games with Applications to Autonomous Driving. (arXiv:2009.10033v2 [cs.AI] UPDATED)</h2>
<h3>Atrisha Sarkar, Krzysztof Czarnecki</h3>
<p>With autonomous vehicles (AV) set to integrate further into regular human
traffic, there is an increasing consensus of treating AV motion planning as a
multi-agent problem. However, the traditional game theoretic assumption of
complete rationality is too strong for the purpose of human driving, and there
is a need for understanding human driving as a bounded rational activity
through a behavioral game theoretic lens. To that end, we adapt three
metamodels of bounded rational behavior; two based on Quantal level-k and one
based on Nash equilibrium with quantal errors. We formalize the different
solution concepts that can be applied in the context of hierarchical games, a
framework used in multi-agent motion planning, for the purpose of creating game
theoretic models of driving behavior. Furthermore, based on a contributed
dataset of human driving at a busy urban intersection with a total of ~4k
agents and ~44k decision points, we evaluate the behavior models on the basis
of model fit to naturalistic data, as well as their predictive capacity. Our
results suggest that among the behavior models evaluated, modeling driving
behavior as pure strategy NE with quantal errors at the level of maneuvers with
bounds sampling of actions at the level of trajectories provides the best fit
to naturalistic driving behavior.
</p>
<a href="http://arxiv.org/abs/2009.10033" target="_blank">arXiv:2009.10033</a> [<a href="http://arxiv.org/pdf/2009.10033" target="_blank">pdf</a>]

<h2>Cloud Cover Nowcasting with Deep Learning. (arXiv:2009.11577v3 [cs.CV] UPDATED)</h2>
<h3>L&#xe9;a Berthomier, Bruno Pradel, Lior Perez</h3>
<p>Nowcasting is a field of meteorology which aims at forecasting weather on a
short term of up to a few hours. In the meteorology landscape, this field is
rather specific as it requires particular techniques, such as data
extrapolation, where conventional meteorology is generally based on physical
modeling. In this paper, we focus on cloud cover nowcasting, which has various
application areas such as satellite shots optimisation and photovoltaic energy
production forecast.

Following recent deep learning successes on multiple imagery tasks, we
applied deep convolutionnal neural networks on Meteosat satellite images for
cloud cover nowcasting. We present the results of several architectures
specialized in image segmentation and time series prediction. We selected the
best models according to machine learning metrics as well as meteorological
metrics. All selected architectures showed significant improvements over
persistence and the well-known U-Net surpasses AROME physical model.
</p>
<a href="http://arxiv.org/abs/2009.11577" target="_blank">arXiv:2009.11577</a> [<a href="http://arxiv.org/pdf/2009.11577" target="_blank">pdf</a>]

<h2>Learning Set Functions that are Sparse in Non-Orthogonal Fourier Bases. (arXiv:2010.00439v2 [cs.LG] UPDATED)</h2>
<h3>Chris Wendler, Andisheh Amrollahi, Bastian Seifert, Andreas Krause, Markus P&#xfc;schel</h3>
<p>Many applications of machine learning on discrete domains, such as learning
preference functions in recommender systems or auctions, can be reduced to
estimating a set function that is sparse in the Fourier domain. In this work,
we present a new family of algorithms for learning Fourier-sparse set
functions. They require at most $nk - k \log_2 k + k$ queries (set function
evaluations), under mild conditions on the Fourier coefficients, where $n$ is
the size of the ground set and $k$ the number of non-zero Fourier coefficients.
In contrast to other work that focused on the orthogonal Walsh-Hadamard
transform, our novel algorithms operate with recently introduced non-orthogonal
Fourier transforms that offer different notions of Fourier-sparsity. These
naturally arise when modeling, e.g., sets of items forming substitutes and
complements. We demonstrate effectiveness on several real-world applications.
</p>
<a href="http://arxiv.org/abs/2010.00439" target="_blank">arXiv:2010.00439</a> [<a href="http://arxiv.org/pdf/2010.00439" target="_blank">pdf</a>]

<h2>On the intrinsic robustness to noise of some leading classifiers and symmetric loss function -- an empirical evaluation. (arXiv:2010.13570v4 [cs.LG] UPDATED)</h2>
<h3>Hugo Le Baher (1), Vincent Lemaire (2), Romain Trinquart (2) ((1) Polytech Nantes (France), (2) Orange Labs (France))</h3>
<p>In some industrial applications such as fraud detection, the performance of
common supervision techniques may be affected by the poor quality of the
available labels : in actual operational use-cases, these labels may be weak in
quantity, quality or trustworthiness. We propose a benchmark to evaluate the
natural robustness of different algorithms taken from various paradigms on
artificially corrupted datasets, with a focus on noisy labels. This paper
studies the intrinsic robustness of some leading classifiers. The algorithms
under scrutiny include SVM, logistic regression, random forests, XGBoost,
Khiops. Furthermore, building on results from recent literature, the study is
supplemented with an investigation into the opportunity to enhance some
algorithms with symmetric loss functions.
</p>
<a href="http://arxiv.org/abs/2010.13570" target="_blank">arXiv:2010.13570</a> [<a href="http://arxiv.org/pdf/2010.13570" target="_blank">pdf</a>]

<h2>Deep Probabilistic Imaging: Uncertainty Quantification and Multi-modal Solution Characterization for Computational Imaging. (arXiv:2010.14462v2 [cs.LG] UPDATED)</h2>
<h3>He Sun, Katherine L. Bouman</h3>
<p>Computational image reconstruction algorithms generally produce a single
image without any measure of uncertainty or confidence. Regularized Maximum
Likelihood (RML) and feed-forward deep learning approaches for inverse problems
typically focus on recovering a point estimate. This is a serious limitation
when working with underdetermined imaging systems, where it is conceivable that
multiple image modes would be consistent with the measured data. Characterizing
the space of probable images that explain the observational data is therefore
crucial. In this paper, we propose a variational deep probabilistic imaging
approach to quantify reconstruction uncertainty. Deep Probabilistic Imaging
(DPI) employs an untrained deep generative model to estimate a posterior
distribution of an unobserved image. This approach does not require any
training data; instead, it optimizes the weights of a neural network to
generate image samples that fit a particular measurement dataset. Once the
network weights have been learned, the posterior distribution can be
efficiently sampled. We demonstrate this approach in the context of
interferometric radio imaging, which is used for black hole imaging with the
Event Horizon Telescope, and compressed sensing Magnetic Resonance Imaging
(MRI).
</p>
<a href="http://arxiv.org/abs/2010.14462" target="_blank">arXiv:2010.14462</a> [<a href="http://arxiv.org/pdf/2010.14462" target="_blank">pdf</a>]

<h2>Formally Verified SAT-Based AI Planning. (arXiv:2010.14648v4 [cs.AI] UPDATED)</h2>
<h3>Mohammad Abdulaziz, Friedrich Kurz</h3>
<p>We present an executable formally verified SAT encoding of classical AI
planning. We use the theorem prover Isabelle/HOL to perform the verification.
We experimentally test the verified encoding and show that it can be used for
reasonably sized standard planning benchmarks. We also use it as a reference to
test a state-of-the-art SAT-based planner, showing that it sometimes falsely
claims that problems have no solutions of certain lengths.
</p>
<a href="http://arxiv.org/abs/2010.14648" target="_blank">arXiv:2010.14648</a> [<a href="http://arxiv.org/pdf/2010.14648" target="_blank">pdf</a>]

<h2>Faster Differentially Private Samplers via R\'enyi Divergence Analysis of Discretized Langevin MCMC. (arXiv:2010.14658v2 [cs.LG] UPDATED)</h2>
<h3>Arun Ganesh, Kunal Talwar</h3>
<p>Various differentially private algorithms instantiate the exponential
mechanism, and require sampling from the distribution $\exp(-f)$ for a suitable
function $f$. When the domain of the distribution is high-dimensional, this
sampling can be computationally challenging. Using heuristic sampling schemes
such as Gibbs sampling does not necessarily lead to provable privacy. When $f$
is convex, techniques from log-concave sampling lead to polynomial-time
algorithms, albeit with large polynomials. Langevin dynamics-based algorithms
offer much faster alternatives under some distance measures such as statistical
distance. In this work, we establish rapid convergence for these algorithms
under distance measures more suitable for differential privacy. For smooth,
strongly-convex $f$, we give the first results proving convergence in R\'enyi
divergence. This gives us fast differentially private algorithms for such $f$.
Our techniques and simple and generic and apply also to underdamped Langevin
dynamics.
</p>
<a href="http://arxiv.org/abs/2010.14658" target="_blank">arXiv:2010.14658</a> [<a href="http://arxiv.org/pdf/2010.14658" target="_blank">pdf</a>]

<h2>Do We Need to Compensate for Motion Distortion and Doppler Effects in Spinning Radar Navigation?. (arXiv:2011.03512v3 [cs.RO] UPDATED)</h2>
<h3>Keenan Burnett, Angela P. Schoellig, Timothy D. Barfoot</h3>
<p>In order to tackle the challenge of unfavorable weather conditions such as
rain and snow, radar is being revisited as a parallel sensing modality to
vision and lidar. Recent works have made tremendous progress in applying
spinning radar to odometry and place recognition. However, these works have so
far ignored the impact of motion distortion and Doppler effects on
spinning-radar-based navigation, which may be significant in the self-driving
car domain where speeds can be high. In this work, we demonstrate the effect of
these distortions on radar odometry using the Oxford Radar RobotCar Dataset and
metric localization using our own data-taking platform. We revisit a
lightweight estimator that can recover the motion between a pair of radar scans
while accounting for both effects. Our conclusion is that both motion
distortion and the Doppler effect are significant in different aspects of
spinning radar navigation, with the former more prominent than the latter. Code
for this project can be found at:
https://github.com/keenan-burnett/yeti_radar_odometry
</p>
<a href="http://arxiv.org/abs/2011.03512" target="_blank">arXiv:2011.03512</a> [<a href="http://arxiv.org/pdf/2011.03512" target="_blank">pdf</a>]

<h2>An Efficient and Scalable Deep Learning Approach for Road Damage Detection. (arXiv:2011.09577v3 [cs.CV] UPDATED)</h2>
<h3>Sadra Naddaf-Sh, M-Mahdi Naddaf-Sh, Amir R. Kashani, Hassan Zargarzadeh</h3>
<p>Pavement condition evaluation is essential to time the preventative or
rehabilitative actions and control distress propagation. Failing to conduct
timely evaluations can lead to severe structural and financial loss of the
infrastructure and complete reconstructions. Automated computer-aided surveying
measures can provide a database of road damage patterns and their locations.
This database can be utilized for timely road repairs to gain the minimum cost
of maintenance and the asphalt's maximum durability. This paper introduces a
deep learning-based surveying scheme to analyze the image-based distress data
in real-time. A database consisting of a diverse population of crack distress
types such as longitudinal, transverse, and alligator cracks, photographed
using mobile-device is used. Then, a family of efficient and scalable models
that are tuned for pavement crack detection is trained, and various
augmentation policies are explored. Proposed models, resulted in F1-scores,
ranging from 52% to 56%, and average inference time from 178-10 images per
second. Finally, the performance of the object detectors are examined, and
error analysis is reported against various images. The source code is available
at https://github.com/mahdi65/roadDamageDetection2020.
</p>
<a href="http://arxiv.org/abs/2011.09577" target="_blank">arXiv:2011.09577</a> [<a href="http://arxiv.org/pdf/2011.09577" target="_blank">pdf</a>]

<h2>Classification by Attention: Scene Graph Classification with Prior Knowledge. (arXiv:2011.10084v2 [cs.CV] UPDATED)</h2>
<h3>Sahand Sharifzadeh, Sina Moayed Baharlou, Volker Tresp</h3>
<p>A major challenge in scene graph classification is that the appearance of
objects and relations can be significantly different from one image to another.
Previous works have addressed this by relational reasoning over all objects in
an image or incorporating prior knowledge into classification. Unlike previous
works, we do not consider separate models for perception and prior knowledge.
Instead, we take a multi-task learning approach, where we implement the
classification as an attention layer. This allows for the prior knowledge to
emerge and propagate within the perception model. By enforcing the model also
to represent the prior, we achieve a strong inductive bias. We show that our
model can accurately generate commonsense knowledge and that the iterative
injection of this knowledge to scene representations leads to significantly
higher classification performance. Additionally, our model can be fine-tuned on
external knowledge given as triples. When combined with self-supervised
learning and with 1% of annotated images only, this gives more than 3%
improvement in object classification, 26% in scene graph classification, and
36% in predicate prediction accuracy.
</p>
<a href="http://arxiv.org/abs/2011.10084" target="_blank">arXiv:2011.10084</a> [<a href="http://arxiv.org/pdf/2011.10084" target="_blank">pdf</a>]

<h2>On the Convergence of Continuous Constrained Optimization for Structure Learning. (arXiv:2011.11150v2 [cs.LG] UPDATED)</h2>
<h3>Ignavier Ng, S&#xe9;bastien Lachapelle, Nan Rosemary Ke, Simon Lacoste-Julien</h3>
<p>Structure learning of directed acyclic graphs (DAGs) is a fundamental problem
in many scientific endeavors. A new line of work, based on NOTEARS (Zheng et
al., 2018), reformulates the structure learning problem as a continuous
optimization one by leveraging an algebraic characterization of DAG constraint.
The constrained problem is typically solved using the augmented Lagrangian
method (ALM) which is often preferred to the quadratic penalty method (QPM) by
virtue of its convergence result that does not require the penalty coefficient
to go to infinity, hence avoiding ill-conditioning. In this work, we review the
standard convergence result of the ALM and show that the required conditions
are not satisfied in the recent continuous constrained formulation for learning
DAGs. We demonstrate empirically that its behavior is akin to that of the QPM
which is prone to ill-conditioning, thus motivating the use of second-order
method in this setting. We also establish the convergence guarantee of QPM to a
DAG solution, under mild conditions, based on a property of the DAG constraint
term.
</p>
<a href="http://arxiv.org/abs/2011.11150" target="_blank">arXiv:2011.11150</a> [<a href="http://arxiv.org/pdf/2011.11150" target="_blank">pdf</a>]

<h2>Benchmarking Inference Performance of Deep Learning Models on Analog Devices. (arXiv:2011.11840v2 [cs.LG] UPDATED)</h2>
<h3>Omobayode Fagbohungbe, Lijun Qian</h3>
<p>Analog hardware implemented deep learning models are promising for
computation and energy constrained systems such as edge computing devices.
However, the analog nature of the device and the associated many noise sources
will cause changes to the value of the weights in the trained deep learning
models deployed on such devices. In this study, systematic evaluation of the
inference performance of trained popular deep learning models for image
classification deployed on analog devices has been carried out, where additive
white Gaussian noise has been added to the weights of the trained models during
inference. It is observed that deeper models and models with more redundancy in
design such as VGG are more robust to the noise in general. However, the
performance is also affected by the design philosophy of the model, the
detailed structure of the model, the exact machine learning task, as well as
the datasets.
</p>
<a href="http://arxiv.org/abs/2011.11840" target="_blank">arXiv:2011.11840</a> [<a href="http://arxiv.org/pdf/2011.11840" target="_blank">pdf</a>]

<h2>Feature Space Singularity for Out-of-Distribution Detection. (arXiv:2011.14654v2 [stat.ML] UPDATED)</h2>
<h3>Haiwen Huang, Zhihan Li, Lulu Wang, Sishuo Chen, Bin Dong, Xinyu Zhou</h3>
<p>Out-of-Distribution (OoD) detection is important for building safe artificial
intelligence systems. However, current OoD detection methods still cannot meet
the performance requirements for practical deployment. In this paper, we
propose a simple yet effective algorithm based on a novel observation: in a
trained neural network, OoD samples with bounded norms well concentrate in the
feature space. We call the center of OoD features the Feature Space Singularity
(FSS), and denote the distance of a sample feature to FSS as FSSD. Then, OoD
samples can be identified by taking a threshold on the FSSD. Our analysis of
the phenomenon reveals why our algorithm works. We demonstrate that our
algorithm achieves state-of-the-art performance on various OoD detection
benchmarks. Besides, FSSD also enjoys robustness to slight corruption in test
data and can be further enhanced by ensembling. These make FSSD a promising
algorithm to be employed in real world. We release our code at
\url{https://github.com/megvii-research/FSSD_OoD_Detection}.
</p>
<a href="http://arxiv.org/abs/2011.14654" target="_blank">arXiv:2011.14654</a> [<a href="http://arxiv.org/pdf/2011.14654" target="_blank">pdf</a>]

<h2>Scale-covariant and scale-invariant Gaussian derivative networks. (arXiv:2011.14759v2 [cs.CV] UPDATED)</h2>
<h3>Tony Lindeberg</h3>
<p>This paper presents a hybrid approach between scale-space theory and deep
learning, where a deep learning architecture is constructed by coupling
parameterized scale-space operations in cascade. By sharing the learnt
parameters between multiple scale channels, and by using the transformation
properties of the scale-space primitives under scaling transformations, the
resulting network becomes provably scale covariant. By in addition performing
max pooling over the multiple scale channels, a resulting network architecture
for image classification also becomes provably scale invariant. We investigate
the performance of such networks on the MNISTLargeScale dataset, which contains
rescaled images from original MNIST over a factor 4 concerning training data
and over a factor of 16 concerning testing data. It is demonstrated that the
resulting approach allows for scale generalization, enabling good performance
for classifying patterns at scales not present in the training data.
</p>
<a href="http://arxiv.org/abs/2011.14759" target="_blank">arXiv:2011.14759</a> [<a href="http://arxiv.org/pdf/2011.14759" target="_blank">pdf</a>]

<h2>Improved Contrastive Divergence Training of Energy Based Models. (arXiv:2012.01316v2 [cs.LG] UPDATED)</h2>
<h3>Yilun Du, Shuang Li, Joshua Tenenbaum, Igor Mordatch</h3>
<p>We propose several different techniques to improve contrastive divergence
training of energy-based models (EBMs). We first show that a gradient term
neglected in the popular contrastive divergence formulation is both tractable
to estimate and is important to avoid training instabilities in previous
models. We further highlight how data augmentation, multi-scale processing, and
reservoir sampling can be used to improve model robustness and generation
quality. Thirdly, we empirically evaluate stability of model architectures and
show improved performance on a host of benchmarks and use cases, such as image
generation, OOD detection, and compositional generation.
</p>
<a href="http://arxiv.org/abs/2012.01316" target="_blank">arXiv:2012.01316</a> [<a href="http://arxiv.org/pdf/2012.01316" target="_blank">pdf</a>]

<h2>Learning to Transfer Visual Effects from Videos to Images. (arXiv:2012.01642v2 [cs.CV] UPDATED)</h2>
<h3>Christopher Thomas, Yale Song, Adriana Kovashka</h3>
<p>We study the problem of animating images by transferring spatio-temporal
visual effects (such as melting) from a collection of videos. We tackle two
primary challenges in visual effect transfer: 1) how to capture the effect we
wish to distill; and 2) how to ensure that only the effect, rather than content
or artistic style, is transferred from the source videos to the input image. To
address the first challenge, we evaluate five loss functions; the most
promising one encourages the generated animations to have similar optical flow
and texture motions as the source videos. To address the second challenge, we
only allow our model to move existing image pixels from the previous frame,
rather than predicting unconstrained pixel values. This forces any visual
effects to occur using the input image's pixels, preventing unwanted artistic
style or content from the source video from appearing in the output. We
evaluate our method in objective and subjective settings, and show interesting
qualitative results which demonstrate objects undergoing atypical
transformations, such as making a face melt or a deer bloom.
</p>
<a href="http://arxiv.org/abs/2012.01642" target="_blank">arXiv:2012.01642</a> [<a href="http://arxiv.org/pdf/2012.01642" target="_blank">pdf</a>]

<h2>Computer Stereo Vision for Autonomous Driving. (arXiv:2012.03194v2 [cs.CV] UPDATED)</h2>
<h3>Rui Fan, Li Wang, Mohammud Junaid Bocus, Ioannis Pitas</h3>
<p>As an important component of autonomous systems, autonomous car perception
has had a big leap with recent advances in parallel computing architectures.
With the use of tiny but full-feature embedded supercomputers, computer stereo
vision has been prevalently applied in autonomous cars for depth perception.
The two key aspects of computer stereo vision are speed and accuracy. They are
both desirable but conflicting properties, as the algorithms with better
disparity accuracy usually have higher computational complexity. Therefore, the
main aim of developing a computer stereo vision algorithm for resource-limited
hardware is to improve the trade-off between speed and accuracy. In this
chapter, we introduce both the hardware and software aspects of computer stereo
vision for autonomous car systems. Then, we discuss four autonomous car
perception tasks, including 1) visual feature detection, description and
matching, 2) 3D information acquisition, 3) object detection/recognition and 4)
semantic image segmentation. The principles of computer stereo vision and
parallel computing on multi-threading CPU and GPU architectures are then
detailed.
</p>
<a href="http://arxiv.org/abs/2012.03194" target="_blank">arXiv:2012.03194</a> [<a href="http://arxiv.org/pdf/2012.03194" target="_blank">pdf</a>]

<h2>Stochastic Gradient Descent with Large Learning Rate. (arXiv:2012.03636v2 [stat.ML] UPDATED)</h2>
<h3>Kangqiao Liu, Liu Ziyin, Masahito Ueda</h3>
<p>As a simple and efficient optimization method in deep learning, stochastic
gradient descent (SGD) has attracted tremendous attention. In the vanishing
learning rate regime, SGD is now relatively well understood, and the majority
of theoretical approaches to SGD set their assumptions in the continuous-time
limit. However, the continuous-time predictions are unlikely to reflect the
experimental observations well because the practice often runs in the large
learning rate regime, where the training is faster and the generalization of
models are often better. In this paper, we propose to study the basic
properties of SGD and its variants in the non-vanishing learning rate regime.
The focus is on deriving exactly solvable results and relating them to
experimental observations. The main contributions of this work are to derive
the stable distribution for discrete-time SGD in a quadratic loss function with
and without momentum. Examples of applications of the proposed theory
considered in this work include the approximation error of variants of SGD, the
effect of mini-batch noise, the escape rate from a sharp minimum, and and the
stationary distribution of a few second order methods.
</p>
<a href="http://arxiv.org/abs/2012.03636" target="_blank">arXiv:2012.03636</a> [<a href="http://arxiv.org/pdf/2012.03636" target="_blank">pdf</a>]

<h2>Sparse encoding for more-interpretable feature-selecting representations in probabilistic matrix factorization. (arXiv:2012.04171v2 [cs.LG] UPDATED)</h2>
<h3>Joshua C. Chang, Patrick Fletcher, Jungmin Han, Ted L. Chang, Shashaank Vattikuti, Bart Desmet, Ayah Zirikly, Carson C. Chow</h3>
<p>Dimensionality reduction methods for count data are critical to a wide range
of applications in medical informatics and other fields where model
interpretability is paramount. For such data, hierarchical Poisson matrix
factorization (HPF) and other sparse probabilistic non-negative matrix
factorization (NMF) methods are considered to be interpretable generative
models. They consist of sparse transformations for decoding their learned
representations into predictions. However, sparsity in representation decoding
does not necessarily imply sparsity in the encoding of representations from the
original data features. HPF is often incorrectly interpreted in the literature
as if it possesses encoder sparsity. The distinction between decoder sparsity
and encoder sparsity is subtle but important. Due to the lack of encoder
sparsity, HPF does not possess the column-clustering property of classical NMF
-- the factor loading matrix does not sufficiently define how each factor is
formed from the original features. We address this deficiency by
self-consistently enforcing encoder sparsity, using a generalized additive
model (GAM), thereby allowing one to relate each representation coordinate to a
subset of the original data features. In doing so, the method also gains the
ability to perform feature selection. We demonstrate our method on simulated
data and give an example of how encoder sparsity is of practical use in a
concrete application of representing inpatient comorbidities in Medicare
patients.
</p>
<a href="http://arxiv.org/abs/2012.04171" target="_blank">arXiv:2012.04171</a> [<a href="http://arxiv.org/pdf/2012.04171" target="_blank">pdf</a>]

<h2>In-N-Out: Pre-Training and Self-Training using Auxiliary Information for Out-of-Distribution Robustness. (arXiv:2012.04550v2 [cs.LG] UPDATED)</h2>
<h3>Sang Michael Xie, Ananya Kumar, Robbie Jones, Fereshte Khani, Tengyu Ma, Percy Liang</h3>
<p>Consider a prediction setting where a few inputs (e.g., satellite images) are
expensively annotated with the prediction targets (e.g., crop types), and many
inputs are cheaply annotated with auxiliary information (e.g., climate
information). How should we best leverage this auxiliary information for the
prediction task? Empirically across three image and time-series datasets, and
theoretically in a multi-task linear regression setting, we show that (i) using
auxiliary information as input features improves in-distribution error but can
hurt out-of-distribution (OOD) error; while (ii) using auxiliary information as
outputs of auxiliary tasks to pre-train a model improves OOD error. To get the
best of both worlds, we introduce In-N-Out, which first trains a model with
auxiliary inputs and uses it to pseudolabel all the in-distribution inputs,
then pre-trains a model on OOD auxiliary outputs and fine-tunes this model with
the pseudolabels (self-training). We show both theoretically and empirically
that In-N-Out outperforms auxiliary inputs or outputs alone on both
in-distribution and OOD error.
</p>
<a href="http://arxiv.org/abs/2012.04550" target="_blank">arXiv:2012.04550</a> [<a href="http://arxiv.org/pdf/2012.04550" target="_blank">pdf</a>]

<h2>Learning Graphons via Structured Gromov-Wasserstein Barycenters. (arXiv:2012.05644v2 [cs.LG] UPDATED)</h2>
<h3>Hongteng Xu, Dixin Luo, Lawrence Carin, Hongyuan Zha</h3>
<p>We propose a novel and principled method to learn a nonparametric graph model
called graphon, which is defined in an infinite-dimensional space and
represents arbitrary-size graphs. Based on the weak regularity lemma from the
theory of graphons, we leverage a step function to approximate a graphon. We
show that the cut distance of graphons can be relaxed to the Gromov-Wasserstein
distance of their step functions. Accordingly, given a set of graphs generated
by an underlying graphon, we learn the corresponding step function as the
Gromov-Wasserstein barycenter of the given graphs. Furthermore, we develop
several enhancements and extensions of the basic algorithm, $e.g.$, the
smoothed Gromov-Wasserstein barycenter for guaranteeing the continuity of the
learned graphons and the mixed Gromov-Wasserstein barycenters for learning
multiple structured graphons. The proposed approach overcomes drawbacks of
prior state-of-the-art methods, and outperforms them on both synthetic and
real-world data. The code is available at
https://github.com/HongtengXu/SGWB-Graphon.
</p>
<a href="http://arxiv.org/abs/2012.05644" target="_blank">arXiv:2012.05644</a> [<a href="http://arxiv.org/pdf/2012.05644" target="_blank">pdf</a>]

<h2>Generative Deep Learning Techniques for Password Generation. (arXiv:2012.05685v2 [cs.LG] UPDATED)</h2>
<h3>David Biesner, Kostadin Cvejoski, Bogdan Georgiev, Rafet Sifa, Erik Krupicka</h3>
<p>Password guessing approaches via deep learning have recently been
investigated with significant breakthroughs in their ability to generate novel,
realistic password candidates. In the present work we study a broad collection
of deep learning and probabilistic based models in the light of password
guessing: attention-based deep neural networks, autoencoding mechanisms and
generative adversarial networks. We provide novel generative deep-learning
models in terms of variational autoencoders exhibiting state-of-art sampling
performance, yielding additional latent-space features such as interpolations
and targeted sampling. Lastly, we perform a thorough empirical analysis in a
unified controlled framework over well-known datasets (RockYou, LinkedIn,
Youku, Zomato, Pwnd). Our results not only identify the most promising schemes
driven by deep neural networks, but also illustrate the strengths of each
approach in terms of generation variability and sample uniqueness.
</p>
<a href="http://arxiv.org/abs/2012.05685" target="_blank">arXiv:2012.05685</a> [<a href="http://arxiv.org/pdf/2012.05685" target="_blank">pdf</a>]

<h2>Neurosymbolic AI: The 3rd Wave. (arXiv:2012.05876v2 [cs.AI] UPDATED)</h2>
<h3>Artur d&#x27;Avila Garcez, Luis C. Lamb</h3>
<p>Current advances in Artificial Intelligence (AI) and Machine Learning (ML)
have achieved unprecedented impact across research communities and industry.
Nevertheless, concerns about trust, safety, interpretability and accountability
of AI were raised by influential thinkers. Many have identified the need for
well-founded knowledge representation and reasoning to be integrated with deep
learning and for sound explainability. Neural-symbolic computing has been an
active area of research for many years seeking to bring together robust
learning in neural networks with reasoning and explainability via symbolic
representations for network models. In this paper, we relate recent and early
research results in neurosymbolic AI with the objective of identifying the key
ingredients of the next wave of AI systems. We focus on research that
integrates in a principled way neural network-based learning with symbolic
knowledge representation and logical reasoning. The insights provided by 20
years of neural-symbolic computing are shown to shed new light onto the
increasingly prominent role of trust, safety, interpretability and
accountability of AI. We also identify promising directions and challenges for
the next decade of AI research from the perspective of neural-symbolic systems.
</p>
<a href="http://arxiv.org/abs/2012.05876" target="_blank">arXiv:2012.05876</a> [<a href="http://arxiv.org/pdf/2012.05876" target="_blank">pdf</a>]

<h2>Fairness in Rating Prediction by Awareness of Verbal and Gesture Quality of Public Speeches. (arXiv:2012.06157v2 [cs.AI] UPDATED)</h2>
<h3>Rupam Acharyya, Ankani Chattoraj, Shouman Das, Md. Iftekhar Tanveer, Ehsan Hoque</h3>
<p>The role of verbal and non-verbal cues towards great public speaking has been
a topic of exploration for many decades. We identify a commonality across
present theories, the element of "variety or heterogeneity" in channels or
modes of communication (e.g. resorting to stories, scientific facts, emotional
connections, facial expressions etc.) which is essential for effectively
communicating information. We use this observation to formalize a novel
HEterogeneity Metric, HEM, that quantifies the quality of a talk both in the
verbal and non-verbal domain (transcript and facial gestures). We use TED talks
as an input repository of public speeches because it consists of speakers from
a diverse community besides having a wide outreach. We show that there is an
interesting relationship between HEM and the ratings of TED talks given to
speakers by viewers. It emphasizes that HEM inherently and successfully
represents the quality of a talk based on "variety or heterogeneity". Further,
we also discover that HEM successfully captures the prevalent bias in ratings
with respect to race and gender, that we call sensitive attributes (because
prediction based on these might result in unfair outcome). We incorporate the
HEM metric into the loss function of a neural network with the goal to reduce
unfairness in rating predictions with respect to race and gender. Our results
show that the modified loss function improves fairness in prediction without
considerably affecting prediction accuracy of the neural network. Our work ties
together a novel metric for public speeches in both verbal and non-verbal
domain with the computational power of a neural network to design a fair
prediction system for speakers.
</p>
<a href="http://arxiv.org/abs/2012.06157" target="_blank">arXiv:2012.06157</a> [<a href="http://arxiv.org/pdf/2012.06157" target="_blank">pdf</a>]

<h2>Street-view Panoramic Video Synthesis from a Single Satellite Image. (arXiv:2012.06628v2 [cs.CV] UPDATED)</h2>
<h3>Zuoyue Li, Zhaopeng Cui, Martin R. Oswald, Marc Pollefeys</h3>
<p>We present a novel method for synthesizing both temporally and geometrically
consistent street-view panoramic video from a given single satellite image and
camera trajectory. Existing cross-view synthesis approaches focus more on
images, while video synthesis in such a case has not yet received enough
attention. Single image synthesis approaches are not well suited for video
synthesis since they lack temporal consistency which is a crucial property of
videos. To this end, our approach explicitly creates a 3D point cloud
representation of the scene and maintains dense 3D-2D correspondences across
frames that reflect the geometric scene configuration inferred from the
satellite view. We implement a cascaded network architecture with two hourglass
modules for successive coarse and fine generation for colorizing the point
cloud from the semantics and per-class latent vectors. By leveraging computed
correspondences, the produced street-view video frames adhere to the 3D
geometric scene structure and maintain temporal consistency. Qualitative and
quantitative experiments demonstrate superior results compared to other
state-of-the-art cross-view synthesis approaches that either lack temporal or
geometric consistency. To the best of our knowledge, our work is the first work
to synthesize cross-view images to video.
</p>
<a href="http://arxiv.org/abs/2012.06628" target="_blank">arXiv:2012.06628</a> [<a href="http://arxiv.org/pdf/2012.06628" target="_blank">pdf</a>]

<h2>D$^2$IM-Net: Learning Detail Disentangled Implicit Fields from Single Images. (arXiv:2012.06650v2 [cs.CV] UPDATED)</h2>
<h3>Manyi Li, Hao Zhang</h3>
<p>We present the first single-view 3D reconstruction network aimed at
recovering geometric details from an input image which encompass both
topological shape structures and surface features. Our key idea is to train the
network to learn a detail disentangled reconstruction consisting of two
functions, one implicit field representing the coarse 3D shape and the other
capturing the details. Given an input image, our network, coined D$^2$IM-Net,
encodes it into global and local features which are respectively fed into two
decoders. The base decoder uses the global features to reconstruct a coarse
implicit field, while the detail decoder reconstructs, from the local features,
two displacement maps, defined over the front and back sides of the captured
object. The final 3D reconstruction is a fusion between the base shape and the
displacement maps, with three losses enforcing the recovery of coarse shape,
overall structure, and surface details via a novel Laplacian term.
</p>
<a href="http://arxiv.org/abs/2012.06650" target="_blank">arXiv:2012.06650</a> [<a href="http://arxiv.org/pdf/2012.06650" target="_blank">pdf</a>]

<h2>Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting. (arXiv:2012.07436v2 [cs.LG] UPDATED)</h2>
<h3>Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, Wancai Zhang</h3>
<p>Many real-world applications require the prediction of long sequence
time-series, such as electricity consumption planning. Long sequence
time-series forecasting (LSTF) demands a high prediction capacity of the model,
which is the ability to capture precise long-range dependency coupling between
output and input efficiently. Recent studies have shown the potential of
Transformer to increase the prediction capacity. However, there are several
severe issues with Transformer that prevent it from being directly applicable
to LSTF, such as quadratic time complexity, high memory usage, and inherent
limitation of the encoder-decoder architecture. To address these issues, we
design an efficient transformer-based model for LSTF, named Informer, with
three distinctive characteristics: (i) a $ProbSparse$ Self-attention mechanism,
which achieves $O(L \log L)$ in time complexity and memory usage, and has
comparable performance on sequences' dependency alignment. (ii) the
self-attention distilling highlights dominating attention by halving cascading
layer input, and efficiently handles extreme long input sequences. (iii) the
generative style decoder, while conceptually simple, predicts the long
time-series sequences at one forward operation rather than a step-by-step way,
which drastically improves the inference speed of long-sequence predictions.
Extensive experiments on four large-scale datasets demonstrate that Informer
significantly outperforms existing methods and provides a new solution to the
LSTF problem.
</p>
<a href="http://arxiv.org/abs/2012.07436" target="_blank">arXiv:2012.07436</a> [<a href="http://arxiv.org/pdf/2012.07436" target="_blank">pdf</a>]

<h2>Cross-Domain Grouping and Alignment for Domain Adaptive Semantic Segmentation. (arXiv:2012.08226v2 [cs.CV] UPDATED)</h2>
<h3>Minsu Kim, Sunghun Joung, Seungryong Kim, JungIn Park, Ig-Jae Kim, Kwanghoon Sohn</h3>
<p>Existing techniques to adapt semantic segmentation networks across the source
and target domains within deep convolutional neural networks (CNNs) deal with
all the samples from the two domains in a global or category-aware manner. They
do not consider an inter-class variation within the target domain itself or
estimated category, providing the limitation to encode the domains having a
multi-modal data distribution. To overcome this limitation, we introduce a
learnable clustering module, and a novel domain adaptation framework called
cross-domain grouping and alignment. To cluster the samples across domains with
an aim to maximize the domain alignment without forgetting precise segmentation
ability on the source domain, we present two loss functions, in particular, for
encouraging semantic consistency and orthogonality among the clusters. We also
present a loss so as to solve a class imbalance problem, which is the other
limitation of the previous methods. Our experiments show that our method
consistently boosts the adaptation performance in semantic segmentation,
outperforming the state-of-the-arts on various domain adaptation settings.
</p>
<a href="http://arxiv.org/abs/2012.08226" target="_blank">arXiv:2012.08226</a> [<a href="http://arxiv.org/pdf/2012.08226" target="_blank">pdf</a>]

<h2>Are we Forgetting about Compositional Optimisers in Bayesian Optimisation?. (arXiv:2012.08240v2 [cs.LG] UPDATED)</h2>
<h3>Antoine Grosnit, Alexander I. Cowen-Rivers, Rasul Tutunov, Ryan-Rhys Griffiths, Jun Wang, Haitham Bou-Ammar</h3>
<p>Bayesian optimisation presents a sample-efficient methodology for global
optimisation. Within this framework, a crucial performance-determining
subroutine is the maximisation of the acquisition function, a task complicated
by the fact that acquisition functions tend to be non-convex and thus
nontrivial to optimise. In this paper, we undertake a comprehensive empirical
study of approaches to maximise the acquisition function. Additionally, by
deriving novel, yet mathematically equivalent, compositional forms for popular
acquisition functions, we recast the maximisation task as a compositional
optimisation problem, allowing us to benefit from the extensive literature in
this field. We highlight the empirical advantages of the compositional approach
to acquisition function maximisation across 3958 individual experiments
comprising synthetic optimisation tasks as well as tasks from Bayesmark. Given
the generality of the acquisition function maximisation subroutine, we posit
that the adoption of compositional optimisers has the potential to yield
performance improvements across all domains in which Bayesian optimisation is
currently being applied.
</p>
<a href="http://arxiv.org/abs/2012.08240" target="_blank">arXiv:2012.08240</a> [<a href="http://arxiv.org/pdf/2012.08240" target="_blank">pdf</a>]

<h2>Grounding Artificial Intelligence in the Origins of Human Behavior. (arXiv:2012.08564v2 [cs.AI] UPDATED)</h2>
<h3>Eleni Nisioti, Cl&#xe9;ment Moulin-Frier</h3>
<p>Recent advances in Artificial Intelligence (AI) have revived the quest for
agents able to acquire an open-ended repertoire of skills. However, although
this ability is fundamentally related to the characteristics of human
intelligence, research in this field rarely considers the processes that may
have guided the emergence of complex cognitive capacities during the evolution
of the species.

Research in Human Behavioral Ecology (HBE) seeks to understand how the
behaviors characterizing human nature can be conceived as adaptive responses to
major changes in the structure of our ecological niche. In this paper, we
propose a framework highlighting the role of environmental complexity in
open-ended skill acquisition, grounded in major hypotheses from HBE and recent
contributions in Reinforcement learning (RL). We use this framework to
highlight fundamental links between the two disciplines, as well as to identify
feedback loops that bootstrap ecological complexity and create promising
research directions for AI researchers.
</p>
<a href="http://arxiv.org/abs/2012.08564" target="_blank">arXiv:2012.08564</a> [<a href="http://arxiv.org/pdf/2012.08564" target="_blank">pdf</a>]

<h2>Energy-Constrained Delivery of Goods with Drones Under Varying Wind Conditions. (arXiv:2012.08602v2 [cs.RO] UPDATED)</h2>
<h3>Francesco Betti Sorbelli, Federico Cor&#xf2;, Sajal K. Das, Cristina M. Pinotti</h3>
<p>In this paper, we study the feasibility of sending drones to deliver goods
from a depot to a customer by solving what we call the Mission-Feasibility
Problem (MFP). Due to payload constraints, the drone can serve only one
customer at a time. To this end, we propose a novel framework based on
time-dependent cost graphs to properly model the MFP and tackle the delivery
dynamics. When the drone moves in the delivery area, the global wind may change
thereby affecting the drone's energy consumption, which in turn can increase or
decrease. This issue is addressed by designing three algorithms, namely: (i)
compute the route of minimum energy once, at the beginning of the mission, (ii)
dynamically reconsider the most convenient trip towards the destination, and
(iii) dynamically select only the best local choice. We evaluate the
performance of our algorithms on both synthetic and real-world data. The
changes in the drone's energy consumption are reflected by changes in the cost
of the edges of the graphs. The algorithms receive the new costs every time the
drone flies over a new vertex, and they have no full knowledge in advance of
the weights. We compare them in terms of the percentage of missions that are
completed with success (the drone delivers the goods and comes back to the
depot), with delivered (the drone delivers the goods but cannot come back to
the depot), and with failure (the drone neither delivers the goods nor comes
back to the depot).
</p>
<a href="http://arxiv.org/abs/2012.08602" target="_blank">arXiv:2012.08602</a> [<a href="http://arxiv.org/pdf/2012.08602" target="_blank">pdf</a>]

<h2>Exploiting Sample Uncertainty for Domain Adaptive Person Re-Identification. (arXiv:2012.08733v2 [cs.CV] UPDATED)</h2>
<h3>Kecheng Zheng, Cuiling Lan, Wenjun Zeng, Zhizheng Zhang, Zheng-Jun Zha</h3>
<p>Many unsupervised domain adaptive (UDA) person re-identification (ReID)
approaches combine clustering-based pseudo-label prediction with feature
fine-tuning. However, because of domain gap, the pseudo-labels are not always
reliable and there are noisy/incorrect labels. This would mislead the feature
representation learning and deteriorate the performance. In this paper, we
propose to estimate and exploit the credibility of the assigned pseudo-label of
each sample to alleviate the influence of noisy labels, by suppressing the
contribution of noisy samples. We build our baseline framework using the mean
teacher method together with an additional contrastive loss. We have observed
that a sample with a wrong pseudo-label through clustering in general has a
weaker consistency between the output of the mean teacher model and the student
model. Based on this finding, we propose to exploit the uncertainty (measured
by consistency levels) to evaluate the reliability of the pseudo-label of a
sample and incorporate the uncertainty to re-weight its contribution within
various ReID losses, including the identity (ID) classification loss per
sample, the triplet loss, and the contrastive loss. Our uncertainty-guided
optimization brings significant improvement and achieves the state-of-the-art
performance on benchmark datasets.
</p>
<a href="http://arxiv.org/abs/2012.08733" target="_blank">arXiv:2012.08733</a> [<a href="http://arxiv.org/pdf/2012.08733" target="_blank">pdf</a>]

