---
title: Latest Deep Learning Papers
date: 2020-11-16 21:00:01 +0800
featured-img: DL-logo
categories: [Deep_Learning]
tags: [Deep_Learning]
mathjax: Yes
---

<h1>Your arXiv Feed (271 Articles)</h1>
<h2>Fast and Robust Cascade Model for Multiple Degradation Single Image Super-Resolution. (arXiv:2011.07068v1 [cs.CV])</h2>
<h3>Santiago L&#xf3;pez-Tapia, Nicol&#xe1;s P&#xe9;rez de la Blanca</h3>
<p>Single Image Super-Resolution (SISR) is one of the low-level computer vision
problems that has received increased attention in the last few years. Current
approaches are primarily based on harnessing the power of deep learning models
and optimization techniques to reverse the degradation model. Owing to its
hardness, isotropic blurring or Gaussians with small anisotropic deformations
have been mainly considered. Here, we widen this scenario by including large
non-Gaussian blurs that arise in real camera movements. Our approach leverages
the degradation model and proposes a new formulation of the Convolutional
Neural Network (CNN) cascade model, where each network sub-module is
constrained to solve a specific degradation: deblurring or upsampling. A new
densely connected CNN-architecture is proposed where the output of each
sub-module is restricted using some external knowledge to focus it on its
specific task. As far we know this use of domain-knowledge to module-level is a
novelty in SISR. To fit the finest model, a final sub-module takes care of the
residual errors propagated by the previous sub-modules. We check our model with
three state of the art (SOTA) datasets in SISR and compare the results with the
SOTA models. The results show that our model is the only one able to manage our
wider set of deformations. Furthermore, our model overcomes all current SOTA
methods for a standard set of deformations. In terms of computational load, our
model also improves on the two closest competitors in terms of efficiency.
Although the approach is non-blind and requires an estimation of the blur
kernel, it shows robustness to blur kernel estimation errors, making it a good
alternative to blind models.
</p>
<a href="http://arxiv.org/abs/2011.07068" target="_blank">arXiv:2011.07068</a> [<a href="http://arxiv.org/pdf/2011.07068" target="_blank">pdf</a>]

<h2>Shaking Force Balancing of the Orthoglide. (arXiv:2011.07069v1 [cs.RO])</h2>
<h3>Jing Geng, Vigen Arakelian (LS2N, RoMas, INSA Rennes), Damien Chablat (LS2N, ReV, CNRS)</h3>
<p>The shaking force balancing is a well-known problem in the design of
high-speed robotic systems because the variable dynamic loads cause noises,
wear and fatigue of mechanical structures. Different solutions, for full or
partial shaking force balancing, via internal mass redistribution or by adding
auxiliary links were developed. The paper deals with the shaking force
balancing of the Orthoglide. The suggested solution based on the optimal
acceleration control of the manipulator's common center of mass allows a
significant reduction of the shaking force. Compared with the balancing method
via adding counterweights or auxiliary substructures, the proposed method can
avoid some drawbacks: the increase of the total mass, the overall size and the
complexity of the mechanism, which become especially challenging for special
parallel manipulators. Using the proposed motion control method, the maximal
value of the total mass center acceleration is reduced, as a consequence, the
shaking force of the manipulator decreases. The efficiency of the suggested
method via numerical simulations carried out with ADAMS is demonstrated.
</p>
<a href="http://arxiv.org/abs/2011.07069" target="_blank">arXiv:2011.07069</a> [<a href="http://arxiv.org/pdf/2011.07069" target="_blank">pdf</a>]

<h2>Robust Quadruped Jumping via Deep Reinforcement Learning. (arXiv:2011.07089v1 [cs.RO])</h2>
<h3>Guillaume Bellegarda, Quan Nguyen</h3>
<p>In this paper we consider a general task of jumping varying distances and
heights for a quadrupedal robot in noisy environments, such as off of uneven
terrain and with variable robot dynamics parameters. To accurately jump in such
conditions, we propose a framework using deep reinforcement learning to
leverage the complex solution of nonlinear trajectory optimization for
quadrupedal jumping. While the standalone optimization limits jumping to
take-off from flat ground and requires accurate assumption of robot dynamics,
our proposed approach improves the robustness to allow jumping off of
significantly uneven terrain with variable robot dynamical parameters. Through
our method, the quadruped is able to jump distances of up to 1 m and heights of
up to 0.4 m, while being robust to environment noise of foot disturbances of up
to 0.1 m in height as well as with 5% variability of its body mass and inertia.
This behavior is learned through just a few thousand simulated jumps, and video
results can be found at https://youtu.be/WVoImmxImL8.
</p>
<a href="http://arxiv.org/abs/2011.07089" target="_blank">arXiv:2011.07089</a> [<a href="http://arxiv.org/pdf/2011.07089" target="_blank">pdf</a>]

<h2>Reducing Inference Latency with Concurrent Architectures for Image Recognition. (arXiv:2011.07092v1 [cs.CV])</h2>
<h3>Ramyad Hadidi, Jiashen Cao, Michael S. Ryoo, Hyesoon Kim</h3>
<p>Satisfying the high computation demand of modern deep learning architectures
is challenging for achieving low inference latency. The current approaches in
decreasing latency only increase parallelism within a layer. This is because
architectures typically capture a single-chain dependency pattern that prevents
efficient distribution with a higher concurrency (i.e., simultaneous execution
of one inference among devices). Such single-chain dependencies are so
widespread that even implicitly biases recent neural architecture search (NAS)
studies. In this visionary paper, we draw attention to an entirely new space of
NAS that relaxes the single-chain dependency to provide higher concurrency and
distribution opportunities. To quantitatively compare these architectures, we
propose a score that encapsulates crucial metrics such as communication,
concurrency, and load balancing. Additionally, we propose a new generator and
transformation block that consistently deliver superior architectures compared
to current state-of-the-art methods. Finally, our preliminary results show that
these new architectures reduce the inference latency and deserve more
attention.
</p>
<a href="http://arxiv.org/abs/2011.07092" target="_blank">arXiv:2011.07092</a> [<a href="http://arxiv.org/pdf/2011.07092" target="_blank">pdf</a>]

<h2>Efficient Data Association and Uncertainty Quantification for Multi-Object Tracking. (arXiv:2011.07101v1 [cs.LG])</h2>
<h3>David S. Hayden, Sue Zheng, John W. Fisher III</h3>
<p>Robust data association is critical for analysis of long-term motion
trajectories in complex scenes. In its absence, trajectory precision suffers
due to periods of kinematic ambiguity degrading the quality of follow-on
analysis. Common optimization-based approaches often neglect uncertainty
quantification arising from these events. Consequently, we propose the Joint
Posterior Tracker (JPT), a Bayesian multi-object tracking algorithm that
robustly reasons over the posterior of associations and trajectories. Novel,
permutation-based proposals are crafted for exploration of posterior modes that
correspond to plausible association hypotheses. JPT exhibits more accurate
uncertainty representation of data associations with superior performance on
standard metrics when compared to existing baselines. We also show the utility
of JPT applied to automatic scheduling of user-in-the-loop annotations for
improved trajectory quality.
</p>
<a href="http://arxiv.org/abs/2011.07101" target="_blank">arXiv:2011.07101</a> [<a href="http://arxiv.org/pdf/2011.07101" target="_blank">pdf</a>]

<h2>Trajectory Optimization for High-Dimensional Nonlinear Systems under STL Specifications. (arXiv:2011.07104v1 [cs.RO])</h2>
<h3>Vince Kurtz, Hai Lin</h3>
<p>Signal Temporal Logic (STL) has gained popularity in recent years as a
specification language for cyber-physical systems, especially in robotics.
Beyond being expressive and easy to understand, STL is appealing because the
synthesis problem---generating a trajectory that satisfies a given
specification---can be formulated as a trajectory optimization problem.
Unfortunately, the associated cost function is nonsmooth and non-convex. As a
result, existing synthesis methods scale poorly to high-dimensional nonlinear
systems. In this letter, we present a new trajectory optimization approach for
STL synthesis based on Differential Dynamic Programming (DDP). It is well known
that DDP scales well to extremely high-dimensional nonlinear systems like
robotic quadrupeds and humanoids: we show that these advantages can be
harnessed for STL synthesis. We prove the soundness of our proposed approach,
demonstrate order-of-magnitude speed improvements over the state-of-the-art on
several benchmark problems, and demonstrate the scalability of our approach to
the full nonlinear dynamics of a 7 degree-of-freedom robot arm.
</p>
<a href="http://arxiv.org/abs/2011.07104" target="_blank">arXiv:2011.07104</a> [<a href="http://arxiv.org/pdf/2011.07104" target="_blank">pdf</a>]

<h2>Benchmarking Domain Randomisation for Visual Sim-to-Real Transfer. (arXiv:2011.07112v1 [cs.RO])</h2>
<h3>Raghad Alghonaim, Edward Johns</h3>
<p>Domain randomisation is a very popular method for visual sim-to-real transfer
in robotics, due to its simplicity and ability to achieve transfer without any
real-world images at all. But a number of design choices must be made to
achieve optimal transfer. In this paper, we perform a large-scale benchmarking
study on these choices, with two key experiments evaluated on a real-world
object pose estimation task, which is also a proxy for end-to-end visual
control. First, we study the quality of the rendering pipeline, and find that a
small number of high-quality images is superior to a large number of
low-quality images. Second, we study the type of randomisation, and find that
both distractors and textures are important for generalisation to novel
environments.
</p>
<a href="http://arxiv.org/abs/2011.07112" target="_blank">arXiv:2011.07112</a> [<a href="http://arxiv.org/pdf/2011.07112" target="_blank">pdf</a>]

<h2>Query-based Targeted Action-Space Adversarial Policies on Deep Reinforcement Learning Agents. (arXiv:2011.07114v1 [cs.LG])</h2>
<h3>Xian Yeow Lee, Yasaman Esfandiari, Kai Liang Tan, Soumik Sarkar</h3>
<p>Advances in computing resources have resulted in the increasing complexity of
cyber-physical systems (CPS). As the complexity of CPS evolved, the focus has
shifted from traditional control methods to deep reinforcement learning-based
(DRL) methods for control of these systems. This is due to the difficulty of
obtaining accurate models of complex CPS for traditional control. However, to
securely deploy DRL in production, it is essential to examine the weaknesses of
DRL-based controllers (policies) towards malicious attacks from all angles. In
this work, we investigate targeted attacks in the action-space domain, also
commonly known as actuation attacks in CPS literature, which perturbs the
outputs of a controller. We show that a query-based black-box attack model that
generates optimal perturbations with respect to an adversarial goal can be
formulated as another reinforcement learning problem. Thus, such an adversarial
policy can be trained using conventional DRL methods. Experimental results
showed that adversarial policies that only observe the nominal policy's output
generate stronger attacks than adversarial policies that observe the nominal
policy's input and output. Further analysis reveals that nominal policies whose
outputs are frequently at the boundaries of the action space are naturally more
robust towards adversarial policies. Lastly, we propose the use of adversarial
training with transfer learning to induce robust behaviors into the nominal
policy, which decreases the rate of successful targeted attacks by half.
</p>
<a href="http://arxiv.org/abs/2011.07114" target="_blank">arXiv:2011.07114</a> [<a href="http://arxiv.org/pdf/2011.07114" target="_blank">pdf</a>]

<h2>Deep Multi-view Image Fusion for Soybean Yield Estimation in Breeding Applications Deep Multi-view Image Fusion for Soybean Yield Estimation in Breeding Applications. (arXiv:2011.07118v1 [cs.CV])</h2>
<h3>Luis G Riera, Matthew E. Carroll, Zhisheng Zhang, Johnathon M. Shook, Sambuddha Ghosal, Tianshuang Gao, Arti Singh, Sourabh Bhattacharya, Baskar Ganapathysubramanian, Asheesh K. Singh, Soumik Sarkar</h3>
<p>Reliable seed yield estimation is an indispensable step in plant breeding
programs geared towards cultivar development in major row crops. The objective
of this study is to develop a machine learning (ML) approach adept at soybean
[\textit{Glycine max} L. (Merr.)] pod counting to enable genotype seed yield
rank prediction from in-field video data collected by a ground robot. To meet
this goal, we developed a multi-view image-based yield estimation framework
utilizing deep learning architectures. Plant images captured from different
angles were fused to estimate the yield and subsequently to rank soybean
genotypes for application in breeding decisions. We used data from controlled
imaging environment in field, as well as from plant breeding test plots in
field to demonstrate the efficacy of our framework via comparing performance
with manual pod counting and yield estimation.

Our results demonstrate the promise of ML models in making breeding decisions
with significant reduction of time and human effort, and opening new breeding
methods avenues to develop cultivars.
</p>
<a href="http://arxiv.org/abs/2011.07118" target="_blank">arXiv:2011.07118</a> [<a href="http://arxiv.org/pdf/2011.07118" target="_blank">pdf</a>]

<h2>Convergence Properties of Stochastic Hypergradients. (arXiv:2011.07122v1 [stat.ML])</h2>
<h3>Riccardo Grazzi, Massimiliano Pontil, Saverio Salzo</h3>
<p>Bilevel optimization problems are receiving increasing attention in machine
learning as they provide a natural framework for hyperparameter optimization
and meta-learning. A key step to tackle these problems in the design of
optimization algorithms for bilevel optimization is the efficient computation
of the gradient of the upper-level objective (hypergradient). In this work, we
study stochastic approximation schemes for the hypergradient, which are
important when the lower-level problem is empirical risk minimization on a
large dataset. We provide iteration complexity bounds for the mean square error
of the hypergradient approximation, under the assumption that the lower-level
problem is accessible only through a stochastic mapping which is a contraction
in expectation. Preliminary numerical experiments support our theoretical
analysis.
</p>
<a href="http://arxiv.org/abs/2011.07122" target="_blank">arXiv:2011.07122</a> [<a href="http://arxiv.org/pdf/2011.07122" target="_blank">pdf</a>]

<h2>Region-Based Planning for 3D Within-Hand-Manipulation via Variable Friction Robot Fingers and Extrinsic Contacts. (arXiv:2011.07132v1 [cs.RO])</h2>
<h3>Alp Sahin, Adam J. Spiers, Berk Calli</h3>
<p>Attempts to achieve robotic Within-Hand-Manipulation (WIHM) generally utilize
either high-DOF robotic hands with elaborate sensing apparatus or multi-arm
robotic systems. In prior work we presented a simple robot hand with variable
friction robot fingers, which allow a low-complexity approach to within-hand
object translation and rotation, though this manipulation was limited to planar
actions. In this work we extend the capabilities of this system to 3D
manipulation with a novel region-based WIHM planning algorithm and utilizing
extrinsic contacts. The ability to modulate finger friction enhances extrinsic
dexterity for three-dimensional WIHM, and allows us to operate in the
quasi-static level. The region-based planner automatically generates 3D
manipulation sequences with a modified A* formulation that navigates the
contact regions between the fingers and the object surface to reach desired
regions. Central to this method is a set of object-motion primitives (i.e.
within-hand sliding, rotation and pivoting), which can easily be achieved via
changing contact friction. A wide range of goal regions can be achieved via
this approach, which is demonstrated via real robot experiments following a
standardized in-hand manipulation benchmarking protocol.
</p>
<a href="http://arxiv.org/abs/2011.07132" target="_blank">arXiv:2011.07132</a> [<a href="http://arxiv.org/pdf/2011.07132" target="_blank">pdf</a>]

<h2>On the Transferability of VAE Embeddings using Relational Knowledge with Semi-Supervision. (arXiv:2011.07137v1 [cs.LG])</h2>
<h3>Harald Str&#xf6;mfelt, Luke Dickens, Artur d&#x27;Avila Garcez, Alessandra Russo</h3>
<p>We propose a new model for relational VAE semi-supervision capable of
balancing disentanglement and low complexity modelling of relations with
different symbolic properties. We compare the relative benefits of
relation-decoder complexity and latent space structure on both inductive and
transductive transfer learning. Our results depict a complex picture where
enforcing structure on semi-supervised representations can greatly improve
zero-shot transductive transfer, but may be less favourable or even impact
negatively the capacity for inductive transfer.
</p>
<a href="http://arxiv.org/abs/2011.07137" target="_blank">arXiv:2011.07137</a> [<a href="http://arxiv.org/pdf/2011.07137" target="_blank">pdf</a>]

<h2>Sparse Representations of Positive Functions via Projected Pseudo-Mirror Descent. (arXiv:2011.07142v1 [stat.ML])</h2>
<h3>Abhishek Chakraborty, Ketan Rajawat, Alec Koppel</h3>
<p>We consider the problem of expected risk minimization when the population
loss is strongly convex and the target domain of the decision variable is
required to be nonnegative, motivated by the settings of maximum likelihood
estimation (MLE) and trajectory optimization. We restrict focus to the case
that the decision variable belongs to a nonparametric Reproducing Kernel
Hilbert Space (RKHS). To solve it, we consider stochastic mirror descent that
employs (i) pseudo-gradients and (ii) projections. Compressive projections are
executed via kernel orthogonal matching pursuit (KOMP), and overcome the fact
that the vanilla RKHS parameterization grows unbounded with time. Moreover,
pseudo-gradients are needed, e.g., when stochastic gradients themselves define
integrals over unknown quantities that must be evaluated numerically, as in
estimating the intensity parameter of an inhomogeneous Poisson Process, and
multi-class kernel logistic regression with latent multi-kernels. We establish
tradeoffs between accuracy of convergence in mean and the projection budget
parameter under constant step-size and compression budget, as well as
non-asymptotic bounds on the model complexity. Experiments demonstrate that we
achieve state-of-the-art accuracy and complexity tradeoffs for inhomogeneous
Poisson Process intensity estimation and multi-class kernel logistic
regression.
</p>
<a href="http://arxiv.org/abs/2011.07142" target="_blank">arXiv:2011.07142</a> [<a href="http://arxiv.org/pdf/2011.07142" target="_blank">pdf</a>]

<h2>An example of prediction which complies with Demographic Parity and equalizes group-wise risks in the context of regression. (arXiv:2011.07158v1 [stat.ML])</h2>
<h3>Evgenii Chzhen, Nicolas Schreuder</h3>
<p>Let $(X, S, Y) \in \mathbb{R}^p \times \{1, 2\} \times \mathbb{R}$ be a
triplet following some joint distribution $\mathbb{P}$ with feature vector $X$,
sensitive attribute $S$ , and target variable $Y$. The Bayes optimal prediction
$f^*$ which does not produce Disparate Treatment is defined as $f^*(x) =
\mathbb{E}[Y | X = x]$. We provide a non-trivial example of a prediction $x \to
f(x)$ which satisfies two common group-fairness notions: Demographic Parity
\begin{align} (f(X) | S = 1) &amp;\stackrel{d}{=} (f(X) | S = 2) \end{align} and
Equal Group-Wise Risks \begin{align}

\mathbb{E}[(f^*(X) - f(X))^2 | S = 1] = \mathbb{E}[(f^*(X) - f(X))^2 | S =
2]. \end{align} To the best of our knowledge this is the first explicit
construction of a non-constant predictor satisfying the above. We discuss
several implications of this result on better understanding of mathematical
notions of algorithmic fairness.
</p>
<a href="http://arxiv.org/abs/2011.07158" target="_blank">arXiv:2011.07158</a> [<a href="http://arxiv.org/pdf/2011.07158" target="_blank">pdf</a>]

<h2>Joint Sampling and Trajectory Optimization over Graphs for Online Motion Planning. (arXiv:2011.07171v1 [cs.RO])</h2>
<h3>Kalyan Vasudev Alwala, Mustafa Mukadam</h3>
<p>Among the most prevailing motion planning techniques, sampling and trajectory
optimization have emerged successful due to their ability to handle tight
constraints and high-dimensional systems respectively. However, limitations in
sampling in higher dimensions and local minima issues in optimization have
hindered their ability to excel beyond static scenes in offline settings. Here
we consider highly dynamic environments with long horizons that necessitate a
fast online solution. We present a unified approach that leverages the
complementary strengths of sampling and optimization, and interleaves them both
in a manner that is well suited to this challenging problem. With benchmarks in
multiple synthetic and realistic simulated environments, we show our approach
is significantly better in performance on various metrics against baselines
that only either employ sampling or optimization. Supplementary video:
https://youtu.be/lfzZ6Vfzjvg
</p>
<a href="http://arxiv.org/abs/2011.07171" target="_blank">arXiv:2011.07171</a> [<a href="http://arxiv.org/pdf/2011.07171" target="_blank">pdf</a>]

<h2>Texture image classification based on a pseudo-parabolic diffusion model. (arXiv:2011.07173v1 [cs.CV])</h2>
<h3>Jardel Vieira, Eduardo Abreu, Joao B. Florindo</h3>
<p>This work proposes a novel method based on a pseudo-parabolic diffusion
process to be employed for texture recognition. The proposed operator is
applied over a range of time scales giving rise to a family of images
transformed by nonlinear filters. Therefore each of those images are encoded by
a local descriptor (we use local binary patterns for that purpose) and they are
summarized by a simple histogram, yielding in this way the image feature
vector. The proposed approach is tested on the classification of well
established benchmark texture databases and on a practical task of plant
species recognition. In both cases, it is compared with several
state-of-the-art methodologies employed for texture recognition. Our proposal
outperforms those methods in terms of classification accuracy, confirming its
competitiveness. The good performance can be justified to a large extent by the
ability of the pseudo-parabolic operator to smooth possibly noisy details
inside homogeneous regions of the image at the same time that it preserves
discontinuities that convey critical information for the object description.
Such results also confirm that model-based approaches like the proposed one can
still be competitive with the omnipresent learning-based approaches, especially
when the user does not have access to a powerful computational structure and a
large amount of labeled data for training.
</p>
<a href="http://arxiv.org/abs/2011.07173" target="_blank">arXiv:2011.07173</a> [<a href="http://arxiv.org/pdf/2011.07173" target="_blank">pdf</a>]

<h2>A Theoretical Perspective on Differentially Private Federated Multi-task Learning. (arXiv:2011.07179v1 [cs.LG])</h2>
<h3>Huiwen Wu, Cen Chen, Li Wang</h3>
<p>In the era of big data, the need to expand the amount of data through data
sharing to improve model performance has become increasingly compelling. As a
result, effective collaborative learning models need to be developed with
respect to both privacy and utility concerns. In this work, we propose a new
federated multi-task learning method for effective parameter transfer with
differential privacy to protect gradients at the client level. Specifically,
the lower layers of the networks are shared across all clients to capture
transferable feature representation, while top layers of the network are
task-specific for on-client personalization. Our proposed algorithm naturally
resolves the statistical heterogeneity problem in federated networks. We are,
to the best of knowledge, the first to provide both privacy and utility
guarantees for such a proposed federated algorithm. The convergences are proved
for the cases with Lipschitz smooth objective functions under the non-convex,
convex, and strongly convex settings. Empirical experiment results on different
datasets have been conducted to demonstrate the effectiveness of the proposed
algorithm and verify the implications of the theoretical findings.
</p>
<a href="http://arxiv.org/abs/2011.07179" target="_blank">arXiv:2011.07179</a> [<a href="http://arxiv.org/pdf/2011.07179" target="_blank">pdf</a>]

<h2>Duality-Gated Mutual Condition Network for RGBT Tracking. (arXiv:2011.07188v1 [cs.CV])</h2>
<h3>Andong Lu, Cun Qian, Chenglong Li, Jin Tang</h3>
<p>Low-quality modalities contain not only a lot of noisy information but also
some discriminative features in RGBT tracking. However, the potentials of
low-quality modalities are not well explored in existing RGBT tracking
algorithms. In this work, we propose a novel duality-gated mutual condition
network to fully exploit the discriminative information of all modalities while
suppressing the effects of data noise. In specific, we design a mutual
condition module, which takes the discriminative information of a modality as
the condition to guide feature learning of target appearance in another
modality. Such module can effectively enhance target representations and
suppress useless features of all modalities even in the presence of low-quality
modalities. To improve the quality of conditions and further reduce data noise,
we propose a duality-gated mechanism in the mutual condition module. To deal
with the tracking failure caused by sudden camera motion, which often occurs in
RGBT tracking, we design a resampling strategy based on optical flow
algorithms. It does not increase much computational cost since we perform
optical flow calculation only when the model prediction is unreliable and then
execute resampling when the sudden camera motion is detected. Extensive
experiments on three RGBT tracking benchmark datasets show that our method
performs favorably against the state-of-the-art tracking algorithms.
</p>
<a href="http://arxiv.org/abs/2011.07188" target="_blank">arXiv:2011.07188</a> [<a href="http://arxiv.org/pdf/2011.07188" target="_blank">pdf</a>]

<h2>RGBT Tracking via Multi-Adapter Network with Hierarchical Divergence Loss. (arXiv:2011.07189v1 [cs.CV])</h2>
<h3>Andong Lu, Chenglong Li, Yuqing Yan, Jin Tang, Bin Luo</h3>
<p>RGBT tracking has attracted increasing attention since RGB and thermal
infrared data have strong complementary advantages, which could make trackers
all-day and all-weather work. However, how to effectively represent RGBT data
for visual tracking remains unstudied well. Existing works usually focus on
extracting modality-shared or modality-specific information, but the potentials
of these two cues are not well explored and exploited in RGBT tracking. In this
paper, we propose a novel multi-adapter network to jointly perform
modality-shared, modality-specific and instance-aware target representation
learning for RGBT tracking. To this end, we design three kinds of adapters
within an end-to-end deep learning framework. In specific, we use the modified
VGG-M as the generality adapter to extract the modality-shared target
representations.To extract the modality-specific features while reducing the
computational complexity, we design a modality adapter, which adds a small
block to the generality adapter in each layer and each modality in a parallel
manner. Such a design could learn multilevel modality-specific representations
with a modest number of parameters as the vast majority of parameters are
shared with the generality adapter. We also design instance adapter to capture
the appearance properties and temporal variations of a certain target.
Moreover, to enhance the shared and specific features, we employ the loss of
multiple kernel maximum mean discrepancy to measure the distribution divergence
of different modal features and integrate it into each layer for more robust
representation learning. Extensive experiments on two RGBT tracking benchmark
datasets demonstrate the outstanding performance of the proposed tracker
against the state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2011.07189" target="_blank">arXiv:2011.07189</a> [<a href="http://arxiv.org/pdf/2011.07189" target="_blank">pdf</a>]

<h2>On the Benefits of Early Fusion in Multimodal Representation Learning. (arXiv:2011.07191v1 [cs.LG])</h2>
<h3>George Barnum, Sabera Talukder, Yisong Yue</h3>
<p>Intelligently reasoning about the world often requires integrating data from
multiple modalities, as any individual modality may contain unreliable or
incomplete information. Prior work in multimodal learning fuses input
modalities only after significant independent processing. On the other hand,
the brain performs multimodal processing almost immediately. This divide
between conventional multimodal learning and neuroscience suggests that a
detailed study of early multimodal fusion could improve artificial multimodal
representations. To facilitate the study of early multimodal fusion, we create
a convolutional LSTM network architecture that simultaneously processes both
audio and visual inputs, and allows us to select the layer at which audio and
visual information combines. Our results demonstrate that immediate fusion of
audio and visual inputs in the initial C-LSTM layer results in higher
performing networks that are more robust to the addition of white noise in both
audio and visual inputs.
</p>
<a href="http://arxiv.org/abs/2011.07191" target="_blank">arXiv:2011.07191</a> [<a href="http://arxiv.org/pdf/2011.07191" target="_blank">pdf</a>]

<h2>Towards Human-Level Learning of Complex Physical Puzzles. (arXiv:2011.07193v1 [cs.LG])</h2>
<h3>Kei Ota, Devesh K. Jha, Diego Romeres, Jeroen van Baar, Kevin A. Smith, Takayuki Semitsu, Tomoaki Oiki, Alan Sullivan, Daniel Nikovski, Joshua B. Tenenbaum</h3>
<p>Humans quickly solve tasks in novel systems with complex dynamics, without
requiring much interaction. While deep reinforcement learning algorithms have
achieved tremendous success in many complex tasks, these algorithms need a
large number of samples to learn meaningful policies. In this paper, we present
a task for navigating a marble to the center of a circular maze. While this
system is very intuitive and easy for humans to solve, it can be very difficult
and inefficient for standard reinforcement learning algorithms to learn
meaningful policies. We present a model that learns to move a marble in the
complex environment within minutes of interacting with the real system.
Learning consists of initializing a physics engine with parameters estimated
using data from the real system. The error in the physics engine is then
corrected using Gaussian process regression, which is used to model the
residual between real observations and physics engine simulations. The physics
engine equipped with the residual model is then used to control the marble in
the maze environment using a model-predictive feedback over a receding horizon.
We contrast the learning behavior against the time taken by humans to solve the
problem to show comparable behavior. To the best of our knowledge, this is the
first time that a hybrid model consisting of a full physics engine along with a
statistical function approximator has been used to control a complex physical
system in real-time using nonlinear model-predictive control (NMPC). Codes for
the simulation environment can be downloaded here
https://www.merl.com/research/license/CME . A video describing our method could
be found here https://youtu.be/xaxNCXBovpc .
</p>
<a href="http://arxiv.org/abs/2011.07193" target="_blank">arXiv:2011.07193</a> [<a href="http://arxiv.org/pdf/2011.07193" target="_blank">pdf</a>]

<h2>On the Existence of Two View Chiral Reconstructions. (arXiv:2011.07197v1 [cs.CV])</h2>
<h3>Andrew Pryhuber, Rainer Sinn, Rekha R. Thomas</h3>
<p>A fundamental question in computer vision is whether a set of point pairs is
the image of a scene that lies in front of two cameras. Such a scene and the
cameras together are known as a chiral reconstruction of the point pairs. In
this paper we provide a complete classification of k point pairs for which a
chiral reconstruction exists. The existence of chiral reconstructions is
equivalent to the non-emptiness of certain semialgebraic sets. For up to three
point pairs, we prove that a chiral reconstruction always exists while the set
of five or more point pairs that do not have a chiral reconstruction is
Zariski-dense. We show that for five generic point pairs, the chiral region is
bounded by line segments in a Schl\"afli double six on a cubic surface with 27
real lines. Four point pairs have a chiral reconstruction unless they belong to
two non-generic combinatorial types, in which case they may or may not.
</p>
<a href="http://arxiv.org/abs/2011.07197" target="_blank">arXiv:2011.07197</a> [<a href="http://arxiv.org/pdf/2011.07197" target="_blank">pdf</a>]

<h2>Deep Spatial Learning with Molecular Vibration. (arXiv:2011.07200v1 [cs.LG])</h2>
<h3>Ziyang Zhang, Yingtao Luo</h3>
<p>Machine learning over-fitting caused by data scarcity greatly limits the
application of machine learning for molecules. Due to manufacturing processes
difference, big data is not always rendered available through computational
chemistry methods for some tasks, causing data scarcity problem for machine
learning algorithms. Here we propose to extract the natural features of
molecular structures and rationally distort them to augment the data
availability. This method allows a machine learning project to leverage the
powerful fit of physics-informed augmentation for providing significant boost
to predictive accuracy. Successfully verified by the prediction of rejection
rate and flux of thin film polyamide nanofiltration membranes, with the
relative error dropping from 16.34% to 6.71% and the coefficient of
determination rising from 0.16 to 0.75, the proposed deep spatial learning with
molecular vibration is widely instructive for molecular science. Experimental
comparison unequivocally demonstrates its superiority over common learning
algorithms.
</p>
<a href="http://arxiv.org/abs/2011.07200" target="_blank">arXiv:2011.07200</a> [<a href="http://arxiv.org/pdf/2011.07200" target="_blank">pdf</a>]

<h2>Bi-Dimensional Feature Alignment for Cross-Domain Object Detection. (arXiv:2011.07205v1 [cs.CV])</h2>
<h3>Zhen Zhao, Yuhong Guo, Jieping Ye</h3>
<p>Recently the problem of cross-domain object detection has started drawing
attention in the computer vision community. In this paper, we propose a novel
unsupervised cross-domain detection model that exploits the annotated data in a
source domain to train an object detector for a different target domain. The
proposed model mitigates the cross-domain representation divergence for object
detection by performing cross-domain feature alignment in two dimensions, the
depth dimension and the spatial dimension. In the depth dimension of channel
layers, it uses inter-channel information to bridge the domain divergence with
respect to image style alignment. In the dimension of spatial layers, it
deploys spatial attention modules to enhance detection relevant regions and
suppress irrelevant regions with respect to cross-domain feature alignment.
Experiments are conducted on a number of benchmark cross-domain detection
datasets. The empirical results show the proposed method outperforms the
state-of-the-art comparison methods.
</p>
<a href="http://arxiv.org/abs/2011.07205" target="_blank">arXiv:2011.07205</a> [<a href="http://arxiv.org/pdf/2011.07205" target="_blank">pdf</a>]

<h2>PLAS: Latent Action Space for Offline Reinforcement Learning. (arXiv:2011.07213v1 [cs.RO])</h2>
<h3>Wenxuan Zhou, Sujay Bajracharya, David Held</h3>
<p>The goal of offline reinforcement learning is to learn a policy from a fixed
dataset, without further interactions with the environment. This setting will
be an increasingly more important paradigm for real-world applications of
reinforcement learning such as robotics, in which data collection is slow and
potentially dangerous. Existing off-policy algorithms have limited performance
on static datasets due to extrapolation errors from out-of-distribution
actions. This leads to the challenge of constraining the policy to select
actions within the support of the dataset during training. We propose to simply
learn the Policy in the Latent Action Space (PLAS) such that this requirement
is naturally satisfied. We evaluate our method on continuous control benchmarks
in simulation and a deformable object manipulation task with a physical robot.
We demonstrate that our method provides competitive performance consistently
across various continuous control tasks and different types of datasets,
outperforming existing offline reinforcement learning methods with explicit
constraints. Videos and code are available at
https://sites.google.com/view/latent-policy.
</p>
<a href="http://arxiv.org/abs/2011.07213" target="_blank">arXiv:2011.07213</a> [<a href="http://arxiv.org/pdf/2011.07213" target="_blank">pdf</a>]

<h2>SoftGym: Benchmarking Deep Reinforcement Learning for Deformable Object Manipulation. (arXiv:2011.07215v1 [cs.RO])</h2>
<h3>Xingyu Lin, Yufei Wang, Jake Olkin, David Held</h3>
<p>Manipulating deformable objects has long been a challenge in robotics due to
its high dimensional state representation and complex dynamics. Recent success
in deep reinforcement learning provides a promising direction for learning to
manipulate deformable objects with data driven methods. However, existing
reinforcement learning benchmarks only cover tasks with direct state
observability and simple low-dimensional dynamics or with relatively simple
image-based environments, such as those with rigid objects. In this paper, we
present SoftGym, a set of open-source simulated benchmarks for manipulating
deformable objects, with a standard OpenAI Gym API and a Python interface for
creating new environments. Our benchmark will enable reproducible research in
this important area. Further, we evaluate a variety of algorithms on these
tasks and highlight challenges for reinforcement learning algorithms, including
dealing with a state representation that has a high intrinsic dimensionality
and is partially observable. The experiments and analysis indicate the
strengths and limitations of existing methods in the context of deformable
object manipulation that can help point the way forward for future methods
development. Code and videos of the learned policies can be found on our
project website.
</p>
<a href="http://arxiv.org/abs/2011.07215" target="_blank">arXiv:2011.07215</a> [<a href="http://arxiv.org/pdf/2011.07215" target="_blank">pdf</a>]

<h2>MP-Boost: Minipatch Boosting via Adaptive Feature and Observation Sampling. (arXiv:2011.07218v1 [stat.ML])</h2>
<h3>Mohammad Taha Toghani, Genevera I. Allen</h3>
<p>Boosting methods are among the best general-purpose and off-the-shelf machine
learning approaches, gaining widespread popularity. In this paper, we seek to
develop a boosting method that yields comparable accuracy to popular AdaBoost
and gradient boosting methods, yet is faster computationally and whose solution
is more interpretable. We achieve this by developing MP-Boost, an algorithm
loosely based on AdaBoost that learns by adaptively selecting small subsets of
instances and features, or what we term minipatches (MP), at each iteration. By
sequentially learning on tiny subsets of the data, our approach is
computationally faster than other classic boosting algorithms. Also as it
progresses, MP-Boost adaptively learns a probability distribution on the
features and instances that upweight the most important features and
challenging instances, hence adaptively selecting the most relevant minipatches
for learning. These learned probability distributions also aid in
interpretation of our method. We empirically demonstrate the interpretability,
comparative accuracy, and computational time of our approach on a variety of
binary classification tasks.
</p>
<a href="http://arxiv.org/abs/2011.07218" target="_blank">arXiv:2011.07218</a> [<a href="http://arxiv.org/pdf/2011.07218" target="_blank">pdf</a>]

<h2>Deep Interpretable Classification and Weakly-Supervised Segmentation of Histology Images via Max-Min Uncertainty. (arXiv:2011.07221v1 [cs.CV])</h2>
<h3>Soufiane Belharbi, J&#xe9;r&#xf4;me Rony, Jose Dolz, Ismail Ben Ayed, Luke McCaffrey, Eric Granger</h3>
<p>Weakly supervised learning (WSL) has recently triggered substantial interest
as it mitigates the lack of pixel-wise annotations, while enabling
interpretable models. Given global image labels, WSL methods yield pixel-level
predictions (segmentations). Despite their recent success, mostly with natural
images, such methods could be seriously challenged when the foreground and
background regions have similar visual cues, yielding high false-positive rates
in segmentations, as is the case of challenging histology images. WSL training
is commonly driven by standard classification losses, which implicitly maximize
model confidence and find the discriminative regions linked to classification
decisions. Therefore, they lack mechanisms for modeling explicitly
non-discriminative regions and reducing false-positive rates. We propose new
regularization terms, which enable the model to seek both non-discriminative
and discriminative regions, while discouraging unbalanced segmentations. We
introduce high uncertainty as a criterion to localize non-discriminative
regions that do not affect classifier decision, and describe it with original
Kullback-Leibler (KL) divergence losses evaluating the deviation of posterior
predictions from the uniform distribution. Our KL terms encourage high
uncertainty of the model when the latter takes the latent non-discriminative
regions as input. Our loss integrates: (i) a cross-entropy seeking a
foreground, where model confidence about class prediction is high; (ii) a KL
regularizer seeking a background, where model uncertainty is high; and (iii)
log-barrier terms discouraging unbalanced segmentations. Comprehensive
experiments and ablation studies over the public GlaS colon cancer data show
substantial improvements over state-of-the-art WSL methods, and confirm the
effect of our new regularizers. Our code is publicly available.
</p>
<a href="http://arxiv.org/abs/2011.07221" target="_blank">arXiv:2011.07221</a> [<a href="http://arxiv.org/pdf/2011.07221" target="_blank">pdf</a>]

<h2>Reinforced Molecular Optimization with Neighborhood-Controlled Grammars. (arXiv:2011.07225v1 [cs.LG])</h2>
<h3>Chencheng Xu, Qiao Liu, Minlie Huang, Tao Jiang</h3>
<p>A major challenge in the pharmaceutical industry is to design novel molecules
with specific desired properties, especially when the property evaluation is
costly. Here, we propose MNCE-RL, a graph convolutional policy network for
molecular optimization with molecular neighborhood-controlled embedding
grammars through reinforcement learning. We extend the original
neighborhood-controlled embedding grammars to make them applicable to molecular
graph generation and design an efficient algorithm to infer grammatical
production rules from given molecules. The use of grammars guarantees the
validity of the generated molecular structures. By transforming molecular
graphs to parse trees with the inferred grammars, the molecular structure
generation task is modeled as a Markov decision process where a policy gradient
strategy is utilized. In a series of experiments, we demonstrate that our
approach achieves state-of-the-art performance in a diverse range of molecular
optimization tasks and exhibits significant superiority in optimizing molecular
properties with a limited number of property evaluations.
</p>
<a href="http://arxiv.org/abs/2011.07225" target="_blank">arXiv:2011.07225</a> [<a href="http://arxiv.org/pdf/2011.07225" target="_blank">pdf</a>]

<h2>OGNet: Towards a Global Oil and Gas Infrastructure Database using Deep Learning on Remotely Sensed Imagery. (arXiv:2011.07227v1 [cs.CV])</h2>
<h3>Hao Sheng, Jeremy Irvin, Sasankh Munukutla, Shawn Zhang, Christopher Cross, Kyle Story, Rose Rustowicz, Cooper Elsworth, Zutao Yang, Mark Omara, Ritesh Gautam, Robert B. Jackson, Andrew Y. Ng</h3>
<p>At least a quarter of the warming that the Earth is experiencing today is due
to anthropogenic methane emissions. There are multiple satellites in orbit and
planned for launch in the next few years which can detect and quantify these
emissions; however, to attribute methane emissions to their sources on the
ground, a comprehensive database of the locations and characteristics of
emission sources worldwide is essential. In this work, we develop deep learning
algorithms that leverage freely available high-resolution aerial imagery to
automatically detect oil and gas infrastructure, one of the largest
contributors to global methane emissions. We use the best algorithm, which we
call OGNet, together with expert review to identify the locations of oil
refineries and petroleum terminals in the U.S. We show that OGNet detects many
facilities which are not present in four standard public datasets of oil and
gas infrastructure. All detected facilities are associated with characteristics
known to contribute to methane emissions, including the infrastructure type and
the number of storage tanks. The data curated and produced in this study is
freely available at this http URL .
</p>
<a href="http://arxiv.org/abs/2011.07227" target="_blank">arXiv:2011.07227</a> [<a href="http://arxiv.org/pdf/2011.07227" target="_blank">pdf</a>]

<h2>CatFedAvg: Optimising Communication-efficiency and Classification Accuracy in Federated Learning. (arXiv:2011.07229v1 [cs.LG])</h2>
<h3>Dipankar Sarkar, Sumit Rai, Ankur Narang</h3>
<p>Federated learning has allowed the training of statistical models over remote
devices without the transfer of raw client data. In practice, training in
heterogeneous and large networks introduce novel challenges in various aspects
like network load, quality of client data, security and privacy. Recent works
in FL have worked on improving communication efficiency and addressing uneven
client data distribution independently, but none have provided a unified
solution for both challenges. We introduce a new family of Federated Learning
algorithms called CatFedAvg which not only improves the communication
efficiency but improves the quality of learning using a category coverage
maximization strategy.

We use the FedAvg framework and introduce a simple and efficient step every
epoch to collect meta-data about the client's training data structure which the
central server uses to request a subset of weight updates. We explore two
distinct variations which allow us to further explore the tradeoffs between
communication efficiency and model accuracy. Our experiments based on a vision
classification task have shown that an increase of 10% absolute points in
accuracy using the MNIST dataset with 70% absolute points lower network
transfer over FedAvg. We also run similar experiments with Fashion MNIST,
KMNIST-10, KMNIST-49 and EMNIST-47. Further, under extreme data imbalance
experiments for both globally and individual clients, we see the model
performing better than FedAvg. The ablation study further explores its
behaviour under varying data and client parameter conditions showcasing the
robustness of the proposed approach.
</p>
<a href="http://arxiv.org/abs/2011.07229" target="_blank">arXiv:2011.07229</a> [<a href="http://arxiv.org/pdf/2011.07229" target="_blank">pdf</a>]

<h2>TDAsweep: A Novel Dimensionality Reduction Method for Image Classification Tasks. (arXiv:2011.07230v1 [cs.CV])</h2>
<h3>Yu-Shih Chen, Melissa Goh, Norm Matloff</h3>
<p>One of the most celebrated achievements of modern machine learning technology
is automatic classification of images. However, success is typically achieved
only with major computational costs. Here we introduce TDAsweep, a machine
learning tool aimed at improving the efficiency of automatic classification of
images.
</p>
<a href="http://arxiv.org/abs/2011.07230" target="_blank">arXiv:2011.07230</a> [<a href="http://arxiv.org/pdf/2011.07230" target="_blank">pdf</a>]

<h2>ActBERT: Learning Global-Local Video-Text Representations. (arXiv:2011.07231v1 [cs.CV])</h2>
<h3>Linchao Zhu, Yi Yang</h3>
<p>In this paper, we introduce ActBERT for self-supervised learning of joint
video-text representations from unlabeled data. First, we leverage global
action information to catalyze the mutual interactions between linguistic texts
and local regional objects. It uncovers global and local visual clues from
paired video sequences and text descriptions for detailed visual and text
relation modeling. Second, we introduce an ENtangled Transformer block (ENT) to
encode three sources of information, i.e., global actions, local regional
objects, and linguistic descriptions. Global-local correspondences are
discovered via judicious clues extraction from contextual information. It
enforces the joint videotext representation to be aware of fine-grained objects
as well as global human intention. We validate the generalization capability of
ActBERT on downstream video-and language tasks, i.e., text-video clip
retrieval, video captioning, video question answering, action segmentation, and
action step localization. ActBERT significantly outperforms the
state-of-the-arts, demonstrating its superiority in video-text representation
learning.
</p>
<a href="http://arxiv.org/abs/2011.07231" target="_blank">arXiv:2011.07231</a> [<a href="http://arxiv.org/pdf/2011.07231" target="_blank">pdf</a>]

<h2>Stable View Synthesis. (arXiv:2011.07233v1 [cs.CV])</h2>
<h3>Gernot Riegler, Vladlen Koltun</h3>
<p>We present Stable View Synthesis (SVS). Given a set of source images
depicting a scene from freely distributed viewpoints, SVS synthesizes new views
of the scene. The method operates on a geometric scaffold computed via
structure-from-motion and multi-view stereo. Each point on this 3D scaffold is
associated with view rays and corresponding feature vectors that encode the
appearance of this point in the input images. The core of SVS is view-dependent
on-surface feature aggregation, in which directional feature vectors at each 3D
point are processed to produce a new feature vector for a ray that maps this
point into the new target view. The target view is then rendered by a
convolutional network from a tensor of features synthesized in this way for all
pixels. The method is composed of differentiable modules and is trained
end-to-end. It supports spatially-varying view-dependent importance weighting
and feature transformation of source images at each point; spatial and temporal
stability due to the smooth dependence of on-surface feature aggregation on the
target view; and synthesis of view-dependent effects such as specular
reflection. Experimental results demonstrate that SVS outperforms
state-of-the-art view synthesis methods both quantitatively and qualitatively
on three diverse real-world datasets, achieving unprecedented levels of realism
in free-viewpoint video of challenging large-scale scenes.
</p>
<a href="http://arxiv.org/abs/2011.07233" target="_blank">arXiv:2011.07233</a> [<a href="http://arxiv.org/pdf/2011.07233" target="_blank">pdf</a>]

<h2>Prototypical Contrast and Reverse Prediction: Unsupervised Skeleton Based Action Recognition. (arXiv:2011.07236v1 [cs.CV])</h2>
<h3>Shihao Xu, Haocong Rao, Xiping Hu, Bin Hu</h3>
<p>In this paper, we focus on unsupervised representation learning for
skeleton-based action recognition. Existing approaches usually learn action
representations by sequential prediction but they suffer from the inability to
fully learn semantic information. To address this limitation, we propose a
novel framework named Prototypical Contrast and Reverse Prediction (PCRP),
which not only creates reverse sequential prediction to learn low-level
information (e.g., body posture at every frame) and high-level pattern (e.g.,
motion order), but also devises action prototypes to implicitly encode semantic
similarity shared among sequences. In general, we regard action prototypes as
latent variables and formulate PCRP as an expectation-maximization task.
Specifically, PCRP iteratively runs (1) E-step as determining the distribution
of prototypes by clustering action encoding from the encoder, and (2) M-step as
optimizing the encoder by minimizing the proposed ProtoMAE loss, which helps
simultaneously pull the action encoding closer to its assigned prototype and
perform reverse prediction task. Extensive experiments on N-UCLA, NTU 60, and
NTU 120 dataset present that PCRP outperforms state-of-the-art unsupervised
methods and even achieves superior performance over some of supervised methods.
Codes are available at https://github.com/Mikexu007/PCRP.
</p>
<a href="http://arxiv.org/abs/2011.07236" target="_blank">arXiv:2011.07236</a> [<a href="http://arxiv.org/pdf/2011.07236" target="_blank">pdf</a>]

<h2>Self Normalizing Flows. (arXiv:2011.07248v1 [cs.LG])</h2>
<h3>T. Anderson Keller, Jorn W.T. Peters, Priyank Jaini, Emiel Hoogeboom, Patrick Forr&#xe9;, Max Welling</h3>
<p>Efficient gradient computation of the Jacobian determinant term is a core
problem of the normalizing flow framework. Thus, most proposed flow models
either restrict to a function class with easy evaluation of the Jacobian
determinant, or an efficient estimator thereof. However, these restrictions
limit the performance of such density models, frequently requiring significant
depth to reach desired performance levels. In this work, we propose Self
Normalizing Flows, a flexible framework for training normalizing flows by
replacing expensive terms in the gradient by learned approximate inverses at
each layer. This reduces the computational complexity of each layer's exact
update from $\mathcal{O}(D^3)$ to $\mathcal{O}(D^2)$, allowing for the training
of flow architectures which were otherwise computationally infeasible, while
also providing efficient sampling. We show experimentally that such models are
remarkably stable and optimize to similar data likelihood values as their exact
gradient counterparts, while surpassing the performance of their functionally
constrained counterparts.
</p>
<a href="http://arxiv.org/abs/2011.07248" target="_blank">arXiv:2011.07248</a> [<a href="http://arxiv.org/pdf/2011.07248" target="_blank">pdf</a>]

<h2>Ego2Hands: A Dataset for Egocentric Two-hand Segmentation and Detection. (arXiv:2011.07252v1 [cs.CV])</h2>
<h3>Fanqing Lin, Tony Martinez</h3>
<p>Hand segmentation and detection in truly unconstrained RGB-based settings is
important for many applications. However, existing datasets are far from
sufficient both in terms of size and variety due to the infeasibility of manual
annotation of large amounts of segmentation and detection data. As a result,
current methods are limited by many underlying assumptions such as constrained
environment, consistent skin color and lighting. In this work, we present a
large-scale RGB-based egocentric hand segmentation/detection dataset Ego2Hands
that is automatically annotated and a color-invariant compositing-based data
generation technique capable of creating unlimited training data with variety.
For quantitative analysis, we manually annotated an evaluation set that
significantly exceeds existing benchmarks in quantity, diversity and annotation
accuracy. We show that our dataset and training technique can produce models
that generalize to unseen environments without domain adaptation. We introduce
Convolutional Segmentation Machine (CSM) as an architecture that better
balances accuracy, size and speed and provide thorough analysis on the
performance of state-of-the-art models on the Ego2Hands dataset.
</p>
<a href="http://arxiv.org/abs/2011.07252" target="_blank">arXiv:2011.07252</a> [<a href="http://arxiv.org/pdf/2011.07252" target="_blank">pdf</a>]

<h2>Factorized Gaussian Process Variational Autoencoders. (arXiv:2011.07255v1 [stat.ML])</h2>
<h3>Metod Jazbec, Michael Pearce, Vincent Fortuin</h3>
<p>Variational autoencoders often assume isotropic Gaussian priors and
mean-field posteriors, hence do not exploit structure in scenarios where we may
expect similarity or consistency across latent variables. Gaussian process
variational autoencoders alleviate this problem through the use of a latent
Gaussian process, but lead to a cubic inference time complexity. We propose a
more scalable extension of these models by leveraging the independence of the
auxiliary features, which is present in many datasets. Our model factorizes the
latent kernel across these features in different dimensions, leading to a
significant speed-up (in theory and practice), while empirically performing
comparably to existing non-scalable approaches. Moreover, our approach allows
for additional modeling of global latent information and for more general
extrapolation to unseen input combinations.
</p>
<a href="http://arxiv.org/abs/2011.07255" target="_blank">arXiv:2011.07255</a> [<a href="http://arxiv.org/pdf/2011.07255" target="_blank">pdf</a>]

<h2>Graph-Based Neural Network Models with Multiple Self-Supervised Auxiliary Tasks. (arXiv:2011.07267v1 [cs.LG])</h2>
<h3>Franco Manessi, Alessandro Rozza</h3>
<p>Self-supervised learning is currently gaining a lot of attention, as it
allows neural networks to learn robust representations from large quantities of
unlabeled data. Additionally, multi-task learning can further improve
representation learning by training networks simultaneously on related tasks,
leading to significant performance improvements. In this paper, we propose a
general framework to improve graph-based neural network models by combining
self-supervised auxiliary learning tasks in a multi-task fashion. Since Graph
Convolutional Networks are among the most promising approaches for capturing
relationships among structured data points, we use them as a building block to
achieve competitive results on standard semi-supervised graph classification
tasks.
</p>
<a href="http://arxiv.org/abs/2011.07267" target="_blank">arXiv:2011.07267</a> [<a href="http://arxiv.org/pdf/2011.07267" target="_blank">pdf</a>]

<h2>Towards Zero-Shot Learning with Fewer Seen Class Examples. (arXiv:2011.07279v1 [cs.CV])</h2>
<h3>Vinay Kumar Verma, Ashish Mishra, Anubha Pandey, Hema A. Murthy, Piyush Rai</h3>
<p>We present a meta-learning based generative model for zero-shot learning
(ZSL) towards a challenging setting when the number of training examples from
each \emph{seen} class is very few. This setup contrasts with the conventional
ZSL approaches, where training typically assumes the availability of a
sufficiently large number of training examples from each of the seen classes.
The proposed approach leverages meta-learning to train a deep generative model
that integrates variational autoencoder and generative adversarial networks. We
propose a novel task distribution where meta-train and meta-validation classes
are disjoint to simulate the ZSL behaviour in training. Once trained, the model
can generate synthetic examples from seen and unseen classes. Synthesize
samples can then be used to train the ZSL framework in a supervised manner. The
meta-learner enables our model to generates high-fidelity samples using only a
small number of training examples from seen classes. We conduct extensive
experiments and ablation studies on four benchmark datasets of ZSL and observe
that the proposed model outperforms state-of-the-art approaches by a
significant margin when the number of examples per seen class is very small.
</p>
<a href="http://arxiv.org/abs/2011.07279" target="_blank">arXiv:2011.07279</a> [<a href="http://arxiv.org/pdf/2011.07279" target="_blank">pdf</a>]

<h2>Analytical Inverse Kinematics for a 5-DoF Robotic Arm with a Prismatic Joint. (arXiv:2011.07286v1 [cs.RO])</h2>
<h3>Vighnesh Vatsal, Guy Hoffman</h3>
<p>We present an analytical solution for the inverse kinematics (IK) of a
robotic arm with one prismatic joint and four revolute joints. This 5-DoF
design is a result of minimizing weight while preserving functionality of the
device in a wearable usage context. Generally, the IK problem for a 5-DoF robot
does not guarantee solutions due to the system being over-constrained. We
obtain an analytical solution by applying geometric projections and limiting
the ranges of motion for each DoF. We validate this solution by reconstructing
randomly sampled end-effector poses, and find position errors below 2 cm and
orientation errors below 4 degrees.
</p>
<a href="http://arxiv.org/abs/2011.07286" target="_blank">arXiv:2011.07286</a> [<a href="http://arxiv.org/pdf/2011.07286" target="_blank">pdf</a>]

<h2>Shortcomings of Counterfactual Fairness and a Proposed Modification. (arXiv:2011.07312v1 [cs.LG])</h2>
<h3>Fabian Beigang</h3>
<p>In this paper, I argue that counterfactual fairness does not constitute a
necessary condition for an algorithm to be fair, and subsequently suggest how
the constraint can be modified in order to remedy this shortcoming. To this
end, I discuss a hypothetical scenario in which counterfactual fairness and an
intuitive judgment of fairness come apart. Then, I turn to the question how the
concept of discrimination can be explicated in order to examine the
shortcomings of counterfactual fairness as a necessary condition of algorithmic
fairness in more detail. I then incorporate the insights of this analysis into
a novel fairness constraint, causal relevance fairness, which is a modification
of the counterfactual fairness constraint that seems to circumvent its
shortcomings.
</p>
<a href="http://arxiv.org/abs/2011.07312" target="_blank">arXiv:2011.07312</a> [<a href="http://arxiv.org/pdf/2011.07312" target="_blank">pdf</a>]

<h2>Planning Paths Through Unknown Space by Imagining What Lies Therein. (arXiv:2011.07316v1 [cs.RO])</h2>
<h3>Yutao Han, Jacopo Banfi, Mark Campbell</h3>
<p>This paper presents a novel framework for planning paths in maps containing
unknown spaces, such as from occlusions. Our approach takes as input a
semantically-annotated point cloud, and leverages an image inpainting neural
network to generate a reasonable model of unknown space as free or occupied.
Our validation campaign shows that it is possible to greatly increase the
performance of standard pathfinding algorithms which adopt the general
optimistic assumption of treating unknown space as free.
</p>
<a href="http://arxiv.org/abs/2011.07316" target="_blank">arXiv:2011.07316</a> [<a href="http://arxiv.org/pdf/2011.07316" target="_blank">pdf</a>]

<h2>A Geometric Perspective on Self-Supervised Policy Adaptation. (arXiv:2011.07318v1 [cs.LG])</h2>
<h3>Cristian Bodnar, Karol Hausman, Gabriel Dulac-Arnold, Rico Jonschkowski</h3>
<p>One of the most challenging aspects of real-world reinforcement learning (RL)
is the multitude of unpredictable and ever-changing distractions that could
divert an agent from what was tasked to do in its training environment. While
an agent could learn from reward signals to ignore them, the complexity of the
real-world can make rewards hard to acquire, or, at best, extremely sparse. A
recent class of self-supervised methods have shown promise that reward-free
adaptation under challenging distractions is possible. However, previous work
focused on a short one-episode adaptation setting. In this paper, we consider a
long-term adaptation setup that is more akin to the specifics of the real-world
and propose a geometric perspective on self-supervised adaptation. We
empirically describe the processes that take place in the embedding space
during this adaptation process, reveal some of its undesirable effects on
performance and show how they can be eliminated. Moreover, we theoretically
study how actor-based and actor-free agents can further generalise to the
target environment by manipulating the geometry of the manifolds described by
the actor and critic functions.
</p>
<a href="http://arxiv.org/abs/2011.07318" target="_blank">arXiv:2011.07318</a> [<a href="http://arxiv.org/pdf/2011.07318" target="_blank">pdf</a>]

<h2>Sparsity-Inducing Optimal Control via Differential Dynamic Programming. (arXiv:2011.07325v1 [cs.RO])</h2>
<h3>Traiko Dinev, Wolfgang Merkt, Vladimir Ivan, Ioannis Havoutis, Sethu Vijayakumar</h3>
<p>Optimal control is a popular approach to synthesize highly dynamic motion.
Commonly, L2 regularization is used on the control inputs in order to minimize
energy used and to ensure smoothness of the control inputs. However, for some
systems, such as satellites, the control needs to be applied in sparse bursts
due to how the propulsion system operates. In this paper, we study approaches
to induce sparsity in optimal control solutions---namely via smooth L1 and
Huber regularization penalties. We apply these loss terms to state-of-the-art
Differential Dynamic Programming (DDP)-based solvers to create a family of
sparsity-inducing optimal control methods. We analyze and compare the effect of
the different losses on inducing sparsity, their numerical conditioning, their
impact on convergence, and discuss hyperparameter settings. We demonstrate our
method in simulation and hardware experiments on canonical dynamics systems,
control of satellites, and the NASA Valkyrie humanoid robot. We provide an
implementation of our method and all examples for reproducibility on GitHub.
</p>
<a href="http://arxiv.org/abs/2011.07325" target="_blank">arXiv:2011.07325</a> [<a href="http://arxiv.org/pdf/2011.07325" target="_blank">pdf</a>]

<h2>Classification based on invisible features and thereby finding the effect of tuberculosis vaccine on COVID-19. (arXiv:2011.07332v1 [cs.LG])</h2>
<h3>Nihal Acharya Adde, Thilo Moshagen</h3>
<p>In the case of clustered data, an artificial neural network with logcosh loss
function learns the bigger cluster rather than the mean of the two. Even more
so, the ANN when used for regression of a set-valued function, will learn a
value close to one of the choices, in other words, it learns one branch of the
set-valued function with high accuracy. This work suggests a method that uses
artificial neural networks with logcosh loss to find the branches of set-valued
mappings in parameter-outcome sample sets and classifies the samples according
to those branches. The method not only classifies the data based on these
branches but also provides an accurate prediction for the majority cluster. The
method successfully classifies the data based on an invisible feature. A neural
network was successfully established to predict the total number of cases, the
logarithmic total number of cases, deaths, active cases and other relevant data
of the coronavirus for each German district from a number of input variables.
As it has been speculated that the Tuberculosis vaccine provides protection
against the virus and since East Germany was vaccinated before reunification,
an attempt was made to classify the Eastern and Western German districts by
considering the vaccine information as an invisible feature.
</p>
<a href="http://arxiv.org/abs/2011.07332" target="_blank">arXiv:2011.07332</a> [<a href="http://arxiv.org/pdf/2011.07332" target="_blank">pdf</a>]

<h2>Speech Prediction in Silent Videos using Variational Autoencoders. (arXiv:2011.07340v1 [cs.CV])</h2>
<h3>Ravindra Yadav, Ashish Sardana, Vinay P Namboodiri, Rajesh M Hegde</h3>
<p>Understanding the relationship between the auditory and visual signals is
crucial for many different applications ranging from computer-generated imagery
(CGI) and video editing automation to assisting people with hearing or visual
impairments. However, this is challenging since the distribution of both audio
and visual modality is inherently multimodal. Therefore, most of the existing
methods ignore the multimodal aspect and assume that there only exists a
deterministic one-to-one mapping between the two modalities. It can lead to
low-quality predictions as the model collapses to optimizing the average
behavior rather than learning the full data distributions. In this paper, we
present a stochastic model for generating speech in a silent video. The
proposed model combines recurrent neural networks and variational deep
generative models to learn the auditory signal's conditional distribution given
the visual signal. We demonstrate the performance of our model on the GRID
dataset based on standard benchmarks.
</p>
<a href="http://arxiv.org/abs/2011.07340" target="_blank">arXiv:2011.07340</a> [<a href="http://arxiv.org/pdf/2011.07340" target="_blank">pdf</a>]

<h2>Representing Deep Neural Networks Latent Space Geometries with Graphs. (arXiv:2011.07343v1 [cs.LG])</h2>
<h3>Carlos Lassance, Vincent Gripon, Antonio Ortega</h3>
<p>Deep Learning (DL) has attracted a lot of attention for its ability to reach
state-of-the-art performance in many machine learning tasks. The core principle
of DL methods consists in training composite architectures in an end-to-end
fashion, where inputs are associated with outputs trained to optimize an
objective function. Because of their compositional nature, DL architectures
naturally exhibit several intermediate representations of the inputs, which
belong to so-called latent spaces. When treated individually, these
intermediate representations are most of the time unconstrained during the
learning process, as it is unclear which properties should be favored. However,
when processing a batch of inputs concurrently, the corresponding set of
intermediate representations exhibit relations (what we call a geometry) on
which desired properties can be sought. In this work, we show that it is
possible to introduce constraints on these latent geometries to address various
problems. In more details, we propose to represent geometries by constructing
similarity graphs from the intermediate representations obtained when
processing a batch of inputs. By constraining these Latent Geometry Graphs
(LGGs), we address the three following problems: i) Reproducing the behavior of
a teacher architecture is achieved by mimicking its geometry, ii) Designing
efficient embeddings for classification is achieved by targeting specific
geometries, and iii) Robustness to deviations on inputs is achieved via
enforcing smooth variation of geometry between consecutive latent spaces. Using
standard vision benchmarks, we demonstrate the ability of the proposed
geometry-based methods in solving the considered problems.
</p>
<a href="http://arxiv.org/abs/2011.07343" target="_blank">arXiv:2011.07343</a> [<a href="http://arxiv.org/pdf/2011.07343" target="_blank">pdf</a>]

<h2>Towards transformation-resilient provenance detection of digital media. (arXiv:2011.07355v1 [cs.LG])</h2>
<h3>Jamie Hayes, Krishnamurthy (Dj) Dvijotham, Yutian Chen, Sander Dieleman, Pushmeet Kohli, Norman Casagrande</h3>
<p>Advancements in deep generative models have made it possible to synthesize
images, videos and audio signals that are difficult to distinguish from natural
signals, creating opportunities for potential abuse of these capabilities. This
motivates the problem of tracking the provenance of signals, i.e., being able
to determine the original source of a signal. Watermarking the signal at the
time of signal creation is a potential solution, but current techniques are
brittle and watermark detection mechanisms can easily be bypassed by applying
post-processing transformations (cropping images, shifting pitch in the audio
etc.). In this paper, we introduce ReSWAT (Resilient Signal Watermarking via
Adversarial Training), a framework for learning transformation-resilient
watermark detectors that are able to detect a watermark even after a signal has
been through several post-processing transformations. Our detection method can
be applied to domains with continuous data representations such as images,
videos or sound signals. Experiments on watermarking image and audio signals
show that our method can reliably detect the provenance of a signal, even if it
has been through several post-processing transformations, and improve upon
related work in this setting. Furthermore, we show that for specific kinds of
transformations (perturbations bounded in the L2 norm), we can even get formal
guarantees on the ability of our model to detect the watermark. We provide
qualitative examples of watermarked image and audio samples in
https://drive.google.com/open?id=1-yZ0WIGNu2Iez7UpXBjtjVgZu3jJjFga.
</p>
<a href="http://arxiv.org/abs/2011.07355" target="_blank">arXiv:2011.07355</a> [<a href="http://arxiv.org/pdf/2011.07355" target="_blank">pdf</a>]

<h2>Solving Physics Puzzles by Reasoning about Paths. (arXiv:2011.07357v1 [cs.AI])</h2>
<h3>Augustin Harter, Andrew Melnik, Gaurav Kumar, Dhruv Agarwal, Animesh Garg, Helge Ritter</h3>
<p>We propose a new deep learning model for goal-driven tasks that require
intuitive physical reasoning and intervention in the scene to achieve a desired
end goal. Its modular structure is motivated by hypothesizing a sequence of
intuitive steps that humans apply when trying to solve such a task. The model
first predicts the path the target object would follow without intervention and
the path the target object should follow in order to solve the task. Next, it
predicts the desired path of the action object and generates the placement of
the action object. All components of the model are trained jointly in a
supervised way; each component receives its own learning signal but learning
signals are also backpropagated through the entire architecture. To evaluate
the model we use PHYRE - a benchmark test for goal-driven physical reasoning in
2D mechanics puzzles.
</p>
<a href="http://arxiv.org/abs/2011.07357" target="_blank">arXiv:2011.07357</a> [<a href="http://arxiv.org/pdf/2011.07357" target="_blank">pdf</a>]

<h2>Bayesian recurrent state space model for rs-fMRI. (arXiv:2011.07365v1 [stat.ML])</h2>
<h3>Arunesh Mittal, Scott Linderman, John Paisley, Paul Sajda</h3>
<p>We propose a hierarchical Bayesian recurrent state space model for modeling
switching network connectivity in resting state fMRI data. Our model allows us
to uncover shared network patterns across disease conditions. We evaluate our
method on the ADNI2 dataset by inferring latent state patterns corresponding to
altered neural circuits in individuals with Mild Cognitive Impairment (MCI). In
addition to states shared across healthy and individuals with MCI, we discover
latent states that are predominantly observed in individuals with MCI. Our
model outperforms current state of the art deep learning method on ADNI2
dataset.
</p>
<a href="http://arxiv.org/abs/2011.07365" target="_blank">arXiv:2011.07365</a> [<a href="http://arxiv.org/pdf/2011.07365" target="_blank">pdf</a>]

<h2>Counting Cows: Tracking Illegal Cattle Ranching From High-Resolution Satellite Imagery. (arXiv:2011.07369v1 [cs.CV])</h2>
<h3>Issam Laradji, Pau Rodriguez, Freddie Kalaitzis, David Vazquez, Ross Young, Ed Davey, Alexandre Lacoste</h3>
<p>Cattle farming is responsible for 8.8\% of greenhouse gas emissions
worldwide. In addition to the methane emitted due to their digestive process,
the growing need for grazing areas is an important driver of deforestation.
While some regulations are in place for preserving the Amazon against
deforestation, these are being flouted in various ways, hence the need to scale
and automate the monitoring of cattle ranching activities. Through a
partnership with \textit{Global Witness}, we explore the feasibility of
tracking and counting cattle at the continental scale from satellite imagery.
With a license from Maxar Technologies, we obtained satellite imagery of the
Amazon at 40cm resolution, and compiled a dataset of 903 images containing a
total of 28498 cattle. Our experiments show promising results and highlight
important directions for the next steps on both counting algorithms and the
data collection process for solving such challenges. The code is available at
\url{https://github.com/IssamLaradji/cownter_strike}.
</p>
<a href="http://arxiv.org/abs/2011.07369" target="_blank">arXiv:2011.07369</a> [<a href="http://arxiv.org/pdf/2011.07369" target="_blank">pdf</a>]

<h2>Locomotion and Control of a Friction-Driven Tripedal Robot. (arXiv:2011.07370v1 [cs.RO])</h2>
<h3>Mark Hermes, Taylor McLaughlin, Mitul Luhar, Quan Nguyen</h3>
<p>This letter considers control of a radially symmetric tripedal
friction-driven robot. The robot features 3 servo motors mounted on a 3-D
printed chassis 7 cm from the center of mass and separated 120 degrees. These
motors drive limbs, which impart frictional reactive forces on the body.
Experimental observations performed on a uniform friction surface validated a
mathematical model for robot motion. This model was used to create a gait map,
which features instantaneous omni-directional control. We demonstrated line
following using live feedback from an overhead tracking camera.
Proportional-Integral error compensation performance was compared to a basic
position update procedure on a rectangular course. The controller reduced path
error by approximately $46\%$. The error compensator is also able to correct
for aerodynamic disturbances generated by a high-volume industrial fan with a
mean flow speed of $5.5ms^{-1}$, reducing path error by $65\%$ relative to the
basic position update procedure.
</p>
<a href="http://arxiv.org/abs/2011.07370" target="_blank">arXiv:2011.07370</a> [<a href="http://arxiv.org/pdf/2011.07370" target="_blank">pdf</a>]

<h2>Mobility Map Inference from Thermal Modeling of a Building. (arXiv:2011.07372v1 [cs.LG])</h2>
<h3>Risul Islam, Andrey Lokhov, Nathan Lemons, Michalis Faloutsos</h3>
<p>We consider the problem of inferring the mobility map, which is the
distribution of the building occupants at each timestamp, from the temperatures
of the rooms. We also want to explore the effects of noise in the temperature
measurement, room layout, etc. in the reconstruction of the movement of people
within the building. Our proposed algorithm tackles down the aforementioned
challenges leveraging a parameter learner, the modified Least Square Estimator.
In the absence of a complete data set with mobility map, room and ambient
temperatures, and HVAC data in the public domain, we simulate a physics-based
thermal model of the rooms in a building and evaluate the performance of our
inference algorithm on this simulated data. We find an upper bound of the noise
standard deviation (&lt;= 1F) in the input temperature data of our model. Within
this bound, our algorithm can reconstruct the mobility map with a reasonable
reconstruction error. Our work can be used in a wide range of applications, for
example, ensuring the physical security of office buildings, elderly and infant
monitoring, building resources management, emergency building evacuation, and
vulnerability assessment of HVAC data. Our work brings together multiple
research areas, Thermal Modeling and Parameter Estimation, towards achieving a
common goal of inferring the distribution of people within a large office
building.
</p>
<a href="http://arxiv.org/abs/2011.07372" target="_blank">arXiv:2011.07372</a> [<a href="http://arxiv.org/pdf/2011.07372" target="_blank">pdf</a>]

<h2>Designing Human-Robot Coexistence Space. (arXiv:2011.07374v1 [cs.RO])</h2>
<h3>Jixuan Zhi, Lap-Fai Yu, Jyh-Ming Lien</h3>
<p>When the human-robot interactions become ubiquitous, the environment
surrounding these interactions will have significant impact on the safety and
comfort of the human and the effectiveness and efficiency of the robot.
Although most robots are designed to work in the spaces created for humans,
many environments, such as living rooms and offices, can be and should be
redesigned to enhance and improve human-robot collaboration and interactions.
This work uses autonomous wheelchair as an example and investigates the
computational design in the human-robot coexistence spaces. Given the room size
and the objects $O$ in the room, the proposed framework computes the optimal
layouts of $O$ that satisfy both human preferences and navigation constraints
of the wheelchair. The key enabling technique is a motion planner that can
efficiently evaluate hundreds of similar motion planning problems. Our
implementation shows that the proposed framework can produce a design around
three to five minutes on average comparing to 10 to 20 minutes without the
proposed motion planner. Our results also show that the proposed method
produces reasonable designs even for tight spaces and for users with different
preferences.
</p>
<a href="http://arxiv.org/abs/2011.07374" target="_blank">arXiv:2011.07374</a> [<a href="http://arxiv.org/pdf/2011.07374" target="_blank">pdf</a>]

<h2>An Autonomous Approach to Measure Social Distances and Hygienic Practices during COVID-19 Pandemic in Public Open Spaces. (arXiv:2011.07375v1 [cs.CV])</h2>
<h3>Peng Sun, Gabriel Draughon, Jerome Lynch</h3>
<p>Coronavirus has been spreading around the world since the end of 2019. The
virus can cause acute respiratory syndrome, which can be lethal, and is easily
transmitted between hosts. Most states have issued state-at-home executive
orders, however, parks and other public open spaces have largely remained open
and are seeing sharp increases in public use. Therefore, in order to ensure
public safety, it is imperative for patrons of public open spaces to practice
safe hygiene and take preventative measures. This work provides a scalable
sensing approach to detect physical activities within public open spaces and
monitor adherence to social distancing guidelines suggested by the US Centers
for Disease Control and Prevention (CDC). A deep learning-based computer vision
sensing framework is designed to investigate the careful and proper utilization
of parks and park facilities with hard surfaces (e.g. benches, fence poles, and
trash cans) using video feeds from a pre-installed surveillance camera network.
The sensing framework consists of a CNN-based object detector, a multi-target
tracker, a mapping module, and a group reasoning module. The experiments are
carried out during the COVID-19 pandemic between March 2020 and May 2020 across
several key locations at the Detroit Riverfront Parks in Detroit, Michigan. The
sensing framework is validated by comparing automatic sensing results with
manually labeled ground-truth results. The proposed approach significantly
improves the efficiency of providing spatial and temporal statistics of users
in public open spaces by creating straightforward data visualizations for
federal and state agencies. The results can also provide on-time triggering
information for an alarming or actuator system which can later be added to
intervene inappropriate behavior during this pandemic.
</p>
<a href="http://arxiv.org/abs/2011.07375" target="_blank">arXiv:2011.07375</a> [<a href="http://arxiv.org/pdf/2011.07375" target="_blank">pdf</a>]

<h2>Search-based Planning for Active Sensing in Goal-Directed Coverage Tasks. (arXiv:2011.07383v1 [cs.RO])</h2>
<h3>Tushar Kusnur, Dhruv Mauria Saxena, Maxim Likhachev</h3>
<p>Path planning for robotic coverage is the task of determining a
collision-free robot trajectory that observes all points of interest in an
environment. Robots employed for such tasks are often capable of exercising
active control over onboard observational sensors during navigation. In this
paper, we tackle the problem of planning robot and sensor trajectories that
maximize information gain in such tasks where the robot needs to cover points
of interest with its sensor footprint. Search-based planners in general
guarantee completeness and provable bounds on suboptimality with respect to an
underlying graph discretization. However, searching for kinodynamically
feasible paths in the joint space of robot and sensor state variables with
standard search is computationally expensive. We propose two alternative
search-based approaches to this problem. The first solves for robot and sensor
trajectories independently in decoupled state spaces while maintaining a
history of sensor headings during the search. The second is a two-step approach
that first quickly computes a solution in decoupled state spaces and then
refines it by searching its local neighborhood in the joint space for a better
solution. We evaluate our approaches in simulation with a kinodynamically
constrained unmanned aerial vehicle performing coverage over a 2D environment
and show their benefits.
</p>
<a href="http://arxiv.org/abs/2011.07383" target="_blank">arXiv:2011.07383</a> [<a href="http://arxiv.org/pdf/2011.07383" target="_blank">pdf</a>]

<h2>Few-shot Object Grounding and Mapping for Natural Language Robot Instruction Following. (arXiv:2011.07384v1 [cs.RO])</h2>
<h3>Valts Blukis, Ross A. Knepper, Yoav Artzi</h3>
<p>We study the problem of learning a robot policy to follow natural language
instructions that can be easily extended to reason about new objects. We
introduce a few-shot language-conditioned object grounding method trained from
augmented reality data that uses exemplars to identify objects and align them
to their mentions in instructions. We present a learned map representation that
encodes object locations and their instructed use, and construct it from our
few-shot grounding output. We integrate this mapping approach into an
instruction-following policy, thereby allowing it to reason about previously
unseen objects at test-time by simply adding exemplars. We evaluate on the task
of learning to map raw observations and instructions to continuous control of a
physical quadcopter. Our approach significantly outperforms the prior state of
the art in the presence of new objects, even when the prior approach observes
all objects during training.
</p>
<a href="http://arxiv.org/abs/2011.07384" target="_blank">arXiv:2011.07384</a> [<a href="http://arxiv.org/pdf/2011.07384" target="_blank">pdf</a>]

<h2>Privacy-Preserving Pose Estimation for Human-Robot Interaction. (arXiv:2011.07387v1 [cs.RO])</h2>
<h3>Youya Xia, Yifan Tang, Yuhan Hu, Guy Hoffman</h3>
<p>Pose estimation is an important technique for nonverbal human-robot
interaction. That said, the presence of a camera in a person's space raises
privacy concerns and could lead to distrust of the robot. In this paper, we
propose a privacy-preserving camera-based pose estimation method. The proposed
system consists of a user-controlled translucent filter that covers the camera
and an image enhancement module designed to facilitate pose estimation from the
filtered (shadow) images, while never capturing clear images of the user. We
evaluate the system's performance on a new filtered image dataset, considering
the effects of distance from the camera, background clutter, and film
thickness. Based on our findings, we conclude that our system can protect
humans' privacy while detecting humans' pose information effectively.
</p>
<a href="http://arxiv.org/abs/2011.07387" target="_blank">arXiv:2011.07387</a> [<a href="http://arxiv.org/pdf/2011.07387" target="_blank">pdf</a>]

<h2>Discovery of the Hidden State in Ionic Models Using a Domain-Specific Recurrent Neural Network. (arXiv:2011.07388v1 [cs.LG])</h2>
<h3>Shahriar Iravanian</h3>
<p>Ionic models, the set of ordinary differential equations (ODEs) describing
the time evolution of the state of excitable cells, are the cornerstone of
modeling in neuro- and cardiac electrophysiology. Modern ionic models can have
tens of state variables and hundreds of tunable parameters. Fitting ionic
models to experimental data, which usually covers only a limited subset of
state variables, remains a challenging problem. In this paper, we describe a
recurrent neural network architecture designed specifically to encode ionic
models. The core of the model is a Gating Neural Network (GNN) layer, capturing
the dynamics of classic (Hodgkin-Huxley) gating variables. The network is
trained in two steps: first, it learns the theoretical model coded in a set of
ODEs, and second, it is retrained on experimental data. The retrained network
is interpretable, such that its results can be incorporated back into the model
ODEs. We tested the GNN networks using simulated ventricular action potential
signals and showed that it could deduce physiologically-feasible alterations of
ionic currents. Such domain-specific neural networks can be employed in the
exploratory phase of data assimilation before further fine-tuning using
standard optimization techniques.
</p>
<a href="http://arxiv.org/abs/2011.07388" target="_blank">arXiv:2011.07388</a> [<a href="http://arxiv.org/pdf/2011.07388" target="_blank">pdf</a>]

<h2>Automatic classification of multiple catheters in neonatal radiographs with deep learning. (arXiv:2011.07394v1 [cs.CV])</h2>
<h3>Robert D. E. Henderson, Xin Yi, Scott J. Adams, Paul Babyn</h3>
<p>We develop and evaluate a deep learning algorithm to classify multiple
catheters on neonatal chest and abdominal radiographs. A convolutional neural
network (CNN) was trained using a dataset of 777 neonatal chest and abdominal
radiographs, with a split of 81%-9%-10% for training-validation-testing,
respectively. We employed ResNet-50 (a CNN), pre-trained on ImageNet. Ground
truth labelling was limited to tagging each image to indicate the presence or
absence of endotracheal tubes (ETTs), nasogastric tubes (NGTs), and umbilical
arterial and venous catheters (UACs, UVCs). The data set included 561 images
containing 2 or more catheters, 167 images with only one, and 49 with none.
Performance was measured with average precision (AP), calculated from the area
under the precision-recall curve. On our test data, the algorithm achieved an
overall AP (95% confidence interval) of 0.977 (0.679-0.999) for NGTs, 0.989
(0.751-1.000) for ETTs, 0.979 (0.873-0.997) for UACs, and 0.937 (0.785-0.984)
for UVCs. Performance was similar for the set of 58 test images consisting of 2
or more catheters, with an AP of 0.975 (0.255-1.000) for NGTs, 0.997
(0.009-1.000) for ETTs, 0.981 (0.797-0.998) for UACs, and 0.937 (0.689-0.990)
for UVCs. Our network thus achieves strong performance in the simultaneous
detection of these four catheter types. Radiologists may use such an algorithm
as a time-saving mechanism to automate reporting of catheters on radiographs.
</p>
<a href="http://arxiv.org/abs/2011.07394" target="_blank">arXiv:2011.07394</a> [<a href="http://arxiv.org/pdf/2011.07394" target="_blank">pdf</a>]

<h2>Cost-Sensitive Machine Learning Classification for Mass Tuberculosis Verbal Screening. (arXiv:2011.07396v1 [cs.LG])</h2>
<h3>Ali Akbar Septiandri, Aditiawarman, Roy Tjiong, Erlina Burhan, Anuraj Shankar</h3>
<p>Score-based algorithms for tuberculosis (TB) verbal screening perform poorly,
causing misclassification that leads to missed cases and unnecessary costly
laboratory tests for false positives. We compared score-based classification
defined by clinicians to machine learning classification such as SVM-RBF,
logistic regression, and XGBoost. We restricted our analyses to data from
adults, the population most affected by TB, and investigated the difference
between untuned and unweighted classifiers to the cost-sensitive ones.
Predictions were compared with the corresponding GeneXpert MTB/Rif results.
After adjusting the weight of the positive class to 40 for XGBoost, we achieved
96.64% sensitivity and 35.06% specificity. As such, the sensitivity of our
identifier increased by 1.26% while specificity increased by 13.19% in absolute
value compared to the traditional score-based method defined by our clinicians.
Our approach further demonstrated that only 2000 data points were sufficient to
enable the model to converge. The results indicate that even with limited data
we can actually devise a better method to identify TB suspects from verbal
screening.
</p>
<a href="http://arxiv.org/abs/2011.07396" target="_blank">arXiv:2011.07396</a> [<a href="http://arxiv.org/pdf/2011.07396" target="_blank">pdf</a>]

<h2>Using Convolutional Variational Autoencoders to Predict Post-Trauma Health Outcomes from Actigraphy Data. (arXiv:2011.07406v1 [cs.LG])</h2>
<h3>Ayse S. Cakmak, Nina Thigpen, Garrett Honke, Erick Perez Alday, Ali Bahrami Rad, Rebecca Adaimi, Chia Jung Chang, Qiao Li, Pramod Gupta, Thomas Neylan, Samuel A. McLean, Gari D. Clifford</h3>
<p>Depression and post-traumatic stress disorder (PTSD) are psychiatric
conditions commonly associated with experiencing a traumatic event. Estimating
mental health status through non-invasive techniques such as activity-based
algorithms can help to identify successful early interventions. In this work,
we used locomotor activity captured from 1113 individuals who wore a research
grade smartwatch post-trauma. A convolutional variational autoencoder (VAE)
architecture was used for unsupervised feature extraction from four weeks of
actigraphy data. By using VAE latent variables and the participant's pre-trauma
physical health status as features, a logistic regression classifier achieved
an area under the receiver operating characteristic curve (AUC) of 0.64 to
estimate mental health outcomes. The results indicate that the VAE model is a
promising approach for actigraphy data analysis for mental health outcomes in
long-term studies.
</p>
<a href="http://arxiv.org/abs/2011.07406" target="_blank">arXiv:2011.07406</a> [<a href="http://arxiv.org/pdf/2011.07406" target="_blank">pdf</a>]

<h2>GENNI: Visualising the Geometry of Equivalences for Neural Network Identifiability. (arXiv:2011.07407v1 [cs.LG])</h2>
<h3>Daniel Lengyel, Janith Petangoda, Isak Falk, Kate Highnam, Michalis Lazarou, Arinbj&#xf6;rn Kolbeinsson, Marc Peter Deisenroth, Nicholas R. Jennings</h3>
<p>We propose an efficient algorithm to visualise symmetries in neural networks.
Typically, models are defined with respect to a parameter space, where
non-equal parameters can produce the same input-output map. Our proposed
method, GENNI, allows us to efficiently identify parameters that are
functionally equivalent and then visualise the subspace of the resulting
equivalence class. By doing so, we are now able to better explore questions
surrounding identifiability, with applications to optimisation and
generalizability, for commonly used or newly developed neural network
architectures.
</p>
<a href="http://arxiv.org/abs/2011.07407" target="_blank">arXiv:2011.07407</a> [<a href="http://arxiv.org/pdf/2011.07407" target="_blank">pdf</a>]

<h2>Accounting for Affect in Pain Level Recognition. (arXiv:2011.07421v1 [cs.CV])</h2>
<h3>Md Taufeeq Uddin, Shaun Canavan, Ghada Zamzmi</h3>
<p>In this work, we address the importance of affect in automated pain
assessment and the implications in real-world settings. To achieve this, we
curate a new physiological dataset by merging the publicly available bioVid
pain and emotion datasets. We then investigate pain level recognition on this
dataset simulating participants' naturalistic affective behaviors. Our findings
demonstrate that acknowledging affect in pain assessment is essential. We
observe degradation in recognition performance when simulating the existence of
affect to validate pain assessment models that do not account for it.
Conversely, we observe a performance boost in recognition when we account for
affect.
</p>
<a href="http://arxiv.org/abs/2011.07421" target="_blank">arXiv:2011.07421</a> [<a href="http://arxiv.org/pdf/2011.07421" target="_blank">pdf</a>]

<h2>Declarative Approaches to Counterfactual Explanations for Classification. (arXiv:2011.07423v1 [cs.AI])</h2>
<h3>Leopoldo Bertossi</h3>
<p>We propose answer-set programs that specify and compute counterfactual
interventions as a basis for causality-based explanations to the outcomes from
classification models. They can be applied with black-box models, and also with
models that can be specified as logic programs, such as rule-based classifiers.
The main focus is on the specification and computation of
maximum-responsibility counterfactual explanations, with responsibility
becoming an explanation score for features of entities under classification. We
also extend the programs to bring into the picture semantic or domain
knowledge. We show how the approach could be extended by means of probabilistic
methods, and how the underlying probability distributions could be modified
through the use of constraints.
</p>
<a href="http://arxiv.org/abs/2011.07423" target="_blank">arXiv:2011.07423</a> [<a href="http://arxiv.org/pdf/2011.07423" target="_blank">pdf</a>]

<h2>Intention-Based Lane Changing and Lane Keeping Haptic Guidance Steering System. (arXiv:2011.07424v1 [cs.RO])</h2>
<h3>Zhanhong Yan, Kaiming Yang, Zheng Wang, Bo Yang, Tsutomu Kaizuka, Kimihiko Nakano</h3>
<p>Haptic guidance in a shared steering assistance system has drawn significant
attention in intelligent vehicle fields, owing to its mutual communication
ability for vehicle control. By exerting continuous torque on the steering
wheel, both the driver and support system can share lateral control of the
vehicle. However, current haptic guidance steering systems demonstrate some
deficiencies in assisting lane changing. This study explored a new steering
interaction method, including the design and evaluation of an intention-based
haptic shared steering system. Such an intention-based method can support both
lane keeping and lane changing assistance, by detecting a driver lane change
intention. By using a deep learning-based method to model a driver decision
timing regarding lane crossing, an adaptive gain control method was proposed
for realizing a steering control system. An intention consistency method was
proposed to detect whether the driver and the system were acting towards the
same target trajectories and to accurately capture the driver intention. A
driving simulator experiment was conducted to test the system performance.
Participants were required to perform six trials with assistive methods and one
trial without assistance. The results demonstrated that the supporting system
decreased the lane departure risk in the lane keeping tasks and could support a
fast and stable lane changing maneuver.
</p>
<a href="http://arxiv.org/abs/2011.07424" target="_blank">arXiv:2011.07424</a> [<a href="http://arxiv.org/pdf/2011.07424" target="_blank">pdf</a>]

<h2>Pollen Grain Microscopic Image Classification Using an Ensemble of Fine-Tuned Deep Convolutional Neural Networks. (arXiv:2011.07428v1 [cs.CV])</h2>
<h3>Amirreza Mahbod, Gerald Schaefer, Rupert Ecker, Isabella Ellinger</h3>
<p>Pollen grain micrograph classification has multiple applications in medicine
and biology. Automatic pollen grain image classification can alleviate the
problems of manual categorisation such as subjectivity and time constraints.
While a number of computer-based methods have been introduced in the literature
to perform this task, classification performance needs to be improved for these
methods to be useful in practice.

In this paper, we present an ensemble approach for pollen grain microscopic
image classification into four categories: Corylus Avellana well-developed
pollen grain, Corylus Avellana anomalous pollen grain, Alnus well-developed
pollen grain, and non-pollen (debris) instances. In our approach, we develop a
classification strategy that is based on fusion of four state-of-the-art
fine-tuned convolutional neural networks, namely EfficientNetB0,
EfficientNetB1, EfficientNetB2 and SeResNeXt-50 deep models. These models are
trained with images of three fixed sizes (224x224, 240x240, and 260x260 pixels)
and their prediction probability vectors are then fused in an ensemble method
to form a final classification vector for a given pollen grain image.

Our proposed method is shown to yield excellent classification performance,
obtaining an accuracy of of 94.48% and a weighted F1-score of 94.54% on the
ICPR 2020 Pollen Grain Classification Challenge training dataset based on
five-fold cross-validation. Evaluated on the test set of the challenge, our
approach achieved a very competitive performance in comparison to the top
ranked approaches with an accuracy and a weighted F1-score of 96.28% and
96.30%, respectively.
</p>
<a href="http://arxiv.org/abs/2011.07428" target="_blank">arXiv:2011.07428</a> [<a href="http://arxiv.org/pdf/2011.07428" target="_blank">pdf</a>]

<h2>Dynamic backdoor attacks against federated learning. (arXiv:2011.07429v1 [cs.LG])</h2>
<h3>Anbu Huang</h3>
<p>Federated Learning (FL) is a new machine learning framework, which enables
millions of participants to collaboratively train machine learning model
without compromising data privacy and security. Due to the independence and
confidentiality of each client, FL does not guarantee that all clients are
honest by design, which makes it vulnerable to adversarial attack naturally. In
this paper, we focus on dynamic backdoor attacks under FL setting, where the
goal of the adversary is to reduce the performance of the model on targeted
tasks while maintaining a good performance on the main task, current existing
studies are mainly focused on static backdoor attacks, that is the poison
pattern injected is unchanged, however, FL is an online learning framework, and
adversarial targets can be changed dynamically by attacker, traditional
algorithms require learning a new targeted task from scratch, which could be
computationally expensive and require a large number of adversarial training
examples, to avoid this, we bridge meta-learning and backdoor attacks under FL
setting, in which case we can learn a versatile model from previous
experiences, and fast adapting to new adversarial tasks with a few of examples.
We evaluate our algorithm on different datasets, and demonstrate that our
algorithm can achieve good results with respect to dynamic backdoor attacks. To
the best of our knowledge, this is the first paper that focus on dynamic
backdoor attacks research under FL setting.
</p>
<a href="http://arxiv.org/abs/2011.07429" target="_blank">arXiv:2011.07429</a> [<a href="http://arxiv.org/pdf/2011.07429" target="_blank">pdf</a>]

<h2>Audio-Visual Event Recognition through the lens of Adversary. (arXiv:2011.07430v1 [cs.CV])</h2>
<h3>Juncheng B Li, Kaixin Ma, Shuhui Qu, Po-Yao Huang, Florian Metze</h3>
<p>As audio/visual classification models are widely deployed for sensitive tasks
like content filtering at scale, it is critical to understand their robustness
along with improving the accuracy. This work aims to study several key
questions related to multimodal learning through the lens of adversarial
noises: 1) The trade-off between early/middle/late fusion affecting its
robustness and accuracy 2) How do different frequency/time domain features
contribute to the robustness? 3) How do different neural modules contribute to
the adversarial noise? In our experiment, we construct adversarial examples to
attack state-of-the-art neural models trained on Google AudioSet. We compare
how much attack potency in terms of adversarial perturbation of size $\epsilon$
using different $L_p$ norms we would need to "deactivate" the victim model.
Using adversarial noise to ablate multimodal models, we are able to provide
insights into what is the best potential fusion strategy to balance the model
parameters/accuracy and robustness trade-off and distinguish the robust
features versus the non-robust features that various neural networks model tend
to learn.
</p>
<a href="http://arxiv.org/abs/2011.07430" target="_blank">arXiv:2011.07430</a> [<a href="http://arxiv.org/pdf/2011.07430" target="_blank">pdf</a>]

<h2>Enhance Gender and Identity Preservation in Face Aging Simulation for Infants and Toddlers. (arXiv:2011.07431v1 [cs.CV])</h2>
<h3>Yao Xiao, Yijun Zhao</h3>
<p>Realistic age-progressed photos provide invaluable biometric information in a
wide range of applications. In recent years, deep learning-based approaches
have made remarkable progress in modeling the aging process of the human face.
Nevertheless, it remains a challenging task to generate accurate age-progressed
faces from infant or toddler photos. In particular, the lack of visually
detectable gender characteristics and the drastic appearance changes in early
life contribute to the difficulty of the task. We propose a new deep learning
method inspired by the successful Conditional Adversarial Autoencoder (CAAE,
2017) model. In our approach, we extend the CAAE architecture to 1) incorporate
gender information, and 2) augment the model's overall architecture with an
identity-preserving component based on facial features. We trained our model
using the publicly available UTKFace dataset and evaluated our model by
simulating up to 100 years of aging on 1,156 male and 1,207 female infant and
toddler face photos. Compared to the CAAE approach, our new model demonstrates
noticeable visual improvements. Quantitatively, our model exhibits an overall
gain of 77.0% (male) and 13.8% (female) in gender fidelity measured by a gender
classifier for the simulated photos across the age spectrum. Our model also
demonstrates a 22.4% gain in identity preservation measured by a facial
recognition neural network.
</p>
<a href="http://arxiv.org/abs/2011.07431" target="_blank">arXiv:2011.07431</a> [<a href="http://arxiv.org/pdf/2011.07431" target="_blank">pdf</a>]

<h2>Functorial Manifold Learning and Overlapping Clustering. (arXiv:2011.07435v1 [cs.LG])</h2>
<h3>Dan Shiebler</h3>
<p>We adapt previous research on topological unsupervised learning to develop a
unified functorial perspective on manifold learning and clustering. We first
introduce overlapping hierachical clustering algorithms as functors and
demonstrate that the maximal and single linkage clustering algorithms factor
through an adaptation of the singular set functor. Next, we characterize
manifold learning algorithms as functors that map uber-metric spaces to
optimization objectives and factor through hierachical clustering functors. We
use this characterization to prove refinement bounds on manifold learning loss
functions and construct a hierarchy of manifold learning algorithms based on
their invariants. We express several state of the art manifold learning
algorithms as functors at different levels of this hierarchy, including
Laplacian Eigenmaps, Metric Multidimensional Scaling, and UMAP. Finally, we
experimentally demonstrate that this perspective enables us to derive and
analyze novel manifold learning algorithms.
</p>
<a href="http://arxiv.org/abs/2011.07435" target="_blank">arXiv:2011.07435</a> [<a href="http://arxiv.org/pdf/2011.07435" target="_blank">pdf</a>]

<h2>Efficient Variational Inference for Sparse Deep Learning with Theoretical Guarantee. (arXiv:2011.07439v1 [stat.ML])</h2>
<h3>Jincheng Bai, Qifan Song, Guang Cheng</h3>
<p>Sparse deep learning aims to address the challenge of huge storage
consumption by deep neural networks, and to recover the sparse structure of
target functions. Although tremendous empirical successes have been achieved,
most sparse deep learning algorithms are lacking of theoretical support. On the
other hand, another line of works have proposed theoretical frameworks that are
computationally infeasible. In this paper, we train sparse deep neural networks
with a fully Bayesian treatment under spike-and-slab priors, and develop a set
of computationally efficient variational inferences via continuous relaxation
of Bernoulli distribution. The variational posterior contraction rate is
provided, which justifies the consistency of the proposed variational Bayes
method. Notably, our empirical results demonstrate that this variational
procedure provides uncertainty quantification in terms of Bayesian predictive
distribution and is also capable to accomplish consistent variable selection by
training a sparse multi-layer neural network.
</p>
<a href="http://arxiv.org/abs/2011.07439" target="_blank">arXiv:2011.07439</a> [<a href="http://arxiv.org/pdf/2011.07439" target="_blank">pdf</a>]

<h2>Online Ensemble Model Compression using Knowledge Distillation. (arXiv:2011.07449v1 [cs.CV])</h2>
<h3>Devesh Walawalkar, Zhiqiang Shen, Marios Savvides</h3>
<p>This paper presents a novel knowledge distillation based model compression
framework consisting of a student ensemble. It enables distillation of
simultaneously learnt ensemble knowledge onto each of the compressed student
models. Each model learns unique representations from the data distribution due
to its distinct architecture. This helps the ensemble generalize better by
combining every model's knowledge. The distilled students and ensemble teacher
are trained simultaneously without requiring any pretrained weights. Moreover,
our proposed method can deliver multi-compressed students with single training,
which is efficient and flexible for different scenarios. We provide
comprehensive experiments using state-of-the-art classification models to
validate our framework's effectiveness. Notably, using our framework a 97%
compressed ResNet110 student model managed to produce a 10.64% relative
accuracy gain over its individual baseline training on CIFAR100 dataset.
Similarly a 95% compressed DenseNet-BC(k=12) model managed a 8.17% relative
accuracy gain.
</p>
<a href="http://arxiv.org/abs/2011.07449" target="_blank">arXiv:2011.07449</a> [<a href="http://arxiv.org/pdf/2011.07449" target="_blank">pdf</a>]

<h2>Coresets for Robust Training of Neural Networks against Noisy Labels. (arXiv:2011.07451v1 [cs.LG])</h2>
<h3>Baharan Mirzasoleiman, Kaidi Cao, Jure Leskovec</h3>
<p>Modern neural networks have the capacity to overfit noisy labels frequently
found in real-world datasets. Although great progress has been made, existing
techniques are limited in providing theoretical guarantees for the performance
of the neural networks trained with noisy labels. Here we propose a novel
approach with strong theoretical guarantees for robust training of deep
networks trained with noisy labels. The key idea behind our method is to select
weighted subsets (coresets) of clean data points that provide an approximately
low-rank Jacobian matrix. We then prove that gradient descent applied to the
subsets do not overfit the noisy labels. Our extensive experiments corroborate
our theory and demonstrate that deep networks trained on our subsets achieve a
significantly superior performance compared to state-of-the art, e.g., 6%
increase in accuracy on CIFAR-10 with 80% noisy labels, and 7% increase in
accuracy on mini Webvision.
</p>
<a href="http://arxiv.org/abs/2011.07451" target="_blank">arXiv:2011.07451</a> [<a href="http://arxiv.org/pdf/2011.07451" target="_blank">pdf</a>]

<h2>Debiasing Convolutional Neural Networks via Meta Orthogonalization. (arXiv:2011.07453v1 [cs.LG])</h2>
<h3>Kurtis Evan David, Qiang Liu, Ruth Fong</h3>
<p>While deep learning models often achieve strong task performance, their
successes are hampered by their inability to disentangle spurious correlations
from causative factors, such as when they use protected attributes (e.g., race,
gender, etc.) to make decisions. In this work, we tackle the problem of
debiasing convolutional neural networks (CNNs) in such instances. Building off
of existing work on debiasing word embeddings and model interpretability, our
Meta Orthogonalization method encourages the CNN representations of different
concepts (e.g., gender and class labels) to be orthogonal to one another in
activation space while maintaining strong downstream task performance. Through
a variety of experiments, we systematically test our method and demonstrate
that it significantly mitigates model bias and is competitive against current
adversarial debiasing methods.
</p>
<a href="http://arxiv.org/abs/2011.07453" target="_blank">arXiv:2011.07453</a> [<a href="http://arxiv.org/pdf/2011.07453" target="_blank">pdf</a>]

<h2>Molecular Mechanics-Driven Graph Neural Network with Multiplex Graph for Molecular Structures. (arXiv:2011.07457v1 [cs.LG])</h2>
<h3>Shuo Zhang, Yang Liu, Lei Xie</h3>
<p>The prediction of physicochemical properties from molecular structures is a
crucial task for artificial intelligence aided molecular design. A growing
number of Graph Neural Networks (GNNs) have been proposed to address this
challenge. These models improve their expressive power by incorporating
auxiliary information in molecules while inevitably increase their
computational complexity. In this work, we aim to design a GNN which is both
powerful and efficient for molecule structures. To achieve such goal, we
propose a molecular mechanics-driven approach by first representing each
molecule as a two-layer multiplex graph, where one layer contains only local
connections that mainly capture the covalent interactions and another layer
contains global connections that can simulate non-covalent interactions. Then
for each layer, a corresponding message passing module is proposed to balance
the trade-off of expression power and computational complexity. Based on these
two modules, we build Multiplex Molecular Graph Neural Network (MXMNet). When
validated by the QM9 dataset for small molecules and PDBBind dataset for large
protein-ligand complexes, MXMNet achieves superior results to the existing
state-of-the-art models under restricted resources.
</p>
<a href="http://arxiv.org/abs/2011.07457" target="_blank">arXiv:2011.07457</a> [<a href="http://arxiv.org/pdf/2011.07457" target="_blank">pdf</a>]

<h2>Direct Classification of Emotional Intensity. (arXiv:2011.07460v1 [cs.CV])</h2>
<h3>Jacob Ouyang, Isaac R Galatzer-Levy, Vidya Koesmahargyo, Li Zhang</h3>
<p>In this paper, we present a model that can directly predict emotion intensity
score from video inputs, instead of deriving from action units. Using a 3d DNN
incorporated with dynamic emotion information, we train a model using videos of
different people smiling that outputs an intensity score from 0-10. Each video
is labeled framewise using a normalized action-unit based intensity score. Our
model then employs an adaptive learning technique to improve performance when
dealing with new subjects. Compared to other models, our model excels in
generalization between different people as well as provides a new framework to
directly classify emotional intensity.
</p>
<a href="http://arxiv.org/abs/2011.07460" target="_blank">arXiv:2011.07460</a> [<a href="http://arxiv.org/pdf/2011.07460" target="_blank">pdf</a>]

<h2>CcGAN: Continuous Conditional Generative Adversarial Networks for Image Generation. (arXiv:2011.07466v1 [cs.CV])</h2>
<h3>Xin Ding, Yongwei Wang, Zuheng Xu, William J. Welch, Z. Jane Wang</h3>
<p>This work proposes the continuous conditional generative adversarial network
(CcGAN), the first generative model for image generation conditional on
continuous, scalar conditions (termed regression labels). Existing conditional
GANs (cGANs) are mainly designed for categorical conditions (e.g., class
labels); conditioning on regression labels is mathematically distinct and
raises two fundamental problems: (P1) Since there may be very few (even zero)
real images for some regression labels, minimizing existing empirical versions
of cGAN losses (a.k.a. empirical cGAN losses) often fails in practice; (P2)
Since regression labels are scalar and infinitely many, conventional label
input methods are not applicable. The proposed CcGAN solves the above problems,
respectively, by (S1) reformulating existing empirical cGAN losses to be
appropriate for the continuous scenario; and (S2) proposing a naive label input
(NLI) method and an improved label input (ILI) method to incorporate regression
labels into the generator and the discriminator. The reformulation in (S1)
leads to two novel empirical discriminator losses, termed the hard vicinal
discriminator loss (HVDL) and the soft vicinal discriminator loss (SVDL)
respectively, and a novel empirical generator loss. The error bounds of a
discriminator trained with HVDL and SVDL are derived under mild assumptions in
this work. Two new benchmark datasets (RC-49 and Cell-200) and a novel
evaluation metric (Sliding Fr\'echet Inception Distance) are also proposed for
this continuous scenario. Our experiments on the Circular 2-D Gaussians, RC-49,
UTKFace, Cell-200, and Steering Angle datasets show that CcGAN can generate
diverse, high-quality samples from the image distribution conditional on a
given regression label. Moreover, in these experiments, CcGAN substantially
outperforms cGAN both visually and quantitatively.
</p>
<a href="http://arxiv.org/abs/2011.07466" target="_blank">arXiv:2011.07466</a> [<a href="http://arxiv.org/pdf/2011.07466" target="_blank">pdf</a>]

<h2>An efficient label-free analyte detection algorithm for time-resolved spectroscopy. (arXiv:2011.07470v1 [cs.LG])</h2>
<h3>Stefano Rini, Hirotsugu Hiramatsu</h3>
<p>Time-resolved spectral techniques play an important analysis tool in many
contexts, from physical chemistry to biomedicine. Customarily, the label-free
detection of analytes is manually performed by experts through the aid of
classic dimensionality-reduction methods, such as Principal Component Analysis
(PCA) and Non-negative Matrix Factorization (NMF). This fundamental reliance on
expert analysis for unknown analyte detection severely hinders the
applicability and the throughput of these such techniques. For this reason, in
this paper, we formulate this detection problem as an unsupervised learning
problem and propose a novel machine learning algorithm for label-free analyte
detection. To show the effectiveness of the proposed solution, we consider the
problem of detecting the amino-acids in Liquid Chromatography coupled with
Raman spectroscopy (LC-Raman).
</p>
<a href="http://arxiv.org/abs/2011.07470" target="_blank">arXiv:2011.07470</a> [<a href="http://arxiv.org/pdf/2011.07470" target="_blank">pdf</a>]

<h2>Right Decisions from Wrong Predictions: A Mechanism Design Alternative to Individual Calibration. (arXiv:2011.07476v1 [stat.ML])</h2>
<h3>Shengjia Zhao, Stefano Ermon</h3>
<p>Decision makers often need to rely on imperfect probabilistic forecasts.
While average performance metrics are typically available, it is difficult to
assess the quality of individual forecasts and the corresponding utilities. To
convey confidence about individual predictions to decision-makers, we propose a
compensation mechanism ensuring that the forecasted utility matches the
actually accrued utility. While a naive scheme to compensate decision-makers
for prediction errors can be exploited and might not be sustainable in the long
run, we propose a mechanism based on fair bets and online learning that
provably cannot be exploited. We demonstrate an application showing how
passengers could confidently optimize individual travel plans based on flight
delay probabilities estimated by an airline.
</p>
<a href="http://arxiv.org/abs/2011.07476" target="_blank">arXiv:2011.07476</a> [<a href="http://arxiv.org/pdf/2011.07476" target="_blank">pdf</a>]

<h2>Towards Understanding the Regularization of Adversarial Robustness on Neural Networks. (arXiv:2011.07478v1 [cs.LG])</h2>
<h3>Yuxin Wen, Shuai Li, Kui Jia</h3>
<p>The problem of adversarial examples has shown that modern Neural Network (NN)
models could be rather fragile. Among the more established techniques to solve
the problem, one is to require the model to be {\it $\epsilon$-adversarially
robust} (AR); that is, to require the model not to change predicted labels when
any given input examples are perturbed within a certain range. However, it is
observed that such methods would lead to standard performance degradation,
i.e., the degradation on natural examples. In this work, we study the
degradation through the regularization perspective. We identify quantities from
generalization analysis of NNs; with the identified quantities we empirically
find that AR is achieved by regularizing/biasing NNs towards less confident
solutions by making the changes in the feature space (induced by changes in the
instance space) of most layers smoother uniformly in all directions; so to a
certain extent, it prevents sudden change in prediction w.r.t. perturbations.
However, the end result of such smoothing concentrates samples around decision
boundaries, resulting in less confident solutions, and leads to worse standard
performance. Our studies suggest that one might consider ways that build AR
into NNs in a gentler way to avoid the problematic regularization.
</p>
<a href="http://arxiv.org/abs/2011.07478" target="_blank">arXiv:2011.07478</a> [<a href="http://arxiv.org/pdf/2011.07478" target="_blank">pdf</a>]

<h2>Towards Trainable Saliency Maps in Medical Imaging. (arXiv:2011.07482v1 [cs.CV])</h2>
<h3>Mehak Aggarwal, Nishanth Arun, Sharut Gupta, Ashwin Vaswani, Bryan Chen, Matthew Li, Ken Chang, Jay Patel, Katherine Hoebel, Mishka Gidwani, Jayashree Kalpathy-Cramer, Praveer Singh</h3>
<p>While success of Deep Learning (DL) in automated diagnosis can be
transformative to the medicinal practice especially for people with little or
no access to doctors, its widespread acceptability is severely limited by
inherent black-box decision making and unsafe failure modes. While saliency
methods attempt to tackle this problem in non-medical contexts, their apriori
explanations do not transfer well to medical usecases. With this study we
validate a model design element agnostic to both architecture complexity and
model task, and show how introducing this element gives an inherently
self-explanatory model. We compare our results with state of the art
non-trainable saliency maps on RSNA Pneumonia Dataset and demonstrate a much
higher localization efficacy using our adopted technique. We also compare, with
a fully supervised baseline and provide a reasonable alternative to it's high
data labelling overhead. We further investigate the validity of our claims
through qualitative evaluation from an expert reader.
</p>
<a href="http://arxiv.org/abs/2011.07482" target="_blank">arXiv:2011.07482</a> [<a href="http://arxiv.org/pdf/2011.07482" target="_blank">pdf</a>]

<h2>Entropic regularization of Wasserstein distance between infinite-dimensional Gaussian measures and Gaussian processes. (arXiv:2011.07489v1 [stat.ML])</h2>
<h3>Minh Ha Quang</h3>
<p>This work studies the entropic regularization formulation of the
2-Wasserstein distance on an infinite-dimensional Hilbert space, in particular
for the Gaussian setting. We first present the Minimum Mutual Information
property, namely the joint measures of two Gaussian measures on Hilbert space
with the smallest mutual information are joint Gaussian measures. This is the
infinite-dimensional generalization of the Maximum Entropy property of Gaussian
densities on Euclidean space. We then give closed form formulas for the optimal
entropic transport plan, 2-Wasserstein distance, and Sinkhorn divergence
between two Gaussian measures on a Hilbert space, along with the fixed point
equations for the barycenter of a set of Gaussian measures. Our formulations
fully exploit the regularization aspect of the entropic formulation and are
valid both in singular and nonsingular settings. In the infinite-dimensional
setting, both the entropic 2-Wasserstein distance and Sinkhorn divergence are
Fr\'echet differentiable, in contrast to the exact 2-Wasserstein distance,
which is not differentiable. Our Sinkhorn barycenter equation is new and always
has a unique solution. In contrast, the finite-dimensional barycenter equation
for the entropic 2-Wasserstein distance fails to generalize to the Hilbert
space setting. In the setting of reproducing kernel Hilbert spaces (RKHS), our
distance formulas are given explicitly in terms of the corresponding kernel
Gram matrices, providing an interpolation between the kernel Maximum Mean
Discrepancy (MMD) and the kernel 2-Wasserstein distance.
</p>
<a href="http://arxiv.org/abs/2011.07489" target="_blank">arXiv:2011.07489</a> [<a href="http://arxiv.org/pdf/2011.07489" target="_blank">pdf</a>]

<h2>Anomaly Detection in Video via Self-Supervised and Multi-Task Learning. (arXiv:2011.07491v1 [cs.CV])</h2>
<h3>Mariana-Iuliana Georgescu, Antonio Barbalau, Radu Tudor Ionescu, Fahad Shahbaz Khan, Marius Popescu, Mubarak Shah</h3>
<p>Anomaly detection in video is a challenging computer vision problem. Due to
the lack of anomalous events at training time, anomaly detection requires the
design of learning methods without full supervision. In this paper, we approach
anomalous event detection in video through self-supervised and multi-task
learning at the object level. We first utilize a pre-trained detector to detect
objects. Then, we train a 3D convolutional neural network to produce
discriminative anomaly-specific information by jointly learning multiple proxy
tasks: three self-supervised and one based on knowledge distillation. The
self-supervised tasks are: (i) discrimination of forward/backward moving
objects (arrow of time), (ii) discrimination of objects in
consecutive/intermittent frames (motion irregularity) and (iii) reconstruction
of object-specific appearance information. The knowledge distillation task
takes into account both classification and detection information, generating
large prediction discrepancies between teacher and student models when
anomalies occur. To the best of our knowledge, we are the first to approach
anomalous event detection in video as a multi-task learning problem,
integrating multiple self-supervised and knowledge distillation proxy tasks in
a single architecture. Our lightweight architecture outperforms the
state-of-the-art methods on three benchmarks: Avenue, ShanghaiTech and UCSD
Ped2. Additionally, we perform an ablation study demonstrating the importance
of integrating self-supervised learning and normality-specific distillation in
a multi-task learning setting.
</p>
<a href="http://arxiv.org/abs/2011.07491" target="_blank">arXiv:2011.07491</a> [<a href="http://arxiv.org/pdf/2011.07491" target="_blank">pdf</a>]

<h2>FAIR: Fair Adversarial Instance Re-weighting. (arXiv:2011.07495v1 [cs.LG])</h2>
<h3>Andrija Petrovi&#x107;, Mladen Nikoli&#x107;, Sandro Radovanovi&#x107;, Boris Deliba&#x161;i&#x107;, Milo&#x161; Jovanovi&#x107;</h3>
<p>With growing awareness of societal impact of artificial intelligence,
fairness has become an important aspect of machine learning algorithms. The
issue is that human biases towards certain groups of population, defined by
sensitive features like race and gender, are introduced to the training data
through data collection and labeling. Two important directions of fairness
ensuring research have focused on (i) instance weighting in order to decrease
the impact of more biased instances and (ii) adversarial training in order to
construct data representations informative of the target variable, but
uninformative of the sensitive attributes. In this paper we propose a Fair
Adversarial Instance Re-weighting (FAIR) method, which uses adversarial
training to learn instance weighting function that ensures fair predictions.
Merging the two paradigms, it inherits desirable properties from both --
interpretability of reweighting and end-to-end trainability of adversarial
training. We propose four different variants of the method and, among other
things, demonstrate how the method can be cast in a fully probabilistic
framework. Additionally, theoretical analysis of FAIR models' properties have
been studied extensively. We compare FAIR models to 7 other related and
state-of-the-art models and demonstrate that FAIR is able to achieve a better
trade-off between accuracy and unfairness. To the best of our knowledge, this
is the first model that merges reweighting and adversarial approaches by means
of a weighting function that can provide interpretable information about
fairness of individual instances.
</p>
<a href="http://arxiv.org/abs/2011.07495" target="_blank">arXiv:2011.07495</a> [<a href="http://arxiv.org/pdf/2011.07495" target="_blank">pdf</a>]

<h2>Generating Negative Commonsense Knowledge. (arXiv:2011.07497v1 [cs.AI])</h2>
<h3>Tara Safavi, Danai Koutra</h3>
<p>The acquisition of commonsense knowledge is an important open challenge in
artificial intelligence. In this work-in-progress paper, we study the task of
automatically augmenting commonsense knowledge bases (KBs) with novel
statements. We show empirically that obtaining meaningful negative samples for
the completion task is nontrivial, and propose NegatER, a framework for
generating negative commonsense knowledge, to address this challenge. In our
evaluation we demonstrate the intrinsic value and extrinsic utility of the
knowledge generated by NegatER, opening up new avenues for future research in
this direction.
</p>
<a href="http://arxiv.org/abs/2011.07497" target="_blank">arXiv:2011.07497</a> [<a href="http://arxiv.org/pdf/2011.07497" target="_blank">pdf</a>]

<h2>BanglaWriting: A multi-purpose offline Bangla handwriting dataset. (arXiv:2011.07499v1 [cs.CV])</h2>
<h3>M. F. Mridha, Abu Quwsar Ohi, M. Ameer Ali, Mazedul Islam Emon, Muhammad Mohsin Kabir</h3>
<p>This article presents a Bangla handwriting dataset named BanglaWriting that
contains single-page handwritings of 260 individuals of different personalities
and ages. Each page includes bounding-boxes that bounds each word, along with
the unicode representation of the writing. This dataset contains 21,234 words
and 32,787 characters in total. Moreover, this dataset includes 5,470 unique
words of Bangla vocabulary. Apart from the usual words, the dataset comprises
261 comprehensible overwriting and 450 incomprehensible overwriting. All of the
bounding boxes and word labels are manually-generated. The dataset can be used
for complex optical character/word recognition, writer identification, and
handwritten word segmentation. Furthermore, this dataset is suitable for
extracting age-based and gender-based variation of handwriting.
</p>
<a href="http://arxiv.org/abs/2011.07499" target="_blank">arXiv:2011.07499</a> [<a href="http://arxiv.org/pdf/2011.07499" target="_blank">pdf</a>]

<h2>Automated Large-scale Class Scheduling in MiniZinc. (arXiv:2011.07507v1 [cs.AI])</h2>
<h3>Md. Mushfiqur Rahman, Sabah Binte Noor, Fazlul Hasan Siddiqui</h3>
<p>Class Scheduling is a highly constrained task. Educational institutes spend a
lot of resources, in the form of time and manual computation, to find a
satisficing schedule that fulfills all the requirements. A satisficing class
schedule accommodates all the students to all their desired courses at
convenient timing. The scheduler also needs to take into account the
availability of course teachers on the given slots. With the added limitation
of available classrooms, the number of solutions satisfying all constraints in
this huge search-space, further decreases.

This paper proposes an efficient system to generate class schedules that can
fulfill every possible need of a typical university. Though it is primarily a
fixed-credit scheduler, it can be adjusted for open-credit systems as well. The
model is designed in MiniZinc and solved using various off-the-shelf solvers.
The proposed scheduling system can find a balanced schedule for a
moderate-sized educational institute in less than a minute.
</p>
<a href="http://arxiv.org/abs/2011.07507" target="_blank">arXiv:2011.07507</a> [<a href="http://arxiv.org/pdf/2011.07507" target="_blank">pdf</a>]

<h2>Automated Intersection Management with MiniZinc. (arXiv:2011.07509v1 [cs.AI])</h2>
<h3>Md. Mushfiqur Rahman, Nahian Muhtasim Zahin, Kazi Raiyan Mahmud, Md. Azmaeen Bin Ansar</h3>
<p>Ill-managed intersections are the primary reasons behind the increasing
traffic problem in urban areas, leading to nonoptimal traffic-flow and
unnecessary deadlocks. In this paper, we propose an automated intersection
management system that extracts data from a well-defined grid of sensors and
optimizes traffic flow by controlling traffic signals. The data extraction
mechanism is independent of the optimization algorithm and this paper primarily
emphasizes the later one. We have used MiniZinc modeling language to define our
system as a constraint satisfaction problem which can be solved using any
off-the-shelf solver. The proposed system performs much better than the systems
currently in use. Our system reduces the mean waiting time and standard
deviation of the waiting time of vehicles and avoids deadlocks.
</p>
<a href="http://arxiv.org/abs/2011.07509" target="_blank">arXiv:2011.07509</a> [<a href="http://arxiv.org/pdf/2011.07509" target="_blank">pdf</a>]

<h2>AmphibianDetector: adaptive computation for moving objects detection. (arXiv:2011.07513v1 [cs.CV])</h2>
<h3>David Svitov, Sergey Alyamkin</h3>
<p>Convolutional neural networks (CNN) allow achieving the highest accuracy for
the task of object detection in images. Major challenges in further development
of object detectors are false-positive detections and high demand of processing
power. In this paper, we propose an approach to object detection, which makes
it possible to reduce the number of false-positive detections by processing
only moving objects and reduce required processing power for algorithm
inference. The proposed approach is modification of the CNN already trained for
object detection task. This method can be used to improve the accuracy of an
existing system by applying minor changes to the existing algorithm. The
efficiency of the proposed approach was demonstrated on the open dataset
"CDNet2014 pedestrian". The implementation of the method proposed in the
article is available on the GitHub:
https://github.com/david-svitov/AmphibianDetector
</p>
<a href="http://arxiv.org/abs/2011.07513" target="_blank">arXiv:2011.07513</a> [<a href="http://arxiv.org/pdf/2011.07513" target="_blank">pdf</a>]

<h2>2CP: Decentralized Protocols to Transparently Evaluate Contributivity in Blockchain Federated Learning Environments. (arXiv:2011.07516v1 [cs.LG])</h2>
<h3>Harry Cai, Daniel Rueckert, Jonathan Passerat-Palmbach</h3>
<p>Federated Learning harnesses data from multiple sources to build a single
model. While the initial model might belong solely to the actor bringing it to
the network for training, determining the ownership of the trained model
resulting from Federated Learning remains an open question. In this paper we
explore how Blockchains (in particular Ethereum) can be used to determine the
evolving ownership of a model trained with Federated Learning.

Firstly, we use the step-by-step evaluation metric to assess the relative
contributivities of participants in a Federated Learning process. Next, we
introduce 2CP, a framework comprising two novel protocols for Blockchained
Federated Learning, which both reward contributors with shares in the final
model based on their relative contributivity. The Crowdsource Protocol allows
an actor to bring a model forward for training, and use their own data to
evaluate the contributions made to it. Potential trainers are guaranteed a fair
share of the resulting model, even in a trustless setting. The Consortium
Protocol gives trainers the same guarantee even when no party owns the initial
model and no evaluator is available.

We conduct experiments with the MNIST dataset that reveal sound
contributivity scores resulting from both Protocols by rewarding larger
datasets with greater shares in the model. Our experiments also showed the
necessity to pair 2CP with a robust model aggregation mechanism to discard low
quality inputs coming from model poisoning attacks.
</p>
<a href="http://arxiv.org/abs/2011.07516" target="_blank">arXiv:2011.07516</a> [<a href="http://arxiv.org/pdf/2011.07516" target="_blank">pdf</a>]

<h2>Data-efficient Alignment of Multimodal Sequences by Aligning Gradient Updates and Internal Feature Distributions. (arXiv:2011.07517v1 [cs.CV])</h2>
<h3>Jianan Wang, Boyang Li, Xiangyu Fan, Jing Lin, Yanwei Fu</h3>
<p>The task of video and text sequence alignment is a prerequisite step toward
joint understanding of movie videos and screenplays. However, supervised
methods face the obstacle of limited realistic training data. With this paper,
we attempt to enhance data efficiency of the end-to-end alignment network
NeuMATCH [15]. Recent research [56] suggests that network components dealing
with different modalities may overfit and generalize at different speeds,
creating difficulties for training. We propose to employ (1) layer-wise
adaptive rate scaling (LARS) to align the magnitudes of gradient updates in
different layers and balance the pace of learning and (2) sequence-wise batch
normalization (SBN) to align the internal feature distributions from different
modalities. Finally, we leverage random projection to reduce the dimensionality
of input features. On the YouTube Movie Summary dataset, the combined use of
these technique closes the performance gap when the pretraining on the LSMDC
dataset is omitted and achieves the state-of-the-art result. Extensive
empirical comparisons and analysis reveal that these techniques improve
optimization and regularize the network more effectively than two different
setups of layer normalization.
</p>
<a href="http://arxiv.org/abs/2011.07517" target="_blank">arXiv:2011.07517</a> [<a href="http://arxiv.org/pdf/2011.07517" target="_blank">pdf</a>]

<h2>Domain Adaptation Gaze Estimation by Embedding with Prediction Consistency. (arXiv:2011.07526v1 [cs.CV])</h2>
<h3>Zidong Guo, Zejian Yuan, Chong Zhang, Wanchao Chi, Yonggen Ling, Shenghao Zhang</h3>
<p>Gaze is the essential manifestation of human attention. In recent years, a
series of work has achieved high accuracy in gaze estimation. However, the
inter-personal difference limits the reduction of the subject-independent gaze
estimation error. This paper proposes an unsupervised method for domain
adaptation gaze estimation to eliminate the impact of inter-personal diversity.
In domain adaption, we design an embedding representation with prediction
consistency to ensure that the linear relationship between gaze directions in
different domains remains consistent on gaze space and embedding space.
Specifically, we employ source gaze to form a locally linear representation in
the gaze space for each target domain prediction. Then the same linear
combinations are applied in the embedding space to generate hypothesis
embedding for the target domain sample, remaining prediction consistency. The
deviation between the target and source domain is reduced by approximating the
predicted and hypothesis embedding for the target domain sample. Guided by the
proposed strategy, we design Domain Adaptation Gaze Estimation Network(DAGEN),
which learns embedding with prediction consistency and achieves
state-of-the-art results on both the MPIIGaze and the EYEDIAP datasets.
</p>
<a href="http://arxiv.org/abs/2011.07526" target="_blank">arXiv:2011.07526</a> [<a href="http://arxiv.org/pdf/2011.07526" target="_blank">pdf</a>]

<h2>Estimation of the number of clusters on d-dimensional sphere. (arXiv:2011.07530v1 [cs.LG])</h2>
<h3>Kazuhisa Fujita</h3>
<p>Spherical data is distributed on the sphere. The data appears in various
fields such as meteorology, biology, and natural language processing. However,
a method for analysis of spherical data does not develop enough yet. One of the
important issues is an estimation of the number of clusters in spherical data.
To address the issue, I propose a new method called the Spherical X-means
(SX-means) that can estimate the number of clusters on d-dimensional sphere.
The SX-means is the model-based method assuming that the data is generated from
a mixture of von Mises-Fisher distributions. The present paper explains the
proposed method and shows its performance of estimation of the number of
clusters.
</p>
<a href="http://arxiv.org/abs/2011.07530" target="_blank">arXiv:2011.07530</a> [<a href="http://arxiv.org/pdf/2011.07530" target="_blank">pdf</a>]

<h2>Tonic: A Deep Reinforcement Learning Library for Fast Prototyping and Benchmarking. (arXiv:2011.07537v1 [cs.LG])</h2>
<h3>Fabio Pardo</h3>
<p>Deep reinforcement learning has been one of the fastest growing fields of
machine learning over the past years and numerous libraries have been open
sourced to support research. However, most codebases have a steep learning
curve or limited flexibility that do not satisfy a need for fast prototyping in
fundamental research. This paper introduces Tonic, a Python library allowing
researchers to quickly implement new ideas and measure their importance by
providing: 1) a collection of configurable modules such as exploration
strategies, replays, neural networks, and updaters 2) a collection of baseline
agents: A2C, TRPO, PPO, MPO, DDPG, D4PG, TD3 and SAC built with these modules
3) support for the two most popular deep learning frameworks: TensorFlow 2 and
PyTorch 4) support for the three most popular sets of continuous-control
environments: OpenAI Gym, DeepMind Control Suite and PyBullet 5) a large-scale
benchmark of the baseline agents on 70 continuous-control tasks 6) scripts to
experiment in a reproducible way, plot results, and play with trained agents.
</p>
<a href="http://arxiv.org/abs/2011.07537" target="_blank">arXiv:2011.07537</a> [<a href="http://arxiv.org/pdf/2011.07537" target="_blank">pdf</a>]

<h2>Discovering long term dependencies in noisy time series data using deep learning. (arXiv:2011.07551v1 [cs.LG])</h2>
<h3>Alexey Kurochkin</h3>
<p>Time series modelling is essential for solving tasks such as predictive
maintenance, quality control and optimisation. Deep learning is widely used for
solving such problems. When managing complex manufacturing process with neural
networks, engineers need to know why machine learning model made specific
decision and what are possible outcomes of following model recommendation. In
this paper we develop framework for capturing and explaining temporal
dependencies in time series data using deep neural networks and test it on
various synthetic and real world datasets.
</p>
<a href="http://arxiv.org/abs/2011.07551" target="_blank">arXiv:2011.07551</a> [<a href="http://arxiv.org/pdf/2011.07551" target="_blank">pdf</a>]

<h2>CDT: Cascading Decision Trees for Explainable Reinforcement Learning. (arXiv:2011.07553v1 [cs.LG])</h2>
<h3>Zihan Ding, Pablo Hernandez-Leal, Gavin Weiguang Ding, Changjian Li, Ruitong Huang</h3>
<p>Deep Reinforcement Learning (DRL) has recently achieved significant advances
in various domains. However, explaining the policy of RL agents still remains
an open problem due to several factors, one being the complexity of explaining
neural networks decisions. Recently, a group of works have used
decision-tree-based models to learn explainable policies. Soft decision trees
(SDTs) and discretized differentiable decision trees (DDTs) have been
demonstrated to achieve both good performance and share the benefit of having
explainable policies. In this work, we further improve the results for
tree-based explainable RL in both performance and explainability. Our proposal,
Cascading Decision Trees (CDTs) apply representation learning on the decision
path to allow richer expressivity. Empirical results show that in both
situations, where CDTs are used as policy function approximators or as
imitation learners to explain black-box policies, CDTs can achieve better
performances with more succinct and explainable models than SDTs. As a second
contribution our study reveals limitations of explaining black-box policies via
imitation learning with tree-based explainable models, due to its inherent
instability.
</p>
<a href="http://arxiv.org/abs/2011.07553" target="_blank">arXiv:2011.07553</a> [<a href="http://arxiv.org/pdf/2011.07553" target="_blank">pdf</a>]

<h2>Learn an Effective Lip Reading Model without Pains. (arXiv:2011.07557v1 [cs.CV])</h2>
<h3>Dalu Feng, Shuang Yang, Shiguang Shan, Xilin Chen</h3>
<p>Lip reading, also known as visual speech recognition, aims to recognize the
speech content from videos by analyzing the lip dynamics. There have been
several appealing progress in recent years, benefiting much from the rapidly
developed deep learning techniques and the recent large-scale lip-reading
datasets. Most existing methods obtained high performance by constructing a
complex neural network, together with several customized training strategies
which were always given in a very brief description or even shown only in the
source code. We find that making proper use of these strategies could always
bring exciting improvements without changing much of the model. Considering the
non-negligible effects of these strategies and the existing tough status to
train an effective lip reading model, we perform a comprehensive quantitative
study and comparative analysis, for the first time, to show the effects of
several different choices for lip reading. By only introducing some easy-to-get
refinements to the baseline pipeline, we obtain an obvious improvement of the
performance from 83.7% to 88.4% and from 38.2% to 55.7% on two largest public
available lip reading datasets, LRW and LRW-1000, respectively. They are
comparable and even surpass the existing state-of-the-art results.
</p>
<a href="http://arxiv.org/abs/2011.07557" target="_blank">arXiv:2011.07557</a> [<a href="http://arxiv.org/pdf/2011.07557" target="_blank">pdf</a>]

<h2>Placement in Integrated Circuits using Cyclic Reinforcement Learning and Simulated Annealing. (arXiv:2011.07577v1 [cs.AI])</h2>
<h3>Dhruv Vashisht, Harshit Rampal, Haiguang Liao, Yang Lu, Devika Shanbhag, Elias Fallon, Levent Burak Kara</h3>
<p>Physical design and production of Integrated Circuits (IC) is becoming
increasingly more challenging as the sophistication in IC technology is
steadily increasing. Placement has been one of the most critical steps in IC
physical design. Through decades of research, partition-based, analytical-based
and annealing-based placers have been enriching the placement solution toolbox.
However, open challenges including long run time and lack of ability to
generalize continue to restrict wider applications of existing placement tools.
We devise a learning-based placement tool based on cyclic application of
Reinforcement Learning (RL) and Simulated Annealing (SA) by leveraging the
advancement of RL. Results show that the RL module is able to provide a better
initialization for SA and thus leads to a better final placement design.
Compared to other recent learning-based placers, our method is majorly
different with its combination of RL and SA. It leverages the RL model's
ability to quickly get a good rough solution after training and the heuristic's
ability to realize greedy improvements in the solution.
</p>
<a href="http://arxiv.org/abs/2011.07577" target="_blank">arXiv:2011.07577</a> [<a href="http://arxiv.org/pdf/2011.07577" target="_blank">pdf</a>]

<h2>Pix2Streams: Dynamic Hydrology Maps from Satellite-LiDAR Fusion. (arXiv:2011.07584v1 [cs.CV])</h2>
<h3>Dolores Garcia, Gonzalo Mateo-Garcia, Hannes Bernhardt, Ron Hagensieker, Ignacio G. Lopez Francos, Jonathan Stock, Guy Schumann, Kevin Dobbs, Freddie Kalaitzis</h3>
<p>Where are the Earth's streams flowing right now? Inland surface waters expand
with floods and contract with droughts, so there is no one map of our streams.
Current satellite approaches are limited to monthly observations that map only
the widest streams. These are fed by smaller tributaries that make up much of
the dendritic surface network but whose flow is unobserved. A complete map of
our daily waters can give us an early warning for where droughts are born: the
receding tips of the flowing network. Mapping them over years can give us a map
of impermanence of our waters, showing where to expect water, and where not to.
To that end, we feed the latest high-res sensor data to multiple deep learning
models in order to map these flowing networks every day, stacking the times
series maps over many years. Specifically, i) we enhance water segmentation to
$50$ cm/pixel resolution, a 60$\times$ improvement over previous
state-of-the-art results. Our U-Net trained on 30-40cm WorldView3 images can
detect streams as narrow as 1-3m (30-60$\times$ over SOTA). Our multi-sensor,
multi-res variant, WasserNetz, fuses a multi-day window of 3m PlanetScope
imagery with 1m LiDAR data, to detect streams 5-7m wide. Both U-Nets produce a
water probability map at the pixel-level. ii) We integrate this water map over
a DEM-derived synthetic valley network map to produce a snapshot of flow at the
stream level. iii) We apply this pipeline, which we call Pix2Streams, to a
2-year daily PlanetScope time-series of three watersheds in the US to produce
the first high-fidelity dynamic map of stream flow frequency. The end result is
a new map that, if applied at the national scale, could fundamentally improve
how we manage our water resources around the world.
</p>
<a href="http://arxiv.org/abs/2011.07584" target="_blank">arXiv:2011.07584</a> [<a href="http://arxiv.org/pdf/2011.07584" target="_blank">pdf</a>]

<h2>Domain-Invariant Representation Learning for Sim-to-Real Transfer. (arXiv:2011.07589v1 [cs.CV])</h2>
<h3>Ajay Kumar Tanwani</h3>
<p>Generating large-scale synthetic data in simulation is a feasible alternative
to collecting/labelling real data for training vision-based deep learning
models, albeit the modelling inaccuracies do not generalize to the physical
world. In this paper, we present a domain-invariant representation learning
(DIRL) algorithm to adapt deep models to the physical environment with a small
amount of real data. Existing approaches that only mitigate the covariate shift
by aligning the marginal distributions across the domains and assume the
conditional distributions to be domain-invariant can lead to ambiguous transfer
in real scenarios. We propose to jointly align the marginal (input domains) and
the conditional (output labels) distributions to mitigate the covariate and the
conditional shift across the domains with adversarial learning, and combine it
with a triplet distribution loss to make the conditional distributions disjoint
in the shared feature space. Experiments on digit domains yield
state-of-the-art performance on challenging benchmarks, while sim-to-real
transfer of object recognition for vision-based decluttering with a mobile
robot improves from 26.8 % to 91.0 %, resulting in 86.5 % grasping accuracy of
a wide variety of objects. Code and supplementary details are available at
https://sites.google.com/view/dirl
</p>
<a href="http://arxiv.org/abs/2011.07589" target="_blank">arXiv:2011.07589</a> [<a href="http://arxiv.org/pdf/2011.07589" target="_blank">pdf</a>]

<h2>Does spontaneous motion lead to intuitive Body-Machine Interfaces? A fitness study of different body segments for wearable telerobotics. (arXiv:2011.07591v1 [cs.RO])</h2>
<h3>Matteo Macchini, Jan Frogg, Fabrizio Schiano, Dario Floreano</h3>
<p>Human-Robot Interfaces (HRIs) represent a crucial component in telerobotic
systems. Body-Machine Interfaces (BoMIs) based on body motion can feel more
intuitive than standard HRIs for naive users as they leverage humans' natural
control capability over their movements. Among the different methods used to
map human gestures into robot commands, data-driven approaches select a set of
body segments and transform their motion into commands for the robot based on
the users' spontaneous motion patterns. Despite being a versatile and generic
method, there is no scientific evidence that implementing an interface based on
spontaneous motion maximizes its effectiveness. In this study, we compare a set
of BoMIs based on different body segments to investigate this aspect. We
evaluate the interfaces in a teleoperation task of a fixed-wing drone and
observe users' performance and feedback. To this aim, we use a framework that
allows a user to control the drone with a single Inertial Measurement Unit
(IMU) and without prior instructions. We show through a user study that
selecting the body segment for a BoMI based on spontaneous motion can lead to
sub-optimal performance. Based on our findings, we suggest additional metrics
based on biomechanical and behavioral factors that might improve data-driven
methods for the design of HRIs.
</p>
<a href="http://arxiv.org/abs/2011.07591" target="_blank">arXiv:2011.07591</a> [<a href="http://arxiv.org/pdf/2011.07591" target="_blank">pdf</a>]

<h2>Deep Ordinal Regression using Optimal Transport Loss and Unimodal Output Probabilities. (arXiv:2011.07607v1 [stat.ML])</h2>
<h3>Uri Shaham, Jonathan Svirsky</h3>
<p>We propose a framework for deep ordinal regression, based on unimodal output
distribution and optimal transport loss. Despite being seemingly appropriate,
in many recent works the unimodality requirement is either absent, or
implemented using soft targets, which do not guarantee unimodal outputs at
inference. In addition, we argue that the standard maximum likelihood objective
is not suitable for ordinal regression problems, and that optimal transport is
better suited for this task, as it naturally captures the order of the classes.
Inspired by the well-known Proportional Odds model, we propose to modify its
design by using an architectural mechanism which guarantees that the model
output distribution will be unimodal. We empirically analyze the different
components of our propose approach and demonstrate their contribution to the
performance of the model. Experimental results on three real-world datasets
demonstrate that our proposed approach performs on par with several recently
proposed deep learning approaches for deep ordinal regression with unimodal
output probabilities, while having guarantee on the output unimodality. In
addition, we demonstrate that the level of prediction uncertainty of the model
correlates with its accuracy.
</p>
<a href="http://arxiv.org/abs/2011.07607" target="_blank">arXiv:2011.07607</a> [<a href="http://arxiv.org/pdf/2011.07607" target="_blank">pdf</a>]

<h2>BirdSLAM: Monocular Multibody SLAM in Bird's-Eye View. (arXiv:2011.07613v1 [cs.RO])</h2>
<h3>Swapnil Daga, Gokul B. Nair, Anirudha Ramesh, Rahul Sajnani, Junaid Ahmed Ansari, K. Madhava Krishna</h3>
<p>In this paper, we present BirdSLAM, a novel simultaneous localization and
mapping (SLAM) system for the challenging scenario of autonomous driving
platforms equipped with only a monocular camera. BirdSLAM tackles challenges
faced by other monocular SLAM systems (such as scale ambiguity in monocular
reconstruction, dynamic object localization, and uncertainty in feature
representation) by using an orthographic (bird's-eye) view as the configuration
space in which localization and mapping are performed. By assuming only the
height of the ego-camera above the ground, BirdSLAM leverages single-view
metrology cues to accurately localize the ego-vehicle and all other traffic
participants in bird's-eye view. We demonstrate that our system outperforms
prior work that uses strictly greater information, and highlight the relevance
of each design decision via an ablation analysis.
</p>
<a href="http://arxiv.org/abs/2011.07613" target="_blank">arXiv:2011.07613</a> [<a href="http://arxiv.org/pdf/2011.07613" target="_blank">pdf</a>]

<h2>Real-Time Polyp Detection, Localisation and Segmentation in Colonoscopy Using Deep Learning. (arXiv:2011.07631v1 [cs.CV])</h2>
<h3>Debesh Jha, Sharib Ali, H&#xe5;vard D. Johansen, Dag D. Johansen, Jens Rittscher, Michael A. Riegler, P&#xe5;l Halvorsen</h3>
<p>Computer-aided detection, localisation, and segmentation methods can help
improve colonoscopy procedures. Even though many methods have been built to
tackle automatic detection and segmentation of polyps, benchmarking of
state-of-the-art methods still remains an open problem. This is due to the
increasing number of researched computer-vision methods that can be applied to
polyp datasets. Benchmarking of novel methods can provide a direction to the
development of automated polyp detection and segmentation tasks. Furthermore,
it ensures that the produced results in the community are reproducible and
provide a fair comparison of developed methods. In this paper, we benchmark
several recent state-of-the-art methods using Kvasir-SEG, an open-access
dataset of colonoscopy images, for polyp detection, localisation, and
segmentation evaluating both method accuracy and speed. Whilst, most methods in
literature have competitive performance over accuracy, we show that YOLOv4 with
a Darknet53 backbone and cross-stage-partial connections achieved a better
trade-off between an average precision of 0.8513 and mean IoU of 0.8025, and
the fastest speed of 48 frames per second for the detection and localisation
task. Likewise, UNet with a ResNet34 backbone achieved the highest dice
coefficient of 0.8757 and the best average speed of 35 frames per second for
the segmentation task. Our comprehensive comparison with various
state-of-the-art methods reveal the importance of benchmarking the deep
learning methods for automated real-time polyp identification and delineations
that can potentially transform current clinical practices and minimise
miss-detection rates.
</p>
<a href="http://arxiv.org/abs/2011.07631" target="_blank">arXiv:2011.07631</a> [<a href="http://arxiv.org/pdf/2011.07631" target="_blank">pdf</a>]

<h2>Precision-Recall Curve (PRC) Classification Trees. (arXiv:2011.07640v1 [stat.ML])</h2>
<h3>Jiaju Miao, Wei Zhu</h3>
<p>The classification of imbalanced data has presented a significant challenge
for most well-known classification algorithms that were often designed for data
with relatively balanced class distributions. Nevertheless skewed class
distribution is a common feature in real world problems. It is especially
prevalent in certain application domains with great need for machine learning
and better predictive analysis such as disease diagnosis, fraud detection,
bankruptcy prediction, and suspect identification. In this paper, we propose a
novel tree-based algorithm based on the area under the precision-recall curve
(AUPRC) for variable selection in the classification context. Our algorithm,
named as the "Precision-Recall Curve classification tree", or simply the "PRC
classification tree" modifies two crucial stages in tree building. The first
stage is to maximize the area under the precision-recall curve in node variable
selection. The second stage is to maximize the harmonic mean of recall and
precision (F-measure) for threshold selection. We found the proposed PRC
classification tree, and its subsequent extension, the PRC random forest, work
well especially for class-imbalanced data sets. We have demonstrated that our
methods outperform their classic counterparts, the usual CART and random forest
for both synthetic and real data. Furthermore, the ROC classification tree
proposed by our group previously has shown good performance in imbalanced data.
The combination of them, the PRC-ROC tree, also shows great promise in
identifying the minority class.
</p>
<a href="http://arxiv.org/abs/2011.07640" target="_blank">arXiv:2011.07640</a> [<a href="http://arxiv.org/pdf/2011.07640" target="_blank">pdf</a>]

<h2>Stein Variational Model Predictive Control. (arXiv:2011.07641v1 [cs.RO])</h2>
<h3>Alexander Lambert, Adam Fishman, Dieter Fox, Byron Boots, Fabio Ramos</h3>
<p>Decision making under uncertainty is critical to real-world, autonomous
systems. Model Predictive Control (MPC) methods have demonstrated favorable
performance in practice, but remain limited when dealing with complex
probability distributions. In this paper, we propose a generalization of MPC
that represents a multitude of solutions as posterior distributions. By casting
MPC as a Bayesian inference problem, we employ variational methods for
posterior computation, naturally encoding the complexity and multi-modality of
the decision making problem. We propose a Stein variational gradient descent
method to estimate the posterior directly over control parameters, given a cost
function and observed state trajectories. We show that this framework leads to
successful planning in challenging, non-convex optimal control problems.
</p>
<a href="http://arxiv.org/abs/2011.07641" target="_blank">arXiv:2011.07641</a> [<a href="http://arxiv.org/pdf/2011.07641" target="_blank">pdf</a>]

<h2>Advances in the training, pruning and enforcement of shape constraints of Morphological Neural Networks using Tropical Algebra. (arXiv:2011.07643v1 [cs.LG])</h2>
<h3>Nikolaos Dimitriadis, Petros Maragos</h3>
<p>In this paper we study an emerging class of neural networks based on the
morphological operators of dilation and erosion. We explore these networks
mathematically from a tropical geometry perspective as well as mathematical
morphology. Our contributions are threefold. First, we examine the training of
morphological networks via Difference-of-Convex programming methods and extend
a binary morphological classifier to multiclass tasks. Second, we focus on the
sparsity of dense morphological networks trained via gradient descent
algorithms and compare their performance to their linear counterparts under
heavy pruning, showing that the morphological networks cope far better and are
characterized with superior compression capabilities. Our approach incorporates
the effect of the training optimizer used and offers quantitative and
qualitative explanations. Finally, we study how the architectural structure of
a morphological network can affect shape constraints, focusing on monotonicity.
Via Maslov Dequantization, we obtain a softened version of a known architecture
and show how this approach can improve training convergence and performance.
</p>
<a href="http://arxiv.org/abs/2011.07643" target="_blank">arXiv:2011.07643</a> [<a href="http://arxiv.org/pdf/2011.07643" target="_blank">pdf</a>]

<h2>Predicting Human Strategies in Simulated Search and Rescue Task. (arXiv:2011.07656v1 [cs.LG])</h2>
<h3>Vidhi Jain, Rohit Jena, Huao Li, Tejus Gupta, Dana Hughes, Michael Lewis, Katia Sycara</h3>
<p>In a search and rescue scenario, rescuers may have different knowledge of the
environment and strategies for exploration. Understanding what is inside a
rescuer's mind will enable an observer agent to proactively assist them with
critical information that can help them perform their task efficiently. To this
end, we propose to build models of the rescuers based on their trajectory
observations to predict their strategies. In our efforts to model the rescuer's
mind, we begin with a simple simulated search and rescue task in Minecraft with
human participants. We formulate neural sequence models to predict the triage
strategy and the next location of the rescuer. As the neural networks are
data-driven, we design a diverse set of artificial "faux human" agents for
training, to test them with limited human rescuer trajectory data. To evaluate
the agents, we compare it to an evidence accumulation method that explicitly
incorporates all available background knowledge and provides an intended upper
bound for the expected performance. Further, we perform experiments where the
observer/predictor is human. We show results in terms of prediction accuracy of
our computational approaches as compared with that of human observers.
</p>
<a href="http://arxiv.org/abs/2011.07656" target="_blank">arXiv:2011.07656</a> [<a href="http://arxiv.org/pdf/2011.07656" target="_blank">pdf</a>]

<h2>Deep multi-modal networks for book genre classification based on its cover. (arXiv:2011.07658v1 [cs.CV])</h2>
<h3>Chandra Kundu, Lukun Zheng</h3>
<p>Book covers are usually the very first impression to its readers and they
often convey important information about the content of the book. Book genre
classification based on its cover would be utterly beneficial to many modern
retrieval systems, considering that the complete digitization of books is an
extremely expensive task. At the same time, it is also an extremely challenging
task due to the following reasons: First, there exists a wide variety of book
genres, many of which are not concretely defined. Second, book covers, as
graphic designs, vary in many different ways such as colors, styles, textual
information, etc, even for books of the same genre. Third, book cover designs
may vary due to many external factors such as country, culture, target reader
populations, etc. With the growing competitiveness in the book industry, the
book cover designers and typographers push the cover designs to its limit in
the hope of attracting sales. The cover-based book classification systems
become a particularly exciting research topic in recent years. In this paper,
we propose a multi-modal deep learning framework to solve this problem. The
contribution of this paper is four-fold. First, our method adds an extra
modality by extracting texts automatically from the book covers. Second,
image-based and text-based, state-of-the-art models are evaluated thoroughly
for the task of book cover classification. Third, we develop an efficient and
salable multi-modal framework based on the images and texts shown on the covers
only. Fourth, a thorough analysis of the experimental results is given and
future works to improve the performance is suggested. The results show that the
multi-modal framework significantly outperforms the current state-of-the-art
image-based models. However, more efforts and resources are needed for this
classification task in order to reach a satisfactory level.
</p>
<a href="http://arxiv.org/abs/2011.07658" target="_blank">arXiv:2011.07658</a> [<a href="http://arxiv.org/pdf/2011.07658" target="_blank">pdf</a>]

<h2>hyper-sinh: An Accurate and Reliable Function from Shallow to Deep Learning in TensorFlow and Keras. (arXiv:2011.07661v1 [cs.CV])</h2>
<h3>Luca Parisi, Renfei Ma, Narrendar RaviChandran, Matteo Lanzillotta</h3>
<p>This paper presents the 'hyper-sinh', a variation of the m-arcsinh activation
function suitable for Deep Learning (DL)-based algorithms for supervised
learning, such as Convolutional Neural Networks (CNN). hyper-sinh, developed in
the open source Python libraries TensorFlow and Keras, is thus described and
validated as an accurate and reliable activation function for both shallow and
deep neural networks. Improvements in accuracy and reliability in image and
text classification tasks on five (N = 5) benchmark data sets available from
Keras are discussed. Experimental results demonstrate the overall competitive
classification performance of both shallow and deep neural networks, obtained
via this novel function. This function is evaluated with respect to gold
standard activation functions, demonstrating its overall competitive accuracy
and reliability for both image and text classification.
</p>
<a href="http://arxiv.org/abs/2011.07661" target="_blank">arXiv:2011.07661</a> [<a href="http://arxiv.org/pdf/2011.07661" target="_blank">pdf</a>]

<h2>Analog Circuit Design with Dyna-Style Reinforcement Learning. (arXiv:2011.07665v1 [cs.LG])</h2>
<h3>Wook Lee, Frans A. Oliehoek</h3>
<p>In this work, we present a learning based approach to analog circuit design,
where the goal is to optimize circuit performance subject to certain design
constraints. One of the aspects that makes this problem challenging to
optimize, is that measuring the performance of candidate configurations with
simulation can be computationally expensive, particularly in the post-layout
design. Additionally, the large number of design constraints and the
interaction between the relevant quantities makes the problem complex.
Therefore, to better facilitate supporting the human designers, it is desirable
to gain knowledge about the whole space of feasible solutions. In order to
tackle these challenges, we take inspiration from model-based reinforcement
learning and propose a method with two key properties. First, it learns a
reward model, i.e., surrogate model of the performance approximated by neural
networks, to reduce the required number of simulation. Second, it uses a
stochastic policy generator to explore the diverse solution space satisfying
constraints. Together we combine these in a Dyna-style optimization framework,
which we call DynaOpt, and empirically evaluate the performance on a circuit
benchmark of a two-stage operational amplifier. The results show that, compared
to the model-free method applied with 20,000 circuit simulations to train the
policy, DynaOpt achieves even much better performance by learning from scratch
with only 500 simulations.
</p>
<a href="http://arxiv.org/abs/2011.07665" target="_blank">arXiv:2011.07665</a> [<a href="http://arxiv.org/pdf/2011.07665" target="_blank">pdf</a>]

<h2>A Large-Scale Database for Graph Representation Learning. (arXiv:2011.07682v1 [cs.LG])</h2>
<h3>Scott Freitas, Yuxiao Dong, Joshua Neil, Duen Horng Chau</h3>
<p>With the rapid emergence of graph representation learning, the construction
of new large-scale datasets are necessary to distinguish model capabilities and
accurately assess the strengths and weaknesses of each technique. By carefully
analyzing existing graph databases, we identify 3 critical components important
for advancing the field of graph representation learning: (1) large graphs, (2)
many graphs, and (3) class diversity. To date, no single graph database offers
all of these desired properties. We introduce MalNet, the largest public graph
database ever constructed, representing a large-scale ontology of software
function call graphs. MalNet contains over 1.2 million graphs, averaging over
17k nodes and 39k edges per graph, across a hierarchy of 47 types and 696
families. Compared to the popular REDDIT-12K database, MalNet offers 105x more
graphs, 44x larger graphs on average, and 63x the classes. We provide a
detailed analysis of MalNet, discussing its properties and provenance. The
unprecedented scale and diversity of MalNet offers exciting opportunities to
advance the frontiers of graph representation learning---enabling new
discoveries and research into imbalanced classification, explainability and the
impact of class hardness. The database is publically available at
www.mal-net.org.
</p>
<a href="http://arxiv.org/abs/2011.07682" target="_blank">arXiv:2011.07682</a> [<a href="http://arxiv.org/pdf/2011.07682" target="_blank">pdf</a>]

<h2>Hypergraph Partitioning using Tensor Eigenvalue Decomposition. (arXiv:2011.07683v1 [cs.LG])</h2>
<h3>Deepak Maurya, Balaraman Ravindran</h3>
<p>Hypergraphs have gained increasing attention in the machine learning
community lately due to their superiority over graphs in capturing super-dyadic
interactions among entities. In this work, we propose a novel approach for the
partitioning of k-uniform hypergraphs. Most of the existing methods work by
reducing the hypergraph to a graph followed by applying standard graph
partitioning algorithms. The reduction step restricts the algorithms to
capturing only some weighted pairwise interactions and hence loses essential
information about the original hypergraph. We overcome this issue by utilizing
the tensor-based representation of hypergraphs, which enables us to capture
actual super-dyadic interactions. We prove that the hypergraph to graph
reduction is a special case of tensor contraction. We extend the notion of
minimum ratio-cut and normalized-cut from graphs to hypergraphs and show the
relaxed optimization problem is equivalent to tensor eigenvalue decomposition.
This novel formulation also enables us to capture different ways of cutting a
hyperedge, unlike the existing reduction approaches. We propose a hypergraph
partitioning algorithm inspired from spectral graph theory that can accommodate
this notion of hyperedge cuts. We also derive a tighter upper bound on the
minimum positive eigenvalue of even-order hypergraph Laplacian tensor in terms
of its conductance, which is utilized in the partitioning algorithm to
approximate the normalized cut. The efficacy of the proposed method is
demonstrated numerically on simple hypergraphs. We also show improvement for
the min-cut solution on 2-uniform hypergraphs (graphs) over the standard
spectral partitioning algorithm.
</p>
<a href="http://arxiv.org/abs/2011.07683" target="_blank">arXiv:2011.07683</a> [<a href="http://arxiv.org/pdf/2011.07683" target="_blank">pdf</a>]

<h2>DART: aDaptive Accept RejecT for non-linear top-K subset identification. (arXiv:2011.07687v1 [cs.LG])</h2>
<h3>Mridul Agarwal, Vaneet Aggarwal, Christopher J. Quinn, Abhishek Umrawal</h3>
<p>We consider the bandit problem of selecting $K$ out of $N$ arms at each time
step. The reward can be a non-linear function of the rewards of the selected
individual arms. The direct use of a multi-armed bandit algorithm requires
choosing among $\binom{N}{K}$ options, making the action space large. To
simplify the problem, existing works on combinatorial bandits {typically}
assume feedback as a linear function of individual rewards. In this paper, we
prove the lower bound for top-$K$ subset selection with bandit feedback with
possibly correlated rewards. We present a novel algorithm for the combinatorial
setting without using individual arm feedback or requiring linearity of the
reward function. Additionally, our algorithm works on correlated rewards of
individual arms. Our algorithm, aDaptive Accept RejecT (DART), sequentially
finds good arms and eliminates bad arms based on confidence bounds. DART is
computationally efficient and uses storage linear in $N$. Further, DART
achieves a regret bound of $\tilde{\mathcal{O}}(K\sqrt{KNT})$ for a time
horizon $T$, which matches the lower bound in bandit feedback up to a factor of
$\sqrt{\log{2NT}}$. When applied to the problem of cross-selling optimization
and maximizing the mean of individual rewards, the performance of the proposed
algorithm surpasses that of state-of-the-art algorithms. We also show that DART
significantly outperforms existing methods for both linear and non-linear joint
reward environments.
</p>
<a href="http://arxiv.org/abs/2011.07687" target="_blank">arXiv:2011.07687</a> [<a href="http://arxiv.org/pdf/2011.07687" target="_blank">pdf</a>]

<h2>Drone LAMS: A Drone-based Face Detection Dataset with Large Angles and Many Scenarios. (arXiv:2011.07689v1 [cs.CV])</h2>
<h3>Yi Luo (1), Siyi Chen (2), X.-G. Ma (2) ((1) School of Energy and Environment, Southeast University, Nanjing, China (2) International Institute for Urban Systems Engineering, Southeast University, Nanjing, China)</h3>
<p>This work presented a new drone-based face detection dataset Drone LAMS in
order to solve issues of low performance of drone-based face detection in
scenarios such as large angles which was a predominant working condition when a
drone flies high. The proposed dataset captured images from 261 videos with
over 43k annotations and 4.0k images with pitch or yaw angle in the range of
-90{\deg} to 90{\deg}. Drone LAMS showed significant improvement over currently
available drone-based face detection datasets in terms of detection
performance, especially with large pitch and yaw angle. Detailed analysis of
how key factors, such as duplication rate, annotation method, etc., impact
dataset performance was also provided to facilitate further usage of a drone on
face detection.
</p>
<a href="http://arxiv.org/abs/2011.07689" target="_blank">arXiv:2011.07689</a> [<a href="http://arxiv.org/pdf/2011.07689" target="_blank">pdf</a>]

<h2>Measuring agreement on linguistic expressions in medical treatment scenarios. (arXiv:2011.07693v1 [cs.AI])</h2>
<h3>J Navrro, C Wagner, Uwe Aickelin, L Green, R Ashford</h3>
<p>Quality of life assessment represents a key process of deciding treatment
success and viability. As such, patients' perceptions of their functional
status and well-being are important inputs for impairment assessment. Given
that patient completed questionnaires are often used to assess patient status
and determine future treatment options, it is important to know the level of
agreement of the words used by patients and different groups of medical
professionals. In this paper, we propose a measure called the Agreement Ratio
which provides a ratio of overall agreement when modelling words through Fuzzy
Sets (FSs). The measure has been specifically designed for assessing this
agreement in fuzzy sets which are generated from data such as patient
responses. The measure relies on using the Jaccard Similarity Measure for
comparing the different levels of agreement in the FSs generated.
</p>
<a href="http://arxiv.org/abs/2011.07693" target="_blank">arXiv:2011.07693</a> [<a href="http://arxiv.org/pdf/2011.07693" target="_blank">pdf</a>]

<h2>Ensemble of Models Trained by Key-based Transformed Images for Adversarially Robust Defense Against Black-box Attacks. (arXiv:2011.07697v1 [cs.CV])</h2>
<h3>MaungMaung AprilPyone, Hitoshi Kiya</h3>
<p>We propose a voting ensemble of models trained by using block-wise
transformed images with secret keys for an adversarially robust defense.
Key-based adversarial defenses were demonstrated to outperform state-of-the-art
defenses against gradient-based (white-box) attacks. However, the key-based
defenses are not effective enough against gradient-free (black-box) attacks
without requiring any secret keys. Accordingly, we aim to enhance robustness
against black-box attacks by using a voting ensemble of models. In the proposed
ensemble, a number of models are trained by using images transformed with
different keys and block sizes, and then a voting ensemble is applied to the
models. In image classification experiments, the proposed defense is
demonstrated to defend state-of-the-art attacks. The proposed defense achieves
a clean accuracy of 95.56 % and an attack success rate of less than 9 % under
attacks with a noise distance of 8/255 on the CIFAR-10 dataset.
</p>
<a href="http://arxiv.org/abs/2011.07697" target="_blank">arXiv:2011.07697</a> [<a href="http://arxiv.org/pdf/2011.07697" target="_blank">pdf</a>]

<h2>Efficient falsification approach for autonomous vehicle validation using a parameter optimisation technique based on reinforcement learning. (arXiv:2011.07699v1 [cs.RO])</h2>
<h3>Dhanoop Karunakaran, Stewart Worrall, Eduardo Nebot</h3>
<p>The widescale deployment of Autonomous Vehicles (AV) appears to be imminent
despite many safety challenges that are yet to be resolved. It is well-known
that there are no universally agreed Verification and Validation (VV)
methodologies guarantee absolute safety, which is crucial for the acceptance of
this technology. The uncertainties in the behaviour of the traffic participants
and the dynamic world cause stochastic reactions in advanced autonomous
systems. The addition of ML algorithms and probabilistic techniques adds
significant complexity to the process for real-world testing when compared to
traditional methods. Most research in this area focuses on generating
challenging concrete scenarios or test cases to evaluate the system performance
by looking at the frequency distribution of extracted parameters as collected
from the real-world data. These approaches generally employ Monte-Carlo
simulation and importance sampling to generate critical cases. This paper
presents an efficient falsification method to evaluate the System Under Test.
The approach is based on a parameter optimisation problem to search for
challenging scenarios. The optimisation process aims at finding the challenging
case that has maximum return. The method applies policy-gradient reinforcement
learning algorithm to enable the learning. The riskiness of the scenario is
measured by the well established RSS safety metric, euclidean distance, and
instance of a collision. We demonstrate that by using the proposed method, we
can more efficiently search for challenging scenarios which could cause the
system to fail in order to satisfy the safety requirements.
</p>
<a href="http://arxiv.org/abs/2011.07699" target="_blank">arXiv:2011.07699</a> [<a href="http://arxiv.org/pdf/2011.07699" target="_blank">pdf</a>]

<h2>Multi-view Sensor Fusion by Integrating Model-based Estimation and Graph Learning for Collaborative Object Localization. (arXiv:2011.07704v1 [cs.CV])</h2>
<h3>Peng Gao, Rui Guo, Hongsheng Lu, Hao Zhang</h3>
<p>Collaborative object localization aims to collaboratively estimate locations
of objects observed from multiple views or perspectives, which is a critical
ability for multi-agent systems such as connected vehicles. To enable
collaborative localization, several model-based state estimation and
learning-based localization methods have been developed. Given their
encouraging performance, model-based state estimation often lacks the ability
to model the complex relationships among multiple objects, while learning-based
methods are typically not able to fuse the observations from an arbitrary
number of views and cannot well model uncertainty. In this paper, we introduce
a novel spatiotemporal graph filter approach that integrates graph learning and
model-based estimation to perform multi-view sensor fusion for collaborative
object localization. Our approach models complex object relationships using a
new spatiotemporal graph representation and fuses multi-view observations in a
Bayesian fashion to improve location estimation under uncertainty. We evaluate
our approach in the applications of connected autonomous driving and multiple
pedestrian localization. Experimental results show that our approach
outperforms previous techniques and achieves the state-of-the-art performance
on collaboration localization.
</p>
<a href="http://arxiv.org/abs/2011.07704" target="_blank">arXiv:2011.07704</a> [<a href="http://arxiv.org/pdf/2011.07704" target="_blank">pdf</a>]

<h2>Mode Penalty Generative Adversarial Network with adapted Auto-encoder. (arXiv:2011.07706v1 [cs.LG])</h2>
<h3>Gahye Lee, Seungkyu Lee</h3>
<p>Generative Adversarial Networks (GAN) are trained to generate sample images
of interest distribution. To this end, generator network of GAN learns implicit
distribution of real data set from the classification with candidate generated
samples. Recently, various GANs have suggested novel ideas for stable
optimizing of its networks. However, in real implementation, sometimes they
still represent a only narrow part of true distribution or fail to converge. We
assume this ill posed problem comes from poor gradient from objective function
of discriminator, which easily trap the generator in a bad situation. To
address this problem, we propose a mode penalty GAN combined with pre-trained
auto encoder for explicit representation of generated and real data samples in
the encoded space. In this space, we make a generator manifold to follow a real
manifold by finding entire modes of target distribution. In addition, penalty
for uncovered modes of target distribution is given to the generator which
encourages it to find overall target distribution. We demonstrate that applying
the proposed method to GANs helps generator's optimization becoming more stable
and having faster convergence through experimental evaluations.
</p>
<a href="http://arxiv.org/abs/2011.07706" target="_blank">arXiv:2011.07706</a> [<a href="http://arxiv.org/pdf/2011.07706" target="_blank">pdf</a>]

<h2>DARE: AI-based Diver Action Recognition System using Multi-Channel CNNs for AUV Supervision. (arXiv:2011.07713v1 [cs.CV])</h2>
<h3>Jing Yang, James P. Wilson, Shalabh Gupta</h3>
<p>With the growth of sensing, control and robotic technologies, autonomous
underwater vehicles (AUVs) have become useful assistants to human divers for
performing various underwater operations. In the current practice, the divers
are required to carry expensive, bulky, and waterproof keyboards or
joystick-based controllers for supervision and control of AUVs. Therefore,
diver action-based supervision is becoming increasingly popular because it is
convenient, easier to use, faster, and cost effective. However, the various
environmental, diver and sensing uncertainties present underwater makes it
challenging to train a robust and reliable diver action recognition system. In
this regard, this paper presents DARE, a diver action recognition system, that
is trained based on Cognitive Autonomous Driving Buddy (CADDY) dataset, which
is a rich set of data containing images of different diver gestures and poses
in several different and realistic underwater environments. DARE is based on
fusion of stereo-pairs of camera images using a multi-channel convolutional
neural network supported with a systematically trained tree-topological deep
neural network classifier to enhance the classification performance. DARE is
fast and requires only a few milliseconds to classify one stereo-pair, thus
making it suitable for real-time underwater implementation. DARE is
comparatively evaluated against several existing classifier architectures and
the results show that DARE supersedes the performance of all classifiers for
diver action recognition in terms of overall as well as individual class
accuracies and F1-scores.
</p>
<a href="http://arxiv.org/abs/2011.07713" target="_blank">arXiv:2011.07713</a> [<a href="http://arxiv.org/pdf/2011.07713" target="_blank">pdf</a>]

<h2>Blind Decision Making: Reinforcement Learning with Delayed Observations. (arXiv:2011.07715v1 [cs.LG])</h2>
<h3>Mridul Agarwal, Vaneet Aggarwal</h3>
<p>Reinforcement learning typically assumes that the state update from the
previous actions happens instantaneously, and thus can be used for making
future decisions. However, this may not always be true. When the state update
is not available, the decision taken is partly in the blind since it cannot
rely on the current state information. This paper proposes an approach, where
the delay in the knowledge of the state can be used, and the decisions are made
based on the available information which may not include the current state
information. One approach could be to include the actions after the last-known
state as a part of the state information, however, that leads to an increased
state-space making the problem complex and slower in convergence. The proposed
algorithm gives an alternate approach where the state space is not enlarged, as
compared to the case when there is no delay in the state update. Evaluations on
the basic RL environments further illustrate the improved performance of the
proposed algorithm.
</p>
<a href="http://arxiv.org/abs/2011.07715" target="_blank">arXiv:2011.07715</a> [<a href="http://arxiv.org/pdf/2011.07715" target="_blank">pdf</a>]

<h2>Distributed Bandits: Probabilistic Communication on $d$-regular Graphs. (arXiv:2011.07720v1 [stat.ML])</h2>
<h3>Udari Madhushani, Naomi Ehrich Leonard</h3>
<p>We study the decentralized multi-agent multi-armed bandit problem for agents
that communicate with probability over a network defined by a $d$-regular
graph. Every edge in the graph has probabilistic weight $p$ to account for the
($1\!-\!p$) probability of a communication link failure. At each time step,
each agent chooses an arm and receives a numerical reward associated with the
chosen arm. After each choice, each agent observes the last obtained reward of
each of its neighbors with probability $p$. We propose a new Upper Confidence
Bound (UCB) based algorithm and analyze how agent-based strategies contribute
to minimizing group regret in this probabilistic communication setting. We
provide theoretical guarantees that our algorithm outperforms state-of-the-art
algorithms. We illustrate our results and validate the theoretical claims using
numerical simulations.
</p>
<a href="http://arxiv.org/abs/2011.07720" target="_blank">arXiv:2011.07720</a> [<a href="http://arxiv.org/pdf/2011.07720" target="_blank">pdf</a>]

<h2>TLab: Second Place Solution Towards Traffic4cast 2020 Competition. (arXiv:2011.07728v1 [cs.LG])</h2>
<h3>Fanyou Wu, Yang Liu, Zhiyuan Liu, Xiaobo Qu, Rado Gazo, Eva Haviarova</h3>
<p>The problem of the effective prediction for large-scale spatio-temporal
traffic data has long haunted researchers in the field of intelligent
transportation. Limited by the quantity of data, citywide traffic state
prediction was seldom achieved. Hence the complex urban transportation system
of an entire city cannot be truly understood. Thanks to the efforts of
organizations like IARAI, the massive open data provided by them has made the
research possible. In our 2020 Competition solution, we further design multiple
variants based on HR-NET and UNet. Through feature engineering, the
hand-crafted features are input into the model in a form of channels. It is
worth noting that, to learn the inherent attributes of geographical locations,
we proposed a novel method called geo-embedding, which contributes to
significant improvement in the accuracy of the model. In addition, we explored
the influence of the selection of activation functions and optimizers, as well
as tricks during model training on the model performance. In terms of
prediction accuracy, our solution has won 2nd place in NeurIPS 2020,
Traffic4cast Challenge.
</p>
<a href="http://arxiv.org/abs/2011.07728" target="_blank">arXiv:2011.07728</a> [<a href="http://arxiv.org/pdf/2011.07728" target="_blank">pdf</a>]

<h2>Theoretical Insights Into Multiclass Classification: A High-dimensional Asymptotic View. (arXiv:2011.07729v1 [cs.LG])</h2>
<h3>Christos Thrampoulidis, Samet Oymak, Mahdi Soltanolkotabi</h3>
<p>Contemporary machine learning applications often involve classification tasks
with many classes. Despite their extensive use, a precise understanding of the
statistical properties and behavior of classification algorithms is still
missing, especially in modern regimes where the number of classes is rather
large. In this paper, we take a step in this direction by providing the first
asymptotically precise analysis of linear multiclass classification. Our
theoretical analysis allows us to precisely characterize how the test error
varies over different training algorithms, data distributions, problem
dimensions as well as number of classes, inter/intra class correlations and
class priors. Specifically, our analysis reveals that the classification
accuracy is highly distribution-dependent with different algorithms achieving
optimal performance for different data distributions and/or training/features
sizes. Unlike linear regression/binary classification, the test error in
multiclass classification relies on intricate functions of the trained model
(e.g., correlation between some of the trained weights) whose asymptotic
behavior is difficult to characterize. This challenge is already present in
simple classifiers, such as those minimizing a square loss. Our novel
theoretical techniques allow us to overcome some of these challenges. The
insights gained may pave the way for a precise understanding of other
classification algorithms beyond those studied in this paper.
</p>
<a href="http://arxiv.org/abs/2011.07729" target="_blank">arXiv:2011.07729</a> [<a href="http://arxiv.org/pdf/2011.07729" target="_blank">pdf</a>]

<h2>Gram Regularization for Multi-view 3D Shape Retrieval. (arXiv:2011.07733v1 [cs.CV])</h2>
<h3>Zhaoqun Li</h3>
<p>How to obtain the desirable representation of a 3D shape is a key challenge
in 3D shape retrieval task. Most existing 3D shape retrieval methods focus on
capturing shape representation with different neural network architectures,
while the learning ability of each layer in the network is neglected. A common
and tough issue that limits the capacity of the network is overfitting. To
tackle this, L2 regularization is applied widely in existing deep learning
frameworks. However,the effect on the generalization ability with L2
regularization is limited as it only controls large value in parameters. To
make up the gap, in this paper, we propose a novel regularization term called
Gram regularization which reinforces the learning ability of the network by
encouraging the weight kernels to extract different information on the
corresponding feature map. By forcing the variance between weight kernels to be
large, the regularizer can help to extract discriminative features. The
proposed Gram regularization is data independent and can converge stably and
quickly without bells and whistles. Moreover, it can be easily plugged into
existing off-the-shelf architectures. Extensive experimental results on the
popular 3D object retrieval benchmark ModelNet demonstrate the effectiveness of
our method.
</p>
<a href="http://arxiv.org/abs/2011.07733" target="_blank">arXiv:2011.07733</a> [<a href="http://arxiv.org/pdf/2011.07733" target="_blank">pdf</a>]

<h2>iPerceive: Applying Common-Sense Reasoning to Multi-Modal Dense Video Captioning and Video Question Answering. (arXiv:2011.07735v1 [cs.CV])</h2>
<h3>Aman Chadha, Gurneet Arora, Navpreet Kaloty</h3>
<p>Most prior art in visual understanding relies solely on analyzing the "what"
(e.g., event recognition) and "where" (e.g., event localization), which in some
cases, fails to describe correct contextual relationships between events or
leads to incorrect underlying visual attention. Part of what defines us as
human and fundamentally different from machines is our instinct to seek
causality behind any association, say an event Y that happened as a direct
result of event X. To this end, we propose iPerceive, a framework capable of
understanding the "why" between events in a video by building a common-sense
knowledge base using contextual cues to infer causal relationships between
objects in the video. We demonstrate the effectiveness of our technique using
the dense video captioning (DVC) and video question answering (VideoQA) tasks.
Furthermore, while most prior work in DVC and VideoQA relies solely on visual
information, other modalities such as audio and speech are vital for a human
observer's perception of an environment. We formulate DVC and VideoQA tasks as
machine translation problems that utilize multiple modalities. By evaluating
the performance of iPerceive DVC and iPerceive VideoQA on the ActivityNet
Captions and TVQA datasets respectively, we show that our approach furthers the
state-of-the-art. Code and samples are available at: iperceive.amanchadha.com.
</p>
<a href="http://arxiv.org/abs/2011.07735" target="_blank">arXiv:2011.07735</a> [<a href="http://arxiv.org/pdf/2011.07735" target="_blank">pdf</a>]

<h2>Reward Biased Maximum Likelihood Estimation for Reinforcement Learning. (arXiv:2011.07738v1 [cs.LG])</h2>
<h3>Akshay Mete, Rahul Singh, P.R. Kumar</h3>
<p>The principle of Reward-Biased Maximum Likelihood Estimate Based Adaptive
Control (RBMLE) that was proposed in Kumar and Becker (1982) is an alternative
approach to the Upper Confidence Bound Based (UCB) Approach (Lai and Robbins,
1985) for employing the principle now known as "optimism in the face of
uncertainty" (Auer et al., 2002). It utilizes a modified maximum likelihood
estimate, with a bias towards those Markov Decision Process (MDP) models that
yield a higher average reward. However, its regret performance has never been
analyzed earlier for reinforcement learning (RL (Sutton et al., 1998)) tasks
that involve the optimal control of unknown MDPs. We show that it has a
learning regret of $O(\log T )$ where $T$ is the time-horizon, similar to the
state-of-art algorithms. It provides an alternative general purpose method for
solving RL problems.
</p>
<a href="http://arxiv.org/abs/2011.07738" target="_blank">arXiv:2011.07738</a> [<a href="http://arxiv.org/pdf/2011.07738" target="_blank">pdf</a>]

<h2>Application of Computer Vision Techniques for Segregation of PlasticWaste based on Resin Identification Code. (arXiv:2011.07747v1 [cs.CV])</h2>
<h3>Shivaank Agarwal, Ravindra Gudi, Paresh Saxena</h3>
<p>This paper presents methods to identify the plastic waste based on its resin
identification code to provide an efficient recycling of post-consumer plastic
waste. We propose the design, training and testing of different machine
learning techniques to (i) identify a plastic waste that belongs to the known
categories of plastic waste when the system is trained and (ii) identify a new
plastic waste that do not belong the any known categories of plastic waste
while the system is trained. For the first case,we propose the use of one-shot
learning techniques using Siamese and Triplet loss networks. Our proposed
approach does not require any augmentation to increase the size of the database
and achieved a high accuracy of 99.74%. For the second case, we propose the use
of supervised and unsupervised dimensionality reduction techniques and achieved
an accuracy of 95% to correctly identify a new plastic waste.
</p>
<a href="http://arxiv.org/abs/2011.07747" target="_blank">arXiv:2011.07747</a> [<a href="http://arxiv.org/pdf/2011.07747" target="_blank">pdf</a>]

<h2>Fast Uncertainty Quantification for Deep Object Pose Estimation. (arXiv:2011.07748v1 [cs.RO])</h2>
<h3>Guanya Shi, Yifeng Zhu, Jonathan Tremblay, Stan Birchfield, Fabio Ramos, Animashree Anandkumar, Yuke Zhu</h3>
<p>Deep learning-based object pose estimators are often unreliable and
overconfident especially when the input image is outside the training domain,
for instance, with sim2real transfer. Efficient and robust uncertainty
quantification (UQ) in pose estimators is critically needed in many robotic
tasks. In this work, we propose a simple, efficient, and plug-and-play UQ
method for 6-DoF object pose estimation. We ensemble 2-3 pre-trained models
with different neural network architectures and/or training data sources, and
compute their average pairwise disagreement against one another to obtain the
uncertainty quantification. We propose four disagreement metrics, including a
learned metric, and show that the average distance (ADD) is the best
learning-free metric and it is only slightly worse than the learned metric,
which requires labeled target data. Our method has several advantages compared
to the prior art: 1) our method does not require any modification of the
training process or the model inputs; and 2) it needs only one forward pass for
each model. We evaluate the proposed UQ method on three tasks where our
uncertainty quantification yields much stronger correlations with pose
estimation errors than the baselines. Moreover, in a real robot grasping task,
our method increases the grasping success rate from 35% to 90%.
</p>
<a href="http://arxiv.org/abs/2011.07748" target="_blank">arXiv:2011.07748</a> [<a href="http://arxiv.org/pdf/2011.07748" target="_blank">pdf</a>]

<h2>Online Monitoring of Object Detection Performance Post-Deployment. (arXiv:2011.07750v1 [cs.CV])</h2>
<h3>Quazi Marufur Rahman, Niko S&#xfc;nderhauf, Feras Dayoub</h3>
<p>Post-deployment, an object detector is expected to operate at a similar level
of performance that was reported on its testing dataset. However, when deployed
onboard mobile robots that operate under varying and complex environmental
conditions, the detector's performance can fluctuate and occasionally degrade
severely without warning. Undetected, this can lead the robot to take unsafe
and risky actions based on low-quality and unreliable object detections. We
address this problem and introduce a cascaded neural network that monitors the
performance of the object detector by predicting the quality of its mean
average precision (mAP) on a sliding window of the input frames. The proposed
cascaded network exploits the internal features from the deep neural network of
the object detector. We evaluate our proposed approach using different
combinations of autonomous driving datasets and object detectors.
</p>
<a href="http://arxiv.org/abs/2011.07750" target="_blank">arXiv:2011.07750</a> [<a href="http://arxiv.org/pdf/2011.07750" target="_blank">pdf</a>]

<h2>Tucker decomposition-based Temporal Knowledge Graph Completion. (arXiv:2011.07751v1 [cs.AI])</h2>
<h3>Pengpeng Shao, Guohua Yang, Dawei Zhang, Jianhua Tao, Feihu Che, Tong Liu</h3>
<p>Knowledge graphs have been demonstrated to be an effective tool for numerous
intelligent applications. However, a large amount of valuable knowledge still
exists implicitly in the knowledge graphs. To enrich the existing knowledge
graphs, recent years witness that many algorithms for link prediction and
knowledge graphs embedding have been designed to infer new facts. But most of
these studies focus on the static knowledge graphs and ignore the temporal
information that reflects the validity of knowledge. Developing the model for
temporal knowledge graphs completion is an increasingly important task. In this
paper, we build a new tensor decomposition model for temporal knowledge graphs
completion inspired by the Tucker decomposition of order 4 tensor. We
demonstrate that the proposed model is fully expressive and report
state-of-the-art results for several public benchmarks. Additionally, we
present several regularization schemes to improve the strategy and study their
impact on the proposed model. Experimental studies on three temporal datasets
(i.e. ICEWS2014, ICEWS2005-15, GDELT) justify our design and demonstrate that
our model outperforms baselines with an explicit margin on link prediction
task.
</p>
<a href="http://arxiv.org/abs/2011.07751" target="_blank">arXiv:2011.07751</a> [<a href="http://arxiv.org/pdf/2011.07751" target="_blank">pdf</a>]

<h2>Zero Cost Improvements for General Object Detection Network. (arXiv:2011.07756v1 [cs.CV])</h2>
<h3>Shaohua Wang, Yaping Dai</h3>
<p>Modern object detection networks pursuit higher precision on general object
detection datasets, at the same time the computation burden is also increasing
along with the improvement of precision. Nevertheless, the inference time and
precision are both critical to object detection system which needs to be
real-time. It is necessary to research precision improvement without extra
computation cost. In this work, two modules are proposed to improve detection
precision with zero cost, which are focus on FPN and detection head improvement
for general object detection networks. We employ the scale attention mechanism
to efficiently fuse multi-level feature maps with less parameters, which is
called SA-FPN module. Considering the correlation of classification head and
regression head, we use sequential head to take the place of widely-used
parallel head, which is called Seq-HEAD module. To evaluate the effectiveness,
we apply the two modules to some modern state-of-art object detection networks,
including anchor-based and anchor-free. Experiment results on coco dataset show
that the networks with the two modules can surpass original networks by 1.1 AP
and 0.8 AP with zero cost for anchor-based and anchor-free networks,
respectively. Code will be available at https://git.io/JTFGl.
</p>
<a href="http://arxiv.org/abs/2011.07756" target="_blank">arXiv:2011.07756</a> [<a href="http://arxiv.org/pdf/2011.07756" target="_blank">pdf</a>]

<h2>Time-Efficient Mars Exploration of Simultaneous Coverage and Charging with Multiple Drones. (arXiv:2011.07759v1 [cs.RO])</h2>
<h3>Yuan Chang, Chao Yan, Xingyu Liu, Xiangke Wang, Han Zhou, Xiaojia Xiang, Dengqing Tang</h3>
<p>This paper presents a time-efficient scheme for Mars exploration by the
cooperation of multiple drones and a rover. To maximize effective coverage of
the Mars surface in the long run, a comprehensive framework has been developed
with joint consideration for limited energy, sensor model, communication range
and safety radius, which we call TIME-SC2 (TIme-efficient Mars Exploration of
Simultaneous Coverage and Charging). First, we propose a multi-drone coverage
control algorithm by leveraging emerging deep reinforcement learning and design
a novel information map to represent dynamic system states. Second, we propose
a near-optimal charging scheduling algorithm to navigate each drone to an
individual charging slot, and we have proven that there always exists feasible
solutions. The attractiveness of this framework not only resides on its ability
to maximize exploration efficiency, but also on its high autonomy that has
greatly reduced the non-exploring time. Extensive simulations have been
conducted to demonstrate the remarkable performance of TIME-SC2 in terms of
time-efficiency, adaptivity and flexibility.
</p>
<a href="http://arxiv.org/abs/2011.07759" target="_blank">arXiv:2011.07759</a> [<a href="http://arxiv.org/pdf/2011.07759" target="_blank">pdf</a>]

<h2>PC-GAIN: Pseudo-label Conditional Generative Adversarial Imputation Networks for Incomplete Data. (arXiv:2011.07770v1 [cs.LG])</h2>
<h3>Yufeng Wang, Dan Li, Xiang Li, Min Yang</h3>
<p>Datasets with missing values are very common in real world applications.
GAIN, a recently proposed deep generative model for missing data imputation,
has been proved to outperform many state-of-the-art methods. But GAIN only uses
a reconstruction loss in the generator to minimize the imputation error of the
non-missing part, ignoring the potential category information which can reflect
the relationship between samples. In this paper, we propose a novel
unsupervised missing data imputation method named PC-GAIN, which utilizes
potential category information to further enhance the imputation power.
Specifically, we first propose a pre-training procedure to learn potential
category information contained in a subset of low-missing-rate data. Then an
auxiliary classifier is determined based on the synthetic pseudo-labels.
Further, this classifier is incorporated into the generative adversarial
framework to help the generator to yield higher quality imputation results. The
proposed method can significantly improve the imputation quality of GAIN.
Experimental results on various benchmark datasets show that our method is also
superior to other baseline models.
</p>
<a href="http://arxiv.org/abs/2011.07770" target="_blank">arXiv:2011.07770</a> [<a href="http://arxiv.org/pdf/2011.07770" target="_blank">pdf</a>]

<h2>Indoor Positioning System based on Visible Light Communication for Mobile Robot in Nuclear Power Plant. (arXiv:2011.07771v1 [cs.RO])</h2>
<h3>Hongyun Xie, Linyi Huang, Wenfei Wu</h3>
<p>Visible light positioning (VLP) is widely believed to be a cost-effective
answer to the growing demanded for robot indoor positioning. Considering that
some extreme environments require robot to be equipped with a precise and
radiation-resistance indoor positioning system for doing difficult work, a
novel VLP system with high accuracy is proposed to realize the long-playing
inspection and intervention under radiation environment. The proposed system
with sufficient radiation-tolerance is critical for operational inspection,
maintenance and intervention tasks in nuclear facilities. Firstly, we designed
intelligent LED lamp with visible light communication (VLC) function to
dynamically create the indoor GPS tracking system. By installing the proposed
lamps that replace standard lighting in key locations in the nuclear power
plant, the proposed system can strengthen the safety of mobile robot and help
for efficient inspection in the large-scale field. Secondly, in order to
enhance the radiation-tolerance and multi-scenario of the proposed system, we
proposed a shielding protection method for the camera vertically installed on
the robot, which ensures that the image elements of the camera namely the
captured VLP information is not affected by radiation. Besides, with the
optimized visible light positioning algorithm based on dispersion calibration
method, the proposed VLP system can achieve an average positioning accuracy of
0.82cm and ensure that 90% positioning errors are less than 1.417cm. Therefore,
the proposed system not only has sufficient radiation-tolerance but achieve
state-of-the-art positioning accuracy in the visible light positioning field.
</p>
<a href="http://arxiv.org/abs/2011.07771" target="_blank">arXiv:2011.07771</a> [<a href="http://arxiv.org/pdf/2011.07771" target="_blank">pdf</a>]

<h2>DSIC: Dynamic Sample-Individualized Connector for Multi-Scale Object Detection. (arXiv:2011.07774v1 [cs.CV])</h2>
<h3>ZekunLi, YufanLiu, BingLi, WeimingHu</h3>
<p>Although object detection has reached a milestone thanks to the great success
of deep learning, the scale variation is still the key challenge. Integrating
multi-level features is presented to alleviate the problems, like the classic
Feature Pyramid Network (FPN) and its improvements. However, the specifically
designed feature integration modules of these methods may not have the optimal
architecture for feature fusion. Moreover, these models have fixed
architectures and data flow paths, when fed with various samples. They cannot
adjust and be compatible with each kind of data. To overcome the above
limitations, we propose a Dynamic Sample-Individualized Connector (DSIC) for
multi-scale object detection. It dynamically adjusts network connections to fit
different samples. In particular, DSIC consists of two components: Intra-scale
Selection Gate (ISG) and Cross-scale Selection Gate (CSG). ISG adaptively
extracts multi-level features from backbone as the input of feature
integration. CSG automatically activate informative data flow paths based on
the multi-level features. Furthermore, these two components are both
plug-and-play and can be embedded in any backbone. Experimental results
demonstrate that the proposed method outperforms the state-of-the-arts.
</p>
<a href="http://arxiv.org/abs/2011.07774" target="_blank">arXiv:2011.07774</a> [<a href="http://arxiv.org/pdf/2011.07774" target="_blank">pdf</a>]

<h2>A metric for sets of trajectories that is practical and mathematically consistent. (arXiv:1601.03094v3 [cs.CV] UPDATED)</h2>
<h3>Jos&#xe9; Bento, Jia Jie Zhu</h3>
<p>Metrics on the space of sets of trajectories are important for scientists in
the field of computer vision, machine learning, robotics, and general
artificial intelligence. However, existing notions of closeness between sets of
trajectories are either mathematically inconsistent or of limited practical
use. In this paper, we outline the limitations in the current
mathematically-consistent metrics, which are based on OSPA (Schuhmacher et al.
2008); and the inconsistencies in the heuristic notions of closeness used in
practice, whose main ideas are common to the CLEAR MOT measures (Keni and
Rainer 2008) widely used in computer vision. In two steps, we then propose a
new intuitive metric between sets of trajectories and address these
limitations. First, we explain a solution that leads to a metric that is hard
to compute. Then we modify this formulation to obtain a metric that is easy to
compute while keeping the useful properties of the previous metric. Our notion
of closeness is the first demonstrating the following three features: the
metric 1) can be quickly computed, 2) incorporates confusion of trajectories'
identity in an optimal way, and 3) is a metric in the mathematical sense.
</p>
<a href="http://arxiv.org/abs/1601.03094" target="_blank">arXiv:1601.03094</a> [<a href="http://arxiv.org/pdf/1601.03094" target="_blank">pdf</a>]

<h2>Discussion among Different Methods of Updating Model Filter in Object Tracking. (arXiv:1711.07829v3 [cs.CV] UPDATED)</h2>
<h3>Taihang Dong, Sheng Zhong</h3>
<p>Discriminative correlation filters (DCF) have recently shown excellent
performance in visual object tracking area. In this paper, we summarize the
methods of updating model filter from discriminative correlation filter (DCF)
based tracking algorithms and analyzes similarities and differences among these
methods. We deduce the relationship between updating coefficient in high
dimension (kernel trick), updating filter in frequency domain and updating
filter in spatial domain, and analyze the difference among these different
ways. We also analyze the difference between the updating filter directly and
updating filter's numerator (object response power) with updating filter's
denominator (filter's power). The experiments about comparing different
updating methods and visualizing the template filters are used to prove our
derivation.
</p>
<a href="http://arxiv.org/abs/1711.07829" target="_blank">arXiv:1711.07829</a> [<a href="http://arxiv.org/pdf/1711.07829" target="_blank">pdf</a>]

<h2>Robust Object Tracking Based on Self-adaptive Search Area. (arXiv:1711.07835v3 [cs.CV] UPDATED)</h2>
<h3>Taihang Dong, Sheng Zhong</h3>
<p>Discriminative correlation filter (DCF) based trackers have recently achieved
excellent performance with great computational efficiency. However, DCF based
trackers suffer boundary effects, which result in unstable performance in
challenging situations exhibiting fast motion. In this paper, we propose a
novel method to mitigate this side-effect in DCF based trackers. We change the
search area according to the prediction of target motion. When the object moves
fast, broad search area could alleviate boundary effects and reserve the
probability of locating the object. When the object moves slowly, narrow search
area could prevent effect of useless background information and improve
computational efficiency to attain real-time performance. This strategy can
impressively soothe boundary effects in situations exhibiting fast motion and
motion blur, and it can be used in almost all DCF based trackers. The
experiments on OTB benchmark show that the proposed framework improves the
performance compared with the baseline trackers.
</p>
<a href="http://arxiv.org/abs/1711.07835" target="_blank">arXiv:1711.07835</a> [<a href="http://arxiv.org/pdf/1711.07835" target="_blank">pdf</a>]

<h2>Online Label Aggregation: A Variational Bayesian Approach. (arXiv:1807.07291v2 [cs.LG] UPDATED)</h2>
<h3>Chi Hong, Amirmasoud Ghiassi, Yichi Zhou, Robert Birke, Lydia Y. Chen</h3>
<p>Noisy labeled data is more a norm than a rarity for crowd sourced contents.
It is effective to distill noise and infer correct labels through aggregation
results from crowd workers. To ensure the time relevance and overcome slow
responses of workers, online label aggregation is increasingly requested,
calling for solutions that can incrementally infer true label distribution via
subsets of data items. In this paper, we propose a novel online label
aggregation framework, BiLA, which employs variational Bayesian inference
method and designs a novel stochastic optimization scheme for incremental
training. BiLA is flexible to accommodate any generating distribution of labels
by the exact computation of its posterior distribution. We also derive the
convergence bound of the proposed optimizer. We compare BiLA with the state of
the art based on minimax entropy, neural networks and expectation maximization
algorithms, on synthetic and real-world data sets. Our evaluation results on
various online scenarios show that BiLA can effectively infer the true labels,
with an error rate reduction of at least 10 to 1.5 percent points for synthetic
and real-world datasets, respectively.
</p>
<a href="http://arxiv.org/abs/1807.07291" target="_blank">arXiv:1807.07291</a> [<a href="http://arxiv.org/pdf/1807.07291" target="_blank">pdf</a>]

<h2>From exploration to control: learning object manipulation skills through novelty search and local adaptation. (arXiv:1901.00811v2 [cs.RO] UPDATED)</h2>
<h3>Seungsu Kim, Alexandre Coninx, Stephane Doncieux</h3>
<p>Programming a robot to deal with open-ended tasks remains a challenge, in
particular if the robot has to manipulate objects. Launching, grasping, pushing
or any other object interaction can be simulated but the corresponding models
are not reversible and the robot behavior thus cannot be directly deduced.
These behaviors are hard to learn without a demonstration as the search space
is large and the reward sparse. We propose a method to autonomously generate a
diverse repertoire of simple object interaction behaviors in simulation. Our
goal is to bootstrap a robot learning and development process with limited
information about what the robot has to achieve and how. This repertoire can be
exploited to solve different tasks in reality thanks to a proposed adaptation
method or could be used as a training set for data-hungry algorithms.

The proposed approach relies on the definition of a goal space and generates
a repertoire of trajectories to reach attainable goals, thus allowing the robot
to control this goal space. The repertoire is built with an off-the-shelf
simulation thanks to a quality diversity algorithm. The result is a set of
solutions tested in simulation only. It may result in two different problems:
(1) as the repertoire is discrete and finite, it may not contain the trajectory
to deal with a given situation or (2) some trajectories may lead to a behavior
in reality that differs from simulation because of a reality gap. We propose an
approach to deal with both issues by using a local linearization of the mapping
between the motion parameters and the observed effects. Furthermore, we present
an approach to update the existing solutions repertoire with the tests done on
the real robot. The approach has been validated on two different experiments on
the Baxter robot: a ball launching and a joystick manipulation tasks.
</p>
<a href="http://arxiv.org/abs/1901.00811" target="_blank">arXiv:1901.00811</a> [<a href="http://arxiv.org/pdf/1901.00811" target="_blank">pdf</a>]

<h2>Object Detection based on Region Decomposition and Assembly. (arXiv:1901.08225v2 [cs.CV] UPDATED)</h2>
<h3>Seung-Hwan Bae</h3>
<p>Region-based object detection infers object regions for one or more
categories in an image. Due to the recent advances in deep learning and region
proposal methods, object detectors based on convolutional neural networks
(CNNs) have been flourishing and provided the promising detection results.
However, the detection accuracy is degraded often because of the low
discriminability of object CNN features caused by occlusions and inaccurate
region proposals. In this paper, we therefore propose a region decomposition
and assembly detector (R-DAD) for more accurate object detection.

In the proposed R-DAD, we first decompose an object region into multiple
small regions. To capture an entire appearance and part details of the object
jointly, we extract CNN features within the whole object region and decomposed
regions. We then learn the semantic relations between the object and its parts
by combining the multi-region features stage by stage with region assembly
blocks, and use the combined and high-level semantic features for the object
classification and localization. In addition, for more accurate region
proposals, we propose a multi-scale proposal layer that can generate object
proposals of various scales. We integrate the R-DAD into several feature
extractors, and prove the distinct performance improvement on PASCAL07/12 and
MSCOCO18 compared to the recent convolutional detectors.
</p>
<a href="http://arxiv.org/abs/1901.08225" target="_blank">arXiv:1901.08225</a> [<a href="http://arxiv.org/pdf/1901.08225" target="_blank">pdf</a>]

<h2>Neural Networks for Lorenz Map Prediction: A Trip Through Time. (arXiv:1903.07768v5 [cs.LG] UPDATED)</h2>
<h3>Denisa Roberts</h3>
<p>In this article the Lorenz dynamical system is revived and revisited and the
current state of the art results for one step ahead forecasting for the Lorenz
trajectories are published. Multitask learning is shown to help learning the
hard to learn z trajectory. The article is a reflection upon the evolution of
neural networks with respect to the prediction performance on this canonical
task.
</p>
<a href="http://arxiv.org/abs/1903.07768" target="_blank">arXiv:1903.07768</a> [<a href="http://arxiv.org/pdf/1903.07768" target="_blank">pdf</a>]

<h2>Blur Removal via Blurred-Noisy Image Pair. (arXiv:1903.10667v4 [cs.CV] UPDATED)</h2>
<h3>Chunzhi Gu, Xuequan Lu, Ying He, Chao Zhang</h3>
<p>Complex blur such as the mixup of space-variant and space-invariant blur,
which is hard to model mathematically, widely exists in real images. In this
paper, we propose a novel image deblurring method that does not need to
estimate blur kernels. We utilize a pair of images that can be easily acquired
in low-light situations: (1) a blurred image taken with low shutter speed and
low ISO noise; and (2) a noisy image captured with high shutter speed and high
ISO noise. Slicing the blurred image into patches, we extend the Gaussian
mixture model (GMM) to model the underlying intensity distribution of each
patch using the corresponding patches in the noisy image. We compute patch
correspondences by analyzing the optical flow between the two images. The
Expectation Maximization (EM) algorithm is utilized to estimate the parameters
of GMM. To preserve sharp features, we add an additional bilateral term to the
objective function in the M-step. We eventually add a detail layer to the
deblurred image for refinement. Extensive experiments on both synthetic and
real-world data demonstrate that our method outperforms state-of-the-art
techniques, in terms of robustness, visual quality, and quantitative metrics.
</p>
<a href="http://arxiv.org/abs/1903.10667" target="_blank">arXiv:1903.10667</a> [<a href="http://arxiv.org/pdf/1903.10667" target="_blank">pdf</a>]

<h2>IMAE for Noise-Robust Learning: Mean Absolute Error Does Not Treat Examples Equally and Gradient Magnitude's Variance Matters. (arXiv:1903.12141v9 [cs.LG] UPDATED)</h2>
<h3>Xinshao Wang, Yang Hua, Elyor Kodirov, Neil M. Robertson</h3>
<p>In this work, we study robust deep learning against abnormal training data
from the perspective of example weighting built in empirical loss functions,
i.e., gradient magnitude with respect to logits, an angle that is not
thoroughly studied so far. Consequently, we have two key findings: (1) Mean
Absolute Error (MAE) Does Not Treat Examples Equally. We present new
observations and insightful analysis about MAE, which is theoretically proved
to be noise-robust. First, we reveal its underfitting problem in practice.
Second, we analyse that MAE's noise-robustness is from emphasising on uncertain
examples instead of treating training samples equally, as claimed in prior
work. (2) The Variance of Gradient Magnitude Matters. We propose an effective
and simple solution to enhance MAE's fitting ability while preserving its
noise-robustness. Without changing MAE's overall weighting scheme, i.e., what
examples get higher weights, we simply change its weighting variance
non-linearly so that the impact ratio between two examples are adjusted. Our
solution is termed Improved MAE (IMAE). We prove IMAE's effectiveness using
extensive experiments: image classification under clean labels, synthetic label
noise, and real-world unknown noise. We conclude IMAE is superior to CCE, the
most popular loss for training DNNs.
</p>
<a href="http://arxiv.org/abs/1903.12141" target="_blank">arXiv:1903.12141</a> [<a href="http://arxiv.org/pdf/1903.12141" target="_blank">pdf</a>]

<h2>Disagreement-based Active Learning in Online Settings. (arXiv:1904.09056v5 [cs.LG] UPDATED)</h2>
<h3>Boshuang Huang, Sudeep Salgia, Qing Zhao</h3>
<p>We study online active learning for classifying streaming instances within
the framework of statistical learning theory. At each time, the learner either
queries the label of the current instance or predicts the label based on past
seen examples. The objective is to minimize the number of queries while
constraining the number of prediction errors over a horizon of length $T$. We
develop a disagreement-based online learning algorithm for a general hypothesis
space and under the Tsybakov noise. We show that the proposed algorithm has a
label complexity of $O(dT^{\frac{2-2\alpha}{2-\alpha}}\log^2 T)$ under a
constraint of bounded regret in terms of classification errors, where $d$ is
the VC dimension of the hypothesis space and $\alpha$ is the Tsybakov noise
parameter. We further establish a matching (up to a poly-logarithmic factor)
lower bound, demonstrating the order optimality of the proposed algorithm. We
address the tradeoff between label complexity and regret and show that the
algorithm can be modified to operate at a different point on the tradeoff
curve.
</p>
<a href="http://arxiv.org/abs/1904.09056" target="_blank">arXiv:1904.09056</a> [<a href="http://arxiv.org/pdf/1904.09056" target="_blank">pdf</a>]

<h2>Understanding Generalization through Visualizations. (arXiv:1906.03291v6 [cs.LG] UPDATED)</h2>
<h3>W. Ronny Huang, Zeyad Emam, Micah Goldblum, Liam Fowl, Justin K. Terry, Furong Huang, Tom Goldstein</h3>
<p>The power of neural networks lies in their ability to generalize to unseen
data, yet the underlying reasons for this phenomenon remain elusive. Numerous
rigorous attempts have been made to explain generalization, but available
bounds are still quite loose, and analysis does not always lead to true
understanding. The goal of this work is to make generalization more intuitive.
Using visualization methods, we discuss the mystery of generalization, the
geometry of loss landscapes, and how the curse (or, rather, the blessing) of
dimensionality causes optimizers to settle into minima that generalize well.
</p>
<a href="http://arxiv.org/abs/1906.03291" target="_blank">arXiv:1906.03291</a> [<a href="http://arxiv.org/pdf/1906.03291" target="_blank">pdf</a>]

<h2>Adaptive Sequential Experiments with Unknown Information Arrival Processes. (arXiv:1907.00107v5 [cs.LG] UPDATED)</h2>
<h3>Yonatan Gur, Ahmadreza Momeni</h3>
<p>Sequential experiments are often characterized by an exploration-exploitation
tradeoff that is captured by the multi-armed bandit (MAB) framework. This
framework has been studied and applied, typically when at each time epoch
feedback is received only on the action that was selected at that epoch.
However, in many practical settings additional information may become available
between decision steps. We introduce a generalized MAB formulation, which
considers a broad class of distributions that are informative about mean
rewards, and allows observations from these distributions to arrive at
arbitrary and a priori unknown times. We characterize the minimax complexity of
this family of problems as a function of the information arrival process, and
identify how salient characteristics of this process impact policy design and
achievable performance. We establish that: (i) upper confidence bound and
posterior sampling policies possess natural robustness with respect to the
information arrival process without any adjustments, which uncovers a novel
property of these popular families of policies and further lends credence to
their appeal; and (ii) policies with exogenous exploration rate do not possess
such robustness. For such policies, we devise a novel virtual time indices
method for dynamically controlling the effective exploration rate to attain the
best performance that is achievable when the information arrival process is a
priori known. When the relation of auxiliary data to rewards is unknown, we
characterize necessary and sufficient conditions under which auxiliary
information still allows performance improvement, and devise new policies based
on upper confidence bound that uniformly guarantee rate optimality. We use data
from a large media site to analyze the value that may be captured in practice
by leveraging auxiliary information for designing content recommendations.
</p>
<a href="http://arxiv.org/abs/1907.00107" target="_blank">arXiv:1907.00107</a> [<a href="http://arxiv.org/pdf/1907.00107" target="_blank">pdf</a>]

<h2>LassoNet: A Neural Network with Feature Sparsity. (arXiv:1907.12207v7 [stat.ML] UPDATED)</h2>
<h3>Ismael Lemhadri, Feng Ruan, Robert Tibshirani</h3>
<p>Much work has been done recently to make neural networks more interpretable,
and one obvious approach is to arrange for the network to use only a subset of
the available features. In linear models, Lasso (or $\ell_1$-regularized)
regression assigns zero weights to the most irrelevant or redundant features,
and is widely used in data science. However the Lasso only applies to linear
models. Here we introduce LassoNet, a neural network framework with global
feature selection. Our approach enforces a hierarchy: specifically a feature
can participate in a hidden unit only if its linear representative is active.
Unlike other approaches to feature selection for neural nets, our method uses a
modified objective function with constraints, and so integrates feature
selection with the parameter learning directly. As a result, it delivers an
entire regularization path of solutions with a range of feature sparsity. On
systematic experiments, LassoNet significantly outperforms state-of-the-art
methods for feature selection and regression. The LassoNet method uses
projected proximal gradient descent, and generalizes directly to deep networks.
It can be implemented by adding just a few lines of code to a standard neural
network.
</p>
<a href="http://arxiv.org/abs/1907.12207" target="_blank">arXiv:1907.12207</a> [<a href="http://arxiv.org/pdf/1907.12207" target="_blank">pdf</a>]

<h2>Driving in Dense Traffic with Model-Free Reinforcement Learning. (arXiv:1909.06710v2 [cs.RO] UPDATED)</h2>
<h3>Dhruv Mauria Saxena, Sangjae Bae, Alireza Nakhaei, Kikuo Fujimura, Maxim Likhachev</h3>
<p>Traditional planning and control methods could fail to find a feasible
trajectory for an autonomous vehicle to execute amongst dense traffic on roads.
This is because the obstacle-free volume in spacetime is very small in these
scenarios for the vehicle to drive through. However, that does not mean the
task is infeasible since human drivers are known to be able to drive amongst
dense traffic by leveraging the cooperativeness of other drivers to open a gap.
The traditional methods fail to take into account the fact that the actions
taken by an agent affect the behaviour of other vehicles on the road. In this
work, we rely on the ability of deep reinforcement learning to implicitly model
such interactions and learn a continuous control policy over the action space
of an autonomous vehicle. The application we consider requires our agent to
negotiate and open a gap in the road in order to successfully merge or change
lanes. Our policy learns to repeatedly probe into the target road lane while
trying to find a safe spot to move in to. We compare against two
model-predictive control-based algorithms and show that our policy outperforms
them in simulation.
</p>
<a href="http://arxiv.org/abs/1909.06710" target="_blank">arXiv:1909.06710</a> [<a href="http://arxiv.org/pdf/1909.06710" target="_blank">pdf</a>]

<h2>Mixed Dimension Embeddings with Application to Memory-Efficient Recommendation Systems. (arXiv:1909.11810v2 [cs.LG] UPDATED)</h2>
<h3>Antonio Ginart, Maxim Naumov, Dheevatsa Mudigere, Jiyan Yang, James Zou</h3>
<p>Embedding representations power machine intelligence in many applications,
including recommendation systems, but they are space intensive -- potentially
occupying hundreds of gigabytes in large-scale settings. To help manage this
outsized memory consumption, we explore mixed dimension embeddings, an
embedding layer architecture in which a particular embedding vector's dimension
scales with its query frequency. Through theoretical analysis and systematic
experiments, we demonstrate that using mixed dimensions can drastically reduce
the memory usage, while maintaining and even improving the ML performance.
Empirically, we show that the proposed mixed dimension layers improve accuracy
by 0.1% using half as many parameters or maintain it using 16X fewer parameters
for click-through rate prediction task on the Criteo Kaggle dataset.
</p>
<a href="http://arxiv.org/abs/1909.11810" target="_blank">arXiv:1909.11810</a> [<a href="http://arxiv.org/pdf/1909.11810" target="_blank">pdf</a>]

<h2>Using GANs for Sharing Networked Time Series Data: Challenges, Initial Promise, and Open Questions. (arXiv:1909.13403v4 [cs.LG] UPDATED)</h2>
<h3>Zinan Lin, Alankar Jain, Chen Wang, Giulia Fanti, Vyas Sekar</h3>
<p>Limited data access is a longstanding barrier to data-driven research and
development in the networked systems community. In this work, we explore if and
how generative adversarial networks (GANs) can be used to incentivize data
sharing by enabling a generic framework for sharing synthetic datasets with
minimal expert knowledge. As a specific target, our focus in this paper is on
time series datasets with metadata (e.g., packet loss rate measurements with
corresponding ISPs). We identify key challenges of existing GAN approaches for
such workloads with respect to fidelity (e.g., long-term dependencies, complex
multidimensional relationships, mode collapse) and privacy (i.e., existing
guarantees are poorly understood and can sacrifice fidelity). To improve
fidelity, we design a custom workflow called DoppelGANger (DG) and demonstrate
that across diverse real-world datasets (e.g., bandwidth measurements, cluster
requests, web sessions) and use cases (e.g., structural characterization,
predictive modeling, algorithm comparison), DG achieves up to 43% better
fidelity than baseline models. Although we do not resolve the privacy problem
in this work, we identify fundamental challenges with both classical notions of
privacy and recent advances to improve the privacy properties of GANs, and
suggest a potential roadmap for addressing these challenges. By shedding light
on the promise and challenges, we hope our work can rekindle the conversation
on workflows for data sharing.
</p>
<a href="http://arxiv.org/abs/1909.13403" target="_blank">arXiv:1909.13403</a> [<a href="http://arxiv.org/pdf/1909.13403" target="_blank">pdf</a>]

<h2>Sub-Optimal Local Minima Exist for Neural Networks with Almost All Non-Linear Activations. (arXiv:1911.01413v3 [cs.LG] UPDATED)</h2>
<h3>Tian Ding, Dawei Li, Ruoyu Sun</h3>
<p>Does over-parameterization eliminate sub-optimal local minima for neural
networks? An affirmative answer was given by a classical result in [59] for
1-hidden-layer wide neural networks. A few recent works have extended the
setting to multi-layer neural networks, but none of them has proved every local
minimum is global. Why is this result never extended to deep networks?

In this paper, we show that the task is impossible because the original
result for 1-hidden-layer network in [59] can not hold. More specifically, we
prove that for any multi-layer network with generic input data and non-linear
activation functions, sub-optimal local minima can exist, no matter how wide
the network is (as long as the last hidden layer has at least two neurons).
While the result of [59] assumes sigmoid activation, our counter-example covers
a large set of activation functions (dense in the set of continuous functions),
indicating that the limitation is not due to the specific activation. Our
result indicates that "no bad local-min" may be unable to explain the benefit
of over-parameterization for training neural nets.
</p>
<a href="http://arxiv.org/abs/1911.01413" target="_blank">arXiv:1911.01413</a> [<a href="http://arxiv.org/pdf/1911.01413" target="_blank">pdf</a>]

<h2>Deep Template-based Object Instance Detection. (arXiv:1911.11822v3 [cs.CV] UPDATED)</h2>
<h3>Jean-Philippe Mercier, Mathieu Garon, Philippe Gigu&#xe8;re, Jean-Fran&#xe7;ois Lalonde</h3>
<p>Much of the focus in the object detection literature has been on the problem
of identifying the bounding box of a particular class of object in an image.
Yet, in contexts such as robotics and augmented reality, it is often necessary
to find a specific object instance---a unique toy or a custom industrial part
for example---rather than a generic object class. Here, applications can
require a rapid shift from one object instance to another, thus requiring fast
turnaround which affords little-to-no training time. What is more, gathering a
dataset and training a model for every new object instance to be detected can
be an expensive and time-consuming process. In this context, we propose a
generic 2D object instance detection approach that uses example viewpoints of
the target object at test time to retrieve its 2D location in RGB images,
without requiring any additional training (i.e. fine-tuning) step. To this end,
we present an end-to-end architecture that extracts global and local
information of the object from its viewpoints. The global information is used
to tune early filters in the backbone while local viewpoints are correlated
with the input image. Our method offers an improvement of almost 30 mAP over
the previous template matching methods on the challenging Occluded Linemod
dataset (overall mAP of 50.7). Our experiments also show that our single
generic model (not trained on any of the test objects) yields detection results
that are on par with approaches that are trained specifically on the target
objects.
</p>
<a href="http://arxiv.org/abs/1911.11822" target="_blank">arXiv:1911.11822</a> [<a href="http://arxiv.org/pdf/1911.11822" target="_blank">pdf</a>]

<h2>Let's Get Dirty: GAN Based Data Augmentation for Camera Lens Soiling Detection in Autonomous Driving. (arXiv:1912.02249v3 [cs.CV] UPDATED)</h2>
<h3>Michal Uricar, Ganesh Sistu, Hazem Rashed, Antonin Vobecky, Varun Ravi Kumar, Pavel Krizek, Fabian Burger, Senthil Yogamani</h3>
<p>Wide-angle fisheye cameras are commonly used in automated driving for parking
and low-speed navigation tasks. Four of such cameras form a surround-view
system that provides a complete and detailed view of the vehicle. These cameras
are directly exposed to harsh environmental settings and can get soiled very
easily by mud, dust, water, frost. Soiling on the camera lens can severely
degrade the visual perception algorithms, and a camera cleaning system
triggered by a soiling detection algorithm is increasingly being deployed.
While adverse weather conditions, such as rain, are getting attention recently,
there is only limited work on general soiling. The main reason is the
difficulty in collecting a diverse dataset as it is a relatively rare event. We
propose a novel GAN based algorithm for generating unseen patterns of soiled
images. Additionally, the proposed method automatically provides the
corresponding soiling masks eliminating the manual annotation cost.
Augmentation of the generated soiled images for training improves the accuracy
of soiling detection tasks significantly by 18% demonstrating its usefulness.
The manually annotated soiling dataset and the generated augmentation dataset
will be made public. We demonstrate the generalization of our fisheye trained
GAN model on the Cityscapes dataset. We provide an empirical evaluation of the
degradation of the semantic segmentation algorithm with the soiled data.
</p>
<a href="http://arxiv.org/abs/1912.02249" target="_blank">arXiv:1912.02249</a> [<a href="http://arxiv.org/pdf/1912.02249" target="_blank">pdf</a>]

<h2>Grounding-Tracking-Integration. (arXiv:1912.06316v2 [cs.CV] UPDATED)</h2>
<h3>Zhengyuan Yang, Tushar Kumar, Tianlang Chen, Jinsong Su, Jiebo Luo</h3>
<p>In this paper, we study Tracking by Language that localizes the target box
sequence in a video based on a language query. We propose a framework called
GTI that decomposes the problem into three sub-tasks: Grounding, Tracking, and
Integration. The three sub-task modules operate simultaneously and predict the
box sequence frame-by-frame. "Grounding" predicts the referred region directly
from the language query. "Tracking" localizes the target based on the history
of the grounded regions in previous frames. "Integration" generates final
predictions by synergistically combining grounding and tracking. With the
"integration" task as the key, we explore how to indicate the quality of the
grounded regions in each frame and achieve the desired mutually beneficial
combination. To this end, we propose an "RT-integration" method that defines
and predicts two scores to guide the integration: 1) R-score represents the
Region correctness whether the grounding prediction accurately covers the
target, and 2) T-score represents the Template quality whether the region
provides informative visual cues to improve tracking in future frames. We
present our real-time GTI implementation with the proposed RT-integration, and
benchmark the framework on LaSOT and Lingual OTB99 with highly promising
results. Moreover, we produce a disambiguated version of LaSOT queries to
facilitate future tracking by language studies.
</p>
<a href="http://arxiv.org/abs/1912.06316" target="_blank">arXiv:1912.06316</a> [<a href="http://arxiv.org/pdf/1912.06316" target="_blank">pdf</a>]

<h2>Adversarial symmetric GANs: bridging adversarial samples and adversarial networks. (arXiv:1912.09670v5 [cs.CV] UPDATED)</h2>
<h3>Faqiang Liu, Mingkun Xu, Guoqi Li, Jing Pei, Luping Shi, Rong Zhao</h3>
<p>Generative adversarial networks have achieved remarkable performance on
various tasks but suffer from training instability. Despite many training
strategies proposed to improve training stability, this issue remains as a
challenge. In this paper, we investigate the training instability from the
perspective of adversarial samples and reveal that adversarial training on fake
samples is implemented in vanilla GANs, but adversarial training on real
samples has long been overlooked. Consequently, the discriminator is extremely
vulnerable to adversarial perturbation and the gradient given by the
discriminator contains non-informative adversarial noises, which hinders the
generator from catching the pattern of real samples. Here, we develop
adversarial symmetric GANs (AS-GANs) that incorporate adversarial training of
the discriminator on real samples into vanilla GANs, making adversarial
training symmetrical. The discriminator is therefore more robust and provides
more informative gradient with less adversarial noise, thereby stabilizing
training and accelerating convergence. The effectiveness of the AS-GANs is
verified on image generation on CIFAR-10 , CelebA, and LSUN with varied network
architectures. Not only the training is more stabilized, but the FID scores of
generated samples are consistently improved by a large margin compared to the
baseline. The bridging of adversarial samples and adversarial networks provides
a new approach to further develop adversarial networks.
</p>
<a href="http://arxiv.org/abs/1912.09670" target="_blank">arXiv:1912.09670</a> [<a href="http://arxiv.org/pdf/1912.09670" target="_blank">pdf</a>]

<h2>Deep Network Approximation for Smooth Functions. (arXiv:2001.03040v2 [cs.LG] UPDATED)</h2>
<h3>Jianfeng Lu, Zuowei Shen, Haizhao Yang, Shijun Zhang</h3>
<p>This paper establishes optimal approximation error characterization of deep
ReLU networks for smooth functions in terms of both width and depth
simultaneously. To that end, we first prove that multivariate polynomials can
be approximated by deep ReLU networks of width $\mathcal{O}(N)$ and depth
$\mathcal{O}(L)$ with an approximation error $\mathcal{O}(N^{-L})$. Through
local Taylor expansions and their deep ReLU network approximations, we show
that deep ReLU networks of width $\mathcal{O}(N\ln N)$ and depth
$\mathcal{O}(L\ln L)$ can approximate $f\in C^s([0,1]^d)$ with a nearly optimal
approximation rate $\mathcal{O}(\|f\|_{C^s([0,1]^d)}N^{-2s/d}L^{-2s/d})$. Our
estimate is non-asymptotic in the sense that it is valid for arbitrary width
and depth specified by $N\in\mathbb{N}^+$ and $L\in\mathbb{N}^+$, respectively.
</p>
<a href="http://arxiv.org/abs/2001.03040" target="_blank">arXiv:2001.03040</a> [<a href="http://arxiv.org/pdf/2001.03040" target="_blank">pdf</a>]

<h2>Image Segmentation Using Deep Learning: A Survey. (arXiv:2001.05566v5 [cs.CV] UPDATED)</h2>
<h3>Shervin Minaee, Yuri Boykov, Fatih Porikli, Antonio Plaza, Nasser Kehtarnavaz, Demetri Terzopoulos</h3>
<p>Image segmentation is a key topic in image processing and computer vision
with applications such as scene understanding, medical image analysis, robotic
perception, video surveillance, augmented reality, and image compression, among
many others. Various algorithms for image segmentation have been developed in
the literature. Recently, due to the success of deep learning models in a wide
range of vision applications, there has been a substantial amount of works
aimed at developing image segmentation approaches using deep learning models.
In this survey, we provide a comprehensive review of the literature at the time
of this writing, covering a broad spectrum of pioneering works for semantic and
instance-level segmentation, including fully convolutional pixel-labeling
networks, encoder-decoder architectures, multi-scale and pyramid based
approaches, recurrent networks, visual attention models, and generative models
in adversarial settings. We investigate the similarity, strengths and
challenges of these deep learning models, examine the most widely used
datasets, report performances, and discuss promising future research directions
in this area.
</p>
<a href="http://arxiv.org/abs/2001.05566" target="_blank">arXiv:2001.05566</a> [<a href="http://arxiv.org/pdf/2001.05566" target="_blank">pdf</a>]

<h2>Ellipse R-CNN: Learning to Infer Elliptical Object from Clustering and Occlusion. (arXiv:2001.11584v2 [cs.CV] UPDATED)</h2>
<h3>Wenbo Dong, Pravakar Roy, Cheng Peng, Volkan Isler</h3>
<p>Images of heavily occluded objects in cluttered scenes, such as fruit
clusters in trees, are hard to segment. To further retrieve the 3D size and 6D
pose of each individual object in such cases, bounding boxes are not reliable
from multiple views since only a little portion of the object's geometry is
captured. We introduce the first CNN-based ellipse detector, called Ellipse
R-CNN, to represent and infer occluded objects as ellipses. We first propose a
robust and compact ellipse regression based on the Mask R-CNN architecture for
elliptical object detection. Our method can infer the parameters of multiple
elliptical objects even they are occluded by other neighboring objects. For
better occlusion handling, we exploit refined feature regions for the
regression stage, and integrate the U-Net structure for learning different
occlusion patterns to compute the final detection score. The correctness of
ellipse regression is validated through experiments performed on synthetic data
of clustered ellipses. We further quantitatively and qualitatively demonstrate
that our approach outperforms the state-of-the-art model (i.e., Mask R-CNN
followed by ellipse fitting) and its three variants on both synthetic and real
datasets of occluded and clustered elliptical objects.
</p>
<a href="http://arxiv.org/abs/2001.11584" target="_blank">arXiv:2001.11584</a> [<a href="http://arxiv.org/pdf/2001.11584" target="_blank">pdf</a>]

<h2>Additive Tree Ensembles: Reasoning About Potential Instances. (arXiv:2001.11905v2 [cs.LG] UPDATED)</h2>
<h3>Laurens Devos, Wannes Meert, Jesse Davis</h3>
<p>Imagine being able to ask questions to a black box model such as "Which
adversarial examples exist?", "Does a specific attribute have a
disproportionate effect on the model's prediction?" or "What kind of
predictions are possible for a partially described example?" This last question
is particularly important if your partial description does not correspond to
any observed example in your data, as it provides insight into how the model
will extrapolate to unseen data. These capabilities would be extremely helpful
as it would allow a user to better understand the model's behavior,
particularly as it relates to issues such as robustness, fairness, and bias. In
this paper, we propose such an approach for an ensemble of trees. Since, in
general, this task is intractable we present a strategy that (1) can prune part
of the input space given the question asked to simplify the problem; and (2)
follows a divide and conquer approach that is incremental and can always return
some answers and indicates which parts of the input domains are still
uncertain. The usefulness of our approach is shown on a diverse set of use
cases.
</p>
<a href="http://arxiv.org/abs/2001.11905" target="_blank">arXiv:2001.11905</a> [<a href="http://arxiv.org/pdf/2001.11905" target="_blank">pdf</a>]

<h2>A Deterministic Streaming Sketch for Ridge Regression. (arXiv:2002.02013v2 [cs.LG] UPDATED)</h2>
<h3>Benwei Shi, Jeff M. Phillips</h3>
<p>We provide a deterministic space-efficient algorithm for estimating ridge
regression. For $n$ data points with $d$ features and a large enough
regularization parameter, we provide a solution within $\varepsilon$ L$_2$
error using only $O(d/\varepsilon)$ space. This is the first $o(d^2)$ space
deterministic streaming algorithm with guaranteed solution error and risk bound
for this classic problem. The algorithm sketches the covariance matrix by
variants of Frequent Directions, which implies it can operate in insertion-only
streams and a variety of distributed data settings. In comparisons to
randomized sketching algorithms on synthetic and real-world datasets, our
algorithm has less empirical error using less space and similar time.
</p>
<a href="http://arxiv.org/abs/2002.02013" target="_blank">arXiv:2002.02013</a> [<a href="http://arxiv.org/pdf/2002.02013" target="_blank">pdf</a>]

<h2>Dynamic Energy Dispatch Based on Deep Reinforcement Learning in IoT-Driven Smart Isolated Microgrids. (arXiv:2002.02581v2 [cs.LG] UPDATED)</h2>
<h3>Lei Lei, Yue Tan, Glenn Dahlenburg, Wei Xiang, Kan Zheng</h3>
<p>Microgrids (MGs) are small, local power grids that can operate independently
from the larger utility grid. Combined with the Internet of Things (IoT), a
smart MG can leverage the sensory data and machine learning techniques for
intelligent energy management. This paper focuses on deep reinforcement
learning (DRL)-based energy dispatch for IoT-driven smart isolated MGs with
diesel generators (DGs), photovoltaic (PV) panels, and a battery. A
finite-horizon Partial Observable Markov Decision Process (POMDP) model is
formulated and solved by learning from historical data to capture the
uncertainty in future electricity consumption and renewable power generation.
In order to deal with the instability problem of DRL algorithms and unique
characteristics of finite-horizon models, two novel DRL algorithms, namely,
finite-horizon deep deterministic policy gradient (FH-DDPG) and finite-horizon
recurrent deterministic policy gradient (FH-RDPG), are proposed to derive
energy dispatch policies with and without fully observable state information. A
case study using real isolated MG data is performed, where the performance of
the proposed algorithms are compared with the other baseline DRL and non-DRL
algorithms. Moreover, the impact of uncertainties on MG performance is
decoupled into two levels and evaluated respectively.
</p>
<a href="http://arxiv.org/abs/2002.02581" target="_blank">arXiv:2002.02581</a> [<a href="http://arxiv.org/pdf/2002.02581" target="_blank">pdf</a>]

<h2>Superpixel Image Classification with Graph Attention Networks. (arXiv:2002.05544v2 [cs.LG] UPDATED)</h2>
<h3>Pedro H. C. Avelar, Anderson R. Tavares, Thiago L. T. da Silveira, Cl&#xe1;udio R. Jung, Lu&#xed;s C. Lamb</h3>
<p>This paper presents a methodology for image classification using Graph Neural
Network (GNN) models. We transform the input images into region adjacency
graphs (RAGs), in which regions are superpixels and edges connect neighboring
superpixels. Our experiments suggest that Graph Attention Networks (GATs),
which combine graph convolutions with self-attention mechanisms, outperforms
other GNN models. Although raw image classifiers perform better than GATs due
to information loss during the RAG generation, our methodology opens an
interesting avenue of research on deep learning beyond rectangular-gridded
images, such as 360-degree field of view panoramas. Traditional convolutional
kernels of current state-of-the-art methods cannot handle panoramas, whereas
the adapted superpixel algorithms and the resulting region adjacency graphs can
naturally feed a GNN, without topology issues.
</p>
<a href="http://arxiv.org/abs/2002.05544" target="_blank">arXiv:2002.05544</a> [<a href="http://arxiv.org/pdf/2002.05544" target="_blank">pdf</a>]

<h2>Learning Functionally Decomposed Hierarchies for Continuous Control Tasks with Path Planning. (arXiv:2002.05954v3 [cs.LG] UPDATED)</h2>
<h3>Sammy Christen, Lukas Jendele, Emre Aksan, Otmar Hilliges</h3>
<p>We present HiDe, a novel hierarchical reinforcement learning architecture
that successfully solves long horizon control tasks and generalizes to unseen
test scenarios. Functional decomposition between planning and low-level control
is achieved by explicitly separating the state-action spaces across the
hierarchy, which allows the integration of task-relevant knowledge per layer.
We propose an RL-based planner to efficiently leverage the information in the
planning layer of the hierarchy, while the control layer learns a
goal-conditioned control policy. The hierarchy is trained jointly but allows
for the composition of different policies such as transferring layers across
multiple agents. We experimentally show that our method generalizes across
unseen test environments and can scale to tasks well beyond 3x horizon length
compared to both learning and non-learning based approaches. We evaluate on
complex continuous control tasks with sparse rewards, including navigation and
robot manipulation.
</p>
<a href="http://arxiv.org/abs/2002.05954" target="_blank">arXiv:2002.05954</a> [<a href="http://arxiv.org/pdf/2002.05954" target="_blank">pdf</a>]

<h2>Estimating Training Data Influence by Tracing Gradient Descent. (arXiv:2002.08484v3 [cs.LG] UPDATED)</h2>
<h3>Garima Pruthi, Frederick Liu, Mukund Sundararajan, Satyen Kale</h3>
<p>We introduce a method called TracIn that computes the influence of a training
example on a prediction made by the model. The idea is to trace how the loss on
the test point changes during the training process whenever the training
example of interest was utilized. We provide a scalable implementation of
TracIn via: (a) a first-order gradient approximation to the exact computation,
(b) saved checkpoints of standard training procedures, and (c) cherry-picking
layers of a deep neural network. In contrast with previously proposed methods,
TracIn is simple to implement; all it needs is the ability to work with
gradients, checkpoints, and loss functions. The method is general. It applies
to any machine learning model trained using stochastic gradient descent or a
variant of it, agnostic of architecture, domain and task. We expect the method
to be widely useful within processes that study and improve training data.
</p>
<a href="http://arxiv.org/abs/2002.08484" target="_blank">arXiv:2002.08484</a> [<a href="http://arxiv.org/pdf/2002.08484" target="_blank">pdf</a>]

<h2>UnMask: Adversarial Detection and Defense Through Robust Feature Alignment. (arXiv:2002.09576v2 [cs.CV] UPDATED)</h2>
<h3>Scott Freitas, Shang-Tse Chen, Zijie J. Wang, Duen Horng Chau</h3>
<p>Deep learning models are being integrated into a wide range of high-impact,
security-critical systems, from self-driving cars to medical diagnosis.
However, recent research has demonstrated that many of these deep learning
architectures are vulnerable to adversarial attacks--highlighting the vital
need for defensive techniques to detect and mitigate these attacks before they
occur. To combat these adversarial attacks, we developed UnMask, an adversarial
detection and defense framework based on robust feature alignment. The core
idea behind UnMask is to protect these models by verifying that an image's
predicted class ("bird") contains the expected robust features (e.g., beak,
wings, eyes). For example, if an image is classified as "bird", but the
extracted features are wheel, saddle and frame, the model may be under attack.
UnMask detects such attacks and defends the model by rectifying the
misclassification, re-classifying the image based on its robust features. Our
extensive evaluation shows that UnMask (1) detects up to 96.75% of attacks, and
(2) defends the model by correctly classifying up to 93% of adversarial images
produced by the current strongest attack, Projected Gradient Descent, in the
gray-box setting. UnMask provides significantly better protection than
adversarial training across 8 attack vectors, averaging 31.18% higher accuracy.
We open source the code repository and data with this paper:
https://github.com/safreita1/unmask.
</p>
<a href="http://arxiv.org/abs/2002.09576" target="_blank">arXiv:2002.09576</a> [<a href="http://arxiv.org/pdf/2002.09576" target="_blank">pdf</a>]

<h2>Neuron Shapley: Discovering the Responsible Neurons. (arXiv:2002.09815v3 [stat.ML] UPDATED)</h2>
<h3>Amirata Ghorbani, James Zou</h3>
<p>We develop Neuron Shapley as a new framework to quantify the contribution of
individual neurons to the prediction and performance of a deep network. By
accounting for interactions across neurons, Neuron Shapley is more effective in
identifying important filters compared to common approaches based on activation
patterns. Interestingly, removing just 30 filters with the highest Shapley
scores effectively destroys the prediction accuracy of Inception-v3 on
ImageNet. Visualization of these few critical filters provides insights into
how the network functions. Neuron Shapley is a flexible framework and can be
applied to identify responsible neurons in many tasks. We illustrate additional
applications of identifying filters that are responsible for biased prediction
in facial recognition and filters that are vulnerable to adversarial attacks.
Removing these filters is a quick way to repair models. Enabling all these
applications is a new multi-arm bandit algorithm that we developed to
efficiently estimate Neuron Shapley values.
</p>
<a href="http://arxiv.org/abs/2002.09815" target="_blank">arXiv:2002.09815</a> [<a href="http://arxiv.org/pdf/2002.09815" target="_blank">pdf</a>]

<h2>Uncertainty-Aware Variational-Recurrent Imputation Network for Clinical Time Series. (arXiv:2003.00662v2 [cs.LG] UPDATED)</h2>
<h3>Ahmad Wisnu Mulyadi, Eunji Jun, Heung-Il Suk</h3>
<p>Electronic health records (EHR) consist of longitudinal clinical observations
portrayed with sparsity, irregularity, and high-dimensionality, which become
major obstacles in drawing reliable downstream clinical outcomes. Although
there exist great numbers of imputation methods to tackle these issues, most of
them ignore correlated features, temporal dynamics and entirely set aside the
uncertainty. Since the missing value estimates involve the risk of being
inaccurate, it is appropriate for the method to handle the less certain
information differently than the reliable data. In that regard, we can use the
uncertainties in estimating the missing values as the fidelity score to be
further utilized to alleviate the risk of biased missing value estimates. In
this work, we propose a novel variational-recurrent imputation network, which
unifies an imputation and a prediction network by taking into account the
correlated features, temporal dynamics, as well as the uncertainty.
Specifically, we leverage the deep generative model in the imputation, which is
based on the distribution among variables, and a recurrent imputation network
to exploit the temporal relations, in conjunction with utilization of the
uncertainty. We validated the effectiveness of our proposed model on two
publicly available real-world EHR datasets: PhysioNet Challenge 2012 and
MIMIC-III, and compared the results with other competing state-of-the-art
methods in the literature.
</p>
<a href="http://arxiv.org/abs/2003.00662" target="_blank">arXiv:2003.00662</a> [<a href="http://arxiv.org/pdf/2003.00662" target="_blank">pdf</a>]

<h2>Evaluation of Cross-View Matching to Improve Ground Vehicle Localization with Aerial Perception. (arXiv:2003.06515v4 [cs.RO] UPDATED)</h2>
<h3>Deeksha Dixit, Surabhi Verma, Pratap Tokekar</h3>
<p>Cross-view matching refers to the problem of finding the closest match for a
given query ground view image to one from a database of aerial images. If the
aerial images are geotagged, then the closest matching aerial image can be used
to localize the query ground view image. Due to the recent success of deep
learning methods, several cross-view matching techniques have been proposed.
These approaches perform well for the matching of isolated query images.
However, their evaluation over a trajectory is limited. In this paper, we
evaluate cross-view matching for the task of localizing a ground vehicle over a
longer trajectory. We treat these cross-view matches as sensor measurements
that are fused using a particle filter. We evaluate the performance of this
method using a city-wide dataset collected in a photorealistic simulation by
varying four parameters: height of aerial images, the pitch of the aerial
camera mount, FOV of the ground camera, and the methodology of fusing
cross-view measurements in the particle filter. We also report the results
obtained using our pipeline on a real-world dataset collected using Google
Street View and satellite view APIs.
</p>
<a href="http://arxiv.org/abs/2003.06515" target="_blank">arXiv:2003.06515</a> [<a href="http://arxiv.org/pdf/2003.06515" target="_blank">pdf</a>]

<h2>Optimal Local Explainer Aggregation for Interpretable Prediction. (arXiv:2003.09466v2 [cs.LG] UPDATED)</h2>
<h3>Qiaomei Li, Rachel Cummings, Yonatan Mintz</h3>
<p>A key challenge for decision makers when incorporating black box machine
learned models into practice is being able to understand the predictions
provided by these models. One proposed set of methods is training surrogate
explainer models which approximate the more complex model. Explainer methods
are generally classified as either local or global, depending on what portion
of the data space they are purported to explain. The improved coverage of
global explainers usually comes at the expense of explainer fidelity. One way
of trading off the advantages of both approaches is to aggregate several local
explainers into a single explainer model with improved coverage. However, the
problem of aggregating these local explainers is computationally challenging,
and existing methods only use heuristics to form these aggregations.

In this paper we propose a local explainer aggregation method which selects
local explainers using non-convex optimization. In contrast to other heuristic
methods, we use an integer optimization framework to combine local explainers
into a near-global aggregate explainer. Our framework allows a decision-maker
to directly tradeoff coverage and fidelity of the resulting aggregation through
the parameters of the optimization problem. We also propose a novel local
explainer algorithm based on information filtering. We evaluate our algorithmic
framework on two healthcare datasets---the Parkinson's Progression Marker
Initiative (PPMI) data set and a geriatric mobility dataset---which is
motivated by the anticipated need for explainable precision medicine. Our
method outperforms existing local explainer aggregation methods in terms of
both fidelity and coverage of classification and improves on fidelity over
existing global explainer methods, particularly in multi-class settings where
state-of-the-art methods achieve 70% and ours achieves 90%.
</p>
<a href="http://arxiv.org/abs/2003.09466" target="_blank">arXiv:2003.09466</a> [<a href="http://arxiv.org/pdf/2003.09466" target="_blank">pdf</a>]

<h2>Robust Hypergraph Clustering via Convex Relaxation of Truncated MLE. (arXiv:2003.10038v3 [stat.ML] UPDATED)</h2>
<h3>Jeonghwan Lee, Daesung Kim, Hye Won Chung</h3>
<p>We study hypergraph clustering in the weighted $d$-uniform hypergraph
stochastic block model ($d$\textsf{-WHSBM}), where each edge consisting of $d$
nodes from the same community has higher expected weight than the edges
consisting of nodes from different communities. We propose a new hypergraph
clustering algorithm, called \textsf{CRTMLE}, and provide its performance
guarantee under the $d$\textsf{-WHSBM} for general parameter regimes. We show
that the proposed method achieves the order-wise optimal or the best existing
results for approximately balanced community sizes. Moreover, our results
settle the first recovery guarantees for growing number of clusters of
unbalanced sizes. Involving theoretical analysis and empirical results, we
demonstrate the robustness of our algorithm against the unbalancedness of
community sizes or the presence of outlier nodes.
</p>
<a href="http://arxiv.org/abs/2003.10038" target="_blank">arXiv:2003.10038</a> [<a href="http://arxiv.org/pdf/2003.10038" target="_blank">pdf</a>]

<h2>Spatiotemporal Adaptive Neural Network for Long-term Forecasting of Financial Time Series. (arXiv:2003.12194v3 [cs.LG] UPDATED)</h2>
<h3>Philippe Chatigny, Jean-Marc Patenaude, Shengrui Wang</h3>
<p>Optimal decision-making in social settings is often based on forecasts from
time series (TS) data. Recently, several approaches using deep neural networks
(DNNs) such as recurrent neural networks (RNNs) have been introduced for TS
forecasting and have shown promising results. However, the applicability of
these approaches is being questioned for TS settings where there is a lack of
quality training data and where the TS to forecast exhibit complex behaviors.
Examples of such settings include financial TS forecasting, where producing
accurate and consistent long-term forecasts is notoriously difficult. In this
work, we investigate whether DNN-based models can be used to forecast these TS
conjointly by learning a joint representation of the series instead of
computing the forecast from the raw time-series representations. To this end,
we make use of the dynamic factor graph (DFG) to build a multivariate
autoregressive model. We investigate a common limitation of RNNs that rely on
the DFG framework and propose a novel variable-length attention-based mechanism
(ACTM) to address it. With ACTM, it is possible to vary the autoregressive
order of a TS model over time and model a larger set of probability
distributions than with previous approaches. Using this mechanism, we propose a
self-supervised DNN architecture for multivariate TS forecasting that learns
and takes advantage of the relationships between them. We test our model on two
datasets covering 19 years of investment fund activities. Our experimental
results show that the proposed approach significantly outperforms typical
DNN-based and statistical models at forecasting the 21-day price trajectory. We
point out how improving forecasting accuracy and knowing which forecaster to
use can improve the excess return of autonomous trading strategies.
</p>
<a href="http://arxiv.org/abs/2003.12194" target="_blank">arXiv:2003.12194</a> [<a href="http://arxiv.org/pdf/2003.12194" target="_blank">pdf</a>]

<h2>Harmonic Decompositions of Convolutional Networks. (arXiv:2003.12756v2 [stat.ML] UPDATED)</h2>
<h3>Meyer Scetbon, Zaid Harchaoui</h3>
<p>We present a description of the function space and the smoothness class
associated with a convolutional network using the machinery of reproducing
kernel Hilbert spaces. We show that the mapping associated with a convolutional
network expands into a sum involving elementary functions akin to spherical
harmonics. This functional decomposition can be related to the functional ANOVA
decomposition in nonparametric statistics. Building off our functional
characterization of convolutional networks, we obtain statistical bounds
highlighting an interesting trade-off between the approximation error and the
estimation error.
</p>
<a href="http://arxiv.org/abs/2003.12756" target="_blank">arXiv:2003.12756</a> [<a href="http://arxiv.org/pdf/2003.12756" target="_blank">pdf</a>]

<h2>Estimate of the Neural Network Dimension Using Algebraic Topology and Lie Theory. (arXiv:2004.02881v9 [stat.ML] UPDATED)</h2>
<h3>Luciano Melodia, Richard Lenz</h3>
<p>In this paper we present an approach to determine the smallest possible
number of neurons in a layer of a neural network in such a way that the
topology of the input space can be learned sufficiently well. We introduce a
general procedure based on persistent homology to investigate topological
invariants of the manifold on which we suspect the data set. We specify the
required dimensions precisely, assuming that there is a smooth manifold on or
near which the data are located. Furthermore, we require that this space is
connected and has a commutative group structure in the mathematical sense.
These assumptions allow us to derive a decomposition of the underlying space
whose topology is well known. We use the representatives of the $k$-dimensional
homology groups from the persistence landscape to determine an integer
dimension for this decomposition. This number is the dimension of the embedding
that is capable of capturing the topology of the data manifold. We derive the
theory and validate it experimentally on toy data sets.
</p>
<a href="http://arxiv.org/abs/2004.02881" target="_blank">arXiv:2004.02881</a> [<a href="http://arxiv.org/pdf/2004.02881" target="_blank">pdf</a>]

<h2>A Universal Approximation Theorem of Deep Neural Networks for Expressing Probability Distributions. (arXiv:2004.08867v3 [cs.LG] UPDATED)</h2>
<h3>Yulong Lu, Jianfeng Lu</h3>
<p>This paper studies the universal approximation property of deep neural
networks for representing probability distributions. Given a target
distribution $\pi$ and a source distribution $p_z$ both defined on
$\mathbb{R}^d$, we prove under some assumptions that there exists a deep neural
network $g:\mathbb{R}^d\rightarrow \mathbb{R}$ with ReLU activation such that
the push-forward measure $(\nabla g)_\# p_z$ of $p_z$ under the map $\nabla g$
is arbitrarily close to the target measure $\pi$. The closeness are measured by
three classes of integral probability metrics between probability
distributions: $1$-Wasserstein distance, maximum mean distance (MMD) and
kernelized Stein discrepancy (KSD). We prove upper bounds for the size (width
and depth) of the deep neural network in terms of the dimension $d$ and the
approximation error $\varepsilon$ with respect to the three discrepancies. In
particular, the size of neural network can grow exponentially in $d$ when
$1$-Wasserstein distance is used as the discrepancy, whereas for both MMD and
KSD the size of neural network only depends on $d$ at most polynomially. Our
proof relies on convergence estimates of empirical measures under
aforementioned discrepancies and semi-discrete optimal transport.
</p>
<a href="http://arxiv.org/abs/2004.08867" target="_blank">arXiv:2004.08867</a> [<a href="http://arxiv.org/pdf/2004.08867" target="_blank">pdf</a>]

<h2>Natural Disaster Classification using Aerial Photography Explainable for Typhoon Damaged Feature. (arXiv:2004.10130v5 [cs.CV] UPDATED)</h2>
<h3>Takato Yasuno, Masazumi Amakata, Masahiro Okano</h3>
<p>Recent years, typhoon damages has become social problem owing to climate
change. In 9 September 2019, Typhoon Faxai passed on the Chiba in Japan, whose
damages included with electric provision stop because of strong wind recorded
on the maximum 45 meter per second. A large amount of tree fell down, and the
neighbor electric poles also fell down at the same time. These disaster
features have caused that it took 18 days for recovery longer than past ones.
Immediate responses are important for faster recovery. As long as we can,
aerial survey for global screening of devastated region would be required for
decision support to respond where to recover ahead. This paper proposes a
practical method to visualize the damaged areas focused on the typhoon disaster
features using aerial photography. This method can classify eight classes which
contains land covers without damages and areas with disaster. Using target
feature class probabilities, we can visualize disaster feature map to scale a
color range. Furthermore, we can realize explainable map on each unit grid
images to compute the convolutional activation map using Grad-CAM. We
demonstrate case studies applied to aerial photographs recorded at the Chiba
region after typhoon.
</p>
<a href="http://arxiv.org/abs/2004.10130" target="_blank">arXiv:2004.10130</a> [<a href="http://arxiv.org/pdf/2004.10130" target="_blank">pdf</a>]

<h2>Continual Deep Learning by Functional Regularisation of Memorable Past. (arXiv:2004.14070v3 [stat.ML] UPDATED)</h2>
<h3>Pingbo Pan, Siddharth Swaroop, Alexander Immer, Runa Eschenhagen, Richard E. Turner, Mohammad Emtiyaz Khan</h3>
<p>Continually learning new skills is important for intelligent systems, yet
standard deep learning methods suffer from catastrophic forgetting of the past.
Recent works address this with weight regularisation. Functional
regularisation, although computationally expensive, is expected to perform
better, but rarely does so in practice. In this paper, we fix this issue by
using a new functional-regularisation approach that utilises a few memorable
past examples crucial to avoid forgetting. By using a Gaussian Process
formulation of deep networks, our approach enables training in weight-space
while identifying both the memorable past and a functional prior. Our method
achieves state-of-the-art performance on standard benchmarks and opens a new
direction for life-long learning where regularisation and memory-based methods
are naturally combined.
</p>
<a href="http://arxiv.org/abs/2004.14070" target="_blank">arXiv:2004.14070</a> [<a href="http://arxiv.org/pdf/2004.14070" target="_blank">pdf</a>]

<h2>A Weighted Difference of Anisotropic and Isotropic Total Variation for Relaxed Mumford-Shah Color and Multiphase Image Segmentation. (arXiv:2005.04401v2 [cs.CV] UPDATED)</h2>
<h3>Kevin Bui, Fredrick Park, Yifei Lou, Jack Xin</h3>
<p>In a class of piecewise-constant image segmentation models, we propose to
incorporate a weighted difference of anisotropic and isotropic total variation
(AITV) to regularize the partition boundaries in an image. To deal with the
nonconvex AITV term, we apply the difference-of-convex algorithm (DCA), in
which the subproblems can be minimized by the primal-dual hybrid gradient
method with line search. We can prove that the DCA iterations converge to a
limit point of the proposed model. We discuss the AITV extension of the
Chan-Vese model and the fuzzy region competition model. A generalization to
color image segmentation is also explained. In the numerical experiments, we
compare our proposed models with the classic convex approaches and the
two-stage segmentation methods (denoising and then thresholding) on various
images, showing that our models are effective in image segmentation and robust
with respect to impulsive noises.
</p>
<a href="http://arxiv.org/abs/2005.04401" target="_blank">arXiv:2005.04401</a> [<a href="http://arxiv.org/pdf/2005.04401" target="_blank">pdf</a>]

<h2>BayesRace: Learning to race autonomously using prior experience. (arXiv:2005.04755v2 [cs.RO] UPDATED)</h2>
<h3>Achin Jain, Matthew O&#x27;Kelly, Pratik Chaudhari, Manfred Morari</h3>
<p>Autonomous race cars require perception, estimation, planning, and control
modules which work together asynchronously while driving at the limit of a
vehicle's handling capability. A fundamental challenge encountered in designing
these software components lies in predicting the vehicle's future state (e.g.
position, orientation, and speed) with high accuracy. The root cause is the
difficulty in identifying vehicle model parameters that capture the effects of
lateral tire slip. We present a model-based planning and control framework for
autonomous racing that significantly reduces the effort required in system
identification and control design. Our approach alleviates the gap induced by
simulation-based controller design by learning from on-board sensor
measurements. A major focus of this work is empirical, thus, we demonstrate our
contributions by experiments on validated 1:43 and 1:10 scale autonomous racing
simulations.
</p>
<a href="http://arxiv.org/abs/2005.04755" target="_blank">arXiv:2005.04755</a> [<a href="http://arxiv.org/pdf/2005.04755" target="_blank">pdf</a>]

<h2>Quantitative Analysis of Image Classification Techniques for Memory-Constrained Devices. (arXiv:2005.04968v4 [cs.CV] UPDATED)</h2>
<h3>Sebastian M&#xfc;ksch, Theo Olausson, John Wilhelm, Pavlos Andreadis</h3>
<p>Convolutional Neural Networks, or CNNs, are the state of the art for image
classification, but typically come at the cost of a large memory footprint.
This limits their usefulness in applications relying on embedded devices, where
memory is often a scarce resource. Recently, there has been significant
progress in the field of image classification on such memory-constrained
devices, with novel contributions like the ProtoNN, Bonsai and FastGRNN
algorithms. These have been shown to reach up to 98.2% accuracy on optical
character recognition using MNIST-10, with a memory footprint as little as 6KB.
However, their potential on more complex multi-class and multi-channel image
classification has yet to be determined. In this paper, we compare CNNs with
ProtoNN, Bonsai and FastGRNN when applied to 3-channel image classification
using CIFAR-10. For our analysis, we use the existing Direct Convolution
algorithm to implement the CNNs memory-optimally and propose new methods of
adjusting the FastGRNN model to work with multi-channel images. We extend the
evaluation of each algorithm to a memory size budget of 8KB, 16KB, 32KB, 64KB
and 128KB to show quantitatively that Direct Convolution CNNs perform best for
all chosen budgets, with a top performance of 65.7% accuracy at a memory
footprint of 58.23KB.
</p>
<a href="http://arxiv.org/abs/2005.04968" target="_blank">arXiv:2005.04968</a> [<a href="http://arxiv.org/pdf/2005.04968" target="_blank">pdf</a>]

<h2>Monitoring and Diagnosability of Perception Systems. (arXiv:2005.11816v3 [cs.RO] UPDATED)</h2>
<h3>Pasquale Antonante, David I. Spivak, Luca Carlone</h3>
<p>Perception is a critical component of high-integrity applications of robotics
and autonomous systems, such as self-driving cars. In these applications,
failure of perception systems may put human life at risk, and a broad adoption
of these technologies relies on the development of methodologies to guarantee
and monitor safe operation as well as detect and mitigate failures. Despite the
paramount importance of perception systems, currently there is no formal
approach for system-level monitoring. In this work, we propose a mathematical
model for runtime monitoring and fault detection of perception systems. Towards
this goal, we draw connections with the literature on self-diagnosability for
multiprocessor systems, and generalize it to (i) account for modules with
heterogeneous outputs, and (ii) add a temporal dimension to the problem, which
is crucial to model realistic perception systems where modules interact over
time. This contribution results in a graph-theoretic approach that, given a
perception system, is able to detect faults at runtime and allows computing an
upper-bound on the number of faulty modules that can be detected. Our second
contribution is to show that the proposed monitoring approach can be elegantly
described with the language of topos theory, which allows formulating
diagnosability over arbitrary time intervals.
</p>
<a href="http://arxiv.org/abs/2005.11816" target="_blank">arXiv:2005.11816</a> [<a href="http://arxiv.org/pdf/2005.11816" target="_blank">pdf</a>]

<h2>Deep convolutional tensor network. (arXiv:2005.14506v2 [cs.LG] UPDATED)</h2>
<h3>Philip Blagoveschensky, Anh Huy Phan</h3>
<p>Neural networks have achieved state of the art results in many areas,
supposedly due to parameter sharing, locality, and depth. Tensor networks (TNs)
are linear algebraic representations of quantum many-body states based on their
entanglement structure. TNs have found use in machine learning. We devise a
novel TN based model called Deep convolutional tensor network (DCTN) for image
classification, which has parameter sharing, locality, and depth. It is based
on the Entangled plaquette states (EPS) TN. We show how EPS can be implemented
as a backpropagatable layer. We test DCTN on MNIST, FashionMNIST, and CIFAR10
datasets. A shallow DCTN performs well on MNIST and FashionMNIST and has a
small parameter count. Unfortunately, depth increases overfitting and thus
decreases test accuracy. Also, DCTN of any depth performs badly on CIFAR10 due
to overfitting. It is to be determined why. We discuss how the hyperparameters
of DCTN affect its training and overfitting.
</p>
<a href="http://arxiv.org/abs/2005.14506" target="_blank">arXiv:2005.14506</a> [<a href="http://arxiv.org/pdf/2005.14506" target="_blank">pdf</a>]

<h2>Randomized Policy Learning for Continuous State and Action MDPs. (arXiv:2006.04331v2 [cs.LG] UPDATED)</h2>
<h3>Hiteshi Sharma, Rahul Jain</h3>
<p>Deep reinforcement learning methods have achieved state-of-the-art results in
a variety of challenging, high-dimensional domains ranging from video games to
locomotion. The key to success has been the use of deep neural networks used to
approximate the policy and value function. Yet, substantial tuning of weights
is required for good results. We instead use randomized function approximation.
Such networks are not only cheaper than training fully connected networks but
also improve the numerical performance. We present \texttt{RANDPOL}, a
generalized policy iteration algorithm for MDPs with continuous state and
action spaces. Both the policy and value functions are represented with
randomized networks. We also give finite time guarantees on the performance of
the algorithm. Then we show the numerical performance on challenging
environments and compare them with deep neural network based algorithms.
</p>
<a href="http://arxiv.org/abs/2006.04331" target="_blank">arXiv:2006.04331</a> [<a href="http://arxiv.org/pdf/2006.04331" target="_blank">pdf</a>]

<h2>SoftFlow: Probabilistic Framework for Normalizing Flow on Manifolds. (arXiv:2006.04604v4 [cs.CV] UPDATED)</h2>
<h3>Hyeongju Kim, Hyeonseung Lee, Woo Hyun Kang, Joun Yeop Lee, Nam Soo Kim</h3>
<p>Flow-based generative models are composed of invertible transformations
between two random variables of the same dimension. Therefore, flow-based
models cannot be adequately trained if the dimension of the data distribution
does not match that of the underlying target distribution. In this paper, we
propose SoftFlow, a probabilistic framework for training normalizing flows on
manifolds. To sidestep the dimension mismatch problem, SoftFlow estimates a
conditional distribution of the perturbed input data instead of learning the
data distribution directly. We experimentally show that SoftFlow can capture
the innate structure of the manifold data and generate high-quality samples
unlike the conventional flow-based models. Furthermore, we apply the proposed
framework to 3D point clouds to alleviate the difficulty of forming thin
structures for flow-based models. The proposed model for 3D point clouds,
namely SoftPointFlow, can estimate the distribution of various shapes more
accurately and achieves state-of-the-art performance in point cloud generation.
</p>
<a href="http://arxiv.org/abs/2006.04604" target="_blank">arXiv:2006.04604</a> [<a href="http://arxiv.org/pdf/2006.04604" target="_blank">pdf</a>]

<h2>Rethinking Pre-training and Self-training. (arXiv:2006.06882v2 [cs.CV] UPDATED)</h2>
<h3>Barret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui, Hanxiao Liu, Ekin D. Cubuk, Quoc V. Le</h3>
<p>Pre-training is a dominant paradigm in computer vision. For example,
supervised ImageNet pre-training is commonly used to initialize the backbones
of object detection and segmentation models. He et al., however, show a
surprising result that ImageNet pre-training has limited impact on COCO object
detection. Here we investigate self-training as another method to utilize
additional data on the same setup and contrast it against ImageNet
pre-training. Our study reveals the generality and flexibility of self-training
with three additional insights: 1) stronger data augmentation and more labeled
data further diminish the value of pre-training, 2) unlike pre-training,
self-training is always helpful when using stronger data augmentation, in both
low-data and high-data regimes, and 3) in the case that pre-training is
helpful, self-training improves upon pre-training. For example, on the COCO
object detection dataset, pre-training benefits when we use one fifth of the
labeled data, and hurts accuracy when we use all labeled data. Self-training,
on the other hand, shows positive improvements from +1.3 to +3.4AP across all
dataset sizes. In other words, self-training works well exactly on the same
setup that pre-training does not work (using ImageNet to help COCO). On the
PASCAL segmentation dataset, which is a much smaller dataset than COCO, though
pre-training does help significantly, self-training improves upon the
pre-trained model. On COCO object detection, we achieve 54.3AP, an improvement
of +1.5AP over the strongest SpineNet model. On PASCAL segmentation, we achieve
90.5 mIOU, an improvement of +1.5% mIOU over the previous state-of-the-art
result by DeepLabv3+.
</p>
<a href="http://arxiv.org/abs/2006.06882" target="_blank">arXiv:2006.06882</a> [<a href="http://arxiv.org/pdf/2006.06882" target="_blank">pdf</a>]

<h2>Hybrid Attentional Memory Network for Computational drug repositioning. (arXiv:2006.06910v2 [cs.LG] UPDATED)</h2>
<h3>Jieyue He, Xinxing Yang (Equal contributor), Zhuo Gong, lbrahim Zamit</h3>
<p>Drug repositioning is designed to discover new uses of known drugs, which is
an important and efficient method of drug discovery. Researchers only use one
certain type of Collaborative Filtering (CF) models for drug repositioning
currently, like the neighborhood based approaches which are good at mining the
local information contained in few strong drug-disease associations, or the
latent factor based models which are effectively capture the global information
shared by a majority of drug-disease associations. Few researchers have
combined these two types of CF models to derive a hybrid model with the
advantages of both of them. Besides, the cold start problem has always been a
major challenge in the field of computational drug repositioning, which
restricts the inference ability of relevant models. Inspired by the memory
network, we propose the Hybrid Attentional Memory Network (HAMN) model, a deep
architecture combines two classes of CF model in a nonlinear manner. Firstly,
the memory unit and the attention mechanism are combined to generate the
neighborhood contribution representation to capture the local structure of few
strong drug-disease associations. Then a variant version of the autoencoder is
used to extract the latent factor of drugs and diseases to capture the overall
information shared by a majority of drug-disease associations. In that process,
ancillary information of drugs and diseases can help to alleviate the cold
start problem. Finally, in the prediction stage, the neighborhood contribution
representation is combined with the drug latent factor and disease latent
factor to produce the predicted value. Comprehensive experimental results on
two real data sets show that our proposed HAMN model is superior to other
comparison models according to the AUC, AUPR and HR indicators.
</p>
<a href="http://arxiv.org/abs/2006.06910" target="_blank">arXiv:2006.06910</a> [<a href="http://arxiv.org/pdf/2006.06910" target="_blank">pdf</a>]

<h2>Deep Reinforcement and InfoMax Learning. (arXiv:2006.07217v3 [cs.LG] UPDATED)</h2>
<h3>Bogdan Mazoure, Remi Tachet des Combes, Thang Doan, Philip Bachman, R Devon Hjelm</h3>
<p>We begin with the hypothesis that a model-free agent whose representations
are predictive of properties of future states (beyond expected rewards) will be
more capable of solving and adapting to new RL problems. To test that
hypothesis, we introduce an objective based on Deep InfoMax (DIM) which trains
the agent to predict the future by maximizing the mutual information between
its internal representation of successive timesteps. We test our approach in
several synthetic settings, where it successfully learns representations that
are predictive of the future. Finally, we augment C51, a strong RL baseline,
with our temporal DIM objective and demonstrate improved performance on a
continual learning task and on the recently introduced Procgen environment.
</p>
<a href="http://arxiv.org/abs/2006.07217" target="_blank">arXiv:2006.07217</a> [<a href="http://arxiv.org/pdf/2006.07217" target="_blank">pdf</a>]

<h2>Match and Reweight Strategy for Generalized Target Shift. (arXiv:2006.08161v2 [cs.LG] UPDATED)</h2>
<h3>Alain Rakotomamonjy (Criteo AI Lab), R&#xe9;mi Flamary (CMAP), Gilles Gasso (DocApp - LITIS), Mokhtar Z. Alaya (LMAC, Compi&#xe8;gne), Maxime Berar (DocApp - LITIS), Nicolas Courty (OBELIX)</h3>
<p>We address the problem of unsupervised domain adaptation under the setting of
generalized target shift (both class-conditional and label shifts occur). We
show that in that setting, for good generalization, it is necessary to learn
with similar source and target label distributions and to match the
class-conditional probabilities. For this purpose, we propose an estimation of
target label proportion by blending mixture estimation and optimal transport.
This estimation comes with theoretical guarantees of correctness. Based on the
estimation, we learn a model by minimizing a importance weighted loss and a
Wasserstein distance between weighted marginals. We prove that this
minimization allows to match class-conditionals given mild assumptions on their
geometry. Our experimental results show that our method performs better on
average than competitors accross a range domain adaptation problems including
digits,VisDA and Office.
</p>
<a href="http://arxiv.org/abs/2006.08161" target="_blank">arXiv:2006.08161</a> [<a href="http://arxiv.org/pdf/2006.08161" target="_blank">pdf</a>]

<h2>Feature Space Saturation during Training. (arXiv:2006.08679v4 [cs.LG] UPDATED)</h2>
<h3>Justin Shenk, Mats L. Richter, Wolf Byttner, Anders Arpteg, Mikael Huss</h3>
<p>We propose layer saturation - a simple, online-computable method for
analyzing the information processing in neural networks. First, we show that a
layer's output can be restricted to the eigenspace of its variance matrix
without performance loss. We propose a computationally lightweight method for
approximating the variance matrix during training. From the dimension of its
lossless eigenspace we derive layer saturation - the ratio between the
eigenspace dimension and layer width. We show that saturation seems to indicate
which layers contribute to network performance. We demonstrate how to alter
layer saturation in a neural network by changing network depth, filter sizes
and input resolution. Furthermore, we show that well-chosen input resolution
increases network performance by distributing the inference process more evenly
across the network.
</p>
<a href="http://arxiv.org/abs/2006.08679" target="_blank">arXiv:2006.08679</a> [<a href="http://arxiv.org/pdf/2006.08679" target="_blank">pdf</a>]

<h2>Using Learning Dynamics to Explore the Role of Implicit Regularization in Adversarial Examples. (arXiv:2006.11440v2 [stat.ML] UPDATED)</h2>
<h3>Josue Ortega Caro, Yilong Ju, Fabio Anselmi, Sourav Dey, Ryan Pyle, Ankit Patel</h3>
<p>Recent work (Ilyas et al., 2019) suggests that adversarial examples are
features not bugs. If adversarial perturbations are indeed useful but
non-robust features, what is their origin? To answer this question, we
performed a novel analysis of the learning dynamics of adversarial
perturbations, both in pixel and frequency domains, and a systematic
steganography experiment to explore the implicit bias induced by different
model parametrizations. We find that: (1) adversarial examples are not present
at initialization but instead emerge during training; (2) the frequency-based
nature of common adversarial perturbations in natural images is critically
dependent on an implicit bias towards L1-sparsity in the frequency domain; and
(3) the origin of this bias is the locality and translation invariance of
convolutional filters, along with (4) the existence of useful frequency-based
features in the datasets. We propose a simple theoretical explanation for these
findings, providing a clear and minimalist target for theorists in future work.
Looking forward, our work shows that analyzing the learning dynamics of
perturbations can provide useful insights for understanding the origin of
adversarial sensitivities and developing robust solutions.
</p>
<a href="http://arxiv.org/abs/2006.11440" target="_blank">arXiv:2006.11440</a> [<a href="http://arxiv.org/pdf/2006.11440" target="_blank">pdf</a>]

<h2>D2P-Fed: Differentially Private Federated Learning With Efficient Communication. (arXiv:2006.13039v4 [stat.ML] UPDATED)</h2>
<h3>Lun Wang, Ruoxi Jia, Dawn Song</h3>
<p>In this paper, we propose the discrete Gaussian based differentially private
federated learning (D2P-Fed), a unified scheme to achieve both differential
privacy (DP) and communication efficiency in federated learning (FL). In
particular, compared with the only prior work taking care of both aspects,
D2P-Fed provides stronger privacy guarantee, better composability and smaller
communication cost. The key idea is to apply the discrete Gaussian noise to the
private data transmission. We provide complete analysis of the privacy
guarantee, communication cost and convergence rate of D2P-Fed. We evaluated
D2P-Fed on INFIMNIST and CIFAR10. The results show that D2P-Fed outperforms
the-state-of-the-art by 4.7% to 13.0% in terms of model accuracy while saving
one third of the communication cost.
</p>
<a href="http://arxiv.org/abs/2006.13039" target="_blank">arXiv:2006.13039</a> [<a href="http://arxiv.org/pdf/2006.13039" target="_blank">pdf</a>]

<h2>Global Convergence and Generalization Bound of Gradient-Based Meta-Learning with Deep Neural Nets. (arXiv:2006.14606v2 [cs.LG] UPDATED)</h2>
<h3>Haoxiang Wang, Ruoyu Sun, Bo Li</h3>
<p>Gradient-based meta-learning (GBML) with deep neural nets (DNNs) has become a
popular approach for few-shot learning. However, due to the non-convexity of
DNNs and the bi-level optimization in GBML, the theoretical properties of GBML
with DNNs remain largely unknown. In this paper, we first aim to answer the
following question: Does GBML with DNNs have global convergence guarantees? We
provide a positive answer to this question by proving that GBML with
over-parameterized DNNs is guaranteed to converge to global optima at a linear
rate. The second question we aim to address is: How does GBML achieve fast
adaption to new tasks with prior experience on past tasks? To answer it, we
theoretically show that GBML is equivalent to a functional gradient descent
operation that explicitly propagates experience from the past tasks to new
ones, and then we prove a generalization error bound of GBML with
over-parameterized DNNs.
</p>
<a href="http://arxiv.org/abs/2006.14606" target="_blank">arXiv:2006.14606</a> [<a href="http://arxiv.org/pdf/2006.14606" target="_blank">pdf</a>]

<h2>Learning to Combine Top-Down and Bottom-Up Signals in Recurrent Neural Networks with Attention over Modules. (arXiv:2006.16981v3 [cs.LG] UPDATED)</h2>
<h3>Sarthak Mittal, Alex Lamb, Anirudh Goyal, Vikram Voleti, Murray Shanahan, Guillaume Lajoie, Michael Mozer, Yoshua Bengio</h3>
<p>Robust perception relies on both bottom-up and top-down signals. Bottom-up
signals consist of what's directly observed through sensation. Top-down signals
consist of beliefs and expectations based on past experience and short-term
memory, such as how the phrase `peanut butter and~...' will be completed. The
optimal combination of bottom-up and top-down information remains an open
question, but the manner of combination must be dynamic and both context and
task dependent. To effectively utilize the wealth of potential top-down
information available, and to prevent the cacophony of intermixed signals in a
bidirectional architecture, mechanisms are needed to restrict information flow.
We explore deep recurrent neural net architectures in which bottom-up and
top-down signals are dynamically combined using attention. Modularity of the
architecture further restricts the sharing and communication of information.
Together, attention and modularity direct information flow, which leads to
reliable performance improvements in perceptual and language tasks, and in
particular improves robustness to distractions and noisy data. We demonstrate
on a variety of benchmarks in language modeling, sequential image
classification, video prediction and reinforcement learning that the
\emph{bidirectional} information flow can improve results over strong
baselines.
</p>
<a href="http://arxiv.org/abs/2006.16981" target="_blank">arXiv:2006.16981</a> [<a href="http://arxiv.org/pdf/2006.16981" target="_blank">pdf</a>]

<h2>On the Similarity between the Laplace and Neural Tangent Kernels. (arXiv:2007.01580v2 [cs.LG] UPDATED)</h2>
<h3>Amnon Geifman, Abhay Yadav, Yoni Kasten, Meirav Galun, David Jacobs, Ronen Basri</h3>
<p>Recent theoretical work has shown that massively overparameterized neural
networks are equivalent to kernel regressors that use Neural Tangent
Kernels(NTK). Experiments show that these kernel methods perform similarly to
real neural networks. Here we show that NTK for fully connected networks is
closely related to the standard Laplace kernel. We show theoretically that for
normalized data on the hypersphere both kernels have the same eigenfunctions
and their eigenvalues decay polynomially at the same rate, implying that their
Reproducing Kernel Hilbert Spaces (RKHS) include the same sets of functions.
This means that both kernels give rise to classes of functions with the same
smoothness properties. The two kernels differ for data off the hypersphere, but
experiments indicate that when data is properly normalized these differences
are not significant. Finally, we provide experiments on real data comparing NTK
and the Laplace kernel, along with a larger class of{\gamma}-exponential
kernels. We show that these perform almost identically. Our results suggest
that much insight about neural networks can be obtained from analysis of the
well-known Laplace kernel, which has a simple closed-form.
</p>
<a href="http://arxiv.org/abs/2007.01580" target="_blank">arXiv:2007.01580</a> [<a href="http://arxiv.org/pdf/2007.01580" target="_blank">pdf</a>]

<h2>Piecewise Linear Regression via a Difference of Convex Functions. (arXiv:2007.02422v3 [stat.ML] UPDATED)</h2>
<h3>Ali Siahkamari, Aditya Gangrade, Brian Kulis, Venkatesh Saligrama</h3>
<p>We present a new piecewise linear regression methodology that utilizes
fitting a difference of convex functions (DC functions) to the data. These are
functions $f$ that may be represented as the difference $\phi_1 - \phi_2$ for a
choice of convex functions $\phi_1, \phi_2$. The method proceeds by estimating
piecewise-liner convex functions, in a manner similar to max-affine regression,
whose difference approximates the data. The choice of the function is
regularised by a new seminorm over the class of DC functions that controls the
$\ell_\infty$ Lipschitz constant of the estimate. The resulting methodology can
be efficiently implemented via Quadratic programming even in high dimensions,
and is shown to have close to minimax statistical risk. We empirically validate
the method, showing it to be practically implementable, and to have comparable
performance to existing regression/classification methods on real-world
datasets.
</p>
<a href="http://arxiv.org/abs/2007.02422" target="_blank">arXiv:2007.02422</a> [<a href="http://arxiv.org/pdf/2007.02422" target="_blank">pdf</a>]

<h2>robo-gym -- An Open Source Toolkit for Distributed Deep Reinforcement Learning on Real and Simulated Robots. (arXiv:2007.02753v2 [cs.RO] UPDATED)</h2>
<h3>Matteo Lucchi, Friedemann Zindler, Stephan M&#xfc;hlbacher-Karrer, Horst Pichler</h3>
<p>Applying Deep Reinforcement Learning (DRL) to complex tasks in the field of
robotics has proven to be very successful in the recent years. However, most of
the publications focus either on applying it to a task in simulation or to a
task in a real world setup. Although there are great examples of combining the
two worlds with the help of transfer learning, it often requires a lot of
additional work and fine-tuning to make the setup work effectively. In order to
increase the use of DRL with real robots and reduce the gap between simulation
and real world robotics, we propose an open source toolkit: robo-gym. We
demonstrate a unified setup for simulation and real environments which enables
a seamless transfer from training in simulation to application on the robot. We
showcase the capabilities and the effectiveness of the framework with two real
world applications featuring industrial robots: a mobile robot and a robot arm.
The distributed capabilities of the framework enable several advantages like
using distributed algorithms, separating the workload of simulation and
training on different physical machines as well as enabling the future
opportunity to train in simulation and real world at the same time. Finally we
offer an overview and comparison of robo-gym with other frequently used
state-of-the-art DRL frameworks.
</p>
<a href="http://arxiv.org/abs/2007.02753" target="_blank">arXiv:2007.02753</a> [<a href="http://arxiv.org/pdf/2007.02753" target="_blank">pdf</a>]

<h2>MeTRAbs: Metric-Scale Truncation-Robust Heatmaps for Absolute 3D Human Pose Estimation. (arXiv:2007.07227v2 [cs.CV] UPDATED)</h2>
<h3>Istv&#xe1;n S&#xe1;r&#xe1;ndi, Timm Linder, Kai O. Arras, Bastian Leibe</h3>
<p>Heatmap representations have formed the basis of human pose estimation
systems for many years, and their extension to 3D has been a fruitful line of
recent research. This includes 2.5D volumetric heatmaps, whose X and Y axes
correspond to image space and Z to metric depth around the subject. To obtain
metric-scale predictions, 2.5D methods need a separate post-processing step to
resolve scale ambiguity. Further, they cannot localize body joints outside the
image boundaries, leading to incomplete estimates for truncated images. To
address these limitations, we propose metric-scale truncation-robust (MeTRo)
volumetric heatmaps, whose dimensions are all defined in metric 3D space,
instead of being aligned with image space. This reinterpretation of heatmap
dimensions allows us to directly estimate complete, metric-scale poses without
test-time knowledge of distance or relying on anthropometric heuristics, such
as bone lengths. To further demonstrate the utility our representation, we
present a differentiable combination of our 3D metric-scale heatmaps with 2D
image-space ones to estimate absolute 3D pose (our MeTRAbs architecture). We
find that supervision via absolute pose loss is crucial for accurate
non-root-relative localization. Using a ResNet-50 backbone without further
learned layers, we obtain state-of-the-art results on Human3.6M, MPI-INF-3DHP
and MuPoTS-3D. Our code will be made publicly available to facilitate further
research.
</p>
<a href="http://arxiv.org/abs/2007.07227" target="_blank">arXiv:2007.07227</a> [<a href="http://arxiv.org/pdf/2007.07227" target="_blank">pdf</a>]

<h2>SqueezeFacePoseNet: Lightweight Face Verification Across Different Poses for Mobile Platforms. (arXiv:2007.08566v2 [cs.CV] UPDATED)</h2>
<h3>Fernando Alonso-Fernandez, Javier Barrachina, Kevin Hernandez-Diaz, Josef Bigun</h3>
<p>Virtual applications through mobile platforms are one of the most critical
and ever-growing fields in AI, where ubiquitous and real-time person
authentication has become critical after the breakthrough of all services
provided via mobile devices. In this context, face verification technologies
can provide reliable and robust user authentication, given the availability of
cameras in these devices, as well as their widespread use in everyday
applications. The rapid development of deep Convolutional Neural Networks has
resulted in many accurate face verification architectures. However, their
typical size (hundreds of megabytes) makes them infeasible to be incorporated
in downloadable mobile applications where the entire file typically may not
exceed 100 Mb. Accordingly, we address the challenge of developing a
lightweight face recognition network of just a few megabytes that can operate
with sufficient accuracy in comparison to much larger models. The network also
should be able to operate under different poses, given the variability
naturally observed in uncontrolled environments where mobile devices are
typically used. In this paper, we adapt the lightweight SqueezeNet model, of
just 4.4MB, to effectively provide cross-pose face recognition. After trained
on the MS-Celeb-1M and VGGFace2 databases, our model achieves an EER of 1.23%
on the difficult frontal vs. profile comparison, and0.54% on profile vs.
profile images. Under less extreme variations involving frontal images in any
of the enrolment/query images pair, EER is pushed down to&lt;0.3%, and the FRR at
FAR=0.1%to less than 1%. This makes our light model suitable for face
recognition where at least acquisition of the enrolment image can be
controlled. At the cost of a slight degradation in performance, we also test an
even lighter model (of just 2.5MB) where regular convolutions are replaced with
depth-wise separable convolutions.
</p>
<a href="http://arxiv.org/abs/2007.08566" target="_blank">arXiv:2007.08566</a> [<a href="http://arxiv.org/pdf/2007.08566" target="_blank">pdf</a>]

<h2>Rethinking CNN Models for Audio Classification. (arXiv:2007.11154v2 [cs.CV] UPDATED)</h2>
<h3>Kamalesh Palanisamy, Dipika Singhania, Angela Yao</h3>
<p>In this paper, we show that ImageNet-Pretrained standard deep CNN models can
be used as strong baseline networks for audio classification. Even though there
is a significant difference between audio Spectrogram and standard ImageNet
image samples, transfer learning assumptions still hold firmly. To understand
what enables the ImageNet pretrained models to learn useful audio
representations, we systematically study how much of pretrained weights is
useful for learning spectrograms. We show (1) that for a given standard model
using pretrained weights is better than using randomly initialized weights (2)
qualitative results of what the CNNs learn from the spectrograms by visualizing
the gradients. Besides, we show that even though we use the pretrained model
weights for initialization, there is variance in performance in various output
runs of the same model. This variance in performance is due to the random
initialization of linear classification layer and random mini-batch orderings
in multiple runs. This brings significant diversity to build stronger ensemble
models with an overall improvement in accuracy. An ensemble of ImageNet
pretrained DenseNet achieves 92.89% validation accuracy on the ESC-50 dataset
and 87.42% validation accuracy on the UrbanSound8K dataset which is the current
state-of-the-art on both of these datasets.
</p>
<a href="http://arxiv.org/abs/2007.11154" target="_blank">arXiv:2007.11154</a> [<a href="http://arxiv.org/pdf/2007.11154" target="_blank">pdf</a>]

<h2>BiTraP: Bi-directional Pedestrian Trajectory Prediction with Multi-modal Goal Estimation. (arXiv:2007.14558v2 [cs.CV] UPDATED)</h2>
<h3>Yu Yao, Ella Atkins, Matthew Johnson-Roberson, Ram Vasudevan, Xiaoxiao Du</h3>
<p>Pedestrian trajectory prediction is an essential task in robotic applications
such as autonomous driving and robot navigation. State-of-the-art trajectory
predictors use a conditional variational autoencoder (CVAE) with recurrent
neural networks (RNNs) to encode observed trajectories and decode multi-modal
future trajectories. This process can suffer from accumulated errors over long
prediction horizons (&gt;=2 seconds). This paper presents BiTraP, a
goal-conditioned bi-directional multi-modal trajectory prediction method based
on the CVAE. BiTraP estimates the goal (end-point) of trajectories and
introduces a novel bi-directional decoder to improve longer-term trajectory
prediction accuracy. Extensive experiments show that BiTraP generalizes to both
first-person view (FPV) and bird's-eye view (BEV) scenarios and outperforms
state-of-the-art results by ~10-50%. We also show that different choices of
non-parametric versus parametric target models in the CVAE directly influence
the predicted multi-modal trajectory distributions. These results provide
guidance on trajectory predictor design for robotic applications such as
collision avoidance and navigation systems.
</p>
<a href="http://arxiv.org/abs/2007.14558" target="_blank">arXiv:2007.14558</a> [<a href="http://arxiv.org/pdf/2007.14558" target="_blank">pdf</a>]

<h2>DANA: Dimension-Adaptive Neural Architecture for Multivariate Sensor Data. (arXiv:2008.02397v2 [cs.LG] UPDATED)</h2>
<h3>Mohammad Malekzadeh, Richard G. Clegg, Andrea Cavallaro, Hamed Haddadi</h3>
<p>Motion sensors embedded in wearable and mobile devices allow for dynamic
selection of sensor streams and sampling rates, enabling useful applications,
e.g. for power management or control of data sharing. While deep neural
networks (DNNs) achieve competitive accuracy in sensor data classification,
current DNN architectures only process data coming from a fixed set of sensors
with a fixed sampling rate, and changes in the dimensions of their inputs cause
considerable accuracy loss, unnecessary computations, or failure in operation.
To address this problem, we introduce a dimension-adaptive pooling (DAP) layer
that makes DNNs robust to temporal changes in sampling rate and in sensor
availability. DAP operates on convolutional filter maps of variable dimensions
and produces an input of fixed dimensions suitable for feedforward and
recurrent layers. Building on this architectural improvement, we propose a
dimension-adaptive training (DAT) procedure to generalize over the entire space
of feasible data dimensions at the inference time. DAT comprises the random
selection of dimensions during the forward passes and optimization with
accumulated gradients of several backward passes. We then combine DAP and DAT
to transform existing non-adaptive DNNs into a Dimension-Adaptive Neural
Architecture (DANA) without altering other architectural aspects. Our solution
does not need up-sampling or imputation, thus reduces unnecessary computations
at inference time. Experimental results, on four benchmark datasets of human
activity recognition, show that DANA prevents losses in classification accuracy
of the state-of-the-art DNNs, under dynamic sensor availability and varying
sampling rates.
</p>
<a href="http://arxiv.org/abs/2008.02397" target="_blank">arXiv:2008.02397</a> [<a href="http://arxiv.org/pdf/2008.02397" target="_blank">pdf</a>]

<h2>SynDistNet: Self-Supervised Monocular Fisheye Camera Distance Estimation Synergized with Semantic Segmentation for Autonomous Driving. (arXiv:2008.04017v3 [cs.CV] UPDATED)</h2>
<h3>Varun Ravi Kumar, Marvin Klingner, Senthil Yogamani, Stefan Milz, Tim Fingscheidt, Patrick Maeder</h3>
<p>State-of-the-art self-supervised learning approaches for monocular depth
estimation usually suffer from scale ambiguity. They do not generalize well
when applied on distance estimation for complex projection models such as in
fisheye and omnidirectional cameras. This paper introduces a novel multi-task
learning strategy to improve self-supervised monocular distance estimation on
fisheye and pinhole camera images. Our contribution to this work is threefold:
Firstly, we introduce a novel distance estimation network architecture using a
self-attention based encoder coupled with robust semantic feature guidance to
the decoder that can be trained in a one-stage fashion. Secondly, we integrate
a generalized robust loss function, which improves performance significantly
while removing the need for hyperparameter tuning with the reprojection loss.
Finally, we reduce the artifacts caused by dynamic objects violating static
world assumptions using a semantic masking strategy. We significantly improve
upon the RMSE of previous work on fisheye by 25% reduction in RMSE. As there is
little work on fisheye cameras, we evaluated the proposed method on KITTI using
a pinhole model. We achieved state-of-the-art performance among self-supervised
methods without requiring an external scale estimation.
</p>
<a href="http://arxiv.org/abs/2008.04017" target="_blank">arXiv:2008.04017</a> [<a href="http://arxiv.org/pdf/2008.04017" target="_blank">pdf</a>]

<h2>Large-scale Open Dataset, Pipeline, and Benchmark for Bandit Algorithms. (arXiv:2008.07146v2 [cs.LG] UPDATED)</h2>
<h3>Yuta Saito, Shunsuke Aihara, Megumi Matsutani, Yusuke Narita</h3>
<p>We build and publicize the Open Bandit Dataset to facilitate scalable and
reproducible research on bandit algorithms. It is especially suitable for
off-policy evaluation (OPE), which attempts to estimate the performance of
hypothetical policies using data generated by a different policy.We construct
the dataset based on experiments and implementations on a large-scale fashion
e-commerce platform, ZOZOTOWN. The data contain the ground-truth about the
performance of several bandit policies and enable fair comparisons of different
OPE estimators. We also build a Python package called the Open Bandit Pipeline
to streamline implementations of bandit algorithms and OPE estimators. Our open
data and pipeline will allow researchers and practitioners to easily evaluate
and compare their bandit algorithms and OPE estimators with others in a large,
real-world setting. Using our data and pipeline, we provide extensive benchmark
experiments of existing OPE estimators. Our experiments open up essential
challenges and new avenues for future OPE research. Our pipeline and example
data are available at https://github.com/st-tech/zr-obp. You can follow the
updates of the whole project at
https://groups.google.com/g/open-bandit-project.
</p>
<a href="http://arxiv.org/abs/2008.07146" target="_blank">arXiv:2008.07146</a> [<a href="http://arxiv.org/pdf/2008.07146" target="_blank">pdf</a>]

<h2>Spatial Temporal Transformer Network for Skeleton-based Action Recognition. (arXiv:2008.07404v2 [cs.CV] UPDATED)</h2>
<h3>Chiara Plizzari, Marco Cannici, Matteo Matteucci</h3>
<p>Skeleton-based Human Activity Recognition has achieved a great interest in
recent years, as skeleton data has been demonstrated to be robust to
illumination changes, body scales, dynamic camera views and complex background.
In particular, Spatial-Temporal Graph Convolutional Networks (ST-GCN)
demonstrated to be effective in learning both spatial and temporal dependencies
on non-Euclidean data such as skeleton graphs. Nevertheless, an effective
encoding of the latent information underlying the 3D skeleton is still an open
problem, especially how to extract effective information from joint motion
patterns and their correlations. In this work, we propose a novel
Spatial-Temporal Transformer network (ST-TR) which models dependencies between
joints using the Transformer self-attention operator. In our ST-TR model a
Spatial Self-Attention module (SSA) is used to understand intra-frame
interactions between different body parts, and a Temporal Self-Attention module
(TSA) to model inter-frame correlations. The two are combined in a two-stream
network, whose performance is evaluated on three large-scale datasets,
NTU-RGB+D 60, NTU-RGB+D 120 and Kinetics Skeleton 400, outperforming the
state-of-the-art on NTU-RGB+D w.r.t. models using the same input data
consisting of joint information.
</p>
<a href="http://arxiv.org/abs/2008.07404" target="_blank">arXiv:2008.07404</a> [<a href="http://arxiv.org/pdf/2008.07404" target="_blank">pdf</a>]

<h2>Learning to Actively Reduce Memory Requirements for Robot Control Tasks. (arXiv:2008.07451v2 [cs.RO] UPDATED)</h2>
<h3>Meghan Booker, Anirudha Majumdar</h3>
<p>Robots equipped with rich sensing modalities (e.g., RGB-D cameras) performing
long-horizon tasks motivate the need for policies that are highly
memory-efficient. State-of-the-art approaches for controlling robots often use
memory representations that are excessively rich for the task or rely on
hand-crafted tricks for memory efficiency. Instead, this work provides a
general approach for jointly synthesizing memory representations and policies;
the resulting policies actively seek to reduce memory requirements.
Specifically, we present a reinforcement learning framework that leverages an
implementation of the group LASSO regularization to synthesize policies that
employ low-dimensional and task-centric memory representations. We demonstrate
the efficacy of our approach with simulated examples including navigation in
discrete and continuous spaces as well as vision-based indoor navigation set in
a photo-realistic simulator. The results on these examples indicate that our
method is capable of finding policies that rely only on low-dimensional memory
representations, improving generalization, and actively reducing memory
requirements.
</p>
<a href="http://arxiv.org/abs/2008.07451" target="_blank">arXiv:2008.07451</a> [<a href="http://arxiv.org/pdf/2008.07451" target="_blank">pdf</a>]

<h2>Relevance of Rotationally Equivariant Convolutions for Predicting Molecular Properties. (arXiv:2008.08461v3 [cs.LG] UPDATED)</h2>
<h3>Benjamin Kurt Miller, Mario Geiger, Tess E. Smidt, Frank No&#xe9;</h3>
<p>Equivariant neural networks (ENNs) are graph neural networks embedded in
$\mathbb{R}^3$ and are well suited for predicting molecular properties. The ENN
library \texttt{e3nn} has customizable convolutions, which can be designed to
depend only on distances between points, or also on angular features, making
them rotationally invariant, or equivariant, respectively. This paper studies
the practical value of including angular dependencies for molecular property
prediction directly via an ablation study with \texttt{e3nn} and the QM9 data
set. We find that, for fixed network depth and parameter count, adding angular
features decreased test error by an average of 23\%. Meanwhile, increasing
network depth decreased test error by only 4\% on average, implying that
rotationally equivariant layers are comparatively parameter efficient. We
present an explanation of the accuracy improvement on the dipole moment, the
target which benefited most from the introduction of angular features.
</p>
<a href="http://arxiv.org/abs/2008.08461" target="_blank">arXiv:2008.08461</a> [<a href="http://arxiv.org/pdf/2008.08461" target="_blank">pdf</a>]

<h2>Policy Design and Inverse Reward Learning with Iterative Reasoning in Bounded Risk-Sensitive Markov Games. (arXiv:2009.01495v4 [cs.LG] UPDATED)</h2>
<h3>Ran Tian, Liting Sun, Masayoshi Tomizuka</h3>
<p>Classical game-theoretic approaches for multi-agent systems in both the
forward policy design problem and the inverse reward learning problem often
make strong rationality assumptions: agents perfectly maximize expected
utilities under uncertainties. Such assumptions, however, substantially
mismatch with observed humans' behaviors such as satisficing with sub-optimal,
risk-seeking, and loss-aversion decisions. In this paper, we investigate the
problem of bounded risk-sensitive Markov Game (BRSMG) and its inverse reward
learning problem. {Drawing on iterative reasoning models and cumulative
prospect theory, we embrace that humans have bounded intelligence and maximize
risk-sensitive utilities in BRSMGs.} Convergence analysis for both the forward
policy design and the inverse reward learning problems are established under
the BRSMG framework. We also validate the proposed forward policy design and
inverse reward learning algorithms in a navigation scenario. The results show
that the behaviors of agents demonstrate both risk-averse and risk-seeking
characteristics. Moreover, in the inverse reward learning task, the proposed
bounded risk-sensitive inverse learning algorithm outperforms a baseline
risk-neutral inverse learning algorithm by effectively recovering not only more
accurate reward values but also the intelligence levels and the risk-measure
parameters given demonstrations of agents' interactive behaviors.
</p>
<a href="http://arxiv.org/abs/2009.01495" target="_blank">arXiv:2009.01495</a> [<a href="http://arxiv.org/pdf/2009.01495" target="_blank">pdf</a>]

<h2>Map-Adaptive Goal-Based Trajectory Prediction. (arXiv:2009.04450v2 [cs.LG] UPDATED)</h2>
<h3>Lingyao Zhang, Po-Hsun Su, Jerrick Hoang, Galen Clark Haynes, Micol Marchetti-Bowick</h3>
<p>We present a new method for multi-modal, long-term vehicle trajectory
prediction. Our approach relies on using lane centerlines captured in rich maps
of the environment to generate a set of proposed goal paths for each vehicle.
Using these paths -- which are generated at run time and therefore dynamically
adapt to the scene -- as spatial anchors, we predict a set of goal-based
trajectories along with a categorical distribution over the goals. This
approach allows us to directly model the goal-directed behavior of traffic
actors, which unlocks the potential for more accurate long-term prediction. Our
experimental results on both a large-scale internal driving dataset and on the
public nuScenes dataset show that our model outperforms state-of-the-art
approaches for vehicle trajectory prediction over a 6-second horizon. We also
empirically demonstrate that our model is better able to generalize to road
scenes from a completely new city than existing methods.
</p>
<a href="http://arxiv.org/abs/2009.04450" target="_blank">arXiv:2009.04450</a> [<a href="http://arxiv.org/pdf/2009.04450" target="_blank">pdf</a>]

<h2>An Atlas of Cultural Commonsense for Machine Reasoning. (arXiv:2009.05664v2 [cs.AI] UPDATED)</h2>
<h3>Anurag Acharya, Kartik Talamadupula, Mark A Finlayson</h3>
<p>Existing commonsense reasoning datasets for AI and NLP tasks fail to address
an important aspect of human life: cultural differences. In this work, we
introduce an approach that extends prior work on crowdsourcing commonsense
knowledge by incorporating differences in knowledge that are attributable to
cultural or national groups. We demonstrate the technique by collecting
commonsense knowledge that surrounds six fairly universal rituals---birth,
coming-of-age, marriage, funerals, new year, and birthdays---across two
national groups: the United States and India. Our study expands the different
types of relationships identified by existing work in the field of commonsense
reasoning for commonplace events, and uses these new types to gather
information that distinguishes the knowledge of the different groups. It also
moves us a step closer towards building a machine that doesn't assume a rigid
framework of universal (and likely Western-biased) commonsense knowledge, but
rather has the ability to reason in a contextually and culturally sensitive
way. Our hope is that cultural knowledge of this sort will lead to more
human-like performance in NLP tasks such as question answering (QA) and text
understanding and generation.
</p>
<a href="http://arxiv.org/abs/2009.05664" target="_blank">arXiv:2009.05664</a> [<a href="http://arxiv.org/pdf/2009.05664" target="_blank">pdf</a>]

<h2>TadGAN: Time Series Anomaly Detection Using Generative Adversarial Networks. (arXiv:2009.07769v3 [cs.LG] UPDATED)</h2>
<h3>Alexander Geiger, Dongyu Liu, Sarah Alnegheimish, Alfredo Cuesta-Infante, Kalyan Veeramachaneni</h3>
<p>Time series anomalies can offer information relevant to critical situations
facing various fields, from finance and aerospace to the IT, security, and
medical domains. However, detecting anomalies in time series data is
particularly challenging due to the vague definition of anomalies and said
data's frequent lack of labels and highly complex temporal correlations.
Current state-of-the-art unsupervised machine learning methods for anomaly
detection suffer from scalability and portability issues, and may have high
false positive rates. In this paper, we propose TadGAN, an unsupervised anomaly
detection approach built on Generative Adversarial Networks (GANs). To capture
the temporal correlations of time series distributions, we use LSTM Recurrent
Neural Networks as base models for Generators and Critics. TadGAN is trained
with cycle consistency loss to allow for effective time-series data
reconstruction. We further propose several novel methods to compute
reconstruction errors, as well as different approaches to combine
reconstruction errors and Critic outputs to compute anomaly scores. To
demonstrate the performance and generalizability of our approach, we test
several anomaly scoring techniques and report the best-suited one. We compare
our approach to 8 baseline anomaly detection methods on 11 datasets from
multiple reputable sources such as NASA, Yahoo, Numenta, Amazon, and Twitter.
The results show that our approach can effectively detect anomalies and
outperform baseline methods in most cases (6 out of 11). Notably, our method
has the highest averaged F1 score across all the datasets. Our code is open
source and is available as a benchmarking tool.
</p>
<a href="http://arxiv.org/abs/2009.07769" target="_blank">arXiv:2009.07769</a> [<a href="http://arxiv.org/pdf/2009.07769" target="_blank">pdf</a>]

<h2>Per-frame mAP Prediction for Continuous Performance Monitoring of Object Detection During Deployment. (arXiv:2009.08650v2 [cs.CV] UPDATED)</h2>
<h3>Quazi Marufur Rahman, Niko S&#xfc;nderhauf, Feras Dayoub</h3>
<p>Performance monitoring of object detection is crucial for safety-critical
applications such as autonomous vehicles that operate under varying and complex
environmental conditions. Currently, object detectors are evaluated using
summary metrics based on a single dataset that is assumed to be representative
of all future deployment conditions. In practice, this assumption does not
hold, and the performance fluctuates as a function of the deployment
conditions. To address this issue, we propose an introspection approach to
performance monitoring during deployment without the need for ground truth
data. We do so by predicting when the per-frame mean average precision drops
below a critical threshold using the detector's internal features. We
quantitatively evaluate and demonstrate our method's ability to reduce risk by
trading off making an incorrect decision by raising the alarm and absenting
from detection.
</p>
<a href="http://arxiv.org/abs/2009.08650" target="_blank">arXiv:2009.08650</a> [<a href="http://arxiv.org/pdf/2009.08650" target="_blank">pdf</a>]

<h2>Deep Learning for Predictive Business Process Monitoring: Review and Benchmark. (arXiv:2009.13251v2 [cs.LG] UPDATED)</h2>
<h3>Efr&#xe9;n Rama-Maneiro, Juan C. Vidal, Manuel Lama</h3>
<p>Predictive monitoring of business processes is concerned with the prediction
of ongoing cases on a business process. Lately, the popularity of deep learning
techniques has propitiated an ever-growing set of approaches focused on
predictive monitoring based on these techniques. However, the high disparity of
process logs and experimental setups used to evaluate these approaches makes it
especially difficult to make a fair comparison. Furthermore, it also difficults
the selection of the most suitable approach to solve a specific problem. In
this paper, we provide both a systematic literature review of approaches that
use deep learning to tackle the predictive monitoring tasks. In addition, we
performed an exhaustive experimental evaluation of 10 different approaches over
12 publicly available process logs.
</p>
<a href="http://arxiv.org/abs/2009.13251" target="_blank">arXiv:2009.13251</a> [<a href="http://arxiv.org/pdf/2009.13251" target="_blank">pdf</a>]

<h2>Semi-Supervised Node Classification by Graph Convolutional Networks and Extracted Side Information. (arXiv:2009.13734v2 [cs.LG] UPDATED)</h2>
<h3>Mohammad Esmaeili, Aria Nosratinia</h3>
<p>The nodes of a graph existing in a cluster are more likely to connect to each
other than with other nodes in the graph. Then revealing some information about
some nodes, the structure of the graph (graph edges) provides this opportunity
to know more information about other nodes. From this perspective, this paper
revisits the node classification task in a semi-supervised scenario by graph
convolutional networks (GCNs). The goal is to benefit from the flow of
information that circulates around the revealed node labels. The contribution
of this paper is twofold. First, this paper provides a method for extracting
side information from a graph realization. Then a new GCN architecture is
presented that combines the output of traditional GCN and the extracted side
information. Another contribution of this paper is relevant to non-graph
observations (independent side information) that exists beside a graph
realization in many applications. Indeed, the extracted side information can be
replaced by a sequence of side information that is independent of the graph
structure. For both cases, the experiments on synthetic and real-world datasets
demonstrate that the proposed model achieves a higher prediction accuracy in
comparison to the existing state-of-the-art methods for the node classification
task.
</p>
<a href="http://arxiv.org/abs/2009.13734" target="_blank">arXiv:2009.13734</a> [<a href="http://arxiv.org/pdf/2009.13734" target="_blank">pdf</a>]

<h2>PettingZoo: Gym for Multi-Agent Reinforcement Learning. (arXiv:2009.14471v3 [cs.LG] UPDATED)</h2>
<h3>Justin K. Terry, Benjamin Black, Mario Jayakumar, Ananth Hari, Luis Santos, Clemens Dieffendahl, Niall L. Williams, Yashas Lokesh, Ryan Sullivan, Caroline Horsch, Praveen Ravi</h3>
<p>This paper introduces PettingZoo, a library of diverse sets of multi-agent
environments under a single elegant Python API. PettingZoo was developed with
the goal of acceleration research in multi-agent reinforcement learning, by
creating a set of benchmark environments easily accessible to all researchers
and a standardized API for the field. This goal is inspired by what OpenAI's
Gym library did for accelerating research in single-agent reinforcement
learning, and PettingZoo draws heavily from Gym in terms of API and user
experience. PettingZoo is unique from other multi-agent environment libraries
in that it's API is based on the model of Agent Environment Cycle ("AEC")
games, which allows for the sensible representation all species of games under
one API for the first time. While retaining a very simple and Gym-like API,
PettingZoo still allows access to low-level environment properties required by
non-traditional learning methods.
</p>
<a href="http://arxiv.org/abs/2009.14471" target="_blank">arXiv:2009.14471</a> [<a href="http://arxiv.org/pdf/2009.14471" target="_blank">pdf</a>]

<h2>GCNNMatch: Graph Convolutional Neural Networks for Multi-Object Tracking via Sinkhorn Normalization. (arXiv:2010.00067v3 [cs.CV] UPDATED)</h2>
<h3>Ioannis Papakis, Abhijit Sarkar, Anuj Karpatne</h3>
<p>This paper proposes a novel method for online Multi-Object Tracking (MOT)
using Graph Convolutional Neural Network (GCNN) based feature extraction and
end-to-end feature matching for object association. The Graph based approach
incorporates both appearance and geometry of objects at past frames as well as
the current frame into the task of feature learning. This new paradigm enables
the network to leverage the "context" information of the geometry of objects
and allows us to model the interactions among the features of multiple objects.
Another central innovation of our proposed framework is the use of the Sinkhorn
algorithm for end-to-end learning of the associations among objects during
model training. The network is trained to predict object associations by taking
into account constraints specific to the MOT task. Experimental results
demonstrate the efficacy of the proposed approach in achieving top performance
on the MOT17 Challenge among state-of-the-art online approaches. The code is
available at https://github.com/IPapakis/GCNNMatch.
</p>
<a href="http://arxiv.org/abs/2010.00067" target="_blank">arXiv:2010.00067</a> [<a href="http://arxiv.org/pdf/2010.00067" target="_blank">pdf</a>]

<h2>The Sparse Vector Technique, Revisited. (arXiv:2010.00917v2 [cs.LG] UPDATED)</h2>
<h3>Haim Kaplan, Yishay Mansour, Uri Stemmer</h3>
<p>We revisit one of the most basic and widely applicable techniques in the
literature of differential privacy - the sparse vector technique [Dwork et al.,
STOC 2009]. This simple algorithm privately tests whether the value of a given
query on a database is close to what we expect it to be. It allows to ask an
unbounded number of queries as long as the answer is close to what we expect,
and halts following the first query for which this is not the case.

We suggest an alternative, equally simple, algorithm that can continue
testing queries as long as any single individual does not contribute to the
answer of too many queries whose answer deviates substantially form what we
expect. Our analysis is subtle and some of its ingredients may be more widely
applicable. In some cases our new algorithm allows to privately extract much
more information from the database than the original.

We demonstrate this by applying our algorithm to the shifting heavy-hitters
problem: On every time step, each of $n$ users gets a new input, and the task
is to privately identify all the current heavy-hitters. That is, on time step
$i$, the goal is to identify all data elements $x$ such that many of the users
have $x$ as their current input. We present an algorithm for this problem with
improved error guarantees over what can be obtained using existing techniques.
Specifically, the error of our algorithm depends on the maximal number of times
that a single user holds a heavy-hitter as input, rather than the total number
of times in which a heavy-hitter exists.
</p>
<a href="http://arxiv.org/abs/2010.00917" target="_blank">arXiv:2010.00917</a> [<a href="http://arxiv.org/pdf/2010.00917" target="_blank">pdf</a>]

<h2>Spatial Frequency Bias in Convolutional Generative Adversarial Networks. (arXiv:2010.01473v2 [cs.LG] UPDATED)</h2>
<h3>Mahyar Khayatkhoei, Ahmed Elgammal</h3>
<p>As the success of Generative Adversarial Networks (GANs) on natural images
quickly propels them into various real-life applications across different
domains, it becomes more and more important to clearly understand their
limitations. Specifically, understanding GANs' capability across the full
spectrum of spatial frequencies, i.e. beyond the low-frequency dominant
spectrum of natural images, is critical for assessing the reliability of GAN
generated data in any detail-sensitive application (e.g. denoising, filling and
super-resolution in medical and satellite images). In this paper, we show that
the ability of convolutional GANs to learn a distribution is significantly
affected by the spatial frequency of the underlying carrier signal, that is,
GANs have a bias against learning high spatial frequencies. Crucially, we show
that this bias is not merely a result of the scarcity of high frequencies in
natural images, rather, it is a systemic bias hindering the learning of high
frequencies regardless of their prominence in a dataset. Furthermore, we
explain why large-scale GANs' ability to generate fine details on natural
images does not exclude them from the adverse effects of this bias. Finally, we
propose a method for manipulating this bias with minimal computational
overhead. This method can be used to explicitly direct computational resources
towards any specific spatial frequency of interest in a dataset, extending the
flexibility of GANs.
</p>
<a href="http://arxiv.org/abs/2010.01473" target="_blank">arXiv:2010.01473</a> [<a href="http://arxiv.org/pdf/2010.01473" target="_blank">pdf</a>]

<h2>Deep Reinforcement Learning for Electric Vehicle Routing Problem with Time Windows. (arXiv:2010.02068v2 [cs.LG] UPDATED)</h2>
<h3>Bo Lin, Bissan Ghaddar, Jatin Nathwani</h3>
<p>The past decade has seen a rapid penetration of electric vehicles (EV) in the
market, more and more logistics and transportation companies start to deploy
EVs for service provision. In order to model the operations of a commercial EV
fleet, we utilize the EV routing problem with time windows (EVRPTW). In this
research, we propose an end-to-end deep reinforcement learning framework to
solve the EVRPTW. In particular, we develop an attention model incorporating
the pointer network and a graph embedding technique to parameterize a
stochastic policy for solving the EVRPTW. The model is then trained using
policy gradient with rollout baseline. Our numerical studies show that the
proposed model is able to efficiently solve EVRPTW instances of large sizes
that are not solvable with any existing approaches.
</p>
<a href="http://arxiv.org/abs/2010.02068" target="_blank">arXiv:2010.02068</a> [<a href="http://arxiv.org/pdf/2010.02068" target="_blank">pdf</a>]

<h2>Deep Representation Learning of Patient Data from Electronic Health Records (EHR): A Systematic Review. (arXiv:2010.02809v2 [cs.LG] UPDATED)</h2>
<h3>Yuqi Si, Jingcheng Du, Zhao Li, Xiaoqian Jiang, Timothy Miller, Fei Wang, W. Jim Zheng, Kirk Roberts</h3>
<p>Patient representation learning refers to learning a dense mathematical
representation of a patient that encodes meaningful information from Electronic
Health Records (EHRs). This is generally performed using advanced deep learning
methods. This study presents a systematic review of this field and provides
both qualitative and quantitative analyses from a methodological perspective.
We identified studies developing patient representations from EHRs with deep
learning methods from MEDLINE, EMBASE, Scopus, the Association for Computing
Machinery (ACM) Digital Library, and Institute of Electrical and Electronics
Engineers (IEEE) Xplore Digital Library. After screening 363 articles, 49
papers were included for a comprehensive data collection. We noticed a typical
workflow starting with feeding raw data, applying deep learning models, and
ending with clinical outcome predictions as evaluations of the learned
representations. Specifically, learning representations from structured EHR
data was dominant (37 out of 49 studies). Recurrent Neural Networks were widely
applied as the deep learning architecture (LSTM: 13 studies, GRU: 11 studies).
Disease prediction was the most common application and evaluation (31 studies).
Benchmark datasets were mostly unavailable (28 studies) due to privacy concerns
of EHR data, and code availability was assured in 20 studies. We show the
importance and feasibility of learning comprehensive representations of patient
EHR data through a systematic review. Advances in patient representation
learning techniques will be essential for powering patient-level EHR analyses.
Future work will still be devoted to leveraging the richness and potential of
available EHR data. Knowledge distillation and advanced learning techniques
will be exploited to assist the capability of learning patient representation
further.
</p>
<a href="http://arxiv.org/abs/2010.02809" target="_blank">arXiv:2010.02809</a> [<a href="http://arxiv.org/pdf/2010.02809" target="_blank">pdf</a>]

<h2>Place Recognition in Forests with Urquhart Tessellations. (arXiv:2010.03026v2 [cs.CV] UPDATED)</h2>
<h3>Guilherme V. Nardari, Avraham Cohen, Steven W. Chen, Xu Liu, Vaibhav Arcot, Roseli A. F. Romero, Vijay Kumar</h3>
<p>In this letter, we present a novel descriptor based on Urquhart tessellations
derived from the position of trees in a forest. We propose a framework that
uses these descriptors to detect previously seen observations and landmark
correspondences, even with partial overlap and noise. We run loop closure
detection experiments in simulation and real-world data map-merging from
different flights of an Unmanned Aerial Vehicle (UAV) in a pine tree forest and
show that our method outperforms state-of-the-art approaches in accuracy and
robustness.
</p>
<a href="http://arxiv.org/abs/2010.03026" target="_blank">arXiv:2010.03026</a> [<a href="http://arxiv.org/pdf/2010.03026" target="_blank">pdf</a>]

<h2>Addressing the Real-world Class Imbalance Problem in Dermatology. (arXiv:2010.04308v2 [cs.CV] UPDATED)</h2>
<h3>Wei-Hung Weng, Jonathan Deaton, Vivek Natarajan, Gamaleldin F. Elsayed, Yuan Liu</h3>
<p>Class imbalance is a common problem in medical diagnosis, causing a standard
classifier to be biased towards the common classes and perform poorly on the
rare classes. This is especially true for dermatology, a specialty with
thousands of skin conditions but many of which have low prevalence in the real
world. Motivated by recent advances, we explore few-shot learning methods as
well as conventional class imbalance techniques for the skin condition
recognition problem and propose an evaluation setup to fairly assess the
real-world utility of such approaches. We find the performance of few-show
learning methods does not reach that of conventional class imbalance
techniques, but combining the two approaches using a novel ensemble improves
model performance, especially for rare classes. We conclude that ensembling can
be useful to address the class imbalance problem, yet progress can further be
accelerated by real-world evaluation setups for benchmarking new methods.
</p>
<a href="http://arxiv.org/abs/2010.04308" target="_blank">arXiv:2010.04308</a> [<a href="http://arxiv.org/pdf/2010.04308" target="_blank">pdf</a>]

<h2>Deep Active Learning for Joint Classification & Segmentation with Weak Annotator. (arXiv:2010.04889v2 [cs.CV] UPDATED)</h2>
<h3>Soufiane Belharbi, Ismail Ben Ayed, Luke McCaffrey, Eric Granger</h3>
<p>CNN visualization and interpretation methods, like class-activation maps
(CAMs), are typically used to highlight the image regions linked to class
predictions. These models allow to simultaneously classify images and extract
class-dependent saliency maps, without the need for costly pixel-level
annotations. However, they typically yield segmentations with high
false-positive rates and, therefore, coarse visualisations, more so when
processing challenging images, as encountered in histology. To mitigate this
issue, we propose an active learning (AL) framework, which progressively
integrates pixel-level annotations during training. Given training data with
global image-level labels, our deep weakly-supervised learning model jointly
performs supervised image-level classification and active learning for
segmentation, integrating pixel annotations by an oracle. Unlike standard AL
methods that focus on sample selection, we also leverage large numbers of
unlabeled images via pseudo-segmentations (i.e., self-learning at the pixel
level), and integrate them with the oracle-annotated samples during training.
We report extensive experiments over two challenging benchmarks --
high-resolution medical images (histology GlaS data for colon cancer) and
natural images (CUB-200-2011 for bird species). Our results indicate that, by
simply using random sample selection, the proposed approach can significantly
outperform state-of the-art CAMs and AL methods, with an identical
oracle-supervision budget. Our code is publicly available.
</p>
<a href="http://arxiv.org/abs/2010.04889" target="_blank">arXiv:2010.04889</a> [<a href="http://arxiv.org/pdf/2010.04889" target="_blank">pdf</a>]

<h2>Towards human performance on automatic motion tracking of infant spontaneous movements. (arXiv:2010.05949v4 [cs.CV] UPDATED)</h2>
<h3>Daniel Groos, Lars Adde, Ragnhild St&#xf8;en, Heri Ramampiaro, Espen A. F. Ihlen</h3>
<p>Assessment of spontaneous movements can predict the long-term developmental
outcomes in high-risk infants. In order to develop algorithms for automated
prediction of later function based on early motor repertoire, high-precision
tracking of segments and joints are required. Four types of convolutional
neural networks were investigated on a novel infant pose dataset, covering the
large variation in 1 424 videos from a clinical international community. The
precision level of the networks was evaluated as the deviation between the
estimated keypoint positions and human expert annotations. The computational
efficiency was also assessed to determine the feasibility of the neural
networks in clinical practice. The study shows that the precision of the best
performing infant motion tracker is similar to the inter-rater error of human
experts, while still operating efficiently. In conclusion, the proposed
tracking of infant movements can pave the way for early detection of motor
disorders in children with perinatal brain injuries by quantifying infant
movements from video recordings with human precision.
</p>
<a href="http://arxiv.org/abs/2010.05949" target="_blank">arXiv:2010.05949</a> [<a href="http://arxiv.org/pdf/2010.05949" target="_blank">pdf</a>]

<h2>Behavior Trees in Action: A Study of Robotics Applications. (arXiv:2010.06256v2 [cs.RO] UPDATED)</h2>
<h3>Razan Ghzouli, Thorsten Berger, Einar Broch Johnsen, Swaib Dragule, Andrzej W&#x105;sowski</h3>
<p>Autonomous robots combine a variety of skills to form increasingly complex
behaviors called missions. While the skills are often programmed at a
relatively low level of abstraction, their coordination is architecturally
separated and often expressed in higher-level languages or frameworks.
Recently, the language of Behavior Trees gained attention among roboticists for
this reason. Originally designed for computer games to model autonomous actors,
Behavior Trees offer an extensible tree-based representation of missions.
However, even though, several implementations of the language are in use,
little is known about its usage and scope in the real world. How do behavior
trees relate to traditional languages for describing behavior? How are behavior
tree concepts used in applications? What are the benefits of using them?

We present a study of the key language concepts in Behavior Trees and their
use in real-world robotic applications. We identify behavior tree languages and
compare their semantics to the most well-known behavior modeling languages:
state and activity diagrams. We mine open source repositories for robotics
applications that use the language and analyze this usage. We find that
Behavior Trees are a pragmatic language, not fully specified, allowing projects
to extend it even for just one model. Behavior trees clearly resemble the
models-at-runtime paradigm. We contribute a dataset of real-world behavior
models, hoping to inspire the community to use and further develop this
language, associated tools, and analysis techniques.
</p>
<a href="http://arxiv.org/abs/2010.06256" target="_blank">arXiv:2010.06256</a> [<a href="http://arxiv.org/pdf/2010.06256" target="_blank">pdf</a>]

<h2>MixCo: Mix-up Contrastive Learning for Visual Representation. (arXiv:2010.06300v2 [cs.CV] UPDATED)</h2>
<h3>Sungnyun Kim, Gihun Lee, Sangmin Bae, Se-Young Yun</h3>
<p>Contrastive learning has shown remarkable results in recent self-supervised
approaches for visual representation. By learning to contrast positive pairs'
representation from the corresponding negatives pairs, one can train good
visual representations without human annotations. This paper proposes Mix-up
Contrast (MixCo), which extends the contrastive learning concept to
semi-positives encoded from the mix-up of positive and negative images. MixCo
aims to learn the relative similarity of representations, reflecting how much
the mixed images have the original positives. We validate the efficacy of MixCo
when applied to the recent self-supervised learning algorithms under the
standard linear evaluation protocol on TinyImageNet, CIFAR10, and CIFAR100. In
the experiments, MixCo consistently improves test accuracy. Remarkably, the
improvement is more significant when the learning capacity (e.g., model size)
is limited, suggesting that MixCo might be more useful in real-world scenarios.
The code is available at: https://github.com/Lee-Gihun/MixCo-Mixup-Contrast.
</p>
<a href="http://arxiv.org/abs/2010.06300" target="_blank">arXiv:2010.06300</a> [<a href="http://arxiv.org/pdf/2010.06300" target="_blank">pdf</a>]

<h2>AMPA-Net: Optimization-Inspired Attention Neural Network for Deep Compressed Sensing. (arXiv:2010.06907v6 [cs.CV] UPDATED)</h2>
<h3>Nanyu Li, Charles C. Zhou</h3>
<p>Compressed sensing (CS) is a challenging problem in image processing due to
reconstructing an almost complete image from a limited measurement. To achieve
fast and accurate CS reconstruction, we synthesize the advantages of two
well-known methods (neural network and optimization algorithm) to propose a
novel optimization inspired neural network which dubbed AMP-Net. AMP-Net
realizes the fusion of the Approximate Message Passing (AMP) algorithm and
neural network. All of its parameters are learned automatically. Furthermore,
we propose an AMPA-Net which uses three attention networks to improve the
representation ability of AMP-Net. Finally, We demonstrate the effectiveness of
AMP-Net and AMPA-Net on four standard CS reconstruction benchmark data sets.
Our code is available on https://github.com/puallee/AMPA-Net.
</p>
<a href="http://arxiv.org/abs/2010.06907" target="_blank">arXiv:2010.06907</a> [<a href="http://arxiv.org/pdf/2010.06907" target="_blank">pdf</a>]

<h2>Differentiable Implicit Layers. (arXiv:2010.07078v2 [cs.LG] UPDATED)</h2>
<h3>Andreas Look, Simona Doneva, Melih Kandemir, Rainer Gemulla, Jan Peters</h3>
<p>In this paper, we introduce an efficient backpropagation scheme for
non-constrained implicit functions. These functions are parametrized by a set
of learnable weights and may optionally depend on some input; making them
perfectly suitable as a learnable layer in a neural network. We demonstrate our
scheme on different applications: (i) neural ODEs with the implicit Euler
method, and (ii) system identification in model predictive control.
</p>
<a href="http://arxiv.org/abs/2010.07078" target="_blank">arXiv:2010.07078</a> [<a href="http://arxiv.org/pdf/2010.07078" target="_blank">pdf</a>]

<h2>Vision-Aided Radio: User Identity Match in Radio and Video Domains Using Machine Learning. (arXiv:2010.07219v2 [cs.CV] UPDATED)</h2>
<h3>Vinicius M. de Pinho, Marcello L. R. de Campos, Luis Uzeda Garcia, Dalia Popescu</h3>
<p>5G is designed to be an essential enabler and a leading infrastructure
provider in the communication technology industry by supporting the demand for
the growing data traffic and a variety of services with distinct requirements.
The use of deep learning and computer vision tools has the means to increase
the environmental awareness of the network with information from visual data.
Information extracted via computer vision tools such as user position, movement
direction, and speed can be promptly available for the network. However, the
network must have a mechanism to match the identity of a user in both visual
and radio systems. This mechanism is absent in the present literature.
Therefore, we propose a framework to match the information from both visual and
radio domains. This is an essential step to practical applications of computer
vision tools in communications. We detail the proposed framework training and
deployment phases for a presented setup. We carried out practical experiments
using data collected in different types of environments. The work compares the
use of Deep Neural Network and Random Forest classifiers and shows that the
former performed better across all experiments, achieving classification
accuracy greater than 99%.
</p>
<a href="http://arxiv.org/abs/2010.07219" target="_blank">arXiv:2010.07219</a> [<a href="http://arxiv.org/pdf/2010.07219" target="_blank">pdf</a>]

<h2>Robot Navigation in Constrained Pedestrian Environments using Reinforcement Learning. (arXiv:2010.08600v2 [cs.RO] UPDATED)</h2>
<h3>Claudia P&#xe9;rez-D&#x27;Arpino, Can Liu, Patrick Goebel, Roberto Mart&#xed;n-Mart&#xed;n, Silvio Savarese</h3>
<p>Navigating fluently around pedestrians is a necessary capability for mobile
robots deployed in human environments, such as buildings and homes. While
research on social navigation has focused mainly on the scalability with the
number of pedestrians in open spaces, typical indoor environments present the
additional challenge of constrained spaces such as corridors and doorways that
limit maneuverability and influence patterns of pedestrian interaction. We
present an approach based on reinforcement learning (RL) to learn policies
capable of dynamic adaptation to the presence of moving pedestrians while
navigating between desired locations in constrained environments. The policy
network receives guidance from a motion planner that provides waypoints to
follow a globally planned trajectory, whereas RL handles the local
interactions. We explore a compositional principle for multi-layout training
and find that policies trained in a small set of geometrically simple layouts
successfully generalize to more complex unseen layouts that exhibit composition
of the structural elements available during training. Going beyond walls-world
like domains, we show transfer of the learned policy to unseen 3D
reconstructions of two real environments. These results support the
applicability of the compositional principle to navigation in real-world
buildings and indicate promising usage of multi-agent simulation within
reconstructed environments for tasks that involve interaction.
</p>
<a href="http://arxiv.org/abs/2010.08600" target="_blank">arXiv:2010.08600</a> [<a href="http://arxiv.org/pdf/2010.08600" target="_blank">pdf</a>]

<h2>Generating Large Convex Polytopes Directly on Point Clouds. (arXiv:2010.08744v2 [cs.RO] UPDATED)</h2>
<h3>Xingguang Zhong, Yuwei Wu, Dong Wang, Qianhao Wang, Chao Xu, Fei Gao</h3>
<p>In this paper, we present a method to efficiently generate large, free, and
guaranteed convex space among arbitrarily cluttered obstacles. Our method
operates directly on point clouds, avoids expensive calculations, and processes
thousands of points within a few milliseconds, which extremely suits embedded
platforms. The base stone of our method is sphere flipping, a one-one
invertible nonlinear transformation, which maps a set of unordered points to a
nonlinear space. With these wrapped points, we obtain a collision-free star
convex polytope. Then, utilizing the star convexity, we efficiently modify the
polytope to convex and guarantee its free of obstacles. Extensive quantitative
evaluations show that our method significantly outperforms state-of-the-art
works in efficiency. We also present practical applications with our method in
3D, including large-scale deformable topological mapping and quadrotor optimal
trajectory planning, to validate its capability and efficiency. The source code
of our method will be released for the reference of the community.
</p>
<a href="http://arxiv.org/abs/2010.08744" target="_blank">arXiv:2010.08744</a> [<a href="http://arxiv.org/pdf/2010.08744" target="_blank">pdf</a>]

<h2>Social-VRNN: One-Shot Multi-modal Trajectory Prediction for Interacting Pedestrians. (arXiv:2010.09056v2 [cs.RO] UPDATED)</h2>
<h3>Bruno Brito, Hai Zhu, Wei Pan, Javier Alonso-Mora</h3>
<p>Prediction of human motions is key for safe navigation of autonomous robots
among humans. In cluttered environments, several motion hypotheses may exist
for a pedestrian, due to its interactions with the environment and other
pedestrians.

Previous works for estimating multiple motion hypotheses require a large
number of samples which limits their applicability in real-time motion
planning. In this paper, we present a variational learning approach for
interaction-aware and multi-modal trajectory prediction based on deep
generative neural networks.

Our approach can achieve faster convergence and requires significantly fewer
samples comparing to state-of-the-art methods. Experimental results on real and
simulation data show that our model can effectively learn to infer different
trajectories. We compare our method with three baseline approaches and present
performance results demonstrating that our generative model can achieve higher
accuracy for trajectory prediction by producing diverse trajectories.
</p>
<a href="http://arxiv.org/abs/2010.09056" target="_blank">arXiv:2010.09056</a> [<a href="http://arxiv.org/pdf/2010.09056" target="_blank">pdf</a>]

<h2>Persian Handwritten Digit, Character and Word Recognition Using Deep Learning. (arXiv:2010.12880v2 [cs.CV] UPDATED)</h2>
<h3>Mehdi Bonyani, Simindokht Jahangard, Morteza Daneshmand</h3>
<p>Digit, letter and word recognition for a particular script has various
applications in todays commercial contexts. Nevertheless, only a limited number
of relevant studies have dealt with Persian scripts. In this paper, deep neural
networks are utilized through various DensNet architectures, as well as the
Xception, are adopted, modified and further boosted through data augmentation
and test time augmentation, in order to come up with an optical character
recognition accounting for the particularities of the Persian language and the
corresponding handwritings. Taking advantage of dividing the databases to
training, validation and test sets, as well as k-fold cross validation, the
comparison of the proposed method with various state-of-the-art alternatives is
performed on the basis of the HODA and Sadri databases, which offer the most
comprehensive collection of samples in terms of the various handwriting styles
possessed by different human beings, as well as different forms each letter may
take, which depend on its position within a word. On the HODA database, we
achieve recognition rates of 99.72% and 89.99% for digits and characters, being
99.72%, 98.32% and 98.82% for digits, characters and words from the Sadri
database, respectively.
</p>
<a href="http://arxiv.org/abs/2010.12880" target="_blank">arXiv:2010.12880</a> [<a href="http://arxiv.org/pdf/2010.12880" target="_blank">pdf</a>]

<h2>Formally Verified SAT-Based AI Planning. (arXiv:2010.14648v2 [cs.AI] UPDATED)</h2>
<h3>Mohammad Abdulaziz, Friedrich Kurz</h3>
<p>We present an executable formally verified SAT encoding of classical AI
planning. We use the theorem prover Isabelle/HOL to perform the verification.
We experimentally test the verified encoding and show that it can be used for
reasonably sized standard planning benchmarks. We also use it as a reference to
test a state-of-the-art SAT-based planner, showing that it sometimes falsely
claims that problems have no solutions of certain lengths.
</p>
<a href="http://arxiv.org/abs/2010.14648" target="_blank">arXiv:2010.14648</a> [<a href="http://arxiv.org/pdf/2010.14648" target="_blank">pdf</a>]

<h2>MedMNIST Classification Decathlon: A Lightweight AutoML Benchmark for Medical Image Analysis. (arXiv:2010.14925v2 [cs.CV] UPDATED)</h2>
<h3>Jiancheng Yang, Rui Shi, Bingbing Ni</h3>
<p>We present MedMNIST, a collection of 10 pre-processed medical open datasets.
MedMNIST is standardized to perform classification tasks on lightweight 28x28
images, which requires no background knowledge. Covering the primary data
modalities in medical image analysis, it is diverse on data scale (from 100 to
100,000) and tasks (binary/multi-class, ordinal regression and multi-label).
MedMNIST could be used for educational purpose, rapid prototyping, multi-modal
machine learning or AutoML in medical image analysis. Moreover, MedMNIST
Classification Decathlon is designed to benchmark AutoML algorithms on all 10
datasets; We have compared several baseline methods, including open-source or
commercial AutoML tools. The datasets, evaluation code and baseline methods for
MedMNIST are publicly available at https://medmnist.github.io/.
</p>
<a href="http://arxiv.org/abs/2010.14925" target="_blank">arXiv:2010.14925</a> [<a href="http://arxiv.org/pdf/2010.14925" target="_blank">pdf</a>]

<h2>General Data Analytics with Applications to Visual Information Analysis: A Provable Backward-Compatible Semisimple Paradigm over T-Algebra. (arXiv:2011.00307v3 [cs.CV] UPDATED)</h2>
<h3>Liang Liao, Stephen John Maybank</h3>
<p>We consider a novel backward-compatible paradigm of general data analytics
over a recently-reported semisimple algebra (called t-algebra). We study the
abstract algebraic framework over the t-algebra by representing the elements of
t-algebra by fix-sized multi-way arrays of complex numbers and the algebraic
structure over the t-algebra by a collection of direct-product constituents.
Over the t-algebra, many algorithms, if not all, are generalized in a
straightforward manner using this new semisimple paradigm. To demonstrate the
new paradigm's performance and its backward-compatibility, we generalize some
canonical algorithms for visual pattern analysis. Experiments on public
datasets show that the generalized algorithms compare favorably with their
canonical counterparts.
</p>
<a href="http://arxiv.org/abs/2011.00307" target="_blank">arXiv:2011.00307</a> [<a href="http://arxiv.org/pdf/2011.00307" target="_blank">pdf</a>]

<h2>Estimating County-Level COVID-19 Exponential Growth Rates Using Generalized Random Forests. (arXiv:2011.01219v4 [cs.LG] UPDATED)</h2>
<h3>Zhaowei She, Zilong Wang, Turgay Ayer, Asmae Toumi, Jagpreet Chhatwal</h3>
<p>Rapid and accurate detection of community outbreaks is critical to address
the threat of resurgent waves of COVID-19. A practical challenge in outbreak
detection is balancing accuracy vs. speed. In particular, while estimation
accuracy improves with longer fitting windows, speed degrades. This paper
presents a machine learning framework to balance this tradeoff using
generalized random forests (GRF), and applies it to detect county level
COVID-19 outbreaks. This algorithm chooses an adaptive fitting window size for
each county based on relevant features affecting the disease spread, such as
changes in social distancing policies. Experiment results show that our method
outperforms any non-adaptive window size choices in 7-day ahead COVID-19
outbreak case number predictions.
</p>
<a href="http://arxiv.org/abs/2011.01219" target="_blank">arXiv:2011.01219</a> [<a href="http://arxiv.org/pdf/2011.01219" target="_blank">pdf</a>]

<h2>Multi-armed Bandits with Cost Subsidy. (arXiv:2011.01488v2 [cs.LG] UPDATED)</h2>
<h3>Deeksha Sinha, Karthik Abinav Sankararama, Abbas Kazerouni, Vashist Avadhanula</h3>
<p>In this paper, we consider a novel variant of the multi-armed bandit (MAB)
problem, MAB with cost subsidy, which models many real-life applications where
the learning agent has to pay to select an arm and is concerned about
optimizing cumulative costs and rewards. We present two applications,
intelligent SMS routing problem and ad audience optimization problem faced by a
number of businesses (especially online platforms) and show how our problem
uniquely captures key features of these applications. We show that naive
generalizations of existing MAB algorithms like Upper Confidence Bound and
Thompson Sampling do not perform well for this problem. We then establish
fundamental lower bound of $\Omega(K^{1/3} T^{2/3})$ on the performance of any
online learning algorithm for this problem, highlighting the hardness of our
problem in comparison to the classical MAB problem (where $T$ is the time
horizon and $K$ is the number of arms). We also present a simple variant of
explore-then-commit and establish near-optimal regret bounds for this
algorithm. Lastly, we perform extensive numerical simulations to understand the
behavior of a suite of algorithms for various instances and recommend a
practical guide to employ different algorithms.
</p>
<a href="http://arxiv.org/abs/2011.01488" target="_blank">arXiv:2011.01488</a> [<a href="http://arxiv.org/pdf/2011.01488" target="_blank">pdf</a>]

<h2>Leveraging Activity Recognition to Enable Protective Behavior Detection in Continuous Data. (arXiv:2011.01776v2 [cs.LG] UPDATED)</h2>
<h3>Chongyang Wang, Yuan Gao, Akhil Mathur, Amanda C. De C. Williams, Nicholas D. Lane, Nadia Bianchi-Berthouze</h3>
<p>Protective behavior exhibited by people with chronic pain (CP) during
physical activities is the key to understanding their physical and emotional
states. Existing automatic protective behavior detection (PBD) methods depend
on pre-segmentation of activity instances as they expect situations where
activity types are predefined. However, during everyday management, people pass
from one activity to another, and support should be delivered continuously and
personalized to the activity type and presence of protective behavior. Hence,
to facilitate ubiquitous CP management, it becomes critical to enable accurate
PBD over continuous data. In this paper, we propose to integrate automatic
human activity recognition (HAR) with PBD via a novel hierarchical HAR-PBD
architecture comprising GC-LSTM networks, and alleviate the class imbalances
therein using a CFCC loss function. Through in-depth evaluation of the approach
using a CP patients' dataset, we show that the leveraging of HAR, GC-LSTM
networks and the CFCC loss function leads to clear increase in PBD performance
against the state-of-the-art (macro F1 score of 0.81 vs. 0.66 and PR-AUC of
0.60 vs. 0.44). We conclude by discussing possible use cases of the HAR-PBD
architecture in the context of CP management and other situations. We also
discuss the current limitations and ways forward.
</p>
<a href="http://arxiv.org/abs/2011.01776" target="_blank">arXiv:2011.01776</a> [<a href="http://arxiv.org/pdf/2011.01776" target="_blank">pdf</a>]

<h2>DTGAN: Dual Attention Generative Adversarial Networks for Text-to-Image Generation. (arXiv:2011.02709v2 [cs.CV] UPDATED)</h2>
<h3>Zhenxing Zhang, Lambert Schomaker</h3>
<p>Most existing text-to-image generation methods adopt a multi-stage modular
architecture which has three significant problems: 1) Training multiple
networks increases the run time and affects the convergence and stability of
the generative model; 2) These approaches ignore the quality of early-stage
generator images; 3) Many discriminators need to be trained. To this end, we
propose the Dual Attention Generative Adversarial Network (DTGAN) which can
synthesize high-quality and semantically consistent images only employing a
single generator/discriminator pair. The proposed model introduces
channel-aware and pixel-aware attention modules that can guide the generator to
focus on text-relevant channels and pixels based on the global sentence vector
and to fine-tune original feature maps using attention weights. Also,
Conditional Adaptive Instance-Layer Normalization (CAdaILN) is presented to
help our attention modules flexibly control the amount of change in shape and
texture by the input natural-language description. Furthermore, a new type of
visual loss is utilized to enhance the image resolution by ensuring vivid shape
and perceptually uniform color distributions of generated images. Experimental
results on benchmark datasets demonstrate the superiority of our proposed
method compared to the state-of-the-art models with a multi-stage framework.
Visualization of the attention maps shows that the channel-aware attention
module is able to localize the discriminative regions, while the pixel-aware
attention module has the ability to capture the globally visual contents for
the generation of an image.
</p>
<a href="http://arxiv.org/abs/2011.02709" target="_blank">arXiv:2011.02709</a> [<a href="http://arxiv.org/pdf/2011.02709" target="_blank">pdf</a>]

<h2>Robust building footprint extraction from big multi-sensor data using deep competition network. (arXiv:2011.02879v2 [cs.CV] UPDATED)</h2>
<h3>Mehdi Khoshboresh-Masouleh, Mohammad R. Saradjian</h3>
<p>Building footprint extraction (BFE) from multi-sensor data such as optical
images and light detection and ranging (LiDAR) point clouds is widely used in
various fields of remote sensing applications. However, it is still challenging
research topic due to relatively inefficient building extraction techniques
from variety of complex scenes in multi-sensor data. In this study, we develop
and evaluate a deep competition network (DCN) that fuses very high spatial
resolution optical remote sensing images with LiDAR data for robust BFE. DCN is
a deep superpixelwise convolutional encoder-decoder architecture using the
encoder vector quantization with classified structure. DCN consists of five
encoding-decoding blocks with convolutional weights for robust binary
representation (superpixel) learning. DCN is trained and tested in a big
multi-sensor dataset obtained from the state of Indiana in the United States
with multiple building scenes. Comparison results of the accuracy assessment
showed that DCN has competitive BFE performance in comparison with other deep
semantic binary segmentation architectures. Therefore, we conclude that the
proposed model is a suitable solution to the robust BFE from big multi-sensor
data.
</p>
<a href="http://arxiv.org/abs/2011.02879" target="_blank">arXiv:2011.02879</a> [<a href="http://arxiv.org/pdf/2011.02879" target="_blank">pdf</a>]

<h2>Do We Need to Compensate for Motion Distortion and Doppler Effects in Radar-Based Navigation?. (arXiv:2011.03512v2 [cs.RO] UPDATED)</h2>
<h3>Keenan Burnett, Angela P. Schoellig, Timothy D. Barfoot</h3>
<p>In order to tackle the challenge of unfavorable weather conditions such as
rain and snow, radar is being revisited as a parallel sensing modality to
vision and lidar. Recent works have made tremendous progress in applying radar
to odometry and place recognition. However, these works have so far ignored the
impact of motion distortion and Doppler effects on radar-based navigation,
which may be significant in the self-driving car domain where speeds can be
high. In this work, we demonstrate the effect of these distortions on
radar-only odometry using the Oxford Radar RobotCar Dataset and metric
localization using our own data-taking platform. We present a lightweight
estimator that can recover the motion between a pair of radar scans while
accounting for both effects. Our conclusion is that both motion distortion and
the Doppler effect are significant in different aspects of radar navigation,
with the former more prominent than the latter.
</p>
<a href="http://arxiv.org/abs/2011.03512" target="_blank">arXiv:2011.03512</a> [<a href="http://arxiv.org/pdf/2011.03512" target="_blank">pdf</a>]

<h2>An HVS-Oriented Saliency Map Prediction Modeling. (arXiv:2011.04076v3 [cs.CV] UPDATED)</h2>
<h3>Qiang Li</h3>
<p>Visual attention is one of the most significant characteristics for selecting
and understanding the outside world. The nature complex scenes, including
larger redundancy and human vision, can't be processing all information
simultaneously because of the information bottleneck. The visual system mainly
focuses on dominant parts of the scenes to reduce the input visual redundancy
information. It's commonly known as visual attention prediction or visual
saliency map. This paper proposes a new saliency prediction architecture
inspired by human low-level visual cortex function. The model considered the
opponent color channel, wavelet energy map, and contrast sensitivity function
for extract image features and maximum approach to real visual neural network
function in the brain. The proposed model is evaluated several datasets,
including MIT1003, MIT300, TORONTO, and SID4VAM to explain its efficiency. The
proposed model results are quantitatively and qualitatively compared to other
state-of-the-art salience prediction models and their achieved out-performing
of visual saliency prediction.
</p>
<a href="http://arxiv.org/abs/2011.04076" target="_blank">arXiv:2011.04076</a> [<a href="http://arxiv.org/pdf/2011.04076" target="_blank">pdf</a>]

<h2>LADA: Look-Ahead Data Acquisition via Augmentation for Active Learning. (arXiv:2011.04194v2 [cs.LG] UPDATED)</h2>
<h3>Yoon-Yeong Kim, Kyungwoo Song, JoonHo Jang, Il-Chul Moon</h3>
<p>Active learning effectively collects data instances for training deep
learning models when the labeled dataset is limited and the annotation cost is
high. Besides active learning, data augmentation is also an effective technique
to enlarge the limited amount of labeled instances. However, the potential gain
from virtual instances generated by data augmentation has not been considered
in the acquisition process of active learning yet. Looking ahead the effect of
data augmentation in the process of acquisition would select and generate the
data instances that are informative for training the model. Hence, this paper
proposes Look-Ahead Data Acquisition via augmentation, or LADA, to integrate
data acquisition and data augmentation. LADA considers both 1) unlabeled data
instance to be selected and 2) virtual data instance to be generated by data
augmentation, in advance of the acquisition process. Moreover, to enhance the
informativeness of the virtual data instances, LADA optimizes the data
augmentation policy to maximize the predictive acquisition score, resulting in
the proposal of InfoMixup and InfoSTN. As LADA is a generalizable framework, we
experiment with the various combinations of acquisition and augmentation
methods. The performance of LADA shows a significant improvement over the
recent augmentation and acquisition baselines which were independently applied
to the benchmark datasets.
</p>
<a href="http://arxiv.org/abs/2011.04194" target="_blank">arXiv:2011.04194</a> [<a href="http://arxiv.org/pdf/2011.04194" target="_blank">pdf</a>]

<h2>Unified Quality Assessment of In-the-Wild Videos with Mixed Datasets Training. (arXiv:2011.04263v2 [cs.CV] UPDATED)</h2>
<h3>Dingquan Li, Tingting Jiang, Ming Jiang</h3>
<p>Video quality assessment (VQA) is an important problem in computer vision.
The videos in computer vision applications are usually captured in the wild. We
focus on automatically assessing the quality of in-the-wild videos, which is a
challenging problem due to the absence of reference videos, the complexity of
distortions, and the diversity of video contents. Moreover, the video contents
and distortions among existing datasets are quite different, which leads to
poor performance of data-driven methods in the cross-dataset evaluation
setting. To improve the performance of quality assessment models, we borrow
intuitions from human perception, specifically, content dependency and
temporal-memory effects of human visual system. To face the cross-dataset
evaluation challenge, we explore a mixed datasets training strategy for
training a single VQA model with multiple datasets. The proposed unified
framework explicitly includes three stages: relative quality assessor,
nonlinear mapping, and dataset-specific perceptual scale alignment, to jointly
predict relative quality, perceptual quality, and subjective quality.
Experiments are conducted on four publicly available datasets for VQA in the
wild, i.e., LIVE-VQC, LIVE-Qualcomm, KoNViD-1k, and CVD2014. The experimental
results verify the effectiveness of the mixed datasets training strategy and
prove the superior performance of the unified model in comparison with the
state-of-the-art models. For reproducible research, we make the PyTorch
implementation of our method available at https://github.com/lidq92/MDTVSFA.
</p>
<a href="http://arxiv.org/abs/2011.04263" target="_blank">arXiv:2011.04263</a> [<a href="http://arxiv.org/pdf/2011.04263" target="_blank">pdf</a>]

<h2>Learning to Compose Hierarchical Object-Centric Controllers for Robotic Manipulation. (arXiv:2011.04627v2 [cs.RO] UPDATED)</h2>
<h3>Mohit Sharma, Jacky Liang, Jialiang Zhao, Alex LaGrassa, Oliver Kroemer</h3>
<p>Manipulation tasks can often be decomposed into multiple subtasks performed
in parallel, e.g., sliding an object to a goal pose while maintaining contact
with a table. Individual subtasks can be achieved by task-axis controllers
defined relative to the objects being manipulated, and a set of object-centric
controllers can be combined in an hierarchy. In prior works, such combinations
are defined manually or learned from demonstrations. By contrast, we propose
using reinforcement learning to dynamically compose hierarchical object-centric
controllers for manipulation tasks. Experiments in both simulation and real
world show how the proposed approach leads to improved sample efficiency,
zero-shot generalization to novel test environments, and simulation-to-reality
transfer without fine-tuning.
</p>
<a href="http://arxiv.org/abs/2011.04627" target="_blank">arXiv:2011.04627</a> [<a href="http://arxiv.org/pdf/2011.04627" target="_blank">pdf</a>]

<h2>Attentive Social Recommendation: Towards User And Item Diversities. (arXiv:2011.04797v2 [cs.AI] UPDATED)</h2>
<h3>Dongsheng Luo, Yuchen Bian, Xiang Zhang, Jun Huan</h3>
<p>Social recommendation system is to predict unobserved user-item rating values
by taking advantage of user-user social relation and user-item ratings.
However, user/item diversities in social recommendations are not well utilized
in the literature. Especially, inter-factor (social and rating factors)
relations and distinct rating values need taking into more consideration. In
this paper, we propose an attentive social recommendation system (ASR) to
address this issue from two aspects. First, in ASR, Rec-conv graph network
layers are proposed to extract the social factor, user-rating and item-rated
factors and then automatically assign contribution weights to aggregate these
factors into the user/item embedding vectors. Second, a disentangling strategy
is applied for diverse rating values. Extensive experiments on benchmarks
demonstrate the effectiveness and advantages of our ASR.
</p>
<a href="http://arxiv.org/abs/2011.04797" target="_blank">arXiv:2011.04797</a> [<a href="http://arxiv.org/pdf/2011.04797" target="_blank">pdf</a>]

<h2>Kinematics-Guided Reinforcement Learning for Object-Aware 3D Ego-Pose Estimation. (arXiv:2011.04837v2 [cs.CV] UPDATED)</h2>
<h3>Zhengyi Luo, Ryo Hachiuma, Ye Yuan, Shun Iwase, Kris M. Kitani</h3>
<p>We propose a method for incorporating object interaction and human body
dynamics into the task of 3D ego-pose estimation using a head-mounted camera.
We use a kinematics model of the human body to represent the entire range of
human motion, and a dynamics model of the body to interact with objects inside
a physics simulator. By bringing together object modeling, kinematics modeling,
and dynamics modeling in a reinforcement learning (RL) framework, we enable
object-aware 3D ego-pose estimation. We devise several representational
innovations through the design of the state and action space to incorporate 3D
scene context and improve pose estimation quality. We also construct a
fine-tuning step to correct the drift and refine the estimated human-object
interaction. This is the first work to estimate a physically valid 3D full-body
interaction sequence with objects (e.g., chairs, boxes, obstacles) from
egocentric videos. Experiments with both controlled and in-the-wild settings
show that our method can successfully extract an object-conditioned 3D ego-pose
sequence that is consistent with the laws of physics.
</p>
<a href="http://arxiv.org/abs/2011.04837" target="_blank">arXiv:2011.04837</a> [<a href="http://arxiv.org/pdf/2011.04837" target="_blank">pdf</a>]

<h2>Removing Brightness Bias in Rectified Gradients. (arXiv:2011.05002v2 [cs.CV] UPDATED)</h2>
<h3>Lennart Brocki, Neo Christopher Chung</h3>
<p>Interpretation and improvement of deep neural networks relies on better
understanding of their underlying mechanisms. In particular, gradients of
classes or concepts with respect to the input features (e.g., pixels in images)
are often used as importance scores, which are visualized in saliency maps.
Thus, a family of saliency methods provide an intuitive way to identify input
features with substantial influences on classifications or latent concepts.
Rectified Gradients \cite{Kim2019} is a new method which introduce layer-wise
thresholding in order to denoise the saliency maps. While visually coherent in
certain cases, we identify a brightness bias in Rectified Gradients. We
demonstrate that dark areas of an input image are not highlighted by a saliency
map using Rectified Gradients, even if it is relevant for the class or concept.
Even in the scaled images, the bias exists around an artificial point in color
spectrum. Our simple modification removes this bias and recovers input features
that were removed due to their colors.

"No Bias Rectified Gradient" is available at
\url{https://github.com/lenbrocki/NoBias-Rectified-Gradient}
</p>
<a href="http://arxiv.org/abs/2011.05002" target="_blank">arXiv:2011.05002</a> [<a href="http://arxiv.org/pdf/2011.05002" target="_blank">pdf</a>]

<h2>MP-ResNet: Multi-path Residual Network for the Semantic segmentation of High-Resolution PolSAR Images. (arXiv:2011.05088v2 [cs.CV] UPDATED)</h2>
<h3>Lei Ding, Kai Zheng, Dong Lin, Yuxing Chen, Bing Liu, Jiansheng Li, Lorenzo Bruzzone</h3>
<p>There are limited studies on the semantic segmentation of high-resolution
Polarimetric Synthetic Aperture Radar (PolSAR) images due to the scarcity of
training data and the inference of speckle noises. The Gaofen contest has
provided open access of a high-quality PolSAR semantic segmentation dataset.
Taking this chance, we propose a Multi-path ResNet (MP-ResNet) architecture for
the semantic segmentation of high-resolution PolSAR images. Compared to
conventional U-shape encoder-decoder convolutional neural network (CNN)
architectures, the MP-ResNet learns semantic context with its parallel
multi-scale branches, which greatly enlarges its valid receptive fields and
improves the embedding of local discriminative features. In addition, MP-ResNet
adopts a multi-level feature fusion design in its decoder to make the best use
of the features learned from its different branches. Ablation studies show that
the MPResNet has significant advantages over its baseline method (FCN with
ResNet34). It also surpasses several classic state-of-the-art methods in terms
of overall accuracy (OA), mean F1 and fwIoU, whereas its computational costs
are not much increased. This CNN architecture can be used as a baseline method
for future studies on the semantic segmentation of PolSAR images. The code is
available at: https://github.com/ggsDing/SARSeg.
</p>
<a href="http://arxiv.org/abs/2011.05088" target="_blank">arXiv:2011.05088</a> [<a href="http://arxiv.org/pdf/2011.05088" target="_blank">pdf</a>]

<h2>Regularization of Persistent Homology Gradient Computation. (arXiv:2011.05804v2 [cs.LG] UPDATED)</h2>
<h3>Padraig Corcoran, Bailin Deng</h3>
<p>Persistent homology is a method for computing the topological features
present in a given data. Recently, there has been much interest in the
integration of persistent homology as a computational step in neural networks
or deep learning. In order for a given computation to be integrated in such a
way, the computation in question must be differentiable. Computing the
gradients of persistent homology is an ill-posed inverse problem with
infinitely many solutions. Consequently, it is important to perform
regularization so that the solution obtained agrees with known priors. In this
work we propose a novel method for regularizing persistent homology gradient
computation through the addition of a grouping term. This has the effect of
helping to ensure gradients are defined with respect to larger entities and not
individual points.
</p>
<a href="http://arxiv.org/abs/2011.05804" target="_blank">arXiv:2011.05804</a> [<a href="http://arxiv.org/pdf/2011.05804" target="_blank">pdf</a>]

<h2>A Review of Uncertainty Quantification in Deep Learning: Techniques, Applications and Challenges. (arXiv:2011.06225v2 [cs.LG] UPDATED)</h2>
<h3>Moloud Abdar, Farhad Pourpanah, Sadiq Hussain, Dana Rezazadegan, Li Liu, Mohammad Ghavamzadeh, Paul Fieguth, Xiaochun Cao, Abbas Khosravi, U Rajendra Acharya, Vladimir Makarenkov, Saeid Nahavandi</h3>
<p>Uncertainty quantification (UQ) plays a pivotal role in reduction of
uncertainties during both optimization and decision making processes. It can be
applied to solve a variety of real-world applications in science and
engineering. Bayesian approximation and ensemble learning techniques are two
most widely-used UQ methods in the literature. In this regard, researchers have
proposed different UQ methods and examined their performance in a variety of
applications such as computer vision (e.g., self-driving cars and object
detection), image processing (e.g., image restoration), medical image analysis
(e.g., medical image classification and segmentation), natural language
processing (e.g., text classification, social media texts and recidivism
risk-scoring), bioinformatics, etc. This study reviews recent advances in UQ
methods used in deep learning. Moreover, we also investigate the application of
these methods in reinforcement learning (RL). Then, we outline a few important
applications of UQ methods. Finally, we briefly highlight the fundamental
research challenges faced by UQ methods and discuss the future research
directions in this field.
</p>
<a href="http://arxiv.org/abs/2011.06225" target="_blank">arXiv:2011.06225</a> [<a href="http://arxiv.org/pdf/2011.06225" target="_blank">pdf</a>]

<h2>Adaptive Future Frame Prediction with Ensemble Network. (arXiv:2011.06788v2 [cs.CV] UPDATED)</h2>
<h3>Wonjik Kim, Masayuki Tanaka, Masatoshi Okutomi, Yoko Sasaki</h3>
<p>Future frame prediction in videos is a challenging problem because videos
include complicated movements and large appearance changes. Learning-based
future frame prediction approaches have been proposed in kinds of literature. A
common limitation of the existing learning-based approaches is a mismatch of
training data and test data. In the future frame prediction task, we can obtain
the ground truth data by just waiting for a few frames. It means we can update
the prediction model online in the test phase. Then, we propose an adaptive
update framework for the future frame prediction task. The proposed adaptive
updating framework consists of a pre-trained prediction network, a
continuous-updating prediction network, and a weight estimation network. We
also show that our pre-trained prediction model achieves comparable performance
to the existing state-of-the-art approaches. We demonstrate that our approach
outperforms existing methods especially for dynamically changing scenes.
</p>
<a href="http://arxiv.org/abs/2011.06788" target="_blank">arXiv:2011.06788</a> [<a href="http://arxiv.org/pdf/2011.06788" target="_blank">pdf</a>]

<h2>Relative Drone -- Ground Vehicle Localization using LiDAR and Fisheye Cameras through Direct and Indirect Observations. (arXiv:2011.07008v2 [cs.RO] UPDATED)</h2>
<h3>Jan Hausberg, Ryoichi Ishikawa, Menandro Roxas, Takeshi Oishi</h3>
<p>Estimating the pose of an unmanned aerial vehicle (UAV) or drone is a
challenging task. It is useful for many applications such as navigation,
surveillance, tracking objects on the ground, and 3D reconstruction. In this
work, we present a LiDAR-camera-based relative pose estimation method between a
drone and a ground vehicle, using a LiDAR sensor and a fisheye camera on the
vehicle's roof and another fisheye camera mounted under the drone. The LiDAR
sensor directly observes the drone and measures its position, and the two
cameras estimate the relative orientation using indirect observation of the
surrounding objects. We propose a dynamically adaptive kernel-based method for
drone detection and tracking using the LiDAR. We detect vanishing points in
both cameras and find their correspondences to estimate the relative
orientation. Additionally, we propose a rotation correction technique by
relying on the observed motion of the drone through the LiDAR. In our
experiments, we were able to achieve very fast initial detection and real-time
tracking of the drone. Our method is fully automatic.
</p>
<a href="http://arxiv.org/abs/2011.07008" target="_blank">arXiv:2011.07008</a> [<a href="http://arxiv.org/pdf/2011.07008" target="_blank">pdf</a>]

<h2>Monitoring and Diagnosability of Perception Systems. (arXiv:2011.07010v2 [cs.RO] UPDATED)</h2>
<h3>Pasquale Antonante, David I. Spivak, Luca Carlone</h3>
<p>Perception is a critical component of high-integrity applications of robotics
and autonomous systems, such as self-driving vehicles. In these applications,
failure of perception systems may put human life at risk, and a broad adoption
of these technologies requires the development of methodologies to guarantee
and monitor safe operation. Despite the paramount importance of perception
systems, currently there is no formal approach for system-level monitoring. In
this work, we propose a mathematical model for runtime monitoring and fault
detection and identification in perception systems. Towards this goal, we draw
connections with the literature on diagnosability in multiprocessor systems,
and generalize it to account for modules with heterogeneous outputs that
interact over time. The resulting temporal diagnostic graphs (i) provide a
framework to reason over the consistency of perception outputs -- across
modules and over time -- thus enabling fault detection, (ii) allow us to
establish formal guarantees on the maximum number of faults that can be
uniquely identified in a given perception system, and (iii) enable the design
of efficient algorithms for fault identification. We demonstrate our monitoring
system, dubbed PerSyS, in realistic simulations using the LGSVL self-driving
simulator and the Apollo Auto autonomy software stack, and show that PerSyS is
able to detect failures in challenging scenarios (including scenarios that have
caused self-driving car accidents in recent years), and is able to correctly
identify faults while entailing a minimal computation overhead (&lt; 5ms on a
single-core CPU).
</p>
<a href="http://arxiv.org/abs/2011.07010" target="_blank">arXiv:2011.07010</a> [<a href="http://arxiv.org/pdf/2011.07010" target="_blank">pdf</a>]

<h2>What Do Compressed Deep Neural Networks Forget?. (arXiv:1911.05248v2 [cs.LG] CROSS LISTED)</h2>
<h3>Sara Hooker, Aaron Courville, Gregory Clark, Yann Dauphin, Andrea Frome</h3>
<p>Deep neural network pruning and quantization techniques have demonstrated it
is possible to achieve high levels of compression with surprisingly little
degradation to test set accuracy. However, this measure of performance conceals
significant differences in how different classes and images are impacted by
model compression techniques. We find that models with radically different
numbers of weights have comparable top-line performance metrics but diverge
considerably in behavior on a narrow subset of the dataset. This small subset
of data points, which we term Pruning Identified Exemplars (PIEs) are
systematically more impacted by the introduction of sparsity. Compression
disproportionately impacts model performance on the underrepresented long-tail
of the data distribution. PIEs over-index on atypical or noisy images that are
far more challenging for both humans and algorithms to classify. Our work
provides intuition into the role of capacity in deep neural networks and the
trade-offs incurred by compression. An understanding of this disparate impact
is critical given the widespread deployment of compressed models in the wild.
</p>
<a href="http://arxiv.org/abs/1911.05248" target="_blank">arXiv:1911.05248</a> [<a href="http://arxiv.org/pdf/1911.05248" target="_blank">pdf</a>]

<h2>Global Image Segmentation Process using Machine Learning algorithm & Convolution Neural Network method for Self- Driving Vehicles. (arXiv:2010.13294v2 [cs.CV] CROSS LISTED)</h2>
<h3>Tirumalapudi Raviteja, Rajay Vedaraj .I.S</h3>
<p>In autonomous Vehicles technology Image segmentation was a major problem in
visual perception. This image segmentation process is mainly used in medical
applications. Here we adopted an image segmentation process to visual
perception tasks for predicting the agents on the surrounding environment,
identifying the road boundaries and tracking the line markings. Main objective
of the paper is to divide the input images using the image segmentation process
and Convolution Neural Network method for efficient results of visual
perception. For Sampling assume a local city data-set samples and validation
process done in Jupyter Notebook using Python language. We proposed this image
segmentation method planning to standard and further the development of
state-of-the art methods for visual inspection system understanding. The
experimental results achieves 73% mean IOU. Our method also achieves 90 FPS
inference speed and using a NVDIA GeForce GTX 1050 GPU.
</p>
<a href="http://arxiv.org/abs/2010.13294" target="_blank">arXiv:2010.13294</a> [<a href="http://arxiv.org/pdf/2010.13294" target="_blank">pdf</a>]

<h2>Safe and Robust Motion Planning for Dynamic Robotics via Control Barrier Functions. (arXiv:2011.06748v1 [cs.RO] CROSS LISTED)</h2>
<h3>Aniketh Manjunath, Quan Nguyen</h3>
<p>Control Barrier Functions (CBF) are widely used to enforce the
safety-critical constraints on nonlinear systems. Recently, these functions are
being incorporated into a path planning framework to design a safety-critical
path planner. However, these methods fall short of providing a realistic path
considering both run-time complexity and safety-critical constraints. This
paper proposes a novel motion planning approach using Rapidly exploring Random
Trees (RRT) algorithm to enforce the robust CBF and kinodynamic constraints to
generate a safety-critical path that is free of any obstacles while taking into
account the model uncertainty from robot dynamics as well as perception. Result
analysis indicates that the proposed method outperforms various conventional
RRT based path planners, guaranteeing a safety-critical path with reduced
computational overhead. We present numerical validation of the algorithm on the
Hamster V7 robot car, a micro autonomous Unmanned Ground Vehicle, where it
performs dynamic navigation on an obstacle-ridden path with various
uncertainties in perception noises, and robot dynamics.
</p>
<a href="http://arxiv.org/abs/2011.06748" target="_blank">arXiv:2011.06748</a> [<a href="http://arxiv.org/pdf/2011.06748" target="_blank">pdf</a>]

<h2>Multi-label classification: do Hamming loss and subset accuracy really conflict with each other?. (arXiv:2011.07805v1 [cs.LG])</h2>
<h3>Guoqiang Wu, Jun Zhu</h3>
<p>Various evaluation measures have been developed for multi-label
classification, including Hamming Loss (HL), Subset Accuracy (SA) and Ranking
Loss (RL). However, there is a gap between empirical results and the existing
theories: 1) an algorithm often empirically performs well on some measure(s)
while poorly on others, while a formal theoretical analysis is lacking; and 2)
in small label space cases, the algorithms optimizing HL often have comparable
or even better performance on the SA measure than those optimizing SA directly,
while existing theoretical results show that SA and HL are conflicting
measures. This paper provides an attempt to fill up this gap by analyzing the
learning guarantees of the corresponding learning algorithms on both SA and HL
measures. We show that when a learning algorithm optimizes HL with its
surrogate loss, it enjoys an error bound for the HL measure independent of $c$
(the number of labels), while the bound for the SA measure depends on at most
$O(c)$. On the other hand, when directly optimizing SA with its surrogate loss,
it has learning guarantees that depend on $O(\sqrt{c})$ for both HL and SA
measures. This explains the observation that when the label space is not large,
optimizing HL with its surrogate loss can have promising performance for SA. We
further show that our techniques are applicable to analyze the learning
guarantees of algorithms on other measures, such as RL. Finally, the
theoretical analyses are supported by experimental results.
</p>
<a href="http://arxiv.org/abs/2011.07805" target="_blank">arXiv:2011.07805</a> [<a href="http://arxiv.org/pdf/2011.07805" target="_blank">pdf</a>]

<h2>Adversarially Robust Classification based on GLRT. (arXiv:2011.07835v1 [stat.ML])</h2>
<h3>Bhagyashree Puranik, Upamanyu Madhow, Ramtin Pedarsani</h3>
<p>Machine learning models are vulnerable to adversarial attacks that can often
cause misclassification by introducing small but well designed perturbations.
In this paper, we explore, in the setting of classical composite hypothesis
testing, a defense strategy based on the generalized likelihood ratio test
(GLRT), which jointly estimates the class of interest and the adversarial
perturbation. We evaluate the GLRT approach for the special case of binary
hypothesis testing in white Gaussian noise under $\ell_{\infty}$ norm-bounded
adversarial perturbations, a setting for which a minimax strategy optimizing
for the worst-case attack is known. We show that the GLRT approach yields
performance competitive with that of the minimax approach under the worst-case
attack, and observe that it yields a better robustness-accuracy trade-off under
weaker attacks, depending on the values of signal components relative to the
attack budget. We also observe that the GLRT defense generalizes naturally to
more complex models for which optimal minimax classifiers are not known.
</p>
<a href="http://arxiv.org/abs/2011.07835" target="_blank">arXiv:2011.07835</a> [<a href="http://arxiv.org/pdf/2011.07835" target="_blank">pdf</a>]

<h2>Cluster-Specific Predictions with Multi-Task Gaussian Processes. (arXiv:2011.07866v1 [cs.LG])</h2>
<h3>Arthur Leroy, Pierre Latouche, Benjamin Guedj, Servane Gey</h3>
<p>A model involving Gaussian processes (GPs) is introduced to simultaneously
handle multi-task learning, clustering, and prediction for multiple functional
data. This procedure acts as a model-based clustering method for functional
data as well as a learning step for subsequent predictions for new tasks. The
model is instantiated as a mixture of multi-task GPs with common mean
processes. A variational EM algorithm is derived for dealing with the
optimisation of the hyper-parameters along with the hyper-posteriors'
estimation of latent variables and processes. We establish explicit formulas
for integrating the mean processes and the latent clustering variables within a
predictive distribution, accounting for uncertainty on both aspects. This
distribution is defined as a mixture of cluster-specific GP predictions, which
enhances the performances when dealing with group-structured data. The model
handles irregular grid of observations and offers different hypotheses on the
covariance structure for sharing additional information across tasks. The
performances on both clustering and prediction tasks are assessed through
various simulated scenarios and real datasets. The overall algorithm, called
MagmaClust, is publicly available as an R package.
</p>
<a href="http://arxiv.org/abs/2011.07866" target="_blank">arXiv:2011.07866</a> [<a href="http://arxiv.org/pdf/2011.07866" target="_blank">pdf</a>]

<h2>A Survey on the Explainability of Supervised Machine Learning. (arXiv:2011.07876v1 [cs.LG])</h2>
<h3>Nadia Burkart, Marco F. Huber</h3>
<p>Predictions obtained by, e.g., artificial neural networks have a high
accuracy but humans often perceive the models as black boxes. Insights about
the decision making are mostly opaque for humans. Particularly understanding
the decision making in highly sensitive areas such as healthcare or fifinance,
is of paramount importance. The decision-making behind the black boxes requires
it to be more transparent, accountable, and understandable for humans. This
survey paper provides essential definitions, an overview of the different
principles and methodologies of explainable Supervised Machine Learning (SML).
We conduct a state-of-the-art survey that reviews past and recent explainable
SML approaches and classifies them according to the introduced definitions.
Finally, we illustrate principles by means of an explanatory case study and
discuss important future directions.
</p>
<a href="http://arxiv.org/abs/2011.07876" target="_blank">arXiv:2011.07876</a> [<a href="http://arxiv.org/pdf/2011.07876" target="_blank">pdf</a>]

<h2>Regularized Mutual Information Neural Estimation. (arXiv:2011.07932v1 [cs.LG])</h2>
<h3>Kwanghee Choi, Siyeong Lee</h3>
<p>With the variational lower bound of mutual information (MI), the estimation
of MI can be understood as an optimization task via stochastic gradient
descent. In this work, we start by showing how Mutual Information Neural
Estimator (MINE) searches for the optimal function $T$ that maximizes the
Donsker-Varadhan representation. With our synthetic dataset, we directly
observe the neural network outputs during the optimization to investigate why
MINE succeeds or fails: We discover the drifting phenomenon, where the constant
term of $T$ is shifting through the optimization process, and analyze the
instability caused by the interaction between the $logsumexp$ and the
insufficient batch size. Next, through theoretical and experimental evidence,
we propose a novel lower bound that effectively regularizes the neural network
to alleviate the problems of MINE. We also introduce an averaging strategy that
produces an unbiased estimate by utilizing multiple batches to mitigate the
batch size limitation. Finally, we show that $L^2$ regularization achieves
significant improvements in both discrete and continuous settings.
</p>
<a href="http://arxiv.org/abs/2011.07932" target="_blank">arXiv:2011.07932</a> [<a href="http://arxiv.org/pdf/2011.07932" target="_blank">pdf</a>]

<h2>Corrupted Contextual Bandits with Action Order Constraints. (arXiv:2011.07989v1 [cs.LG])</h2>
<h3>Alexander Galozy, Slawomir Nowaczyk, Mattias Ohlsson</h3>
<p>We consider a variant of the novel contextual bandit problem with corrupted
context, which we call the contextual bandit problem with corrupted context and
action correlation, where actions exhibit a relationship structure that can be
exploited to guide the exploration of viable next decisions. Our setting is
primarily motivated by adaptive mobile health interventions and related
applications, where users might transitions through different stages requiring
more targeted action selection approaches. In such settings, keeping user
engagement is paramount for the success of interventions and therefore it is
vital to provide relevant recommendations in a timely manner. The context
provided by users might not always be informative at every decision point and
standard contextual approaches to action selection will incur high regret. We
propose a meta-algorithm using a referee that dynamically combines the policies
of a contextual bandit and multi-armed bandit, similar to previous work, as
wells as a simple correlation mechanism that captures action to action
transition probabilities allowing for more efficient exploration of
time-correlated actions. We evaluate empirically the performance of said
algorithm on a simulation where the sequence of best actions is determined by a
hidden state that evolves in a Markovian manner. We show that the proposed
meta-algorithm improves upon regret in situations where the performance of both
policies varies such that one is strictly superior to the other for a given
time period. To demonstrate that our setting has relevant practical
applicability, we evaluate our method on several real world data sets, clearly
showing better empirical performance compared to a set of simple algorithms.
</p>
<a href="http://arxiv.org/abs/2011.07989" target="_blank">arXiv:2011.07989</a> [<a href="http://arxiv.org/pdf/2011.07989" target="_blank">pdf</a>]

<h2>Risk-Constrained Thompson Sampling for CVaR Bandits. (arXiv:2011.08046v1 [cs.LG])</h2>
<h3>Joel Q. L. Chang, Qiuyu Zhu, Vincent Y. F. Tan</h3>
<p>The multi-armed bandit (MAB) problem is a ubiquitous decision-making problem
that exemplifies the exploration-exploitation tradeoff. Standard formulations
exclude risk in decision making. Risk notably complicates the basic
reward-maximising objective, in part because there is no universally agreed
definition of it. In this paper, we consider a popular risk measure in
quantitative finance known as the Conditional Value at Risk (CVaR). We explore
the performance of a Thompson Sampling-based algorithm CVaR-TS under this risk
measure. We provide comprehensive comparisons between our regret bounds with
state-of-the-art L/UCB-based algorithms in comparable settings and demonstrate
their clear improvement in performance. We also include numerical simulations
to empirically verify that CVaR-TS outperforms other L/UCB-based algorithms.
</p>
<a href="http://arxiv.org/abs/2011.08046" target="_blank">arXiv:2011.08046</a> [<a href="http://arxiv.org/pdf/2011.08046" target="_blank">pdf</a>]

<h2>A comparative study of semi- and self-supervised semantic segmentation of biomedical microscopy data. (arXiv:2011.08076v1 [cs.CV])</h2>
<h3>Nastassya Horlava, Alisa Mironenko, Sebastian Niehaus, Sebastian Wagner, Ingo Roeder, Nico Scherf</h3>
<p>In recent years, Convolutional Neural Networks (CNNs) have become the
state-of-the-art method for biomedical image analysis. However, these networks
are usually trained in a supervised manner, requiring large amounts of labelled
training data. These labelled data sets are often difficult to acquire in the
biomedical domain. In this work, we validate alternative ways to train CNNs
with fewer labels for biomedical image segmentation using. We adapt two semi-
and self-supervised image classification methods and analyse their performance
for semantic segmentation of biomedical microscopy images.
</p>
<a href="http://arxiv.org/abs/2011.08076" target="_blank">arXiv:2011.08076</a> [<a href="http://arxiv.org/pdf/2011.08076" target="_blank">pdf</a>]

<h2>Combining GANs and AutoEncoders for Efficient Anomaly Detection. (arXiv:2011.08102v1 [cs.CV])</h2>
<h3>Fabio Carrara (1), Giuseppe Amato (1), Luca Brombin, Fabrizio Falchi (1), Claudio Gennaro (1) ((1) ISTI CNR, Pisa, Italy)</h3>
<p>Deep learned models are now largely adopted in different fields, and they
generally provide superior performances with respect to classical signal-based
approaches. Notwithstanding this, their actual reliability when working in an
unprotected environment is far enough to be proven. In this work, we consider a
novel deep neural network architecture, named Neural Ordinary Differential
Equations (N-ODE), that is getting particular attention due to an attractive
property --- a test-time tunable trade-off between accuracy and efficiency.
This paper analyzes the robustness of N-ODE image classifiers when faced
against a strong adversarial attack and how its effectiveness changes when
varying such a tunable trade-off. We show that adversarial robustness is
increased when the networks operate in different tolerance regimes during test
time and training time. On this basis, we propose a novel adversarial detection
strategy for N-ODE nets based on the randomization of the adaptive ODE solver
tolerance. Our evaluation performed on standard image classification benchmarks
shows that our detection technique provides high rejection of adversarial
examples while maintaining most of the original samples under white-box attacks
and zero-knowledge adversaries.
</p>
<a href="http://arxiv.org/abs/2011.08102" target="_blank">arXiv:2011.08102</a> [<a href="http://arxiv.org/pdf/2011.08102" target="_blank">pdf</a>]

<h2>Coarse-grained and emergent distributed parameter systems from data. (arXiv:2011.08138v1 [stat.ML])</h2>
<h3>Hassan Arbabi, Felix P. Kemeth, Tom Bertalan, Ioannis Kevrekidis</h3>
<p>We explore the derivation of distributed parameter system evolution laws (and
in particular, partial differential operators and associated partial
differential equations, PDEs) from spatiotemporal data. This is, of course, a
classical identification problem; our focus here is on the use of manifold
learning techniques (and, in particular, variations of Diffusion Maps) in
conjunction with neural network learning algorithms that allow us to attempt
this task when the dependent variables, and even the independent variables of
the PDE are not known a priori and must be themselves derived from the data.
The similarity measure used in Diffusion Maps for dependent coarse variable
detection involves distances between local particle distribution observations;
for independent variable detection we use distances between local short-time
dynamics. We demonstrate each approach through an illustrative established PDE
example. Such variable-free, emergent space identification algorithms connect
naturally with equation-free multiscale computation tools.
</p>
<a href="http://arxiv.org/abs/2011.08138" target="_blank">arXiv:2011.08138</a> [<a href="http://arxiv.org/pdf/2011.08138" target="_blank">pdf</a>]

