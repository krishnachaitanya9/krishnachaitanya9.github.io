---
title: Latest Deep Learning Papers
date: 2021-03-07 21:00:01 +0800
featured-img: DL-logo
categories: [Deep_Learning]
tags: [Deep_Learning]
mathjax: Yes
---

<h1>Your arXiv Feed (96 Articles)</h1>
<h2>A Deep Learning Approach to Mapping Irrigation: IrrMapper-U-Net. (arXiv:2103.03278v1 [cs.CV])</h2>
<h3>Thomas Colligan, David Ketchum, Douglas Brinkerhoff, Marco Maneta</h3>
<p>Accurate maps of irrigation are essential for understanding and managing
water resources. We present a new method of mapping irrigation and demonstrate
its accuracy for the state of Montana from years 2000-2019. The method is based
off of an ensemble of convolutional neural networks that use reflectance
information from Landsat imagery to classify irrigated pixels, that we call
IrrMapper-U-Net. The methodology does not rely on extensive feature engineering
and does not condition the classification with land use information from
existing geospatial datasets. The ensemble does not need exhaustive
hyperparameter tuning and the analysis pipeline is lightweight enough to be
implemented on a personal computer. Furthermore, the proposed methodology
provides an estimate of the uncertainty associated with classification. We
evaluated our methodology and the resulting irrigation maps using a highly
accurate novel spatially-explicit ground truth data set, using county-scale
USDA surveys of irrigation extent, and using cadastral surveys. We found that
that our method outperforms other methods of mapping irrigation in Montana in
terms of overall accuracy and precision. We found that our method agrees better
statewide with the USDA National Agricultural Statistics Survey estimates of
irrigated area compared to other methods, and has far fewer errors of
commission in rainfed agriculture areas. The method learns to mask clouds and
ignore Landsat 7 scan-line failures without supervision, reducing the need for
preprocessing data. This methodology has the potential to be applied across the
entire United States and for the complete Landsat record.
</p>
<a href="http://arxiv.org/abs/2103.03278" target="_blank">arXiv:2103.03278</a> [<a href="http://arxiv.org/pdf/2103.03278" target="_blank">pdf</a>]

<h2>Tensor-Free Second-Order Differential Dynamic Programming. (arXiv:2103.03293v1 [cs.RO])</h2>
<h3>John N. Nganga, Patrick M. Wensing</h3>
<p>This paper presents a method to reduce the computational complexity of
including second-order dynamics sensitivity information into the Differential
Dynamic Programming (DDP) trajectory optimization algorithm. A tensor-free
approach to DDP is developed where all the necessary derivatives are computed
with the same complexity as in the iterative Linear Quadratic Regulator~(iLQR).
Compared to linearized models used in iLQR, DDP more accurately represents the
dynamics locally, but it is not often used since the second-order derivatives
of the dynamics are tensorial and expensive to compute. This work shows how to
avoid the need for computing the derivative tensor by instead leveraging
reverse-mode accumulation of derivative information to compute a key
vector-tensor product directly. We benchmark this approach for trajectory
optimization with multi-link manipulators and show that the benefits of DDP can
often be included without sacrificing evaluation time, and can be done in fewer
iterations than iLQR.
</p>
<a href="http://arxiv.org/abs/2103.03293" target="_blank">arXiv:2103.03293</a> [<a href="http://arxiv.org/pdf/2103.03293" target="_blank">pdf</a>]

<h2>Learning High Fidelity Depths of Dressed Humans by Watching Social Media Dance Videos. (arXiv:2103.03319v1 [cs.CV])</h2>
<h3>Yasamin Jafarian, Hyun Soo Park</h3>
<p>A key challenge of learning the geometry of dressed humans lies in the
limited availability of the ground truth data (e.g., 3D scanned models), which
results in the performance degradation of 3D human reconstruction when applying
to real-world imagery. We address this challenge by leveraging a new data
resource: a number of social media dance videos that span diverse appearance,
clothing styles, performances, and identities. Each video depicts dynamic
movements of the body and clothes of a single person while lacking the 3D
ground truth geometry. To utilize these videos, we present a new method to use
the local transformation that warps the predicted local geometry of the person
from an image to that of another image at a different time instant. This allows
self-supervision as enforcing a temporal coherence over the predictions. In
addition, we jointly learn the depth along with the surface normals that are
highly responsive to local texture, wrinkle, and shade by maximizing their
geometric consistency. Our method is end-to-end trainable, resulting in high
fidelity depth estimation that predicts fine geometry faithful to the input
real image. We demonstrate that our method outperforms the state-of-the-art
human depth estimation and human shape recovery approaches on both real and
rendered images.
</p>
<a href="http://arxiv.org/abs/2103.03319" target="_blank">arXiv:2103.03319</a> [<a href="http://arxiv.org/pdf/2103.03319" target="_blank">pdf</a>]

<h2>Evaluation of Complexity Measures for Deep Learning Generalization in Medical Image Analysis. (arXiv:2103.03328v1 [cs.CV])</h2>
<h3>Aleksandar Vakanski, Min Xian</h3>
<p>The generalization error of deep learning models for medical image analysis
often decreases on images collected with different devices for data
acquisition, device settings, or patient population. A better understanding of
the generalization capacity on new images is crucial for clinicians'
trustworthiness in deep learning. Although significant research efforts have
been recently directed toward establishing generalization bounds and complexity
measures, still, there is often a significant discrepancy between the predicted
and actual generalization performance. As well, related large empirical studies
have been primarily based on validation with general-purpose image datasets.
This paper presents an empirical study that investigates the correlation
between 25 complexity measures and the generalization abilities of supervised
deep learning classifiers for breast ultrasound images. The results indicate
that PAC-Bayes flatness-based and path norm-based measures produce the most
consistent explanation for the combination of models and data. We also
investigate the use of multi-task classification and segmentation approach for
breast images, and report that such learning approach acts as an implicit
regularizer and is conducive toward improved generalization.
</p>
<a href="http://arxiv.org/abs/2103.03328" target="_blank">arXiv:2103.03328</a> [<a href="http://arxiv.org/pdf/2103.03328" target="_blank">pdf</a>]

<h2>Differential Flatness as a Sufficient Condition to Generate Optimal Trajectories in Real Time. (arXiv:2103.03339v1 [cs.RO])</h2>
<h3>Logan E. Beaver, Michael Dorothy, Christopher Kroninger, Andreas A. Malikopoulos</h3>
<p>As robotic systems increase in autonomy, there is a strong need to plan
efficient trajectories in real-time. In this paper, we propose an approach to
significantly reduce the complexity of solving optimal control problems both
numerically and analytically. We exploit the property of differential flatness
to show that it is always possible to decouple the forward dynamics of the
system's state from the backward dynamics that emerge from the Euler-Lagrange
equations. This coupling generally leads to instabilities in numerical
approaches; thus, we expect our method to make traditional "shooting" methods a
viable choice for optimal trajectory planning in differentially flat systems.
To provide intuition for our approach, we also present an illustrative example
of generating minimum-thrust trajectories for a quadrotor. Furthermore, we
employ quaternions to track the quadrotor's orientation, which, unlike the
Euler-angle representation, do not introduce additional singularities into the
model.
</p>
<a href="http://arxiv.org/abs/2103.03339" target="_blank">arXiv:2103.03339</a> [<a href="http://arxiv.org/pdf/2103.03339" target="_blank">pdf</a>]

<h2>Nutrition5k: Towards Automatic Nutritional Understanding of Generic Food. (arXiv:2103.03375v1 [cs.CV])</h2>
<h3>Quin Thames, Arjun Karpur, Wade Norris, Fangting Xia, Liviu Panait, Tobias Weyand, Jack Sim</h3>
<p>Understanding the nutritional content of food from visual data is a
challenging computer vision problem, with the potential to have a positive and
widespread impact on public health. Studies in this area are limited to
existing datasets in the field that lack sufficient diversity or labels
required for training models with nutritional understanding capability. We
introduce Nutrition5k, a novel dataset of 5k diverse, real world food dishes
with corresponding video streams, depth images, component weights, and high
accuracy nutritional content annotation. We demonstrate the potential of this
dataset by training a computer vision algorithm capable of predicting the
caloric and macronutrient values of a complex, real world dish at an accuracy
that outperforms professional nutritionists. Further we present a baseline for
incorporating depth sensor data to improve nutrition predictions. We will
publicly release Nutrition5k in the hope that it will accelerate innovation in
the space of nutritional understanding.
</p>
<a href="http://arxiv.org/abs/2103.03375" target="_blank">arXiv:2103.03375</a> [<a href="http://arxiv.org/pdf/2103.03375" target="_blank">pdf</a>]

<h2>PolarNet: Accelerated Deep Open Space Segmentation Using Automotive Radar in Polar Domain. (arXiv:2103.03387v1 [cs.CV])</h2>
<h3>Farzan Erlik Nowruzi, Dhanvin Kolhatkar, Prince Kapoor, Elnaz Jahani Heravi, Fahed Al Hassanat, Robert Laganiere, Julien Rebut, Waqas Malik</h3>
<p>Camera and Lidar processing have been revolutionized with the rapid
development of deep learning model architectures. Automotive radar is one of
the crucial elements of automated driver assistance and autonomous driving
systems. Radar still relies on traditional signal processing techniques, unlike
camera and Lidar based methods. We believe this is the missing link to achieve
the most robust perception system. Identifying drivable space and occupied
space is the first step in any autonomous decision making task. Occupancy grid
map representation of the environment is often used for this purpose. In this
paper, we propose PolarNet, a deep neural model to process radar information in
polar domain for open space segmentation. We explore various input-output
representations. Our experiments show that PolarNet is a effective way to
process radar data that achieves state-of-the-art performance and processing
speeds while maintaining a compact size.
</p>
<a href="http://arxiv.org/abs/2103.03387" target="_blank">arXiv:2103.03387</a> [<a href="http://arxiv.org/pdf/2103.03387" target="_blank">pdf</a>]

<h2>Limits of Probabilistic Safety Guarantees when Considering Human Uncertainty. (arXiv:2103.03388v1 [cs.RO])</h2>
<h3>Richard Cheng, Richard M. Murray, Joel W. Burdick</h3>
<p>When autonomous robots interact with humans, such as during autonomous
driving, explicit safety guarantees are crucial in order to avoid potentially
life-threatening accidents. Many data-driven methods have explored learning
probabilistic bounds over human agents' trajectories (i.e. confidence tubes
that contain trajectories with probability $\delta$), which can then be used to
guarantee safety with probability $1-\delta$. However, almost all existing
works consider $\delta \geq 0.001$. The purpose of this paper is to argue that
(1) in safety-critical applications, it is necessary to provide safety
guarantees with $\delta &lt; 10^{-8}$, and (2) current learning-based methods are
ill-equipped to compute accurate confidence bounds at such low $\delta$. Using
human driving data (from the highD dataset), as well as synthetically generated
data, we show that current uncertainty models use inaccurate distributional
assumptions to describe human behavior and/or require infeasible amounts of
data to accurately learn confidence bounds for $\delta \leq 10^{-8}$. These two
issues result in unreliable confidence bounds, which can have dangerous
implications if deployed on safety-critical systems.
</p>
<a href="http://arxiv.org/abs/2103.03388" target="_blank">arXiv:2103.03388</a> [<a href="http://arxiv.org/pdf/2103.03388" target="_blank">pdf</a>]

<h2>An Analytical Solution to the IMU Initialization Problem for Visual-Inertial Systems. (arXiv:2103.03389v1 [cs.RO])</h2>
<h3>David Zu&#xf1;iga-No&#xeb;l, Francisco-Angel Moreno, Javier Gonzalez-Jimenez</h3>
<p>The fusion of visual and inertial measurements is becoming more and more
popular in the robotics community since both sources of information complement
well each other. However, in order to perform this fusion, the biases of the
Inertial Measurement Unit (IMU) as well as the direction of gravity must be
initialized first. Additionally, in case of a monocular camera, the metric
scale is also needed. The most popular visual-inertial initialization
approaches rely on accurate vision-only motion estimates to build a non-linear
optimization problem that solves for these parameters in an iterative way. In
this paper, we rely on the previous work in [1] and propose an analytical
solution to estimate the accelerometer bias, the direction of gravity and the
scale factor in a maximum-likelihood framework. This formulation results in a
very efficient estimation approach and, due to the non-iterative nature of the
solution, avoids the intrinsic issues of previous iterative solutions. We
present an extensive validation of the proposed IMU initialization approach and
a performance comparison against the state-of-the-art approach described in [2]
with real data from the publicly available EuRoC dataset, achieving comparable
accuracy at a fraction of its computational cost and without requiring an
initial guess for the scale factor. We also provide a C++ open source reference
implementation.
</p>
<a href="http://arxiv.org/abs/2103.03389" target="_blank">arXiv:2103.03389</a> [<a href="http://arxiv.org/pdf/2103.03389" target="_blank">pdf</a>]

<h2>An Effective Loss Function for Generating 3D Models from Single 2D Image without Rendering. (arXiv:2103.03390v1 [cs.CV])</h2>
<h3>Nikola Zubi&#x107;, Pietro Li&#xf2;</h3>
<p>Differentiable rendering is a very successful technique that applies to a
Single-View 3D Reconstruction. Current renderers use losses based on pixels
between a rendered image of some 3D reconstructed object and ground-truth
images from given matched viewpoints to optimise parameters of the 3D shape.

These models require a rendering step, along with visibility handling and
evaluation of the shading model. The main goal of this paper is to demonstrate
that we can avoid these steps and still get reconstruction results as other
state-of-the-art models that are equal or even better than existing
category-specific reconstruction methods. First, we use the same CNN
architecture for the prediction of a point cloud shape and pose prediction like
the one used by Insafutdinov \&amp; Dosovitskiy. Secondly, we propose the novel
effective loss function that evaluates how well the projections of
reconstructed 3D point clouds cover the ground truth object's silhouette. Then
we use Poisson Surface Reconstruction to transform the reconstructed point
cloud into a 3D mesh. Finally, we perform a GAN-based texture mapping on a
particular 3D mesh and produce a textured 3D mesh from a single 2D image. We
evaluate our method on different datasets (including ShapeNet, CUB-200-2011,
and Pascal3D+) and achieve state-of-the-art results, outperforming all the
other supervised and unsupervised methods and 3D representations, all in terms
of performance, accuracy, and training time.
</p>
<a href="http://arxiv.org/abs/2103.03390" target="_blank">arXiv:2103.03390</a> [<a href="http://arxiv.org/pdf/2103.03390" target="_blank">pdf</a>]

<h2>Point Cloud based Hierarchical Deep Odometry Estimation. (arXiv:2103.03394v1 [cs.CV])</h2>
<h3>Farzan Erlik Nowruzi, Dhanvin Kolhatkar, Prince Kapoor, Robert Laganiere</h3>
<p>Processing point clouds using deep neural networks is still a challenging
task. Most existing models focus on object detection and registration with deep
neural networks using point clouds. In this paper, we propose a deep model that
learns to estimate odometry in driving scenarios using point cloud data. The
proposed model consumes raw point clouds in order to extract frame-to-frame
odometry estimation through a hierarchical model architecture. Also, a local
bundle adjustment variation of this model using LSTM layers is implemented.
These two approaches are comprehensively evaluated and are compared against the
state-of-the-art.
</p>
<a href="http://arxiv.org/abs/2103.03394" target="_blank">arXiv:2103.03394</a> [<a href="http://arxiv.org/pdf/2103.03394" target="_blank">pdf</a>]

<h2>Rover Relocalization for Mars Sample Return by Virtual Template Synthesis and Matching. (arXiv:2103.03395v1 [cs.RO])</h2>
<h3>Tu-Hoa Pham, William Seto, Shreyansh Daftry, Barry Ridge, Johanna Hansen, Tristan Thrush, Mark Van der Merwe, Gerard Maggiolino, Alexander Brinkman, John Mayo, Yang Cheng, Curtis Padgett, Eric Kulczycki, Renaud Detry</h3>
<p>We consider the problem of rover relocalization in the context of the
notional Mars Sample Return campaign. In this campaign, a rover (R1) needs to
be capable of autonomously navigating and localizing itself within an area of
approximately 50 x 50 m using reference images collected years earlier by
another rover (R0). We propose a visual localizer that exhibits robustness to
the relatively barren terrain that we expect to find in relevant areas, and to
large lighting and viewpoint differences between R0 and R1. The localizer
synthesizes partial renderings of a mesh built from reference R0 images and
matches those to R1 images. We evaluate our method on a dataset totaling 2160
images covering the range of expected environmental conditions (terrain,
lighting, approach angle). Experimental results show the effectiveness of our
approach. This work informs the Mars Sample Return campaign on the choice of a
site where Perseverance (R0) will place a set of sample tubes for future
retrieval by another rover (R1).
</p>
<a href="http://arxiv.org/abs/2103.03395" target="_blank">arXiv:2103.03395</a> [<a href="http://arxiv.org/pdf/2103.03395" target="_blank">pdf</a>]

<h2>Measuring Model Biases in the Absence of Ground Truth. (arXiv:2103.03417v1 [cs.CV])</h2>
<h3>Osman Aka, Ken Burke, Alex B&#xe4;uerle, Christina Greer, Margaret Mitchell</h3>
<p>Recent advances in computer vision have led to the development of image
classification models that can predict tens of thousands of object classes.
Training these models can require millions of examples, leading to a demand of
potentially billions of annotations. In practice, however, images are typically
sparsely annotated, which can lead to problematic biases in the distribution of
ground truth labels that are collected. This potential for annotation bias may
then limit the utility of ground truth-dependent fairness metrics (e.g.,
Equalized Odds). To address this problem, in this work we introduce a new
framing to the measurement of fairness and bias that does not rely on ground
truth labels. Instead, we treat the model predictions for a given image as a
set of labels, analogous to a 'bag of words' approach used in Natural Language
Processing (NLP). This allows us to explore different association metrics
between prediction sets in order to detect patterns of bias. We apply this
approach to examine the relationship between identity labels, and all other
labels in the dataset, using labels associated with 'male' and 'female') as a
concrete example. We demonstrate how the statistical properties (especially
normalization) of the different association metrics can lead to different sets
of labels detected as having "gender bias". We conclude by demonstrating that
pointwise mutual information normalized by joint probability (nPMI) is able to
detect many labels with significant gender bias despite differences in the
labels' marginal frequencies. Finally, we announce an open-sourced nPMI
visualization tool using TensorBoard.
</p>
<a href="http://arxiv.org/abs/2103.03417" target="_blank">arXiv:2103.03417</a> [<a href="http://arxiv.org/pdf/2103.03417" target="_blank">pdf</a>]

<h2>Compact pneumatic clutch with integrated stiffness variation and position feedback. (arXiv:2103.03422v1 [cs.RO])</h2>
<h3>Yongkang Jiang, Junlin Ma, Diansheng Chen, Jamie Paik</h3>
<p>Stiffness variation and real-time position feedback are critical for any
robotic system but most importantly for active and wearable devices to interact
with the user and environment. Currently, for compact sizes, there is a lack of
solutions bringing high-fidelity feedback and maintaining design and functional
integrity. In this work, we propose a novel minimal clutch with integrated
stiffness variation and real-time position feedback whose performance surpasses
conventional jamming solutions. We introduce integrated design, modeling, and
verification of the clutch in detail. Preliminary experimental results show the
change in impedance force of the clutch is close to 24-fold at the maximum
force density of 15.64 N/cm2. We validated the clutch experimentally in (1)
enhancing the bending stiffness of a soft actuator to increase a soft
manipulator's gripping force by 73%; (2) enabling a soft cylindrical actuator
to execute omnidirectional movement; (3) providing real-time position feedback
for hand posture detection and impedance force for kinesthetic haptic feedback.
This manuscript presents the functional components with a focus on the
integrated design methodology, which will have an impact on the development of
soft robots and wearable devices.
</p>
<a href="http://arxiv.org/abs/2103.03422" target="_blank">arXiv:2103.03422</a> [<a href="http://arxiv.org/pdf/2103.03422" target="_blank">pdf</a>]

<h2>Constrained Contrastive Distribution Learning for Unsupervised Anomaly Detection and Localisation in Medical Images. (arXiv:2103.03423v1 [cs.CV])</h2>
<h3>Yu Tian, Guansong Pang, Fengbei Liu, Yuanhong chen, Seon Ho Shin, Johan W. Verjans, Rajvinder Singh, Gustavo Carneiro</h3>
<p>Unsupervised anomaly detection (UAD) learns one-class classifiers exclusively
with normal (i.e., healthy) images to detect any abnormal (i.e., unhealthy)
samples that do not conform to the expected normal patterns. UAD has two main
advantages over its fully supervised counterpart. Firstly, it is able to
directly leverage large datasets available from health screening programs that
contain mostly normal image samples, avoiding the costly manual labelling of
abnormal samples and the subsequent issues involved in training with extremely
class-imbalanced data. Further, UAD approaches can potentially detect and
localise any type of lesions that deviate from the normal patterns. One
significant challenge faced by UAD methods is how to learn effective
low-dimensional image representations to detect and localise subtle
abnormalities, generally consisting of small lesions. To address this
challenge, we propose a novel self-supervised representation learning method,
called Constrained Contrastive Distribution learning for anomaly detection
(CCD), which learns fine-grained feature representations by simultaneously
predicting the distribution of augmented data and image contexts using
contrastive learning with pretext constraints. The learned representations can
be leveraged to train more anomaly-sensitive detection models. Extensive
experiment results show that our method outperforms current state-of-the-art
UAD approaches on three different colonoscopy and fundus screening datasets.
Our code is available at https://github.com/tianyu0207/CCD.
</p>
<a href="http://arxiv.org/abs/2103.03423" target="_blank">arXiv:2103.03423</a> [<a href="http://arxiv.org/pdf/2103.03423" target="_blank">pdf</a>]

<h2>Goal-Oriented Gaze Estimation for Zero-Shot Learning. (arXiv:2103.03433v1 [cs.CV])</h2>
<h3>Yang Liu, Lei Zhou, Xiao Bai, Yifei Huang, Lin Gu, Jun Zhou, Tatsuya Harada</h3>
<p>Zero-shot learning (ZSL) aims to recognize novel classes by transferring
semantic knowledge from seen classes to unseen classes. Since semantic
knowledge is built on attributes shared between different classes, which are
highly local, strong prior for localization of object attribute is beneficial
for visual-semantic embedding. Interestingly, when recognizing unseen images,
human would also automatically gaze at regions with certain semantic clue.
Therefore, we introduce a novel goal-oriented gaze estimation module (GEM) to
improve the discriminative attribute localization based on the class-level
attributes for ZSL. We aim to predict the actual human gaze location to get the
visual attention regions for recognizing a novel object guided by attribute
description. Specifically, the task-dependent attention is learned with the
goal-oriented GEM, and the global image features are simultaneously optimized
with the regression of local attribute features. Experiments on three ZSL
benchmarks, i.e., CUB, SUN and AWA2, show the superiority or competitiveness of
our proposed method against the state-of-the-art ZSL methods. The ablation
analysis on real gaze data CUB-VWSW also validates the benefits and accuracy of
our gaze estimation module. This work implies the promising benefits of
collecting human gaze dataset and automatic gaze estimation algorithms on
high-level computer vision tasks. The code is available at
https://github.com/osierboy/GEM-ZSL.
</p>
<a href="http://arxiv.org/abs/2103.03433" target="_blank">arXiv:2103.03433</a> [<a href="http://arxiv.org/pdf/2103.03433" target="_blank">pdf</a>]

<h2>Implicit Integration of Superpixel Segmentation into Fully Convolutional Networks. (arXiv:2103.03435v1 [cs.CV])</h2>
<h3>Teppei Suzuki</h3>
<p>Superpixels are a useful representation to reduce the complexity of image
data. However, to combine superpixels with convolutional neural networks (CNNs)
in an end-to-end fashion, one requires extra models to generate superpixels and
special operations such as graph convolution. In this paper, we propose a way
to implicitly integrate a superpixel scheme into CNNs, which makes it easy to
use superpixels with CNNs in an end-to-end fashion. Our proposed method
hierarchically groups pixels at downsampling layers and generates superpixels.
Our method can be plugged into many existing architectures without a change in
their feed-forward path because our method does not use superpixels in the
feed-forward path but use them to recover the lost resolution instead of
bilinear upsampling. As a result, our method preserves detailed information
such as object boundaries in the form of superpixels even when the model
contains downsampling layers. We evaluate our method on several tasks such as
semantic segmentation, superpixel segmentation, and monocular depth estimation,
and confirm that it speeds up modern architectures and/or improves their
prediction accuracy in these tasks.
</p>
<a href="http://arxiv.org/abs/2103.03435" target="_blank">arXiv:2103.03435</a> [<a href="http://arxiv.org/pdf/2103.03435" target="_blank">pdf</a>]

<h2>Towards Evaluating the Robustness of Deep Diagnostic Models by Adversarial Attack. (arXiv:2103.03438v1 [cs.CV])</h2>
<h3>Mengting Xu, Tao Zhang, Zhongnian Li, Mingxia Liu, Daoqiang Zhang</h3>
<p>Deep learning models (with neural networks) have been widely used in
challenging tasks such as computer-aided disease diagnosis based on medical
images. Recent studies have shown deep diagnostic models may not be robust in
the inference process and may pose severe security concerns in clinical
practice. Among all the factors that make the model not robust, the most
serious one is adversarial examples. The so-called "adversarial example" is a
well-designed perturbation that is not easily perceived by humans but results
in a false output of deep diagnostic models with high confidence. In this
paper, we evaluate the robustness of deep diagnostic models by adversarial
attack. Specifically, we have performed two types of adversarial attacks to
three deep diagnostic models in both single-label and multi-label
classification tasks, and found that these models are not reliable when
attacked by adversarial example. We have further explored how adversarial
examples attack the models, by analyzing their quantitative classification
results, intermediate features, discriminability of features and correlation of
estimated labels for both original/clean images and those adversarial ones. We
have also designed two new defense methods to handle adversarial examples in
deep diagnostic models, i.e., Multi-Perturbations Adversarial Training (MPAdvT)
and Misclassification-Aware Adversarial Training (MAAdvT). The experimental
results have shown that the use of defense methods can significantly improve
the robustness of deep diagnostic models against adversarial attacks.
</p>
<a href="http://arxiv.org/abs/2103.03438" target="_blank">arXiv:2103.03438</a> [<a href="http://arxiv.org/pdf/2103.03438" target="_blank">pdf</a>]

<h2>Haptic Feedback Improves Human-Robot Agreement and User Satisfaction in Shared-Autonomy Teleoperation. (arXiv:2103.03453v1 [cs.RO])</h2>
<h3>Dawei Zhang, Roberto Tron, Rebecca P.Khurshid</h3>
<p>Shared autonomy teleoperation can guarantee safety, but does so by reducing
the human operator's control authority, which can lead to reduced levels of
human-robot agreement and user satisfaction. This paper presents a novel haptic
shared autonomy teleoperation paradigm that uses haptic feedback to inform the
user about the inner state of a shared autonomy paradigm, while still
guaranteeing safety. This differs from haptic shared control, which uses haptic
feedback to inform the user's actions, but gives the human operator full
control over the robot's actions. We conducted a user study in which twelve
users flew a simulated UAV in a search-and-rescue task with no assistance or
assistance provided by haptic shared control, shared autonomy, or haptic shared
autonomy. All assistive teleoperation methods use control barrier functions to
find a control command that is both safe and as close as possible to the
human-generated control command. For assistive teleoperation conditions with
haptic feedback, we apply a force to the user that is proportional to the
difference between the human-generated control and the safe control. We find
that haptic shared autonomy improves the user's task performance and
satisfaction. We also find that haptic feedback in assistive teleoperation can
improve the user's situational awareness. Finally, results show that adding
haptic feedback to shared-autonomy teleoperation can improve human-robot
agreement.
</p>
<a href="http://arxiv.org/abs/2103.03453" target="_blank">arXiv:2103.03453</a> [<a href="http://arxiv.org/pdf/2103.03453" target="_blank">pdf</a>]

<h2>Structured Scene Memory for Vision-Language Navigation. (arXiv:2103.03454v1 [cs.CV])</h2>
<h3>Hanqing Wang, Wenguan Wang, Wei Liang, Caiming Xiong, Jianbing Shen</h3>
<p>Recently, numerous algorithms have been developed to tackle the problem of
vision-language navigation (VLN), i.e., entailing an agent to navigate 3D
environments through following linguistic instructions. However, current VLN
agents simply store their past experiences/observations as latent states in
recurrent networks, failing to capture environment layouts and make long-term
planning. To address these limitations, we propose a crucial architecture,
called Structured Scene Memory (SSM). It is compartmentalized enough to
accurately memorize the percepts during navigation. It also serves as a
structured scene representation, which captures and disentangles visual and
geometric cues in the environment. SSM has a collect-read controller that
adaptively collects information for supporting current decision making and
mimics iterative algorithms for long-range reasoning. As SSM provides a
complete action space, i.e., all the navigable places on the map, a
frontier-exploration based navigation decision making strategy is introduced to
enable efficient and global planning. Experiment results on two VLN datasets
(i.e., R2R and R4R) show that our method achieves state-of-the-art performance
on several metrics.
</p>
<a href="http://arxiv.org/abs/2103.03454" target="_blank">arXiv:2103.03454</a> [<a href="http://arxiv.org/pdf/2103.03454" target="_blank">pdf</a>]

<h2>Vicinal and categorical domain adaptation. (arXiv:2103.03460v1 [cs.CV])</h2>
<h3>Hui Tang, Kui Jia</h3>
<p>Unsupervised domain adaptation aims to learn a task classifier that performs
well on the unlabeled target domain, by utilizing the labeled source domain.
Inspiring results have been acquired by learning domain-invariant deep features
via domain-adversarial training. However, its parallel design of task and
domain classifiers limits the ability to achieve a finer category-level domain
alignment. To promote categorical domain adaptation (CatDA), based on a joint
category-domain classifier, we propose novel losses of adversarial training at
both domain and category levels. Since the joint classifier can be regarded as
a concatenation of individual task classifiers respectively for the two
domains, our design principle is to enforce consistency of category predictions
between the two task classifiers. Moreover, we propose a concept of vicinal
domains whose instances are produced by a convex combination of pairs of
instances respectively from the two domains. Intuitively, alignment of the
possibly infinite number of vicinal domains enhances that of original domains.
We propose novel adversarial losses for vicinal domain adaptation (VicDA) based
on CatDA, leading to Vicinal and Categorical Domain Adaptation (ViCatDA). We
also propose Target Discriminative Structure Recovery (TDSR) to recover the
intrinsic target discrimination damaged by adversarial feature alignment. We
also analyze the principles underlying the ability of our key designs to align
the joint distributions. Extensive experiments on several benchmark datasets
demonstrate that we achieve the new state of the art.
</p>
<a href="http://arxiv.org/abs/2103.03460" target="_blank">arXiv:2103.03460</a> [<a href="http://arxiv.org/pdf/2103.03460" target="_blank">pdf</a>]

<h2>Unsupervised Motion Representation Enhanced Network for Action Recognition. (arXiv:2103.03465v1 [cs.CV])</h2>
<h3>Xiaohang Yang, Lingtong Kong, Jie Yang</h3>
<p>Learning reliable motion representation between consecutive frames, such as
optical flow, has proven to have great promotion to video understanding.
However, the TV-L1 method, an effective optical flow solver, is time-consuming
and expensive in storage for caching the extracted optical flow. To fill the
gap, we propose UF-TSN, a novel end-to-end action recognition approach enhanced
with an embedded lightweight unsupervised optical flow estimator. UF-TSN
estimates motion cues from adjacent frames in a coarse-to-fine manner and
focuses on small displacement for each level by extracting pyramid of feature
and warping one to the other according to the estimated flow of the last level.
Due to the lack of labeled motion for action datasets, we constrain the flow
prediction with multi-scale photometric consistency and edge-aware smoothness.
Compared with state-of-the-art unsupervised motion representation learning
methods, our model achieves better accuracy while maintaining efficiency, which
is competitive with some supervised or more complicated approaches.
</p>
<a href="http://arxiv.org/abs/2103.03465" target="_blank">arXiv:2103.03465</a> [<a href="http://arxiv.org/pdf/2103.03465" target="_blank">pdf</a>]

<h2>Teachers Do More Than Teach: Compressing Image-to-Image Models. (arXiv:2103.03467v1 [cs.CV])</h2>
<h3>Qing Jin, Jian Ren, Oliver J. Woodford, Jiazhuo Wang, Geng Yuan, Yanzhi Wang, Sergey Tulyakov</h3>
<p>Generative Adversarial Networks (GANs) have achieved huge success in
generating high-fidelity images, however, they suffer from low efficiency due
to tremendous computational cost and bulky memory usage. Recent efforts on
compression GANs show noticeable progress in obtaining smaller generators by
sacrificing image quality or involving a time-consuming searching process. In
this work, we aim to address these issues by introducing a teacher network that
provides a search space in which efficient network architectures can be found,
in addition to performing knowledge distillation. First, we revisit the search
space of generative models, introducing an inception-based residual block into
generators. Second, to achieve target computation cost, we propose a one-step
pruning algorithm that searches a student architecture from the teacher model
and substantially reduces searching cost. It requires no l1 sparsity
regularization and its associated hyper-parameters, simplifying the training
procedure. Finally, we propose to distill knowledge through maximizing feature
similarity between teacher and student via an index named Global Kernel
Alignment (GKA). Our compressed networks achieve similar or even better image
fidelity (FID, mIoU) than the original models with much-reduced computational
cost, e.g., MACs. Code will be released at
https://github.com/snap-research/CAT.
</p>
<a href="http://arxiv.org/abs/2103.03467" target="_blank">arXiv:2103.03467</a> [<a href="http://arxiv.org/pdf/2103.03467" target="_blank">pdf</a>]

<h2>IAFA: Instance-aware Feature Aggregation for 3D Object Detection from a Single Image. (arXiv:2103.03480v1 [cs.CV])</h2>
<h3>Dingfu Zhou, Xibin Song, Yuchao Dai, Junbo Yin, Feixiang Lu, Jin Fang, Miao Liao, Liangjun Zhang</h3>
<p>3D object detection from a single image is an important task in Autonomous
Driving (AD), where various approaches have been proposed. However, the task is
intrinsically ambiguous and challenging as single image depth estimation is
already an ill-posed problem. In this paper, we propose an instance-aware
approach to aggregate useful information for improving the accuracy of 3D
object detection with the following contributions. First, an instance-aware
feature aggregation (IAFA) module is proposed to collect local and global
features for 3D bounding boxes regression. Second, we empirically find that the
spatial attention module can be well learned by taking coarse-level instance
annotations as a supervision signal. The proposed module has significantly
boosted the performance of the baseline method on both 3D detection and 2D
bird-eye's view of vehicle detection among all three categories. Third, our
proposed method outperforms all single image-based approaches (even these
methods trained with depth as auxiliary inputs) and achieves state-of-the-art
3D detection performance on the KITTI benchmark.
</p>
<a href="http://arxiv.org/abs/2103.03480" target="_blank">arXiv:2103.03480</a> [<a href="http://arxiv.org/pdf/2103.03480" target="_blank">pdf</a>]

<h2>Causal Attention for Vision-Language Tasks. (arXiv:2103.03493v1 [cs.CV])</h2>
<h3>Xu Yang, Hanwang Zhang, Guojun Qi, Jianfei Cai</h3>
<p>We present a novel attention mechanism: Causal Attention (CATT), to remove
the ever-elusive confounding effect in existing attention-based vision-language
models. This effect causes harmful bias that misleads the attention module to
focus on the spurious correlations in training data, damaging the model
generalization. As the confounder is unobserved in general, we use the
front-door adjustment to realize the causal intervention, which does not
require any knowledge on the confounder. Specifically, CATT is implemented as a
combination of 1) In-Sample Attention (IS-ATT) and 2) Cross-Sample Attention
(CS-ATT), where the latter forcibly brings other samples into every IS-ATT,
mimicking the causal intervention. CATT abides by the Q-K-V convention and
hence can replace any attention module such as top-down attention and
self-attention in Transformers. CATT improves various popular attention-based
vision-language models by considerable margins. In particular, we show that
CATT has great potential in large-scale pre-training, e.g., it can promote the
lighter LXMERT~\cite{tan2019lxmert}, which uses fewer data and less
computational power, comparable to the heavier UNITER~\cite{chen2020uniter}.
Code is published in \url{https://github.com/yangxuntu/catt}.
</p>
<a href="http://arxiv.org/abs/2103.03493" target="_blank">arXiv:2103.03493</a> [<a href="http://arxiv.org/pdf/2103.03493" target="_blank">pdf</a>]

<h2>Analysis of User Preferences for Robot Motions in Immersive Telepresence. (arXiv:2103.03496v1 [cs.RO])</h2>
<h3>Katherine J. Mimnaugh, Markku Suomalainen, Israel Becerra, Eliezer Lozano, Rafael Murrieta-Cid, Steven M. LaValle</h3>
<p>This paper considers how the motions of a telepresence robot moving
autonomously affect a person immersed in the robot through a head-mounted
display. In particular, we explore the preference, comfort, and naturalness of
elements of piecewise linear paths compared to the same elements on a smooth
path. In a user study, thirty-six subjects watched panoramic videos of three
different paths through a simulated museum in virtual reality and responded to
questionnaires regarding each path. Preference for a particular path was
influenced the most by comfort, forward speed, and characteristics of the
turns. Preference was also strongly associated with the users' perceived
naturalness, which was primarily determined by the ability to see salient
objects, the distance to the walls and objects, as well as the turns.
Participants favored the paths that had a one meter per second forward speed
and rated the path with the least amount of turns as the most comfortable.
</p>
<a href="http://arxiv.org/abs/2103.03496" target="_blank">arXiv:2103.03496</a> [<a href="http://arxiv.org/pdf/2103.03496" target="_blank">pdf</a>]

<h2>Unsupervised Learning for Robust Fitting:A Reinforcement Learning Approach. (arXiv:2103.03501v1 [cs.CV])</h2>
<h3>Giang Truong, Huu Le, David Suter, Erchuan Zhang, Syed Zulqarnain Gilani</h3>
<p>Robust model fitting is a core algorithm in a large number of computer vision
applications. Solving this problem efficiently for datasets highly contaminated
with outliers is, however, still challenging due to the underlying
computational complexity. Recent literature has focused on learning-based
algorithms. However, most approaches are supervised which require a large
amount of labelled training data. In this paper, we introduce a novel
unsupervised learning framework that learns to directly solve robust model
fitting. Unlike other methods, our work is agnostic to the underlying input
features, and can be easily generalized to a wide variety of LP-type problems
with quasi-convex residuals. We empirically show that our method outperforms
existing unsupervised learning approaches, and achieves competitive results
compared to traditional methods on several important computer vision problems.
</p>
<a href="http://arxiv.org/abs/2103.03501" target="_blank">arXiv:2103.03501</a> [<a href="http://arxiv.org/pdf/2103.03501" target="_blank">pdf</a>]

<h2>NPT-Loss: A Metric Loss with Implicit Mining for Face Recognition. (arXiv:2103.03503v1 [cs.CV])</h2>
<h3>Syed Safwan Khalid, Muhammad Awais, Chi-Ho Chan, Zhenhua Feng, Ammarah Farooq, Ali Akbari, Josef Kittler</h3>
<p>Face recognition (FR) using deep convolutional neural networks (DCNNs) has
seen remarkable success in recent years. One key ingredient of DCNN-based FR is
the appropriate design of a loss function that ensures discrimination between
various identities. The state-of-the-art (SOTA) solutions utilise normalised
Softmax loss with additive and/or multiplicative margins. Despite being
popular, these Softmax+margin based losses are not theoretically motivated and
the effectiveness of a margin is justified only intuitively. In this work, we
utilise an alternative framework that offers a more direct mechanism of
achieving discrimination among the features of various identities. We propose a
novel loss that is equivalent to a triplet loss with proxies and an implicit
mechanism of hard-negative mining. We give theoretical justification that
minimising the proposed loss ensures a minimum separability between all
identities. The proposed loss is simple to implement and does not require heavy
hyper-parameter tuning as in the SOTA solutions. We give empirical evidence
that despite its simplicity, the proposed loss consistently achieves SOTA
performance in various benchmarks for both high-resolution and low-resolution
FR tasks.
</p>
<a href="http://arxiv.org/abs/2103.03503" target="_blank">arXiv:2103.03503</a> [<a href="http://arxiv.org/pdf/2103.03503" target="_blank">pdf</a>]

<h2>Variational Structured Attention Networks for Deep Visual Representation Learning. (arXiv:2103.03510v1 [cs.CV])</h2>
<h3>Guanglei Yang, Paolo Rota, Xavier Alameda-Pineda, Dan Xu, Mingli Ding, Elisa Ricci</h3>
<p>Convolutional neural networks have enabled major progress in addressing
pixel-level prediction tasks such as semantic segmentation, depth estimation,
surface normal prediction, and so on, benefiting from their powerful
capabilities in visual representation learning. Typically, state-of-the-art
models integrates attention mechanisms for improved deep feature
representations. Recently, some works have demonstrated the significance of
learning and combining both spatial- and channel-wise attentions for deep
feature refinement. In this paper, we aim at effectively boosting previous
approaches and propose a unified deep framework to jointly learn both spatial
attention maps and channel attention vectors in a principled manner so as to
structure the resulting attention tensors and model interactions between these
two types of attentions. Specifically, we integrate the estimation and the
interaction of the attentions within a probabilistic representation learning
framework, leading to Variational STructured Attention networks (VISTA-Net). We
implement the inference rules within the neural network, thus allowing for
end-to-end learning of the probabilistic and the CNN front-end parameters. As
demonstrated by our extensive empirical evaluation on six large-scale datasets
for dense visual prediction, VISTA-Net outperforms the state-of-the-art in
multiple continuous and discrete prediction tasks, thus confirming the benefit
of the proposed approach in joint structured spatial-channel attention
estimation for deep representation learning. The code is available at
https://github.com/ygjwd12345/VISTA-Net.
</p>
<a href="http://arxiv.org/abs/2103.03510" target="_blank">arXiv:2103.03510</a> [<a href="http://arxiv.org/pdf/2103.03510" target="_blank">pdf</a>]

<h2>Anomaly detection and automatic labeling for solar cell quality inspection based on Generative Adversarial Network. (arXiv:2103.03518v1 [cs.CV])</h2>
<h3>Balzategui Julen, Eciolaza Luka, Maestro-Watson Daniel</h3>
<p>In this manuscript, a pipeline to develop an inspection system for defect
detection of solar cells is proposed. The pipeline is divided into two phases:
In the first phase, a Generative Adversarial Network (GAN) employed in the
medical domain for anomaly detection is adapted for inspection improving the
detection rate and reducing the processing rates. This initial approach allows
obtaining a model that does not require defective samples for training and can
start detecting and location anomaly cells from the very beginning of a new
production line. Then, in a second stage, as defective samples arise, they will
be automatically labeled at pixel-level with the trained model and employed for
supervised training of a second model. The experimental results show that the
use of such automatically generated labels can improve the detection rates with
respect to the anomaly detection model and the model trained on manual labels
made by experts.
</p>
<a href="http://arxiv.org/abs/2103.03518" target="_blank">arXiv:2103.03518</a> [<a href="http://arxiv.org/pdf/2103.03518" target="_blank">pdf</a>]

<h2>LoRa Backscatter Assisted State Estimator for Micro Aerial Vehicles with Online Initialization. (arXiv:2103.03542v1 [cs.RO])</h2>
<h3>Shengkai Zhang, Wei Wang, Ning Zhang, Tao Jiang</h3>
<p>The advances in agile micro aerial vehicles (MAVs) have shown great potential
in replacing humans for labor-intensive or dangerous indoor investigation, such
as warehouse management and fire rescue. However, the design of a state
estimation system that enables autonomous flight poses fundamental challenges
in such dim or smoky environments. Current dominated computer-vision based
solutions only work in well-lighted texture-rich environments. This paper
addresses the challenge by proposing Marvel, an RF backscatter-based state
estimation system with online initialization and calibration. Marvel is
nonintrusive to commercial MAVs by attaching backscatter tags to their landing
gears without internal hardware modifications, and works in a plug-and-play
fashion with an automatic initialization module. Marvel is enabled by three new
designs, a backscatter-based pose sensing module, an online initialization and
calibration module, and a backscatter-inertial super-accuracy state estimation
algorithm. We demonstrate our design by programming a commercial MAV to
autonomously fly in different trajectories. The results show that Marvel
supports navigation within a range of 50 m or through three concrete walls,
with an accuracy of 34 cm for localization and 4.99 degrees for orientation
estimation. We further demonstrate our online initialization and calibration by
comparing to the perfect initial parameter measurements from burdensome manual
operations.
</p>
<a href="http://arxiv.org/abs/2103.03542" target="_blank">arXiv:2103.03542</a> [<a href="http://arxiv.org/pdf/2103.03542" target="_blank">pdf</a>]

<h2>A Hybrid Dynamical Modeling Framework for Shape Memory Alloy Wire Actuated Structures. (arXiv:2103.03586v1 [cs.RO])</h2>
<h3>Michele A. Mandolino, Francesco Ferrante, Gianluca Rizzello</h3>
<p>In this paper, a hybrid model for single-crystal Shape Memory Alloy (SMA)
wire actuators is presented. The result is based on a mathematical
reformulation of the M\"uller-Achenbach-Seelecke (MAS) model, which provides an
accurate and interconnection-oriented description of the SMA hysteretic
response. The strong nonlinearity and high numerical stiffness of the MAS
model, however, hinder its practical use for simulation and control of complex
SMA-driven systems. The main idea behind the hybrid reformulation is based on
dividing the mechanical hysteresis of the SMA into five operating modes, each
one representing a different physical state of the material. By properly
deriving the switching conditions among those modes in a physically-consistent
way, the MAS model is effectively reformulated within a hybrid dynamical
setting. The main advantage of the hybrid reformulation is the possibility of
describing the material dynamics with a simplified set of state equations while
maintaining all benefits of the physics-based description offered by the MAS
model After describing the novel approach, simulation studies are conducted on
a flexible robotic module actuated by protagonist-antagonist SMA wires. Through
comparative numerical analysis, it is shown how the hybrid model provides the
same accuracy as the MAS model while saving up to 80% of the simulation time.
Moreover, the new modeling framework opens up the possibility of addressing SMA
control from a hybrid systems perspective.
</p>
<a href="http://arxiv.org/abs/2103.03586" target="_blank">arXiv:2103.03586</a> [<a href="http://arxiv.org/pdf/2103.03586" target="_blank">pdf</a>]

<h2>Use of Transfer Learning and Wavelet Transform for Breast Cancer Detection. (arXiv:2103.03602v1 [cs.CV])</h2>
<h3>Ahmed Rasheed, Muhammad Shahzad Younis, Junaid Qadir, Muhammad Bilal</h3>
<p>Breast cancer is one of the most common cause of deaths among women.
Mammography is a widely used imaging modality that can be used for cancer
detection in its early stages. Deep learning is widely used for the detection
of cancerous masses in the images obtained via mammography. The need to improve
accuracy remains constant due to the sensitive nature of the datasets so we
introduce segmentation and wavelet transform to enhance the important features
in the image scans. Our proposed system aids the radiologist in the screening
phase of cancer detection by using a combination of segmentation and wavelet
transforms as pre-processing augmentation that leads to transfer learning in
neural networks. The proposed system with these pre-processing techniques
significantly increases the accuracy of detection on Mini-MIAS.
</p>
<a href="http://arxiv.org/abs/2103.03602" target="_blank">arXiv:2103.03602</a> [<a href="http://arxiv.org/pdf/2103.03602" target="_blank">pdf</a>]

<h2>FloMo: Tractable Motion Prediction with Normalizing Flows. (arXiv:2103.03614v1 [cs.CV])</h2>
<h3>Christoph Sch&#xf6;ller, Alois Knoll</h3>
<p>The future motion of traffic participants is inherently uncertain. To plan
safely, therefore, an autonomous agent must take into account multiple possible
outcomes and prioritize them. Recently, this problem has been addressed with
generative neural networks. However, most generative models either do not learn
the true underlying trajectory distribution reliably, or do not allow
likelihoods to be associated with predictions. In our work, we model motion
prediction directly as a density estimation problem with a normalizing flow
between a noise sample and the future motion distribution. Our model, named
FloMo, allows likelihoods to be computed in a single network pass and can be
trained directly with maximum likelihood estimation. Furthermore, we propose a
method to stabilize training flows on trajectory datasets and a new data
augmentation transformation that improves the performance and generalization of
our model. Our method achieves state-of-the-art performance on three popular
prediction datasets, with a significant gap to most competing models.
</p>
<a href="http://arxiv.org/abs/2103.03614" target="_blank">arXiv:2103.03614</a> [<a href="http://arxiv.org/pdf/2103.03614" target="_blank">pdf</a>]

<h2>Fail-Aware LIDAR-Based Odometry for Autonomous Vehicles. (arXiv:2103.03626v1 [cs.RO])</h2>
<h3>Iv&#xe1;n Garc&#xed;a Daza, Monica Rentero, Carlota Salinas Maldonado, Rub&#xe9;n Izquierdo Gonzalo, Noelia Hern&#xe1;ndez Parra, Augusto Luis Ballardini, David Fern&#xe1;ndez Llorca</h3>
<p>Autonomous driving systems are set to become a reality in transport systems
and, so, maximum acceptance is being sought among users. Currently, the most
advanced architectures require driver intervention when functional system
failures or critical sensor operations take place, presenting problems related
to driver state, distractions, fatigue, and other factors that prevent safe
control. Therefore, this work presents a redundant, accurate, robust, and
scalable LiDAR odometry system with fail-aware system features that can allow
other systems to perform a safe stop manoeuvre without driver mediation. All
odometry systems have drift error, making it difficult to use them for
localisation tasks over extended periods. For this reason, the paper presents
an accurate LiDAR odometry system with a fail-aware indicator. This indicator
estimates a time window in which the system manages the localisation tasks
appropriately. The odometry error is minimised by applying a dynamic 6-DoF
model and fusing measures based on the Iterative Closest Points (ICP),
environment feature extraction, and Singular Value Decomposition (SVD) methods.
The obtained results are promising for two reasons: First, in the KITTI
odometry data set, the ranking achieved by the proposed method is twelfth,
considering only LiDAR-based methods, where its translation and rotation errors
are 1.00% and 0.0041 deg/m, respectively. Second, the encouraging results of
the fail-aware indicator demonstrate the safety of the proposed LiDAR odometry
system. The results depict that, in order to achieve an accurate odometry
system, complex models and measurement fusion techniques must be used to
improve its behaviour. Furthermore, if an odometry system is to be used for
redundant localisation features, it must integrate a fail-aware indicator for
use in a safe manner.
</p>
<a href="http://arxiv.org/abs/2103.03626" target="_blank">arXiv:2103.03626</a> [<a href="http://arxiv.org/pdf/2103.03626" target="_blank">pdf</a>]

<h2>Self-supervised Mean Teacher for Semi-supervised Chest X-ray Classification. (arXiv:2103.03629v1 [cs.CV])</h2>
<h3>Fengbei Liu, Yu Tian, Filipe R. Cordeiro, Vasileios Belagiannis, Ian Reid, Gustavo Carneiro</h3>
<p>The training of deep learning models generally requires a large amount of
annotated data for effective convergence and generalisation. However, obtaining
high-quality annotations is a laboursome and expensive process due to the need
of expert radiologists for the labelling task. The study of semi-supervised
learning in medical image analysis is then of crucial importance given that it
is much less expensive to obtain unlabelled images than to acquire images
labelled by expert radiologists.Essentially, semi-supervised methods leverage
large sets of unlabelled data to enable better training convergence and
generalisation than if we use only the small set of labelled images.In this
paper, we propose the Self-supervised Mean Teacher for Semi-supervised
(S$^2$MTS$^2$) learning that combines self-supervised mean-teacher pre-training
with semi-supervised fine-tuning. The main innovation of S$^2$MTS$^2$ is the
self-supervised mean-teacher pre-training based on the joint contrastive
learning, which uses an infinite number of pairs of positive query and key
features to improve the mean-teacher representation. The model is then
fine-tuned using the exponential moving average teacher framework trained with
semi-supervised learning.We validate S$^2$MTS$^2$ on the thorax disease
multi-label classification problem from the dataset Chest X-ray14, where we
show that it outperforms the previous SOTA semi-supervised learning methods by
a large margin.
</p>
<a href="http://arxiv.org/abs/2103.03629" target="_blank">arXiv:2103.03629</a> [<a href="http://arxiv.org/pdf/2103.03629" target="_blank">pdf</a>]

<h2>Contrastive Disentanglement in Generative Adversarial Networks. (arXiv:2103.03636v1 [cs.CV])</h2>
<h3>Lili Pan, Peijun Tang, Zhiyong Chen, Zenglin Xu</h3>
<p>Disentanglement is defined as the problem of learninga representation that
can separate the distinct, informativefactors of variations of data. Learning
such a representa-tion may be critical for developing explainable and
human-controllable Deep Generative Models (DGMs) in artificialintelligence.
However, disentanglement in GANs is not a triv-ial task, as the absence of
sample likelihood and posteriorinference for latent variables seems to prohibit
the forwardstep. Inspired by contrastive learning (CL), this paper, froma new
perspective, proposes contrastive disentanglement ingenerative adversarial
networks (CD-GAN). It aims at dis-entangling the factors of inter-class
variation of visual datathrough contrasting image features, since the same
factorvalues produce images in the same class. More importantly,we probe a
novel way to make use of limited amount ofsupervision to the largest extent, to
promote inter-class dis-entanglement performance. Extensive experimental
resultson many well-known datasets demonstrate the efficacy ofCD-GAN for
disentangling inter-class variation.
</p>
<a href="http://arxiv.org/abs/2103.03636" target="_blank">arXiv:2103.03636</a> [<a href="http://arxiv.org/pdf/2103.03636" target="_blank">pdf</a>]

<h2>Fine-Grained Off-Road Semantic Segmentation and Mapping via Contrastive Learning. (arXiv:2103.03651v1 [cs.CV])</h2>
<h3>Biao Gao, Shaochi Hu, Xijun Zhao, Huijing Zhao</h3>
<p>Road detection or traversability analysis has been a key technique for a
mobile robot to traverse complex off-road scenes. The problem has been mainly
formulated in early works as a binary classification one, e.g. associating
pixels with road or non-road labels. Whereas understanding scenes with
fine-grained labels are needed for off-road robots, as scenes are very diverse,
and the various mechanical performance of off-road robots may lead to different
definitions of safe regions to traverse. How to define and annotate
fine-grained labels to achieve meaningful scene understanding for a robot to
traverse off-road is still an open question. This research proposes a
contrastive learning based method. With a set of human-annotated anchor
patches, a feature representation is learned to discriminate regions with
different traversability, a method of fine-grained semantic segmentation and
mapping is subsequently developed for off-road scene understanding. Experiments
are conducted on a dataset of three driving segments that represent very
diverse off-road scenes. An anchor accuracy of 89.8% is achieved by evaluating
the matching with human-annotated image patches in cross-scene validation.
Examined by associated 3D LiDAR data, the fine-grained segments of visual
images are demonstrated to have different levels of toughness and terrain
elevation, which represents their semantical meaningfulness. The resultant maps
contain both fine-grained labels and confidence values, providing rich
information to support a robot traversing complex off-road scenes.
</p>
<a href="http://arxiv.org/abs/2103.03651" target="_blank">arXiv:2103.03651</a> [<a href="http://arxiv.org/pdf/2103.03651" target="_blank">pdf</a>]

<h2>Effects of Image Compression on Face Image Manipulation Detection: A Case Study on Facial Retouching. (arXiv:2103.03654v1 [cs.CV])</h2>
<h3>Christian Rathgeb, Kevin Bernardo, Nathania E. Haryanto, Christoph Busch</h3>
<p>In the past years, numerous methods have been introduced to reliably detect
digital face image manipulations. Lately, the generalizability of these schemes
has been questioned in particular with respect to image post-processing. Image
compression represents a post-processing which is frequently applied in diverse
biometric application scenarios. Severe compression might erase digital traces
of face image manipulation and hence hamper a reliable detection thereof. In
this work, the effects of image compression on face image manipulation
detection are analyzed. In particular, a case study on facial retouching
detection under the influence of image compression is presented. To this end,
ICAO-compliant subsets of two public face databases are used to automatically
create a database containing more than 9,000 retouched reference images
together with unconstrained probe images. Subsequently, reference images are
compressed applying JPEG and JPEG 2000 at compression levels recommended for
face image storage in electronic travel documents. Novel detection algorithms
utilizing texture descriptors and deep face representations are proposed and
evaluated in a single image and differential scenario. Results obtained from
challenging cross-database experiments in which the analyzed retouching
technique is unknown during training yield interesting findings: (1) most
competitive detection performance is achieved for differential scenarios
employing deep face representations; (2) image compression severely impacts the
performance of face image manipulation detection schemes based on texture
descriptors while methods utilizing deep face representations are found to be
highly robust; (3) in some cases, the application of image compression might as
well improve detection performance.
</p>
<a href="http://arxiv.org/abs/2103.03654" target="_blank">arXiv:2103.03654</a> [<a href="http://arxiv.org/pdf/2103.03654" target="_blank">pdf</a>]

<h2>MAMBPO: Sample-efficient multi-robot reinforcement learning using learned world models. (arXiv:2103.03662v1 [cs.RO])</h2>
<h3>Dani&#xeb;l Willemsen, Mario Coppola, Guido C.H.E. de Croon</h3>
<p>Multi-robot systems can benefit from reinforcement learning (RL) algorithms
that learn behaviours in a small number of trials, a property known as sample
efficiency. This research thus investigates the use of learned world models to
improve sample efficiency. We present a novel multi-agent model-based RL
algorithm: Multi-Agent Model-Based Policy Optimization (MAMBPO), utilizing the
Centralized Learning for Decentralized Execution (CLDE) framework. CLDE
algorithms allow a group of agents to act in a fully decentralized manner after
training. This is a desirable property for many systems comprising of multiple
robots. MAMBPO uses a learned world model to improve sample efficiency compared
to model-free Multi-Agent Soft Actor-Critic (MASAC). We demonstrate this on two
simulated multi-robot tasks, where MAMBPO achieves a similar performance to
MASAC, but requires far fewer samples to do so. Through this, we take an
important step towards making real-life learning for multi-robot systems
possible.
</p>
<a href="http://arxiv.org/abs/2103.03662" target="_blank">arXiv:2103.03662</a> [<a href="http://arxiv.org/pdf/2103.03662" target="_blank">pdf</a>]

<h2>Real-time RGBD-based Extended Body Pose Estimation. (arXiv:2103.03663v1 [cs.CV])</h2>
<h3>Renat Bashirov, Anastasia Ianina, Karim Iskakov, Yevgeniy Kononenko, Valeriya Strizhkova, Victor Lempitsky, Alexander Vakhitov</h3>
<p>We present a system for real-time RGBD-based estimation of 3D human pose. We
use parametric 3D deformable human mesh model (SMPL-X) as a representation and
focus on the real-time estimation of parameters for the body pose, hands pose
and facial expression from Kinect Azure RGB-D camera. We train estimators of
body pose and facial expression parameters. Both estimators use previously
published landmark extractors as input and custom annotated datasets for
supervision, while hand pose is estimated directly by a previously published
method. We combine the predictions of those estimators into a temporally-smooth
human pose. We train the facial expression extractor on a large talking face
dataset, which we annotate with facial expression parameters. For the body pose
we collect and annotate a dataset of 56 people captured from a rig of 5 Kinect
Azure RGB-D cameras and use it together with a large motion capture AMASS
dataset. Our RGB-D body pose model outperforms the state-of-the-art RGB-only
methods and works on the same level of accuracy compared to a slower RGB-D
optimization-based solution. The combined system runs at 30 FPS on a server
with a single GPU. The code will be available at
https://saic-violet.github.io/rgbd-kinect-pose
</p>
<a href="http://arxiv.org/abs/2103.03663" target="_blank">arXiv:2103.03663</a> [<a href="http://arxiv.org/pdf/2103.03663" target="_blank">pdf</a>]

<h2>ASC-Net : Adversarial-based Selective Network for Unsupervised Anomaly Segmentation. (arXiv:2103.03664v1 [cs.CV])</h2>
<h3>Raunak Dey, Yi Hong</h3>
<p>We introduce a neural network framework, utilizing adversarial learning to
partition an image into two cuts, with one cut falling into a reference
distribution provided by the user. This concept tackles the task of
unsupervised anomaly segmentation, which has attracted increasing attention in
recent years due to their broad applications in tasks with unlabelled data.
This Adversarial-based Selective Cutting network (ASC-Net) bridges the two
domains of cluster-based deep learning methods and adversarial-based
anomaly/novelty detection algorithms. We evaluate this unsupervised learning
model on BraTS brain tumor segmentation, LiTS liver lesion segmentation, and
MS-SEG2015 segmentation tasks. Compared to existing methods like the AnoGAN
family, our model demonstrates tremendous performance gains in unsupervised
anomaly segmentation tasks. Although there is still room to further improve
performance compared to supervised learning algorithms, the promising
experimental results shed light on building an unsupervised learning algorithm
using user-defined knowledge.
</p>
<a href="http://arxiv.org/abs/2103.03664" target="_blank">arXiv:2103.03664</a> [<a href="http://arxiv.org/pdf/2103.03664" target="_blank">pdf</a>]

<h2>An Application-Driven Conceptualization of Corner Cases for Perception in Highly Automated Driving. (arXiv:2103.03678v1 [cs.CV])</h2>
<h3>Florian Heidecker, Jasmin Breitenstein, Kevin R&#xf6;sch, Jonas L&#xf6;hdefink, Maarten Bieshaar, Christoph Stiller, Tim Fingscheidt, Bernhard Sick</h3>
<p>Systems and functions that rely on machine learning (ML) are the basis of
highly automated driving. An essential task of such ML models is to reliably
detect and interpret unusual, new, and potentially dangerous situations. The
detection of those situations, which we refer to as corner cases, is highly
relevant for successfully developing, applying, and validating automotive
perception functions in future vehicles where multiple sensor modalities will
be used. A complication for the development of corner case detectors is the
lack of consistent definitions, terms, and corner case descriptions, especially
when taking into account various automotive sensors. In this work, we provide
an application-driven view of corner cases in highly automated driving. To
achieve this goal, we first consider existing definitions from the general
outlier, novelty, anomaly, and out-of-distribution detection to show relations
and differences to corner cases. Moreover, we extend an existing camera-focused
systematization of corner cases by adding RADAR (radio detection and ranging)
and LiDAR (light detection and ranging) sensors. For this, we describe an
exemplary toolchain for data acquisition and processing, highlighting the
interfaces of the corner case detection. We also define a novel level of corner
cases, the method layer corner cases, which appear due to uncertainty inherent
in the methodology or the data distribution.
</p>
<a href="http://arxiv.org/abs/2103.03678" target="_blank">arXiv:2103.03678</a> [<a href="http://arxiv.org/pdf/2103.03678" target="_blank">pdf</a>]

<h2>Signal-level Fusion for Indexing and Retrieval of Facial Biometric Data. (arXiv:2103.03692v1 [cs.CV])</h2>
<h3>Pawel Drozdowski, Fabian Stockhardt, Christian Rathgeb, Christoph Busch</h3>
<p>The growing scope, scale, and number of biometric deployments around the
world emphasise the need for research into technologies facilitating efficient
and reliable biometric identification queries. This work presents a method of
indexing biometric databases, which relies on signal-level fusion of facial
images (morphing) to create a multi-stage data-structure and retrieval
protocol. By successively pre-filtering the list of potential candidate
identities, the proposed method makes it possible to reduce the necessary
number of biometric template comparisons to complete a biometric identification
transaction. The proposed method is extensively evaluated on publicly available
databases using open-source and commercial off-the-shelf recognition systems.
The results show that using the proposed method, the computational workload can
be reduced down to around 30%, while the biometric performance of a baseline
exhaustive search-based retrieval is fully maintained, both in closed-set and
open-set identification scenarios.
</p>
<a href="http://arxiv.org/abs/2103.03692" target="_blank">arXiv:2103.03692</a> [<a href="http://arxiv.org/pdf/2103.03692" target="_blank">pdf</a>]

<h2>Bayesian Meta-Learning for Few-Shot Policy Adaptation Across Robotic Platforms. (arXiv:2103.03697v1 [cs.RO])</h2>
<h3>Ali Ghadirzadeh, Xi Chen, Petra Poklukar, Chelsea Finn, M&#xe5;rten Bj&#xf6;rkman, Danica Kragic</h3>
<p>Reinforcement learning methods can achieve significant performance but
require a large amount of training data collected on the same robotic platform.
A policy trained with expensive data is rendered useless after making even a
minor change to the robot hardware. In this paper, we address the challenging
problem of adapting a policy, trained to perform a task, to a novel robotic
hardware platform given only few demonstrations of robot motion trajectories on
the target robot. We formulate it as a few-shot meta-learning problem where the
goal is to find a meta-model that captures the common structure shared across
different robotic platforms such that data-efficient adaptation can be
performed. We achieve such adaptation by introducing a learning framework
consisting of a probabilistic gradient-based meta-learning algorithm that
models the uncertainty arising from the few-shot setting with a low-dimensional
latent variable. We experimentally evaluate our framework on a simulated
reaching and a real-robot picking task using 400 simulated robots generated by
varying the physical parameters of an existing set of robotic platforms. Our
results show that the proposed method can successfully adapt a trained policy
to different robotic platforms with novel physical parameters and the
superiority of our meta-learning algorithm compared to state-of-the-art methods
for the introduced few-shot policy adaptation problem.
</p>
<a href="http://arxiv.org/abs/2103.03697" target="_blank">arXiv:2103.03697</a> [<a href="http://arxiv.org/pdf/2103.03697" target="_blank">pdf</a>]

<h2>Peer Learning for Skin Lesion Classification. (arXiv:2103.03703v1 [cs.CV])</h2>
<h3>Tariq Bdair, Nassir Navab, Shadi Albarqouni</h3>
<p>Skin cancer is one of the most deadly cancers worldwide. Yet, it can be
reduced by early detection. Recent deep-learning methods have shown a
dermatologist-level performance in skin cancer classification. Yet, this
success demands a large amount of centralized data, which is oftentimes not
available. Federated learning has been recently introduced to train machine
learning models in a privacy-preserved distributed fashion demanding annotated
data at the clients, which is usually expensive and not available, especially
in the medical field. To this end, we propose FedPerl, a semi-supervised
federated learning method that utilizes peer learning from social sciences and
ensemble averaging from committee machines to build communities and encourage
its members to learn from each other such that they produce more accurate
pseudo labels. We also propose the peer anonymization (PA) technique as a core
component of FedPerl. PA preserves privacy and reduces the communication cost
while maintaining the performance without additional complexity. We validated
our method on 38,000 skin lesion images collected from 4 publicly available
datasets. FedPerl achieves superior performance over the baselines and
state-of-the-art SSFL by 15.8%, and 1.8% respectively. Further, FedPerl shows
less sensitivity to noisy clients.
</p>
<a href="http://arxiv.org/abs/2103.03703" target="_blank">arXiv:2103.03703</a> [<a href="http://arxiv.org/pdf/2103.03703" target="_blank">pdf</a>]

<h2>Ground-SLAM: Ground Constrained LiDAR SLAM for Structured Multi-Floor Environments. (arXiv:2103.03713v1 [cs.RO])</h2>
<h3>Xin Wei, Jixin Lv, Jie Sun, Shiliang Pu</h3>
<p>This paper proposes a 3D LiDAR SLAM algorithm named Ground-SLAM, which
exploits grounds in structured multi-floor environments to compress the pose
drift mainly caused by LiDAR measurement bias. Ground-SLAM is developed based
on the well-known pose graph optimization framework. In the front-end, motion
estimation is conducted using LiDAR Odometry (LO) with a novel sensor-centric
sliding map introduced, which is maintained by filtering out expired features
based on the model of error propagation. At each key-frame, the sliding map is
recorded as a local map. The ground nearby is extracted and modelled as an
infinite planar landmark in the form of Closest Point (CP) parameterization.
Then, ground planes observed at different key-frames are associated, and the
ground constraints are fused into the pose graph optimization framework to
compress the pose drift of LO. Finally, loop-closure detection is carried out,
and the residual error is jointly minimized, which could lead to a globally
consistent map. Experimental results demonstrate superior performances in the
accuracy of the proposed approach.
</p>
<a href="http://arxiv.org/abs/2103.03713" target="_blank">arXiv:2103.03713</a> [<a href="http://arxiv.org/pdf/2103.03713" target="_blank">pdf</a>]

<h2>NemaNet: A convolutional neural network model for identification of nematodes soybean crop in brazil. (arXiv:2103.03717v1 [cs.CV])</h2>
<h3>Andre da Silva Abade, Lucas Faria Porto, Paulo Afonso Ferreira, Flavio de Barros Vidal</h3>
<p>Phytoparasitic nematodes (or phytonematodes) are causing severe damage to
crops and generating large-scale economic losses worldwide. In soybean crops,
annual losses are estimated at 10.6% of world production. Besides, identifying
these species through microscopic analysis by an expert with taxonomy knowledge
is often laborious, time-consuming, and susceptible to failure. In this
perspective, robust and automatic approaches are necessary for identifying
phytonematodes capable of providing correct diagnoses for the classification of
species and subsidizing the taking of all control and prevention measures. This
work presents a new public data set called NemaDataset containing 3,063
microscopic images from five nematode species with the most significant damage
relevance for the soybean crop. Additionally, we propose a new Convolutional
Neural Network (CNN) model defined as NemaNet and a comparative assessment with
thirteen popular models of CNNs, all of them representing the state of the art
classification and recognition. The general average calculated for each model,
on a from-scratch training, the NemaNet model reached 96.99% accuracy, while
the best evaluation fold reached 98.03%. In training with transfer learning,
the average accuracy reached 98.88\%. The best evaluation fold reached 99.34%
and achieve an overall accuracy improvement over 6.83% and 4.1%, for
from-scratch and transfer learning training, respectively, when compared to
other popular models.
</p>
<a href="http://arxiv.org/abs/2103.03717" target="_blank">arXiv:2103.03717</a> [<a href="http://arxiv.org/pdf/2103.03717" target="_blank">pdf</a>]

<h2>Landmark-based Distributed Topological Mapping and Navigation in GPS-denied Urban Environments Using Teams of Low-cost Robots. (arXiv:2103.03741v1 [cs.RO])</h2>
<h3>Mohammad Saleh Teymouri, Subhrajit Bhattacharya</h3>
<p>In this paper, we address the problem of autonomous multi-robot mapping,
exploration and navigation in unknown, GPS-denied indoor or urban environments
using a swarm of robots equipped with directional sensors with limited sensing
capabilities and limited computational resources. The robots have no a priori
knowledge of the environment and need to rapidly explore and construct a map in
a distributed manner using existing landmarks, the presence of which can be
detected using onboard senors, although little to no metric information
(distance or bearing to the landmarks) is available. In order to correctly and
effectively achieve this, the presence of a necessary density/distribution of
landmarks is ensured by design of the urban/indoor environment. We thus address
this problem in two phases: 1) During the design/construction of the
urban/indoor environment we can ensure that sufficient landmarks are placed
within the environment. To that end we develop a filtration-based approach for
designing strategic placement of landmarks in an environment. 2) We develop a
distributed algorithm using which a team of robots, with no a priori knowledge
of the environment, can explore such an environment, construct a topological
map requiring no metric/distance information, and use that map to navigate
within the environment. This is achieved using a topological representation of
the environment (called a Landmark Complex), instead of constructing a complete
metric/pixel map. The representation is built by the robot as well as used by
them for navigation through a balance between exploration and exploitation. We
use tools from homology theory for identifying "holes" in the
coverage/exploration of the unknown environment and hence guiding the robots
towards achieving a complete exploration and mapping of the environment.
</p>
<a href="http://arxiv.org/abs/2103.03741" target="_blank">arXiv:2103.03741</a> [<a href="http://arxiv.org/pdf/2103.03741" target="_blank">pdf</a>]

<h2>A Convolutional Architecture for 3D Model Embedding. (arXiv:2103.03764v1 [cs.CV])</h2>
<h3>Arniel Labrada, Benjamin Bustos, Ivan Sipiran</h3>
<p>During the last years, many advances have been made in tasks like3D model
retrieval, 3D model classification, and 3D model segmentation.The typical 3D
representations such as point clouds, voxels, and poly-gon meshes are mostly
suitable for rendering purposes, while their use forcognitive processes
(retrieval, classification, segmentation) is limited dueto their high
redundancy and complexity. We propose a deep learningarchitecture to handle 3D
models as an input. We combine this architec-ture with other standard
architectures like Convolutional Neural Networksand autoencoders for computing
3D model embeddings. Our goal is torepresent a 3D model as a vector with enough
information to substitutethe 3D model for high-level tasks. Since this vector
is a learned repre-sentation which tries to capture the relevant information of
a 3D model,we show that the embedding representation conveys semantic
informationthat helps to deal with the similarity assessment of 3D objects. Our
ex-periments show the benefit of computing the embeddings of a 3D modeldata set
and use them for effective 3D Model Retrieval.
</p>
<a href="http://arxiv.org/abs/2103.03764" target="_blank">arXiv:2103.03764</a> [<a href="http://arxiv.org/pdf/2103.03764" target="_blank">pdf</a>]

<h2>VIPriors 1: Visual Inductive Priors for Data-Efficient Deep Learning Challenges. (arXiv:2103.03768v1 [cs.CV])</h2>
<h3>Robert-Jan Bruintjes, Attila Lengyel, Marcos Baptista Rios, Osman Semih Kayhan, Jan van Gemert</h3>
<p>We present the first edition of "VIPriors: Visual Inductive Priors for
Data-Efficient Deep Learning" challenges. We offer four data-impaired
challenges, where models are trained from scratch, and we reduce the number of
training samples to a fraction of the full set. Furthermore, to encourage data
efficient solutions, we prohibited the use of pre-trained models and other
transfer learning techniques. The majority of top ranking solutions make heavy
use of data augmentation, model ensembling, and novel and efficient network
architectures to achieve significant performance increases compared to the
provided baselines.
</p>
<a href="http://arxiv.org/abs/2103.03768" target="_blank">arXiv:2103.03768</a> [<a href="http://arxiv.org/pdf/2103.03768" target="_blank">pdf</a>]

<h2>A Geometric Algebra Solution to Wahba's Problem. (arXiv:2103.03773v1 [cs.RO])</h2>
<h3>Timothy D Barfoot</h3>
<p>We retrace Davenport's solution to Wahba's classic problem of aligning two
pointclouds using the formalism of Geometric Algebra (GA). GA proves to be a
natural backdrop for this problem involving three-dimensional rotations due to
the isomorphism between unit-length quaternions and rotors. While the solution
to this problem is not a new result, it is hoped that its treatment in GA will
have tutorial value as well as open the door to addressing more complex
problems in a similar way.
</p>
<a href="http://arxiv.org/abs/2103.03773" target="_blank">arXiv:2103.03773</a> [<a href="http://arxiv.org/pdf/2103.03773" target="_blank">pdf</a>]

<h2>Self-Attentive Spatial Adaptive Normalization for Cross-Modality Domain Adaptation. (arXiv:2103.03781v1 [cs.CV])</h2>
<h3>Devavrat Tomar, Manana Lortkipanidze, Guillaume Vray, Behzad Bozorgtabar, Jean-Philippe Thiran</h3>
<p>Despite the successes of deep neural networks on many challenging vision
tasks, they often fail to generalize to new test domains that are not
distributed identically to the training data. The domain adaptation becomes
more challenging for cross-modality medical data with a notable domain shift.
Given that specific annotated imaging modalities may not be accessible nor
complete. Our proposed solution is based on the cross-modality synthesis of
medical images to reduce the costly annotation burden by radiologists and
bridge the domain gap in radiological images. We present a novel approach for
image-to-image translation in medical images, capable of supervised or
unsupervised (unpaired image data) setups. Built upon adversarial training, we
propose a learnable self-attentive spatial normalization of the deep
convolutional generator network's intermediate activations. Unlike previous
attention-based image-to-image translation approaches, which are either
domain-specific or require distortion of the source domain's structures, we
unearth the importance of the auxiliary semantic information to handle the
geometric changes and preserve anatomical structures during image translation.
We achieve superior results for cross-modality segmentation between unpaired
MRI and CT data for multi-modality whole heart and multi-modal brain tumor MRI
(T1/T2) datasets compared to the state-of-the-art methods. We also observe
encouraging results in cross-modality conversion for paired MRI and CT images
on a brain dataset. Furthermore, a detailed analysis of the cross-modality
image translation, thorough ablation studies confirm our proposed method's
efficacy.
</p>
<a href="http://arxiv.org/abs/2103.03781" target="_blank">arXiv:2103.03781</a> [<a href="http://arxiv.org/pdf/2103.03781" target="_blank">pdf</a>]

<h2>Learning Collision-free and Torque-limited Robot Trajectories based on Alternative Safe Behaviors. (arXiv:2103.03793v1 [cs.RO])</h2>
<h3>Jonas C. Kiemel, Torsten Kr&#xf6;ger</h3>
<p>This paper presents an approach to learn online generation of collision-free
and torque-limited trajectories for industrial robots. A neural network, which
is trained via reinforcement learning, is periodically invoked to predict
future motions. For each robot joint, the network outputs the kinematic state
that is desired at the end of the current time interval. Compliance with
kinematic joint limits is ensured by the design of the action space. Given the
current kinematic state and the network prediction, a trajectory for the
current time interval can be computed. The main idea of our paper is to execute
the predicted motion only if a collision-free and torque-limited way to
continue the trajectory is known. In practice, the predicted motion is expanded
by a braking trajectory and simulated using a physics engine. If the simulated
trajectory complies with all safety constraints, the predicted motion is
carried out. Otherwise, the braking trajectory calculated in the previous
decision step serves as an alternative safe behavior. For evaluation, up to
three simulated robots are trained to reach as many randomly placed target
points as possible. We show that our method reliably prevents collisions with
static obstacles and collisions between the robots, while generating motions
that respect both torque limits and kinematic joint limits. Experiments with a
real robot demonstrate that safe trajectories can be generated in real-time.
</p>
<a href="http://arxiv.org/abs/2103.03793" target="_blank">arXiv:2103.03793</a> [<a href="http://arxiv.org/pdf/2103.03793" target="_blank">pdf</a>]

<h2>Fast Interactive Video Object Segmentation with Graph Neural Networks. (arXiv:2103.03821v1 [cs.CV])</h2>
<h3>Viktor Varga, Andr&#xe1;s L&#x151;rincz</h3>
<p>Pixelwise annotation of image sequences can be very tedious for humans.
Interactive video object segmentation aims to utilize automatic methods to
speed up the process and reduce the workload of the annotators. Most
contemporary approaches rely on deep convolutional networks to collect and
process information from human annotations throughout the video. However, such
networks contain millions of parameters and need huge amounts of labeled
training data to avoid overfitting. Beyond that, label propagation is usually
executed as a series of frame-by-frame inference steps, which is difficult to
be parallelized and is thus time consuming. In this paper we present a graph
neural network based approach for tackling the problem of interactive video
object segmentation. Our network operates on superpixel-graphs which allow us
to reduce the dimensionality of the problem by several magnitudes. We show,
that our network possessing only a few thousand parameters is able to achieve
state-of-the-art performance, while inference remains fast and can be trained
quickly with very little data.
</p>
<a href="http://arxiv.org/abs/2103.03821" target="_blank">arXiv:2103.03821</a> [<a href="http://arxiv.org/pdf/2103.03821" target="_blank">pdf</a>]

<h2>A Deep-Learning Framework to Predict the Dynamics of a Human-Driven Vehicle Based on the Road Geometry. (arXiv:2103.03825v1 [cs.RO])</h2>
<h3>Luca Paparusso, Stefano Melzi, Francesco Braghin</h3>
<p>Many trajectory forecasting methods, implementing deterministic and
stochastic models, have been presented in the last decade for automotive
applications. In this work, a deep-learning framework is proposed to model and
predict the evolution of the coupled driver-vehicle system dynamics.
Particularly, we aim to describe how the road geometry affects the actions
performed by the driver. Differently from other works, the problem is
formulated in such a way that the user may specify the features of interest.
Nonetheless, we propose a set of features that is commonly used for automotive
control applications to practically show the functioning of the algorithm. To
solve the prediction problem, a deep recurrent neural network based on Long
Short-Term Memory autoencoders is designed. It fuses the information on the
road geometry and the past driver-vehicle system dynamics to produce
context-aware predictions. Also, the complexity of the neural network is
constrained to favour its use in online control tasks. The efficacy of the
proposed approach was verified in a case study centered on motion cueing
algorithms, using a dataset collected during test sessions of a
non-professional driver on a dynamic driving simulator. A 3D track with complex
geometry was employed as driving environment to render the prediction task
challenging. Finally, the robustness of the neural network to changes in the
driver and track was investigated to set guidelines for future works.
</p>
<a href="http://arxiv.org/abs/2103.03825" target="_blank">arXiv:2103.03825</a> [<a href="http://arxiv.org/pdf/2103.03825" target="_blank">pdf</a>]

<h2>Multi-Session Visual SLAM for Illumination Invariant Localization in Indoor Environments. (arXiv:2103.03827v1 [cs.RO])</h2>
<h3>Mathieu Labb&#xe9;, Fran&#xe7;ois Michaud</h3>
<p>For robots navigating using only a camera, illumination changes in indoor
environments can cause localization failures during autonomous navigation. In
this paper, we present a multi-session visual SLAM approach to create a map
made of multiple variations of the same locations in different illumination
conditions. The multi-session map can then be used at any hour of the day for
improved localization capability. The approach presented is independent of the
visual features used, and this is demonstrated by comparing localization
performance between multi-session maps created using the RTAB-Map library with
SURF, SIFT, BRIEF, FREAK, BRISK, KAZE, DAISY and SuperPoint visual features.
The approach is tested on six mapping and six localization sessions recorded at
30 minutes intervals during sunset using a Google Tango phone in a real
apartment.
</p>
<a href="http://arxiv.org/abs/2103.03827" target="_blank">arXiv:2103.03827</a> [<a href="http://arxiv.org/pdf/2103.03827" target="_blank">pdf</a>]

<h2>Self-Supervised Longitudinal Neighbourhood Embedding. (arXiv:2103.03840v1 [cs.CV])</h2>
<h3>Jiahong Ouyang, Qingyu Zhao, Ehsan Adeli, Edith V Sullivan, Adolf Pfefferbaum, Greg Zaharchuk, Kilian M Pohl</h3>
<p>Longitudinal MRIs are often used to capture the gradual deterioration of
brain structure and function caused by aging or neurological diseases.
Analyzing this data via machine learning generally requires a large number of
ground-truth labels, which are often missing or expensive to obtain. Reducing
the need for labels, we propose a self-supervised strategy for representation
learning named Longitudinal Neighborhood Embedding (LNE). Motivated by concepts
in contrastive learning, LNE explicitly models the similarity between
trajectory vectors across different subjects. We do so by building a graph in
each training iteration defining neighborhoods in the latent space so that the
progression direction of a subject follows the direction of its neighbors. This
results in a smooth trajectory field that captures the global morphological
change of the brain while maintaining the local continuity. We apply LNE to
longitudinal T1w MRIs of two neuroimaging studies: a dataset composed of 274
healthy subjects, and Alzheimer's Disease Neuroimaging Initiative (ADNI,
$N=632$). The visualization of the smooth trajectory vector field and superior
performance on downstream tasks demonstrate the strength of the proposed method
over existing self-supervised methods in extracting information associated with
normal aging and in revealing the impact of neurodegenerative disorders. The
code is available at
\url{https://github.com/ouyangjiahong/longitudinal-neighbourhood-embedding.git}.
</p>
<a href="http://arxiv.org/abs/2103.03840" target="_blank">arXiv:2103.03840</a> [<a href="http://arxiv.org/pdf/2103.03840" target="_blank">pdf</a>]

<h2>Generating Images with Sparse Representations. (arXiv:2103.03841v1 [cs.CV])</h2>
<h3>Charlie Nash, Jacob Menick, Sander Dieleman, Peter W. Battaglia</h3>
<p>The high dimensionality of images presents architecture and
sampling-efficiency challenges for likelihood-based generative models. Previous
approaches such as VQ-VAE use deep autoencoders to obtain compact
representations, which are more practical as inputs for likelihood-based
models. We present an alternative approach, inspired by common image
compression methods like JPEG, and convert images to quantized discrete cosine
transform (DCT) blocks, which are represented sparsely as a sequence of DCT
channel, spatial location, and DCT coefficient triples. We propose a
Transformer-based autoregressive architecture, which is trained to sequentially
predict the conditional distribution of the next element in such sequences, and
which scales effectively to high resolution images. On a range of image
datasets, we demonstrate that our approach can generate high quality, diverse
images, with sample metric scores competitive with state of the art methods. We
additionally show that simple modifications to our method yield effective image
colorization and super-resolution models.
</p>
<a href="http://arxiv.org/abs/2103.03841" target="_blank">arXiv:2103.03841</a> [<a href="http://arxiv.org/pdf/2103.03841" target="_blank">pdf</a>]

<h2>Optimal Path Planning using CAMIS: a Continuous Anisotropic Model for Inclined Surfaces. (arXiv:2103.03849v1 [cs.RO])</h2>
<h3>J. Ricardo S&#xe1;nchez-Ib&#xe1;&#xf1;ez, Carlos J. P&#xe9;rez-del-Pulgar, Javier Ser&#xf3;n, Alfonso Garc&#xed;a-Cerezo</h3>
<p>The optimal traverse of irregular terrains made by ground mobile robots
heavily depends on the adequacy of the cost models used to plan the path they
follow. The criteria to define optimality may be based on minimizing energy
consumption and/or preserving the robot stability. This entails the proper
assessment of anisotropy to account for the robot driving on top of slopes with
different directions. To fulfill this demand, this paper presents the
Continuous Anisotropic Model for Inclined Surfaces, a cost model compatible
with anisotropic path planners like the bi-directional Ordered Upwind Method.
This model acknowledges how the orientation of the robot with respect to any
slope determines its energetic cost, considering the action of gravity and
terramechanic effects such as the slippage. Moreover, the proposed model can be
tuned to define a trade-off between energy minimization and Roll angle
reduction. The results from two simulation tests demonstrate how, to find the
optimal path in scenarios containing slopes, in certain situations the use of
this model can be more advantageous than relying on isotropic cost functions.
Finally, the outcome of a field experiment involving a skid-steering robot that
drives on top of a real slope is also discussed.
</p>
<a href="http://arxiv.org/abs/2103.03849" target="_blank">arXiv:2103.03849</a> [<a href="http://arxiv.org/pdf/2103.03849" target="_blank">pdf</a>]

<h2>Harnessing Geometric Constraints from Auxiliary Labels to Improve Embedding Functions for One-Shot Learning. (arXiv:2103.03862v1 [cs.CV])</h2>
<h3>Anand Ramakrishnan, Minh Pham, Jacob Whitehill</h3>
<p>We explore the utility of harnessing auxiliary labels (e.g., facial
expression) to impose geometric structure when training embedding models for
one-shot learning (e.g., for face verification). We introduce novel geometric
constraints on the embedding space learned by a deep model using either
manually annotated or automatically detected auxiliary labels. We contrast
their performances (AUC) on four different face datasets(CK+, VGGFace-2, Tufts
Face, and PubFig). Due to the additional structure encoded in the embedding
space, our methods provide a higher verification accuracy (99.7, 86.2, 99.4,
and 79.3% with our proposed TL+PDP+FBV loss, versus 97.5, 72.6, 93.1, and 70.5%
using a standard Triplet Loss on the four datasets, respectively). Our method
is implemented purely in terms of the loss function. It does not require any
changes to the backbone of the embedding functions.
</p>
<a href="http://arxiv.org/abs/2103.03862" target="_blank">arXiv:2103.03862</a> [<a href="http://arxiv.org/pdf/2103.03862" target="_blank">pdf</a>]

<h2>OperA: Attention-Regularized Transformers for Surgical Phase Recognition. (arXiv:2103.03873v1 [cs.CV])</h2>
<h3>Tobias Czempiel, Magdalini Paschali, Daniel Ostler, Seong Tae Kim, Benjamin Busam, Nassir Navab</h3>
<p>In this paper we introduce OperA, a transformer-based model that accurately
predicts surgical phases from long video sequences. A novel attention
regularization loss encourages the model to focus on high-quality frames during
training. Moreover, the attention weights are utilized to identify
characteristic high attention frames for each surgical phase, which could
further be used for surgery summarization. OperA is thoroughly evaluated on two
datasets of laparoscopic cholecystectomy videos, outperforming various
state-of-the-art temporal refinement approaches.
</p>
<a href="http://arxiv.org/abs/2103.03873" target="_blank">arXiv:2103.03873</a> [<a href="http://arxiv.org/pdf/2103.03873" target="_blank">pdf</a>]

<h2>Learning Deep Stochastic Optimal Control Policies using Forward-Backward SDEs. (arXiv:1902.03986v3 [cs.RO] UPDATED)</h2>
<h3>Marcus Pereira, Ziyi Wang, Ioannis Exarchos, Evangelos A. Theodorou</h3>
<p>In this paper we propose a new methodology for decision-making under
uncertainty using recent advancements in the areas of nonlinear stochastic
optimal control theory, applied mathematics, and machine learning. Grounded on
the fundamental relation between certain nonlinear partial differential
equations and forward-backward stochastic differential equations, we develop a
control framework that is scalable and applicable to general classes of
stochastic systems and decision-making problem formulations in robotics and
autonomy. The proposed deep neural network architectures for stochastic control
consist of recurrent and fully connected layers. The performance and
scalability of the aforementioned algorithm are investigated in three
non-linear systems in simulation with and without control constraints. We
conclude with a discussion on future directions and their implications to
robotics.
</p>
<a href="http://arxiv.org/abs/1902.03986" target="_blank">arXiv:1902.03986</a> [<a href="http://arxiv.org/pdf/1902.03986" target="_blank">pdf</a>]

<h2>TIE: Time-Informed Exploration For Robot Motion Planning. (arXiv:2004.05241v2 [cs.RO] UPDATED)</h2>
<h3>Sagar Suhas Joshi, Seth Hutchinson, Panagiotis Tsiotras</h3>
<p>Anytime sampling-based methods are an attractive technique for solving
kino-dynamic motion planning problems. These algorithms scale well to higher
dimensions and can efficiently handle state and control constraints. However,
an intelligent exploration strategy is required to accelerate their convergence
and avoid redundant computations. Using ideas from reachability analysis, this
work defines a "Time-Informed Set", that focuses the search for time-optimal
kino-dynamic planning after an initial solution is found. Such a Time-Informed
Set (TIS) includes all trajectories that can potentially improve the current
best solution and hence exploration outside this set is redundant. Benchmarking
experiments show that an exploration strategy based on the TIS can accelerate
the convergence of sampling-based kino-dynamic motion planners.
</p>
<a href="http://arxiv.org/abs/2004.05241" target="_blank">arXiv:2004.05241</a> [<a href="http://arxiv.org/pdf/2004.05241" target="_blank">pdf</a>]

<h2>Unmanned Aerial Systems for Wildland and Forest Fires. (arXiv:2004.13883v2 [cs.RO] UPDATED)</h2>
<h3>Moulay A. Akhloufi, Nicolas A. Castro, Andy Couturier</h3>
<p>Wildfires represent an important natural risk causing economic losses, human
death and important environmental damage. In recent years, we witness an
increase in fire intensity and frequency. Research has been conducted towards
the development of dedicated solutions for wildland and forest fire assistance
and fighting. Systems were proposed for the remote detection and tracking of
fires. These systems have shown improvements in the area of efficient data
collection and fire characterization within small scale environments. However,
wildfires cover large areas making some of the proposed ground-based systems
unsuitable for optimal coverage. To tackle this limitation, Unmanned Aerial
Systems (UAS) were proposed. UAS have proven to be useful due to their
maneuverability, allowing for the implementation of remote sensing, allocation
strategies and task planning. They can provide a low-cost alternative for the
prevention, detection and real-time support of firefighting. In this paper we
review previous work related to the use of UAS in wildfires. Onboard sensor
instruments, fire perception algorithms and coordination strategies are
considered. In addition, we present some of the recent frameworks proposing the
use of both aerial vehicles and Unmanned Ground Vehicles (UV) for a more
efficient wildland firefighting strategy at a larger scale.
</p>
<a href="http://arxiv.org/abs/2004.13883" target="_blank">arXiv:2004.13883</a> [<a href="http://arxiv.org/pdf/2004.13883" target="_blank">pdf</a>]

<h2>Relevance Attack on Detectors. (arXiv:2008.06822v2 [cs.CV] UPDATED)</h2>
<h3>Sizhe Chen, Fan He, Xiaolin Huang, Kun Zhang</h3>
<p>This paper focuses on high-transferable adversarial attacks on detectors,
which are hard to attack in a black-box manner, because of their
multiple-output characteristics and the diversity across architectures. To
pursue a high attack transferability, one plausible way is to find a common
property across detectors, which facilitates the discovery of common
weaknesses. We are the first to suggest that the relevance map from
interpreters for detectors is such a property. Based on it, we design a
Relevance Attack on Detectors (RAD), which achieves a state-of-the-art
transferability, exceeding existing results by above 20%. On MS COCO, the
detection mAPs for all 8 black-box architectures are more than halved and the
segmentation mAPs are also significantly influenced. Given the great
transferability of RAD, we generate the first adversarial dataset for object
detection and instance segmentation, i.e., Adversarial Objects in COntext
(AOCO), which helps to quickly evaluate and improve the robustness of
detectors.
</p>
<a href="http://arxiv.org/abs/2008.06822" target="_blank">arXiv:2008.06822</a> [<a href="http://arxiv.org/pdf/2008.06822" target="_blank">pdf</a>]

<h2>A review of deep learning in medical imaging: Imaging traits, technology trends, case studies with progress highlights, and future promises. (arXiv:2008.09104v2 [cs.CV] UPDATED)</h2>
<h3>S. Kevin Zhou, Hayit Greenspan, Christos Davatzikos, James S. Duncan, Bram van Ginneken, Anant Madabhushi, Jerry L. Prince, Daniel Rueckert, Ronald M. Summers</h3>
<p>Since its renaissance, deep learning has been widely used in various medical
imaging tasks and has achieved remarkable success in many medical imaging
applications, thereby propelling us into the so-called artificial intelligence
(AI) era. It is known that the success of AI is mostly attributed to the
availability of big data with annotations for a single task and the advances in
high performance computing. However, medical imaging presents unique challenges
that confront deep learning approaches. In this survey paper, we first present
traits of medical imaging, highlight both clinical needs and technical
challenges in medical imaging, and describe how emerging trends in deep
learning are addressing these issues. We cover the topics of network
architecture, sparse and noisy labels, federating learning, interpretability,
uncertainty quantification, etc. Then, we present several case studies that are
commonly found in clinical practice, including digital pathology and chest,
brain, cardiovascular, and abdominal imaging. Rather than presenting an
exhaustive literature survey, we instead describe some prominent research
highlights related to these case study applications. We conclude with a
discussion and presentation of promising future directions.
</p>
<a href="http://arxiv.org/abs/2008.09104" target="_blank">arXiv:2008.09104</a> [<a href="http://arxiv.org/pdf/2008.09104" target="_blank">pdf</a>]

<h2>A Survey of FPGA-Based Robotic Computing. (arXiv:2009.06034v3 [cs.RO] UPDATED)</h2>
<h3>Zishen Wan, Bo Yu, Thomas Yuang Li, Jie Tang, Yuhao Zhu, Yu Wang, Arijit Raychowdhury, Shaoshan Liu</h3>
<p>Recent researches on robotics have shown significant improvement, spanning
from algorithms, mechanics to hardware architectures. Robotics, including
manipulators, legged robots, drones, and autonomous vehicles, are now widely
applied in diverse scenarios. However, the high computation and data complexity
of robotic algorithms pose great challenges to its applications. On the one
hand, CPU platform is flexible to handle multiple robotic tasks. GPU platform
has higher computational capacities and easy-touse development frameworks, so
they have been widely adopted in several applications. On the other hand,
FPGA-based robotic accelerators are becoming increasingly competitive
alternatives, especially in latency-critical and power-limited scenarios. With
specialized designed hardware logic and algorithm kernels, FPGA-based
accelerators can surpass CPU and GPU in performance and energy efficiency. In
this paper, we give an overview of previous work on FPGA-based robotic
accelerators covering different stages of the robotic system pipeline. An
analysis of software and hardware optimization techniques and main technical
issues is presented, along with some commercial and space applications, to
serve as a guide for future work.
</p>
<a href="http://arxiv.org/abs/2009.06034" target="_blank">arXiv:2009.06034</a> [<a href="http://arxiv.org/pdf/2009.06034" target="_blank">pdf</a>]

<h2>Multi-Label Activity Recognition using Activity-specific Features and Activity Correlations. (arXiv:2009.07420v2 [cs.CV] UPDATED)</h2>
<h3>Yanyi Zhang, Xinyu Li, Ivan Marsic</h3>
<p>Multi-label activity recognition is designed for recognizing multiple
activities that are performed simultaneously or sequentially in each video.
Most recent activity recognition networks focus on single-activities, that
assume only one activity in each video. These networks extract shared features
for all the activities, which are not designed for multi-label activities. We
introduce an approach to multi-label activity recognition that extracts
independent feature descriptors for each activity and learns activity
correlations. This structure can be trained end-to-end and plugged into any
existing network structures for video classification. Our method outperformed
state-of-the-art approaches on four multi-label activity recognition datasets.
To better understand the activity-specific features that the system generated,
we visualized these activity-specific features in the Charades dataset.
</p>
<a href="http://arxiv.org/abs/2009.07420" target="_blank">arXiv:2009.07420</a> [<a href="http://arxiv.org/pdf/2009.07420" target="_blank">pdf</a>]

<h2>VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning. (arXiv:2009.13682v2 [cs.CV] UPDATED)</h2>
<h3>Xiaowei Hu, Xi Yin, Kevin Lin, Lijuan Wang, Lei Zhang, Jianfeng Gao, Zicheng Liu</h3>
<p>It is highly desirable yet challenging to generate image captions that can
describe novel objects which are unseen in caption-labeled training data, a
capability that is evaluated in the novel object captioning challenge (nocaps).
In this challenge, no additional image-caption training data, other thanCOCO
Captions, is allowed for model training. Thus, conventional Vision-Language
Pre-training (VLP) methods cannot be applied. This paper presents VIsual
VOcabulary pretraining (VIVO) that performs pre-training in the absence of
caption annotations. By breaking the dependency of paired image-caption
training data in VLP, VIVO can leverage large amounts of paired image-tag data
to learn a visual vocabulary. This is done by pre-training a multi-layer
Transformer model that learns to align image-level tags with their
corresponding image region features. To address the unordered nature of image
tags, VIVO uses a Hungarian matching loss with masked tag prediction to conduct
pre-training. We validate the effectiveness of VIVO by fine-tuning the
pre-trained model for image captioning. In addition, we perform an analysis of
the visual-text alignment inferred by our model. The results show that our
model can not only generate fluent image captions that describe novel objects,
but also identify the locations of these objects. Our single model has achieved
new state-of-the-art results on nocaps and surpassed the human CIDEr score.
</p>
<a href="http://arxiv.org/abs/2009.13682" target="_blank">arXiv:2009.13682</a> [<a href="http://arxiv.org/pdf/2009.13682" target="_blank">pdf</a>]

<h2>Non-anchor-based vehicle detection for traffic surveillance using bounding ellipses. (arXiv:2010.02059v2 [cs.CV] UPDATED)</h2>
<h3>Byeonghyeop Yu, Johyun Shin, Gyeongjun Kim, Seungbin Roh, Keemin Sohn</h3>
<p>Cameras for traffic surveillance are usually pole-mounted and produce images
that reflect a birds-eye view. Vehicles in such images, in general, assume an
ellipse form. A bounding box for the vehicles usually includes a large empty
space when the vehicle orientation is not parallel to the edges of the box. To
circumvent this problem, the present study applied bounding ellipses to a
non-anchor-based, single-shot detection model (CenterNet). Since this model
does not depend on anchor boxes, non-max suppression (NMS) that requires
computing the intersection over union (IOU) between predicted bounding boxes is
unnecessary for inference. The SpotNet that extends the CenterNet model by
adding a segmentation head was also tested with bounding ellipses. Two other
anchor-based, single-shot detection models (YOLO4 and SSD) were chosen as
references for comparison. The model performance was compared based on a local
dataset that was doubly annotated with bounding boxes and ellipses. As a
result, the performance of the two models with bounding ellipses exceeded that
of the reference models with bounding boxes. When the backbone of the ellipse
models was pretrained on an open dataset (UA-DETRAC), the performance was
further enhanced. The data augmentation schemes developed for YOLO4 also
improved the performance of the proposed models. As a result, the best mAP
score of a CenterNet with bounding ellipses exceeds 0.9.
</p>
<a href="http://arxiv.org/abs/2010.02059" target="_blank">arXiv:2010.02059</a> [<a href="http://arxiv.org/pdf/2010.02059" target="_blank">pdf</a>]

<h2>Uncertainty-aware Contact-safe Model-based Reinforcement Learning. (arXiv:2010.08169v2 [cs.RO] UPDATED)</h2>
<h3>Cheng-Yu Kuo, Andreas Schaarschmidt, Yunduan Cui, Tamim Asfour, Takamitsu Matsubara</h3>
<p>This letter presents contact-safe Model-based Reinforcement Learning (MBRL)
for robot applications that achieves contact-safe behaviors in the learning
process. In typical MBRL, we cannot expect the data-driven model to generate
accurate and reliable policies to the intended robotic tasks during the
learning process due to sample scarcity. Operating these unreliable policies in
a contact-rich environment could cause damage to the robot and its
surroundings. To alleviate the risk of causing damage through unexpected
intensive physical contacts, we present the contact-safe MBRL that associates
the probabilistic Model Predictive Control's (pMPC) control limits with the
model uncertainty so that the allowed acceleration of controlled behavior is
adjusted according to learning progress. Control planning with such
uncertainty-aware control limits is formulated as a deterministic MPC problem
using a computation-efficient approximated GP dynamics and an approximated
inference technique. Our approach's effectiveness is evaluated through bowl
mixing tasks with simulated and real robots, scooping tasks with a real robot
as examples of contact-rich manipulation skills. (video:
https://youtu.be/LfzYhJaHies)
</p>
<a href="http://arxiv.org/abs/2010.08169" target="_blank">arXiv:2010.08169</a> [<a href="http://arxiv.org/pdf/2010.08169" target="_blank">pdf</a>]

<h2>A Unified Approach for Autonomous Volumetric Exploration of Large Scale Environments under Severe Odometry Drift. (arXiv:2010.09859v2 [cs.RO] UPDATED)</h2>
<h3>Lukas Schmid, Victor Reijgwart, Lionel Ott, Juan Nieto, Roland Siegwart, Cesar Cadena</h3>
<p>Exploration is a fundamental problem in robot autonomy. A major limitation,
however, is that during exploration robots oftentimes have to rely on on-board
systems alone for state estimation, accumulating significant drift over time in
large environments. Drift can be detrimental to robot safety and exploration
performance. In this work, a submap-based, multi-layer approach for both
mapping and planning is proposed to enable safe and efficient volumetric
exploration of large scale environments despite odometry drift. The central
idea of our approach combines local (temporally and spatially) and global
mapping to guarantee safety and efficiency. Similarly, our planning approach
leverages the presented map to compute global volumetric frontiers in a
changing global map and utilizes the nature of exploration dealing with partial
information for efficient local and global planning. The presented system is
thoroughly evaluated and shown to outperform state of the art methods even
under drift-free conditions. Our system, termed GLoca}, will be made available
open source.
</p>
<a href="http://arxiv.org/abs/2010.09859" target="_blank">arXiv:2010.09859</a> [<a href="http://arxiv.org/pdf/2010.09859" target="_blank">pdf</a>]

<h2>Dynamic Movement Primitive based Motion Retargeting for Dual-Arm Sign Language Motions. (arXiv:2011.03914v2 [cs.RO] UPDATED)</h2>
<h3>Yuwei Liang, Weijie Li, Yue Wang, Rong Xiong, Yichao Mao, Jiafan Zhang</h3>
<p>We aim to develop an efficient programming method for equipping service
robots with the skill of performing sign language motions. This paper addresses
the problem of transferring complex dual-arm sign language motions
characterized by the coordination among arms and hands from human to robot,
which is seldom considered in previous studies of motion retargeting
techniques. In this paper, we propose a novel motion retargeting method that
leverages graph optimization and Dynamic Movement Primitives (DMPs) for this
problem. We employ DMPs in a leader-follower manner to parameterize the
original trajectories while preserving motion rhythm and relative movements
between human body parts, and adopt a three-step optimization procedure to find
deformed trajectories for robot motion planning while ensuring feasibility for
robot execution. Experimental results of several Chinese Sign Language (CSL)
motions have been successfully performed on ABB's YuMi dual-arm collaborative
robot (14-DOF) with two 6-DOF Inspire-Robotics' multi-fingered hands, a system
with 26 DOFs in total.
</p>
<a href="http://arxiv.org/abs/2011.03914" target="_blank">arXiv:2011.03914</a> [<a href="http://arxiv.org/pdf/2011.03914" target="_blank">pdf</a>]

<h2>Neural Scene Graphs for Dynamic Scenes. (arXiv:2011.10379v3 [cs.CV] UPDATED)</h2>
<h3>Julian Ost, Fahim Mannan, Nils Thuerey, Julian Knodt, Felix Heide</h3>
<p>Recent implicit neural rendering methods have demonstrated that it is
possible to learn accurate view synthesis for complex scenes by predicting
their volumetric density and color supervised solely by a set of RGB images.
However, existing methods are restricted to learning efficient representations
of static scenes that encode all scene objects into a single neural network,
and lack the ability to represent dynamic scenes and decompositions into
individual scene objects. In this work, we present the first neural rendering
method that decomposes dynamic scenes into scene graphs. We propose a learned
scene graph representation, which encodes object transformation and radiance,
to efficiently render novel arrangements and views of the scene. To this end,
we learn implicitly encoded scenes, combined with a jointly learned latent
representation to describe objects with a single implicit function. We assess
the proposed method on synthetic and real automotive data, validating that our
approach learns dynamic scenes -- only by observing a video of this scene --
and allows for rendering novel photo-realistic views of novel scene
compositions with unseen sets of objects at unseen poses.
</p>
<a href="http://arxiv.org/abs/2011.10379" target="_blank">arXiv:2011.10379</a> [<a href="http://arxiv.org/pdf/2011.10379" target="_blank">pdf</a>]

<h2>Theoretical Evidence Supporting Harmonic Reaching Trajectories. (arXiv:2012.00453v2 [cs.RO] UPDATED)</h2>
<h3>Carlo Tiseo, Sydney Rebecca Charitos, Michael Mistry</h3>
<p>Minimum Jerk trajectories have been long thought to be the reference
trajectories for human movements due to their impressive similarity with human
movements. Nevertheless, minimum jerk trajectories are not the only choice for
$C^\infty$ (i.e., smooth) functions. For example, harmonic trajectories are
smooth functions that can be superimposed to describe the evolution of physical
systems. This paper analyses the possibility that motor control plans using
harmonic trajectories, will be experimentally observed to have a minimum jerk
likeness due to control signals being transported through the Central Nervous
System (CNS) and muscle-skeletal system. We tested our theory on a 3-link arm
simulation using a recently developed planner that we reformulated into a motor
control architecture, inspired by the passive motion paradigm. The arm
performed 100 movements, reaching for each target defined by the clock
experiment. We analysed the shape of the trajectory planned in the CNS and
executed in the physical simulator. We observed that even under ideal
conditions (i.e., absence of delays and noise) the executed trajectories are
similar to a minimum jerk trajectory; thus, supporting the thesis that the
human brain might plan harmonic trajectories.
</p>
<a href="http://arxiv.org/abs/2012.00453" target="_blank">arXiv:2012.00453</a> [<a href="http://arxiv.org/pdf/2012.00453" target="_blank">pdf</a>]

<h2>SAFFIRE: System for Autonomous Feature Filtering and Intelligent ROI Estimation. (arXiv:2012.02502v2 [cs.CV] UPDATED)</h2>
<h3>Marco Boschi, Luigi Di Stefano, Martino Alessandrini</h3>
<p>This work introduces a new framework, named SAFFIRE, to automatically extract
a dominant recurrent image pattern from a set of image samples. Such a pattern
shall be used to eliminate pose variations between samples, which is a common
requirement in many computer vision and machine learning tasks. The framework
is specialized here in the context of a machine vision system for automated
product inspection. Here, it is customary to ask the user for the
identification of an anchor pattern, to be used by the automated system to
normalize data before further processing. Yet, this is a very sensitive
operation which is intrinsically subjective and requires high expertise.
Hereto, SAFFIRE provides a unique and disruptive framework for unsupervised
identification of an optimal anchor pattern in a way which is fully transparent
to the user. SAFFIRE is thoroughly validated on several realistic case studies
for a machine vision inspection pipeline.
</p>
<a href="http://arxiv.org/abs/2012.02502" target="_blank">arXiv:2012.02502</a> [<a href="http://arxiv.org/pdf/2012.02502" target="_blank">pdf</a>]

<h2>Critical Evaluation of Deep Neural Networks for Wrist Fracture Detection. (arXiv:2012.02577v2 [cs.CV] UPDATED)</h2>
<h3>Abu Mohammed Raisuddin, Elias Vaattovaara, Mika Nevalainen, Marko Nikki, Elina J&#xe4;rvenp&#xe4;&#xe4;, Kaisa Makkonen, Pekka Pinola, Tuula Palsio, Arttu Niemensivu, Osmo Tervonen, Aleksei Tiulpin</h3>
<p>Wrist Fracture is the most common type of fracture with a high incidence
rate. Conventional radiography (i.e. X-ray imaging) is used for wrist fracture
detection routinely, but occasionally fracture delineation poses issues and an
additional confirmation by computed tomography (CT) is needed for diagnosis.
Recent advances in the field of Deep Learning (DL), a subfield of Artificial
Intelligence (AI), have shown that wrist fracture detection can be automated
using Convolutional Neural Networks. However, previous studies did not pay
close attention to the difficult cases which can only be confirmed via CT
imaging. In this study, we have developed and analyzed a state-of-the-art
DL-based pipeline for wrist (distal radius) fracture detection -- DeepWrist,
and evaluated it against one general population test set, and one challenging
test set comprising only cases requiring confirmation by CT. Our results reveal
that a typical state-of-the-art approach, such as DeepWrist, while having a
near-perfect performance on the general independent test set, has a
substantially lower performance on the challenging test set -- average
precision of 0.99 (0.99-0.99) vs 0.64 (0.46-0.83), respectively. Similarly, the
area under the ROC curve was of 0.99 (0.98-0.99) vs 0.84 (0.72-0.93),
respectively. Our findings highlight the importance of a meticulous analysis of
DL-based models before clinical use, and unearth the need for more challenging
settings for testing medical AI systems.
</p>
<a href="http://arxiv.org/abs/2012.02577" target="_blank">arXiv:2012.02577</a> [<a href="http://arxiv.org/pdf/2012.02577" target="_blank">pdf</a>]

<h2>FlowMOT: 3D Multi-Object Tracking by Scene Flow Association. (arXiv:2012.07541v3 [cs.CV] UPDATED)</h2>
<h3>Guangyao Zhai, Xin Kong, Jinhao Cui, Yong Liu, Zhen Yang</h3>
<p>Most end-to-end Multi-Object Tracking (MOT) methods face the problems of low
accuracy and poor generalization ability. Although traditional filter-based
methods can achieve better results, they are difficult to be endowed with
optimal hyperparameters and often fail in varying scenarios. To alleviate these
drawbacks, we propose a LiDAR-based 3D MOT framework named FlowMOT, which
integrates point-wise motion information with the traditional matching
algorithm, enhancing the robustness of the motion prediction. We firstly
utilize a scene flow estimation network to obtain implicit motion information
between two adjacent frames and calculate the predicted detection for each old
tracklet in the previous frame. Then we use Hungarian algorithm to generate
optimal matching relations with the ID propagation strategy to finish the
tracking task. Experiments on KITTI MOT dataset show that our approach
outperforms recent end-to-end methods and achieves competitive performance with
the state-of-the-art filter-based method. In addition, ours can work steadily
in the various-speed scenarios where the filter-based methods may fail.
</p>
<a href="http://arxiv.org/abs/2012.07541" target="_blank">arXiv:2012.07541</a> [<a href="http://arxiv.org/pdf/2012.07541" target="_blank">pdf</a>]

<h2>KOALAnet: Blind Super-Resolution using Kernel-Oriented Adaptive Local Adjustment. (arXiv:2012.08103v2 [cs.CV] UPDATED)</h2>
<h3>Soo Ye Kim, Hyeonjun Sim, Munchurl Kim</h3>
<p>Blind super-resolution (SR) methods aim to generate a high quality high
resolution image from a low resolution image containing unknown degradations.
However, natural images contain various types and amounts of blur: some may be
due to the inherent degradation characteristics of the camera, but some may
even be intentional, for aesthetic purposes (eg. Bokeh effect). In the case of
the latter, it becomes highly difficult for SR methods to disentangle the blur
to remove, and that to leave as is. In this paper, we propose a novel blind SR
framework based on kernel-oriented adaptive local adjustment (KOALA) of SR
features, called KOALAnet, which jointly learns spatially-variant degradation
and restoration kernels in order to adapt to the spatially-variant blur
characteristics in real images. Our KOALAnet outperforms recent blind SR
methods for synthesized LR images obtained with randomized degradations, and we
further show that the proposed KOALAnet produces the most natural results for
artistic photographs with intentional blur, which are not over-sharpened, by
effectively handling images mixed with in-focus and out-of-focus areas.
</p>
<a href="http://arxiv.org/abs/2012.08103" target="_blank">arXiv:2012.08103</a> [<a href="http://arxiv.org/pdf/2012.08103" target="_blank">pdf</a>]

<h2>Regularization Strategy for Point Cloud via Rigidly Mixed Sample. (arXiv:2102.01929v2 [cs.CV] UPDATED)</h2>
<h3>Dogyoon Lee, Jaeha Lee, Junhyeop Lee, Hyeongmin Lee, Minhyeok Lee, Sungmin Woo, Sangyoun Lee</h3>
<p>Data augmentation is an effective regularization strategy to alleviate the
overfitting, which is an inherent drawback of the deep neural networks.
However, data augmentation is rarely considered for point cloud processing
despite many studies proposing various augmentation methods for image data.
Actually, regularization is essential for point clouds since lack of generality
is more likely to occur in point cloud due to small datasets. This paper
proposes a Rigid Subset Mix (RSMix), a novel data augmentation method for point
clouds that generates a virtual mixed sample by replacing part of the sample
with shape-preserved subsets from another sample. RSMix preserves structural
information of the point cloud sample by extracting subsets from each sample
without deformation using a neighboring function. The neighboring function was
carefully designed considering unique properties of point cloud, unordered
structure and non-grid. Experiments verified that RSMix successfully
regularized the deep neural networks with remarkable improvement for shape
classification. We also analyzed various combinations of data augmentations
including RSMix with single and multi-view evaluations, based on abundant
ablation studies.
</p>
<a href="http://arxiv.org/abs/2102.01929" target="_blank">arXiv:2102.01929</a> [<a href="http://arxiv.org/pdf/2102.01929" target="_blank">pdf</a>]

<h2>Adversarial Branch Architecture Search for Unsupervised Domain Adaptation. (arXiv:2102.06679v2 [cs.CV] UPDATED)</h2>
<h3>Luca Robbiano, Muhammad Rameez Ur Rahman, Fabio Galasso, Barbara Caputo, Fabio Maria Carlucci</h3>
<p>Unsupervised Domain Adaptation (UDA) is a key field in visual recognition, as
it enables robust performances across different visual domains. In the deep
learning era, the performance of UDA methods has been driven by better losses
and by improved network architectures, specifically the addition of auxiliary
domain-alignment branches to pre-trained backbones. However, all the neural
architectures proposed so far are hand-crafted, which might hinder further
progress.

The current copious offspring of Neural Architecture Search (NAS) only
alleviates hand-crafting so far, as it requires labels for model selection,
which are not available in UDA, and is usually applied to the whole
architecture, while using pre-trained models is a strict requirement for high
performance. No prior work has addressed these aspects in the context of NAS
for UDA.

Here we propose an Adversarial Branch Architecture Search (ABAS) for UDA, to
learn the auxiliary branch network from data without handcrafting. Our main
contribution include i. a novel data-driven ensemble approach for model
selection, to circumvent the lack of target labels, and ii. a pipeline to
automatically search for the best performing auxiliary branch.

To the best of our knowledge, ABAS is the first NAS method for UDA to comply
with a pre-trained backbone, a strict requirement for high performance. ABAS
outputs both the optimal auxiliary branch and its trained parameters. When
applied to two modern UDA techniques, DANN and ALDA, it improves performance on
three standard CV datasets (Office31, Office-Home and PACS). In all cases, ABAS
robustly finds the branch architectures which yield best performances. Code
will be released.
</p>
<a href="http://arxiv.org/abs/2102.06679" target="_blank">arXiv:2102.06679</a> [<a href="http://arxiv.org/pdf/2102.06679" target="_blank">pdf</a>]

<h2>OpenICS: Open Image Compressive Sensing Toolbox and Benchmark. (arXiv:2103.00652v2 [cs.CV] UPDATED)</h2>
<h3>Jonathan Zhao, Matthew Westerham, Mark Lakatos-Toth, Zhikang Zhang, Avi Moskoff, Fengbo Ren</h3>
<p>We present OpenICS, an image compressive sensing toolbox that includes
multiple image compressive sensing and reconstruction algorithms proposed in
the past decade. Due to the lack of standardization in the implementation and
evaluation of the proposed algorithms, the application of image compressive
sensing in the real-world is limited. We believe this toolbox is the first
framework that provides a unified and standardized implementation of multiple
image compressive sensing algorithms. In addition, we also conduct a
benchmarking study on the methods included in this framework from two aspects:
reconstruction accuracy and reconstruction efficiency. We wish this toolbox and
benchmark can serve the growing research community of compressive sensing and
the industry applying image compressive sensing to new problems as well as
developing new methods more efficiently. Code and models are available at
https://github.com/PSCLab-ASU/OpenICS. The project is still under maintenance,
and we will keep this document updated.
</p>
<a href="http://arxiv.org/abs/2103.00652" target="_blank">arXiv:2103.00652</a> [<a href="http://arxiv.org/pdf/2103.00652" target="_blank">pdf</a>]

<h2>Diverse Critical Interaction Generation for Planning and Planner Evaluation. (arXiv:2103.00906v2 [cs.RO] UPDATED)</h2>
<h3>Zhao-Heng Yin, Lingfeng Sun, Liting Sun, Masayoshi Tomizuka, Wei Zhan</h3>
<p>Generating diverse and comprehensive interacting agents to evaluate the
decision-making modules is essential for the safe and robust planning of
autonomous vehicles~(AV). Due to efficiency and safety concerns, most
researchers choose to train interactive adversary~(competitive or weakly
competitive) agents in simulators and generate test cases to interact with
evaluated AVs. However, most existing methods fail to provide both natural and
critical interaction behaviors in various traffic scenarios. To tackle this
problem, we propose a styled generative model RouteGAN that generates diverse
interactions by controlling the vehicles separately with desired styles. By
altering its style coefficients, the model can generate trajectories with
different safety levels serve as an online planner. Experiments show that our
model can generate diverse interactions in various scenarios. We evaluate
different planners with our model by testing their collision rate in
interaction with RouteGAN planners of multiple critical levels.
</p>
<a href="http://arxiv.org/abs/2103.00906" target="_blank">arXiv:2103.00906</a> [<a href="http://arxiv.org/pdf/2103.00906" target="_blank">pdf</a>]

<h2>Diversifying Sample Generation for Accurate Data-Free Quantization. (arXiv:2103.01049v2 [cs.CV] UPDATED)</h2>
<h3>Xiangguo Zhang, Haotong Qin, Yifu Ding, Ruihao Gong, Qinghua Yan, Renshuai Tao, Yuhang Li, Fengwei Yu, Xianglong Liu</h3>
<p>Quantization has emerged as one of the most prevalent approaches to compress
and accelerate neural networks. Recently, data-free quantization has been
widely studied as a practical and promising solution. It synthesizes data for
calibrating the quantized model according to the batch normalization (BN)
statistics of FP32 ones and significantly relieves the heavy dependency on real
training data in traditional quantization methods. Unfortunately, we find that
in practice, the synthetic data identically constrained by BN statistics
suffers serious homogenization at both distribution level and sample level and
further causes a significant performance drop of the quantized model. We
propose Diverse Sample Generation (DSG) scheme to mitigate the adverse effects
caused by homogenization. Specifically, we slack the alignment of feature
statistics in the BN layer to relax the constraint at the distribution level
and design a layerwise enhancement to reinforce specific layers for different
data samples. Our DSG scheme is versatile and even able to be applied to the
state-of-the-art post-training quantization method like AdaRound. We evaluate
the DSG scheme on the large-scale image classification task and consistently
obtain significant improvements over various network architectures and
quantization methods, especially when quantized to lower bits (e.g., up to 22%
improvement on W4A4). Moreover, benefiting from the enhanced diversity, models
calibrated by synthetic data perform close to those calibrated by real data and
even outperform them on W4A4.
</p>
<a href="http://arxiv.org/abs/2103.01049" target="_blank">arXiv:2103.01049</a> [<a href="http://arxiv.org/pdf/2103.01049" target="_blank">pdf</a>]

<h2>BPActuators: Lightweight and Low-Cost Soft Actuators by Balloons and Plastics. (arXiv:2103.01409v2 [cs.RO] UPDATED)</h2>
<h3>Qiukai Qi, Shogo Yoshida, Genki Kakihana, Takuma Torii, Van Anh Ho, Haoran Xie</h3>
<p>To increase the awareness and impact, soft robotics needs to go beyond the
lab environment and should be readily accessible to those even with no robotic
expertise. However, most prevailing manufacturing methodologies require either
professional equipment or materials that are not usually available to common
people, thereby constraining the accessibility of soft robotics. In this
communication, we propose a lightweight and low-cost soft bending actuator,
called BPActuator, that can be easily fabricated with plastics and balloons. We
fabricated a range of actuators with various morphology for characterization in
terms of deformation and load-bearing capacity, and demonstrated that they can
bend up to 35 degrees and exert force at the tip around 0.070$\pm$0.015N, which
is over 5 times higher than their average gravity. We further implemented a
gripper with three fingers using the proposed actuators, and found that the
gripper can realize human-like grasp of a range of daily objects. The gripper
can lift objects at least 8 times heavier than its own weight. Furthermore, the
BPActuator is cost effective and each costs about 0.22 USD. Given these
advantages, the BPActuators are expected to significantly improve the
accessibility of soft robotics to a wider group without robotic expertise.
</p>
<a href="http://arxiv.org/abs/2103.01409" target="_blank">arXiv:2103.01409</a> [<a href="http://arxiv.org/pdf/2103.01409" target="_blank">pdf</a>]

<h2>Model-based Constrained Reinforcement Learning using Generalized Control Barrier Function. (arXiv:2103.01556v2 [cs.RO] UPDATED)</h2>
<h3>Haitong Ma, Jianyu Chen, Shengbo Eben Li, Ziyu Lin, Yang Guan, Yangang Ren, Sifa Zheng</h3>
<p>Model information can be used to predict future trajectories, so it has huge
potential to avoid dangerous region when implementing reinforcement learning
(RL) on real-world tasks, like autonomous driving. However, existing studies
mostly use model-free constrained RL, which causes inevitable constraint
violations. This paper proposes a model-based feasibility enhancement technique
of constrained RL, which enhances the feasibility of policy using generalized
control barrier function (GCBF) defined on the distance to constraint boundary.
By using the model information, the policy can be optimized safely without
violating actual safety constraints, and the sample efficiency is increased.
The major difficulty of infeasibility in solving the constrained policy
gradient is handled by an adaptive coefficient mechanism. We evaluate the
proposed method in both simulations and real vehicle experiments in a complex
autonomous driving collision avoidance task. The proposed method achieves up to
four times fewer constraint violations and converges 3.36 times faster than
baseline constrained RL approaches.
</p>
<a href="http://arxiv.org/abs/2103.01556" target="_blank">arXiv:2103.01556</a> [<a href="http://arxiv.org/pdf/2103.01556" target="_blank">pdf</a>]

<h2>Geometry-Guided Street-View Panorama Synthesis from Satellite Imagery. (arXiv:2103.01623v2 [cs.CV] UPDATED)</h2>
<h3>Yujiao Shi, Dylan Campbell, Xin Yu, Hongdong Li</h3>
<p>This paper presents a new approach for synthesizing a novel street-view
panorama given an overhead satellite image. Taking a small satellite image
patch as input, our method generates a Google's omnidirectional street-view
type panorama, as if it is captured from the same geographical location as the
center of the satellite patch. Existing works tackle this task as an image
generation problem which adopts generative adversarial networks to implicitly
learn the cross-view transformations, while ignoring the domain relevance. In
this paper, we propose to explicitly establish the geometric correspondences
between the two-view images so as to facilitate the cross-view transformation
learning. Specifically, we observe that when a 3D point in the real world is
visible in both views, there is a deterministic mapping between the projected
points in the two-view images given the height information of this 3D point.
Motivated by this, we develop a novel Satellite to Street-view image Projection
(S2SP) module which explicitly establishes such geometric correspondences and
projects the satellite images to the street viewpoint. With these projected
satellite images as network input, we next employ a generator to synthesize
realistic street-view panoramas that are geometrically consistent with the
satellite images. Our S2SP module is differentiable and the whole framework is
trained in an end-to-end manner. Extensive experimental results on two
cross-view benchmark datasets demonstrate that our method generates images that
better respect the scene geometry than existing approaches.
</p>
<a href="http://arxiv.org/abs/2103.01623" target="_blank">arXiv:2103.01623</a> [<a href="http://arxiv.org/pdf/2103.01623" target="_blank">pdf</a>]

<h2>Spatial-Phase Shallow Learning: Rethinking Face Forgery Detection in Frequency Domain. (arXiv:2103.01856v2 [cs.CV] UPDATED)</h2>
<h3>Honggu Liu, Xiaodan Li, Wenbo Zhou, Yuefeng Chen, Yuan He, Hui Xue, Weiming Zhang, Nenghai Yu</h3>
<p>The remarkable success in face forgery techniques has received considerable
attention in computer vision due to security concerns. We observe that
up-sampling is a necessary step of most face forgery techniques, and cumulative
up-sampling will result in obvious changes in the frequency domain, especially
in the phase spectrum. According to the property of natural images, the phase
spectrum preserves abundant frequency components that provide extra information
and complement the loss of the amplitude spectrum. To this end, we present a
novel Spatial-Phase Shallow Learning (SPSL) method, which combines spatial
image and phase spectrum to capture the up-sampling artifacts of face forgery
to improve the transferability, for face forgery detection. And we also
theoretically analyze the validity of utilizing the phase spectrum. Moreover,
we notice that local texture information is more crucial than high-level
semantic information for the face forgery detection task. So we reduce the
receptive fields by shallowing the network to suppress high-level features and
focus on the local region. Extensive experiments show that SPSL can achieve the
state-of-the-art performance on cross-datasets evaluation as well as
multi-class classification and obtain comparable results on single dataset
evaluation.
</p>
<a href="http://arxiv.org/abs/2103.01856" target="_blank">arXiv:2103.01856</a> [<a href="http://arxiv.org/pdf/2103.01856" target="_blank">pdf</a>]

<h2>Meta-Learning-Based Robust Adaptive Flight Control Under Uncertain Wind Conditions. (arXiv:2103.01932v2 [cs.RO] UPDATED)</h2>
<h3>Michael O&#x27;Connell, Guanya Shi, Xichen Shi, Soon-Jo Chung</h3>
<p>Realtime model learning proves challenging for complex dynamical systems,
such as drones flying in variable wind conditions. Machine learning technique
such as deep neural networks have high representation power but is often too
slow to update onboard. On the other hand, adaptive control relies on simple
linear parameter models can update as fast as the feedback control loop. We
propose an online composite adaptation method that treats outputs from a deep
neural network as a set of basis functions capable of representing different
wind conditions. To help with training, meta-learning techniques are used to
optimize the network output useful for adaptation. We validate our approach by
flying a drone in an open air wind tunnel under varying wind conditions and
along challenging trajectories. We compare the result with other adaptive
controller with different basis function sets and show improvement over
tracking and prediction errors.
</p>
<a href="http://arxiv.org/abs/2103.01932" target="_blank">arXiv:2103.01932</a> [<a href="http://arxiv.org/pdf/2103.01932" target="_blank">pdf</a>]

<h2>Self-supervised Pretraining of Visual Features in the Wild. (arXiv:2103.01988v2 [cs.CV] UPDATED)</h2>
<h3>Priya Goyal, Mathilde Caron, Benjamin Lefaudeux, Min Xu, Pengchao Wang, Vivek Pai, Mannat Singh, Vitaliy Liptchinsky, Ishan Misra, Armand Joulin, Piotr Bojanowski</h3>
<p>Recently, self-supervised learning methods like MoCo, SimCLR, BYOL and SwAV
have reduced the gap with supervised methods. These results have been achieved
in a control environment, that is the highly curated ImageNet dataset. However,
the premise of self-supervised learning is that it can learn from any random
image and from any unbounded dataset. In this work, we explore if
self-supervision lives to its expectation by training large models on random,
uncurated images with no supervision. Our final SElf-supERvised (SEER) model, a
RegNetY with 1.3B parameters trained on 1B random images with 512 GPUs achieves
84.2% top-1 accuracy, surpassing the best self-supervised pretrained model by
1% and confirming that self-supervised learning works in a real world setting.
Interestingly, we also observe that self-supervised models are good few-shot
learners achieving 77.9% top-1 with access to only 10% of ImageNet. Code:
https://github.com/facebookresearch/vissl
</p>
<a href="http://arxiv.org/abs/2103.01988" target="_blank">arXiv:2103.01988</a> [<a href="http://arxiv.org/pdf/2103.01988" target="_blank">pdf</a>]

<h2>Multi-attentional Deepfake Detection. (arXiv:2103.02406v2 [cs.CV] UPDATED)</h2>
<h3>Hanqing Zhao, Wenbo Zhou, Dongdong Chen, Tianyi Wei, Weiming Zhang, Nenghai Yu</h3>
<p>Face forgery by deepfake is widely spread over the internet and has raised
severe societal concerns. Recently, how to detect such forgery contents has
become a hot research topic and many deepfake detection methods have been
proposed. Most of them model deepfake detection as a vanilla binary
classification problem, i.e, first use a backbone network to extract a global
feature and then feed it into a binary classifier (real/fake). But since the
difference between the real and fake images in this task is often subtle and
local, we argue this vanilla solution is not optimal. In this paper, we instead
formulate deepfake detection as a fine-grained classification problem and
propose a new multi-attentional deepfake detection network. Specifically, it
consists of three key components: 1) multiple spatial attention heads to make
the network attend to different local parts; 2) textural feature enhancement
block to zoom in the subtle artifacts in shallow features; 3) aggregate the
low-level textural feature and high-level semantic features guided by the
attention maps. Moreover, to address the learning difficulty of this network,
we further introduce a new regional independence loss and an attention guided
data augmentation strategy. Through extensive experiments on different
datasets, we demonstrate the superiority of our method over the vanilla binary
classifier counterparts, and achieve state-of-the-art performance.
</p>
<a href="http://arxiv.org/abs/2103.02406" target="_blank">arXiv:2103.02406</a> [<a href="http://arxiv.org/pdf/2103.02406" target="_blank">pdf</a>]

<h2>A comprehensive survey on point cloud registration. (arXiv:2103.02690v2 [cs.CV] UPDATED)</h2>
<h3>Xiaoshui Huang, Guofeng Mei, Jian Zhang, Rana Abbas</h3>
<p>Registration is a transformation estimation problem between two point clouds,
which has a unique and critical role in numerous computer vision applications.
The developments of optimization-based methods and deep learning methods have
improved registration robustness and efficiency. Recently, the combinations of
optimization-based and deep learning methods have further improved performance.
However, the connections between optimization-based and deep learning methods
are still unclear. Moreover, with the recent development of 3D sensors and 3D
reconstruction techniques, a new research direction emerges to align
cross-source point clouds. This survey conducts a comprehensive survey,
including both same-source and cross-source registration methods, and summarize
the connections between optimization-based and deep learning methods, to
provide further research insight. This survey also builds a new benchmark to
evaluate the state-of-the-art registration algorithms in solving cross-source
challenges. Besides, this survey summarizes the benchmark data sets and
discusses point cloud registration applications across various domains.
Finally, this survey proposes potential research directions in this rapidly
growing field.
</p>
<a href="http://arxiv.org/abs/2103.02690" target="_blank">arXiv:2103.02690</a> [<a href="http://arxiv.org/pdf/2103.02690" target="_blank">pdf</a>]

<h2>Multi-Stage Raw Video Denoising with Adversarial Loss and Gradient Mask. (arXiv:2103.02861v2 [cs.CV] UPDATED)</h2>
<h3>Avinash Paliwal, Libing Zeng, Nima Khademi Kalantari</h3>
<p>In this paper, we propose a learning-based approach for denoising raw videos
captured under low lighting conditions. We propose to do this by first
explicitly aligning the neighboring frames to the current frame using a
convolutional neural network (CNN). We then fuse the registered frames using
another CNN to obtain the final denoised frame. To avoid directly aligning the
temporally distant frames, we perform the two processes of alignment and fusion
in multiple stages. Specifically, at each stage, we perform the denoising
process on three consecutive input frames to generate the intermediate denoised
frames which are then passed as the input to the next stage. By performing the
process in multiple stages, we can effectively utilize the information of
neighboring frames without directly aligning the temporally distant frames. We
train our multi-stage system using an adversarial loss with a conditional
discriminator. Specifically, we condition the discriminator on a soft gradient
mask to prevent introducing high-frequency artifacts in smooth regions. We show
that our system is able to produce temporally coherent videos with realistic
details. Furthermore, we demonstrate through extensive experiments that our
approach outperforms state-of-the-art image and video denoising methods both
numerically and visually.
</p>
<a href="http://arxiv.org/abs/2103.02861" target="_blank">arXiv:2103.02861</a> [<a href="http://arxiv.org/pdf/2103.02861" target="_blank">pdf</a>]

<h2>Modeling Multi-Label Action Dependencies for Temporal Action Localization. (arXiv:2103.03027v2 [cs.CV] UPDATED)</h2>
<h3>Praveen Tirupattur, Kevin Duarte, Yogesh Rawat, Mubarak Shah</h3>
<p>Real-world videos contain many complex actions with inherent relationships
between action classes. In this work, we propose an attention-based
architecture that models these action relationships for the task of temporal
action localization in untrimmed videos. As opposed to previous works that
leverage video-level co-occurrence of actions, we distinguish the relationships
between actions that occur at the same time-step and actions that occur at
different time-steps (i.e. those which precede or follow each other). We define
these distinct relationships as action dependencies. We propose to improve
action localization performance by modeling these action dependencies in a
novel attention-based Multi-Label Action Dependency (MLAD)layer. The MLAD layer
consists of two branches: a Co-occurrence Dependency Branch and a Temporal
Dependency Branch to model co-occurrence action dependencies and temporal
action dependencies, respectively. We observe that existing metrics used for
multi-label classification do not explicitly measure how well action
dependencies are modeled, therefore, we propose novel metrics that consider
both co-occurrence and temporal dependencies between action classes. Through
empirical evaluation and extensive analysis, we show improved performance over
state-of-the-art methods on multi-label action localization
benchmarks(MultiTHUMOS and Charades) in terms of f-mAP and our proposed metric.
</p>
<a href="http://arxiv.org/abs/2103.03027" target="_blank">arXiv:2103.03027</a> [<a href="http://arxiv.org/pdf/2103.03027" target="_blank">pdf</a>]

<h2>A Structural Causal Model for MR Images of Multiple Sclerosis. (arXiv:2103.03158v2 [cs.CV] UPDATED)</h2>
<h3>Jacob C. Reinhold, Aaron Carass, Jerry L. Prince</h3>
<p>Precision medicine involves answering counterfactual questions such as "Would
this patient respond better to treatment A or treatment B?" These types of
questions are causal in nature and require the tools of causal inference to be
answered, e.g., with a structural causal model (SCM). In this work, we develop
an SCM that models the interaction between demographic information, disease
covariates, and magnetic resonance (MR) images of the brain for people with
multiple sclerosis (MS). Inference in the SCM generates counterfactual images
that show what an MR image of the brain would look like when demographic or
disease covariates are changed. These images can be used for modeling disease
progression or used for downstream image processing tasks where controlling for
confounders is necessary.
</p>
<a href="http://arxiv.org/abs/2103.03158" target="_blank">arXiv:2103.03158</a> [<a href="http://arxiv.org/pdf/2103.03158" target="_blank">pdf</a>]

