---
title: Latest Deep Learning Papers
date: 2020-09-28 21:00:01 +0800
featured-img: DL-logo
categories: [Deep_Learning]
tags: [Deep_Learning]
mathjax: Yes
---

<h1>Your arXiv Feed</h1>
<h2>DeepControl: 2D RF pulses facilitating $B_1^+$ inhomogeneity and $B_0$ off-resonance compensation in vivo at 7T. (arXiv:2009.12408v1 [physics.med-ph])</h2>
<h3>Mads Sloth Vinding, Christoph Stefan Aigner, Sebastian Schmitter, Torben Ellegaard Lund</h3>
<p>Purpose: Rapid 2D RF pulse design with subject specific $B_1^+$ inhomogeneity
and $B_0$ off-resonance compensation at 7 T predicted from convolutional neural
networks is presented.

Methods: The convolution neural network was trained on half a million
single-channel transmit, 2D RF pulses optimized with an optimal control method
using artificial 2D targets, $B_1^+$ and $B_0$ maps. Predicted pulses were
tested in a phantom and in vivo at 7 T with measured $B_1^+$ and $B_0$ maps
from a high-resolution GRE sequence.

Results: Pulse prediction by the trained convolutional neural network was
done on the fly during the MR session in approximately 9 ms for multiple hand
drawn ROIs and the measured $B_1^+$ and $B_0$ maps. Compensation of $B_1^+$
inhomogeneity and $B_0$ off-resonances has been confirmed in the phantom and in
vivo experiments. The reconstructed image data agrees well with the simulations
using the acquired $B_1^+$ and $B_0$ maps and the 2D RF pulse predicted by the
convolutional neural networks is as good as the conventional RF pulse obtained
by optimal control.

Conclusion: The proposed convolutional neural network based 2D RF pulse
design method predicts 2D RF pulses with an excellent excitation pattern and
compensated $B_1^+$ and $B_0$ variations at 7 T. The rapid 2D RF pulse
prediction (9 ms) enables subject-specific high-quality 2D RF pulses without
the need to run lengthy optimizations.
</p>
<a href="http://arxiv.org/abs/2009.12408">arXiv:2009.12408</a> [<a href="http://arxiv.org/pdf/2009.12408">pdf</a>]

<h2>Bandwidth-Agile Image Transmission with Deep Joint Source-Channel Coding. (arXiv:2009.12480v1 [cs.IT])</h2>
<h3>David Burth Kurka, Deniz G&#xfc;nd&#xfc;z</h3>
<p>We introduce deep learning based communication methods for adaptive-bandwidth
transmission of images over wireless channels. We consider the scenario in
which images are transmitted progressively in discrete layers over time or
frequency, and such layers can be aggregated by receivers in order to increase
the quality of their reconstructions. We investigate two scenarios, one in
which the layers are sent sequentially, and incrementally contribute to the
refinement of a reconstruction, and another in which the layers are independent
and can be retrieved in any order. Those scenarios correspond to the well known
problems of successive refinement and multiple descriptions, respectively, in
the context of joint source-channel coding (JSCC). We propose DeepJSCC-$l$, an
innovative solution that uses convolutional autoencoders, and present three
different architectures with different complexity trade-offs. To the best of
our knowledge, this is the first practical multiple-description JSCC scheme
developed and tested for practical information sources and channels. Numerical
results show that DeepJSCC-$l$ can learn different strategies to divide the
sources into a layered representation with negligible losses to the end-to-end
performance when compared to a single transmission. Moreover, compared to
state-of-the-art digital communication schemes, DeepJSCC-$l$ performs well in
the challenging low signal-to-noise ratio (SNR) and small bandwidth regimes,
and provides graceful degradation with channel SNR.
</p>
<a href="http://arxiv.org/abs/2009.12480">arXiv:2009.12480</a> [<a href="http://arxiv.org/pdf/2009.12480">pdf</a>]

<h2>An outer reflected forward-backward splitting algorithm for solving monotone inclusions. (arXiv:2009.12493v1 [math.OC])</h2>
<h3>Hui Yu, Chunxiang Zong, Yuchao Tang</h3>
<p>Monotone inclusions have wide applications in solving various convex
optimization problems arising in signal and image processing, machine learning,
and medical image reconstruction. In this paper, we propose a new splitting
algorithm for finding a zero of the sum of a maximally monotone operator, a
monotone Lipschitzian operator, and a cocoercive operator, which is called
outer reflected forward-backward splitting algorithm. Under mild conditions on
the iterative parameters, we prove the convergence of the proposed algorithm.
As applications, we employ the proposed algorithm to solve composite monotone
inclusions involving monotone Lipschitzian operator, cocoercive operator, and
the parallel sum of operators. The advantage of the obtained algorithm is that
it is a completely splitting algorithm, in which the Lipschitzian operator and
the cocoercive operator are processed via explicit steps and the maximally
monotone operators are processed via their resolvents.
</p>
<a href="http://arxiv.org/abs/2009.12493">arXiv:2009.12493</a> [<a href="http://arxiv.org/pdf/2009.12493">pdf</a>]

<h2>Flight-connection Prediction for Airline Crew Scheduling to Construct Initial Clusters for OR Optimizer. (arXiv:2009.12501v1 [cs.LG])</h2>
<h3>Yassine Yaakoubi, Simon Lacoste-Julien, Fran&#xe7;ois Soumis</h3>
<p>We present a case study of using machine learning classification algorithms
to initialize a large scale commercial operations research solver (GENCOL) in
the context of the airline crew pairing problem, where small savings of as
little as 1% translate to increasing annual revenue by millions of dollars in a
large airline. We focus on the problem of predicting the next connecting flight
of a crew, framed as a multiclass classification problem trained from
historical data, and design an adapted neural network approach that achieves
high accuracy (99.7% overall or 82.5% on harder instances). We demonstrate the
usefulness of our approach by using simple heuristics to combine the
flight-connection predictions to form initial crew-pairing clusters that can be
fed in the GENCOL solver, yielding a 10x speed improvement and up to 0.2% cost
saving.
</p>
<a href="http://arxiv.org/abs/2009.12501">arXiv:2009.12501</a> [<a href="http://arxiv.org/pdf/2009.12501">pdf</a>]

<h2>Unavoidable Induced Subgraphs of Large 2-Connected Graphs. (arXiv:2009.12503v1 [math.CO])</h2>
<h3>Sarah Allred, Guoli Ding, Bogdan Oporowski</h3>
<p>Ramsey proved that for every positive integer $n$, every sufficiently large
graph contains an induced $K_n$ or $\overline{K}_n$. Among the many extensions
of Ramsey's Theorem there is an analogue for connected graphs: for every
positive integer $n$, every sufficiently large connected graph contains an
induced $K_n$, $K_{1,n}$, or $P_n$. In this paper, we establish an analogue for
2-connected graphs. In particular, we prove that for every integer exceeding
two, every sufficiently large 2-connected graph contains one of the following
as an induced subgraph: $K_n$, a subdivision of $K_{2,n}$, a subdivision of
$K_{2,n}$ with an edge between the two vertices of degree $n$, and a
well-defined structure similar to a ladder.
</p>
<a href="http://arxiv.org/abs/2009.12503">arXiv:2009.12503</a> [<a href="http://arxiv.org/pdf/2009.12503">pdf</a>]

<h2>Latency Analysis for IMT-2020 Radio Interface Technology Evaluation. (arXiv:2009.12533v1 [cs.IT])</h2>
<h3>A. Phani Kumar Reddy, Navin Kumar, Sri Sai Apoorva Tirumalasetty, Srinivasan S, Vinosh Babu James J</h3>
<p>The International Telecommunication Union (ITU) is currently deliberating on
the finalization of candidate radio interface technologies (RITs) for IMT-2020
(International Mobile Telecommunications) suitability. The candidate
technologies are currently being evaluated and after a couple of
ITU-Radiocommunication sector (ITU-R) working party (WP) meetings, they will
become official. Although, products based on the candidate technology from 3GPP
(5G new radio (NR)) is already commercial in several operator networks, the ITU
is yet to officially declare it as IMT-2020 qualified. Along with evaluation of
the 3GPP 5G NR specifications, our group has evaluated many other proponent
technologies. 3GPP entire specifications were examined and evaluated through
simulation using Matlab and using own developed simulator which is based on the
Go-language. The simulator can evaluate complete 5G NR performance using the
IMT-2020 evaluation framework. In this work, we are presenting latency
parameters which has shown some minor differences from the 3GPP report.
Especially, for time division duplexing (TDD) mode of operation, the
differences are observed. It might be possible that the differences are due to
assumptions made outside the scope of the evaluation. However, we considered
the worst case parameter. Although, the report is submitted to ITU but it is
also important for the research community to understand why the differences and
what were the assumptions in scenario for which differences are observed.
</p>
<a href="http://arxiv.org/abs/2009.12533">arXiv:2009.12533</a> [<a href="http://arxiv.org/pdf/2009.12533">pdf</a>]

<h2>Estimating Linear Dynamical Networks of Cyclostationary Processes. (arXiv:2009.12667v1 [math.OC])</h2>
<h3>Harish Doddi, Deepjyoti Deka, Saurav Talukdar, Murti Salapaka</h3>
<p>Topology learning is an important problem in dynamical systems with
implications to security and optimal control. The majority of prior work in
consistent topology estimation relies on dynamical systems excited by
temporally uncorrelated processes. In this article, we present a novel
algorithm for guaranteed topology learning, in networks that are excited by
temporally colored, cyclostationary processes. Furthermore, unlike prior work,
the framework applies to linear dynamic system with complex valued
dependencies. In the second part of the article, we analyze conditions for
consistent topology learning for bidirected radial networks when a subset of
the network is unobserved. Here, few agents are unobserved and the full
topology along with unobserved nodes are recovered from observed agents data
alone. Our theoretical contributions are validated on test networks.
</p>
<a href="http://arxiv.org/abs/2009.12667">arXiv:2009.12667</a> [<a href="http://arxiv.org/pdf/2009.12667">pdf</a>]

<h2>Multi-scale Deep Neural Network (MscaleDNN) Methods for Oscillatory Stokes Flows in Complex Domains. (arXiv:2009.12729v1 [math.NA])</h2>
<h3>Bo Wang, Wenzhong Zhang, Wei Cai</h3>
<p>In this paper, we study a multi-scale deep neural network (MscaleDNN) as a
meshless numerical method for computing oscillatory Stokes flows in complex
domains. The MscaleDNN employs a multiscale structure in the design of its DNN
using radial scalings to convert the approximation of high frequency components
of the highly oscillatory Stokes solution to one of lower frequencies. The
MscaleDNN solution to the Stokes problem is obtained by minimizing a loss
function in terms of $L^2$ norm of the residual of the Stokes equation. Three
forms of loss functions are investigated based on vorticity-velocity-pressure,
velocity-stress-pressure, and velocity gradient-velocity-pressure formulations
of the Stokes equation. We first conduct a systematic study of the MscaleDNN
methods with various loss functions on the Kovasznay flow in comparison with
normal fully connected DNNs. Then, Stokes flows with highly oscillatory
solutions in a 2-D domain with six randomly placed holes are simulated by the
MscaleDNN. The results show that MscaleDNN has faster convergence and
consistent error decays than normal fully connnected DNNs in the simulation of
Kovasznay flow for all four tested loss functions. More importantly, the
MscaleDNN is capable of learning highly oscillatory solutions while the normal
DNNs fail to converge.
</p>
<a href="http://arxiv.org/abs/2009.12729">arXiv:2009.12729</a> [<a href="http://arxiv.org/pdf/2009.12729">pdf</a>]

<h2>A Statistical Learning Assessment of Huber Regression. (arXiv:2009.12755v1 [math.ST])</h2>
<h3>Yunlong Feng, Qiang Wu</h3>
<p>As one of the triumphs and milestones of robust statistics, Huber regression
plays an important role in robust inference and estimation. It has also been
finding a great variety of applications in machine learning. In a parametric
setup, it has been extensively studied. However, in the statistical learning
context where a function is typically learned in a nonparametric way, there is
still a lack of theoretical understanding of how Huber regression estimators
learn the conditional mean function and why it works in the absence of
light-tailed noise assumptions. To address these fundamental questions, we
conduct an assessment of Huber regression from a statistical learning
viewpoint. First, we show that the usual risk consistency property of Huber
regression estimators, which is usually pursued in machine learning, cannot
guarantee their learnability in mean regression. Second, we argue that Huber
regression should be implemented in an adaptive way to perform mean regression,
implying that one needs to tune the scale parameter in accordance with the
sample size and the moment condition of the noise. Third, with an adaptive
choice of the scale parameter, we demonstrate that Huber regression estimators
can be asymptotic mean regression calibrated under $(1+\epsilon)$-moment
conditions ($\epsilon&gt;0$). Last but not least, under the same moment
conditions, we establish almost sure convergence rates for Huber regression
estimators. Note that the $(1+\epsilon)$-moment conditions accommodate the
special case where the response variable possesses infinite variance and so the
established convergence rates justify the robustness feature of Huber
regression estimators. In the above senses, the present study provides a
systematic statistical learning assessment of Huber regression estimators and
justifies their merits in terms of robustness from a theoretical viewpoint.
</p>
<a href="http://arxiv.org/abs/2009.12755">arXiv:2009.12755</a> [<a href="http://arxiv.org/pdf/2009.12755">pdf</a>]

<h2>A Hybrid Framework Using a QUBO Solver For Permutation-Based Combinatorial Optimization. (arXiv:2009.12767v1 [math.OC])</h2>
<h3>Siong Thye Goh, Sabrish Gopalakrishnan, Jianyuan Bo, Hoong Chuin Lau</h3>
<p>In this paper, we propose a hybrid framework to solve large-scale
permutation-based combinatorial problems effectively using a high-performance
quadratic unconstrained binary optimization (QUBO) solver. To do so,
transformations are required to change a constrained optimization model to an
unconstrained model that involves parameter tuning. We propose techniques to
overcome the challenges in using a QUBO solver that typically comes with
limited numbers of bits. First, to smooth the energy landscape, we reduce the
magnitudes of the input without compromising optimality. We propose a machine
learning approach to tune the parameters for good performance effectively. To
handle possible infeasibility, we introduce a polynomial-time projection
algorithm. Finally, to solve large-scale problems, we introduce a
divide-and-conquer approach that calls the QUBO solver repeatedly on small
sub-problems. We tested our approach on provably hard Euclidean Traveling
Salesman (E-TSP) instances and Flow Shop Problem (FSP). Optimality gap that is
less than $10\%$ and $11\%$ are obtained respectively compared to the
best-known approach.
</p>
<a href="http://arxiv.org/abs/2009.12767">arXiv:2009.12767</a> [<a href="http://arxiv.org/pdf/2009.12767">pdf</a>]

<h2>Over-the-Air Federated Learning from Heterogeneous Data. (arXiv:2009.12787v1 [cs.LG])</h2>
<h3>Tomer Sery, Nir Shlezinger, Kobi Cohen, Yonina C. Eldar</h3>
<p>Federated learning (FL) is a framework for distributed learning of
centralized models. In FL, a set of edge devices train a model using their
local data, while repeatedly exchanging their trained updates with a central
server. This procedure allows tuning a centralized model in a distributed
fashion without having the users share their possibly private data. In this
paper, we focus on over-the-air (OTA) FL, which has been suggested recently to
reduce the communication overhead of FL due to the repeated transmissions of
the model updates by a large number of users over the wireless channel. In OTA
FL, all users simultaneously transmit their updates as analog signals over a
multiple access channel, and the server receives a superposition of the analog
transmitted signals. However, this approach results in the channel noise
directly affecting the optimization procedure, which may degrade the accuracy
of the trained model. We develop a Convergent OTA FL (COTAF) algorithm which
enhances the common local stochastic gradient descent (SGD) FL algorithm,
introducing precoding at the users and scaling at the server, which gradually
mitigates the effect of the noise. We analyze the convergence of COTAF to the
loss minimizing model and quantify the effect of a statistically heterogeneous
setup, i.e. when the training data of each user obeys a different distribution.
Our analysis reveals the ability of COTAF to achieve a convergence rate similar
to that achievable over error-free channels. Our simulations demonstrate the
improved convergence of COTAF over vanilla OTA local SGD for training using
non-synthetic datasets. Furthermore, we numerically show that the precoding
induced by COTAF notably improves the convergence rate and the accuracy of
models trained via OTA FL.
</p>
<a href="http://arxiv.org/abs/2009.12787">arXiv:2009.12787</a> [<a href="http://arxiv.org/pdf/2009.12787">pdf</a>]

<h2>Learning Optimal Representations with the Decodable Information Bottleneck. (arXiv:2009.12789v1 [cs.LG])</h2>
<h3>Yann Dubois, Douwe Kiela, David J. Schwab, Ramakrishna Vedantam</h3>
<p>We address the question of characterizing and finding optimal representations
for supervised learning. Traditionally, this question has been tackled using
the Information Bottleneck, which compresses the inputs while retaining
information about the targets, in a decoder-agnostic fashion. In machine
learning, however, our goal is not compression but rather generalization, which
is intimately linked to the predictive family or decoder of interest (e.g.
linear classifier). We propose the Decodable Information Bottleneck (DIB) that
considers information retention and compression from the perspective of the
desired predictive family. As a result, DIB gives rise to representations that
are optimal in terms of expected test performance and can be estimated with
guarantees. Empirically, we show that the framework can be used to enforce a
small generalization gap on downstream classifiers and to predict the
generalization ability of neural networks.
</p>
<a href="http://arxiv.org/abs/2009.12789">arXiv:2009.12789</a> [<a href="http://arxiv.org/pdf/2009.12789">pdf</a>]

<h2>Strong replica symmetry for high-dimensional disordered log-concave Gibbs measures. (arXiv:2009.12939v1 [math.PR])</h2>
<h3>Jean Barbier, Dmitry Panchenko, Manuel S&#xe1;enz</h3>
<p>We consider a generic class of log-concave, possibly random, (Gibbs)
measures. Using a new type of perturbation we prove concentration of an
infinite family of order parameters called multioverlaps. These completely
parametrise the quenched Gibbs measure of the system, so that their
self-averaging behavior implies a simple representation of asymptotic Gibbs
measures, as well as decoupling of the variables at hand in a strong sense. Our
concentration results may prove themselves useful in several contexts. In
particular in machine learning and high-dimensional inference, log-concave
measures appear in convex empirical risk minimisation, maximum a-posteriori
inference or M-estimation. We believe that our results may be applicable in
establishing some type of "replica symmetric formulas" for the free energy,
inference or generalisation error in such settings.
</p>
<a href="http://arxiv.org/abs/2009.12939">arXiv:2009.12939</a> [<a href="http://arxiv.org/pdf/2009.12939">pdf</a>]

<h2>Metrics and Uniqueness Criteria on the Signatures of Closed Curves. (arXiv:2009.13004v1 [math.DG])</h2>
<h3>Alex Kokot, Irina Kogan, Ian Klein</h3>
<p>This paper explores the paradigm of the differential signature introduced in
1996 by Calabi et al. This methodology has vast implications in fields such as
computer vision, where these techniques can potentially be used to verify a
person's handwriting is consistent with prior documents, or in medical imaging,
to name a few examples. Motivated by examples provided by Hickman in 2011 and
Musso and Nicolodi in 2009 regarding key failures in this invariant, we provide
new criteria for the correspondence between a curve and its signature to be
unique in a general setting. To show this result, we introduce new methods
regarding the signature, particularly through the lens of differential
equations, and the extension of the signature to include information on higher
order derivatives of the curvature function corresponding to the curve and
desired group action. We additionally show results regarding the robustness of
the signature, showing that under a suitable metric on the space of subsets of
$\mathbb{R}^n$, if two signatures are sufficiently close then so too will the
corresponding equivalence classes of curves they correspond to, given certain
conditions on these signatures.
</p>
<a href="http://arxiv.org/abs/2009.13004">arXiv:2009.13004</a> [<a href="http://arxiv.org/pdf/2009.13004">pdf</a>]

<h2>Shrinkage Estimation of the Frechet Mean in Lie groups. (arXiv:2009.13020v1 [math.ST])</h2>
<h3>Chun-Hao Yang, Baba C. Vemuri</h3>
<p>Data in non-Euclidean spaces are commonly encountered in many fields of
Science and Engineering. For instance, in Robotics, attitude sensors capture
orientation which is an element of a Lie group. In the recent past, several
researchers have reported methods that take into account the geometry of Lie
Groups in designing parameter estimation algorithms in nonlinear spaces.
Maximum likelihood estimators (MLE) are quite commonly used for such tasks and
it is well known in the field of statistics that Stein's shrinkage estimators
dominate the MLE in a mean-squared sense assuming the observations are from a
normal population. In this paper, we present a novel shrinkage estimator for
data residing in Lie groups, specifically, abelian or compact Lie groups. The
key theoretical results presented in this paper are: (i) Stein's Lemma and its
proof for Lie groups and, (ii) proof of dominance of the proposed shrinkage
estimator over MLE for abelian and compact Lie groups. We present examples of
simulation studies of the dominance of the proposed shrinkage estimator and an
application of shrinkage estimation to multiple-robot localization.
</p>
<a href="http://arxiv.org/abs/2009.13020">arXiv:2009.13020</a> [<a href="http://arxiv.org/pdf/2009.13020">pdf</a>]

<h2>Inferring Global Dynamics Using a Learning Machine. (arXiv:2009.13032v1 [nlin.AO])</h2>
<h3>Hong Zhao</h3>
<p>Given a segment of time series of a system at a particular set of parameter
values, can one infers the global behavior of the system in its parameter
space? Here we show that by using a learning machine we can achieve such a goal
to a certain extent. It is found that following an appropriate training
strategy that monotonously decreases the cost function, the learning machine in
different training stage can mimic the system at different parameter set.
Consequently, the global dynamical properties of the system is subsequently
revealed, usually in the simple-to-complex order. The underlying mechanism is
attributed to the training strategy, which causes the learning machine to
collapse to a qualitatively equivalent system of the system behind the time
series. Thus, the learning machine opens up a novel way to probe the global
dynamical properties of a black-box system without artificially establish the
equations of motion. The given illustrating examples include a representative
model of low-dimensional nonlinear dynamical systems and a spatiotemporal model
of reaction-diffusion systems.
</p>
<a href="http://arxiv.org/abs/2009.13032">arXiv:2009.13032</a> [<a href="http://arxiv.org/pdf/2009.13032">pdf</a>]

<h2>Hamilton-Jacobi-Bellman Equations for Maximum Entropy Optimal Control. (arXiv:2009.13097v1 [math.OC])</h2>
<h3>Jeongho Kim, Insoon Yang</h3>
<p>Maximum entropy reinforcement learning (RL) methods have been successfully
applied to a range of challenging sequential decision-making and control tasks.
However, most of existing techniques are designed for discrete-time systems. As
a first step toward their extension to continuous-time systems, this paper
considers continuous-time deterministic optimal control problems with entropy
regularization. Applying the dynamic programming principle, we derive a novel
class of Hamilton-Jacobi-Bellman (HJB) equations and prove that the optimal
value function of the maximum entropy control problem corresponds to the unique
viscosity solution of the HJB equation. Our maximum entropy formulation is
shown to enhance the regularity of the viscosity solution and to be
asymptotically consistent as the effect of entropy regularization diminishes. A
salient feature of the HJB equations is computational tractability. Generalized
Hopf-Lax formulas can be used to solve the HJB equations in a tractable
grid-free manner without the need for numerically optimizing the Hamiltonian.
We further show that the optimal control is uniquely characterized as Gaussian
in the case of control affine systems and that, for linear-quadratic problems,
the HJB equation is reduced to a Riccati equation, which can be used to obtain
an explicit expression of the optimal control. Lastly, we discuss how to extend
our results to continuous-time model-free RL by taking an adaptive dynamic
programming approach. To our knowledge, the resulting algorithms are the first
data-driven control methods that use an information theoretic exploration
mechanism in continuous time.
</p>
<a href="http://arxiv.org/abs/2009.13097">arXiv:2009.13097</a> [<a href="http://arxiv.org/pdf/2009.13097">pdf</a>]

<h2>On totally split primes in high-degree torsion fields of elliptic curves. (arXiv:2009.13119v1 [math.NT])</h2>
<h3>Jori Merikoski</h3>
<p>Analogously to primes in arithmetic progressions to large moduli, we can
study primes that are totally split in extensions of $\mathbb{Q}$ of high
degree. Motivated by a question of Kowalski we focus on the extensions
$\mathbb{Q}(E[d])$ obtained by adjoining the coordinates of $d$-torsion points
of a non-CM elliptic curve $E/\mathbb{Q}$. A prime $p$ is said to be an outside
prime of $E$ if it is totally split in $\mathbb{Q}(E[d])$ for some $d$ with
$p&lt;|\text{Gal}(\mathbb{Q}(E[d])/\mathbb{Q})| = d^{4-o(1)}$ (so that $p$ is not
accounted for by the expected main term in the Chebotarev Density Theorem). We
show that for almost all integers $d$ there exists a non-CM elliptic curve
$E/\mathbb{Q}$ and a prime $p&lt;|\text{Gal}(\mathbb{Q}(E[d])/\mathbb{Q})|$ which
is totally split in $\mathbb{Q}(E[d])$. Furthermore, we prove that for almost
all $d$ that factorize suitably there exists a non-CM elliptic curve
$E/\mathbb{Q}$ and a prime $p$ with $p^{0.2694} &lt; d$ which is totally split in
$\mathbb{Q}(E[d])$.

To show this we use work of Kowalski to relate the question to the
distribution of primes in certain residue classes modulo $d^2$. Hence, the
barrier $p &lt; d^4$ is related to the limit in the classical Bombieri-Vinogradov
Theorem. To break past this we make use of the assumption that $d$ factorizes
conveniently, similarly as in the works on primes in arithmetic progression to
large moduli by Bombieri, Friedlander, Fouvry, and Iwaniec, and in the more
recent works of Zhang, Polymath, and the author. In contrast to these works we
do not require any of the deep exponential sum bounds (ie. sums of Kloosterman
sums or Weil/Deligne bound). Instead, we only require the classical large sieve
for multiplicative characters. We use Harman's sieve method to obtain a
combinatorial decomposition for primes.
</p>
<a href="http://arxiv.org/abs/2009.13119">arXiv:2009.13119</a> [<a href="http://arxiv.org/pdf/2009.13119">pdf</a>]

<h2>Index and Composition Modulation. (arXiv:2009.13214v1 [eess.SP])</h2>
<h3>Ferhat Yarkin, Justin P. Coon</h3>
<p>In this paper, we propose a novel modulation concept which we call
\emph{index and composition modulation (ICM)}. In the proposed concept, we use
indices of active/deactive codeword elements and compositions of an integer to
encode information. In this regard, we first determine the activated codeword
elements, then we exploit energy levels of these elements to identify the
compositions. We depict a practical scheme for using ICM with orthogonal
frequency division multiplexing (OFDM) and show that OFDM with ICM (OFDM-ICM)
can enhance the spectral efficiency (SE) and error performance of OFDM-IM. We
design an efficient low-complexity detector for the proposed technique.
Moreover, we analyze the error and SE performance of the OFDM-ICM technique and
show that it is capable of outperforming existing OFDM benchmarks in terms of
error and SE performance.
</p>
<a href="http://arxiv.org/abs/2009.13214">arXiv:2009.13214</a> [<a href="http://arxiv.org/pdf/2009.13214">pdf</a>]

<h2>Communicate to Learn at the Edge. (arXiv:2009.13269v1 [eess.SP])</h2>
<h3>Deniz Gunduz, David Burth Kurka, Mikolaj Jankowski, Mohammad Mohammadi Amiri, Emre Ozfatura, Sreejith Sreekumar</h3>
<p>Bringing the success of modern machine learning (ML) techniques to mobile
devices can enable many new services and businesses, but also poses significant
technical and research challenges. Two factors that are critical for the
success of ML algorithms are massive amounts of data and processing power, both
of which are plentiful, yet highly distributed at the network edge. Moreover,
edge devices are connected through bandwidth- and power-limited wireless links
that suffer from noise, time-variations, and interference. Information and
coding theory have laid the foundations of reliable and efficient
communications in the presence of channel imperfections, whose application in
modern wireless networks have been a tremendous success. However, there is a
clear disconnect between the current coding and communication schemes, and the
ML algorithms deployed at the network edge. In this paper, we challenge the
current approach that treats these problems separately, and argue for a joint
communication and learning paradigm for both the training and inference stages
of edge learning.
</p>
<a href="http://arxiv.org/abs/2009.13269">arXiv:2009.13269</a> [<a href="http://arxiv.org/pdf/2009.13269">pdf</a>]

<h2>The model reduction of the Vlasov-Poisson-Fokker-Planck system to the Poisson-Nernst-Planck system via the Deep Neural Network Approach. (arXiv:2009.13280v1 [math.NA])</h2>
<h3>Jae Yong Lee, Jin Woo Jang, Hyung Ju Hwang</h3>
<p>The model reduction of a mesoscopic kinetic dynamics to a macroscopic
continuum dynamics has been one of the fundamental questions in mathematical
physics since Hilbert's time. In this paper, we consider a diagram of the
diffusion limit from the Vlasov-Poisson-Fokker-Planck (VPFP) system on a
bounded interval with the specular reflection boundary condition to the
Poisson-Nernst-Planck (PNP) system with the no-flux boundary condition. We
provide a Deep Learning algorithm to simulate the VPFP system and the PNP
system by computing the time-asymptotic behaviors of the solution and the
physical quantities. We analyze the convergence of the neural network solution
of the VPFP system to that of the PNP system via the Asymptotic-Preserving (AP)
scheme. Also, we provide several theoretical evidence that the Deep Neural
Network (DNN) solutions to the VPFP and the PNP systems converge to the a
priori classical solutions of each system if the total loss function vanishes.
</p>
<a href="http://arxiv.org/abs/2009.13280">arXiv:2009.13280</a> [<a href="http://arxiv.org/pdf/2009.13280">pdf</a>]

<h2>Bezout-like polynomial equations associated with dual univariate interpolating subdivision schemes. (arXiv:2009.13396v1 [math.NA])</h2>
<h3>Luca Gemignani, Lucia Romani, Alberto Viscardi</h3>
<p>The algebraic characterization of dual univariate interpolating subdivision
schemes is investigated. Specifically, we provide a constructive approach for
finding dual univariate interpolating subdivision schemes based on the
solutions of certain associated polynomial equations. The proposed approach
also makes possible to identify conditions for the existence of the sought
schemes.
</p>
<a href="http://arxiv.org/abs/2009.13396">arXiv:2009.13396</a> [<a href="http://arxiv.org/pdf/2009.13396">pdf</a>]

<h2>Learning Interpretable and Thermodynamically Stable Partial Differential Equations. (arXiv:2009.13415v1 [physics.comp-ph])</h2>
<h3>Juntao Huang, Zhiting Ma, Yizhou Zhou, Wen-An Yong</h3>
<p>In this work, we develop a method for learning interpretable and
thermodynamically stable partial differential equations (PDEs) based on the
Conservation-dissipation Formalism of irreversible thermodynamics. As governing
equations for non-equilibrium flows in one dimension, the learned PDEs are
parameterized by fully-connected neural networks and satisfy the
conservation-dissipation principle automatically. In particular, they are
hyperbolic balance laws. The training data are generated from a kinetic model
with smooth initial data. Numerical results indicate that the learned PDEs can
achieve good accuracy in a wide range of Knudsen numbers. Remarkably, the
learned dynamics can give satisfactory results with randomly sampled
discontinuous initial data although it is trained only with smooth initial
data.
</p>
<a href="http://arxiv.org/abs/2009.13415">arXiv:2009.13415</a> [<a href="http://arxiv.org/pdf/2009.13415">pdf</a>]

<h2>Why resampling outperforms reweighting for correcting sampling bias. (arXiv:2009.13447v1 [cs.LG])</h2>
<h3>Jing An, Lexing Ying, Yuhua Zhu</h3>
<p>A data set sampled from a certain population is biased if the subgroups of
the population are sampled at proportions that are significantly different from
their underlying proportions. Training machine learning models on biased data
sets requires correction techniques to compensate for potential biases. We
consider two commonly-used techniques, resampling and reweighting, that
rebalance the proportions of the subgroups to maintain the desired objective
function. Though statistically equivalent, it has been observed that
reweighting outperforms resampling when combined with stochastic gradient
algorithms. By analyzing illustrative examples, we explain the reason behind
this phenomenon using tools from dynamical stability and stochastic
asymptotics. We also present experiments from regression, classification, and
off-policy prediction to demonstrate that this is a general phenomenon. We
argue that it is imperative to consider the objective function design and the
optimization algorithm together while addressing the sampling bias.
</p>
<a href="http://arxiv.org/abs/2009.13447">arXiv:2009.13447</a> [<a href="http://arxiv.org/pdf/2009.13447">pdf</a>]

<h2>A structure theorem for rooted binary phylogenetic networks and its implications for tree-based networks. (arXiv:1811.05849v4 [math.CO] UPDATED)</h2>
<h3>Momoko Hayamizu</h3>
<p>Attempting to recognize a tree inside a phylogenetic network is a fundamental
undertaking in evolutionary analysis. In the last few years, therefore,
tree-based phylogenetic networks, which are defined by a spanning tree called a
subdivision tree, have attracted attention of theoretical biologists. However,
the application of such networks is still not easy, due to many problems whose
time complexities are not clearly understood. In this paper, we provide a
general framework for solving those various old or new problems from a coherent
perspective, rather than analyzing the complexity of each individual problem or
developing an algorithm one by one. More precisely, we establish a structure
theorem that gives a way to canonically decompose any rooted binary
phylogenetic network N into maximal zig-zag trails that are uniquely
determined, and use it to characterize the set of subdivision trees of N in the
form of a direct product, in a way reminiscent of the structure theorem for
finitely generated Abelian groups. From the main results, we derive a series of
linear time and linear time delay algorithms for the following problems: given
a rooted binary phylogenetic network N, 1) determine whether or not N has a
subdivision tree and find one if there exists any; 2) measure the deviation of
N from being tree-based; 3) compute the number of subdivision trees of N; 4)
list all subdivision trees of N; and 5) find a subdivision tree to maximize or
minimize a prescribed objective function. All algorithms proposed here are
optimal in terms of time complexity. Our results do not only imply and unify
various known results, but also answer many open questions and moreover enable
novel applications, such as the estimation of a maximum likelihood tree
underlying a tree-based network. The results and algorithms in this paper still
hold true for a special class of rooted non-binary phylogenetic networks.
</p>
<a href="http://arxiv.org/abs/1811.05849">arXiv:1811.05849</a> [<a href="http://arxiv.org/pdf/1811.05849">pdf</a>]

<h2>A Deterministic Gradient-Based Approach to Avoid Saddle Points. (arXiv:1901.06827v2 [cs.LG] UPDATED)</h2>
<h3>Lisa Maria Kreusser, Stanley J. Osher, Bao Wang</h3>
<p>Loss functions with a large number of saddle points are one of the major
obstacles for training modern machine learning models efficiently. First-order
methods such as gradient descent are usually the methods of choice for training
machine learning models. However, these methods converge to saddle points for
certain choices of initial guesses. In this paper, we propose a modification of
the recently proposed Laplacian smoothing gradient descent [Osher et al.,
arXiv:1806.06317], called modified Laplacian smoothing gradient descent
(mLSGD), and demonstrate its potential to avoid saddle points without
sacrificing the convergence rate. Our analysis is based on the attraction
region, formed by all starting points for which the considered numerical scheme
converges to a saddle point. We investigate the attraction region's dimension
both analytically and numerically. For a canonical class of quadratic
functions, we show that the dimension of the attraction region for mLSGD is
floor((n-1)/2), and hence it is significantly smaller than that of the gradient
descent whose dimension is n-1.
</p>
<a href="http://arxiv.org/abs/1901.06827">arXiv:1901.06827</a> [<a href="http://arxiv.org/pdf/1901.06827">pdf</a>]

<h2>Machine Learning from a Continuous Viewpoint. (arXiv:1912.12777v2 [math.NA] UPDATED)</h2>
<h3>Weinan E, Chao Ma, Lei Wu</h3>
<p>We present a continuous formulation of machine learning, as a problem in the
calculus of variations and differential-integral equations, in the spirit of
classical numerical analysis. We demonstrate that conventional machine learning
models and algorithms, such as the random feature model, the two-layer neural
network model and the residual neural network model, can all be recovered (in a
scaled form) as particular discretizations of different continuous
formulations. We also present examples of new models, such as the flow-based
random feature model, and new algorithms, such as the smoothed particle method
and spectral method, that arise naturally from this continuous formulation. We
discuss how the issues of generalization error and implicit regularization can
be studied under this framework.
</p>
<a href="http://arxiv.org/abs/1912.12777">arXiv:1912.12777</a> [<a href="http://arxiv.org/pdf/1912.12777">pdf</a>]

<h2>Positive entropy using Hecke operators at a single place. (arXiv:2002.08057v3 [math.RT] UPDATED)</h2>
<h3>Zvi Shem-Tov</h3>
<p>We prove the following statement: Let $X=\text{SL}_n(\mathbb{Z})\backslash
\text{SL}_n(\mathbb{R})$, and consider the standard action of the diagonal
group $A&lt;\text{SL}_n(\mathbb{R})$ on it. Let $\mu$ be an $A$-invariant
probability measure on $X$, which is a limit $$ \mu=\lambda\lim_i|\phi_i|^2dx,
$$ where $\phi_i$ are normalized eigenfunctions of the Hecke algebra at some
fixed place $p$, and $\lambda&gt;0$ is some positive constant. Then any regular
element $a\in A$ acts on $\mu$ with positive entropy on almost every ergodic
component. We also prove a similar result for lattices coming from division
algebras over $\mathbb{Q}$, and derive a quantum unique ergodicity result for
the associated locally symmetric spaces. This generalizes a result of Brooks
and Lindenstrauss.
</p>
<a href="http://arxiv.org/abs/2002.08057">arXiv:2002.08057</a> [<a href="http://arxiv.org/pdf/2002.08057">pdf</a>]

<h2>Mehler's Formula, Branching Process, and Compositional Kernels of Deep Neural Networks. (arXiv:2004.04767v2 [stat.ML] UPDATED)</h2>
<h3>Tengyuan Liang, Hai Tran-Bach</h3>
<p>We utilize a connection between compositional kernels and branching processes
via Mehler's formula to study deep neural networks. This new probabilistic
insight provides us a novel perspective on the mathematical role of activation
functions in compositional neural networks. We study the unscaled and rescaled
limits of the compositional kernels and explore the different phases of the
limiting behavior, as the compositional depth increases. We investigate the
memorization capacity of the compositional kernels and neural networks by
characterizing the interplay among compositional depth, sample size,
dimensionality, and non-linearity of the activation. Explicit formulas on the
eigenvalues of the compositional kernel are provided, which quantify the
complexity of the corresponding reproducing kernel Hilbert space. On the
methodological front, we propose a new random features algorithm, which
compresses the compositional layers by devising a new activation function.
</p>
<a href="http://arxiv.org/abs/2004.04767">arXiv:2004.04767</a> [<a href="http://arxiv.org/pdf/2004.04767">pdf</a>]

<h2>Sample Complexity of Asynchronous Q-Learning: Sharper Analysis and Variance Reduction. (arXiv:2006.03041v2 [cs.LG] UPDATED)</h2>
<h3>Gen Li, Yuting Wei, Yuejie Chi, Yuantao Gu, Yuxin Chen</h3>
<p>Asynchronous Q-learning aims to learn the optimal action-value function (or
Q-function) of a Markov decision process (MDP), based on a single trajectory of
Markovian samples induced by a behavior policy. Focusing on a
$\gamma$-discounted MDP with state space $\mathcal{S}$ and action space
$\mathcal{A}$, we demonstrate that the $\ell_{\infty}$-based sample complexity
of classical asynchronous Q-learning -- namely, the number of samples needed to
yield an entrywise $\varepsilon$-accurate estimate of the Q-function -- is at
most on the order of \begin{equation*}
\frac{1}{\mu_{\mathsf{min}}(1-\gamma)^5\varepsilon^2}+
\frac{t_{\mathsf{mix}}}{\mu_{\mathsf{min}}(1-\gamma)} \end{equation*} up to
some logarithmic factor, provided that a proper constant learning rate is
adopted. Here, $t_{\mathsf{mix}}$ and $\mu_{\mathsf{min}}$ denote respectively
the mixing time and the minimum state-action occupancy probability of the
sample trajectory. The first term of this bound matches the complexity in the
case with independent samples drawn from the stationary distribution of the
trajectory. The second term reflects the expense taken for the empirical
distribution of the Markovian trajectory to reach a steady state, which is
incurred at the very beginning and becomes amortized as the algorithm runs.
Encouragingly, the above bound improves upon the state-of-the-art result by a
factor of at least $|\mathcal{S}||\mathcal{A}|$. Further, the scaling on the
discount complexity can be improved by means of variance reduction.
</p>
<a href="http://arxiv.org/abs/2006.03041">arXiv:2006.03041</a> [<a href="http://arxiv.org/pdf/2006.03041">pdf</a>]

<h2>Privacy For Free: Wireless Federated Learning Via Uncoded Transmission With Adaptive Power Control. (arXiv:2006.05459v3 [cs.IT] UPDATED)</h2>
<h3>Dongzhu Liu, Osvaldo Simeone</h3>
<p>Federated Learning (FL) refers to distributed protocols that avoid direct raw
data exchange among the participating devices while training for a common
learning task. This way, FL can potentially reduce the information on the local
data sets that is leaked via communications. In order to provide formal privacy
guarantees, however, it is generally necessary to put in place additional
masking mechanisms. When FL is implemented in wireless systems via uncoded
transmission, the channel noise can directly act as a privacy-inducing
mechanism. This paper demonstrates that, as long as the privacy constraint
level, measured via differential privacy (DP), is below a threshold that
decreases with the signal-to-noise ratio (SNR), uncoded transmission achieves
privacy "for free", i.e., without affecting the learning performance. More
generally, this work studies adaptive power allocation (PA) for decentralized
gradient descent in wireless FL with the aim of minimizing the learning
optimality gap under privacy and power constraints. Both orthogonal multiple
access (OMA) and non-orthogonal multiple access (NOMA) transmission with
"over-the-air-computing" are studied, and solutions are obtained in closed form
for an offline optimization setting. Furthermore, heuristic online methods are
proposed that leverage iterative one-step-ahead optimization. The importance of
dynamic PA and the potential benefits of NOMA versus OMA are demonstrated
through extensive simulations.
</p>
<a href="http://arxiv.org/abs/2006.05459">arXiv:2006.05459</a> [<a href="http://arxiv.org/pdf/2006.05459">pdf</a>]

<h2>Multi-scale Deep Neural Network (MscaleDNN) for Solving Poisson-Boltzmann Equation in Complex Domains. (arXiv:2007.11207v3 [physics.comp-ph] UPDATED)</h2>
<h3>Ziqi Liu, Wei Cai, Zhi-Qin John Xu</h3>
<p>In this paper, we propose multi-scale deep neural networks (MscaleDNNs) using
the idea of radial scaling in frequency domain and activation functions with
compact support. The radial scaling converts the problem of approximation of
high frequency contents of PDEs' solutions to a problem of learning about lower
frequency functions, and the compact support activation functions facilitate
the separation of frequency contents of the target function to be approximated
by corresponding DNNs. As a result, the MscaleDNNs achieve fast uniform
convergence over multiple scales. The proposed MscaleDNNs are shown to be
superior to traditional fully connected DNNs and be an effective mesh-less
numerical method for Poisson-Boltzmann equations with ample frequency contents
over complex and singular domains.
</p>
<a href="http://arxiv.org/abs/2007.11207">arXiv:2007.11207</a> [<a href="http://arxiv.org/pdf/2007.11207">pdf</a>]

<h2>Definable Eilenberg--Mac Lane Universal Coefficient Theorems. (arXiv:2009.10805v2 [math.AT] UPDATED)</h2>
<h3>Martino Lupini</h3>
<p>We prove definable versions of the Universal Coefficient Theorems of
Eilenberg--Mac Lane expressing the (Steenrod) homology groups of a compact
metrizable space in terms of its integral cohomology groups, and the (\v{C}ech)
cohomology groups of a polyhedron in terms of its integral homology groups.\
Precisely, we show that, given a compact metrizable space $X$, a (not
necessarily compact) polyhedron $Y$, and an abelian Polish group $G$ with the
division closure property, there are natural definable exact sequences
\begin{equation*} 0\rightarrow \mathrm{Ext}\left( H^{\bullet +1}(X),G\right)
\rightarrow H_{\bullet }(X;G)\rightarrow \mathrm{Hom}\left( H^{\bullet
}(X),G\right) \rightarrow 0 \end{equation*} and \begin{equation*} 0\rightarrow
\mathrm{Ext}\left( H_{\bullet -1}(Y),G\right) \rightarrow H^{\bullet
}(Y;G)\rightarrow \mathrm{Hom}\left( H_{\bullet }(Y),G\right) \rightarrow 0
\end{equation*} which definably split, where $H_{\bullet }(X;G)$ is the
definable homology group of $X$ with coefficients in $G$ and $H^{\bullet
}(Y;G)$ is the definable cohomology group of $Y$ with coefficients in $G$. In
particular, the integral cohomology of $X$ completely determines the definable
homology of $X$ with coefficients, and the integral homology of $Y$ completely
determines the definable cohomology of $Y$ with coefficients.

Both of these results are obtained as corollaries of a general algebraic
Universal Coefficient Theorem relating the cohomology of a cochain complex of
countable free abelian groups to the definable homology of its $G$-dual chain
complex of Polish groups.
</p>
<a href="http://arxiv.org/abs/2009.10805">arXiv:2009.10805</a> [<a href="http://arxiv.org/pdf/2009.10805">pdf</a>]

<h2>Event-Driven Receding Horizon Control for Distributed Estimation in Network Systems. (arXiv:2009.11958v2 [eess.SY] UPDATED)</h2>
<h3>Shirantha Welikala, Christos G. Cassandras</h3>
<p>This paper considers the multi-agent persistent monitoring problem defined on
a network (graph) of nodes (targets) with uncertain states. The agent team's
goal is to persistently observe the target states so that an overall measure of
estimation error covariance evaluated over a finite period is minimized. Each
agent's trajectory is fully defined by the sequence of targets it visits and
the corresponding dwell times spent at each visited target. To find the optimal
set of agent trajectories, we propose a distributed and on-line estimation
process that requires each agent to solve a sequence of receding horizon
control problems (RHCPs) in an event-driven manner. We use a novel objective
function form for these RHCPs to optimize the effectiveness of this distributed
estimation process and establish its unimodality under certain conditions.
Moreover, we show that agents can use machine learning to efficiently solve a
significant portion of each RHCP they face without compromising accuracy.
Finally, extensive numerical results are provided, indicating significant
improvements compared to other agent control methods.
</p>
<a href="http://arxiv.org/abs/2009.11958">arXiv:2009.11958</a> [<a href="http://arxiv.org/pdf/2009.11958">pdf</a>]

<h2>Visually Grounded Compound PCFGs. (arXiv:2009.12404v1 [cs.CL])</h2>
<h3>Yanpeng Zhao, Ivan Titov</h3>
<p>Exploiting visual groundings for language understanding has recently been
drawing much attention. In this work, we study visually grounded grammar
induction and learn a constituency parser from both unlabeled text and its
visual groundings. Existing work on this task (Shi et al., 2019) optimizes a
parser via Reinforce and derives the learning signal only from the alignment of
images and sentences. While their model is relatively accurate overall, its
error distribution is very uneven, with low performance on certain constituents
types (e.g., 26.2% recall on verb phrases, VPs) and high on others (e.g., 79.6%
recall on noun phrases, NPs). This is not surprising as the learning signal is
likely insufficient for deriving all aspects of phrase-structure syntax and
gradient estimates are noisy. We show that using an extension of probabilistic
context-free grammar model we can do fully-differentiable end-to-end visually
grounded learning. Additionally, this enables us to complement the image-text
alignment loss with a language modeling objective. On the MSCOCO test captions,
our model establishes a new state of the art, outperforming its non-grounded
version and, thus, confirming the effectiveness of visual groundings in
constituency grammar induction. It also substantially outperforms the previous
grounded model, with largest improvements on more `abstract' categories (e.g.,
+55.1% recall on VPs).
</p>
<a href="http://arxiv.org/abs/2009.12404">arXiv:2009.12404</a> [<a href="http://arxiv.org/pdf/2009.12404">pdf</a>]

<h2>Fair and Efficient Online Allocations with Normalized Valuations. (arXiv:2009.12405v1 [cs.GT])</h2>
<h3>Vasilis Gkatzelis, Alexandros Psomas, Xizhi Tan</h3>
<p>A set of divisible resources becomes available over a sequence of rounds and
needs to be allocated immediately and irrevocably. Our goal is to distribute
these resources to maximize fairness and efficiency. Achieving any non-trivial
guarantees in an adversarial setting is impossible. However, we show that
normalizing the agent values, a very common assumption in fair division, allows
us to escape this impossibility. Our main result is an online algorithm for the
case of two agents that ensures the outcome is envy-free while guaranteeing
91.6% of the optimal social welfare. We also show that this is near-optimal:
there is no envy-free algorithm that guarantees more than 93.3% of the optimal
social welfare.
</p>
<a href="http://arxiv.org/abs/2009.12405">arXiv:2009.12405</a> [<a href="http://arxiv.org/pdf/2009.12405">pdf</a>]

<h2>Why have a Unified Predictive Uncertainty? Disentangling it using Deep Split Ensembles. (arXiv:2009.12406v1 [cs.LG])</h2>
<h3>Utkarsh Sarawgi, Wazeer Zulfikar, Rishab Khincha, Pattie Maes</h3>
<p>Understanding and quantifying uncertainty in black box Neural Networks (NNs)
is critical when deployed in real-world settings such as healthcare. Recent
works using Bayesian and non-Bayesian methods have shown how a unified
predictive uncertainty can be modelled for NNs. Decomposing this uncertainty to
disentangle the granular sources of heteroscedasticity in data provides rich
information about its underlying causes. We propose a conceptually simple
non-Bayesian approach, deep split ensemble, to disentangle the predictive
uncertainties using a multivariate Gaussian mixture model. The NNs are trained
with clusters of input features, for uncertainty estimates per cluster. We
evaluate our approach on a series of benchmark regression datasets, while also
comparing with unified uncertainty methods. Extensive analyses using dataset
shits and empirical rule highlight our inherently well-calibrated models. Our
work further demonstrates its applicability in a multi-modal setting using a
benchmark Alzheimer's dataset and also shows how deep split ensembles can
highlight hidden modality-specific biases. The minimal changes required to NNs
and the training procedure, and the high flexibility to group features into
clusters makes it readily deployable and useful. The source code is available
at https://github.com/wazeerzulfikar/deep-split-ensembles
</p>
<a href="http://arxiv.org/abs/2009.12406">arXiv:2009.12406</a> [<a href="http://arxiv.org/pdf/2009.12406">pdf</a>]

<h2>Hierarchical Sparse Variational Autoencoder for Text Encoding. (arXiv:2009.12421v1 [cs.CL])</h2>
<h3>Victor Prokhorov, Yingzhen Li, Ehsan Shareghi, Nigel Collier</h3>
<p>In this paper we focus on unsupervised representation learning and propose a
novel framework, Hierarchical Sparse Variational Autoencoder (HSVAE), that
imposes sparsity on sentence representations via direct optimisation of
Evidence Lower Bound (ELBO). Our experimental results illustrate that HSVAE is
flexible and adapts nicely to the underlying characteristics of the corpus
which is reflected by the level of sparsity and its distributional patterns.
</p>
<a href="http://arxiv.org/abs/2009.12421">arXiv:2009.12421</a> [<a href="http://arxiv.org/pdf/2009.12421">pdf</a>]

<h2>Pareto-Optimal Bit Allocation for Collaborative Intelligence. (arXiv:2009.12430v1 [eess.IV])</h2>
<h3>Saeed Ranjbar Alvar, Ivan V. Baji&#x107;</h3>
<p>In recent studies, collaborative intelligence (CI) has emerged as a promising
framework for deployment of Artificial Intelligence (AI)-based services on
mobile/edge devices. In CI, the AI model (a deep neural network) is split
between the edge and the cloud, and intermediate features are sent from the
edge sub-model to the cloud sub-model. In this paper, we study bit allocation
for feature coding in multi-stream CI systems. We model task distortion as a
function of rate using convex surfaces similar to those found in
distortion-rate theory. Using such models, we are able to provide closed-form
bit allocation solutions for single-task systems and scalarized multi-task
systems. Moreover, we provide analytical characterization of the full Pareto
set for 2-stream k-task systems, and bounds on the Pareto set for 3-stream
2-task systems. Analytical results are examined on a variety of DNN models from
the literature to demonstrate wide applicability of the results
</p>
<a href="http://arxiv.org/abs/2009.12430">arXiv:2009.12430</a> [<a href="http://arxiv.org/pdf/2009.12430">pdf</a>]

<h2>Deep Artifact-Free Residual Network for Single Image Super-Resolution. (arXiv:2009.12433v1 [eess.IV])</h2>
<h3>Hamdollah Nasrollahi, Kamran Farajzadeh, Vahid Hosseini, Esmaeil Zarezadeh, Milad Abdollahzadeh</h3>
<p>Recently, convolutional neural networks have shown promising performance for
single-image super-resolution. In this paper, we propose Deep Artifact-Free
Residual (DAFR) network which uses the merits of both residual learning and
usage of ground-truth image as target. Our framework uses a deep model to
extract the high-frequency information which is necessary for high-quality
image reconstruction. We use a skip-connection to feed the low-resolution image
to the network before the image reconstruction. In this way, we are able to use
the ground-truth images as target and avoid misleading the network due to
artifacts in difference image. In order to extract clean high-frequency
information, we train the network in two steps. The first step is a traditional
residual learning which uses the difference image as target. Then, the trained
parameters of this step are transferred to the main training in the second
step. Our experimental results show that the proposed method achieves better
quantitative and qualitative image quality compared to the existing methods.
</p>
<a href="http://arxiv.org/abs/2009.12433">arXiv:2009.12433</a> [<a href="http://arxiv.org/pdf/2009.12433">pdf</a>]

<h2>Online Learnable Keyframe Extraction in Videos and its Application with Semantic Word Vector in Action Recognition. (arXiv:2009.12434v1 [cs.CV])</h2>
<h3>G M Mashrur E Elahi, Yee-Hong Yang</h3>
<p>Video processing has become a popular research direction in computer vision
due to its various applications such as video summarization, action
recognition, etc. Recently, deep learning-based methods have achieved
impressive results in action recognition. However, these methods need to
process a full video sequence to recognize the action, even though most of
these frames are similar and non-essential to recognizing a particular action.
Additionally, these non-essential frames increase the computational cost and
can confuse a method in action recognition. Instead, the important frames
called keyframes not only are helpful in the recognition of an action but also
can reduce the processing time of each video sequence for classification or in
other applications, e.g. summarization. As well, current methods in video
processing have not yet been demonstrated in an online fashion.

Motivated by the above, we propose an online learnable module for keyframe
extraction. This module can be used to select key-shots in video and thus can
be applied to video summarization. The extracted keyframes can be used as input
to any deep learning-based classification model to recognize action. We also
propose a plugin module to use the semantic word vector as input along with
keyframes and a novel train/test strategy for the classification models. To our
best knowledge, this is the first time such an online module and train/test
strategy have been proposed.

The experimental results on many commonly used datasets in video
summarization and in action recognition have shown impressive results using the
proposed module.
</p>
<a href="http://arxiv.org/abs/2009.12434">arXiv:2009.12434</a> [<a href="http://arxiv.org/pdf/2009.12434">pdf</a>]

<h2>Democratizing Artificial Intelligence in Healthcare: A Study of Model Development Across Two Institutions Incorporating Transfer Learning. (arXiv:2009.12437v1 [eess.IV])</h2>
<h3>Vikash Gupta1, Holger Roth, Varun Buch3, Marcio A.B.C. Rockenbach, Richard D White, Dong Yang, Olga Laur, Brian Ghoshhajra, Ittai Dayan, Daguang Xu, Mona G. Flores, Barbaros Selnur Erdal</h3>
<p>The training of deep learning models typically requires extensive data, which
are not readily available as large well-curated medical-image datasets for
development of artificial intelligence (AI) models applied in Radiology.
Recognizing the potential for transfer learning (TL) to allow a fully trained
model from one institution to be fine-tuned by another institution using a much
small local dataset, this report describes the challenges, methodology, and
benefits of TL within the context of developing an AI model for a basic
use-case, segmentation of Left Ventricular Myocardium (LVM) on images from
4-dimensional coronary computed tomography angiography. Ultimately, our results
from comparisons of LVM segmentation predicted by a model locally trained using
random initialization, versus one training-enhanced by TL, showed that a
use-case model initiated by TL can be developed with sparse labels with
acceptable performance. This process reduces the time required to build a new
model in the clinical environment at a different institution.
</p>
<a href="http://arxiv.org/abs/2009.12437">arXiv:2009.12437</a> [<a href="http://arxiv.org/pdf/2009.12437">pdf</a>]

<h2>BET: A Backtranslation Approach for Easy Data Augmentation in Transformer-based Paraphrase Identification Context. (arXiv:2009.12452v1 [cs.CL])</h2>
<h3>Jean-Philippe Corbeil, Hadi Abdi Ghadivel</h3>
<p>Newly-introduced deep learning architectures, namely BERT, XLNet, RoBERTa and
ALBERT, have been proved to be robust on several NLP tasks. However, the
datasets trained on these architectures are fixed in terms of size and
generalizability. To relieve this issue, we apply one of the most inexpensive
solutions to update these datasets. We call this approach BET by which we
analyze the backtranslation data augmentation on the transformer-based
architectures. Using the Google Translate API with ten intermediary languages
from ten different language families, we externally evaluate the results in the
context of automatic paraphrase identification in a transformer-based
framework. Our findings suggest that BET improves the paraphrase identification
performance on the Microsoft Research Paraphrase Corpus (MRPC) to more than 3%
on both accuracy and F1 score. We also analyze the augmentation in the low-data
regime with downsampled versions of MRPC, Twitter Paraphrase Corpus (TPC) and
Quora Question Pairs. In many low-data cases, we observe a switch from a
failing model on the test set to reasonable performances. The results
demonstrate that BET is a highly promising data augmentation technique: to push
the current state-of-the-art of existing datasets and to bootstrap the
utilization of deep learning architectures in the low-data regime of a hundred
samples.
</p>
<a href="http://arxiv.org/abs/2009.12452">arXiv:2009.12452</a> [<a href="http://arxiv.org/pdf/2009.12452">pdf</a>]

<h2>Blind Image Super-Resolution with Spatial Context Hallucination. (arXiv:2009.12461v1 [eess.IV])</h2>
<h3>Dong Huo, Yee-Hong Yang</h3>
<p>Deep convolution neural networks (CNNs) play a critical role in single image
super-resolution (SISR) since the amazing improvement of high performance
computing. However, most of the super-resolution (SR) methods only focus on
recovering bicubic degradation. Reconstructing high-resolution (HR) images from
randomly blurred and noisy low-resolution (LR) images is still a challenging
problem. In this paper, we propose a novel Spatial Context Hallucination
Network (SCHN) for blind super-resolution without knowing the degradation
kernel. We find that when the blur kernel is unknown, separate deblurring and
super-resolution could limit the performance because of the accumulation of
error. Thus, we integrate denoising, deblurring and super-resolution within one
framework to avoid such a problem. We train our model on two high quality
datasets, DIV2K and Flickr2K. Our method performs better than state-of-the-art
methods when input images are corrupted with random blur and noise.
</p>
<a href="http://arxiv.org/abs/2009.12461">arXiv:2009.12461</a> [<a href="http://arxiv.org/pdf/2009.12461">pdf</a>]

<h2>Symbolic Relational Deep Reinforcement Learning based on Graph Neural Networks. (arXiv:2009.12462v1 [cs.LG])</h2>
<h3>Jarom&#xed;r Janisch, Tom&#xe1;&#x161; Pevn&#xfd;, Viliam Lis&#xfd;</h3>
<p>We present a novel deep reinforcement learning framework for solving
relational problems. The method operates with a symbolic representation of
objects, their relations and multi-parameter actions, where the objects are the
parameters. Our framework, based on graph neural networks, is completely
domain-independent and can be applied to any relational problem with existing
symbolic-relational representation. We show how to represent relational states
with arbitrary goals, multi-parameter actions and concurrent actions. We
evaluate the method on a set of three domains: BlockWorld, Sokoban and
SysAdmin. The method displays impressive generalization over different problem
sizes (e.g., in BlockWorld, the method trained exclusively with 5 blocks still
solves 78% of problems with 20 blocks) and readiness for curriculum learning.
</p>
<a href="http://arxiv.org/abs/2009.12462">arXiv:2009.12462</a> [<a href="http://arxiv.org/pdf/2009.12462">pdf</a>]

<h2>Lateral Force Prediction using Gaussian Process Regression for Intelligent Tire Systems. (arXiv:2009.12463v1 [eess.SP])</h2>
<h3>Bruno Henrique Groenner Barbosa, Nan Xu, Hassan Askari, Amir Khajepour</h3>
<p>Understanding the dynamic behavior of tires and their interactions with road
plays an important role in designing integrated vehicle control strategies.
Accordingly, having access to reliable information about the tire-road
interactions through tire embedded sensors is very demanding for developing
enhanced vehicle control systems. Thus, the main objectives of the present
research work are i. to analyze data from an experimental accelerometer-based
intelligent tire acquired over a wide range of maneuvers, with different
vertical loads, velocities, and high slip angles; and ii. to develop a lateral
force predictor based on a machine learning tool, more specifically the
Gaussian Process Regression (GPR) technique. It is delineated that the proposed
intelligent tire system can provide reliable information about the tire-road
interactions even in the case of high slip angles. Besides, the lateral forces
model based on GPR can predict forces with acceptable accuracy and provide
level of uncertainties that can be very useful for designing vehicle control
strategies.
</p>
<a href="http://arxiv.org/abs/2009.12463">arXiv:2009.12463</a> [<a href="http://arxiv.org/pdf/2009.12463">pdf</a>]

<h2>SIA-GCN: A Spatial Information Aware Graph Neural Network with 2D Convolutions for Hand Pose Estimation. (arXiv:2009.12473v1 [cs.CV])</h2>
<h3>Deying Kong, Haoyu Ma, Xiaohui Xie</h3>
<p>Graph Neural Networks (GNNs) generalize neural networks from applications on
regular structures to applications on arbitrary graphs, and have shown success
in many application domains such as computer vision, social networks and
chemistry. In this paper, we extend GNNs along two directions: a) allowing
features at each node to be represented by 2D spatial confidence maps instead
of 1D vectors; and b) proposing an efficient operation to integrate information
from neighboring nodes through 2D convolutions with different learnable kernels
at each edge. The proposed SIA-GCN can efficiently extract spatial information
from 2D maps at each node and propagate them through graph convolution. By
associating each edge with a designated convolution kernel, the SIA-GCN could
capture different spatial relationships for different pairs of neighboring
nodes. We demonstrate the utility of SIA-GCN on the task of estimating hand
keypoints from single-frame images, where the nodes represent the 2D coordinate
heatmaps of keypoints and the edges denote the kinetic relationships between
keypoints. Experiments on multiple datasets show that SIA-GCN provides a
flexible and yet powerful framework to account for structural constraints
between keypoints, and can achieve state-of-the-art performance on the task of
hand pose estimation.
</p>
<a href="http://arxiv.org/abs/2009.12473">arXiv:2009.12473</a> [<a href="http://arxiv.org/pdf/2009.12473">pdf</a>]

<h2>Generating Realistic COVID19 X-rays with a Mean Teacher + Transfer Learning GAN. (arXiv:2009.12478v1 [cs.LG])</h2>
<h3>Sumeet Menon (1), Joshua Galita (1), David Chapman (1), Aryya Gangopadhyay (1), Jayalakshmi Mangalagiri (1), Phuong Nguyen (1), Yaacov Yesha (1), Yelena Yesha (1), Babak Saboury (1 and 2), Michael Morris (1, 2, and 3) ((1) University of Maryland, Baltimore County, (2) National Institutes of Health Clinical Center, (3) Networking Health)</h3>
<p>COVID-19 is a novel infectious disease responsible for over 800K deaths
worldwide as of August 2020. The need for rapid testing is a high priority and
alternative testing strategies including X-ray image classification are a
promising area of research. However, at present, public datasets for COVID19
x-ray images have low data volumes, making it challenging to develop accurate
image classifiers. Several recent papers have made use of Generative
Adversarial Networks (GANs) in order to increase the training data volumes. But
realistic synthetic COVID19 X-rays remain challenging to generate. We present a
novel Mean Teacher + Transfer GAN (MTT-GAN) that generates COVID19 chest X-ray
images of high quality. In order to create a more accurate GAN, we employ
transfer learning from the Kaggle Pneumonia X-Ray dataset, a highly relevant
data source orders of magnitude larger than public COVID19 datasets.
Furthermore, we employ the Mean Teacher algorithm as a constraint to improve
stability of training. Our qualitative analysis shows that the MTT-GAN
generates X-ray images that are greatly superior to a baseline GAN and visually
comparable to real X-rays. Although board-certified radiologists can
distinguish MTT-GAN fakes from real COVID19 X-rays. Quantitative analysis shows
that MTT-GAN greatly improves the accuracy of both a binary COVID19 classifier
as well as a multi-class Pneumonia classifier as compared to a baseline GAN.
Our classification accuracy is favourable as compared to recently reported
results in the literature for similar binary and multi-class COVID19 screening
tasks.
</p>
<a href="http://arxiv.org/abs/2009.12478">arXiv:2009.12478</a> [<a href="http://arxiv.org/pdf/2009.12478">pdf</a>]

<h2>SEMI: Self-supervised Exploration via Multisensory Incongruity. (arXiv:2009.12494v1 [cs.LG])</h2>
<h3>Jianren Wang, Ziwen Zhuang, Hang Zhao</h3>
<p>Efficient exploration is a long-standing problem in reinforcement learning.
In this work, we introduce a self-supervised exploration policy by
incentivizing the agent to maximize multisensory incongruity, which can be
measured in two aspects: perception incongruity and action incongruity. The
former represents the uncertainty in multisensory fusion model, while the
latter represents the uncertainty in an agent's policy. Specifically, an
alignment predictor is trained to detect whether multiple sensory inputs are
aligned, the error of which is used to measure perception incongruity. The
policy takes the multisensory observations with sensory-wise dropout as input
and outputs actions for exploration. The variance of actions is further used to
measure action incongruity. Our formulation allows the agent to learn skills by
exploring in a self-supervised manner without any external rewards. Besides,
our method enables the agent to learn a compact multimodal representation from
hard examples, which further improves the sample efficiency of our policy
learning. We demonstrate the efficacy of this formulation across a variety of
benchmark environments including object manipulation and audio-visual games.
</p>
<a href="http://arxiv.org/abs/2009.12494">arXiv:2009.12494</a> [<a href="http://arxiv.org/pdf/2009.12494">pdf</a>]

<h2>Rubik: A Hierarchical Architecture for Efficient Graph Learning. (arXiv:2009.12495v1 [cs.AR])</h2>
<h3>Xiaobing Chen, Yuke Wang, Xinfeng Xie, Xing Hu, Abanti Basak, Ling Liang, Mingyu Yan, Lei Deng, Yufei Ding, Zidong Du, Yunji Chen, Yuan Xie</h3>
<p>Graph convolutional network (GCN) emerges as a promising direction to learn
the inductive representation in graph data commonly used in widespread
applications, such as E-commerce, social networks, and knowledge graphs.
However, learning from graphs is non-trivial because of its mixed computation
model involving both graph analytics and neural network computing. To this end,
we decompose the GCN learning into two hierarchical paradigms: graph-level and
node-level computing. Such a hierarchical paradigm facilitates the software and
hardware accelerations for GCN learning.

We propose a lightweight graph reordering methodology, incorporated with a
GCN accelerator architecture that equips a customized cache design to fully
utilize the graph-level data reuse. We also propose a mapping methodology aware
of data reuse and task-level parallelism to handle various graphs inputs
effectively. Results show that Rubik accelerator design improves energy
efficiency by 26.3x to 1375.2x than GPU platforms across different datasets and
GCN models.
</p>
<a href="http://arxiv.org/abs/2009.12495">arXiv:2009.12495</a> [<a href="http://arxiv.org/pdf/2009.12495">pdf</a>]

<h2>Modeling Dyadic Conversations for Personality Inference. (arXiv:2009.12496v1 [cs.CL])</h2>
<h3>Qiang Liu</h3>
<p>Nowadays, automatical personality inference is drawing extensive attention
from both academia and industry. Conventional methods are mainly based on user
generated contents, e.g., profiles, likes, and texts of an individual, on
social media, which are actually not very reliable. In contrast, dyadic
conversations between individuals can not only capture how one expresses
oneself, but also reflect how one reacts to different situations. Rich
contextual information in dyadic conversation can explain an individual's
response during his or her conversation. In this paper, we propose a novel
augmented Gated Recurrent Unit (GRU) model for learning unsupervised Personal
Conversational Embeddings (PCE) based on dyadic conversations between
individuals. We adjust the formulation of each layer of a conventional GRU with
sequence to sequence learning and personal information of both sides of the
conversation. Based on the learned PCE, we can infer the personality of each
individual. We conduct experiments on the Movie Script dataset, which is
collected from conversations between characters in movie scripts. We find that
modeling dyadic conversations between individuals can significantly improve
personality inference accuracy. Experimental results illustrate the successful
performance of our proposed method.
</p>
<a href="http://arxiv.org/abs/2009.12496">arXiv:2009.12496</a> [<a href="http://arxiv.org/pdf/2009.12496">pdf</a>]

<h2>Machine Learning Algorithms for Active Monitoring of High Performance Computing as a Service (HPCaaS) Cloud Environments. (arXiv:2009.12498v1 [cs.DC])</h2>
<h3>Gianluca Longoni (1), Ryan LaMothe (1), Jeremy Teuton (1), Mark Greaves (1), Nicole Nichols (1), William Smith (1) ((1) Pacific Northwest National Laboratory)</h3>
<p>Cloud computing provides ubiquitous and on-demand access to vast
reconfigurable resources that can meet any computational need. Many service
models are available, but the Infrastructure as a Service (IaaS) model is
particularly suited to operate as a high performance computing (HPC) platform,
by networking large numbers of cloud computing nodes. We used the Pacific
Northwest National Laboratory (PNNL) cloud computing environment to perform our
experiments. A number of cloud computing providers such as Amazon Web Services,
Microsoft Azure, or IBM Cloud, offer flexible and scalable computing resources.
This paper explores the viability identifying types of engineering applications
running on a cloud infrastructure configured as an HPC platform using privacy
preserving features as input to statistical models. The engineering
applications considered in this work include MCNP6, a radiation transport code
developed by Los Alamos National Laboratory, OpenFOAM, an open source
computational fluid dynamics code, and CADO-NFS, a numerical implementation of
the general number field sieve algorithm used for prime number factorization.
Our experiments use the OpenStack cloud management tool to create a cloud HPC
environment and the privacy preserving Ceilometer billing meters as
classification features to demonstrate identification of these applications.
</p>
<a href="http://arxiv.org/abs/2009.12498">arXiv:2009.12498</a> [<a href="http://arxiv.org/pdf/2009.12498">pdf</a>]

<h2>Learning to Plan and Realize Separately for Open-Ended Dialogue Systems. (arXiv:2009.12506v1 [cs.CL])</h2>
<h3>Sashank Santhanam, Zhuo Cheng, Brodie Mather, Bonnie Dorr, Archna Bhatia, Bryanna Hebenstreit, Alan Zemel, Adam Dalton, Tomek Strzalkowski, Samira Shaikh</h3>
<p>Achieving true human-like ability to conduct a conversation remains an
elusive goal for open-ended dialogue systems. We posit this is because extant
approaches towards natural language generation (NLG) are typically construed as
end-to-end architectures that do not adequately model human generation
processes. To investigate, we decouple generation into two separate phases:
planning and realization. In the planning phase, we train two planners to
generate plans for response utterances. The realization phase uses response
plans to produce an appropriate response. Through rigorous evaluations, both
automated and human, we demonstrate that decoupling the process into planning
and realization performs better than an end-to-end approach.
</p>
<a href="http://arxiv.org/abs/2009.12506">arXiv:2009.12506</a> [<a href="http://arxiv.org/pdf/2009.12506">pdf</a>]

<h2>Dictionary Learning with Low-rank Coding Coefficients for Tensor Completion. (arXiv:2009.12507v1 [cs.CV])</h2>
<h3>Tai-Xiang Jiang, Xi-Le Zhao, Hao Zhang, Michael K. Ng</h3>
<p>In this paper, we propose a novel tensor learning and coding model for
third-order data completion. Our model is to learn a data-adaptive dictionary
from the given observations, and determine the coding coefficients of
third-order tensor tubes. In the completion process, we minimize the
low-rankness of each tensor slice containing the coding coefficients. By
comparison with the traditional pre-defined transform basis, the advantages of
the proposed model are that (i) the dictionary can be learned based on the
given data observations so that the basis can be more adaptively and accurately
constructed, and (ii) the low-rankness of the coding coefficients can allow the
linear combination of dictionary features more effectively. Also we develop a
multi-block proximal alternating minimization algorithm for solving such tensor
learning and coding model, and show that the sequence generated by the
algorithm can globally converge to a critical point. Extensive experimental
results for real data sets such as videos, hyperspectral images, and traffic
data are reported to demonstrate these advantages and show the performance of
the proposed tensor learning and coding method is significantly better than the
other tensor completion methods in terms of several evaluation metrics.
</p>
<a href="http://arxiv.org/abs/2009.12507">arXiv:2009.12507</a> [<a href="http://arxiv.org/pdf/2009.12507">pdf</a>]

<h2>Dense-View GEIs Set: View Space Covering for Gait Recognition based on Dense-View GAN. (arXiv:2009.12516v1 [cs.CV])</h2>
<h3>Rijun Liao, Weizhi An, Shiqi Yu, Zhu Li, Yongzhen Huang</h3>
<p>Gait recognition has proven to be effective for long-distance human
recognition. But view variance of gait features would change human appearance
greatly and reduce its performance. Most existing gait datasets usually collect
data with a dozen different angles, or even more few. Limited view angles would
prevent learning better view invariant feature. It can further improve
robustness of gait recognition if we collect data with various angles at 1
degree interval. But it is time consuming and labor consuming to collect this
kind of dataset. In this paper, we, therefore, introduce a Dense-View GEIs Set
(DV-GEIs) to deal with the challenge of limited view angles. This set can cover
the whole view space, view angle from 0 degree to 180 degree with 1 degree
interval. In addition, Dense-View GAN (DV-GAN) is proposed to synthesize this
dense view set. DV-GAN consists of Generator, Discriminator and Monitor, where
Monitor is designed to preserve human identification and view information. The
proposed method is evaluated on the CASIA-B and OU-ISIR dataset. The
experimental results show that DV-GEIs synthesized by DV-GAN is an effective
way to learn better view invariant feature. We believe the idea of dense view
generated samples will further improve the development of gait recognition.
</p>
<a href="http://arxiv.org/abs/2009.12516">arXiv:2009.12516</a> [<a href="http://arxiv.org/pdf/2009.12516">pdf</a>]

<h2>Neural Twins Talk. (arXiv:2009.12524v1 [cs.CV])</h2>
<h3>Zanyar Zohourianshahzadi (UCCS), Jugal Kumar Kalita (UCCS)</h3>
<p>Inspired by how the human brain employs more neural pathways when increasing
the focus on a subject, we introduce a novel twin cascaded attention model that
outperforms a state-of-the-art image captioning model that was originally
implemented using one channel of attention for the visual grounding task.
Visual grounding ensures the existence of words in the caption sentence that
are grounded into a particular region in the input image. After a deep learning
model is trained on visual grounding task, the model employs the learned
patterns regarding the visual grounding and the order of objects in the caption
sentences, when generating captions. We report the results of our experiments
in three image captioning tasks on the COCO dataset. The results are reported
using standard image captioning metrics to show the improvements achieved by
our model over the previous image captioning model. The results gathered from
our experiments suggest that employing more parallel attention pathways in a
deep neural network leads to higher performance. Our implementation of NTT is
publicly available at: https://github.com/zanyarz/NeuralTwinsTalk.
</p>
<a href="http://arxiv.org/abs/2009.12524">arXiv:2009.12524</a> [<a href="http://arxiv.org/pdf/2009.12524">pdf</a>]

<h2>Cross-individual Recognition of Emotions by a Dynamic Entropy based on Pattern Learning with EEG features. (arXiv:2009.12525v1 [cs.LG])</h2>
<h3>Xiaolong Zhong, Zhong Yin</h3>
<p>Use of the electroencephalogram (EEG) and machine learning approaches to
recognize emotions can facilitate affective human computer interactions.
However, the type of EEG data constitutes an obstacle for cross-individual EEG
feature modelling and classification. To address this issue, we propose a
deep-learning framework denoted as a dynamic entropy-based pattern learning
(DEPL) to abstract informative indicators pertaining to the neurophysiological
features among multiple individuals. DEPL enhanced the capability of
representations generated by a deep convolutional neural network by modelling
the interdependencies between the cortical locations of dynamical entropy based
features. The effectiveness of the DEPL has been validated with two public
databases, commonly referred to as the DEAP and MAHNOB-HCI multimodal tagging
databases. Specifically, the leave one subject out training and testing
paradigm has been applied. Numerous experiments on EEG emotion recognition
demonstrate that the proposed DEPL is superior to those traditional machine
learning (ML) methods, and could learn between electrode dependencies w.r.t.
different emotions, which is meaningful for developing the effective
human-computer interaction systems by adapting to human emotions in the real
world applications.
</p>
<a href="http://arxiv.org/abs/2009.12525">arXiv:2009.12525</a> [<a href="http://arxiv.org/pdf/2009.12525">pdf</a>]

<h2>Deep Selective Combinatorial Embedding and Consistency Regularization for Light Field Super-resolution. (arXiv:2009.12537v1 [eess.IV])</h2>
<h3>Jing Jin, Junhui Hou, Zhiyu Zhu, Jie Chen, Sam Kwong</h3>
<p>Light field (LF) images acquired by hand-held devices usually suffer from low
spatial resolution as the limited detector resolution has to be shared with the
angular dimension. LF spatial super-resolution (SR) thus becomes an
indispensable part of the LF camera processing pipeline. The
high-dimensionality characteristic and complex geometrical structure of LF
images make the problem more challenging than traditional single-image SR. The
performance of existing methods is still limited as they fail to thoroughly
explore the coherence among LF sub-aperture images (SAIs) and are insufficient
in accurately preserving the scene's parallax structure. To tackle this
challenge, we propose a novel learning-based LF spatial SR framework.
Specifically, each SAI of an LF image is first coarsely and individually
super-resolved by exploring the complementary information among SAIs with
selective combinatorial geometry embedding. To achieve efficient and effective
selection of the complementary information, we propose two novel sub-modules
conducted hierarchically: the patch selector provides an option of retrieving
similar image patches based on offline disparity estimation to handle
large-disparity correlations; and the SAI selector adaptively and flexibly
selects the most informative SAIs to improve the embedding efficiency. To
preserve the parallax structure among the reconstructed SAIs, we subsequently
append a consistency regularization network trained over a structure-aware loss
function to refine the parallax relationships over the coarse estimation. In
addition, we extend the proposed method to irregular LF data. To the best of
our knowledge, this is the first learning-based SR method for irregular LF
data. Experimental results over both synthetic and real-world LF datasets
demonstrate the significant advantage of our approach over state-of-the-art
methods.
</p>
<a href="http://arxiv.org/abs/2009.12537">arXiv:2009.12537</a> [<a href="http://arxiv.org/pdf/2009.12537">pdf</a>]

<h2>A light-weight method to foster the (Grad)CAM interpretability and explainability of classification networks. (arXiv:2009.12546v1 [cs.CV])</h2>
<h3>Alfred Sch&#xf6;ttl</h3>
<p>We consider a light-weight method which allows to improve the explainability
of localized classification networks. The method considers (Grad)CAM maps
during the training process by modification of the training loss and does not
require additional structural elements. It is demonstrated that the (Grad)CAM
interpretability, as measured by several indicators, can be improved in this
way. Since the method shall be applicable on embedded systems and on standard
deeper architectures, it essentially takes advantage of second order
derivatives during the training and does not require additional model layers.
</p>
<a href="http://arxiv.org/abs/2009.12546">arXiv:2009.12546</a> [<a href="http://arxiv.org/pdf/2009.12546">pdf</a>]

<h2>Affinity Space Adaptation for Semantic Segmentation Across Domains. (arXiv:2009.12559v1 [cs.CV])</h2>
<h3>Wei Zhou, Yukang Wang, Jiajia Chu, Jiehua Yang, Xiang Bai, Yongchao Xu</h3>
<p>Semantic segmentation with dense pixel-wise annotation has achieved excellent
performance thanks to deep learning. However, the generalization of semantic
segmentation in the wild remains challenging. In this paper, we address the
problem of unsupervised domain adaptation (UDA) in semantic segmentation.
Motivated by the fact that source and target domain have invariant semantic
structures, we propose to exploit such invariance across domains by leveraging
co-occurring patterns between pairwise pixels in the output of structured
semantic segmentation. This is different from most existing approaches that
attempt to adapt domains based on individual pixel-wise information in image,
feature, or output level. Specifically, we perform domain adaptation on the
affinity relationship between adjacent pixels termed affinity space of source
and target domain. To this end, we develop two affinity space adaptation
strategies: affinity space cleaning and adversarial affinity space alignment.
Extensive experiments demonstrate that the proposed method achieves superior
performance against some state-of-the-art methods on several challenging
benchmarks for semantic segmentation across domains. The code is available at
https://github.com/idealwei/ASANet.
</p>
<a href="http://arxiv.org/abs/2009.12559">arXiv:2009.12559</a> [<a href="http://arxiv.org/pdf/2009.12559">pdf</a>]

<h2>Differentially Private and Fair Deep Learning: A Lagrangian Dual Approach. (arXiv:2009.12562v1 [cs.LG])</h2>
<h3>Cuong Tran, Ferdinando Fioretto, Pascal Van Hentenryck</h3>
<p>A critical concern in data-driven decision making is to build models whose
outcomes do not discriminate against some demographic groups, including gender,
ethnicity, or age. To ensure non-discrimination in learning tasks, knowledge of
the sensitive attributes is essential, while, in practice, these attributes may
not be available due to legal and ethical requirements. To address this
challenge, this paper studies a model that protects the privacy of the
individuals sensitive information while also allowing it to learn
non-discriminatory predictors. The method relies on the notion of differential
privacy and the use of Lagrangian duality to design neural networks that can
accommodate fairness constraints while guaranteeing the privacy of sensitive
attributes. The paper analyses the tension between accuracy, privacy, and
fairness and the experimental evaluation illustrates the benefits of the
proposed model on several prediction tasks.
</p>
<a href="http://arxiv.org/abs/2009.12562">arXiv:2009.12562</a> [<a href="http://arxiv.org/pdf/2009.12562">pdf</a>]

<h2>Metaphor Detection using Deep Contextualized Word Embeddings. (arXiv:2009.12565v1 [cs.CL])</h2>
<h3>Shashwat Aggarwal, Ramesh Singh</h3>
<p>Metaphors are ubiquitous in natural language, and their detection plays an
essential role in many natural language processing tasks, such as language
understanding, sentiment analysis, etc. Most existing approaches for metaphor
detection rely on complex, hand-crafted and fine-tuned feature pipelines, which
greatly limit their applicability. In this work, we present an end-to-end
method composed of deep contextualized word embeddings, bidirectional LSTMs and
multi-head attention mechanism to address the task of automatic metaphor
detection. Our method, unlike many other existing approaches, requires only the
raw text sequences as input features to detect the metaphoricity of a phrase.
We compare the performance of our method against the existing baselines on two
benchmark datasets, TroFi, and MOH-X respectively. Experimental evaluations
confirm the effectiveness of our approach.
</p>
<a href="http://arxiv.org/abs/2009.12565">arXiv:2009.12565</a> [<a href="http://arxiv.org/pdf/2009.12565">pdf</a>]

<h2>An Explainable Model for EEG Seizure Detection based on Connectivity Features. (arXiv:2009.12566v1 [cs.LG])</h2>
<h3>Mohammad Mansour, Fouad Khnaisser, Hmayag Partamian</h3>
<p>Epilepsy which is characterized by seizures is studied using EEG signals by
recording the electrical activity of the brain. Different types of
communication between different parts of the brain are characterized by many
state of the art connectivity measures which can be directed and undirected. We
propose to employ a set of undirected (spectral matrix, the inverse of the
spectral matrix, coherence, partial coherence, and phaselocking value) and
directed features (directed coherence, the partial directed coherence) to learn
a deep neural network that detects whether a particular data window belongs to
a seizure or not, which is a new approach to standard seizure classification.
Taking our data as a sequence of ten sub-windows, we aim at designing an
optimal deep learning model using attention, CNN, BiLstm, and fully connected
layers. We also compute the relevance using the weights of the learned model
based on the activation values of the receptive fields at a particular layer.
Our best model architecture resulted in 97.03% accuracy using balanced MITBIH
data subset. Also, we were able to explain the relevance of each feature across
all patients. We were able to experimentally validate some of the scientific
facts concerning seizures by studying the impact of the contributions of the
activations on the decision.
</p>
<a href="http://arxiv.org/abs/2009.12566">arXiv:2009.12566</a> [<a href="http://arxiv.org/pdf/2009.12566">pdf</a>]

<h2>DT-Net: A novel network based on multi-directional integrated convolution and threshold convolution. (arXiv:2009.12569v1 [cs.CV])</h2>
<h3>Hongfeng You, Long Yu, Shengwei Tian, Xiang Ma, Yan Xing, Xiaojie Ma</h3>
<p>Since medical image data sets contain few samples and singular features,
lesions are viewed as highly similar to other tissues. The traditional neural
network has a limited ability to learn features. Even if a host of feature maps
is expanded to obtain more semantic information, the accuracy of segmenting the
final medical image is slightly improved, and the features are excessively
redundant. To solve the above problems, in this paper, we propose a novel
end-to-end semantic segmentation algorithm, DT-Net, and use two new convolution
strategies to better achieve end-to-end semantic segmentation of medical
images. 1. In the feature mining and feature fusion stage, we construct a
multi-directional integrated convolution (MDIC). The core idea is to use the
multi-scale convolution to enhance the local multi-directional feature maps to
generate enhanced feature maps and to mine the generated features that contain
more semantics without increasing the number of feature maps. 2. We also aim to
further excavate and retain more meaningful deep features reduce a host of
noise features in the training process. Therefore, we propose a convolution
thresholding strategy. The central idea is to set a threshold to eliminate a
large number of redundant features and reduce computational complexity. Through
the two strategies proposed above, the algorithm proposed in this paper
produces state-of-the-art results on two public medical image datasets. We
prove in detail that our proposed strategy plays an important role in feature
mining and eliminating redundant features. Compared with the existing semantic
segmentation algorithms, our proposed algorithm has better robustness.
</p>
<a href="http://arxiv.org/abs/2009.12569">arXiv:2009.12569</a> [<a href="http://arxiv.org/pdf/2009.12569">pdf</a>]

<h2>Quantifying the effect of image compression on supervised learning applications in optical microscopy. (arXiv:2009.12570v1 [eess.IV])</h2>
<h3>Enrico Pomarico, C&#xe9;dric Schmidt, Florian Chays, David Nguyen, Arielle Planchette, Audrey Tissot, Adrien Roux, St&#xe9;phane Pag&#xe8;s, Laura Batti, Christoph Clausen, Theo Lasser, Aleksandra Radenovic, Bruno Sanguinetti, J&#xe9;r&#xf4;me Extermann</h3>
<p>The impressive growth of data throughput in optical microscopy has triggered
a widespread use of supervised learning (SL) models running on compressed image
datasets for efficient automated analysis. However, since lossy image
compression risks to produce unpredictable artifacts, quantifying the effect of
data compression on SL applications is of pivotal importance to assess their
reliability, especially for clinical use. We propose an experimental method to
evaluate the tolerability of image compression distortions in 2D and 3D cell
segmentation SL tasks: predictions on compressed data are compared to the raw
predictive uncertainty, which is numerically estimated from the raw noise
statistics measured through sensor calibration. We show that predictions on
object- and image-specific segmentation parameters can be altered by up to 15%
and more than 10 standard deviations after 16-to-8 bits downsampling or JPEG
compression. In contrast, a recently developed lossless compression algorithm
provides a prediction spread which is statistically equivalent to that stemming
from raw noise, while providing a compression ratio of up to 10:1. By setting a
lower bound to the SL predictive uncertainty, our technique can be generalized
to validate a variety of data analysis pipelines in SL-assisted fields.
</p>
<a href="http://arxiv.org/abs/2009.12570">arXiv:2009.12570</a> [<a href="http://arxiv.org/pdf/2009.12570">pdf</a>]

<h2>Physics-Guided Recurrent Graph Networks for Predicting Flow and Temperature in River Networks. (arXiv:2009.12575v1 [physics.geo-ph])</h2>
<h3>Xiaowei Jia, Jacob Zwart, Jeffery Sadler, Alison Appling, Samantha Oliver, Steven Markstrom, Jared Willard, Shaoming Xu, Michael Steinbach, Jordan Read, Vipin Kumar</h3>
<p>This paper proposes a physics-guided machine learning approach that combines
advanced machine learning models and physics-based models to improve the
prediction of water flow and temperature in river networks. We first build a
recurrent graph network model to capture the interactions among multiple
segments in the river network. Then we present a pre-training technique which
transfers knowledge from physics-based models to initialize the machine
learning model and learn the physics of streamflow and thermodynamics. We also
propose a new loss function that balances the performance over different river
segments. We demonstrate the effectiveness of the proposed method in predicting
temperature and streamflow in a subset of the Delaware River Basin. In
particular, we show that the proposed method brings a 33\%/14\% improvement
over the state-of-the-art physics-based model and 24\%/14\% over traditional
machine learning models (e.g., Long-Short Term Memory Neural Network) in
temperature/streamflow prediction using very sparse (0.1\%) observation data
for training. The proposed method has also been shown to produce better
performance when generalized to different seasons or river segments with
different streamflow ranges.
</p>
<a href="http://arxiv.org/abs/2009.12575">arXiv:2009.12575</a> [<a href="http://arxiv.org/pdf/2009.12575">pdf</a>]

<h2>Inverse Rational Control with Partially Observable Continuous Nonlinear Dynamics. (arXiv:2009.12576v1 [cs.LG])</h2>
<h3>Minhae Kwon, Saurabh Daptardar, Paul Schrater, Xaq Pitkow</h3>
<p>A fundamental question in neuroscience is how the brain creates an internal
model of the world to guide actions using sequences of ambiguous sensory
information. This is naturally formulated as a reinforcement learning problem
under partial observations, where an agent must estimate relevant latent
variables in the world from its evidence, anticipate possible future states,
and choose actions that optimize total expected reward. This problem can be
solved by control theory, which allows us to find the optimal actions for a
given system dynamics and objective function. However, animals often appear to
behave suboptimally. Why? We hypothesize that animals have their own flawed
internal model of the world, and choose actions with the highest expected
subjective reward according to that flawed model. We describe this behavior as
rational but not optimal. The problem of Inverse Rational Control (IRC) aims to
identify which internal model would best explain an agent's actions. Our
contribution here generalizes past work on Inverse Rational Control which
solved this problem for discrete control in partially observable Markov
decision processes. Here we accommodate continuous nonlinear dynamics and
continuous actions, and impute sensory observations corrupted by unknown noise
that is private to the animal. We first build an optimal Bayesian agent that
learns an optimal policy generalized over the entire model space of dynamics
and subjective rewards using deep reinforcement learning. Crucially, this
allows us to compute a likelihood over models for experimentally observable
action trajectories acquired from a suboptimal agent. We then find the model
parameters that maximize the likelihood using gradient ascent.
</p>
<a href="http://arxiv.org/abs/2009.12576">arXiv:2009.12576</a> [<a href="http://arxiv.org/pdf/2009.12576">pdf</a>]

<h2>A Few-shot Learning Approach for Historical Ciphered Manuscript Recognition. (arXiv:2009.12577v1 [cs.CV])</h2>
<h3>Mohamed Ali Souibgui, Alicia Forn&#xe9;s, Yousri Kessentini, Crina Tudor</h3>
<p>Encoded (or ciphered) manuscripts are a special type of historical documents
that contain encrypted text. The automatic recognition of this kind of
documents is challenging because: 1) the cipher alphabet changes from one
document to another, 2) there is a lack of annotated corpus for training and 3)
touching symbols make the symbol segmentation difficult and complex. To
overcome these difficulties, we propose a novel method for handwritten ciphers
recognition based on few-shot object detection. Our method first detects all
symbols of a given alphabet in a line image, and then a decoding step maps the
symbol similarity scores to the final sequence of transcribed symbols. By
training on synthetic data, we show that the proposed architecture is able to
recognize handwritten ciphers with unseen alphabets. In addition, if few
labeled pages with the same alphabet are used for fine tuning, our method
surpasses existing unsupervised and supervised HTR methods for ciphers
recognition.
</p>
<a href="http://arxiv.org/abs/2009.12577">arXiv:2009.12577</a> [<a href="http://arxiv.org/pdf/2009.12577">pdf</a>]

<h2>Inductive Graph Embeddings through Locality Encodings. (arXiv:2009.12585v1 [cs.LG])</h2>
<h3>Nurudin Alvarez-Gonzalez, Andreas Kaltenbrunner, Vicen&#xe7; G&#xf3;mez</h3>
<p>Learning embeddings from large-scale networks is an open challenge. Despite
the overwhelming number of existing methods, is is unclear how to exploit
network structure in a way that generalizes easily to unseen nodes, edges or
graphs. In this work, we look at the problem of finding inductive network
embeddings in large networks without domain-dependent node/edge attributes. We
propose to use a set of basic predefined local encodings as the basis of a
learning algorithm. In particular, we consider the degree frequencies at
different distances from a node, which can be computed efficiently for
relatively short distances and a large number of nodes. Interestingly, the
resulting embeddings generalize well across unseen or distant regions in the
network, both in unsupervised settings, when combined with language model
learning, as well as in supervised tasks, when used as additional features in a
neural network. Despite its simplicity, this method achieves state-of-the-art
performance in tasks such as role detection, link prediction and node
classification, and represents an inductive network embedding method directly
applicable to large unattributed networks.
</p>
<a href="http://arxiv.org/abs/2009.12585">arXiv:2009.12585</a> [<a href="http://arxiv.org/pdf/2009.12585">pdf</a>]

<h2>TraceSim: A Method for Calculating Stack Trace Similarity. (arXiv:2009.12590v1 [cs.SE])</h2>
<h3>Roman Vasiliev, Dmitrij Koznov, George Chernishev, Aleksandr Khvorov, Dmitry Luciv, Nikita Povarov</h3>
<p>Many contemporary software products have subsystems for automatic crash
reporting. However, it is well-known that the same bug can produce slightly
different reports. To manage this problem, reports are usually grouped, often
manually by developers. Manual triaging, however, becomes infeasible for
products that have large userbases, which is the reason for many different
approaches to automating this task. Moreover, it is important to improve
quality of triaging due to the big volume of reports that needs to be processed
properly. Therefore, even a relatively small improvement could play a
significant role in overall accuracy of report bucketing. The majority of
existing studies use some kind of a stack trace similarity metric, either based
on information retrieval techniques or string matching methods. However, it
should be stressed that the quality of triaging is still insufficient. In this
paper, we describe TraceSim -- a novel approach to address this problem which
combines TF-IDF, Levenshtein distance, and machine learning to construct a
similarity metric. Our metric has been implemented inside an industrial-grade
report triaging system. The evaluation on a manually labeled dataset shows
significantly better results compared to baseline approaches.
</p>
<a href="http://arxiv.org/abs/2009.12590">arXiv:2009.12590</a> [<a href="http://arxiv.org/pdf/2009.12590">pdf</a>]

<h2>ProDOMA: improve PROtein DOMAin classification for third-generation sequencing reads using deep learning. (arXiv:2009.12591v1 [q-bio.GN])</h2>
<h3>Du Nan, Jiayu Shang, Yanni Sun</h3>
<p>Motivation: With the development of third-generation sequencing technologies,
people are able to obtain DNA sequences with lengths from 10s to 100s of kb.
These long reads allow protein domain annotation without assembly, thus can
produce important insights into the biological functions of the underlying
data. However, the high error rate in third-generation sequencing data raises a
new challenge to established domain analysis pipelines. The state-of-the-art
methods are not optimized for noisy reads and have shown unsatisfactory
accuracy of domain classification in third-generation sequencing data. New
computational methods are still needed to improve the performance of domain
prediction in long noisy reads. Results: In this work, we introduce ProDOMA, a
deep learning model that conducts domain classification for third-generation
sequencing reads. It uses deep neural networks with 3-frame translation
encoding to learn conserved features from partially correct translations. In
addition, we formulate our problem as an open-set problem and thus our model
can reject unrelated DNA reads such as those from noncoding regions. In the
experiments on simulated reads of protein coding sequences and real reads from
the human genome, our model outperforms HMMER and DeepFam on protein domain
classification. In summary, ProDOMA is a useful end-to-end protein domain
analysis tool for long noisy reads without relying on error correction.
Availability: The source code and the trained model are freely available at
https://github.com/strideradu/ProDOMA. Contact: yannisun@cityu.edu.hk
</p>
<a href="http://arxiv.org/abs/2009.12591">arXiv:2009.12591</a> [<a href="http://arxiv.org/pdf/2009.12591">pdf</a>]

<h2>Online Learning of Non-Markovian Reward Models. (arXiv:2009.12600v1 [cs.AI])</h2>
<h3>Gavin Rens, Jean-Fran&#xe7;ois Raskin, Rapha&#xeb;l Reynouad, Giuseppe Marra</h3>
<p>There are situations in which an agent should receive rewards only after
having accomplished a series of previous tasks, that is, rewards are {\em
non-Markovian}. One natural and quite general way to represent
history-dependent rewards is via a {\em Mealy machine}, a finite state
automaton that produces output sequences from input sequences. In our formal
setting, we consider a Markov decision process (MDP) that models the dynamics
of the environment in which the agent evolves and a Mealy machine synchronized
with this MDP to formalize the non-Markovian reward function. While the MDP is
known by the agent, the reward function is unknown to the agent and must be
learned.

Our approach to overcome this challenge is to use Angluin's $L^*$ active
learning algorithm to learn a Mealy machine representing the underlying
non-Markovian reward machine (MRM). Formal methods are used to determine the
optimal strategy for answering so-called membership queries posed by $L^*$.

Moreover, we prove that the expected reward achieved will eventually be at
least as much as a given, reasonable value provided by a domain expert. We
evaluate our framework on three problems. The results show that using $L^*$ to
learn an MRM in a non-Markovian reward decision process is effective.
</p>
<a href="http://arxiv.org/abs/2009.12600">arXiv:2009.12600</a> [<a href="http://arxiv.org/pdf/2009.12600">pdf</a>]

<h2>fMRI Multiple Missing Values Imputation Regularized by a Recurrent Denoiser. (arXiv:2009.12602v1 [eess.IV])</h2>
<h3>David Calhas, Rui Henriques</h3>
<p>Functional Magnetic Resonance Imaging (fMRI) is a neuroimaging technique with
pivotal importance due to its scientific and clinical applications. As with any
widely used imaging modality, there is a need to ensure the quality of the
same, with missing values being highly frequent due to the presence of
artifacts or sub-optimal imaging resolutions. Our work focus on missing values
imputation on multivariate signal data. To do so, a new imputation method is
proposed consisting on two major steps: spatial-dependent signal imputation and
time-dependent regularization of the imputed signal. A novel layer, to be used
in deep learning architectures, is proposed in this work, bringing back the
concept of chained equations for multiple imputation. Finally, a recurrent
layer is applied to tune the signal, such that it captures its true patterns.
Both operations yield an improved robustness against state-of-the-art
alternatives.
</p>
<a href="http://arxiv.org/abs/2009.12602">arXiv:2009.12602</a> [<a href="http://arxiv.org/pdf/2009.12602">pdf</a>]

<h2>Graph neural induction of value iteration. (arXiv:2009.12604v1 [cs.LG])</h2>
<h3>Andreea Deac, Pierre-Luc Bacon, Jian Tang</h3>
<p>Many reinforcement learning tasks can benefit from explicit planning based on
an internal model of the environment. Previously, such planning components have
been incorporated through a neural network that partially aligns with the
computational graph of value iteration. Such network have so far been focused
on restrictive environments (e.g. grid-worlds), and modelled the planning
procedure only indirectly. We relax these constraints, proposing a graph neural
network (GNN) that executes the value iteration (VI) algorithm, across
arbitrary environment models, with direct supervision on the intermediate steps
of VI. The results indicate that GNNs are able to model value iteration
accurately, recovering favourable metrics and policies across a variety of
out-of-distribution tests. This suggests that GNN executors with strong
supervision are a viable component within deep reinforcement learning systems.
</p>
<a href="http://arxiv.org/abs/2009.12604">arXiv:2009.12604</a> [<a href="http://arxiv.org/pdf/2009.12604">pdf</a>]

<h2>Grasp Proposal Networks: An End-to-End Solution for Visual Learning of Robotic Grasps. (arXiv:2009.12606v1 [cs.CV])</h2>
<h3>Chaozheng Wu, Jian Chen, Qiaoyu Cao, Jianchi Zhang, Yunxin Tai, Lin Sun, Kui Jia</h3>
<p>Learning robotic grasps from visual observations is a promising yet
challenging task. Recent research shows its great potential by preparing and
learning from large-scale synthetic datasets. For the popular, 6
degree-of-freedom (6-DOF) grasp setting of parallel-jaw gripper, most of
existing methods take the strategy of heuristically sampling grasp candidates
and then evaluating them using learned scoring functions. This strategy is
limited in terms of the conflict between sampling efficiency and coverage of
optimal grasps. To this end, we propose in this work a novel, end-to-end
\emph{Grasp Proposal Network (GPNet)}, to predict a diverse set of 6-DOF grasps
for an unseen object observed from a single and unknown camera view. GPNet
builds on a key design of grasp proposal module that defines \emph{anchors of
grasp centers} at discrete but regular 3D grid corners, which is flexible to
support either more precise or more diverse grasp predictions. To test GPNet,
we contribute a synthetic dataset of 6-DOF object grasps; evaluation is
conducted using rule-based criteria, simulation test, and real test.
Comparative results show the advantage of our methods over existing ones.
Notably, GPNet gains better simulation results via the specified coverage,
which helps achieve a ready translation in real test. We will make our dataset
publicly available.
</p>
<a href="http://arxiv.org/abs/2009.12606">arXiv:2009.12606</a> [<a href="http://arxiv.org/pdf/2009.12606">pdf</a>]

<h2>Deep Learning-based Four-region Lung Segmentation in Chest Radiography for COVID-19 Diagnosis. (arXiv:2009.12610v1 [eess.IV])</h2>
<h3>Young-Gon Kim, Kyungsang Kim, Dufan Wu, Hui Ren, Won Young Tak, Soo Young Park, Yu Rim Lee, Min Kyu Kang, Jung Gil Park, Byung Seok Kim, Woo Jin Chung, Mannudeep K. Kalra, Quanzheng Li</h3>
<p>Purpose. Imaging plays an important role in assessing severity of COVID 19
pneumonia. However, semantic interpretation of chest radiography (CXR) findings
does not include quantitative description of radiographic opacities. Most
current AI assisted CXR image analysis framework do not quantify for regional
variations of disease. To address these, we proposed a four region lung
segmentation method to assist accurate quantification of COVID 19 pneumonia.
Methods. A segmentation model to separate left and right lung is firstly
applied, and then a carina and left hilum detection network is used, which are
the clinical landmarks to separate the upper and lower lungs. To improve the
segmentation performance of COVID 19 images, ensemble strategy incorporating
five models is exploited. Using each region, we evaluated the clinical
relevance of the proposed method with the Radiographic Assessment of the
Quality of Lung Edema (RALE). Results. The proposed ensemble strategy showed
dice score of 0.900, which is significantly higher than conventional methods
(0.854 0.889). Mean intensities of segmented four regions indicate positive
correlation to the extent and density scores of pulmonary opacities under the
RALE framework. Conclusion. A deep learning based model in CXR can accurately
segment and quantify regional distribution of pulmonary opacities in patients
with COVID 19 pneumonia.
</p>
<a href="http://arxiv.org/abs/2009.12610">arXiv:2009.12610</a> [<a href="http://arxiv.org/pdf/2009.12610">pdf</a>]

<h2>Neurosymbolic Reinforcement Learning with Formally Verified Exploration. (arXiv:2009.12612v1 [cs.LG])</h2>
<h3>Greg Anderson, Abhinav Verma, Isil Dillig, Swarat Chaudhuri</h3>
<p>We present Revel, a partially neural reinforcement learning (RL) framework
for provably safe exploration in continuous state and action spaces. A key
challenge for provably safe deep RL is that repeatedly verifying neural
networks within a learning loop is computationally infeasible. We address this
challenge using two policy classes: a general, neurosymbolic class with
approximate gradients and a more restricted class of symbolic policies that
allows efficient verification. Our learning algorithm is a mirror descent over
policies: in each iteration, it safely lifts a symbolic policy into the
neurosymbolic space, performs safe gradient updates to the resulting policy,
and projects the updated policy into the safe symbolic subset, all without
requiring explicit verification of neural networks. Our empirical results show
that Revel enforces safe exploration in many scenarios in which Constrained
Policy Optimization does not, and that it can discover policies that outperform
those learned through prior approaches to verified exploration.
</p>
<a href="http://arxiv.org/abs/2009.12612">arXiv:2009.12612</a> [<a href="http://arxiv.org/pdf/2009.12612">pdf</a>]

<h2>Automatic Arabic Dialect Identification Systems for Written Texts: A Survey. (arXiv:2009.12622v1 [cs.CL])</h2>
<h3>Maha J. Althobaiti</h3>
<p>Arabic dialect identification is a specific task of natural language
processing, aiming to automatically predict the Arabic dialect of a given text.
Arabic dialect identification is the first step in various natural language
processing applications such as machine translation, multilingual
text-to-speech synthesis, and cross-language text generation. Therefore, in the
last decade, interest has increased in addressing the problem of Arabic dialect
identification. In this paper, we present a comprehensive survey of Arabic
dialect identification research in written texts. We first define the problem
and its challenges. Then, the survey extensively discusses in a critical manner
many aspects related to Arabic dialect identification task. So, we review the
traditional machine learning methods, deep learning architectures, and complex
learning approaches to Arabic dialect identification. We also detail the
features and techniques for feature representations used to train the proposed
systems. Moreover, we illustrate the taxonomy of Arabic dialects studied in the
literature, the various levels of text processing at which Arabic dialect
identification are conducted (e.g., token, sentence, and document level), as
well as the available annotated resources, including evaluation benchmark
corpora. Open challenges and issues are discussed at the end of the survey.
</p>
<a href="http://arxiv.org/abs/2009.12622">arXiv:2009.12622</a> [<a href="http://arxiv.org/pdf/2009.12622">pdf</a>]

<h2>DWIE: an entity-centric dataset for multi-task document-level information extraction. (arXiv:2009.12626v1 [cs.CL])</h2>
<h3>Klim Zaporojets, Johannes Deleu, Chris Develder, Thomas Demeester</h3>
<p>This paper presents DWIE, the 'Deutsche Welle corpus for Information
Extraction', a newly created multi-task dataset that combines four main
Information Extraction (IE) annotation sub-tasks: (i) Named Entity Recognition
(NER), (ii) Coreference Resolution, (iii) Relation Extraction (RE), and (iv)
Entity Linking. DWIE is conceived as an entity-centric dataset that describes
interactions and properties of conceptual entities on the level of the complete
document. This contrasts with currently dominant mention-driven approaches that
start from the detection and classification of named entity mentions in
individual sentences. Further, DWIE presented two main challenges when building
and evaluating IE models for it. First, the use of traditional mention-level
evaluation metrics for NER and RE tasks on entity-centric DWIE dataset can
result in measurements dominated by predictions on more frequently mentioned
entities. We tackle this issue by proposing a new entity-driven metric that
takes into account the number of mentions that compose each of the predicted
and ground truth entities. Second, the document-level multi-task annotations
require the models to transfer information between entity mentions located in
different parts of the document, as well as between different tasks, in a joint
learning setting. To realize this, we propose to use graph-based neural message
passing techniques between document-level mention spans. Our experiments show
an improvement of up to 5.5 F1 percentage points when incorporating neural
graph propagation into our joint model. This demonstrates DWIE's potential to
stimulate further research in graph neural networks for representation learning
in multi-task IE. We make DWIE publicly available at
https://github.com/klimzaporojets/DWIE.
</p>
<a href="http://arxiv.org/abs/2009.12626">arXiv:2009.12626</a> [<a href="http://arxiv.org/pdf/2009.12626">pdf</a>]

<h2>Complementary Meta-Reinforcement Learning for Fault-Adaptive Control. (arXiv:2009.12634v1 [cs.LG])</h2>
<h3>Ibrahim Ahmed, Marcos Quinones-Grueiro, Gautam Biswas</h3>
<p>Faults are endemic to all systems. Adaptive fault-tolerant control maintains
degraded performance when faults occur as opposed to unsafe conditions or
catastrophic events. In systems with abrupt faults and strict time constraints,
it is imperative for control to adapt quickly to system changes to maintain
system operations. We present a meta-reinforcement learning approach that
quickly adapts its control policy to changing conditions. The approach builds
upon model-agnostic meta learning (MAML). The controller maintains a complement
of prior policies learned under system faults. This "library" is evaluated on a
system after a new fault to initialize the new policy. This contrasts with
MAML, where the controller derives intermediate policies anew, sampled from a
distribution of similar systems, to initialize a new policy. Our approach
improves sample efficiency of the reinforcement learning process. We evaluate
our approach on an aircraft fuel transfer system under abrupt faults.
</p>
<a href="http://arxiv.org/abs/2009.12634">arXiv:2009.12634</a> [<a href="http://arxiv.org/pdf/2009.12634">pdf</a>]

<h2>Quantitative and Qualitative Evaluation of Explainable Deep Learning Methods for Ophthalmic Diagnosis. (arXiv:2009.12648v1 [eess.IV])</h2>
<h3>Amitojdeep Singh, J. Jothi Balaji, Varadharajan Jayakumar, Mohammed Abdul Rasheed, Rajiv Raman, Vasudevan Lakshminarayanan</h3>
<p>Background: The lack of explanations for the decisions made by algorithms
such as deep learning has hampered their acceptance by the clinical community
despite highly accurate results on multiple problems. Recently, attribution
methods have emerged for explaining deep learning models, and they have been
tested on medical imaging problems. The performance of attribution methods is
compared on standard machine learning datasets and not on medical images. In
this study, we perform a comparative analysis to determine the most suitable
explainability method for retinal OCT diagnosis.

Methods: A commonly used deep learning model known as Inception v3 was
trained to diagnose 3 retinal diseases - choroidal neovascularization (CNV),
diabetic macular edema (DME), and drusen. The explanations from 13 different
attribution methods were rated by a panel of 14 clinicians for clinical
significance. Feedback was obtained from the clinicians regarding the current
and future scope of such methods.

Results: An attribution method based on a Taylor series expansion, called
Deep Taylor was rated the highest by clinicians with a median rating of 3.85/5.
It was followed by two other attribution methods, Guided backpropagation and
SHAP (SHapley Additive exPlanations).

Conclusion: Explanations of deep learning models can make them more
transparent for clinical diagnosis. This study compared different explanations
methods in the context of retinal OCT diagnosis and found that the best
performing method may not be the one considered best for other deep learning
tasks. Overall, there was a high degree of acceptance from the clinicians
surveyed in the study.

Keywords: explainable AI, deep learning, machine learning, image processing,
Optical coherence tomography, retina, Diabetic macular edema, Choroidal
Neovascularization, Drusen
</p>
<a href="http://arxiv.org/abs/2009.12648">arXiv:2009.12648</a> [<a href="http://arxiv.org/pdf/2009.12648">pdf</a>]

<h2>Bidirectional Representation Learning from Transformers using Multimodal Electronic Health Record Data for Chronic to Predict Depression. (arXiv:2009.12656v1 [cs.LG])</h2>
<h3>Yiwen Meng, William Speier, Michael K. Ong, Corey W. Arnold</h3>
<p>Advancements in machine learning algorithms have had a beneficial impact on
representation learning, classification, and prediction models built using
electronic health record (EHR) data. Effort has been put both on increasing
models' overall performance as well as improving their interpretability,
particularly regarding the decision-making process. In this study, we present a
temporal deep learning model to perform bidirectional representation learning
on EHR sequences with a transformer architecture to predict future diagnosis of
depression. This model is able to aggregate five heterogenous and
high-dimensional data sources from the EHR and process them in a temporal
manner for chronic disease prediction at various prediction windows. We applied
the current trend of pretraining and fine-tuning on EHR data to outperform the
current state-of-the-art in chronic disease prediction, and to demonstrate the
underlying relation between EHR codes in the sequence. The model generated the
highest increases of precision-recall area under the curve (PRAUC) from 0.70 to
0.76 in depression prediction compared to the best baseline model. Furthermore,
the self-attention weights in each sequence quantitatively demonstrated the
inner relationship between various codes, which improved the model's
interpretability. These results demonstrate the model's ability to utilize
heterogeneous EHR data to predict depression while achieving high accuracy and
interpretability, which may facilitate constructing clinical decision support
systems in the future for chronic disease screening and early detection.
</p>
<a href="http://arxiv.org/abs/2009.12656">arXiv:2009.12656</a> [<a href="http://arxiv.org/pdf/2009.12656">pdf</a>]

<h2>Domain Generalization via Semi-supervised Meta Learning. (arXiv:2009.12658v1 [cs.LG])</h2>
<h3>Hossein Sharifi-Noghabi, Hossein Asghari, Nazanin Mehrasa, Martin Ester</h3>
<p>The goal of domain generalization is to learn from multiple source domains to
generalize to unseen target domains under distribution discrepancy. Current
state-of-the-art methods in this area are fully supervised, but for many
real-world problems it is hardly possible to obtain enough labeled samples. In
this paper, we propose the first method of domain generalization to leverage
unlabeled samples, combining of meta learning's episodic training and
semi-supervised learning, called DGSML. DGSML employs an entropy-based
pseudo-labeling approach to assign labels to unlabeled samples and then
utilizes a novel discrepancy loss to ensure that class centroids before and
after labeling unlabeled samples are close to each other. To learn a
domain-invariant representation, it also utilizes a novel alignment loss to
ensure that the distance between pairs of class centroids, computed after
adding the unlabeled samples, is preserved across different domains. DGSML is
trained by a meta learning approach to mimic the distribution shift between the
input source domains and unseen target domains. Experimental results on
benchmark datasets indicate that DGSML outperforms state-of-the-art domain
generalization and semi-supervised learning methods.
</p>
<a href="http://arxiv.org/abs/2009.12658">arXiv:2009.12658</a> [<a href="http://arxiv.org/pdf/2009.12658">pdf</a>]

<h2>Reliability-Performance Trade-offs in Neuromorphic Computing. (arXiv:2009.12672v1 [cs.NE])</h2>
<h3>Twisha Titirsha, Anup Das</h3>
<p>Neuromorphic architectures built with Non-Volatile Memory (NVM) can
significantly improve the energy efficiency of machine learning tasks designed
with Spiking Neural Networks (SNNs). A major source of voltage drop in a
crossbar of these architectures are the parasitic components on the crossbar's
bitlines and wordlines, which are deliberately made longer to achieve lower
cost-per-bit. We observe that the parasitic voltage drops create a significant
asymmetry in programming speed and reliability of NVM cells in a crossbar.
Specifically, NVM cells that are on shorter current paths are faster to program
but have lower endurance than those on longer current paths, and vice versa.
This asymmetry in neuromorphic architectures create reliability-performance
trade-offs, which can be exploited efficiently using SNN mapping techniques. In
this work, we demonstrate such trade-offs using a previously-proposed SNN
mapping technique with 10 workloads from contemporary machine learning tasks
for a state-of-the art neuromoorphic hardware.
</p>
<a href="http://arxiv.org/abs/2009.12672">arXiv:2009.12672</a> [<a href="http://arxiv.org/pdf/2009.12672">pdf</a>]

<h2>Enhancing a Neurocognitive Shared Visuomotor Model for Object Identification, Localization, and Grasping With Learning From Auxiliary Tasks. (arXiv:2009.12674v1 [cs.CV])</h2>
<h3>Matthias Kerzel (1), Fares Abawi (1), Manfred Eppe (1), Stefan Wermter (1) ((1) University of Hamburg)</h3>
<p>We present a follow-up study on our unified visuomotor neural model for the
robotic tasks of identifying, localizing, and grasping a target object in a
scene with multiple objects. Our Retinanet-based model enables end-to-end
training of visuomotor abilities in a biologically inspired developmental
approach. In our initial implementation, a neural model was able to grasp
selected objects from a planar surface. We embodied the model on the NICO
humanoid robot. In this follow-up study, we expand the task and the model to
reaching for objects in a three-dimensional space with a novel dataset based on
augmented reality and a simulation environment. We evaluate the influence of
training with auxiliary tasks, i.e., if learning of the primary visuomotor task
is supported by learning to classify and locate different objects. We show that
the proposed visuomotor model can learn to reach for objects in a
three-dimensional space. We analyze the results for biologically-plausible
biases based on object locations or properties. We show that the primary
visuomotor task can be successfully trained simultaneously with one of the two
auxiliary tasks. This is enabled by a complex neurocognitive model with shared
and task-specific components, similar to models found in biological systems.
</p>
<a href="http://arxiv.org/abs/2009.12674">arXiv:2009.12674</a> [<a href="http://arxiv.org/pdf/2009.12674">pdf</a>]

<h2>A Primal-Dual Subgradient Approach for Fair Meta Learning. (arXiv:2009.12675v1 [cs.LG])</h2>
<h3>Chen Zhao, Feng Chen, Zhuoyi Wang, Latifur Khan</h3>
<p>The problem of learning to generalize on unseen classes during the training
step, also known as few-shot classification, has attracted considerable
attention. Initialization based methods, such as the gradient-based model
agnostic meta-learning (MAML), tackle the few-shot learning problem by
"learning to fine-tune". The goal of these approaches is to learn proper model
initialization, so that the classifiers for new classes can be learned from a
few labeled examples with a small number of gradient update steps. Few shot
meta-learning is well-known with its fast-adapted capability and accuracy
generalization onto unseen tasks. Learning fairly with unbiased outcomes is
another significant hallmark of human intelligence, which is rarely touched in
few-shot meta-learning. In this work, we propose a novel Primal-Dual Fair
Meta-learning framework, namely PDFM, which learns to train fair machine
learning models using only a few examples based on data from related tasks. The
key idea is to learn a good initialization of a fair model's primal and dual
parameters so that it can adapt to a new fair learning task via a few gradient
update steps. Instead of manually tunning the dual parameters as
hyperparameters via a grid search, PDFM optimizes the initialization of the
primal and dual parameters jointly for fair meta-learning via a subgradient
primal-dual approach. We further instantiate an example of bias controlling
using decision boundary covariance (DBC) as the fairness constraint for each
task, and demonstrate the versatility of our proposed approach by applying it
to classification on a variety of three real-world datasets. Our experiments
show substantial improvements over the best prior work for this setting.
</p>
<a href="http://arxiv.org/abs/2009.12675">arXiv:2009.12675</a> [<a href="http://arxiv.org/pdf/2009.12675">pdf</a>]

<h2>I Like to Move It: 6D Pose Estimation as an Action Decision Process. (arXiv:2009.12678v1 [cs.CV])</h2>
<h3>Benjamin Busam, Hyun Jun Jung, Nassir Navab</h3>
<p>Object pose estimation is an integral part of robot vision and augmented
reality. Robust and accurate pose prediction of both object rotation and
translation is a crucial element to enable precise and safe human-machine
interactions and to allow visualization in mixed reality. Previous 6D pose
estimation methods treat the problem either as a regression task or discretize
the pose space to classify. We reformulate the problem as an action decision
process where an initial pose is updated in incremental discrete steps that
sequentially move a virtual 3D rendering towards the correct solution. A neural
network estimates likely moves from a single RGB image iteratively and
determines so an acceptable final pose. In comparison to previous approaches
that learn an object-specific pose embedding, a decision process allows for a
lightweight architecture while it naturally generalizes to unseen objects.
Moreover, the coherent action for process termination enables dynamic reduction
of the computation cost if there are insignificant changes in a video sequence.
While other methods only provide a static inference time, we can thereby
automatically increase the runtime depending on the object motion. We evaluate
robustness and accuracy of our action decision network on video scenes with
known and unknown objects and show how this can improve the state-of-the-art on
YCB videos significantly.
</p>
<a href="http://arxiv.org/abs/2009.12678">arXiv:2009.12678</a> [<a href="http://arxiv.org/pdf/2009.12678">pdf</a>]

<h2>Clustering-based Unsupervised Generative Relation Extraction. (arXiv:2009.12681v1 [cs.CL])</h2>
<h3>Chenhan Yuan, Ryan Rossi, Andrew Katz, Hoda Eldardiry</h3>
<p>This paper focuses on the problem of unsupervised relation extraction.
Existing probabilistic generative model-based relation extraction methods work
by extracting sentence features and using these features as inputs to train a
generative model. This model is then used to cluster similar relations.
However, these methods do not consider correlations between sentences with the
same entity pair during training, which can negatively impact model
performance. To address this issue, we propose a Clustering-based Unsupervised
generative Relation Extraction (CURE) framework that leverages an
"Encoder-Decoder" architecture to perform self-supervised learning so the
encoder can extract relation information. Given multiple sentences with the
same entity pair as inputs, self-supervised learning is deployed by predicting
the shortest path between entity pairs on the dependency graph of one of the
sentences. After that, we extract the relation information using the
well-trained encoder. Then, entity pairs that share the same relation are
clustered based on their corresponding relation information. Each cluster is
labeled with a few words based on the words in the shortest paths corresponding
to the entity pairs in each cluster. These cluster labels also describe the
meaning of these relation clusters. We compare the triplets extracted by our
proposed framework (CURE) and baseline methods with a ground-truth Knowledge
Base. Experimental results show that our model performs better than
state-of-the-art models on both New York Times (NYT) and United Nations
Parallel Corpus (UNPC) standard datasets.
</p>
<a href="http://arxiv.org/abs/2009.12681">arXiv:2009.12681</a> [<a href="http://arxiv.org/pdf/2009.12681">pdf</a>]

<h2>Reinforcement Learning-based N-ary Cross-Sentence Relation Extraction. (arXiv:2009.12683v1 [cs.LG])</h2>
<h3>Chenhan Yuan, Ryan Rossi, Andrew Katz, Hoda Eldardiry</h3>
<p>The models of n-ary cross sentence relation extraction based on distant
supervision assume that consecutive sentences mentioning n entities describe
the relation of these n entities. However, on one hand, this assumption
introduces noisy labeled data and harms the models' performance. On the other
hand, some non-consecutive sentences also describe one relation and these
sentences cannot be labeled under this assumption. In this paper, we relax this
strong assumption by a weaker distant supervision assumption to address the
second issue and propose a novel sentence distribution estimator model to
address the first problem. This estimator selects correctly labeled sentences
to alleviate the effect of noisy data is a two-level agent reinforcement
learning model. In addition, a novel universal relation extractor with a hybrid
approach of attention mechanism and PCNN is proposed such that it can be
deployed in any tasks, including consecutive and nonconsecutive sentences.
Experiments demonstrate that the proposed model can reduce the impact of noisy
data and achieve better performance on general n-ary cross sentence relation
extraction task compared to baseline models.
</p>
<a href="http://arxiv.org/abs/2009.12683">arXiv:2009.12683</a> [<a href="http://arxiv.org/pdf/2009.12683">pdf</a>]

<h2>MicroAnalyzer: A Python Tool for Automated Bacterial Analysis with Fluorescence Microscopy. (arXiv:2009.12684v1 [cs.CV])</h2>
<h3>Jonathan Reiner, Guy Azran, Gal Hyams</h3>
<p>Fluorescence microscopy is a widely used method among cell biologists for
studying the localization and co-localization of fluorescent protein. For
microbial cell biologists, these studies often include tedious and
time-consuming manual segmentation of bacteria and of the fluorescence clusters
or working with multiple programs. Here, we present MicroAnalyzer - a tool that
automates these tasks by providing an end-to-end platform for microscope image
analysis. While such tools do exist, they are costly, black-boxed programs.
Microanalyzer offers an open-source alternative to these tools, allowing
flexibility and expandability by advanced users. MicroAnalyzer provides
accurate cell and fluorescence cluster segmentation based on state-of-the-art
deep-learning segmentation models, combined with ad-hoc post-processing and
Colicoords - an open-source cell image analysis tool for calculating general
cell and fluorescence measurements. Using these methods, it performs better
than generic approaches since the dynamic nature of neural networks allows for
a quick adaptation to experiment restrictions and assumptions. Other existing
tools do not consider experiment assumptions, nor do they provide fluorescence
cluster detection without the need for any specialized equipment. The key goal
of MicroAnalyzer is to automate the entire process of cell and fluorescence
image analysis "from microscope to database", meaning it does not require any
further input from the researcher except for the initial deep-learning model
training. In this fashion, it allows the researchers to concentrate on the
bigger picture instead of granular, eye-straining labor
</p>
<a href="http://arxiv.org/abs/2009.12684">arXiv:2009.12684</a> [<a href="http://arxiv.org/pdf/2009.12684">pdf</a>]

<h2>Adaptive Non-reversible Stochastic Gradient Langevin Dynamics. (arXiv:2009.12690v1 [cs.LG])</h2>
<h3>Vikram Krishnamurthy, George Yin</h3>
<p>It is well known that adding any skew symmetric matrix to the gradient of
Langevin dynamics algorithm results in a non-reversible diffusion with improved
convergence rate. This paper presents a gradient algorithm to adaptively
optimize the choice of the skew symmetric matrix. The resulting algorithm
involves a non-reversible diffusion algorithm cross coupled with a stochastic
gradient algorithm that adapts the skew symmetric matrix. The algorithm uses
the same data as the classical Langevin algorithm. A weak convergence proof is
given for the optimality of the choice of the skew symmetric matrix. The
improved convergence rate of the algorithm is illustrated numerically in
Bayesian learning and tracking examples.
</p>
<a href="http://arxiv.org/abs/2009.12690">arXiv:2009.12690</a> [<a href="http://arxiv.org/pdf/2009.12690">pdf</a>]

<h2>COVID-19 Infection Map Generation and Detection from Chest X-Ray Images. (arXiv:2009.12698v1 [eess.IV])</h2>
<h3>Aysen Degerli, Mete Ahishali, Mehmet Yamac, Serkan Kiranyaz, Muhammad E. H. Chowdhury, Khalid Hameed, Tahir Hamid, Rashid Mazhar, Moncef Gabbouj</h3>
<p>Computer-aided diagnosis has become a necessity for accurate and immediate
coronavirus disease 2019 (COVID-19) detection to aid treatment and prevent the
spread of the virus. Compared to other diagnosis methodologies, chest X-ray
(CXR) imaging is an advantageous tool since it is fast, low-cost, and easily
accessible. Thus, CXR has a great potential not only to help diagnose COVID-19
but also to track the progression of the disease. Numerous studies have
proposed to use Deep Learning techniques for COVID-19 diagnosis. However, they
have used very limited CXR image repositories for evaluation with a small
number, a few hundreds, of COVID-19 samples. Moreover, these methods can
neither localize nor grade the severity of COVID-19 infection. For this
purpose, recent studies proposed to explore the activation maps of deep
networks. However, they remain inaccurate for localizing the actual infestation
making them unreliable for clinical use. This study proposes a novel method for
the joint localization, severity grading, and detection of COVID-19 from CXR
images by generating the so-called infection maps that can accurately localize
and grade the severity of COVID-19 infection. To accomplish this, we have
compiled the largest COVID-19 dataset up to date with 2951 COVID-19 CXR images,
where the annotation of the ground-truth segmentation masks is performed on
CXRs by a novel collaborative expert human-machine approach. Furthermore, we
publicly release the first CXR dataset with the ground-truth segmentation masks
of the COVID-19 infected regions. A detailed set of experiments show that
state-of-the-art segmentation networks can learn to localize COVID-19 infection
with an F1-score of 85.81%, that is significantly superior to the activation
maps created by the previous methods. Finally, the proposed approach achieved a
COVID-19 detection performance with 98.37% sensitivity and 99.16% specificity.
</p>
<a href="http://arxiv.org/abs/2009.12698">arXiv:2009.12698</a> [<a href="http://arxiv.org/pdf/2009.12698">pdf</a>]

<h2>Neural Proof Nets. (arXiv:2009.12702v1 [cs.CL])</h2>
<h3>Konstantinos Kogkalidis, Michael Moortgat, Richard Moot</h3>
<p>Linear logic and the linear {\lambda}-calculus have a long standing tradition
in the study of natural language form and meaning. Among the proof calculi of
linear logic, proof nets are of particular interest, offering an attractive
geometric representation of derivations that is unburdened by the bureaucratic
complications of conventional prooftheoretic formats. Building on recent
advances in set-theoretic learning, we propose a neural variant of proof nets
based on Sinkhorn networks, which allows us to translate parsing as the problem
of extracting syntactic primitives and permuting them into alignment. Our
methodology induces a batch-efficient, end-to-end differentiable architecture
that actualizes a formally grounded yet highly efficient neuro-symbolic parser.
We test our approach on {\AE}Thel, a dataset of type-logical derivations for
written Dutch, where it manages to correctly transcribe raw text sentences into
proofs and terms of the linear {\lambda}-calculus with an accuracy of as high
as 70%.
</p>
<a href="http://arxiv.org/abs/2009.12702">arXiv:2009.12702</a> [<a href="http://arxiv.org/pdf/2009.12702">pdf</a>]

<h2>An Adaptive EM Accelerator for Unsupervised Learning of Gaussian Mixture Models. (arXiv:2009.12703v1 [cs.LG])</h2>
<h3>Truong Nguyen, Guangye Chen, Luis Chacon</h3>
<p>We propose an Anderson Acceleration (AA) scheme for the adaptive
Expectation-Maximization (EM) algorithm for unsupervised learning a finite
mixture model from multivariate data (Figueiredo and Jain 2002). The proposed
algorithm is able to determine the optimal number of mixture components
autonomously, and converges to the optimal solution much faster than its
non-accelerated version. The success of the AA-based algorithm stems from
several developments rather than a single breakthrough (and without these, our
tests demonstrate that AA fails catastrophically). To begin, we ensure the
monotonicity of the likelihood function (a the key feature of the standard EM
algorithm) with a recently proposed monotonicity-control algorithm (Henderson
and Varahdan 2019), enhanced by a novel monotonicity test with little overhead.
We propose nimble strategies for AA to preserve the positive definiteness of
the Gaussian weights and covariance matrices strictly, and to conserve up to
the second moments of the observed data set exactly. Finally, we employ a
K-means clustering algorithm using the gap statistic to avoid excessively
overestimating the initial number of components, thereby maximizing
performance. We demonstrate the accuracy and efficiency of the algorithm with
several synthetic data sets that are mixtures of Gaussians distributions of
known number of components, as well as data sets generated from
particle-in-cell simulations. Our numerical results demonstrate speed-ups with
respect to non-accelerated EM of up to 60X when the exact number of mixture
components is known, and between a few and more than an order of magnitude with
component adaptivity.
</p>
<a href="http://arxiv.org/abs/2009.12703">arXiv:2009.12703</a> [<a href="http://arxiv.org/pdf/2009.12703">pdf</a>]

<h2>Local and non-local dependency learning and emergence of rule-like representations in speech data by Deep Convolutional Generative Adversarial Networks. (arXiv:2009.12711v1 [cs.CL])</h2>
<h3>Ga&#x161;per Begu&#x161;</h3>
<p>This paper argues that training GANs on local and non-local dependencies in
speech data offers insights into how deep neural networks discretize continuous
data and how symbolic-like rule-based morphophonological processes emerge in a
deep convolutional architecture. Acquisition of speech has recently been
modeled as a dependency between latent space and data generated by GANs in
Begu\v{s} (arXiv:2006.03965), who models learning of a simple local allophonic
distribution. We extend this approach to test learning of local and non-local
phonological processes that include approximations of morphological processes.
We further parallel outputs of the model to results of a behavioral experiment
where human subjects are trained on the data used for training the GAN network.
Four main conclusions emerge: (i) the networks provide useful information for
computational models of language acquisition even if trained on a comparatively
small dataset of an artificial grammar learning experiment; (ii) local
processes are easier to learn than non-local processes, which matches both
behavioral data in human subjects and typology in the world's languages. This
paper also proposes (iii) how we can actively observe the network's progress in
learning and explore the effect of training steps on learning representations
by keeping latent space constant across different training steps. Finally, this
paper shows that (iv) the network learns to encode the presence of a prefix
with a single latent variable; by interpolating this variable, we can actively
observe the operation of a non-local phonological process. The proposed
technique for retrieving learning representations has general implications for
our understanding of how GANs discretize continuous speech data and suggests
that rule-like generalizations in the training data are represented as an
interaction between variables in the network's latent space.
</p>
<a href="http://arxiv.org/abs/2009.12711">arXiv:2009.12711</a> [<a href="http://arxiv.org/pdf/2009.12711">pdf</a>]

<h2>Differentially Private Adversarial Robustness Through Randomized Perturbations. (arXiv:2009.12718v1 [cs.LG])</h2>
<h3>Nan Xu, Oluwaseyi Feyisetan, Abhinav Aggarwal, Zekun Xu, Nathanael Teissier</h3>
<p>Deep Neural Networks, despite their great success in diverse domains, are
provably sensitive to small perturbations on correctly classified examples and
lead to erroneous predictions. Recently, it was proposed that this behavior can
be combatted by optimizing the worst case loss function over all possible
substitutions of training examples. However, this can be prone to weighing
unlikely substitutions higher, limiting the accuracy gain. In this paper, we
study adversarial robustness through randomized perturbations, which has two
immediate advantages: (1) by ensuring that substitution likelihood is weighted
by the proximity to the original word, we circumvent optimizing the worst case
guarantees and achieve performance gains; and (2) the calibrated randomness
imparts differentially-private model training, which additionally improves
robustness against adversarial attacks on the model outputs. Our approach uses
a novel density-based mechanism based on truncated Gumbel noise, which ensures
training on substitutions of both rare and dense words in the vocabulary while
maintaining semantic similarity for model robustness.
</p>
<a href="http://arxiv.org/abs/2009.12718">arXiv:2009.12718</a> [<a href="http://arxiv.org/pdf/2009.12718">pdf</a>]

<h2>Multi-timescale representation learning in LSTM Language Models. (arXiv:2009.12727v1 [cs.CL])</h2>
<h3>Shivangi Mahto, Vy A. Vo, Javier S. Turek, Alexander G. Huth</h3>
<p>Although neural language models are effective at capturing statistics of
natural language, their representations are challenging to interpret. In
particular, it is unclear how these models retain information over multiple
timescales. In this work, we construct explicitly multi-timescale language
models by manipulating the input and forget gate biases in a long short-term
memory (LSTM) network. The distribution of timescales is selected to
approximate power law statistics of natural language through a combination of
exponentially decaying memory cells. We then empirically analyze the timescale
of information routed through each part of the model using word ablation
experiments and forget gate visualizations. These experiments show that the
multi-timescale model successfully learns representations at the desired
timescales, and that the distribution includes longer timescales than a
standard LSTM. Further, information about high-,mid-, and low-frequency words
is routed preferentially through units with the appropriate timescales. Thus we
show how to construct language models with interpretable representations of
different information timescales.
</p>
<a href="http://arxiv.org/abs/2009.12727">arXiv:2009.12727</a> [<a href="http://arxiv.org/pdf/2009.12727">pdf</a>]

<h2>STAN: Synthetic Network Traffic Generation using Autoregressive Neural Models. (arXiv:2009.12740v1 [cs.LG])</h2>
<h3>Shengzhe Xu, Manish Marwah, Naren Ramakrishnan</h3>
<p>Deep learning models have achieved great success in recent years. However,
large amounts of data are typically required to train such models. While some
types of data, such as images, videos, and text, are easier to find, data in
certain domains is difficult to obtain. For instance, cybersecurity
applications routinely use network traffic data which organizations are
reluctant to share, even internally, due to privacy reasons. An alternative is
to use synthetically generated data; however, most existing data generating
methods lack the ability to capture complex dependency structures that are
usually prevalent in real data by assuming independence either temporally or
between attributes. This paper presents our approach called STAN, Synthetic
Network Traffic Generation using Autoregressive Neural models, to generate
realistic synthetic network traffic data. Our novel autoregressive neural
architecture captures both temporal dependence and dependence between
attributes at any given time. It integrates convolutional neural layers (CNN)
with mixture density layers (MDN) and softmax layers to model both continuous
and discrete variables. We evaluate performance of STAN by training it on both
a simulated dataset and a real network traffic data set. Multiple metrics are
used to compare the generated data with real data and with data generated via
several baseline methods. Finally, to answer the question -- can real network
traffic data be substituted with synthetic data to train models of comparable
accuracy -- we consider two commonly used models for anomaly detection in such
data, and compare F1/MSE measures of models trained on real data and those on
increasing proportions of generated data. The results show only a small decline
in accuracy of models trained solely on synthetic data.
</p>
<a href="http://arxiv.org/abs/2009.12740">arXiv:2009.12740</a> [<a href="http://arxiv.org/pdf/2009.12740">pdf</a>]

<h2>Faster Biological Gradient Descent Learning. (arXiv:2009.12745v1 [cs.NE])</h2>
<h3>Ho Ling Li</h3>
<p>Back-propagation is a popular machine learning algorithm that uses gradient
descent in training neural networks for supervised learning, but can be very
slow. A number of algorithms have been developed to speed up convergence and
improve robustness of the learning. However, they are complicated to implement
biologically as they require information from previous updates. Inspired by
synaptic competition in biology, we have come up with a simple and local
gradient descent optimization algorithm that can reduce training time, with no
demand on past details. Our algorithm, named dynamic learning rate (DLR), works
similarly to the traditional gradient descent used in back-propagation, except
that instead of having a uniform learning rate across all synapses, the
learning rate depends on the current neuronal connection weights. Our algorithm
is found to speed up learning, particularly for small networks.
</p>
<a href="http://arxiv.org/abs/2009.12745">arXiv:2009.12745</a> [<a href="http://arxiv.org/pdf/2009.12745">pdf</a>]

<h2>Smart Irrigation IoT Solution using Transfer Learning for Neural Networks. (arXiv:2009.12747v1 [cs.LG])</h2>
<h3>A. Risheh, A. Jalili, E. Nazerfard</h3>
<p>In this paper we develop a reliable system for smart irrigation of
greenhouses using artificial neural networks, and an IoT architecture. Our
solution uses four sensors in different layers of soil to predict future
moisture. Using a dataset we collected by running experiments on different
soils, we show high performance of neural networks compared to existing
alternative method of support vector regression. To reduce the processing power
of neural network for the IoT edge devices, we propose using transfer learning.
Transfer learning also speeds up training performance with small amount of
training data, and allows integrating climate sensors to a pre-trained model,
which are the other two challenges of smart irrigation of greenhouses. Our
proposed IoT architecture shows a complete solution for smart irrigation.
</p>
<a href="http://arxiv.org/abs/2009.12747">arXiv:2009.12747</a> [<a href="http://arxiv.org/pdf/2009.12747">pdf</a>]

<h2>Iterative Reconstruction for Low-Dose CT using Deep Gradient Priors of Generative Model. (arXiv:2009.12760v1 [eess.IV])</h2>
<h3>Zhuonan He, Yikun Zhang, Yu Guan, Shanzhou Niu, Yi Zhang, Yang Chen, Qiegen Liu</h3>
<p>Dose reduction in computed tomography (CT) is essential for decreasing
radiation risk in clinical applications. Iterative reconstruction is one of the
most promising ways to compensate for the increased noise due to reduction of
photon flux. Rather than most existing prior-driven algorithms that benefit
from manually designed prior functions or supervised learning schemes, in this
work we integrate the data-consistency as a conditional term into the iterative
generative model for low-dose CT. At first, a score-based generative network is
used for unsupervised distribution learning and the gradient of generative
density prior is learned from normal-dose images. Then, the annealing Langevin
dynamics is employed to update the trained priors with conditional scheme,
i.e., the distance between the reconstructed image and the manifold is
minimized along with data fidelity during reconstruction. Experimental
comparisons demonstrated the noise reduction and detail preservation abilities
of the proposed method.
</p>
<a href="http://arxiv.org/abs/2009.12760">arXiv:2009.12760</a> [<a href="http://arxiv.org/pdf/2009.12760">pdf</a>]

<h2>Semi-Supervised Learning for In-Game Expert-Level Music-to-Dance Translation. (arXiv:2009.12763v1 [cs.CV])</h2>
<h3>Yinglin Duan (1), Tianyang Shi (1), Zhengxia Zou (2), Jia Qin (1 and 3), Yifei Zhao (1), Yi Yuan (1), Jie Hou (1), Xiang Wen (1 and 3), Changjie Fan (1) ((1) NetEase Fuxi AI Lab, (2) University of Michigan, Ann Arbor, (3) Zhejiang University)</h3>
<p>Music-to-dance translation is a brand-new and powerful feature in recent
role-playing games. Players can now let their characters dance along with
specified music clips and even generate fan-made dance videos. Previous works
of this topic consider music-to-dance as a supervised motion generation problem
based on time-series data. However, these methods suffer from limited training
data pairs and the degradation of movements. This paper provides a new
perspective for this task where we re-formulate the translation problem as a
piece-wise dance phrase retrieval problem based on the choreography theory.
With such a design, players are allowed to further edit the dance movements on
top of our generation while other regression based methods ignore such user
interactivity. Considering that the dance motion capture is an expensive and
time-consuming procedure which requires the assistance of professional dancers,
we train our method under a semi-supervised learning framework with a large
unlabeled dataset (20x than labeled data) collected. A co-ascent mechanism is
introduced to improve the robustness of our network. Using this unlabeled
dataset, we also introduce self-supervised pre-training so that the translator
can understand the melody, rhythm, and other components of music phrases. We
show that the pre-training significantly improves the translation accuracy than
that of training from scratch. Experimental results suggest that our method not
only generalizes well over various styles of music but also succeeds in
expert-level choreography for game players.
</p>
<a href="http://arxiv.org/abs/2009.12763">arXiv:2009.12763</a> [<a href="http://arxiv.org/pdf/2009.12763">pdf</a>]

<h2>Inductively Representing Out-of-Knowledge-Graph Entities by Optimal Estimation Under Translational Assumptions. (arXiv:2009.12765v1 [cs.CL])</h2>
<h3>Damai Dai, Hua Zheng, Fuli Luo, Pengcheng Yang, Baobao Chang, Zhifang Sui</h3>
<p>Conventional Knowledge Graph Completion (KGC) assumes that all test entities
appear during training. However, in real-world scenarios, Knowledge Graphs (KG)
evolve fast with out-of-knowledge-graph (OOKG) entities added frequently, and
we need to represent these entities efficiently. Most existing Knowledge Graph
Embedding (KGE) methods cannot represent OOKG entities without costly
retraining on the whole KG. To enhance efficiency, we propose a simple and
effective method that inductively represents OOKG entities by their optimal
estimation under translational assumptions. Given pretrained embeddings of the
in-knowledge-graph (IKG) entities, our method needs no additional learning.
Experimental results show that our method outperforms the state-of-the-art
methods with higher efficiency on two KGC tasks with OOKG entities.
</p>
<a href="http://arxiv.org/abs/2009.12765">arXiv:2009.12765</a> [<a href="http://arxiv.org/pdf/2009.12765">pdf</a>]

<h2>Hierarchical Deep Multi-modal Network for Medical Visual Question Answering. (arXiv:2009.12770v1 [cs.CL])</h2>
<h3>Deepak Gupta, Swati Suman, Asif Ekbal</h3>
<p>Visual Question Answering in Medical domain (VQA-Med) plays an important role
in providing medical assistance to the end-users. These users are expected to
raise either a straightforward question with a Yes/No answer or a challenging
question that requires a detailed and descriptive answer. The existing
techniques in VQA-Med fail to distinguish between the different question types
sometimes complicates the simpler problems, or over-simplifies the complicated
ones. It is certainly true that for different question types, several distinct
systems can lead to confusion and discomfort for the end-users. To address this
issue, we propose a hierarchical deep multi-modal network that analyzes and
classifies end-user questions/queries and then incorporates a query-specific
approach for answer prediction. We refer our proposed approach as Hierarchical
Question Segregation based Visual Question Answering, in short HQS-VQA. Our
contributions are three-fold, viz. firstly, we propose a question segregation
(QS) technique for VQAMed; secondly, we integrate the QS model to the
hierarchical deep multi-modal neural network to generate proper answers to the
queries related to medical images; and thirdly, we study the impact of QS in
Medical-VQA by comparing the performance of the proposed model with QS and a
model without QS. We evaluate the performance of our proposed model on two
benchmark datasets, viz. RAD and CLEF18. Experimental results show that our
proposed HQS-VQA technique outperforms the baseline models with significant
margins. We also conduct a detailed quantitative and qualitative analysis of
the obtained results and discover potential causes of errors and their
solutions.
</p>
<a href="http://arxiv.org/abs/2009.12770">arXiv:2009.12770</a> [<a href="http://arxiv.org/pdf/2009.12770">pdf</a>]

<h2>Machine Learning in Event-Triggered Control: Recent Advances and Open Issues. (arXiv:2009.12783v1 [eess.SY])</h2>
<h3>Leila Sedghi, Zohaib Ijaz, Md. Noor-A-Rahim, Kritchai Witheephanich, Dirk Pesch</h3>
<p>Network Control Systems (NCSs) have attracted much interest over the past
decade as part of a move towards more decentralised control applications and
the rise of cyberphysical system applications. Many practical NCSs face the
challenges of limited communication bandwidth resources, reliability and lack
of knowledge of network dynamics, particularly when wireless networks are
involved. Machine learning (ML) combined with event-triggered control (ETC) has
the potential to ease some of these challenges. For example, ML can be used to
overcome the problem of a lack of network models by learning system behaviour
or adapt to dynamically changing models by continually learning model dynamics.
ETC can help to conserve bandwidth resources by communicating only when needed
or when resources are available. Here, we present a review of the literature on
work that combines ML and ETC. The literature on supervised, semi-supervised,
unsupervised and reinforcement learning based approaches such as deep
reinforcement learning and statistical learning in combination with ETC is
explored. Furthermore, the difference between the application of these learning
algorithms on model-based and model-free systems are discussed. Following the
analysis of the literature, we highlight open research questions and challenges
related to ML-based ETC and propose approaches to possible solutions to these
challenges.
</p>
<a href="http://arxiv.org/abs/2009.12783">arXiv:2009.12783</a> [<a href="http://arxiv.org/pdf/2009.12783">pdf</a>]

<h2>Agile Reactive Navigation for A Non-Holonomic Mobile Robot Using A Pixel Processor Array. (arXiv:2009.12796v1 [cs.RO])</h2>
<h3>Yanan Liu, Laurie Bose, Colin Greatwood, Jianing Chen, Rui Fan, Thomas Richardson, Stephen J. Carey, Piotr Dudek, Walterio Mayol-Cuevas</h3>
<p>This paper presents an agile reactive navigation strategy for driving a
non-holonomic ground vehicle around a preset course of gates in a cluttered
environment using a low-cost processor array sensor. This enables machine
vision tasks to be performed directly upon the sensor's image plane, rather
than using a separate general-purpose computer. We demonstrate a small ground
vehicle running through or avoiding multiple gates at high speed using minimal
computational resources. To achieve this, target tracking algorithms are
developed for the Pixel Processing Array and captured images are then processed
directly on the vision sensor acquiring target information for controlling the
ground vehicle. The algorithm can run at up to 2000 fps outdoors and 200fps at
indoor illumination levels. Conducting image processing at the sensor level
avoids the bottleneck of image transfer encountered in conventional sensors.
The real-time performance of on-board image processing and robustness is
validated through experiments. Experimental results demonstrate that the
algorithm's ability to enable a ground vehicle to navigate at an average speed
of 2.20 m/s for passing through multiple gates and 3.88 m/s for a 'slalom' task
in an environment featuring significant visual clutter.
</p>
<a href="http://arxiv.org/abs/2009.12796">arXiv:2009.12796</a> [<a href="http://arxiv.org/pdf/2009.12796">pdf</a>]

<h2>Dynamic aspiration based on Win-Stay-Lose-Learn rule in Spatial Prisoner's Dilemma Gam. (arXiv:2009.12819v1 [physics.soc-ph])</h2>
<h3>Zhenyu Shi, Wei Wei, Xiangnan Feng, Xing Li, Zhiming Zheng</h3>
<p>Prisoner's dilemma game is the most commonly used model of spatial
evolutionary game which is considered as a paradigm to portray competition
among selfish individuals. In recent years, Win-Stay-Lose-Learn, a strategy
updating rule base on aspiration, has been proved to be an effective model to
promote cooperation in spatial prisoner's dilemma game, which leads aspiration
to receive lots of attention. But in many research the assumption that
individual's aspiration is fixed is inconsistent with recent results from
psychology. In this paper, according to Expected Value Theory and Achievement
Motivation Theory, we propose a dynamic aspiration model based on
Win-Stay-Lose-Learn rule in which individual's aspiration is inspired by its
payoff. It is found that dynamic aspiration has a significant impact on the
evolution process, and different initial aspirations lead to different results,
which are called Stable Coexistence under Low Aspiration, Dependent Coexistence
under Moderate aspiration and Defection Explosion under High Aspiration
respectively. Furthermore, a deep analysis is performed on the local structures
which cause cooperator's existence or defector's expansion, and the evolution
process for different parameters including strategy and aspiration. As a
result, the intrinsic structures leading to defectors' expansion and
cooperators' survival are achieved for different evolution process, which
provides a penetrating understanding of the evolution. Compared to fixed
aspiration model, dynamic aspiration introduces a more satisfactory explanation
on population evolution laws and can promote deeper comprehension for the
principle of prisoner's dilemma.
</p>
<a href="http://arxiv.org/abs/2009.12819">arXiv:2009.12819</a> [<a href="http://arxiv.org/pdf/2009.12819">pdf</a>]

<h2>Experimental Design for Overparameterized Learning with Application to Single Shot Deep Active Learning. (arXiv:2009.12820v1 [cs.LG])</h2>
<h3>Neta Shoham, Haim Avron</h3>
<p>The impressive performance exhibited by modern machine learning models hinges
on the ability to train such models on a very large amounts of labeled data.
However, since access to large volumes of labeled data is often limited or
expensive, it is desirable to alleviate this bottleneck by carefully curating
the training set. Optimal experimental design is a well-established paradigm
for selecting data point to be labeled so to maximally inform the learning
process. Unfortunately, classical theory on optimal experimental design focuses
on selecting examples in order to learn underparameterized (and thus,
non-interpolative) models, while modern machine learning models such as deep
neural networks are overparameterized, and oftentimes are trained to be
interpolative. As such, classical experimental design methods are not
applicable in many modern learning setups. Indeed, the predictive performance
of underparameterized models tends to be variance dominated, so classical
experimental design focuses on variance reduction, while the predictive
performance of overparameterized models can also be, as is shown in this paper,
bias dominated or of mixed nature. In this paper we propose a design strategy
that is well suited for overparameterized regression and interpolation, and we
demonstrate the applicability of our method in the context of deep learning by
proposing a new algorithm for single-shot deep active learning.
</p>
<a href="http://arxiv.org/abs/2009.12820">arXiv:2009.12820</a> [<a href="http://arxiv.org/pdf/2009.12820">pdf</a>]

<h2>Multi-task Causal Learning with Gaussian Processes. (arXiv:2009.12821v1 [stat.ML])</h2>
<h3>Virginia Aglietti, Theodoros Damoulas, Mauricio &#xc1;lvarez, Javier Gonz&#xe1;lez</h3>
<p>This paper studies the problem of learning the correlation structure of a set
of intervention functions defined on the directed acyclic graph (DAG) of a
causal model. This is useful when we are interested in jointly learning the
causal effects of interventions on different subsets of variables in a DAG,
which is common in field such as healthcare or operations research. We propose
the first multi-task causal Gaussian process (GP) model, which we call DAG-GP,
that allows for information sharing across continuous interventions and across
experiments on different variables. DAG-GP accommodates different assumptions
in terms of data availability and captures the correlation between functions
lying in input spaces of different dimensionality via a well-defined integral
operator. We give theoretical results detailing when and how the DAG-GP model
can be formulated depending on the DAG. We test both the quality of its
predictions and its calibrated uncertainties. Compared to single-task models,
DAG-GP achieves the best fitting performance in a variety of real and synthetic
settings. In addition, it helps to select optimal interventions faster than
competing approaches when used within sequential decision making frameworks,
like active learning or Bayesian optimization.
</p>
<a href="http://arxiv.org/abs/2009.12821">arXiv:2009.12821</a> [<a href="http://arxiv.org/pdf/2009.12821">pdf</a>]

<h2>Domain Generalization for Medical Imaging Classification with Linear-Dependency Regularization. (arXiv:2009.12829v1 [cs.CV])</h2>
<h3>Haoliang Li (1), YuFei Wang (1), Renjie Wan (1), Shiqi Wang (2), Tie-Qiang Li (3), Alex C. Kot (1)</h3>
<p>Recently, we have witnessed great progress in the field of medical imaging
classification by adopting deep neural networks. However, the recent advanced
models still require accessing sufficiently large and representative datasets
for training, which is often unfeasible in clinically realistic environments.
When trained on limited datasets, the deep neural network is lack of
generalization capability, as the trained deep neural network on data within a
certain distribution (e.g. the data captured by a certain device vendor or
patient population) may not be able to generalize to the data with another
distribution.

In this paper, we introduce a simple but effective approach to improve the
generalization capability of deep neural networks in the field of medical
imaging classification. Motivated by the observation that the domain
variability of the medical images is to some extent compact, we propose to
learn a representative feature space through variational encoding with a novel
linear-dependency regularization term to capture the shareable information
among medical data collected from different domains. As a result, the trained
neural network is expected to equip with better generalization capability to
the "unseen" medical data. Experimental results on two challenging medical
imaging classification tasks indicate that our method can achieve better
cross-domain generalization capability compared with state-of-the-art
baselines.
</p>
<a href="http://arxiv.org/abs/2009.12829">arXiv:2009.12829</a> [<a href="http://arxiv.org/pdf/2009.12829">pdf</a>]

<h2>Learning event-driven switched linear systems. (arXiv:2009.12831v1 [eess.SY])</h2>
<h3>Atreyee Kundu, Pavithra Prabhakar</h3>
<p>We propose an automata theoretic learning algorithm for the identification of
black-box switched linear systems whose switching logics are event-driven. A
switched system is expressed by a deterministic finite automaton (FA) whose
node labels are the subsystem matrices. With information about the dimensions
of the matrices and the set of events, and with access to two oracles, that can
simulate the system on a given input, and provide counter-examples when given
an incorrect hypothesis automaton, we provide an algorithm that outputs the
unknown FA. Our algorithm first uses the oracle to obtain the node labels of
the system run on a given input sequence of events, and then extends Angluin's
\(L^*\)-algorithm to determine the FA that accepts the language of the given
FA. We demonstrate the performance of our learning algorithm on a set of
benchmark examples.
</p>
<a href="http://arxiv.org/abs/2009.12831">arXiv:2009.12831</a> [<a href="http://arxiv.org/pdf/2009.12831">pdf</a>]

<h2>QLens: Visual Analytics of Multi-step Problem-solving Behaviors for Improving Question Design. (arXiv:2009.12833v1 [cs.HC])</h2>
<h3>Meng Xia, Reshika Palaniyappan Velumani, Yong Wang, Huamin Qu, Xiaojuan Ma</h3>
<p>With the rapid development of online education in recent years, there has
been an increasing number of learning platforms that provide students with
multi-step questions to cultivate their problem-solving skills. To guarantee
the high quality of such learning materials, question designers need to inspect
how students' problem-solving processes unfold step by step to infer whether
students' problem-solving logic matches their design intent. They also need to
compare the behaviors of different groups (e.g., students from different
grades) to distribute questions to students with the right level of knowledge.
The availability of fine-grained interaction data, such as mouse movement
trajectories from the online platforms, provides the opportunity to analyze
problem-solving behaviors. However, it is still challenging to interpret,
summarize, and compare the high dimensional problem-solving sequence data. In
this paper, we present a visual analytics system, QLens, to help question
designers inspect detailed problem-solving trajectories, compare different
student groups, distill insights for design improvements. In particular, QLens
models problem-solving behavior as a hybrid state transition graph and
visualizes it through a novel glyph-embedded Sankey diagram, which reflects
students' problem-solving logic, engagement, and encountered difficulties. We
conduct three case studies and three expert interviews to demonstrate the
usefulness of QLens on real-world datasets that consist of thousands of
problem-solving traces.
</p>
<a href="http://arxiv.org/abs/2009.12833">arXiv:2009.12833</a> [<a href="http://arxiv.org/pdf/2009.12833">pdf</a>]

<h2>Accurate and confident prediction of electron beam longitudinal properties using spectral virtual diagnostics. (arXiv:2009.12835v1 [physics.acc-ph])</h2>
<h3>A. Hanuka, C. Emma, T. Maxwell, A. Fisher, B. Jacobson, M. J. Hogan, Z. Huang</h3>
<p>Longitudinal phase space (LPS) provides a critical information about electron
beam dynamics for various scientific applications. For example, it can give
insight into the high-brightness X-ray radiation from a free electron laser.
Existing diagnostics are invasive, and often times cannot operate at the
required resolution. In this work we present a machine learning-based Virtual
Diagnostic (VD) tool to accurately predict the LPS for every shot using
spectral information collected non-destructively from the radiation of
relativistic electron beam. We demonstrate the tool's accuracy for three
different case studies with experimental or simulated data. For each case, we
introduce a method to increase the confidence in the VD tool. We anticipate
that spectral VD would improve the setup and understanding of experimental
configurations at DOE's user facilities as well as data sorting and analysis.
The spectral VD can provide confident knowledge of the longitudinal bunch
properties at the next generation of high-repetition rate linear accelerators
while reducing the load on data storage, readout and streaming requirements.
</p>
<a href="http://arxiv.org/abs/2009.12835">arXiv:2009.12835</a> [<a href="http://arxiv.org/pdf/2009.12835">pdf</a>]

<h2>Normalization Techniques in Training DNNs: Methodology, Analysis and Application. (arXiv:2009.12836v1 [cs.LG])</h2>
<h3>Lei Huang, Jie Qin, Yi Zhou, Fan Zhu, Li Liu, Ling Shao</h3>
<p>Normalization techniques are essential for accelerating the training and
improving the generalization of deep neural networks (DNNs), and have
successfully been used in various applications. This paper reviews and comments
on the past, present and future of normalization methods in the context of DNN
training. We provide a unified picture of the main motivation behind different
approaches from the perspective of optimization, and present a taxonomy for
understanding the similarities and differences between them. Specifically, we
decompose the pipeline of the most representative normalizing activation
methods into three components: the normalization area partitioning,
normalization operation and normalization representation recovery. In doing so,
we provide insight for designing new normalization technique. Finally, we
discuss the current progress in understanding normalization methods, and
provide a comprehensive review of the applications of normalization for
particular tasks, in which it can effectively solve the key issues.
</p>
<a href="http://arxiv.org/abs/2009.12836">arXiv:2009.12836</a> [<a href="http://arxiv.org/pdf/2009.12836">pdf</a>]

<h2>Self-Organizing Software Models for the Internet of Things. (arXiv:2009.12844v1 [cs.SE])</h2>
<h3>Damian Arellanes</h3>
<p>The Internet of Things (IoT) envisions the integration of physical objects
into software systems for automating crucial aspects of our lives, such as
healthcare, security, agriculture, and city management. Although the vision is
promising, with the rapid advancement of hardware and communication
technologies, IoT systems are becoming increasingly dynamic, large, and complex
to the extent that manual management becomes infeasible. Thus, it is of
paramount importance to provide software engineering foundations for
constructing autonomic IoT systems. In this paper, I introduce a novel paradigm
referred to as self-organizing software models in which IoT software systems
are not explicitly programmed, but emerge in a decentralized manner during
system operation, with minimal or without human intervention. I particularly
present an overview of those models by including their definition, motivation,
research challenges, and potential directions.
</p>
<a href="http://arxiv.org/abs/2009.12844">arXiv:2009.12844</a> [<a href="http://arxiv.org/pdf/2009.12844">pdf</a>]

<h2>Machine Learning for Searching the Dark Energy Survey for Trans-Neptunian Objects. (arXiv:2009.12856v1 [astro-ph.EP])</h2>
<h3>B. Henghes, O. Lahav, D. W. Gerdes, E. Lin, R. Morgan, T. M. C. Abbott, M. Aguena, S. Allam, J. Annis, S. Avila, E. Bertin, D. Brooks, D. L. Burke, A. CarneroRosell, M. CarrascoKind, J. Carretero, C. Conselice, M. Costanzi, L. N. da Costa, J. DeVicente, S. Desai, H. T. Diehl, P. Doel, S. Everett, I. Ferrero, J. Frieman, J. Garc&#xed;a-Bellido, E. Gaztanaga, D. Gruen, R. A. Gruendl, J. Gschwend, G. Gutierrez, W. G. Hartley, S. R. Hinton, K. Honscheid, B. Hoyle, D. J. James, K. Kuehn, N. Kuropatkin, J. L. Marshall, P. Melchior, F. Menanteau, R. Miquel, R. L. C. Ogando, A. Palmese, F. Paz-Chinch&#xf3;n, A. A. Plazas, A. K. Romer, C. S&#xe1;nchez, E. Sanchez, V. Scarpine, M. Schubnell, S. Serrano, M. Smith, M. Soares-Santos, E. Suchyta, G. Tarle, C. To, R. D. Wilkinson (DES collaboration)</h3>
<p>In this paper we investigate how implementing machine learning could improve
the efficiency of the search for Trans-Neptunian Objects (TNOs) within Dark
Energy Survey (DES) data when used alongside orbit fitting. The discovery of
multiple TNOs that appear to show a similarity in their orbital parameters has
led to the suggestion that one or more undetected planets, an as yet
undiscovered "Planet 9", may be present in the outer Solar System. DES is well
placed to detect such a planet and has already been used to discover many other
TNOs. Here, we perform tests on eight different supervised machine learning
algorithms, using a dataset consisting of simulated TNOs buried within real DES
noise data. We found that the best performing classifier was the Random Forest
which, when optimised, performed well at detecting the rare objects. We achieve
an area under the receiver operating characteristic (ROC) curve, (AUC) $= 0.996
\pm 0.001$. After optimizing the decision threshold of the Random Forest, we
achieve a recall of 0.96 while maintaining a precision of 0.80. Finally, by
using the optimized classifier to pre-select objects, we are able to run the
orbit-fitting stage of our detection pipeline five times faster.
</p>
<a href="http://arxiv.org/abs/2009.12856">arXiv:2009.12856</a> [<a href="http://arxiv.org/pdf/2009.12856">pdf</a>]

<h2>RRA-U-Net: a Residual Encoder to Attention Decoder by Residual Connections Framework for Spine Segmentation under Noisy Labels. (arXiv:2009.12873v1 [eess.IV])</h2>
<h3>Ziyang Wang, Zhengdong Zhang, Irina Voiculescu</h3>
<p>Segmentation algorithms of medical image volumes are widely studied for many
clinical and research purposes. We propose a novel and efficient framework for
medical image segmentation. The framework functions under a deep learning
paradigm, incorporating four novel contributions. Firstly, a residual
interconnection is explored in different scale encoders. Secondly, four copy
and crop connections are replaced to residual-block-based concatenation to
alleviate the disparity between encoders and decoders, respectively. Thirdly,
convolutional attention modules for feature refinement are studied on all scale
decoders. Finally, an adaptive clean noisy label learning strategy(ACNLL) based
on the training process from underfitting to overfitting is studied.
Experimental results are illustrated on a publicly available benchmark database
of spine CTs. Our segmentation framework achieves competitive performance with
other state-of-the-art methods over a variety of different evaluation measures.
</p>
<a href="http://arxiv.org/abs/2009.12873">arXiv:2009.12873</a> [<a href="http://arxiv.org/pdf/2009.12873">pdf</a>]

<h2>Learning Self-Expression Metrics for Scalable and Inductive Subspace Clustering. (arXiv:2009.12875v1 [cs.LG])</h2>
<h3>Julian Busch, Evgeniy Faerman, Matthias Schubert, Thomas Seidl</h3>
<p>Subspace clustering has established itself as a state-of-the-art approach to
clustering high-dimensional data. In particular, methods relying on the
self-expressiveness property have recently proved especially successful.
However, they suffer from two major shortcomings: First, a quadratic-size
coefficient matrix is learned directly, preventing these methods from scaling
beyond small datasets. Secondly, the trained models are transductive and thus
cannot be used to cluster out-of-sample data unseen during training. Instead of
learning self-expression coefficients directly, we propose a novel metric
learning approach to learn instead a subspace affinity function using a siamese
neural network architecture. Consequently, our model benefits from a constant
number of parameters and a constant-size memory footprint, allowing it to scale
to considerably larger datasets. In addition, we can formally show that out
model is still able to exactly recover subspace clusters given an independence
assumption. The siamese architecture in combination with a novel geometric
classifier further makes our model inductive, allowing it to cluster
out-of-sample data. Additionally, non-linear clusters can be detected by simply
adding an auto-encoder module to the architecture. The whole model can then be
trained end-to-end in a self-supervised manner. This work in progress reports
promising preliminary results on the MNIST dataset. In the spirit of
reproducible research, me make all code publicly available. In future work we
plan to investigate several extensions of our model and to expand experimental
evaluation.
</p>
<a href="http://arxiv.org/abs/2009.12875">arXiv:2009.12875</a> [<a href="http://arxiv.org/pdf/2009.12875">pdf</a>]

<h2>Virtual Experience to Real World Application: Sidewalk Obstacle Avoidance Using Reinforcement Learning for Visually Impaired. (arXiv:2009.12877v1 [cs.CV])</h2>
<h3>Faruk Ahmed, Md Sultan Mahmud, Kazi Ashraf Moinuddin, Mohammed Istiaque Hyder, Mohammed Yeasin</h3>
<p>Finding a path free from obstacles that poses minimal risk is critical for
safe navigation. People who are sighted and people who are visually impaired
require navigation safety while walking on a sidewalk. In this research we
developed an assistive navigation on a sidewalk by integrating sensory inputs
using reinforcement learning. We trained a Sidewalk Obstacle Avoidance Agent
(SOAA) through reinforcement learning in a simulated robotic environment. A
Sidewalk Obstacle Conversational Agent (SOCA) is built by training a natural
language conversation agent with real conversation data. The SOAA along with
SOCA was integrated in a prototype device called augmented guide (AG).
Empirical analysis showed that this prototype improved the obstacle avoidance
experience about 5% from a base case of 81.29%
</p>
<a href="http://arxiv.org/abs/2009.12877">arXiv:2009.12877</a> [<a href="http://arxiv.org/pdf/2009.12877">pdf</a>]

<h2>ESTAN: Enhanced Small Tumor-Aware Network for Breast Ultrasound Image Segmentation. (arXiv:2009.12894v1 [eess.IV])</h2>
<h3>Bryar Shareef, Alex Vakanski, Min Xian, Phoebe E. Freer</h3>
<p>Breast tumor segmentation is a critical task in computer-aided diagnosis
(CAD) systems for breast cancer detection because accurate tumor size, shape
and location are important for further tumor quantification and classification.
However, segmenting small tumors in ultrasound images is challenging, due to
the speckle noise, varying tumor shapes and sizes among patients, and the
existence of tumor-like image regions. Recently, deep learning-based approaches
have achieved great success for biomedical image analysis, but current
state-of-the-art approaches achieve poor performance for segmenting small
breast tumors. In this paper, we propose a novel deep neural network
architecture, namely Enhanced Small Tumor-Aware Network (ESTAN), to accurately
and robustly segment breast tumors. ESTAN introduces two encoders to extract
and fuse image context information at different scales and utilizes
row-column-wise kernels in the encoder to adapt to breast anatomy. We validate
the proposed approach and compare it to nine state-of-the-art approaches on
three public breast ultrasound datasets using seven quantitative metrics. The
results demonstrate that the proposed approach achieves the best overall
performance and outperforms all other approaches on small tumor segmentation.
</p>
<a href="http://arxiv.org/abs/2009.12894">arXiv:2009.12894</a> [<a href="http://arxiv.org/pdf/2009.12894">pdf</a>]

<h2>Benchmarking deep inverse models over time, and the neural-adjoint method. (arXiv:2009.12919v1 [cs.LG])</h2>
<h3>Simiao Ren, Willie Padilla, Jordan Malof</h3>
<p>We consider the task of solving generic inverse problems, where one wishes to
determine the hidden parameters of a natural system that will give rise to a
particular set of measurements. Recently many new approaches based upon deep
learning have arisen generating impressive results. We conceptualize these
models as different schemes for efficiently, but randomly, exploring the space
of possible inverse solutions. As a result, the accuracy of each approach
should be evaluated as a function of time rather than a single estimated
solution, as is often done now. Using this metric, we compare several
state-of-the-art inverse modeling approaches on four benchmark tasks: two
existing tasks, one simple task for visualization and one new task from
metamaterial design. Finally, inspired by our conception of the inverse
problem, we explore a solution that uses a deep learning model to approximate
the forward model, and then uses backpropagation to search for good inverse
solutions. This approach, termed the neural-adjoint, achieves the best
performance in many scenarios.
</p>
<a href="http://arxiv.org/abs/2009.12919">arXiv:2009.12919</a> [<a href="http://arxiv.org/pdf/2009.12919">pdf</a>]

<h2>Privacy-Preserving Dynamic Personalized Pricing with Demand Learning. (arXiv:2009.12920v1 [cs.CR])</h2>
<h3>Xi Chen, David Simchi-Levi, Yining Wang</h3>
<p>The prevalence of e-commerce has made detailed customers' personal
information readily accessible to retailers, and this information has been
widely used in pricing decisions. When involving personalized information, how
to protect the privacy of such information becomes a critical issue in
practice. In this paper, we consider a dynamic pricing problem over $T$ time
periods with an \emph{unknown} demand function of posted price and personalized
information. At each time $t$, the retailer observes an arriving customer's
personal information and offers a price. The customer then makes the purchase
decision, which will be utilized by the retailer to learn the underlying demand
function. There is potentially a serious privacy concern during this process: a
third party agent might infer the personalized information and purchase
decisions from price changes from the pricing system. Using the fundamental
framework of differential privacy from computer science, we develop a
privacy-preserving dynamic pricing policy, which tries to maximize the retailer
revenue while avoiding information leakage of individual customer's information
and purchasing decisions. To this end, we first introduce a notion of
\emph{anticipating} $(\varepsilon, \delta)$-differential privacy that is
tailored to dynamic pricing problem. Our policy achieves both the privacy
guarantee and the performance guarantee in terms of regret. Roughly speaking,
for $d$-dimensional personalized information, our algorithm achieves the
expected regret at the order of $\tilde{O}(\varepsilon^{-1} \sqrt{d^3 T})$,
when the customers' information is adversarially chosen. For stochastic
personalized information, the regret bound can be further improved to
$\tilde{O}(\sqrt{d^2T} + \varepsilon^{-2} d^2)$
</p>
<a href="http://arxiv.org/abs/2009.12920">arXiv:2009.12920</a> [<a href="http://arxiv.org/pdf/2009.12920">pdf</a>]

<h2>Association Learning Between the COVID-19 Infections and Global Demographic Characteristics Using the Class Rule Mining and Pattern Matching. (arXiv:2009.12923v1 [cs.LG])</h2>
<h3>Wasiq Khan, Abir Hussain, Sohail Ahmed Khan, Mohammed Al-Jumailey, Raheel Nawaz</h3>
<p>Over 26 million cases have been confirmed worldwide (by 20 August 2020) since
the Coronavirus disease (COIVD_19) outbreak in December 2019. Research studies
have been addressing diverse aspects in relation to COVID_19 including
potential symptoms, predictive tools and specifically, correlations with
various demographic attributes. However, very limited work is performed towards
the modelling of complex associations between the combined demographic
attributes and varying nature of the COVID_19 infections across the globe.
Investigating the underlying disease associations with the combined
demographical characteristics might help in comprehensive analysis this
devastating disease as well as contribute to its effective management. In this
study, we present an intelligent model to investigate the multi-dimensional
associations between the potentially relevant demographic attributes and the
COVID_19 severity levels across the globe. We gather multiple demographic
attributes and COVID_19 infection data (by 20 August 2020) from various
reliable sources, which is then fed-into pattern matching algorithms that
include self-organizing maps, class association rules and statistical
approaches, to identify the significant associations within the processed
dataset. Statistical results and the experts report indicate strong
associations between the COVID_19 severity levels and measures of certain
demographic attributes such as female smokers, when combined together with
other attributes. These results strongly suggest that the mechanism underlying
COVID_19 infection severity is associated to distribution of the certain
demographic attributes within different regions of the world. The outcomes will
aid the understanding of the dynamics of disease spread and its progression
that might in turn help the policy makers and the society, in better
understanding and management of the disease.
</p>
<a href="http://arxiv.org/abs/2009.12923">arXiv:2009.12923</a> [<a href="http://arxiv.org/pdf/2009.12923">pdf</a>]

<h2>Measure Utility, Gain Trust: Practical Advice for XAI Researcher. (arXiv:2009.12924v1 [cs.HC])</h2>
<h3>Brittany Davis, Maria Glenski, William Sealy, Dustin Arendt</h3>
<p>Research into the explanation of machine learning models, i.e., explainable
AI (XAI), has seen a commensurate exponential growth alongside deep artificial
neural networks throughout the past decade. For historical reasons, explanation
and trust have been intertwined. However, the focus on trust is too narrow, and
has led the research community astray from tried and true empirical methods
that produced more defensible scientific knowledge about people and
explanations. To address this, we contribute a practical path forward for
researchers in the XAI field. We recommend researchers focus on the utility of
machine learning explanations instead of trust. We outline five broad use cases
where explanations are useful and, for each, we describe pseudo-experiments
that rely on objective empirical measurements and falsifiable hypotheses. We
believe that this experimental rigor is necessary to contribute to scientific
knowledge in the field of XAI.
</p>
<a href="http://arxiv.org/abs/2009.12924">arXiv:2009.12924</a> [<a href="http://arxiv.org/pdf/2009.12924">pdf</a>]

<h2>Learning to Improve Image Compression without Changing the Standard Decoder. (arXiv:2009.12927v1 [eess.IV])</h2>
<h3>Yannick Str&#xfc;mpler, Ren Yang, Radu Timofte</h3>
<p>In recent years we have witnessed an increasing interest in applying Deep
Neural Networks (DNNs) to improve the rate-distortion performance in image
compression. However, the existing approaches either train a post-processing
DNN on the decoder side, or propose learning for image compression in an
end-to-end manner. This way, the trained DNNs are required in the decoder,
leading to the incompatibility to the standard image decoders (\eg, JPEG) in
personal computers and mobiles. Therefore, we propose learning to improve the
encoding performance with the standard decoder. In this paper, We work on JPEG
as an example. Specifically, a frequency-domain pre-editing method is proposed
to optimize the distribution of DCT coefficients, aiming at facilitating the
JPEG compression. Moreover, we propose learning the JPEG quantization table
jointly with the pre-editing network. Most importantly, we do not modify the
JPEG decoder and therefore our approach is applicable when viewing images with
the widely used standard JPEG decoder. The experiments validate that our
approach successfully improves the rate-distortion performance of JPEG in terms
of various quality metrics, such as PSNR, MS-SSIM and LPIPS. Visually, this
translates to better overall color retention especially when strong compression
is applied.
</p>
<a href="http://arxiv.org/abs/2009.12927">arXiv:2009.12927</a> [<a href="http://arxiv.org/pdf/2009.12927">pdf</a>]

<h2>A Survey on Deep Learning Methods for Semantic Image Segmentation in Real-Time. (arXiv:2009.12942v1 [cs.CV])</h2>
<h3>Georgios Takos</h3>
<p>Semantic image segmentation is one of fastest growing areas in computer
vision with a variety of applications. In many areas, such as robotics and
autonomous vehicles, semantic image segmentation is crucial, since it provides
the necessary context for actions to be taken based on a scene understanding at
the pixel level. Moreover, the success of medical diagnosis and treatment
relies on the extremely accurate understanding of the data under consideration
and semantic image segmentation is one of the important tools in many cases.
Recent developments in deep learning have provided a host of tools to tackle
this problem efficiently and with increased accuracy. This work provides a
comprehensive analysis of state-of-the-art deep learning architectures in image
segmentation and, more importantly, an extensive list of techniques to achieve
fast inference and computational efficiency. The origins of these techniques as
well as their strengths and trade-offs are discussed with an in-depth analysis
of their impact in the area. The best-performing architectures are summarized
with a list of methods used to achieve these state-of-the-art results.
</p>
<a href="http://arxiv.org/abs/2009.12942">arXiv:2009.12942</a> [<a href="http://arxiv.org/pdf/2009.12942">pdf</a>]

<h2>Learning from eXtreme Bandit Feedback. (arXiv:2009.12947v1 [stat.ML])</h2>
<h3>Romain Lopez, Inderjit Dhillon, Michael I. Jordan</h3>
<p>We study the problem of batch learning from bandit feedback in the setting of
extremely large action spaces. Learning from extreme bandit feedback is
ubiquitous in recommendation systems, in which billions of decisions are made
over sets consisting of millions of choices in a single day, yielding massive
observational data. In these large-scale real-world applications, supervised
learning frameworks such as eXtreme Multi-label Classification (XMC) are widely
used despite the fact that they incur significant biases due to the mismatch
between bandit feedback and supervised labels. Such biases can be mitigated by
importance sampling techniques, but these techniques suffer from impractical
variance when dealing with a large number of actions. In this paper, we
introduce a selective importance sampling estimator (sIS) that operates in a
significantly more favorable bias-variance regime. The sIS estimator is
obtained by performing importance sampling on the conditional expectation of
the reward with respect to a small subset of actions for each instance (a form
of Rao-Blackwellization). We employ this estimator in a novel algorithmic
procedure---named Policy Optimization for eXtreme Models (POXM)---for learning
from bandit feedback on XMC tasks. In POXM, the selected actions for the sIS
estimator are the top-p actions of the logging policy, where p is adjusted from
the data and is significantly smaller than the size of the action space. We use
a supervised-to-bandit conversion on three XMC datasets to benchmark our POXM
method against three competing methods: BanditNet, a previously applied partial
matching pruning strategy, and a supervised learning baseline. Whereas
BanditNet sometimes improves marginally over the logging policy, our
experiments show that POXM systematically and significantly improves over all
baselines.
</p>
<a href="http://arxiv.org/abs/2009.12947">arXiv:2009.12947</a> [<a href="http://arxiv.org/pdf/2009.12947">pdf</a>]

<h2>Human-Object Interaction Detection:A Quick Survey and Examination of Methods. (arXiv:2009.12950v1 [cs.CV])</h2>
<h3>Trevor Bergstrom, Humphrey Shi</h3>
<p>Human-object interaction detection is a relatively new task in the world of
computer vision and visual semantic information extraction. With the goal of
machines identifying interactions that humans perform on objects, there are
many real-world use cases for the research in this field. To our knowledge,
this is the first general survey of the state-of-the-art and milestone works in
this field. We provide a basic survey of the developments in the field of
human-object interaction detection. Many works in this field use multi-stream
convolutional neural network architectures, which combine features from
multiple sources in the input image. Most commonly these are the humans and
objects in question, as well as the spatial quality of the two. As far as we
are aware, there have not been in-depth studies performed that look into the
performance of each component individually. In order to provide insight to
future researchers, we perform an individualized study that examines the
performance of each component of a multi-stream convolutional neural network
architecture for human-object interaction detection. Specifically, we examine
the HORCNN architecture as it is a foundational work in the field. In addition,
we provide an in-depth look at the HICO-DET dataset, a popular benchmark in the
field of human-object interaction detection. Code and papers can be found at
https://github.com/SHI-Labs/Human-Object-Interaction-Detection.
</p>
<a href="http://arxiv.org/abs/2009.12950">arXiv:2009.12950</a> [<a href="http://arxiv.org/pdf/2009.12950">pdf</a>]

<h2>Unsupervised Pre-training for Biomedical Question Answering. (arXiv:2009.12952v1 [cs.CL])</h2>
<h3>Vaishnavi Kommaraju, Karthick Gunasekaran, Kun Li, Trapit Bansal, Andrew McCallum, Ivana Williams, Ana-Maria Istrate</h3>
<p>We explore the suitability of unsupervised representation learning methods on
biomedical text -- BioBERT, SciBERT, and BioSentVec -- for biomedical question
answering. To further improve unsupervised representations for biomedical QA,
we introduce a new pre-training task from unlabeled data designed to reason
about biomedical entities in the context. Our pre-training method consists of
corrupting a given context by randomly replacing some mention of a biomedical
entity with a random entity mention and then querying the model with the
correct entity mention in order to locate the corrupted part of the context.
This de-noising task enables the model to learn good representations from
abundant, unlabeled biomedical text that helps QA tasks and minimizes the
train-test mismatch between the pre-training task and the downstream QA tasks
by requiring the model to predict spans. Our experiments show that pre-training
BioBERT on the proposed pre-training task significantly boosts performance and
outperforms the previous best model from the 7th BioASQ Task 7b-Phase B
challenge.
</p>
<a href="http://arxiv.org/abs/2009.12952">arXiv:2009.12952</a> [<a href="http://arxiv.org/pdf/2009.12952">pdf</a>]

<h2>Analysis of label noise in graph-based semi-supervised learning. (arXiv:2009.12966v1 [cs.LG])</h2>
<h3>Bruno Klaus de Aquino Afonso, Lilian Berton</h3>
<p>In machine learning, one must acquire labels to help supervise a model that
will be able to generalize to unseen data. However, the labeling process can be
tedious, long, costly, and error-prone. It is often the case that most of our
data is unlabeled. Semi-supervised learning (SSL) alleviates that by making
strong assumptions about the relation between the labels and the input data
distribution. This paradigm has been successful in practice, but most SSL
algorithms end up fully trusting the few available labels. In real life, both
humans and automated systems are prone to mistakes; it is essential that our
algorithms are able to work with labels that are both few and also unreliable.
Our work aims to perform an extensive empirical evaluation of existing
graph-based semi-supervised algorithms, like Gaussian Fields and Harmonic
Functions, Local and Global Consistency, Laplacian Eigenmaps, Graph
Transduction Through Alternating Minimization. To do that, we compare the
accuracy of classifiers while varying the amount of labeled data and label
noise for many different samples. Our results show that, if the dataset is
consistent with SSL assumptions, we are able to detect the noisiest instances,
although this gets harder when the number of available labels decreases. Also,
the Laplacian Eigenmaps algorithm performed better than label propagation when
the data came from high-dimensional clusters.
</p>
<a href="http://arxiv.org/abs/2009.12966">arXiv:2009.12966</a> [<a href="http://arxiv.org/pdf/2009.12966">pdf</a>]

<h2>Recognition and Synthesis of Object Transport Motion. (arXiv:2009.12967v1 [cs.CV])</h2>
<h3>Connor Daly</h3>
<p>Deep learning typically requires vast numbers of training examples in order
to be used successfully. Conversely, motion capture data is often expensive to
generate, requiring specialist equipment, along with actors to generate the
prescribed motions, meaning that motion capture datasets tend to be relatively
small. Motion capture data does however provide a rich source of information
that is becoming increasingly useful in a wide variety of applications, from
gesture recognition in human-robot interaction, to data driven animation.

This project illustrates how deep convolutional networks can be used,
alongside specialized data augmentation techniques, on a small motion capture
dataset to learn detailed information from sequences of a specific type of
motion (object transport). The project shows how these same augmentation
techniques can be scaled up for use in the more complex task of motion
synthesis.

By exploring recent developments in the concept of Generative Adversarial
Models (GANs), specifically the Wasserstein GAN, this project outlines a model
that is able to successfully generate lifelike object transportation motions,
with the generated samples displaying varying styles and transport strategies.
</p>
<a href="http://arxiv.org/abs/2009.12967">arXiv:2009.12967</a> [<a href="http://arxiv.org/pdf/2009.12967">pdf</a>]

<h2>VATLD: A Visual Analytics System to Assess, Understand and Improve Traffic Light Detection. (arXiv:2009.12975v1 [cs.HC])</h2>
<h3>Liang Gou, Lincan Zou, Nanxiang Li, Michael Hofmann, Arvind Kumar Shekar, Axel Wendt, Liu Ren</h3>
<p>Traffic light detection is crucial for environment perception and
decision-making in autonomous driving. State-of-the-art detectors are built
upon deep Convolutional Neural Networks (CNNs) and have exhibited promising
performance. However, one looming concern with CNN based detectors is how to
thoroughly evaluate the performance of accuracy and robustness before they can
be deployed to autonomous vehicles. In this work, we propose a visual analytics
system, VATLD, equipped with a disentangled representation learning and
semantic adversarial learning, to assess, understand, and improve the accuracy
and robustness of traffic light detectors in autonomous driving applications.
The disentangled representation learning extracts data semantics to augment
human cognition with human-friendly visual summarization, and the semantic
adversarial learning efficiently exposes interpretable robustness risks and
enables minimal human interaction for actionable insights. We also demonstrate
the effectiveness of various performance improvement strategies derived from
actionable insights with our visual analytics system, VATLD, and illustrate
some practical implications for safety-critical applications in autonomous
driving.
</p>
<a href="http://arxiv.org/abs/2009.12975">arXiv:2009.12975</a> [<a href="http://arxiv.org/pdf/2009.12975">pdf</a>]

<h2>Parametric UMAP: learning embeddings with deep neural networks for representation and semi-supervised learning. (arXiv:2009.12981v1 [cs.LG])</h2>
<h3>Tim Sainburg, Leland McInnes, Timothy Q Gentner</h3>
<p>We propose Parametric UMAP, a parametric variation of the UMAP (Uniform
Manifold Approximation and Projection) algorithm. UMAP is a non-parametric
graph-based dimensionality reduction algorithm using applied Riemannian
geometry and algebraic topology to find low-dimensional embeddings of
structured data. The UMAP algorithm consists of two steps: (1) Compute a
graphical representation of a dataset (fuzzy simplicial complex), and (2)
Through stochastic gradient descent, optimize a low-dimensional embedding of
the graph. Here, we replace the second step of UMAP with a deep neural network
that learns a parametric relationship between data and embedding. We
demonstrate that our method performs similarly to its non-parametric
counterpart while conferring the benefit of a learned parametric mapping (e.g.
fast online embeddings for new data). We then show that UMAP loss can be
extended to arbitrary deep learning applications, for example constraining the
latent distribution of autoencoders, and improving classifier accuracy for
semi-supervised learning by capturing structure in unlabeled data. Our code is
available at https://github.com/timsainb/ParametricUMAP_paper.
</p>
<a href="http://arxiv.org/abs/2009.12981">arXiv:2009.12981</a> [<a href="http://arxiv.org/pdf/2009.12981">pdf</a>]

<h2>Long-Tailed Classification by Keeping the Good and Removing the Bad Momentum Causal Effect. (arXiv:2009.12991v1 [cs.CV])</h2>
<h3>Kaihua Tang, Jianqiang Huang, Hanwang Zhang</h3>
<p>As the class size grows, maintaining a balanced dataset across many classes
is challenging because the data are long-tailed in nature; it is even
impossible when the sample-of-interest co-exists with each other in one
collectable unit, e.g., multiple visual instances in one image. Therefore,
long-tailed classification is the key to deep learning at scale. However,
existing methods are mainly based on re-weighting/re-sampling heuristics that
lack a fundamental theory. In this paper, we establish a causal inference
framework, which not only unravels the whys of previous methods, but also
derives a new principled solution. Specifically, our theory shows that the SGD
momentum is essentially a confounder in long-tailed classification. On one
hand, it has a harmful causal effect that misleads the tail prediction biased
towards the head. On the other hand, its induced mediation also benefits the
representation learning and head prediction. Our framework elegantly
disentangles the paradoxical effects of the momentum, by pursuing the direct
causal effect caused by an input sample. In particular, we use causal
intervention in training, and counterfactual reasoning in inference, to remove
the "bad" while keep the "good". We achieve new state-of-the-arts on three
long-tailed visual recognition benchmarks: Long-tailed CIFAR-10/-100,
ImageNet-LT for image classification and LVIS for instance segmentation.
</p>
<a href="http://arxiv.org/abs/2009.12991">arXiv:2009.12991</a> [<a href="http://arxiv.org/pdf/2009.12991">pdf</a>]

<h2>Fancy Man Lauches Zippo at WNUT 2020 Shared Task-1: A Bert Case Model for Wet Lab Entity Extraction. (arXiv:2009.12997v1 [cs.CL])</h2>
<h3>Haoding Meng, Qingcheng Zeng, Xiaoyang Fang, Zhexin Liang</h3>
<p>Automatic or semi-automatic conversion of protocols specifying steps in
performing a lab procedure into machine-readable format benefits biological
research a lot. These noisy, dense, and domain-specific lab protocols
processing draws more and more interests with the development of deep learning.
This paper presents our teamwork on WNUT 2020 shared task-1: wet lab entity
extract, that we conducted studies in several models, including a BiLSTM CRF
model and a Bert case model which can be used to complete wet lab entity
extraction. And we mainly discussed the performance differences of \textbf{Bert
case} under different situations such as \emph{transformers} versions, case
sensitivity that may don't get enough attention before.
</p>
<a href="http://arxiv.org/abs/2009.12997">arXiv:2009.12997</a> [<a href="http://arxiv.org/pdf/2009.12997">pdf</a>]

<h2>Loosely Coupled Federated Learning Over Generative Models. (arXiv:2009.12999v1 [cs.LG])</h2>
<h3>Shaoming Song, Yunfeng Shao, Jian Li</h3>
<p>Federated learning (FL) was proposed to achieve collaborative machine
learning among various clients without uploading private data. However, due to
model aggregation strategies, existing frameworks require strict model
homogeneity, limiting the application in more complicated scenarios. Besides,
the communication cost of FL's model and gradient transmission is extremely
high. This paper proposes Loosely Coupled Federated Learning (LC-FL), a
framework using generative models as transmission media to achieve low
communication cost and heterogeneous federated learning. LC-FL can be applied
on scenarios where clients possess different kinds of machine learning models.
Experiments on real-world datasets covering different multiparty scenarios
demonstrate the effectiveness of our proposal.
</p>
<a href="http://arxiv.org/abs/2009.12999">arXiv:2009.12999</a> [<a href="http://arxiv.org/pdf/2009.12999">pdf</a>]

<h2>Interventional Few-Shot Learning. (arXiv:2009.13000v1 [cs.LG])</h2>
<h3>Zhongqi Yue, Hanwang Zhang, Qianru Sun, Xian-Sheng Hua</h3>
<p>We uncover an ever-overlooked deficiency in the prevailing Few-Shot Learning
(FSL) methods: the pre-trained knowledge is indeed a confounder that limits the
performance. This finding is rooted from our causal assumption: a Structural
Causal Model (SCM) for the causalities among the pre-trained knowledge, sample
features, and labels. Thanks to it, we propose a novel FSL paradigm:
Interventional Few-Shot Learning (IFSL). Specifically, we develop three
effective IFSL algorithmic implementations based on the backdoor adjustment,
which is essentially a causal intervention towards the SCM of many-shot
learning: the upper-bound of FSL in a causal view. It is worth noting that the
contribution of IFSL is orthogonal to existing fine-tuning and meta-learning
based FSL methods, hence IFSL can improve all of them, achieving a new
1-/5-shot state-of-the-art on \textit{mini}ImageNet, \textit{tiered}ImageNet,
and cross-domain CUB. Code is released at https://github.com/yue-zhongqi/ifsl.
</p>
<a href="http://arxiv.org/abs/2009.13000">arXiv:2009.13000</a> [<a href="http://arxiv.org/pdf/2009.13000">pdf</a>]

<h2>On Efficient Constructions of Checkpoints. (arXiv:2009.13003v1 [cs.LG])</h2>
<h3>Yu Chen, Zhenming Liu, Bin Ren, Xin Jin</h3>
<p>Efficient construction of checkpoints/snapshots is a critical tool for
training and diagnosing deep learning models. In this paper, we propose a lossy
compression scheme for checkpoint constructions (called LC-Checkpoint).
LC-Checkpoint simultaneously maximizes the compression rate and optimizes the
recovery speed, under the assumption that SGD is used to train the model.
LC-Checkpointuses quantization and priority promotion to store the most crucial
information for SGD to recover, and then uses a Huffman coding to leverage the
non-uniform distribution of the gradient scales. Our extensive experiments show
that LC-Checkpoint achieves a compression rate up to $28\times$ and recovery
speedup up to $5.77\times$ over a state-of-the-art algorithm (SCAR).
</p>
<a href="http://arxiv.org/abs/2009.13003">arXiv:2009.13003</a> [<a href="http://arxiv.org/pdf/2009.13003">pdf</a>]

<h2>Visual Steering for One-Shot Deep Neural Network Synthesis. (arXiv:2009.13008v1 [cs.LG])</h2>
<h3>Anjul Tyagi, Cong Xie, Klaus Mueller</h3>
<p>Recent advancements in the area of deep learning have shown the effectiveness
of very large neural networks in several applications. However, as these deep
neural networks continue to grow in size, it becomes more and more difficult to
configure their many parameters to obtain good results. Presently, analysts
must experiment with many different configurations and parameter settings,
which is labor-intensive and time-consuming. On the other hand, the capacity of
fully automated techniques for neural network architecture search is limited
without the domain knowledge of human experts. To deal with the problem, we
formulate the task of neural network architecture optimization as a graph space
exploration, based on the one-shot architecture search technique. In this
approach, a super-graph of all candidate architectures is trained in one-shot
and the optimal neural network is identified as a sub-graph. In this paper, we
present a framework that allows analysts to effectively build the solution
sub-graph space and guide the network search by injecting their domain
knowledge. Starting with the network architecture space composed of basic
neural network components, analysts are empowered to effectively select the
most promising components via our one-shot search scheme. Applying this
technique in an iterative manner allows analysts to converge to the best
performing neural network architecture for a given application. During the
exploration, analysts can use their domain knowledge aided by cues provided
from a scatterplot visualization of the search space to edit different
components and guide the search for faster convergence. We designed our
interface in collaboration with several deep learning researchers and its final
effectiveness is evaluated with a user study and two case studies.
</p>
<a href="http://arxiv.org/abs/2009.13008">arXiv:2009.13008</a> [<a href="http://arxiv.org/pdf/2009.13008">pdf</a>]

<h2>Variational Temporal Deep Generative Model for Radar HRRP Target Recognition. (arXiv:2009.13011v1 [stat.ML])</h2>
<h3>Dandan Guo, Bo Chen (Senior Member, IEEE), Wenchao Chen, Chaojie Wang, Hongwei Liu (Member, IEEE), Mingyuan Zhou</h3>
<p>We develop a recurrent gamma belief network (rGBN) for radar automatic target
recognition (RATR) based on high-resolution range profile (HRRP), which
characterizes the temporal dependence across the range cells of HRRP. The
proposed rGBN adopts a hierarchy of gamma distributions to build its temporal
deep generative model. For scalable training and fast out-of-sample prediction,
we propose the hybrid of a stochastic-gradient Markov chain Monte Carlo (MCMC)
and a recurrent variational inference model to perform posterior inference. To
utilize the label information to extract more discriminative latent
representations, we further propose supervised rGBN to jointly model the HRRP
samples and their corresponding labels. Experimental results on synthetic and
measured HRRP data show that the proposed models are efficient in computation,
have good classification accuracy and generalization ability, and provide
highly interpretable multi-stochastic-layer latent structure.
</p>
<a href="http://arxiv.org/abs/2009.13011">arXiv:2009.13011</a> [<a href="http://arxiv.org/pdf/2009.13011">pdf</a>]

<h2>Federated Learning for Internet of Things: Recent Advances, Taxonomy, and Open Challenges. (arXiv:2009.13012v1 [cs.NI])</h2>
<h3>Latif U. Khan, Walid Saad, Zhu Han, Ekram Hossain, Choong Seon Hong</h3>
<p>The Internet of Things (IoT) will be ripe for the deployment of novel machine
learning algorithms for both network and application management. However, given
the presence of massively distributed and private datasets, it is challenging
to use classical centralized learning algorithms in the IoT. To overcome this
challenge, federated learning can be a promising solution that enables
on-device machine learning without the need to migrate the private end-user
data to a central cloud. In federated learning, only learning model updates are
transferred between end-devices and the aggregation server. Although federated
learning can offer better privacy preservation than centralized machine
learning, it has still privacy concerns. In this paper, first, we present the
recent advances of federated learning towards enabling federated
learning-powered IoT applications. A set of metrics such as sparsification,
robustness, quantization, scalability, security, and privacy, is delineated in
order to rigorously evaluate the recent advances. Second, we devise a taxonomy
for federated learning over IoT networks. Third, we propose two IoT use cases
of dispersed federated learning that can offer better privacy preservation than
federated learning. Finally, we present several open research challenges with
their possible solutions.
</p>
<a href="http://arxiv.org/abs/2009.13012">arXiv:2009.13012</a> [<a href="http://arxiv.org/pdf/2009.13012">pdf</a>]

<h2>Mitigating Gender Bias for Neural Dialogue Generation with Adversarial Learning. (arXiv:2009.13028v1 [cs.CL])</h2>
<h3>Haochen Liu, Wentao Wang, Yiqi Wang, Hui Liu, Zitao Liu, Jiliang Tang</h3>
<p>Dialogue systems play an increasingly important role in various aspects of
our daily life. It is evident from recent research that dialogue systems
trained on human conversation data are biased. In particular, they can produce
responses that reflect people's gender prejudice. Many debiasing methods have
been developed for various natural language processing tasks, such as word
embedding. However, they are not directly applicable to dialogue systems
because they are likely to force dialogue models to generate similar responses
for different genders. This greatly degrades the diversity of the generated
responses and immensely hurts the performance of the dialogue models. In this
paper, we propose a novel adversarial learning framework Debiased-Chat to train
dialogue models free from gender bias while keeping their performance.
Extensive experiments on two real-world conversation datasets show that our
framework significantly reduces gender bias in dialogue models while
maintaining the response quality.
</p>
<a href="http://arxiv.org/abs/2009.13028">arXiv:2009.13028</a> [<a href="http://arxiv.org/pdf/2009.13028">pdf</a>]

<h2>RoGAT: a robust GNN combined revised GAT with adjusted graphs. (arXiv:2009.13038v1 [cs.LG])</h2>
<h3>Xianchen Zhou, Hongxia Wang</h3>
<p>Graph Neural Networks(GNNs) are useful deep learning models to deal with the
non-Euclid data. However, recent works show that GNNs are vulnerable to
adversarial attacks. Small perturbations can lead to poor performance in many
GNNs, such as Graph attention networks(GATs). Therefore, enhancing the
robustness of GNNs is a critical problem.

Robust GAT(RoGAT) is proposed to improve the robustness of GNNs in this
paper, . Note that the original GAT uses the attention mechanism for different
edges but is still sensitive to the perturbation, RoGAT adjusts the edges'
weight to adjust the attention scores progressively. Firstly, RoGAT tunes the
edges weight based on the assumption that the adjacent nodes should have
similar nodes. Secondly, RoGAT further tunes the features to eliminate
feature's noises since even for the clean graph, there exists some unreasonable
data. Then, we trained the adjusted GAT model to defense the adversarial
attacks. Different experiments against targeted and untargeted attacks
demonstrate that RoGAT outperforms significantly than most the state-of-the-art
defense methods. The implementation of RoGAT based on the DeepRobust repository
for adversarial attacks.
</p>
<a href="http://arxiv.org/abs/2009.13038">arXiv:2009.13038</a> [<a href="http://arxiv.org/pdf/2009.13038">pdf</a>]

<h2>Kernel Based Progressive Distillation for Adder Neural Networks. (arXiv:2009.13044v1 [cs.CV])</h2>
<h3>Yixing Xu, Chang Xu, Xinghao Chen, Wei Zhang, Chunjing Xu, Yunhe Wang</h3>
<p>Adder Neural Networks (ANNs) which only contain additions bring us a new way
of developing deep neural networks with low energy consumption. Unfortunately,
there is an accuracy drop when replacing all convolution filters by adder
filters. The main reason here is the optimization difficulty of ANNs using
$\ell_1$-norm, in which the estimation of gradient in back propagation is
inaccurate. In this paper, we present a novel method for further improving the
performance of ANNs without increasing the trainable parameters via a
progressive kernel based knowledge distillation (PKKD) method. A convolutional
neural network (CNN) with the same architecture is simultaneously initialized
and trained as a teacher network, features and weights of ANN and CNN will be
transformed to a new space to eliminate the accuracy drop. The similarity is
conducted in a higher-dimensional space to disentangle the difference of their
distributions using a kernel based method. Finally, the desired ANN is learned
based on the information from both the ground-truth and teacher, progressively.
The effectiveness of the proposed method for learning ANN with higher
performance is then well-verified on several benchmarks. For instance, the
ANN-50 trained using the proposed PKKD method obtains a 76.8\% top-1 accuracy
on ImageNet dataset, which is 0.6\% higher than that of the ResNet-50.
</p>
<a href="http://arxiv.org/abs/2009.13044">arXiv:2009.13044</a> [<a href="http://arxiv.org/pdf/2009.13044">pdf</a>]

<h2>Event-based Action Recognition Using Timestamp Image Encoding Network. (arXiv:2009.13049v1 [cs.CV])</h2>
<h3>Chaoxing Huang</h3>
<p>Event camera is an asynchronous, high frequency vision sensor with low power
consumption, which is suitable for human action recognition task. It is vital
to encode the spatial-temporal information of event data properly and use
standard computer vision tool to learn from the data. In this work, we propose
a timestamp image encoding 2D network, which takes the encoded spatial-temporal
images of the event data as input and output the action label. Experiment
results show that our method can achieve the same level of performance as those
RGB-based benchmarks on real world action recognition, and also achieve the
SOTA result on gesture recognition.
</p>
<a href="http://arxiv.org/abs/2009.13049">arXiv:2009.13049</a> [<a href="http://arxiv.org/pdf/2009.13049">pdf</a>]

<h2>Agent Environment Cycle Games. (arXiv:2009.13051v1 [cs.LG])</h2>
<h3>Justin K Terry, Nathaniel Grammel, Benjamin Black, Ananth Hari, Caroline Horsch, Luis Santos</h3>
<p>Partially Observable Stochastic Games (POSGs), are the most general model of
games used in Multi-Agent Reinforcement Learning (MARL), modeling actions and
observations as happening sequentially for all agents. We introduce Agent
Environment Cycle Games (AEC Games), a model of games based on sequential agent
actions and observations. AEC Games can be thought of as sequential versions of
POSGs, and we prove that they are equally powerful. We argue conceptually and
through case studies that the AEC games model is useful in important scenarios
in MARL for which the POSG model is not well suited. We additionally introduce
"cyclically expansive curriculum learning," a new MARL curriculum learning
method motivated by the AEC games model. It can be applied "for free," and
experimentally we show this technique to achieve up to 35.1% more total reward
on average.
</p>
<a href="http://arxiv.org/abs/2009.13051">arXiv:2009.13051</a> [<a href="http://arxiv.org/pdf/2009.13051">pdf</a>]

<h2>Rotated Binary Neural Network. (arXiv:2009.13055v1 [cs.CV])</h2>
<h3>Mingbao Lin, Rongrong Ji, Zihan Xu, Baochang Zhang, Yan Wang, Yongjian Wu, Feiyue Huang, Chia-Wen Lin</h3>
<p>Binary Neural Network (BNN) shows its predominance in reducing the complexity
of deep neural networks. However, it suffers severe performance degradation.
One of the major impediments is the large quantization error between the
full-precision weight vector and its binary vector. Previous works focus on
compensating for the norm gap while leaving the angular bias hardly touched. In
this paper, for the first time, we explore the influence of angular bias on the
quantization error and then introduce a Rotated Binary Neural Network (RBNN),
which considers the angle alignment between the full-precision weight vector
and its binarized version. At the beginning of each training epoch, we propose
to rotate the full-precision weight vector to its binary vector to reduce the
angular bias. To avoid the high complexity of learning a large rotation matrix,
we further introduce a bi-rotation formulation that learns two smaller rotation
matrices. In the training stage, we devise an adjustable rotated weight vector
for binarization to escape the potential local optimum. Our rotation leads to
around 50% weight flips which maximize the information gain. Finally, we
propose a training-aware approximation of the sign function for the gradient
backward. Experiments on CIFAR-10 and ImageNet demonstrate the superiorities of
RBNN over many state-of-the-arts. Our source code, experimental settings,
training logs and binary models are available at
https://github.com/lmbxmu/RBNN.
</p>
<a href="http://arxiv.org/abs/2009.13055">arXiv:2009.13055</a> [<a href="http://arxiv.org/pdf/2009.13055">pdf</a>]

<h2>A Simple and Efficient Ensemble Classifier Combining Multiple Neural Network Models on Social Media Datasets in Vietnamese. (arXiv:2009.13060v1 [cs.CL])</h2>
<h3>Huy Duc Huynh, Hang Thi-Thuy Do, Kiet Van Nguyen, Ngan Luu-Thuy Nguyen</h3>
<p>Text classification is a popular topic of natural language processing, which
has currently attracted numerous research efforts worldwide. The significant
increase of data in social media requires the vast attention of researchers to
analyze such data. There are various studies in this field in many languages
but limited to the Vietnamese language. Therefore, this study aims to classify
Vietnamese texts on social media from three different Vietnamese benchmark
datasets. Advanced deep learning models are used and optimized in this study,
including CNN, LSTM, and their variants. We also implement the BERT, which has
never been applied to the datasets. Our experiments find a suitable model for
classification tasks on each specific dataset. To take advantage of single
models, we propose an ensemble model, combining the highest-performance models.
Our single models reach positive results on each dataset. Moreover, our
ensemble model achieves the best performance on all three datasets. We reach
86.96% of F1- score for the HSD-VLSP dataset, 65.79% of F1-score for the
UIT-VSMEC dataset, 92.79% and 89.70% for sentiments and topics on the UIT-VSFC
dataset, respectively. Therefore, our models achieve better performances as
compared to previous studies on these datasets.
</p>
<a href="http://arxiv.org/abs/2009.13060">arXiv:2009.13060</a> [<a href="http://arxiv.org/pdf/2009.13060">pdf</a>]

<h2>Accelerating Multi-Model Inference by Merging DNNs of Different Weights. (arXiv:2009.13062v1 [cs.LG])</h2>
<h3>Joo Seong Jeong, Soojeong Kim, Gyeong-In Yu, Yunseong Lee, Byung-Gon Chun</h3>
<p>Standardized DNN models that have been proved to perform well on machine
learning tasks are widely used and often adopted as-is to solve downstream
tasks, forming the transfer learning paradigm. However, when serving multiple
instances of such DNN models from a cluster of GPU servers, existing techniques
to improve GPU utilization such as batching are inapplicable because models
often do not share weights due to fine-tuning. We propose NetFuse, a technique
of merging multiple DNN models that share the same architecture but have
different weights and different inputs. NetFuse is made possible by replacing
operations with more general counterparts that allow a set of weights to be
associated with only a certain set of inputs. Experiments on ResNet-50,
ResNeXt-50, BERT, and XLNet show that NetFuse can speed up DNN inference time
up to 3.6x on a NVIDIA V100 GPU, and up to 3.0x on a TITAN Xp GPU when merging
32 model instances, while only using up a small additional amount of GPU
memory.
</p>
<a href="http://arxiv.org/abs/2009.13062">arXiv:2009.13062</a> [<a href="http://arxiv.org/pdf/2009.13062">pdf</a>]

<h2>Vision based Target Interception using Aerial Manipulation. (arXiv:2009.13066v1 [cs.RO])</h2>
<h3>Lima Agnel Tony, Shuvrangshu Jana, Aashay Bhise, Varun V P, Aruul Mozhi Varman S, Vidyadhara B V, Mohitvishnu S Gadde, Debasish Ghose, Raghu Krishnapuram</h3>
<p>Selective interception of objects in unknown environment autonomously by UAVs
is an interesting problem. In this work, vision based interception is carried
out. This problem is a part of challenge 1 of Mohammed Bin Zayed International
Robotic Challenge, 2020, where, balloons are kept at five random locations for
the UAVs to autonomously explore, detect, approach and intercept. The problem
requires a different formulation to execute compared to the normal interception
problems in literature. This work details the different aspect of this problem
from vision to manipulator design. The frame work is implemented on hardware
using Robot Operating System (ROS) communication architecture.
</p>
<a href="http://arxiv.org/abs/2009.13066">arXiv:2009.13066</a> [<a href="http://arxiv.org/pdf/2009.13066">pdf</a>]

<h2>A Hybrid Intrusion Detection with Decision Tree for Feature Selection. (arXiv:2009.13067v1 [cs.CR])</h2>
<h3>Mubarak Albarka Umar, Chen Zhanfang, Yan Liu</h3>
<p>Due to the size and nature of intrusion detection datasets, intrusion
detection systems (IDS) typically take high computational complexity to examine
features of data and identify intrusive patterns. Data preprocessing techniques
such as feature selection can be used to reduce such complexity by eliminating
irrelevant and redundant features in the dataset. The objective of this study
is to analyze the efficiency and effectiveness of some feature selection
approaches namely, wrapper-based and filter-based modeling approaches. To
achieve that, a hybrid of feature selection algorithm in combination with
wrapper and filter selection processes is designed. We propose a wrapper-based
hybrid intrusion detection modeling with a decision tree algorithm to guide the
selection process. Five machine learning algorithms are used on the wrapper and
filter-based feature selection methods to build IDS models using the UNSW-NB15
dataset. The three filter-based methods namely, information gain, gain ratio,
and relief are used for comparison to determine the efficiency and
effectiveness of the proposed approach. Furthermore, a fair comparison with
other state-of-the-art intrusion detection approaches is also performed. The
experimental results show that our approach is quite effective in comparison to
state-of-the-art works, however, it takes high computational time in comparison
to the filter-based methods whilst achieves similar results. Our work also
revealed unobserved issues about the conformity of the UNSW-NB15 dataset.
</p>
<a href="http://arxiv.org/abs/2009.13067">arXiv:2009.13067</a> [<a href="http://arxiv.org/pdf/2009.13067">pdf</a>]

<h2>Semi-Supervised Image Deraining using Gaussian Processes. (arXiv:2009.13075v1 [cs.CV])</h2>
<h3>Rajeev Yasarla, V.A. Sindagi, V.M. Patel</h3>
<p>Recent CNN-based methods for image deraining have achieved excellent
performance in terms of reconstruction error as well as visual quality.
However, these methods are limited in the sense that they can be trained only
on fully labeled data. Due to various challenges in obtaining real world
fully-labeled image deraining datasets, existing methods are trained only on
synthetically generated data and hence, generalize poorly to real-world images.
The use of real-world data in training image deraining networks is relatively
less explored in the literature. We propose a Gaussian Process-based
semi-supervised learning framework which enables the network in learning to
derain using synthetic dataset while generalizing better using unlabeled
real-world images. More specifically, we model the latent space vectors of
unlabeled data using Gaussian Processes, which is then used to compute
pseudo-ground-truth for supervising the network on unlabeled data. Through
extensive experiments and ablations on several challenging datasets (such as
Rain800, Rain200L and DDN-SIRR), we show that the proposed method is able to
effectively leverage unlabeled data thereby resulting in significantly better
performance as compared to labeled-only training. Additionally, we demonstrate
that using unlabeled real-world images in the proposed GP-based framework
results
</p>
<a href="http://arxiv.org/abs/2009.13075">arXiv:2009.13075</a> [<a href="http://arxiv.org/pdf/2009.13075">pdf</a>]

<h2>Reactive Supervision: A New Method for Collecting Sarcasm Data. (arXiv:2009.13080v1 [cs.CL])</h2>
<h3>Boaz Shmueli, Lun-Wei Ku, Soumya Ray</h3>
<p>Sarcasm detection is an important task in affective computing, requiring
large amounts of labeled data. We introduce reactive supervision, a novel data
collection method that utilizes the dynamics of online conversations to
overcome the limitations of existing data collection techniques. We use the new
method to create and release a first-of-its-kind large dataset of tweets with
sarcasm perspective labels and new contextual features. The dataset is expected
to advance sarcasm detection research. Our method can be adapted to other
affective computing domains, thus opening up new research opportunities.
</p>
<a href="http://arxiv.org/abs/2009.13080">arXiv:2009.13080</a> [<a href="http://arxiv.org/pdf/2009.13080">pdf</a>]

<h2>Deep Reinforcement Learning for DER Cyber-Attack Mitigation. (arXiv:2009.13088v1 [eess.SY])</h2>
<h3>Ciaran Roberts, Sy-Toan Ngo, Alexandre Milesi, Sean Peisert, Daniel Arnold, Shammya Saha, Anna Scaglione, Nathan Johnson, Anton Kocheturov, Dmitriy Fradkin</h3>
<p>The increasing penetration of DER with smart-inverter functionality is set to
transform the electrical distribution network from a passive system, with fixed
injection/consumption, to an active network with hundreds of distributed
controllers dynamically modulating their operating setpoints as a function of
system conditions. This transition is being achieved through standardization of
functionality through grid codes and/or international standards. DER, however,
are unique in that they are typically neither owned nor operated by
distribution utilities and, therefore, represent a new emerging attack vector
for cyber-physical attacks. Within this work we consider deep reinforcement
learning as a tool to learn the optimal parameters for the control logic of a
set of uncompromised DER units to actively mitigate the effects of a
cyber-attack on a subset of network DER.
</p>
<a href="http://arxiv.org/abs/2009.13088">arXiv:2009.13088</a> [<a href="http://arxiv.org/pdf/2009.13088">pdf</a>]

<h2>Learning Classifiers under Delayed Feedback with a Time Window Assumption. (arXiv:2009.13092v1 [cs.LG])</h2>
<h3>Masahiro Kato, Shota Yasui</h3>
<p>We consider training a binary classifier under delayed feedback (DF
Learning). In DF Learning, we first receive negative samples; subsequently,
some samples turn positive. This problem is conceivable in various real-world
applications such as online advertisements, where the user action takes place
long after the first click. Owing to the delayed feedback, simply separating
the positive and negative data causes a sample selection bias. One solution is
to assume that a long time window after first observing a sample reduces the
sample selection bias. However, existing studies report that only using a
portion of all samples based on the time window assumption yields suboptimal
performance, and the use of all samples along with the time window assumption
improves empirical performance. Extending these existing studies, we propose a
method with an unbiased and convex empirical risk constructed from the whole
samples under the time window assumption. We provide experimental results to
demonstrate the effectiveness of the proposed method using a real traffic log
dataset.
</p>
<a href="http://arxiv.org/abs/2009.13092">arXiv:2009.13092</a> [<a href="http://arxiv.org/pdf/2009.13092">pdf</a>]

<h2>Improved generalization by noise enhancement. (arXiv:2009.13094v1 [cs.LG])</h2>
<h3>Takashi Mori, Masahito Ueda</h3>
<p>Recent studies have demonstrated that noise in stochastic gradient descent
(SGD) is closely related to generalization: A larger SGD noise, if not too
large, results in better generalization. Since the covariance of the SGD noise
is proportional to $\eta^2/B$, where $\eta$ is the learning rate and $B$ is the
minibatch size of SGD, the SGD noise has so far been controlled by changing
$\eta$ and/or $B$. However, too large $\eta$ results in instability in the
training dynamics and a small $B$ prevents scalable parallel computation. It is
thus desirable to develop a method of controlling the SGD noise without
changing $\eta$ and $B$. In this paper, we propose a method that achieves this
goal using ``noise enhancement'', which is easily implemented in practice. We
expound the underlying theoretical idea and demonstrate that the noise
enhancement actually improves generalization for real datasets. It turns out
that large-batch training with the noise enhancement even shows better
generalization compared with small-batch training.
</p>
<a href="http://arxiv.org/abs/2009.13094">arXiv:2009.13094</a> [<a href="http://arxiv.org/pdf/2009.13094">pdf</a>]

<h2>Distillation of Weighted Automata from Recurrent Neural Networks using a Spectral Approach. (arXiv:2009.13101v1 [cs.LG])</h2>
<h3>Remi Eyraud, Stephane Ayache</h3>
<p>This paper is an attempt to bridge the gap between deep learning and
grammatical inference. Indeed, it provides an algorithm to extract a
(stochastic) formal language from any recurrent neural network trained for
language modelling. In detail, the algorithm uses the already trained network
as an oracle -- and thus does not require the access to the inner
representation of the black-box -- and applies a spectral approach to infer a
weighted automaton.

As weighted automata compute linear functions, they are computationally more
efficient than neural networks and thus the nature of the approach is the one
of knowledge distillation. We detail experiments on 62 data sets (both
synthetic and from real-world applications) that allow an in-depth study of the
abilities of the proposed algorithm. The results show the WA we extract are
good approximations of the RNN, validating the approach. Moreover, we show how
the process provides interesting insights toward the behavior of RNN learned on
data, enlarging the scope of this work to the one of explainability of deep
learning models.
</p>
<a href="http://arxiv.org/abs/2009.13101">arXiv:2009.13101</a> [<a href="http://arxiv.org/pdf/2009.13101">pdf</a>]

<h2>Deep Transformers with Latent Depth. (arXiv:2009.13102v1 [cs.CL])</h2>
<h3>Xian Li, Asa Cooper Stickland, Yuqing Tang, Xiang Kong</h3>
<p>The Transformer model has achieved state-of-the-art performance in many
sequence modeling tasks. However, how to leverage model capacity with large or
variable depths is still an open challenge. We present a probabilistic
framework to automatically learn which layer(s) to use by learning the
posterior distributions of layer selection. As an extension of this framework,
we propose a novel method to train one shared Transformer network for
multilingual machine translation with different layer selection posteriors for
each language pair. The proposed method alleviates the vanishing gradient issue
and enables stable training of deep Transformers (e.g. 100 layers). We evaluate
on WMT English-German machine translation and masked language modeling tasks,
where our method outperforms existing approaches for training deeper
Transformers. Experiments on multilingual machine translation demonstrate that
this approach can effectively leverage increased model capacity and bring
universal improvement for both many-to-one and one-to-many translation with
diverse language pairs.
</p>
<a href="http://arxiv.org/abs/2009.13102">arXiv:2009.13102</a> [<a href="http://arxiv.org/pdf/2009.13102">pdf</a>]

<h2>NITI: Training Integer Neural Networks Using Integer-only Arithmetic. (arXiv:2009.13108v1 [cs.CV])</h2>
<h3>Maolin Wang, Seyedramin Rasoulinezhad, Philip H.W. Leong, Hayden K.H. So</h3>
<p>While integer arithmetic has been widely adopted for improved performance in
deep quantized neural network inference, training remains a task primarily
executed using floating point arithmetic. This is because both high dynamic
range and numerical accuracy are central to the success of most modern training
algorithms. However, due to its potential for computational, storage and energy
advantages in hardware accelerators, neural network training methods that can
be implemented with low precision integer-only arithmetic remains an active
research challenge. In this paper, we present NITI, an efficient deep neural
network training framework that stores all parameters and intermediate values
as integers, and computes exclusively with integer arithmetic. A pseudo
stochastic rounding scheme that eliminates the need for external random number
generation is proposed to facilitate conversion from wider intermediate results
to low precision storage. Furthermore, a cross-entropy loss backpropagation
scheme computed with integer-only arithmetic is proposed. A proof-of-concept
open-source software implementation of NITI that utilizes native 8-bit integer
operations in modern GPUs to achieve end-to-end training is presented. When
compared with an equivalent training setup implemented with floating point
storage and arithmetic, NITI achieves negligible accuracy degradation on the
MNIST and CIFAR10 datasets using 8-bit integer storage and computation. On
ImageNet, 16-bit integers are needed for weight accumulation with an 8-bit
datapath. This achieves training results comparable to all-floating-point
implementations.
</p>
<a href="http://arxiv.org/abs/2009.13108">arXiv:2009.13108</a> [<a href="http://arxiv.org/pdf/2009.13108">pdf</a>]

<h2>Learning to Stop: A Simple yet Effective Approach to Urban Vision-Language Navigation. (arXiv:2009.13112v1 [cs.CV])</h2>
<h3>Jiannan Xiang, Xin Eric Wang, William Yang Wang</h3>
<p>Vision-and-Language Navigation (VLN) is a natural language grounding task
where an agent learns to follow language instructions and navigate to specified
destinations in real-world environments. A key challenge is to recognize and
stop at the correct location, especially for complicated outdoor environments.
Existing methods treat the STOP action equally as other actions, which results
in undesirable behaviors that the agent often fails to stop at the destination
even though it might be on the right path. Therefore, we propose Learning to
Stop (L2Stop), a simple yet effective policy module that differentiates STOP
and other actions. Our approach achieves the new state of the art on a
challenging urban VLN dataset Touchdown, outperforming the baseline by 6.89%
(absolute improvement) on Success weighted by Edit Distance (SED).
</p>
<a href="http://arxiv.org/abs/2009.13112">arXiv:2009.13112</a> [<a href="http://arxiv.org/pdf/2009.13112">pdf</a>]

<h2>Automated Identification of On-hold Self-admitted Technical Debt. (arXiv:2009.13113v1 [cs.SE])</h2>
<h3>Rungroj Maipradit, Bin Lin, Csaba Nagy, Gabriele Bavota, Michele Lanza, Hideaki Hata, Kenichi Matsumoto</h3>
<p>Modern software is developed under considerable time pressure, which implies
that developers more often than not have to resort to compromises when it comes
to code that is well written and code that just does the job. This has led over
the past decades to the concept of "technical debt", a short-term hack that
potentially generates long-term maintenance problems. Self-admitted technical
debt (SATD) is a particular form of technical debt: developers consciously
perform the hack but also document it in the code by adding comments as a
reminder (or as an admission of guilt). We focus on a specific type of SATD,
namely "On-hold" SATD, in which developers document in their comments the need
to halt an implementation task due to conditions outside of their scope of work
(e.g., an open issue must be closed before a function can be implemented). We
present an approach, based on regular expressions and machine learning, which
is able to detect issues referenced in code comments, and to automatically
classify the detected instances as either "On-hold" (the issue is referenced to
indicate the need to wait for its resolution before completing a task), or as
"cross-reference", (the issue is referenced to document the code, for example
to explain the rationale behind an implementation choice). Our approach also
mines the issue tracker of the projects to check if the On-hold SATD instances
are "superfluous" and can be removed (i.e., the referenced issue has been
closed, but the SATD is still in the code). Our evaluation confirms that our
approach can indeed identify relevant instances of On-hold SATD. We illustrate
its usefulness by identifying superfluous On-hold SATD instances in open source
projects as confirmed by the original developers.
</p>
<a href="http://arxiv.org/abs/2009.13113">arXiv:2009.13113</a> [<a href="http://arxiv.org/pdf/2009.13113">pdf</a>]

<h2>RRPN++: Guidance Towards More Accurate Scene Text Detection. (arXiv:2009.13118v1 [cs.CV])</h2>
<h3>Jianqi Ma</h3>
<p>RRPN is among the outstanding scene text detection approaches, but the
manually-designed anchor and coarse proposal refinement make the performance
still far from perfection. In this paper, we propose RRPN++ to exploit the
potential of RRPN-based model by several improvements. Based on RRPN, we
propose the Anchor-free Pyramid Proposal Networks (APPN) to generate
first-stage proposals, which adopts the anchor-free design to reduce proposal
number and accelerate the inference speed. In our second stage, both the
detection branch and the recognition branch are incorporated to perform
multi-task learning. In inference stage, the detection branch outputs the
proposal refinement and the recognition branch predicts the transcript of the
refined text region. Further, the recognition branch also helps rescore the
proposals and eliminate the false positive proposals by the jointing filtering
strategy. With these enhancements, we boost the detection results by $6\%$ of
F-measure in ICDAR2015 compared to RRPN. Experiments conducted on other
benchmarks also illustrate the superior performance and efficiency of our
model.
</p>
<a href="http://arxiv.org/abs/2009.13118">arXiv:2009.13118</a> [<a href="http://arxiv.org/pdf/2009.13118">pdf</a>]

<h2>Medical Image Segmentation Using Deep Learning: A Survey. (arXiv:2009.13120v1 [eess.IV])</h2>
<h3>Tao Lei, Risheng Wang, Yong Wan, Xiaogang Du, Hongying Meng, Asoke K. Nandi</h3>
<p>Deep learning has been widely used for medical image segmentation and a large
number of papers has been presented recording the success of deep learning in
the field. In this paper, we present a comprehensive thematic survey on medical
image segmentation using deep learning techniques. This paper makes two
original contributions. Firstly, compared to traditional surveys that directly
divide literatures of deep learning on medical image segmentation into many
groups and introduce literatures in detail for each group, we classify
currently popular literatures according to a multi-level structure from coarse
to fine. Secondly, this paper focuses on supervised and weakly supervised
learning approaches, without including unsupervised approaches since they have
been introduced in many old surveys and they are not popular currently. For
supervised learning approaches, we analyze literatures in three aspects: the
selection of backbone networks, the design of network blocks, and the
improvement of loss functions. For weakly supervised learning approaches, we
investigate literature according to data augmentation, transfer learning, and
interactive segmentation, separately. Compared to existing surveys, this survey
classifies the literatures very differently from before and is more convenient
for readers to understand the relevant rationale and will guide them to think
of appropriate improvements in medical image segmentation based on deep
learning approaches.
</p>
<a href="http://arxiv.org/abs/2009.13120">arXiv:2009.13120</a> [<a href="http://arxiv.org/pdf/2009.13120">pdf</a>]

<h2>Interpretable Detail-Fidelity Attention Network for Single Image Super-Resolution. (arXiv:2009.13134v1 [cs.CV])</h2>
<h3>Yuanfei Huang, Jie Li, Xinbo Gao, Yanting Hu, Wen Lu</h3>
<p>Benefiting from the strong capabilities of deep CNNs for feature
representation and nonlinear mapping, deep-learning-based methods have achieved
excellent performance in single image super-resolution. However, most existing
SR methods depend on the high capacity of networks which is initially designed
for visual recognition, and rarely consider the initial intention of
super-resolution for detail fidelity. Aiming at pursuing this intention, there
are two challenging issues to be solved: (1) learning appropriate operators
which is adaptive to the diverse characteristics of smoothes and details; (2)
improving the ability of model to preserve the low-frequency smoothes and
reconstruct the high-frequency details. To solve them, we propose a purposeful
and interpretable detail-fidelity attention network to progressively process
these smoothes and details in divide-and-conquer manner, which is a novel and
specific prospect of image super-resolution for the purpose on improving the
detail fidelity, instead of blindly designing or employing the deep CNNs
architectures for merely feature representation in local receptive fields.
Particularly, we propose a Hessian filtering for interpretable feature
representation which is high-profile for detail inference, a dilated
encoder-decoder and a distribution alignment cell to improve the inferred
Hessian features in morphological manner and statistical manner respectively.
Extensive experiments demonstrate that the proposed methods achieve superior
performances over the state-of-the-art methods quantitatively and
qualitatively. Code is available at https://github.com/YuanfeiHuang/DeFiAN.
</p>
<a href="http://arxiv.org/abs/2009.13134">arXiv:2009.13134</a> [<a href="http://arxiv.org/pdf/2009.13134">pdf</a>]

<h2>Availability Evaluation of Multi-tenant Service Function Chaining Infrastructures by Multidimensional Universal Generating Function. (arXiv:2009.13141v1 [cs.NI])</h2>
<h3>Mario Di Mauro, Maurizio Longo, Fabio Postiglione</h3>
<p>The Network Function Virtualization (NFV) paradigm has been devised as an
enabler of next generation network infrastructures by speeding up the
provisioning and the composition of novel network services. The latter are
implemented via a chain of virtualized network functions, a process known as
Service Function Chaining. In this paper, we evaluate the availability of
multi-tenant SFC infrastructures, where every network function is modeled as a
multi-state system and is shared among different and independent tenants. To
this aim, we propose a Universal Generating Function (UGF) approach, suitably
extended to handle performance vectors, that we call Multidimensional UGF. This
novel methodology is validated in a realistic multi-tenant telecommunication
network scenario, where the service chain is composed by the network elements
of an IP Multimedia Subsystem implemented via NFV. A steady-state availability
evaluation of such an exemplary system is presented and a redundancy
optimization problem is solved, so providing the SFC infrastructure which
minimizes deployment cost while respecting a given availability requirement.
</p>
<a href="http://arxiv.org/abs/2009.13141">arXiv:2009.13141</a> [<a href="http://arxiv.org/pdf/2009.13141">pdf</a>]

<h2>Amodal 3D Reconstruction for Robotic Manipulation via Stability and Connectivity. (arXiv:2009.13146v1 [cs.RO])</h2>
<h3>William Agnew, Christopher Xie, Aaron Walsman, Octavian Murad, Caelen Wang, Pedro Domingos, Siddhartha Srinivasa</h3>
<p>Learning-based 3D object reconstruction enables single- or few-shot
estimation of 3D object models. For robotics, this holds the potential to allow
model-based methods to rapidly adapt to novel objects and scenes. Existing 3D
reconstruction techniques optimize for visual reconstruction fidelity,
typically measured by chamfer distance or voxel IOU. We find that when applied
to realistic, cluttered robotics environments, these systems produce
reconstructions with low physical realism, resulting in poor task performance
when used for model-based control. We propose ARM, an amodal 3D reconstruction
system that introduces (1) a stability prior over object shapes, (2) a
connectivity prior, and (3) a multi-channel input representation that allows
for reasoning over relationships between groups of objects. By using these
priors over the physical properties of objects, our system improves
reconstruction quality not just by standard visual metrics, but also
performance of model-based control on a variety of robotics manipulation tasks
in challenging, cluttered environments. Code is available at
github.com/wagnew3/ARM.
</p>
<a href="http://arxiv.org/abs/2009.13146">arXiv:2009.13146</a> [<a href="http://arxiv.org/pdf/2009.13146">pdf</a>]

<h2>Automated Pancreas Segmentation Using Multi-institutional Collaborative Deep Learning. (arXiv:2009.13148v1 [eess.IV])</h2>
<h3>Pochuan Wang, Chen Shen, Holger R. Roth, Dong Yang, Daguang Xu, Masahiro Oda, Kazunari Misawa, Po-Ting Chen, Kao-Lang Liu, Wei-Chih Liao, Weichung Wang, Kensaku Mori</h3>
<p>The performance of deep learning-based methods strongly relies on the number
of datasets used for training. Many efforts have been made to increase the data
in the medical image analysis field. However, unlike photography images, it is
hard to generate centralized databases to collect medical images because of
numerous technical, legal, and privacy issues. In this work, we study the use
of federated learning between two institutions in a real-world setting to
collaboratively train a model without sharing the raw data across national
boundaries. We quantitatively compare the segmentation models obtained with
federated learning and local training alone. Our experimental results show that
federated learning models have higher generalizability than standalone
training.
</p>
<a href="http://arxiv.org/abs/2009.13148">arXiv:2009.13148</a> [<a href="http://arxiv.org/pdf/2009.13148">pdf</a>]

<h2>Statistical Assessment of IP Multimedia Subsystem in a Softwarized Environment: a Queueing Networks Approach. (arXiv:2009.13149v1 [cs.NI])</h2>
<h3>Mario Di Mauro, Antonio Liotta</h3>
<p>The Next Generation 5G Networks can greatly benefit from the synergy between
virtualization paradigms, such as the Network Function Virtualization (NFV),
and service provisioning platforms such as the IP Multimedia Subsystem (IMS).
The NFV concept is evolving towards a lightweight solution based on containers
that, by contrast to classic virtual machines, do not carry a whole operating
system and result in more efficient and scalable deployments. On the other
hand, IMS has become an integral part of the 5G core network, for instance, to
provide advanced services like Voice over LTE (VoLTE). In this paper we combine
these virtualization and service provisioning concepts, deriving a
containerized IMS infrastructure, dubbed cIMS, providing its assessment through
statistical characterization and experimental measurements. Specifically, we:
i) model cIMS through the queueing networks methodology to characterize the
utilization of virtual resources under constrained conditions; ii) draw an
extended version of the Pollaczek-Khinchin formula, which is useful to deal
with bulk arrivals; iii) afford an optimization problem focused at maximizing
the whole cIMS performance in the presence of capacity constraints, thus
providing new means for the service provider to manage service level agreements
(SLAs); $iv)$ evaluate a range of cIMS scenarios, considering different queuing
disciplines including also multiple job classes. An experimental testbed based
on the open source platform Clearwater has been deployed to derive some
realistic values of key parameters (e.g. arrival and service times).
</p>
<a href="http://arxiv.org/abs/2009.13149">arXiv:2009.13149</a> [<a href="http://arxiv.org/pdf/2009.13149">pdf</a>]

<h2>Balancing thermal comfort datasets: We GAN, but should we?. (arXiv:2009.13154v1 [cs.LG])</h2>
<h3>Matias Quintana, Stefano Schiavon, Kwok Wai Tham, Clayton Miller</h3>
<p>Thermal comfort assessment for the built environment has become more
available to analysts and researchers due to the proliferation of sensors and
subjective feedback methods. These data can be used for modeling comfort
behavior to support design and operations towards energy efficiency and
well-being. By nature, occupant subjective feedback is imbalanced as indoor
conditions are designed for comfort, and responses indicating otherwise are
less common. This situation creates a scenario for the machine learning
workflow where class balancing as a pre-processing step might be valuable for
developing predictive thermal comfort classification models with
high-performance. This paper investigates the various thermal comfort dataset
class balancing techniques from the literature and proposes a modified
conditional Generative Adversarial Network (GAN), $\texttt{comfortGAN}$, to
address this imbalance scenario. These approaches are applied to three publicly
available datasets, ranging from 30 and 67 participants to a global collection
of thermal comfort datasets, with 1,474; 2,067; and 66,397 data points,
respectively. This work finds that a classification model trained on a balanced
dataset, comprised of real and generated samples from $\texttt{comfortGAN}$,
has higher performance (increase between 4% and 17% in classification accuracy)
than other augmentation methods tested. However, when classes representing
discomfort are merged and reduced to three, better imbalanced performance is
expected, and the additional increase in performance by $\texttt{comfortGAN}$
shrinks to 1-2%. These results illustrate that class balancing for thermal
comfort modeling is beneficial using advanced techniques such as GANs, but its
value is diminished in certain scenarios. A discussion is provided to assist
potential users in determining which scenarios this process is useful and which
method works best.
</p>
<a href="http://arxiv.org/abs/2009.13154">arXiv:2009.13154</a> [<a href="http://arxiv.org/pdf/2009.13154">pdf</a>]

<h2>Towards Heterogeneous Multi-Agent Reinforcement Learning with Graph Neural Networks. (arXiv:2009.13161v1 [cs.AI])</h2>
<h3>Douglas De Rizzo Meneghetti, Reinaldo Augusto da Costa Bianchi</h3>
<p>This work proposes a neural network architecture that learns policies for
multiple agent classes in a heterogeneous multi-agent reinforcement setting.
The proposed network uses directed labeled graph representations for states,
encodes feature vectors of different sizes for different entity classes, uses
relational graph convolution layers to model different communication channels
between entity types and learns distinct policies for different agent classes,
sharing parameters wherever possible. Results have shown that specializing the
communication channels between entity classes is a promising step to achieve
higher performance in environments composed of heterogeneous entities.
</p>
<a href="http://arxiv.org/abs/2009.13161">arXiv:2009.13161</a> [<a href="http://arxiv.org/pdf/2009.13161">pdf</a>]

<h2>Quantal synaptic dilution enhances sparse encoding and dropout regularisation in deep networks. (arXiv:2009.13165v1 [cs.LG])</h2>
<h3>Gardave S Bhumbra</h3>
<p>Dropout is a technique that silences the activity of units stochastically
while training deep networks to reduce overfitting. Here we introduce Quantal
Synaptic Dilution (QSD), a biologically plausible model of dropout
regularisation based on the quantal properties of neuronal synapses, that
incorporates heterogeneities in response magnitudes and release probabilities
for vesicular quanta. QSD outperforms standard dropout in ReLU multilayer
perceptrons, with enhanced sparse encoding at test time when dropout masks are
replaced with identity functions, without shifts in trainable weight or bias
distributions. For convolutional networks, the method also improves
generalisation in computer vision tasks with and without inclusion of
additional forms of regularisation. QSD also outperforms standard dropout in
recurrent networks for language modelling and sentiment analysis. An advantage
of QSD over many variations of dropout is that it can be implemented generally
in all conventional deep networks where standard dropout is applicable.
</p>
<a href="http://arxiv.org/abs/2009.13165">arXiv:2009.13165</a> [<a href="http://arxiv.org/pdf/2009.13165">pdf</a>]

<h2>CASTLE: Regularization via Auxiliary Causal Graph Discovery. (arXiv:2009.13180v1 [cs.LG])</h2>
<h3>Trent Kyono, Yao Zhang, Mihaela van der Schaar</h3>
<p>Regularization improves generalization of supervised models to out-of-sample
data. Prior works have shown that prediction in the causal direction (effect
from cause) results in lower testing error than the anti-causal direction.
However, existing regularization methods are agnostic of causality. We
introduce Causal Structure Learning (CASTLE) regularization and propose to
regularize a neural network by jointly learning the causal relationships
between variables. CASTLE learns the causal directed acyclical graph (DAG) as
an adjacency matrix embedded in the neural network's input layers, thereby
facilitating the discovery of optimal predictors. Furthermore, CASTLE
efficiently reconstructs only the features in the causal DAG that have a causal
neighbor, whereas reconstruction-based regularizers suboptimally reconstruct
all input features. We provide a theoretical generalization bound for our
approach and conduct experiments on a plethora of synthetic and real publicly
available datasets demonstrating that CASTLE consistently leads to better
out-of-sample predictions as compared to other popular benchmark regularizers.
</p>
<a href="http://arxiv.org/abs/2009.13180">arXiv:2009.13180</a> [<a href="http://arxiv.org/pdf/2009.13180">pdf</a>]

<h2>A thermodynamically consistent chemical spiking neuron capable of autonomous Hebbian learning. (arXiv:2009.13207v1 [cs.NE])</h2>
<h3>Jakub Fil, Dominique Chu</h3>
<p>We propose a fully autonomous, thermodynamically consistent set of chemical
reactions that implements a spiking neuron. This chemical neuron is able to
learn input patterns in a Hebbian fashion. The system is scalable to
arbitrarily many input channels. We demonstrate its performance in learning
frequency biases in the input as well as correlations between different input
channels. Efficient computation of time-correlations requires a highly
non-linear activation function. The resource requirements of a non-linear
activation function are discussed. In addition to the thermodynamically
consistent model of the CN, we also propose a biologically plausible version
that could be engineered in a synthetic biology context.
</p>
<a href="http://arxiv.org/abs/2009.13207">arXiv:2009.13207</a> [<a href="http://arxiv.org/pdf/2009.13207">pdf</a>]

<h2>Instance-Based Counterfactual Explanations for Time Series Classification. (arXiv:2009.13211v1 [cs.LG])</h2>
<h3>Eoin Delaney, Derek Greene, Mark T. Keane</h3>
<p>In recent years there has been a cascade of research in attempting to make AI
systems more interpretable by providing explanations; so-called Explainable AI
(XAI). Most of this research has dealt with the challenges that arise in
explaining black-box deep learning systems in classification and regression
tasks, with a focus on tabular and image data; for example, there is a rich
seam of work on post-hoc counterfactual explanations for a variety of black-box
classifiers (e.g., when a user is refused a loan, the counterfactual
explanation tells the user about the conditions under which they would get the
loan). However, less attention has been paid to the parallel interpretability
challenges arising in AI systems dealing with time series data. This paper
advances a novel technique, called Native-Guide, for the generation of proximal
and plausible counterfactual explanations for instance-based time series
classification tasks (e.g., where users are provided with alternative time
series to explain how a classification might change). The Native-Guide method
retrieves and uses native in-sample counterfactuals that already exist in the
training data as "guides" for perturbation in time series counterfactual
generation. This method can be coupled with both Euclidean and Dynamic Time
Warping (DTW) distance measures. After illustrating the technique on a case
study involving a climate classification task, we reported on a comprehensive
series of experiments on both real-world and synthetic data sets from the UCR
archive. These experiments provide computational evidence of the quality of the
counterfactual explanations generated.
</p>
<a href="http://arxiv.org/abs/2009.13211">arXiv:2009.13211</a> [<a href="http://arxiv.org/pdf/2009.13211">pdf</a>]

<h2>Deep EvoGraphNet Architecture For Time-Dependent Brain Graph Data Synthesis From a Single Timepoint. (arXiv:2009.13217v1 [eess.IV])</h2>
<h3>Ahmed Nebli, Ugur Ali Kaplan, Islem Rekik</h3>
<p>Learning how to predict the brain connectome (i.e. graph) development and
aging is of paramount importance for charting the future of within-disorder and
cross-disorder landscape of brain dysconnectivity evolution. Indeed, predicting
the longitudinal (i.e., time-dependent ) brain dysconnectivity as it emerges
and evolves over time from a single timepoint can help design personalized
treatments for disordered patients in a very early stage. Despite its
significance, evolution models of the brain graph are largely overlooked in the
literature. Here, we propose EvoGraphNet, the first end-to-end geometric deep
learning-powered graph-generative adversarial network (gGAN) for predicting
time-dependent brain graph evolution from a single timepoint. Our EvoGraphNet
architecture cascades a set of time-dependent gGANs, where each gGAN
communicates its predicted brain graphs at a particular timepoint to train the
next gGAN in the cascade at follow-up timepoint. Therefore, we obtain each next
predicted timepoint by setting the output of each generator as the input of its
successor which enables us to predict a given number of timepoints using only
one single timepoint in an end- to-end fashion. At each timepoint, to better
align the distribution of the predicted brain graphs with that of the
ground-truth graphs, we further integrate an auxiliary Kullback-Leibler
divergence loss function. To capture time-dependency between two consecutive
observations, we impose an l1 loss to minimize the sparse distance between two
serialized brain graphs. A series of benchmarks against variants and ablated
versions of our EvoGraphNet showed that we can achieve the lowest brain graph
evolution prediction error using a single baseline timepoint. Our EvoGraphNet
code is available at this http URL
</p>
<a href="http://arxiv.org/abs/2009.13217">arXiv:2009.13217</a> [<a href="http://arxiv.org/pdf/2009.13217">pdf</a>]

<h2>ECGDetect: Detecting Ischemia via Deep Learning. (arXiv:2009.13232v1 [cs.LG])</h2>
<h3>Atandra Burman, Jitto Titus, David Gbadebo, Melissa Burman</h3>
<p>Coronary artery disease(CAD) is the most common type of heart disease and the
leading cause of death worldwide[1]. A progressive state of this disease marked
by plaque rupture and clot formation in the coronary arteries, also known as an
acute coronary syndrome (ACS), is a condition of the heart associated with
sudden, reduced blood flow caused due to partial or full occlusion of coronary
vasculature that normally perfuses the myocardium and nerve bundles,
compromising the proper functioning of the heart. Often manifesting with pain
or tightness in the chest as the second most common cause of emergency
department visits in the United States, it is imperative to detect ACS at the
earliest. This is particularly relevant to diabetic patients at home, that may
not feel classic chest pain symptoms, and are susceptible to silent myocardial
injury. In this study, we developed the RCE- ECG-Detect algorithm, a machine
learning model to detect the morphological patterns in significant ST change
associated with myocardial ischemia. We developed the RCE- ECG-Detect using
data from the LTST database which has a sufficiently large sample set to train
a reliable model. We validated the predictive performance of the machine
learning model on a holdout test set collected using RCE's ECG wearable. Our
deep neural network model, equipped with convolution layers, achieves 90.31%
ROC-AUC, 89.34% sensitivity, 87.81% specificity.
</p>
<a href="http://arxiv.org/abs/2009.13232">arXiv:2009.13232</a> [<a href="http://arxiv.org/pdf/2009.13232">pdf</a>]

<h2>Sense and Learn: Self-Supervision for Omnipresent Sensors. (arXiv:2009.13233v1 [cs.LG])</h2>
<h3>Aaqib Saeed, Victor Ungureanu, Beat Gfeller</h3>
<p>Learning general-purpose representations from multisensor data produced by
the omnipresent sensing systems (or IoT in general) has numerous applications
in diverse use areas. Existing purely supervised end-to-end deep learning
techniques depend on the availability of a massive amount of well-curated data,
acquiring which is notoriously difficult but required to achieve a sufficient
level of generalization on a task of interest. In this work, we leverage the
self-supervised learning paradigm towards realizing the vision of continual
learning from unlabeled inputs. We present a generalized framework named Sense
and Learn for representation or feature learning from raw sensory data. It
consists of eight auxiliary tasks that can learn high-level and broadly useful
features entirely from unannotated data without any human involvement in the
tedious labeling process. We demonstrate the efficacy of our approach on
several publicly available datasets from different domains and in various
settings, including linear separability, semi-supervised or few shot learning,
and transfer learning. Our methodology achieves results that are competitive
with the supervised approaches and close the gap through fine-tuning a network
while learning the downstream tasks in most cases. In particular, we show that
the self-supervised network can be utilized as initialization to significantly
boost the performance in a low-data regime with as few as 5 labeled instances
per class, which is of high practical importance to real-world problems.
Likewise, the learned representations with self-supervision are found to be
highly transferable between related datasets, even when few labeled instances
are available from the target domains. The self-learning nature of our
methodology opens up exciting possibilities for on-device continual learning.
</p>
<a href="http://arxiv.org/abs/2009.13233">arXiv:2009.13233</a> [<a href="http://arxiv.org/pdf/2009.13233">pdf</a>]

<h2>Scalable Transfer Learning with Expert Models. (arXiv:2009.13239v1 [cs.LG])</h2>
<h3>Joan Puigcerver, Carlos Riquelme, Basil Mustafa, Cedric Renggli, Andr&#xe9; Susano Pinto, Sylvain Gelly, Daniel Keysers, Neil Houlsby</h3>
<p>Transfer of pre-trained representations can improve sample efficiency and
reduce computational requirements for new tasks. However, representations used
for transfer are usually generic, and are not tailored to a particular
distribution of downstream tasks. We explore the use of expert representations
for transfer with a simple, yet effective, strategy. We train a diverse set of
experts by exploiting existing label structures, and use cheap-to-compute
performance proxies to select the relevant expert for each target task. This
strategy scales the process of transferring to new tasks, since it does not
revisit the pre-training data during transfer. Accordingly, it requires little
extra compute per target task, and results in a speed-up of 2-3 orders of
magnitude compared to competing approaches. Further, we provide an
adapter-based architecture able to compress many experts into a single model.
We evaluate our approach on two different data sources and demonstrate that it
outperforms baselines on over 20 diverse vision tasks in both cases.
</p>
<a href="http://arxiv.org/abs/2009.13239">arXiv:2009.13239</a> [<a href="http://arxiv.org/pdf/2009.13239">pdf</a>]

<h2>Texture Memory-Augmented Deep Patch-Based Image Inpainting. (arXiv:2009.13240v1 [cs.CV])</h2>
<h3>Rui Xu, Minghao Guo, Jiaqi Wang, Xiaoxiao Li, Bolei Zhou, Chen Change Loy</h3>
<p>Patch-based methods and deep networks have been employed to tackle image
inpainting problem, with their own strengths and weaknesses. Patch-based
methods are capable of restoring a missing region with high-quality texture
through searching nearest neighbor patches from the unmasked regions. However,
these methods bring problematic contents when recovering large missing regions.
Deep networks, on the other hand, show promising results in completing large
regions. Nonetheless, the results often lack faithful and sharp details that
resemble the surrounding area. By bringing together the best of both paradigms,
we propose a new deep inpainting framework where texture generation is guided
by a texture memory of patch samples extracted from unmasked regions. The
framework has a novel design that allows texture memory retrieval to be trained
end-to-end with the deep inpainting network. In addition, we introduce a patch
distribution loss to encourage high-quality patch synthesis. The proposed
method shows superior performance both qualitatively and quantitatively on
three challenging image benchmarks, i.e., Places, CelebA-HQ, and Paris
Street-View datasets.
</p>
<a href="http://arxiv.org/abs/2009.13240">arXiv:2009.13240</a> [<a href="http://arxiv.org/pdf/2009.13240">pdf</a>]

<h2>Generating End-to-End Adversarial Examples for Malware Classifiers Using Explainability. (arXiv:2009.13243v1 [cs.CR])</h2>
<h3>Ishai Rosenberg, Shai Meir, Jonathan Berrebi, Ilay Gordon, Guillaume Sicard, Eli (Omid) David</h3>
<p>In recent years, the topic of explainable machine learning (ML) has been
extensively researched. Up until now, this research focused on regular ML users
use-cases such as debugging a ML model. This paper takes a different posture
and show that adversaries can leverage explainable ML to bypass multi-feature
types malware classifiers. Previous adversarial attacks against such
classifiers only add new features and not modify existing ones to avoid harming
the modified malware executable's functionality. Current attacks use a single
algorithm that both selects which features to modify and modifies them blindly,
treating all features the same. In this paper, we present a different approach.
We split the adversarial example generation task into two parts: First we find
the importance of all features for a specific sample using explainability
algorithms, and then we conduct a feature-specific modification,
feature-by-feature. In order to apply our attack in black-box scenarios, we
introduce the concept of transferability of explainability, that is, applying
explainability algorithms to different classifiers using different features
subsets and trained on different datasets still result in a similar subset of
important features. We conclude that explainability algorithms can be leveraged
by adversaries and thus the advocates of training more interpretable
classifiers should consider the trade-off of higher vulnerability of those
classifiers to adversarial attacks.
</p>
<a href="http://arxiv.org/abs/2009.13243">arXiv:2009.13243</a> [<a href="http://arxiv.org/pdf/2009.13243">pdf</a>]

<h2>Landscape of R packages for eXplainable Artificial Intelligence. (arXiv:2009.13248v1 [cs.LG])</h2>
<h3>Szymon Maksymiuk, Alicja Gosiewska, Przemyslaw Biecek</h3>
<p>The growing availability of data and computing power fuels the development of
predictive models. In order to ensure the safe and effective functioning of
such models, we need methods for exploration, debugging, and validation. New
methods and tools for this purpose are being developed within the eXplainable
Artificial Intelligence (XAI) subdomain of machine learning. In this work (1)
we present the taxonomy of methods for model explanations, (2) we identify and
compare 27 packages available in R to perform XAI analysis, (3) we present an
example of an application of particular packages, (4) we acknowledge recent
trends in XAI. The article is primarily devoted to the tools available in R,
but since it is easy to integrate the Python code, we will also show examples
for the most popular libraries from Python.
</p>
<a href="http://arxiv.org/abs/2009.13248">arXiv:2009.13248</a> [<a href="http://arxiv.org/pdf/2009.13248">pdf</a>]

<h2>Advancing the Research and Development of Assured Artificial Intelligence and Machine Learning Capabilities. (arXiv:2009.13250v1 [cs.LG])</h2>
<h3>Tyler J. Shipp, Daniel J. Clouse, Michael J. De Lucia, Metin B. Ahiskali, Kai Steverson, Jonathan M. Mullin, Nathaniel D. Bastian</h3>
<p>Artificial intelligence (AI) and machine learning (ML) have become
increasingly vital in the development of novel defense and intelligence
capabilities across all domains of warfare. An adversarial AI (A2I) and
adversarial ML (AML) attack seeks to deceive and manipulate AI/ML models. It is
imperative that AI/ML models can defend against these attacks. A2I/AML defenses
will help provide the necessary assurance of these advanced capabilities that
use AI/ML models. The A2I Working Group (A2IWG) seeks to advance the research
and development of assured AI/ML capabilities via new A2I/AML defenses by
fostering a collaborative environment across the U.S. Department of Defense and
U.S. Intelligence Community. The A2IWG aims to identify specific challenges
that it can help solve or address more directly, with initial focus on three
topics: AI Trusted Robustness, AI System Security, and AI/ML Architecture
Vulnerabilities.
</p>
<a href="http://arxiv.org/abs/2009.13250">arXiv:2009.13250</a> [<a href="http://arxiv.org/pdf/2009.13250">pdf</a>]

<h2>Deep Learning for Predictive Business Process Monitoring: Review and Benchmark. (arXiv:2009.13251v1 [cs.LG])</h2>
<h3>Efr&#xe9;n Rama-Maneiro, Juan C. Vidal, Manuel Lama</h3>
<p>Predictive monitoring of business processes is concerned with the prediction
of ongoing cases on a business process. Lately, the popularity of deep learning
techniques has propitiated an ever-growing set of approaches focused on
predictive monitoring based on these techniques. However, the high disparity of
process logs and experimental setups used to evaluate these approaches makes it
especially difficult to make a fair comparison. Furthermore, it also difficults
the selection of the most suitable approach to solve a specific problem. In
this paper, we provide both a systematic literature review of approaches that
use deep learning to tackle the predictive monitoring tasks. In addition, we
performed an exhaustive experimental evaluation of 10 different approaches over
12 publicly available process logs.
</p>
<a href="http://arxiv.org/abs/2009.13251">arXiv:2009.13251</a> [<a href="http://arxiv.org/pdf/2009.13251">pdf</a>]

<h2>BiteNet: Bidirectional Temporal Encoder Network to Predict Medical Outcomes. (arXiv:2009.13252v1 [cs.LG])</h2>
<h3>Xueping Peng, Guodong Long, Tao Shen, Sen Wang, Jing Jiang, Chengqi Zhang</h3>
<p>Electronic health records (EHRs) are longitudinal records of a patient's
interactions with healthcare systems. A patient's EHR data is organized as a
three-level hierarchy from top to bottom: patient journey - all the experiences
of diagnoses and treatments over a period of time; individual visit - a set of
medical codes in a particular visit; and medical code - a specific record in
the form of medical codes. As EHRs begin to amass in millions, the potential
benefits, which these data might hold for medical research and medical outcome
prediction, are staggering - including, for example, predicting future
admissions to hospitals, diagnosing illnesses or determining the efficacy of
medical treatments. Each of these analytics tasks requires a domain knowledge
extraction method to transform the hierarchical patient journey into a vector
representation for further prediction procedure. The representations should
embed a sequence of visits and a set of medical codes with a specific
timestamp, which are crucial to any downstream prediction tasks. Hence,
expressively powerful representations are appealing to boost learning
performance. To this end, we propose a novel self-attention mechanism that
captures the contextual dependency and temporal relationships within a
patient's healthcare journey. An end-to-end bidirectional temporal encoder
network (BiteNet) then learns representations of the patient's journeys, based
solely on the proposed attention mechanism. We have evaluated the effectiveness
of our methods on two supervised prediction and two unsupervised clustering
tasks with a real-world EHR dataset. The empirical results demonstrate the
proposed BiteNet model produces higher-quality representations than
state-of-the-art baseline methods.
</p>
<a href="http://arxiv.org/abs/2009.13252">arXiv:2009.13252</a> [<a href="http://arxiv.org/pdf/2009.13252">pdf</a>]

<h2>A Derivative-free Method for Quantum Perceptron Training in Multi-layered Neural Networks. (arXiv:2009.13264v1 [quant-ph])</h2>
<h3>Tariq M. Khan, Antonio Robles-Kelly</h3>
<p>In this paper, we present a gradient-free approach for training multi-layered
neural networks based upon quantum perceptrons. Here, we depart from the
classical perceptron and the elemental operations on quantum bits, i.e. qubits,
so as to formulate the problem in terms of quantum perceptrons. We then make
use of measurable operators to define the states of the network in a manner
consistent with a Markov process. This yields a Dirac-Von Neumann formulation
consistent with quantum mechanics. Moreover, the formulation presented here has
the advantage of having a computational efficiency devoid of the number of
layers in the network. This, paired with the natural efficiency of quantum
computing, can imply a significant improvement in efficiency, particularly for
deep networks. Finally, but not least, the developments here are quite general
in nature since the approach presented here can also be used for
quantum-inspired neural networks implemented on conventional computers.
</p>
<a href="http://arxiv.org/abs/2009.13264">arXiv:2009.13264</a> [<a href="http://arxiv.org/pdf/2009.13264">pdf</a>]

<h2>Deep Reinforcement Learning for Process Synthesis. (arXiv:2009.13265v1 [cs.LG])</h2>
<h3>Laurence Illing Midgley</h3>
<p>This paper demonstrates the application of reinforcement learning (RL) to
process synthesis by presenting Distillation Gym, a set of RL environments in
which an RL agent is tasked with designing a distillation train, given a user
defined multi-component feed stream. Distillation Gym interfaces with a process
simulator (COCO and ChemSep) to simulate the environment. A demonstration of
two distillation problem examples are discussed in this paper (a Benzene,
Toluene, P-xylene separation problem and a hydrocarbon separation problem), in
which a deep RL agent is successfully able to learn within Distillation Gym to
produce reasonable designs. Finally, this paper proposes the creation of
Chemical Engineering Gym, an all-purpose reinforcement learning software
toolkit for chemical engineering process synthesis.
</p>
<a href="http://arxiv.org/abs/2009.13265">arXiv:2009.13265</a> [<a href="http://arxiv.org/pdf/2009.13265">pdf</a>]

<h2>Embedding and generation of indoor climbing routes with variational autoencoder. (arXiv:2009.13271v1 [cs.LG])</h2>
<h3>K. H. Lo</h3>
<p>Recent increase in popularity of indoor climbing allows possible applications
of deep learning algorthms to classify and generate climbing routes. In this
work, we employ a variational autoencoder to climbing routes in a standardized
training apparatus MoonBoard, a well-known training tool within the climbing
community. By sampling the encoded latent space, it is observed that the
algorithm can generate high quality climbing routes. 22 generated problems are
uploaded to the Moonboard app for user review. This algorithm could serve as a
first step to facilitate indoor climbing route setting.
</p>
<a href="http://arxiv.org/abs/2009.13271">arXiv:2009.13271</a> [<a href="http://arxiv.org/pdf/2009.13271">pdf</a>]

<h2>Learning to Adapt Multi-View Stereo by Self-Supervision. (arXiv:2009.13278v1 [cs.CV])</h2>
<h3>Arijit Mallick, J&#xf6;rg St&#xfc;ckler, Hendrik Lensch</h3>
<p>3D scene reconstruction from multiple views is an important classical problem
in computer vision. Deep learning based approaches have recently demonstrated
impressive reconstruction results. When training such models, self-supervised
methods are favourable since they do not rely on ground truth data which would
be needed for supervised training and is often difficult to obtain. Moreover,
learned multi-view stereo reconstruction is prone to environment changes and
should robustly generalise to different domains. We propose an adaptive
learning approach for multi-view stereo which trains a deep neural network for
improved adaptability to new target domains. We use model-agnostic
meta-learning (MAML) to train base parameters which, in turn, are adapted for
multi-view stereo on new domains through self-supervised training. Our
evaluations demonstrate that the proposed adaptation method is effective in
learning self-supervised multi-view stereo reconstruction in new domains.
</p>
<a href="http://arxiv.org/abs/2009.13278">arXiv:2009.13278</a> [<a href="http://arxiv.org/pdf/2009.13278">pdf</a>]

<h2>Graph-based Multi-hop Reasoning for Long Text Generation. (arXiv:2009.13282v1 [cs.CL])</h2>
<h3>Liang Zhao, Jingjing Xu, Junyang Lin, Yichang Zhang, Hongxia Yang, Xu Sun</h3>
<p>Long text generation is an important but challenging task.The main problem
lies in learning sentence-level semantic dependencies which traditional
generative models often suffer from. To address this problem, we propose a
Multi-hop Reasoning Generation (MRG) approach that incorporates multi-hop
reasoning over a knowledge graph to learn semantic dependencies among
sentences. MRG consists of twoparts, a graph-based multi-hop reasoning module
and a path-aware sentence realization module. The reasoning module is
responsible for searching skeleton paths from a knowledge graph to imitate the
imagination process in the human writing for semantic transfer. Based on the
inferred paths, the sentence realization module then generates a complete
sentence. Unlike previous black-box models, MRG explicitly infers the skeleton
path, which provides explanatory views tounderstand how the proposed model
works. We conduct experiments on three representative tasks, including story
generation, review generation, and product description generation. Automatic
and manual evaluation show that our proposed method can generate more
informative and coherentlong text than strong baselines, such as pre-trained
models(e.g. GPT-2) and knowledge-enhanced models.
</p>
<a href="http://arxiv.org/abs/2009.13282">arXiv:2009.13282</a> [<a href="http://arxiv.org/pdf/2009.13282">pdf</a>]

<h2>The Elements of End-to-end Deep Face Recognition: A Survey of Recent Advances. (arXiv:2009.13290v1 [cs.CV])</h2>
<h3>Hang Du, Hailin Shi, Dan Zeng, Tao Mei</h3>
<p>Face recognition is one of the most fundamental and long-standing topics in
computer vision community. With the recent developments of deep convolutional
neural networks and large-scale datasets, deep face recognition has made
remarkable progress and been widely used in the real-world applications. Given
a natural image or video frame as input, an end-to-end deep face recognition
system outputs the face feature for recognition. To achieve this, the whole
system is generally built with three key elements: face detection, face
preprocessing, and face representation. The face detection locates faces in the
image or frame. Then, the face preprocessing is proceeded to calibrate the
faces to a canonical view and crop them to a normalized pixel size. Finally, in
the stage of face representation, the discriminative features are extracted
from the preprocessed faces for recognition. All of the three elements are
fulfilled by deep convolutional neural networks. In this paper, we present a
comprehensive survey about the recent advances of every element of the
end-to-end deep face recognition, since the thriving deep learning techniques
have greatly improved the capability of them. To start with, we introduce an
overview of the end-to-end deep face recognition, which, as mentioned above,
includes face detection, face preprocessing, and face representation. Then, we
review the deep learning based advances of each element, respectively, covering
many aspects such as the up-to-date algorithm designs, evaluation metrics,
datasets, performance comparison, existing challenges, and promising directions
for future research. We hope this survey could bring helpful thoughts to one
for better understanding of the big picture of end-to-end face recognition and
deeper exploration in a systematic way.
</p>
<a href="http://arxiv.org/abs/2009.13290">arXiv:2009.13290</a> [<a href="http://arxiv.org/pdf/2009.13290">pdf</a>]

<h2>Physics Informed Neural Networks for Simulating Radiative Transfer. (arXiv:2009.13291v1 [cs.LG])</h2>
<h3>Siddhartha Mishra, Roberto Molinaro</h3>
<p>We propose a novel machine learning algorithm for simulating radiative
transfer. Our algorithm is based on physics informed neural networks (PINNs),
which are trained by minimizing the residual of the underlying radiative
tranfer equations. We present extensive experiments and theoretical error
estimates to demonstrate that PINNs provide a very easy to implement, fast,
robust and accurate method for simulating radiative transfer. We also present a
PINN based algorithm for simulating inverse problems for radiative transfer
efficiently.
</p>
<a href="http://arxiv.org/abs/2009.13291">arXiv:2009.13291</a> [<a href="http://arxiv.org/pdf/2009.13291">pdf</a>]

<h2>RecoBERT: A Catalog Language Model for Text-Based Recommendations. (arXiv:2009.13292v1 [cs.IR])</h2>
<h3>Itzik Malkiel, Oren Barkan, Avi Caciularu, Noam Razin, Ori Katz, Noam Koenigstein</h3>
<p>Language models that utilize extensive self-supervised pre-training from
unlabeled text, have recently shown to significantly advance the
state-of-the-art performance in a variety of language understanding tasks.
However, it is yet unclear if and how these recent models can be harnessed for
conducting text-based recommendations. In this work, we introduce RecoBERT, a
BERT-based approach for learning catalog-specialized language models for
text-based item recommendations. We suggest novel training and inference
procedures for scoring similarities between pairs of items, that don't require
item similarity labels. Both the training and the inference techniques were
designed to utilize the unlabeled structure of textual catalogs, and minimize
the discrepancy between them. By incorporating four scores during inference,
RecoBERT can infer text-based item-to-item similarities more accurately than
other techniques. In addition, we introduce a new language understanding task
for wine recommendations using similarities based on professional wine reviews.
As an additional contribution, we publish annotated recommendations dataset
crafted by human wine experts. Finally, we evaluate RecoBERT and compare it to
various state-of-the-art NLP models on wine and fashion recommendations tasks.
</p>
<a href="http://arxiv.org/abs/2009.13292">arXiv:2009.13292</a> [<a href="http://arxiv.org/pdf/2009.13292">pdf</a>]

<h2>Virtual Proximity Citation (VCP): A Supervised Deep Learning Method to Relate Uncited Papers On Grounds of Citation Proximity. (arXiv:2009.13294v1 [cs.DL])</h2>
<h3>Rohit Rawat</h3>
<p>Citation based approaches have seen good progress for recommending research
papers using citations in the paper. Citation proximity analysis which uses the
in-text citation proximity to find relatedness between two research papers is
better than co-citation analysis and bibliographic analysis. However, one
common problem which exists in each approach is that paper should be well
cited. If documents are not cited properly or not cited at all, then using
these approaches will not be helpful. To overcome the problem, this paper
discusses the approach Virtual Citation Proximity (VCP) which uses Siamese
Neural Network along with the notion of citation proximity analysis and
content-based filtering. To train this model, the actual distance between the
two citations in a document is used as ground truth, this distance is the word
count between the two citations. VCP is trained on Wikipedia articles for which
the actual word count is available which is used to calculate the similarity
between the documents. This can be used to calculate relatedness between two
documents in a way they would have been cited in the proximity even if the
documents are uncited. This approach has shown a great improvement in
predicting proximity with basic neural networks over the approach which uses
the Average Citation Proximity index value as the ground truth. This can be
improved by using a complex neural network and proper hyper tuning of
parameters.
</p>
<a href="http://arxiv.org/abs/2009.13294">arXiv:2009.13294</a> [<a href="http://arxiv.org/pdf/2009.13294">pdf</a>]

<h2>A Diagnostic Study of Explainability Techniques for Text Classification. (arXiv:2009.13295v1 [cs.CL])</h2>
<h3>Pepa Atanasova, Jakob Grue Simonsen, Christina Lioma, Isabelle Augenstein</h3>
<p>Recent developments in machine learning have introduced models that approach
human performance at the cost of increased architectural complexity. Efforts to
make the rationales behind the models' predictions transparent have inspired an
abundance of new explainability techniques. Provided with an already trained
model, they compute saliency scores for the words of an input instance.
However, there exists no definitive guide on (i) how to choose such a technique
given a particular application task and model architecture, and (ii) the
benefits and drawbacks of using each such technique. In this paper, we develop
a comprehensive list of diagnostic properties for evaluating existing
explainability techniques. We then employ the proposed list to compare a set of
diverse explainability techniques on downstream text classification tasks and
neural network architectures. We also compare the saliency scores assigned by
the explainability techniques with human annotations of salient input regions
to find relations between a model's performance and the agreement of its
rationales with human ones. Overall, we find that the gradient-based
explanations perform best across tasks and model architectures, and we present
further insights into the properties of the reviewed explainability techniques.
</p>
<a href="http://arxiv.org/abs/2009.13295">arXiv:2009.13295</a> [<a href="http://arxiv.org/pdf/2009.13295">pdf</a>]

<h2>Learning to Match Jobs with Resumes from Sparse Interaction Data using Multi-View Co-Teaching Network. (arXiv:2009.13299v1 [cs.CL])</h2>
<h3>Shuqing Bian, Xu Chen, Wayne Xin Zhao, Kun Zhou, Yupeng Hou, Yang Song, Tao Zhang, Ji-Rong Wen</h3>
<p>With the ever-increasing growth of online recruitment data, job-resume
matching has become an important task to automatically match jobs with suitable
resumes. This task is typically casted as a supervised text matching problem.
Supervised learning is powerful when the labeled data is sufficient. However,
on online recruitment platforms, job-resume interaction data is sparse and
noisy, which affects the performance of job-resume match algorithms. To
alleviate these problems, in this paper, we propose a novel multi-view
co-teaching network from sparse interaction data for job-resume matching. Our
network consists of two major components, namely text-based matching model and
relation-based matching model. The two parts capture semantic compatibility in
two different views, and complement each other. In order to address the
challenges from sparse and noisy data, we design two specific strategies to
combine the two components. First, two components share the learned parameters
or representations, so that the original representations of each component can
be enhanced. More importantly, we adopt a co-teaching mechanism to reduce the
influence of noise in training data. The core idea is to let the two components
help each other by selecting more reliable training instances. The two
strategies focus on representation enhancement and data enhancement,
respectively. Compared with pure text-based matching models, the proposed
approach is able to learn better data representations from limited or even
sparse interaction data, which is more resistible to noise in training data.
Experiment results have demonstrated that our model is able to outperform
state-of-the-art methods for job-resume matching.
</p>
<a href="http://arxiv.org/abs/2009.13299">arXiv:2009.13299</a> [<a href="http://arxiv.org/pdf/2009.13299">pdf</a>]

<h2>Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics: a Survey. (arXiv:2009.13303v1 [cs.LG])</h2>
<h3>Wenshuai Zhao, Jorge Pe&#xf1;a Queralta, Tomi Westerlund</h3>
<p>Deep reinforcement learning has recently seen huge success across multiple
areas in the robotics domain. Owing to the limitations of gathering real-world
data, i.e., sample inefficiency and the cost of collecting it, simulation
environments are utilized for training the different agents. This not only aids
in providing a potentially infinite data source, but also alleviates safety
concerns with real robots. Nonetheless, the gap between the simulated and real
worlds degrades the performance of the policies once the models are transferred
into real robots. Multiple research efforts are therefore now being directed
towards closing this sim-to-real gap and accomplish more efficient policy
transfer. Recent years have seen the emergence of multiple methods applicable
to different domains, but there is a lack, to the best of our knowledge, of a
comprehensive review summarizing and putting into context the different
methods. In this survey paper, we cover the fundamental background behind
sim-to-real transfer in deep reinforcement learning and overview the main
methods being utilized at the moment: domain randomization, domain adaptation,
imitation learning, meta-learning and knowledge distillation. We categorize
some of the most relevant recent works, and outline the main application
scenarios. Finally, we discuss the main opportunities and challenges of the
different approaches and point to the most promising directions.
</p>
<a href="http://arxiv.org/abs/2009.13303">arXiv:2009.13303</a> [<a href="http://arxiv.org/pdf/2009.13303">pdf</a>]

<h2>High-throughput molecular imaging via deep learning enabled Raman spectroscopy. (arXiv:2009.13318v1 [eess.IV])</h2>
<h3>Conor C. Horgan, Magnus Jensen, Anika Nagelkerke, Jean-Phillipe St-Pierre, Tom Vercauteren, Molly M. Stevens, Mads S. Bergholt</h3>
<p>Raman spectroscopy enables non-destructive, label-free imaging with
unprecedented molecular contrast but is limited by slow data acquisition,
largely preventing high-throughput imaging applications. Here, we present a
comprehensive framework for higher-throughput molecular imaging via deep
learning enabled Raman spectroscopy, termed DeepeR, trained on a large dataset
of hyperspectral Raman images, with over 1.5 million spectra (400 hours of
acquisition) in total. We firstly perform denoising and reconstruction of low
signal-to-noise ratio Raman molecular signatures via deep learning, with a 9x
improvement in mean squared error over state-of-the-art Raman filtering
methods. Next, we develop a neural network for robust 2-4x super-resolution of
hyperspectral Raman images that preserves molecular cellular information.
Combining these approaches, we achieve Raman imaging speed-ups of up to 160x,
enabling high resolution, high signal-to-noise ratio cellular imaging in under
one minute. Finally, transfer learning is applied to extend DeepeR from cell to
tissue-scale imaging. DeepeR provides a foundation that will enable a host of
higher-throughput Raman spectroscopy and molecular imaging applications across
biomedicine.
</p>
<a href="http://arxiv.org/abs/2009.13318">arXiv:2009.13318</a> [<a href="http://arxiv.org/pdf/2009.13318">pdf</a>]

<h2>Aircraft Fuselage Defect Detection using Deep Neural Networks. (arXiv:1712.09213v2 [cs.CV] UPDATED)</h2>
<h3>Touba Malekzadeh, Milad Abdollahzadeh, Hossein Nejati, Ngai-Man Cheung</h3>
<p>To ensure flight safety of aircraft structures, it is necessary to have
regular maintenance using visual and nondestructive inspection (NDI) methods.
In this paper, we propose an automatic image-based aircraft defect detection
using Deep Neural Networks (DNNs). To the best of our knowledge, this is the
first work for aircraft defect detection using DNNs. We perform a comprehensive
evaluation of state-of-the-art feature descriptors and show that the best
performance is achieved by vgg-f DNN as feature extractor with a linear SVM
classifier. To reduce the processing time, we propose to apply SURF key point
detector to identify defect patch candidates. Our experiment results suggest
that we can achieve over 96% accuracy at around 15s processing time for a
high-resolution (20-megapixel) image on a laptop.
</p>
<a href="http://arxiv.org/abs/1712.09213">arXiv:1712.09213</a> [<a href="http://arxiv.org/pdf/1712.09213">pdf</a>]

<h2>A Divide-and-Conquer Approach to Geometric Sampling for Active Learning. (arXiv:1805.12321v3 [cs.LG] UPDATED)</h2>
<h3>Xiaofeng Cao</h3>
<p>Active learning (AL) repeatedly trains the classifier with the minimum
labeling budget to improve the current classification model. The training
process is usually supervised by an uncertainty evaluation strategy. However,
the uncertainty evaluation always suffers from performance degeneration when
the initial labeled set has insufficient labels. To completely eliminate the
dependence on the uncertainty evaluation sampling in AL, this paper proposes a
divide-and-conquer idea that directly transfers the AL sampling as the
geometric sampling over the clusters. By dividing the points of the clusters
into cluster boundary and core points, we theoretically discuss their margin
distance and {hypothesis relationship}. With the advantages of cluster boundary
points in the above two properties, we propose a Geometric Active Learning
(GAL) algorithm by knight's tour. Experimental studies of the two reported
experimental tasks including cluster boundary detection and AL classification
show that the proposed GAL method significantly outperforms the
state-of-the-art baselines.
</p>
<a href="http://arxiv.org/abs/1805.12321">arXiv:1805.12321</a> [<a href="http://arxiv.org/pdf/1805.12321">pdf</a>]

<h2>A Structured Perspective of Volumes on Active Learning. (arXiv:1807.08904v2 [cs.LG] UPDATED)</h2>
<h3>Xiaofeng Cao, Ivor W. Tsang, Guandong Xu</h3>
<p>Active Learning (AL) is a learning task that requires learners interactively
query the labels of the sampled unlabeled instances to minimize the training
outputs with human supervisions. In theoretical study, learners approximate the
version space which covers all possible classification hypothesis into a
bounded convex body and try to shrink the volume of it into a half-space by a
given cut size. However, only the hypersphere with finite VC dimensions has
obtained formal approximation guarantees that hold when the classes of
Euclidean space are separable with a margin. In this paper, we approximate the
version space to a structured {hypersphere} that covers most of the hypotheses,
and then divide the available AL sampling approaches into two kinds of
strategies: Outer Volume Sampling and Inner Volume Sampling. After providing
provable guarantees for the performance of AL in version space, we aggregate
the two kinds of volumes to eliminate their sampling biases via finding the
optimal inscribed hyperspheres in the enclosing space of outer volume. To touch
the version space from Euclidean space, we propose a theoretical bridge called
Volume-based Model that increases the `sampling target-independent'. In
non-linear feature space, spanned by kernel, we use sequential optimization to
globally optimize the original space to a sparse space by halving the size of
the kernel space. Then, the EM (Expectation Maximization) model which returns
the local center helps us to find a local representation. To describe this
process, we propose an easy-to-implement algorithm called Volume-based AL
(VAL).
</p>
<a href="http://arxiv.org/abs/1807.08904">arXiv:1807.08904</a> [<a href="http://arxiv.org/pdf/1807.08904">pdf</a>]

<h2>Content-based Propagation of User Markings for Interactive Segmentation of Patterned Images. (arXiv:1809.02226v3 [cs.CV] UPDATED)</h2>
<h3>Vedrana Andersen Dahl, Monica Jane Emerson, Camilla Himmelstrup Trinderup, Anders Bjorholm Dahl</h3>
<p>Efficient and easy segmentation of images and volumes is of great practical
importance. Segmentation problems that motivate our approach originate from
microscopy imaging commonly used in materials science, medicine, and biology.
We formulate image segmentation as a probabilistic pixel classification
problem, and we apply segmentation as a step towards characterising image
content. Our method allows the user to define structures of interest by
interactively marking a subset of pixels. Thanks to the real-time feedback, the
user can place new markings strategically, depending on the current outcome.
The final pixel classification may be obtained from a very modest user input.
An important ingredient of our method is a graph that encodes image content.
This graph is built in an unsupervised manner during initialisation and is
based on clustering of image features. Since we combine a limited amount of
user-labelled data with the clustering information obtained from the unlabelled
parts of the image, our method fits in the general framework of semi-supervised
learning. We demonstrate how this can be a very efficient approach to
segmentation through pixel classification.
</p>
<a href="http://arxiv.org/abs/1809.02226">arXiv:1809.02226</a> [<a href="http://arxiv.org/pdf/1809.02226">pdf</a>]

<h2>Target-Independent Active Learning via Distribution-Splitting. (arXiv:1809.10962v2 [cs.LG] UPDATED)</h2>
<h3>Xiaofeng Cao, Ivor W. Tsang, Xiaofeng Xu, Guandong Xu</h3>
<p>To reduce the label complexity in Agnostic Active Learning (A^2 algorithm),
volume-splitting splits the hypothesis edges to reduce the Vapnik-Chervonenkis
(VC) dimension in version space. However, the effectiveness of volume-splitting
critically depends on the initial hypothesis and this problem is also known as
target-dependent label complexity gap. This paper attempts to minimize this gap
by introducing a novel notion of number density which provides a more natural
and direct way to describe the hypothesis distribution than volume. By
discovering the connections between hypothesis and input distribution, we map
the volume of version space into the number density and propose a
target-independent distribution-splitting strategy with the following
advantages: 1) provide theoretical guarantees on reducing label complexity and
error rate as volume-splitting; 2) break the curse of initial hypothesis; 3)
provide model guidance for a target-independent AL algorithm in real AL tasks.
With these guarantees, for AL application, we then split the input distribution
into more near-optimal spheres and develop an application algorithm called
Distribution-based A^2 (DA^2). Experiments further verify the effectiveness of
the halving and querying abilities of DA^2. Contributions of this paper are as
follows.
</p>
<a href="http://arxiv.org/abs/1809.10962">arXiv:1809.10962</a> [<a href="http://arxiv.org/pdf/1809.10962">pdf</a>]

<h2>Wikipedia2Vec: An Efficient Toolkit for Learning and Visualizing the Embeddings of Words and Entities from Wikipedia. (arXiv:1812.06280v4 [cs.CL] UPDATED)</h2>
<h3>Ikuya Yamada, Akari Asai, Jin Sakuma, Hiroyuki Shindo, Hideaki Takeda, Yoshiyasu Takefuji, Yuji Matsumoto</h3>
<p>The embeddings of entities in a large knowledge base (e.g., Wikipedia) are
highly beneficial for solving various natural language tasks that involve real
world knowledge. In this paper, we present Wikipedia2Vec, a Python-based
open-source tool for learning the embeddings of words and entities from
Wikipedia. The proposed tool enables users to learn the embeddings efficiently
by issuing a single command with a Wikipedia dump file as an argument. We also
introduce a web-based demonstration of our tool that allows users to visualize
and explore the learned embeddings. In our experiments, our tool achieved a
state-of-the-art result on the KORE entity relatedness dataset, and competitive
results on various standard benchmark datasets. Furthermore, our tool has been
used as a key component in various recent studies. We publicize the source
code, demonstration, and the pretrained embeddings for 12 languages at
https://wikipedia2vec.github.io.
</p>
<a href="http://arxiv.org/abs/1812.06280">arXiv:1812.06280</a> [<a href="http://arxiv.org/pdf/1812.06280">pdf</a>]

<h2>Recurrent Neural Filters: Learning Independent Bayesian Filtering Steps for Time Series Prediction. (arXiv:1901.08096v6 [stat.ML] UPDATED)</h2>
<h3>Bryan Lim, Stefan Zohren, Stephen Roberts</h3>
<p>Despite the recent popularity of deep generative state space models, few
comparisons have been made between network architectures and the inference
steps of the Bayesian filtering framework -- with most models simultaneously
approximating both state transition and update steps with a single recurrent
neural network (RNN). In this paper, we introduce the Recurrent Neural Filter
(RNF), a novel recurrent autoencoder architecture that learns distinct
representations for each Bayesian filtering step, captured by a series of
encoders and decoders. Testing this on three real-world time series datasets,
we demonstrate that the decoupled representations learnt not only improve the
accuracy of one-step-ahead forecasts while providing realistic uncertainty
estimates, but also facilitate multistep prediction through the separation of
encoder stages.
</p>
<a href="http://arxiv.org/abs/1901.08096">arXiv:1901.08096</a> [<a href="http://arxiv.org/pdf/1901.08096">pdf</a>]

<h2>Unsupervised Multi-modal Hashing for Cross-modal retrieval. (arXiv:1904.00726v4 [cs.CV] UPDATED)</h2>
<h3>Jun Yu, Xiao-Jun Wu</h3>
<p>With the advantage of low storage cost and high efficiency, hashing learning
has received much attention in the domain of Big Data. In this paper, we
propose a novel unsupervised hashing learning method to cope with this open
problem to directly preserve the manifold structure by hashing. To address this
problem, both the semantic correlation in textual space and the locally
geometric structure in the visual space are explored simultaneously in our
framework. Besides, the `2;1-norm constraint is imposed on the projection
matrices to learn the discriminative hash function for each modality. Extensive
experiments are performed to evaluate the proposed method on the three publicly
available datasets and the experimental results show that our method can
achieve superior performance over the state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/1904.00726">arXiv:1904.00726</a> [<a href="http://arxiv.org/pdf/1904.00726">pdf</a>]

<h2>Enhancing Time Series Momentum Strategies Using Deep Neural Networks. (arXiv:1904.04912v3 [stat.ML] UPDATED)</h2>
<h3>Bryan Lim, Stefan Zohren, Stephen Roberts</h3>
<p>While time series momentum is a well-studied phenomenon in finance, common
strategies require the explicit definition of both a trend estimator and a
position sizing rule. In this paper, we introduce Deep Momentum Networks -- a
hybrid approach which injects deep learning based trading rules into the
volatility scaling framework of time series momentum. The model also
simultaneously learns both trend estimation and position sizing in a
data-driven manner, with networks directly trained by optimising the Sharpe
ratio of the signal. Backtesting on a portfolio of 88 continuous futures
contracts, we demonstrate that the Sharpe-optimised LSTM improved traditional
methods by more than two times in the absence of transactions costs, and
continue outperforming when considering transaction costs up to 2-3 basis
points. To account for more illiquid assets, we also propose a turnover
regularisation term which trains the network to factor in costs at run-time.
</p>
<a href="http://arxiv.org/abs/1904.04912">arXiv:1904.04912</a> [<a href="http://arxiv.org/pdf/1904.04912">pdf</a>]

<h2>Gotta Catch 'Em All: Using Honeypots to Catch Adversarial Attacks on Neural Networks. (arXiv:1904.08554v6 [cs.LG] UPDATED)</h2>
<h3>Shawn Shan, Emily Wenger, Bolun Wang, Bo Li, Haitao Zheng, Ben Y. Zhao</h3>
<p>Deep neural networks (DNN) are known to be vulnerable to adversarial attacks.
Numerous efforts either try to patch weaknesses in trained models, or try to
make it difficult or costly to compute adversarial examples that exploit them.
In our work, we explore a new "honeypot" approach to protect DNN models. We
intentionally inject trapdoors, honeypot weaknesses in the classification
manifold that attract attackers searching for adversarial examples. Attackers'
optimization algorithms gravitate towards trapdoors, leading them to produce
attacks similar to trapdoors in the feature space. Our defense then identifies
attacks by comparing neuron activation signatures of inputs to those of
trapdoors. In this paper, we introduce trapdoors and describe an implementation
of a trapdoor-enabled defense. First, we analytically prove that trapdoors
shape the computation of adversarial attacks so that attack inputs will have
feature representations very similar to those of trapdoors. Second, we
experimentally show that trapdoor-protected models can detect, with high
accuracy, adversarial examples generated by state-of-the-art attacks (PGD,
optimization-based CW, Elastic Net, BPDA), with negligible impact on normal
classification. These results generalize across classification domains,
including image, facial, and traffic-sign recognition. We also present
significant results measuring trapdoors' robustness against customized adaptive
attacks (countermeasures).
</p>
<a href="http://arxiv.org/abs/1904.08554">arXiv:1904.08554</a> [<a href="http://arxiv.org/pdf/1904.08554">pdf</a>]

<h2>Biometric Backdoors: A Poisoning Attack Against Unsupervised Template Updating. (arXiv:1905.09162v2 [cs.CR] UPDATED)</h2>
<h3>Giulio Lovisotto, Simon Eberz, Ivan Martinovic</h3>
<p>In this work, we investigate the concept of biometric backdoors: a template
poisoning attack on biometric systems that allows adversaries to stealthily and
effortlessly impersonate users in the long-term by exploiting the template
update procedure. We show that such attacks can be carried out even by
attackers with physical limitations (no digital access to the sensor) and zero
knowledge of training data (they know neither decision boundaries nor user
template). Based on the adversaries' own templates, they craft several
intermediate samples that incrementally bridge the distance between their own
template and the legitimate user's. As these adversarial samples are added to
the template, the attacker is eventually accepted alongside the legitimate
user. To avoid detection, we design the attack to minimize the number of
rejected samples.

We design our method to cope with the weak assumptions for the attacker and
we evaluate the effectiveness of this approach on state-of-the-art face
recognition pipelines based on deep neural networks. We find that in scenarios
where the deep network is known, adversaries can successfully carry out the
attack over 70% of cases with less than ten injection attempts. Even in
black-box scenarios, we find that exploiting the transferability of adversarial
samples from surrogate models can lead to successful attacks in around 15% of
cases. Finally, we design a poisoning detection technique that leverages the
consistent directionality of template updates in feature space to discriminate
between legitimate and malicious updates. We evaluate such a countermeasure
with a set of intra-user variability factors which may present the same
directionality characteristics, obtaining equal error rates for the detection
between 7-14% and leading to over 99% of attacks being detected after only two
sample injections.
</p>
<a href="http://arxiv.org/abs/1905.09162">arXiv:1905.09162</a> [<a href="http://arxiv.org/pdf/1905.09162">pdf</a>]

<h2>Adaptive norms for deep learning with regularized Newton methods. (arXiv:1905.09201v4 [cs.LG] UPDATED)</h2>
<h3>Jonas Kohler, Leonard Adolphs, Aurelien Lucchi</h3>
<p>We investigate the use of regularized Newton methods with adaptive norms for
optimizing neural networks. This approach can be seen as a second-order
counterpart of adaptive gradient methods, which we here show to be
interpretable as first-order trust region methods with ellipsoidal constraints.
In particular, we prove that the preconditioning matrix used in RMSProp and
Adam satisfies the necessary conditions for provable convergence of
second-order trust region methods with standard worst-case complexities on
general non-convex objectives. Furthermore, we run experiments across different
neural architectures and datasets to find that the ellipsoidal constraints
constantly outperform their spherical counterpart both in terms of number of
backpropagations and asymptotic loss value. Finally, we find comparable
performance to state-of-the-art first-order methods in terms of
backpropagations, but further advances in hardware are needed to render Newton
methods competitive in terms of computational time.
</p>
<a href="http://arxiv.org/abs/1905.09201">arXiv:1905.09201</a> [<a href="http://arxiv.org/pdf/1905.09201">pdf</a>]

<h2>Privacy-Preserving Deep Action Recognition: An Adversarial Learning Framework and A New Dataset. (arXiv:1906.05675v3 [cs.CV] UPDATED)</h2>
<h3>Zhenyu Wu, Haotao Wang, Zhaowen Wang, Hailin Jin, Zhangyang Wang</h3>
<p>We investigate privacy-preserving, video-based action recognition in deep
learning, a problem with growing importance in smart camera applications. A
novel adversarial training framework is formulated to learn an anonymization
transform for input videos such that the trade-off between target utility task
performance and the associated privacy budgets is explicitly optimized on the
anonymized videos. Notably, the privacy budget, often defined and measured in
task-driven contexts, cannot be reliably indicated using any single model
performance because strong protection of privacy should sustain against any
malicious model that tries to steal private information. To tackle this
problem, we propose two new optimization strategies of model restarting and
model ensemble to achieve stronger universal privacy protection against any
attacker models. Extensive experiments have been carried out and analyzed. On
the other hand, given few public datasets available with both utility and
privacy labels, the data-driven (supervised) learning cannot exert its full
power on this task. We first discuss an innovative heuristic of cross-dataset
training and evaluation, enabling the use of multiple single-task datasets (one
with target task labels and the other with privacy labels) in our problem. To
further address this dataset challenge, we have constructed a new dataset,
termed PA-HMDB51, with both target task labels (action) and selected privacy
attributes (gender, age, race, nudity, and relationship) annotated on a
per-frame basis. This first-of-its-kind video dataset and evaluation protocol
can greatly facilitate visual privacy research and open up other opportunities.
Our codes, models, and the PA-HMDB51 dataset are available at:
https://github.com/VITA-Group/PA-HMDB51.
</p>
<a href="http://arxiv.org/abs/1906.05675">arXiv:1906.05675</a> [<a href="http://arxiv.org/pdf/1906.05675">pdf</a>]

<h2>End-to-end Recurrent Multi-Object Tracking and Trajectory Prediction with Relational Reasoning. (arXiv:1907.12887v5 [cs.CV] UPDATED)</h2>
<h3>Fabian B. Fuchs, Adam R. Kosiorek, Li Sun, Oiwi Parker Jones, Ingmar Posner</h3>
<p>The majority of contemporary object-tracking approaches do not model
interactions between objects. This contrasts with the fact that objects' paths
are not independent: a cyclist might abruptly deviate from a previously planned
trajectory in order to avoid colliding with a car. Building upon HART, a neural
class-agnostic single-object tracker, we introduce a multi-object tracking
method MOHART capable of relational reasoning. Importantly, the entire system,
including the understanding of interactions and relations between objects, is
class-agnostic and learned simultaneously in an end-to-end fashion. We explore
a number of relational reasoning architectures and show that
permutation-invariant models outperform non-permutation-invariant alternatives.
We also find that architectures using a single permutation invariant operation
like DeepSets, despite, in theory, being universal function approximators, are
nonetheless outperformed by a more complex architecture based on multi-headed
attention. The latter better accounts for complex physical interactions in a
challenging toy experiment. Further, we find that modelling interactions leads
to consistent performance gains in tracking as well as future trajectory
prediction on three real-world datasets (MOTChallenge, UA-DETRAC, and Stanford
Drone dataset), particularly in the presence of ego-motion, occlusions, crowded
scenes, and faulty sensor inputs.
</p>
<a href="http://arxiv.org/abs/1907.12887">arXiv:1907.12887</a> [<a href="http://arxiv.org/pdf/1907.12887">pdf</a>]

<h2>Februus: Input Purification Defense Against Trojan Attacks on Deep Neural Network Systems. (arXiv:1908.03369v7 [cs.CR] UPDATED)</h2>
<h3>Bao Gia Doan, Ehsan Abbasnejad, Damith C. Ranasinghe</h3>
<p>We propose Februus; a new idea to neutralize highly potent and insidious
Trojan attacks on Deep Neural Network (DNN) systems at run-time. In Trojan
attacks, an adversary activates a backdoor crafted in a deep neural network
model using a secret trigger, a Trojan, applied to any input to alter the
model's decision to a target prediction---a target determined by and only known
to the attacker. Februus sanitizes the incoming input by surgically removing
the potential trigger artifacts and restoring the input for the classification
task. Februus enables effective Trojan mitigation by sanitizing inputs with no
loss of performance for sanitized inputs, Trojaned or benign. Our extensive
evaluations on multiple infected models based on four popular datasets across
three contrasting vision applications and trigger types demonstrate the high
efficacy of Februus. We dramatically reduced attack success rates from 100% to
near 0% for all cases (achieving 0% on multiple cases) and evaluated the
generalizability of Februus to defend against complex adaptive attacks;
notably, we realized the first defense against the advanced partial Trojan
attack. To the best of our knowledge, Februus is the first backdoor defense
method for operation at run-time capable of sanitizing Trojaned inputs without
requiring anomaly detection methods, model retraining or costly labeled data.
</p>
<a href="http://arxiv.org/abs/1908.03369">arXiv:1908.03369</a> [<a href="http://arxiv.org/pdf/1908.03369">pdf</a>]

<h2>Adversarial shape perturbations on 3D point clouds. (arXiv:1908.06062v2 [cs.CV] UPDATED)</h2>
<h3>Daniel Liu, Ronald Yu, Hao Su</h3>
<p>The importance of training robust neural network grows as 3D data is
increasingly utilized in deep learning for vision tasks in robotics, drone
control, and autonomous driving. One commonly used 3D data type is 3D point
clouds, which describe shape information. We examine the problem of creating
robust models from the perspective of the attacker, which is necessary in
understanding how 3D neural networks can be exploited. We explore two
categories of attacks: distributional attacks that involve imperceptible
perturbations to the distribution of points, and shape attacks that involve
deforming the shape represented by a point cloud. We explore three possible
shape attacks for attacking 3D point cloud classification and show that some of
them are able to be effective even against preprocessing steps, like the
previously proposed point-removal defenses.
</p>
<a href="http://arxiv.org/abs/1908.06062">arXiv:1908.06062</a> [<a href="http://arxiv.org/pdf/1908.06062">pdf</a>]

<h2>OpenSpiel: A Framework for Reinforcement Learning in Games. (arXiv:1908.09453v6 [cs.LG] UPDATED)</h2>
<h3>Marc Lanctot, Edward Lockhart, Jean-Baptiste Lespiau, Vinicius Zambaldi, Satyaki Upadhyay, Julien P&#xe9;rolat, Sriram Srinivasan, Finbarr Timbers, Karl Tuyls, Shayegan Omidshafiei, Daniel Hennes, Dustin Morrill, Paul Muller, Timo Ewalds, Ryan Faulkner, J&#xe1;nos Kram&#xe1;r, Bart De Vylder, Brennan Saeta, James Bradbury, David Ding, Sebastian Borgeaud, Matthew Lai, Julian Schrittwieser, Thomas Anthony, Edward Hughes, Ivo Danihelka, Jonah Ryan-Davis</h3>
<p>OpenSpiel is a collection of environments and algorithms for research in
general reinforcement learning and search/planning in games. OpenSpiel supports
n-player (single- and multi- agent) zero-sum, cooperative and general-sum,
one-shot and sequential, strictly turn-taking and simultaneous-move, perfect
and imperfect information games, as well as traditional multiagent environments
such as (partially- and fully- observable) grid worlds and social dilemmas.
OpenSpiel also includes tools to analyze learning dynamics and other common
evaluation metrics. This document serves both as an overview of the code base
and an introduction to the terminology, core concepts, and algorithms across
the fields of reinforcement learning, computational game theory, and search.
</p>
<a href="http://arxiv.org/abs/1908.09453">arXiv:1908.09453</a> [<a href="http://arxiv.org/pdf/1908.09453">pdf</a>]

<h2>Deep Coarse-to-fine Dense Light Field Reconstruction with Flexible Sampling and Geometry-aware Fusion. (arXiv:1909.01341v3 [eess.IV] UPDATED)</h2>
<h3>Jing Jin, Junhui Hou, Jie Chen, Huanqiang Zeng, Sam Kwong, Jingyi Yu</h3>
<p>A densely-sampled light field (LF) is highly desirable in various
applications, such as 3-D reconstruction, post-capture refocusing and virtual
reality. However, it is costly to acquire such data. Although many
computational methods have been proposed to reconstruct a densely-sampled LF
from a sparsely-sampled one, they still suffer from either low reconstruction
quality, low computational efficiency, or the restriction on the regularity of
the sampling pattern. To this end, we propose a novel learning-based method,
which accepts sparsely-sampled LFs with irregular structures, and produces
densely-sampled LFs with arbitrary angular resolution accurately and
efficiently. We also propose a simple yet effective method for optimizing the
sampling pattern. Our proposed method, an end-to-end trainable network,
reconstructs a densely-sampled LF in a coarse-to-fine manner. Specifically, the
coarse sub-aperture image (SAI) synthesis module first explores the scene
geometry from an unstructured sparsely-sampled LF and leverages it to
independently synthesize novel SAIs, in which a confidence-based blending
strategy is proposed to fuse the information from different input SAIs, giving
an intermediate densely-sampled LF. Then, the efficient LF refinement module
learns the angular relationship within the intermediate result to recover the
LF parallax structure. Comprehensive experimental evaluations demonstrate the
superiority of our method on both real-world and synthetic LF images when
compared with state-of-the-art methods. In addition, we illustrate the benefits
and advantages of the proposed approach when applied in various LF-based
applications, including image-based rendering and depth estimation enhancement.
</p>
<a href="http://arxiv.org/abs/1909.01341">arXiv:1909.01341</a> [<a href="http://arxiv.org/pdf/1909.01341">pdf</a>]

<h2>Data-efficient Model Learning and Prediction for Contact-rich Manipulation Tasks. (arXiv:1909.04915v3 [cs.RO] UPDATED)</h2>
<h3>Shahbaz Abdul Khader, Hang Yin, Pietro Falco, Danica Kragic</h3>
<p>In this letter, we investigate learning forward dynamics models and
multi-step prediction of state variables (long-term prediction) for
contact-rich manipulation. The problems are formulated in the context of
model-based reinforcement learning (MBRL). We focus on two
aspects-discontinuous dynamics and data-efficiency-both of which are important
in the identified scope and pose significant challenges to State-of-the-Art
methods. We contribute to closing this gap by proposing a method that
explicitly adopts a specific hybrid structure for the model while leveraging
the uncertainty representation and data-efficiency of Gaussian process. Our
experiments on an illustrative moving block task and a 7-DOF robot demonstrate
a clear advantage when compared to popular baselines in low data regimes.
</p>
<a href="http://arxiv.org/abs/1909.04915">arXiv:1909.04915</a> [<a href="http://arxiv.org/pdf/1909.04915">pdf</a>]

<h2>Part-Guided Attention Learning for Vehicle Instance Retrieval. (arXiv:1909.06023v4 [cs.CV] UPDATED)</h2>
<h3>Xinyu Zhang, Rufeng Zhang, Jiewei Cao, Dong Gong, Mingyu You, Chunhua Shen</h3>
<p>Vehicle instance retrieval often requires one to recognize the fine-grained
visual differences between vehicles. Besides the holistic appearance of
vehicles which is easily affected by the viewpoint variation and distortion,
vehicle parts also provide crucial cues to differentiate near-identical
vehicles. Motivated by these observations, we introduce a Part-Guided Attention
Network (PGAN) to pinpoint the prominent part regions and effectively combine
the global and part information for discriminative feature learning. PGAN first
detects the locations of different part components and salient regions
regardless of the vehicle identity, which serve as the bottom-up attention to
narrow down the possible searching regions. To estimate the importance of
detected parts, we propose a Part Attention Module (PAM) to adaptively locate
the most discriminative regions with high-attention weights and suppress the
distraction of irrelevant parts with relatively low weights. The PAM is guided
by the instance retrieval loss and therefore provides top-down attention that
enables attention to be calculated at the level of car parts and other salient
regions. Finally, we aggregate the global appearance and part features to
improve the feature performance further. The PGAN combines part-guided
bottom-up and top-down attention, global and part visual features in an
end-to-end framework. Extensive experiments demonstrate that the proposed
method achieves new state-of-the-art vehicle instance retrieval performance on
four large-scale benchmark datasets.
</p>
<a href="http://arxiv.org/abs/1909.06023">arXiv:1909.06023</a> [<a href="http://arxiv.org/pdf/1909.06023">pdf</a>]

<h2>Student Engagement Detection Using Emotion Analysis, Eye Tracking and Head Movement with Machine Learning. (arXiv:1909.12913v3 [cs.CV] UPDATED)</h2>
<h3>Prabin Sharma, Shubham Joshi, Subash Gautam, Sneha Maharjan, Vitor Filipe, Manuel J. C. S. Reis</h3>
<p>With the increase of distance learning, in general, and e-learning, in
particular, having a system capable of determining the engagement of students
is of primordial importance, and one of the biggest challenges, both for
teachers, researchers and policy makers. Here, we present a system to detect
the engagement level of the students. It uses only information provided by the
typical built-in web-camera present in a laptop computer, and was designed to
work in real time. We combine information about the movements of the eyes and
head, and facial emotions to produce a concentration index with three classes
of engagement: "very engaged", "nominally engaged" and "not engaged at all".
The system was tested in a typical e-learning scenario, and the results show
that it correctly identifies each period of time where students were "very
engaged", "nominally engaged" and "not engaged at all". Additionally, the
results also show that the students with best scores also have higher
concentration indexes.
</p>
<a href="http://arxiv.org/abs/1909.12913">arXiv:1909.12913</a> [<a href="http://arxiv.org/pdf/1909.12913">pdf</a>]

<h2>Deep Latent Defence. (arXiv:1910.03916v2 [cs.LG] UPDATED)</h2>
<h3>Giulio Zizzo, Chris Hankin, Sergio Maffeis, Kevin Jones</h3>
<p>Deep learning methods have shown state of the art performance in a range of
tasks from computer vision to natural language processing. However, it is well
known that such systems are vulnerable to attackers who craft inputs in order
to cause misclassification. The level of perturbation an attacker needs to
introduce in order to cause such a misclassification can be extremely small,
and often imperceptible. This is of significant security concern, particularly
where misclassification can cause harm to humans.

We thus propose Deep Latent Defence, an architecture which seeks to combine
adversarial training with a detection system. At its core Deep Latent Defence
has a adversarially trained neural network. A series of encoders take the
intermediate layer representation of data as it passes though the network and
project it to a latent space which we use for detecting adversarial samples via
a $k$-nn classifier. We present results using both grey and white box
attackers, as well as an adaptive $L_{\infty}$ bounded attack which was
constructed specifically to try and evade our defence. We find that even under
the strongest attacker model that we have investigated our defence is able to
offer significant defensive benefits.
</p>
<a href="http://arxiv.org/abs/1910.03916">arXiv:1910.03916</a> [<a href="http://arxiv.org/pdf/1910.03916">pdf</a>]

<h2>Pathological spectra of the Fisher information metric and its variants in deep neural networks. (arXiv:1910.05992v2 [stat.ML] UPDATED)</h2>
<h3>Ryo Karakida, Shotaro Akaho, Shun-ichi Amari</h3>
<p>The Fisher information matrix (FIM) plays an essential role in statistics and
machine learning as a Riemannian metric tensor or a component of the Hessian
matrix of loss functions. Focusing on the FIM and its variants in deep neural
networks (DNNs), we reveal their characteristic scale dependence on the network
width, depth and sample size when the network has random weights and is
sufficiently wide. This study covers two widely-used FIMs for regression with
linear output and for classification with softmax output. Both FIMs
asymptotically show pathological eigenvalue spectra in the sense that a small
number of eigenvalues become large outliers depending the width or sample size
while the others are much smaller. It implies that the local shape of the
parameter space or loss landscape is very sharp in a few specific directions
while almost flat in the other directions. In particular, the softmax output
disperses the outliers and makes a tail of the eigenvalue density spread from
the bulk. We also show that pathological spectra appear in other variants of
FIMs: one is the neural tangent kernel; another is a metric for the input
signal and feature space that arises from feedforward signal propagation. Thus,
we provide a unified perspective on the FIM and its variants that will lead to
more quantitative understanding of learning in large-scale DNNs.
</p>
<a href="http://arxiv.org/abs/1910.05992">arXiv:1910.05992</a> [<a href="http://arxiv.org/pdf/1910.05992">pdf</a>]

<h2>ODE guided Neural Data Augmentation Techniques for Time Series Data and its Benefits on Robustness. (arXiv:1910.06813v3 [cs.LG] UPDATED)</h2>
<h3>Anindya Sarkar, Anirudh Sunder Raj, Raghu Sesha Iyengar</h3>
<p>Exploring adversarial attack vectors and studying their effects on machine
learning algorithms has been of interest to researchers. Deep neural networks
working with time series data have received lesser interest compared to their
image counterparts in this context. In a recent finding, it has been revealed
that current state-of-the-art deep learning time series classifiers are
vulnerable to adversarial attacks. In this paper, we introduce two local
gradient based and one spectral density based time series data augmentation
techniques. We show that a model trained with data obtained using our
techniques obtains state-of-the-art classification accuracy on various time
series benchmarks. In addition, it improves the robustness of the model against
some of the most common corruption techniques,such as Fast Gradient Sign Method
(FGSM) and Basic Iterative Method (BIM).
</p>
<a href="http://arxiv.org/abs/1910.06813">arXiv:1910.06813</a> [<a href="http://arxiv.org/pdf/1910.06813">pdf</a>]

<h2>Empirical Analysis of Session-Based Recommendation Algorithms. (arXiv:1910.12781v2 [cs.IR] UPDATED)</h2>
<h3>Malte Ludewig, Noemi Mauro, Sara Latifi, Dietmar Jannach</h3>
<p>Recommender systems are tools that support online users by pointing them to
potential items of interest in situations of information overload. In recent
years, the class of session-based recommendation algorithms received more
attention in the research literature. These algorithms base their
recommendations solely on the observed interactions with the user in an ongoing
session and do not require the existence of long-term preference profiles. Most
recently, a number of deep learning based ("neural") approaches to
session-based recommendations were proposed. However, previous research
indicates that today's complex neural recommendation methods are not always
better than comparably simple algorithms in terms of prediction accuracy.

With this work, our goal is to shed light on the state-of-the-art in the area
of session-based recommendation and on the progress that is made with neural
approaches. For this purpose, we compare twelve algorithmic approaches, among
them six recent neural methods, under identical conditions on various datasets.
We find that the progress in terms of prediction accuracy that is achieved with
neural methods is still limited. In most cases, our experiments show that
simple heuristic methods based on nearest-neighbors schemes are preferable over
conceptually and computationally more complex methods. Observations from a user
study furthermore indicate that recommendations based on heuristic methods were
also well accepted by the study participants. To support future progress and
reproducibility in this area, we publicly share the session-rec evaluation
framework that was used in our research.
</p>
<a href="http://arxiv.org/abs/1910.12781">arXiv:1910.12781</a> [<a href="http://arxiv.org/pdf/1910.12781">pdf</a>]

<h2>Variance Reduced Advantage Estimation with $\delta$ Hindsight Credit Assignment. (arXiv:1911.08362v4 [cs.LG] UPDATED)</h2>
<h3>Kenny Young</h3>
<p>Hindsight Credit Assignment (HCA) refers to a recently proposed family of
methods for producing more efficient credit assignment in reinforcement
learning. These methods work by explicitly estimating the probability that
certain actions were taken in the past given present information. Prior work
has studied the properties of such methods and demonstrated their behaviour
empirically. We extend this work by introducing a particular HCA algorithm
which has provably lower variance than the conventional Monte-Carlo estimator
when the necessary functions can be estimated exactly. This result provides a
strong theoretical basis for how HCA could be broadly useful.
</p>
<a href="http://arxiv.org/abs/1911.08362">arXiv:1911.08362</a> [<a href="http://arxiv.org/pdf/1911.08362">pdf</a>]

<h2>Artificial neural networks in action for an automated cell-type classification of biological neural networks. (arXiv:1911.09977v3 [cs.NE] UPDATED)</h2>
<h3>Eirini Troullinou, Grigorios Tsagkatakis, Spyridon Chavlis, Gergely Turi, Wen-Ke Li, Attila Losonczy, Panagiotis Tsakalides, Panayiota Poirazi</h3>
<p>Identification of different neuronal cell types is critical for understanding
their contribution to brain functions. Yet, automated and reliable
classification of neurons remains a challenge, primarily because of their
biological complexity. Typical approaches include laborious and expensive
immunohistochemical analysis while feature extraction algorithms based on
cellular characteristics have recently been proposed. The former rely on
molecular markers, which are often expressed in many cell types, while the
latter suffer from similar issues: finding features that are distinctive for
each class has proven to be equally challenging. Moreover, both approaches are
time consuming and demand a lot of human intervention. In this work we
establish the first, automated cell-type classification method that relies on
neuronal activity rather than molecular or cellular features. We test our
method on a real-world dataset comprising of raw calcium activity signals for
four neuronal types. We compare the performance of three different deep
learning models and demonstrate that our method can achieve automated
classification of neuronal cell types with unprecedented accuracy.
</p>
<a href="http://arxiv.org/abs/1911.09977">arXiv:1911.09977</a> [<a href="http://arxiv.org/pdf/1911.09977">pdf</a>]

<h2>Non-Intrusive Load Monitoring with an Attention-based Deep Neural Network. (arXiv:1912.00759v2 [cs.LG] UPDATED)</h2>
<h3>Antonio M. Sudoso, Veronica Piccialli</h3>
<p>Energy disaggregation, known in the literature as Non-Intrusive Load
Monitoring (NILM), is the task of inferring the system appliance loads given an
aggregate energy signal, for example coming from a residential power monitor.
In this paper, we propose a deep neural network that combines a regression
subnetwork with a classification subnetwork for solving the NILM problem.
Specifically, we improve the representational power of the overall architecture
by including an encoder-decoder with a tailored attention mechanism in the
regression subnetwork. The attention mechanism is inspired by the temporal
attention that has been recently successfully applied in neural machine
translation, text summarization and speech recognition. The experiments have
been conducted on two publicly available datasets REDD and UK-DALE. The results
show that our proposed deep neural network outperforms the state-of-the-art in
all the considered experimental conditions. We also show that modeling
attention translates into the network's ability to correctly detect the turning
on or off an appliance and to locate signal sections with high power
consumption, that are of extreme interest in the field of energy
disaggregation.
</p>
<a href="http://arxiv.org/abs/1912.00759">arXiv:1912.00759</a> [<a href="http://arxiv.org/pdf/1912.00759">pdf</a>]

<h2>Extending the Morphological Hit-or-Miss Transform to Deep Neural Networks. (arXiv:1912.02259v2 [cs.CV] UPDATED)</h2>
<h3>Muhammad Aminul Islam, Bryce Murray, Andrew Buck, Derek T. Anderson, Grant Scott, Mihail Popescu, James Keller</h3>
<p>While most deep learning architectures are built on convolution, alternative
foundations like morphology are being explored for purposes like
interpretability and its connection to the analysis and processing of geometric
structures. The morphological hit-or-miss operation has the advantage that it
takes into account both foreground and background information when evaluating
target shape in an image. Herein, we identify limitations in existing
hit-or-miss neural definitions and we formulate an optimization problem to
learn the transform relative to deeper architectures. To this end, we model the
semantically important condition that the intersection of the hit and miss
structuring elements (SEs) should be empty and we present a way to express
Don't Care (DNC), which is important for denoting regions of an SE that are not
relevant to detecting a target pattern. Our analysis shows that convolution, in
fact, acts like a hit-miss transform through semantic interpretation of its
filter differences. On these premises, we introduce an extension that
outperforms conventional convolution on benchmark data. Quantitative
experiments are provided on synthetic and benchmark data, showing that the
direct encoding hit-or-miss transform provides better interpretability on
learned shapes consistent with objects whereas our morphologically inspired
generalized convolution yields higher classification accuracy. Last,
qualitative hit and miss filter visualizations are provided relative to single
morphological layer.
</p>
<a href="http://arxiv.org/abs/1912.02259">arXiv:1912.02259</a> [<a href="http://arxiv.org/pdf/1912.02259">pdf</a>]

<h2>Training Deep Neural Networks for Interpretability and Adversarial Robustness. (arXiv:1912.03430v4 [cs.LG] UPDATED)</h2>
<h3>Adam Noack, Isaac Ahern, Dejing Dou, Boyang Li</h3>
<p>Deep neural networks (DNNs) have had many successes, but suffer from two
major issues: (1) a vulnerability to adversarial examples and (2) a tendency to
elude human interpretation. Interestingly, recent empirical and theoretical
evidence suggest these two seemingly disparate issues are actually connected.
In particular, robust models tend to provide more interpretable gradients than
non-robust models. However, whether this relationship works in the opposite
direction remains obscure. With this paper, we seek empirical answers to the
following question: can models acquire adversarial robustness when they are
trained to have interpretable gradients? We introduce a theoretically inspired
technique called Interpretation Regularization (IR), which encourages a model's
gradients to (1) match the direction of interpretable target salience maps and
(2) have small magnitude. To assess model performance and tease apart factors
that contribute to adversarial robustness, we conduct extensive experiments on
MNIST and CIFAR-10 with both $\ell_2$ and $\ell_\infty$ attacks. We demonstrate
that networks trained to have interpretable gradients are more robust to
adversarial perturbations. Applying the network interpretation technique
SmoothGrad yields additional performance gains, especially in cross-norm
attacks and under heavy perturbation. The results indicate that the
interpretability of the model gradients is a crucial factor for adversarial
robustness.
</p>
<a href="http://arxiv.org/abs/1912.03430">arXiv:1912.03430</a> [<a href="http://arxiv.org/pdf/1912.03430">pdf</a>]

<h2>Targeted transfer learning to improve performance in small medical physics datasets. (arXiv:1912.06761v3 [cs.LG] UPDATED)</h2>
<h3>Miguel Romero, Yannet Interian, Timothy Solberg, Gilmer Valdes</h3>
<p>The growing use of Machine Learning has produced significant advances in many
fields. For image-based tasks, however, the use of deep learning remains
challenging in small datasets. In this article, we review, evaluate and compare
the current state-of-the-art techniques in training neural networks to
elucidate which techniques work best for small datasets. We further propose a
path forward for the improvement of model accuracy in medical imaging
applications. We observed best results from one cycle training, discriminative
learning rates with gradual freezing and parameter modification after transfer
learning. We also established that when datasets are small, transfer learning
plays an important role beyond parameter initialization by reusing previously
learned features. Surprisingly we observed that there is little advantage in
using pre-trained networks in images from another part of the body compared to
Imagenet. On the contrary, if images from the same part of the body are
available then transfer learning can produce a significant improvement in
performance with as little as 50 images in the training data.
</p>
<a href="http://arxiv.org/abs/1912.06761">arXiv:1912.06761</a> [<a href="http://arxiv.org/pdf/1912.06761">pdf</a>]

<h2>Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting. (arXiv:1912.09363v3 [stat.ML] UPDATED)</h2>
<h3>Bryan Lim, Sercan O. Arik, Nicolas Loeff, Tomas Pfister</h3>
<p>Multi-horizon forecasting problems often contain a complex mix of inputs --
including static (i.e. time-invariant) covariates, known future inputs, and
other exogenous time series that are only observed historically -- without any
prior information on how they interact with the target. While several deep
learning models have been proposed for multi-step prediction, they typically
comprise black-box models which do not account for the full range of inputs
present in common scenarios. In this paper, we introduce the Temporal Fusion
Transformer (TFT) -- a novel attention-based architecture which combines
high-performance multi-horizon forecasting with interpretable insights into
temporal dynamics. To learn temporal relationships at different scales, the TFT
utilizes recurrent layers for local processing and interpretable self-attention
layers for learning long-term dependencies. The TFT also uses specialized
components for the judicious selection of relevant features and a series of
gating layers to suppress unnecessary components, enabling high performance in
a wide range of regimes. On a variety of real-world datasets, we demonstrate
significant performance improvements over existing benchmarks, and showcase
three practical interpretability use-cases of TFT.
</p>
<a href="http://arxiv.org/abs/1912.09363">arXiv:1912.09363</a> [<a href="http://arxiv.org/pdf/1912.09363">pdf</a>]

<h2>Turbulent scalar flux in inclined jets in crossflow: counter gradient transport and deep learning modelling. (arXiv:2001.04600v2 [physics.flu-dyn] UPDATED)</h2>
<h3>Pedro M. Milani, Julia Ling, John K. Eaton</h3>
<p>A cylindrical and inclined jet in crossflow is studied under two distinct
velocity ratios, $r=1$ and $r=2$, using highly resolved large eddy simulations
(LES). First, an investigation of turbulent scalar mixing sheds light onto the
previously observed but unexplained phenomenon of negative turbulent
diffusivity. We identify two distinct types of counter gradient transport,
prevalent in different regions: the first, throughout the windward shear layer,
is caused by cross-gradient transport; the second, close to the wall right
after injection, is caused by non-local effects. Then, we propose a deep
learning approach for modelling the turbulent scalar flux by adapting the
tensor basis neural network previously developed to model Reynolds stresses
(Ling et al. 2016a). This approach uses a deep neural network with embedded
coordinate frame invariance to predict a tensorial turbulent diffusivity that
is not explicitly available in the high fidelity data used for training. After
ensuring analytically that the matrix diffusivity leads to a stable solution
for the advection diffusion equation, we apply this approach in the inclined
jets in crossflow under study. The results show significant improvement
compared to a simple model, particularly where cross-gradient effects play an
important role in turbulent mixing. The model proposed herein is not limited to
jets in crossflow; it can be used in any turbulent flow where the Reynolds
averaged transport of a scalar is considered.
</p>
<a href="http://arxiv.org/abs/2001.04600">arXiv:2001.04600</a> [<a href="http://arxiv.org/pdf/2001.04600">pdf</a>]

<h2>GraphLIME: Local Interpretable Model Explanations for Graph Neural Networks. (arXiv:2001.06216v2 [cs.LG] UPDATED)</h2>
<h3>Qiang Huang, Makoto Yamada, Yuan Tian, Dinesh Singh, Dawei Yin, Yi Chang</h3>
<p>Graph structured data has wide applicability in various domains such as
physics, chemistry, biology, computer vision, and social networks, to name a
few. Recently, graph neural networks (GNN) were shown to be successful in
effectively representing graph structured data because of their good
performance and generalization ability. GNN is a deep learning based method
that learns a node representation by combining specific nodes and the
structural/topological information of a graph. However, like other deep models,
explaining the effectiveness of GNN models is a challenging task because of the
complex nonlinear transformations made over the iterations. In this paper, we
propose GraphLIME, a local interpretable model explanation for graphs using the
Hilbert-Schmidt Independence Criterion (HSIC) Lasso, which is a nonlinear
feature selection method. GraphLIME is a generic GNN-model explanation
framework that learns a nonlinear interpretable model locally in the subgraph
of the node being explained. More specifically, to explain a node, we generate
a nonlinear interpretable model from its $N$-hop neighborhood and then compute
the K most representative features as the explanations of its prediction using
HSIC Lasso. Through experiments on two real-world datasets, the explanations of
GraphLIME are found to be of extraordinary degree and more descriptive in
comparison to the existing explanation methods.
</p>
<a href="http://arxiv.org/abs/2001.06216">arXiv:2001.06216</a> [<a href="http://arxiv.org/pdf/2001.06216">pdf</a>]

<h2>Comparison Between Traditional Machine Learning Models And Neural Network Models For Vietnamese Hate Speech Detection. (arXiv:2002.00759v2 [cs.CL] UPDATED)</h2>
<h3>Son T. Luu, Hung P. Nguyen, Kiet Van Nguyen, Ngan Luu-Thuy Nguyen</h3>
<p>Hate-speech detection on social network language has become one of the main
researching fields recently due to the spreading of social networks like
Facebook and Twitter. In Vietnam, the threat of offensive and harassment cause
bad impacts for online user. The VLSP - Shared task about Hate Speech Detection
on social networks showed many proposed approaches for detecting whatever
comment is clean or not. However, this problem still needs further researching.
Consequently, we compare traditional machine learning and deep learning on a
large dataset about the user's comments on social network in Vietnamese and
find out what is the advantage and disadvantage of each model by comparing
their accuracy on F1-score, then we pick two models in which has highest
accuracy in traditional machine learning models and deep neural models
respectively. Next, we compare these two models capable of predicting the right
label by referencing their confusion matrices and considering the advantages
and disadvantages of each model. Finally, from the comparison result, we
propose our ensemble method that concentrates the abilities of traditional
methods and deep learning methods.
</p>
<a href="http://arxiv.org/abs/2002.00759">arXiv:2002.00759</a> [<a href="http://arxiv.org/pdf/2002.00759">pdf</a>]

<h2>Detecting Changes in Asset Co-Movement Using the Autoencoder Reconstruction Ratio. (arXiv:2002.02008v2 [q-fin.ST] UPDATED)</h2>
<h3>Bryan Lim, Stefan Zohren, Stephen Roberts</h3>
<p>Detecting changes in asset co-movements is of much importance to financial
practitioners, with numerous risk management benefits arising from the timely
detection of breakdowns in historical correlations. In this article, we propose
a real-time indicator to detect temporary increases in asset co-movements, the
Autoencoder Reconstruction Ratio, which measures how well a basket of asset
returns can be modelled using a lower-dimensional set of latent variables. The
ARR uses a deep sparse denoising autoencoder to perform the dimensionality
reduction on the returns vector, which replaces the PCA approach of the
standard Absorption Ratio, and provides a better model for non-Gaussian
returns. Through a systemic risk application on forecasting on the CRSP US
Total Market Index, we show that lower ARR values coincide with higher
volatility and larger drawdowns, indicating that increased asset co-movement
does correspond with periods of market weakness. We also demonstrate that
short-term (i.e. 5-min and 1-hour) predictors for realised volatility and
market crashes can be improved by including additional ARR inputs.
</p>
<a href="http://arxiv.org/abs/2002.02008">arXiv:2002.02008</a> [<a href="http://arxiv.org/pdf/2002.02008">pdf</a>]

<h2>A Diffusion Theory For Deep Learning Dynamics: Stochastic Gradient Descent Exponentially Favors Flat Minima. (arXiv:2002.03495v12 [cs.LG] UPDATED)</h2>
<h3>Zeke Xie, Issei Sato, Masashi Sugiyama</h3>
<p>Stochastic Gradient Descent (SGD) and its variants are mainstream methods for
training deep networks in practice. SGD is known to find a flat minimum that
generalizes well. However, it is mathematically unclear how deep learning can
select a flat minimum among so many minima. To answer the question
quantitatively, we develop a density diffusion theory (DDT) to reveal how
minima selection quantitatively depends on the minima sharpness and the
hyperparameters. We empirically verify a key property of stochastic gradient
noise (SGN) that the SGN covariance is approximately proportional to the
Hessian and inverse to the batch size. To the best of our knowledge, we are the
first to prove that, benefited from the Hessian-dependent structure of SGN, SGD
favors flat minima exponentially more than sharp minima, while Gradient Descent
(GD) with injected white noise favors flat minima only polynomially more than
sharp minima. We also reveal that either a small learning rate or large-batch
training requires exponentially many iterations to escape from minima in terms
of the ratio of batch size and learning rate. Thus, large-batch training cannot
search flat minima efficiently in a realistic computational time.
</p>
<a href="http://arxiv.org/abs/2002.03495">arXiv:2002.03495</a> [<a href="http://arxiv.org/pdf/2002.03495">pdf</a>]

<h2>Watch out! Motion is Blurring the Vision of Your Deep Neural Networks. (arXiv:2002.03500v2 [cs.CV] UPDATED)</h2>
<h3>Qing Guo, Felix Juefei-Xu, Xiaofei Xie, Lei Ma, Jian Wang, Bing Yu, Wei Feng, Yang Liu</h3>
<p>The state-of-the-art deep neural networks (DNNs) are vulnerable against
adversarial examples with additive random-like noise perturbations. While such
examples are hardly found in the physical world, the image blurring effect
caused by object motion, on the other hand, commonly occurs in practice, making
the study of which greatly important especially for the widely adopted
real-time image processing tasks (e.g., object detection, tracking). In this
paper, we initiate the first step to comprehensively investigate the potential
hazards of the blur effect for DNN, caused by object motion. We propose a novel
adversarial attack method that can generate visually natural motion-blurred
adversarial examples, named motion-based adversarial blur attack (ABBA). To
this end, we first formulate the kernel-prediction-based attack where an input
image is convolved with kernels in a pixel-wise way, and the misclassification
capability is achieved by tuning the kernel weights. To generate visually more
natural and plausible examples, we further propose the saliency-regularized
adversarial kernel prediction, where the salient region serves as a moving
object, and the predicted kernel is regularized to achieve naturally visual
effects. Besides, the attack is further enhanced by adaptively tuning the
translations of object and background. A comprehensive evaluation on the
NeurIPS'17 adversarial competition dataset demonstrates the effectiveness of
ABBA by considering various kernel sizes, translations, and regions. The
in-depth study further confirms that our method shows more effective
penetrating capability to the state-of-the-art GAN-based deblurring mechanisms
compared with other blurring methods. We release the code to
https://github.com/tsingqguo/ABBA.
</p>
<a href="http://arxiv.org/abs/2002.03500">arXiv:2002.03500</a> [<a href="http://arxiv.org/pdf/2002.03500">pdf</a>]

<h2>Object condensation: one-stage grid-free multi-object reconstruction in physics detectors, graph and image data. (arXiv:2002.03605v3 [physics.data-an] UPDATED)</h2>
<h3>Jan Kieseler</h3>
<p>High-energy physics detectors, images, and point clouds share many
similarities in terms of object detection. However, while detecting an unknown
number of objects in an image is well established in computer vision, even
machine learning assisted object reconstruction algorithms in particle physics
almost exclusively predict properties on an object-by-object basis. Traditional
approaches from computer vision either impose implicit constraints on the
object size or density and are not well suited for sparse detector data or rely
on objects being dense and solid. The object condensation method proposed here
is independent of assumptions on object size, sorting or object density, and
further generalises to non-image-like data structures, such as graphs and point
clouds, which are more suitable to represent detector signals. The pixels or
vertices themselves serve as representations of the entire object, and a
combination of learnable local clustering in a latent space and confidence
assignment allows one to collect condensates of the predicted object properties
with a simple algorithm. As proof of concept, the object condensation method is
applied to a simple object classification problem in images and used to
reconstruct multiple particles from detector signals. The latter results are
also compared to a classic particle flow approach.
</p>
<a href="http://arxiv.org/abs/2002.03605">arXiv:2002.03605</a> [<a href="http://arxiv.org/pdf/2002.03605">pdf</a>]

<h2>Meta-Learning across Meta-Tasks for Few-Shot Learning. (arXiv:2002.04274v4 [cs.LG] UPDATED)</h2>
<h3>Nanyi Fei, Zhiwu Lu, Yizhao Gao, Jia Tian, Tao Xiang, Ji-Rong Wen</h3>
<p>Existing meta-learning based few-shot learning (FSL) methods typically adopt
an episodic training strategy whereby each episode contains a meta-task. Across
episodes, these tasks are sampled randomly and their relationships are ignored.
In this paper, we argue that the inter-meta-task relationships should be
exploited and those tasks are sampled strategically to assist in meta-learning.
Specifically, we consider the relationships defined over two types of meta-task
pairs and propose different strategies to exploit them. (1) Two meta-tasks with
disjoint sets of classes: this pair is interesting because it is reminiscent of
the relationship between the source seen classes and target unseen classes,
featured with domain gap caused by class differences. A novel learning
objective termed meta-domain adaptation (MDA) is proposed to make the
meta-learned model more robust to the domain gap. (2) Two meta-tasks with
identical sets of classes: this pair is useful because it can be employed to
learn models that are robust against poorly sampled few-shots. To that end, a
novel meta-knowledge distillation (MKD) objective is formulated. There are some
mistakes in the experiments. We thus choose to withdraw this paper.
</p>
<a href="http://arxiv.org/abs/2002.04274">arXiv:2002.04274</a> [<a href="http://arxiv.org/pdf/2002.04274">pdf</a>]

<h2>Pointfilter: Point Cloud Filtering via Encoder-Decoder Modeling. (arXiv:2002.05968v2 [cs.GR] UPDATED)</h2>
<h3>Dongbo Zhang, Xuequan Lu, Hong Qin, Ying He</h3>
<p>Point cloud filtering is a fundamental problem in geometry modeling and
processing. Despite of significant advancement in recent years, the existing
methods still suffer from two issues: 1) they are either designed without
preserving sharp features or less robust in feature preservation; and 2) they
usually have many parameters and require tedious parameter tuning. In this
paper, we propose a novel deep learning approach that automatically and
robustly filters point clouds by removing noise and preserving their sharp
features. Our point-wise learning architecture consists of an encoder and a
decoder. The encoder directly takes points (a point and its neighbors) as
input, and learns a latent representation vector which goes through the decoder
to relate the ground-truth position with a displacement vector. The trained
neural network can automatically generate a set of clean points from a noisy
input. Extensive experiments show that our approach outperforms the
state-of-the-art deep learning techniques in terms of both visual quality and
quantitative error metrics. The source code and dataset can be found at
https://github.com/dongbo-BUAA-VR/Pointfilter.
</p>
<a href="http://arxiv.org/abs/2002.05968">arXiv:2002.05968</a> [<a href="http://arxiv.org/pdf/2002.05968">pdf</a>]

<h2>Reliable Estimation of Kullback-Leibler Divergence by Controlling Discriminator Complexity in the Reproducing Kernel Hilbert Space. (arXiv:2002.11187v3 [cs.LG] UPDATED)</h2>
<h3>Sandesh Ghimire, Prashnna K Gyawali, Linwei Wang</h3>
<p>Several scalable sample-based methods to compute the Kullback Leibler (KL)
divergence between two distributions have been proposed and applied in
large-scale machine learning models. While they have been found to be unstable,
the theoretical root cause of the problem is not clear. In this paper, we study
a generative adversarial network based approach that uses a neural network
discriminator to estimate KL divergence. We argue that, in such case, high
fluctuations in the estimates are a consequence of not controlling the
complexity of the discriminator function space. We provide a theoretical
underpinning and remedy for this problem by first constructing a discriminator
in the Reproducing Kernel Hilbert Space (RKHS). This enables us to leverage
sample complexity and mean embedding to theoretically relate the error
probability bound of the KL estimates to the complexity of the discriminator in
RKHS. Based on this theory, we then present a scalable way to control the
complexity of the discriminator for a reliable estimation of KL divergence. We
support both our proposed theory and method to control the complexity of the
RKHS discriminator through controlled experiments.
</p>
<a href="http://arxiv.org/abs/2002.11187">arXiv:2002.11187</a> [<a href="http://arxiv.org/pdf/2002.11187">pdf</a>]

<h2>Adversarial Monte Carlo Meta-Learning of Optimal Prediction Procedures. (arXiv:2002.11275v2 [stat.ML] UPDATED)</h2>
<h3>Alex Luedtke, Incheoul Chung, Oleg Sofrygin</h3>
<p>We frame the meta-learning of prediction procedures as a search for an
optimal strategy in a two-player game. In this game, Nature selects a prior
over distributions that generate labeled data consisting of features and an
associated outcome, and the Predictor observes data sampled from a distribution
drawn from this prior. The Predictor's objective is to learn a function that
maps from a new feature to an estimate of the associated outcome. We establish
that, under reasonable conditions, the Predictor has an optimal strategy that
is equivariant to shifts and rescalings of the outcome and is invariant to
permutations of the observations and to shifts, rescalings, and permutations of
the features. We introduce a neural network architecture that satisfies these
properties. The proposed strategy performs favorably compared to standard
practice in both parametric and nonparametric experiments.
</p>
<a href="http://arxiv.org/abs/2002.11275">arXiv:2002.11275</a> [<a href="http://arxiv.org/pdf/2002.11275">pdf</a>]

<h2>Relevance-Guided Modeling of Object Dynamics for Reinforcement Learning. (arXiv:2003.01384v2 [cs.LG] UPDATED)</h2>
<h3>William Agnew, Pedro Domingos</h3>
<p>Current deep reinforcement learning approaches incorporate minimal prior
knowledge about the environment, limiting computational and sample efficiency.
Objects provide a succinct and causal description of the world, and several
recent works have studied unsupervised object representation learning using
priors and losses over static object properties like visual consistency.
However, object dynamics and interaction are critical cues for objectness. In
addition, extensive research has shown humans have a working memory limited to
only a small number of task relevant objects. In this paper we propose a
framework for reasoning about object dynamics and behavior to rapidly determine
minimal and task-specific object representations. We show the need for this
reasoning over object behavior and dynamics by introducing a suite of RGBD
MuJoCo object collection and avoidance tasks that, while intuitive and visually
simple, confound state of the art unsupervised object representation learning
algorithms. We also demonstrate the potential of this framework on a number of
Atari games, using our object representation and standard RL and planning
algorithms to learn over 10,000x faster than standard deep RL algorithms, and
faster even than human players.
</p>
<a href="http://arxiv.org/abs/2003.01384">arXiv:2003.01384</a> [<a href="http://arxiv.org/pdf/2003.01384">pdf</a>]

<h2>Removing Disparate Impact of Differentially Private Stochastic Gradient Descent on Model Accuracy. (arXiv:2003.03699v2 [cs.LG] UPDATED)</h2>
<h3>Depeng Xu, Wei Du, Xintao Wu</h3>
<p>When we enforce differential privacy in machine learning, the utility-privacy
trade-off is different w.r.t. each group. Gradient clipping and random noise
addition disproportionately affect underrepresented and complex classes and
subgroups, which results in inequality in utility loss. In this work, we
analyze the inequality in utility loss by differential privacy and propose a
modified differentially private stochastic gradient descent (DPSGD), called
DPSGD-F, to remove the potential disparate impact of differential privacy on
the protected group. DPSGD-F adjusts the contribution of samples in a group
depending on the group clipping bias such that differential privacy has no
disparate impact on group utility. Our experimental evaluation shows how group
sample size and group clipping bias affect the impact of differential privacy
in DPSGD, and how adaptive clipping for each group helps to mitigate the
disparate impact caused by differential privacy in DPSGD-F.
</p>
<a href="http://arxiv.org/abs/2003.03699">arXiv:2003.03699</a> [<a href="http://arxiv.org/pdf/2003.03699">pdf</a>]

<h2>AI4COVID-19: AI Enabled Preliminary Diagnosis for COVID-19 from Cough Samples via an App. (arXiv:2004.01275v6 [eess.AS] UPDATED)</h2>
<h3>Ali Imran, Iryna Posokhova, Haneya N. Qureshi, Usama Masood, Muhammad Sajid Riaz, Kamran Ali, Charles N. John, MD Iftikhar Hussain, Muhammad Nabeel</h3>
<p>Background: The inability to test at scale has become humanity's Achille's
heel in the ongoing war against the COVID-19 pandemic. A scalable screening
tool would be a game changer. Building on the prior work on cough-based
diagnosis of respiratory diseases, we propose, develop and test an Artificial
Intelligence (AI)-powered screening solution for COVID-19 infection that is
deployable via a smartphone app. The app, named AI4COVID-19 records and sends
three 3-second cough sounds to an AI engine running in the cloud, and returns a
result within two minutes. Methods: Cough is a symptom of over thirty
non-COVID-19 related medical conditions. This makes the diagnosis of a COVID-19
infection by cough alone an extremely challenging multidisciplinary problem. We
address this problem by investigating the distinctness of pathomorphological
alterations in the respiratory system induced by COVID-19 infection when
compared to other respiratory infections. To overcome the COVID-19 cough
training data shortage we exploit transfer learning. To reduce the misdiagnosis
risk stemming from the complex dimensionality of the problem, we leverage a
multi-pronged mediator centered risk-averse AI architecture. Results: Results
show AI4COVID-19 can distinguish among COVID-19 coughs and several types of
non-COVID-19 coughs. The accuracy is promising enough to encourage a
large-scale collection of labeled cough data to gauge the generalization
capability of AI4COVID-19. AI4COVID-19 is not a clinical grade testing tool.
Instead, it offers a screening tool deployable anytime, anywhere, by anyone. It
can also be a clinical decision assistance tool used to channel
clinical-testing and treatment to those who need it the most, thereby saving
more lives.
</p>
<a href="http://arxiv.org/abs/2004.01275">arXiv:2004.01275</a> [<a href="http://arxiv.org/pdf/2004.01275">pdf</a>]

<h2>Spatio-temporal Learning from Longitudinal Data for Multiple Sclerosis Lesion Segmentation. (arXiv:2004.03675v2 [eess.IV] UPDATED)</h2>
<h3>Stefan Denner, Ashkan Khakzar, Moiz Sajid, Mahdi Saleh, Ziga Spiclin, Seong Tae Kim, Nassir Navab</h3>
<p>Segmentation of Multiple Sclerosis (MS) lesions in longitudinal brain MR
scans is performed for monitoring the progression of MS lesions. We hypothesize
that the spatio-temporal cues in longitudinal data can aid the segmentation
algorithm. Therefore, we propose a multi-task learning approach by defining an
auxiliary self-supervised task of deformable registration between two
time-points to guide the neural network toward learning from spatio-temporal
changes. We show the efficacy of our method on a clinical dataset comprised of
70 patients with one follow-up study for each patient. Our results show that
spatio-temporal information in longitudinal data is a beneficial cue for
improving segmentation. We improve the result of current state-of-the-art by
2.6% in terms of overall score (p&lt;0.05). Code is publicly available.
</p>
<a href="http://arxiv.org/abs/2004.03675">arXiv:2004.03675</a> [<a href="http://arxiv.org/pdf/2004.03675">pdf</a>]

<h2>Dendrite Net: A White-Box Module for Classification, Regression, and System Identification. (arXiv:2004.03955v4 [cs.LG] UPDATED)</h2>
<h3>Gang Liu, Jing Wang</h3>
<p>This paper presents a basic machine learning algorithm, named Dendrite Net or
DD, just like Support Vector Machine (SVM) or Multilayer Perceptron (MLP). DD's
main concept is that the algorithm can recognize this class after learning, if
the output's logical expression contains the corresponding class's logical
relationship among inputs ($ and \backslash or \backslash not $). Experiments
and results: DD, the first white-box machine learning algorithm, showed
excellent system identification performance for the black-box system. Secondly,
it was verified by nine real-world applications that DD brought better
generalization capability relative to MLP architecture that imitated neurons'
cell body (Cell body Net) for regression. Thirdly, by MINIST and FASHION-MINIST
datasets, it was verified that DD showed higher testing accuracy under greater
training loss than Cell body Net for classification. The number of modules can
effectively adjust DD's logical expression capacity, which avoids over-fitting
and makes it easy to get a model with outstanding generalization capability.
Finally, repeated experiments in $ MATLAB $ and $ PyTorch $ ($ Python $)
demonstrated that DD was faster than Cell body Net both in epoch and
forward-propagation. We highlight DD's white-box attribute, controllable
precision for better generalization capability, and lower computational
complexity. Not only can DD be used for generalized engineering, but DD has
vast development potential as a module for deep learning. DD code is available
at https://github.com/liugang1234567/Gang-neuron.
</p>
<a href="http://arxiv.org/abs/2004.03955">arXiv:2004.03955</a> [<a href="http://arxiv.org/pdf/2004.03955">pdf</a>]

<h2>Models Genesis. (arXiv:2004.07882v2 [cs.CV] UPDATED)</h2>
<h3>Zongwei Zhou, Vatsal Sodha, Jiaxuan Pang, Michael B. Gotway, Jianming Liang</h3>
<p>Transfer learning from natural image to medical image has been established as
one of the most practical paradigms in deep learning for medical image
analysis. To fit this paradigm, however, 3D imaging tasks in the most prominent
imaging modalities (e.g., CT and MRI) have to be reformulated and solved in 2D,
losing rich 3D anatomical information, thereby inevitably compromising its
performance. To overcome this limitation, we have built a set of models, called
Generic Autodidactic Models, nicknamed Models Genesis, because they are created
ex nihilo (with no manual labeling), self-taught (learnt by self-supervision),
and generic (served as source models for generating application-specific target
models). Our extensive experiments demonstrate that our Models Genesis
significantly outperform learning from scratch and existing pre-trained 3D
models in all five target 3D applications covering both segmentation and
classification. More importantly, learning a model from scratch simply in 3D
may not necessarily yield performance better than transfer learning from
ImageNet in 2D, but our Models Genesis consistently top any 2D/2.5D approaches
including fine-tuning the models pre-trained from ImageNet as well as
fine-tuning the 2D versions of our Models Genesis, confirming the importance of
3D anatomical information and significance of Models Genesis for 3D medical
imaging. This performance is attributed to our unified self-supervised learning
framework, built on a simple yet powerful observation: the sophisticated and
recurrent anatomy in medical images can serve as strong yet free supervision
signals for deep models to learn common anatomical representation automatically
via self-supervision. As open science, all codes and pre-trained Models Genesis
are available at https://github.com/MrGiovanni/ModelsGenesis.
</p>
<a href="http://arxiv.org/abs/2004.07882">arXiv:2004.07882</a> [<a href="http://arxiv.org/pdf/2004.07882">pdf</a>]

<h2>BERT-ATTACK: Adversarial Attack Against BERT Using BERT. (arXiv:2004.09984v2 [cs.CL] UPDATED)</h2>
<h3>Linyang Li, Ruotian Ma, Qipeng Guo, Xiangyang Xue, Xipeng Qiu</h3>
<p>Adversarial attacks for discrete data (such as text) has been proved
significantly more challenging than continuous data (such as image), since it
is difficult to generate adversarial samples with gradient-based methods.
Currently, the successful attack methods for text usually adopt heuristic
replacement strategies on character or word level, which remains challenging to
find the optimal solution in the massive space of possible combination of
replacements, while preserving semantic consistency and language fluency. In
this paper, we propose \textbf{BERT-Attack}, a high-quality and effective
method to generate adversarial samples using pre-trained masked language models
exemplified by BERT. We turn BERT against its fine-tuned models and other deep
neural models for downstream tasks. Our method successfully misleads the target
models to predict incorrectly, outperforming state-of-the-art attack strategies
in both success rate and perturb percentage, while the generated adversarial
samples are fluent and semantically preserved. Also, the cost of calculation is
low, thus possible for large-scale generations.
</p>
<a href="http://arxiv.org/abs/2004.09984">arXiv:2004.09984</a> [<a href="http://arxiv.org/pdf/2004.09984">pdf</a>]

<h2>Stability-Guaranteed Reinforcement Learning for Contact-rich Manipulation. (arXiv:2004.10886v2 [cs.RO] UPDATED)</h2>
<h3>Shahbaz A. Khader, Hang Yin, Pietro Falco, Danica Kragic</h3>
<p>Reinforcement learning (RL) has had its fair share of success in contact-rich
manipulation tasks but it still lags behind in benefiting from advances in
robot control theory such as impedance control and stability guarantees.
Recently, the concept of variable impedance control (VIC) was adopted into RL
with encouraging results. However, the more important issue of stability
remains unaddressed. To clarify the challenge in stable RL, we introduce the
term all-the-time-stability that unambiguously means that every possible
rollout will be stability certified. Our contribution is a model-free RL method
that not only adopts VIC but also achieves all-the-time-stability. Building on
a recently proposed stable VIC controller as the policy parameterization, we
introduce a novel policy search algorithm that is inspired by Cross-Entropy
Method and inherently guarantees stability. Our experimental studies confirm
the feasibility and usefulness of stability guarantee and also features, to the
best of our knowledge, the first successful application of RL with
all-the-time-stability on the benchmark problem of peg-in-hole.
</p>
<a href="http://arxiv.org/abs/2004.10886">arXiv:2004.10886</a> [<a href="http://arxiv.org/pdf/2004.10886">pdf</a>]

<h2>Time Series Forecasting With Deep Learning: A Survey. (arXiv:2004.13408v2 [stat.ML] UPDATED)</h2>
<h3>Bryan Lim, Stefan Zohren</h3>
<p>Numerous deep learning architectures have been developed to accommodate the
diversity of time series datasets across different domains. In this article, we
survey common encoder and decoder designs used in both one-step-ahead and
multi-horizon time series forecasting -- describing how temporal information is
incorporated into predictions by each model. Next, we highlight recent
developments in hybrid deep learning models, which combine well-studied
statistical models with neural network components to improve pure methods in
either category. Lastly, we outline some ways in which deep learning can also
facilitate decision support with time series data.
</p>
<a href="http://arxiv.org/abs/2004.13408">arXiv:2004.13408</a> [<a href="http://arxiv.org/pdf/2004.13408">pdf</a>]

<h2>Unsupervised Transfer of Semantic Role Models from Verbal to Nominal Domain. (arXiv:2005.00278v2 [cs.CL] UPDATED)</h2>
<h3>Yanpeng Zhao, Ivan Titov</h3>
<p>Semantic role labeling (SRL) is an NLP task involving the assignment of
predicate arguments to types, called semantic roles. Though research on SRL has
primarily focused on verbal predicates and many resources available for SRL
provide annotations only for verbs, semantic relations are often triggered by
other linguistic constructions, e.g., nominalizations. In this work, we
investigate a transfer scenario where we assume role-annotated data for the
source verbal domain but only unlabeled data for the target nominal domain. Our
key assumption, enabling the transfer between the two domains, is that
selectional preferences of a role (i.e., preferences or constraints on the
admissible arguments) do not strongly depend on whether the relation is
triggered by a verb or a noun. For example, the same set of arguments can fill
the Acquirer role for the verbal predicate `acquire' and its nominal form
`acquisition'. We approach the transfer task from the variational autoencoding
perspective. The labeler serves as an encoder (predicting role labels given a
sentence), whereas selectional preferences are captured in the decoder
component (generating arguments for the predicting roles). Nominal roles are
not labeled in the training data, and the learning objective instead pushes the
labeler to assign roles predictive of the arguments. Sharing the decoder
parameters across the domains encourages consistency between labels predicted
for both domains and facilitates the transfer. The method substantially
outperforms baselines, such as unsupervised and `direct transfer' methods, on
the English CoNLL-2009 dataset.
</p>
<a href="http://arxiv.org/abs/2005.00278">arXiv:2005.00278</a> [<a href="http://arxiv.org/pdf/2005.00278">pdf</a>]

<h2>POINTER: Constrained Progressive Text Generation via Insertion-based Generative Pre-training. (arXiv:2005.00558v2 [cs.CL] UPDATED)</h2>
<h3>Yizhe Zhang, Guoyin Wang, Chunyuan Li, Zhe Gan, Chris Brockett, Bill Dolan</h3>
<p>Large-scale pre-trained language models, such as BERT and GPT-2, have
achieved excellent performance in language representation learning and
free-form text generation. However, these models cannot be directly employed to
generate text under specified lexical constraints. To address this challenge,
we present POINTER (PrOgressive INsertion-based TransformER), a simple yet
novel insertion-based approach for hard-constrained text generation. The
proposed method operates by progressively inserting new tokens between existing
tokens in a parallel manner. This procedure is recursively applied until a
sequence is completed. The resulting coarse-to-fine hierarchy makes the
generation process intuitive and interpretable. We pre-train our model with the
proposed progressive insertion-based objective on a 12GB Wikipedia dataset, and
fine-tune it on downstream hard-constrained generation tasks.
Non-autoregressive decoding yields an empirically logarithmic time complexity
during inference time. Experimental results on both News and Yelp datasets
demonstrate that POINTER achieves state-of-the-art performance on constrained
text generation. We released the pre-trained models and the source code to
facilitate future research (https://github.com/dreasysnail/POINTER).
</p>
<a href="http://arxiv.org/abs/2005.00558">arXiv:2005.00558</a> [<a href="http://arxiv.org/pdf/2005.00558">pdf</a>]

<h2>Formal Policy Synthesis for Continuous-Space Systems via Reinforcement Learning. (arXiv:2005.01319v2 [eess.SY] UPDATED)</h2>
<h3>Milad Kazemi, Sadegh Soudjani</h3>
<p>This paper studies satisfaction of temporal properties on unknown stochastic
processes that have continuous state spaces. We show how reinforcement learning
(RL) can be applied for computing policies that are finite-memory and
deterministic using only the paths of the stochastic process. We address
properties expressed in linear temporal logic (LTL) and use their automaton
representation to give a path-dependent reward function maximised via the RL
algorithm. We develop the required assumptions and theories for the convergence
of the learned policy to the optimal policy in the continuous state space. To
improve the performance of the learning on the constructed sparse reward
function, we propose a sequential learning procedure based on a sequence of
labelling functions obtained from the positive normal form of the LTL
specification. We use this procedure to guide the RL algorithm towards a policy
that converges to an optimal policy under suitable assumptions on the process.
We demonstrate the approach on a 4-dim cart-pole system and 6-dim boat driving
problem.
</p>
<a href="http://arxiv.org/abs/2005.01319">arXiv:2005.01319</a> [<a href="http://arxiv.org/pdf/2005.01319">pdf</a>]

<h2>Flexible Example-based Image Enhancement with Task Adaptive Global Feature Self-Guided Network. (arXiv:2005.06654v2 [cs.CV] UPDATED)</h2>
<h3>Dario Kneubuehler, Shuhang Gu, Luc Van Gool, Radu Timofte</h3>
<p>We propose the first practical multitask image enhancement network, that is
able to learn one-to-many and many-to-one image mappings. We show that our
model outperforms the current state of the art in learning a single enhancement
mapping, while having significantly fewer parameters than its competitors.
Furthermore, the model achieves even higher performance on learning multiple
mappings simultaneously, by taking advantage of shared representations. Our
network is based on the recently proposed SGN architecture, with modifications
targeted at incorporating global features and style adaption. Finally, we
present an unpaired learning method for multitask image enhancement, that is
based on generative adversarial networks (GANs).
</p>
<a href="http://arxiv.org/abs/2005.06654">arXiv:2005.06654</a> [<a href="http://arxiv.org/pdf/2005.06654">pdf</a>]

<h2>An Adversarial Approach for Explaining the Predictions of Deep Neural Networks. (arXiv:2005.10284v4 [cs.LG] UPDATED)</h2>
<h3>Arash Rahnama, Andrew Tseng</h3>
<p>Machine learning models have been successfully applied to a wide range of
applications including computer vision, natural language processing, and speech
recognition. A successful implementation of these models however, usually
relies on deep neural networks (DNNs) which are treated as opaque black-box
systems due to their incomprehensible complexity and intricate internal
mechanism. In this work, we present a novel algorithm for explaining the
predictions of a DNN using adversarial machine learning. Our approach
identifies the relative importance of input features in relation to the
predictions based on the behavior of an adversarial attack on the DNN. Our
algorithm has the advantage of being fast, consistent, and easy to implement
and interpret. We present our detailed analysis that demonstrates how the
behavior of an adversarial attack, given a DNN and a task, stays consistent for
any input test data point proving the generality of our approach. Our analysis
enables us to produce consistent and efficient explanations. We illustrate the
effectiveness of our approach by conducting experiments using a variety of
DNNs, tasks, and datasets. Finally, we compare our work with other well-known
techniques in the current literature.
</p>
<a href="http://arxiv.org/abs/2005.10284">arXiv:2005.10284</a> [<a href="http://arxiv.org/pdf/2005.10284">pdf</a>]

<h2>Bayesian Restoration of Audio Degraded by Low-Frequency Pulses Modeled via Gaussian Process. (arXiv:2005.14181v2 [eess.AS] UPDATED)</h2>
<h3>Hugo Tremonte de Carvalho, Fl&#xe1;vio Rainho &#xc1;vila, Luiz Wagner Pereira Biscainho</h3>
<p>A common defect found when reproducing old vinyl and gramophone recordings
with mechanical devices are the long pulses with significant low-frequency
content caused by the interaction of the arm-needle system with deep scratches
or even breakages on the media surface. Previous approaches to their
suppression on digital counterparts of the recordings depend on a prior
estimation of the pulse location, usually performed via heuristic methods. This
paper proposes a novel Bayesian approach capable of jointly estimating the
pulse location; interpolating the almost annihilated signal underlying the
strong discontinuity that initiates the pulse; and also estimating the long
pulse tail by a simple Gaussian Process, allowing its suppression from the
corrupted signal. The posterior distribution for the model parameters as well
for the pulse is explored via Markov-Chain Monte Carlo (MCMC) algorithms.
Controlled experiments indicate that the proposed method, while requiring
significantly less user intervention, achieves perceptual results similar to
those of previous approaches and performs well when dealing with naturally
degraded signals.
</p>
<a href="http://arxiv.org/abs/2005.14181">arXiv:2005.14181</a> [<a href="http://arxiv.org/pdf/2005.14181">pdf</a>]

<h2>Detection of gravitational-wave signals from binary neutron star mergers using machine learning. (arXiv:2006.01509v2 [astro-ph.HE] UPDATED)</h2>
<h3>Marlin B. Sch&#xe4;fer (1 and 2), Frank Ohme (1 and 2), Alexander H. Nitz (1 and 2) ((1) Max-Planck-Institut f&#xfc;r Gravitationsphysik (Albert-Einstein-Institut), (2) Leibniz Universit&#xe4;t Hannover)</h3>
<p>As two neutron stars merge, they emit gravitational waves that can
potentially be detected by earth bound detectors. Matched-filtering based
algorithms have traditionally been used to extract quiet signals embedded in
noise. We introduce a novel neural-network based machine learning algorithm
that uses time series strain data from gravitational-wave detectors to detect
signals from non-spinning binary neutron star mergers. For the Advanced LIGO
design sensitivity, our network has an average sensitive distance of 130 Mpc at
a false-alarm rate of 10 per month. Compared to other state-of-the-art machine
learning algorithms, we find an improvement by a factor of 6 in sensitivity to
signals with signal-to-noise ratio below 25. However, this approach is not yet
competitive with traditional matched-filtering based methods. A conservative
estimate indicates that our algorithm introduces on average 10.2 s of latency
between signal arrival and generating an alert. We give an exact description of
our testing procedure, which can not only be applied to machine learning based
algorithms but all other search algorithms as well. We thereby improve the
ability to compare machine learning and classical searches.
</p>
<a href="http://arxiv.org/abs/2006.01509">arXiv:2006.01509</a> [<a href="http://arxiv.org/pdf/2006.01509">pdf</a>]

<h2>Learning Functions to Study the Benefit of Multitask Learning. (arXiv:2006.05561v2 [cs.LG] UPDATED)</h2>
<h3>Gabriele Bettgenh&#xe4;user, Michael A. Hedderich, Dietrich Klakow</h3>
<p>We study and quantify the generalization patterns of multitask learning (MTL)
models for sequence labeling tasks. MTL models are trained to optimize a set of
related tasks jointly. Although multitask learning has achieved improved
performance in some problems, there are also tasks that lose performance when
trained together. These mixed results motivate us to study the factors that
impact the performance of MTL models. We note that theoretical bounds and
convergence rates for MTL models exist, but they rely on strong assumptions
such as task relatedness and the use of balanced datasets. To remedy these
limitations, we propose the creation of a task simulator and the use of
Symbolic Regression to learn expressions relating model performance to possible
factors of influence. For MTL, we study the model performance against the
number of tasks (T), the number of samples per task (n) and the task
relatedness measured by the adjusted mutual information (AMI). In our
experiments, we could empirically find formulas relating model performance with
factors of sqrt(n), sqrt(T), which are equivalent to sound mathematical proofs
in Maurer[2016], and we went beyond by discovering that performance relates to
a factor of sqrt(AMI).
</p>
<a href="http://arxiv.org/abs/2006.05561">arXiv:2006.05561</a> [<a href="http://arxiv.org/pdf/2006.05561">pdf</a>]

<h2>Integrated Replay Spoofing-aware Text-independent Speaker Verification. (arXiv:2006.05599v2 [eess.AS] UPDATED)</h2>
<h3>Hye-jin Shim, Jee-weon Jung, Ju-ho Kim, Seung-bin Kim, Ha-Jin Yu</h3>
<p>A number of studies have successfully developed speaker verification or
presentation attack detection systems. However, studies integrating the two
tasks remain in the preliminary stages. In this paper, we propose two
approaches for building an integrated system of speaker verification and
presentation attack detection: an end-to-end monolithic approach and a back-end
modular approach. The first approach simultaneously trains speaker
identification, presentation attack detection, and the integrated system using
multi-task learning using a common feature. However, through experiments, we
hypothesize that the information required for performing speaker verification
and presentation attack detection might differ because speaker verification
systems try to remove device-specific information from speaker embeddings,
while presentation attack detection systems exploit such information.
Therefore, we propose a back-end modular approach using a separate deep neural
network (DNN) for speaker verification and presentation attack detection. This
approach has thee input components: two speaker embeddings (for enrollment and
test each) and prediction of presentation attacks. Experiments are conducted
using the ASVspoof 2017-v2 dataset, which includes official trials on the
integration of speaker verification and presentation attack detection. The
proposed back-end approach demonstrates a relative improvement of 21.77% in
terms of the equal error rate for integrated trials compared to a conventional
speaker verification system.
</p>
<a href="http://arxiv.org/abs/2006.05599">arXiv:2006.05599</a> [<a href="http://arxiv.org/pdf/2006.05599">pdf</a>]

<h2>Rethinking the Value of Labels for Improving Class-Imbalanced Learning. (arXiv:2006.07529v2 [cs.LG] UPDATED)</h2>
<h3>Yuzhe Yang, Zhi Xu</h3>
<p>Real-world data often exhibits long-tailed distributions with heavy class
imbalance, posing great challenges for deep recognition models. We identify a
persisting dilemma on the value of labels in the context of imbalanced
learning: on the one hand, supervision from labels typically leads to better
results than its unsupervised counterparts; on the other hand, heavily
imbalanced data naturally incurs "label bias" in the classifier, where the
decision boundary can be drastically altered by the majority classes. In this
work, we systematically investigate these two facets of labels. We demonstrate,
theoretically and empirically, that class-imbalanced learning can significantly
benefit in both semi-supervised and self-supervised manners. Specifically, we
confirm that (1) positively, imbalanced labels are valuable: given more
unlabeled data, the original labels can be leveraged with the extra data to
reduce label bias in a semi-supervised manner, which greatly improves the final
classifier; (2) negatively however, we argue that imbalanced labels are not
useful always: classifiers that are first pre-trained in a self-supervised
manner consistently outperform their corresponding baselines. Extensive
experiments on large-scale imbalanced datasets verify our theoretically
grounded strategies, showing superior performance over previous
state-of-the-arts. Our intriguing findings highlight the need to rethink the
usage of imbalanced labels in realistic long-tailed tasks. Code is available at
https://github.com/YyzHarry/imbalanced-semi-self.
</p>
<a href="http://arxiv.org/abs/2006.07529">arXiv:2006.07529</a> [<a href="http://arxiv.org/pdf/2006.07529">pdf</a>]

<h2>LFD-ProtoNet: Prototypical Network Based on Local Fisher Discriminant Analysis for Few-shot Learning. (arXiv:2006.08306v2 [cs.LG] UPDATED)</h2>
<h3>Kei Mukaiyama, Issei Sato, Masashi Sugiyama</h3>
<p>The prototypical network (ProtoNet) is a few-shot learning framework that
performs metric learning and classification using the distance to prototype
representations of each class. It has attracted a great deal of attention
recently since it is simple to implement, highly extensible, and performs well
in experiments. However, it only takes into account the mean of the support
vectors as prototypes and thus it performs poorly when the support set has high
variance. In this paper, we propose to combine ProtoNet with local Fisher
discriminant analysis to reduce the local within-class covariance and increase
the local between-class covariance of the support set. We show the usefulness
of the proposed method by theoretically providing an expected risk bound and
empirically demonstrating its superior classification accuracy on miniImageNet
and tieredImageNet.
</p>
<a href="http://arxiv.org/abs/2006.08306">arXiv:2006.08306</a> [<a href="http://arxiv.org/pdf/2006.08306">pdf</a>]

<h2>PhishGAN: Data Augmentation and Identification of Homoglpyh Attacks. (arXiv:2006.13742v3 [cs.CV] UPDATED)</h2>
<h3>Joon Sern Lee, Gui Peng David Yam, Jin Hao Chan</h3>
<p>Homoglyph attacks are a common technique used by hackers to conduct phishing.
Domain names or links that are visually similar to actual ones are created via
punycode to obfuscate the attack, making the victim more susceptible to
phishing. For example, victims may mistake "|inkedin.com" for "linkedin.com"
and in the process, divulge personal details to the fake website. Current State
of The Art (SOTA) typically make use of string comparison algorithms (e.g.
Levenshtein Distance), which are computationally heavy. One reason for this is
the lack of publicly available datasets thus hindering the training of more
advanced Machine Learning (ML) models. Furthermore, no one font is able to
render all types of punycode correctly, posing a significant challenge to the
creation of a dataset that is unbiased toward any particular font. This coupled
with the vast number of internet domains pose a challenge in creating a dataset
that can capture all possible variations. Here, we show how a conditional
Generative Adversarial Network (GAN), PhishGAN, can be used to generate images
of hieroglyphs, conditioned on non-homoglpyh input text images. Practical
changes to current SOTA were required to facilitate the generation of more
varied homoglyph text-based images. We also demonstrate a workflow of how
PhishGAN together with a Homoglyph Identifier (HI) model can be used to
identify the domain the homoglyph was trying to imitate. Furthermore, we
demonstrate how PhishGAN's ability to generate datasets on the fly facilitate
the quick adaptation of cybersecurity systems to detect new threats as they
emerge.
</p>
<a href="http://arxiv.org/abs/2006.13742">arXiv:2006.13742</a> [<a href="http://arxiv.org/pdf/2006.13742">pdf</a>]

<h2>Explanation Augmented Feedback in Human-in-the-Loop Reinforcement Learning. (arXiv:2006.14804v3 [cs.AI] UPDATED)</h2>
<h3>Lin Guan, Mudit Verma, Sihang Guo, Ruohan Zhang, Subbarao Kambhampati</h3>
<p>Human-in-the-loop Reinforcement Learning (HRL) aims to integrate human
guidance with Reinforcement Learning (RL) algorithms to improve sample
efficiency and performance. A common type of human guidance in HRL is binary
evaluative "good" or "bad" feedback for queried states and actions. However,
this type of learning scheme suffers from the problems of weak supervision and
poor efficiency in leveraging human feedback. To address this, we present
EXPAND (EXPlanation AugmeNted feeDback) which provides a visual explanation in
the form of saliency maps from humans in addition to the binary feedback.
EXPAND employs a state perturbation approach based on salient information in
the state to augment the binary feedback. We choose five tasks, namely
Pixel-Taxi and four Atari games, to evaluate this approach. We demonstrate the
effectiveness of our method using two metrics: environment sample efficiency
and human feedback sample efficiency. We show that our method significantly
outperforms previous methods. We also analyze the results qualitatively by
visualizing the agent's attention. Finally, we present an ablation study to
confirm our hypothesis that augmenting binary feedback with state salient
information results in a boost in performance.
</p>
<a href="http://arxiv.org/abs/2006.14804">arXiv:2006.14804</a> [<a href="http://arxiv.org/pdf/2006.14804">pdf</a>]

<h2>Learning an arbitrary mixture of two multinomial logits. (arXiv:2007.00204v2 [stat.ML] UPDATED)</h2>
<h3>Wenpin Tang</h3>
<p>In this paper, we consider mixtures of multinomial logistic models (MNL),
which are known to $\epsilon$-approximate any random utility model. Despite its
long history and broad use, rigorous results are only available for learning a
uniform mixture of two MNLs. Continuing this line of research, we study the
problem of learning an arbitrary mixture of two MNLs. We show that the
identifiability of the mixture models may only fail on an algebraic variety of
a negligible measure. This is done by reducing the problem of learning a
mixture of two MNLs to the problem of solving a system of univariate quartic
equations. We also devise an algorithm to learn any mixture of two MNLs using a
polynomial number of samples and a linear number of queries, provided that a
mixture of two MNLs over some finite universe is identifiable. Several
numerical experiments and conjectures are also presented.
</p>
<a href="http://arxiv.org/abs/2007.00204">arXiv:2007.00204</a> [<a href="http://arxiv.org/pdf/2007.00204">pdf</a>]

<h2>Generating Adversarial Examples with Controllable Non-transferability. (arXiv:2007.01299v2 [cs.CR] UPDATED)</h2>
<h3>Renzhi Wang, Tianwei Zhang, Xiaofei Xie, Lei Ma, Cong Tian, Felix Juefei-Xu, Yang Liu</h3>
<p>Adversarial attacks against Deep Neural Networks have been widely studied.
One significant feature that makes such attacks particularly powerful is
transferability, where the adversarial examples generated from one model can be
effective against other similar models as well. A large number of works have
been done to increase the transferability. However, how to decrease the
transferability and craft malicious samples only for specific target models are
not explored yet.

In this paper, we design novel attack methodologies to generate adversarial
examples with controllable non-transferability. With these methods, an
adversary can efficiently produce precise adversarial examples to attack a set
of target models he desires, while keeping benign to other models. The first
method is Reversed Loss Function Ensemble, where the adversary can craft
qualified examples from the gradients of a reversed loss function. This
approach is effective for the white-box and gray-box settings. The second
method is Transferability Classification: the adversary trains a
transferability-aware classifier from the perturbations of adversarial
examples. This classifier further provides the guidance for the generation of
non-transferable adversarial examples. This approach can be applied to the
black-box scenario. Evaluation results demonstrate the effectiveness and
efficiency of our proposed methods. This work opens up a new route for
generating adversarial examples with new features and applications.
</p>
<a href="http://arxiv.org/abs/2007.01299">arXiv:2007.01299</a> [<a href="http://arxiv.org/pdf/2007.01299">pdf</a>]

<h2>Meta-Learning through Hebbian Plasticity in Random Networks. (arXiv:2007.02686v2 [cs.NE] UPDATED)</h2>
<h3>Elias Najarro, Sebastian Risi</h3>
<p>Lifelong learning and adaptability are two defining aspects of biological
agents. Modern reinforcement learning (RL) approaches have shown significant
progress in solving complex tasks, however once training is concluded, the
found solutions are typically static and incapable of adapting to new
information or perturbations. While it is still not completely understood how
biological brains learn and adapt so efficiently from experience, it is
believed that synaptic plasticity plays a prominent role in this process.
Inspired by this biological mechanism, we propose a search method that, instead
of optimizing the weight parameters of neural networks directly, only searches
for synapse-specific Hebbian learning rules that allow the network to
continuously self-organize its weights during the lifetime of the agent. We
demonstrate our approach on several reinforcement learning tasks with different
sensory modalities and more than 450K trainable plasticity parameters. We find
that starting from completely random weights, the discovered Hebbian rules
enable an agent to navigate a dynamical 2D-pixel environment; likewise they
allow a simulated 3D quadrupedal robot to learn how to walk while adapting to
morphological damage not seen during training and in the absence of any
explicit reward or error signal in less than 100 timesteps. Code is available
at https://github.com/enajx/HebbianMetaLearning
</p>
<a href="http://arxiv.org/abs/2007.02686">arXiv:2007.02686</a> [<a href="http://arxiv.org/pdf/2007.02686">pdf</a>]

<h2>Continual BERT: Continual Learning for Adaptive Extractive Summarization of COVID-19 Literature. (arXiv:2007.03405v2 [cs.CL] UPDATED)</h2>
<h3>Jong Won Park</h3>
<p>The scientific community continues to publish an overwhelming amount of new
research related to COVID-19 on a daily basis, leading to much literature
without little to no attention. To aid the community in understanding the
rapidly flowing array of COVID-19 literature, we propose a novel BERT
architecture that provides a brief yet original summarization of lengthy
papers. The model continually learns on new data in online fashion while
minimizing catastrophic forgetting, thus fitting to the need of the community.
Benchmark and manual examination of its performance show that the model provide
a sound summary of new scientific literature.
</p>
<a href="http://arxiv.org/abs/2007.03405">arXiv:2007.03405</a> [<a href="http://arxiv.org/pdf/2007.03405">pdf</a>]

<h2>Memory Based Attentive Fusion. (arXiv:2007.08076v2 [cs.LG] UPDATED)</h2>
<h3>Darshana Priyasad, Tharindu Fernando, Simon Denman, Sridha Sridharan, Clinton Fookes</h3>
<p>The use of multi-modal data for deep machine learning has shown promise when
compared to uni-modal approaches with fusion of multi-modal features resulting
in improved performance in several applications. However, most state-of-the-art
methods use naive fusion which processes feature streams independently,
ignoring possible long-term dependencies within the data during fusion. In this
paper, we present a novel Memory based Attentive Fusion layer, which fuses
modes by incorporating both the current features and longterm dependencies in
the data, thus allowing the model to understand the relative importance of
modes over time. We introduce an explicit memory block within the fusion layer
which stores features containing long-term dependencies of the fused data. The
feature inputs from uni-modal encoders are fused through attentive composition
and transformation followed by naive fusion of the resultant memory derived
features with layer inputs. Following state-of-the-art methods, we have
evaluated the performance and the generalizability of the proposed fusion
approach on two different datasets with different modalities. In our
experiments, we replace the naive fusion layer in benchmark networks with our
proposed layer to enable a fair comparison. Experimental results indicate that
the MBAF layer can generalise across different modalities and networks to
enhance fusion and improve performance.
</p>
<a href="http://arxiv.org/abs/2007.08076">arXiv:2007.08076</a> [<a href="http://arxiv.org/pdf/2007.08076">pdf</a>]

<h2>Learning Differentiable Programs with Admissible Neural Heuristics. (arXiv:2007.12101v3 [cs.LG] UPDATED)</h2>
<h3>Ameesh Shah, Eric Zhan, Jennifer J. Sun, Abhinav Verma, Yisong Yue, Swarat Chaudhuri</h3>
<p>We study the problem of learning differentiable functions expressed as
programs in a domain-specific language. Such programmatic models can offer
benefits such as composability and interpretability; however, learning them
requires optimizing over a combinatorial space of program "architectures". We
frame this optimization problem as a search in a weighted graph whose paths
encode top-down derivations of program syntax. Our key innovation is to view
various classes of neural networks as continuous relaxations over the space of
programs, which can then be used to complete any partial program. This relaxed
program is differentiable and can be trained end-to-end, and the resulting
training loss is an approximately admissible heuristic that can guide the
combinatorial search. We instantiate our approach on top of the A-star
algorithm and an iteratively deepened branch-and-bound search, and use these
algorithms to learn programmatic classifiers in three sequence classification
tasks. Our experiments show that the algorithms outperform state-of-the-art
methods for program learning, and that they discover programmatic classifiers
that yield natural interpretations and achieve competitive accuracy.
</p>
<a href="http://arxiv.org/abs/2007.12101">arXiv:2007.12101</a> [<a href="http://arxiv.org/pdf/2007.12101">pdf</a>]

<h2>Visualizing classification results. (arXiv:2007.14495v2 [stat.ML] UPDATED)</h2>
<h3>Jakob Raymaekers, Peter J. Rousseeuw, Mia Hubert</h3>
<p>Classification is a major tool of statistics and machine learning. A
classification method first processes a training set of objects with given
classes (labels), with the goal of afterward assigning new objects to one of
these classes. When running the resulting prediction method on the training
data or on test data, it can happen that an object is predicted to lie in a
class that differs from its given label. This is sometimes called label bias,
and raises the question whether the object was mislabeled. Our goal is to
visualize aspects of the data classification to obtain insight. The proposed
display reflects to what extent each object's label is (dis)similar to its
prediction, how far each object lies from the other objects in its class, and
whether some objects lie far from all classes. The display is constructed for
discriminant analysis, the k-nearest neighbor classifier, support vector
machines, logistic regression, and majority voting. It is illustrated on
several benchmark datasets containing images and texts.
</p>
<a href="http://arxiv.org/abs/2007.14495">arXiv:2007.14495</a> [<a href="http://arxiv.org/pdf/2007.14495">pdf</a>]

<h2>Presentation and Analysis of a Multimodal Dataset for Grounded Language Learning. (arXiv:2007.14987v4 [cs.RO] UPDATED)</h2>
<h3>Patrick Jenkins, Rishabh Sachdeva, Gaoussou Youssouf Kebe, Padraig Higgins, Kasra Darvish, Edward Raff, Don Engel, John Winder, Francis Ferraro, Cynthia Matuszek</h3>
<p>Grounded language acquisition -- learning how language-based interactions
refer to the world around them -- is amajor area of research in robotics, NLP,
and HCI. In practice the data used for learning consists almost entirely of
textual descriptions, which tend to be cleaner, clearer, and more grammatical
than actual human interactions. In this work, we present the Grounded Language
Dataset (GoLD), a multimodal dataset of common household objects described by
people using either spoken or written language. We analyze the differences and
present an experiment showing how the different modalities affect language
learning from human in-put. This will enable researchers studying the
intersection of robotics, NLP, and HCI to better investigate how the multiple
modalities of image, text, and speech interact, as well as show differences in
the vernacular of these modalities impact results.
</p>
<a href="http://arxiv.org/abs/2007.14987">arXiv:2007.14987</a> [<a href="http://arxiv.org/pdf/2007.14987">pdf</a>]

<h2>Accurate and Efficient Intracranial Hemorrhage Detection and Subtype Classification in 3D CT Scans with Convolutional and Long Short-Term Memory Neural Networks. (arXiv:2008.00302v2 [cs.CV] UPDATED)</h2>
<h3>Mihail Burduja, Radu Tudor Ionescu, Nicolae Verga</h3>
<p>In this paper, we present our system for the RSNA Intracranial Hemorrhage
Detection challenge. The proposed system is based on a lightweight deep neural
network architecture composed of a convolutional neural network (CNN) that
takes as input individual CT slices, and a Long Short-Term Memory (LSTM)
network that takes as input feature embeddings provided by the CNN. For
efficient processing, we consider various feature selection methods to produce
a subset of useful CNN features for the LSTM. Furthermore, we reduce the CT
slices by a factor of 2x, allowing ourselves to train the model faster. Even if
our model is designed to balance speed and accuracy, we report a weighted mean
log loss of 0.04989 on the final test set, which places us in the top 30
ranking (2%) from a total of 1345 participants. Although our computing
infrastructure does not allow it, processing CT slices at their original scale
is likely to improve performance. In order to enable others to reproduce our
results, we provide our code as open source at
https://github.com/warchildmd/ihd. After the challenge, we conducted a
subjective intracranial hemorrhage detection assessment by radiologists,
indicating that the performance of our deep model is on par with that of
doctors specialized in reading CT scans. Another contribution of our work is to
integrate Grad-CAM visualizations in our system, providing useful explanations
for its predictions. We therefore consider our system as a viable option when a
fast diagnosis or a second opinion on intracranial hemorrhage detection are
needed.
</p>
<a href="http://arxiv.org/abs/2008.00302">arXiv:2008.00302</a> [<a href="http://arxiv.org/pdf/2008.00302">pdf</a>]

<h2>Hierarchial Reinforcement Learning in StarCraft II with Human Expertise in Subgoals Selection. (arXiv:2008.03444v2 [cs.AI] UPDATED)</h2>
<h3>Xinyi Xu, Tiancheng Huang, Pengfei Wei, Akshay Narayan, Tze-Yun Leong</h3>
<p>This work is inspired by recent advances in hierarchical reinforcement
learning (HRL) (Barto and Mahadevan 2003; Hengst 2010), and improvements in
learning efficiency from heuristic-based subgoal selection, experience replay
(Lin 1993; Andrychowicz et al. 2017), and task-based curriculum learning
(Bengio et al. 2009; Zaremba and Sutskever 2014). We propose a new method to
integrate HRL, experience replay and effective subgoal selection through an
implicit curriculum design based on human expertise to support sample-efficient
learning and enhance interpretability of the agent's behavior. Human expertise
remains indispensable in many areas such as medicine (Buch, Ahmed, and
Maruthappu 2018) and law (Cath 2018), where interpretability, explainability
and transparency are crucial in the decision making process, for ethical and
legal reasons. Our method simplifies the complex task sets for achieving the
overall objectives by decomposing them into subgoals at different levels of
abstraction. Incorporating relevant subjective knowledge also significantly
reduces the computational resources spent in exploration for RL, especially in
high speed, changing, and complex environments where the transition dynamics
cannot be effectively learned and modelled in a short time. Experimental
results in two StarCraft II (SC2) (Vinyals et al. 2017) minigames demonstrate
that our method can achieve better sample efficiency than flat and end-to-end
RL methods, and provides an effective method for explaining the agent's
performance.
</p>
<a href="http://arxiv.org/abs/2008.03444">arXiv:2008.03444</a> [<a href="http://arxiv.org/pdf/2008.03444">pdf</a>]

<h2>The nlogistic-sigmoid function. (arXiv:2008.04210v2 [cs.NE] UPDATED)</h2>
<h3>Oluwasegun A. Somefun, Kayode Akingbade, Folasade Dahunsi</h3>
<p>The variants of the logistic-sigmoid functions used in artificial neural
networks are inherently, by definition, limited by vanishing gradients.
Defining the logistic-sigmoid function to become n-times repeated over a finite
input-output mapping can significantly reduce the presence of this limitation.
Here we propose the nlogistic-sigmoid function as a generalization for the
definition of logistic-sigmoid functions. Our results demonstrate that by its
definition, the nlogistic-sigmoid function can reduce this vanishing of
gradients, as it outperforms both the classic logistic-sigmoid function and the
rectified linear-unit function in terms of learning and generalization on two
representative case studies. We anticipate that this function will be the
choice sigmoid activation function for deep learning.
</p>
<a href="http://arxiv.org/abs/2008.04210">arXiv:2008.04210</a> [<a href="http://arxiv.org/pdf/2008.04210">pdf</a>]

<h2>Federated Learning via Synthetic Data. (arXiv:2008.04489v2 [cs.LG] UPDATED)</h2>
<h3>Jack Goetz, Ambuj Tewari</h3>
<p>Federated learning allows for the training of a model using data on multiple
clients without the clients transmitting that raw data. However the standard
method is to transmit model parameters (or updates), which for modern neural
networks can be on the scale of millions of parameters, inflicting significant
computational costs on the clients. We propose a method for federated learning
where instead of transmitting a gradient update back to the server, we instead
transmit a small amount of synthetic `data'. We describe the procedure and show
some experimental results suggesting this procedure has potential, providing
more than an order of magnitude reduction in communication costs with minimal
model degradation.
</p>
<a href="http://arxiv.org/abs/2008.04489">arXiv:2008.04489</a> [<a href="http://arxiv.org/pdf/2008.04489">pdf</a>]

<h2>RAF-AU Database: In-the-Wild Facial Expressions with Subjective Emotion Judgement and Objective AU Annotations. (arXiv:2008.05196v3 [cs.CV] UPDATED)</h2>
<h3>Wenjing Yan, Shan Li, Chengtao Que, JiQuan Pei, Weihong Deng</h3>
<p>Much of the work on automatic facial expression recognition relies on
databases containing a certain number of emotion classes and their exaggerated
facial configurations (generally six prototypical facial expressions), based on
Ekman's Basic Emotion Theory. However, recent studies have revealed that facial
expressions in our human life can be blended with multiple basic emotions. And
the emotion labels for these in-the-wild facial expressions cannot easily be
annotated solely on pre-defined AU patterns. How to analyze the action units
for such complex expressions is still an open question. To address this issue,
we develop a RAF-AU database that employs a sign-based (i.e., AUs) and
judgement-based (i.e., perceived emotion) approach to annotating blended facial
expressions in the wild. We first reviewed the annotation methods in existing
databases and identified crowdsourcing as a promising strategy for labeling
in-the-wild facial expressions. Then, RAF-AU was finely annotated by
experienced coders, on which we also conducted a preliminary investigation of
which key AUs contribute most to a perceived emotion, and the relationship
between AUs and facial expressions. Finally, we provided a baseline for AU
recognition in RAF-AU using popular features and multi-label learning methods.
</p>
<a href="http://arxiv.org/abs/2008.05196">arXiv:2008.05196</a> [<a href="http://arxiv.org/pdf/2008.05196">pdf</a>]

<h2>Towards Unsupervised Crowd Counting via Regression-Detection Bi-knowledge Transfer. (arXiv:2008.05383v2 [cs.CV] UPDATED)</h2>
<h3>Yuting Liu, Zheng Wang, Miaojing Shi, Shin&#x27;ichi Satoh, Qijun Zhao, Hongyu Yang</h3>
<p>Unsupervised crowd counting is a challenging yet not largely explored task.
In this paper, we explore it in a transfer learning setting where we learn to
detect and count persons in an unlabeled target set by transferring
bi-knowledge learnt from regression- and detection-based models in a labeled
source set. The dual source knowledge of the two models is heterogeneous and
complementary as they capture different modalities of the crowd distribution.
We formulate the mutual transformations between the outputs of regression- and
detection-based models as two scene-agnostic transformers which enable
knowledge distillation between the two models. Given the regression- and
detection-based models and their mutual transformers learnt in the source, we
introduce an iterative self-supervised learning scheme with
regression-detection bi-knowledge transfer in the target. Extensive experiments
on standard crowd counting benchmarks, ShanghaiTech, UCF\_CC\_50, and UCF\_QNRF
demonstrate a substantial improvement of our method over other
state-of-the-arts in the transfer learning setting.
</p>
<a href="http://arxiv.org/abs/2008.05383">arXiv:2008.05383</a> [<a href="http://arxiv.org/pdf/2008.05383">pdf</a>]

<h2>Geometric Deep Learning for Post-Menstrual Age Prediction based on the Neonatal White Matter Cortical Surface. (arXiv:2008.06098v2 [cs.CV] UPDATED)</h2>
<h3>Vitalis Vosylius, Andy Wang, Cemlyn Waters, Alexey Zakharov, Francis Ward, Loic Le Folgoc, John Cupitt, Antonios Makropoulos, Andreas Schuh, Daniel Rueckert, Amir Alansary</h3>
<p>Accurate estimation of the age in neonates is essential for measuring
neurodevelopmental, medical, and growth outcomes. In this paper, we propose a
novel approach to predict the post-menstrual age (PA) at scan, using techniques
from geometric deep learning, based on the neonatal white matter cortical
surface. We utilize and compare multiple specialized neural network
architectures that predict the age using different geometric representations of
the cortical surface; we compare MeshCNN, Pointnet++, GraphCNN, and a
volumetric benchmark. The dataset is part of the Developing Human Connectome
Project (dHCP), and is a cohort of healthy and premature neonates. We evaluate
our approach on 650 subjects (727scans) with PA ranging from 27 to 45 weeks.
Our results show accurate prediction of the estimated PA, with mean error less
than one week.
</p>
<a href="http://arxiv.org/abs/2008.06098">arXiv:2008.06098</a> [<a href="http://arxiv.org/pdf/2008.06098">pdf</a>]

<h2>Polyth-Net: Classification of Polythene Bags for Garbage Segregation Using Deep Learning. (arXiv:2008.07592v2 [cs.CV] UPDATED)</h2>
<h3>Divyansh Singh</h3>
<p>Polythene has always been a threat to the environment since its invention. It
is non-biodegradable and very difficult to recycle. Even after many awareness
campaigns and practices, Separation of polythene bags from waste has been a
challenge for human civilization. The primary method of segregation deployed is
manual handpicking, which causes a dangerous health hazards to the workers and
is also highly inefficient due to human errors. In this paper I have designed
and researched on image-based classification of polythene bags using a
deep-learning model and its efficiency. This paper focuses on the architecture
and statistical analysis of its performance on the data set as well as problems
experienced in the classification. It also suggests a modified loss function to
specifically detect polythene irrespective of its individual features. It aims
to help the current environment protection endeavours and save countless lives
lost to the hazards caused by current methods.
</p>
<a href="http://arxiv.org/abs/2008.07592">arXiv:2008.07592</a> [<a href="http://arxiv.org/pdf/2008.07592">pdf</a>]

<h2>Communicative Reinforcement Learning Agents for Landmark Detection in Brain Images. (arXiv:2008.08055v2 [cs.CV] UPDATED)</h2>
<h3>Guy Leroy, Daniel Rueckert, Amir Alansary</h3>
<p>Accurate detection of anatomical landmarks is an essential step in several
medical imaging tasks. We propose a novel communicative multi-agent
reinforcement learning (C-MARL) system to automatically detect landmarks in 3D
brain images. C-MARL enables the agents to learn explicit communication
channels, as well as implicit communication signals by sharing certain weights
of the architecture among all the agents. The proposed approach is evaluated on
two brain imaging datasets from adult magnetic resonance imaging (MRI) and
fetal ultrasound scans. Our experiments show that involving multiple
cooperating agents by learning their communication with each other outperforms
previous approaches using single agents.
</p>
<a href="http://arxiv.org/abs/2008.08055">arXiv:2008.08055</a> [<a href="http://arxiv.org/pdf/2008.08055">pdf</a>]

<h2>Multi-Agent Reinforcement Learning with Graph Clustering. (arXiv:2008.08808v2 [cs.AI] UPDATED)</h2>
<h3>Tianze Zhou, Fubiao Zhang, Pan Tang, Chenfei Wang</h3>
<p>In this paper, the group concept is introduced into multi-agent reinforcement
learning. Agents, in this method, are divided into several groups, each of
which completes a specific subtask, cooperating to accomplish the main task. In
order to exchange information between agents, present methods mainly use the
communication vector; this can lead to communication redundancy. To solve this
problem, a MARL based method is proposed on graph clustering. It allows agents
to learn group features adaptively and replaces the communication operation. In
this approach, agent features are divided into two types, including in-group
and individual features. The generality and differences between agents are
represented by them, respectively. Based on the graph attention network(GAT),
the graph clustering method is introduced to optimize agent group feature.
These features are then applied to generate individual Q value. The split loss
is presented to distinguish agent features in order to overcome the consistent
problem brought by GAT. The proposed method is easy to be converted into the
CTDE framework by using the Kullback-Leibler divergence method. Empirical
results are evaluated on a challenging set of StarCraft II micromanagement
tasks. The result reveals that the proposed method achieves significant
performance improvements in the SMAC domain, and can maintain a great
performance with the increase in the number of agents.
</p>
<a href="http://arxiv.org/abs/2008.08808">arXiv:2008.08808</a> [<a href="http://arxiv.org/pdf/2008.08808">pdf</a>]

<h2>Perceptual underwater image enhancement with deep learning and physical priors. (arXiv:2008.09697v2 [cs.CV] UPDATED)</h2>
<h3>Long Chen, Zheheng Jiang, Lei Tong, Zhihua Liu, Aite Zhao, Qianni Zhang, Junyu Dong, Huiyu Zhou</h3>
<p>Underwater image enhancement, as a pre-processing step to improve the
accuracy of the following object detection task, has drawn considerable
attention in the field of underwater navigation and ocean exploration. However,
most of the existing underwater image enhancement strategies tend to consider
enhancement and detection as two independent modules with no interaction, and
the practice of separate optimization does not always help the underwater
object detection task. In this paper, we propose two perceptual enhancement
models, each of which uses a deep enhancement model with a detection perceptor.
The detection perceptor provides coherent information in the form of gradients
to the enhancement model, guiding the enhancement model to generate patch level
visually pleasing images or detection favourable images. In addition, due to
the lack of training data, a hybrid underwater image synthesis model, which
fuses physical priors and data-driven cues, is proposed to synthesize training
data and generalise our enhancement model for real-world underwater images.
Experimental results show the superiority of our proposed method over several
state-of-the-art methods on both real-world and synthetic underwater datasets.
</p>
<a href="http://arxiv.org/abs/2008.09697">arXiv:2008.09697</a> [<a href="http://arxiv.org/pdf/2008.09697">pdf</a>]

<h2>Recognition Oriented Iris Image Quality Assessment in the Feature Space. (arXiv:2009.00294v2 [eess.IV] UPDATED)</h2>
<h3>Leyuan Wang, Kunbo Zhang, Min Ren, Yunlong Wang, Zhenan Sun</h3>
<p>A large portion of iris images captured in real world scenarios are poor
quality due to the uncontrolled environment and the non-cooperative subject. To
ensure that the recognition algorithm is not affected by low-quality images,
traditional hand-crafted factors based methods discard most images, which will
cause system timeout and disrupt user experience. In this paper, we propose a
recognition-oriented quality metric and assessment method for iris image to
deal with the problem. The method regards the iris image embeddings Distance in
Feature Space (DFS) as the quality metric and the prediction is based on deep
neural networks with the attention mechanism. The quality metric proposed in
this paper can significantly improve the performance of the recognition
algorithm while reducing the number of images discarded for recognition, which
is advantageous over hand-crafted factors based iris quality assessment
methods. The relationship between Image Rejection Rate (IRR) and Equal Error
Rate (EER) is proposed to evaluate the performance of the quality assessment
algorithm under the same image quality distribution and the same recognition
algorithm. Compared with hand-crafted factors based methods, the proposed
method is a trial to bridge the gap between the image quality assessment and
biometric recognition. The code is available at
https://github.com/Debatrix/DFSNet.
</p>
<a href="http://arxiv.org/abs/2009.00294">arXiv:2009.00294</a> [<a href="http://arxiv.org/pdf/2009.00294">pdf</a>]

<h2>How Visualization PhD Students Cope with Paper Rejections. (arXiv:2009.00406v5 [cs.HC] UPDATED)</h2>
<h3>Shivam Agarwal, Shahid Latif, Fabian Beck</h3>
<p>We conducted a questionnaire study aimed towards PhD students in the field of
visualization research to understand how they cope with paper rejections. We
collected responses from 24 participants and performed a qualitative analysis
of the data in relation to the provided support by collaborators, resubmission
strategies, handling multiple rejects, and personal impression of the reviews.
The results indicate that the PhD students in the visualization community
generally cope well with the negative reviews and, with experience, learn how
to act accordingly to improve and resubmit their work. Our results reveal the
main coping strategies that can be applied for constructively handling rejected
visualization papers. The most prominent strategies include: discussing reviews
with collaborators and making a resubmission plan, doing a major revision to
improve the work, shortening the work, and seeing rejection as a positive
learning experience.
</p>
<a href="http://arxiv.org/abs/2009.00406">arXiv:2009.00406</a> [<a href="http://arxiv.org/pdf/2009.00406">pdf</a>]

<h2>High-Resolution Poverty Maps in Sub-Saharan Africa. (arXiv:2009.00544v4 [cs.CY] UPDATED)</h2>
<h3>Kamwoo Lee, Jeanine Braithwaite</h3>
<p>Up-to-date poverty maps are an important tool for policy makers, but until
now, have been prohibitively expensive to produce. We propose a generalizable
prediction methodology to produce poverty maps at the village level using
geospatial data and machine learning algorithms. We tested the proposed method
for 25 Sub-Saharan African countries and validated them against survey data.
The proposed method can increase the validity of both single country and
cross-country estimations leading to higher precision in poverty maps of 44
Sub-Saharan African countries than previously available. More importantly, our
cross-country estimation enables the creation of poverty maps when it is not
practical or cost-effective to field new national household surveys, as is the
case with many low- and middle-income countries.
</p>
<a href="http://arxiv.org/abs/2009.00544">arXiv:2009.00544</a> [<a href="http://arxiv.org/pdf/2009.00544">pdf</a>]

<h2>Twitter Corpus of the #BlackLivesMatter Movement And Counter Protests: 2013 to 2020. (arXiv:2009.00596v2 [cs.SI] UPDATED)</h2>
<h3>Salvatore Giorgi, Sharath Chandra Guntuku, Muhammad Rahman, McKenzie Himelein-Wachowiak, Amy Kwarteng, Brenda Curtis</h3>
<p>Black Lives Matter (BLM) is a grassroots movement protesting violence towards
Black individuals and communities with a focus on police brutality. The
movement has gained significant media and political attention following the
killings of Ahmaud Arbery, Breonna Taylor, and George Floyd and the shooting of
Jacob Blake in 2020. Due to its decentralized nature, the #BlackLivesMatter
social media hashtag has come to both represent the movement and been used as a
call to action. Similar hashtags have appeared to counter the BLM movement,
such as #AllLivesMatter and #BlueLivesMatter. We introduce a data set of 41.8
million tweets from 10 million users which contain one of the following
keywords: BlackLivesMatter, AllLivesMatter and BlueLivesMatter. This data set
contains all currently available tweets from the beginning of the BLM movement
in 2013 to June 2020. We summarize the data set and show temporal trends in use
of both the BlackLivesMatter keyword and keywords associated with counter
movements. In the past, similarly themed, though much smaller in scope, BLM
data sets have been used for studying discourse in protest and counter protest
movements, predicting retweets, examining the role of social media in protest
movements and exploring narrative agency. This paper open-sources a large-scale
data set to facilitate research in the areas of computational social science,
communications, political science, natural language processing, and machine
learning.
</p>
<a href="http://arxiv.org/abs/2009.00596">arXiv:2009.00596</a> [<a href="http://arxiv.org/pdf/2009.00596">pdf</a>]

<h2>MALCOM: Generating Malicious Comments to Attack Neural Fake News Detection Models. (arXiv:2009.01048v2 [cs.CL] UPDATED)</h2>
<h3>Thai Le, Suhang Wang, Dongwon Lee</h3>
<p>In recent years, the proliferation of so-called "fake news" has caused much
disruptions in society and weakened the news ecosystem. Therefore, to mitigate
such problems, researchers have developed state-of-the-art models to
auto-detect fake news on social media using sophisticated data science and
machine learning techniques. In this work, then, we ask "what if adversaries
attempt to attack such detection models?" and investigate related issues by (i)
proposing a novel threat model against fake news detectors, in which
adversaries can post malicious comments toward news articles to mislead fake
news detectors, and (ii) developing MALCOM, an end-to-end adversarial comment
generation framework to achieve such an attack. Through a comprehensive
evaluation, we demonstrate that about 94% and 93.5% of the time on average
MALCOM can successfully mislead five of the latest neural detection models to
always output targeted real and fake news labels. Furthermore, MALCOM can also
fool black box fake news detectors to always output real news labels 90% of the
time on average. We also compare our attack model with four baselines across
two real-world datasets, not only on attack performance but also on generated
quality, coherency, transferability, and robustness.
</p>
<a href="http://arxiv.org/abs/2009.01048">arXiv:2009.01048</a> [<a href="http://arxiv.org/pdf/2009.01048">pdf</a>]

<h2>KoSpeech: Open-Source Toolkit for End-to-End Korean Speech Recognition. (arXiv:2009.03092v2 [eess.AS] UPDATED)</h2>
<h3>Soohwan Kim, Seyoung Bae, Cheolhwang Won</h3>
<p>We present KoSpeech, an open-source software, which is modular and extensible
end-to-end Korean automatic speech recognition (ASR) toolkit based on the deep
learning library PyTorch. Several automatic speech recognition open-source
toolkits have been released, but all of them deal with non-Korean languages,
such as English (e.g. ESPnet, Espresso). Although AI Hub opened 1,000 hours of
Korean speech corpus known as KsponSpeech, there is no established
preprocessing method and baseline model to compare model performances.
Therefore, we propose preprocessing methods for KsponSpeech corpus and a
baseline model for benchmarks. Our baseline model is based on Listen, Attend
and Spell (LAS) architecture and ables to customize various training
hyperparameters conveniently. By KoSpeech, we hope this could be a guideline
for those who research Korean speech recognition. Our baseline model achieved
10.31% character error rate (CER) at KsponSpeech corpus only with the acoustic
model. Our source code is available here.
</p>
<a href="http://arxiv.org/abs/2009.03092">arXiv:2009.03092</a> [<a href="http://arxiv.org/pdf/2009.03092">pdf</a>]

<h2>QR-MIX: Distributional Value Function Factorisation for Cooperative Multi-Agent Reinforcement Learning. (arXiv:2009.04197v3 [cs.LG] UPDATED)</h2>
<h3>Jian Hu, Seth Austin Harding, Haibin Wu, Shih-wei Liao</h3>
<p>In Cooperative Multi-Agent Reinforcement Learning (MARL) and under the
setting of Centralized Training with Decentralized Execution (CTDE), agents
observe and interact with their environment locally and independently. With
local observation and random sampling, the randomness in rewards and
observations leads to randomness in long-term returns. Existing methods such as
Value Decomposition Network (VDN) and QMIX estimate the mean value of long-term
returns while ignoring randomness. Our proposed model QR-MIX introduces
quantile regression, modeling joint state-action values as a distribution,
combining QMIX with Implicit Quantile Network (IQN). Besides, because the
monotonicity in QMIX limits the expression of joint state-action value
distribution and may lead to incorrect estimation results in nonmonotonic
cases, we design a flexible loss function to replace the absolute weights found
in QMIX. Our methods enhance the expressiveness of our mixing network and are
more tolerant of randomness and nonmonotonicity. The experiments demonstrate
that QR-MIX outperforms prior works in the StarCraft Multi-Agent Challenge
(SMAC) environment.
</p>
<a href="http://arxiv.org/abs/2009.04197">arXiv:2009.04197</a> [<a href="http://arxiv.org/pdf/2009.04197">pdf</a>]

<h2>Self-Supervised Annotation of Seismic Images using Latent Space Factorization. (arXiv:2009.04631v2 [eess.IV] UPDATED)</h2>
<h3>Oluwaseun Joseph Aribido, Ghassan AlRegib, Mohamed Deriche</h3>
<p>Annotating seismic data is expensive, laborious and subjective due to the
number of years required for seismic interpreters to attain proficiency in
interpretation. In this paper, we develop a framework to automate annotating
pixels of a seismic image to delineate geological structural elements given
image-level labels assigned to each image. Our framework factorizes the latent
space of a deep encoder-decoder network by projecting the latent space to
learned sub-spaces. Using constraints in the pixel space, the seismic image is
further factorized to reveal confidence values on pixels associated with the
geological element of interest. Details of the annotated image are provided for
analysis and qualitative comparison is made with similar frameworks.
</p>
<a href="http://arxiv.org/abs/2009.04631">arXiv:2009.04631</a> [<a href="http://arxiv.org/pdf/2009.04631">pdf</a>]

<h2>GTEA: Representation Learning for Temporal Interaction Graphs via Edge Aggregation. (arXiv:2009.05266v2 [cs.LG] UPDATED)</h2>
<h3>Yiming Li, Da Sun Handason Tam, Siyue Xie, Xiaxin Liu, Qiu Fang Ying, Wing Cheong Lau, Dah Ming Chiu, Shou Zhi Chen</h3>
<p>We consider the problem of representation learning for temporal interaction
graphs where a network of entities with complex interactions over an extended
period of time is modeled as a graph with a rich set of node and edge
attributes. In particular, an edge between a node-pair within the graph
corresponds to a multi-dimensional time-series. To fully capture and model the
dynamics of the network, we propose GTEA, a framework of representation
learning for temporal interaction graphs with per-edge time-based aggregation.
Under GTEA, a Graph Neural Network (GNN) is integrated with a state-of-the-art
sequence model, such as LSTM, Transformer and their time-aware variants. The
sequence model generates edge embeddings to encode temporal interaction
patterns between each pair of nodes, while the GNN-based backbone learns the
topological dependencies and relationships among different nodes. GTEA also
incorporates a sparsity-inducing self-attention mechanism to distinguish and
focus on the more important neighbors of each node during the aggregation
process. By capturing temporal interactive dynamics together with
multi-dimensional node and edge attributes in a network, GTEA can learn
fine-grained representations for a temporal interaction graph to enable or
facilitate other downstream data analytic tasks. Experimental results show that
GTEA outperforms state-of-the-art schemes including GraphSAGE, APPNP, and TGAT
by delivering higher accuracy (100.00%, 98.51%, 98.05% ,79.90%) and macro-F1
score (100.00%, 98.51%, 96.68% ,79.90%) over four large-scale real-world
datasets for binary/ multi-class node classification.
</p>
<a href="http://arxiv.org/abs/2009.05266">arXiv:2009.05266</a> [<a href="http://arxiv.org/pdf/2009.05266">pdf</a>]

<h2>Compressed Deep Networks: Goodbye SVD, Hello Robust Low-Rank Approximation. (arXiv:2009.05647v2 [cs.LG] UPDATED)</h2>
<h3>Murad Tukan, Alaa Maalouf, Matan Weksler, Dan Feldman</h3>
<p>A common technique for compressing a neural network is to compute the
$k$-rank $\ell_2$ approximation $A_{k,2}$ of the matrix
$A\in\mathbb{R}^{n\times d}$ that corresponds to a fully connected layer (or
embedding layer). Here, $d$ is the number of the neurons in the layer, $n$ is
the number in the next one, and $A_{k,2}$ can be stored in $O((n+d)k)$ memory
instead of $O(nd)$.

This $\ell_2$-approximation minimizes the sum over every entry to the power
of $p=2$ in the matrix $A - A_{k,2}$, among every matrix
$A_{k,2}\in\mathbb{R}^{n\times d}$ whose rank is $k$. While it can be computed
efficiently via SVD, the $\ell_2$-approximation is known to be very sensitive
to outliers ("far-away" rows). Hence, machine learning uses e.g. Lasso
Regression, $\ell_1$-regularization, and $\ell_1$-SVM that use the
$\ell_1$-norm.

This paper suggests to replace the $k$-rank $\ell_2$ approximation by
$\ell_p$, for $p\in [1,2]$. We then provide practical and provable
approximation algorithms to compute it for any $p\geq1$, based on modern
techniques in computational geometry.

Extensive experimental results on the GLUE benchmark for compressing BERT,
DistilBERT, XLNet, and RoBERTa confirm this theoretical advantage. For example,
our approach achieves $28\%$ compression of RoBERTa's embedding layer with only
$0.63\%$ additive drop in the accuracy (without fine-tuning) in average over
all tasks in GLUE, compared to $11\%$ drop using the existing
$\ell_2$-approximation. Open code is provided for reproducing and extending our
results.
</p>
<a href="http://arxiv.org/abs/2009.05647">arXiv:2009.05647</a> [<a href="http://arxiv.org/pdf/2009.05647">pdf</a>]

<h2>An Agent-Based Model of Delegation Relationships With Hidden-Action: On the Effects of Heterogeneous Memory on Performance. (arXiv:2009.07124v2 [cs.MA] UPDATED)</h2>
<h3>Patrick Reinwald, Stephan Leitner, Friederike Wall</h3>
<p>We introduce an agent-based model of delegation relationships between a
principal and an agent, which is based on the standard-hidden action model
introduced by Holmstr\"om and, by doing so, provide a model which can be used
to further explore theoretical topics in managerial economics, such as the
efficiency of incentive mechanisms. We employ the concept of agentization,
i.e., we systematically transform the standard hidden-action model into an
agent-based model. Our modeling approach allows for a relaxation of some of the
rather "heroic" assumptions included in the standard hidden-action model,
whereby we particularly focus on assumptions related to the (i) availability of
information about the environment and the (ii) principal's and agent's
cognitive capabilities (with a particular focus on their learning capabilities
and their memory). Our analysis focuses on how close and how fast the incentive
scheme, which endogenously emerges from the agent-based model, converges to the
solution proposed by the standard hidden-action model. Also, we investigate
whether a stable solution can emerge from the agent-based model variant. The
results show that in stable environments the emergent result can nearly reach
the solution proposed by the standard hidden-action model. Surprisingly, the
results indicate that turbulence in the environment leads to stability in
earlier time periods.
</p>
<a href="http://arxiv.org/abs/2009.07124">arXiv:2009.07124</a> [<a href="http://arxiv.org/pdf/2009.07124">pdf</a>]

<h2>SCREENet: A Multi-view Deep Convolutional Neural Network for Classification of High-resolution Synthetic Mammographic Screening Scans. (arXiv:2009.08563v3 [eess.IV] UPDATED)</h2>
<h3>Saeed Seyyedi, Margaret J. Wong, Debra M. Ikeda, Curtis P. Langlotz</h3>
<p>Purpose: To develop and evaluate the accuracy of a multi-view deep learning
approach to the analysis of high-resolution synthetic mammograms from digital
breast tomosynthesis screening cases, and to assess the effect on accuracy of
image resolution and training set size. Materials and Methods: In a
retrospective study, 21,264 screening digital breast tomosynthesis (DBT) exams
obtained at our institution were collected along with associated radiology
reports. The 2D synthetic mammographic images from these exams, with varying
resolutions and data set sizes, were used to train a multi-view deep
convolutional neural network (MV-CNN) to classify screening images into BI-RADS
classes (BI-RADS 0, 1 and 2) before evaluation on a held-out set of exams.

Results: Area under the receiver operating characteristic curve (AUC) for
BI-RADS 0 vs non-BI-RADS 0 class was 0.912 for the MV-CNN trained on the full
dataset. The model obtained accuracy of 84.8%, recall of 95.9% and precision of
95.0%. This AUC value decreased when the same model was trained with 50% and
25% of images (AUC = 0.877, P=0.010 and 0.834, P=0.009 respectively). Also, the
performance dropped when the same model was trained using images that were
under-sampled by 1/2 and 1/4 (AUC = 0.870, P=0.011 and 0.813, P=0.009
respectively).

Conclusion: This deep learning model classified high-resolution synthetic
mammography scans into normal vs needing further workup using tens of thousands
of high-resolution images. Smaller training data sets and lower resolution
images both caused significant decrease in performance.
</p>
<a href="http://arxiv.org/abs/2009.08563">arXiv:2009.08563</a> [<a href="http://arxiv.org/pdf/2009.08563">pdf</a>]

<h2>A Machine Learning Approach to Detect Suicidal Ideation in US Veterans Based on Acoustic and Linguistic Features of Speech. (arXiv:2009.09069v2 [cs.CY] UPDATED)</h2>
<h3>Vaibhav Sourirajan, Anas Belouali, Mary Ann Dutton, Matthew Reinhard, Jyotishman Pathak</h3>
<p>Preventing Veteran suicide is a national priority. The US Department of
Veterans Affairs (VA) collects, analyzes, and publishes data to inform suicide
prevention strategies. Current approaches for detecting suicidal ideation
mostly rely on patient self report which are inadequate and time consuming. In
this research study, our goal was to automate suicidal ideation detection from
acoustic and linguistic features of an individual's speech using machine
learning (ML) algorithms. Using voice data collected from Veterans enrolled in
a large interventional study on Gulf War Illness at the Washington DC VA
Medical Center, we conducted an evaluation of the performance of different ML
approaches in achieving our objective. By fitting both classical ML and deep
learning models to the dataset, we identified the algorithms that were most
effective for each feature set. Among classical machine learning algorithms,
the Support Vector Machine (SVM) trained on acoustic features performed best in
classifying suicidal Veterans. Among deep learning methods, the Convolutional
Neural Network (CNN) trained on the linguistic features performed best. Our
study shows that speech analysis in a machine learning pipeline is a promising
approach for detecting suicidality among Veterans.
</p>
<a href="http://arxiv.org/abs/2009.09069">arXiv:2009.09069</a> [<a href="http://arxiv.org/pdf/2009.09069">pdf</a>]

<h2>Computer Assisted Translation with Neural Quality Estimation and Automatic Post-Editing. (arXiv:2009.09126v2 [cs.CL] UPDATED)</h2>
<h3>Jiayi Wang, Ke Wang, Niyu Ge, Yangbing Shi, Yu Zhao, Kai Fan</h3>
<p>With the advent of neural machine translation, there has been a marked shift
towards leveraging and consuming the machine translation results. However, the
gap between machine translation systems and human translators needs to be
manually closed by post-editing. In this paper, we propose an end-to-end deep
learning framework of the quality estimation and automatic post-editing of the
machine translation output. Our goal is to provide error correction suggestions
and to further relieve the burden of human translators through an interpretable
model. To imitate the behavior of human translators, we design three efficient
delegation modules -- quality estimation, generative post-editing, and atomic
operation post-editing and construct a hierarchical model based on them. We
examine this approach with the English--German dataset from WMT 2017 APE shared
task and our experimental results can achieve the state-of-the-art performance.
We also verify that the certified translators can significantly expedite their
post-editing processing with our model in human evaluation.
</p>
<a href="http://arxiv.org/abs/2009.09126">arXiv:2009.09126</a> [<a href="http://arxiv.org/pdf/2009.09126">pdf</a>]

<h2>A Survey on Machine Learning Applied to Dynamic Physical Systems. (arXiv:2009.09719v2 [cs.LG] UPDATED)</h2>
<h3>Sagar Verma</h3>
<p>This survey is on recent advancements in the intersection of physical
modeling and machine learning. We focus on the modeling of nonlinear systems
which are closer to electric motors. Survey on motor control and fault
detection in operation of electric motors has been done.
</p>
<a href="http://arxiv.org/abs/2009.09719">arXiv:2009.09719</a> [<a href="http://arxiv.org/pdf/2009.09719">pdf</a>]

<h2>Conditional Automated Channel Pruning for Deep Neural Networks. (arXiv:2009.09724v2 [cs.CV] UPDATED)</h2>
<h3>Yixin Liu, Yong Guo, Zichang Liu, Haohua Liu, Jingjie Zhang, Zejun Chen, Jing Liu, Jian Chen</h3>
<p>Model compression aims to reduce the redundancy of deep networks to obtain
compact models. Recently, channel pruning has become one of the predominant
compression methods to deploy deep models on resource-constrained devices. Most
channel pruning methods often use a fixed compression rate for all the layers
of the model, which, however, may not be optimal. To address this issue, given
a target compression rate for the whole model, one can search for the optimal
compression rate for each layer. Nevertheless, these methods perform channel
pruning for a specific target compression rate. When we consider multiple
compression rates, they have to repeat the channel pruning process multiple
times, which is very inefficient yet unnecessary. To address this issue, we
propose a Conditional Automated Channel Pruning(CACP) method to obtain the
compressed models with different compression rates through single channel
pruning process. To this end, we develop a conditional model that takes an
arbitrary compression rate as input and outputs the corresponding compressed
model. In the experiments, the resultant models with different compression
rates consistently outperform the models compressed by existing methods with a
channel pruning process for each target compression rate.
</p>
<a href="http://arxiv.org/abs/2009.09724">arXiv:2009.09724</a> [<a href="http://arxiv.org/pdf/2009.09724">pdf</a>]

<h2>Energy-based Surprise Minimization for Multi-Agent Value Factorization. (arXiv:2009.09842v2 [cs.LG] UPDATED)</h2>
<h3>Karush Suri, Xiao Qi Shi, Konstantinos Plataniotis, Yuri Lawryshyn</h3>
<p>Multi-Agent Reinforcement Learning (MARL) has demonstrated significant
success in training decentralised policies in a centralised manner by making
use of value factorization methods. However, addressing surprise across
spurious states and approximation bias remain open problems for multi-agent
settings. We introduce the Energy-based MIXer (EMIX), an algorithm which
minimizes surprise utilizing the energy across agents. Our contributions are
threefold; (1) EMIX introduces a novel surprise minimization technique across
multiple agents in the case of multi-agent partially-observable settings. (2)
EMIX highlights the first practical use of energy functions in MARL (to our
knowledge) with theoretical guarantees and experiment validations of the energy
operator. Lastly, (3) EMIX presents a novel technique for addressing
overestimation bias across agents in MARL. When evaluated on a range of
challenging StarCraft II micromanagement scenarios, EMIX demonstrates
consistent state-of-the-art performance for multi-agent surprise minimization.
Moreover, our ablation study highlights the necessity of the energy-based
scheme and the need for elimination of overestimation bias in MARL. Our
implementation of EMIX and videos of agents are available at
https://karush17.github.io/emix-web/.
</p>
<a href="http://arxiv.org/abs/2009.09842">arXiv:2009.09842</a> [<a href="http://arxiv.org/pdf/2009.09842">pdf</a>]

<h2>"Click" Is Not Equal to "Like": Counterfactual Recommendation for Mitigating Clickbait Issue. (arXiv:2009.09945v2 [cs.IR] UPDATED)</h2>
<h3>Wenjie Wang, Fuli Feng, Xiangnan He, Hanwang Zhang, Tat-Seng Chua</h3>
<p>Recommendation is a prevalent and critical service in information systems. To
provide personalized suggestions to users, industry players embrace machine
learning, more specifically, building predictive models based on the click
behavior data. This is known as the Click-Through Rate (CTR) prediction, which
has become the gold standard for building personalized recommendation service.
However, we argue that there is a significant gap between clicks and user
satisfaction -- it is common that a user is "cheated" to click an item by the
attractive title/cover of the item. This will severely hurt user's trust on the
system if the user finds the actual content of the clicked item disappointing.
What's even worse, optimizing CTR models on such flawed data will result in the
Matthew Effect, making the seemingly attractive but actually low-quality items
be more frequently recommended. In this paper, we formulate the recommendation
process as a causal graph that reflects the cause-effect factors in
recommendation, and address the clickbait issue by performing counterfactual
inference on the causal graph. We imagine a counterfactual world where each
item has only exposure features (i.e., the features that the user can see
before making a click decision). By estimating the click likelihood of a user
in the counterfactual world, we are able to remove the effect of exposure
features and eliminate the clickbait issue. Experiments on real-world datasets
demonstrate that our method significantly improves the post-click satisfaction
of CTR models.
</p>
<a href="http://arxiv.org/abs/2009.09945">arXiv:2009.09945</a> [<a href="http://arxiv.org/pdf/2009.09945">pdf</a>]

<h2>Crafting Adversarial Examples for Deep Learning Based Prognostics (Extended Version). (arXiv:2009.10149v2 [cs.LG] UPDATED)</h2>
<h3>Gautam Raj Mode, Khaza Anuarul Hoque</h3>
<p>In manufacturing, unexpected failures are considered a primary operational
risk, as they can hinder productivity and can incur huge losses.
State-of-the-art Prognostics and Health Management (PHM) systems incorporate
Deep Learning (DL) algorithms and Internet of Things (IoT) devices to ascertain
the health status of equipment, and thus reduce the downtime, maintenance cost
and increase the productivity. Unfortunately, IoT sensors and DL algorithms,
both are vulnerable to cyber attacks, and hence pose a significant threat to
PHM systems. In this paper, we adopt the adversarial example crafting
techniques from the computer vision domain and apply them to the PHM domain.
Specifically, we craft adversarial examples using the Fast Gradient Sign Method
(FGSM) and Basic Iterative Method (BIM) and apply them on the Long Short-Term
Memory (LSTM), Gated Recurrent Unit (GRU), and Convolutional Neural Network
(CNN) based PHM models. We evaluate the impact of adversarial attacks using
NASA's turbofan engine dataset. The obtained results show that all the
evaluated PHM models are vulnerable to adversarial attacks and can cause a
serious defect in the remaining useful life estimation. The obtained results
also show that the crafted adversarial examples are highly transferable and may
cause significant damages to PHM systems.
</p>
<a href="http://arxiv.org/abs/2009.10149">arXiv:2009.10149</a> [<a href="http://arxiv.org/pdf/2009.10149">pdf</a>]

<h2>Dynamic Fusion based Federated Learning for COVID-19 Detection. (arXiv:2009.10401v3 [cs.DC] UPDATED)</h2>
<h3>Weishan Zhang, Tao Zhou, Qinghua Lu, Xiao Wang, Chunsheng Zhu, Haoyun Sun, Zhipeng Wang, Sin Kit Lo, Fei-Yue Wang</h3>
<p>Medical diagnostic image analysis (e.g., CT scan or X-Ray) using machine
learning is an efficient and accurate way to detect COVID-19 infections.
However, sharing diagnostic images across medical institutions is usually not
allowed due to the concern of patients' privacy. This causes the issue of
insufficient datasets for training the image classification model. Federated
learning is an emerging privacy-preserving machine learning paradigm that
produces an unbiased global model based on the received updates of local models
trained by clients without exchanging clients' local data. Nevertheless, the
default setting of federated learning introduces huge communication cost of
transferring model updates and can hardly ensure model performance when data
heterogeneity of clients heavily exists. To improve communication efficiency
and model performance, in this paper, we propose a novel dynamic fusion-based
federated learning approach for medical diagnostic image analysis to detect
COVID-19 infections. First, we design an architecture for dynamic fusion-based
federated learning systems to analyse medical diagnostic images. Further, we
present a dynamic fusion method to dynamically decide the participating clients
according to their local model performance and schedule the model fusion-based
on participating clients' training time. In addition, we summarise a category
of medical diagnostic image datasets for COVID-19 detection, which can be used
by the machine learning community for image analysis. The evaluation results
show that the proposed approach is feasible and performs better than the
default setting of federated learning in terms of model performance,
communication efficiency and fault tolerance.
</p>
<a href="http://arxiv.org/abs/2009.10401">arXiv:2009.10401</a> [<a href="http://arxiv.org/pdf/2009.10401">pdf</a>]

<h2>Dual Encoder Fusion U-Net (DEFU-Net) for Cross-manufacturer Chest X-ray Segmentation. (arXiv:2009.10608v2 [eess.IV] UPDATED)</h2>
<h3>Lipei Zhang, Aozhi Liu, Jing Xiao, Paul Taylor</h3>
<p>A number of methods based on the deep learning have been applied to medical
image segmentation and have achieved state-of-the-art performance. Due to the
importance of chest x-ray data in studying COVID-19, there is a demand for
state-of-the-art models capable of precisely segmenting soft tissue on the
chest x-rays before obtaining mask annotations about this sort of dataset. The
dataset for exploring best pre-trained model is from Montgomery and Shenzhen
hospital which had opened in 2014. The most famous technique is U-Net which has
been used to many medical datasets including the Chest X-ray. However, most
variant U-Nets mainly focus on extraction of contextual information and skip
connection. There is still a large space for improving extraction of spatial
features. In this paper, we propose a dual encoder fusion U-Net framework for
Chest X-rays based on Inception Convolutional Neural Network with dilation,
Densely Connected Recurrent Convolutional Neural Network, which is named
DEFU-Net. The densely connected recurrent path extends the network deeper for
facilitating context feature extraction. In order to increase the width of
network and enrich representation of features, the inception blocks with
dilation have been used. The inception blocks can capture globally and locally
spatial information by various receptive fields. At the same time, the two
paths are fused by summing features, thus preserving context and the spatial
information for decoding part. This multi-learning-scale model is benefiting in
Chest X-ray dataset from two different manufacturers (Montgomery and Shenzhen
hospital). The DEFU-Net achieves the better performance than basic U-Net,
residual U-Net, BCDU-Net, modified R2U-Net and modified attention R2U-Net. This
model has proved the feasibility for mixed dataset. The open source code for
this proposed framework will be public soon.
</p>
<a href="http://arxiv.org/abs/2009.10608">arXiv:2009.10608</a> [<a href="http://arxiv.org/pdf/2009.10608">pdf</a>]

<h2>AutoRC: Improving BERT Based Relation Classification Models via Architecture Search. (arXiv:2009.10680v2 [cs.CL] UPDATED)</h2>
<h3>Wei Zhu, Xipeng Qiu, Yuan Ni, Guotong Xie</h3>
<p>Although BERT based relation classification (RC) models have achieved
significant improvements over the traditional deep learning models, it seems
that no consensus can be reached on what is the optimal architecture. Firstly,
there are multiple alternatives for entity span identification. Second, there
are a collection of pooling operations to aggregate the representations of
entities and contexts into fixed length vectors. Third, it is difficult to
manually decide which feature vectors, including their interactions, are
beneficial for classifying the relation types. In this work, we design a
comprehensive search space for BERT based RC models and employ neural
architecture search (NAS) method to automatically discover the design choices
mentioned above. Experiments on seven benchmark RC tasks show that our method
is efficient and effective in finding better architectures than the baseline
BERT based RC model. Ablation study demonstrates the necessity of our search
space design and the effectiveness of our search method.
</p>
<a href="http://arxiv.org/abs/2009.10680">arXiv:2009.10680</a> [<a href="http://arxiv.org/pdf/2009.10680">pdf</a>]

<h2>Estimation error analysis of deep learning on the regression problem on the variable exponent Besov space. (arXiv:2009.11285v2 [stat.ML] UPDATED)</h2>
<h3>Kazuma Tsuji, Taiji Suzuki</h3>
<p>Deep learning has achieved notable success in various fields, including image
and speech recognition. One of the factors in the successful performance of
deep learning is its high feature extraction ability. In this study, we focus
on the adaptivity of deep learning; consequently, we treat the variable
exponent Besov space, which has a different smoothness depending on the input
location $x$. In other words, the difficulty of the estimation is not uniform
within the domain. We analyze the general approximation error of the variable
exponent Besov space and the approximation and estimation errors of deep
learning. We note that the improvement based on adaptivity is remarkable when
the region upon which the target function has less smoothness is small and the
dimension is large. Moreover, the superiority to linear estimators is shown
with respect to the convergence rate of the estimation error.
</p>
<a href="http://arxiv.org/abs/2009.11285">arXiv:2009.11285</a> [<a href="http://arxiv.org/pdf/2009.11285">pdf</a>]

<h2>Cloud Cover Nowcasting with Deep Learning. (arXiv:2009.11577v2 [cs.CV] UPDATED)</h2>
<h3>L&#xe9;a Berthomier, Bruno Pradel, Lior Perez</h3>
<p>Nowcasting is a field of meteorology which aims at forecasting weather on a
short term of up to a few hours. In the meteorology landscape, this field is
rather specific as it requires particular techniques, such as data
extrapolation, where conventional meteorology is generally based on physical
modeling. In this paper, we focus on cloud cover nowcasting, which has various
application areas such as satellite shots optimisation and photovoltaic energy
production forecast.

Following recent deep learning successes on multiple imagery tasks, we
applied deep convolutionnal neural networks on Meteosat satellite images for
cloud cover nowcasting. We present the results of several architectures
specialized in image segmentation and time series prediction. We selected the
best models according to machine learning metrics as well as meteorological
metrics. All selected architectures showed significant improvements over
persistence and the well-known U-Net surpasses AROME physical model.
</p>
<a href="http://arxiv.org/abs/2009.11577">arXiv:2009.11577</a> [<a href="http://arxiv.org/pdf/2009.11577">pdf</a>]

<h2>A Unifying Review of Deep and Shallow Anomaly Detection. (arXiv:2009.11732v2 [cs.LG] UPDATED)</h2>
<h3>Lukas Ruff, Jacob R. Kauffmann, Robert A. Vandermeulen, Gr&#xe9;goire Montavon, Wojciech Samek, Marius Kloft, Thomas G. Dietterich, Klaus-Robert M&#xfc;ller</h3>
<p>Deep learning approaches to anomaly detection have recently improved the
state of the art in detection performance on complex datasets such as large
collections of images or text. These results have sparked a renewed interest in
the anomaly detection problem and led to the introduction of a great variety of
new methods. With the emergence of numerous such methods, including approaches
based on generative models, one-class classification, and reconstruction, there
is a growing need to bring methods of this field into a systematic and unified
perspective. In this review we aim to identify the common underlying principles
as well as the assumptions that are often made implicitly by various methods.
In particular, we draw connections between classic 'shallow' and novel deep
approaches and show how this relation might cross-fertilize or extend both
directions. We further provide an empirical assessment of major existing
methods that is enriched by the use of recent explainability techniques, and
present specific worked-through examples together with practical advice.
Finally, we outline critical open challenges and identify specific paths for
future research in anomaly detection.
</p>
<a href="http://arxiv.org/abs/2009.11732">arXiv:2009.11732</a> [<a href="http://arxiv.org/pdf/2009.11732">pdf</a>]

<h2>Learning in a Small/Big World. (arXiv:2009.11917v2 [econ.TH] UPDATED)</h2>
<h3>Benson Tsz Kin Leung</h3>
<p>Savage (1972) lays down the foundation of Bayesian decision theory, but
asserts that it is not applicable in big worlds where the environment is
complex. Using the theory of finite automaton to model belief formation, this
paper studies the characteristics of optimal learning behavior in small and big
worlds, where the complexity of the environment is low and high, respectively,
relative to the cognitive ability of the decision maker. Confirming Savage's
claim, optimal learning behavior is closed to Bayesian in small worlds but
significantly different in big worlds. In addition, I show that in big worlds,
the optimal learning behavior could exhibit a wide range of well-documented
non-Bayesian learning behavior, including the use of heuristic, correlation
neglect, persistent over-confidence, inattentive learning, and other behaviors
of model simplification or misspecification. These results establish a clear
and testable relationship between the prominence of non-Bayesian learning
behavior, complexity and cognitive ability.
</p>
<a href="http://arxiv.org/abs/2009.11917">arXiv:2009.11917</a> [<a href="http://arxiv.org/pdf/2009.11917">pdf</a>]

<h2>Image-Based Sorghum Head Counting When You Only Look Once. (arXiv:2009.11929v2 [cs.CV] UPDATED)</h2>
<h3>Lawrence Mosley, Hieu Pham, Yogesh Bansal, Eric Hare</h3>
<p>Modern trends in digital agriculture have seen a shift towards artificial
intelligence for crop quality assessment and yield estimation. In this work, we
document how a parameter tuned single-shot object detection algorithm can be
used to identify and count sorghum head from aerial drone images. Our approach
involves a novel exploratory analysis that identified key structural elements
of the sorghum images and motivated the selection of parameter-tuned anchor
boxes that contributed significantly to performance. These insights led to the
development of a deep learning model that outperformed the baseline model and
achieved an out-of-sample mean average precision of 0.95.
</p>
<a href="http://arxiv.org/abs/2009.11929">arXiv:2009.11929</a> [<a href="http://arxiv.org/pdf/2009.11929">pdf</a>]

<h2>MinTL: Minimalist Transfer Learning for Task-Oriented Dialogue Systems. (arXiv:2009.12005v2 [cs.CL] UPDATED)</h2>
<h3>Zhaojiang Lin, Andrea Madotto, Genta Indra Winata, Pascale Fung</h3>
<p>In this paper, we propose Minimalist Transfer Learning (MinTL) to simplify
the system design process of task-oriented dialogue systems and alleviate the
over-dependency on annotated data. MinTL is a simple yet effective transfer
learning framework, which allows us to plug-and-play pre-trained seq2seq
models, and jointly learn dialogue state tracking and dialogue response
generation. Unlike previous approaches, which use a copy mechanism to
"carryover" the old dialogue states to the new one, we introduce Levenshtein
belief spans (Lev), that allows efficient dialogue state tracking with a
minimal generation length. We instantiate our learning framework with two
pre-trained backbones: T5 and BART, and evaluate them on MultiWOZ. Extensive
experiments demonstrate that: 1) our systems establish new state-of-the-art
results on end-to-end response generation, 2) MinTL-based systems are more
robust than baseline methods in the low resource setting, and they achieve
competitive results with only 20\% training data, and 3) Lev greatly improves
the inference efficiency.
</p>
<a href="http://arxiv.org/abs/2009.12005">arXiv:2009.12005</a> [<a href="http://arxiv.org/pdf/2009.12005">pdf</a>]

<h2>GEFA: Early Fusion Approach in Drug-Target Affinity Prediction. (arXiv:2009.12146v2 [cs.LG] UPDATED)</h2>
<h3>Tri Minh Nguyen, Thin Nguyen, Thao Minh Le, Truyen Tran</h3>
<p>Predicting the interaction between a compound and a target is crucial for
rapid drug repurposing. Deep learning has been successfully applied in
drug-target affinity (DTA) problem. However, previous deep learning-based
methods ignore modeling the direct interactions between drug and protein
residues. This would lead to inaccurate learning of target representation which
may change due to the drug binding effects. In addition, previous DTA methods
learn protein representation solely based on a small number of protein
sequences in DTA datasets while neglecting the use of proteins outside of the
DTA datasets. We propose GEFA (Graph Early Fusion Affinity), a novel
graph-in-graph neural network with attention mechanism to address the changes
in target representation because of the binding effects. Specifically, a drug
is modeled as a graph of atoms, which then serves as a node in a larger graph
of residues-drug complex. The resulting model is an expressive deep nested
graph neural network. We also use pre-trained protein representation powered by
the recent effort of learning contextualized protein representation. The
experiments are conducted under different settings to evaluate scenarios such
as novel drugs or targets. The results demonstrate the effectiveness of the
pre-trained protein embedding and the advantages our GEFA in modeling the
nested graph for drug-target interaction.
</p>
<a href="http://arxiv.org/abs/2009.12146">arXiv:2009.12146</a> [<a href="http://arxiv.org/pdf/2009.12146">pdf</a>]

<h2>Flexible Performant GEMM Kernels on GPUs. (arXiv:2009.12263v2 [cs.MS] UPDATED)</h2>
<h3>Thomas Faingnaert, Tim Besard, Bjorn De Sutter</h3>
<p>General Matrix Multiplication or GEMM kernels take center place in high
performance computing and machine learning. Recent NVIDIA GPUs include GEMM
accelerators, such as NVIDIA's Tensor Cores. Their exploitation is hampered by
the two-language problem: it requires either low-level programming which
implies low programmer productivity or using libraries that only offer a
limited set of components. Because rephrasing algorithms in terms of
established components often introduces overhead, the libraries' lack of
flexibility limits the freedom to explore new algorithms. Researchers using
GEMMs can hence not enjoy programming productivity, high performance, and
research flexibility at once.

In this paper we solve this problem. We present three sets of abstractions
and interfaces to program GEMMs within the scientific Julia programming
language. The interfaces and abstractions are co-designed for researchers'
needs and Julia's features to achieve sufficient separation of concerns and
flexibility to easily extend basic GEMMs in many different ways without paying
a performance price. Comparing our GEMMs to state-of-the-art libraries cuBLAS
and CUTLASS, we demonstrate that our performance is mostly on par with, and in
some cases even exceeds, the libraries, without having to write a single line
of code in CUDA C++ or assembly, and without facing flexibility limitations.
</p>
<a href="http://arxiv.org/abs/2009.12263">arXiv:2009.12263</a> [<a href="http://arxiv.org/pdf/2009.12263">pdf</a>]

<h2>Interpretable Multi-Step Reasoning with Knowledge Extraction on Complex Healthcare Question Answering. (arXiv:2008.02434v1 [cs.AI] CROSS LISTED)</h2>
<h3>Ye Liu, Shaika Chowdhury, Chenwei Zhang, Cornelia Caragea, Philip S. Yu</h3>
<p>Healthcare question answering assistance aims to provide customer healthcare
information, which widely appears in both Web and mobile Internet. The
questions usually require the assistance to have proficient healthcare
background knowledge as well as the reasoning ability on the knowledge.
Recently a challenge involving complex healthcare reasoning, HeadQA dataset,
has been proposed, which contains multiple-choice questions authorized for the
public healthcare specialization exam. Unlike most other QA tasks that focus on
linguistic understanding, HeadQA requires deeper reasoning involving not only
knowledge extraction, but also complex reasoning with healthcare knowledge.
These questions are the most challenging for current QA systems, and the
current performance of the state-of-the-art method is slightly better than a
random guess. In order to solve this challenging task, we present a Multi-step
reasoning with Knowledge extraction framework (MurKe). The proposed framework
first extracts the healthcare knowledge as supporting documents from the large
corpus. In order to find the reasoning chain and choose the correct answer,
MurKe iterates between selecting the supporting documents, reformulating the
query representation using the supporting documents and getting entailment
score for each choice using the entailment model. The reformulation module
leverages selected documents for missing evidence, which maintains
interpretability. Moreover, we are striving to make full use of off-the-shelf
pre-trained models. With less trainable weight, the pre-trained model can
easily adapt to healthcare tasks with limited training samples. From the
experimental results and ablation study, our system is able to outperform
several strong baselines on the HeadQA dataset.
</p>
<a href="http://arxiv.org/abs/2008.02434">arXiv:2008.02434</a> [<a href="http://arxiv.org/pdf/2008.02434">pdf</a>]

<h2>SeqROCTM: A Matlab toolbox for the analysis of Sequence of Random Objects driven by Context Tree Models. (arXiv:2009.06371v2 [cs.AI] CROSS LISTED)</h2>
<h3>Noslen Hern&#xe1;ndez, Aline Duarte</h3>
<p>In several research problems we face probabilistic sequences of inputs (e.g.,
sequence of stimuli) from which an agent generates a corresponding sequence of
responses and it is of interest to model/discover some kind of relation between
them. To model such relation in the context of statistical learning in
neuroscience, a new class of stochastic process have been introduced [5],
namely sequences of random objects driven by context tree models. In this paper
we introduce a freely available Matlab toolbox (SeqROCTM) that implements three
model selection methods to make inference about the parameters of this kind of
stochastic process.
</p>
<a href="http://arxiv.org/abs/2009.06371">arXiv:2009.06371</a> [<a href="http://arxiv.org/pdf/2009.06371">pdf</a>]

<h2>Theoretical Analysis of the Advantage of Deepening Neural Networks. (arXiv:2009.11479v1 [cs.LG] CROSS LISTED)</h2>
<h3>Yasushi Esaki, Yuta Nakahara, Toshiyasu Matsushima</h3>
<p>We propose two new criteria to understand the advantage of deepening neural
networks. It is important to know the expressivity of functions computable by
deep neural networks in order to understand the advantage of deepening neural
networks. Unless deep neural networks have enough expressivity, they cannot
have good performance even though learning is successful. In this situation,
the proposed criteria contribute to understanding the advantage of deepening
neural networks since they can evaluate the expressivity independently from the
efficiency of learning. The first criterion shows the approximation accuracy of
deep neural networks to the target function. This criterion has the background
that the goal of deep learning is approximating the target function by deep
neural networks. The second criterion shows the property of linear regions of
functions computable by deep neural networks. This criterion has the background
that deep neural networks whose activation functions are piecewise linear are
also piecewise linear. Furthermore, by the two criteria, we show that to
increase layers is more effective than to increase units at each layer on
improving the expressivity of deep neural networks.
</p>
<a href="http://arxiv.org/abs/2009.11479">arXiv:2009.11479</a> [<a href="http://arxiv.org/pdf/2009.11479">pdf</a>]

<h2>Kernel learning approaches for summarising and combining posterior similarity matrices. (arXiv:2009.12852v1 [stat.ME])</h2>
<h3>Alessandra Cabassi, Sylvia Richardson, Paul D. W. Kirk</h3>
<p>When using Markov chain Monte Carlo (MCMC) algorithms to perform inference
for Bayesian clustering models, such as mixture models, the output is typically
a sample of clusterings (partitions) drawn from the posterior distribution. In
practice, a key challenge is how to summarise this output. Here we build upon
the notion of the posterior similarity matrix (PSM) in order to suggest new
approaches for summarising the output of MCMC algorithms for Bayesian
clustering models. A key contribution of our work is the observation that PSMs
are positive semi-definite, and hence can be used to define
probabilistically-motivated kernel matrices that capture the clustering
structure present in the data. This observation enables us to employ a range of
kernel methods to obtain summary clusterings, and otherwise exploit the
information summarised by PSMs. For example, if we have multiple PSMs, each
corresponding to a different dataset on a common set of statistical units, we
may use standard methods for combining kernels in order to perform integrative
clustering. We may moreover embed PSMs within predictive kernel models in order
to perform outcome-guided data integration. We demonstrate the performances of
the proposed methods through a range of simulation studies as well as two real
data applications. R code is available at
https://github.com/acabassi/combine-psms.
</p>
<a href="http://arxiv.org/abs/2009.12852">arXiv:2009.12852</a> [<a href="http://arxiv.org/pdf/2009.12852">pdf</a>]

<h2>Group Whitening: Balancing Learning Efficiency and Representational Capacity. (arXiv:2009.13333v1 [cs.LG])</h2>
<h3>Lei Huang, Li Liu, Fan Zhu, Ling Shao</h3>
<p>Batch normalization (BN) is an important technique commonly incorporated into
deep learning models to perform standardization within mini-batches. The merits
of BN in improving model's learning efficiency can be further amplified by
applying whitening, while its drawbacks in estimating population statistics for
inference can be avoided through group normalization (GN). This paper proposes
group whitening (GW), which elaborately exploits the advantages of the
whitening operation and avoids the disadvantages of normalization within
mini-batches. Specifically, GW divides the neurons of a sample into groups for
standardization, like GN, and then further decorrelates the groups. In
addition, we quantitatively analyze the constraint imposed by normalization,
and show how the batch size (group number) affects the performance of batch
(group) normalized networks, from the perspective of model's representational
capacity. This analysis provides theoretical guidance for applying GW in
practice. Finally, we apply the proposed GW to ResNet and ResNeXt architectures
and conduct experiments on the ImageNet and COCO benchmarks. Results show that
GW consistently improves the performance of different architectures, with
absolute gains of $1.02\%$ $\sim$ $1.49\%$ in top-1 accuracy on ImageNet and
$1.82\%$ $\sim$ $3.21\%$ in bounding box AP on COCO.
</p>
<a href="http://arxiv.org/abs/2009.13333">arXiv:2009.13333</a> [<a href="http://arxiv.org/pdf/2009.13333">pdf</a>]

<h2>BOML: A Modularized Bilevel Optimization Library in Python for Meta Learning. (arXiv:2009.13357v1 [cs.LG])</h2>
<h3>Yaohua Liu, Risheng Liu</h3>
<p>Meta-learning (a.k.a. learning to learn) has recently emerged as a promising
paradigm for a variety of applications. There are now many meta-learning
methods, each focusing on different modeling aspects of base and meta learners,
but all can be (re)formulated as specific bilevel optimization problems. This
work presents BOML, a modularized optimization library that unifies several
meta-learning algorithms into a common bilevel optimization framework. It
provides a hierarchical optimization pipeline together with a variety of
iteration modules, which can be used to solve the mainstream categories of
meta-learning methods, such as meta-feature-based and meta-initialization-based
formulations. The library is written in Python and is available at
https://github.com/dut-media-lab/BOML.
</p>
<a href="http://arxiv.org/abs/2009.13357">arXiv:2009.13357</a> [<a href="http://arxiv.org/pdf/2009.13357">pdf</a>]

<h2>Transparency, Auditability and eXplainability of Machine Learning Models in Credit Scoring. (arXiv:2009.13384v1 [stat.ML])</h2>
<h3>Michael B&#xfc;cker, Gero Szepannek, Alicja Gosiewska, Przemyslaw Biecek</h3>
<p>A major requirement for credit scoring models is to provide a maximally
accurate risk prediction. Additionally, regulators demand these models to be
transparent and auditable. Thus, in credit scoring, very simple predictive
models such as logistic regression or decision trees are still widely used and
the superior predictive power of modern machine learning algorithms cannot be
fully leveraged. Significant potential is therefore missed, leading to higher
reserves or more credit defaults. This paper works out different dimensions
that have to be considered for making credit scoring models understandable and
presents a framework for making ``black box'' machine learning models
transparent, auditable and explainable. Following this framework, we present an
overview of techniques, demonstrate how they can be applied in credit scoring
and how results compare to the interpretability of score cards. A real world
case study shows that a comparable degree of interpretability can be achieved
while machine learning techniques keep their ability to improve predictive
power.
</p>
<a href="http://arxiv.org/abs/2009.13384">arXiv:2009.13384</a> [<a href="http://arxiv.org/pdf/2009.13384">pdf</a>]

<h2>Best Policy Identification in discounted MDPs: Problem-specific Sample Complexity. (arXiv:2009.13405v1 [stat.ML])</h2>
<h3>Aymen Al Marjani, Alexandre Proutiere</h3>
<p>We investigate the problem of best-policy identification in discounted Markov
Decision Processes (MDPs) with finite state and action spaces. We assume that
the agent has access to a generative model and that the MDP possesses a unique
optimal policy. In this setting, we derive a problem-specific lower bound of
the sample complexity satisfied by any learning algorithm. This lower bound
corresponds to an optimal sample allocation that solves a non-convex program,
and hence, is hard to exploit in the design of efficient algorithms. We provide
a simple and tight upper bound of the sample complexity lower bound, whose
corresponding nearly-optimal sample allocation becomes explicit. The upper
bound depends on specific functionals of the MDP such as the sub-optimal gaps
and the variance of the next-state value function, and thus really summarizes
the hardness of the MDP. We devise KLB-TS (KL Ball Track-and-Stop), an
algorithm tracking this nearly-optimal allocation, and provide asymptotic
guarantees for its sample complexity (both almost surely and in expectation).
The advantages of KLB-TS against state-of-the-art algorithms are finally
discussed.
</p>
<a href="http://arxiv.org/abs/2009.13405">arXiv:2009.13405</a> [<a href="http://arxiv.org/pdf/2009.13405">pdf</a>]

<h2>Ensemble Forecasting of the Zika Space-TimeSpread with Topological Data Analysis. (arXiv:2009.13423v1 [q-bio.PE])</h2>
<h3>Marwah Soliman, Vyacheslav Lyubchich, Yulia R. Gel</h3>
<p>As per the records of theWorld Health Organization, the first formally
reported incidence of Zika virus occurred in Brazil in May 2015. The disease
then rapidly spread to other countries in Americas and East Asia, affecting
more than 1,000,000 people. Zika virus is primarily transmitted through bites
of infected mosquitoes of the species Aedes (Aedes aegypti and Aedes
albopictus). The abundance of mosquitoes and, as a result, the prevalence of
Zika virus infections are common in areas which have high precipitation, high
temperature, and high population density.Nonlinear spatio-temporal dependency
of such data and lack of historical public health records make prediction of
the virus spread particularly challenging. In this article, we enhance Zika
forecasting by introducing the concepts of topological data analysis and,
specifically, persistent homology of atmospheric variables, into the virus
spread modeling. The topological summaries allow for capturing higher order
dependencies among atmospheric variables that otherwise might be unassessable
via conventional spatio-temporal modeling approaches based on geographical
proximity assessed via Euclidean distance. We introduce a new concept of
cumulative Betti numbers and then integrate the cumulative Betti numbers as
topological descriptors into three predictive machine learning models: random
forest, generalized boosted regression, and deep neural network. Furthermore,
to better quantify for various sources of uncertainties, we combine the
resulting individual model forecasts into an ensemble of the Zika spread
predictions using Bayesian model averaging. The proposed methodology is
illustrated in application to forecasting of the Zika space-time spread in
Brazil in the year 2018.
</p>
<a href="http://arxiv.org/abs/2009.13423">arXiv:2009.13423</a> [<a href="http://arxiv.org/pdf/2009.13423">pdf</a>]

<h2>An Iterative Approach based on Explainability to Improve the Learning of Fraud Detection Models. (arXiv:2009.13437v1 [cs.LG])</h2>
<h3>Bernat Coma-Puig, Josep Carmona</h3>
<p>Implementing predictive models in utility companies to detect Non-Technical
Losses (i.e. fraud and other meter problems) is challenging: the data available
is biased, and the algorithms usually used are black-boxes that can not be
either easily trusted or understood by the stakeholders. In this work, we
explain our approach to mitigate these problems in a real supervised system to
detect non-technical losses for an international utility company from Spain.
This approach exploits human knowledge (e.g. from the data scientists or the
company's stakeholders), and the information provided by explanatory methods to
implement smart feature engineering. This simple, efficient method that can be
easily implemented in other industrial projects is tested in a real dataset and
the results evidence that the derived prediction model is better in terms of
accuracy, interpretability, robustness and flexibility.
</p>
<a href="http://arxiv.org/abs/2009.13437">arXiv:2009.13437</a> [<a href="http://arxiv.org/pdf/2009.13437">pdf</a>]

<h2>Targeted VAE: Structured Inference and Targeted Learning for Causal Parameter Estimation. (arXiv:2009.13472v1 [stat.ML])</h2>
<h3>Matthew James Vowels, Necati Cihan Camgoz, Richard Bowden</h3>
<p>Undertaking causal inference with observational data is extremely useful
across a wide range of domains including the development of medical treatments,
advertisements and marketing, and policy making. There are two main challenges
associated with undertaking causal inference using observational data:
treatment assignment heterogeneity (i.e., differences between the treated and
untreated groups), and an absence of counterfactual data (i.e. not knowing what
would have happened if an individual who did get treatment, were instead to
have not been treated). We address these two challenges by combining structured
inference and targeted learning. To our knowledge, Targeted Variational
AutoEncoder (TVAE) is the first method to incorporate targeted learning into
deep latent variable models. Results demonstrate competitive and state of the
art performance.
</p>
<a href="http://arxiv.org/abs/2009.13472">arXiv:2009.13472</a> [<a href="http://arxiv.org/pdf/2009.13472">pdf</a>]

<h2>Is Reinforcement Learning More Difficult Than Bandits? A Near-optimal Algorithm Escaping the Curse of Horizon. (arXiv:2009.13503v1 [cs.LG])</h2>
<h3>Zihan Zhang, Xiangyang Ji, Simon S. Du</h3>
<p>Episodic reinforcement learning and contextual bandits are two widely studied
sequential decision-making problems. Episodic reinforcement learning
generalizes contextual bandits and is often perceived to be more difficult due
to long planning horizon and unknown state-dependent transitions. The current
paper shows that the long planning horizon and the unknown state-dependent
transitions (at most) pose little additional difficulty on sample complexity.
We consider the episodic reinforcement learning with $S$ states, $A$ actions,
planning horizon $H$, total reward bounded by $1$, and the agent plays for $K$
episodes. We propose a new algorithm, \textbf{M}onotonic \textbf{V}alue
\textbf{P}ropagation (MVP), which relies on a new Bernstein-type bonus. The new
bonus only requires tweaking the \emph{constants} to ensure optimism and thus
is significantly simpler than existing bonus constructions. We show MVP enjoys
an $O\left(\left(\sqrt{SAK} + S^2A\right) \text{poly}\log
\left(SAHK\right)\right)$ regret, approaching the
$\Omega\left(\sqrt{SAK}\right)$ lower bound of \emph{contextual bandits}.
Notably, this result 1) \emph{exponentially} improves the state-of-the-art
polynomial-time algorithms by Dann et al. [2019], Zanette et al. [2019] and
Zhang et al. [2020] in terms of the dependency on $H$, and 2)
\emph{exponentially} improves the running time in [Wang et al. 2020] and
significantly improves the dependency on $S$, $A$ and $K$ in sample complexity.
</p>
<a href="http://arxiv.org/abs/2009.13503">arXiv:2009.13503</a> [<a href="http://arxiv.org/pdf/2009.13503">pdf</a>]

<h2>Graph Adversarial Networks: Protecting Information against Adversarial Attacks. (arXiv:2009.13504v1 [cs.LG])</h2>
<h3>Peiyuan Liao, Han Zhao, Keyulu Xu, Tommi Jaakkola, Geoffrey Gordon, Stefanie Jegelka, Ruslan Salakhutdinov</h3>
<p>We explore the problem of protecting information when learning with
graph-structured data. While the advent of Graph Neural Networks (GNNs) has
greatly improved node and graph representational learning in many applications,
the neighborhood aggregation paradigm exposes additional vulnerabilities to
attackers seeking to extract node-level information about sensitive attributes.
To counter this, we propose a minimax game between the desired GNN encoder and
the worst-case attacker. The resulting adversarial training creates a strong
defense against inference attacks, while only suffering small loss in task
performance. We analyze the effectiveness of our framework against a worst-case
adversary, and characterize the trade-off between predictive accuracy and
adversarial defense. Experiments across multiple datasets from recommender
systems, knowledge graphs and quantum chemistry demonstrate that the proposed
approach provides a robust defense across various graph structures and tasks,
while producing competitive GNN encoders.
</p>
<a href="http://arxiv.org/abs/2009.13504">arXiv:2009.13504</a> [<a href="http://arxiv.org/pdf/2009.13504">pdf</a>]

<h2>Learning Deep ReLU Networks Is Fixed-Parameter Tractable. (arXiv:2009.13512v1 [cs.LG])</h2>
<h3>Sitan Chen, Adam R. Klivans, Raghu Meka</h3>
<p>We consider the problem of learning an unknown ReLU network with respect to
Gaussian inputs and obtain the first nontrivial results for networks of depth
more than two. We give an algorithm whose running time is a fixed polynomial in
the ambient dimension and some (exponentially large) function of only the
network's parameters.

Our bounds depend on the number of hidden units, depth, spectral norm of the
weight matrices, and Lipschitz constant of the overall network (we show that
some dependence on the Lipschitz constant is necessary). We also give a bound
that is doubly exponential in the size of the network but is independent of
spectral norm. These results provably cannot be obtained using gradient-based
methods and give the first example of a class of efficiently learnable neural
networks that gradient descent will fail to learn.

In contrast, prior work for learning networks of depth three or higher
requires exponential time in the ambient dimension, even when the above
parameters are bounded by a constant. Additionally, all prior work for the
depth-two case requires well-conditioned weights and/or positive coefficients
to obtain efficient run-times. Our algorithm does not require these
assumptions.

Our main technical tool is a type of filtered PCA that can be used to
iteratively recover an approximate basis for the subspace spanned by the hidden
units in the first layer. Our analysis leverages new structural results on
lattice polynomials from tropical geometry.
</p>
<a href="http://arxiv.org/abs/2009.13512">arXiv:2009.13512</a> [<a href="http://arxiv.org/pdf/2009.13512">pdf</a>]

<h2>The impact of signal-to-noise, redshift, and angular range on the bias of weak lensing 2-point functions. (arXiv:2007.07253v2 [astro-ph.CO] UPDATED)</h2>
<h3>Amy J. Louca, Elena Sellentin</h3>
<p>Weak lensing data follow a naturally skewed distribution, implying the data
vector most likely yielded from a survey will systematically fall below its
mean. Although this effect is qualitatively known from CMB-analyses, correctly
accounting for it in weak lensing is challenging, as a direct transfer of the
CMB results is quantitatively incorrect. While a previous study (Sellentin et
al. 2018) focused on the magnitude of this bias, we here focus on the frequency
of this bias, its scaling with redshift, and its impact on the signal-to-noise
of a survey. Filtering weak lensing data with COSEBIs, we show that weak
lensing likelihoods are skewed up until $\ell \approx 100$, whereas
CMB-likelihoods Gaussianize already at $\ell \approx 20$. While
COSEBI-compressed data on KiDS- and DES-like redshift- and angular ranges
follow Gaussian distributions, we detect skewness at 6$\sigma$ significance for
half of a Euclid- or LSST-like data set, caused by the wider coverage and
deeper reach of these surveys. Computing the signal-to-noise ratio per data
point, we show that precisely the data points of highest signal-to-noise are
the most biased. Over all redshifts, this bias affects at least 10% of a
survey's total signal-to-noise, at high redshifts up to 25%. The bias is
accordingly expected to impact parameter inference. The bias can be handled by
developing non-Gaussian likelihoods. Otherwise, it could be reduced by removing
the data points of highest signal-to-noise.
</p>
<a href="http://arxiv.org/abs/2007.07253">arXiv:2007.07253</a> [<a href="http://arxiv.org/pdf/2007.07253">pdf</a>]

