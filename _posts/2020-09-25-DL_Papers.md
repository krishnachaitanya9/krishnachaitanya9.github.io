---
title: Latest Deep Learning Papers
date: 2020-11-17 21:00:01 +0800
featured-img: DL-logo
categories: [Deep_Learning]
tags: [Deep_Learning]
mathjax: Yes
---

<h1>Your arXiv Feed (203 Articles)</h1>
<h2>Obstacle avoidance-driven controller for safety-critical aerial robots. (arXiv:2011.08178v1 [cs.RO])</h2>
<h3>Johann Lange</h3>
<p>The goal of this thesis is to propose the combination of
Control-Barrier-Functions (CBF) with Model-Predictive-Control (MPC) resulting
in the novel Model-Predictive-Control-Barrier-Function (MPCBF). It can be
shown, that the performance of the MPCBF surpasses the performance of the CBF
due to the increased time horizon of the MPC. Moreover, the MPCBF was applied
to a quadrotor, a system strongly in need of fast and predictive control. Using
the MPCBF, the quadrotor was able to avoid obstacles, which the CBF failed to
avoid due to the relative speed of the obstacle. The results of this work are
experimentally validated.
</p>
<a href="http://arxiv.org/abs/2011.08178" target="_blank">arXiv:2011.08178</a> [<a href="http://arxiv.org/pdf/2011.08178" target="_blank">pdf</a>]

<h2>Explaining the Adaptive Generalisation Gap. (arXiv:2011.08181v1 [stat.ML])</h2>
<h3>Diego Granziol, Samuel Albanie, Xingchen Wan, Stephen Roberts</h3>
<p>We conjecture that the reason for the difference in generalisation between
adaptive and non adaptive gradient methods stems from the failure of adaptive
methods to account for the greater levels of noise associated with flatter
directions in their estimates of local curvature. This conjecture motivated by
results in random matrix theory has implications for optimisation in both
simple convex settings and deep neural networks. We demonstrate that typical
schedules used for adaptive methods (with low numerical stability or damping
constants) serve to bias relative movement towards flat directions relative to
sharp directions, effectively amplifying the noise-to-signal ratio and harming
generalisation. We show that the numerical stability/damping constant used in
these methods can be decomposed into a learning rate reduction and linear
shrinkage of the estimated curvature matrix. We then demonstrate significant
generalisation improvements by increasing the shrinkage coefficient, closing
the generalisation gap entirely in our neural network experiments. Finally, we
show that other popular modifications to adaptive methods, such as decoupled
weight decay and partial adaptivity can be shown to calibrate parameter updates
to make better use of sharper, more reliable directions.
</p>
<a href="http://arxiv.org/abs/2011.08181" target="_blank">arXiv:2011.08181</a> [<a href="http://arxiv.org/pdf/2011.08181" target="_blank">pdf</a>]

<h2>Uncertainty measures for probabilistic hesitant fuzzy sets in multiple criteria decision making. (arXiv:2011.08182v1 [cs.AI])</h2>
<h3>Bahram Farhadinia, Uwe Aickelin, Hadi Akbarzadeh Khorshidi</h3>
<p>This contribution reviews critically the existing entropy measures for
probabilistic hesitant fuzzy sets (PHFSs), and demonstrates that these entropy
measures fail to effectively distinguish a variety of different PHFSs in some
cases. In the sequel, we develop a new axiomatic framework of entropy measures
for probabilistic hesitant fuzzy elements (PHFEs) by considering two facets of
uncertainty associated with PHFEs which are known as fuzziness and
nonspecificity. Respect to each kind of uncertainty, a number of formulae are
derived to permit flexible selection of PHFE entropy measures. Moreover, based
on the proposed PHFE entropy measures, we introduce some entropy-based distance
measures which are used in the portion of comparative analysis.
</p>
<a href="http://arxiv.org/abs/2011.08182" target="_blank">arXiv:2011.08182</a> [<a href="http://arxiv.org/pdf/2011.08182" target="_blank">pdf</a>]

<h2>Higher order hesitant fuzzy Choquet integral operator and its application to multiple criteria decision making. (arXiv:2011.08183v1 [cs.AI])</h2>
<h3>B Farhadinia, Uwe Aickelin, HA Khorshidi</h3>
<p>Generally, the criteria involved in a decision making problem are interactive
or inter-dependent, and therefore aggregating them by the use of traditional
operators which are based on additive measures is not logical. This verifies
that we have to implement fuzzy measures for modelling the interaction
phenomena among the criteria.On the other hand, based on the recent extension
of hesitant fuzzy set, called higher order hesitant fuzzy set (HOHFS) which
allows the membership of a given element to be defined in forms of several
possible generalized types of fuzzy set, we encourage to propose the higher
order hesitant fuzzy (HOHF) Choquet integral operator. This concept not only
considers the importance of the higher order hesitant fuzzy arguments, but also
it can reflect the correlations among those arguments. Then,a detailed
discussion on the aggregation properties of the HOHF Choquet integral operator
will be presented.To enhance the application of HOHF Choquet integral operator
in decision making, we first assess the appropriate energy policy for the
socio-economic development. Then, the efficiency of the proposed HOHF Choquet
integral operator-based technique over a number of exiting techniques is
further verified by employing another decision making problem associated with
the technique of TODIM (an acronym in Portuguese of Interactive and
Multicriteria Decision Making).
</p>
<a href="http://arxiv.org/abs/2011.08183" target="_blank">arXiv:2011.08183</a> [<a href="http://arxiv.org/pdf/2011.08183" target="_blank">pdf</a>]

<h2>Assistive Diagnostic Tool for Brain Tumor Detection using Computer Vision. (arXiv:2011.08185v1 [cs.CV])</h2>
<h3>Sahithi Ankireddy</h3>
<p>Today, over 700,000 people are living with brain tumors in the United States.
Brain tumors can spread very quickly to other parts of the brain and the spinal
cord unless necessary preventive action is taken. Thus, the survival rate for
this disease is less than 40% for both men and women. A conclusive and early
diagnosis of a brain tumor could be the difference between life and death for
some. However, brain tumor detection and segmentation are tedious and
time-consuming processes as it can only be done by radiologists and clinical
experts. The use of computer vision techniques, such as Mask R Convolutional
Neural Network (Mask R CNN), to detect and segment brain tumors can mitigate
the possibility of human error while increasing prediction accuracy rates. The
goal of this project is to create an assistive diagnostics tool for brain tumor
detection and segmentation. Transfer learning was used with the Mask R CNN, and
necessary parameters were accordingly altered, as a starting point. The model
was trained with 20 epochs and later tested. The prediction segmentation
matched 90% with the ground truth. This suggests that the model was able to
perform at a high level. Once the model was finalized, the application running
on Flask was created. The application will serve as a tool for medical
professionals. It allows doctors to upload patient brain tumor MRI images in
order to receive immediate results on the diagnosis and segmentation for each
patient.
</p>
<a href="http://arxiv.org/abs/2011.08185" target="_blank">arXiv:2011.08185</a> [<a href="http://arxiv.org/pdf/2011.08185" target="_blank">pdf</a>]

<h2>Hierarchical clustering in particle physics through reinforcement learning. (arXiv:2011.08191v1 [cs.AI])</h2>
<h3>Johann Brehmer, Sebastian Macaluso, Duccio Pappadopulo, Kyle Cranmer</h3>
<p>Particle physics experiments often require the reconstruction of decay
patterns through a hierarchical clustering of the observed final-state
particles. We show that this task can be phrased as a Markov Decision Process
and adapt reinforcement learning algorithms to solve it. In particular, we show
that Monte-Carlo Tree Search guided by a neural policy can construct
high-quality hierarchical clusterings and outperform established greedy and
beam search baselines.
</p>
<a href="http://arxiv.org/abs/2011.08191" target="_blank">arXiv:2011.08191</a> [<a href="http://arxiv.org/pdf/2011.08191" target="_blank">pdf</a>]

<h2>Automatic selection of clustering algorithms using supervised graph embedding. (arXiv:2011.08225v1 [cs.LG])</h2>
<h3>Noy Cohen-Shapira, Lior Rokach</h3>
<p>The widespread adoption of machine learning (ML) techniques and the extensive
expertise required to apply them have led to increased interest in automated ML
solutions that reduce the need for human intervention. One of the main
challenges in applying ML to previously unseen problems is algorithm selection
- the identification of high-performing algorithm(s) for a given dataset, task,
and evaluation measure. This study addresses the algorithm selection challenge
for data clustering, a fundamental task in data mining that is aimed at
grouping similar objects. We present MARCO-GE, a novel meta-learning approach
for the automated recommendation of clustering algorithms. MARCO-GE first
transforms datasets into graphs and then utilizes a graph convolutional neural
network technique to extract their latent representation. Using the embedding
representations obtained, MARCO-GE trains a ranking meta-model capable of
accurately recommending top-performing algorithms for a new dataset and
clustering evaluation measure. Extensive evaluation on 210 datasets, 13
clustering algorithms, and 10 clustering measures demonstrates the
effectiveness of our approach and its dominance in terms of predictive and
generalization performance over state-of-the-art clustering meta-learning
approaches.
</p>
<a href="http://arxiv.org/abs/2011.08225" target="_blank">arXiv:2011.08225</a> [<a href="http://arxiv.org/pdf/2011.08225" target="_blank">pdf</a>]

<h2>Personalized Cardiovascular Disease Risk Mitigation via Longitudinal Inverse Classification. (arXiv:2011.08254v1 [cs.LG])</h2>
<h3>Michael T. Lash, W. Nick Street</h3>
<p>Cardiovascular disease (CVD) is a serious illness affecting millions
world-wide and is the leading cause of death in the US. Recent years, however,
have seen tremendous growth in the area of personalized medicine, a field of
medicine that places the patient at the center of the medical decision-making
and treatment process. Many CVD-focused personalized medicine innovations focus
on genetic biomarkers, which provide person-specific CVD insights at the
genetic level, but do not focus on the practical steps a patient could take to
mitigate their risk of CVD development. In this work we propose longitudinal
inverse classification, a recommendation framework that provides personalized
lifestyle recommendations that minimize the predicted probability of CVD risk.
Our framework takes into account historical CVD risk, as well as other patient
characteristics, to provide recommendations. Our experiments show that earlier
adoption of the recommendations elicited from our framework produce significant
CVD risk reduction.
</p>
<a href="http://arxiv.org/abs/2011.08254" target="_blank">arXiv:2011.08254</a> [<a href="http://arxiv.org/pdf/2011.08254" target="_blank">pdf</a>]

<h2>Large-scale kernelized GRANGER causality to infer topology of directed graphs with applications to brain networks. (arXiv:2011.08261v1 [cs.LG])</h2>
<h3>M. Ali Vosoughi, Axel Wismuller</h3>
<p>Graph topology inference of network processes with co-evolving and
interacting time-series is crucial for network studies. Vector autoregressive
models (VAR) are popular approaches for topology inference of directed graphs;
however, in large networks with short time-series, topology estimation becomes
ill-posed. The present paper proposes a novel nonlinearity-preserving topology
inference method for directed networks with co-evolving nodal processes that
solves the ill-posedness problem. The proposed method, large-scale kernelized
Granger causality (lsKGC), uses kernel functions to transform data into a
low-dimensional feature space and solves the autoregressive problem in the
feature space, then finds the pre-images in the input space to infer the
topology. Extensive simulations on synthetic datasets with nonlinear and linear
dependencies and known ground-truth demonstrate significant improvement in the
Area Under the receiver operating characteristic Curve ( AUC ) of the receiver
operating characteristic for network recovery compared to existing methods.
Furthermore, tests on real datasets from a functional magnetic resonance
imaging (fMRI) study demonstrate 96.3 percent accuracy in diagnosis tasks of
schizophrenia patients, which is the highest in the literature with only brain
time-series information.
</p>
<a href="http://arxiv.org/abs/2011.08261" target="_blank">arXiv:2011.08261</a> [<a href="http://arxiv.org/pdf/2011.08261" target="_blank">pdf</a>]

<h2>Machine Learning and Soil Humidity Sensing: Signal Strength Approach. (arXiv:2011.08273v1 [cs.LG])</h2>
<h3>Lea Duji&#x107; Rodi&#x107;, Tomislav &#x17d;upanovi&#x107;, Toni Perkovi&#x107;, Petar &#x160;oli&#x107; (Corresponding Author, University of Split, Croatia), Joel J. P. C. Rodrigues (Federal University of Piau&#xed; (UFPI), Teresina - PI, Brazil and Instituto de Telecomunica&#xe7;&#xf5;es, Portugal)</h3>
<p>The IoT vision of ubiquitous and pervasive computing gives rise to future
smart irrigation systems comprising physical and digital world. Smart
irrigation ecosystem combined with Machine Learning can provide solutions that
successfully solve the soil humidity sensing task in order to ensure optimal
water usage. Existing solutions are based on data received from the power
hungry/expensive sensors that are transmitting the sensed data over the
wireless channel. Over time, the systems become difficult to maintain,
especially in remote areas due to the battery replacement issues with large
number of devices. Therefore, a novel solution must provide an alternative,
cost and energy effective device that has unique advantage over the existing
solutions. This work explores a concept of a novel, low-power, LoRa-based,
cost-effective system which achieves humidity sensing using Deep learning
techniques that can be employed to sense soil humidity with the high accuracy
simply by measuring signal strength of the given underground beacon device.
</p>
<a href="http://arxiv.org/abs/2011.08273" target="_blank">arXiv:2011.08273</a> [<a href="http://arxiv.org/pdf/2011.08273" target="_blank">pdf</a>]

<h2>Where Are You? Localization from Embodied Dialog. (arXiv:2011.08277v1 [cs.CV])</h2>
<h3>Meera Hahn, Jacob Krantz, Dhruv Batra, Devi Parikh, James M. Rehg, Stefan Lee, Peter Anderson</h3>
<p>We present Where Are You? (WAY), a dataset of ~6k dialogs in which two humans
-- an Observer and a Locator -- complete a cooperative localization task. The
Observer is spawned at random in a 3D environment and can navigate from
first-person views while answering questions from the Locator. The Locator must
localize the Observer in a detailed top-down map by asking questions and giving
instructions. Based on this dataset, we define three challenging tasks:
Localization from Embodied Dialog or LED (localizing the Observer from dialog
history), Embodied Visual Dialog (modeling the Observer), and Cooperative
Localization (modeling both agents). In this paper, we focus on the LED task --
providing a strong baseline model with detailed ablations characterizing both
dataset biases and the importance of various modeling choices. Our best model
achieves 32.7% success at identifying the Observer's location within 3m in
unseen buildings, vs. 70.4% for human Locators.
</p>
<a href="http://arxiv.org/abs/2011.08277" target="_blank">arXiv:2011.08277</a> [<a href="http://arxiv.org/pdf/2011.08277" target="_blank">pdf</a>]

<h2>Avoiding Communication in Logistic Regression. (arXiv:2011.08281v1 [cs.LG])</h2>
<h3>Aditya Devarakonda, James Demmel</h3>
<p>Stochastic gradient descent (SGD) is one of the most widely used optimization
methods for solving various machine learning problems. SGD solves an
optimization problem by iteratively sampling a few data points from the input
data, computing gradients for the selected data points, and updating the
solution. However, in a parallel setting, SGD requires interprocess
communication at every iteration. We introduce a new communication-avoiding
technique for solving the logistic regression problem using SGD. This technique
re-organizes the SGD computations into a form that communicates every $s$
iterations instead of every iteration, where $s$ is a tuning parameter. We
prove theoretical flops, bandwidth, and latency upper bounds for SGD and its
new communication-avoiding variant. Furthermore, we show experimental results
that illustrate that the new Communication-Avoiding SGD (CA-SGD) method can
achieve speedups of up to $4.97\times$ on a high-performance Infiniband cluster
without altering the convergence behavior or accuracy.
</p>
<a href="http://arxiv.org/abs/2011.08281" target="_blank">arXiv:2011.08281</a> [<a href="http://arxiv.org/pdf/2011.08281" target="_blank">pdf</a>]

<h2>Foundations of Bayesian Learning from Synthetic Data. (arXiv:2011.08299v1 [cs.LG])</h2>
<h3>Harrison Wilde, Jack Jewson, Sebastian Vollmer, Chris Holmes</h3>
<p>There is significant growth and interest in the use of synthetic data as an
enabler for machine learning in environments where the release of real data is
restricted due to privacy or availability constraints. Despite a large number
of methods for synthetic data generation, there are comparatively few results
on the statistical properties of models learnt on synthetic data, and fewer
still for situations where a researcher wishes to augment real data with
another party's synthesised data. We use a Bayesian paradigm to characterise
the updating of model parameters when learning in these settings, demonstrating
that caution should be taken when applying conventional learning algorithms
without appropriate consideration of the synthetic data generating process and
learning task. Recent results from general Bayesian updating support a novel
and robust approach to Bayesian synthetic-learning founded on decision theory
that outperforms standard approaches across repeated experiments on supervised
learning and inference problems.
</p>
<a href="http://arxiv.org/abs/2011.08299" target="_blank">arXiv:2011.08299</a> [<a href="http://arxiv.org/pdf/2011.08299" target="_blank">pdf</a>]

<h2>Overcomplete Deep Subspace Clustering Networks. (arXiv:2011.08306v1 [cs.CV])</h2>
<h3>Jeya Maria Jose Valanarasu, Vishal M. Patel</h3>
<p>Deep Subspace Clustering Networks (DSC) provide an efficient solution to the
problem of unsupervised subspace clustering by using an undercomplete deep
auto-encoder with a fully-connected layer to exploit the self expressiveness
property. This method uses undercomplete representations of the input data
which makes it not so robust and more dependent on pre-training. To overcome
this, we propose a simple yet efficient alternative method - Overcomplete Deep
Subspace Clustering Networks (ODSC) where we use overcomplete representations
for subspace clustering. In our proposed method, we fuse the features from both
undercomplete and overcomplete auto-encoder networks before passing them
through the self-expressive layer thus enabling us to extract a more meaningful
and robust representation of the input data for clustering. Experimental
results on four benchmark datasets show the effectiveness of the proposed
method over DSC and other clustering methods in terms of clustering error. Our
method is also not as dependent as DSC is on where pre-training should be
stopped to get the best performance and is also more robust to noise. Code -
\href{https://github.com/jeya-maria-jose/Overcomplete-Deep-Subspace-Clustering}{https://github.com/jeya-maria-jose/Overcomplete-Deep-Subspace-Clustering
</p>
<a href="http://arxiv.org/abs/2011.08306" target="_blank">arXiv:2011.08306</a> [<a href="http://arxiv.org/pdf/2011.08306" target="_blank">pdf</a>]

<h2>Educational robotics for children and their teachers. (arXiv:2011.08311v1 [cs.RO])</h2>
<h3>Cristina Gena, Claudio Mattutino, Davide Cellie, Enrico Mosca</h3>
<p>This paper describes a Google Educator funded project devoted to the training
of teachers (primary and secondary school) through an e-learning platform that
will introduce them to educational robotics using Wolly, a social, educational
and affective robot.
</p>
<a href="http://arxiv.org/abs/2011.08311" target="_blank">arXiv:2011.08311</a> [<a href="http://arxiv.org/pdf/2011.08311" target="_blank">pdf</a>]

<h2>Privacy-preserving Data Analysis through Representation Learning and Transformation. (arXiv:2011.08315v1 [cs.LG])</h2>
<h3>Omid Hajihassani, Omid Ardakanian, Hamzeh Khazaei</h3>
<p>The abundance of data from the sensors embedded in mobile and Internet of
Things (IoT) devices and the remarkable success of deep neural networks in
uncovering hidden patterns in time series data have led to mounting privacy
concerns in recent years. In this paper, we aim to navigate the trade-off
between data utility and privacy by learning low-dimensional representations
that are useful for data anonymization. We propose probabilistic
transformations in the latent space of a variational autoencoder to synthesize
time series data such that intrusive inferences are prevented while desired
inferences can still be made with a satisfactory level of accuracy. We compare
our technique with state-of-the-art autoencoder-based anonymization techniques
and additionally show that it can anonymize data in real time on
resource-constrained edge devices.
</p>
<a href="http://arxiv.org/abs/2011.08315" target="_blank">arXiv:2011.08315</a> [<a href="http://arxiv.org/pdf/2011.08315" target="_blank">pdf</a>]

<h2>Feature Sharing and Integration for Cooperative Cognition and Perception with Volumetric Sensors. (arXiv:2011.08317v1 [cs.CV])</h2>
<h3>Ehsan Emad Marvasti, Arash Raftari, Yaser P.Fallah, Rui Guo, Hongsheng Lu</h3>
<p>The recent advancement in computational and communication systems has led to
the introduction of high-performing neural networks and high-speed wireless
vehicular communication networks. As a result, new technologies such as
cooperative perception and cognition have emerged, addressing the inherent
limitations of sensory devices by providing solutions for the detection of
partially occluded targets and expanding the sensing range. However, designing
a reliable cooperative cognition or perception system requires addressing the
challenges caused by limited network resources and discrepancies between the
data shared by different sources. In this paper, we examine the requirements,
limitations, and performance of different cooperative perception techniques,
and present an in-depth analysis of the notion of Deep Feature Sharing (DFS).
We explore different cooperative object detection designs and evaluate their
performance in terms of average precision. We use the Volony dataset for our
experimental study. The results confirm that the DFS methods are significantly
less sensitive to the localization error caused by GPS noise. Furthermore, the
results attest that detection gain of DFS methods caused by adding more
cooperative participants in the scenes is comparable to raw information sharing
technique while DFS enables flexibility in design toward satisfying
communication requirements.
</p>
<a href="http://arxiv.org/abs/2011.08317" target="_blank">arXiv:2011.08317</a> [<a href="http://arxiv.org/pdf/2011.08317" target="_blank">pdf</a>]

<h2>Multi-Step Recurrent Q-Learning for Robotic Velcro Peeling. (arXiv:2011.08319v1 [cs.RO])</h2>
<h3>Jiacheng Yuan, Nicolai H&#xe4;ni, Volkan Isler</h3>
<p>Learning object manipulation is a critical skill for robots to interact with
their environment. Even though there has been significant progress in robotic
manipulation of rigid objects, interacting with non-rigid objects remains
challenging for robots. In this work, we introduce velcro peeling as a
representative application for robotic manipulation of non-rigid objects in
complex environments. We present a method of learning force-based manipulation
from noisy and incomplete sensor inputs in partially observable environments by
modeling long term dependencies between measurements with a multi-step deep
recurrent network. We present experiments on a real robot to show the necessity
of modeling these long term dependencies and validate our approach in
simulation and robot experiments. Our results show that using tactile input
enables the robot to overcome geometric uncertainties present in the
environment with high fidelity in ~90% of all cases, outperforming the
baselines by a large margin.
</p>
<a href="http://arxiv.org/abs/2011.08319" target="_blank">arXiv:2011.08319</a> [<a href="http://arxiv.org/pdf/2011.08319" target="_blank">pdf</a>]

<h2>A New Similarity Space Tailored for Supervised Deep Metric Learning. (arXiv:2011.08325v1 [cs.CV])</h2>
<h3>Pedro H. Barros, Fabiane Queiroz, Flavio Figueredo, Jefersson A. dos Santos, Heitor S. Ramos</h3>
<p>We propose a novel deep metric learning method. Differently from many works
on this area, we defined a novel latent space obtained through an autoencoder.
The new space, namely S-space, is divided into different regions that describe
the positions where pairs of objects are similar/dissimilar. We locate makers
to identify these regions. We estimate the similarities between objects through
a kernel-based t-student distribution to measure the markers' distance and the
new data representation. In our approach, we simultaneously estimate the
markers' position in the S-space and represent the objects in the same space.
Moreover, we propose a new regularization function to avoid similar markers to
collapse altogether. We present evidences that our proposal can represent
complex spaces, for instance, when groups similar objects are located in
disjoint regions. We compare our proposal to 9 different distance metric
learning approaches (four of them are based on deep-learning) on 28 real-world
heterogeneous datasets. According to the four quantitative metrics used, our
method overcomes all the nine strategies from the literature.
</p>
<a href="http://arxiv.org/abs/2011.08325" target="_blank">arXiv:2011.08325</a> [<a href="http://arxiv.org/pdf/2011.08325" target="_blank">pdf</a>]

<h2>EffiScene: Efficient Per-Pixel Rigidity Inference for Unsupervised Joint Learning of Optical Flow, Depth, Camera Pose and Motion Segmentation. (arXiv:2011.08332v1 [cs.CV])</h2>
<h3>Yang Jiao, Guangming Shi, Trac D. Tran</h3>
<p>This paper addresses the challenging unsupervised scene flow estimation
problem by jointly learning four low-level vision sub-tasks: optical flow
$\textbf{F}$, stereo-depth $\textbf{D}$, camera pose $\textbf{P}$ and motion
segmentation $\textbf{S}$. Our key insight is that the rigidity of the scene
shares the same inherent geometrical structure with object movements and scene
depth. Hence, rigidity from $\textbf{S}$ can be inferred by jointly coupling
$\textbf{F}$, $\textbf{D}$ and $\textbf{P}$ to achieve more robust estimation.
To this end, we propose a novel scene flow framework named EffiScene with
efficient joint rigidity learning, going beyond existing pipeline with
independent auxiliary structures. In EffiScene, we first estimate optical flow
and depth at the coarse level and then compute camera pose by
Perspective-$n$-Points method. To jointly learn local rigidity, we design a
novel Rigidity From Motion (RfM) layer with three principal components: (i)
correlation extraction; (ii) boundary learning; and (iii) outlier exclusion.
Final outputs are fused based on the rigid map $M_R$ from RfM at finer level.
To efficiently train EffiScene, two new losses $\mathcal{L}_{bnd}$ and
$\mathcal{L}_{unc}$ are designed to prevent trivial solutions and to regularize
the flow boundary discontinuity. Extensive experiments on scene flow benchmark
KITTI show that our method is effective and significantly improves the
state-of-the-art approaches for all sub-tasks, i.e. optical flow (5.19
$\rightarrow$ 4.20), depth estimation (3.78 $\rightarrow$ 3.46), visual
odometry (0.012 $\rightarrow$ 0.011) and motion segmentation (0.57
$\rightarrow$ 0.62).
</p>
<a href="http://arxiv.org/abs/2011.08332" target="_blank">arXiv:2011.08332</a> [<a href="http://arxiv.org/pdf/2011.08332" target="_blank">pdf</a>]

<h2>2D+3D Facial Expression Recognition via Discriminative Dynamic Range Enhancement and Multi-Scale Learning. (arXiv:2011.08333v1 [cs.CV])</h2>
<h3>Yang Jiao, Yi Niu, Trac D. Tran, Guangming Shi</h3>
<p>In 2D+3D facial expression recognition (FER), existing methods generate
multi-view geometry maps to enhance the depth feature representation. However,
this may introduce false estimations due to local plane fitting from incomplete
point clouds. In this paper, we propose a novel Map Generation technique from
the viewpoint of information theory, to boost the slight 3D expression
differences from strong personality variations. First, we examine the HDR depth
data to extract the discriminative dynamic range $r_{dis}$, and maximize the
entropy of $r_{dis}$ to a global optimum. Then, to prevent the large
deformation caused by over-enhancement, we introduce a depth distortion
constraint and reduce the complexity from $O(KN^2)$ to $O(KN\tau)$.
Furthermore, the constrained optimization is modeled as a $K$-edges maximum
weight path problem in a directed acyclic graph, and we solve it efficiently
via dynamic programming. Finally, we also design an efficient Facial Attention
structure to automatically locate subtle discriminative facial parts for
multi-scale learning, and train it with a proposed loss function
$\mathcal{L}_{FA}$ without any facial landmarks. Experimental results on
different datasets show that the proposed method is effective and outperforms
the state-of-the-art 2D+3D FER methods in both FER accuracy and the output
entropy of the generated maps.
</p>
<a href="http://arxiv.org/abs/2011.08333" target="_blank">arXiv:2011.08333</a> [<a href="http://arxiv.org/pdf/2011.08333" target="_blank">pdf</a>]

<h2>Robust Deep Learning with Active Noise Cancellation for Spatial Computing. (arXiv:2011.08341v1 [cs.LG])</h2>
<h3>Li Chen, David Yang, Purvi Goel, Ilknur Kabul</h3>
<p>This paper proposes CANC, a Co-teaching Active Noise Cancellation method,
applied in spatial computing to address deep learning trained with extreme
noisy labels. Deep learning algorithms have been successful in spatial
computing for land or building footprint recognition. However a lot of noise
exists in ground truth labels due to how labels are collected in spatial
computing and satellite imagery. Existing methods to deal with extreme label
noise conduct clean sample selection and do not utilize the remaining samples.
Such techniques can be wasteful due to the cost of data retrieval. Our proposed
CANC algorithm not only conserves high-cost training samples but also provides
active label correction to better improve robust deep learning with extreme
noisy labels. We demonstrate the effectiveness of CANC for building footprint
recognition for spatial computing.
</p>
<a href="http://arxiv.org/abs/2011.08341" target="_blank">arXiv:2011.08341</a> [<a href="http://arxiv.org/pdf/2011.08341" target="_blank">pdf</a>]

<h2>Distilling a Hierarchical Policy for Planning and Control via Representation and Reinforcement Learning. (arXiv:2011.08345v1 [cs.LG])</h2>
<h3>Jung-Su Ha, Young-Jin Park, Hyeok-Joo Chae, Soon-Seo Park, Han-Lim Choi</h3>
<p>We present a hierarchical planning and control framework that enables an
agent to perform various tasks and adapt to a new task flexibly. Rather than
learning an individual policy for each particular task, the proposed framework,
DISH, distills a hierarchical policy from a set of tasks by representation and
reinforcement learning. The framework is based on the idea of latent variable
models that represent high-dimensional observations using low-dimensional
latent variables. The resulting policy consists of two levels of hierarchy: (i)
a planning module that reasons a sequence of latent intentions that would lead
to an optimistic future and (ii) a feedback control policy, shared across the
tasks, that executes the inferred intention. Because the planning is performed
in low-dimensional latent space, the learned policy can immediately be used to
solve or adapt to new tasks without additional training. We demonstrate the
proposed framework can learn compact representations (3- and 1-dimensional
latent states and commands for a humanoid with 197- and 36-dimensional state
features and actions) while solving a small number of imitation tasks, and the
resulting policy is directly applicable to other types of tasks, i.e.,
navigation in cluttered environments.
</p>
<a href="http://arxiv.org/abs/2011.08345" target="_blank">arXiv:2011.08345</a> [<a href="http://arxiv.org/pdf/2011.08345" target="_blank">pdf</a>]

<h2>Phenotyping Clusters of Patient Trajectories suffering from Chronic Complex Disease. (arXiv:2011.08356v1 [cs.LG])</h2>
<h3>Henrique Aguiar, Mauro Santos, Peter Watkinson, Tingting Zhu</h3>
<p>Recent years have seen an increased focus into the tasks of predicting
hospital inpatient risk of deterioration and trajectory evolution due to the
availability of electronic patient data. A common approach to these problems
involves clustering patients time-series information such as vital sign
observations) to determine dissimilar subgroups of the patient population. Most
clustering methods assume time-invariance of vital-signs and are unable to
provide interpretability in clusters that is clinically relevant, for instance,
event or outcome information. In this work, we evaluate three different
clustering models on a large hospital dataset of vital-sign observations from
patients suffering from Chronic Obstructive Pulmonary Disease. We further
propose novel modifications to deal with unevenly sampled time-series data and
unbalanced class distribution to improve phenotype separation. Lastly, we
discuss further avenues of investigation for models to learn patient subgroups
with distinct behaviour and phenotype.
</p>
<a href="http://arxiv.org/abs/2011.08356" target="_blank">arXiv:2011.08356</a> [<a href="http://arxiv.org/pdf/2011.08356" target="_blank">pdf</a>]

<h2>Knowledge-Augmented Dexterous Grasping with Incomplete Sensing. (arXiv:2011.08361v1 [cs.RO])</h2>
<h3>Bharath Rao, Hui Li, Krishna Krishnan, Enkhsaikhan Boldsaikhan, Hongsheng He</h3>
<p>Humans can determine a proper strategy to grasp an object according to the
measured physical attributes or the prior knowledge of the object. This paper
proposes an approach to determining the strategy of dexterous grasping by using
an anthropomorphic robotic hand simply based on a label or a description of an
object. Object attributes are parsed from natural-language descriptions and
augmented with an object knowledge base that is scraped from retailer websites.
A novel metric named joint probability distance is defined to measure distance
between object attributes. The probability distribution of grasp types for the
given object is learned using a deep neural network which takes in object
features as input. The action of the multi-fingered hand with redundant degrees
of freedom (DoF) is controlled by a linear inverse-kinematics model of grasp
topology and scales. The grasping strategy generated by the proposed approach
is evaluated both by simulation and execution on a Sawyer robot with an AR10
robotic hand.
</p>
<a href="http://arxiv.org/abs/2011.08361" target="_blank">arXiv:2011.08361</a> [<a href="http://arxiv.org/pdf/2011.08361" target="_blank">pdf</a>]

<h2>CG-Net: Conditional GIS-aware Network for Individual Building Segmentation in VHR SAR Images. (arXiv:2011.08362v1 [cs.CV])</h2>
<h3>Yao Sun, Yuansheng Hua, Lichao Mou, Xiao Xiang Zhu</h3>
<p>Object retrieval and reconstruction from very high resolution (VHR) synthetic
aperture radar (SAR) images are of great importance for urban SAR applications,
yet highly challenging owing to the complexity of SAR data. This paper
addresses the issue of individual building segmentation from a single VHR SAR
image in large-scale urban areas. To achieve this, we introduce building
footprints from GIS data as complementary information and propose a novel
conditional GIS-aware network (CG-Net). The proposed model learns multi-level
visual features and employs building footprints to normalize the features for
predicting building masks in the SAR image. We validate our method using a high
resolution spotlight TerraSAR-X image collected over Berlin. Experimental
results show that the proposed CG-Net effectively brings improvements with
variant backbones. We further compare two representations of building
footprints, namely complete building footprints and sensor-visible footprint
segments, for our task, and conclude that the use of the former leads to better
segmentation results. Moreover, we investigate the impact of inaccurate GIS
data on our CG-Net, and this study shows that CG-Net is robust against
positioning errors in GIS data. In addition, we propose an approach of ground
truth generation of buildings from an accurate digital elevation model (DEM),
which can be used to generate large-scale SAR image datasets. The segmentation
results can be applied to reconstruct 3D building models at level-of-detail
(LoD) 1, which is demonstrated in our experiments.
</p>
<a href="http://arxiv.org/abs/2011.08362" target="_blank">arXiv:2011.08362</a> [<a href="http://arxiv.org/pdf/2011.08362" target="_blank">pdf</a>]

<h2>Vis-CRF, A Classical Receptive Field Model for VISION. (arXiv:2011.08363v1 [cs.CV])</h2>
<h3>Nasim Nematzadeh, David MW Powers, Trent Lewis</h3>
<p>Over the last decade, a variety of new neurophysiological experiments have
led to new insights as to how, when and where retinal processing takes place,
and the nature of the retinal representation encoding sent to the cortex for
further processing. Based on these neurobiological discoveries, in our previous
work, we provided computer simulation evidence to suggest that Geometrical
illusions are explained in part, by the interaction of multiscale visual
processing performed in the retina. The output of our retinal stage model,
named Vis-CRF, is presented here for a sample of natural image and for several
types of Tilt Illusion, in which the final tilt percept arises from multiple
scale processing of Difference of Gaussians (DoG) and the perceptual
interaction of foreground and background elements (Nematzadeh and Powers, 2019;
Nematzadeh, 2018; Nematzadeh, Powers and Lewis, 2017; Nematzadeh, Lewis and
Powers, 2015).
</p>
<a href="http://arxiv.org/abs/2011.08363" target="_blank">arXiv:2011.08363</a> [<a href="http://arxiv.org/pdf/2011.08363" target="_blank">pdf</a>]

<h2>Extreme Value Preserving Networks. (arXiv:2011.08367v1 [cs.CV])</h2>
<h3>Mingjie Sun, Jianguo Li, Changshui Zhang</h3>
<p>Recent evidence shows that convolutional neural networks (CNNs) are biased
towards textures so that CNNs are non-robust to adversarial perturbations over
textures, while traditional robust visual features like SIFT (scale-invariant
feature transforms) are designed to be robust across a substantial range of
affine distortion, addition of noise, etc with the mimic of human perception
nature. This paper aims to leverage good properties of SIFT to renovate CNN
architectures towards better accuracy and robustness. We borrow the scale-space
extreme value idea from SIFT, and propose extreme value preserving networks
(EVPNets). Experiments demonstrate that EVPNets can achieve similar or better
accuracy than conventional CNNs, while achieving much better robustness on a
set of adversarial attacks (FGSM,PGD,etc) even without adversarial training.
</p>
<a href="http://arxiv.org/abs/2011.08367" target="_blank">arXiv:2011.08367</a> [<a href="http://arxiv.org/pdf/2011.08367" target="_blank">pdf</a>]

<h2>Learning Efficient GANs via Differentiable Masks and co-Attention Distillation. (arXiv:2011.08382v1 [cs.CV])</h2>
<h3>Shaojie Li, Mingbao Lin, Yan Wang, Mingliang Xu, Feiyue Huang, Yongjian Wu, Ling Shao, Rongrong Ji</h3>
<p>Generative Adversarial Networks (GANs) have been widely-used in image
translation, but their high computational and storage costs impede the
deployment on mobile devices. Prevalent methods for CNN compression cannot be
directly applied to GANs due to the complicated generator architecture and the
unstable adversarial training. To solve these, in this paper, we introduce a
novel GAN compression method, termed DMAD, by proposing a Differentiable Mask
and a co-Attention Distillation. The former searches for a light-weight
generator architecture in a training-adaptive manner. To overcome channel
inconsistency when pruning the residual connections, an adaptive cross-block
group sparsity is further incorporated. The latter simultaneously distills
informative attention maps from both the generator and discriminator of a
pre-trained model to the searched generator, effectively stabilizing the
adversarial training of our light-weight model. Experiments show that DMAD can
reduce the Multiply Accumulate Operations (MACs) of CycleGAN by 13x and that of
Pix2Pix by 4x while retaining a comparable performance against the full model.
Code is available at https://github.com/SJLeo/DMAD.
</p>
<a href="http://arxiv.org/abs/2011.08382" target="_blank">arXiv:2011.08382</a> [<a href="http://arxiv.org/pdf/2011.08382" target="_blank">pdf</a>]

<h2>Domain Adaptation based Technique for Image Emotion Recognition using Pre-trained Facial Expression Recognition Models. (arXiv:2011.08388v1 [cs.CV])</h2>
<h3>Puneet Kumar, Balasubramanian Raman</h3>
<p>In this paper, a domain adaptation based technique for recognizing the
emotions in images containing facial, non-facial, and non-human components has
been proposed. We have also proposed a novel technique to explain the proposed
system's predictions in terms of Intersection Score. Image emotion recognition
is useful for graphics, gaming, animation, entertainment, and cinematography.
However, well-labeled large scale datasets and pre-trained models are not
available for image emotion recognition. To overcome this challenge, we have
proposed a deep learning approach based on an attentional convolutional network
that adapts pre-trained facial expression recognition models. It detects the
visual features of an image and performs emotion classification based on them.
The experiments have been performed on the Flickr image dataset, and the images
have been classified in 'angry,' 'happy,' 'sad,' and 'neutral' emotion classes.
The proposed system has demonstrated better performance than the benchmark
results with an accuracy of 63.87% for image emotion recognition. We have also
analyzed the embedding plots for various emotion classes to explain the
proposed system's predictions.
</p>
<a href="http://arxiv.org/abs/2011.08388" target="_blank">arXiv:2011.08388</a> [<a href="http://arxiv.org/pdf/2011.08388" target="_blank">pdf</a>]

<h2>Augmented Fairness: An Interpretable Model Augmenting Decision-Makers' Fairness. (arXiv:2011.08398v1 [cs.LG])</h2>
<h3>Tong Wang, Maytal Saar-Tsechansky</h3>
<p>We propose a model-agnostic approach for mitigating the prediction bias of a
black-box decision-maker, and in particular, a human decision-maker. Our method
detects in the feature space where the black-box decision-maker is biased and
replaces it with a few short decision rules, acting as a "fair surrogate". The
rule-based surrogate model is trained under two objectives, predictive
performance and fairness. Our model focuses on a setting that is common in
practice but distinct from other literature on fairness. We only have black-box
access to the model, and only a limited set of true labels can be queried under
a budget constraint. We formulate a multi-objective optimization for building a
surrogate model, where we simultaneously optimize for both predictive
performance and bias. To train the model, we propose a novel training algorithm
that combines a nondominated sorting genetic algorithm with active learning. We
test our model on public datasets where we simulate various biased "black-box"
classifiers (decision-makers) and apply our approach for interpretable
augmented fairness.
</p>
<a href="http://arxiv.org/abs/2011.08398" target="_blank">arXiv:2011.08398</a> [<a href="http://arxiv.org/pdf/2011.08398" target="_blank">pdf</a>]

<h2>Reinforcement Learning of Graph Neural Networks for Service Function Chaining. (arXiv:2011.08406v1 [cs.AI])</h2>
<h3>DongNyeong Heo, Doyoung Lee, Hee-Gon Kim, Suhyun Park, Heeyoul Choi</h3>
<p>In the management of computer network systems, the service function chaining
(SFC) modules play an important role by generating efficient paths for network
traffic through physical servers with virtualized network functions (VNF). To
provide the highest quality of services, the SFC module should generate a valid
path quickly even in various network topology situations including dynamic VNF
resources, various requests, and changes of topologies. The previous supervised
learning method demonstrated that the network features can be represented by
graph neural networks (GNNs) for the SFC task. However, the performance was
limited to only the fixed topology with labeled data. In this paper, we apply
reinforcement learning methods for training models on various network
topologies with unlabeled data. In the experiments, compared to the previous
supervised learning method, the proposed methods demonstrated remarkable
flexibility in new topologies without re-designing and re-training, while
preserving a similar level of performance.
</p>
<a href="http://arxiv.org/abs/2011.08406" target="_blank">arXiv:2011.08406</a> [<a href="http://arxiv.org/pdf/2011.08406" target="_blank">pdf</a>]

<h2>Sub-clusters of Normal Data for Anomaly Detection. (arXiv:2011.08408v1 [cs.LG])</h2>
<h3>Gahye Lee, Seungkyu Lee</h3>
<p>Anomaly detection in data analysis is an interesting but still challenging
research topic in real world applications. As the complexity of data dimension
increases, it requires to understand the semantic contexts in its description
for effective anomaly characterization. However, existing anomaly detection
methods show limited performances with high dimensional data such as ImageNet.
Existing studies have evaluated their performance on low dimensional, clean and
well separated data set such as MNIST and CIFAR-10. In this paper, we study
anomaly detection with high dimensional and complex normal data. Our
observation is that, in general, anomaly data is defined by semantically
explainable features which are able to be used in defining semantic
sub-clusters of normal data as well. We hypothesize that if there exists
reasonably good feature space semantically separating sub-clusters of given
normal data, unseen anomaly also can be well distinguished in the space from
the normal data. We propose to perform semantic clustering on given normal data
and train a classifier to learn the discriminative feature space where anomaly
detection is finally performed. Based on our careful and extensive experimental
evaluations with MNIST, CIFAR-10, and ImageNet with various combinations of
normal and anomaly data, we show that our anomaly detection scheme outperforms
state of the art methods especially with high dimensional real world images.
</p>
<a href="http://arxiv.org/abs/2011.08408" target="_blank">arXiv:2011.08408</a> [<a href="http://arxiv.org/pdf/2011.08408" target="_blank">pdf</a>]

<h2>Semi-Supervised Few-Shot Atomic Action Recognition. (arXiv:2011.08410v1 [cs.CV])</h2>
<h3>Xiaoyuan Ni, Sizhe Song, Yu-Wing Tai, Chi-Keung Tang</h3>
<p>Despite excellent progress has been made, the performance on action
recognition still heavily relies on specific datasets, which are difficult to
extend new action classes due to labor-intensive labeling. Moreover, the high
diversity in Spatio-temporal appearance requires robust and representative
action feature aggregation and attention. To address the above issues, we focus
on atomic actions and propose a novel model for semi-supervised few-shot atomic
action recognition. Our model features unsupervised and contrastive video
embedding, loose action alignment, multi-head feature comparison, and
attention-based aggregation, together of which enables action recognition with
only a few training examples through extracting more representative features
and allowing flexibility in spatial and temporal alignment and variations in
the action. Experiments show that our model can attain high accuracy on
representative atomic action datasets outperforming their respective
state-of-the-art classification accuracy in full supervision setting.
</p>
<a href="http://arxiv.org/abs/2011.08410" target="_blank">arXiv:2011.08410</a> [<a href="http://arxiv.org/pdf/2011.08410" target="_blank">pdf</a>]

<h2>Adaptive Tracking Control of Soft Robots using Integrated Sensing Skin and Recurrent Neural Networks. (arXiv:2011.08412v1 [cs.RO])</h2>
<h3>Lasitha Weerakoon, Zepeng Ye, Rahul Subramonian Bama, Elisabeth Smela, Miao Yu, Nikhil Chopra</h3>
<p>In this paper, we study integrated estimation and control of soft robots. A
significant challenge in deploying closed loop controllers is reliable
proprioception via integrated sensing in soft robots. Despite the considerable
advances accomplished in fabrication, modelling, and model-based control of
soft robots, integrated sensing and estimation is still in its infancy. To that
end, this paper introduces a new method of estimating the degree of curvature
of a soft robot using a stretchable sensing skin. The skin is a spray-coated
piezoresistive sensing layer on a latex membrane. The mapping from the strain
signal to the degree of curvature is estimated by using a recurrent neural
network. We investigate uni-directional bending as well as bi-directional
bending of a single-segment soft robot. Moreover, an adaptive controller is
developed to track the degree of curvature of the soft robot in the presence of
dynamic uncertainties. Subsequently, using the integrated soft sensing skin, we
experimentally demonstrate successful curvature tracking control of the soft
robot.
</p>
<a href="http://arxiv.org/abs/2011.08412" target="_blank">arXiv:2011.08412</a> [<a href="http://arxiv.org/pdf/2011.08412" target="_blank">pdf</a>]

<h2>Quantifying Sources of Uncertainty in Deep Learning-Based Image Reconstruction. (arXiv:2011.08413v1 [cs.CV])</h2>
<h3>Riccardo Barbano, &#x17d;eljko Kereta, Chen Zhang, Andreas Hauptmann, Simon Arridge, Bangti Jin</h3>
<p>Image reconstruction methods based on deep neural networks have shown
outstanding performance, equalling or exceeding the state-of-the-art results of
conventional approaches, but often do not provide uncertainty information about
the reconstruction. In this work we propose a scalable and efficient framework
to simultaneously quantify aleatoric and epistemic uncertainties in learned
iterative image reconstruction. We build on a Bayesian deep gradient descent
method for quantifying epistemic uncertainty, and incorporate the
heteroscedastic variance of the noise to account for the aleatoric uncertainty.
We show that our method exhibits competitive performance against conventional
benchmarks for computed tomography with both sparse view and limited angle
data. The estimated uncertainty captures the variability in the
reconstructions, caused by the restricted measurement model, and by missing
information, due to the limited angle geometry.
</p>
<a href="http://arxiv.org/abs/2011.08413" target="_blank">arXiv:2011.08413</a> [<a href="http://arxiv.org/pdf/2011.08413" target="_blank">pdf</a>]

<h2>Improved Visual-Inertial Localization for Low-cost Rescue Robots. (arXiv:2011.08418v1 [cs.RO])</h2>
<h3>Xiaoling Long, Qingwen Xu, Yijun Yuan, Zhenpeng He, S&#xf6;ren Schwertfeger</h3>
<p>This paper improves visual-inertial systems to boost the localization
accuracy for low-cost rescue robots. When robots traverse on rugged terrain,
the performance of pose estimation suffers from big noise on the measurements
of the inertial sensors due to ground contact forces, especially for low-cost
sensors. Therefore, we propose \textit{Threshold}-based and \textit{Dynamic
Time Warping}-based methods to detect abnormal measurements and mitigate such
faults. The two methods are embedded into the popular VINS-Mono system to
evaluate their performance. Experiments are performed on simulation and real
robot data, which show that both methods increase the pose estimation accuracy.
Moreover, the \textit{Threshold}-based method performs better when the noise is
small and the \textit{Dynamic Time Warping}-based one shows greater potential
on large noise.
</p>
<a href="http://arxiv.org/abs/2011.08418" target="_blank">arXiv:2011.08418</a> [<a href="http://arxiv.org/pdf/2011.08418" target="_blank">pdf</a>]

<h2>Transducer Adaptive Ultrasound Volume Reconstruction. (arXiv:2011.08419v1 [cs.CV])</h2>
<h3>Hengtao Guo, Sheng Xu, Bradford J. Wood, Pingkun Yan</h3>
<p>Reconstructed 3D ultrasound volume provides more context information compared
to a sequence of 2D scanning frames, which is desirable for various clinical
applications such as ultrasound-guided prostate biopsy. Nevertheless, 3D volume
reconstruction from freehand 2D scans is a very challenging problem, especially
without the use of external tracking devices. Recent deep learning based
methods demonstrate the potential of directly estimating inter-frame motion
between consecutive ultrasound frames. However, such algorithms are specific to
particular transducers and scanning trajectories associated with the training
data, which may not be generalized to other image acquisition settings. In this
paper, we tackle the data acquisition difference as a domain shift problem and
propose a novel domain adaptation strategy to adapt deep learning algorithms to
data acquired with different transducers. Specifically, feature extractors that
generate transducer-invariant features from different datasets are trained by
minimizing the discrepancy between deep features of paired samples in a latent
space. Our results show that the proposed domain adaptation method can
successfully align different feature distributions while preserving the
transducer-specific information for universal freehand ultrasound volume
reconstruction.
</p>
<a href="http://arxiv.org/abs/2011.08419" target="_blank">arXiv:2011.08419</a> [<a href="http://arxiv.org/pdf/2011.08419" target="_blank">pdf</a>]

<h2>Reachability-based Trajectory Safeguard (RTS): A Safe and Fast Reinforcement Learning Safety Layer for Continuous Control. (arXiv:2011.08421v1 [cs.RO])</h2>
<h3>Yifei Simon Shao, Chen Chao, Shreyas Kousik, Ram Vasudevan</h3>
<p>Reinforcement Learning (RL) algorithms have achieved remarkable performance
in decision making and control tasks due to their ability to reason about
long-term, cumulative reward using trial and error. However, during RL
training, applying this trial-and-error approach to real-world robots operating
in safety critical environment may lead to collisions. To address this
challenge, this paper proposes a Reachability-based Trajectory Safeguard (RTS),
which leverages trajectory parameterization and reachability analysis to ensure
safety while a policy is being learned. This method ensures a robot with
continuous action space can be trained from scratch safely in real-time.
Importantly, this safety layer can still be applied after a policy has been
learned. The efficacy of this method is illustrated on three nonlinear robot
models, including a 12-D quadrotor drone, in simulation. By ensuring safety
with RTS, this paper demonstrates that the proposed algorithm is not only safe,
but can achieve a higher reward in a considerably shorter training time when
compared to a non-safe counterpart.
</p>
<a href="http://arxiv.org/abs/2011.08421" target="_blank">arXiv:2011.08421</a> [<a href="http://arxiv.org/pdf/2011.08421" target="_blank">pdf</a>]

<h2>Deep Affordance Foresight: Planning Through What Can Be Done in the Future. (arXiv:2011.08424v1 [cs.RO])</h2>
<h3>Danfei Xu, Ajay Mandlekar, Roberto Mart&#xed;n-Mart&#xed;n, Yuke Zhu, Silvio Savarese, Li Fei-Fei</h3>
<p>Planning in realistic environments requires searching in large planning
spaces. Affordances are a powerful concept to simplify this search, because
they model what actions can be successful in a given situation. However, the
classical notion of affordance is not suitable for long horizon planning
because it only informs the robot about the immediate outcome of actions
instead of what actions are best for achieving a long-term goal. In this paper,
we introduce a new affordance representation that enables the robot to reason
about the long-term effects of actions through modeling what actions are
afforded in the future, thereby informing the robot the best actions to take
next to achieve a task goal. Based on the new representation, we develop a
learning-to-plan method, Deep Affordance Foresight (DAF), that learns partial
environment models of affordances of parameterized motor skills through
trial-and-error. We evaluate DAF on two challenging manipulation domains and
show that it can effectively learn to carry out multi-step tasks, share learned
affordance representations among different tasks, and learn to plan with
high-dimensional image inputs. Additional material is available at
https://sites.google.com/stanford.edu/daf
</p>
<a href="http://arxiv.org/abs/2011.08424" target="_blank">arXiv:2011.08424</a> [<a href="http://arxiv.org/pdf/2011.08424" target="_blank">pdf</a>]

<h2>Deep Reinforcement Learning for Stochastic Computation Offloading in Digital Twin Networks. (arXiv:2011.08430v1 [cs.LG])</h2>
<h3>Yueyue Dai (Member, IEEE), Ke Zhang, Sabita Maharjan (Senior Member, IEEE), Yan Zhang (Fellow, IEEE)</h3>
<p>The rapid development of Industrial Internet of Things (IIoT) requires
industrial production towards digitalization to improve network efficiency.
Digital Twin is a promising technology to empower the digital transformation of
IIoT by creating virtual models of physical objects. However, the provision of
network efficiency in IIoT is very challenging due to resource-constrained
devices, stochastic tasks, and resources heterogeneity. Distributed resources
in IIoT networks can be efficiently exploited through computation offloading to
reduce energy consumption while enhancing data processing efficiency. In this
paper, we first propose a new paradigm Digital Twin Networks (DTN) to build
network topology and the stochastic task arrival model in IIoT systems. Then,
we formulate the stochastic computation offloading and resource allocation
problem to minimize the long-term energy efficiency. As the formulated problem
is a stochastic programming problem, we leverage Lyapunov optimization
technique to transform the original problem into a deterministic per-time slot
problem. Finally, we present Asynchronous Actor-Critic (AAC) algorithm to find
the optimal stochastic computation offloading policy. Illustrative results
demonstrate that our proposed scheme is able to significantly outperforms the
benchmarks.
</p>
<a href="http://arxiv.org/abs/2011.08430" target="_blank">arXiv:2011.08430</a> [<a href="http://arxiv.org/pdf/2011.08430" target="_blank">pdf</a>]

<h2>Revisiting the Sample Complexity of Sparse Spectrum Approximation of Gaussian Processes. (arXiv:2011.08432v1 [cs.LG])</h2>
<h3>Quang Minh Hoang, Trong Nghia Hoang, Hai Pham, David P. Woodruff</h3>
<p>We introduce a new scalable approximation for Gaussian processes with
provable guarantees which hold simultaneously over its entire parameter space.
Our approximation is obtained from an improved sample complexity analysis for
sparse spectrum Gaussian processes (SSGPs). In particular, our analysis shows
that under a certain data disentangling condition, an SSGP's prediction and
model evidence (for training) can well-approximate those of a full GP with low
sample complexity. We also develop a new auto-encoding algorithm that finds a
latent space to disentangle latent input coordinates into well-separated
clusters, which is amenable to our sample complexity analysis. We validate our
proposed method on several benchmarks with promising results supporting our
theoretical analysis.
</p>
<a href="http://arxiv.org/abs/2011.08432" target="_blank">arXiv:2011.08432</a> [<a href="http://arxiv.org/pdf/2011.08432" target="_blank">pdf</a>]

<h2>AdCo: Adversarial Contrast for Efficient Learning of Unsupervised Representations from Self-Trained Negative Adversaries. (arXiv:2011.08435v1 [cs.LG])</h2>
<h3>Qianjiang Hu, Xiao Wang, Wei Hu, Guo-Jun Qi</h3>
<p>Contrastive learning relies on constructing a collection of negative examples
that are sufficiently hard to discriminate against positive queries when their
representations are self-trained. Existing contrastive learning methods either
maintain a queue of negative samples over minibatches while only a small
portion of them are updated in an iteration, or only use the other examples
from the current minibatch as negatives. They could not closely track the
change of the learned representation over iterations by updating the entire
queue as a whole, or discard the useful information from the past minibatches.
Alternatively, we present to directly learn a set of negative adversaries
playing against the self-trained representation. Two players, the
representation network and negative adversaries, are alternately updated to
obtain the most challenging negative examples against which the representation
of positive queries will be trained to discriminate. We further show that the
negative adversaries are updated towards a weighted combination of positive
queries by maximizing the adversarial contrastive loss, thereby allowing them
to closely track the change of representations over time. Experiment results
demonstrate the proposed Adversarial Contrastive (AdCo) model not only achieves
superior performances with little computational overhead to the
state-of-the-art contrast models, but also can be pretrained more rapidly with
fewer epochs.
</p>
<a href="http://arxiv.org/abs/2011.08435" target="_blank">arXiv:2011.08435</a> [<a href="http://arxiv.org/pdf/2011.08435" target="_blank">pdf</a>]

<h2>Shared Cross-Modal Trajectory Prediction for Autonomous Driving. (arXiv:2011.08436v1 [cs.CV])</h2>
<h3>Chiho Choi, Joon Hee Choi, Jiachen Li, Srikanth Malla</h3>
<p>Predicting future trajectories of traffic agents in highly interactive
environments is an essential and challenging problem for the safe operation of
autonomous driving systems. On the basis of the fact that self-driving vehicles
are equipped with various types of sensors (e.g., LiDAR scanner, RGB camera,
radar, etc.), we propose a Cross-Modal Embedding framework that aims to benefit
from the use of multiple input modalities. At training time, our model learns
to embed a set of complementary features in a shared latent space by jointly
optimizing the objective functions across different types of input data. At
test time, a single input modality (e.g., LiDAR data) is required to generate
predictions from the input perspective (i.e., in the LiDAR space), while taking
advantages from the model trained with multiple sensor modalities. An extensive
evaluation is conducted to show the efficacy of the proposed framework using
two benchmark driving datasets.
</p>
<a href="http://arxiv.org/abs/2011.08436" target="_blank">arXiv:2011.08436</a> [<a href="http://arxiv.org/pdf/2011.08436" target="_blank">pdf</a>]

<h2>Edge Intelligence for Energy-efficient Computation Offloading and Resource Allocation in 5G Beyond. (arXiv:2011.08442v1 [cs.LG])</h2>
<h3>Yueyue Dai (Member, IEEE), Ke Zhang, Sabita Maharjan (Senior Member, IEEE), Yan Zhang (Fellow, IEEE)</h3>
<p>5G beyond is an end-edge-cloud orchestrated network that can exploit
heterogeneous capabilities of the end devices, edge servers, and the cloud and
thus has the potential to enable computation-intensive and delay-sensitive
applications via computation offloading. However, in multi user wireless
networks, diverse application requirements and the possibility of various radio
access modes for communication among devices make it challenging to design an
optimal computation offloading scheme. In addition, having access to complete
network information that includes variables such as wireless channel state, and
available bandwidth and computation resources, is a major issue. Deep
Reinforcement Learning (DRL) is an emerging technique to address such an issue
with limited and less accurate network information. In this paper, we utilize
DRL to design an optimal computation offloading and resource allocation
strategy for minimizing system energy consumption. We first present a
multi-user end-edge-cloud orchestrated network where all devices and base
stations have computation capabilities. Then, we formulate the joint
computation offloading and resource allocation problem as a Markov Decision
Process (MDP) and propose a new DRL algorithm to minimize system energy
consumption. Numerical results based on a real-world dataset demonstrate that
the proposed DRL-based algorithm significantly outperforms the benchmark
policies in terms of system energy consumption. Extensive simulations show that
learning rate, discount factor, and number of devices have considerable
influence on the performance of the proposed algorithm.
</p>
<a href="http://arxiv.org/abs/2011.08442" target="_blank">arXiv:2011.08442</a> [<a href="http://arxiv.org/pdf/2011.08442" target="_blank">pdf</a>]

<h2>EvoPose2D: Pushing the Boundaries of 2D Human Pose Estimation using Neuroevolution. (arXiv:2011.08446v1 [cs.CV])</h2>
<h3>William McNally, Kanav Vats, Alexander Wong, John McPhee</h3>
<p>Neural architecture search has proven to be highly effective in the design of
computationally efficient, task-specific convolutional neural networks across
several areas of computer vision. In 2D human pose estimation, however, its
application has been limited by high computational demands. Hypothesizing that
neural architecture search holds great potential for 2D human pose estimation,
we propose a new weight transfer scheme that relaxes function-preserving
mutations, enabling us to accelerate neuroevolution in a flexible manner. Our
method produces 2D human pose network designs that are more efficient and more
accurate than state-of-the-art hand-designed networks. In fact, the generated
networks can process images at higher resolutions using less computation than
previous networks at lower resolutions, permitting us to push the boundaries of
2D human pose estimation. Our baseline network designed using neuroevolution,
which we refer to as EvoPose2D-S, provides comparable accuracy to
SimpleBaseline while using 4.9x fewer floating-point operations and 13.5x fewer
parameters. Our largest network, EvoPose2D-L, achieves new state-of-the-art
accuracy on the Microsoft COCO Keypoints benchmark while using 2.0x fewer
operations and 4.3x fewer parameters than its nearest competitor.
</p>
<a href="http://arxiv.org/abs/2011.08446" target="_blank">arXiv:2011.08446</a> [<a href="http://arxiv.org/pdf/2011.08446" target="_blank">pdf</a>]

<h2>A Quantitative Perspective on Values of Domain Knowledge for Machine Learning. (arXiv:2011.08450v1 [cs.LG])</h2>
<h3>Jianyi Yang, Shaolei Ren</h3>
<p>With the exploding popularity of machine learning, domain knowledge in
various forms has been playing a crucial role in improving the learning
performance, especially when training data is limited. Nonetheless, there is
little understanding of to what extent domain knowledge can affect a machine
learning task from a quantitative perspective. To increase the transparency and
rigorously explain the role of domain knowledge in machine learning, we study
the problem of quantifying the values of domain knowledge in terms of its
contribution to the learning performance in the context of informed machine
learning. We propose a quantification method based on Shapley value that fairly
attributes the overall learning performance improvement to different domain
knowledge. We also present Monte-Carlo sampling to approximate the fair value
of domain knowledge with a polynomial time complexity. We run experiments of
injecting symbolic domain knowledge into semi-supervised learning tasks on both
MNIST and CIFAR10 datasets, providing quantitative values of different symbolic
knowledge and rigorously explaining how it affects the machine learning
performance in terms of test accuracy.
</p>
<a href="http://arxiv.org/abs/2011.08450" target="_blank">arXiv:2011.08450</a> [<a href="http://arxiv.org/pdf/2011.08450" target="_blank">pdf</a>]

<h2>Learning Dense Rewards for Contact-Rich Manipulation Tasks. (arXiv:2011.08458v1 [cs.RO])</h2>
<h3>Zheng Wu, Wenzhao Lian, Vaibhav Unhelkar, Masayoshi Tomizuka, Stefan Schaal</h3>
<p>Rewards play a crucial role in reinforcement learning. To arrive at the
desired policy, the design of a suitable reward function often requires
significant domain expertise as well as trial-and-error. Here, we aim to
minimize the effort involved in designing reward functions for contact-rich
manipulation tasks. In particular, we provide an approach capable of extracting
dense reward functions algorithmically from robots' high-dimensional
observations, such as images and tactile feedback. In contrast to
state-of-the-art high-dimensional reward learning methodologies, our approach
does not leverage adversarial training, and is thus less prone to the
associated training instabilities. Instead, our approach learns rewards by
estimating task progress in a self-supervised manner. We demonstrate the
effectiveness and efficiency of our approach on two contact-rich manipulation
tasks, namely, peg-in-hole and USB insertion. The experimental results indicate
that the policies trained with the learned reward function achieves better
performance and faster convergence compared to the baselines.
</p>
<a href="http://arxiv.org/abs/2011.08458" target="_blank">arXiv:2011.08458</a> [<a href="http://arxiv.org/pdf/2011.08458" target="_blank">pdf</a>]

<h2>SRF-GAN: Super-Resolved Feature GAN for Multi-Scale Representation. (arXiv:2011.08459v1 [cs.CV])</h2>
<h3>Seong-Ho Lee, Seung-Hwan Bae</h3>
<p>Recent convolutional object detectors exploit multi-scale feature
representations added with top-down pathway in order to detect objects at
different scales and learn stronger semantic feature responses. In general,
during the top-down feature propagation, the coarser feature maps are upsampled
to be combined with the features forwarded from bottom-up pathway, and the
combined stronger semantic features are inputs of detector's headers. However,
simple interpolation methods (e.g. nearest neighbor and bilinear) are still
used for increasing feature resolutions although they cause noisy and blurred
features. In this paper, we propose a novel generator for super-resolving
features of the convolutional object detectors. To achieve this, we first
design super-resolved feature GAN (SRF-GAN) consisting of a detection-based
generator and a feature patch discriminator. In addition, we present SRF-GAN
losses for generating the high quality of super-resolved features and improving
detection accuracy together. Our SRF generator can substitute for the
traditional interpolation methods, and easily fine-tuned combined with other
conventional detectors. To prove this, we have implemented our SRF-GAN by using
the several recent one-stage and two-stage detectors, and improved detection
accuracy over those detectors. Code is available at
https://github.com/SHLee-cv/SRF-GAN.
</p>
<a href="http://arxiv.org/abs/2011.08459" target="_blank">arXiv:2011.08459</a> [<a href="http://arxiv.org/pdf/2011.08459" target="_blank">pdf</a>]

<h2>Meta Automatic Curriculum Learning. (arXiv:2011.08463v1 [cs.LG])</h2>
<h3>R&#xe9;my Portelas, Cl&#xe9;ment Romac, Katja Hofmann, Pierre-Yves Oudeyer</h3>
<p>A major challenge in the Deep RL (DRL) community is to train agents able to
generalize their control policy over situations never seen in training.
Training on diverse tasks has been identified as a key ingredient for good
generalization, which pushed researchers towards using rich procedural task
generation systems controlled through complex continuous parameter spaces. In
such complex task spaces, it is essential to rely on some form of Automatic
Curriculum Learning (ACL) to adapt the task sampling distribution to a given
learning agent, instead of randomly sampling tasks, as many could end up being
either trivial or unfeasible. Since it is hard to get prior knowledge on such
task spaces, many ACL algorithms explore the task space to detect progress
niches over time, a costly tabula-rasa process that needs to be performed for
each new learning agents, although they might have similarities in their
capabilities profiles. To address this limitation, we introduce the concept of
Meta-ACL, and formalize it in the context of black-box RL learners, i.e.
algorithms seeking to generalize curriculum generation to an (unknown)
distribution of learners. In this work, we present AGAIN, a first instantiation
of Meta-ACL, and showcase its benefits for curriculum generation over classical
ACL in multiple simulated environments including procedurally generated parkour
environments with learners of varying morphologies. Videos and code are
available at https://sites.google.com/view/meta-acl .
</p>
<a href="http://arxiv.org/abs/2011.08463" target="_blank">arXiv:2011.08463</a> [<a href="http://arxiv.org/pdf/2011.08463" target="_blank">pdf</a>]

<h2>Exploring Intermediate Representation for Monocular Vehicle Pose Estimation. (arXiv:2011.08464v1 [cs.CV])</h2>
<h3>Shichao Li, Zengqiang Yan, Hongyang Li, Kwang-Ting Cheng</h3>
<p>We present a new learning-based approach to recover egocentric 3D vehicle
pose from a single RGB image. In contrast to previous works that directly map
from local appearance to 3D angles, we explore a progressive approach by
extracting meaningful Intermediate Geometrical Representations (IGRs) for 3D
pose estimation. We design a deep model that transforms perceived intensities
to IGRs, which are mapped to a 3D representation encoding object orientation in
the camera coordinate system. To fulfill our goal, we need to specify what IGRs
to use and how to learn them more effectively. We answer the former question by
designing an interpolated cuboid representation that derives from primitive 3D
annotation readily. The latter question motivates us to incorporate geometry
knowledge by designing a new loss function based on a projective invariant.
This loss function allows unlabeled data to be used in the training stage which
is validated to improve representation learning. Our system outperforms
previous monocular RGB-based methods for joint vehicle detection and pose
estimation on the KITTI benchmark, achieving performance even comparable to
stereo methods. Code and pre-trained models will be available at the project
website.
</p>
<a href="http://arxiv.org/abs/2011.08464" target="_blank">arXiv:2011.08464</a> [<a href="http://arxiv.org/pdf/2011.08464" target="_blank">pdf</a>]

<h2>Towards All-around Knowledge Transferring: Learning From Task-irrelevant Labels. (arXiv:2011.08470v1 [cs.LG])</h2>
<h3>Yinghui Li, Ruiyang Liu, ZiHao Zhang, Ning Ding, Ying Shen, Linmi Tao, Hai-Tao Zheng</h3>
<p>Deep neural models have hitherto achieved significant performances on
numerous classification tasks, but meanwhile require sufficient manually
annotated data. Since it is extremely time-consuming and expensive to annotate
adequate data for each classification task, learning an empirically effective
model with generalization on small dataset has received increased attention.
Existing efforts mainly focus on transferring task-relevant knowledge from
other similar data to tackle the issue. These approaches have yielded
remarkable improvements, yet neglecting the fact that the task-irrelevant
features could bring out massive negative transfer effects. To date, no
large-scale studies have been performed to investigate the impact of
task-irrelevant features, let alone the utilization of this kind of features.
In this paper, we firstly propose Task-Irrelevant Transfer Learning (TIRTL) to
exploit task-irrelevant features, which mainly are extracted from
task-irrelevant labels. Particularly, we suppress the expression of
task-irrelevant information and facilitate the learning process of
classification. We also provide a theoretical explanation of our method. In
addition, TIRTL does not conflict with those that have previously exploited
task-relevant knowledge and can be well combined to enable the simultaneous
utilization of task-relevant and task-irrelevant features for the first time.
In order to verify the effectiveness of our theory and method, we conduct
extensive experiments on facial expression recognition and digit recognition
tasks. Our source code will be also available in the future for
reproducibility.
</p>
<a href="http://arxiv.org/abs/2011.08470" target="_blank">arXiv:2011.08470</a> [<a href="http://arxiv.org/pdf/2011.08470" target="_blank">pdf</a>]

<h2>Federated Composite Optimization. (arXiv:2011.08474v1 [cs.LG])</h2>
<h3>Honglin Yuan, Manzil Zaheer, Sashank Reddi</h3>
<p>Federated Learning (FL) is a distributed learning paradigm which scales
on-device learning collaboratively and privately. Standard FL algorithms such
as Federated Averaging (FedAvg) are primarily geared towards smooth
unconstrained settings. In this paper, we study the Federated Composite
Optimization (FCO) problem, where the objective function in FL includes an
additive (possibly) non-smooth component. Such optimization problems are
fundamental to machine learning and arise naturally in the context of
regularization (e.g., sparsity, low-rank, monotonicity, and constraint). To
tackle this problem, we propose different primal/dual averaging approaches and
study their communication and computation complexities. Of particular interest
is Federated Dual Averaging (FedDualAvg), a federated variant of the dual
averaging algorithm. FedDualAvg uses a novel double averaging procedure, which
involves gradient averaging step in standard dual averaging and an average of
client updates akin to standard federated averaging. Our theoretical analysis
and empirical experiments demonstrate that FedDualAvg outperforms baselines for
FCO.
</p>
<a href="http://arxiv.org/abs/2011.08474" target="_blank">arXiv:2011.08474</a> [<a href="http://arxiv.org/pdf/2011.08474" target="_blank">pdf</a>]

<h2>DLRRMS: Deep Learning based Respiratory Rate Monitoring System using Mobile Robots and Edges. (arXiv:2011.08482v1 [cs.RO])</h2>
<h3>Haimiao Mo, Shuai Ding (Member, IEEE), Shanlin Yang, Xi Zheng (Member, IEEE), Athanasios V. Vasilakos</h3>
<p>Deep learning technology has been widely used in edges. However, the limited
storage and computing resources of mobile devices cannot meet the real-time
requirements of deep neural network computing. In this paper, we propose a
safer respiratory rate monitoring system using a three-tier architecture: robot
layers, edge layers, and cloud layers. We decompose the task into a three-tier
architecture of a lightweight network according to the computing resources of
different devices, including mobile robots, edges, and clouds. We deploy
feature extraction tasks, Spatio-temporal feature tracking tasks, and signal
extraction and preprocessing tasks to robot layers, edge layers, and cloud
layers, respectively. We have deployed this non-contact respiratory monitoring
system in the Second Affiliated Hospital of the Anhui Medical University of
China. Experimental results show that the proposed approach in this paper
significantly outperforms other approaches. It is supported by the computation
time cost of robot+edge+cloud architecture, which are 2.26 ms per frame, 27.48
ms per frame, 0.78 seconds for processing one-minute length respiratory
signals, respectively. Furthermore, the computation time costs of using the
proposed system to calculate the respiratory rate are less than that of
edge+cloud architecture and cloud architecture.
</p>
<a href="http://arxiv.org/abs/2011.08482" target="_blank">arXiv:2011.08482</a> [<a href="http://arxiv.org/pdf/2011.08482" target="_blank">pdf</a>]

<h2>Combining Reinforcement Learning with Model Predictive Control for On-Ramp Merging. (arXiv:2011.08484v1 [cs.RO])</h2>
<h3>Joseph Lubars, Harsh Gupta, Adnan Raja, R. Srikant, Liyun Li, Xinzhou Wu</h3>
<p>We consider the problem of designing an algorithm to allow a car to
autonomously merge on to a highway from an on-ramp. Two broad classes of
techniques have been proposed to solve motion planning problems in autonomous
driving: Model Predictive Control (MPC) and Reinforcement Learning (RL). In
this paper, we first establish the strengths and weaknesses of state-of-the-art
MPC and RL-based techniques through simulations. We show that the performance
of the RL agent is worse than that of the MPC solution from the perspective of
safety and robustness to out-of-distribution traffic patterns, i.e., traffic
patterns which were not seen by the RL agent during training. On the other
hand, the performance of the RL agent is better than that of the MPC solution
when it comes to efficiency and passenger comfort. We subsequently present an
algorithm which blends the model-free RL agent with the MPC solution and show
that it provides better trade-offs between all metrics -- passenger comfort,
efficiency, crash rate and robustness.
</p>
<a href="http://arxiv.org/abs/2011.08484" target="_blank">arXiv:2011.08484</a> [<a href="http://arxiv.org/pdf/2011.08484" target="_blank">pdf</a>]

<h2>Close Category Generalization. (arXiv:2011.08485v1 [cs.LG])</h2>
<h3>Yao-Yuan Yang, Cyrus Rashtchian, Ruslan Salakhutdinov, Kamalika Chaudhuri</h3>
<p>Out-of-distribution generalization is a core challenge in machine learning.
We introduce and propose a solution to a new type of out-of-distribution
evaluation, which we call close category generalization. This task specifies
how a classifier should extrapolate to unseen classes by considering a
bi-criteria objective: (i) on in-distribution examples, output the correct
label, and (ii) on out-of-distribution examples, output the label of the
nearest neighbor in the training set. In addition to formalizing this problem,
we present a new training algorithm to improve the close category
generalization of neural networks. We compare to many baselines, including
robust algorithms and out-of-distribution detection methods, and we show that
our method has better or comparable close category generalization. Then, we
investigate a related representation learning task, and we find that performing
well on close category generalization correlates with learning a good
representation of an unseen class and with finding a good initialization for
few-shot learning. Code available at
https://github.com/yangarbiter/close-category-generalization
</p>
<a href="http://arxiv.org/abs/2011.08485" target="_blank">arXiv:2011.08485</a> [<a href="http://arxiv.org/pdf/2011.08485" target="_blank">pdf</a>]

<h2>A Time-Frequency based Suspicious Activity Detection for Anti-Money Laundering. (arXiv:2011.08492v1 [cs.LG])</h2>
<h3>Utku G&#xf6;rkem Ketenci, Tolga Kurt, Selim &#xd6;nal, Cenk Erbil, Sinan Akt&#xfc;rko&#x11f;lu, Hande &#x15e;erban &#x130;lhan</h3>
<p>Money laundering is the crucial mechanism utilized by criminals to inject
proceeds of crime to the financial system. The primary responsibility of the
detection of suspicious activity related to money laundering is with the
financial institutions. Most of the current systems in these institutions are
rule-based and ineffective. The available data science-based anti-money
laundering (AML) models in order to replace the existing rule-based systems
work on customer relationship management (CRM) features and time
characteristics of transaction behaviour. However, there is still a challenge
on accuracy and problems around feature engineering due to thousands of
possible features.

Aiming to improve the detection performance of suspicious transaction
monitoring systems for AML systems, in this article, we introduce a novel
feature set based on time-frequency analysis, that makes use of 2-D
representations of financial transactions. Random forest is utilized as a
machine learning method, and simulated annealing is adopted for hyperparameter
tuning. The designed algorithm is tested on real banking data, proving the
efficacy of the results in practically relevant environments. It is shown that
the time-frequency characteristics of suspicious and non-suspicious entities
differentiate significantly, which would substantially improve the precision of
data science-based transaction monitoring systems looking at only time-series
transaction and CRM features.
</p>
<a href="http://arxiv.org/abs/2011.08492" target="_blank">arXiv:2011.08492</a> [<a href="http://arxiv.org/pdf/2011.08492" target="_blank">pdf</a>]

<h2>Unsupervised BatchNorm Adaptation (UBNA): A Domain Adaptation Method for Semantic Segmentation Without Using Source Domain Representations. (arXiv:2011.08502v1 [cs.CV])</h2>
<h3>Marvin Klingner, Jan-Aike Term&#xf6;hlen, Jacob Ritterbach, Tim Fingscheidt</h3>
<p>In this paper we present a solution to the task of "unsupervised domain
adaptation (UDA) of a pre-trained semantic segmentation model without relying
on any source domain representations". Previous UDA approaches for semantic
segmentation either employed simultaneous training of the model in the source
and target domains, or they relied on a generator network, replaying source
domain data to the model during adaptation. In contrast, we present our novel
Unsupervised BatchNorm Adaptation (UBNA) method, which adapts a pre-trained
model to an unseen target domain without using---beyond the existing model
parameters from pre-training---any source domain representations (neither data,
nor generators) and which can also be applied in an online setting or using
just a few unlabeled images from the target domain in a few-shot manner.
Specifically, we partially adapt the normalization layer statistics to the
target domain using an exponentially decaying momentum factor, thereby mixing
the statistics from both domains. By evaluation on standard UDA benchmarks for
semantic segmentation we show that this is superior to a model without
adaptation and to baseline approaches using statistics from the target domain
only. Compared to standard UDA approaches we report a trade-off between
performance and usage of source domain representations.
</p>
<a href="http://arxiv.org/abs/2011.08502" target="_blank">arXiv:2011.08502</a> [<a href="http://arxiv.org/pdf/2011.08502" target="_blank">pdf</a>]

<h2>Digging Deeper into CRNN Model in Chinese Text Images Recognition. (arXiv:2011.08505v1 [cs.CV])</h2>
<h3>Kunhong Yu, Yuze Zhang</h3>
<p>Automatic text image recognition is a prevalent application in computer
vision field. One efficient way is use Convolutional Recurrent Neural
Network(CRNN) to accomplish task in an end-to-end(End2End) fashion. However,
CRNN notoriously fails to detect multi-row images and excel-like images. In
this paper, we present one alternative to first recognize single-row images,
then extend the same architecture to recognize multi-row images with proposed
multiple methods. To recognize excel-like images containing box lines, we
propose Line-Deep Denoising Convolutional AutoEncoder(Line-DDeCAE) to recover
box lines. Finally, we present one Knowledge Distillation(KD) method to
compress original CRNN model without loss of generality. To carry out
experiments, we first generate artificial samples from one Chinese novel book,
then conduct various experiments to verify our methods.
</p>
<a href="http://arxiv.org/abs/2011.08505" target="_blank">arXiv:2011.08505</a> [<a href="http://arxiv.org/pdf/2011.08505" target="_blank">pdf</a>]

<h2>Generalized Continual Zero-Shot Learning. (arXiv:2011.08508v1 [cs.CV])</h2>
<h3>Chandan Gautam, Sethupathy Parameswaran, Ashish Mishra, Suresh Sundaram</h3>
<p>Recently, zero-shot learning (ZSL) emerged as an exciting topic and attracted
a lot of attention. ZSL aims to classify unseen classes by transferring the
knowledge from seen classes to unseen classes based on the class description.
Despite showing promising performance, ZSL approaches assume that the training
samples from all seen classes are available during the training, which is
practically not feasible. To address this issue, we propose a more generalized
and practical setup for ZSL, i.e., continual ZSL (CZSL), where classes arrive
sequentially in the form of a task and it actively learns from the changing
environment by leveraging the past experience. Further, to enhance the
reliability, we develop CZSL for a single head continual learning setting where
task identity is revealed during the training process but not during the
testing. To avoid catastrophic forgetting and intransigence, we use knowledge
distillation and storing and replay the few samples from previous tasks using a
small episodic memory. We develop baselines and evaluate generalized CZSL on
five ZSL benchmark datasets for two different settings of continual learning:
with and without class incremental. Moreover, CZSL is developed for two types
of variational autoencoders, which generates two types of features for
classification: (i) generated features at output space and (ii) generated
discriminative features at the latent space. The experimental results clearly
indicate the single head CZSL is more generalizable and suitable for practical
applications.
</p>
<a href="http://arxiv.org/abs/2011.08508" target="_blank">arXiv:2011.08508</a> [<a href="http://arxiv.org/pdf/2011.08508" target="_blank">pdf</a>]

<h2>A Digital Image Processing Approach for Hepatic Diseases Staging based on the Glisson's Capsule. (arXiv:2011.08513v1 [cs.CV])</h2>
<h3>Marco Trombini, Paolo Borro, Sebastiano Ziola, Silvana Dellepiane</h3>
<p>Due to the need for quick and effective treatments for liver diseases, which
are among the most common health problems in the world, staging fibrosis
through non-invasive and economic methods has become of great importance.
Taking inspiration from diagnostic laparoscopy, used in the past for hepatic
diseases, in this paper ultrasound images of the liver are studied, focusing on
a specific region of the organ where the Glisson's capsule is visible. In
ultrasound images, the Glisson's capsule appears in the shape of a line which
can be extracted via classical methods in literature. By making use of a
combination of standard image processing techniques and Convolutional Neural
Network approaches, the scope of this work is to give evidence to the idea that
a great informative potential relies on smoothness of the Glisson's capsule
surface. To this purpose, several classifiers are taken into consideration,
which deal with different type of data, namely ultrasound images, binary images
depicting the Glisson's line, and features vector extracted from the original
image. This is a preliminary study that has been retrospectively conducted,
based on the results of the elastosonography examination.
</p>
<a href="http://arxiv.org/abs/2011.08513" target="_blank">arXiv:2011.08513</a> [<a href="http://arxiv.org/pdf/2011.08513" target="_blank">pdf</a>]

<h2>ACSC: Automatic Calibration for Non-repetitive Scanning Solid-State LiDAR and Camera Systems. (arXiv:2011.08516v1 [cs.CV])</h2>
<h3>Jiahe Cui, Jianwei Niu, Zhenchao Ouyang, Yunxiang He, Dian Liu</h3>
<p>Recently, the rapid development of Solid-State LiDAR (SSL) enables low-cost
and efficient obtainment of 3D point clouds from the environment, which has
inspired a large quantity of studies and applications. However, the
non-uniformity of its scanning pattern, and the inconsistency of the ranging
error distribution bring challenges to its calibration task. In this paper, we
proposed a fully automatic calibration method for the non-repetitive scanning
SSL and camera systems. First, a temporal-spatial-based geometric feature
refinement method is presented, to extract effective features from SSL point
clouds; then, the 3D corners of the calibration target (a printed checkerboard)
are estimated with the reflectance distribution of points. Based on the above,
a target-based extrinsic calibration method is finally proposed. We evaluate
the proposed method on different types of LiDAR and camera sensor combinations
in real conditions, and achieve accuracy and robustness calibration results.
The code is available at https://github.com/HViktorTsoi/ACSC.git .
</p>
<a href="http://arxiv.org/abs/2011.08516" target="_blank">arXiv:2011.08516</a> [<a href="http://arxiv.org/pdf/2011.08516" target="_blank">pdf</a>]

<h2>Bridging the Performance Gap Between Pose Estimation Networks Trained on Real And Synthetic Data Using Domain Randomization. (arXiv:2011.08517v1 [cs.CV])</h2>
<h3>Frederik Hagelskjaer, Anders Glent Buch</h3>
<p>Since the introduction of deep learning methods, pose estimation performance
has increased drastically. Usually, large amounts of manually annotated
training data are required for these networks to perform. While training on
synthetic data can avoid the manual annotation, this introduces another
obstacle. There is currently a large performance gap between methods trained on
real and synthetic data. This paper introduces a new method, which bridges the
gap between real and synthetically trained networks. As opposed to other
methods, the network utilizes 3D point clouds. This allows both for domain
randomization in 3D and to use neighboring geometric information during
inference. Experiments on three large pose estimation benchmarks show that the
presented method outperforms previous methods trained on synthetic data and
achieves comparable-and sometimes superior-results to existing methods trained
on real data.
</p>
<a href="http://arxiv.org/abs/2011.08517" target="_blank">arXiv:2011.08517</a> [<a href="http://arxiv.org/pdf/2011.08517" target="_blank">pdf</a>]

<h2>DeepSeqSLAM: A Trainable CNN+RNN for Joint Global Description and Sequence-based Place Recognition. (arXiv:2011.08518v1 [cs.CV])</h2>
<h3>Marvin Chanc&#xe1;n, Michael Milford</h3>
<p>Sequence-based place recognition methods for all-weather navigation are
well-known for producing state-of-the-art results under challenging day-night
or summer-winter transitions. These systems, however, rely on complex
handcrafted heuristics for sequential matching - which are applied on top of a
pre-computed pairwise similarity matrix between reference and query image
sequences of a single route - to further reduce false-positive rates compared
to single-frame retrieval methods. As a result, performing multi-frame place
recognition can be extremely slow for deployment on autonomous vehicles or
evaluation on large datasets, and fail when using relatively short parameter
values such as a sequence length of 2 frames. In this paper, we propose
DeepSeqSLAM: a trainable CNN+RNN architecture for jointly learning visual and
positional representations from a single monocular image sequence of a route.
We demonstrate our approach on two large benchmark datasets, Nordland and
Oxford RobotCar - recorded over 728 km and 10 km routes, respectively, each
during 1 year with multiple seasons, weather, and lighting conditions. On
Nordland, we compare our method to two state-of-the-art sequence-based methods
across the entire route under summer-winter changes using a sequence length of
2 and show that our approach can get over 72% AUC compared to 27% AUC for Delta
Descriptors and 2% AUC for SeqSLAM; while drastically reducing the deployment
time from around 1 hour to 1 minute against both. The framework code and video
are available at https://mchancan.github.io/deepseqslam
</p>
<a href="http://arxiv.org/abs/2011.08518" target="_blank">arXiv:2011.08518</a> [<a href="http://arxiv.org/pdf/2011.08518" target="_blank">pdf</a>]

<h2>Slender Object Detection: Diagnoses and Improvements. (arXiv:2011.08529v1 [cs.CV])</h2>
<h3>Zhaoyi Wan, Yimin Chen, Sutao Deng, Cong Yao, Jiebo Luo</h3>
<p>In this paper, we are concerned with the detection of a particular type of
objects with extreme aspect ratios, namely slender objects. In real-world
scenarios as well as widely-used datasets (such as COCO), slender objects are
actually very common. However, this type of object has been largely overlooked
by previous object detection algorithms. Upon our investigation, for a
classical object detection method, a drastic drop of 18.9% mAP on COCO is
observed, if solely evaluated on slender objects. Therefore, We systematically
study the problem of slender object detection in this work. Accordingly, an
analytical framework with carefully designed benchmark and evaluation protocols
is established, in which different algorithms and modules can be inspected and
compared. Our key findings include: 1) the essential role of anchors in label
assignment; 2) the descriptive capability of the 2-point representation; 3) the
crucial strategies for improving the detection of slender objects and regular
objects. Our work identifies and extends the insights of existing methods that
are previously underexploited. Furthermore, we propose a feature adaption
strategy that achieves clear and consistent improvements over current
representative object detection methods. In particular, a natural and effective
extension of the center prior, which leads to a significant improvement on
slender objects, is devised. We believe this work opens up new opportunities
and calibrates ablation standards for future research in the field of object
detection.
</p>
<a href="http://arxiv.org/abs/2011.08529" target="_blank">arXiv:2011.08529</a> [<a href="http://arxiv.org/pdf/2011.08529" target="_blank">pdf</a>]

<h2>A Divide et Impera Approach for 3D Shape Reconstruction from Multiple Views. (arXiv:2011.08534v1 [cs.CV])</h2>
<h3>Riccardo Spezialetti, David Joseph Tan, Alessio Tonioni, Keisuke Tateno, Federico Tombari</h3>
<p>Estimating the 3D shape of an object from a single or multiple images has
gained popularity thanks to the recent breakthroughs powered by deep learning.
Most approaches regress the full object shape in a canonical pose, possibly
extrapolating the occluded parts based on the learned priors. However, their
viewpoint invariant technique often discards the unique structures visible from
the input images. In contrast, this paper proposes to rely on viewpoint variant
reconstructions by merging the visible information from the given views. Our
approach is divided into three steps. Starting from the sparse views of the
object, we first align them into a common coordinate system by estimating the
relative pose between all the pairs. Then, inspired by the traditional voxel
carving, we generate an occupancy grid of the object taken from the silhouette
on the images and their relative poses. Finally, we refine the initial
reconstruction to build a clean 3D model which preserves the details from each
viewpoint. To validate the proposed method, we perform a comprehensive
evaluation on the ShapeNet reference benchmark in terms of relative pose
estimation and 3D shape reconstruction.
</p>
<a href="http://arxiv.org/abs/2011.08534" target="_blank">arXiv:2011.08534</a> [<a href="http://arxiv.org/pdf/2011.08534" target="_blank">pdf</a>]

<h2>Efficient Exploration of Reward Functions in Inverse Reinforcement Learning via Bayesian Optimization. (arXiv:2011.08541v1 [cs.LG])</h2>
<h3>Sreejith Balakrishnan, Quoc Phong Nguyen, Bryan Kian Hsiang Low, Harold Soh</h3>
<p>The problem of inverse reinforcement learning (IRL) is relevant to a variety
of tasks including value alignment and robot learning from demonstration.
Despite significant algorithmic contributions in recent years, IRL remains an
ill-posed problem at its core; multiple reward functions coincide with the
observed behavior and the actual reward function is not identifiable without
prior knowledge or supplementary information. This paper presents an IRL
framework called Bayesian optimization-IRL (BO-IRL) which identifies multiple
solutions that are consistent with the expert demonstrations by efficiently
exploring the reward function space. BO-IRL achieves this by utilizing Bayesian
Optimization along with our newly proposed kernel that (a) projects the
parameters of policy invariant reward functions to a single point in a latent
space and (b) ensures nearby points in the latent space correspond to reward
functions yielding similar likelihoods. This projection allows the use of
standard stationary kernels in the latent space to capture the correlations
present across the reward function space. Empirical results on synthetic and
real-world environments (model-free and model-based) show that BO-IRL discovers
multiple reward functions while minimizing the number of expensive exact policy
optimizations.
</p>
<a href="http://arxiv.org/abs/2011.08541" target="_blank">arXiv:2011.08541</a> [<a href="http://arxiv.org/pdf/2011.08541" target="_blank">pdf</a>]

<h2>Structural and Functional Decomposition for Personality Image Captioning in a Communication Game. (arXiv:2011.08543v1 [cs.LG])</h2>
<h3>Thu Nguyen, Duy Phung, Minh Hoai, Thien Huu Nguyen</h3>
<p>Personality image captioning (PIC) aims to describe an image with a natural
language caption given a personality trait. In this work, we introduce a novel
formulation for PIC based on a communication game between a speaker and a
listener. The speaker attempts to generate natural language captions while the
listener encourages the generated captions to contain discriminative
information about the input images and personality traits. In this way, we
expect that the generated captions can be improved to naturally represent the
images and express the traits. In addition, we propose to adapt the language
model GPT2 to perform caption generation for PIC. This enables the speaker and
listener to benefit from the language encoding capacity of GPT2. Our
experiments show that the proposed model achieves the state-of-the-art
performance for PIC.
</p>
<a href="http://arxiv.org/abs/2011.08543" target="_blank">arXiv:2011.08543</a> [<a href="http://arxiv.org/pdf/2011.08543" target="_blank">pdf</a>]

<h2>Recursive Inference for Variational Autoencoders. (arXiv:2011.08544v1 [cs.LG])</h2>
<h3>Minyoung Kim, Vladimir Pavlovic</h3>
<p>Inference networks of traditional Variational Autoencoders (VAEs) are
typically amortized, resulting in relatively inaccurate posterior approximation
compared to instance-wise variational optimization. Recent semi-amortized
approaches were proposed to address this drawback; however, their iterative
gradient update procedures can be computationally demanding. To address these
issues, in this paper we introduce an accurate amortized inference algorithm.
We propose a novel recursive mixture estimation algorithm for VAEs that
iteratively augments the current mixture with new components so as to maximally
reduce the divergence between the variational and the true posteriors. Using
the functional gradient approach, we devise an intuitive learning criteria for
selecting a new mixture component: the new component has to improve the data
likelihood (lower bound) and, at the same time, be as divergent from the
current mixture distribution as possible, thus increasing representational
diversity. Compared to recently proposed boosted variational inference (BVI),
our method relies on amortized inference in contrast to BVI's non-amortized
single optimization instance. A crucial benefit of our approach is that the
inference at test time requires a single feed-forward pass through the mixture
inference network, making it significantly faster than the semi-amortized
approaches. We show that our approach yields higher test data likelihood than
the state-of-the-art on several benchmark datasets.
</p>
<a href="http://arxiv.org/abs/2011.08544" target="_blank">arXiv:2011.08544</a> [<a href="http://arxiv.org/pdf/2011.08544" target="_blank">pdf</a>]

<h2>Dynamic Hard Pruning of Neural Networks at the Edge of the Internet. (arXiv:2011.08545v1 [cs.LG])</h2>
<h3>Lorenzo Valerio, Franco Maria Nardini, Andrea Passarella, Raffaele Perego</h3>
<p>Neural Networks (NN), although successfully applied to several Artificial
Intelligence tasks, are often unnecessarily over-parametrized. In fog/edge
computing, this might make their training prohibitive on resource-constrained
devices, contrasting with the current trend of decentralising intelligence from
remote data-centres to local constrained devices. Therefore, we investigate the
problem of training effective NN models on constrained devices having a fixed,
potentially small, memory budget. We target techniques that are both
resource-efficient and performance effective while enabling significant network
compression. Our technique, called Dynamic Hard Pruning (DynHP), incrementally
prunes the network during training, identifying neurons that marginally
contribute to the model accuracy. DynHP enables a tunable size reduction of the
final neural network and reduces the NN memory occupancy during training. Freed
memory is reused by a \emph{dynamic batch sizing} approach to counterbalance
the accuracy degradation caused by the hard pruning strategy, improving its
convergence and effectiveness. We assess the performance of DynHP through
reproducible experiments on two public datasets, comparing them against
reference competitors. Results show that DynHP compresses a NN up to $10$ times
without significant performance drops (up to $5\%$ relative error w.r.t.
competitors), reducing up to $80\%$ the training memory occupancy.
</p>
<a href="http://arxiv.org/abs/2011.08545" target="_blank">arXiv:2011.08545</a> [<a href="http://arxiv.org/pdf/2011.08545" target="_blank">pdf</a>]

<h2>Generating universal language adversarial examples by understanding and enhancing the transferability across neural models. (arXiv:2011.08558v1 [cs.LG])</h2>
<h3>Liping Yuan, Xiaoqing Zheng, Yi Zhou, Cho-Jui Hsieh, Kai-wei Chang, Xuanjing Huang</h3>
<p>Deep neural network models are vulnerable to adversarial attacks. In many
cases, malicious inputs intentionally crafted for one model can fool another
model in the black-box attack setting. However, there is a lack of systematic
studies on the transferability of adversarial examples and how to generate
universal adversarial examples. In this paper, we systematically study the
transferability of adversarial attacks for text classification models. In
particular, we conduct extensive experiments to investigate how various
factors, such as network architecture, input format, word embedding, and model
capacity, affect the transferability of adversarial attacks. Based on these
studies, we then propose universal black-box attack algorithms that can induce
adversarial examples to attack almost all existing models. These universal
adversarial examples reflect the defects of the learning process and the bias
in the training dataset. Finally, we generalize these adversarial examples into
universal word replacement rules that can be used for model diagnostics.
</p>
<a href="http://arxiv.org/abs/2011.08558" target="_blank">arXiv:2011.08558</a> [<a href="http://arxiv.org/pdf/2011.08558" target="_blank">pdf</a>]

<h2>A Deep Neural Network for SSVEP-based Brain Computer Interfaces. (arXiv:2011.08562v1 [cs.LG])</h2>
<h3>Osman Berke Guney, Muhtasham Oblokulov, Huseyin Ozkan</h3>
<p>The target identification in brain-computer interface (BCI) speller systems
refers to the multi-channel electroencephalogram (EEG) classification for
predicting the target character that the user intends to spell. The EEG in such
systems is known to include the steady-state visually evoked potentials (SSVEP)
signal, which is the brain response when the user concentrates on the target
while being visually presented a matrix of certain alphanumeric each of which
flickers at a unique frequency. The SSVEP in this setting is characteristically
dominated at varying degrees by the harmonics of the stimulation frequency;
hence, a pattern analysis of the SSVEP can solve for the mentioned multi-class
classification problem. To this end, we propose a novel deep neural network
(DNN) architecture for the target identification in BCI SSVEP spellers. The
proposed DNN is an end-to-end system: it receives the multi-channel SSVEP
signal, proceeds with convolutions across the sub-bands of the harmonics,
channels and time, and classifies at the fully connected layer. Our experiments
are on two publicly available (the benchmark and the BETA) datasets consisting
of in total 105 subjects with 40 characters. We train in two stages. The first
stage obtains a global perspective into the whole SSVEP data by exploiting the
commonalities, and transfers the global model to the second stage that fine
tunes it down to each subject separately by exploiting the individual
statistics. In our extensive comparisons, our DNN is demonstrated to
significantly outperform the state-of-the-art on the both two datasets, by
achieving the information transfer rates (ITR) 265.23 bits/min and 196.59
bits/min, respectively. To the best of our knowledge, our ITRs are the highest
ever reported performance results on these datasets. The code, and the proposed
DNN model are available at https://github.com/osmanberke/Deep-SSVEP-BCI.
</p>
<a href="http://arxiv.org/abs/2011.08562" target="_blank">arXiv:2011.08562</a> [<a href="http://arxiv.org/pdf/2011.08562" target="_blank">pdf</a>]

<h2>Identification of state functions by physically-guided neural networks with physically-meaningful internal layers. (arXiv:2011.08567v1 [cs.LG])</h2>
<h3>Jacobo Ayensa-Jim&#xe9;nez, Mohamed H. Doweidar, Jose Antonio Sanz-Herrera, Manuel Doblar&#xe9;</h3>
<p>Substitution of well-grounded theoretical models by data-driven predictions
is not as simple in engineering and sciences as it is in social and economic
fields. Scientific problems suffer most times from paucity of data, while they
may involve a large number of variables and parameters that interact in complex
and non-stationary ways, obeying certain physical laws. Moreover, a
physically-based model is not only useful for making predictions, but to gain
knowledge by the interpretation of its structure, parameters, and mathematical
properties. The solution to these shortcomings seems to be the seamless
blending of the tremendous predictive power of the data-driven approach with
the scientific consistency and interpretability of physically-based models. We
use here the concept of physically-constrained neural networks (PCNN) to
predict the input-output relation in a physical system, while, at the same time
fulfilling the physical constraints. With this goal, the internal hidden state
variables of the system are associated with a set of internal neuron layers,
whose values are constrained by known physical relations, as well as any
additional knowledge on the system. Furthermore, when having enough data, it is
possible to infer knowledge about the internal structure of the system and, if
parameterized, to predict the state parameters for a particular input-output
relation. We show that this approach, besides getting physically-based
predictions, accelerates the training process, reduces the amount of data
required to get similar accuracy, filters partly the intrinsic noise in the
experimental data and provides improved extrapolation capacity.
</p>
<a href="http://arxiv.org/abs/2011.08567" target="_blank">arXiv:2011.08567</a> [<a href="http://arxiv.org/pdf/2011.08567" target="_blank">pdf</a>]

<h2>Audience Creation for Consumables -- Simple and Scalable Precision Merchandising for a Growing Marketplace. (arXiv:2011.08575v1 [cs.LG])</h2>
<h3>Shreyas S, Harsh Maheshwari, Avijit Saha, Samik Datta, Shashank Jain, Disha Makhija, Anuj Nagpal, Sneha Shukla, Suyash S</h3>
<p>Consumable categories, such as grocery and fast-moving consumer goods, are
quintessential to the growth of e-commerce marketplaces in developing
countries. In this work, we present the design and implementation of a
precision merchandising system, which creates audience sets from over 10
million consumers and is deployed at Flipkart Supermart, one of the largest
online grocery stores in India. We employ temporal point process to model the
latent periodicity and mutual-excitation in the purchase dynamics of
consumables. Further, we develop a likelihood-free estimation procedure that is
robust against data sparsity, censure and noise typical of a growing
marketplace. Lastly, we scale the inference by quantizing the triggering
kernels and exploiting sparse matrix-vector multiplication primitive available
on a commercial distributed linear algebra backend. In operation spanning more
than a year, we have witnessed a consistent increase in click-through rate in
the range of 25-70% for banner-based merchandising in the storefront, and in
the range of 12-26% for push notification-based campaigns.
</p>
<a href="http://arxiv.org/abs/2011.08575" target="_blank">arXiv:2011.08575</a> [<a href="http://arxiv.org/pdf/2011.08575" target="_blank">pdf</a>]

<h2>Multi Receptive Field Network for Semantic Segmentation. (arXiv:2011.08577v1 [cs.CV])</h2>
<h3>Yuan Jianlong, Zelu Deng, Wang Shu, Luo Zhenbo</h3>
<p>Semantic segmentation is one of the key tasks in computer vision, which is to
assign a category label to each pixel in an image. Despite significant progress
achieved recently, most existing methods still suffer from two challenging
issues: 1) the size of objects and stuff in an image can be very diverse,
demanding for incorporating multi-scale features into the fully convolutional
networks (FCNs); 2) the pixels close to or at the boundaries of object/stuff
are hard to classify due to the intrinsic weakness of convolutional networks.
To address the first issue, we propose a new Multi-Receptive Field Module
(MRFM), explicitly taking multi-scale features into account. For the second
issue, we design an edge-aware loss which is effective in distinguishing the
boundaries of object/stuff. With these two designs, our Multi Receptive Field
Network achieves new state-of-the-art results on two widely-used semantic
segmentation benchmark datasets. Specifically, we achieve a mean IoU of 83.0 on
the Cityscapes dataset and 88.4 mean IoU on the Pascal VOC2012 dataset.
</p>
<a href="http://arxiv.org/abs/2011.08577" target="_blank">arXiv:2011.08577</a> [<a href="http://arxiv.org/pdf/2011.08577" target="_blank">pdf</a>]

<h2>Demonstrations of Cooperative Perception: Safety and Robustness in Connected and Automated Vehicle Operations. (arXiv:2011.08581v1 [cs.RO])</h2>
<h3>Mao Shan, Karan Narula, Ricky Wong, Stewart Worrall, Malik Khan, Paul Alexander, Eduardo Nebot</h3>
<p>Cooperative perception, or collective perception (CP) is an emerging and
promising technology for intelligent transportation systems (ITS). It enables
an ITS station (ITS-S) to share its local perception information with others by
means of vehicle-to-X (V2X) communication, thereby achieving improved
efficiency and safety in road transportation. In this paper, we present our
recent progress on the development of a connected and automated vehicle (CAV)
and intelligent roadside unit (IRSU). We present three different experiments to
demonstrate the use of CP service within intelligent infrastructure to improve
awareness of vulnerable road users (VRU) and thus safety for CAVs in various
traffic scenarios. We demonstrate in the experiments that a connected vehicle
(CV) can "see" a pedestrian around the corners. More importantly, we
demonstrate how CAVs can autonomously and safely interact with walking and
running pedestrians, relying only on the CP information from the IRSU through
vehicle-to-infrastructure (V2I) communication. This is one of the first
demonstrations of urban vehicle automation using only CP information. We also
address in the paper the handling of collective perception messages (CPMs)
received from the IRSU, and passing them through a pipeline of CP information
coordinate transformation with uncertainty, multiple road user tracking, and
eventually path planning/decision making within the CAV. The experimental
results were obtained with manually driven CV, fully autonomous CAV, and an
IRSU retrofitted with vision and laser sensors and a road user tracking system.
</p>
<a href="http://arxiv.org/abs/2011.08581" target="_blank">arXiv:2011.08581</a> [<a href="http://arxiv.org/pdf/2011.08581" target="_blank">pdf</a>]

<h2>DS-UI: Dual-Supervised Mixture of Gaussian Mixture Models for Uncertainty Inference. (arXiv:2011.08595v1 [cs.LG])</h2>
<h3>Jiyang Xie, Zhanyu Ma, Jing-Hao Xue, Guoqiang Zhang, Jun Guo</h3>
<p>This paper proposes a dual-supervised uncertainty inference (DS-UI) framework
for improving Bayesian estimation-based uncertainty inference (UI) in deep
neural network (DNN)-based image recognition. In the DS-UI, we combine the
classifier of a DNN, i.e., the last fully-connected (FC) layer, with a mixture
of Gaussian mixture models (MoGMM) to obtain an MoGMM-FC layer. Unlike existing
UI methods for DNNs, which only calculate the means or modes of the DNN
outputs' distributions, the proposed MoGMM-FC layer acts as a probabilistic
interpreter for the features that are inputs of the classifier to directly
calculate the probability density of them for the DS-UI. In addition, we
propose a dual-supervised stochastic gradient-based variational Bayes (DS-SGVB)
algorithm for the MoGMM-FC layer optimization. Unlike conventional SGVB and
optimization algorithms in other UI methods, the DS-SGVB not only models the
samples in the specific class for each Gaussian mixture model (GMM) in the
MoGMM, but also considers the negative samples from other classes for the GMM
to reduce the intra-class distances and enlarge the inter-class margins
simultaneously for enhancing the learning ability of the MoGMM-FC layer in the
DS-UI. Experimental results show the DS-UI outperforms the state-of-the-art UI
methods in misclassification detection. We further evaluate the DS-UI in
open-set out-of-domain/-distribution detection and find statistically
significant improvements. Visualizations of the feature spaces demonstrate the
superiority of the DS-UI.
</p>
<a href="http://arxiv.org/abs/2011.08595" target="_blank">arXiv:2011.08595</a> [<a href="http://arxiv.org/pdf/2011.08595" target="_blank">pdf</a>]

<h2>Learning outside the Black-Box: The pursuit of interpretable models. (arXiv:2011.08596v1 [cs.LG])</h2>
<h3>Jonathan Crabb&#xe9;, Yao Zhang, William Zame, Mihaela van der Schaar</h3>
<p>Machine Learning has proved its ability to produce accurate models but the
deployment of these models outside the machine learning community has been
hindered by the difficulties of interpreting these models. This paper proposes
an algorithm that produces a continuous global interpretation of any given
continuous black-box function. Our algorithm employs a variation of projection
pursuit in which the ridge functions are chosen to be Meijer G-functions,
rather than the usual polynomial splines. Because Meijer G-functions are
differentiable in their parameters, we can tune the parameters of the
representation by gradient descent; as a consequence, our algorithm is
efficient. Using five familiar data sets from the UCI repository and two
familiar machine learning algorithms, we demonstrate that our algorithm
produces global interpretations that are both highly accurate and parsimonious
(involve a small number of terms). Our interpretations permit easy
understanding of the relative importance of features and feature interactions.
Our interpretation algorithm represents a leap forward from the previous state
of the art.
</p>
<a href="http://arxiv.org/abs/2011.08596" target="_blank">arXiv:2011.08596</a> [<a href="http://arxiv.org/pdf/2011.08596" target="_blank">pdf</a>]

<h2>Optimizing Offer Sets in Sub-Linear Time. (arXiv:2011.08606v1 [cs.AI])</h2>
<h3>Vivek F. Farias, Andrew A. Li, Deeksha Sinha</h3>
<p>Personalization and recommendations are now accepted as core competencies in
just about every online setting, ranging from media platforms to e-commerce to
social networks. While the challenge of estimating user preferences has
garnered significant attention, the operational problem of using such
preferences to construct personalized offer sets to users is still a challenge,
particularly in modern settings where a massive number of items and a
millisecond response time requirement mean that even enumerating all of the
items is impossible. Faced with such settings, existing techniques are either
(a) entirely heuristic with no principled justification, or (b) theoretically
sound, but simply too slow to work.

Thus motivated, we propose an algorithm for personalized offer set
optimization that runs in time sub-linear in the number of items while enjoying
a uniform performance guarantee. Our algorithm works for an extremely general
class of problems and models of user choice that includes the mixed multinomial
logit model as a special case. We achieve a sub-linear runtime by leveraging
the dimensionality reduction from learning an accurate latent factor model,
along with existing sub-linear time approximate near neighbor algorithms. Our
algorithm can be entirely data-driven, relying on samples of the user, where a
`sample' refers to the user interaction data typically collected by firms. We
evaluate our approach on a massive content discovery dataset from Outbrain that
includes millions of advertisements. Results show that our implementation
indeed runs fast and with increased performance relative to existing fast
heuristics.
</p>
<a href="http://arxiv.org/abs/2011.08606" target="_blank">arXiv:2011.08606</a> [<a href="http://arxiv.org/pdf/2011.08606" target="_blank">pdf</a>]

<h2>Empowering Things with Intelligence: A Survey of the Progress, Challenges, and Opportunities in Artificial Intelligence of Things. (arXiv:2011.08612v1 [cs.AI])</h2>
<h3>Jing Zhang, Dacheng Tao</h3>
<p>In the Internet of Things (IoT) era, billions of sensors and devices collect
and process data from the environment, transmit them to cloud centers, and
receive feedback via the internet for connectivity and perception. However,
transmitting massive amounts of heterogeneous data, perceiving complex
environments from these data, and then making smart decisions in a timely
manner are difficult. Artificial intelligence (AI), especially deep learning,
is now a proven success in various areas including computer vision, speech
recognition, and natural language processing. AI introduced into the IoT
heralds the era of artificial intelligence of things (AIoT). This paper
presents a comprehensive survey on AIoT to show how AI can empower the IoT to
make it faster, smarter, greener, and safer. Specifically, we briefly present
the AIoT architecture in the context of cloud computing, fog computing, and
edge computing. Then, we present progress in AI research for IoT from four
perspectives: perceiving, learning, reasoning, and behaving. Next, we summarize
some promising applications of AIoT that are likely to profoundly reshape our
world. Finally, we highlight the challenges facing AIoT and some potential
research opportunities.
</p>
<a href="http://arxiv.org/abs/2011.08612" target="_blank">arXiv:2011.08612</a> [<a href="http://arxiv.org/pdf/2011.08612" target="_blank">pdf</a>]

<h2>Mutual Information Based Method for Unsupervised Disentanglement of Video Representation. (arXiv:2011.08614v1 [cs.CV])</h2>
<h3>P Aditya Sreekar, Ujjwal Tiwari, Anoop Namboodiri</h3>
<p>Video Prediction is an interesting and challenging task of predicting future
frames from a given set context frames that belong to a video sequence. Video
prediction models have found prospective applications in Maneuver Planning,
Health care, Autonomous Navigation and Simulation. One of the major challenges
in future frame generation is due to the high dimensional nature of visual
data. In this work, we propose Mutual Information Predictive Auto-Encoder
(MIPAE) framework, that reduces the task of predicting high dimensional video
frames by factorising video representations into content and low dimensional
pose latent variables that are easy to predict. A standard LSTM network is used
to predict these low dimensional pose representations. Content and the
predicted pose representations are decoded to generate future frames. Our
approach leverages the temporal structure of the latent generative factors of a
video and a novel mutual information loss to learn disentangled video
representations. We also propose a metric based on mutual information gap (MIG)
to quantitatively access the effectiveness of disentanglement on DSprites and
MPI3D-real datasets. MIG scores corroborate with the visual superiority of
frames predicted by MIPAE. We also compare our method quantitatively on
evaluation metrics LPIPS, SSIM and PSNR.
</p>
<a href="http://arxiv.org/abs/2011.08614" target="_blank">arXiv:2011.08614</a> [<a href="http://arxiv.org/pdf/2011.08614" target="_blank">pdf</a>]

<h2>Theory-guided Auto-Encoder for Surrogate Construction and Inverse Modeling. (arXiv:2011.08618v1 [cs.LG])</h2>
<h3>Nanzhe Wang, Haibin Chang, Dongxiao Zhang</h3>
<p>A Theory-guided Auto-Encoder (TgAE) framework is proposed for surrogate
construction and is further used for uncertainty quantification and inverse
modeling tasks. The framework is built based on the Auto-Encoder (or
Encoder-Decoder) architecture of convolutional neural network (CNN) via a
theory-guided training process. In order to achieve the theory-guided training,
the governing equations of the studied problems can be discretized and the
finite difference scheme of the equations can be embedded into the training of
CNN. The residual of the discretized governing equations as well as the data
mismatch constitute the loss function of the TgAE. The trained TgAE can be used
to construct a surrogate that approximates the relationship between the model
parameters and responses with limited labeled data. In order to test the
performance of the TgAE, several subsurface flow cases are introduced. The
results show the satisfactory accuracy of the TgAE surrogate and efficiency of
uncertainty quantification tasks can be improved with the TgAE surrogate. The
TgAE also shows good extrapolation ability for cases with different correlation
lengths and variances. Furthermore, the parameter inversion task has been
implemented with the TgAE surrogate and satisfactory results can be obtained.
</p>
<a href="http://arxiv.org/abs/2011.08618" target="_blank">arXiv:2011.08618</a> [<a href="http://arxiv.org/pdf/2011.08618" target="_blank">pdf</a>]

<h2>Can Semantic Labels Assist Self-Supervised Visual Representation Learning?. (arXiv:2011.08621v1 [cs.CV])</h2>
<h3>Longhui Wei, Lingxi Xie, Jianzhong He, Jianlong Chang, Xiaopeng Zhang, Wengang Zhou, Houqiang Li, Qi Tian</h3>
<p>Recently, contrastive learning has largely advanced the progress of
unsupervised visual representation learning. Pre-trained on ImageNet, some
self-supervised algorithms reported higher transfer learning performance
compared to fully-supervised methods, seeming to deliver the message that human
labels hardly contribute to learning transferrable visual features. In this
paper, we defend the usefulness of semantic labels but point out that
fully-supervised and self-supervised methods are pursuing different kinds of
features. To alleviate this issue, we present a new algorithm named Supervised
Contrastive Adjustment in Neighborhood (SCAN) that maximally prevents the
semantic guidance from damaging the appearance feature embedding. In a series
of downstream tasks, SCAN achieves superior performance compared to previous
fully-supervised and self-supervised methods, and sometimes the gain is
significant. More importantly, our study reveals that semantic labels are
useful in assisting self-supervised methods, opening a new direction for the
community.
</p>
<a href="http://arxiv.org/abs/2011.08621" target="_blank">arXiv:2011.08621</a> [<a href="http://arxiv.org/pdf/2011.08621" target="_blank">pdf</a>]

<h2>Beyond Static Features for Temporally Consistent 3D Human Pose and Shape from a Video. (arXiv:2011.08627v1 [cs.CV])</h2>
<h3>Hongsuk Choi, Gyeongsik Moon, Kyoung Mu Lee</h3>
<p>Despite the recent success of single image-based 3D human pose and shape
estimation methods, recovering temporally consistent and smooth 3D human motion
from a video is still challenging. Several video-based methods have been
proposed; however, they fail to resolve the single image-based methods'
temporal inconsistency issue due to a strong dependency on a static feature of
the current frame. In this regard, we present a temporally consistent mesh
recovery system (TCMR). It effectively focuses on the past and future frames'
temporal information without being dominated by the current static feature. Our
TCMR significantly outperforms previous video-based methods in temporal
consistency with better per-frame 3D pose and shape accuracy. We will release
the codes.
</p>
<a href="http://arxiv.org/abs/2011.08627" target="_blank">arXiv:2011.08627</a> [<a href="http://arxiv.org/pdf/2011.08627" target="_blank">pdf</a>]

<h2>Exploring Self-Attention for Visual Odometry. (arXiv:2011.08634v1 [cs.CV])</h2>
<h3>Hamed Damirchi, Rooholla Khorrambakht, Hamid D. Taghirad</h3>
<p>Visual odometry networks commonly use pretrained optical flow networks in
order to derive the ego-motion between consecutive frames. The features
extracted by these networks represent the motion of all the pixels between
frames. However, due to the existence of dynamic objects and texture-less
surfaces in the scene, the motion information for every image region might not
be reliable for inferring odometry due to the ineffectiveness of dynamic
objects in derivation of the incremental changes in position. Recent works in
this area lack attention mechanisms in their structures to facilitate dynamic
reweighing of the feature maps for extracting more refined egomotion
information. In this paper, we explore the effectiveness of self-attention in
visual odometry. We report qualitative and quantitative results against the
SOTA methods. Furthermore, saliency-based studies alongside specially designed
experiments are utilized to investigate the effect of self-attention on VO. Our
experiments show that using self-attention allows for the extraction of better
features while achieving a better odometry performance compared to networks
that lack such structures.
</p>
<a href="http://arxiv.org/abs/2011.08634" target="_blank">arXiv:2011.08634</a> [<a href="http://arxiv.org/pdf/2011.08634" target="_blank">pdf</a>]

<h2>A Review of Generalized Zero-Shot Learning Methods. (arXiv:2011.08641v1 [cs.CV])</h2>
<h3>Farhad Pourpanah, Moloud Abdar, Yuxuan Luo, Xinlei Zhou, Ran Wang, Chee Peng Lim, Xi-Zhao Wang</h3>
<p>Generalized zero-shot learning (GZSL) aims to train a model for classifying
data samples under the condition that some output classes are unknown during
supervised learning. To address this challenging task, GZSL leverages semantic
information of both seen (source) and unseen (target) classes to bridge the gap
between both seen and unseen classes. Since its introduction, many GZSL models
have been formulated. In this review paper, we present a comprehensive review
of GZSL. Firstly, we provide an overview of GZSL including the problems and
challenging issues. Then, we introduce a hierarchical categorization of the
GZSL methods and discuss the representative methods of each category. In
addition, we discuss several research directions for future studies.
</p>
<a href="http://arxiv.org/abs/2011.08641" target="_blank">arXiv:2011.08641</a> [<a href="http://arxiv.org/pdf/2011.08641" target="_blank">pdf</a>]

<h2>Leveraging the Variance of Return Sequences for Exploration Policy. (arXiv:2011.08649v1 [cs.LG])</h2>
<h3>Zerong Xi, Gita Sukthankar</h3>
<p>This paper introduces a method for constructing an upper bound for
exploration policy using either the weighted variance of return sequences or
the weighted temporal difference (TD) error. We demonstrate that the variance
of the return sequence for a specific state-action pair is an important
information source that can be leveraged to guide exploration in reinforcement
learning. The intuition is that fluctuation in the return sequence indicates
greater uncertainty in the near future returns. This divergence occurs because
of the cyclic nature of value-based reinforcement learning; the evolving value
function begets policy improvements which in turn modify the value function.
Although both variance and TD errors capture different aspects of this
uncertainty, our analysis shows that both can be valuable to guide exploration.
We propose a two-stream network architecture to estimate weighted variance/TD
errors within DQN agents for our exploration method and show that it
outperforms the baseline on a wide range of Atari games.
</p>
<a href="http://arxiv.org/abs/2011.08649" target="_blank">arXiv:2011.08649</a> [<a href="http://arxiv.org/pdf/2011.08649" target="_blank">pdf</a>]

<h2>Reducing the Variance of Variational Estimates of Mutual Information by Limiting the Critic's Hypothesis Space to RKHS. (arXiv:2011.08651v1 [cs.LG])</h2>
<h3>P Aditya Sreekar, Ujjwal Tiwari, Anoop Namboodiri</h3>
<p>Mutual information (MI) is an information-theoretic measure of dependency
between two random variables. Several methods to estimate MI, from samples of
two random variables with unknown underlying probability distributions have
been proposed in the literature. Recent methods realize parametric probability
distributions or critic as a neural network to approximate unknown density
ratios. The approximated density ratios are used to estimate different
variational lower bounds of MI. While these methods provide reliable estimation
when the true MI is low, they produce high variance estimates in cases of high
MI. We argue that the high variance characteristic is due to the uncontrolled
complexity of the critic's hypothesis space. In support of this argument, we
use the data-driven Rademacher complexity of the hypothesis space associated
with the critic's architecture to analyse generalization error bound of
variational lower bound estimates of MI. In the proposed work, we show that it
is possible to negate the high variance characteristics of these estimators by
constraining the critic's hypothesis space to Reproducing Hilbert Kernel Space
(RKHS), which corresponds to a kernel learned using Automated Spectral Kernel
Learning (ASKL). By analysing the aforementioned generalization error bounds,
we augment the overall optimisation objective with effective regularisation
term. We empirically demonstrate the efficacy of this regularization in
enforcing proper bias variance tradeoff on four variational lower bounds,
namely NWJ, MINE, JS and SMILE.
</p>
<a href="http://arxiv.org/abs/2011.08651" target="_blank">arXiv:2011.08651</a> [<a href="http://arxiv.org/pdf/2011.08651" target="_blank">pdf</a>]

<h2>3D CNNs with Adaptive Temporal Feature Resolutions. (arXiv:2011.08652v1 [cs.CV])</h2>
<h3>Mohsen Fayyaz, Emad Bahrami Rad, Ali Diba, Mehdi Noroozi, Ehsan Adeli, Luc Van Gool, Juergen Gall</h3>
<p>While state-of-the-art 3D Convolutional Neural Networks (CNN) achieve very
good results on action recognition datasets, they are computationally very
expensive and require many GFLOPs. While the GFLOPs of a 3D CNN can be
decreased by reducing the temporal feature resolution within the network, there
is no setting that is optimal for all input clips. In this work, we, therefore,
introduce a differentiable Similarity Guided Sampling (SGS) module, which can
be plugged into any existing 3D CNN architecture. SGS empowers 3D CNNs by
learning the similarity of temporal features and grouping similar features
together. As a result, the temporal feature resolution is not anymore static
but it varies for each input video clip. By integrating SGS as an additional
layer within current 3D CNNs, we can convert them into much more efficient 3D
CNNs with adaptive temporal feature resolutions (ATFR). Our evaluations show
that the proposed module improves the state-of-the-art by reducing the
computational cost (GFLOPs)by half while preserving or even improving the
accuracy. We evaluate our module by adding it to multiple state-of-the-art 3D
CNNs on various datasets such as Kinetics-600, Kinetics-400, mini-Kinetics,
Something-Something V2, UCF101, and HMDB51
</p>
<a href="http://arxiv.org/abs/2011.08652" target="_blank">arXiv:2011.08652</a> [<a href="http://arxiv.org/pdf/2011.08652" target="_blank">pdf</a>]

<h2>Dynamic Occupancy Grid Mapping with Recurrent Neural Networks. (arXiv:2011.08659v1 [cs.RO])</h2>
<h3>Marcel Schreiber, Vasileios Belagiannis, Claudius Gl&#xe4;ser, Klaus Dietmayer</h3>
<p>Modeling and understanding the environment is an essential task for
autonomous driving. In addition to the detection of objects, in complex traffic
scenarios the motion of other road participants is of special interest.
Therefore, we propose to use a recurrent neural network to predict a dynamic
occupancy grid map, which divides the vehicle surrounding in cells, each
containing the occupancy probability and a velocity estimate. During training,
our network is fed with sequences of measurement grid maps, which encode the
lidar measurements of a single time step. Due to the combination of
convolutional and recurrent layers, our approach is capable to use spatial and
temporal information for the robust detection of static and dynamic
environment. In order to apply our approach with measurements from a moving
ego-vehicle, we propose a method for ego-motion compensation that is applicable
in neural network architectures with recurrent layers working on different
resolutions. In our evaluations, we compare our approach with a
state-of-the-art particle-based algorithm on a large publicly available dataset
to demonstrate the improved accuracy of velocity estimates and the more robust
separation of the environment in static and dynamic area. Additionally, we show
that our proposed method for ego-motion compensation leads to comparable
results in scenarios with stationary and with moving ego-vehicle.
</p>
<a href="http://arxiv.org/abs/2011.08659" target="_blank">arXiv:2011.08659</a> [<a href="http://arxiv.org/pdf/2011.08659" target="_blank">pdf</a>]

<h2>Long-Term Pipeline Failure Prediction Using Nonparametric Survival Analysis. (arXiv:2011.08671v1 [cs.LG])</h2>
<h3>Dilusha Weeraddana, Sudaraka MallawaArachchi, Tharindu Warnakula, Zhidong Li, Yang Wang</h3>
<p>Australian water infrastructure is more than a hundred years old, thus has
begun to show its age through water main failures. Our work concerns
approximately half a million pipelines across major Australian cities that
deliver water to houses and businesses, serving over five million customers.
Failures on these buried assets cause damage to properties and water supply
disruptions. We applied Machine Learning techniques to find a cost-effective
solution to the pipe failure problem in these Australian cities, where on
average 1500 of water main failures occur each year. To achieve this objective,
we construct a detailed picture and understanding of the behaviour of the water
pipe network by developing a Machine Learning model to assess and predict the
failure likelihood of water main breaking using historical failure records,
descriptors of pipes and other environmental factors. Our results indicate that
our system incorporating a nonparametric survival analysis technique called
"Random Survival Forest" outperforms several popular algorithms and expert
heuristics in long-term prediction. In addition, we construct a statistical
inference technique to quantify the uncertainty associated with the long-term
predictions.
</p>
<a href="http://arxiv.org/abs/2011.08671" target="_blank">arXiv:2011.08671</a> [<a href="http://arxiv.org/pdf/2011.08671" target="_blank">pdf</a>]

<h2>Flame Stability Analysis of Flame Spray Pyrolysis by Artificial Intelligence. (arXiv:2011.08673v1 [cs.LG])</h2>
<h3>Jessica Pan, Joseph A. Libera, Noah H. Paulson, Marius Stan</h3>
<p>Flame spray pyrolysis (FSP) is a process used to synthesize nanoparticles
through the combustion of an atomized precursor solution; this process has
applications in catalysts, battery materials, and pigments. Current limitations
revolve around understanding how to consistently achieve a stable flame and the
reliable production of nanoparticles. Machine learning and artificial
intelligence algorithms that detect unstable flame conditions in real time may
be a means of streamlining the synthesis process and improving FSP efficiency.
In this study, the FSP flame stability is first quantified by analyzing the
brightness of the flame's anchor point. This analysis is then used to label
data for both unsupervised and supervised machine learning approaches. The
unsupervised learning approach allows for autonomous labelling and
classification of new data by representing data in a reduced dimensional space
and identifying combinations of features that most effectively cluster it. The
supervised learning approach, on the other hand, requires human labeling of
training and test data, but is able to classify multiple objects of interest
(such as the burner and pilot flames) within the video feed. The accuracy of
each of these techniques is compared against the evaluations of human experts.
Both the unsupervised and supervised approaches can track and classify FSP
flame conditions in real time to alert users of unstable flame conditions. This
research has the potential to autonomously track and manage flame spray
pyrolysis as well as other flame technologies by monitoring and classifying the
flame stability.
</p>
<a href="http://arxiv.org/abs/2011.08673" target="_blank">arXiv:2011.08673</a> [<a href="http://arxiv.org/pdf/2011.08673" target="_blank">pdf</a>]

<h2>On Numerosity of Deep Neural Networks. (arXiv:2011.08674v1 [cs.LG])</h2>
<h3>Xi Zhang, Xiaolin Wu</h3>
<p>Recently, a provocative claim was published that number sense spontaneously
emerges in a deep neural network trained merely for visual object recognition.
This has, if true, far reaching significance to the fields of machine learning
and cognitive science alike. In this paper, we prove the above claim to be
unfortunately incorrect. The statistical analysis to support the claim is
flawed in that the sample set used to identify number-aware neurons is too
small, compared to the huge number of neurons in the object recognition
network. By this flawed analysis one could mistakenly identify number-sensing
neurons in any randomly initialized deep neural networks that are not trained
at all. With the above critique we ask the question what if a deep
convolutional neural network is carefully trained for numerosity? Our findings
are mixed. Even after being trained with number-depicting images, the deep
learning approach still has difficulties to acquire the abstract concept of
numbers, a cognitive task that preschoolers perform with ease. But on the other
hand, we do find some encouraging evidences suggesting that deep neural
networks are more robust to distribution shift for small numbers than for large
numbers.
</p>
<a href="http://arxiv.org/abs/2011.08674" target="_blank">arXiv:2011.08674</a> [<a href="http://arxiv.org/pdf/2011.08674" target="_blank">pdf</a>]

<h2>Non-Local Robust Quaternion Matrix Completion for Large-Scale Color Images and Videos Inpainting. (arXiv:2011.08675v1 [cs.CV])</h2>
<h3>Zhigang Jia, Qiyu Jin, Michael K. Ng, Xile Zhao</h3>
<p>The image nonlocal self-similarity (NSS) prior refers to the fact that a
local patch often has many nonlocal similar patches to it across the image. In
this paper we apply such NSS prior to enhance the robust quaternion matrix
completion (QMC) method and significantly improve the inpainting performance. A
patch group based NSS prior learning scheme is proposed to learn explicit NSS
models from natural color images. The NSS-based QMC algorithm computes an
optimal low-rank approximation to the high-rank color image, resulting in high
PSNR and SSIM measures and particularly the better visual quality. A new joint
NSS-base QMC method is also presented to solve the color video inpainting
problem based quaternion tensor representation. The numerical experiments on
large-scale color images and videos indicate the advantages of NSS-based QMC
over the state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2011.08675" target="_blank">arXiv:2011.08675</a> [<a href="http://arxiv.org/pdf/2011.08675" target="_blank">pdf</a>]

<h2>Topology-Based Feature Design and Tracking for Multi-Center Cyclones. (arXiv:2011.08676v1 [cs.LG])</h2>
<h3>Wito Engelke, Talha Bin Masood, Jakob Beran, Rodrigo Caballero, Ingrid Hotz</h3>
<p>In this paper, we propose a concept to design, track, and compare
application-specific feature definitions expressed as sets of critical points.
Our work has been inspired by the observation that in many applications a large
variety of different feature definitions for the same concept are used. Often,
these definitions compete with each other and it is unclear which definition
should be used in which context. A prominent example is the definition of
cyclones in climate research. Despite the differences, frequently these feature
definitions can be related to topological concepts.

In our approach, we provide a cyclone tracking framework that supports
interactive feature definition and comparison based on a precomputed tracking
graph that stores all extremal points as well as their temporal correspondents.
The framework combines a set of independent building blocks: critical point
extraction, critical point tracking, feature definition, and track exploration.
One of the major advantages of such an approach is the flexibility it provides,
that is, each block is exchangeable. Moreover, it also enables us to perform
the most expensive analysis, the construction of a full tracking graph, as a
prepossessing step, while keeping the feature definition interactive. Different
feature definitions can be explored and compared interactively based on this
tracking graph. Features are specified by rules for grouping critical points,
while feature tracking corresponds to filtering and querying the full tracking
graph by specific requests. We demonstrate this method for cyclone
identification and tracking in the context of climate research.
</p>
<a href="http://arxiv.org/abs/2011.08676" target="_blank">arXiv:2011.08676</a> [<a href="http://arxiv.org/pdf/2011.08676" target="_blank">pdf</a>]

<h2>SeekNet: Improved Human Instance Segmentation via Reinforcement Learning Based Optimized Robot Relocation. (arXiv:2011.08682v1 [cs.CV])</h2>
<h3>Venkatraman Narayanan, Bala Murali Manoghar, Rama Prashanth RV, Aniket Bera</h3>
<p>Amodal recognition is the ability of the system to detect occluded objects.
Most state-of-the-art Visual Recognition systems lack the ability to perform
amodal recognition. Few studies have achieved amodal recognition through
passive prediction or embodied recognition approaches. However, these
approaches suffer from challenges in real-world applications, such as dynamic
objects. We propose SeekNet, an improved optimization method for amodal
recognition through embodied visual recognition. Additionally, we implement
SeekNet for social robots, where there are multiple interactions with crowded
humans. Hence, we focus on occluded human detection &amp; tracking and showcase the
superiority of our algorithm over other baselines. We also experiment with
SeekNet to improve the confidence of COVID-19 symptoms pre-screening algorithms
using our efficient embodied recognition system.
</p>
<a href="http://arxiv.org/abs/2011.08682" target="_blank">arXiv:2011.08682</a> [<a href="http://arxiv.org/pdf/2011.08682" target="_blank">pdf</a>]

<h2>ABC-Net: Semi-Supervised Multimodal GAN-based Engagement Detection using an Affective, Behavioral and Cognitive Model. (arXiv:2011.08690v1 [cs.CV])</h2>
<h3>Pooja Guhan, Manas Agarwal, Naman Awasthi, Gloria Reeves, Dinesh Manocha, Aniket Bera</h3>
<p>We present ABC-Net, a novel semi-supervised multimodal GAN framework to
detect engagement levels in video conversations based on psychology literature.
We use three constructs: behavioral, cognitive, and affective engagement, to
extract various features that can effectively capture engagement levels. We
feed these features to our semi-supervised GAN network that does regression
using these latent representations to obtain the corresponding valence and
arousal values, which are then categorized into different levels of
engagements. We demonstrate the efficiency of our network through experiments
on the RECOLA database. To evaluate our method, we analyze and compare our
performance on RECOLA and report a relative performance improvement of more
than 5% over the baseline methods. To the best of our knowledge, our approach
is the first method to classify engagement based on a multimodal
semi-supervised network.
</p>
<a href="http://arxiv.org/abs/2011.08690" target="_blank">arXiv:2011.08690</a> [<a href="http://arxiv.org/pdf/2011.08690" target="_blank">pdf</a>]

<h2>Pyramid Point: A Multi-Level Focusing Network for Revisiting Feature Layers. (arXiv:2011.08692v1 [cs.CV])</h2>
<h3>Nina Varney, Vijayan K. Asari, Quinn Graehling</h3>
<p>We present a method to learn a diverse group of object categories from an
unordered point set. We propose our Pyramid Point network, which uses a dense
pyramid structure instead of the traditional 'U' shape, typically seen in
semantic segmentation networks. This pyramid structure gives a second look,
allowing the network to revisit different layers simultaneously, increasing the
contextual information by creating additional layers with less noise. We
introduce a Focused Kernel Point convolution (FKP Conv), which expands on the
traditional point convolutions by adding an attention mechanism to the kernel
outputs. This FKP Conv increases our feature quality and allows us to weigh the
kernel outputs dynamically. These FKP Convs are the central part of our
Recurrent FKP Bottleneck block, which makes up the backbone of our encoder.
With this distinct network, we demonstrate competitive performance on three
benchmark data sets. We also perform an ablation study to show the positive
effects of each element in our FKP Conv.
</p>
<a href="http://arxiv.org/abs/2011.08692" target="_blank">arXiv:2011.08692</a> [<a href="http://arxiv.org/pdf/2011.08692" target="_blank">pdf</a>]

<h2>Sim-to-Real Task Planning and Execution from Perception via Reactivity and Recovery. (arXiv:2011.08694v1 [cs.RO])</h2>
<h3>Shohin Mukherjee, Chris Paxton, Arsalan Mousavian, Adam Fishman, Maxim Likhachev, Dieter Fox</h3>
<p>Zero-shot execution of unseen robotic tasks is an important problem in
robotics. One potential approach is through task planning: combining known
skills based on their preconditions and effects to achieve a user-specified
goal. In this work, we propose such a task planning approach to build a
reactive system for multi-step manipulation tasks that can be trained on
simulation data and applied in the real-world. We explore a block-stacking task
because it has a clear structure, where multiple skills must be chained
together: pick up a block, place it on top of another block, etc. We learn
these skills, along with a set of predicate preconditions and termination
conditions, entirely in simulation. All components are learned as PointNet++
models, parameterized by the masks of relevant objects. The predicates allow us
to create high-level plans combining different skills. They also serve as
precondition functions for the skills, which enables the system to recognize
failures and accomplish long-horizon tasks from perceptual input, which is
critical for real-world execution. We evaluate our proposed approach in both
simulation and in the real-world, showing an increase in success rate from
91.6% to 98% in simulation and from 10% to 80% success rate in the real-world
as compared with naive baselines.
</p>
<a href="http://arxiv.org/abs/2011.08694" target="_blank">arXiv:2011.08694</a> [<a href="http://arxiv.org/pdf/2011.08694" target="_blank">pdf</a>]

<h2>Denoising Score-Matching for Uncertainty Quantification in Inverse Problems. (arXiv:2011.08698v1 [stat.ML])</h2>
<h3>Zaccharie Ramzi, Benjamin Remy, Francois Lanusse, Jean-Luc Starck, Philippe Ciuciu</h3>
<p>Deep neural networks have proven extremely efficient at solving a wide
rangeof inverse problems, but most often the uncertainty on the solution they
provideis hard to quantify. In this work, we propose a generic Bayesian
framework forsolving inverse problems, in which we limit the use of deep neural
networks tolearning a prior distribution on the signals to recover. We adopt
recent denoisingscore matching techniques to learn this prior from data, and
subsequently use it aspart of an annealed Hamiltonian Monte-Carlo scheme to
sample the full posteriorof image inverse problems. We apply this framework to
Magnetic ResonanceImage (MRI) reconstruction and illustrate how this approach
not only yields highquality reconstructions but can also be used to assess the
uncertainty on particularfeatures of a reconstructed image.
</p>
<a href="http://arxiv.org/abs/2011.08698" target="_blank">arXiv:2011.08698</a> [<a href="http://arxiv.org/pdf/2011.08698" target="_blank">pdf</a>]

<h2>Redesigning the classification layer by randomizing the class representation vectors. (arXiv:2011.08704v1 [cs.LG])</h2>
<h3>Gabi Shalev, Gal-Lev Shalev, Joseph Keshet</h3>
<p>Neural image classification models typically consist of two components. The
first is an image encoder, which is responsible for encoding a given raw image
into a representative vector. The second is the classification component, which
is often implemented by projecting the representative vector onto target class
vectors. The target class vectors, along with the rest of the model parameters,
are estimated so as to minimize the loss function. In this paper, we analyze
how simple design choices for the classification layer affect the learning
dynamics. We show that the standard cross-entropy training implicitly captures
visual similarities between different classes, which might deteriorate accuracy
or even prevents some models from converging. We propose to draw the class
vectors randomly and set them as fixed during training, thus invalidating the
visual similarities encoded in these vectors. We analyze the effects of keeping
the class vectors fixed and show that it can increase the inter-class
separability, intra-class compactness, and the overall model accuracy, while
maintaining the robustness to image corruptions and the generalization of the
learned concepts.
</p>
<a href="http://arxiv.org/abs/2011.08704" target="_blank">arXiv:2011.08704</a> [<a href="http://arxiv.org/pdf/2011.08704" target="_blank">pdf</a>]

<h2>VIB is Half Bayes. (arXiv:2011.08711v1 [stat.ML])</h2>
<h3>Alexander A Alemi, Warren R Morningstar, Ben Poole, Ian Fischer, Joshua V Dillon</h3>
<p>In discriminative settings such as regression and classification there are
two random variables at play, the inputs X and the targets Y. Here, we
demonstrate that the Variational Information Bottleneck can be viewed as a
compromise between fully empirical and fully Bayesian objectives, attempting to
minimize the risks due to finite sampling of Y only. We argue that this
approach provides some of the benefits of Bayes while requiring only some of
the work.
</p>
<a href="http://arxiv.org/abs/2011.08711" target="_blank">arXiv:2011.08711</a> [<a href="http://arxiv.org/pdf/2011.08711" target="_blank">pdf</a>]

<h2>Uncertainty Modelling in Deep Neural Networks for Image Data. (arXiv:2011.08712v1 [cs.CV])</h2>
<h3>Aria Khoshsirat</h3>
<p>Quantifying uncertainty in a model's predictions is important as it enables,
for example, the safety of an AI system to be increased by acting on the
model's output in an informed manner. We cannot expect a system to be 100%
accurate or perfect at its task, however, we can equip the system with some
tools to inform us if it is not certain about a prediction. This way, a second
check can be performed, or the task can be passed to a human specialist. This
is crucial for applications where the cost of an error is high, such as in
autonomous vehicle control, medical image analysis, financial estimations or
legal fields. Deep Neural Networks are powerful black box predictors that have
recently achieved impressive performance on a wide spectrum of tasks.
Quantifying predictive uncertainty in DNNs is a challenging and yet on-going
problem. Although there have been many efforts to equip NNs with tools to
estimate uncertainty, such as Monte Carlo Dropout, most of the previous methods
only focus on one of the three types of model, data or distributional
uncertainty. In this paper we propose a complete framework to capture and
quantify all of these three types of uncertainties in DNNs for image
classification. This framework includes an ensemble of CNNs for model
uncertainty, a supervised reconstruction auto-encoder to capture distributional
uncertainty and using the output of activation functions in the last layer of
the network, to capture data uncertainty. Finally we demonstrate the efficiency
of our method on popular image datasets for classification.
</p>
<a href="http://arxiv.org/abs/2011.08712" target="_blank">arXiv:2011.08712</a> [<a href="http://arxiv.org/pdf/2011.08712" target="_blank">pdf</a>]

<h2>Semi-supervised Learning of Galaxy Morphology using Equivariant Transformer Variational Autoencoders. (arXiv:2011.08714v1 [stat.ML])</h2>
<h3>Mizu Nishikawa-Toomey, Lewis Smith, Yarin Gal</h3>
<p>The growth in the number of galaxy images is much faster than the speed at
which these galaxies can be labelled by humans. However, by leveraging the
information present in the ever growing set of unlabelled images,
semi-supervised learning could be an effective way of reducing the required
labelling and increasing classification accuracy. We develop a Variational
Autoencoder (VAE) with Equivariant Transformer layers with a classifier network
from the latent space. We show that this novel architecture leads to
improvements in accuracy when used for the galaxy morphology classification
task on the Galaxy Zoo data set. In addition we show that pre-training the
classifier network as part of the VAE using the unlabelled data leads to higher
accuracy with fewer labels compared to exiting approaches. This novel VAE has
the potential to automate galaxy morphology classification with reduced human
labelling efforts.
</p>
<a href="http://arxiv.org/abs/2011.08714" target="_blank">arXiv:2011.08714</a> [<a href="http://arxiv.org/pdf/2011.08714" target="_blank">pdf</a>]

<h2>RAIST: Learning Risk Aware Traffic Interactions via Spatio-Temporal Graph Convolutional Networks. (arXiv:2011.08722v1 [cs.CV])</h2>
<h3>Videsh Suman, Aniket Bera</h3>
<p>A key aspect of driving a road vehicle is to interact with the other road
users, assess their intentions and make risk-aware tactical decisions. An
intuitive approach of enabling an intelligent automated driving system would be
to incorporate some aspects of the human driving behavior. To this end, we
propose a novel driving framework for egocentric views, which is based on
spatio-temporal traffic graphs. The traffic graphs not only model the spatial
interactions amongst the road users, but also their individual intentions
through temporally associated message passing. We leverage spatio-temporal
graph convolutional network (ST-GCN) to train the graph edges. These edges are
formulated using parameterized functions of 3D positions and scene-aware
appearance features of road agents. Along with tactical behavior prediction, it
is crucial to evaluate the risk assessing ability of the proposed framework. We
claim that our framework learns risk aware representations by improving on the
task of risk object identification, especially in identifying objects with
vulnerable interactions like pedestrians and cyclists.
</p>
<a href="http://arxiv.org/abs/2011.08722" target="_blank">arXiv:2011.08722</a> [<a href="http://arxiv.org/pdf/2011.08722" target="_blank">pdf</a>]

<h2>Modality-Buffet for Real-Time Object Detection. (arXiv:2011.08726v1 [cs.LG])</h2>
<h3>Nicolai Dorka, Johannes Meyer, Wolfram Burgard</h3>
<p>Real-time object detection in videos using lightweight hardware is a crucial
component of many robotic tasks. Detectors using different modalities and with
varying computational complexities offer different trade-offs. One option is to
have a very lightweight model that can predict from all modalities at once for
each frame. However, in some situations (e.g., in static scenes) it might be
better to have a more complex but more accurate model and to extrapolate from
previous predictions for the frames coming in at processing time. We formulate
this task as a sequential decision making problem and use reinforcement
learning (RL) to generate a policy that decides from the RGB input which
detector out of a portfolio of different object detectors to take for the next
prediction. The objective of the RL agent is to maximize the accuracy of the
predictions per image. We evaluate the approach on the Waymo Open Dataset and
show that it exceeds the performance of each single detector.
</p>
<a href="http://arxiv.org/abs/2011.08726" target="_blank">arXiv:2011.08726</a> [<a href="http://arxiv.org/pdf/2011.08726" target="_blank">pdf</a>]

<h2>Fault-Aware Robust Control via Adversarial Reinforcement Learning. (arXiv:2011.08728v1 [cs.RO])</h2>
<h3>Fan Yang, Chao Yang, Di Guo, Huaping Liu, Fuchun Sun</h3>
<p>Robots have limited adaptation ability compared to humans and animals in the
case of damage. However, robot damages are prevalent in real-world
applications, especially for robots deployed in extreme environments. The
fragility of robots greatly limits its widespread application. We propose an
adversarial reinforcement learning framework, which significantly increases
robot robustness over joint damage cases in both manipulation tasks and
locomotion tasks. The agent is trained iteratively under the joint damage cases
where it has poor performance. We validate our algorithm on a three-fingered
robot hand and a quadruped robot. Our algorithm can be trained only in
simulation and directly deployed on a real robot without any fine-tuning. It
also demonstrates exceeding success rates over arbitrary joint damage cases.
</p>
<a href="http://arxiv.org/abs/2011.08728" target="_blank">arXiv:2011.08728</a> [<a href="http://arxiv.org/pdf/2011.08728" target="_blank">pdf</a>]

<h2>Control Strategies for Autonomous Vehicles. (arXiv:2011.08729v1 [cs.RO])</h2>
<h3>Chinmay Vilas Samak, Tanmay Vilas Samak, Sivanathan Kandhasamy</h3>
<p>This chapter focuses on the self-driving technology from a control
perspective and investigates the control strategies used in autonomous vehicles
and advanced driver-assistance systems from both theoretical and practical
viewpoints. First, we introduce the self-driving technology as a whole,
including perception, planning and control techniques required for
accomplishing the challenging task of autonomous driving. We then dwell upon
each of these operations to explain their role in the autonomous system
architecture, with a prime focus on control strategies. The core portion of
this chapter commences with detailed mathematical modeling of autonomous
vehicles followed by a comprehensive discussion on control strategies. The
chapter covers longitudinal as well as lateral control strategies for
autonomous vehicles with coupled and de-coupled control schemes. We as well
discuss some of the machine learning techniques applied to autonomous vehicle
control task. Finally, we briefly summarize some of the research works that our
team has carried out at the Autonomous Systems Lab and conclude the chapter
with a few thoughtful remarks.
</p>
<a href="http://arxiv.org/abs/2011.08729" target="_blank">arXiv:2011.08729</a> [<a href="http://arxiv.org/pdf/2011.08729" target="_blank">pdf</a>]

<h2>Using Explainable Scheduling for the Mars 2020 Rover Mission. (arXiv:2011.08733v1 [cs.AI])</h2>
<h3>Jagriti Agrawal, Amruta Yelamanchili, Steve Chien</h3>
<p>Understanding the reasoning behind the behavior of an automated scheduling
system is essential to ensure that it will be trusted and consequently used to
its full capabilities in critical applications. In cases where a scheduler
schedules activities in an invalid location, it is usually easy for the user to
infer the missing constraint by inspecting the schedule with the invalid
activity to determine the missing constraint. If a scheduler fails to schedule
activities because constraints could not be satisfied, determining the cause
can be more challenging. In such cases it is important to understand which
constraints caused the activities to fail to be scheduled and how to alter
constraints to achieve the desired schedule. In this paper, we describe such a
scheduling system for NASA's Mars 2020 Perseverance Rover, as well as
Crosscheck, an explainable scheduling tool that explains the scheduler
behavior. The scheduling system and Crosscheck are the baseline for operational
use to schedule activities for the Mars 2020 rover. As we describe, the
scheduler generates a schedule given a set of activities and their constraints
and Crosscheck: (1) provides a visual representation of the generated schedule;
(2) analyzes and explains why activities failed to schedule given the
constraints provided; and (3) provides guidance on potential constraint
relaxations to enable the activities to schedule in future scheduler runs.
</p>
<a href="http://arxiv.org/abs/2011.08733" target="_blank">arXiv:2011.08733</a> [<a href="http://arxiv.org/pdf/2011.08733" target="_blank">pdf</a>]

<h2>Predicting Rigid Body Dynamics using Dual Quaternion Recurrent Neural Networks with Quaternion Attention. (arXiv:2011.08734v1 [cs.LG])</h2>
<h3>Johannes P&#xf6;ppelbaum, Andreas Schwung</h3>
<p>We propose a novel neural network architecture based on dual quaternions
which allow for a compact representation of informations with a main focus on
describing rigid body movements. To cover the dynamic behavior inherent to
rigid body movements, we propose recurrent architectures in the neural network.
To further model the interactions between individual rigid bodies as well as
external inputs efficiently, we incorporate a novel attention mechanism
employing dual quaternion algebra. The introduced architecture is trainable by
means of gradient based algorithms. We apply our approach to a parcel
prediction problem where a rigid body with an initial position, orientation,
velocity and angular velocity moves through a fixed simulation environment
which exhibits rich interactions between the parcel and the boundaries.
</p>
<a href="http://arxiv.org/abs/2011.08734" target="_blank">arXiv:2011.08734</a> [<a href="http://arxiv.org/pdf/2011.08734" target="_blank">pdf</a>]

<h2>Global Road Damage Detection: State-of-the-art Solutions. (arXiv:2011.08740v1 [cs.CV])</h2>
<h3>Deeksha Arya (1, 2), Hiroya Maeda (2), Sanjay Kumar Ghosh (1), Durga Toshniwal (1), Hiroshi Omata (2), Takehiro Kashiyama (2), Yoshihide Sekimoto (2) ((1) Indian Institute of Technology Roorkee, India, (2) The University of Tokyo, Japan)</h3>
<p>This paper summarizes the Global Road Damage Detection Challenge (GRDDC), a
Big Data Cup organized as a part of the IEEE International Conference on Big
Data'2020. The Big Data Cup challenges involve a released dataset and a
well-defined problem with clear evaluation metrics. The challenges run on a
data competition platform that maintains a leaderboard for the participants. In
the presented case, the data constitute 26336 road images collected from India,
Japan, and the Czech Republic to propose methods for automatically detecting
road damages in these countries. In total, 121 teams from several countries
registered for this competition. The submitted solutions were evaluated using
two datasets test1 and test2, comprising 2,631 and 2,664 images. This paper
encapsulates the top 12 solutions proposed by these teams. The best performing
model utilizes YOLO-based ensemble learning to yield an F1 score of 0.67 on
test1 and 0.66 on test2. The paper concludes with a review of the facets that
worked well for the presented challenge and those that could be improved in
future challenges.
</p>
<a href="http://arxiv.org/abs/2011.08740" target="_blank">arXiv:2011.08740</a> [<a href="http://arxiv.org/pdf/2011.08740" target="_blank">pdf</a>]

<h2>Curiosity Based Reinforcement Learning on Robot Manufacturing Cell. (arXiv:2011.08743v1 [cs.RO])</h2>
<h3>Mohammed Sharafath Abdul Hameed, Md Muzahid Khan, Andreas Schwung</h3>
<p>This paper introduces a novel combination of scheduling control on a flexible
robot manufacturing cell with curiosity based reinforcement learning.
Reinforcement learning has proved to be highly successful in solving tasks like
robotics and scheduling. But this requires hand tuning of rewards in problem
domains like robotics and scheduling even where the solution is not obvious. To
this end, we apply a curiosity based reinforcement learning, using intrinsic
motivation as a form of reward, on a flexible robot manufacturing cell to
alleviate this problem. Further, the learning agents are embedded into the
transportation robots to enable a generalized learning solution that can be
applied to a variety of environments. In the first approach, the curiosity
based reinforcement learning is applied to a simple structured robot
manufacturing cell. And in the second approach, the same algorithm is applied
to a graph structured robot manufacturing cell. Results from the experiments
show that the agents are able to solve both the environments with the ability
to transfer the curiosity module directly from one environment to another. We
conclude that curiosity based learning on scheduling tasks provide a viable
alternative to the reward shaped reinforcement learning traditionally used.
</p>
<a href="http://arxiv.org/abs/2011.08743" target="_blank">arXiv:2011.08743</a> [<a href="http://arxiv.org/pdf/2011.08743" target="_blank">pdf</a>]

<h2>Iterative Semi-parametric Dynamics Model Learning For Autonomous Racing. (arXiv:2011.08750v1 [cs.RO])</h2>
<h3>Ignat Georgiev, Christoforos Chatzikomis, Timo V&#xf6;lkl, Joshua Smith, Michael Mistry</h3>
<p>Accurately modeling robot dynamics is crucial to safe and efficient motion
control. In this paper, we develop and apply an iterative learning
semi-parametric model, with a neural network, to the task of autonomous racing
with a Model Predictive Controller (MPC). We present a novel non-linear
semi-parametric dynamics model where we represent the known dynamics with a
parametric model, and a neural network captures the unknown dynamics. We show
that our model can learn more accurately than a purely parametric model and
generalize better than a purely non-parametric model, making it ideal for
real-world applications where collecting data from the full state space is not
feasible. We present a system where the model is bootstrapped on pre-recorded
data and then updated iteratively at run time. Then we apply our iterative
learning approach to the simulated problem of autonomous racing and show that
it can safely adapt to modified dynamics online and even achieve better
performance than models trained on data from manual driving.
</p>
<a href="http://arxiv.org/abs/2011.08750" target="_blank">arXiv:2011.08750</a> [<a href="http://arxiv.org/pdf/2011.08750" target="_blank">pdf</a>]

<h2>Multi-frame Feature Aggregation for Real-time Instrument Segmentation in Endoscopic Video. (arXiv:2011.08752v1 [cs.CV])</h2>
<h3>Shan Lin, Fangbo Qin, Haonan Peng, Randall A. Bly, Kris S. Moe, Blake Hannaford</h3>
<p>Deep learning-based methods have achieved promising results on surgical
instrument segmentation. However, the high computation cost may limit the
applications of deep models to time-sensitive tasks such as online surgical
video analysis for robotic-assisted surgery. Also, current performance may
still suffer from challenging conditions in surgical images such as various
lighting conditions and the presence of blood. We propose a novel Multi-frame
Feature Aggregation (MFFA) module that leverages information of neighboring
frames for segmentation while reducing the influence of spatial misalignment
between frames. The MFFA module also further aggregates features spatially
based on the spatial self-attention mechanism. Neighboring frames usually have
similar appearances, so we consider feature aggregation over a frame sequence
as an iterative feature aggregation procedure. By distributing the
computational workload of deep feature extraction over each frame in a
sequence, we can use a lightweight encoder to reduce the computation costs.
Moreover, public surgical videos usually are not labeled by frame, so we
develop a method that can randomly synthesize a surgical frame sequence from a
labeled frame to assist network training. We demonstrate that our approach
achieves superior performance to corresponding deeper segmentation models on a
public endoscopic sinus surgery dataset.
</p>
<a href="http://arxiv.org/abs/2011.08752" target="_blank">arXiv:2011.08752</a> [<a href="http://arxiv.org/pdf/2011.08752" target="_blank">pdf</a>]

<h2>Confounding Feature Acquisition for Causal Effect Estimation. (arXiv:2011.08753v1 [stat.ML])</h2>
<h3>Shirly Wang, Seung Eun Yi, Shalmali Joshi, Marzyeh Ghassemi</h3>
<p>Reliable treatment effect estimation from observational data depends on the
availability of all confounding information. While much work has targeted
treatment effect estimation from observational data, there is relatively little
work in the setting of confounding variable missingness, where collecting more
information on confounders is often costly or time-consuming. In this work, we
frame this challenge as a problem of feature acquisition of confounding
features for causal inference. Our goal is to prioritize acquiring values for a
fixed and known subset of missing confounders in samples that lead to efficient
average treatment effect estimation. We propose two acquisition strategies
based on i) covariate balancing (CB), and ii) reducing statistical estimation
error on observed factual outcome error (OE). We compare CB and OE on five
common causal effect estimation methods, and demonstrate improved sample
efficiency of OE over baseline methods under various settings. We also provide
visualizations for further analysis on the difference between our proposed
methods.
</p>
<a href="http://arxiv.org/abs/2011.08753" target="_blank">arXiv:2011.08753</a> [<a href="http://arxiv.org/pdf/2011.08753" target="_blank">pdf</a>]

<h2>Stochastic Client Selection for Federated Learning with Volatile Clients. (arXiv:2011.08756v1 [cs.LG])</h2>
<h3>Tiansheng Huang, Weiwei Lin, Keqin Li, Albert Y. Zomaya</h3>
<p>Federated Learning (FL), arising as a novel secure learning paradigm, has
received notable attention from the public. In each round of synchronous FL
training, only a fraction of available clients are chosen to participate and
the selection of which might have a direct or indirect effect on the training
efficiency, as well as the final model performance. In this paper, we
investigate the client selection problem under a volatile context, in which the
local training of heterogeneous clients is likely to fail due to various kinds
of reasons and in different levels of frequency. Intuitively, too much training
failure might potentially reduce the training efficiency and therefore should
be regulated through proper selection of clients. Being inspired, effective
participation under a deadline-based aggregation mechanism is modeled as the
objective function in our problem model, and the fairness degree, another
critical factor that might influence the training performance, is covered as an
expected constraint. For an efficient settlement for the proposed selection
problem, we propose E3CS, a stochastic client selection scheme on the basis of
an adversarial bandit solution and we further corroborate its effectiveness by
conducting real data-based experiments. According to the experimental results,
under a proper setting, our proposed selection scheme is able to achieve at
least 20 percent and up to 50 percent of acceleration to a fixed model accuracy
while maintaining the same level of final model accuracy, in comparison to the
vanilla selection scheme in FL.
</p>
<a href="http://arxiv.org/abs/2011.08756" target="_blank">arXiv:2011.08756</a> [<a href="http://arxiv.org/pdf/2011.08756" target="_blank">pdf</a>]

<h2>Recognition and standardization of cardiac MRI orientation via multi-tasking learning and deep neural networks. (arXiv:2011.08761v1 [cs.CV])</h2>
<h3>Ke Zhang, Xiahai Zhuang</h3>
<p>In this paper, we study the problem of imaging orientation in cardiac MRI,
and propose a framework to categorize the orientation for recognition and
standardization via deep neural networks. The method uses a new multi-tasking
strategy, where both the tasks of cardiac segmentation and orientation
recognition are simultaneously achieved. For multiple sequences and modalities
of MRI, we propose a transfer learning strategy, which adapts our proposed
model from a single modality to multiple modalities. We embed the orientation
recognition network in a Cardiac MRI Orientation Adjust Tool, i.e.,
CMRadjustNet. We implemented two versions of CMRadjustNet, including a
user-interface (UI) software, and a command-line tool. The former version
supports MRI image visualization, orientation prediction, adjustment, and
storage operations; and the latter version enables the batch operations. The
source code, neural network models and tools have been released and open via
https://zmiclab.github.io/projects.html.
</p>
<a href="http://arxiv.org/abs/2011.08761" target="_blank">arXiv:2011.08761</a> [<a href="http://arxiv.org/pdf/2011.08761" target="_blank">pdf</a>]

<h2>Anatomy Prior Based U-net for Pathology Segmentation with Attention. (arXiv:2011.08769v1 [cs.CV])</h2>
<h3>Yuncheng Zhou, Ke Zhang, Xinzhe Luo, Sihan Wang, Xiahai Zhuang</h3>
<p>Pathological area segmentation in cardiac magnetic resonance (MR) images
plays a vital role in the clinical diagnosis of cardiovascular diseases.
Because of the irregular shape and small area, pathological segmentation has
always been a challenging task. We propose an anatomy prior based framework,
which combines the U-net segmentation network with the attention technique.
Leveraging the fact that the pathology is inclusive, we propose a neighborhood
penalty strategy to gauge the inclusion relationship between the myocardium and
the myocardial infarction and no-reflow areas. This neighborhood penalty
strategy can be applied to any two labels with inclusive relationships (such as
the whole infarction and myocardium, etc.) to form a neighboring loss. The
proposed framework is evaluated on the EMIDEC dataset. Results show that our
framework is effective in pathological area segmentation.
</p>
<a href="http://arxiv.org/abs/2011.08769" target="_blank">arXiv:2011.08769</a> [<a href="http://arxiv.org/pdf/2011.08769" target="_blank">pdf</a>]

<h2>A Method to Generate High Precision Mesh Model and RGB-D Datasetfor 6D Pose Estimation Task. (arXiv:2011.08771v1 [cs.CV])</h2>
<h3>Minglei Lu, Yu Guo, Fei Wang, Zheng Dang</h3>
<p>Recently, 3D version has been improved greatly due to the development of deep
neural networks. A high quality dataset is important to the deep learning
method. Existing datasets for 3D vision has been constructed, such as Bigbird
and YCB. However, the depth sensors used to make these datasets are out of
date, which made the resolution and accuracy of the datasets cannot full fill
the higher standards of demand. Although the equipment and technology got
better, but no one was trying to collect new and better dataset. Here we are
trying to fill that gap. To this end, we propose a new method for object
reconstruction, which takes into account the speed, accuracy and robustness.
Our method could be used to produce large dataset with better and more accurate
annotation. More importantly, our data is more close to the rendering data,
which shrinking the gap between the real data and synthetic data further.
</p>
<a href="http://arxiv.org/abs/2011.08771" target="_blank">arXiv:2011.08771</a> [<a href="http://arxiv.org/pdf/2011.08771" target="_blank">pdf</a>]

<h2>Exploring Energy-Accuracy Tradeoffs in AI Hardware. (arXiv:2011.08779v1 [cs.LG])</h2>
<h3>Cory Merkel</h3>
<p>Artificial intelligence (AI) is playing an increasingly significant role in
our everyday lives. This trend is expected to continue, especially with recent
pushes to move more AI to the edge. However, one of the biggest challenges
associated with AI on edge devices (mobile phones, unmanned vehicles, sensors,
etc.) is their associated size, weight, and power constraints. In this work, we
consider the scenario where an AI system may need to operate at
less-than-maximum accuracy in order to meet application-dependent energy
requirements. We propose a simple function that divides the cost of using an AI
system into the cost of the decision making process and the cost of decision
execution. For simple binary decision problems with convolutional neural
networks, it is shown that minimizing the cost corresponds to using fewer than
the maximum number of resources (e.g. convolutional neural network layers and
filters). Finally, it is shown that the cost associated with energy can be
significantly reduced by leveraging high-confidence predictions made in
lower-level layers of the network.
</p>
<a href="http://arxiv.org/abs/2011.08779" target="_blank">arXiv:2011.08779</a> [<a href="http://arxiv.org/pdf/2011.08779" target="_blank">pdf</a>]

<h2>Towards Meta-Algorithm Selection. (arXiv:2011.08784v1 [cs.LG])</h2>
<h3>Alexander Tornede, Marcel Wever, Eyke H&#xfc;llermeier</h3>
<p>Instance-specific algorithm selection (AS) deals with the automatic selection
of an algorithm from a fixed set of candidates most suitable for a specific
instance of an algorithmic problem class, where "suitability" often refers to
an algorithm's runtime. Over the past years, a plethora of algorithm selectors
have been proposed. As an algorithm selector is again an algorithm solving a
specific problem, the idea of algorithm selection could also be applied to AS
algorithms, leading to a meta-AS approach: Given an instance, the goal is to
select an algorithm selector, which is then used to select the actual algorithm
for solving the problem instance. We elaborate on consequences of applying AS
on a meta-level and identify possible problems. Empirically, we show that
meta-algorithm-selection can indeed prove beneficial in some cases. In general,
however, successful AS approaches have problems with solving the meta-level
problem.
</p>
<a href="http://arxiv.org/abs/2011.08784" target="_blank">arXiv:2011.08784</a> [<a href="http://arxiv.org/pdf/2011.08784" target="_blank">pdf</a>]

<h2>PaDiM: a Patch Distribution Modeling Framework for Anomaly Detection and Localization. (arXiv:2011.08785v1 [cs.CV])</h2>
<h3>Thomas Defard, Aleksandr Setkov, Angelique Loesch, Romaric Audigier</h3>
<p>We present a new framework for Patch Distribution Modeling, PaDiM, to
concurrently detect and localize anomalies in images in a one-class learning
setting. PaDiM makes use of a pretrained convolutional neural network (CNN) for
patch embedding, and of multivariate Gaussian distributions to get a
probabilistic representation of the normal class. It also exploits correlations
between the different semantic levels of CNN to better localize anomalies.
PaDiM outperforms current state-of-the-art approaches for both anomaly
detection and localization on the MVTec AD and STC datasets. To match
real-world visual industrial inspection, we extend the evaluation protocol to
assess performance of anomaly localization algorithms on non-aligned dataset.
The state-of-the-art performance and low complexity of PaDiM make it a good
candidate for many industrial applications.
</p>
<a href="http://arxiv.org/abs/2011.08785" target="_blank">arXiv:2011.08785</a> [<a href="http://arxiv.org/pdf/2011.08785" target="_blank">pdf</a>]

<h2>P1AC: Revisiting Absolute Pose From a Single Affine Correspondence. (arXiv:2011.08790v1 [cs.CV])</h2>
<h3>Jonathan Ventura</h3>
<p>We introduce a novel solution to the problem of estimating the pose of a
calibrated camera given a single observation of an oriented point and an affine
correspondence to a reference image. Affine correspondences have traditionally
been used to improve feature matching over wide baselines; however, little
previous work has considered the use of such correspondences for absolute
camera pose computation. The advantage of our approach (P1AC) is that it
requires only a single correspondence in the minimal case in comparison to the
traditional point-based approach (P3P) which requires at least three points.
Our method removes the limiting assumptions made in previous work and provides
a general solution that is applicable to large-scale image-based localization.
Our evaluation on synthetic data shows that our approach is numerically stable
and more robust to point observation noise than P3P. We also evaluate the
application of our approach for large-scale image-based localization and
demonstrate a practical reduction in the number of iterations and computation
time required to robustly localize an image.
</p>
<a href="http://arxiv.org/abs/2011.08790" target="_blank">arXiv:2011.08790</a> [<a href="http://arxiv.org/pdf/2011.08790" target="_blank">pdf</a>]

<h2>Linear Separation via Optimism. (arXiv:2011.08797v1 [cs.LG])</h2>
<h3>Rafael Hanashiro, Jacob Abernethy</h3>
<p>Binary linear classification has been explored since the very early days of
the machine learning literature. Perhaps the most classical algorithm is the
Perceptron, where a weight vector used to classify examples is maintained, and
additive updates are made as incorrect examples are discovered. The Perceptron
has been thoroughly studied and several versions have been proposed over many
decades. The key theoretical fact about the Perceptron is that, so long as a
perfect linear classifier exists with some margin $\gamma &gt; 0$, the number of
required updates to find such a perfect linear separator is bounded by
$\frac{1}{\gamma^2}$. What has never been fully addressed is: does there exist
an algorithm that can achieve this with fewer updates? In this paper we answer
this in the affirmative: we propose the Optimistic Perceptron algorithm, a
simple procedure that finds a separating hyperplane in no more than
$\frac{1}{\gamma}$ updates. We also show experimentally that this procedure can
significantly outperform Perceptron.
</p>
<a href="http://arxiv.org/abs/2011.08797" target="_blank">arXiv:2011.08797</a> [<a href="http://arxiv.org/pdf/2011.08797" target="_blank">pdf</a>]

<h2>Facial Expressions as a Vulnerability in Face Recognition. (arXiv:2011.08809v1 [cs.CV])</h2>
<h3>Alejandro Pe&#xf1;a, Ignacio Serna, Aythami Morales, Julian Fierrez, Agata Lapedriza</h3>
<p>This work explores facial expression bias as a security vulnerability of face
recognition systems. Face recognition technology has experienced great advances
during the last decades. However, despite the great performance achieved by
state of the art face recognition systems, the algorithms are still sensitive
to a large range of covariates. This work presents a comprehensive analysis of
how facial expression bias impacts the performance of face recognition
technologies. Our study analyzes: i) facial expression biases in the most
popular face recognition databases; and ii) the impact of facial expression in
face recognition performances. Our experimental framework includes four face
detectors, three face recognition models, and four different databases. Our
results demonstrate a huge facial expression bias in the most widely used
databases, as well as a related impact of face expression in the performance of
state-of-the-art algorithms. This work opens the door to new research lines
focused on mitigating the observed vulnerability.
</p>
<a href="http://arxiv.org/abs/2011.08809" target="_blank">arXiv:2011.08809</a> [<a href="http://arxiv.org/pdf/2011.08809" target="_blank">pdf</a>]

<h2>Circus ANYmal: A Quadruped Learning Dexterous Manipulation with Its Limbs. (arXiv:2011.08811v1 [cs.RO])</h2>
<h3>Fan Shi, Timon Homberger, Joonho Lee, Takahiro Miki, Moju Zhao, Farbod Farshidian, Kei Okada, Masayuki Inaba, Marco Hutter</h3>
<p>Quadrupedal robots are skillful at locomotion tasks while lacking
manipulation skills, not to mention dexterous manipulation abilities. Inspired
by the animal behavior and the duality between multi-legged locomotion and
multi-fingered manipulation, we showcase a circus ball challenge on a
quadrupedal robot, ANYmal. We employ a model-free reinforcement learning
approach to train a deep policy that enables the robot to balance and
manipulate a light-weight ball robustly using its limbs without any contact
measurement sensor. The policy is trained in the simulation, in which we
randomize many physical properties with additive noise and inject random
disturbance force during manipulation, and achieves zero-shot deployment on the
real robot without any adjustment. In the hardware experiments, dynamic
performance is achieved with a maximum rotation speed of 15 deg/s, and robust
recovery is showcased under external poking. To our best knowledge, it is the
first work that demonstrates the dexterous dynamic manipulation on a real
quadrupedal robot.
</p>
<a href="http://arxiv.org/abs/2011.08811" target="_blank">arXiv:2011.08811</a> [<a href="http://arxiv.org/pdf/2011.08811" target="_blank">pdf</a>]

<h2>Spatio-Temporal Analysis of Facial Actions using Lifecycle-Aware Capsule Networks. (arXiv:2011.08819v1 [cs.CV])</h2>
<h3>Nikhil Churamani, Sinan Kalkan, Hatice Gunes</h3>
<p>Most state-of-the-art approaches for Facial Action Unit (AU) detection rely
upon evaluating facial expressions from static frames, encoding a snapshot of
heightened facial activity. In real-world interactions, however, facial
expressions are usually more subtle and evolve in a temporal manner requiring
AU detection models to learn spatial as well as temporal information. In this
paper, we focus on both spatial and spatio-temporal features encoding the
temporal evolution of facial AU activation. For this purpose, we propose the
Action Unit Lifecycle-Aware Capsule Network (AULA-Caps) that performs AU
detection using both frame and sequence-level features. While at the
frame-level the capsule layers of AULA-Caps learn spatial feature primitives to
determine AU activations, at the sequence-level, it learns temporal
dependencies between contiguous frames by focusing on relevant spatio-temporal
segments in the sequence. The learnt feature capsules are routed together such
that the model learns to selectively focus more on spatial or spatio-temporal
information depending upon the AU lifecycle. The proposed model is evaluated on
the commonly used BP4D and GFT benchmark datasets obtaining state-of-the-art
results on both the datasets.
</p>
<a href="http://arxiv.org/abs/2011.08819" target="_blank">arXiv:2011.08819</a> [<a href="http://arxiv.org/pdf/2011.08819" target="_blank">pdf</a>]

<h2>REALab: An Embedded Perspective on Tampering. (arXiv:2011.08820v1 [cs.LG])</h2>
<h3>Ramana Kumar, Jonathan Uesato, Richard Ngo, Tom Everitt, Victoria Krakovna, Shane Legg</h3>
<p>This paper describes REALab, a platform for embedded agency research in
reinforcement learning (RL). REALab is designed to model the structure of
tampering problems that may arise in real-world deployments of RL. Standard
Markov Decision Process (MDP) formulations of RL and simulated environments
mirroring the MDP structure assume secure access to feedback (e.g., rewards).
This may be unrealistic in settings where agents are embedded and can corrupt
the processes producing feedback (e.g., human supervisors, or an implemented
reward function). We describe an alternative Corrupt Feedback MDP formulation
and the REALab environment platform, which both avoid the secure feedback
assumption. We hope the design of REALab provides a useful perspective on
tampering problems, and that the platform may serve as a unit test for the
presence of tampering incentives in RL agent designs.
</p>
<a href="http://arxiv.org/abs/2011.08820" target="_blank">arXiv:2011.08820</a> [<a href="http://arxiv.org/pdf/2011.08820" target="_blank">pdf</a>]

<h2>Learning Canonical Transformations. (arXiv:2011.08822v1 [cs.CV])</h2>
<h3>Zachary Dulberg, Jonathan Cohen</h3>
<p>Humans understand a set of canonical geometric transformations (such as
translation and rotation) that support generalization by being untethered to
any specific object. We explore inductive biases that help a neural network
model learn these transformations in pixel space in a way that can generalize
out-of-domain. Specifically, we find that high training set diversity is
sufficient for the extrapolation of translation to unseen shapes and scales,
and that an iterative training scheme achieves significant extrapolation of
rotation in time.
</p>
<a href="http://arxiv.org/abs/2011.08822" target="_blank">arXiv:2011.08822</a> [<a href="http://arxiv.org/pdf/2011.08822" target="_blank">pdf</a>]

<h2>Improving Calibration in Deep Metric Learning With Cross-Example Softmax. (arXiv:2011.08824v1 [cs.LG])</h2>
<h3>Andreas Veit, Kimberly Wilber</h3>
<p>Modern image retrieval systems increasingly rely on the use of deep neural
networks to learn embedding spaces in which distance encodes the relevance
between a given query and image. In this setting, existing approaches tend to
emphasize one of two properties. Triplet-based methods capture top-$k$
relevancy, where all top-$k$ scoring documents are assumed to be relevant to a
given query Pairwise contrastive models capture threshold relevancy, where all
documents scoring higher than some threshold are assumed to be relevant. In
this paper, we propose Cross-Example Softmax which combines the properties of
top-$k$ and threshold relevancy. In each iteration, the proposed loss
encourages all queries to be closer to their matching images than all queries
are to all non-matching images. This leads to a globally more calibrated
similarity metric and makes distance more interpretable as an absolute measure
of relevance. We further introduce Cross-Example Negative Mining, in which each
pair is compared to the hardest negative comparisons across the entire batch.
Empirically, we show in a series of experiments on Conceptual Captions and
Flickr30k, that the proposed method effectively improves global calibration and
also retrieval performance.
</p>
<a href="http://arxiv.org/abs/2011.08824" target="_blank">arXiv:2011.08824</a> [<a href="http://arxiv.org/pdf/2011.08824" target="_blank">pdf</a>]

<h2>Deep Active Surface Models. (arXiv:2011.08826v1 [cs.CV])</h2>
<h3>Udaranga Wickramasinghe, Graham Knott, Pascal Fua</h3>
<p>Active Surface Models have a long history of being useful to model complex 3D
surfaces but only Active Contours have been used in conjunction with deep
networks, and then only to produce the data term as well as meta-parameter maps
controlling them. In this paper, we advocate a much tighter integration. We
introduce layers that implement them that can be integrated seamlessly into
Graph Convolutional Networks to enforce sophisticated smoothness priors at an
acceptable computational cost. We will show that the resulting Deep Active
Surface Models outperform equivalent architectures that use traditional
regularization loss terms to impose smoothness priors for 3D surface
reconstruction from 2D images and for 3D volume segmentation.
</p>
<a href="http://arxiv.org/abs/2011.08826" target="_blank">arXiv:2011.08826</a> [<a href="http://arxiv.org/pdf/2011.08826" target="_blank">pdf</a>]

<h2>Avoiding Tampering Incentives in Deep RL via Decoupled Approval. (arXiv:2011.08827v1 [cs.LG])</h2>
<h3>Jonathan Uesato, Ramana Kumar, Victoria Krakovna, Tom Everitt, Richard Ngo, Shane Legg</h3>
<p>How can we design agents that pursue a given objective when all feedback
mechanisms are influenceable by the agent? Standard RL algorithms assume a
secure reward function, and can thus perform poorly in settings where agents
can tamper with the reward-generating mechanism. We present a principled
solution to the problem of learning from influenceable feedback, which combines
approval with a decoupled feedback collection procedure. For a natural class of
corruption functions, decoupled approval algorithms have aligned incentives
both at convergence and for their local updates. Empirically, they also scale
to complex 3D environments where tampering is possible.
</p>
<a href="http://arxiv.org/abs/2011.08827" target="_blank">arXiv:2011.08827</a> [<a href="http://arxiv.org/pdf/2011.08827" target="_blank">pdf</a>]

<h2>Design Space for Graph Neural Networks. (arXiv:2011.08843v1 [cs.LG])</h2>
<h3>Jiaxuan You, Rex Ying, Jure Leskovec</h3>
<p>The rapid evolution of Graph Neural Networks (GNNs) has led to a growing
number of new architectures as well as novel applications. However, current
research focuses on proposing and evaluating specific architectural designs of
GNNs, as opposed to studying the more general design space of GNNs that
consists of a Cartesian product of different design dimensions, such as the
number of layers or the type of the aggregation function. Additionally, GNN
designs are often specialized to a single task, yet few efforts have been made
to understand how to quickly find the best GNN design for a novel task or a
novel dataset. Here we define and systematically study the architectural design
space for GNNs which consists of 315,000 different designs over 32 different
predictive tasks. Our approach features three key innovations: (1) A general
GNN design space; (2) a GNN task space with a similarity metric, so that for a
given novel task/dataset, we can quickly identify/transfer the best performing
architecture; (3) an efficient and effective design space evaluation method
which allows insights to be distilled from a huge number of model-task
combinations. Our key results include: (1) A comprehensive set of guidelines
for designing well-performing GNNs; (2) while best GNN designs for different
tasks vary significantly, the GNN task space allows for transferring the best
designs across different tasks; (3) models discovered using our design space
achieve state-of-the-art performance. Overall, our work offers a principled and
scalable approach to transition from studying individual GNN designs for
specific tasks, to systematically studying the GNN design space and the task
space. Finally, we release GraphGym, a powerful platform for exploring
different GNN designs and tasks. GraphGym features modularized GNN
implementation, standardized GNN evaluation, and reproducible and scalable
experiment management.
</p>
<a href="http://arxiv.org/abs/2011.08843" target="_blank">arXiv:2011.08843</a> [<a href="http://arxiv.org/pdf/2011.08843" target="_blank">pdf</a>]

<h2>Learning color space adaptation from synthetic to real images of cirrus clouds. (arXiv:1810.10286v2 [cs.CV] UPDATED)</h2>
<h3>Qing Lyu, Minghao Chen, Xiang Chen</h3>
<p>Cloud segmentation plays a crucial role in image analysis for climate
modeling. Manually labeling the training data for cloud segmentation is
time-consuming and error-prone. We explore to train segmentation networks with
synthetic data due to the natural acquisition of pixel-level labels.
Nevertheless, the domain gap between synthetic and real images significantly
degrades the performance of the trained model. We propose a color space
adaptation method to bridge the gap, by training a color-sensitive generator
and discriminator to adapt synthetic data to real images in color space.
Instead of transforming images by general convolutional kernels, we adopt a set
of closed-form operations to make color-space adjustments while preserving the
labels. We also construct a synthetic-to-real cirrus cloud dataset SynCloud and
demonstrate the adaptation efficacy on the semantic segmentation task of cirrus
clouds. With our adapted synthetic data for training the semantic segmentation,
we achieve an improvement of 6:59% when applied to real images, superior to
alternative methods.
</p>
<a href="http://arxiv.org/abs/1810.10286" target="_blank">arXiv:1810.10286</a> [<a href="http://arxiv.org/pdf/1810.10286" target="_blank">pdf</a>]

<h2>Coresets for Data-efficient Training of Machine Learning Models. (arXiv:1906.01827v3 [cs.LG] UPDATED)</h2>
<h3>Baharan Mirzasoleiman, Jeff Bilmes, Jure Leskovec</h3>
<p>Incremental gradient (IG) methods, such as stochastic gradient descent and
its variants are commonly used for large scale optimization in machine
learning. Despite the sustained effort to make IG methods more data-efficient,
it remains an open question how to select a training data subset that can
theoretically and practically perform on par with the full dataset. Here we
develop CRAIG, a method to select a weighted subset (or coreset) of training
data that closely estimates the full gradient by maximizing a submodular
function. We prove that applying IG to this subset is guaranteed to converge to
the (near)optimal solution with the same convergence rate as that of IG for
convex optimization. As a result, CRAIG achieves a speedup that is inversely
proportional to the size of the subset. To our knowledge, this is the first
rigorous method for data-efficient training of general machine learning models.
Our extensive set of experiments show that CRAIG, while achieving practically
the same solution, speeds up various IG methods by up to 6x for logistic
regression and 3x for training deep neural networks.
</p>
<a href="http://arxiv.org/abs/1906.01827" target="_blank">arXiv:1906.01827</a> [<a href="http://arxiv.org/pdf/1906.01827" target="_blank">pdf</a>]

<h2>Situation-Aware Left-Turning Connected and Automated Vehicle Operation at Signalized Intersections. (arXiv:1908.00981v2 [cs.RO] UPDATED)</h2>
<h3>Sakib Mahmud Khan, Mashrur Chowdhury</h3>
<p>One challenging aspect of the Connected and Automated Vehicle (CAV) operation
in mixed traffic is the development of a situation-awareness module for CAVs.
While operating on public roads, CAVs need to assess their surroundings,
especially the intentions of non-CAVs. Generally, CAVs demonstrate a defensive
driving behavior, and CAVs expect other non-autonomous entities on the road
will follow the traffic rules or common driving behavior. However, the presence
of aggressive human drivers in the surrounding environment, who may not follow
traffic rules and behave abruptly, can lead to serious safety consequences. In
this paper, we have addressed the CAV and non-CAV interaction by evaluating a
situation-awareness module for left-turning CAV operations in an urban area.
Existing literature does not consider the intent of the following vehicle for a
CAVs left-turning movement, and existing CAV controllers do not assess the
following non-CAVs intents. Based on our simulation study, the situation-aware
CAV controller module reduces up to 27% of the abrupt braking of the following
non-CAVs for scenarios with different opposing through movement compared to the
base scenario with the autonomous vehicle, without considering the following
vehicles intent. The analysis shows that the average travel time reductions for
the opposite through traffic volumes of 600, 800, and 1000 vehicle/hour/lane
are 58%, 52%, and 62%, respectively, for the aggressive human driver following
the CAV if the following vehicles intent is considered by a CAV in making a
left turn at an intersection.
</p>
<a href="http://arxiv.org/abs/1908.00981" target="_blank">arXiv:1908.00981</a> [<a href="http://arxiv.org/pdf/1908.00981" target="_blank">pdf</a>]

<h2>Universal Adversarial Audio Perturbations. (arXiv:1908.03173v5 [cs.LG] UPDATED)</h2>
<h3>Sajjad Abdoli, Luiz G. Hafemann, Jerome Rony, Ismail Ben Ayed, Patrick Cardinal, Alessandro L. Koerich</h3>
<p>We demonstrate the existence of universal adversarial perturbations, which
can fool a family of audio classification architectures, for both targeted and
untargeted attack scenarios. We propose two methods for finding such
perturbations. The first method is based on an iterative, greedy approach that
is well-known in computer vision: it aggregates small perturbations to the
input so as to push it to the decision boundary. The second method, which is
the main contribution of this work, is a novel penalty formulation, which finds
targeted and untargeted universal adversarial perturbations. Differently from
the greedy approach, the penalty method minimizes an appropriate objective
function on a batch of samples. Therefore, it produces more successful attacks
when the number of training samples is limited. Moreover, we provide a proof
that the proposed penalty method theoretically converges to a solution that
corresponds to universal adversarial perturbations. We also demonstrate that it
is possible to provide successful attacks using the penalty method when only
one sample from the target dataset is available for the attacker. Experimental
results on attacking various 1D CNN architectures have shown attack success
rates higher than 85.0% and 83.1% for targeted and untargeted attacks,
respectively using the proposed penalty method.
</p>
<a href="http://arxiv.org/abs/1908.03173" target="_blank">arXiv:1908.03173</a> [<a href="http://arxiv.org/pdf/1908.03173" target="_blank">pdf</a>]

<h2>Gradient Boosting Survival Tree with Applications in Credit Scoring. (arXiv:1908.03385v4 [cs.LG] UPDATED)</h2>
<h3>Miaojun Bai, Yan Zheng, Yun Shen</h3>
<p>Credit scoring plays a vital role in the field of consumer finance. Survival
analysis provides an advanced solution to the credit-scoring problem by
quantifying the probability of survival time. In order to deal with highly
heterogeneous industrial data collected in Chinese market of consumer finance,
we propose a nonparametric ensemble tree model called gradient boosting
survival tree (GBST) that extends the survival tree models with a gradient
boosting algorithm. The survival tree ensemble is learned by minimizing the
negative log-likelihood in an additive manner. The proposed model optimizes the
survival probability simultaneously for each time period, which can reduce the
overall error significantly. Finally, as a test of the applicability, we apply
the GBST model to quantify the credit risk with large-scale real market
datasets. The results show that the GBST model outperforms the existing
survival models measured by the concordance index (C-index), Kolmogorov-Smirnov
(KS) index, as well as by the area under the receiver operating characteristic
curve (AUC) of each time period.
</p>
<a href="http://arxiv.org/abs/1908.03385" target="_blank">arXiv:1908.03385</a> [<a href="http://arxiv.org/pdf/1908.03385" target="_blank">pdf</a>]

<h2>Enforcing Perceptual Consistency on Generative Adversarial Networks by Using the Normalised Laplacian Pyramid Distance. (arXiv:1908.04347v2 [cs.CV] UPDATED)</h2>
<h3>Alexander Hepburn, Valero Laparra, Ryan McConville, Raul Santos-Rodriguez</h3>
<p>In recent years there has been a growing interest in image generation through
deep learning. While an important part of the evaluation of the generated
images usually involves visual inspection, the inclusion of human perception as
a factor in the training process is often overlooked. In this paper we propose
an alternative perceptual regulariser for image-to-image translation using
conditional generative adversarial networks (cGANs). To do so automatically
(avoiding visual inspection), we use the Normalised Laplacian Pyramid Distance
(NLPD) to measure the perceptual similarity between the generated image and the
original image. The NLPD is based on the principle of normalising the value of
coefficients with respect to a local estimate of mean energy at different
scales and has already been successfully tested in different experiments
involving human perception. We compare this regulariser with the originally
proposed L1 distance and note that when using NLPD the generated images contain
more realistic values for both local and global contrast. We found that using
NLPD as a regulariser improves image segmentation accuracy on generated images
as well as improving two no-reference image quality metrics.
</p>
<a href="http://arxiv.org/abs/1908.04347" target="_blank">arXiv:1908.04347</a> [<a href="http://arxiv.org/pdf/1908.04347" target="_blank">pdf</a>]

<h2>Zero-Shot Action Recognition in Videos: A Survey. (arXiv:1909.06423v2 [cs.CV] UPDATED)</h2>
<h3>Valter Estevam, Helio Pedrini, David Menotti</h3>
<p>Zero-Shot Action Recognition has attracted attention in the last years and
many approaches have been proposed for recognition of objects, events and
actions in images and videos. There is a demand for methods that can classify
instances from classes that are not present in the training of models,
especially in the complex problem of automatic video understanding, since
collecting, annotating and labeling videos are difficult and laborious tasks.
We have identified that there are many methods available in the literature,
however, it is difficult to categorize which techniques can be considered state
of the art. Despite the existence of some surveys about zero-shot action
recognition in still images and experimental protocol, there is no work focused
on videos. Therefore, we present a survey of the methods that comprise
techniques to perform visual feature extraction and semantic feature extraction
as well to learn the mapping between these features considering specifically
zero-shot action recognition in videos. We also provide a complete description
of datasets, experiments and protocols, presenting open issues and directions
for future work, essential for the development of the computer vision research
field.
</p>
<a href="http://arxiv.org/abs/1909.06423" target="_blank">arXiv:1909.06423</a> [<a href="http://arxiv.org/pdf/1909.06423" target="_blank">pdf</a>]

<h2>Recurrent Independent Mechanisms. (arXiv:1909.10893v6 [cs.LG] UPDATED)</h2>
<h3>Anirudh Goyal, Alex Lamb, Jordan Hoffmann, Shagun Sodhani, Sergey Levine, Yoshua Bengio, Bernhard Sch&#xf6;lkopf</h3>
<p>Learning modular structures which reflect the dynamics of the environment can
lead to better generalization and robustness to changes which only affect a few
of the underlying causes. We propose Recurrent Independent Mechanisms (RIMs), a
new recurrent architecture in which multiple groups of recurrent cells operate
with nearly independent transition dynamics, communicate only sparingly through
the bottleneck of attention, and are only updated at time steps where they are
most relevant. We show that this leads to specialization amongst the RIMs,
which in turn allows for dramatically improved generalization on tasks where
some factors of variation differ systematically between training and
evaluation.
</p>
<a href="http://arxiv.org/abs/1909.10893" target="_blank">arXiv:1909.10893</a> [<a href="http://arxiv.org/pdf/1909.10893" target="_blank">pdf</a>]

<h2>Discovering the Compositional Structure of Vector Representations with Role Learning Networks. (arXiv:1910.09113v3 [cs.LG] UPDATED)</h2>
<h3>Paul Soulos, Tom McCoy, Tal Linzen, Paul Smolensky</h3>
<p>How can neural networks perform so well on compositional tasks even though
they lack explicit compositional representations? We use a novel analysis
technique called ROLE to show that recurrent neural networks perform well on
such tasks by converging to solutions which implicitly represent symbolic
structure. This method uncovers a symbolic structure which, when properly
embedded in vector space, closely approximates the encodings of a standard
seq2seq network trained to perform the compositional SCAN task. We verify the
causal importance of the discovered symbolic structure by showing that, when we
systematically manipulate hidden embeddings based on this symbolic structure,
the model's output is changed in the way predicted by our analysis.
</p>
<a href="http://arxiv.org/abs/1910.09113" target="_blank">arXiv:1910.09113</a> [<a href="http://arxiv.org/pdf/1910.09113" target="_blank">pdf</a>]

<h2>PerceptNet: A Human Visual System Inspired Neural Network for Estimating Perceptual Distance. (arXiv:1910.12548v2 [cs.LG] UPDATED)</h2>
<h3>Alexander Hepburn, Valero Laparra, Jes&#xfa;s Malo, Ryan McConville, Raul Santos-Rodriguez</h3>
<p>Traditionally, the vision community has devised algorithms to estimate the
distance between an original image and images that have been subject to
perturbations. Inspiration was usually taken from the human visual perceptual
system and how the system processes different perturbations in order to
replicate to what extent it determines our ability to judge image quality.
While recent works have presented deep neural networks trained to predict human
perceptual quality, very few borrow any intuitions from the human visual
system. To address this, we present PerceptNet, a convolutional neural network
where the architecture has been chosen to reflect the structure and various
stages in the human visual system. We evaluate PerceptNet on various
traditional perception datasets and note strong performance on a number of them
as compared with traditional image quality metrics. We also show that including
a nonlinearity inspired by the human visual system in classical deep neural
networks architectures can increase their ability to judge perceptual
similarity. Compared to similar deep learning methods, the performance is
similar, although our network has a number of parameters that is several orders
of magnitude less.
</p>
<a href="http://arxiv.org/abs/1910.12548" target="_blank">arXiv:1910.12548</a> [<a href="http://arxiv.org/pdf/1910.12548" target="_blank">pdf</a>]

<h2>Self-Supervised 3D Keypoint Learning for Ego-motion Estimation. (arXiv:1912.03426v2 [cs.CV] UPDATED)</h2>
<h3>Jiexiong Tang, Rares Ambrus, Vitor Guizilini, Sudeep Pillai, Hanme Kim, Patric Jensfelt, Adrien Gaidon</h3>
<p>Detecting and matching robust viewpoint-invariant keypoints is critical for
visual SLAM and Structure-from-Motion. State-of-the-art learning-based methods
generate training samples via homography adaptation to create 2D synthetic
views with known keypoint matches from a single image. This approach, however,
does not generalize to non-planar 3D scenes with illumination variations
commonly seen in real-world videos. In this work, we propose self-supervised
learning of depth-aware keypoints directly from unlabeled videos. We jointly
learn keypoint and depth estimation networks by combining appearance and
geometric matching via a differentiable structure-from-motion module based on
Procrustean residual pose correction. We describe how our self-supervised
keypoints can be integrated into state-of-the-art visual odometry frameworks
for robust and accurate ego-motion estimation of autonomous vehicles in
real-world conditions
</p>
<a href="http://arxiv.org/abs/1912.03426" target="_blank">arXiv:1912.03426</a> [<a href="http://arxiv.org/pdf/1912.03426" target="_blank">pdf</a>]

<h2>Understanding Image Captioning Models beyond Visualizing Attention. (arXiv:2001.01037v4 [cs.CV] UPDATED)</h2>
<h3>Jiamei Sun, Sebastian Lapuschkin, Wojciech Samek, Alexander Binder</h3>
<p>This paper interprets the predictions of image captioning models with
attention mechanisms beyond visualizing the attention itself. In this paper, we
develop variants of layer-wise relevance propagation (LRP) and gradient-based
explanation methods, tailored to image captioning models with attention
mechanisms. We compare the interpretability of attention heatmaps
systematically against the explanations computed with explanation methods such
as LRP, Grad-CAM, and Guided Grad-CAM. We show that explanation methods provide
simultaneously pixel-wise image explanation (supporting and opposing pixels of
the input image) and linguistic explanation (supporting and opposing words of
the preceding sequence) for each word in the predicted captions. We demonstrate
with extensive experiments that explanation methods can 1) reveal more related
evidence used by the model to make decisions than attention; 2) correlate to
object locations with high precision; 3) is helpful to `debug' the model such
as analyzing the reasons for hallucinated object words. With the observed
properties of explanations, we further design an LRP-inference fine-tuning
strategy that can alleviate the object hallucination of image captioning
models, meanwhile, maintain the sentence fluency. We conduct experiments with
two widely used attention mechanisms: the adaptive attention mechanism
calculated with the additive attention and the multi-head attention calculated
with the scaled dot product.
</p>
<a href="http://arxiv.org/abs/2001.01037" target="_blank">arXiv:2001.01037</a> [<a href="http://arxiv.org/pdf/2001.01037" target="_blank">pdf</a>]

<h2>Compressing Language Models using Doped Kronecker Products. (arXiv:2001.08896v5 [cs.LG] UPDATED)</h2>
<h3>Urmish Thakker, Paul N. Whatmough, Zhi-Gang Liu, Matthew Mattina, Jesse Beu</h3>
<p>Kronecker Products (KP) have been used to compress IoT RNN Applications by
15-38x compression factors, achieving better results than traditional
compression methods. However when KP is applied to large Natural Language
Processing tasks, it leads to significant accuracy loss (approx 26%). This
paper proposes a way to recover accuracy otherwise lost when applying KP to
large NLP tasks, by allowing additional degrees of freedom in the KP matrix.
More formally, we propose doping, a process of adding an extremely sparse
overlay matrix on top of the pre-defined KP structure. We call this compression
method doped kronecker product compression. To train these models, we present a
new solution to the phenomenon of co-matrix adaption (CMA), which uses a new
regularization scheme called co matrix dropout regularization (CMR). We present
experimental results that demonstrate compression of a large language model
with LSTM layers of size 25 MB by 25x with 1.4% loss in perplexity score. At
25x compression, an equivalent pruned network leads to 7.9% loss in perplexity
score, while HMD and LMF lead to 15% and 27% loss in perplexity score
respectively.
</p>
<a href="http://arxiv.org/abs/2001.08896" target="_blank">arXiv:2001.08896</a> [<a href="http://arxiv.org/pdf/2001.08896" target="_blank">pdf</a>]

<h2>Learning CHARME models with neural networks. (arXiv:2002.03237v2 [stat.ML] UPDATED)</h2>
<h3>Jos&#xe9; G. G&#xf3;mez Garc&#xed;a, Jalal Fadili, Christophe Chesneau</h3>
<p>In this paper, we consider a model called CHARME (Conditional Heteroscedastic
Autoregressive Mixture of Experts), a class of generalized mixture of nonlinear
nonparametric AR-ARCH time series. Under certain Lipschitz-type conditions on
the autoregressive and volatility functions, we prove that this model is
stationary, ergodic and $\tau$-weakly dependent. These conditions are much
weaker than those presented in the literature that treats this model. Moreover,
this result forms the theoretical basis for deriving an asymptotic theory of
the underlying (non)parametric estimation, which we present for this model. As
an application, from the universal approximation property of neural networks
(NN), we develop a learning theory for the NN-based autoregressive functions of
the model, where the strong consistency and asymptotic normality of the
considered estimator of the NN weights and biases are guaranteed under weak
conditions.
</p>
<a href="http://arxiv.org/abs/2002.03237" target="_blank">arXiv:2002.03237</a> [<a href="http://arxiv.org/pdf/2002.03237" target="_blank">pdf</a>]

<h2>Efficient Structure-preserving Support Tensor Train Machine. (arXiv:2002.05079v2 [cs.LG] UPDATED)</h2>
<h3>Kirandeep Kour, Sergey Dolgov, Martin Stoll, Peter Benner</h3>
<p>An increasing amount of collected data are high-dimensional and it is crucial
for efficient learning algorithms to exploit the tensorial structure as much as
possible. The ever present curse of dimensionality for high dimensional data
and the loss of structure when vectorizing the data motivates the use of
tailored low-rank tensor methods. In the presence of small amounts of training
data kernel methods offer an attractive choice as they provide the possibility
for a nonlinear decision boundary. We introduce the Tensor Train Multi-way
Multi-level Kernel (TT-MMK) as a method that combines the simplicity of
Canonical Polyadic (CP) with the robustness of the tensor train (TT)
decomposition. We embed this approach into a Dual Structure-preserving Support
Vector Machine and show that the TT-MMK method is more reliable
computationally, less sensitive to tuning parameters, and gives higher
prediction accuracy in the SVM classification when benchmarked against other
state-of-the-art techniques.
</p>
<a href="http://arxiv.org/abs/2002.05079" target="_blank">arXiv:2002.05079</a> [<a href="http://arxiv.org/pdf/2002.05079" target="_blank">pdf</a>]

<h2>Adaptive Sampling Distributed Stochastic Variance Reduced Gradient for Heterogeneous Distributed Datasets. (arXiv:2002.08528v3 [cs.LG] UPDATED)</h2>
<h3>Ilqar Ramazanli, Han Nguyen, Hai Pham, Sashank J. Reddi, Barnabas Poczos</h3>
<p>We study distributed optimization algorithms for minimizing the average of
\emph{heterogeneous} functions distributed across several machines with a focus
on communication efficiency. In such settings, naively using the classical
stochastic gradient descent (SGD) or its variants (e.g., SVRG) with a uniform
sampling of machines typically yields poor performance. It often leads to the
dependence of convergence rate on maximum Lipschitz constant of gradients
across the devices. In this paper, we propose a novel \emph{adaptive} sampling
of machines specially catered to these settings. Our method relies on an
adaptive estimate of local Lipschitz constants base on the information of past
gradients. We show that the new way improves the dependence of convergence rate
from maximum Lipschitz constant to \emph{average} Lipschitz constant across
machines, thereby, significantly accelerating the convergence. Our experiments
demonstrate that our method indeed speeds up the convergence of the standard
SVRG algorithm in heterogeneous environments.
</p>
<a href="http://arxiv.org/abs/2002.08528" target="_blank">arXiv:2002.08528</a> [<a href="http://arxiv.org/pdf/2002.08528" target="_blank">pdf</a>]

<h2>Private Stochastic Convex Optimization: Efficient Algorithms for Non-smooth Objectives. (arXiv:2002.09609v3 [cs.LG] UPDATED)</h2>
<h3>Raman Arora, Teodor V. Marinov, Enayat Ullah</h3>
<p>In this paper, we revisit the problem of private stochastic convex
optimization. We propose an algorithm based on noisy mirror descent, which
achieves optimal rates both in terms of statistical complexity and number of
queries to a first-order stochastic oracle in the regime when the privacy
parameter is inversely proportional to the number of samples.
</p>
<a href="http://arxiv.org/abs/2002.09609" target="_blank">arXiv:2002.09609</a> [<a href="http://arxiv.org/pdf/2002.09609" target="_blank">pdf</a>]

<h2>NeurIPS 2019 Disentanglement Challenge: Improved Disentanglement through Learned Aggregation of Convolutional Feature Maps. (arXiv:2002.12356v2 [cs.LG] UPDATED)</h2>
<h3>Maximilian Seitzer, Andreas Foltyn, Felix P. Kemeth</h3>
<p>This report to our stage 2 submission to the NeurIPS 2019 disentanglement
challenge presents a simple image preprocessing method for learning
disentangled latent factors. We propose to train a variational autoencoder on
regionally aggregated feature maps obtained from networks pretrained on the
ImageNet database, utilizing the implicit inductive bias contained in those
features for disentanglement. This bias can be further enhanced by explicitly
fine-tuning the feature maps on auxiliary tasks useful for the challenge, such
as angle, position estimation, or color classification. Our approach achieved
the 2nd place in stage 2 of the challenge. Code is available at
https://github.com/mseitzer/neurips2019-disentanglement-challenge.
</p>
<a href="http://arxiv.org/abs/2002.12356" target="_blank">arXiv:2002.12356</a> [<a href="http://arxiv.org/pdf/2002.12356" target="_blank">pdf</a>]

<h2>ROAM: Random Layer Mixup for Semi-Supervised Learning in Medical Imaging. (arXiv:2003.09439v4 [cs.CV] UPDATED)</h2>
<h3>Tariq Bdair, Benedikt Wiestler, Nassir Navab, Shadi Albarqouni</h3>
<p>Medical image segmentation is one of the major challenges addressed by
machine learning methods. Yet, deep learning methods profoundly depend on a
large amount of annotated data, which is time-consuming and costly. Though,
semi-supervised learning methods approach this problem by leveraging an
abundant amount of unlabeled data along with a small amount of labeled data in
the training process. Recently, MixUp regularizer has been successfully
introduced to semi-supervised learning methods showing superior performance.
MixUp augments the model with new data points through linear interpolation of
the data at the input space. We argue that this option is limited. Instead, we
propose ROAM, a RandOm lAyer Mixup, which encourages the network to be less
confident for interpolated data points at randomly selected space. ROAM
generates more data points that have never seen before, and hence it avoids
over-fitting and enhances the generalization ability. We conduct extensive
experiments to validate our method on three publicly available datasets on
whole-brain image segmentation. ROAM achieves state-of-the-art (SOTA) results
in fully supervised (89.5%) and semi-supervised (87.0%) settings with a
relative improvement of up to 2.40% and 16.50%, respectively for the
whole-brain segmentation.
</p>
<a href="http://arxiv.org/abs/2003.09439" target="_blank">arXiv:2003.09439</a> [<a href="http://arxiv.org/pdf/2003.09439" target="_blank">pdf</a>]

<h2>Improved Techniques for Training Single-Image GANs. (arXiv:2003.11512v2 [cs.CV] UPDATED)</h2>
<h3>Tobias Hinz, Matthew Fisher, Oliver Wang, Stefan Wermter</h3>
<p>Recently there has been an interest in the potential of learning generative
models from a single image, as opposed to from a large dataset. This task is of
practical significance, as it means that generative models can be used in
domains where collecting a large dataset is not feasible. However, training a
model capable of generating realistic images from only a single sample is a
difficult problem. In this work, we conduct a number of experiments to
understand the challenges of training these methods and propose some best
practices that we found allowed us to generate improved results over previous
work in this space. One key piece is that unlike prior single image generation
methods, we concurrently train several stages in a sequential multi-stage
manner, allowing us to learn models with fewer stages of increasing image
resolution. Compared to a recent state of the art baseline, our model is up to
six times faster to train, has fewer parameters, and can better capture the
global structure of images.
</p>
<a href="http://arxiv.org/abs/2003.11512" target="_blank">arXiv:2003.11512</a> [<a href="http://arxiv.org/pdf/2003.11512" target="_blank">pdf</a>]

<h2>Temporally Coherent Embeddings for Self-Supervised Video Representation Learning. (arXiv:2004.02753v5 [cs.CV] UPDATED)</h2>
<h3>Joshua Knights, Ben Harwood, Daniel Ward, Anthony Vanderkop, Olivia Mackenzie-Ross, Peyman Moghadam</h3>
<p>This paper presents TCE: Temporally Coherent Embeddings for self-supervised
video representation learning. The proposed method exploits inherent structure
of unlabeled video data to explicitly enforce temporal coherency in the
embedding space, rather than indirectly learning it through ranking or
predictive proxy tasks. In the same way that high-level visual information in
the world changes smoothly, we believe that nearby frames in learned
representations will benefit from demonstrating similar properties. Using this
assumption, we train our TCE model to encode videos such that adjacent frames
exist close to each other and videos are separated from one another. Using TCE
we learn robust representations from large quantities of unlabeled video data.
We thoroughly analyse and evaluate our self-supervised learned TCE models on a
downstream task of video action recognition using multiple challenging
benchmarks (Kinetics400, UCF101, HMDB51). With a simple but effective 2D-CNN
backbone and only RGB stream inputs, TCE pre-trained representations outperform
all previous selfsupervised 2D-CNN and 3D-CNN pre-trained on UCF101. The code
and pre-trained models for this paper can be downloaded at:
https://github.com/csiro-robotics/TCE
</p>
<a href="http://arxiv.org/abs/2004.02753" target="_blank">arXiv:2004.02753</a> [<a href="http://arxiv.org/pdf/2004.02753" target="_blank">pdf</a>]

<h2>Particle-based Energetic Variational Inference. (arXiv:2004.06443v3 [stat.ML] UPDATED)</h2>
<h3>Yiwei Wang, Jiuhai Chen, Chun Liu, Lulu Kang</h3>
<p>We introduce a new variational inference (VI) framework, called energetic
variational inference (EVI). It minimizes the VI object function based on a
prescribed energy-dissipation law. Using the EVI framework, we can derive many
existing Particle-based Variational Inference (ParVI) methods, including the
popular Stein Variational Gradient Descent (SVGD) approach. More importantly,
many new ParVI schemes can be created under this framework. For illustration,
we propose a new particle-based EVI scheme, which performs the particle-based
approximation of the density first and then uses the approximated density in
the variational procedure, or "Approximation-then-Variation" for short. Thanks
to this order of approximation and variation, the new scheme can maintain the
variational structure at the particle level and can significantly decrease the
KL-divergence in each iteration. Numerical experiments show the proposed method
outperforms some existing ParVI methods in terms of fidelity to the target
distribution.
</p>
<a href="http://arxiv.org/abs/2004.06443" target="_blank">arXiv:2004.06443</a> [<a href="http://arxiv.org/pdf/2004.06443" target="_blank">pdf</a>]

<h2>Order preserving hierarchical agglomerative clustering. (arXiv:2004.12488v2 [cs.LG] UPDATED)</h2>
<h3>Daniel Bakkelund</h3>
<p>We present a method for hierarchical clustering of directed acyclic graphs
and other strictly partially ordered data that preserves the data structure. In
particular, if we have $a&lt;b$ in the original data and denote their respective
clusters by $[a]$ and $[b]$, we get $[a]&lt;[b]$ in the produced clustering. The
clustering uses standard linkage functions, such as single- and complete
linkage, and is a generalisation of hierarchical clustering of non-ordered
sets. To achieve this, we define the output from running hierarchical
clustering algorithms on strictly ordered data to be partial dendrograms;
sub-trees of classical dendrograms with several connected components. We then
construct an embedding of partial dendrograms over a set into the family of
ultrametrics over the same set. An optimal hierarchical clustering is now
defined as follows: Given a collection of partial dendrograms, the optimal
clustering is the partial dendrogram corresponding to the ultrametric closest
to the original dissimilarity measure, measured in the $p$-norm. Thus, the
method is a combination of classical hierarchical clustering and ultrametric
fitting.
</p>
<a href="http://arxiv.org/abs/2004.12488" target="_blank">arXiv:2004.12488</a> [<a href="http://arxiv.org/pdf/2004.12488" target="_blank">pdf</a>]

<h2>Navigating the Landscape of Multiplayer Games. (arXiv:2005.01642v3 [cs.AI] UPDATED)</h2>
<h3>Shayegan Omidshafiei, Karl Tuyls, Wojciech M. Czarnecki, Francisco C. Santos, Mark Rowland, Jerome Connor, Daniel Hennes, Paul Muller, Julien Perolat, Bart De Vylder, Audrunas Gruslys, Remi Munos</h3>
<p>Multiplayer games have long been used as testbeds in artificial intelligence
research, aptly referred to as the Drosophila of artificial intelligence.
Traditionally, researchers have focused on using well-known games to build
strong agents. This progress, however, can be better informed by characterizing
games and their topological landscape. Tackling this latter question can
facilitate understanding of agents and help determine what game an agent should
target next as part of its training. Here, we show how network measures applied
to response graphs of large-scale games enable the creation of a landscape of
games, quantifying relationships between games of varying sizes and
characteristics. We illustrate our findings in domains ranging from canonical
games to complex empirical games capturing the performance of trained agents
pitted against one another. Our results culminate in a demonstration leveraging
this information to generate new and interesting games, including mixtures of
empirical games synthesized from real world games.
</p>
<a href="http://arxiv.org/abs/2005.01642" target="_blank">arXiv:2005.01642</a> [<a href="http://arxiv.org/pdf/2005.01642" target="_blank">pdf</a>]

<h2>Dense-Resolution Network for Point Cloud Classification and Segmentation. (arXiv:2005.06734v2 [cs.CV] UPDATED)</h2>
<h3>Shi Qiu, Saeed Anwar, Nick Barnes</h3>
<p>Point cloud analysis is attracting attention from Artificial Intelligence
research since it can be widely used in applications such as robotics,
Augmented Reality, self-driving. However, it is always challenging due to
irregularities, unorderedness, and sparsity. In this article, we propose a
novel network named Dense-Resolution Network (DRNet) for point cloud analysis.
Our DRNet is designed to learn local point features from the point cloud in
different resolutions. In order to learn local point groups more effectively,
we present a novel grouping method for local neighborhood searching and an
error-minimizing module for capturing local features. In addition to validating
the network on widely used point cloud segmentation and classification
benchmarks, we also test and visualize the performance of the components.
Comparing with other state-of-the-art methods, our network shows superiority on
ModelNet40, ShapeNet synthetic and ScanObjectNN real point cloud datasets.
</p>
<a href="http://arxiv.org/abs/2005.06734" target="_blank">arXiv:2005.06734</a> [<a href="http://arxiv.org/pdf/2005.06734" target="_blank">pdf</a>]

<h2>Spatiotemporal Attacks for Embodied Agents. (arXiv:2005.09161v3 [cs.CV] UPDATED)</h2>
<h3>Aishan Liu, Tairan Huang, Xianglong Liu, Yitao Xu, Yuqing Ma, Xinyun Chen, Stephen J. Maybank, Dacheng Tao</h3>
<p>Adversarial attacks are valuable for providing insights into the blind-spots
of deep learning models and help improve their robustness. Existing work on
adversarial attacks have mainly focused on static scenes; however, it remains
unclear whether such attacks are effective against embodied agents, which could
navigate and interact with a dynamic environment. In this work, we take the
first step to study adversarial attacks for embodied agents. In particular, we
generate spatiotemporal perturbations to form 3D adversarial examples, which
exploit the interaction history in both the temporal and spatial dimensions.
Regarding the temporal dimension, since agents make predictions based on
historical observations, we develop a trajectory attention module to explore
scene view contributions, which further help localize 3D objects appeared with
the highest stimuli. By conciliating with clues from the temporal dimension,
along the spatial dimension, we adversarially perturb the physical properties
(e.g., texture and 3D shape) of the contextual objects that appeared in the
most important scene views. Extensive experiments on the EQA-v1 dataset for
several embodied tasks in both the white-box and black-box settings have been
conducted, which demonstrate that our perturbations have strong attack and
generalization abilities.
</p>
<a href="http://arxiv.org/abs/2005.09161" target="_blank">arXiv:2005.09161</a> [<a href="http://arxiv.org/pdf/2005.09161" target="_blank">pdf</a>]

<h2>DJEnsemble: On the Selection of a Disjoint Ensemble of Deep Learning Black-Box Spatio-Temporal Models. (arXiv:2005.11093v3 [cs.AI] UPDATED)</h2>
<h3>Yania Molina Souto, Rafael Pereira, Roc&#xed;o Zorrilla, Anderson Chaves, Brian Tsan, Florin Rusu, Eduardo Ogasawara, Artur Ziviani, Fabio Porto</h3>
<p>In this paper, we present a cost-based approach for the automatic selection
and allocation of a disjoint ensemble of black-box predictors to answer
predictive spatio-temporal queries. Our approach is divided into two parts --
offline and online. During the offline part, we preprocess the predictive
domain data -- transforming it into a regular grid -- and the black-box models
-- computing their spatio-temporal learning function. In the online part, we
compute a DJEnsemble plan which minimizes a multivariate cost function based on
estimates for the prediction error and the execution cost -- producing a model
spatial allocation matrix -- and run the optimal ensemble plan. We conduct a
set of extensive experiments that evaluate the DJEnsemble approach and
highlight its efficiency. We show that our cost model produces plans with
performance close to the actual best plan. When compared against the
traditional ensemble approach, DJEnsemble achieves up to $4X$ improvement in
execution time and almost $9X$ improvement in prediction accuracy. To the best
of our knowledge, this is the first work to solve the problem of optimizing the
allocation of black-box models to answer predictive spatio-temporal queries.
</p>
<a href="http://arxiv.org/abs/2005.11093" target="_blank">arXiv:2005.11093</a> [<a href="http://arxiv.org/pdf/2005.11093" target="_blank">pdf</a>]

<h2>The Neural Tangent Link Between CNN Denoisers and Non-Local Filters. (arXiv:2006.02379v4 [cs.CV] UPDATED)</h2>
<h3>Juli&#xe1;n Tachella, Junqi Tang, Mike Davies</h3>
<p>Convolutional Neural Networks (CNNs) are now a well-established tool for
solving computational imaging problems. Modern CNN-based algorithms obtain
state-of-the-art performance in diverse image restoration problems.
Furthermore, it has been recently shown that, despite being highly
overparameterized, networks trained with a single corrupted image can still
perform as well as fully trained networks. We introduce a formal link between
such networks through their neural tangent kernel (NTK), and well-known
non-local filtering techniques, such as non-local means or BM3D. The filtering
function associated with a given network architecture can be obtained in closed
form without need to train the network, being fully characterized by the random
initialization of the network weights. While the NTK theory accurately predicts
the filter associated with networks trained using standard gradient descent,
our analysis shows that it falls short to explain the behaviour of networks
trained using the popular Adam optimizer. The latter achieves a larger change
of weights in hidden layers, adapting the non-local filtering function during
training. We evaluate our findings via extensive image denoising experiments.
</p>
<a href="http://arxiv.org/abs/2006.02379" target="_blank">arXiv:2006.02379</a> [<a href="http://arxiv.org/pdf/2006.02379" target="_blank">pdf</a>]

<h2>Dilated Convolutions with Lateral Inhibitions for Semantic Image Segmentation. (arXiv:2006.03708v3 [cs.CV] UPDATED)</h2>
<h3>Yujiang Wang, Mingzhi Dong, Jie Shen, Yiming Lin, Maja Pantic</h3>
<p>Dilated convolutions are widely used in deep semantic segmentation models as
they can enlarge the filters' receptive field without adding additional weights
nor sacrificing spatial resolution. However, as dilated convolutional filters
do not possess positional knowledge about the pixels on semantically meaningful
contours, they could lead to ambiguous predictions on object boundaries. In
addition, although dilating the filter can expand its receptive field, the
total number of sampled pixels remains unchanged, which usually comprises a
small fraction of the receptive field's total area. Inspired by the Lateral
Inhibition (LI) mechanisms in human visual systems, we propose the dilated
convolution with lateral inhibitions (LI-Convs) to overcome these limitations.
Introducing LI mechanisms improves the convolutional filter's sensitivity to
semantic object boundaries. Moreover, since LI-Convs also implicitly take the
pixels from the laterally inhibited zones into consideration, they can also
extract features at a denser scale. By integrating LI-Convs into the Deeplabv3+
architecture, we propose the Lateral Inhibited Atrous Spatial Pyramid Pooling
(LI-ASPP), the Lateral Inhibited MobileNet-V2 (LI-MNV2) and the Lateral
Inhibited ResNet (LI-ResNet). Experimental results on three benchmark datasets
(PASCAL VOC 2012, CelebAMask-HQ and ADE20K) show that our LI-based segmentation
models outperform the baseline on all of them, thus verify the effectiveness
and generality of the proposed LI-Convs.
</p>
<a href="http://arxiv.org/abs/2006.03708" target="_blank">arXiv:2006.03708</a> [<a href="http://arxiv.org/pdf/2006.03708" target="_blank">pdf</a>]

<h2>Generalisation Guarantees for Continual Learning with Orthogonal Gradient Descent. (arXiv:2006.11942v3 [stat.ML] UPDATED)</h2>
<h3>Mehdi Abbana Bennani, Thang Doan, Masashi Sugiyama</h3>
<p>In Continual Learning settings, deep neural networks are prone to
Catastrophic Forgetting. Orthogonal Gradient Descent was proposed to tackle the
challenge. However, no theoretical guarantees have been proven yet. We present
a theoretical framework to study Continual Learning algorithms in the Neural
Tangent Kernel regime. This framework comprises closed form expression of the
model through tasks and proxies for Transfer Learning, generalisation and tasks
similarity. In this framework, we prove that OGD is robust to Catastrophic
Forgetting then derive the first generalisation bound for SGD and OGD for
Continual Learning. Finally, we study the limits of this framework in practice
for OGD and highlight the importance of the Neural Tangent Kernel variation for
Continual Learning with OGD.
</p>
<a href="http://arxiv.org/abs/2006.11942" target="_blank">arXiv:2006.11942</a> [<a href="http://arxiv.org/pdf/2006.11942" target="_blank">pdf</a>]

<h2>Approaches For Multi-View Redescription Mining. (arXiv:2006.12227v2 [cs.LG] UPDATED)</h2>
<h3>Matej Mihel&#x10d;i&#x107;, Tomislav &#x160;muc</h3>
<p>The task of redescription mining explores ways to re-describe different
subsets of entities contained in a dataset and to reveal non-trivial
associations between different subsets of attributes, called views. This
interesting and challenging task is encountered in different scientific fields,
and is addressed by a number of approaches that obtain redescriptions and allow
for the exploration and analyses of attribute associations. The main limitation
of existing approaches to this task is their inability to use more than two
views. Our work alleviates this drawback. We present a memory efficient,
extensible multi-view redescription mining framework that can be used to relate
multiple, i.e. more than two views, disjoint sets of attributes describing one
set of entities. The framework can use any multi-target regression or
multi-label classification algorithm, with models that can be represented as
sets of rules, to generate redescriptions. Multi-view redescriptions are built
using incremental view-extending heuristic from initially created two-view
redescriptions. In this work, we use different types of Predictive Clustering
trees algorithms (regular, extra, with random output selection) and the Random
Forest thereof in order to improve the quality of final redescription sets
and/or execution time needed to generate them. We provide multiple performance
analyses of the proposed framework and compare it against the naive approach to
multi-view redescription mining. We demonstrate the usefulness of the proposed
multi-view extension on several datasets, including a use-case on understanding
of machine learning models - a topic of growing importance in machine learning
and artificial intelligence in general.
</p>
<a href="http://arxiv.org/abs/2006.12227" target="_blank">arXiv:2006.12227</a> [<a href="http://arxiv.org/pdf/2006.12227" target="_blank">pdf</a>]

<h2>One Thousand and One Hours: Self-driving Motion Prediction Dataset. (arXiv:2006.14480v2 [cs.CV] UPDATED)</h2>
<h3>John Houston, Guido Zuidhof, Luca Bergamini, Yawei Ye, Long Chen, Ashesh Jain, Sammy Omari, Vladimir Iglovikov, Peter Ondruska</h3>
<p>Motivated by the impact of large-scale datasets on ML systems we present the
largest self-driving dataset for motion prediction to date, containing over
1,000 hours of data. This was collected by a fleet of 20 autonomous vehicles
along a fixed route in Palo Alto, California, over a four-month period. It
consists of 170,000 scenes, where each scene is 25 seconds long and captures
the perception output of the self-driving system, which encodes the precise
positions and motions of nearby vehicles, cyclists, and pedestrians over time.
On top of this, the dataset contains a high-definition semantic map with 15,242
labelled elements and a high-definition aerial view over the area. We show that
using a dataset of this size dramatically improves performance for key
self-driving problems. Combined with the provided software kit, this collection
forms the largest and most detailed dataset to date for the development of
self-driving machine learning tasks, such as motion forecasting, motion
planning and simulation. The full dataset is available at
this http URL
</p>
<a href="http://arxiv.org/abs/2006.14480" target="_blank">arXiv:2006.14480</a> [<a href="http://arxiv.org/pdf/2006.14480" target="_blank">pdf</a>]

<h2>Training highly effective connectivities within neural networks with randomly initialized, fixed weights. (arXiv:2006.16627v2 [cs.LG] UPDATED)</h2>
<h3>Cristian Ivan, Razvan Florian</h3>
<p>We present some novel, straightforward methods for training the connection
graph of a randomly initialized neural network without training the weights.
These methods do not use hyperparameters defining cutoff thresholds and
therefore remove the need for iteratively searching optimal values of such
hyperparameters. We can achieve similar or higher performances than in the case
of training all weights, with a similar computational cost as for standard
training techniques. Besides switching connections on and off, we introduce a
novel way of training a network by flipping the signs of the weights. If we try
to minimize the number of changed connections, by changing less than 10% of the
total it is already possible to reach more than 90% of the accuracy achieved by
standard training. We obtain good results even with weights of constant
magnitude or even when weights are drawn from highly asymmetric distributions.
These results shed light on the over-parameterization of neural networks and on
how they may be reduced to their effective size.
</p>
<a href="http://arxiv.org/abs/2006.16627" target="_blank">arXiv:2006.16627</a> [<a href="http://arxiv.org/pdf/2006.16627" target="_blank">pdf</a>]

<h2>Robust Linear Regression: Optimal Rates in Polynomial Time. (arXiv:2007.01394v3 [stat.ML] UPDATED)</h2>
<h3>Ainesh Bakshi, Adarsh Prasad</h3>
<p>We obtain robust and computationally efficient estimators for learning
several linear models that achieve statistically optimal convergence rate under
minimal distributional assumptions. Concretely, we assume our data is drawn
from a $k$-hypercontractive distribution and an $\epsilon$-fraction is
adversarially corrupted. We then describe an estimator that converges to the
optimal least-squares minimizer for the true distribution at a rate
proportional to $\epsilon^{2-2/k}$, when the noise is independent of the
covariates. We note that no such estimator was known prior to our work, even
with access to unbounded computation. The rate we achieve is
information-theoretically optimal and thus we resolve the main open question in
Klivans, Kothari and Meka [COLT'18].

Our key insight is to identify an analytic condition that serves as a
polynomial relaxation of independence of random variables. In particular, we
show that when the moments of the noise and covariates are
negatively-correlated, we obtain the same rate as independent noise. Further,
when the condition is not satisfied, we obtain a rate proportional to
$\epsilon^{2-4/k}$, and again match the information-theoretic lower bound. Our
central technical contribution is to algorithmically exploit independence of
random variables in the "sum-of-squares" framework by formulating it as the
aforementioned polynomial inequality.
</p>
<a href="http://arxiv.org/abs/2007.01394" target="_blank">arXiv:2007.01394</a> [<a href="http://arxiv.org/pdf/2007.01394" target="_blank">pdf</a>]

<h2>Harnessing Wireless Channels for Scalable and Privacy-Preserving Federated Learning. (arXiv:2007.01790v2 [cs.LG] UPDATED)</h2>
<h3>Anis Elgabli, Jihong Park, Chaouki Ben Issaid, Mehdi Bennis</h3>
<p>Wireless connectivity is instrumental in enabling scalable federated learning
(FL), yet wireless channels bring challenges for model training, in which
channel randomness perturbs each worker's model update while multiple workers'
updates incur significant interference under limited bandwidth. To address
these challenges, in this work we formulate a novel constrained optimization
problem, and propose an FL framework harnessing wireless channel perturbations
and interference for improving privacy, bandwidth-efficiency, and scalability.
The resultant algorithm is coined analog federated ADMM (A-FADMM) based on
analog transmissions and the alternating direction method of multipliers
(ADMM). In A-FADMM, all workers upload their model updates to the parameter
server (PS) using a single channel via analog transmissions, during which all
models are perturbed and aggregated over-the-air. This not only saves
communication bandwidth, but also hides each worker's exact model update
trajectory from any eavesdropper including the honest-but-curious PS, thereby
preserving data privacy against model inversion attacks. We formally prove the
convergence and privacy guarantees of A-FADMM for convex functions under
time-varying channels, and numerically show the effectiveness of A-FADMM under
noisy channels and stochastic non-convex functions, in terms of convergence
speed and scalability, as well as communication bandwidth and energy
efficiency.
</p>
<a href="http://arxiv.org/abs/2007.01790" target="_blank">arXiv:2007.01790</a> [<a href="http://arxiv.org/pdf/2007.01790" target="_blank">pdf</a>]

<h2>Interactive Imitation Learning in State-Space. (arXiv:2008.00524v2 [cs.RO] UPDATED)</h2>
<h3>Snehal Jauhri, Carlos Celemin, Jens Kober</h3>
<p>Imitation Learning techniques enable programming the behavior of agents
through demonstrations rather than manual engineering. However, they are
limited by the quality of available demonstration data. Interactive Imitation
Learning techniques can improve the efficacy of learning since they involve
teachers providing feedback while the agent executes its task. In this work, we
propose a novel Interactive Learning technique that uses human feedback in
state-space to train and improve agent behavior (as opposed to alternative
methods that use feedback in action-space). Our method titled Teaching
Imitative Policies in State-space~(TIPS) enables providing guidance to the
agent in terms of `changing its state' which is often more intuitive for a
human demonstrator. Through continuous improvement via corrective feedback,
agents trained by non-expert demonstrators using TIPS outperformed the
demonstrator and conventional Imitation Learning agents.
</p>
<a href="http://arxiv.org/abs/2008.00524" target="_blank">arXiv:2008.00524</a> [<a href="http://arxiv.org/pdf/2008.00524" target="_blank">pdf</a>]

<h2>An Imitation from Observation Approach to Transfer Learning with Dynamics Mismatch. (arXiv:2008.01594v3 [cs.AI] UPDATED)</h2>
<h3>Siddharth Desai, Ishan Durugkar, Haresh Karnan, Garrett Warnell, Josiah Hanna, Peter Stone</h3>
<p>We examine the problem of transferring a policy learned in a source
environment to a target environment with different dynamics, particularly in
the case where it is critical to reduce the amount of interaction with the
target environment during learning. This problem is particularly important in
sim-to-real transfer because simulators inevitably model real-world dynamics
imperfectly. In this paper, we show that one existing solution to this transfer
problem - grounded action transformation - is closely related to the problem of
imitation from observation (IfO): learning behaviors that mimic the
observations of behavior demonstrations. After establishing this relationship,
we hypothesize that recent state-of-the-art approaches from the IfO literature
can be effectively repurposed for grounded transfer learning.To validate our
hypothesis we derive a new algorithm - generative adversarial reinforced action
transformation (GARAT) - based on adversarial imitation from observation
techniques. We run experiments in several domains with mismatched dynamics, and
find that agents trained with GARAT achieve higher returns in the target
environment compared to existing black-box transfer methods
</p>
<a href="http://arxiv.org/abs/2008.01594" target="_blank">arXiv:2008.01594</a> [<a href="http://arxiv.org/pdf/2008.01594" target="_blank">pdf</a>]

<h2>Abstracting Deep Neural Networks into Concept Graphs for Concept Level Interpretability. (arXiv:2008.06457v2 [cs.CV] UPDATED)</h2>
<h3>Avinash Kori, Parth Natekar, Ganapathy Krishnamurthi, Balaji Srinivasan</h3>
<p>The black-box nature of deep learning models prevents them from being
completely trusted in domains like biomedicine. Most explainability techniques
do not capture the concept-based reasoning that human beings follow. In this
work, we attempt to understand the behavior of trained models that perform
image processing tasks in the medical domain by building a graphical
representation of the concepts they learn. Extracting such a graphical
representation of the model's behavior on an abstract, higher conceptual level
would unravel the learnings of these models and would help us to evaluate the
steps taken by the model for predictions. We show the application of our
proposed implementation on two biomedical problems - brain tumor segmentation
and fundus image classification. We provide an alternative graphical
representation of the model by formulating a concept level graph as discussed
above, which makes the problem of intervention to find active inference trails
more tractable. Understanding these trails would provide an understanding of
the hierarchy of the decision-making process followed by the model. [As well as
overall nature of model]. Our framework is available at
https://github.com/koriavinash1/BioExp
</p>
<a href="http://arxiv.org/abs/2008.06457" target="_blank">arXiv:2008.06457</a> [<a href="http://arxiv.org/pdf/2008.06457" target="_blank">pdf</a>]

<h2>InstanceMotSeg: Real-time Instance Motion Segmentation for Autonomous Driving. (arXiv:2008.07008v2 [cs.CV] UPDATED)</h2>
<h3>Eslam Mohamed, Mahmoud Ewaisha, Mennatullah Siam, Hazem Rashed, Senthil Yogamani, Waleed Hamdy, Muhammad Helmi, Ahmad El-Sallab</h3>
<p>Moving object segmentation is a crucial task for autonomous vehicles as it
can be used to segment objects in a class agnostic manner based on their motion
cues. It enables the detection of unknown objects during training (e.g., moose
or a construction truck) based on their motion. Although pixel-wise motion
segmentation has been studied in autonomous driving literature, it is not dealt
with on the instance level, which would help separate connected segments of
moving objects leading to better trajectory planning. Other generic video
object segmentation tasks have dealt with instance-wise motion segmentation but
on much simpler setting and have not dealt with the multi-task learning problem
for both semantic and class agnostic instance segmentation. In this paper, we
propose a motion-based instance segmentation task and provide a new annotated
dataset based on KITTIMoSeg, which will be released publicly. Our dataset
provide extra class annotations which is crucial for studying class agnostic
segmentation. We further propose an efficient multi-task learning approach that
learns an extra class agnostic instance segmentation head through sharing the
prototype generation network with the semantic head. The model then learns
separate prototype coefficients within the class agnostic and semantic heads.
To obtain real-time performance, we study different efficient encoders and
obtain 39 fps on a Titan Xp GPU using MobileNetV2 with an improvement of 10%
mAP relative to the baseline. Our model outperforms state of the art motion
segmentation methods with 3.3% improvement. We summarize our work in a short
video with qualitative results at https://youtu.be/3mOeDmiR8BU.
</p>
<a href="http://arxiv.org/abs/2008.07008" target="_blank">arXiv:2008.07008</a> [<a href="http://arxiv.org/pdf/2008.07008" target="_blank">pdf</a>]

<h2>AID: Pushing the Performance Boundary of Human Pose Estimation with Information Dropping Augmentation. (arXiv:2008.07139v2 [cs.CV] UPDATED)</h2>
<h3>Junjie Huang, Zheng Zhu, Guan Huang, Dalong Du</h3>
<p>Both appearance cue and constraint cue are vital for human pose estimation.
However, there is a tendency in most existing works to overfitting the former
and overlook the latter. In this paper, we propose Augmentation by Information
Dropping (AID) to verify and tackle this dilemma. Alone with AID as a
prerequisite for effectively exploiting its potential, we propose customized
training schedules, which are designed by analyzing the pattern of loss and
performance in training process from the perspective of information supplying.
In experiments, as a model-agnostic approach, AID promotes various
state-of-the-art methods in both bottom-up and top-down paradigms with
different input sizes, frameworks, backbones, training and testing sets. On
popular COCO human pose estimation test set, AID consistently boosts the
performance of different configurations by around 0.6 AP in top-down paradigm
and up to 1.5 AP in bottom-up paradigm. On more challenging CrowdPose dataset,
the improvement is more than 1.5 AP. As AID successfully pushes the performance
boundary of human pose estimation problem by considerable margin and sets a new
state-of-the-art, we hope AID to be a regular configuration for training human
pose estimators. The source code will be publicly available for further
research.
</p>
<a href="http://arxiv.org/abs/2008.07139" target="_blank">arXiv:2008.07139</a> [<a href="http://arxiv.org/pdf/2008.07139" target="_blank">pdf</a>]

<h2>Critical analysis on the reproducibility of visual quality assessment using deep features. (arXiv:2009.05369v2 [cs.CV] UPDATED)</h2>
<h3>Franz G&#xf6;tz-Hahn, Vlad Hosu, Dietmar Saupe</h3>
<p>Data used to train supervised machine learning models are commonly split into
independent training, validation, and test sets. In this paper we illustrate
that intricate cases of data leakage have occurred in the no-reference image
and video quality assessment literature. Recently, papers in several journals
reported performance results well above the best in the field. However, our
analysis shows that information from the test set was inappropriately used in
the training process in different ways, and that the claimed results cannot be
reached. When correcting for the data leakage, the performances of the
approaches drop even below the state-of-the-art by a large margin.
Additionally, we investigate end-to-end variations to the discussed approaches,
which do not improve upon the original.
</p>
<a href="http://arxiv.org/abs/2009.05369" target="_blank">arXiv:2009.05369</a> [<a href="http://arxiv.org/pdf/2009.05369" target="_blank">pdf</a>]

<h2>Residual Learning for Effective joint Demosaicing-Denoising. (arXiv:2009.06205v2 [cs.CV] UPDATED)</h2>
<h3>Yu Guo, Qiyu Jin, Gabriele Facciolo, Tieyong Zeng, Jean-Michel Morel</h3>
<p>Image demosaicking and denoising are the two key steps for color image
production pipeline. The classical processing sequence consists of applying
denoising first, and then demosaicking. However, this sequence leads to
oversmoothing and unpleasant checkerboard effect. Moreover, it is very
difficult to change this order, because once the image is demosaicked, the
statistical properties of the noise will be changed dramatically. This is
extremely challenging for traditional denoising models that strongly rely on
statistical assumptions. In this paper, we attempt to tackle this prickly
problem. Indeed, here we invert the traditional CFA processing pipeline by
first demosaicking and then denoising. In the first stage, we design a
demosaicking algorithm that combines traditional methods and a convolutional
neural network (CNN) to reconstruct a full color image ignoring the noise. To
improve the performance in image demosaicking, we modify an Inception
architecture for fusing R, G and B three channels information. This stage
retains all known information that is the key point to obtain pleasurable final
results. After demosaicking, we get a noisy full-color image and use another
CNN to learn the demosaicking residual noise (including artifacts) of it, that
allows to obtain a restored full color image. Our proposed algorithm completely
avoids the checkerboard effect and retains more image detail. Furthermore, it
can process very high-level noise while the performances of other CNN based
methods for noise higher than 20 are rather limited. Experimental results show
clearly that our method outperforms state-of-the-art methods both
quantitatively as well as in terms of visual quality.
</p>
<a href="http://arxiv.org/abs/2009.06205" target="_blank">arXiv:2009.06205</a> [<a href="http://arxiv.org/pdf/2009.06205" target="_blank">pdf</a>]

<h2>Few-Shot Unsupervised Continual Learning through Meta-Examples. (arXiv:2009.08107v3 [cs.LG] UPDATED)</h2>
<h3>Alessia Bertugli, Stefano Vincenzi, Simone Calderara, Andrea Passerini</h3>
<p>In real-world applications, data do not reflect the ones commonly used for
neural networks training, since they are usually few, unlabeled and can be
available as a stream. Hence many existing deep learning solutions suffer from
a limited range of applications, in particular in the case of online streaming
data that evolve over time. To narrow this gap, in this work we introduce a
novel and complex setting involving unsupervised meta-continual learning with
unbalanced tasks. These tasks are built through a clustering procedure applied
to a fitted embedding space. We exploit a meta-learning scheme that
simultaneously alleviates catastrophic forgetting and favors the generalization
to new tasks. Moreover, to encourage feature reuse during the
meta-optimization, we exploit a single inner loop taking advantage of an
aggregated representation achieved through the use of a self-attention
mechanism. Experimental results on few-shot learning benchmarks show
competitive performance even compared to the supervised case. Additionally, we
empirically observe that in an unsupervised scenario, the small tasks and the
variability in the clusters pooling play a crucial role in the generalization
capability of the network. Further, on complex datasets, the exploitation of
more clusters than the true number of classes leads to higher results, even
compared to the ones obtained with full supervision, suggesting that a
predefined partitioning into classes can miss relevant structural information.
</p>
<a href="http://arxiv.org/abs/2009.08107" target="_blank">arXiv:2009.08107</a> [<a href="http://arxiv.org/pdf/2009.08107" target="_blank">pdf</a>]

<h2>Discriminative Segmentation Tracking Using Dual Memory Banks. (arXiv:2009.09669v4 [cs.CV] UPDATED)</h2>
<h3>Fei Xie, Wankou Yang, Bo Liu, Kaihua Zhang, Wanli Xue, Wangmeng Zuo</h3>
<p>Existing template-based trackers usually localize the target in each frame
with bounding box, thereby being limited in learning pixel-wise representation
and handling complex and non-rigid transformation of the target. Further,
existing segmentation tracking methods are still insufficient in modeling and
exploiting dense correspondence of target pixels across frames. To overcome
these limitations, this work presents a novel discriminative segmentation
tracking architecture equipped with dual memory banks, i.e., appearance memory
bank and spatial memory bank. In particular, the appearance memory bank
utilizes spatial and temporal non-local similarity to propagate segmentation
mask to the current frame, and we further treat discriminative correlation
filter as spatial memory bank to store the mapping between feature map and
spatial map. Without bells and whistles, our simple-yet-effective tracking
architecture sets a new state-of-the-art on the VOT2016, VOT2018, VOT2019,
GOT-10K and TrackingNet benchmarks, especially achieving the EAO of 0.535 and
0.506 respectively on VOT2016 and VOT2018. Moreover, our approach outperforms
the leading segmentation tracker D3S on two video object segmentation
benchmarks DAVIS16 and DAVIS17. The source code will be released at
https://github.com/phiphiphi31/DMB.
</p>
<a href="http://arxiv.org/abs/2009.09669" target="_blank">arXiv:2009.09669</a> [<a href="http://arxiv.org/pdf/2009.09669" target="_blank">pdf</a>]

<h2>Towards image-based automatic meter reading in unconstrained scenarios: A robust and efficient approach. (arXiv:2009.10181v2 [cs.CV] UPDATED)</h2>
<h3>Rayson Laroca, Alessandra B. Araujo, Luiz A. Zanlorensi, Eduardo C. de Almeida, David Menotti</h3>
<p>Existing approaches for image-based Automatic Meter Reading (AMR) have been
evaluated on images captured in well-controlled scenarios. However, real-world
meter reading presents unconstrained scenarios that are way more challenging
due to dirt, various lighting conditions, scale variations, in-plane and
out-of-plane rotations, among other factors. In this work, we present an
end-to-end approach for AMR focusing on unconstrained scenarios. Our main
contribution is the insertion of a new stage in the AMR pipeline, called corner
detection and counter classification, which enables the counter region to be
rectified -- as well as the rejection of illegible/faulty meters -- prior to
the recognition stage. We also introduce a publicly available dataset, called
Copel-AMR, that contains 12,500 meter images acquired in the field by the
service company's employees themselves, including 2,500 images of faulty meters
or cases where the reading is illegible due to occlusions. Experimental
evaluation demonstrates that the proposed system, which has three networks
operating in a cascaded mode, outperforms six baselines in terms of recognition
rate while still being quite efficient. Moreover, as very few reading errors
are tolerated in real-world applications, we show that our AMR system achieves
impressive recognition rates (i.e., &gt; 99%) when rejecting readings made with
lower confidence values.
</p>
<a href="http://arxiv.org/abs/2009.10181" target="_blank">arXiv:2009.10181</a> [<a href="http://arxiv.org/pdf/2009.10181" target="_blank">pdf</a>]

<h2>Asymmetric Loss For Multi-Label Classification. (arXiv:2009.14119v2 [cs.CV] UPDATED)</h2>
<h3>Emanuel Ben-Baruch, Tal Ridnik, Nadav Zamir, Asaf Noy, Itamar Friedman, Matan Protter, Lihi Zelnik-Manor</h3>
<p>In a typical multi-label setting, a picture contains on average few positive
labels, and many negative ones. This positive-negative imbalance dominates the
optimization process, and can lead to under-emphasizing gradients from positive
labels during training, resulting in poor accuracy. In this paper, we introduce
a novel asymmetric loss ("ASL"), which operates differently on positive and
negative samples. The loss enables to dynamically down-weights and
hard-thresholds easy negative samples, while also discarding possibly
mislabeled samples. We demonstrate how ASL can balance the probabilities of
different samples, and how this balancing is translated to better mAP scores.
With ASL, we reach state-of-the-art results on multiple popular multi-label
datasets: MS-COCO, Pascal-VOC, NUS-WIDE and Open Images. We also demonstrate
ASL applicability for other tasks, such as single-label classification and
object detection. ASL is effective, easy to implement, and does not increase
the training time or complexity.

Implementation is available at: https://github.com/Alibaba-MIIL/ASL.
</p>
<a href="http://arxiv.org/abs/2009.14119" target="_blank">arXiv:2009.14119</a> [<a href="http://arxiv.org/pdf/2009.14119" target="_blank">pdf</a>]

<h2>DOTS: Decoupling Operation and Topology in Differentiable Architecture Search. (arXiv:2010.00969v2 [cs.CV] UPDATED)</h2>
<h3>Yu-Chao Gu, Li-Juan Wang, Yun Liu, Yi Yang, Yu-Huan Wu, Shao-Ping Lu, Ming-Ming Cheng</h3>
<p>Differentiable Architecture Search (DARTS) has attracted extensive attention
due to its efficiency in searching for cell structures. DARTS mainly focuses on
the operation search and derives the cell topology from the operation weights.
However, the operation weights can not indicate the importance of cell topology
and result in poor topology rating correctness. To tackle this, we propose to
Decouple the Operation and Topology Search (DOTS), which decouples the topology
representation from operation weight and make an explicit topology search. DOTS
is achieved by introducing a topology search space that contains combinations
of candidate edges. The proposed search space directly reflects the search
objective and can be easily extended to support a flexible number of edges in
the searched cell. Existing gradient-based NAS methods can be incorporated into
DOTS for further improvement by topology search. Considering that some
operations (e.g., Skip-Connection) can affect the topology, we adopt the
operation search with the group strategy to preserve topology-related
operations for better topology search. The experiments on CIFAR10/100 and
ImageNet demonstrate that DOTS is an effective solution for differentiable NAS.
The code will be released.
</p>
<a href="http://arxiv.org/abs/2010.00969" target="_blank">arXiv:2010.00969</a> [<a href="http://arxiv.org/pdf/2010.00969" target="_blank">pdf</a>]

<h2>BayesAdapter: Being Bayesian, Inexpensively and Robustly, via Bayesian Fine-tuning. (arXiv:2010.01979v2 [cs.LG] UPDATED)</h2>
<h3>Zhijie Deng, Xiao Yang, Hao Zhang, Yinpeng Dong, Jun Zhu</h3>
<p>Despite their theoretical appealingness, Bayesian neural networks (BNNs) are
falling far behind in terms of adoption in real-world applications compared
with normal NNs, mainly due to their limited scalability in training, and low
fidelity in their uncertainty estimates. In this work, we develop a new
framework, named BayesAdapter, to address these issues and bring Bayesian deep
learning to the masses. The core notion of BayesAdapter is to adapt pre-trained
deterministic NNs to be BNNs via Bayesian fine-tuning. We implement Bayesian
fine-tuning with a plug-and-play instantiation of stochastic variational
inference, and propose exemplar reparameterization to reduce gradient variance
and stabilize the fine-tuning. Together, they enable training BNNs as if one
were training deterministic NNs with minimal added overheads. During Bayesian
fine-tuning, we further propose an uncertainty regularization to supervise and
calibrate the uncertainty quantification of learned BNNs at low cost. To
empirically evaluate BayesAdapter, we conduct extensive experiments on a
diverse set of challenging benchmarks, and observe satisfactory training
efficiency, competitive predictive performance, and calibrated and faithful
uncertainty estimates.
</p>
<a href="http://arxiv.org/abs/2010.01979" target="_blank">arXiv:2010.01979</a> [<a href="http://arxiv.org/pdf/2010.01979" target="_blank">pdf</a>]

<h2>Robust Multi-class Feature Selection via $l_{2,0}$-Norm Regularization Minimization. (arXiv:2010.03728v2 [cs.LG] UPDATED)</h2>
<h3>Zhenzhen Sun, Yuanlong Yu</h3>
<p>Feature selection is an important data pre-processing in data mining and
machine learning, which can reduce feature size without deteriorating model's
performance. Recently, sparse regression based feature selection methods have
received considerable attention due to their good performance. However, because
the $l_{2,0}$-norm regularization term is non-convex, this problem is very hard
to solve. In this paper, unlike most of the other methods which only solve the
approximate problem, a novel method based on homotopy iterative hard threshold
(HIHT) is proposed to solve the $l_{2,0}$-norm regularization least square
problem directly for multi-class feature selection, which can produce exact
row-sparsity solution for the weights matrix. What'more, in order to reduce the
computational time of HIHT, an acceleration version of HIHT (AHIHT) is derived.
Extensive experiments on eight biological datasets show that the proposed
method can achieve higher classification accuracy (ACC) with fewest number of
selected features (No.fea) comparing with the approximate convex counterparts
and state-of-the-art feature selection methods. The robustness of
classification accuracy to the regularization parameter and the number of
selected feature are also exhibited.
</p>
<a href="http://arxiv.org/abs/2010.03728" target="_blank">arXiv:2010.03728</a> [<a href="http://arxiv.org/pdf/2010.03728" target="_blank">pdf</a>]

<h2>IF-Defense: 3D Adversarial Point Cloud Defense via Implicit Function based Restoration. (arXiv:2010.05272v2 [cs.CV] UPDATED)</h2>
<h3>Ziyi Wu, Yueqi Duan, He Wang, Qingnan Fan, Leonidas J. Guibas</h3>
<p>Point cloud is an important 3D data representation widely used in many
essential applications. Leveraging deep neural networks, recent works have
shown great success in processing 3D point clouds. However, those deep neural
networks are vulnerable to various 3D adversarial attacks, which can be
summarized as two primary types: point perturbation that affects local point
distribution, and surface distortion that causes dramatic changes in geometry.
In this paper, we propose a novel 3D adversarial point cloud defense method
leveraging implicit function based restoration (IF-Defense) to address both the
aforementioned attacks. It is composed of two steps: 1) it predicts an implicit
function that captures the clean shape through a surface recovery module, and
2) restores a clean and complete point cloud via minimizing the difference
between the attacked point cloud and the predicted implicit function under
geometry- and distribution- aware constraints. Our experimental results show
that IF-Defense achieves the state-of-the-art defense performance against all
existing adversarial attacks on PointNet, PointNet++, DGCNN and PointConv.
Comparing with previous methods, IF-Defense presents 20.02% improvement in
classification accuracy against salient point dropping attack and 16.29%
against LG-GAN attack on PointNet. Our code is available at
https://github.com/Wuziyi616/IF-Defense.
</p>
<a href="http://arxiv.org/abs/2010.05272" target="_blank">arXiv:2010.05272</a> [<a href="http://arxiv.org/pdf/2010.05272" target="_blank">pdf</a>]

<h2>Back to the Future: Cycle Encoding Prediction for Self-supervised Contrastive Video Representation Learning. (arXiv:2010.07217v3 [cs.CV] UPDATED)</h2>
<h3>Xinyu Yang, Majid Mirmehdi, Tilo Burghardt</h3>
<p>In this paper we show that learning video feature spaces in which temporal
cycles are maximally predictable benefits action classification. In particular,
we propose a novel learning approach termed Cycle Encoding Prediction (CEP)
that is able to effectively represent high-level spatio-temporal structure of
unlabelled video content. CEP builds a latent space wherein the concept of
closed forward-backward as well as backward-forward temporal loops is
approximately preserved. As a self-supervision signal, CEP leverages the
bi-directional temporal coherence of the video stream and applies loss
functions that encourage both temporal cycle closure as well as contrastive
feature separation. Architecturally, the underpinning network structure
utilises a single feature encoder for all video snippets, adding two predictive
modules that learn temporal forward and backward transitions. We apply our
framework for pretext training of networks for action recognition tasks. We
report significantly improved results for the standard datasets UCF101 and
HMDB51. Detailed ablation studies support the effectiveness of the proposed
components. We publish source code for the CEP components in full with this
paper.
</p>
<a href="http://arxiv.org/abs/2010.07217" target="_blank">arXiv:2010.07217</a> [<a href="http://arxiv.org/pdf/2010.07217" target="_blank">pdf</a>]

<h2>Show and Speak: Directly Synthesize Spoken Description of Images. (arXiv:2010.12267v2 [cs.CV] UPDATED)</h2>
<h3>Xinsheng Wang, Siyuan Feng, Jihua Zhu, Mark Hasegawa-Johnson, Odette Scharenborg</h3>
<p>This paper proposes a new model, referred to as the show and speak (SAS)
model that, for the first time, is able to directly synthesize spoken
descriptions of images, bypassing the need for any text or phonemes. The basic
structure of SAS is an encoder-decoder architecture that takes an image as
input and predicts the spectrogram of speech that describes this image. The
final speech audio is obtained from the predicted spectrogram via WaveNet.
Extensive experiments on the public benchmark database Flickr8k demonstrate
that the proposed SAS is able to synthesize natural spoken descriptions for
images, indicating that synthesizing spoken descriptions for images while
bypassing text and phonemes is feasible.
</p>
<a href="http://arxiv.org/abs/2010.12267" target="_blank">arXiv:2010.12267</a> [<a href="http://arxiv.org/pdf/2010.12267" target="_blank">pdf</a>]

<h2>Dynamically Feasible Deep Reinforcement Learning Policy for Robot Navigation in Dense Mobile Crowds. (arXiv:2010.14838v2 [cs.RO] UPDATED)</h2>
<h3>Utsav Patel, Nithish Kumar, Adarsh Jagan Sathyamoorthy, Dinesh Manocha</h3>
<p>We present a novel Deep Reinforcement Learning (DRL) based policy to compute
dynamically feasible and spatially aware velocities for a robot navigating
among mobile obstacles. Our approach combines the benefits of the Dynamic
Window Approach (DWA) in terms of satisfying the robot's dynamics constraints
with state-of-the-art DRL-based navigation methods that can handle moving
obstacles and pedestrians well. Our formulation achieves these goals by
embedding the environmental obstacles' motions in a novel low-dimensional
observation space. It also uses a novel reward function to positively reinforce
velocities that move the robot away from the obstacle's heading direction
leading to significantly lower number of collisions. We evaluate our method in
realistic 3-D simulated environments and on a real differential drive robot in
challenging dense indoor scenarios with several walking pedestrians. We compare
our method with state-of-the-art collision avoidance methods and observe
significant improvements in terms of success rate (up to 33\% increase), number
of dynamics constraint violations (up to 61\% decrease), and smoothness. We
also conduct ablation studies to highlight the advantages of our observation
space formulation, and reward structure.
</p>
<a href="http://arxiv.org/abs/2010.14838" target="_blank">arXiv:2010.14838</a> [<a href="http://arxiv.org/pdf/2010.14838" target="_blank">pdf</a>]

<h2>Bayes-Adaptive Deep Model-Based Policy Optimisation. (arXiv:2010.15948v2 [cs.RO] UPDATED)</h2>
<h3>Tai Hoang, Ngo Anh Vien</h3>
<p>We introduce a Bayesian (deep) model-based reinforcement learning method
(RoMBRL) that can capture model uncertainty to achieve sample-efficient policy
optimisation. We propose to formulate the model-based policy optimisation
problem as a Bayes-adaptive Markov decision process (BAMDP). RoMBRL maintains
model uncertainty via belief distributions through a deep Bayesian neural
network whose samples are generated via stochastic gradient Hamiltonian Monte
Carlo. Uncertainty is propagated through simulations controlled by sampled
models and history-based policies. As beliefs are encoded in visited histories,
we propose a history-based policy network that can be end-to-end trained to
generalise across history space and will be trained using recurrent
Trust-Region Policy Optimisation. We show that RoMBRL outperforms existing
approaches on many challenging control benchmark tasks in terms of sample
complexity and task performance. The source code of this paper is also publicly
available on https://github.com/thobotics/RoMBRL.
</p>
<a href="http://arxiv.org/abs/2010.15948" target="_blank">arXiv:2010.15948</a> [<a href="http://arxiv.org/pdf/2010.15948" target="_blank">pdf</a>]

<h2>MBB: Model-Based Baseline for Efficient Reinforcement Learning. (arXiv:2011.02073v2 [cs.LG] UPDATED)</h2>
<h3>Xubo Lyu, Site Li, Seth Siriya, Ye Pu, Mo Chen</h3>
<p>Model-free reinforcement learning (RL) is capable of learning control
policies for high-dimensional, complex robotic tasks, but tends to be
data-inefficient. Model-based RL and optimal control have been proven to be
much more data-efficient if an accurate model of the system and environment is
known, but can be difficult to scale to expressive models for high-dimensional
problems. In this paper, we propose a novel approach to alleviate data
inefficiency of model-free RL by warm-starting the learning process using a
lower-dimensional model-based solutions. Particularly, we propose a baseline
function that is initialized via supervision from a low-dimensional value
function. Such a lower-dimensional value function can be obtained by applying
model-based techniques on a low-dimensional problem featuring a known
approximate system model. Therefore, our approach exploits the model priors
from a simplified problem space implicitly and avoids the direct use of
high-dimensional, expressive models. We demonstrate our approach on two
representative robotic learning tasks and observe significant improvement in
performance and efficiency, and analyze our method empirically with a third
task.
</p>
<a href="http://arxiv.org/abs/2011.02073" target="_blank">arXiv:2011.02073</a> [<a href="http://arxiv.org/pdf/2011.02073" target="_blank">pdf</a>]

<h2>Pixel-wise Dense Detector for Image Inpainting. (arXiv:2011.02293v2 [cs.CV] UPDATED)</h2>
<h3>Ruisong Zhang, Weize Quan, Baoyuan Wu, Zhifeng Li, Dong-Ming Yan</h3>
<p>Recent GAN-based image inpainting approaches adopt an average strategy to
discriminate the generated image and output a scalar, which inevitably lose the
position information of visual artifacts. Moreover, the adversarial loss and
reconstruction loss (e.g., l1 loss) are combined with tradeoff weights, which
are also difficult to tune. In this paper, we propose a novel detection-based
generative framework for image inpainting, which adopts the min-max strategy in
an adversarial process. The generator follows an encoder-decoder architecture
to fill the missing regions, and the detector using weakly supervised learning
localizes the position of artifacts in a pixel-wise manner. Such position
information makes the generator pay attention to artifacts and further enhance
them. More importantly, we explicitly insert the output of the detector into
the reconstruction loss with a weighting criterion, which balances the weight
of the adversarial loss and reconstruction loss automatically rather than
manual operation. Experiments on multiple public datasets show the superior
performance of the proposed framework. The source code is available at
https://github.com/Evergrow/GDN_Inpainting.
</p>
<a href="http://arxiv.org/abs/2011.02293" target="_blank">arXiv:2011.02293</a> [<a href="http://arxiv.org/pdf/2011.02293" target="_blank">pdf</a>]

<h2>Hypersim: A Photorealistic Synthetic Dataset for Holistic Indoor Scene Understanding. (arXiv:2011.02523v4 [cs.CV] UPDATED)</h2>
<h3>Mike Roberts, Nathan Paczan</h3>
<p>For many fundamental scene understanding tasks, it is difficult or impossible
to obtain per-pixel ground truth labels from real images. We address this
challenge by introducing Hypersim, a photorealistic synthetic dataset for
holistic indoor scene understanding. To create our dataset, we leverage a large
repository of synthetic scenes created by professional artists, and we generate
77,400 images of 461 indoor scenes with detailed per-pixel labels and
corresponding ground truth geometry. Our dataset: (1) relies exclusively on
publicly available 3D assets; (2) includes complete scene geometry, material
information, and lighting information for every scene; (3) includes dense
per-pixel semantic instance segmentations for every image; and (4) factors
every image into diffuse reflectance, diffuse illumination, and a non-diffuse
residual term that captures view-dependent lighting effects. Together, these
features make our dataset well-suited for geometric learning problems that
require direct 3D supervision, multi-task learning problems that require
reasoning jointly over multiple input and output modalities, and inverse
rendering problems. We analyze our dataset at the level of scenes, objects, and
pixels, and we analyze costs in terms of money, annotation effort, and
computation time. Remarkably, we find that it is possible to generate our
entire dataset from scratch, for roughly half the cost of training a
state-of-the-art natural language processing model. All the code we used to
generate our dataset is available online.
</p>
<a href="http://arxiv.org/abs/2011.02523" target="_blank">arXiv:2011.02523</a> [<a href="http://arxiv.org/pdf/2011.02523" target="_blank">pdf</a>]

<h2>LADA: Look-Ahead Data Acquisition via Augmentation for Active Learning. (arXiv:2011.04194v3 [cs.LG] UPDATED)</h2>
<h3>Yoon-Yeong Kim, Kyungwoo Song, JoonHo Jang, Il-Chul Moon</h3>
<p>Active learning effectively collects data instances for training deep
learning models when the labeled dataset is limited and the annotation cost is
high. Besides active learning, data augmentation is also an effective technique
to enlarge the limited amount of labeled instances. However, the potential gain
from virtual instances generated by data augmentation has not been considered
in the acquisition process of active learning yet. Looking ahead the effect of
data augmentation in the process of acquisition would select and generate the
data instances that are informative for training the model. Hence, this paper
proposes Look-Ahead Data Acquisition via augmentation, or LADA, to integrate
data acquisition and data augmentation. LADA considers both 1) unlabeled data
instance to be selected and 2) virtual data instance to be generated by data
augmentation, in advance of the acquisition process. Moreover, to enhance the
informativeness of the virtual data instances, LADA optimizes the data
augmentation policy to maximize the predictive acquisition score, resulting in
the proposal of InfoMixup and InfoSTN. As LADA is a generalizable framework, we
experiment with the various combinations of acquisition and augmentation
methods. The performance of LADA shows a significant improvement over the
recent augmentation and acquisition baselines which were independently applied
to the benchmark datasets.
</p>
<a href="http://arxiv.org/abs/2011.04194" target="_blank">arXiv:2011.04194</a> [<a href="http://arxiv.org/pdf/2011.04194" target="_blank">pdf</a>]

<h2>Deep Transfer Learning for Automated Diagnosis of Skin Lesions from Photographs. (arXiv:2011.04475v2 [cs.CV] UPDATED)</h2>
<h3>Emma Rocheteau, Doyoon Kim</h3>
<p>Melanoma is not the most common form of skin cancer, but it is the most
deadly. Currently, the disease is diagnosed by expert dermatologists, which is
costly and requires timely access to medical treatment. Recent advances in deep
learning have the potential to improve diagnostic performance, expedite urgent
referrals and reduce burden on clinicians. Through smart phones, the technology
could reach people who would not normally have access to such healthcare
services, e.g. in remote parts of the world, due to financial constraints or in
2020, COVID-19 cancellations. To this end, we have investigated various
transfer learning approaches by leveraging model parameters pre-trained on
ImageNet with finetuning on melanoma detection. We compare EfficientNet,
MnasNet, MobileNet, DenseNet, SqueezeNet, ShuffleNet, GoogleNet, ResNet,
ResNeXt, VGG and a simple CNN with and without transfer learning. We find the
mobile network, EfficientNet (with transfer learning) achieves the best mean
performance with an area under the receiver operating characteristic curve
(AUROC) of 0.931$\pm$0.005 and an area under the precision recall curve (AUPRC)
of 0.840$\pm$0.010. This is significantly better than general practitioners
(0.83$\pm$0.03 AUROC) and dermatologists (0.91$\pm$0.02 AUROC).
</p>
<a href="http://arxiv.org/abs/2011.04475" target="_blank">arXiv:2011.04475</a> [<a href="http://arxiv.org/pdf/2011.04475" target="_blank">pdf</a>]

<h2>Sparse Longitudinal Representations of Electronic Health Record Data for the Early Detection of Chronic Kidney Disease in Diabetic Patients. (arXiv:2011.04802v2 [cs.LG] UPDATED)</h2>
<h3>Jinghe Zhang, Kamran Kowsari, Mehdi Boukhechba, James Harrison, Jennifer Lobo, Laura Barnes</h3>
<p>Chronic kidney disease (CKD) is a gradual loss of renal function over time,
and it increases the risk of mortality, decreased quality of life, as well as
serious complications. The prevalence of CKD has been increasing in the last
couple of decades, which is partly due to the increased prevalence of diabetes
and hypertension. To accurately detect CKD in diabetic patients, we propose a
novel framework to learn sparse longitudinal representations of patients'
medical records. The proposed method is also compared with widely used
baselines such as Aggregated Frequency Vector and Bag-of-Pattern in Sequences
on real EHR data, and the experimental results indicate that the proposed model
achieves higher predictive performance. Additionally, the learned
representations are interpreted and visualized to bring clinical insights.
</p>
<a href="http://arxiv.org/abs/2011.04802" target="_blank">arXiv:2011.04802</a> [<a href="http://arxiv.org/pdf/2011.04802" target="_blank">pdf</a>]

<h2>Deep Sketch-Based Modeling: Tips and Tricks. (arXiv:2011.06133v2 [cs.CV] UPDATED)</h2>
<h3>Yue Zhong, Yulia Gryaditskaya, Honggang Zhang, Yi-Zhe Song</h3>
<p>Deep image-based modeling received lots of attention in recent years, yet the
parallel problem of sketch-based modeling has only been briefly studied, often
as a potential application. In this work, for the first time, we identify the
main differences between sketch and image inputs: (i) style variance, (ii)
imprecise perspective, and (iii) sparsity. We discuss why each of these
differences can pose a challenge, and even make a certain class of image-based
methods inapplicable. We study alternative solutions to address each of the
difference. By doing so, we drive out a few important insights: (i) sparsity
commonly results in an incorrect prediction of foreground versus background,
(ii) diversity of human styles, if not taken into account, can lead to very
poor generalization properties, and finally (iii) unless a dedicated sketching
interface is used, one can not expect sketches to match a perspective of a
fixed viewpoint. Finally, we compare a set of representative deep single-image
modeling solutions and show how their performance can be improved to tackle
sketch input by taking into consideration the identified critical differences.
</p>
<a href="http://arxiv.org/abs/2011.06133" target="_blank">arXiv:2011.06133</a> [<a href="http://arxiv.org/pdf/2011.06133" target="_blank">pdf</a>]

<h2>A Review of Uncertainty Quantification in Deep Learning: Techniques, Applications and Challenges. (arXiv:2011.06225v3 [cs.LG] UPDATED)</h2>
<h3>Moloud Abdar, Farhad Pourpanah, Sadiq Hussain, Dana Rezazadegan, Li Liu, Mohammad Ghavamzadeh, Paul Fieguth, Xiaochun Cao, Abbas Khosravi, U Rajendra Acharya, Vladimir Makarenkov, Saeid Nahavandi</h3>
<p>Uncertainty quantification (UQ) plays a pivotal role in reduction of
uncertainties during both optimization and decision making processes. It can be
applied to solve a variety of real-world applications in science and
engineering. Bayesian approximation and ensemble learning techniques are two
most widely-used UQ methods in the literature. In this regard, researchers have
proposed different UQ methods and examined their performance in a variety of
applications such as computer vision (e.g., self-driving cars and object
detection), image processing (e.g., image restoration), medical image analysis
(e.g., medical image classification and segmentation), natural language
processing (e.g., text classification, social media texts and recidivism
risk-scoring), bioinformatics, etc. This study reviews recent advances in UQ
methods used in deep learning. Moreover, we also investigate the application of
these methods in reinforcement learning (RL). Then, we outline a few important
applications of UQ methods. Finally, we briefly highlight the fundamental
research challenges faced by UQ methods and discuss the future research
directions in this field.
</p>
<a href="http://arxiv.org/abs/2011.06225" target="_blank">arXiv:2011.06225</a> [<a href="http://arxiv.org/pdf/2011.06225" target="_blank">pdf</a>]

<h2>RIFE: Real-Time Intermediate Flow Estimation for Video Frame Interpolation. (arXiv:2011.06294v2 [cs.CV] UPDATED)</h2>
<h3>Zhewei Huang, Tianyuan Zhang, Wen Heng, Boxin Shi, Shuchang Zhou</h3>
<p>We propose RIFE, a Real-time Intermediate Flow Estimation algorithm for Video
Frame Interpolation (VFI). Most existing methods first estimate the
bi-directional optical flows and then linearly combine them to approximate
intermediate flows, leading to artifacts on motion boundaries. RIFE uses a
neural network named IFNet that can directly estimate the intermediate flows
from images. With the more precise flows and our simplified fusion process,
RIFE can improve interpolation quality and have much better speed. Based on our
proposed leakage distillation loss, RIFE can be trained in an end-to-end
fashion. Experiments demonstrate that our method is significantly faster than
existing VFI methods and can achieve state-of-the-art performance on public
benchmarks. The code is available at https://github.com/hzwer/arXiv2020-RIFE.
</p>
<a href="http://arxiv.org/abs/2011.06294" target="_blank">arXiv:2011.06294</a> [<a href="http://arxiv.org/pdf/2011.06294" target="_blank">pdf</a>]

<h2>A differential evolution-based optimization tool for interplanetary transfer trajectory design. (arXiv:2011.06780v2 [cs.AI] UPDATED)</h2>
<h3>Mingcheng Zuo, Guangming Dai, Lei Peng, Zhe Tang</h3>
<p>The extremely sensitive and highly nonlinear search space of interplanetary
transfer trajectory design bring about big challenges on global optimization.
As a representative, the current known best solution of the global trajectory
optimization problem (GTOP) designed by the European space agency (ESA) is very
hard to be found. To deal with this difficulty, a powerful differential
evolution-based optimization tool named COoperative Differential Evolution
(CODE) is proposed in this paper. CODE employs a two-stage evolutionary
process, which concentrates on learning global structure in the earlier
process, and tends to self-adaptively learn the structures of different local
spaces. Besides, considering the spatial distribution of global optimum on
different problems and the gradient information on different variables, a
multiple boundary check technique has been employed. Also, Covariance Matrix
Adaptation Evolutionary Strategies (CMA-ES) is used as a local optimizer. The
previous studies have shown that a specific swarm intelligent optimization
algorithm usually can solve only one or two GTOP problems. However, the
experimental test results show that CODE can find the current known best
solutions of Cassini1 and Sagas directly, and the cooperation with CMA-ES can
solve Cassini2, GTOC1, Messenger (reduced) and Rosetta. For the most
complicated Messenger (full) problem, even though CODE cannot find the current
known best solution, the found best solution with objective function equaling
to 3.38 km/s is still a level that other swarm intelligent algorithms cannot
easily reach.
</p>
<a href="http://arxiv.org/abs/2011.06780" target="_blank">arXiv:2011.06780</a> [<a href="http://arxiv.org/pdf/2011.06780" target="_blank">pdf</a>]

<h2>Relative Drone-Ground Vehicle Localization using LiDAR and Fisheye Cameras through Direct and Indirect Observations. (arXiv:2011.07008v3 [cs.RO] UPDATED)</h2>
<h3>Jan Hausberg, Ryoichi Ishikawa, Menandro Roxas, Takeshi Oishi</h3>
<p>Estimating the pose of an unmanned aerial vehicle (UAV) or drone is a
challenging task. It is useful for many applications such as navigation,
surveillance, tracking objects on the ground, and 3D reconstruction. In this
work, we present a LiDAR-camera-based relative pose estimation method between a
drone and a ground vehicle, using a LiDAR sensor and a fisheye camera on the
vehicle's roof and another fisheye camera mounted under the drone. The LiDAR
sensor directly observes the drone and measures its position, and the two
cameras estimate the relative orientation using indirect observation of the
surrounding objects. We propose a dynamically adaptive kernel-based method for
drone detection and tracking using the LiDAR. We detect vanishing points in
both cameras and find their correspondences to estimate the relative
orientation. Additionally, we propose a rotation correction technique by
relying on the observed motion of the drone through the LiDAR. In our
experiments, we were able to achieve very fast initial detection and real-time
tracking of the drone. Our method is fully automatic.
</p>
<a href="http://arxiv.org/abs/2011.07008" target="_blank">arXiv:2011.07008</a> [<a href="http://arxiv.org/pdf/2011.07008" target="_blank">pdf</a>]

<h2>Ego2Hands: A Dataset for Egocentric Two-hand Segmentation and Detection. (arXiv:2011.07252v2 [cs.CV] UPDATED)</h2>
<h3>Fanqing Lin, Tony Martinez</h3>
<p>Hand segmentation and detection in truly unconstrained RGB-based settings is
important for many applications. However, existing datasets are far from
sufficient both in terms of size and variety due to the infeasibility of manual
annotation of large amounts of segmentation and detection data. As a result,
current methods are limited by many underlying assumptions such as constrained
environment, consistent skin color and lighting. In this work, we present a
large-scale RGB-based egocentric hand segmentation/detection dataset Ego2Hands
that is automatically annotated and a color-invariant compositing-based data
generation technique capable of creating unlimited training data with variety.
For quantitative analysis, we manually annotated an evaluation set that
significantly exceeds existing benchmarks in quantity, diversity and annotation
accuracy. We show that our dataset and training technique can produce models
that generalize to unseen environments without domain adaptation. We introduce
Convolutional Segmentation Machine (CSM) as an architecture that better
balances accuracy, size and speed and provide thorough analysis on the
performance of state-of-the-art models on the Ego2Hands dataset.
</p>
<a href="http://arxiv.org/abs/2011.07252" target="_blank">arXiv:2011.07252</a> [<a href="http://arxiv.org/pdf/2011.07252" target="_blank">pdf</a>]

<h2>TLab: Traffic Map Movie Forecasting Based on HR-NET. (arXiv:2011.07728v2 [cs.LG] UPDATED)</h2>
<h3>Fanyou Wu, Yang Liu, Zhiyuan Liu, Xiaobo Qu, Rado Gazo, Eva Haviarova</h3>
<p>The problem of the effective prediction for large-scale spatio-temporal
traffic data has long haunted researchers in the field of intelligent
transportation. Limited by the quantity of data, citywide traffic state
prediction was seldom achieved. Hence the complex urban transportation system
of an entire city cannot be truly understood. Thanks to the efforts of
organizations like IARAI, the massive open data provided by them has made the
research possible. In our 2020 Competition solution, we further design multiple
variants based on HR-NET and UNet. Through feature engineering, the
hand-crafted features are input into the model in a form of channels. It is
worth noting that, to learn the inherent attributes of geographical locations,
we proposed a novel method called geo-embedding, which contributes to
significant improvement in the accuracy of the model. In addition, we explored
the influence of the selection of activation functions and optimizers, as well
as tricks during model training on the model performance. In terms of
prediction accuracy, our solution has won 2nd place in NeurIPS 2020,
Traffic4cast Challenge.
</p>
<a href="http://arxiv.org/abs/2011.07728" target="_blank">arXiv:2011.07728</a> [<a href="http://arxiv.org/pdf/2011.07728" target="_blank">pdf</a>]

<h2>Cluster-Specific Predictions with Multi-Task Gaussian Processes. (arXiv:2011.07866v2 [cs.LG] UPDATED)</h2>
<h3>Arthur Leroy, Pierre Latouche, Benjamin Guedj, Servane Gey</h3>
<p>A model involving Gaussian processes (GPs) is introduced to simultaneously
handle multi-task learning, clustering, and prediction for multiple functional
data. This procedure acts as a model-based clustering method for functional
data as well as a learning step for subsequent predictions for new tasks. The
model is instantiated as a mixture of multi-task GPs with common mean
processes. A variational EM algorithm is derived for dealing with the
optimisation of the hyper-parameters along with the hyper-posteriors'
estimation of latent variables and processes. We establish explicit formulas
for integrating the mean processes and the latent clustering variables within a
predictive distribution, accounting for uncertainty on both aspects. This
distribution is defined as a mixture of cluster-specific GP predictions, which
enhances the performances when dealing with group-structured data. The model
handles irregular grid of observations and offers different hypotheses on the
covariance structure for sharing additional information across tasks. The
performances on both clustering and prediction tasks are assessed through
various simulated scenarios and real datasets. The overall algorithm, called
MagmaClust, is publicly available as an R package.
</p>
<a href="http://arxiv.org/abs/2011.07866" target="_blank">arXiv:2011.07866</a> [<a href="http://arxiv.org/pdf/2011.07866" target="_blank">pdf</a>]

<h2>Risk-Constrained Thompson Sampling for CVaR Bandits. (arXiv:2011.08046v2 [cs.LG] UPDATED)</h2>
<h3>Joel Q. L. Chang, Qiuyu Zhu, Vincent Y. F. Tan</h3>
<p>The multi-armed bandit (MAB) problem is a ubiquitous decision-making problem
that exemplifies the exploration-exploitation tradeoff. Standard formulations
exclude risk in decision making. Risk notably complicates the basic
reward-maximising objective, in part because there is no universally agreed
definition of it. In this paper, we consider a popular risk measure in
quantitative finance known as the Conditional Value at Risk (CVaR). We explore
the performance of a Thompson Sampling-based algorithm CVaR-TS under this risk
measure. We provide comprehensive comparisons between our regret bounds with
state-of-the-art L/UCB-based algorithms in comparable settings and demonstrate
their clear improvement in performance. We also include numerical simulations
to empirically verify that CVaR-TS outperforms other L/UCB-based algorithms.
</p>
<a href="http://arxiv.org/abs/2011.08046" target="_blank">arXiv:2011.08046</a> [<a href="http://arxiv.org/pdf/2011.08046" target="_blank">pdf</a>]

<h2>Coarse-grained and emergent distributed parameter systems from data. (arXiv:2011.08138v2 [stat.ML] UPDATED)</h2>
<h3>Hassan Arbabi, Felix P. Kemeth, Tom Bertalan, Ioannis Kevrekidis</h3>
<p>We explore the derivation of distributed parameter system evolution laws (and
in particular, partial differential operators and associated partial
differential equations, PDEs) from spatiotemporal data. This is, of course, a
classical identification problem; our focus here is on the use of manifold
learning techniques (and, in particular, variations of Diffusion Maps) in
conjunction with neural network learning algorithms that allow us to attempt
this task when the dependent variables, and even the independent variables of
the PDE are not known a priori and must be themselves derived from the data.
The similarity measure used in Diffusion Maps for dependent coarse variable
detection involves distances between local particle distribution observations;
for independent variable detection we use distances between local short-time
dynamics. We demonstrate each approach through an illustrative established PDE
example. Such variable-free, emergent space identification algorithms connect
naturally with equation-free multiscale computation tools.
</p>
<a href="http://arxiv.org/abs/2011.08138" target="_blank">arXiv:2011.08138</a> [<a href="http://arxiv.org/pdf/2011.08138" target="_blank">pdf</a>]

