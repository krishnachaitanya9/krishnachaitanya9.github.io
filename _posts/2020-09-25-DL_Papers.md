---
title: Latest Deep Learning Papers
date: 2020-12-01 21:00:01 +0800
featured-img: DL-logo
categories: [Deep_Learning]
tags: [Deep_Learning]
mathjax: Yes
---

<h1>Your arXiv Feed (210 Articles)</h1>
<h2>Attention-Based Planning with Active Perception. (arXiv:2012.00053v1 [cs.RO])</h2>
<h3>Haoxiang Ma, Jie Fu</h3>
<p>Attention control is a key cognitive ability for humans to select information
relevant to the current task. This paper develops a computational model of
attention and an algorithm for attention-based probabilistic planning in Markov
decision processes. In attention-based planning, the robot decides to be in
different attention modes. An attention mode corresponds to a subset of state
variables monitored by the robot. By switching between different attention
modes, the robot actively perceives task-relevant information to reduce the
cost of information acquisition and processing, while achieving near-optimal
task performance. Though planning with attention-based active perception
inevitably introduces partial observations, a partially observable MDP
formulation makes the problem computational expensive to solve. Instead, our
proposed method employs a hierarchical planning framework in which the robot
determines what to pay attention to and for how long the attention should be
sustained before shifting to other information sources. During the attention
sustaining phase, the robot carries out a sub-policy, computed from an
abstraction of the original MDP given the current attention. We use an example
where a robot is tasked to capture a set of intruders in a stochastic
gridworld. The experimental results show that the proposed method enables
information- and computation-efficient optimal planning in stochastic
environments.
</p>
<a href="http://arxiv.org/abs/2012.00053" target="_blank">arXiv:2012.00053</a> [<a href="http://arxiv.org/pdf/2012.00053" target="_blank">pdf</a>]

<h2>Move to See Better: Towards Self-Supervised Amodal Object Detection. (arXiv:2012.00057v1 [cs.CV])</h2>
<h3>Zhaoyuan Fang, Ayush Jain, Gabriel Sarch, Adam W. Harley, Katerina Fragkiadaki</h3>
<p>Humans learn to better understand the world by moving around their
environment to get more informative viewpoints of the scene. Most methods for
2D visual recognition tasks such as object detection and segmentation treat
images of the same scene as individual samples and do not exploit object
permanence in multiple views. Generalization to novel scenes and views thus
requires additional training with lots of human annotations. In this paper, we
propose a self-supervised framework to improve an object detector in unseen
scenarios by moving an agent around in a 3D environment and aggregating
multi-view RGB-D information. We unproject confident 2D object detections from
the pre-trained detector and perform unsupervised 3D segmentation on the point
cloud. The segmented 3D objects are then re-projected to all other views to
obtain pseudo-labels for fine-tuning. Experiments on both indoor and outdoor
datasets show that (1) our framework performs high-quality 3D segmentation from
raw RGB-D data and a pre-trained 2D detector; (2) fine-tuning with
self-supervision improves the 2D detector significantly where an unseen RGB
image is given as input at test time; (3) training a 3D detector with
self-supervision outperforms a comparable self-supervised method by a large
margin.
</p>
<a href="http://arxiv.org/abs/2012.00057" target="_blank">arXiv:2012.00057</a> [<a href="http://arxiv.org/pdf/2012.00057" target="_blank">pdf</a>]

<h2>PMLB v1.0: an open source dataset collection for benchmarking machine learning methods. (arXiv:2012.00058v1 [cs.LG])</h2>
<h3>Trang T. Le, William La Cava, Joseph D. Romano, John T. Gregg, Daniel J. Goldberg, Praneel Chakraborty, Natasha L. Ray, Daniel Himmelstein, Weixuan Fu, Jason H. Moore</h3>
<p>PMLB (Penn Machine Learning Benchmark) is an open-source data repository
containing a curated collection of datasets for evaluating and comparing
machine learning (ML) algorithms. Compiled from a broad range of existing ML
benchmark collections, PMLB synthesizes and standardizes hundreds of publicly
available datasets from diverse sources such as the UCI ML repository and
OpenML, enabling systematic assessment of different ML methods. These datasets
cover a range of applications, from binary/multi-class classification to
regression problems with combinations of categorical and continuous features.
PMLB has both a Python interface (pmlb) and an R interface (pmlbr), both with
detailed documentation that allows the user to access cleaned and formatted
datasets using a single function call. PMLB also provides a comprehensive
description of each dataset and advanced functions to explore the dataset
space, allowing for smoother user experience and handling of data. The resource
is designed to facilitate open-source contributions in the form of datasets as
well as improvements to curation.
</p>
<a href="http://arxiv.org/abs/2012.00058" target="_blank">arXiv:2012.00058</a> [<a href="http://arxiv.org/pdf/2012.00058" target="_blank">pdf</a>]

<h2>FCM-RDpA: TSK Fuzzy Regression Model Construction Using Fuzzy C-Means Clustering, Regularization, DropRule, and Powerball AdaBelief. (arXiv:2012.00060v1 [cs.LG])</h2>
<h3>Zhenhua Shi, Dongrui Wu, Chenfeng Guo, Changming Zhao, Yuqi Cui, Fei-Yue Wang</h3>
<p>To effectively optimize Takagi-Sugeno-Kang (TSK) fuzzy systems for regression
problems, a mini-batch gradient descent with regularization, DropRule, and
AdaBound (MBGD-RDA) algorithm was recently proposed. This paper further
proposes FCM-RDpA, which improves MBGD-RDA by replacing the grid partition
approach in rule initialization by fuzzy c-means clustering, and AdaBound by
Powerball AdaBelief, which integrates recently proposed Powerball gradient and
AdaBelief to further expedite and stabilize parameter optimization. Extensive
experiments on 22 regression datasets with various sizes and dimensionalities
validated the superiority of FCM-RDpA over MBGD-RDA, especially when the
feature dimensionality is higher. We also propose an additional approach,
FCM-RDpAx, that further improves FCM-RDpA by using augmented features in both
the antecedents and consequents of the rules.
</p>
<a href="http://arxiv.org/abs/2012.00060" target="_blank">arXiv:2012.00060</a> [<a href="http://arxiv.org/pdf/2012.00060" target="_blank">pdf</a>]

<h2>Deep reinforcement learning with a particle dynamics environment applied to emergency evacuation of a room with obstacles. (arXiv:2012.00065v1 [cs.LG])</h2>
<h3>Yihao Zhang, Zhaojie Chai, George Lykotrafitis</h3>
<p>A very successful model for simulating emergency evacuation is the
social-force model. At the heart of the model is the self-driven force that is
applied to an agent and is directed towards the exit. However, it is not clear
if the application of this force results in optimal evacuation, especially in
complex environments with obstacles. Here, we develop a deep reinforcement
learning algorithm in association with the social force model to train agents
to find the fastest evacuation path. During training, we penalize every step of
an agent in the room and give zero reward at the exit. We adopt the Dyna-Q
learning approach. We first show that in the case of a room without obstacles
the resulting self-driven force points directly towards the exit as in the
social force model and that the median exit time intervals calculated using the
two methods are not significantly different. Then, we investigate evacuation of
a room with one obstacle and one exit. We show that our method produces similar
results with the social force model when the obstacle is convex. However, in
the case of concave obstacles, which sometimes can act as traps for agents
governed purely by the social force model and prohibit complete room
evacuation, our approach is clearly advantageous since it derives a policy that
results in object avoidance and complete room evacuation without additional
assumptions. We also study evacuation of a room with multiple exits. We show
that agents are able to evacuate efficiently from the nearest exit through a
shared network trained for a single agent. Finally, we test the robustness of
the Dyna-Q learning approach in a complex environment with multiple exits and
obstacles. Overall, we show that our model can efficiently simulate emergency
evacuation in complex environments with multiple room exits and obstacles where
it is difficult to obtain an intuitive rule for fast evacuation.
</p>
<a href="http://arxiv.org/abs/2012.00065" target="_blank">arXiv:2012.00065</a> [<a href="http://arxiv.org/pdf/2012.00065" target="_blank">pdf</a>]

<h2>TimeSHAP: Explaining Recurrent Models through Sequence Perturbations. (arXiv:2012.00073v1 [cs.LG])</h2>
<h3>Jo&#xe3;o Bento, Pedro Saleiro, Andr&#xe9; F. Cruz, M&#xe1;rio A.T. Figueiredo, Pedro Bizarro</h3>
<p>Recurrent neural networks are a standard building block in numerous machine
learning domains, from natural language processing to time-series
classification. While their application has grown ubiquitous, understanding of
their inner workings is still lacking. In practice, the complex decision-making
in these models is seen as a black-box, creating a tension between accuracy and
interpretability. Moreover, the ability to understand the reasoning process of
a model is important in order to debug it and, even more so, to build trust in
its decisions. Although considerable research effort has been guided towards
explaining black-box models in recent years, recurrent models have received
relatively little attention. Any method that aims to explain decisions from a
sequence of instances should assess, not only feature importance, but also
event importance, an ability that is missing from state-of-the-art explainers.
In this work, we contribute to filling these gaps by presenting TimeSHAP, a
model-agnostic recurrent explainer that leverages KernelSHAP's sound
theoretical footing and strong empirical results. As the input sequence may be
arbitrarily long, we further propose a pruning method that is shown to
dramatically improve its efficiency in practice.
</p>
<a href="http://arxiv.org/abs/2012.00073" target="_blank">arXiv:2012.00073</a> [<a href="http://arxiv.org/pdf/2012.00073" target="_blank">pdf</a>]

<h2>Using dynamical quantization to perform split attempts in online tree regressors. (arXiv:2012.00083v1 [cs.LG])</h2>
<h3>Saulo Martiello Mastelini, Andre Carlos Ponce de Leon Ferreira de Carvalho</h3>
<p>A central aspect of online decision tree solutions is evaluating the incoming
data and enabling model growth. For such, trees much deal with different kinds
of input features and partition them to learn from the data. Numerical features
are no exception, and they pose additional challenges compared to other kinds
of features, as there is no trivial strategy to choose the best point to make a
split decision. The problem is even more challenging in regression tasks
because both the features and the target are continuous. Typical online
solutions evaluate and store all the points monitored between split attempts,
which goes against the constraints posed in real-time applications. In this
paper, we introduce the Quantization Observer (QO), a simple yet effective
hashing-based algorithm to monitor and evaluate split point candidates in
numerical features for online tree regressors. QO can be easily integrated into
incremental decision trees, such as Hoeffding Trees, and it has a monitoring
cost of $O(1)$ per instance and sub-linear cost to evaluate split candidates.
Previous solutions had a $O(\log n)$ cost per insertion (in the best case) and
a linear cost to evaluate split points. Our extensive experimental setup
highlights QO's effectiveness in providing accurate split point suggestions
while spending much less memory and processing time than its competitors.
</p>
<a href="http://arxiv.org/abs/2012.00083" target="_blank">arXiv:2012.00083</a> [<a href="http://arxiv.org/pdf/2012.00083" target="_blank">pdf</a>]

<h2>Nothing But Geometric Constraints: A Model-Free Method for Articulated Object Pose Estimation. (arXiv:2012.00088v1 [cs.CV])</h2>
<h3>Qihao Liu, Weichao Qiu, Weiyao Wang, Gregory D. Hager, Alan L. Yuille</h3>
<p>We propose an unsupervised vision-based system to estimate the joint
configurations of the robot arm from a sequence of RGB or RGB-D images without
knowing the model a priori, and then adapt it to the task of
category-independent articulated object pose estimation. We combine a classical
geometric formulation with deep learning and extend the use of epipolar
constraint to multi-rigid-body systems to solve this task. Given a video
sequence, the optical flow is estimated to get the pixel-wise dense
correspondences. After that, the 6D pose is computed by a modified PnP
algorithm. The key idea is to leverage the geometric constraints and the
constraint between multiple frames. Furthermore, we build a synthetic dataset
with different kinds of robots and multi-joint articulated objects for the
research of vision-based robot control and robotic vision. We demonstrate the
effectiveness of our method on three benchmark datasets and show that our
method achieves higher accuracy than the state-of-the-art supervised methods in
estimating joint angles of robot arms and articulated objects.
</p>
<a href="http://arxiv.org/abs/2012.00088" target="_blank">arXiv:2012.00088</a> [<a href="http://arxiv.org/pdf/2012.00088" target="_blank">pdf</a>]

<h2>Contagion Dynamics for Manifold Learning. (arXiv:2012.00091v1 [stat.ML])</h2>
<h3>Barbara I. Mahler</h3>
<p>Contagion maps exploit activation times in threshold contagions to assign
vectors in high-dimensional Euclidean space to the nodes of a network. A point
cloud that is the image of a contagion map reflects both the structure
underlying the network and the spreading behaviour of the contagion on it.
Intuitively, such a point cloud exhibits features of the network's underlying
structure if the contagion spreads along that structure, an observation which
suggests contagion maps as a viable manifold-learning technique. We test
contagion maps as a manifold-learning tool on a number of different real-world
and synthetic data sets, and we compare their performance to that of Isomap,
one of the most well-known manifold-learning algorithms. We find that, under
certain conditions, contagion maps are able to reliably detect underlying
manifold structure in noisy data, while Isomap fails due to noise-induced
error. This consolidates contagion maps as a technique for manifold learning.
</p>
<a href="http://arxiv.org/abs/2012.00091" target="_blank">arXiv:2012.00091</a> [<a href="http://arxiv.org/pdf/2012.00091" target="_blank">pdf</a>]

<h2>Why model why? Assessing the strengths and limitations of LIME. (arXiv:2012.00093v1 [cs.LG])</h2>
<h3>J&#xfc;rgen Dieber, Sabrina Kirrane</h3>
<p>When it comes to complex machine learning models, commonly referred to as
black boxes, understanding the underlying decision making process is crucial
for domains such as healthcare and financial services, and also when it is used
in connection with safety critical systems such as autonomous vehicles. As such
interest in explainable artificial intelligence (xAI) tools and techniques has
increased in recent years. However, the effectiveness of existing xAI
frameworks, especially concerning algorithms that work with data as opposed to
images, is still an open research question. In order to address this gap, in
this paper we examine the effectiveness of the Local Interpretable
Model-Agnostic Explanations (LIME) xAI framework, one of the most popular model
agnostic frameworks found in the literature, with a specific focus on its
performance in terms of making tabular models more interpretable. In
particular, we apply several state of the art machine learning algorithms on a
tabular dataset, and demonstrate how LIME can be used to supplement
conventional performance assessment methods. In addition, we evaluate the
understandability of the output produced by LIME both via a usability study,
involving participants who are not familiar with LIME, and its overall
usability via an assessment framework, which is derived from the International
Organisation for Standardisation 9241-11:1998 standard.
</p>
<a href="http://arxiv.org/abs/2012.00093" target="_blank">arXiv:2012.00093</a> [<a href="http://arxiv.org/pdf/2012.00093" target="_blank">pdf</a>]

<h2>Multi-Modal Detection of Alzheimer's Disease from Speech and Text. (arXiv:2012.00096v1 [cs.LG])</h2>
<h3>Amish Mittal, Sourav Sahoo, Arnhav Datar, Juned Kadiwala, Hrithwik Shalu, Jimson Mathew</h3>
<p>Reliable detection of the prodromal stages of Alzheimer's disease (AD)
remains difficult even today because, unlike other neurocognitive impairments,
there is no definitive diagnosis of AD in vivo. In this context, existing
research has shown that patients often develop language impairment even in mild
AD conditions. We propose a multimodal deep learning method that utilizes
speech and the corresponding transcript simultaneously to detect AD. For audio
signals, the proposed audio-based network, a convolutional neural network (CNN)
based model, predicts the diagnosis for multiple speech segments, which are
combined for the final prediction. Similarly, we use contextual embedding
extracted from BERT concatenated with a CNN-generated embedding for classifying
the transcript. The individual predictions of the two models are then combined
to make the final classification. We also perform experiments to analyze the
model performance when Automated Speech Recognition (ASR) system generated
transcripts are used instead of manual transcription in the text-based model.
The proposed method achieves 85.3% 10-fold cross-validation accuracy when
trained and evaluated on the Dementiabank Pitt corpus.
</p>
<a href="http://arxiv.org/abs/2012.00096" target="_blank">arXiv:2012.00096</a> [<a href="http://arxiv.org/pdf/2012.00096" target="_blank">pdf</a>]

<h2>Towards Auditability for Fairness in Deep Learning. (arXiv:2012.00106v1 [cs.LG])</h2>
<h3>Ivoline C. Ngong, Krystal Maughan, Joseph P. Near</h3>
<p>Group fairness metrics can detect when a deep learning model behaves
differently for advantaged and disadvantaged groups, but even models that score
well on these metrics can make blatantly unfair predictions. We present smooth
prediction sensitivity, an efficiently computed measure of individual fairness
for deep learning models that is inspired by ideas from interpretability in
deep learning. smooth prediction sensitivity allows individual predictions to
be audited for fairness. We present preliminary experimental results suggesting
that smooth prediction sensitivity can help distinguish between fair and unfair
predictions, and that it may be helpful in detecting blatantly unfair
predictions from "group-fair" models.
</p>
<a href="http://arxiv.org/abs/2012.00106" target="_blank">arXiv:2012.00106</a> [<a href="http://arxiv.org/pdf/2012.00106" target="_blank">pdf</a>]

<h2>Representing and Denoising Wearable ECG Recordings. (arXiv:2012.00110v1 [stat.ML])</h2>
<h3>Jeffrey Chan, Andrew C. Miller, Emily B. Fox</h3>
<p>Modern wearable devices are embedded with a range of noninvasive biomarker
sensors that hold promise for improving detection and treatment of disease. One
such sensor is the single-lead electrocardiogram (ECG) which measures
electrical signals in the heart. The benefits of the sheer volume of ECG
measurements with rich longitudinal structure made possible by wearables come
at the price of potentially noisier measurements compared to clinical ECGs,
e.g., due to movement. In this work, we develop a statistical model to simulate
a structured noise process in ECGs derived from a wearable sensor, design a
beat-to-beat representation that is conducive for analyzing variation, and
devise a factor analysis-based method to denoise the ECG. We study synthetic
data generated using a realistic ECG simulator and a structured noise model. At
varying levels of signal-to-noise, we quantitatively measure an upper bound on
performance and compare estimates from linear and non-linear models. Finally,
we apply our method to a set of ECGs collected by wearables in a mobile health
study.
</p>
<a href="http://arxiv.org/abs/2012.00110" target="_blank">arXiv:2012.00110</a> [<a href="http://arxiv.org/pdf/2012.00110" target="_blank">pdf</a>]

<h2>The FEDHC Bayesian network learning algorithm. (arXiv:2012.00113v1 [stat.ML])</h2>
<h3>Michail Tsagris</h3>
<p>The paper proposes a new hybrid Bayesian network learning algorithm, termed
Forward Early Dropping Hill Climbing (FEDHC), designed to work with either
continuous or categorical data. FEDHC consists of a skeleton identification
phase (learning the conditional associations among the variables) followed by
the scoring phase that assigns the causal directions. Specifically for the case
of continuous data, a robust to outliers version of FEDHC is also proposed. The
paper manifests that the only implementation of MMHC in the statistical
software \textit{R}, is prohibitively expensive and a new implementation is
offered. The FEDHC is tested via Monte Carlo simulations that distinctly show
it is computationally efficient, and produces Bayesian networks of similar to,
or of higher accuracy than MMHC and PCHC. FEDHC yields more accurate Bayesian
networks than PCHC with continuous data but less accurate with categorical
data. Finally, an application of FEDHC, PCHC and MMHC algorithms to real data,
from the field of economics, is demonstrated using the statistical software
\textit{R}.
</p>
<a href="http://arxiv.org/abs/2012.00113" target="_blank">arXiv:2012.00113</a> [<a href="http://arxiv.org/pdf/2012.00113" target="_blank">pdf</a>]

<h2>Dynamic Image for 3D MRI Image Alzheimer's Disease Classification. (arXiv:2012.00119v1 [cs.CV])</h2>
<h3>Xin Xing, Gongbo Liang, Hunter Blanton, Muhammad Usman Rafique, Chris Wang, Ai-Ling Lin, Nathan Jacobs</h3>
<p>We propose to apply a 2D CNN architecture to 3D MRI image Alzheimer's disease
classification. Training a 3D convolutional neural network (CNN) is
time-consuming and computationally expensive. We make use of approximate rank
pooling to transform the 3D MRI image volume into a 2D image to use as input to
a 2D CNN. We show our proposed CNN model achieves $9.5\%$ better Alzheimer's
disease classification accuracy than the baseline 3D models. We also show that
our method allows for efficient training, requiring only 20% of the training
time compared to 3D CNN models. The code is available online:
https://github.com/UkyVision/alzheimer-project.
</p>
<a href="http://arxiv.org/abs/2012.00119" target="_blank">arXiv:2012.00119</a> [<a href="http://arxiv.org/pdf/2012.00119" target="_blank">pdf</a>]

<h2>A Hypergradient Approach to Robust Regression without Correspondence. (arXiv:2012.00123v1 [cs.LG])</h2>
<h3>Yujia Xie, Yixiu Mao, Simiao Zuo, Hongteng Xu, Xiaojing Ye, Tuo Zhao, Hongyuan Zha</h3>
<p>We consider a regression problem, where the correspondence between input and
output data is not available. Such shuffled data is commonly observed in many
real world problems. Taking flow cytometry as an example, the measuring
instruments are unable to preserve the correspondence between the samples and
the measurements. Due to the combinatorial nature, most of existing methods are
only applicable when the sample size is small, and limited to linear regression
models. To overcome such bottlenecks, we propose a new computational framework
- ROBOT- for the shuffled regression problem, which is applicable to large data
and complex models. Specifically, we propose to formulate the regression
without correspondence as a continuous optimization problem. Then by exploiting
the interaction between the regression model and the data correspondence, we
propose to develop a hypergradient approach based on differentiable programming
techniques. Such a hypergradient approach essentially views the data
correspondence as an operator of the regression, and therefore allows us to
find a better descent direction for the model parameter by differentiating
through the data correspondence. ROBOT is quite general, and can be further
extended to the inexact correspondence setting, where the input and output data
are not necessarily exactly aligned. Thorough numerical experiments show that
ROBOT achieves better performance than existing methods in both linear and
nonlinear regression tasks, including real-world applications such as flow
cytometry and multi-object tracking.
</p>
<a href="http://arxiv.org/abs/2012.00123" target="_blank">arXiv:2012.00123</a> [<a href="http://arxiv.org/pdf/2012.00123" target="_blank">pdf</a>]

<h2>Utilizing UNet for the future traffic map prediction task Traffic4cast challenge 2020. (arXiv:2012.00125v1 [cs.CV])</h2>
<h3>Sungbin Choi</h3>
<p>This paper describes our UNet based experiments on the Traffic4cast challenge
2020. Similar to the Traffic4cast challenge 2019, the task is to predict
traffic flow volume, direction and speed on a high resolution map of three
large cities worldwide. We mainly experimented with UNet based deep
convolutional networks with various compositions of densely connected
convolution layers, average pooling layers and max pooling layers. Three base
UNet model types are tried and predictions are combined by averaging prediction
scores or taking median value. Our method achieved best performance in this
years newly built challenge dataset.
</p>
<a href="http://arxiv.org/abs/2012.00125" target="_blank">arXiv:2012.00125</a> [<a href="http://arxiv.org/pdf/2012.00125" target="_blank">pdf</a>]

<h2>HydroNet: Benchmark Tasks for Preserving Intermolecular Interactions and Structural Motifs in Predictive and Generative Models for Molecular Data. (arXiv:2012.00131v1 [cs.LG])</h2>
<h3>Sutanay Choudhury, Jenna A. Bilbrey, Logan Ward, Sotiris S. Xantheas, Ian Foster, Joseph P. Heindel, Ben Blaiszik, Marcus E. Schwarting</h3>
<p>Intermolecular and long-range interactions are central to phenomena as
diverse as gene regulation, topological states of quantum materials,
electrolyte transport in batteries, and the universal solvation properties of
water. We present a set of challenge problems for preserving intermolecular
interactions and structural motifs in machine-learning approaches to chemical
problems, through the use of a recently published dataset of 4.95 million water
clusters held together by hydrogen bonding interactions and resulting in longer
range structural patterns. The dataset provides spatial coordinates as well as
two types of graph representations, to accommodate a variety of
machine-learning practices.
</p>
<a href="http://arxiv.org/abs/2012.00131" target="_blank">arXiv:2012.00131</a> [<a href="http://arxiv.org/pdf/2012.00131" target="_blank">pdf</a>]

<h2>Robust error bounds for quantised and pruned neural networks. (arXiv:2012.00138v1 [cs.LG])</h2>
<h3>Jiaqi Li, Ross Drummond, Stephen R. Duncan</h3>
<p>With the rise of smartphones and the internet-of-things, data is increasingly
getting generated at the edge on local, personal devices. For privacy, latency
and energy saving reasons, this shift is causing machine learning algorithms to
move towards a decentralised approach, with the data and algorithms stored and
even trained locally on devices. The device hardware becomes the main
bottleneck for model performance in this set-up, creating a need for slimmed
down, more efficient neural networks. Neural network pruning and quantisation
are two methods that have been developed to achieve this, with both approaches
demonstrating impressive results in reducing the computational cost without
sacrificing too much on model performance. However, our understanding behind
these methods remains underdeveloped. To address this issue, a semi-definite
program to robustly bound the error caused by pruning and quantising a neural
network is introduced in this paper. The method can be applied to generic
neural networks, accounts for the many nonlinearities of the problem and holds
robustly for all inputs in specified sets. It is hoped that the computed bounds
will give certainty to software/control/machine learning engineers implementing
these algorithms efficiently on limited hardware.
</p>
<a href="http://arxiv.org/abs/2012.00138" target="_blank">arXiv:2012.00138</a> [<a href="http://arxiv.org/pdf/2012.00138" target="_blank">pdf</a>]

<h2>Task Allocation for Asynchronous Mobile Edge Learning with Delay and Energy Constraints. (arXiv:2012.00143v1 [cs.LG])</h2>
<h3>Umair Mohammad, Sameh Sorour, Mohamed Hefeida</h3>
<p>This paper extends the paradigm of "mobile edge learning (MEL)" by designing
an optimal task allocation scheme for training a machine learning model in an
asynchronous manner across mutiple edge nodes or learners connected via a
resource-constrained wireless edge network. The optimization is done such that
the portion of the task allotted to each learner is completed within a given
global delay constraint and a local maximum energy consumption limit. The time
and energy consumed are related directly to the heterogeneous communication and
computational capabilities of the learners; i.e. the proposed model is
heterogeneity aware (HA). Because the resulting optimization is an NP-hard
quadratically-constrained integer linear program (QCILP), a two-step
suggest-and-improve (SAI) solution is proposed based on using the solution of
the relaxed synchronous problem to obtain the solution to the asynchronous
problem. The proposed HA asynchronous (HA-Asyn) approach is compared against
the HA synchronous (HA-Sync) scheme and the heterogeneity unaware (HU) equal
batch allocation scheme. Results from a system of 20 learners tested for
various completion time and energy consumption constraints show that the
proposed HA-Asyn method works better than the HU synchronous/asynchronous
(HU-Sync/Asyn) approach and can provide gains of up-to 25\% compared to the
HA-Sync scheme.
</p>
<a href="http://arxiv.org/abs/2012.00143" target="_blank">arXiv:2012.00143</a> [<a href="http://arxiv.org/pdf/2012.00143" target="_blank">pdf</a>]

<h2>Improved Diagnosis of Tibiofemoral Cartilage Defects on MRI Images Using Deep Learning. (arXiv:2012.00144v1 [cs.CV])</h2>
<h3>Gergo Merkely, Alireza Borjali, Molly Zgoda, Evan M. Farina, Simon Gortz, Orhun Muratoglu, Christian Lattermann, Kartik M. Varadarajan</h3>
<p>Background: MRI is the modality of choice for cartilage imaging; however, its
diagnostic performance is variable and significantly lower than the gold
standard diagnostic knee arthroscopy. In recent years, deep learning has been
used to automatically interpret medical images to improve diagnostic accuracy
and speed. Purpose: The primary purpose of this study was to evaluate whether
deep learning applied to the interpretation of knee MRI images can be utilized
to identify cartilage defects accurately. Methods: We analyzed data from
patients who underwent knee MRI evaluation and consequently had arthroscopic
knee surgery (207 with cartilage defect, 90 without cartilage defect).
Patients' arthroscopic findings were compared to preoperative MRI images to
verify the presence or absence of isolated tibiofemoral cartilage defects. We
developed three convolutional neural networks (CNNs) to analyze the MRI images
and implemented image-specific saliency maps to visualize the CNNs'
decision-making process. To compare the CNNs' performance against human
interpretation, the same test dataset images were provided to an experienced
orthopaedic surgeon and an orthopaedic resident. Results: Saliency maps
demonstrated that the CNNs learned to focus on the clinically relevant areas of
the tibiofemoral articular cartilage on MRI images during the decision-making
processes. One CNN achieved higher performance than the orthopaedic surgeon,
with two more accurate diagnoses made by the CNN. All the CNNs outperformed the
orthopaedic resident. Conclusion: CNN can be used to enhance the diagnostic
performance of MRI in identifying isolated tibiofemoral cartilage defects and
may replace diagnostic knee arthroscopy in certain cases in the future.
</p>
<a href="http://arxiv.org/abs/2012.00144" target="_blank">arXiv:2012.00144</a> [<a href="http://arxiv.org/pdf/2012.00144" target="_blank">pdf</a>]

<h2>MUSCLE: Strengthening Semi-Supervised Learning Via Concurrent Unsupervised Learning Using Mutual Information Maximization. (arXiv:2012.00150v1 [cs.LG])</h2>
<h3>Hanchen Xie, Mohamed E. Hussein, Aram Galstyan, Wael Abd-Almageed</h3>
<p>Deep neural networks are powerful, massively parameterized machine learning
models that have been shown to perform well in supervised learning tasks.
However, very large amounts of labeled data are usually needed to train deep
neural networks. Several semi-supervised learning approaches have been proposed
to train neural networks using smaller amounts of labeled data with a large
amount of unlabeled data. The performance of these semi-supervised methods
significantly degrades as the size of labeled data decreases. We introduce
Mutual-information-based Unsupervised &amp; Semi-supervised Concurrent LEarning
(MUSCLE), a hybrid learning approach that uses mutual information to combine
both unsupervised and semi-supervised learning. MUSCLE can be used as a
stand-alone training scheme for neural networks, and can also be incorporated
into other learning approaches. We show that the proposed hybrid model
outperforms state of the art on several standard benchmarks, including
CIFAR-10, CIFAR-100, and Mini-Imagenet. Furthermore, the performance gain
consistently increases with the reduction in the amount of labeled data, as
well as in the presence of bias. We also show that MUSCLE has the potential to
boost the classification performance when used in the fine-tuning phase for a
model pre-trained only on unlabeled data.
</p>
<a href="http://arxiv.org/abs/2012.00150" target="_blank">arXiv:2012.00150</a> [<a href="http://arxiv.org/pdf/2012.00150" target="_blank">pdf</a>]

<h2>Every Model Learned by Gradient Descent Is Approximately a Kernel Machine. (arXiv:2012.00152v1 [cs.LG])</h2>
<h3>Pedro Domingos</h3>
<p>Deep learning's successes are often attributed to its ability to
automatically discover new representations of the data, rather than relying on
handcrafted features like other learning methods. We show, however, that deep
networks learned by the standard gradient descent algorithm are in fact
mathematically approximately equivalent to kernel machines, a learning method
that simply memorizes the data and uses it directly for prediction via a
similarity function (the kernel). This greatly enhances the interpretability of
deep network weights, by elucidating that they are effectively a superposition
of the training examples. The network architecture incorporates knowledge of
the target function into the kernel. This improved understanding should lead to
better learning algorithms.
</p>
<a href="http://arxiv.org/abs/2012.00152" target="_blank">arXiv:2012.00152</a> [<a href="http://arxiv.org/pdf/2012.00152" target="_blank">pdf</a>]

<h2>An accelerated hybrid data-driven/model-based approach for poroelasticity problems with multi-fidelity multi-physics data. (arXiv:2012.00165v1 [cs.LG])</h2>
<h3>Bahador Bahmani, WaiChing Sun</h3>
<p>We present a hybrid model/model-free data-driven approach to solve
poroelasticity problems. Extending the data-driven modeling framework
originated from Kirchdoerfer and Ortiz (2016), we introduce one model-free and
two hybrid model-based/data-driven formulations capable of simulating the
coupled diffusion-deformation of fluid-infiltrating porous media with different
amounts of available data. To improve the efficiency of the model-free data
search, we introduce a distance-minimized algorithm accelerated by a
k-dimensional tree search. To handle the different fidelities of the solid
elasticity and fluid hydraulic constitutive responses, we introduce a
hybridized model in which either the solid and the fluid solver can switch from
a model-based to a model-free approach depending on the availability and the
properties of the data. Numerical experiments are designed to verify the
implementation and compare the performance of the proposed model to other
alternatives.
</p>
<a href="http://arxiv.org/abs/2012.00165" target="_blank">arXiv:2012.00165</a> [<a href="http://arxiv.org/pdf/2012.00165" target="_blank">pdf</a>]

<h2>A Survey on Principles, Models and Methods for Learning from Irregularly Sampled Time Series: From Discretization to Attention and Invariance. (arXiv:2012.00168v1 [cs.LG])</h2>
<h3>Satya Narayan Shukla, Benjamin M. Marlin</h3>
<p>Irregularly sampled time series data arise naturally in many application
domains including biology, ecology, climate science, astronomy, and health.
Such data represent fundamental challenges to many classical models from
machine learning and statistics due to the presence of non-uniform intervals
between observations. However, there has been significant progress within the
machine learning community over the last decade on developing specialized
models and architectures for learning from irregularly sampled univariate and
multivariate time series data. In this survey, we first describe several axes
along which approaches differ including what data representations they are
based on, what modeling primitives they leverage to deal with the fundamental
problem of irregular sampling, and what inference tasks they are designed to
perform. We then survey the recent literature organized primarily along the
axis of modeling primitives. We describe approaches based on temporal
discretization, interpolation, recurrence, attention, and structural
invariance. We discuss similarities and differences between approaches and
highlight primary strengths and weaknesses.
</p>
<a href="http://arxiv.org/abs/2012.00168" target="_blank">arXiv:2012.00168</a> [<a href="http://arxiv.org/pdf/2012.00168" target="_blank">pdf</a>]

<h2>Deconstructing the Structure of Sparse Neural Networks. (arXiv:2012.00172v1 [cs.LG])</h2>
<h3>Maxwell Van Gelder, Mitchell Wortsman, Kiana Ehsani</h3>
<p>Although sparse neural networks have been studied extensively, the focus has
been primarily on accuracy. In this work, we focus instead on network
structure, and analyze three popular algorithms. We first measure performance
when structure persists and weights are reset to a different random
initialization, thereby extending experiments in Deconstructing Lottery Tickets
(Zhou et al., 2019). This experiment reveals that accuracy can be derived from
structure alone. Second, to measure structural robustness we investigate the
sensitivity of sparse neural networks to further pruning after training,
finding a stark contrast between algorithms. Finally, for a recent dynamic
sparsity algorithm we investigate how early in training the structure emerges.
We find that even after one epoch the structure is mostly determined, allowing
us to propose a more efficient algorithm which does not require dense gradients
throughout training. In looking back at algorithms for sparse neural networks
and analyzing their performance from a different lens, we uncover several
interesting properties and promising directions for future research.
</p>
<a href="http://arxiv.org/abs/2012.00172" target="_blank">arXiv:2012.00172</a> [<a href="http://arxiv.org/pdf/2012.00172" target="_blank">pdf</a>]

<h2>Crowd-Sourced Road Quality Mapping in the Developing World. (arXiv:2012.00179v1 [cs.LG])</h2>
<h3>Benjamin Choi, John Kamalu</h3>
<p>Road networks are among the most essential components of a country's
infrastructure. By facilitating the movement and exchange of goods, people, and
ideas, they support economic and cultural activity both within and across
borders. Up-to-date mapping of the the geographical distribution of roads and
their quality is essential in high-impact applications ranging from land use
planning to wilderness conservation. Mapping presents a particularly pressing
challenge in developing countries, where documentation is poor and
disproportionate amounts of road construction are expected to occur in the
coming decades. We present a new crowd-sourced approach capable of assessing
road quality and identify key challenges and opportunities in the
transferability of deep learning based methods across domains.
</p>
<a href="http://arxiv.org/abs/2012.00179" target="_blank">arXiv:2012.00179</a> [<a href="http://arxiv.org/pdf/2012.00179" target="_blank">pdf</a>]

<h2>Data Preprocessing to Mitigate Bias with Boosted Fair Mollifiers. (arXiv:2012.00188v1 [stat.ML])</h2>
<h3>Alexander Soen, Hisham Husain, Richard Nock</h3>
<p>In a recent paper, Celis et al. (2020) introduced a new approach to fairness
that corrects the data distribution itself. The approach is computationally
appealing, but its approximation guarantees with respect to the target
distribution can be quite loose as they need to rely on a (typically limited)
number of constraints on data-based aggregated statistics; also resulting on a
fairness guarantee which can be data dependent.

Our paper makes use of a mathematical object recently introduced in privacy
-- mollifiers of distributions -- and a popular approach to machine learning --
boosting -- to get an approach in the same lineage as Celis et al. but without
those impediments, including in particular, better guarantees in terms of
accuracy and finer guarantees in terms of fairness. The approach involves
learning the sufficient statistics of an exponential family. When training data
is tabular, it is defined by decision trees whose interpretability can provide
clues on the source of (un)fairness. Experiments display the quality of the
results obtained for simulated and real-world data.
</p>
<a href="http://arxiv.org/abs/2012.00188" target="_blank">arXiv:2012.00188</a> [<a href="http://arxiv.org/pdf/2012.00188" target="_blank">pdf</a>]

<h2>Open Source 3-D Filament Diameter Sensor for Recycling, Winding and Additive Manufacturing Machines. (arXiv:2012.00191v1 [cs.CV])</h2>
<h3>Aliaksei L. Petsiuk, Joshua M. Pearce</h3>
<p>To overcome the challenge of upcycling plastic waste into 3-D printing
filament in the distributed recycling and additive manufacturing systems, this
study designs, builds, tests and validates an open source 3-D filament diameter
sensor for recycling and winding machines. The modular system for multi-axis
optical control of the diameter of the recycled 3-D-printer filament makes it
possible to analyze the surface structure of the processed filament, save the
history of measurements along the entire length of the spool, as well as mark
defective areas. The sensor is developed as an independent module and
integrated into a recyclebot. The diameter sensor was tested on different kinds
of polymers (ABS, PLA) different sources of plastic (recycled 3-D prints and
virgin plastic waste) and different colors including clear plastic. The results
of the diameter measurements using the camera were compared with the manual
measurements, and the measurements obtained with a one-dimensional digital
light caliper. The results found that the developed open source filament
sensing method allows users to obtain significantly more information in
comparison with basic one-dimensional light sensors and using the received data
not only for more accurate diameter measurements, but also for a detailed
analysis of the recycled filament surface. The developed method ensures greater
availability of plastics recycling technologies for the manufacturing community
and stimulates the growth of composite materials creation. The presented system
can greatly enhance the user possibilities and serve as a starting point for a
complete recycling control system that will regulate motor parameters to
achieve the desired filament diameter with acceptable deviations and even
control the extrusion rate on a printer to recover from filament
irregularities.
</p>
<a href="http://arxiv.org/abs/2012.00191" target="_blank">arXiv:2012.00191</a> [<a href="http://arxiv.org/pdf/2012.00191" target="_blank">pdf</a>]

<h2>Solvable Model for Inheriting the Regularization through Knowledge Distillation. (arXiv:2012.00194v1 [cs.LG])</h2>
<h3>Luca Saglietti, Lenka Zdeborov&#xe1;</h3>
<p>In recent years the empirical success of transfer learning with neural
networks has stimulated an increasing interest in obtaining a theoretical
understanding of its core properties. Knowledge distillation where a smaller
neural network is trained using the outputs of a larger neural network is a
particularly interesting case of transfer learning. In the present work, we
introduce a statistical physics framework that allows an analytic
characterization of the properties of knowledge distillation (KD) in shallow
neural networks. Focusing the analysis on a solvable model that exhibits a
non-trivial generalization gap, we investigate the effectiveness of KD. We are
able to show that, through KD, the regularization properties of the larger
teacher model can be inherited by the smaller student and that the yielded
generalization performance is closely linked to and limited by the optimality
of the teacher. Finally, we analyze the double descent phenomenology that can
arise in the considered KD setting.
</p>
<a href="http://arxiv.org/abs/2012.00194" target="_blank">arXiv:2012.00194</a> [<a href="http://arxiv.org/pdf/2012.00194" target="_blank">pdf</a>]

<h2>Profile Prediction: An Alignment-Based Pre-Training Task for Protein Sequence Models. (arXiv:2012.00195v1 [cs.LG])</h2>
<h3>Pascal Sturmfels, Jesse Vig, Ali Madani, Nazneen Fatema Rajani</h3>
<p>For protein sequence datasets, unlabeled data has greatly outpaced labeled
data due to the high cost of wet-lab characterization. Recent deep-learning
approaches to protein prediction have shown that pre-training on unlabeled data
can yield useful representations for downstream tasks. However, the optimal
pre-training strategy remains an open question. Instead of strictly borrowing
from natural language processing (NLP) in the form of masked or autoregressive
language modeling, we introduce a new pre-training task: directly predicting
protein profiles derived from multiple sequence alignments. Using a set of
five, standardized downstream tasks for protein models, we demonstrate that our
pre-training task along with a multi-task objective outperforms masked language
modeling alone on all five tasks. Our results suggest that protein sequence
models may benefit from leveraging biologically-inspired inductive biases that
go beyond existing language modeling techniques in NLP.
</p>
<a href="http://arxiv.org/abs/2012.00195" target="_blank">arXiv:2012.00195</a> [<a href="http://arxiv.org/pdf/2012.00195" target="_blank">pdf</a>]

<h2>Detect, Reject, Correct: Crossmodal Compensation of Corrupted Sensors. (arXiv:2012.00201v1 [cs.RO])</h2>
<h3>Michelle A. Lee, Matthew Tan, Yuke Zhu, Jeannette Bohg</h3>
<p>Using sensor data from multiple modalities presents an opportunity to encode
redundant and complementary features that can be useful when one modality is
corrupted or noisy. Humans do this everyday, relying on touch and
proprioceptive feedback in visually-challenging environments. However, robots
might not always know when their sensors are corrupted, as even broken sensors
can return valid values. In this work, we introduce the Crossmodal Compensation
Model (CCM), which can detect corrupted sensor modalities and compensate for
them. CMM is a representation model learned with self-supervision that
leverages unimodal reconstruction loss for corruption detection. CCM then
discards the corrupted modality and compensates for it with information from
the remaining sensors. We show that CCM learns rich state representations that
can be used for contact-rich manipulation policies, even when input modalities
are corrupted in ways not seen during training time.
</p>
<a href="http://arxiv.org/abs/2012.00201" target="_blank">arXiv:2012.00201</a> [<a href="http://arxiv.org/pdf/2012.00201" target="_blank">pdf</a>]

<h2>Field-wise Learning for Multi-field Categorical Data. (arXiv:2012.00202v1 [cs.LG])</h2>
<h3>Zhibin Li, Jian Zhang, Yongshun Gong, Yazhou Yao, Qiang Wu</h3>
<p>We propose a new method for learning with multi-field categorical data.
Multi-field categorical data are usually collected over many heterogeneous
groups. These groups can reflect in the categories under a field. The existing
methods try to learn a universal model that fits all data, which is challenging
and inevitably results in learning a complex model. In contrast, we propose a
field-wise learning method leveraging the natural structure of data to learn
simple yet efficient one-to-one field-focused models with appropriate
constraints. In doing this, the models can be fitted to each category and thus
can better capture the underlying differences in data. We present a model that
utilizes linear models with variance and low-rank constraints, to help it
generalize better and reduce the number of parameters. The model is also
interpretable in a field-wise manner. As the dimensionality of multi-field
categorical data can be very high, the models applied to such data are mostly
over-parameterized. Our theoretical analysis can potentially explain the effect
of over-parametrization on the generalization of our model. It also supports
the variance constraints in the learning objective. The experiment results on
two large-scale datasets show the superior performance of our model, the trend
of the generalization error bound, and the interpretability of learning
outcomes. Our code is available at
https://github.com/lzb5600/Field-wise-Learning.
</p>
<a href="http://arxiv.org/abs/2012.00202" target="_blank">arXiv:2012.00202</a> [<a href="http://arxiv.org/pdf/2012.00202" target="_blank">pdf</a>]

<h2>How to fine-tune deep neural networks in few-shot learning?. (arXiv:2012.00204v1 [cs.LG])</h2>
<h3>Peng Peng, Jiugen Wang</h3>
<p>Deep learning has been widely used in data-intensive applications. However,
training a deep neural network often requires a large data set. When there is
not enough data available for training, the performance of deep learning models
is even worse than that of shallow networks. It has been proved that few-shot
learning can generalize to new tasks with few training samples. Fine-tuning of
a deep model is simple and effective few-shot learning method. However, how to
fine-tune deep learning models (fine-tune convolution layer or BN layer?) still
lack deep investigation. Hence, we study how to fine-tune deep models through
experimental comparison in this paper. Furthermore, the weight of the models is
analyzed to verify the feasibility of the fine-tuning method.
</p>
<a href="http://arxiv.org/abs/2012.00204" target="_blank">arXiv:2012.00204</a> [<a href="http://arxiv.org/pdf/2012.00204" target="_blank">pdf</a>]

<h2>Toward Accurate Platform-Aware Performance Modeling for Deep Neural Networks. (arXiv:2012.00211v1 [cs.LG])</h2>
<h3>Chuan-Chi Wang, Ying-Chiao Liao, Ming-Chang Kao, Wen-Yew Liang, Shih-Hao Hung</h3>
<p>In this paper, we provide a fine-grain machine learning-based method,
PerfNetV2, which improves the accuracy of our previous work for modeling the
neural network performance on a variety of GPU accelerators. Given an
application, the proposed method can be used to predict the inference time and
training time of the convolutional neural networks used in the application,
which enables the system developer to optimize the performance by choosing the
neural networks and/or incorporating the hardware accelerators to deliver
satisfactory results in time. Furthermore, the proposed method is capable of
predicting the performance of an unseen or non-existing device, e.g. a new GPU
which has a higher operating frequency with less processor cores, but more
memory capacity. This allows a system developer to quickly search the hardware
design space and/or fine-tune the system configuration. Compared to the
previous works, PerfNetV2 delivers more accurate results by modeling detailed
host-accelerator interactions in executing the full neural networks and
improving the architecture of the machine learning model used in the predictor.
Our case studies show that PerfNetV2 yields a mean absolute percentage error
within 13.1% on LeNet, AlexNet, and VGG16 on NVIDIA GTX-1080Ti, while the error
rate on a previous work published in ICBD 2018 could be as large as 200%.
</p>
<a href="http://arxiv.org/abs/2012.00211" target="_blank">arXiv:2012.00211</a> [<a href="http://arxiv.org/pdf/2012.00211" target="_blank">pdf</a>]

<h2>UPFlow: Upsampling Pyramid for Unsupervised Optical Flow Learning. (arXiv:2012.00212v1 [cs.CV])</h2>
<h3>Kunming Luo, Chuan Wang, Shuaicheng Liu, Haoqiang Fan, Jue Wang, Jian Sun</h3>
<p>We present an unsupervised learning approach for optical flow estimation by
improving the upsampling and learning of pyramid network. We design a
self-guided upsample module to tackle the interpolation blur problem caused by
bilinear upsampling between pyramid levels. Moreover, we propose a pyramid
distillation loss to add supervision for intermediate levels via distilling the
finest flow as pseudo labels. By integrating these two components together, our
method achieves the best performance for unsupervised optical flow learning on
multiple leading benchmarks, including MPI-SIntel, KITTI 2012 and KITTI 2015.
In particular, we achieve EPE=1.4 on KITTI 2012 and F1=9.38% on KITTI 2015,
which outperform the previous state-of-the-art methods by 22.2% and 15.7%,
respectively.
</p>
<a href="http://arxiv.org/abs/2012.00212" target="_blank">arXiv:2012.00212</a> [<a href="http://arxiv.org/pdf/2012.00212" target="_blank">pdf</a>]

<h2>Uncertainty-Constrained Differential Dynamic Programming in Belief Space for Vision Based Robots. (arXiv:2012.00218v1 [cs.RO])</h2>
<h3>Shatil Rahman, Steven L. Waslander</h3>
<p>Most mobile robots follow a modular sense-planact system architecture that
can lead to poor performance or even catastrophic failure for visual inertial
navigation systems due to trajectories devoid of feature matches. Planning in
belief space provides a unified approach to tightly couple the perception,
planning and control modules, leading to trajectories that are robust to noisy
measurements and disturbances. However, existing methods handle uncertainties
as costs that require manual tuning for varying environments and hardware. We
therefore propose a novel trajectory optimization formulation that incorporates
inequality constraints on uncertainty and a novel Augmented Lagrangian based
stochastic differential dynamic programming method in belief space.
Furthermore, we develop a probabilistic visibility model that accounts for
discontinuities due to feature visibility limits. Our simulation tests
demonstrate that our method can handle inequality constraints in different
environments, for holonomic and nonholonomic motion models with no manual
tuning of uncertainty costs involved. We also show the improved optimization
performance in belief space due to our visibility model.
</p>
<a href="http://arxiv.org/abs/2012.00218" target="_blank">arXiv:2012.00218</a> [<a href="http://arxiv.org/pdf/2012.00218" target="_blank">pdf</a>]

<h2>Imputation of Missing Data with Class Imbalance using Conditional Generative Adversarial Networks. (arXiv:2012.00220v1 [cs.LG])</h2>
<h3>Saqib Ejaz Awan, Mohammed Bennamoun, Ferdous Sohel, Frank M Sanfilippo, Girish Dwivedi</h3>
<p>Missing data is a common problem faced with real-world datasets. Imputation
is a widely used technique to estimate the missing data. State-of-the-art
imputation approaches, such as Generative Adversarial Imputation Nets (GAIN),
model the distribution of observed data to approximate the missing values. Such
an approach usually models a single distribution for the entire dataset, which
overlooks the class-specific characteristics of the data. Class-specific
characteristics are especially useful when there is a class imbalance. We
propose a new method for imputing missing data based on its class-specific
characteristics by adapting the popular Conditional Generative Adversarial
Networks (CGAN). Our Conditional Generative Adversarial Imputation Network
(CGAIN) imputes the missing data using class-specific distributions, which can
produce the best estimates for the missing values. We tested our approach on
benchmark datasets and achieved superior performance compared with the
state-of-the-art and popular imputation approaches.
</p>
<a href="http://arxiv.org/abs/2012.00220" target="_blank">arXiv:2012.00220</a> [<a href="http://arxiv.org/pdf/2012.00220" target="_blank">pdf</a>]

<h2>Point2Skeleton: Learning Skeletal Representations from Point Clouds. (arXiv:2012.00230v1 [cs.CV])</h2>
<h3>Cheng Lin, Changjian Li, Yuan Liu, Nenglun Chen, Yi-King Choi, Wenping Wang</h3>
<p>We introduce Point2Skeleton, an unsupervised method to learn skeletal
representations from point clouds. Existing skeletonization methods are limited
to tubular shapes and the stringent requirement of watertight input, while our
method aims to produce more generalized skeletal representations for complex
structures and handle point clouds. Our key idea is to use the insights of the
medial axis transform (MAT) to capture the intrinsic geometric and topological
natures of the original input points. We first predict a set of skeletal points
by learning a geometric transformation, and then analyze the connectivity of
the skeletal points to form skeletal mesh structures. Extensive evaluations and
comparisons show our method has superior performance and robustness. The
learned skeletal representation will benefit several unsupervised tasks for
point clouds, such as surface reconstruction and segmentation.
</p>
<a href="http://arxiv.org/abs/2012.00230" target="_blank">arXiv:2012.00230</a> [<a href="http://arxiv.org/pdf/2012.00230" target="_blank">pdf</a>]

<h2>RaP-Net: A Region-wise and Point-wise Weighting Network to Extract Robust Keypoints for Indoor Localization. (arXiv:2012.00234v1 [cs.CV])</h2>
<h3>Dongjiang Li, Jinyu Miao, Xuesong Shi, Yuxin Tian, Qiwei Long, Ping Guo, Hongfei Yu, Wei Yang, Haosong Yue, Qi Wei, Fei Qiao</h3>
<p>Image keypoint extraction is an important step for visual localization. The
localization in indoor environment is challenging for that there may be many
unreliable features on dynamic or repetitive objects. Such kind of reliability
cannot be well learned by existing Convolutional Neural Network (CNN) based
feature extractors. We propose a novel network, RaP-Net, which explicitly
addresses feature invariability with a region-wise predictor, and combines it
with a point-wise predictor to select reliable keypoints in an image. We also
build a new dataset, OpenLORIS-Location, to train this network. The dataset
contains 1553 indoor images with location labels. There are various scene
changes between images on the same location, which can help a network to learn
the invariability in typical indoor scenes. Experimental results show that the
proposed RaP-Net trained with the OpenLORIS-Location dataset significantly
outperforms existing CNN-based keypoint extraction algorithms for indoor
localization. The code and data are available at
https://github.com/ivipsourcecode/RaP-Net.
</p>
<a href="http://arxiv.org/abs/2012.00234" target="_blank">arXiv:2012.00234</a> [<a href="http://arxiv.org/pdf/2012.00234" target="_blank">pdf</a>]

<h2>Sim2Real for Self-Supervised Monocular Depth and Segmentation. (arXiv:2012.00238v1 [cs.CV])</h2>
<h3>Nithin Raghavan, Punarjay Chakravarty, Shubham Shrivastava</h3>
<p>Image-based learning methods for autonomous vehicle perception tasks require
large quantities of labelled, real data in order to properly train without
overfitting, which can often be incredibly costly. While leveraging the power
of simulated data can potentially aid in mitigating these costs, networks
trained in the simulation domain usually fail to perform adequately when
applied to images in the real domain. Recent advances in domain adaptation have
indicated that a shared latent space assumption can help to bridge the gap
between the simulation and real domains, allowing the transference of the
predictive capabilities of a network from the simulation domain to the real
domain. We demonstrate that a twin VAE-based architecture with a shared latent
space and auxiliary decoders is able to bridge the sim2real gap without
requiring any paired, ground-truth data in the real domain. Using only paired,
ground-truth data in the simulation domain, this architecture has the potential
to generate perception tasks such as depth and segmentation maps. We compare
this method to networks trained in a supervised manner to indicate the merit of
these results.
</p>
<a href="http://arxiv.org/abs/2012.00238" target="_blank">arXiv:2012.00238</a> [<a href="http://arxiv.org/pdf/2012.00238" target="_blank">pdf</a>]

<h2>3D Guided Weakly Supervised Semantic Segmentation. (arXiv:2012.00242v1 [cs.CV])</h2>
<h3>Weixuan Sun, Jing Zhang, Nick Barnes</h3>
<p>Pixel-wise clean annotation is necessary for fully-supervised semantic
segmentation, which is laborious and expensive to obtain. In this paper, we
propose a weakly supervised 2D semantic segmentation model by incorporating
sparse bounding box labels with available 3D information, which is much easier
to obtain with advanced sensors. We manually labeled a subset of the 2D-3D
Semantics(2D-3D-S) dataset with bounding boxes, and introduce our 2D-3D
inference module to generate accurate pixel-wise segment proposal masks. Guided
by 3D information, we first generate a point cloud of objects and calculate
objectness probability score for each point. Then we project the point cloud
with objectness probabilities back to 2D images followed by a refinement step
to obtain segment proposals, which are treated as pseudo labels to train a
semantic segmentation network. Our method works in a recursive manner to
gradually refine the above-mentioned segment proposals. Extensive experimental
results on the 2D-3D-S dataset show that the proposed method can generate
accurate segment proposals when bounding box labels are available on only a
small subset of training images. Performance comparison with recent
state-of-the-art methods further illustrates the effectiveness of our method.
</p>
<a href="http://arxiv.org/abs/2012.00242" target="_blank">arXiv:2012.00242</a> [<a href="http://arxiv.org/pdf/2012.00242" target="_blank">pdf</a>]

<h2>A New Action Recognition Framework for Video Highlights Summarization in Sporting Events. (arXiv:2012.00253v1 [cs.CV])</h2>
<h3>Cheng Yan, Xin Li, Guoqiang Li</h3>
<p>To date, machine learning for human action recognition in video has been
widely implemented in sports activities. Although some studies have been
successful in the past, precision is still the most significant concern. In
this study, we present a high-accuracy framework to automatically clip the
sports video stream by using a three-level prediction algorithm based on two
classical open-source structures, i.e., YOLO-v3 and OpenPose. It is found that
by using a modest amount of sports video training data, our methodology can
perform sports activity highlights clipping accurately. Comparing with the
previous systems, our methodology shows some advantages in accuracy. This study
may serve as a new clipping system to extend the potential applications of the
video summarization in sports field, as well as facilitates the development of
match analysis system.
</p>
<a href="http://arxiv.org/abs/2012.00253" target="_blank">arXiv:2012.00253</a> [<a href="http://arxiv.org/pdf/2012.00253" target="_blank">pdf</a>]

<h2>Confluence: A Robust Non-IoU Alternative to Non-Maxima Suppression in Object Detection. (arXiv:2012.00257v1 [cs.CV])</h2>
<h3>Andrew Shepley, Greg Falzon, Paul Kwan</h3>
<p>This paper presents a novel alternative to Greedy Non-Maxima Suppression
(NMS) in the task of bounding box selection and suppression in object
detection. It proposes Confluence, an algorithm which does not rely solely on
individual confidence scores to select optimal bounding boxes, nor does it rely
on Intersection Over Union (IoU) to remove false positives. Using Manhattan
Distance, it selects the bounding box which is closest to every other bounding
box within the cluster and removes highly confluent neighboring boxes. Thus,
Confluence represents a paradigm shift in bounding box selection and
suppression as it is based on fundamentally different theoretical principles to
Greedy NMS and its variants. Confluence is experimentally validated on
RetinaNet, YOLOv3 and Mask-RCNN, using both the MS COCO and PASCAL VOC 2007
datasets. Confluence outperforms Greedy NMS in both mAP and recall on both
datasets, using the challenging 0.50:0.95 mAP evaluation metric. On each
detector and dataset, mAP was improved by 0.3-0.7% while recall was improved by
1.4-2.5%. A theoretical comparison of Greedy NMS and the Confluence Algorithm
is provided, and quantitative results are supported by extensive qualitative
results analysis. Furthermore, sensitivity analysis experiments across mAP
thresholds support the conclusion that Confluence is more robust than NMS.
</p>
<a href="http://arxiv.org/abs/2012.00257" target="_blank">arXiv:2012.00257</a> [<a href="http://arxiv.org/pdf/2012.00257" target="_blank">pdf</a>]

<h2>fairfaceGAN: Fairness-aware Facial Image-to-Image Translation. (arXiv:2012.00282v1 [cs.CV])</h2>
<h3>Sunhee Hwang, Sungho Park, Dohyung Kim, Mirae Do, Hyeran Byun</h3>
<p>In this paper, we introduce FairFaceGAN, a fairness-aware facial
Image-to-Image translation model, mitigating the problem of unwanted
translation in protected attributes (e.g., gender, age, race) during facial
attributes editing. Unlike existing models, FairFaceGAN learns fair
representations with two separate latents - one related to the target
attributes to translate, and the other unrelated to them. This strategy enables
FairFaceGAN to separate the information about protected attributes and that of
target attributes. It also prevents unwanted translation in protected
attributes while target attributes editing. To evaluate the degree of fairness,
we perform two types of experiments on CelebA dataset. First, we compare the
fairness-aware classification performances when augmenting data by existing
image translation methods and FairFaceGAN respectively. Moreover, we propose a
new fairness metric, namely Frechet Protected Attribute Distance (FPAD), which
measures how well protected attributes are preserved. Experimental results
demonstrate that FairFaceGAN shows consistent improvements in terms of fairness
over the existing image translation models. Further, we also evaluate image
translation performances, where FairFaceGAN shows competitive results, compared
to those of existing methods.
</p>
<a href="http://arxiv.org/abs/2012.00282" target="_blank">arXiv:2012.00282</a> [<a href="http://arxiv.org/pdf/2012.00282" target="_blank">pdf</a>]

<h2>Visual Identification of Articulated Object Parts. (arXiv:2012.00284v1 [cs.RO])</h2>
<h3>Vicky Zeng, Timothy E. Lee, Jacky Liang, Oliver Kroemer</h3>
<p>As autonomous robots interact and navigate around real-world environments
such as homes, it is useful to reliably identify and manipulate articulated
objects, such as doors and cabinets. Many prior works in object articulation
identification require manipulation of the object, either by the robot or a
human. While recent works have addressed predicting articulation types from
visual observations alone, they often assume prior knowledge of category-level
kinematic motion models or sequence of observations where the articulated parts
are moving according to their kinematic constraints. In this work, we propose
training a neural network through large-scale domain randomization to identify
the articulation type of object parts from a single image observation. Training
data is generated via photorealistic rendering in simulation. Our proposed
model predicts motion residual flows of object parts, and these residuals are
used to determine the articulation type and parameters. We train the network on
six object categories with 149 objects and 100K rendered images, achieving an
accuracy of 82.5%. Experiments show our method generalizes to novel object
categories in simulation and can be applied to real-world images without
fine-tuning.
</p>
<a href="http://arxiv.org/abs/2012.00284" target="_blank">arXiv:2012.00284</a> [<a href="http://arxiv.org/pdf/2012.00284" target="_blank">pdf</a>]

<h2>MILP-based Imitation Learning for HVAC control. (arXiv:2012.00286v1 [cs.LG])</h2>
<h3>Huy Truong Dinh, Daehee Kim</h3>
<p>To optimize the operation of a HVAC system with advanced techniques such as
artificial neural network, previous studies usually need forecast information
in their method. However, the forecast information inevitably contains errors
all the time, which degrade the performance of the HVAC operation. Hence, in
this study, we propose MILP-based imitation learning method to control a HVAC
system without using the forecast information in order to reduce energy cost
and maintain thermal comfort at a given level. Our proposed controller is a
deep neural network (DNN) trained by using data labeled by a MILP solver with
historical data. After training, our controller is used to control the HVAC
system with real-time data. For comparison, we also develop a second method
named forecast-based MILP which control the HVAC system using the forecast
information. The performance of the two methods is verified by using real
outdoor temperatures and real day-ahead prices in Detroit city, Michigan,
United States. Numerical results clearly show that the performance of the
MILP-based imitation learning is better than that of the forecast-based MILP
method in terms of hourly power consumption, daily energy cost, and thermal
comfort. Moreover, the difference between results of the MILP-based imitation
learning method and optimal results is almost negligible. These optimal results
are achieved only by using the MILP solver at the end of a day when we have
full information on the weather and prices for the day.
</p>
<a href="http://arxiv.org/abs/2012.00286" target="_blank">arXiv:2012.00286</a> [<a href="http://arxiv.org/pdf/2012.00286" target="_blank">pdf</a>]

<h2>CycleGAN without checkerboard artifacts for counter-forensics of fake-image detection. (arXiv:2012.00287v1 [cs.CV])</h2>
<h3>Takayuki Osakabe, Miki Tanaka, Yuma Kinoshita, Hitoshi Kiya</h3>
<p>In this paper, we propose a novel CycleGAN without checkerboard artifacts for
counter-forensics of fake-image detection. Recent rapid advances in image
manipulation tools and deep image synthesis techniques, such as Generative
Adversarial Networks (GANs) have easily generated fake images, so detecting
manipulated images has become an urgent issue. Most state-of-the-art forgery
detection methods assume that images include checkerboard artifacts which are
generated by using DNNs. Accordingly, we propose a novel CycleGAN without any
checkerboard artifacts for counter-forensics of fake-mage detection methods for
the first time, as an example of GANs without checkerboard artifacts.
</p>
<a href="http://arxiv.org/abs/2012.00287" target="_blank">arXiv:2012.00287</a> [<a href="http://arxiv.org/pdf/2012.00287" target="_blank">pdf</a>]

<h2>End-to-End UAV Simulation for Visual SLAM and Navigation. (arXiv:2012.00298v1 [cs.RO])</h2>
<h3>S. Chen, H. Chen, W. Zhou, C.-Y. Wen, B. Li</h3>
<p>Visual Simultaneous Localization and Mapping (v-SLAM) and navigation of
multirotor Unmanned Aerial Vehicles (UAV) in an unknown environment have grown
in popularity for both research and education. However, due to the complex
hardware setup, safety precautions, and battery constraints, extensive physical
testing can be expensive and time-consuming. As an alternative solution,
simulation tools lower the barrier to carry out the algorithm testing and
validation before field trials. In this letter, we customize the ROS-Gazebo-PX4
simulator in deep and provide an end-to-end simulation solution for the UAV
v-SLAM and navigation study. A set of localization, mapping, and path planning
kits were also integrated into the simulation platform. In our simulation,
various aspects, including complex environments and onboard sensors, can
simultaneously interact with our navigation framework to achieve specific
surveillance missions. In this end-to-end simulation, we achieved click and fly
level autonomy UAV navigation. The source code is open to the research
community.
</p>
<a href="http://arxiv.org/abs/2012.00298" target="_blank">arXiv:2012.00298</a> [<a href="http://arxiv.org/pdf/2012.00298" target="_blank">pdf</a>]

<h2>Dual Pixel Exploration: Simultaneous Depth Estimation and Image Restoration. (arXiv:2012.00301v1 [cs.CV])</h2>
<h3>Liyuan Pan, Shah Chowdhury, Richard Hartley, Miaomiao Liu, Hongguang Zhang, Hongdong Li</h3>
<p>The dual-pixel (DP) hardware works by splitting each pixel in half and
creating an image pair in a single snapshot. Several works estimate
depth/inverse depth by treating the DP pair as a stereo pair. However,
dual-pixel disparity only occurs in image regions with the defocus blur. The
heavy defocus blur in DP pairs affects the performance of matching-based depth
estimation approaches. Instead of removing the blur effect blindly, we study
the formation of the DP pair which links the blur and the depth information. In
this paper, we propose a mathematical DP model which can benefit depth
estimation by the blur. These explorations motivate us to propose an end-to-end
DDDNet (DP-based Depth and Deblur Network) to jointly estimate the depth and
restore the image. Moreover, we define a reblur loss, which reflects the
relationship of the DP image formation process with depth information, to
regularise our depth estimate in training. To meet the requirement of a large
amount of data for learning, we propose the first DP image simulator which
allows us to create datasets with DP pairs from any existing RGBD dataset. As a
side contribution, we collect a real dataset for further research. Extensive
experimental evaluation on both synthetic and real datasets shows that our
approach achieves competitive performance compared to state-of-the-art
approaches.
</p>
<a href="http://arxiv.org/abs/2012.00301" target="_blank">arXiv:2012.00301</a> [<a href="http://arxiv.org/pdf/2012.00301" target="_blank">pdf</a>]

<h2>A Stitching Algorithm for Automated Surface Inspection of Rotationally Symmetric Components. (arXiv:2012.00308v1 [cs.CV])</h2>
<h3>Tobias Schlagenhauf, Tim Brander, Juergen Fleischer</h3>
<p>This paper provides a novel approach to stitching surface images of
rotationally symmetric parts. It presents a process pipeline that uses a
feature-based stitching approach to create a distortion-free and true-to-life
image from a video file. The developed process thus enables, for example,
condition monitoring without having to view many individual images. For
validation purposes, this will be demonstrated in the paper using the concrete
example of a worn ball screw drive spindle. The developed algorithm aims at
reproducing the functional principle of a line scan camera system, whereby the
physical measuring systems are replaced by a feature-based approach. For
evaluation of the stitching algorithms, metrics are used, some of which have
only been developed in this work or have been supplemented by test procedures
already in use. The applicability of the developed algorithm is not only
limited to machine tool spindles. Instead, the developed method allows a
general approach to the surface inspection of various rotationally symmetric
components and can therefore be used in a variety of industrial applications.
Deep-learning-based detection Algorithms can easily be implemented to generate
a complete pipeline for failure detection and condition monitoring on
rotationally symmetric parts.
</p>
<a href="http://arxiv.org/abs/2012.00308" target="_blank">arXiv:2012.00308</a> [<a href="http://arxiv.org/pdf/2012.00308" target="_blank">pdf</a>]

<h2>Unsupervised Part Discovery via Feature Alignment. (arXiv:2012.00313v1 [cs.CV])</h2>
<h3>Mengqi Guo, Yutong Bai, Zhishuai Zhang, Adam Kortylewski, Alan Yuille</h3>
<p>Understanding objects in terms of their individual parts is important,
because it enables a precise understanding of the objects' geometrical
structure, and enhances object recognition when the object is seen in a novel
pose or under partial occlusion. However, the manual annotation of parts in
large scale datasets is time consuming and expensive. In this paper, we aim at
discovering object parts in an unsupervised manner, i.e., without ground-truth
part or keypoint annotations. Our approach builds on the intuition that objects
of the same class in a similar pose should have their parts aligned at similar
spatial locations. We exploit the property that neural network features are
largely invariant to nuisance variables and the main remaining source of
variations between images of the same object category is the object pose.
Specifically, given a training image, we find a set of similar images that show
instances of the same object category in the same pose, through an affine
alignment of their corresponding feature maps. The average of the aligned
feature maps serves as pseudo ground-truth annotation for a supervised training
of the deep network backbone. During inference, part detection is simple and
fast, without any extra modules or overheads other than a feed-forward neural
network. Our experiments on several datasets from different domains verify the
effectiveness of the proposed method. For example, we achieve 37.8 mAP on
VehiclePart, which is at least 4.2 better than previous methods.
</p>
<a href="http://arxiv.org/abs/2012.00313" target="_blank">arXiv:2012.00313</a> [<a href="http://arxiv.org/pdf/2012.00313" target="_blank">pdf</a>]

<h2>Decentralized Multi-Agent Linear Bandits with Safety Constraints. (arXiv:2012.00314v1 [cs.LG])</h2>
<h3>Sanae Amani, Christos Thrampoulidis</h3>
<p>We study decentralized stochastic linear bandits, where a network of $N$
agents acts cooperatively to efficiently solve a linear bandit-optimization
problem over a $d$-dimensional space. For this problem, we propose DLUCB: a
fully decentralized algorithm that minimizes the cumulative regret over the
entire network. At each round of the algorithm each agent chooses its actions
following an upper confidence bound (UCB) strategy and agents share information
with their immediate neighbors through a carefully designed consensus procedure
that repeats over cycles. Our analysis adjusts the duration of these
communication cycles ensuring near-optimal regret performance
$\mathcal{O}(d\log{NT}\sqrt{NT})$ at a communication rate of
$\mathcal{O}(dN^2)$ per round. The structure of the network affects the regret
performance via a small additive term - coined the regret of delay - that
depends on the spectral gap of the underlying graph. Notably, our results apply
to arbitrary network topologies without a requirement for a dedicated agent
acting as a server. In consideration of situations with high communication
cost, we propose RC-DLUCB: a modification of DLUCB with rare communication
among agents. The new algorithm trades off regret performance for a
significantly reduced total communication cost of $\mathcal{O}(d^3N^{2.5})$
over all $T$ rounds. Finally, we show that our ideas extend naturally to the
emerging, albeit more challenging, setting of safe bandits. For the recently
studied problem of linear bandits with unknown linear safety constraints, we
propose the first safe decentralized algorithm. Our study contributes towards
applying bandit techniques in safety-critical distributed systems that
repeatedly deal with unknown stochastic environments. We present numerical
simulations for various network topologies that corroborate our theoretical
findings.
</p>
<a href="http://arxiv.org/abs/2012.00314" target="_blank">arXiv:2012.00314</a> [<a href="http://arxiv.org/pdf/2012.00314" target="_blank">pdf</a>]

<h2>Fast and Robust Bin-picking System for Densely Piled Industrial Objects. (arXiv:2012.00316v1 [cs.RO])</h2>
<h3>Jiaxin Guo, Lian Fu, Mingkai Jia, Kaijun Wang, Shan Liu</h3>
<p>Objects grasping, also known as the bin-picking, is one of the most common
tasks faced by industrial robots. While much work has been done in related
topics, grasping randomly piled objects still remains a challenge because much
of the existing work either lack robustness or costs too much resource. In this
paper, we develop a fast and robust bin-picking system for grasping densely
piled objects adaptively and safely. The proposed system starts with point
cloud segmentation using improved density-based spatial clustering of
application with noise (DBSCAN) algorithm, which is improved by combining the
region growing algorithm and using Octree to speed up the calculation. The
system then uses principle component analysis (PCA) for coarse registration and
iterative closest point (ICP) for fine registration. We propose a grasp risk
score (GRS) to evaluate each object by the collision probability, the stability
of the object, and the whole pile's stability. Through real tests with the Anno
robot, our method is verified to be advanced in speed and robustness.
</p>
<a href="http://arxiv.org/abs/2012.00316" target="_blank">arXiv:2012.00316</a> [<a href="http://arxiv.org/pdf/2012.00316" target="_blank">pdf</a>]

<h2>Diverse Temporal Aggregation and Depthwise Spatiotemporal Factorization for Efficient Video Classification. (arXiv:2012.00317v1 [cs.CV])</h2>
<h3>Youngwan Lee, Hyung-Il Kim, Kimin Yun, Jinyoung Moon</h3>
<p>Video classification researches that have recently attracted attention are
the fields of temporal modeling and 3D efficient architecture. However, the
temporal modeling methods are not efficient or the 3D efficient architecture is
less interested in temporal modeling. For bridging the gap between them, we
propose an efficient temporal modeling 3D architecture, called VoV3D, that
consists of a temporal one-shot aggregation (T-OSA) module and depthwise
factorized component, D(2+1)D. The T-OSA is devised to build a feature
hierarchy by aggregating temporal features with different temporal receptive
fields. Stacking this T-OSA enables the network itself to model short-range as
well as long-range temporal relationships across frames without any external
modules. Inspired by kernel factorization and channel factorization, we also
design a depthwise spatiotemporal factorization module, named, D(2+1)D that
decomposes a 3D depthwise convolution into two spatial and temporal depthwise
convolutions for making our network more lightweight and efficient. By using
the proposed temporal modeling method (T-OSA), and the efficient factorized
component (D(2+1)D), we construct two types of VoV3D networks, VoV3D-M and
VoV3D-L. Thanks to its efficiency and effectiveness of temporal modeling,
VoV3D-L has 6x fewer model parameters and 16x less computation, surpassing a
state-of-the-art temporal modeling method on both Something-Something and
Kinetics-400. Furthermore, VoV3D shows better temporal modeling ability than a
state-of-the-art efficient 3D architecture, X3D having comparable model
capacity. We hope that VoV3D can serve as a baseline for efficient video
classification.
</p>
<a href="http://arxiv.org/abs/2012.00317" target="_blank">arXiv:2012.00317</a> [<a href="http://arxiv.org/pdf/2012.00317" target="_blank">pdf</a>]

<h2>Fast Class-wise Updating for Online Hashing. (arXiv:2012.00318v1 [cs.CV])</h2>
<h3>Mingbao Lin, Rongrong Ji, Xiaoshuai Sun, Baochang Zhang, Feiyue Huang, Yonghong Tian, Dacheng Tao</h3>
<p>Online image hashing has received increasing research attention recently,
which processes large-scale data in a streaming fashion to update the hash
functions on-the-fly. To this end, most existing works exploit this problem
under a supervised setting, i.e., using class labels to boost the hashing
performance, which suffers from the defects in both adaptivity and efficiency:
First, large amounts of training batches are required to learn up-to-date hash
functions, which leads to poor online adaptivity. Second, the training is
time-consuming, which contradicts with the core need of online learning. In
this paper, a novel supervised online hashing scheme, termed Fast Class-wise
Updating for Online Hashing (FCOH), is proposed to address the above two
challenges by introducing a novel and efficient inner product operation. To
achieve fast online adaptivity, a class-wise updating method is developed to
decompose the binary code learning and alternatively renew the hash functions
in a class-wise fashion, which well addresses the burden on large amounts of
training batches. Quantitatively, such a decomposition further leads to at
least 75% storage saving. To further achieve online efficiency, we propose a
semi-relaxation optimization, which accelerates the online training by treating
different binary constraints independently. Without additional constraints and
variables, the time complexity is significantly reduced. Such a scheme is also
quantitatively shown to well preserve past information during updating hashing
functions. We have quantitatively demonstrated that the collective effort of
class-wise updating and semi-relaxation optimization provides a superior
performance comparing to various state-of-the-art methods, which is verified
through extensive experiments on three widely-used datasets.
</p>
<a href="http://arxiv.org/abs/2012.00318" target="_blank">arXiv:2012.00318</a> [<a href="http://arxiv.org/pdf/2012.00318" target="_blank">pdf</a>]

<h2>Disentangling Label Distribution for Long-tailed Visual Recognition. (arXiv:2012.00321v1 [cs.CV])</h2>
<h3>Youngkyu Hong, Seungju Han, Kwanghee Choi, Seokjun Seo, Beomsu Kim, Buru Chang</h3>
<p>The current evaluation protocol of long-tailed visual recognition trains the
classification model on the long-tailed source label distribution and evaluates
its performance on the uniform target label distribution. Such protocol has
questionable practicality since the target may also be long-tailed. Therefore,
we formulate long-tailed visual recognition as a label shift problem where the
target and source label distributions are different. One of the significant
hurdles in dealing with the label shift problem is the entanglement between the
source label distribution and the model prediction. In this paper, we focus on
disentangling the source label distribution from the model prediction. We first
introduce a simple baseline method that matches the target label distribution
by post-processing the model prediction trained by the cross-entropy loss and
the Softmax function. Although this method surpasses state-of-the-art methods
on benchmark datasets, it can be further improved by directly disentangling the
source label distribution from the model prediction in the training phase.
Thus, we propose a novel method, LAbel distribution DisEntangling (LADE) loss
based on the optimal bound of Donsker-Varadhan representation. LADE achieves
state-of-the-art performance on benchmark datasets such as CIFAR-100-LT,
Places-LT, ImageNet-LT, and iNaturalist 2018. Moreover, LADE outperforms
existing methods on various shifted target label distributions, showing the
general adaptability of our proposed method.
</p>
<a href="http://arxiv.org/abs/2012.00321" target="_blank">arXiv:2012.00321</a> [<a href="http://arxiv.org/pdf/2012.00321" target="_blank">pdf</a>]

<h2>Low Bandwidth Video-Chat Compression using Deep Generative Models. (arXiv:2012.00328v1 [cs.CV])</h2>
<h3>Maxime Oquab, Pierre Stock, Oran Gafni, Daniel Haziza, Tao Xu, Peizhao Zhang, Onur Celebi, Yana Hasson, Patrick Labatut, Bobo Bose-Kolanu, Thibault Peyronel, Camille Couprie</h3>
<p>To unlock video chat for hundreds of millions of people hindered by poor
connectivity or unaffordable data costs, we propose to authentically
reconstruct faces on the receiver's device using facial landmarks extracted at
the sender's side and transmitted over the network. In this context, we discuss
and evaluate the benefits and disadvantages of several deep adversarial
approaches. In particular, we explore quality and bandwidth trade-offs for
approaches based on static landmarks, dynamic landmarks or segmentation maps.
We design a mobile-compatible architecture based on the first order animation
model of Siarohin et al. In addition, we leverage SPADE blocks to refine
results in important areas such as the eyes and lips. We compress the networks
down to about 3MB, allowing models to run in real time on iPhone 8 (CPU). This
approach enables video calling at a few kbits per second, an order of magnitude
lower than currently available alternatives.
</p>
<a href="http://arxiv.org/abs/2012.00328" target="_blank">arXiv:2012.00328</a> [<a href="http://arxiv.org/pdf/2012.00328" target="_blank">pdf</a>]

<h2>Semi-Supervised Noisy Student Pre-training on EfficientNet Architectures for Plant Pathology Classification. (arXiv:2012.00332v1 [cs.CV])</h2>
<h3>Sedrick Scott Keh</h3>
<p>In recent years, deep learning has vastly improved the identification and
diagnosis of various diseases in plants. In this report, we investigate the
problem of pathology classification using images of a single leaf. We explore
the use of standard benchmark models such as VGG16, ResNet101, and DenseNet 161
to achieve a 0.945 score on the task. Furthermore, we explore the use of the
newer EfficientNet model, improving the accuracy to 0.962. Finally, we
introduce the state-of-the-art idea of semi-supervised Noisy Student training
to the EfficientNet, resulting in significant improvements in both accuracy and
convergence rate. The final ensembled Noisy Student model performs very well on
the task, achieving a test score of 0.982.
</p>
<a href="http://arxiv.org/abs/2012.00332" target="_blank">arXiv:2012.00332</a> [<a href="http://arxiv.org/pdf/2012.00332" target="_blank">pdf</a>]

<h2>Ultra-low bitrate video conferencing using deep image animation. (arXiv:2012.00346v1 [cs.CV])</h2>
<h3>Goluck Konuko, Giuseppe Valenzise, St&#xe9;phane Lathuili&#xe8;re</h3>
<p>In this work we propose a novel deep learning approach for ultra-low bitrate
video compression for video conferencing applications. To address the
shortcomings of current video compression paradigms when the available
bandwidth is extremely limited, we adopt a model-based approach that employs
deep neural networks to encode motion information as keypoint displacement and
reconstruct the video signal at the decoder side. The overall system is trained
in an end-to-end fashion minimizing a reconstruction error on the encoder
output. Objective and subjective quality evaluation experiments demonstrate
that the proposed approach provides an average bitrate reduction for the same
visual quality of more than 80% compared to HEVC.
</p>
<a href="http://arxiv.org/abs/2012.00346" target="_blank">arXiv:2012.00346</a> [<a href="http://arxiv.org/pdf/2012.00346" target="_blank">pdf</a>]

<h2>Deep Learning-Based Arrhythmia Detection Using RR-Interval Framed Electrocardiograms. (arXiv:2012.00348v1 [cs.LG])</h2>
<h3>Song-Kyoo Kim, Chan Yeob Yeun, Paul D. Yoo, Nai-Wei Lo, Ernesto Damiani</h3>
<p>Deep learning applied to electrocardiogram (ECG) data can be used to achieve
personal authentication in biometric security applications, but it has not been
widely used to diagnose cardiovascular disorders. We developed a deep learning
model for the detection of arrhythmia in which time-sliced ECG data
representing the distance between successive R-peaks are used as the input for
a convolutional neural network (CNN). The main objective is developing the
compact deep learning based detect system which minimally uses the dataset but
delivers the confident accuracy rate of the Arrhythmia detection. This compact
system can be implemented in wearable devices or real-time monitoring equipment
because the feature extraction step is not required for complex ECG waveforms,
only the R-peak data is needed. The results of both tests indicated that the
Compact Arrhythmia Detection System (CADS) matched the performance of
conventional systems for the detection of arrhythmia in two consecutive test
runs. All features of the CADS are fully implemented and publicly available in
MATLAB.
</p>
<a href="http://arxiv.org/abs/2012.00348" target="_blank">arXiv:2012.00348</a> [<a href="http://arxiv.org/pdf/2012.00348" target="_blank">pdf</a>]

<h2>HORAE: an annotated dataset of books of hours. (arXiv:2012.00351v1 [cs.CV])</h2>
<h3>M&#xe9;lodie Boillet, Marie-Laurence Bonhomme, Dominique Stutzmann, Christopher Kermorvant</h3>
<p>We introduce in this paper a new dataset of annotated pages from books of
hours, a type of handwritten prayer books owned and used by rich lay people in
the late middle ages. The dataset was created for conducting historical
research on the evolution of the religious mindset in Europe at this period
since the book of hours represent one of the major sources of information
thanks both to their rich illustrations and the different types of religious
sources they contain. We first describe how the corpus was collected and
manually annotated then present the evaluation of a state-of-the-art system for
text line detection and for zone detection and typing. The corpus is freely
available for research.
</p>
<a href="http://arxiv.org/abs/2012.00351" target="_blank">arXiv:2012.00351</a> [<a href="http://arxiv.org/pdf/2012.00351" target="_blank">pdf</a>]

<h2>Robust and Accurate Object Velocity Detection by Stereo Camera for Autonomous Driving. (arXiv:2012.00353v1 [cs.CV])</h2>
<h3>Toru Saito, Toshimi Okubo, Naoki Takahashi</h3>
<p>Although the number of camera-based sensors mounted on vehicles has recently
increased dramatically, robust and accurate object velocity detection is
difficult. Additionally, it is still common to use radar as a fusion system. We
have developed a method to accurately detect the velocity of object using a
camera, based on a large-scale dataset collected over 20 years by the
automotive manufacturer, SUBARU. The proposed method consists of three methods:
an High Dynamic Range (HDR) detection method that fuses multiple stereo
disparity images, a fusion method that combines the results of monocular and
stereo recognitions, and a new velocity calculation method. The evaluation was
carried out using measurement devices and a test course that can quantitatively
reproduce severe environment by mounting the developed stereo camera on an
actual vehicle.
</p>
<a href="http://arxiv.org/abs/2012.00353" target="_blank">arXiv:2012.00353</a> [<a href="http://arxiv.org/pdf/2012.00353" target="_blank">pdf</a>]

<h2>Symbolic AI for XAI: Evaluating LFIT Inductive Programming for Fair and Explainable Automatic Recruitment. (arXiv:2012.00360v1 [cs.AI])</h2>
<h3>Alfonso Ortega, Julian Fierrez, Aythami Morales, Zilong Wang, Tony Ribeiro</h3>
<p>Machine learning methods are growing in relevance for biometrics and personal
information processing in domains such as forensics, e-health, recruitment, and
e-learning. In these domains, white-box (human-readable) explanations of
systems built on machine learning methods can become crucial. Inductive Logic
Programming (ILP) is a subfield of symbolic AI aimed to automatically learn
declarative theories about the process of data. Learning from Interpretation
Transition (LFIT) is an ILP technique that can learn a propositional logic
theory equivalent to a given black-box system (under certain conditions). The
present work takes a first step to a general methodology to incorporate
accurate declarative explanations to classic machine learning by checking the
viability of LFIT in a specific AI application scenario: fair recruitment based
on an automatic tool generated with machine learning methods for ranking
Curricula Vitae that incorporates soft biometric information (gender and
ethnicity). We show the expressiveness of LFIT for this specific problem and
propose a scheme that can be applicable to other domains.
</p>
<a href="http://arxiv.org/abs/2012.00360" target="_blank">arXiv:2012.00360</a> [<a href="http://arxiv.org/pdf/2012.00360" target="_blank">pdf</a>]

<h2>Rethinking Positive Aggregation and Propagation of Gradients in Gradient-based Saliency Methods. (arXiv:2012.00362v1 [cs.LG])</h2>
<h3>Ashkan Khakzar, Soroosh Baselizadeh, Nassir Navab</h3>
<p>Saliency methods interpret the prediction of a neural network by showing the
importance of input elements for that prediction. A popular family of saliency
methods utilize gradient information. In this work, we empirically show that
two approaches for handling the gradient information, namely positive
aggregation, and positive propagation, break these methods. Though these
methods reflect visually salient information in the input, they do not explain
the model prediction anymore as the generated saliency maps are insensitive to
the predicted output and are insensitive to model parameter randomization.
Specifically for methods that aggregate the gradients of a chosen layer such as
GradCAM++ and FullGrad, exclusively aggregating positive gradients is
detrimental. We further support this by proposing several variants of
aggregation methods with positive handling of gradient information. For methods
that backpropagate gradient information such as LRP, RectGrad, and Guided
Backpropagation, we show the destructive effect of exclusively propagating
positive gradient information.
</p>
<a href="http://arxiv.org/abs/2012.00362" target="_blank">arXiv:2012.00362</a> [<a href="http://arxiv.org/pdf/2012.00362" target="_blank">pdf</a>]

<h2>Pre-Trained Image Processing Transformer. (arXiv:2012.00364v1 [cs.CV])</h2>
<h3>Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu, Wen Gao</h3>
<p>As the computing power of modern hardware is increasing strongly, pre-trained
deep learning models (\eg, BERT, GPT-3) learned on large-scale datasets have
shown their effectiveness over conventional methods. The big progress is mainly
contributed to the representation ability of transformer and its variant
architectures. In this paper, we study the low-level computer vision task (\eg,
denoising, super-resolution and deraining) and develop a new pre-trained model,
namely, image processing transformer (IPT). To maximally excavate the
capability of transformer, we present to utilize the well-known ImageNet
benchmark for generating a large amount of corrupted image pairs. The IPT model
is trained on these images with multi-heads and multi-tails. In addition, the
contrastive learning is introduced for well adapting to different image
processing tasks. The pre-trained model can therefore efficiently employed on
desired task after fine-tuning. With only one pre-trained model, IPT
outperforms the current state-of-the-art methods on various low-level
benchmarks.
</p>
<a href="http://arxiv.org/abs/2012.00364" target="_blank">arXiv:2012.00364</a> [<a href="http://arxiv.org/pdf/2012.00364" target="_blank">pdf</a>]

<h2>Python Workflows on HPC Systems. (arXiv:2012.00365v1 [cs.LG])</h2>
<h3>Dominik Strassel, Philipp Reusch, Janis Keuper</h3>
<p>The recent successes and wide spread application of compute intensive machine
learning and data analytics methods have been boosting the usage of the Python
programming language on HPC systems. While Python provides many advantages for
the users, it has not been designed with a focus on multi-user environments or
parallel programming - making it quite challenging to maintain stable and
secure Python workflows on a HPC system. In this paper, we analyze the key
problems induced by the usage of Python on HPC clusters and sketch appropriate
workarounds for efficiently maintaining multi-user Python software
environments, securing and restricting resources of Python jobs and containing
Python processes, while focusing on Deep Learning applications running on GPU
clusters.
</p>
<a href="http://arxiv.org/abs/2012.00365" target="_blank">arXiv:2012.00365</a> [<a href="http://arxiv.org/pdf/2012.00365" target="_blank">pdf</a>]

<h2>Latent Programmer: Discrete Latent Codes for Program Synthesis. (arXiv:2012.00377v1 [cs.LG])</h2>
<h3>Joey Hong, David Dohan, Rishabh Singh, Charles Sutton, Manzil Zaheer</h3>
<p>In many sequence learning tasks, such as program synthesis and document
summarization, a key problem is searching over a large space of possible output
sequences. We propose to learn representations of the outputs that are
specifically meant for search: rich enough to specify the desired output but
compact enough to make search more efficient. Discrete latent codes are
appealing for this purpose, as they naturally allow sophisticated combinatorial
search strategies. The latent codes are learned using a self-supervised
learning principle, in which first a discrete autoencoder is trained on the
output sequences, and then the resulting latent codes are used as intermediate
targets for the end-to-end sequence prediction task. Based on these insights,
we introduce the \emph{Latent Programmer}, a program synthesis method that
first predicts a discrete latent code from input/output examples, and then
generates the program in the target language. We evaluate the Latent Programmer
on two domains: synthesis of string transformation programs, and generation of
programs from natural language descriptions. We demonstrate that the discrete
latent representation significantly improves synthesis accuracy.
</p>
<a href="http://arxiv.org/abs/2012.00377" target="_blank">arXiv:2012.00377</a> [<a href="http://arxiv.org/pdf/2012.00377" target="_blank">pdf</a>]

<h2>Non-Stationary Latent Bandits. (arXiv:2012.00386v1 [cs.LG])</h2>
<h3>Joey Hong, Branislav Kveton, Manzil Zaheer, Yinlam Chow, Amr Ahmed, Mohammad Ghavamzadeh, Craig Boutilier</h3>
<p>Users of recommender systems often behave in a non-stationary fashion, due to
their evolving preferences and tastes over time. In this work, we propose a
practical approach for fast personalization to non-stationary users. The key
idea is to frame this problem as a latent bandit, where the prototypical models
of user behavior are learned offline and the latent state of the user is
inferred online from its interactions with the models. We call this problem a
non-stationary latent bandit. We propose Thompson sampling algorithms for
regret minimization in non-stationary latent bandits, analyze them, and
evaluate them on a real-world dataset. The main strength of our approach is
that it can be combined with rich offline-learned models, which can be
misspecified, and are subsequently fine-tuned online using posterior sampling.
In this way, we naturally combine the strengths of offline and online learning.
</p>
<a href="http://arxiv.org/abs/2012.00386" target="_blank">arXiv:2012.00386</a> [<a href="http://arxiv.org/pdf/2012.00386" target="_blank">pdf</a>]

<h2>Use of Remote Sensing Data to Identify Air Pollution Signatures in India. (arXiv:2012.00402v1 [cs.LG])</h2>
<h3>Sivaramakrishnan KN, Lipika Deka, Manik Gupta</h3>
<p>Air quality has major impact on a country's socio-economic position and
identifying major air pollution sources is at the heart of tackling the issue.
Spatially and temporally distributed air quality data acquisition across a
country as varied as India has been a challenge to such analysis. The launch of
the Sentinel-5P satellite has helped in the observation of a wider variety of
air pollutants than measured before at a global scale on a daily basis. In this
chapter, spatio-temporal multi pollutant data retrieved from Sentinel-5P
satellite is used to cluster states as well as districts in India and
associated average monthly pollution signature and trends depicted by each of
the clusters are derived and presented.The clustering signatures can be used to
identify states and districts based on the types of pollutants emitted by
various pollution sources.
</p>
<a href="http://arxiv.org/abs/2012.00402" target="_blank">arXiv:2012.00402</a> [<a href="http://arxiv.org/pdf/2012.00402" target="_blank">pdf</a>]

<h2>Directed Graph Attention Neural Network Utilizing 3D Coordinates for Molecular Property Prediction. (arXiv:2012.00404v1 [cs.LG])</h2>
<h3>Chen Qian, Yunhai Xiong, Xiang Chen</h3>
<p>The prosperity of computer vision (CV) and natural language procession (NLP)
in recent years has spurred the development of deep learning in many other
domains. The advancement in machine learning provides us with an alternative
option besides the computationally expensive density functional theories (DFT).
Kernel method and graph neural networks have been widely studied as two
mainstream methods for property prediction. The promising graph neural networks
have achieved comparable accuracy to the DFT method for specific objects in the
recent study. However, most of the graph neural networks with high precision so
far require fully connected graphs with pairwise distance distribution as edge
information. In this work, we shed light on the Directed Graph Attention Neural
Network (DGANN), which only takes chemical bonds as edges and operates on bonds
and atoms of molecules. DGANN distinguishes from previous models with those
features: (1) It learns the local chemical environment encoding by graph
attention mechanism on chemical bonds. Every initial edge message only flows
into every message passing trajectory once. (2) The transformer blocks
aggregate the global molecular representation from the local atomic encoding.
(3) The position vectors and coordinates are used as inputs instead of
distances. Our model has matched or outperformed most baseline graph neural
networks on QM9 datasets even without thorough hyper-parameters searching.
Moreover, this work suggests that models directly utilizing 3D coordinates can
still reach high accuracies for molecule representation even without rotational
and translational invariance incorporated.
</p>
<a href="http://arxiv.org/abs/2012.00404" target="_blank">arXiv:2012.00404</a> [<a href="http://arxiv.org/pdf/2012.00404" target="_blank">pdf</a>]

<h2>Learning to Generalize Unseen Domains via Memory-based Multi-Source Meta-Learning for Person Re-Identification. (arXiv:2012.00417v1 [cs.CV])</h2>
<h3>Yuyang Zhao, Zhun Zhong, Fengxiang Yang, Zhiming Luo, Yaojin Lin, Shaozi Li, Nicu Sebe</h3>
<p>Recent advances in person re-identification (ReID) obtain impressive accuracy
in the supervised and unsupervised learning settings. However, most of the
existing methods need to train a new model for a new domain by accessing data.
Due to public privacy, the new domain data are not always accessible, leading
to a limited applicability of these methods. In this paper, we study the
problem of multi-source domain generalization in ReID, which aims to learn a
model that can perform well on unseen domains with only several labeled source
domains. To address this problem, we propose the Memory-based Multi-Source
Meta-Learning (M$^3$L) framework to train a generalizable model for unseen
domains. Specifically, a meta-learning strategy is introduced to simulate the
train-test process of domain generalization for learning more generalizable
models. To overcome the unstable meta-optimization caused by the parametric
classifier, we propose a memory-based identification loss that is
non-parametric and harmonizes with meta-learning. We also present a meta batch
normalization layer (MetaBN) to diversify meta-test features, further
establishing the advantage of meta-learning. Experiments demonstrate that our
M$^3$L can effectively enhance the generalization ability of the model for
unseen domains and can outperform the state-of-the-art methods on four
large-scale ReID datasets.
</p>
<a href="http://arxiv.org/abs/2012.00417" target="_blank">arXiv:2012.00417</a> [<a href="http://arxiv.org/pdf/2012.00417" target="_blank">pdf</a>]

<h2>Machine Learning Systems in the IoT: Trustworthiness Trade-offs for Edge Intelligence. (arXiv:2012.00419v1 [cs.LG])</h2>
<h3>Wiebke Toussaint, Aaron Yi Ding</h3>
<p>Machine learning systems (MLSys) are emerging in the Internet of Things (IoT)
to provision edge intelligence, which is paving our way towards the vision of
ubiquitous intelligence. However, despite the maturity of machine learning
systems and the IoT, we are facing severe challenges when integrating MLSys and
IoT in practical context. For instance, many machine learning systems have been
developed for large-scale production (e.g., cloud environments), but IoT
introduces additional demands due to heterogeneous and resource-constrained
devices and decentralized operation environment. To shed light on this
convergence of MLSys and IoT, this paper analyzes the trade-offs by covering
the latest developments (up to 2020) on scaling and distributing ML across
cloud, edge, and IoT devices. We position machine learning systems as a
component of the IoT, and edge intelligence as a socio-technical system. On the
challenges of designing trustworthy edge intelligence, we advocate a holistic
design approach that takes multi-stakeholder concerns, design requirements and
trade-offs into consideration, and highlight the future research opportunities
in edge intelligence.
</p>
<a href="http://arxiv.org/abs/2012.00419" target="_blank">arXiv:2012.00419</a> [<a href="http://arxiv.org/pdf/2012.00419" target="_blank">pdf</a>]

<h2>Does Fair Ranking Improve Minority Outcomes? Understanding the Interplay of Human and Algorithmic Biases in Online Hiring. (arXiv:2012.00423v1 [cs.LG])</h2>
<h3>Tom S&#xfc;hr, Sophie Hilgard, Himabindu Lakkaraju</h3>
<p>Ranking algorithms are being widely employed in various online hiring
platforms including LinkedIn, TaskRabbit, and Fiverr. Since these platforms
impact the livelihood of millions of people, it is important to ensure that the
underlying algorithms are not adversely affecting minority groups. However,
prior research has demonstrated that ranking algorithms employed by these
platforms are prone to a variety of undesirable biases. To address this
problem, fair ranking algorithms (e.g.,Det-Greedy) which increase exposure of
underrepresented candidates have been proposed in recent literature. However,
there is little to no work that explores if these proposed fair ranking
algorithms actually improve real world outcomes (e.g., hiring decisions) for
minority groups. Furthermore, there is no clear understanding as to how other
factors (e.g., jobcontext, inherent biases of the employers) play a role in
impacting the real world outcomes of minority groups. In this work, we study
how gender biases manifest in online hiring platforms and how they impact real
world hiring decisions. More specifically, we analyze various sources of gender
biases including the nature of the ranking algorithm, the job context, and
inherent biases of employers, and establish how these factors interact and
affect real world hiring decisions. To this end, we experiment with three
different ranking algorithms on three different job contexts using real world
data from TaskRabbit. We simulate the hiring scenarios on TaskRabbit by
carrying out a large-scale user study with Amazon Mechanical Turk. We then
leverage the responses from this study to understand the effect of each of the
aforementioned factors. Our results demonstrate that fair ranking algorithms
can be an effective tool at increasing hiring of underrepresented gender
candidates but induces inconsistent outcomes across candidate features and job
contexts.
</p>
<a href="http://arxiv.org/abs/2012.00423" target="_blank">arXiv:2012.00423</a> [<a href="http://arxiv.org/pdf/2012.00423" target="_blank">pdf</a>]

<h2>Weakly-Supervised Arbitrary-Shaped Text Detection with Expectation-Maximization Algorithm. (arXiv:2012.00424v1 [cs.CV])</h2>
<h3>Mengbiao Zhao, Wei Feng, Fei Yin, Xu-Yao Zhang, Cheng-Lin Liu</h3>
<p>Arbitrary-shaped text detection is an important and challenging task in
computer vision. Most existing methods require heavy data labeling efforts to
produce polygon-level text region labels for supervised training. In order to
reduce the cost in data labeling, we study weakly-supervised arbitrary-shaped
text detection for combining various weak supervision forms (e.g., image-level
tags, coarse, loose and tight bounding boxes), which are far easier for
annotation. We propose an Expectation-Maximization (EM) based weakly-supervised
learning framework to train an accurate arbitrary-shaped text detector using
only a small amount of polygon-level annotated data combined with a large
amount of weakly annotated data. Meanwhile, we propose a contour-based
arbitrary-shaped text detector, which is suitable for incorporating
weakly-supervised learning. Extensive experiments on three arbitrary-shaped
text benchmarks (CTW1500, Total-Text and ICDAR-ArT) show that (1) using only
10% strongly annotated data and 90% weakly annotated data, our method yields
comparable performance to state-of-the-art methods, (2) with 100% strongly
annotated data, our method outperforms existing methods on all three
benchmarks. We will make the weakly annotated datasets publicly available in
the future.
</p>
<a href="http://arxiv.org/abs/2012.00424" target="_blank">arXiv:2012.00424</a> [<a href="http://arxiv.org/pdf/2012.00424" target="_blank">pdf</a>]

<h2>Edge-assisted Democratized Learning Towards Federated Analytics. (arXiv:2012.00425v1 [cs.LG])</h2>
<h3>Shashi Raj Pandey, Minh N.H. Nguyen, Tri Nguyen Dang, Nguyen H. Tran, Kyi Thar, Zhu Han, Choong Seon Hong</h3>
<p>A recent take towards Federated Analytics (FA), which allows analytical
insights of distributed datasets, reuses the Federated Learning (FL)
infrastructure to evaluate the population-level summary of model performances.
However, the current realization of FL adopts single server-multiple client
architecture with limited scope for FA, which often results in learning models
with poor generalization, i.e., an ability to handle new/unseen data, for
real-world applications. Moreover, a hierarchical FL structure with distributed
computing platforms demonstrates incoherent model performances at different
aggregation levels. Therefore, we need to design a robust learning mechanism
than the FL that (i) unleashes a viable infrastructure for FA and (ii) trains
learning models with better generalization capability. In this work, we adopt
the novel democratized learning (Dem-AI) principles and designs to meet these
objectives. Firstly, we show the hierarchical learning structure of the
proposed edge-assisted democratized learning mechanism, namely Edge-DemLearn,
as a practical framework to empower generalization capability in support of FA.
Secondly, we validate Edge-DemLearn as a flexible model training mechanism to
build a distributed control and aggregation methodology in regions by
leveraging the distributed computing infrastructure. The distributed edge
computing servers construct regional models, minimize the communication loads,
and ensure distributed data analytic application's scalability. To that end, we
adhere to a near-optimal two-sided many-to-one matching approach to handle the
combinatorial constraints in Edge-DemLearn and solve it for fast knowledge
acquisition with optimization of resource allocation and associations between
multiple servers and devices. Extensive simulation results on real datasets
demonstrate the effectiveness of the proposed methods.
</p>
<a href="http://arxiv.org/abs/2012.00425" target="_blank">arXiv:2012.00425</a> [<a href="http://arxiv.org/pdf/2012.00425" target="_blank">pdf</a>]

<h2>Probabilistic Grammars for Equation Discovery. (arXiv:2012.00428v1 [cs.LG])</h2>
<h3>Jure Brence, Ljup&#x10d;o Todorovski, Sa&#x161;o D&#x17e;eroski</h3>
<p>Equation discovery, also known as symbolic regression, is a type of automated
modeling that discovers scientific laws, expressed in the form of equations,
from observed data and expert knowledge. Deterministic grammars, such as
context-free grammars, have been used to limit the search spaces in equation
discovery by providing hard constraints that specify which equations to
consider and which not. In this paper, we propose the use of probabilistic
context-free grammars in the context of equation discovery. Such grammars
encode soft constraints on the space of equations, specifying a prior
probability distribution on the space of possible equations. We show that
probabilistic grammars can be used to elegantly and flexibly formulate the
parsimony principle, that favors simpler equations, through probabilities
attached to the rules in the grammars. We demonstrate that the use of
probabilistic, rather than deterministic grammars, in the context of a
Monte-Carlo algorithm for grammar-based equation discovery, leads to more
efficient equation discovery. Finally, by specifying prior probability
distributions over equation spaces, the foundations are laid for Bayesian
approaches to equation discovery.
</p>
<a href="http://arxiv.org/abs/2012.00428" target="_blank">arXiv:2012.00428</a> [<a href="http://arxiv.org/pdf/2012.00428" target="_blank">pdf</a>]

<h2>A Generative Model to Synthesize EEG Data for Epileptic Seizure Prediction. (arXiv:2012.00430v1 [cs.LG])</h2>
<h3>Khansa Rasheed, Junaid Qadir, Terence J.O&#x27;Brien, Levin Kuhlmann, Adeel Razi</h3>
<p>Prediction of seizure before they occur is vital for bringing normalcy to the
lives of patients. Researchers employed machine learning methods using
hand-crafted features for seizure prediction. However, ML methods are too
complicated to select the best ML model or best features. Deep Learning methods
are beneficial in the sense of automatic feature extraction. One of the
roadblocks for accurate seizure prediction is scarcity of epileptic seizure
data. This paper addresses this problem by proposing a deep convolutional
generative adversarial network to generate synthetic EEG samples. We use two
methods to validate synthesized data namely, one-class SVM and a new proposal
which we refer to as convolutional epileptic seizure predictor (CESP). Another
objective of our study is to evaluate performance of well-known deep learning
models (e.g., VGG16, VGG19, ResNet50, and Inceptionv3) by training models on
augmented data using transfer learning with average time of 10 min between true
prediction and seizure onset. Our results show that CESP model achieves
sensitivity of 78.11% and 88.21%, and FPR of 0.27/h and 0.14/h for training on
synthesized and testing on real Epilepsyecosystem and CHB-MIT datasets,
respectively. Effective results of CESP trained on synthesized data shows that
synthetic data acquired the correlation between features and labels very well.
We also show that employment of idea of transfer learning and data augmentation
in patient-specific manner provides highest accuracy with sensitivity of 90.03%
and 0.03 FPR/h which was achieved using Inceptionv3, and that augmenting data
with samples generated from DCGAN increased prediction results of our CESP
model and Inceptionv3 by 4-5% as compared to state-of-the-art traditional
augmentation techniques. Finally, we note that prediction results of CESP
achieved by using augmented data are better than chance level for both
datasets.
</p>
<a href="http://arxiv.org/abs/2012.00430" target="_blank">arXiv:2012.00430</a> [<a href="http://arxiv.org/pdf/2012.00430" target="_blank">pdf</a>]

<h2>SRG-Net: Unsupervised Segmentation for Terracotta Warrior Point Cloud with 3D Pointwise CNN methods. (arXiv:2012.00433v1 [cs.CV])</h2>
<h3>Yao Hu, Guohua Geng, Kang Li, Wei Zhou, Xingxing Hao, Xin Cao</h3>
<p>In this paper, we present a seed-region-growing CNN(SRG-Net) for unsupervised
part segmentation with 3D point clouds of terracotta warriors. Previous neural
network researches in 3D are mainly about supervised classification,
clustering, unsupervised representation and reconstruction. There are few
researches focusing on unsupervised point cloud part segmentation. To address
these problems, we present a seed-region-growing CNN(SRG-Net) for unsupervised
part segmentation with 3D point clouds of terracotta warriors. Firstly, we
propose our customized seed region growing algorithm to coarsely segment the
point cloud. Then we present our supervised segmentation and unsupervised
reconstruction networks to better understand the characteristics of 3D point
clouds. Finally, we combine the SRG algorithm with our improved CNN using a
refinement method called SRG-Net to conduct the segmentation tasks on the
terracotta warriors. Our proposed SRG-Net are evaluated on the terracotta
warriors data and the benchmark dataset of ShapeNet with measuring mean
intersection over union(mIoU) and latency. The experimental results show that
our SRG-Net outperforms the state-of-the-art methods. Our code is available at
https://github.com/hyoau/SRG-Net.
</p>
<a href="http://arxiv.org/abs/2012.00433" target="_blank">arXiv:2012.00433</a> [<a href="http://arxiv.org/pdf/2012.00433" target="_blank">pdf</a>]

<h2>A Unified Structure for Efficient RGB and RGB-D Salient Object Detection. (arXiv:2012.00437v1 [cs.CV])</h2>
<h3>Peng Peng, Yong-Jie Li</h3>
<p>Salient object detection (SOD) has been well studied in recent years,
especially using deep neural networks. However, SOD with RGB and RGB-D images
is usually treated as two different tasks with different network structures
that need to be designed specifically. In this paper, we proposed a unified and
efficient structure with a cross-attention context extraction (CRACE) module to
address both tasks of SOD efficiently. The proposed CRACE module receives and
appropriately fuses two (for RGB SOD) or three (for RGB-D SOD) inputs. The
simple unified feature pyramid network (FPN)-like structure with CRACE modules
conveys and refines the results under the multi-level supervisions of saliency
and boundaries. The proposed structure is simple yet effective; the rich
context information of RGB and depth can be appropriately extracted and fused
by the proposed structure efficiently. Experimental results show that our
method outperforms other state-of-the-art methods in both RGB and RGB-D SOD
tasks on various datasets and in terms of most metrics.
</p>
<a href="http://arxiv.org/abs/2012.00437" target="_blank">arXiv:2012.00437</a> [<a href="http://arxiv.org/pdf/2012.00437" target="_blank">pdf</a>]

<h2>Just Ask: Learning to Answer Questions from Millions of Narrated Videos. (arXiv:2012.00451v1 [cs.CV])</h2>
<h3>Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, Cordelia Schmid</h3>
<p>Modern approaches to visual question answering require large annotated
datasets for training. Manual annotation of questions and answers for videos,
however, is tedious, expensive and prevents scalability. In this work, we
propose to avoid manual annotation and to learn video question answering
(VideoQA) from millions of readily-available narrated videos. We propose to
automatically generate question-answer pairs from transcribed video narrations
leveraging a state-of-the-art text transformer pipeline and obtain a new
large-scale VideoQA training dataset. To handle the open vocabulary of diverse
answers in this dataset, we propose a training procedure based on a contrastive
loss between a video-question multi-modal transformer and an answer embedding.
We evaluate our model on the zero-shot VideoQA task and show excellent results,
in particular for rare answers. Furthermore, we demonstrate that finetuning our
model on target datasets significantly outperforms the state of the art on
MSRVTT-QA, MSVD-QA and ActivityNet-QA. Finally, for a detailed evaluation we
introduce a new manually annotated VideoQA dataset with reduced language biases
and high quality annotations. Our code and datasets will be made publicly
available at https://www.di.ens.fr/willow/research/just-ask/ .
</p>
<a href="http://arxiv.org/abs/2012.00451" target="_blank">arXiv:2012.00451</a> [<a href="http://arxiv.org/pdf/2012.00451" target="_blank">pdf</a>]

<h2>Counting People by Estimating People Flows. (arXiv:2012.00452v1 [cs.CV])</h2>
<h3>Weizhe Liu, Mathieu Salzmann, Pascal Fua</h3>
<p>Modern methods for counting people in crowded scenes rely on deep networks to
estimate people densities in individual images. As such, only very few take
advantage of temporal consistency in video sequences, and those that do only
impose weak smoothness constraints across consecutive frames. In this paper, we
advocate estimating people flows across image locations between consecutive
images and inferring the people densities from these flows instead of directly
regressing them. This enables us to impose much stronger constraints encoding
the conservation of the number of people. As a result, it significantly boosts
performance without requiring a more complex architecture. Furthermore, it
allows us to exploit the correlation between people flow and optical flow to
further improve the results. We also show that leveraging people conservation
constraints in both a spatial and temporal manner makes it possible to train a
deep crowd counting model in an active learning setting with much fewer
annotations. This significantly reduces the annotation cost while still leading
to similar performance to the full supervision case.
</p>
<a href="http://arxiv.org/abs/2012.00452" target="_blank">arXiv:2012.00452</a> [<a href="http://arxiv.org/pdf/2012.00452" target="_blank">pdf</a>]

<h2>Theoretical Evidence Supporting Harmonic Reaching Trajectories. (arXiv:2012.00453v1 [cs.RO])</h2>
<h3>Carlo Tiseo, Sydney Rebecca Charitos, Michael Mistry</h3>
<p>Minimum Jerk trajectories have been long thought to be the reference
trajectories for human movements due to their impressive similarity with human
movements. Nevertheless, minimum jerk trajectories are not the only choice for
$C^\infty$ (i.e., smooth) functions. For example, harmonic trajectories are
smooth functions that can be superimposed to describe the evolution of physical
systems. This paper analyses the possibility that motor control plans using
harmonic trajectories, will be experimentally observed to have a minimum jerk
likeness due to control signals being transported through the Central Nervous
System (CNS) and muscle-skeletal system. We tested our theory on a 3-link arm
simulation using a recently developed planner that we reformulated into a motor
control architecture, inspired by the passive motion paradigm. The arm
performed 100 movements, reaching for each target defined by the clock
experiment. We analysed the shape of the trajectory planned in the CNS and
executed in the physical simulator. We observed that even under ideal
conditions (i.e., absence of delays and noise) the executed trajectories are
similar to a minimum jerk trajectory; thus, supporting the thesis that the
human brain might plan harmonic trajectories.
</p>
<a href="http://arxiv.org/abs/2012.00453" target="_blank">arXiv:2012.00453</a> [<a href="http://arxiv.org/pdf/2012.00453" target="_blank">pdf</a>]

<h2>Globally Optimal Relative Pose Estimation with Gravity Prior. (arXiv:2012.00458v1 [cs.CV])</h2>
<h3>Yaqing Ding, Daniel Barath, Jian Yang, Hui Kong, Zuzana Kukelova</h3>
<p>Smartphones, tablets and camera systems used, e.g., in cars and UAVs, are
typically equipped with IMUs (inertial measurement units) that can measure the
gravity vector accurately. Using this additional information, the $y$-axes of
the cameras can be aligned, reducing their relative orientation to a single
degree-of-freedom. With this assumption, we propose a novel globally optimal
solver, minimizing the algebraic error in the least-squares sense, to estimate
the relative pose in the over-determined case. Based on the epipolar
constraint, we convert the optimization problem into solving two polynomials
with only two unknowns. Also, a fast solver is proposed using the first-order
approximation of the rotation. The proposed solvers are compared with the
state-of-the-art ones on four real-world datasets with approx. 50000 image
pairs in total. Moreover, we collected a dataset, by a smartphone, consisting
of 10933 image pairs, gravity directions, and ground truth 3D reconstructions.
</p>
<a href="http://arxiv.org/abs/2012.00458" target="_blank">arXiv:2012.00458</a> [<a href="http://arxiv.org/pdf/2012.00458" target="_blank">pdf</a>]

<h2>Unsupervised Anomaly Detection From Semantic Similarity Scores. (arXiv:2012.00461v1 [cs.LG])</h2>
<h3>Nima Rafiee, Rahil Gholamipoor, Markus Kollmann</h3>
<p>In this paper, we present SemSAD, a simple and generic framework for
detecting examples that lie out-of-distribution (OOD) for a given training set.
The approach is based on learning a semantic similarity measure to find for a
given test example the semantically closest example in the training set and
then using a discriminator to classify whether the two examples show sufficient
semantic dissimilarity such that the test example can be rejected as OOD. We
are able to outperform previous approaches for anomaly, novelty, or
out-of-distribution detection in the visual domain by a large margin. In
particular, we obtain AUROC values close to one for the challenging task of
detecting examples from CIFAR-10 as out-of-distribution given CIFAR-100 as
in-distribution, without making use of label information
</p>
<a href="http://arxiv.org/abs/2012.00461" target="_blank">arXiv:2012.00461</a> [<a href="http://arxiv.org/pdf/2012.00461" target="_blank">pdf</a>]

<h2>(k, l)-Medians Clustering of Trajectories Using Continuous Dynamic Time Warping. (arXiv:2012.00464v1 [cs.LG])</h2>
<h3>Milutin Brankovic, Kevin Buchin, Koen Klaren, Andr&#xe9; Nusser, Aleksandr Popov, Sampson Wong</h3>
<p>Due to the massively increasing amount of available geospatial data and the
need to present it in an understandable way, clustering this data is more
important than ever. As clusters might contain a large number of objects,
having a representative for each cluster significantly facilitates
understanding a clustering. Clustering methods relying on such representatives
are called center-based. In this work we consider the problem of center-based
clustering of trajectories.

In this setting, the representative of a cluster is again a trajectory. To
obtain a compact representation of the clusters and to avoid overfitting, we
restrict the complexity of the representative trajectories by a parameter l.
This restriction, however, makes discrete distance measures like dynamic time
warping (DTW) less suited.

There is recent work on center-based clustering of trajectories with a
continuous distance measure, namely, the Fr\'echet distance. While the
Fr\'echet distance allows for restriction of the center complexity, it can also
be sensitive to outliers, whereas averaging-type distance measures, like DTW,
are less so. To obtain a trajectory clustering algorithm that allows
restricting center complexity and is more robust to outliers, we propose the
usage of a continuous version of DTW as distance measure, which we call
continuous dynamic time warping (CDTW). Our contribution is twofold:

1. To combat the lack of practical algorithms for CDTW, we develop an
approximation algorithm that computes it.

2. We develop the first clustering algorithm under this distance measure and
show a practical way to compute a center from a set of trajectories and
subsequently iteratively improve it.

To obtain insights into the results of clustering under CDTW on practical
data, we conduct extensive experiments.
</p>
<a href="http://arxiv.org/abs/2012.00464" target="_blank">arXiv:2012.00464</a> [<a href="http://arxiv.org/pdf/2012.00464" target="_blank">pdf</a>]

<h2>Minimal Solutions for Panoramic Stitching Given Gravity Prior. (arXiv:2012.00465v1 [cs.CV])</h2>
<h3>Yaqing Ding, Daniel Barath, Zuzana Kukelova</h3>
<p>When capturing panoramas, people tend to align their cameras with the
vertical axis, i.e., the direction of gravity. Moreover, modern devices, such
as smartphones and tablets, are equipped with an IMU (Inertial Measurement
Unit) that can measure the gravity vector accurately. Using this prior, the
y-axes of the cameras can be aligned or assumed to be already aligned, reducing
their relative orientation to 1-DOF (degree of freedom). Exploiting this
assumption, we propose new minimal solutions to panoramic image stitching of
images taken by cameras with coinciding optical centers, i.e., undergoing pure
rotation. We consider four practical camera configurations, assuming unknown
fixed or varying focal length with or without radial distortion. The solvers
are tested both on synthetic scenes and on more than 500k real image pairs from
the Sun360 dataset and from scenes captured by us using two smartphones
equipped with IMUs. It is shown, that they outperform the state-of-the-art both
in terms of accuracy and processing time.
</p>
<a href="http://arxiv.org/abs/2012.00465" target="_blank">arXiv:2012.00465</a> [<a href="http://arxiv.org/pdf/2012.00465" target="_blank">pdf</a>]

<h2>Boosting CNN-based primary quantization matrix estimation of double JPEG images via a classification-like architecture. (arXiv:2012.00468v1 [cs.CV])</h2>
<h3>Benedetta Tondi, Andrea Costranzo, Dequ Huang, Bin Li</h3>
<p>The problem of estimation of the primary quantization matrix in double JPEG
images is of relevant importance in several applications, and in particular,
for splicing localization. In addition to traditional statistical-based
approaches, recently, deep learning has been exploited to design a well
performing estimator, by training a Convolutional Neural Network (CNN) model to
solve the estimation as a standard regression problem. In this paper, we
propose the use of a simil-classification CNN architecture to solve the
estimation, by exploiting the integer nature of the quantization coefficients,
and using a proper loss function for training, that takes into account both the
accuracy and the Mean Square Error of the estimation. The capability of the
method to work under general operative conditions, regarding the alignment of
the second compression grid with the one of first compression and the
combinations of the JPEG qualities of former and second compression, is very
relevant in practical applications, where these information are unknown a
priori. Results confirm the effectiveness of the proposed technique, compared
to the state-of-the art methods based on statistical analysis and deep learning
regression.
</p>
<a href="http://arxiv.org/abs/2012.00468" target="_blank">arXiv:2012.00468</a> [<a href="http://arxiv.org/pdf/2012.00468" target="_blank">pdf</a>]

<h2>Improving cluster recovery with feature rescaling factors. (arXiv:2012.00477v1 [cs.LG])</h2>
<h3>Renato Cordeiro de Amorim, Vladimir Makarenkov</h3>
<p>The data preprocessing stage is crucial in clustering. Features may describe
entities using different scales. To rectify this, one usually applies feature
normalisation aiming at rescaling features so that none of them overpowers the
others in the objective function of the selected clustering algorithm. In this
paper, we argue that the rescaling procedure should not treat all features
identically. Instead, it should favour the features that are more meaningful
for clustering. With this in mind, we introduce a feature rescaling method that
takes into account the within-cluster degree of relevance of each feature. Our
comprehensive simulation study, carried out on real and synthetic data, with
and without noise features, clearly demonstrates that clustering methods that
use the proposed data normalization strategy clearly outperform those that use
traditional data normalization.
</p>
<a href="http://arxiv.org/abs/2012.00477" target="_blank">arXiv:2012.00477</a> [<a href="http://arxiv.org/pdf/2012.00477" target="_blank">pdf</a>]

<h2>Consistent Representation Learning for High Dimensional Data Analysis. (arXiv:2012.00481v1 [cs.LG])</h2>
<h3>Stan Z. Li, Lirong Wu, Zelin Zang</h3>
<p>High dimensional data analysis for exploration and discovery includes three
fundamental tasks: dimensionality reduction, clustering, and visualization.
When the three associated tasks are done separately, as is often the case thus
far, inconsistencies can occur among the tasks in terms of data geometry and
others. This can lead to confusing or misleading data interpretation. In this
paper, we propose a novel neural network-based method, called Consistent
Representation Learning (CRL), to accomplish the three associated tasks
end-to-end and improve the consistencies. The CRL network consists of two
nonlinear dimensionality reduction (NLDR) transformations: (1) one from the
input data space to the latent feature space for clustering, and (2) the other
from the clustering space to the final 2D or 3D space for visualization.
Importantly, the two NLDR transformations are performed to best satisfy local
geometry preserving (LGP) constraints across the spaces or network layers, to
improve data consistencies along with the processing flow. Also, we propose a
novel metric, clustering-visualization inconsistency (CVI), for evaluating the
inconsistencies. Extensive comparative results show that the proposed CRL
neural network method outperforms the popular t-SNE and UMAP-based and other
contemporary clustering and visualization algorithms in terms of evaluation
metrics and visualization.
</p>
<a href="http://arxiv.org/abs/2012.00481" target="_blank">arXiv:2012.00481</a> [<a href="http://arxiv.org/pdf/2012.00481" target="_blank">pdf</a>]

<h2>Deep Gravity: enhancing mobility flows generation with deep neural networks and geographic information. (arXiv:2012.00489v1 [cs.LG])</h2>
<h3>Filippo Simini, Gianni Barlacchi, Massimiliano Luca, Luca Pappalardo</h3>
<p>The movements of individuals within and among cities influence key aspects of
our society, such as the objective and subjective well-being, the diffusion of
innovations, the spreading of epidemics, and the quality of the environment.
For this reason, there is increasing interest around the challenging problem of
flow generation, which consists in generating the flows between a set of
geographic locations, given the characteristics of the locations and without
any information about the real flows. Existing solutions to flow generation are
mainly based on mechanistic approaches, such as the gravity model and the
radiation model, which suffer from underfitting and overdispersion, neglect
important variables such as land use and the transportation network, and cannot
describe non-linear relationships between these variables. In this paper, we
propose the Multi-Feature Deep Gravity (MFDG) model as an effective solution to
flow generation. On the one hand, the MFDG model exploits a large number of
variables (e.g., characteristics of land use and the road network; transport,
food, and health facilities) extracted from voluntary geographic information
data (OpenStreetMap). On the other hand, our model exploits deep neural
networks to describe complex non-linear relationships between those variables.
Our experiments, conducted on commuting flows in England, show that the MFDG
model achieves a significant increase in the performance (up to 250\% for
highly populated areas) than mechanistic models that do not use deep neural
networks, or that do not exploit geographic voluntary data. Our work presents a
precise definition of the flow generation problem, which is a novel task for
the deep learning community working with spatio-temporal data, and proposes a
deep neural network model that significantly outperforms current
state-of-the-art statistical models.
</p>
<a href="http://arxiv.org/abs/2012.00489" target="_blank">arXiv:2012.00489</a> [<a href="http://arxiv.org/pdf/2012.00489" target="_blank">pdf</a>]

<h2>Problems of representation of electrocardiograms in convolutional neural networks. (arXiv:2012.00493v1 [cs.LG])</h2>
<h3>Iana Sereda, Sergey Alekseev, Aleksandra Koneva, Alexey Khorkin, Grigory Osipov</h3>
<p>Using electrocardiograms as an example, we demonstrate the characteristic
problems that arise when modeling one-dimensional signals containing inaccurate
repeating pattern by means of standard convolutional networks. We show that
these problems are systemic in nature. They are due to how convolutional
networks work with composite objects, parts of which are not fixed rigidly, but
have significant mobility. We also demonstrate some counterintuitive effects
related to generalization in deep networks.
</p>
<a href="http://arxiv.org/abs/2012.00493" target="_blank">arXiv:2012.00493</a> [<a href="http://arxiv.org/pdf/2012.00493" target="_blank">pdf</a>]

<h2>Analysis of Drifting Features. (arXiv:2012.00499v1 [cs.LG])</h2>
<h3>Fabian Hinder, Jonathan Jakob, Barbara Hammer</h3>
<p>The notion of concept drift refers to the phenomenon that the distribution,
which is underlying the observed data, changes over time. We are interested in
an identification of those features, that are most relevant for the observed
drift. We distinguish between drift inducing features, for which the observed
feature drift cannot be explained by any other feature, and faithfully drifting
features, which correlate with the present drift of other features. This notion
gives rise to minimal subsets of the feature space, which are able to
characterize the observed drift as a whole. We relate this problem to the
problems of feature selection and feature relevance learning, which allows us
to derive a detection algorithm. We demonstrate its usefulness on different
benchmarks.
</p>
<a href="http://arxiv.org/abs/2012.00499" target="_blank">arXiv:2012.00499</a> [<a href="http://arxiv.org/pdf/2012.00499" target="_blank">pdf</a>]

<h2>A Multi-intersection Vehicular Cooperative Control based on End-Edge-Cloud Computing. (arXiv:2012.00500v1 [cs.AI])</h2>
<h3>Mingzhi Jiang, Tianhao Wu, Zhe Wang, Yi Gong, Lin Zhang, Ren Ping Liu</h3>
<p>Cooperative Intelligent Transportation Systems (C-ITS) will change the modes
of road safety and traffic management, especially at intersections without
traffic lights, namely unsignalized intersections. Existing researches focus on
vehicle control within a small area around an unsignalized intersection. In
this paper, we expand the control domain to a large area with multiple
intersections. In particular, we propose a Multi-intersection Vehicular
Cooperative Control (MiVeCC) to enable cooperation among vehicles in a large
area with multiple unsignalized intersections. Firstly, a vehicular
end-edge-cloud computing framework is proposed to facilitate end-edge-cloud
vertical cooperation and horizontal cooperation among vehicles. Then, the
vehicular cooperative control problems in the cloud and edge layers are
formulated as Markov Decision Process (MDP) and solved by two-stage
reinforcement learning. Furthermore, to deal with high-density traffic, vehicle
selection methods are proposed to reduce the state space and accelerate
algorithm convergence without performance degradation. A multi-intersection
simulation platform is developed to evaluate the proposed scheme. Simulation
results show that the proposed MiVeCC can improve travel efficiency at multiple
intersections by up to 4.59 times without collision compared with existing
methods.
</p>
<a href="http://arxiv.org/abs/2012.00500" target="_blank">arXiv:2012.00500</a> [<a href="http://arxiv.org/pdf/2012.00500" target="_blank">pdf</a>]

<h2>Boosting the Performance of Semi-Supervised Learning with Unsupervised Clustering. (arXiv:2012.00504v1 [cs.CV])</h2>
<h3>Boaz Lerner, Guy Shiran, Daphna Weinshall</h3>
<p>Recently, Semi-Supervised Learning (SSL) has shown much promise in leveraging
unlabeled data while being provided with very few labels. In this paper, we
show that ignoring the labels altogether for whole epochs intermittently during
training can significantly improve performance in the small sample regime. More
specifically, we propose to train a network on two tasks jointly. The primary
classification task is exposed to both the unlabeled and the scarcely annotated
data, whereas the secondary task seeks to cluster the data without any labels.
As opposed to hand-crafted pretext tasks frequently used in self-supervision,
our clustering phase utilizes the same classification network and head in an
attempt to relax the primary task and propagate the information from the labels
without overfitting them. On top of that, the self-supervised technique of
classifying image rotations is incorporated during the unsupervised learning
phase to stabilize training. We demonstrate our method's efficacy in boosting
several state-of-the-art SSL algorithms, significantly improving their results
and reducing running time in various standard semi-supervised benchmarks,
including 92.6% accuracy on CIFAR-10 and 96.9% on SVHN, using only 4 labels per
class in each task. We also notably improve the results in the extreme cases of
1,2 and 3 labels per class, and show that features learned by our model are
more meaningful for separating the data.
</p>
<a href="http://arxiv.org/abs/2012.00504" target="_blank">arXiv:2012.00504</a> [<a href="http://arxiv.org/pdf/2012.00504" target="_blank">pdf</a>]

<h2>Gaussian Process Based Message Filtering for Robust Multi-Agent Cooperation in the Presence of Adversarial Communication. (arXiv:2012.00508v1 [cs.RO])</h2>
<h3>Rupert Mitchell, Jan Blumenkamp, Amanda Prorok</h3>
<p>In this paper, we consider the problem of providing robustness to adversarial
communication in multi-agent systems. Specifically, we propose a solution
towards robust cooperation, which enables the multi-agent system to maintain
high performance in the presence of anonymous non-cooperative agents that
communicate faulty, misleading or manipulative information. In pursuit of this
goal, we propose a communication architecture based on Graph Neural Networks
(GNNs), which is amenable to a novel Gaussian Process (GP)-based probabilistic
model characterizing the mutual information between the simultaneous
communications of different agents due to their physical proximity and relative
position. This model allows agents to locally compute approximate posterior
probabilities, or confidences, that any given one of their communication
partners is being truthful. These confidences can be used as weights in a
message filtering scheme, thereby suppressing the influence of suspicious
communication on the receiving agent's decisions. In order to assess the
efficacy of our method, we introduce a taxonomy of non-cooperative agents,
which distinguishes them by the amount of information available to them. We
demonstrate in two distinct experiments that our method performs well across
this taxonomy, outperforming alternative methods. For all but the best informed
adversaries, our filtering method is able to reduce the impact that
non-cooperative agents cause, reducing it to the point of negligibility, and
with negligible cost to performance in the absence of adversaries.
</p>
<a href="http://arxiv.org/abs/2012.00508" target="_blank">arXiv:2012.00508</a> [<a href="http://arxiv.org/pdf/2012.00508" target="_blank">pdf</a>]

<h2>Multi-Modal Hybrid Architecture for Pedestrian Action Prediction. (arXiv:2012.00514v1 [cs.CV])</h2>
<h3>Amir Rasouli, Tiffany Yau, Mohsen Rohani, Jun Luo</h3>
<p>Pedestrian behavior prediction is one of the major challenges for intelligent
driving systems in urban environments. Pedestrians often exhibit a wide range
of behaviors and adequate interpretations of those depend on various sources of
information such as pedestrian appearance, states of other road users, the
environment layout, etc. To address this problem, we propose a novel
multi-modal prediction algorithm that incorporates different sources of
information captured from the environment to predict future crossing actions of
pedestrians. The proposed model benefits from a hybrid learning architecture
consisting of feedforward and recurrent networks for analyzing visual features
of the environment and dynamics of the scene. Using the existing 2D pedestrian
behavior benchmarks and a newly annotated 3D driving dataset, we show that our
proposed model achieves state-of-the-art performance in pedestrian crossing
prediction.
</p>
<a href="http://arxiv.org/abs/2012.00514" target="_blank">arXiv:2012.00514</a> [<a href="http://arxiv.org/pdf/2012.00514" target="_blank">pdf</a>]

<h2>One-Pixel Attack Deceives Automatic Detection of Breast Cancer. (arXiv:2012.00517v1 [cs.CV])</h2>
<h3>Joni Korpihalkola, Tuomo Sipola, Samir Puuska, Tero Kokkonen</h3>
<p>In this article we demonstrate that a state-of-the-art machine learning model
predicting whether a whole slide image contains mitosis can be fooled by
changing just a single pixel in the input image. Computer vision and machine
learning can be used to automate various tasks in cancer diagnostic and
detection. If an attacker can manipulate the automated processing, the results
can be devastating and in the worst case lead to wrong diagnostic and
treatments. In this research one-pixel attack is demonstrated in a real-life
scenario with a real tumor dataset. The results indicate that a minor one-pixel
modification of a whole slide image under analysis can affect the diagnosis.
The attack poses a threat from the cyber security perspective: the one-pixel
method can be used as an attack vector by a motivated attacker.
</p>
<a href="http://arxiv.org/abs/2012.00517" target="_blank">arXiv:2012.00517</a> [<a href="http://arxiv.org/pdf/2012.00517" target="_blank">pdf</a>]

<h2>SeMantic AnsweR Type prediction task (SMART) at ISWC 2020 Semantic Web Challenge. (arXiv:2012.00555v1 [cs.AI])</h2>
<h3>Nandana Mihindukulasooriya, Mohnish Dubey, Alfio Gliozzo, Jens Lehmann, Axel-Cyrille Ngonga Ngomo, Ricardo Usbeck</h3>
<p>Each year the International Semantic Web Conference accepts a set of Semantic
Web Challenges to establish competitions that will advance the state of the art
solutions in any given problem domain. The SeMantic AnsweR Type prediction task
(SMART) was part of ISWC 2020 challenges. Question type and answer type
prediction can play a key role in knowledge base question answering systems
providing insights that are helpful to generate correct queries or rank the
answer candidates. More concretely, given a question in natural language, the
task of SMART challenge is, to predict the answer type using a target ontology
(e.g., DBpedia or Wikidata).
</p>
<a href="http://arxiv.org/abs/2012.00555" target="_blank">arXiv:2012.00555</a> [<a href="http://arxiv.org/pdf/2012.00555" target="_blank">pdf</a>]

<h2>Iterative VAE as a predictive brain model for out-of-distribution generalization. (arXiv:2012.00557v1 [cs.LG])</h2>
<h3>Victor Boutin, Aimen Zerroug, Minju Jung, Thomas Serre</h3>
<p>Our ability to generalize beyond training data to novel, out-of-distribution,
image degradations is a hallmark of primate vision. The predictive brain,
exemplified by predictive coding networks (PCNs), has become a prominent
neuroscience theory of neural computation. Motivated by the recent successes of
variational autoencoders (VAEs) in machine learning, we rigorously derive a
correspondence between PCNs and VAEs. This motivates us to consider iterative
extensions of VAEs (iVAEs) as plausible variational extensions of the PCNs. We
further demonstrate that iVAEs generalize to distributional shifts
significantly better than both PCNs and VAEs. In addition, we propose a novel
measure of recognizability for individual samples which can be tested against
human psychophysical data. Overall, we hope this work will spur interest in
iVAEs as a promising new direction for modeling in neuroscience.
</p>
<a href="http://arxiv.org/abs/2012.00557" target="_blank">arXiv:2012.00557</a> [<a href="http://arxiv.org/pdf/2012.00557" target="_blank">pdf</a>]

<h2>Robustness Out of the Box: Compositional Representations Naturally Defend Against Black-Box Patch Attacks. (arXiv:2012.00558v1 [cs.CV])</h2>
<h3>Christian Cosgrove, Adam Kortylewski, Chenglin Yang, Alan Yuille</h3>
<p>Patch-based adversarial attacks introduce a perceptible but localized change
to the input that induces misclassification. While progress has been made in
defending against imperceptible attacks, it remains unclear how patch-based
attacks can be resisted. In this work, we study two different approaches for
defending against black-box patch attacks. First, we show that adversarial
training, which is successful against imperceptible attacks, has limited
effectiveness against state-of-the-art location-optimized patch attacks.
Second, we find that compositional deep networks, which have part-based
representations that lead to innate robustness to natural occlusion, are robust
to patch attacks on PASCAL3D+ and the German Traffic Sign Recognition
Benchmark, without adversarial training. Moreover, the robustness of
compositional models outperforms that of adversarially trained standard models
by a large margin. However, on GTSRB, we observe that they have problems
discriminating between similar traffic signs with fine-grained differences. We
overcome this limitation by introducing part-based finetuning, which improves
fine-grained recognition. By leveraging compositional representations, this is
the first work that defends against black-box patch attacks without expensive
adversarial training. This defense is more robust than adversarial training and
more interpretable because it can locate and ignore adversarial patches.
</p>
<a href="http://arxiv.org/abs/2012.00558" target="_blank">arXiv:2012.00558</a> [<a href="http://arxiv.org/pdf/2012.00558" target="_blank">pdf</a>]

<h2>Quick and Robust Feature Selection: the Strength of Energy-efficient Sparse Training for Autoencoders. (arXiv:2012.00560v1 [cs.LG])</h2>
<h3>Zahra Atashgahi, Ghada Sokar, Tim van der Lee, Elena Mocanu, Decebal Constantin Mocanu, Raymond Veldhuis, Mykola Pechenizkiy</h3>
<p>Major complications arise from the recent increase in the amount of
high-dimensional data, including high computational costs and memory
requirements. Feature selection, which identifies the most relevant and
informative attributes of a dataset, has been introduced as a solution to this
problem. Most of the existing feature selection methods are computationally
inefficient; inefficient algorithms lead to high energy consumption, which is
not desirable for devices with limited computational and energy resources. In
this paper, a novel and flexible method for unsupervised feature selection is
proposed. This method, named QuickSelection, introduces the strength of the
neuron in sparse neural networks as a criterion to measure the feature
importance. This criterion, blended with sparsely connected denoising
autoencoders trained with the sparse evolutionary training procedure, derives
the importance of all input features simultaneously. We implement
QuickSelection in a purely sparse manner as opposed to the typical approach of
using a binary mask over connections to simulate sparsity. It results in a
considerable speed increase and memory reduction. When tested on several
benchmark datasets, including five low-dimensional and three high-dimensional
datasets, the proposed method is able to achieve the best trade-off of
classification and clustering accuracy, running time, and maximum memory usage,
among widely used approaches for feature selection. Besides, our proposed
method requires the least amount of energy among the state-of-the-art
autoencoder-based feature selection methods.
</p>
<a href="http://arxiv.org/abs/2012.00560" target="_blank">arXiv:2012.00560</a> [<a href="http://arxiv.org/pdf/2012.00560" target="_blank">pdf</a>]

<h2>Facetwise Mesh Refinement for Multi-View Stereo. (arXiv:2012.00564v1 [cs.CV])</h2>
<h3>Andrea Romanoni, Matteo Matteucci</h3>
<p>Mesh refinement is a fundamental step for accurate Multi-View Stereo. It
modifies the geometry of an initial manifold mesh to minimize the photometric
error induced in a set of camera pairs. This initial mesh is usually the output
of volumetric 3D reconstruction based on min-cut over Delaunay Triangulations.
Such methods produce a significant amount of non-manifold vertices, therefore
they require a vertex split step to explicitly repair them. In this paper, we
extend this method to preemptively fix the non-manifold vertices by reasoning
directly on the Delaunay Triangulation and avoid most vertex splits. The main
contribution of this paper addresses the problem of choosing the camera pairs
adopted by the refinement process. We treat the problem as a mesh labeling
process, where each label corresponds to a camera pair. Differently from the
state-of-the-art methods, which use each camera pair to refine all the visible
parts of the mesh, we choose, for each facet, the best pair that enforces both
the overall visibility and coverage. The refinement step is applied for each
facet using only the camera pair selected. This facetwise refinement helps the
process to be applied in the most evenly way possible.
</p>
<a href="http://arxiv.org/abs/2012.00564" target="_blank">arXiv:2012.00564</a> [<a href="http://arxiv.org/pdf/2012.00564" target="_blank">pdf</a>]

<h2>Improving the Transferability of Adversarial Examples with the Adam Optimizer. (arXiv:2012.00567v1 [cs.CV])</h2>
<h3>Heng Yin, Hengwei Zhang, Jindong Wang, Ruiyu Dou</h3>
<p>Convolutional neural networks have outperformed humans in image recognition
tasks, but they remain vulnerable to attacks from adversarial examples. Since
these data are produced by adding imperceptible noise to normal images, their
existence poses potential security threats to deep learning systems.
Sophisticated adversarial examples with strong attack performance can also be
used as a tool to evaluate the robustness of a model. However, the success rate
of adversarial attacks remains to be further improved in black-box
environments. Therefore, this study combines an improved Adam gradient descent
algorithm with the iterative gradient-based attack method. The resulting Adam
Iterative Fast Gradient Method is then used to improve the transferability of
adversarial examples. Extensive experiments on ImageNet showed that the
proposed method offers a higher attack success rate than existing iterative
methods. Our best black-box attack achieved a success rate of 81.9% on a
normally trained network and 38.7% on an adversarially trained network.
</p>
<a href="http://arxiv.org/abs/2012.00567" target="_blank">arXiv:2012.00567</a> [<a href="http://arxiv.org/pdf/2012.00567" target="_blank">pdf</a>]

<h2>Multi-level Knowledge Distillation. (arXiv:2012.00573v1 [cs.CV])</h2>
<h3>Fei Ding, Feng Luo, Hongxin Hu, Yin Yang</h3>
<p>Knowledge distillation has become an important technique for model
compression and acceleration. The conventional knowledge distillation
approaches aim to transfer knowledge from teacher to student networks by
minimizing the KL-divergence between their probabilistic outputs, which only
consider the mutual relationship between individual representations of teacher
and student networks. Recently, the contrastive loss-based knowledge
distillation is proposed to enable a student to learn the instance
discriminative knowledge of a teacher by mapping the same image close and
different images far away in the representation space. However, all of these
methods ignore that the teacher's knowledge is multi-level, e.g., individual,
relational and categorical level. These different levels of knowledge cannot be
effectively captured by only one kind of supervisory signal. Here, we introduce
Multi-level Knowledge Distillation (MLKD) to transfer richer representational
knowledge from teacher to student networks. MLKD employs three novel
teacher-student similarities: individual similarity, relational similarity, and
categorical similarity, to encourage the student network to learn sample-wise,
structure-wise and category-wise knowledge in the teacher network. Experiments
demonstrate that MLKD outperforms other state-of-the-art methods on both
similar-architecture and cross-architecture tasks. We further show that MLKD
can improve the transferability of learned representations in the student
network.
</p>
<a href="http://arxiv.org/abs/2012.00573" target="_blank">arXiv:2012.00573</a> [<a href="http://arxiv.org/pdf/2012.00573" target="_blank">pdf</a>]

<h2>Obtain Employee Turnover Rate and Optimal Reduction Strategy Based On Neural Network and Reinforcement Learning. (arXiv:2012.00583v1 [cs.AI])</h2>
<h3>Xiaohan Cheng</h3>
<p>Nowadays, human resource is an important part of various resources of
enterprises. For enterprises, high-loyalty and high-quality talented persons
are often the core competitiveness of enterprises. Therefore, it is of great
practical significance to predict whether employees leave and reduce the
turnover rate of employees. First, this paper established a multi-layer
perceptron predictive model of employee turnover rate. A model based on Sarsa
which is a kind of reinforcement learning algorithm is proposed to
automatically generate a set of strategies to reduce the employee turnover
rate. These strategies are a collection of strategies that can reduce the
employee turnover rate the most and cost less from the perspective of the
enterprise, and can be used as a reference plan for the enterprise to optimize
the employee system. The experimental results show that the algorithm can
indeed improve the efficiency and accuracy of the specific strategy.
</p>
<a href="http://arxiv.org/abs/2012.00583" target="_blank">arXiv:2012.00583</a> [<a href="http://arxiv.org/pdf/2012.00583" target="_blank">pdf</a>]

<h2>Train Tracks with Gaps: Applying the Probabilistic Method to Trains. (arXiv:2012.00589v1 [cs.LG])</h2>
<h3>William Kuszmaul</h3>
<p>We identify a tradeoff curve between the number of wheels on a train car, and
the amount of track that must be installed in order to ensure that the train
car is supported by the track at all times. The goal is to build an elevated
track that covers some large distance $\ell$, but that consists primarily of
gaps, so that the total amount of feet of train track that is actually
installed is only a small fraction of $\ell$. In order so that the train track
can support the train at all points, the requirement is that as the train
drives across the track, at least one set of wheels from the rear quarter and
at least one set of wheels from the front quarter of the train must be touching
the track at all times.

We show that, if a train car has $n$ sets of wheels evenly spaced apart in
its rear and $n$ sets of wheels evenly spaced apart in its front, then it is
possible to build a train track that supports the train car but uses only
$\Theta( \ell / n )$ feet of track. We then consider what happens if the wheels
on the train car are not evenly spaced (and may even be configured
adversarially). We show that for any configuration of the train car, with $n$
wheels in each of the front and rear quarters of the car, it is possible to
build a track that supports the car for distance $\ell$ and uses only
$O\left(\frac{\ell \log n}{n}\right)$ feet of track. Additionally, we show that
there exist configurations of the train car for which this tradeoff curve is
asymptotically optimal. Both the upper and lower bounds are achieved via
applications of the probabilistic method.
</p>
<a href="http://arxiv.org/abs/2012.00589" target="_blank">arXiv:2012.00589</a> [<a href="http://arxiv.org/pdf/2012.00589" target="_blank">pdf</a>]

<h2>DeFMO: Deblurring and Shape Recovery of Fast Moving Objects. (arXiv:2012.00595v1 [cs.CV])</h2>
<h3>Denys Rozumnyi, Martin R. Oswald, Vittorio Ferrari, Jiri Matas, Marc Pollefeys</h3>
<p>Objects moving at high speed appear significantly blurred when captured with
cameras. The blurry appearance is especially ambiguous when the object has
complex shape or texture. In such cases, classical methods, or even humans, are
unable to recover the object's appearance and motion. We propose a method that,
given a single image with its estimated background, outputs the object's
appearance and position in a series of sub-frames as if captured by a
high-speed camera (i.e. temporal super-resolution). The proposed generative
model embeds an image of the blurred object into a latent space representation,
disentangles the background, and renders the sharp appearance. Inspired by the
image formation model, we design novel self-supervised loss function terms that
boost performance and show good generalization capabilities. The proposed DeFMO
method is trained on a complex synthetic dataset, yet it performs well on
real-world data from several datasets. DeFMO outperforms the state of the art
and generates high-quality temporal super-resolution frames.
</p>
<a href="http://arxiv.org/abs/2012.00595" target="_blank">arXiv:2012.00595</a> [<a href="http://arxiv.org/pdf/2012.00595" target="_blank">pdf</a>]

<h2>6.7ms on Mobile with over 78% ImageNet Accuracy: Unified Network Pruning and Architecture Search for Beyond Real-Time Mobile Acceleration. (arXiv:2012.00596v1 [cs.LG])</h2>
<h3>Zhengang Li, Geng Yuan, Wei Niu, Yanyu Li, Pu Zhao, Yuxuan Cai, Xuan Shen, Zheng Zhan, Zhenglun Kong, Qing Jin, Zhiyu Chen, Sijia Liu, Kaiyuan Yang, Bin Ren, Yanzhi Wang, Xue Lin</h3>
<p>With the increasing demand to efficiently deploy DNNs on mobile edge devices,
it becomes much more important to reduce unnecessary computation and increase
the execution speed. Prior methods towards this goal, including model
compression and network architecture search (NAS), are largely performed
independently and do not fully consider compiler-level optimizations which is a
must-do for mobile acceleration. In this work, we first propose (i) a general
category of fine-grained structured pruning applicable to various DNN layers,
and (ii) a comprehensive, compiler automatic code generation framework
supporting different DNNs and different pruning schemes, which bridge the gap
of model compression and NAS. We further propose NPAS, a compiler-aware unified
network pruning, and architecture search. To deal with large search space, we
propose a meta-modeling procedure based on reinforcement learning with fast
evaluation and Bayesian optimization, ensuring the total number of training
epochs comparable with representative NAS frameworks. Our framework achieves
6.7ms, 5.9ms, 3.9ms ImageNet inference times with 78.2%, 75% (MobileNet-V3
level), and 71% (MobileNet-V2 level) Top-1 accuracy respectively on an
off-the-shelf mobile phone, consistently outperforming prior work.
</p>
<a href="http://arxiv.org/abs/2012.00596" target="_blank">arXiv:2012.00596</a> [<a href="http://arxiv.org/pdf/2012.00596" target="_blank">pdf</a>]

<h2>Enabling Fingerprint Presentation Attacks: Fake Fingerprint Fabrication Techniques and Recognition Performance. (arXiv:2012.00606v1 [cs.CV])</h2>
<h3>Christof Kauba, Luca Debiasi, Andreas Uhl</h3>
<p>Fake fingerprint representation pose a severe threat for fingerprint based
authentication systems. Despite advances in presentation attack detection
technologies, which are often integrated directly into the fingerprint scanner
devices, many fingerprint scanners are still susceptible to presentation
attacks using physical fake fingerprint representation. In this work we
evaluate five different commercial-off-the-shelf fingerprint scanners based on
different sensing technologies, including optical, optical multispectral,
passive capacitive, active capacitive and thermal regarding their
susceptibility to presentation attacks using fake fingerprint representations.
Several different materials to create the fake representation are tested and
evaluated, including wax, cast, latex, silicone, different types of glue,
window colours, modelling clay, etc. The quantitative evaluation includes
assessing the fingerprint quality of the samples captured from the fake
representations as well as comparison experiments where the achieved matching
scores of the fake representations against the corresponding real fingerprints
indicate the effectiveness of the fake representations. Our results confirmed
that all except one of the tested devices are susceptible to at least one
type/material of fake fingerprint representations.
</p>
<a href="http://arxiv.org/abs/2012.00606" target="_blank">arXiv:2012.00606</a> [<a href="http://arxiv.org/pdf/2012.00606" target="_blank">pdf</a>]

<h2>We are More than Our Joints: Predicting how 3D Bodies Move. (arXiv:2012.00619v1 [cs.CV])</h2>
<h3>Yan Zhang, Michael J. Black, Siyu Tang</h3>
<p>A key step towards understanding human behavior is the prediction of 3D human
motion. Successful solutions have many applications in human tracking, HCI, and
graphics. Most previous work focuses on predicting a time series of future 3D
joint locations given a sequence 3D joints from the past. This Euclidean
formulation generally works better than predicting pose in terms of joint
rotations. Body joint locations, however, do not fully constrain 3D human pose,
leaving degrees of freedom undefined, making it hard to animate a realistic
human from only the joints. Note that the 3D joints can be viewed as a sparse
point cloud. Thus the problem of human motion prediction can be seen as point
cloud prediction. With this observation, we instead predict a sparse set of
locations on the body surface that correspond to motion capture markers. Given
such markers, we fit a parametric body model to recover the 3D shape and pose
of the person. These sparse surface markers also carry detailed information
about human movement that is not present in the joints, increasing the
naturalness of the predicted motions. Using the AMASS dataset, we train MOJO,
which is a novel variational autoencoder that generates motions from latent
frequencies. MOJO preserves the full temporal resolution of the input motion,
and sampling from the latent frequencies explicitly introduces high-frequency
components into the generated motion. We note that motion prediction methods
accumulate errors over time, resulting in joints or markers that diverge from
true human bodies. To address this, we fit SMPL-X to the predictions at each
time step, projecting the solution back onto the space of valid bodies. These
valid markers are then propagated in time. Experiments show that our method
produces state-of-the-art results and realistic 3D body animations. The code
for research purposes is at https://yz-cnsdqz.github.io/MOJO/MOJO.html
</p>
<a href="http://arxiv.org/abs/2012.00619" target="_blank">arXiv:2012.00619</a> [<a href="http://arxiv.org/pdf/2012.00619" target="_blank">pdf</a>]

<h2>Structured Context Enhancement Network for Mouse Pose Estimation. (arXiv:2012.00630v1 [cs.CV])</h2>
<h3>Feixiang Zhou, Zheheng Jiang, Zhihua Liu, Fang Chen, Long Chen, Lei Tong, Zhile Yang, Haikuan Wang, Minrui Fei, Ling Li, Huiyu Zhou</h3>
<p>Automated analysis of mouse behaviours is crucial for many applications in
neuroscience. However, quantifying mouse behaviours from videos or images
remains a challenging problem, where pose estimation plays an important role in
describing mouse behaviours. Although deep learning based methods have made
promising advances in mouse or other animal pose estimation, they cannot
properly handle complicated scenarios (e.g., occlusions, invisible keypoints,
and abnormal poses). Particularly, since mouse body is highly deformable, it is
a big challenge to accurately locate different keypoints on the mouse body. In
this paper, we propose a novel hourglass network based model, namely Graphical
Model based Structured Context Enhancement Network (GM-SCENet) where two
effective modules, i.e., Structured Context Mixer (SCM) and Cascaded
Multi-Level Supervision module (CMLS) are designed. The SCM can adaptively
learn and enhance the proposed structured context information of each mouse
part by a novel graphical model with close consideration on the difference
between body parts. Then, the CMLS module is designed to jointly train the
proposed SCM and the hourglass network by generating multi-level information,
which increases the robustness of the whole network. Based on the multi-level
predictions from the SCM and the CMLS module, we also propose an inference
method to enhance the localization results. Finally, we evaluate our proposed
approach against several baselines...
</p>
<a href="http://arxiv.org/abs/2012.00630" target="_blank">arXiv:2012.00630</a> [<a href="http://arxiv.org/pdf/2012.00630" target="_blank">pdf</a>]

<h2>Communication-Efficient Federated Distillation. (arXiv:2012.00632v1 [cs.LG])</h2>
<h3>Felix Sattler, Arturo Marban, Roman Rischke, Wojciech Samek</h3>
<p>Communication constraints are one of the major challenges preventing the
wide-spread adoption of Federated Learning systems. Recently, Federated
Distillation (FD), a new algorithmic paradigm for Federated Learning with
fundamentally different communication properties, emerged. FD methods leverage
ensemble distillation techniques and exchange model outputs, presented as soft
labels on an unlabeled public data set, between the central server and the
participating clients. While for conventional Federated Learning algorithms,
like Federated Averaging (FA), communication scales with the size of the
jointly trained model, in FD communication scales with the distillation data
set size, resulting in advantageous communication properties, especially when
large models are trained. In this work, we investigate FD from the perspective
of communication efficiency by analyzing the effects of active
distillation-data curation, soft-label quantization and delta-coding
techniques. Based on the insights gathered from this analysis, we present
Compressed Federated Distillation (CFD), an efficient Federated Distillation
method. Extensive experiments on Federated image classification and language
modeling problems demonstrate that our method can reduce the amount of
communication necessary to achieve fixed performance targets by more than two
orders of magnitude, when compared to FD and by more than four orders of
magnitude when compared with FA.
</p>
<a href="http://arxiv.org/abs/2012.00632" target="_blank">arXiv:2012.00632</a> [<a href="http://arxiv.org/pdf/2012.00632" target="_blank">pdf</a>]

<h2>Deep dynamic modeling with just two time points: Can we still allow for individual trajectories?. (arXiv:2012.00634v1 [stat.ML])</h2>
<h3>Maren Hackenberg, Philipp Harms, Thorsten Schmidt, Harald Binder</h3>
<p>Longitudinal biomedical data are often characterized by a sparse time grid
and individual-specific development patterns. Specifically, in epidemiological
cohort studies and clinical registries we are facing the question of what can
be learned from the data in an early phase of the study, when only a baseline
characterization and one follow-up measurement are available. Inspired by
recent advances that allow to combine deep learning with dynamic modeling, we
investigate whether such approaches can be useful for uncovering complex
structure, in particular for an extreme small data setting with only two
observations time points for each individual. Irregular spacing in time could
then be used to gain more information on individual dynamics by leveraging
similarity of individuals. We provide a brief overview of how variational
autoencoders (VAEs), as a deep learning approach, can be linked to ordinary
differential equations (ODEs) for dynamic modeling, and then specifically
investigate the feasibility of such an approach that infers individual-specific
latent trajectories by including regularity assumptions and individuals'
similarity. We also provide a description of this deep learning approach as a
filtering task to give a statistical perspective. Using simulated data, we show
to what extent the approach can recover individual trajectories from ODE
systems with two and four unknown parameters and infer groups of individuals
with similar trajectories, and where it breaks down. The results show that such
dynamic deep learning approaches can be useful even in extreme small data
settings, but need to be carefully adapted.
</p>
<a href="http://arxiv.org/abs/2012.00634" target="_blank">arXiv:2012.00634</a> [<a href="http://arxiv.org/pdf/2012.00634" target="_blank">pdf</a>]

<h2>A Decade Survey of Content Based Image Retrieval using Deep Learning. (arXiv:2012.00641v1 [cs.CV])</h2>
<h3>Shiv Ram Dubey</h3>
<p>The content based image retrieval aims to find the similar images from a
large scale dataset against a query image. Generally, the similarity between
the representative features of the query image and dataset images is used to
rank the images for retrieval. In early days, various hand designed feature
descriptors have been investigated based on the visual cues such as color,
texture, shape, etc. that represent the images. However, the deep learning has
emerged as a dominating alternative of hand-designed feature engineering from a
decade. It learns the features automatically from the data. This paper presents
a comprehensive survey of deep learning based developments in the past decade
for content based image retrieval. The categorization of existing
state-of-the-art methods from different perspectives is also performed for
greater understanding of the progress. The taxonomy used in this survey covers
different supervision, different networks, different descriptor type and
different retrieval type. A performance analysis is also performed using the
state-of-the-art methods. The insights are also presented for the benefit of
the researchers to observe the progress and to make the best choices. The
survey presented in this paper will help in further research progress in image
retrieval using deep learning.
</p>
<a href="http://arxiv.org/abs/2012.00641" target="_blank">arXiv:2012.00641</a> [<a href="http://arxiv.org/pdf/2012.00641" target="_blank">pdf</a>]

<h2>Procedure for the Safety Assessment of an Autonomous Vehicle Using Real-World Scenarios. (arXiv:2012.00643v1 [cs.RO])</h2>
<h3>Erwin de Gelder, Olaf Op den Camp</h3>
<p>The development of Autonomous Vehicles (AVs) has made significant progress in
the last years. An important aspect in the development of AVs is the assessment
of their safety. New approaches need to be worked out. Among these, real-world
scenario-based assessment is widely supported by many players in the automotive
field. Scenario-based assessment allows for using virtual simulation tools in
addition to physical tests, such as on a test track, proving ground, or public
road.

We propose a procedure for real-world scenario-based road-approval assessment
considering three stakeholders: the applicant, the assessor, and the (road or
vehicle) authority. The challenges are as follows. Firstly, the tests need to
be tailored to the operational design domain (ODD) and dynamic driving task
(DDT) description of the AV. Secondly, it is assumed that the applicant does
not want to disclose all of the detailed test results because of proprietary or
confidential information contained in these results. Thirdly, due to the
complex ODD and DDT, many test scenarios are required to obtain sufficient
confidence in the assessment of the AV. Consequently, it is assumed that due to
limited resources, it is infeasible for the assessor to conduct all (physical)
tests.

We propose a systematic approach for determining the tests that are based on
the requirements set by the authority and the AV's ODD and DDT description,
such that the tests are tailored to the applicable ODD and DDT. Each test comes
with metrics that enables the applicant to provide a performance rating of the
AV for each of the tests. By only providing a performance rating for each test,
the applicant does not need to disclose the details of the test results. In our
proposed procedure, the assessor only conducts a limited number of tests. The
main purpose of these tests is to verify the fidelity of the results provided
by the applicant.
</p>
<a href="http://arxiv.org/abs/2012.00643" target="_blank">arXiv:2012.00643</a> [<a href="http://arxiv.org/pdf/2012.00643" target="_blank">pdf</a>]

<h2>Unpaired Image-to-Image Translation via Latent Energy Transport. (arXiv:2012.00649v1 [cs.CV])</h2>
<h3>Yang Zhao, Changyou Chen</h3>
<p>Image-to-image translation aims to preserve source contents while translating
to discriminative target styles between two visual domains. Most works apply
adversarial learning in the ambient image space, which could be computationally
expensive and challenging to train. In this paper, we propose to deploy an
energy-based model (EBM) in the latent space of a pretrained autoencoder for
this task. The pretrained autoencoder serves as both a latent code extractor
and an image reconstruction worker. Our model is based on the assumption that
two domains share the same latent space, where latent representation is
implicitly decomposed as a content code and a domain-specific style code.
Instead of explicitly extracting the two codes and applying adaptive instance
normalization to combine them, our latent EBM can implicitly learn to transport
the source style code to the target style code while preserving the content
code, which is an advantage over existing image translation methods. This
simplified solution also brings us far more efficiency in the one-sided
unpaired image translation setting. Qualitative and quantitative comparisons
demonstrate superior translation quality and faithfulness for content
preservation. To the best of our knowledge, our model is the first to be
applicable to 1024$\times$1024-resolution unpaired image translation.
</p>
<a href="http://arxiv.org/abs/2012.00649" target="_blank">arXiv:2012.00649</a> [<a href="http://arxiv.org/pdf/2012.00649" target="_blank">pdf</a>]

<h2>Decomposition, Compression, and Synthesis (DCS)-based Video Coding: A Neural Exploration via Resolution-Adaptive Learning. (arXiv:2012.00650v1 [cs.CV])</h2>
<h3>Ming Lu, Tong Chen, Dandan Ding, Fengqing Zhu, Zhan Ma</h3>
<p>Inspired by the facts that retinal cells actually segregate the visual scene
into different attributes (e.g., spatial details, temporal motion) for
respective neuronal processing, we propose to first decompose the input video
into respective spatial texture frames (STF) at its native spatial resolution
that preserve the rich spatial details, and the other temporal motion frames
(TMF) at a lower spatial resolution that retain the motion smoothness; then
compress them together using any popular video coder; and finally synthesize
decoded STFs and TMFs for high-fidelity video reconstruction at the same
resolution as its native input. This work simply applies the bicubic resampling
in decomposition and HEVC compliant codec in compression, and puts the focus on
the synthesis part. For resolution-adaptive synthesis, a motion compensation
network (MCN) is devised on TMFs to efficiently align and aggregate temporal
motion features that will be jointly processed with corresponding STFs using a
non-local texture transfer network (NL-TTN) to better augment spatial details,
by which the compression and resolution resampling noises can be effectively
alleviated with better rate-distortion efficiency. Such "Decomposition,
Compression, Synthesis (DCS)" based scheme is codec agnostic, currently
exemplifying averaged $\approx$1 dB PSNR gain or $\approx$25% BD-rate saving,
against the HEVC anchor using reference software. In addition, experimental
comparisons to the state-of-the-art methods and ablation studies are conducted
to further report the efficiency and generalization of DCS algorithm, promising
an encouraging direction for future video coding.
</p>
<a href="http://arxiv.org/abs/2012.00650" target="_blank">arXiv:2012.00650</a> [<a href="http://arxiv.org/pdf/2012.00650" target="_blank">pdf</a>]

<h2>Cross-modal registration using point clouds and graph-matching in the context of correlative microscopies. (arXiv:2012.00656v1 [cs.CV])</h2>
<h3>Stephan Kunne (1), Guillaume Potier (1), Jean M&#xe9;rot (1), Perrine Paul-Gilloteaux (1 and 2) ((1) l&#x27;institut du thorax Nantes (2) MicroPICell SFR Sante F. Bonamy)</h3>
<p>Correlative microscopy aims at combining two or more modalities to gain more
information than the one provided by one modality on the same biological
structure. Registration is needed at different steps of correlative
microscopies workflows. Biologists want to select the image content used for
registration not to introduce bias in the correlation of unknown structures.
Intensity-based methods might not allow this selection and might be too slow
when the images are very large. We propose an approach based on point clouds
created from selected content by the biologist. These point clouds may be prone
to big differences in densities but also missing parts and outliers. In this
paper we present a method of registration for point clouds based on graph
building and graph matching, and compare the method to iterative closest point
based methods.
</p>
<a href="http://arxiv.org/abs/2012.00656" target="_blank">arXiv:2012.00656</a> [<a href="http://arxiv.org/pdf/2012.00656" target="_blank">pdf</a>]

<h2>Learning Sampling Distributions for Efficient High-Dimensional Motion Planning. (arXiv:2012.00658v1 [cs.RO])</h2>
<h3>Naman Shah, Abhyudaya Srinet, Siddharth Srivastava</h3>
<p>Robot motion planning involves computing a sequence of valid robot
configurations that take the robot from its initial state to a goal state.
Solving a motion planning problem optimally using analytical methods is proven
to be PSPACE-Hard. Sampling-based approaches have tried to approximate the
optimal solution efficiently. Generally, sampling-based planners use uniform
samplers to cover the entire state space. In this paper, we propose a
deep-learning-based framework that identifies robot configurations in the
environment that are important to solve the given motion planning problem.
These states are used to bias the sampling distribution in order to reduce the
planning time. Our approach works with a unified network and generates
domain-dependent network parameters based on the environment and the robot. We
evaluate our approach with Learn and Link planner in three different settings.
Results show significant improvement in motion planning times when compared
with current sampling-based motion planners.
</p>
<a href="http://arxiv.org/abs/2012.00658" target="_blank">arXiv:2012.00658</a> [<a href="http://arxiv.org/pdf/2012.00658" target="_blank">pdf</a>]

<h2>Emotion Detection using Image Processing in Python. (arXiv:2012.00659v1 [cs.CV])</h2>
<h3>Raghav Puri, Archit Gupta, Manas Sikri, Mohit Tiwari, Nitish Pathak, Shivendra Goel</h3>
<p>In this work, user's emotion using its facial expressions will be detected.
These expressions can be derived from the live feed via system's camera or any
pre-exisiting image available in the memory. Emotions possessed by humans can
be recognized and has a vast scope of study in the computer vision industry
upon which several researches have already been done. The work has been
implemented using Python (2.7, Open Source Computer Vision Library (OpenCV) and
NumPy. The scanned image(testing dataset) is being compared to the training
dataset and thus emotion is predicted. The objective of this paper is to
develop a system which can analyze the image and predict the expression of the
person. The study proves that this procedure is workable and produces valid
results.
</p>
<a href="http://arxiv.org/abs/2012.00659" target="_blank">arXiv:2012.00659</a> [<a href="http://arxiv.org/pdf/2012.00659" target="_blank">pdf</a>]

<h2>Fast-Convergent Federated Learning with Adaptive Weighting. (arXiv:2012.00661v1 [cs.LG])</h2>
<h3>Hongda Wu, Ping Wang</h3>
<p>Federated learning (FL) enables resource-constrained edge nodes to
collaboratively learn a global model under the orchestration of a central
server while keeping privacy-sensitive data locally. The
non-independent-and-identically-distributed (non-IID) data samples across
participating nodes slow model training and impose additional communication
rounds for FL to converge. In this paper, we propose Federated Adaptive
Weighting (FedAdp) algorithm that aims to accelerate model convergence under
the presence of nodes with non-IID dataset. We observe the implicit connection
between the node contribution to the global model aggregation and data
distribution on the local node through theoretical and empirical analysis. We
then propose to assign different weights for updating the global model based on
node contribution adaptively through each training round. The contribution of
participating nodes is first measured by the angle between the local gradient
vector and the global gradient vector, and then, weight is quantified by a
designed non-linear mapping function subsequently. The simple yet effective
strategy can reinforce positive (suppress negative) node contribution
dynamically, resulting in communication round reduction drastically. Its
superiority over the commonly adopted Federated Averaging (FedAvg) is verified
both theoretically and experimentally. With extensive experiments performed in
Pytorch and PySyft, we show that FL training with FedAdp can reduce the number
of communication rounds by up to 54.1% on MNIST dataset and up to 45.4% on
FashionMNIST dataset, as compared to FedAvg algorithm.
</p>
<a href="http://arxiv.org/abs/2012.00661" target="_blank">arXiv:2012.00661</a> [<a href="http://arxiv.org/pdf/2012.00661" target="_blank">pdf</a>]

<h2>Learning Disentangled Latent Factors from Paired Data in Cross-Modal Retrieval: An Implicit Identifiable VAE Approach. (arXiv:2012.00682v1 [cs.LG])</h2>
<h3>Minyoung Kim, Ricardo Guerrero, Vladimir Pavlovic</h3>
<p>We deal with the problem of learning the underlying disentangled latent
factors that are shared between the paired bi-modal data in cross-modal
retrieval. Our assumption is that the data in both modalities are complex,
structured, and high dimensional (e.g., image and text), for which the
conventional deep auto-encoding latent variable models such as the Variational
Autoencoder (VAE) often suffer from difficulty of accurate decoder training or
realistic synthesis. A suboptimally trained decoder can potentially harm the
model's capability of identifying the true factors. In this paper we propose a
novel idea of the implicit decoder, which completely removes the ambient data
decoding module from a latent variable model, via implicit encoder inversion
that is achieved by Jacobian regularization of the low-dimensional embedding
function. Motivated from the recent Identifiable VAE (IVAE) model, we modify it
to incorporate the query modality data as conditioning auxiliary input, which
allows us to prove that the true parameters of the model can be identified
under some regularity conditions. Tested on various datasets where the true
factors are fully/partially available, our model is shown to identify the
factors accurately, significantly outperforming conventional encoder-decoder
latent variable models. We also test our model on the Recipe1M, the large-scale
food image/recipe dataset, where the learned factors by our approach highly
coincide with the most pronounced food factors that are widely agreed on,
including savoriness, wateriness, and greenness.
</p>
<a href="http://arxiv.org/abs/2012.00682" target="_blank">arXiv:2012.00682</a> [<a href="http://arxiv.org/pdf/2012.00682" target="_blank">pdf</a>]

<h2>Mutual Information Constraints for Monte-Carlo Objectives. (arXiv:2012.00708v1 [stat.ML])</h2>
<h3>G&#xe1;bor Melis, Andr&#xe1;s Gy&#xf6;rgy, Phil Blunsom</h3>
<p>A common failure mode of density models trained as variational autoencoders
is to model the data without relying on their latent variables, rendering these
variables useless. Two contributing factors, the underspecification of the
model and the looseness of the variational lower bound, have been studied
separately in the literature. We weave these two strands of research together,
specifically the tighter bounds of Monte-Carlo objectives and constraints on
the mutual information between the observable and the latent variables.
Estimating the mutual information as the average Kullback-Leibler divergence
between the easily available variational posterior $q(z|x)$ and the prior does
not work with Monte-Carlo objectives because $q(z|x)$ is no longer a direct
approximation to the model's true posterior $p(z|x)$. Hence, we construct
estimators of the Kullback-Leibler divergence of the true posterior from the
prior by recycling samples used in the objective, with which we train models of
continuous and discrete latents at much improved rate-distortion and no
posterior collapse. While alleviated, the tradeoff between modelling the data
and using the latents still remains, and we urge for evaluating inference
methods across a range of mutual information values.
</p>
<a href="http://arxiv.org/abs/2012.00708" target="_blank">arXiv:2012.00708</a> [<a href="http://arxiv.org/pdf/2012.00708" target="_blank">pdf</a>]

<h2>Debiasing Evaluations That are Biased by Evaluations. (arXiv:2012.00714v1 [stat.ML])</h2>
<h3>Jingyan Wang, Ivan Stelmakh, Yuting Wei, Nihar B. Shah</h3>
<p>It is common to evaluate a set of items by soliciting people to rate them.
For example, universities ask students to rate the teaching quality of their
instructors, and conference organizers ask authors of submissions to evaluate
the quality of the reviews. However, in these applications, students often give
a higher rating to a course if they receive higher grades in a course, and
authors often give a higher rating to the reviews if their papers are accepted
to the conference. In this work, we call these external factors the "outcome"
experienced by people, and consider the problem of mitigating these
outcome-induced biases in the given ratings when some information about the
outcome is available. We formulate the information about the outcome as a known
partial ordering on the bias. We propose a debiasing method by solving a
regularized optimization problem under this ordering constraint, and also
provide a carefully designed cross-validation method that adaptively chooses
the appropriate amount of regularization. We provide theoretical guarantees on
the performance of our algorithm, as well as experimental evaluations.
</p>
<a href="http://arxiv.org/abs/2012.00714" target="_blank">arXiv:2012.00714</a> [<a href="http://arxiv.org/pdf/2012.00714" target="_blank">pdf</a>]

<h2>Simulating Surface Wave Dynamics with Convolutional Networks. (arXiv:2012.00718v1 [cs.LG])</h2>
<h3>Mario Lino, Chris Cantwell, Stathi Fotiadis, Eduardo Pignatelli, Anil Bharath</h3>
<p>We investigate the performance of fully convolutional networks to simulate
the motion and interaction of surface waves in open and closed complex
geometries. We focus on a U-Net architecture and analyse how well it
generalises to geometric configurations not seen during training. We
demonstrate that a modified U-Net architecture is capable of accurately
predicting the height distribution of waves on a liquid surface within curved
and multi-faceted open and closed geometries, when only simple box and
right-angled corner geometries were seen during training. We also consider a
separate and independent 3D CNN for performing time-interpolation on the
predictions produced by our U-Net. This allows generating simulations with a
smaller time-step size than the one the U-Net has been trained for.
</p>
<a href="http://arxiv.org/abs/2012.00718" target="_blank">arXiv:2012.00718</a> [<a href="http://arxiv.org/pdf/2012.00718" target="_blank">pdf</a>]

<h2>Fully Convolutional Networks for Panoptic Segmentation. (arXiv:2012.00720v1 [cs.CV])</h2>
<h3>Yanwei Li, Hengshuang Zhao, Xiaojuan Qi, Liwei Wang, Zeming Li, Jian Sun, Jiaya Jia</h3>
<p>In this paper, we present a conceptually simple, strong, and efficient
framework for panoptic segmentation, called Panoptic FCN. Our approach aims to
represent and predict foreground things and background stuff in a unified fully
convolutional pipeline. In particular, Panoptic FCN encodes each object
instance or stuff category into a specific kernel weight with the proposed
kernel generator and produces the prediction by convolving the high-resolution
feature directly. With this approach, instance-aware and semantically
consistent properties for things and stuff can be respectively satisfied in a
simple generate-kernel-then-segment workflow. Without extra boxes for
localization or instance separation, the proposed approach outperforms previous
box-based and -free models with high efficiency on COCO, Cityscapes, and
Mapillary Vistas datasets with single scale input. Our code is made publicly
available at https://github.com/yanwei-li/PanopticFCN.
</p>
<a href="http://arxiv.org/abs/2012.00720" target="_blank">arXiv:2012.00720</a> [<a href="http://arxiv.org/pdf/2012.00720" target="_blank">pdf</a>]

<h2>Assessing and Accelerating Coverage in Deep Reinforcement Learning. (arXiv:2012.00724v1 [cs.LG])</h2>
<h3>Arpan Kusari</h3>
<p>Current deep reinforcement learning (DRL) algorithms utilize randomness in
simulation environments to assume complete coverage in the state space.
However, particularly in high dimensions, relying on randomness may lead to
gaps in coverage of the trained DRL neural network model, which in turn may
lead to drastic and often fatal real-world situations. To the best of the
author's knowledge, the assessment of coverage for DRL is lacking in current
research literature. Therefore, in this paper, a novel measure, Approximate
Pseudo-Coverage (APC), is proposed for assessing the coverage in DRL
applications. We propose to calculate APC by projecting the high dimensional
state space on to a lower dimensional manifold and quantifying the occupied
space. Furthermore, we utilize an exploration-exploitation strategy for
coverage maximization using Rapidly-Exploring Random Tree (RRT). The efficacy
of the assessment and the acceleration of coverage is demonstrated on standard
tasks such as Cartpole, highway-env.
</p>
<a href="http://arxiv.org/abs/2012.00724" target="_blank">arXiv:2012.00724</a> [<a href="http://arxiv.org/pdf/2012.00724" target="_blank">pdf</a>]

<h2>RAFT-3D: Scene Flow using Rigid-Motion Embeddings. (arXiv:2012.00726v1 [cs.CV])</h2>
<h3>Zachary Teed, Jia Deng</h3>
<p>We address the problem of scene flow: given a pair of stereo or RGB-D video
frames, estimate pixelwise 3D motion. We introduce RAFT-3D, a new deep
architecture for scene flow. RAFT-3D is based on the RAFT model developed for
optical flow but iteratively updates a dense field of pixelwise SE3 motion
instead of 2D motion. A key innovation of RAFT-3D is rigid-motion embeddings,
which represent a soft grouping of pixels into rigid objects. Integral to
rigid-motion embeddings is Dense-SE3, a differentiable layer that enforces
geometric consistency of the embeddings. Experiments show that RAFT-3D achieves
state-of-the-art performance. On FlyingThings3D, under the two-view evaluation,
we improved the best published accuracy (d &lt; 0.05) from 30.33% to 83.71%. On
KITTI, we achieve an error of 5.77, outperforming the best published method
(6.31), despite using no object instance supervision.
</p>
<a href="http://arxiv.org/abs/2012.00726" target="_blank">arXiv:2012.00726</a> [<a href="http://arxiv.org/pdf/2012.00726" target="_blank">pdf</a>]

<h2>Convergence and Sample Complexity of SGD in GANs. (arXiv:2012.00732v1 [cs.LG])</h2>
<h3>Vasilis Kontonis, Sihan Liu, Christos Tzamos</h3>
<p>We provide theoretical convergence guarantees on training Generative
Adversarial Networks (GANs) via SGD. We consider learning a target distribution
modeled by a 1-layer Generator network with a non-linear activation function
$\phi(\cdot)$ parametrized by a $d \times d$ weight matrix $\mathbf W_*$, i.e.,
$f_*(\mathbf x) = \phi(\mathbf W_* \mathbf x)$.

Our main result is that by training the Generator together with a
Discriminator according to the Stochastic Gradient Descent-Ascent iteration
proposed by Goodfellow et al. yields a Generator distribution that approaches
the target distribution of $f_*$. Specifically, we can learn the target
distribution within total-variation distance $\epsilon$ using $\tilde
O(d^2/\epsilon^2)$ samples which is (near-)information theoretically optimal.

Our results apply to a broad class of non-linear activation functions $\phi$,
including ReLUs and is enabled by a connection with truncated statistics and an
appropriate design of the Discriminator network. Our approach relies on a
bilevel optimization framework to show that vanilla SGDA works.
</p>
<a href="http://arxiv.org/abs/2012.00732" target="_blank">arXiv:2012.00732</a> [<a href="http://arxiv.org/pdf/2012.00732" target="_blank">pdf</a>]

<h2>GLEAN: Generative Latent Bank for Large-Factor Image Super-Resolution. (arXiv:2012.00739v1 [cs.CV])</h2>
<h3>Kelvin C.K. Chan, Xintao Wang, Xiangyu Xu, Jinwei Gu, Chen Change Loy</h3>
<p>We show that pre-trained Generative Adversarial Networks (GANs), e.g.,
StyleGAN, can be used as a latent bank to improve the restoration quality of
large-factor image super-resolution (SR). While most existing SR approaches
attempt to generate realistic textures through learning with adversarial loss,
our method, Generative LatEnt bANk (GLEAN), goes beyond existing practices by
directly leveraging rich and diverse priors encapsulated in a pre-trained GAN.
But unlike prevalent GAN inversion methods that require expensive
image-specific optimization at runtime, our approach only needs a single
forward pass to generate the upscaled image. GLEAN can be easily incorporated
in a simple encoder-bank-decoder architecture with multi-resolution skip
connections. Switching the bank allows the method to deal with images from
diverse categories, e.g., cat, building, human face, and car. Images upscaled
by GLEAN show clear improvements in terms of fidelity and texture faithfulness
in comparison to existing methods.
</p>
<a href="http://arxiv.org/abs/2012.00739" target="_blank">arXiv:2012.00739</a> [<a href="http://arxiv.org/pdf/2012.00739" target="_blank">pdf</a>]

<h2>Minimum weight norm models do not always generalize well for over-parameterized problems. (arXiv:1811.07055v3 [stat.ML] UPDATED)</h2>
<h3>Vatsal Shah, Anastasios Kyrillidis, Sujay Sanghavi</h3>
<p>This work is substituted by the paper in arXiv:2011.14066.

Stochastic gradient descent is the de facto algorithm for training deep
neural networks (DNNs). Despite its popularity, it still requires fine tuning
in order to achieve its best performance. This has led to the development of
adaptive methods, that claim automatic hyper-parameter optimization. Recently,
researchers have studied both algorithmic classes via toy examples: e.g., for
over-parameterized linear regression, Wilson et. al. (2017) shows that, while
SGD always converges to the minimum-norm solution, adaptive methods show no
such inclination, leading to worse generalization capabilities. Our aim is to
study this conjecture further. We empirically show that the minimum weight norm
is not necessarily the proper gauge of good generalization in simplified
scenaria, and different models found by adaptive methods could outperform plain
gradient methods. In practical DNN settings, we observe that adaptive methods
can outperform SGD, with larger weight norm output models, but without
necessarily reducing the amount of tuning required.
</p>
<a href="http://arxiv.org/abs/1811.07055" target="_blank">arXiv:1811.07055</a> [<a href="http://arxiv.org/pdf/1811.07055" target="_blank">pdf</a>]

<h2>Semi-Supervised Regression using Cluster Ensemble and Low-Rank Co-Association Matrix Decomposition under Uncertainties. (arXiv:1901.03919v2 [stat.ML] UPDATED)</h2>
<h3>Vladimir Berikov, Alexander Litvinenko</h3>
<p>In this paper, we solve a semi-supervised regression problem. Due to the lack
of knowledge about the data structure and the presence of random noise, the
considered data model is uncertain. We propose a method which combines graph
Laplacian regularization and cluster ensemble methodologies. The co-association
matrix of the ensemble is calculated on both labeled and unlabeled data; this
matrix is used as a similarity matrix in the regularization framework to derive
the predicted outputs. We use the low-rank decomposition of the co-association
matrix to significantly speedup calculations and reduce memory. Numerical
experiments using the Monte Carlo approach demonstrate robustness, efficiency,
and scalability of the proposed method.
</p>
<a href="http://arxiv.org/abs/1901.03919" target="_blank">arXiv:1901.03919</a> [<a href="http://arxiv.org/pdf/1901.03919" target="_blank">pdf</a>]

<h2>Robust Semantic Segmentation By Dense Fusion Network On Blurred VHR Remote Sensing Images. (arXiv:1903.02702v2 [cs.CV] UPDATED)</h2>
<h3>Yi Peng, Shihao Sun, Zheng Wang, Yining Pan, Ruirui Li</h3>
<p>Robust semantic segmentation of VHR remote sensing images from UAV sensors is
critical for earth observation, land use, land cover or mapping applications.
Several factors such as shadows, weather disruption and camera shakes making
this problem highly challenging, especially only using RGB images. In this
paper, we propose the use of multi-modality data including NIR, RGB and DSM to
increase robustness of segmentation in blurred or partially damaged VHR remote
sensing images. By proposing a cascaded dense encoder-decoder network and the
SELayer based fusion and assembling techniques, the proposed RobustDenseNet
achieves steady performance when the image quality is decreasing, compared with
the state-of-the-art semantic segmentation model.
</p>
<a href="http://arxiv.org/abs/1903.02702" target="_blank">arXiv:1903.02702</a> [<a href="http://arxiv.org/pdf/1903.02702" target="_blank">pdf</a>]

<h2>Concentration bounds for linear Monge mapping estimation and optimal transport domain adaptation. (arXiv:1905.10155v2 [stat.ML] UPDATED)</h2>
<h3>R&#xe9;mi Flamary, Karim Lounici, Andr&#xe9; Ferrari</h3>
<p>This article investigates the quality of the estimator of the linear Monge
mapping between distributions. We provide the first concentration result on the
linear mapping operator and prove a sample complexity of $n^{-1/2}$ when using
empirical estimates of first and second order moments. This result is then used
to derive a generalization bound for domain adaptation with optimal transport.
As a consequence, this method approaches the performance of theoretical Bayes
predictor under mild conditions on the covariance structure of the problem. We
also discuss the computational complexity of the linear mapping estimation and
show that when the source and target are stationary the mapping is a
convolution that can be estimated very efficiently using fast Fourier
transforms. Numerical experiments reproduce the behavior of the proven bounds
on simulated and real data for mapping estimation and domain adaptation on
images.
</p>
<a href="http://arxiv.org/abs/1905.10155" target="_blank">arXiv:1905.10155</a> [<a href="http://arxiv.org/pdf/1905.10155" target="_blank">pdf</a>]

<h2>Making Neural Networks FAIR. (arXiv:1907.11569v4 [cs.LG] UPDATED)</h2>
<h3>Anna Nguyen, Tobias Weller, Michael F&#xe4;rber, York Sure-Vetter</h3>
<p>Research on neural networks has gained significant momentum over the past few
years. Because training is a resource-intensive process and training data
cannot always be made available to everyone, there has been a trend to reuse
pre-trained neural networks. As such, neural networks themselves have become
research data. In this paper, we first present the neural network ontology
FAIRnets Ontology, an ontology to make existing neural network models findable,
accessible, interoperable, and reusable according to the FAIR principles. Our
ontology allows us to model neural networks on a meta-level in a structured
way, including the representation of all network layers and their
characteristics. Secondly, we have modeled over 18,400 neural networks from
GitHub based on this ontology, which we provide to the public as a knowledge
graph called FAIRnets, ready to be used for recommending suitable neural
networks to data scientists.
</p>
<a href="http://arxiv.org/abs/1907.11569" target="_blank">arXiv:1907.11569</a> [<a href="http://arxiv.org/pdf/1907.11569" target="_blank">pdf</a>]

<h2>Entropic Out-of-Distribution Detection. (arXiv:1908.05569v11 [cs.LG] UPDATED)</h2>
<h3>David Mac&#xea;do, Tsang Ing Ren, Cleber Zanchettin, Adriano L. I. Oliveira, Teresa Ludermir</h3>
<p>In this paper, we argue that the unsatisfactory out-of-distribution (OOD)
detection performance of neural networks is mainly due to the SoftMax loss
anisotropy and propensity to produce low entropy probability distributions in
disagreement with the principle of maximum entropy. Current out-of-distribution
(OOD) detection approaches usually do not directly fix the SoftMax loss
drawbacks but rather build techniques to circumvent it. Unfortunately, those
methods usually produce undesired side effects (e.g., additional
hyperparameters, slower inferences, and collecting extra data). In the opposite
direction, we propose replacing SoftMax loss with a novel loss function that
does not suffer from the mentioned weaknesses. The proposed IsoMax loss is
isotropic (exclusively distance based) and provides high entropy posterior
probability distributions. Replacing the SoftMax loss by IsoMax loss requires
no model or training changes. Additionally, the models trained with IsoMax loss
produce as fast and energy-efficient inferences as those trained using SoftMax
loss. Further, no classification accuracy drop is observed. The proposed method
does not rely on outlier/background data, hyperparameter tuning, temperature
calibration, feature extraction, metric learning, adversarial training,
ensemble procedures, or generative models. Our experiments showed that IsoMax
loss works as a seamless SoftMax loss drop-in replacement that significantly
improves neural networks OOD detection performance. Therefore, it may be used
as a baseline solution to be combined with current or future OOD detection
techniques to achieve even higher results.
</p>
<a href="http://arxiv.org/abs/1908.05569" target="_blank">arXiv:1908.05569</a> [<a href="http://arxiv.org/pdf/1908.05569" target="_blank">pdf</a>]

<h2>Learning a manifold from a teacher's demonstrations. (arXiv:1910.04615v3 [cs.LG] UPDATED)</h2>
<h3>Pei Wang, Arash Givchi, Patrick Shafto</h3>
<p>We consider the problem of learning a manifold from a teacher's
demonstration. Extending existing approaches of learning from randomly sampled
data points, we consider contexts where data may be chosen by a teacher. We
analyze learning from teachers who can provide structured data such as
individual examples (isolated data points) and demonstrations (sequences of
points). Our analysis shows that for the purpose of teaching the topology of a
manifold, demonstrations can yield remarkable decreases in the amount of data
points required in comparison to teaching with randomly sampled points. We also
discuss the implications of our analysis for learning in humans and machines.
</p>
<a href="http://arxiv.org/abs/1910.04615" target="_blank">arXiv:1910.04615</a> [<a href="http://arxiv.org/pdf/1910.04615" target="_blank">pdf</a>]

<h2>Fractal Impedance for Passive Controllers. (arXiv:1911.04788v2 [cs.RO] UPDATED)</h2>
<h3>Keyhan Kouhkiloui Babarahmati, Carlo Tiseo, Joshua Smith, Hsiu Chin Lin, Mustafa Suphi Erden, Michael Mistry</h3>
<p>There is increasing interest in control frameworks capable of moving robots
from industrial cages to unstructured environments and coexisting with humans.
Despite significant improvement in some specific applications (e.g.\ medical
robotics), there is still the need for a general control framework that
improves interaction robustness and motion dynamics. Passive controllers show
promising results in this direction; however, they often rely on virtual energy
tanks that can guarantee passivity as long as they do not run out of energy. In
this paper, a fractal attractor is proposed to implement a variable impedance
controller that can retain passivity without relying on the energy tanks. The
controller generates a fractal attractor around the desired state using an
asymptotic stable potential field, making the controller robust to
discretization and numerical integration errors. Thus, the Fractal Impedance
Controller is robust for low-bandwidth applications. We have tested this
controller with a torque controlled 7-DoF manipulator. The results prove that
it can accurately track both trajectories and end-effector forces during
interaction. Furthermore, it can automatically deal with the extra energy
introduced by changes in interaction conditions, null-space controller and
environment. Therefore, these properties make the controller ideal for
applications where the dynamic interaction at the end-effector is challenging
to be characterized a priori, such as human-robot interaction and unknown
dynamics.
</p>
<a href="http://arxiv.org/abs/1911.04788" target="_blank">arXiv:1911.04788</a> [<a href="http://arxiv.org/pdf/1911.04788" target="_blank">pdf</a>]

<h2>Abstract Argumentation and the Rational Man. (arXiv:1911.13024v5 [cs.AI] UPDATED)</h2>
<h3>Timotheus Kampik, Juan Carlos Nieves</h3>
<p>Abstract argumentation has emerged as a method for non-monotonic reasoning
that has gained tremendous traction in the symbolic artificial intelligence
community. In the literature, the different approaches to abstract
argumentation that were refined over the years are typically evaluated from a
logics perspective; an analysis that is based on models of ideal, rational
decision-making does not exist. In this paper, we work towards addressing this
issue by analyzing abstract argumentation from the perspective of the rational
man paradigm in microeconomic theory. To assess under which conditions abstract
argumentation-based decision-making can be considered economically rational, we
derive reference independence as a non-monotonic inference property from a
formal model of economic rationality and create a new argumentation principle
that ensures compliance with these properties. We then compare the reference
independence principle with other reasoning principles, in particular with
cautious monotony and rational monotony. We show that the argumentation
semantics as proposed in Dung's classical paper, as well as other semantics we
evaluate--with the exception of the SCC-recursive CF2 semantics--do not comply
with the reference independence principle. Consequently, we investigate how
structural properties of argumentation frameworks impact the reference
independence principle, and identify cyclic expansions (both even and odd
cycles) as the root of the problem. Finally, we put reference independence into
the context of preference-based argumentation and show that for this
argumentation variant, which explicitly models preferences, reference
independence cannot be ensured in a straight-forward manner.
</p>
<a href="http://arxiv.org/abs/1911.13024" target="_blank">arXiv:1911.13024</a> [<a href="http://arxiv.org/pdf/1911.13024" target="_blank">pdf</a>]

<h2>Outliers Detection in Networks with Missing Links. (arXiv:1911.13122v2 [stat.ML] UPDATED)</h2>
<h3>Solenne Gaucher (LMO), Olga Klopp (CREST), Genevi&#xe8;ve Robin (ENPC, MATHERIALS)</h3>
<p>Outliers arise in networks due to different reasons such as fraudulent
behavior of malicious users or default in measurement instruments and can
significantly impair network analyses. In addition, real-life networks are
likely to be incompletely observed, with missing links due to individual
non-response or machine failures. Identifying outliers in the presence of
missing links is therefore a crucial problem in network analysis. In this work,
we introduce a new algorithm to detect outliers in a network that
simultaneously predicts the missing links. The proposed method is statistically
sound: we prove that, under fairly general assumptions, our algorithm exactly
detects the outliers, and achieves the best known error for the prediction of
missing links with polynomial computation cost. It is also computationally
efficient: we prove sub-linear convergence of our algorithm. We provide a
simulation study which demonstrates the good behavior of the algorithm in terms
of outliers detection and prediction of the missing links. We also illustrate
the method with an application in epidemiology, and with the analysis of a
political Twitter network. The method is freely available as an R package on
the Comprehensive R Archive Network.
</p>
<a href="http://arxiv.org/abs/1911.13122" target="_blank">arXiv:1911.13122</a> [<a href="http://arxiv.org/pdf/1911.13122" target="_blank">pdf</a>]

<h2>On Interpretability of Artificial Neural Networks: A Survey. (arXiv:2001.02522v2 [cs.LG] UPDATED)</h2>
<h3>Fenglei Fan, Jinjun Xiong, Mengzhou Li, Ge Wang</h3>
<p>Deep learning as represented by the artificial deep neural networks (DNNs)
has achieved great success in many important areas that deal with text, images,
videos, graphs, and so on. However, the black-box nature of DNNs has become one
of the primary obstacles for their wide acceptance in mission-critical
applications such as medical diagnosis and therapy. Due to the huge potential
of deep learning, interpreting neural networks has recently attracted much
research attention. In this paper, based on our comprehensive taxonomy, we
systematically review recent studies in understanding the mechanism of neural
networks, describe applications of interpretability especially in medicine, and
discuss future directions of interpretability research, such as in relation to
fuzzy logic and brain science.
</p>
<a href="http://arxiv.org/abs/2001.02522" target="_blank">arXiv:2001.02522</a> [<a href="http://arxiv.org/pdf/2001.02522" target="_blank">pdf</a>]

<h2>Towards a Kernel based Uncertainty Decomposition Framework for Data and Models. (arXiv:2001.11495v4 [cs.LG] UPDATED)</h2>
<h3>Rishabh Singh, Jose C. Principe</h3>
<p>This paper introduces a new framework for quantifying predictive uncertainty
for both data and models that relies on projecting the data into a Gaussian
reproducing kernel Hilbert space (RKHS) and transforming the data probability
density function (PDF) in a way that quantifies the flow of its gradient as a
topological potential field quantified at all points in the sample space. This
enables the decomposition of the PDF gradient flow by formulating it as a
moment decomposition problem using operators from quantum physics, specifically
the Schrodinger's formulation. We experimentally show that the higher order
modes systematically cluster the different tail regions of the PDF, thereby
providing unprecedented discriminative resolution of data regions having high
epistemic uncertainty. In essence, this approach decomposes local realizations
of the data PDF in terms of uncertainty moments. We apply this framework as a
surrogate tool for predictive uncertainty quantification of point-prediction
neural network models, overcoming various limitations of conventional Bayesian
based uncertainty quantification methods. Experimental comparisons with some
established methods illustrate performance advantages exhibited by our
framework.
</p>
<a href="http://arxiv.org/abs/2001.11495" target="_blank">arXiv:2001.11495</a> [<a href="http://arxiv.org/pdf/2001.11495" target="_blank">pdf</a>]

<h2>Weakly-supervised Object Localization for Few-shot Learning. (arXiv:2003.00874v2 [cs.CV] UPDATED)</h2>
<h3>Xiaojian He, Jinfu Lin, Junming Shen</h3>
<p>Few-shot learning (FSL) aims to learn novel visual categories from very few
samples, which is a challenging problem in real-world applications. Many
methods of few-shot classification work well on general images to learn global
representation. However, they can not deal with fine-grained categories well at
the same time due to a lack of subtle and local information. We argue that
localization is an efficient approach because it directly provides the
discriminative regions, which is critical for both general classification and
fine-grained classification in a low data regime. In this paper, we propose a
Self-Attention Based Complementary Module (SAC Module) to fulfill the
weakly-supervised object localization, and more importantly produce the
activated masks for selecting discriminative deep descriptors for few-shot
classification. Based on each selected deep descriptor, Semantic Alignment
Module (SAM) calculates the semantic alignment distance between the query and
support images to boost classification performance. Extensive experiments show
our method outperforms the state-of-the-art methods on benchmark datasets under
various settings, especially on the fine-grained few-shot tasks. Besides, our
method achieves superior performance over previous methods when training the
model on miniImageNet and evaluating it on the different datasets,
demonstrating its superior generalization capacity. Extra visualization shows
the proposed method can localize the key objects more interval.
</p>
<a href="http://arxiv.org/abs/2003.00874" target="_blank">arXiv:2003.00874</a> [<a href="http://arxiv.org/pdf/2003.00874" target="_blank">pdf</a>]

<h2>Provably Efficient Exploration for Reinforcement Learning Using Unsupervised Learning. (arXiv:2003.06898v4 [cs.LG] UPDATED)</h2>
<h3>Fei Feng, Ruosong Wang, Wotao Yin, Simon S. Du, Lin F. Yang</h3>
<p>Motivated by the prevailing paradigm of using unsupervised learning for
efficient exploration in reinforcement learning (RL) problems
[tang2017exploration,bellemare2016unifying], we investigate when this paradigm
is provably efficient. We study episodic Markov decision processes with rich
observations generated from a small number of latent states. We present a
general algorithmic framework that is built upon two components: an
unsupervised learning algorithm and a no-regret tabular RL algorithm.
Theoretically, we prove that as long as the unsupervised learning algorithm
enjoys a polynomial sample complexity guarantee, we can find a near-optimal
policy with sample complexity polynomial in the number of latent states, which
is significantly smaller than the number of observations. Empirically, we
instantiate our framework on a class of hard exploration problems to
demonstrate the practicality of our theory.
</p>
<a href="http://arxiv.org/abs/2003.06898" target="_blank">arXiv:2003.06898</a> [<a href="http://arxiv.org/pdf/2003.06898" target="_blank">pdf</a>]

<h2>SS-IL: Separated Softmax for Incremental Learning. (arXiv:2003.13947v2 [cs.CV] UPDATED)</h2>
<h3>Hongjoon Ahn, Jihwan Kwak, Subin Lim, Hyeonsu Bang, Hyojun Kim, Taesup Moon</h3>
<p>We consider class incremental learning (CIL) problem, in which a learning
agent continuously learns new classes from incrementally arriving training data
batches and aims to predict well on all the classes learned so far. The main
challenge of the problem is the catastrophic forgetting, and for the
exemplar-memory based CIL methods, it is generally known that the forgetting is
commonly caused by the prediction score bias that is injected due to the data
imbalance between the new classes and the old classes (in the exemplar-memory).
While several methods have been proposed to correct such score bias by some
additional post-processing, e.g., score re-scaling or balanced fine-tuning, no
systematic analysis on the root cause of such bias has been done. To that end,
we analyze that computing the softmax probabilities by combining the output
scores for all old and new classes could be the main source of the bias and
propose a new CIL method, Separated Softmax for Incremental Learning (SS-IL).
Our SS-IL consists of separated softmax (SS) output layer and ratio-preserving
(RP) mini-batches combined with task-wise knowledge distillation (TKD), and
through extensive experimental results, we show our SS-IL achieves very strong
state-of-the-art accuracy on several large-scale benchmarks. We also show SS-IL
makes much more balanced prediction, without any additional post-processing
steps as is done in other baselines.
</p>
<a href="http://arxiv.org/abs/2003.13947" target="_blank">arXiv:2003.13947</a> [<a href="http://arxiv.org/pdf/2003.13947" target="_blank">pdf</a>]

<h2>Task Dynamics of Prior Training Influence Visual Force Estimation Ability During Teleoperation. (arXiv:2004.13226v3 [cs.RO] UPDATED)</h2>
<h3>Zonghe Chua, Anthony M. Jarc, Sherry Wren, Ilana Nisky, Allison M. Okamura</h3>
<p>The lack of haptic feedback in Robot-assisted Minimally Invasive Surgery
(RMIS) is a potential barrier to safe tissue handling during surgery. Bayesian
modeling theory suggests that surgeons with experience in open or laparoscopic
surgery can develop priors of tissue stiffness that translate to better force
estimation abilities during RMIS compared to surgeons with no experience. To
test if prior haptic experience leads to improved force estimation ability in
teleoperation, 33 participants were assigned to one of three training
conditions: manual manipulation, teleoperation with force feedback, or
teleoperation without force feedback, and learned to tension a silicone sample
to a set of force values. They were then asked to perform the tension task, and
a previously unencountered palpation task, to a different set of force values
under teleoperation without force feedback. Compared to the teleoperation
groups, the manual group had higher force error in the tension task outside the
range of forces they had trained on, but showed better speed-accuracy functions
in the palpation task at low force levels. This suggests that the dynamics of
the training modality affect force estimation ability during teleoperation,
with the prior haptic experience accessible if formed under the same dynamics
as the task.
</p>
<a href="http://arxiv.org/abs/2004.13226" target="_blank">arXiv:2004.13226</a> [<a href="http://arxiv.org/pdf/2004.13226" target="_blank">pdf</a>]

<h2>Lossy Event Compression based on Image-derived Quad Trees and Poisson Disk Sampling. (arXiv:2005.00974v2 [cs.CV] UPDATED)</h2>
<h3>Srutarshi Banerjee, Zihao W. Wang, Henry H. Chopp, Oliver Cossairt, Aggelos Katsaggelos</h3>
<p>With several advantages over conventional RGB cameras, event cameras have
provided new opportunities for tackling visual tasks under challenging
scenarios with fast motion, high dynamic range, and/or power constraint. Yet
unlike image/video compression, the performance of event compression algorithm
is far from satisfying and practical. The main challenge for compressing events
is the unique event data form, i.e., a stream of asynchronously fired event
tuples each encoding the 2D spatial location, timestamp, and polarity (denoting
an increase or decrease in brightness). Since events only encode temporal
variations, they lack spatial structure which is crucial for compression. To
address this problem, we propose a novel event compression algorithm based on a
quad tree (QT) segmentation map derived from the adjacent intensity images. The
QT informs 2D spatial priority within the 3D space-time volume. In the event
encoding step, events are first aggregated over time to form polarity-based
event histograms. The histograms are then variably sampled via Poisson Disk
Sampling prioritized by the QT based segmentation map. Next, differential
encoding and run length encoding are employed for encoding the spatial and
polarity information of the sampled events, respectively, followed by Huffman
encoding to produce the final encoded events. Our Poisson Disk Sampling based
Lossy Event Compression (PDS-LEC) algorithm performs rate-distortion based
optimal allocation. On average, our algorithm achieves greater than 6x
compression compared to the state of the art.
</p>
<a href="http://arxiv.org/abs/2005.00974" target="_blank">arXiv:2005.00974</a> [<a href="http://arxiv.org/pdf/2005.00974" target="_blank">pdf</a>]

<h2>Clustering Residential Electricity Consumption Data to Create Archetypes that Capture Household Behaviour in South Africa. (arXiv:2006.07197v4 [cs.LG] UPDATED)</h2>
<h3>Wiebke Toussaint, Deshendran Moodley</h3>
<p>Clustering is frequently used in the energy domain to identify dominant
electricity consumption patterns of households, which can be used to construct
customer archetypes for long term energy planning. Selecting a useful set of
clusters however requires extensive experimentation and domain knowledge. While
internal clustering validation measures are well established in the electricity
domain, they are limited for selecting useful clusters. Based on an application
case study in South Africa, we present an approach for formalising implicit
expert knowledge as external evaluation measures to create customer archetypes
that capture variability in residential electricity consumption behaviour. By
combining internal and external validation measures in a structured manner, we
were able to evaluate clustering structures based on the utility they present
for our application. We validate the selected clusters in a use case where we
successfully reconstruct customer archetypes previously developed by experts.
Our approach shows promise for transparent and repeatable cluster ranking and
selection by data scientists, even if they have limited domain knowledge.
</p>
<a href="http://arxiv.org/abs/2006.07197" target="_blank">arXiv:2006.07197</a> [<a href="http://arxiv.org/pdf/2006.07197" target="_blank">pdf</a>]

<h2>Mitigating Face Recognition Bias via Group Adaptive Classifier. (arXiv:2006.07576v2 [cs.CV] UPDATED)</h2>
<h3>Sixue Gong, Xiaoming Liu, Anil K. Jain</h3>
<p>Face recognition is known to exhibit bias - subjects in a certain demographic
group can be better recognized than other groups. This work aims to learn a
fair face representation, where faces of every group could be more equally
represented. Our proposed group adaptive classifier mitigates bias by using
adaptive convolution kernels and attention mechanisms on faces based on their
demographic attributes. The adaptive module comprises kernel masks and
channel-wise attention maps for each demographic group so as to activate
different facial regions for identification, leading to more discriminative
features pertinent to their demographics. Our introduced automated adaptation
strategy determines whether to apply adaptation to a certain layer by
iteratively computing the dissimilarity among demographic-adaptive parameters.
A new de-biasing loss function is proposed to mitigate the gap of average
intra-class distance between demographic groups. Experiments on face benchmarks
(RFW, LFW, IJB-A, and IJB-C) show that our work is able to mitigate face
recognition bias across demographic groups while maintaining the competitive
accuracy.
</p>
<a href="http://arxiv.org/abs/2006.07576" target="_blank">arXiv:2006.07576</a> [<a href="http://arxiv.org/pdf/2006.07576" target="_blank">pdf</a>]

<h2>Piecewise-Stationary Off-Policy Optimization. (arXiv:2006.08236v2 [cs.LG] UPDATED)</h2>
<h3>Joey Hong, Branislav Kveton, Manzil Zaheer, Yinlam Chow, Amr Ahmed</h3>
<p>Off-policy learning is a framework for evaluating and optimizing policies
without deploying them, from data collected by another policy. Real-world
environments are typically non-stationary and the offline learned policies
should adapt to these changes. To address this challenge, we study the novel
problem of off-policy optimization in piecewise-stationary contextual bandits.
Our proposed solution has two phases. In the offline learning phase, we
partition logged data into categorical latent states and learn a near-optimal
sub-policy for each state. In the online deployment phase, we adaptively switch
between the learned sub-policies based on their performance. This approach is
practical and analyzable, and we provide guarantees on both the quality of
off-policy optimization and the regret during online deployment. To show the
effectiveness of our approach, we compare it to state-of-the-art baselines on
both synthetic and real-world datasets. Our approach outperforms methods that
act only on observed context.
</p>
<a href="http://arxiv.org/abs/2006.08236" target="_blank">arXiv:2006.08236</a> [<a href="http://arxiv.org/pdf/2006.08236" target="_blank">pdf</a>]

<h2>Efficient Model-Based Reinforcement Learning through Optimistic Policy Search and Planning. (arXiv:2006.08684v3 [cs.LG] UPDATED)</h2>
<h3>Sebastian Curi, Felix Berkenkamp, Andreas Krause</h3>
<p>Model-based reinforcement learning algorithms with probabilistic dynamical
models are amongst the most data-efficient learning methods. This is often
attributed to their ability to distinguish between epistemic and aleatoric
uncertainty. However, while most algorithms distinguish these two uncertainties
for learning the model, they ignore it when optimizing the policy, which leads
to greedy and insufficient exploration. At the same time, there are no
practical solvers for optimistic exploration algorithms. In this paper, we
propose a practical optimistic exploration algorithm (H-UCRL). H-UCRL
reparameterizes the set of plausible models and hallucinates control directly
on the epistemic uncertainty. By augmenting the input space with the
hallucinated inputs, H-UCRL can be solved using standard greedy planners.
Furthermore, we analyze H-UCRL and construct a general regret bound for
well-calibrated models, which is provably sublinear in the case of Gaussian
Process models. Based on this theoretical foundation, we show how optimistic
exploration can be easily combined with state-of-the-art reinforcement learning
algorithms and different probabilistic models. Our experiments demonstrate that
optimistic exploration significantly speeds-up learning when there are
penalties on actions, a setting that is notoriously difficult for existing
model-based reinforcement learning algorithms.
</p>
<a href="http://arxiv.org/abs/2006.08684" target="_blank">arXiv:2006.08684</a> [<a href="http://arxiv.org/pdf/2006.08684" target="_blank">pdf</a>]

<h2>Fast Matrix Square Roots with Applications to Gaussian Processes and Bayesian Optimization. (arXiv:2006.11267v2 [cs.LG] UPDATED)</h2>
<h3>Geoff Pleiss, Martin Jankowiak, David Eriksson, Anil Damle, Jacob R. Gardner</h3>
<p>Matrix square roots and their inverses arise frequently in machine learning,
e.g., when sampling from high-dimensional Gaussians $\mathcal{N}(\mathbf 0,
\mathbf K)$ or whitening a vector $\mathbf b$ against covariance matrix
$\mathbf K$. While existing methods typically require $O(N^3)$ computation, we
introduce a highly-efficient quadratic-time algorithm for computing $\mathbf
K^{1/2} \mathbf b$, $\mathbf K^{-1/2} \mathbf b$, and their derivatives through
matrix-vector multiplication (MVMs). Our method combines Krylov subspace
methods with a rational approximation and typically achieves $4$ decimal places
of accuracy with fewer than $100$ MVMs. Moreover, the backward pass requires
little additional computation. We demonstrate our method's applicability on
matrices as large as $50,\!000 \times 50,\!000$ - well beyond traditional
methods - with little approximation error. Applying this increased scalability
to variational Gaussian processes, Bayesian optimization, and Gibbs sampling
results in more powerful models with higher accuracy.
</p>
<a href="http://arxiv.org/abs/2006.11267" target="_blank">arXiv:2006.11267</a> [<a href="http://arxiv.org/pdf/2006.11267" target="_blank">pdf</a>]

<h2>Approximate Cross-Validation for Structured Models. (arXiv:2006.12669v2 [stat.ML] UPDATED)</h2>
<h3>Soumya Ghosh, William T. Stephenson, Tin D. Nguyen, Sameer K. Deshpande, Tamara Broderick</h3>
<p>Many modern data analyses benefit from explicitly modeling dependence
structure in data -- such as measurements across time or space, ordered words
in a sentence, or genes in a genome. A gold standard evaluation technique is
structured cross-validation (CV), which leaves out some data subset (such as
data within a time interval or data in a geographic region) in each fold. But
CV here can be prohibitively slow due to the need to re-run already-expensive
learning algorithms many times. Previous work has shown approximate
cross-validation (ACV) methods provide a fast and provably accurate alternative
in the setting of empirical risk minimization. But this existing ACV work is
restricted to simpler models by the assumptions that (i) data across CV folds
are independent and (ii) an exact initial model fit is available. In structured
data analyses, both these assumptions are often untrue. In the present work, we
address (i) by extending ACV to CV schemes with dependence structure between
the folds. To address (ii), we verify -- both theoretically and empirically --
that ACV quality deteriorates smoothly with noise in the initial fit. We
demonstrate the accuracy and computational benefits of our proposed methods on
a diverse set of real-world applications.
</p>
<a href="http://arxiv.org/abs/2006.12669" target="_blank">arXiv:2006.12669</a> [<a href="http://arxiv.org/pdf/2006.12669" target="_blank">pdf</a>]

<h2>Multi-Class Uncertainty Calibration via Mutual Information Maximization-based Binning. (arXiv:2006.13092v3 [cs.LG] UPDATED)</h2>
<h3>Kanil Patel, William Beluch, Bin Yang, Michael Pfeiffer, Dan Zhang</h3>
<p>Post-hoc multi-class calibration is a common approach for providing
high-quality confidence estimates of deep neural network predictions. Recent
work has shown that widely used scaling methods underestimate their calibration
error, while alternative Histogram Binning (HB) methods often fail to preserve
classification accuracy. When classes have small prior probabilities, HB also
faces the issue of severe sample-inefficiency after the conversion into K
one-vs-rest class-wise calibration problems. The goal of this paper is to
resolve the identified issues of HB in order to provide calibrated confidence
estimates using only a small holdout calibration dataset for bin optimization
while preserving multi-class ranking accuracy. From an information-theoretic
perspective, we derive the I-Max concept for binning, which maximizes the
mutual information between labels and quantized logits. This concept mitigates
potential loss in ranking performance due to lossy quantization, and by
disentangling the optimization of bin edges and representatives allows
simultaneous improvement of ranking and calibration performance. To improve the
sample efficiency and estimates from a small calibration set, we propose a
shared class-wise (sCW) calibration strategy, sharing one calibrator among
similar classes (e.g., with similar class priors) so that the training sets of
their class-wise calibration problems can be merged to train the single
calibrator. The combination of sCW and I-Max binning outperforms the state of
the art calibration methods on various evaluation metrics across different
benchmark datasets and models, using a small calibration set (e.g., 1k samples
for ImageNet).
</p>
<a href="http://arxiv.org/abs/2006.13092" target="_blank">arXiv:2006.13092</a> [<a href="http://arxiv.org/pdf/2006.13092" target="_blank">pdf</a>]

<h2>The NetHack Learning Environment. (arXiv:2006.13760v2 [cs.LG] UPDATED)</h2>
<h3>Heinrich K&#xfc;ttler, Nantas Nardelli, Alexander H. Miller, Roberta Raileanu, Marco Selvatici, Edward Grefenstette, Tim Rockt&#xe4;schel</h3>
<p>Progress in Reinforcement Learning (RL) algorithms goes hand-in-hand with the
development of challenging environments that test the limits of current
methods. While existing RL environments are either sufficiently complex or
based on fast simulation, they are rarely both. Here, we present the NetHack
Learning Environment (NLE), a scalable, procedurally generated, stochastic,
rich, and challenging environment for RL research based on the popular
single-player terminal-based roguelike game, NetHack. We argue that NetHack is
sufficiently complex to drive long-term research on problems such as
exploration, planning, skill acquisition, and language-conditioned RL, while
dramatically reducing the computational resources required to gather a large
amount of experience. We compare NLE and its task suite to existing
alternatives, and discuss why it is an ideal medium for testing the robustness
and systematic generalization of RL agents. We demonstrate empirical success
for early stages of the game using a distributed Deep RL baseline and Random
Network Distillation exploration, alongside qualitative analysis of various
agents trained in the environment. NLE is open source at
https://github.com/facebookresearch/nle.
</p>
<a href="http://arxiv.org/abs/2006.13760" target="_blank">arXiv:2006.13760</a> [<a href="http://arxiv.org/pdf/2006.13760" target="_blank">pdf</a>]

<h2>Long-term Pedestrian Trajectory Prediction using Mutable Intention Filter and Warp LSTM. (arXiv:2007.00113v2 [cs.RO] UPDATED)</h2>
<h3>Zhe Huang, Aamir Hasan, Kazuki Shin, Ruohua Li, Katherine Driggs-Campbell</h3>
<p>Trajectory prediction is one of the key capabilities for robots to safely
navigate and interact with pedestrians. Critical insights from human intention
and behavioral patterns need to be integrated to effectively forecast long-term
pedestrian behavior. Thus, we propose a framework incorporating a Mutable
Intention Filter and a Warp LSTM (MIF-WLSTM) to simultaneously estimate human
intention and perform trajectory prediction. The Mutable Intention Filter is
inspired by particle filtering and genetic algorithms, where particles
represent intention hypotheses that can be mutated throughout the pedestrian
motion. Instead of predicting sequential displacement over time, our Warp LSTM
learns to generate offsets on a full trajectory predicted by a nominal
intention-aware linear model, which considers the intention hypotheses during
filtering process. Through experiments on a publicly available dataset, we show
that our method outperforms baseline approaches and demonstrate the robust
performance of our method under abnormal intention-changing scenarios.
</p>
<a href="http://arxiv.org/abs/2007.00113" target="_blank">arXiv:2007.00113</a> [<a href="http://arxiv.org/pdf/2007.00113" target="_blank">pdf</a>]

<h2>Learning to Discover Multi-Class Attentional Regions for Multi-Label Image Recognition. (arXiv:2007.01755v2 [cs.CV] UPDATED)</h2>
<h3>Bin-Bin Gao, Hong-Yu Zhou</h3>
<p>Multi-label image recognition is a practical and challenging task compared to
single-label image classification. However, previous works may be suboptimal
because of a great number of object proposals or complex attentional region
generation modules. In this paper, we propose a simple but efficient two-stream
framework to recognize multi-category objects from global image to local
regions, similar to how human beings perceive objects. To bridge the gap
between global and local streams, we propose a multi-class attentional region
module which aims to make the number of attentional regions as small as
possible and keep the diversity of these regions as high as possible. Our
method can efficiently and effectively recognize multi-class objects with an
affordable computation cost and a parameter-free region localization module.
Over three benchmarks on multi-label image classification, we create new
state-of-the-art results with a single model only using image semantics without
label dependency. In addition, the effectiveness of the proposed method is
extensively demonstrated under different factors such as global pooling
strategy, input size and network architecture.
</p>
<a href="http://arxiv.org/abs/2007.01755" target="_blank">arXiv:2007.01755</a> [<a href="http://arxiv.org/pdf/2007.01755" target="_blank">pdf</a>]

<h2>Near-Optimal Provable Uniform Convergence in Offline Policy Evaluation for Reinforcement Learning. (arXiv:2007.03760v2 [cs.LG] UPDATED)</h2>
<h3>Ming Yin, Yu Bai, Yu-Xiang Wang</h3>
<p>The problem of Offline Policy Evaluation (OPE) in Reinforcement Learning (RL)
is a critical step towards applying RL in real-life applications. Existing work
on OPE mostly focus on evaluating a fixed target policy $\pi$, which does not
provide useful bounds for offline policy learning as $\pi$ will then be
data-dependent. We address this problem by simultaneously evaluating all
policies in a policy class $\Pi$ -- uniform convergence in OPE -- and obtain
nearly optimal error bounds for a number of global / local policy classes. Our
results imply that the model-based planning achieves an optimal episode
complexity of $\widetilde{O}(H^3/d_m\epsilon^2)$ in identifying an
$\epsilon$-optimal policy under the time-inhomogeneous episodic MDP model ($H$
is the planning horizon, $d_m$ is a quantity that reflects the exploration of
the logging policy $\mu$). To the best of our knowledge, this is the first time
the optimal rate is shown to be possible for the offline RL setting and the
paper is the first that systematically investigates the uniform convergence in
OPE.
</p>
<a href="http://arxiv.org/abs/2007.03760" target="_blank">arXiv:2007.03760</a> [<a href="http://arxiv.org/pdf/2007.03760" target="_blank">pdf</a>]

<h2>Whitening for Self-Supervised Representation Learning. (arXiv:2007.06346v3 [cs.LG] UPDATED)</h2>
<h3>Aleksandr Ermolov, Aliaksandr Siarohin, Enver Sangineto, Nicu Sebe</h3>
<p>Most of the self-supervised representation learning methods are based on the
contrastive loss and the instance-discrimination task, where augmented versions
of the same image instance ("positives") are contrasted with instances
extracted from other images ("negatives"). For the learning to be effective, a
lot of negatives should be compared with a positive pair, which is
computationally demanding. In this paper, we propose a different direction and
a new loss function for self-supervised representation learning which is based
on the whitening of the latent-space features. The whitening operation has a
"scattering" effect on the batch samples, which compensates the use of
negatives, avoiding degenerate solutions where all the sample representations
collapse to a single point. Our Whitening MSE (W-MSE) loss does not require
special heuristics (e.g. additional networks) and it is conceptually simple.
Since negatives are not needed, we can extract multiple positive pairs from the
same image instance. We empirically show that W-MSE is competitive with respect
to popular, more complex self-supervised methods. The source code of the method
and all the experiments is available at
https://github.com/htdt/self-supervised.
</p>
<a href="http://arxiv.org/abs/2007.06346" target="_blank">arXiv:2007.06346</a> [<a href="http://arxiv.org/pdf/2007.06346" target="_blank">pdf</a>]

<h2>An Empirical Characterization of Fair Machine Learning For Clinical Risk Prediction. (arXiv:2007.10306v2 [stat.ML] UPDATED)</h2>
<h3>Stephen R. Pfohl, Agata Foryciarz, Nigam H. Shah</h3>
<p>The use of machine learning to guide clinical decision making has the
potential to worsen existing health disparities. Several recent works frame the
problem as that of algorithmic fairness, a framework that has attracted
considerable attention and criticism. However, the appropriateness of this
framework is unclear due to both ethical as well as technical considerations,
the latter of which include trade-offs between measures of fairness and model
performance that are not well-understood for predictive models of clinical
outcomes. To inform the ongoing debate, we conduct an empirical study to
characterize the impact of penalizing group fairness violations on an array of
measures of model performance and group fairness. We repeat the analyses across
multiple observational healthcare databases, clinical outcomes, and sensitive
attributes. We find that procedures that penalize differences between the
distributions of predictions across groups induce nearly-universal degradation
of multiple performance metrics within groups. On examining the secondary
impact of these procedures, we observe heterogeneity of the effect of these
procedures on measures of fairness in calibration and ranking across
experimental conditions. Beyond the reported trade-offs, we emphasize that
analyses of algorithmic fairness in healthcare lack the contextual grounding
and causal awareness necessary to reason about the mechanisms that lead to
health disparities, as well as about the potential of algorithmic fairness
methods to counteract those mechanisms. In light of these limitations, we
encourage researchers building predictive models for clinical use to step
outside the algorithmic fairness frame and engage critically with the broader
sociotechnical context surrounding the use of machine learning in healthcare.
</p>
<a href="http://arxiv.org/abs/2007.10306" target="_blank">arXiv:2007.10306</a> [<a href="http://arxiv.org/pdf/2007.10306" target="_blank">pdf</a>]

<h2>Learning Differentiable Programs with Admissible Neural Heuristics. (arXiv:2007.12101v4 [cs.LG] UPDATED)</h2>
<h3>Ameesh Shah, Eric Zhan, Jennifer J. Sun, Abhinav Verma, Yisong Yue, Swarat Chaudhuri</h3>
<p>We study the problem of learning differentiable functions expressed as
programs in a domain-specific language. Such programmatic models can offer
benefits such as composability and interpretability; however, learning them
requires optimizing over a combinatorial space of program "architectures". We
frame this optimization problem as a search in a weighted graph whose paths
encode top-down derivations of program syntax. Our key innovation is to view
various classes of neural networks as continuous relaxations over the space of
programs, which can then be used to complete any partial program. This relaxed
program is differentiable and can be trained end-to-end, and the resulting
training loss is an approximately admissible heuristic that can guide the
combinatorial search. We instantiate our approach on top of the A-star
algorithm and an iteratively deepened branch-and-bound search, and use these
algorithms to learn programmatic classifiers in three sequence classification
tasks. Our experiments show that the algorithms outperform state-of-the-art
methods for program learning, and that they discover programmatic classifiers
that yield natural interpretations and achieve competitive accuracy.
</p>
<a href="http://arxiv.org/abs/2007.12101" target="_blank">arXiv:2007.12101</a> [<a href="http://arxiv.org/pdf/2007.12101" target="_blank">pdf</a>]

<h2>Deep Co-Training with Task Decomposition for Semi-Supervised Domain Adaptation. (arXiv:2007.12684v2 [cs.CV] UPDATED)</h2>
<h3>Luyu Yang, Yan Wang, Mingfei Gao, Abhinav Shrivastava, Kilian Q. Weinberger, Wei-Lun Chao, Ser-Nam Lim</h3>
<p>Semi-supervised domain adaptation (SSDA) aims to adapt models from a labeled
source domain to a different but related target domain, from which unlabeled
data and a small set of labeled data are provided. In this paper we propose a
new approach for SSDA, which is to explicitly decompose the SSDA task into two
sub-tasks: a semi-supervised learning (SSL) task in the target domain and an
unsupervised domain adaptation (UDA) task across domains. We show that these
two sub-tasks yield very different classifiers and thus naturally fits into the
well established co-training framework, in which the two classifiers exchange
their high confident predictions to iteratively "teach each other" so that both
classifiers can excel in the target domain. We call our approach Deep
Co-Training with Task Decomposition (DeCoTa). DeCoTa requires no adversarial
training, making it fairly easy to implement. DeCoTa achieves state-of-the-art
results on several SSDA datasets, outperforming the prior art by a notable 4%
margin on DomainNet.
</p>
<a href="http://arxiv.org/abs/2007.12684" target="_blank">arXiv:2007.12684</a> [<a href="http://arxiv.org/pdf/2007.12684" target="_blank">pdf</a>]

<h2>COVID-19 in differential diagnosis of online symptom assessments. (arXiv:2008.03323v3 [cs.AI] UPDATED)</h2>
<h3>Anitha Kannan, Richard Chen, Vignesh Venkataraman, Geoffrey J. Tso, Xavier Amatriain</h3>
<p>The COVID-19 pandemic has magnified an already existing trend of people
looking for healthcare solutions online. One class of solutions are symptom
checkers, which have become very popular in the context of COVID-19.
Traditional symptom checkers, however, are based on manually curated expert
systems that are inflexible and hard to modify, especially in a quickly
changing situation like the one we are facing today. That is why all COVID-19
existing solutions are manual symptom checkers that can only estimate the
probability of this disease and cannot contemplate alternative hypothesis or
come up with a differential diagnosis. While machine learning offers an
alternative, the lack of reliable data does not make it easy to apply to
COVID-19 either. In this paper we present an approach that combines the
strengths of traditional AI expert systems and novel deep learning models. In
doing so we can leverage prior knowledge as well as any amount of existing data
to quickly derive models that best adapt to the current state of the world and
latest scientific knowledge. We use the approach to train a COVID-19 aware
differential diagnosis model that can be used for medical decision support both
for doctors or patients. We show that our approach is able to accurately model
new incoming data about COVID-19 while still preserving accuracy on conditions
that had been modeled in the past. While our approach shows evident and clear
advantages for an extreme situation like the one we are currently facing, we
also show that its flexibility generalizes beyond this concrete, but very
important, example.
</p>
<a href="http://arxiv.org/abs/2008.03323" target="_blank">arXiv:2008.03323</a> [<a href="http://arxiv.org/pdf/2008.03323" target="_blank">pdf</a>]

<h2>LPMNet: Latent Part Modification and Generation for 3D Point Clouds. (arXiv:2008.03560v2 [cs.CV] UPDATED)</h2>
<h3>Cihan &#xd6;ng&#xfc;n, Alptekin Temizel</h3>
<p>In this paper, we focus on latent modification and generation of 3D point
cloud object models with respect to their semantic parts. Different to the
existing methods which use separate networks for part generation and assembly,
we propose a single end-to-end Autoencoder model that can handle generation and
modification of both semantic parts, and global shapes. The proposed method
supports part exchange between 3D point cloud models and composition by
different parts to form new models by directly editing latent representations.
This holistic approach does not need part-based training to learn part
representations and does not introduce any extra loss besides the standard
reconstruction loss. The experiments demonstrate the robustness of the proposed
method with different object categories and varying number of points. The
method can generate new models by integration of generative models such as GANs
and VAEs and can work with unannotated point clouds by integration of a
segmentation module.
</p>
<a href="http://arxiv.org/abs/2008.03560" target="_blank">arXiv:2008.03560</a> [<a href="http://arxiv.org/pdf/2008.03560" target="_blank">pdf</a>]

<h2>Soft Multicopter Control using Neural Dynamics Identification. (arXiv:2008.07689v4 [cs.RO] UPDATED)</h2>
<h3>Yitong Deng, Yaorui Zhang, Xingzhe He, Shuqi Yang, Yunjin Tong, Michael Zhang, Daniel DiPietro, Bo Zhu</h3>
<p>Dynamic control of a soft-body robot to deliver complex behaviors with
low-dimensional actuation inputs is challenging. In this paper, we present a
computational approach to automatically generate versatile, underactuated
control policies that drives soft-bodied machines with complicated structures
and nonlinear dynamics. Our target application is focused on the autonomous
control of a soft multicopter, featured by its elastic material components,
non-conventional shapes, and asymmetric rotor layouts, to precisely deliver
compliant deformation and agile locomotion. The central piece of our approach
lies in a lightweight neural surrogate model to identify and predict the
temporal evolution of a set of geometric variables characterizing an elastic
soft body. This physics-based learning model is further integrated into a
Linear Quadratic Regulator (LQR) control loop enhanced by a novel online
fixed-point relinearization scheme to accommodate the dynamic body balance,
allowing an aggressive reduction of the computational overhead caused by the
conventional full-scale sensing-simulation-control workflow. We demonstrate the
efficacy of our approach by generating controllers for a broad spectrum of
customized soft multicopter designs and testing them in a high-fidelity physics
simulation environment. The control algorithm enables the multicopters to
perform a variety of tasks, including hovering, trajectory tracking, cruising
and active deforming.
</p>
<a href="http://arxiv.org/abs/2008.07689" target="_blank">arXiv:2008.07689</a> [<a href="http://arxiv.org/pdf/2008.07689" target="_blank">pdf</a>]

<h2>Generative Adversarial Networks for Spatio-temporal Data: A Survey. (arXiv:2008.08903v2 [cs.LG] UPDATED)</h2>
<h3>Nan Gao, Hao Xue, Wei Shao, Sichen Zhao, Kyle Kai Qin, Arian Prabowo, Mohammad Saiedur Rahaman, Flora D. Salim</h3>
<p>Generative Adversarial Networks (GANs) have shown remarkable success in the
computer vision area for producing realistic-looking images. Recently,
GAN-based techniques are shown to be promising for spatiotemporal-based
applications such as trajectory prediction, events generation and time-series
data imputation. While several reviews for GANs in computer vision been
presented, nobody has considered addressing the practical applications and
challenges relevant to spatio-temporal data. In this paper, we conduct a
comprehensive review of the recent developments of GANs in spatio-temporal
data. we summarise the popular GAN architectures in spatio-temporal data and
common practices for evaluating the performance of spatio-temporal applications
with GANs. In the end, we point out the future directions with the hope of
benefiting researchers interested in this area.
</p>
<a href="http://arxiv.org/abs/2008.08903" target="_blank">arXiv:2008.08903</a> [<a href="http://arxiv.org/pdf/2008.08903" target="_blank">pdf</a>]

<h2>Dynamical Variational Autoencoders: A Comprehensive Review. (arXiv:2008.12595v2 [cs.LG] UPDATED)</h2>
<h3>Laurent Girin, Simon Leglaive, Xiaoyu Bie, Julien Diard, Thomas Hueber, Xavier Alameda-Pineda</h3>
<p>The Variational Autoencoder (VAE) is a powerful deep generative model that is
now extensively used to represent high-dimensional complex data via a
low-dimensional latent space learned in an unsupervised manner. In the original
VAE model, input data vectors are processed independently. In recent years, a
series of papers have presented different extensions of the VAE to process
sequential data, that not only model the latent space, but also model the
temporal dependencies within a sequence of data vectors and corresponding
latent vectors, relying on recurrent neural networks or state space models. In
this paper we perform an extensive literature review of these models.
Importantly, we introduce and discuss a general class of models called
Dynamical Variational Autoencoders (DVAEs) that encompasses a large subset of
these temporal VAE extensions. Then we present in detail seven different
instances of DVAE that were recently proposed in the literature, with an effort
to homogenize the notations and presentation lines, as well as to relate these
models with existing classical temporal models. We reimplemented those seven
DVAE models and we present the results of an experimental benchmark conducted
on the speech analysis-resynthesis task (the PyTorch code is made publicly
available). The paper is concluded with an extensive discussion on important
issues concerning the DVAE class of models and future research guidelines.
</p>
<a href="http://arxiv.org/abs/2008.12595" target="_blank">arXiv:2008.12595</a> [<a href="http://arxiv.org/pdf/2008.12595" target="_blank">pdf</a>]

<h2>Machine Reasoning Explainability. (arXiv:2009.00418v2 [cs.AI] UPDATED)</h2>
<h3>Kristijonas Cyras, Ramamurthy Badrinath, Swarup Kumar Mohalik, Anusha Mujumdar, Alexandros Nikou, Alessandro Previti, Vaishnavi Sundararajan, Aneta Vulgarakis Feljan</h3>
<p>As a field of AI, Machine Reasoning (MR) uses largely symbolic means to
formalize and emulate abstract reasoning. Studies in early MR have notably
started inquiries into Explainable AI (XAI) -- arguably one of the biggest
concerns today for the AI community. Work on explainable MR as well as on MR
approaches to explainability in other areas of AI has continued ever since. It
is especially potent in modern MR branches, such as argumentation, constraint
and logic programming, planning. We hereby aim to provide a selective overview
of MR explainability techniques and studies in hopes that insights from this
long track of research will complement well the current XAI landscape. This
document reports our work in-progress on MR explainability.
</p>
<a href="http://arxiv.org/abs/2009.00418" target="_blank">arXiv:2009.00418</a> [<a href="http://arxiv.org/pdf/2009.00418" target="_blank">pdf</a>]

<h2>I Like to Move It: 6D Pose Estimation as an Action Decision Process. (arXiv:2009.12678v2 [cs.CV] UPDATED)</h2>
<h3>Benjamin Busam, Hyun Jun Jung, Nassir Navab</h3>
<p>Object pose estimation is an integral part of robot vision and AR. Previous
6D pose retrieval pipelines treat the problem either as a regression task or
discretize the pose space to classify. We change this paradigm and reformulate
the problem as an action decision process where an initial pose is updated in
incremental discrete steps that sequentially move a virtual 3D rendering
towards the correct solution. A neural network estimates likely moves from a
single RGB image iteratively and determines so an acceptable final pose. In
comparison to other approaches that train object-specific pose models, we learn
a decision process. This allows for a lightweight architecture while it
naturally generalizes to unseen objects. A coherent stop action for process
termination enables dynamic reduction of the computation cost if there are
insignificant changes in a video sequence. Instead of a static inference time,
we thereby automatically increase the runtime depending on the object motion.
Robustness and accuracy of our action decision network are evaluated on Laval
and YCB video scenes where we significantly improve the state-of-the-art.
</p>
<a href="http://arxiv.org/abs/2009.12678" target="_blank">arXiv:2009.12678</a> [<a href="http://arxiv.org/pdf/2009.12678" target="_blank">pdf</a>]

<h2>Demographic Influences on Contemporary Art with Unsupervised Style Embeddings. (arXiv:2009.14545v2 [cs.CV] UPDATED)</h2>
<h3>Nikolai Huckle, Noa Garcia, Yuta Nakashima</h3>
<p>Computational art analysis has, through its reliance on classification tasks,
prioritised historical datasets in which the artworks are already well sorted
with the necessary annotations. Art produced today, on the other hand, is
numerous and easily accessible, through the internet and social networks that
are used by professional and amateur artists alike to display their work.
Although this art, yet unsorted in terms of style and genre, is less suited for
supervised analysis, the data sources come with novel information that may help
frame the visual content in equally novel ways. As a first step in this
direction, we present contempArt, a multi-modal dataset of exclusively
contemporary artworks. contempArt is a collection of paintings and drawings, a
detailed graph network based on social connections on Instagram and additional
socio-demographic information; all attached to 442 artists at the beginning of
their career. We evaluate three methods suited for generating unsupervised
style embeddings of images and correlate them with the remaining data. We find
no connections between visual style on the one hand and social proximity,
gender, and nationality on the other.
</p>
<a href="http://arxiv.org/abs/2009.14545" target="_blank">arXiv:2009.14545</a> [<a href="http://arxiv.org/pdf/2009.14545" target="_blank">pdf</a>]

<h2>Deep Imitation Learning for Bimanual Robotic Manipulation. (arXiv:2010.05134v2 [cs.RO] UPDATED)</h2>
<h3>Fan Xie, Alexander Chowdhury, M. Clara De Paolis Kaluza, Linfeng Zhao, Lawson L.S. Wong, Rose Yu</h3>
<p>We present a deep imitation learning framework for robotic bimanual
manipulation in a continuous state-action space. A core challenge is to
generalize the manipulation skills to objects in different locations. We
hypothesize that modeling the relational information in the environment can
significantly improve generalization. To achieve this, we propose to (i)
decompose the multi-modal dynamics into elemental movement primitives, (ii)
parameterize each primitive using a recurrent graph neural network to capture
interactions, and (iii) integrate a high-level planner that composes primitives
sequentially and a low-level controller to combine primitive dynamics and
inverse kinematics control. Our model is a deep, hierarchical, modular
architecture. Compared to baselines, our model generalizes better and achieves
higher success rates on several simulated bimanual robotic manipulation tasks.
We open source the code for simulation, data, and models at:
https://github.com/Rose-STL-Lab/HDR-IL.
</p>
<a href="http://arxiv.org/abs/2010.05134" target="_blank">arXiv:2010.05134</a> [<a href="http://arxiv.org/pdf/2010.05134" target="_blank">pdf</a>]

<h2>Invariant Representation Learning for Infant Pose Estimation with Small Data. (arXiv:2010.06100v2 [cs.CV] UPDATED)</h2>
<h3>Xiaofei Huang, Nihang Fu, Shuangjun Liu, Kathan Vyas, Amirreza Farnoosh, Sarah Ostadabbas</h3>
<p>With the increasing maturity of the human pose estimation domain, its
applications have become more and more broaden. Yet, the state-of-the-art pose
estimation models performance degrades significantly in the applications that
include novel subjects or poses, such as infants with their unique movements.
Infant motion analysis is a topic with critical importance in early
developmental studies. However, models trained on large-scale adult pose
datasets are barely successful in estimating infant poses due to the
significant differences in their body ratio and the versatility of poses they
can take compared to adults. Moreover, the privacy and security considerations
hinder the availability of adequate infant images required for training of a
robust pose inference model from scratch. Here, we present an invariant
representation learning strategy that allows us to augment the limited
available real infant pose data by incorporating the knowledge from the
adjacent domains of adult poses as well as synthetic infant models. We
introduce a multi-stage training strategy to gradually transfer these knowledge
into our fine-tuned domain-adapted infant pose (FiDIP) estimation model. In
developing FiDIP, we also built and publicly released a synthetic and real
infant pose (SyRIP) dataset with small yet diverse real infant images as well
as generated synthetic infant data. We demonstrated that our FiDIP model
outperforms state-of-the-art human pose estimation model for the infant pose
estimation, with the mean average precision (AP) as high as 90.1.
</p>
<a href="http://arxiv.org/abs/2010.06100" target="_blank">arXiv:2010.06100</a> [<a href="http://arxiv.org/pdf/2010.06100" target="_blank">pdf</a>]

<h2>Training Invertible Linear Layers through Rank-One Perturbations. (arXiv:2010.07033v2 [stat.ML] UPDATED)</h2>
<h3>Andreas Kr&#xe4;mer, Jonas K&#xf6;hler, Frank No&#xe9;</h3>
<p>Many types of neural network layers rely on matrix properties such as
invertibility or orthogonality. Retaining such properties during optimization
with gradient-based stochastic optimizers is a challenging task, which is
usually addressed by either reparameterization of the affected parameters or by
directly optimizing on the manifold. This work presents a novel approach for
training invertible linear layers. In lieu of directly optimizing the network
parameters, we train rank-one perturbations and add them to the actual weight
matrices infrequently. This P$^{4}$Inv update allows keeping track of inverses
and determinants without ever explicitly computing them. We show how such
invertible blocks improve the mixing and thus the mode separation of the
resulting normalizing flows. Furthermore, we outline how the P$^4$ concept can
be utilized to retain properties other than invertibility.
</p>
<a href="http://arxiv.org/abs/2010.07033" target="_blank">arXiv:2010.07033</a> [<a href="http://arxiv.org/pdf/2010.07033" target="_blank">pdf</a>]

<h2>Deep Learning Models for Predicting Wildfires from Historical Remote-Sensing Data. (arXiv:2010.07445v2 [cs.CV] UPDATED)</h2>
<h3>Fantine Huot, R. Lily Hu, Matthias Ihme, Qing Wang, John Burge, Tianjian Lu, Jason Hickey, Yi-Fan Chen, John Anderson</h3>
<p>Identifying regions that have high likelihood for wildfires is a key
component of land and forestry management and disaster preparedness. We create
a data set by aggregating nearly a decade of remote-sensing data and historical
fire records to predict wildfires. This prediction problem is framed as three
machine learning tasks. Results are compared and analyzed for four different
deep learning models to estimate wildfire likelihood. The results demonstrate
that deep learning models can successfully identify areas of high fire
likelihood using aggregated data about vegetation, weather, and topography with
an AUC of 83%.
</p>
<a href="http://arxiv.org/abs/2010.07445" target="_blank">arXiv:2010.07445</a> [<a href="http://arxiv.org/pdf/2010.07445" target="_blank">pdf</a>]

<h2>Smooth activations and reproducibility in deep networks. (arXiv:2010.09931v2 [cs.LG] UPDATED)</h2>
<h3>Gil I. Shamir, Dong Lin, Lorenzo Coviello</h3>
<p>Deep networks are gradually penetrating almost every domain in our lives due
to their amazing success. However, with substantive performance accuracy
improvements comes the price of \emph{irreproducibility}. Two identical models,
trained on the exact same training dataset may exhibit large differences in
predictions on individual examples even when average accuracy is similar,
especially when trained on highly distributed parallel systems. The popular
Rectified Linear Unit (ReLU) activation has been key to recent success of deep
networks. We demonstrate, however, that ReLU is also a catalyzer to
irreproducibility in deep networks. We show that not only can activations
smoother than ReLU provide better accuracy, but they can also provide better
accuracy-reproducibility tradeoffs. We propose a new family of activations;
Smooth ReLU (\emph{SmeLU}), designed to give such better tradeoffs, while also
keeping the mathematical expression simple, and thus implementation cheap.
SmeLU is monotonic, mimics ReLU, while providing continuous gradients, yielding
better reproducibility. We generalize SmeLU to give even more flexibility and
then demonstrate that SmeLU and its generalized form are special cases of a
more general methodology of REctified Smooth Continuous Unit (RESCU)
activations. Empirical results demonstrate the superior
accuracy-reproducibility tradeoffs with smooth activations, SmeLU in
particular.
</p>
<a href="http://arxiv.org/abs/2010.09931" target="_blank">arXiv:2010.09931</a> [<a href="http://arxiv.org/pdf/2010.09931" target="_blank">pdf</a>]

<h2>SOrT-ing VQA Models : Contrastive Gradient Learning for Improved Consistency. (arXiv:2010.10038v2 [cs.CV] UPDATED)</h2>
<h3>Sameer Dharur, Purva Tendulkar, Dhruv Batra, Devi Parikh, Ramprasaath R. Selvaraju</h3>
<p>Recent research in Visual Question Answering (VQA) has revealed
state-of-the-art models to be inconsistent in their understanding of the world
-- they answer seemingly difficult questions requiring reasoning correctly but
get simpler associated sub-questions wrong. These sub-questions pertain to
lower level visual concepts in the image that models ideally should understand
to be able to answer the higher level question correctly. To address this, we
first present a gradient-based interpretability approach to determine the
questions most strongly correlated with the reasoning question on an image, and
use this to evaluate VQA models on their ability to identify the relevant
sub-questions needed to answer a reasoning question. Next, we propose a
contrastive gradient learning based approach called Sub-question Oriented
Tuning (SOrT) which encourages models to rank relevant sub-questions higher
than irrelevant questions for an &lt;image, reasoning-question&gt; pair. We show that
SOrT improves model consistency by upto 6.5% points over existing baselines,
while also improving visual grounding.
</p>
<a href="http://arxiv.org/abs/2010.10038" target="_blank">arXiv:2010.10038</a> [<a href="http://arxiv.org/pdf/2010.10038" target="_blank">pdf</a>]

<h2>Robust Constrained Reinforcement Learning for Continuous Control with Model Misspecification. (arXiv:2010.10644v3 [cs.LG] UPDATED)</h2>
<h3>Daniel J. Mankowitz, Dan A. Calian, Rae Jeong, Cosmin Paduraru, Nicolas Heess, Sumanth Dathathri, Martin Riedmiller, Timothy Mann</h3>
<p>Many real-world physical control systems are required to satisfy constraints
upon deployment. Furthermore, real-world systems are often subject to effects
such as non-stationarity, wear-and-tear, uncalibrated sensors and so on. Such
effects effectively perturb the system dynamics and can cause a policy trained
successfully in one domain to perform poorly when deployed to a perturbed
version of the same domain. This can affect a policy's ability to maximize
future rewards as well as the extent to which it satisfies constraints. We
refer to this as constrained model misspecification. We present an algorithm
with theoretical guarantees that mitigates this form of misspecification, and
showcase its performance in multiple Mujoco tasks from the Real World
Reinforcement Learning (RWRL) suite.
</p>
<a href="http://arxiv.org/abs/2010.10644" target="_blank">arXiv:2010.10644</a> [<a href="http://arxiv.org/pdf/2010.10644" target="_blank">pdf</a>]

<h2>Nonvacuous Loss Bounds with Fast Rates for Neural Networks via Conditional Information Measures. (arXiv:2010.11552v2 [cs.LG] UPDATED)</h2>
<h3>Fredrik Hellstr&#xf6;m, Giuseppe Durisi</h3>
<p>We present a framework to derive bounds on the test loss of randomized
learning algorithms for the case of bounded loss functions. This framework
leads to bounds that depend on the conditional information density between the
the output hypothesis and the choice of the training set, given a larger set of
data samples from which the training set is formed. Furthermore, the bounds
pertain to the average test loss as well as to its tail probability, both for
the PAC-Bayesian and the single-draw settings. If the conditional information
density is bounded uniformly in the size $n$ of the training set, our bounds
decay as $1/n$, which is referred to as a fast rate. This is in contrast with
the tail bounds involving conditional information measures available in the
literature, which have a less benign $1/\sqrt{n}$ dependence. We demonstrate
the usefulness of our tail bounds by showing that they lead to estimates of the
test loss achievable with several neural network architectures trained on MNIST
and Fashion-MNIST that match the state-of-the-art bounds available in the
literature.
</p>
<a href="http://arxiv.org/abs/2010.11552" target="_blank">arXiv:2010.11552</a> [<a href="http://arxiv.org/pdf/2010.11552" target="_blank">pdf</a>]

<h2>Learning Invariances in Neural Networks. (arXiv:2010.11882v2 [cs.LG] UPDATED)</h2>
<h3>Gregory Benton, Marc Finzi, Pavel Izmailov, Andrew Gordon Wilson</h3>
<p>Invariances to translations have imbued convolutional neural networks with
powerful generalization properties. However, we often do not know a priori what
invariances are present in the data, or to what extent a model should be
invariant to a given symmetry group. We show how to \emph{learn} invariances
and equivariances by parameterizing a distribution over augmentations and
optimizing the training loss simultaneously with respect to the network
parameters and augmentation parameters. With this simple procedure we can
recover the correct set and extent of invariances on image classification,
regression, segmentation, and molecular property prediction from a large space
of augmentations, on training data alone.
</p>
<a href="http://arxiv.org/abs/2010.11882" target="_blank">arXiv:2010.11882</a> [<a href="http://arxiv.org/pdf/2010.11882" target="_blank">pdf</a>]

<h2>Deep Manifold Transformation for Dimension Reduction. (arXiv:2010.14831v2 [cs.LG] UPDATED)</h2>
<h3>Stan Z. Li, Zelin Zang, Lirong Wu</h3>
<p>Nonlinear dimensionality reduction (NLDR) plays an important role in feature
extraction and visualization of high dimensional data. It transforms patterns
of interest, or manifolds, in data to regions in a lower-dimensional latent
space. When the manifolds are highly complicated, the NLDR transformation has
to provide sufficient nonlinearity for downstream tasks. This paper proposes a
novel method, called {\em deep manifold transformation} (DMT), to tackle this
problem. As a multi-layer neural network, DMT can support a higher degree of
nonlinearity than single-layer methods. Continuation strategy is applied during
training to address the local minimum problem common in manifold learning
algorithms. Such a learned DMT network can generalize to unseen data whereas
traditional manifold learning methods only provide an embedding of the training
data. Extensive experiments, comparisons, and ablation studies demonstrate that
DMT can deliver results superior to UMAP and t-SNE, and other leading
manifold-based NLDR methods.
</p>
<a href="http://arxiv.org/abs/2010.14831" target="_blank">arXiv:2010.14831</a> [<a href="http://arxiv.org/pdf/2010.14831" target="_blank">pdf</a>]

<h2>Multi-Constitutive Neural Network for Large Deformation Poromechanics Problem. (arXiv:2010.15549v2 [cs.LG] UPDATED)</h2>
<h3>Qi Zhang, Yilin Chen, Ziyi Yang, Eric Darve</h3>
<p>In this paper, we study the problem of large-strain consolidation in
poromechanics with deep neural networks (DNN). Given different material
properties and different loading conditions, the goal is to predict pore
pressure and settlement. We propose a novel method "multi-constitutive neural
network" (MCNN) such that one model can solve several different constitutive
laws. We introduce a one-hot encoding vector as an additional input vector,
which is used to label the constitutive law we wish to solve. Then we build a
DNN which takes $(\hat{X}, \hat{t})$ as input along with a constitutive law
label and outputs the corresponding solution. It is the first time, to our
knowledge, that we can evaluate multi-constitutive laws through only one
training process while still obtaining good accuracies. We found that MCNN
trained to solve multiple PDEs outperforms individual neural network solvers
trained with PDE in some cases.
</p>
<a href="http://arxiv.org/abs/2010.15549" target="_blank">arXiv:2010.15549</a> [<a href="http://arxiv.org/pdf/2010.15549" target="_blank">pdf</a>]

<h2>Generating Unobserved Alternatives. (arXiv:2011.01926v4 [cs.LG] UPDATED)</h2>
<h3>Shichong Peng, Ke Li</h3>
<p>We consider problems where multiple predictions can be considered correct,
but only one of them is given as supervision. This setting differs from both
the regression and class-conditional generative modelling settings: in the
former, there is a unique observed output for each input, which is provided as
supervision; in the latter, there are many observed outputs for each input, and
many are provided as supervision. Applying either regression methods and
conditional generative models to the present setting often results in a model
that can only make a single prediction for each input. We explore several
problems that have this property and develop an approach that can generate
multiple high-quality predictions given the same input. As a result, it can be
used to generate high-quality outputs that are different from the observed
output.
</p>
<a href="http://arxiv.org/abs/2011.01926" target="_blank">arXiv:2011.01926</a> [<a href="http://arxiv.org/pdf/2011.01926" target="_blank">pdf</a>]

<h2>Toward Force Estimation in Robot-Assisted Surgery using Deep Learning with Vision and Robot State. (arXiv:2011.02112v3 [cs.RO] UPDATED)</h2>
<h3>Zonghe Chua, Anthony M. Jarc, Allison M. Okamura</h3>
<p>Knowledge of interaction forces during teleoperated robot-assisted surgery
could be used to enable force feedback to human operators and evaluate tissue
handling skill. However, direct force sensing at the end-effector is
challenging because it requires biocompatible, sterilizable, and cost-effective
sensors. Vision-based deep learning using convolutional neural networks is a
promising approach for providing useful force estimates, though questions
remain about generalization to new scenarios and real-time inference. We
present a force estimation neural network that uses RGB images and robot state
as inputs. Using a self-collected dataset, we compared the network to variants
that included only a single input type, and evaluated how they generalized to
new viewpoints, workspace positions, materials, and tools. We found that
vision-based networks were sensitive to shifts in viewpoints, while state-only
networks were robust to changes in workspace. The network with both state and
vision inputs had the highest accuracy for an unseen tool, and was moderately
robust to changes in viewpoints. Through feature removal studies, we found that
using only position features produced better accuracy than using only force
features as input. The network with both state and vision inputs outperformed a
physics-based baseline model in accuracy. It showed comparable accuracy but
faster computation times than a baseline recurrent neural network, making it
better suited for real-time applications.
</p>
<a href="http://arxiv.org/abs/2011.02112" target="_blank">arXiv:2011.02112</a> [<a href="http://arxiv.org/pdf/2011.02112" target="_blank">pdf</a>]

<h2>Hi-UCD: A Large-scale Dataset for Urban Semantic Change Detection in Remote Sensing Imagery. (arXiv:2011.03247v5 [cs.CV] UPDATED)</h2>
<h3>Shiqi Tian, Zhuo Zheng, Ailong Ma, Yanfei Zhong</h3>
<p>With the acceleration of the urban expansion, urban change detection (UCD),
as a significant and effective approach, can provide the change information
with respect to geospatial objects for dynamical urban analysis. However,
existing datasets suffer from three bottlenecks: (1) lack of high spatial
resolution images; (2) lack of semantic annotation; (3) lack of long-range
multi-temporal images. In this paper, we propose a large scale benchmark
dataset, termed Hi-UCD. This dataset uses aerial images with a spatial
resolution of 0.1 m provided by the Estonia Land Board, including three-time
phases, and semantically annotated with nine classes of land cover to obtain
the direction of ground objects change. It can be used for detecting and
analyzing refined urban changes. We benchmark our dataset using some classic
methods in binary and multi-class change detection. Experimental results show
that Hi-UCD is challenging yet useful. We hope the Hi-UCD can become a strong
benchmark accelerating future research.
</p>
<a href="http://arxiv.org/abs/2011.03247" target="_blank">arXiv:2011.03247</a> [<a href="http://arxiv.org/pdf/2011.03247" target="_blank">pdf</a>]

<h2>Deep learning architectures for inference of AC-OPF solutions. (arXiv:2011.03352v2 [cs.LG] UPDATED)</h2>
<h3>Thomas Falconer, Letif Mones</h3>
<p>We present a systematic comparison between neural network (NN) architectures
for inference of AC-OPF solutions. Using fully connected NNs as a baseline we
demonstrate the efficacy of leveraging network topology in the models by
constructing abstract representations of electrical grids in the graph domain,
for both convolutional and graph NNs. The performance of the NN architectures
is compared for regression (predicting optimal generator set-points) and
classification (predicting the active set of constraints) settings.
Computational gains for obtaining optimal solutions are also presented.
</p>
<a href="http://arxiv.org/abs/2011.03352" target="_blank">arXiv:2011.03352</a> [<a href="http://arxiv.org/pdf/2011.03352" target="_blank">pdf</a>]

<h2>An improved helmet detection method for YOLOv3 on an unbalanced dataset. (arXiv:2011.04214v2 [cs.CV] UPDATED)</h2>
<h3>Rui Geng, Yixuan Ma, Wanhong Huang</h3>
<p>The YOLOv3 target detection algorithm is widely used in industry due to its
high speed and high accuracy, but it has some limitations, such as the accuracy
degradation of unbalanced datasets. The YOLOv3 target detection algorithm is
based on a Gaussian fuzzy data augmentation approach to pre-process the data
set and improve the YOLOv3 target detection algorithm. Through the efficient
pre-processing, the confidence level of YOLOv3 is generally improved by
0.01-0.02 without changing the recognition speed of YOLOv3, and the processed
images also perform better in image localization due to effective feature
fusion, which is more in line with the requirement of recognition speed and
accuracy in production.
</p>
<a href="http://arxiv.org/abs/2011.04214" target="_blank">arXiv:2011.04214</a> [<a href="http://arxiv.org/pdf/2011.04214" target="_blank">pdf</a>]

<h2>Geometry Perspective Of Estimating Learning Capability Of Neural Networks. (arXiv:2011.04588v2 [cs.LG] UPDATED)</h2>
<h3>Ankan Dutta, Arnab Rakshit</h3>
<p>The paper uses statistical and differential geometric motivation to acquire
prior information about the learning capability of an artificial neural network
on a given dataset. The paper considers a broad class of neural networks with
generalized architecture performing simple least square regression with
stochastic gradient descent (SGD). The system characteristics at two critical
epochs in the learning trajectory are analyzed. During some epochs of the
training phase, the system reaches equilibrium with the generalization
capability attaining a maximum. The system can also be coherent with localized,
non-equilibrium states, which is characterized by the stabilization of the
Hessian matrix. The paper proves that neural networks with higher
generalization capability will have a slower convergence rate. The relationship
between the generalization capability with the stability of the neural network
has also been discussed. By correlating the principles of high-energy physics
with the learning theory of neural networks, the paper establishes a variant of
the Complexity-Action conjecture from an artificial neural network perspective.
</p>
<a href="http://arxiv.org/abs/2011.04588" target="_blank">arXiv:2011.04588</a> [<a href="http://arxiv.org/pdf/2011.04588" target="_blank">pdf</a>]

<h2>Input Bias in Rectified Gradients and Modified Saliency Maps. (arXiv:2011.05002v3 [cs.CV] UPDATED)</h2>
<h3>Lennart Brocki, Neo Christopher Chung</h3>
<p>Interpretation and improvement of deep neural networks relies on better
understanding of their underlying mechanisms. In particular, gradients of
classes or concepts with respect to the input features (e.g., pixels in images)
are often used as importance scores or estimators, which are visualized in
saliency maps. Thus, a family of saliency methods provide an intuitive way to
identify input features with substantial influences on classifications or
latent concepts. Several modifications to conventional saliency maps, such as
Rectified Gradients and Layer-wise Relevance Propagation (LRP), have been
introduced to allegedly denoise and improve interpretability. While visually
coherent in certain cases, Rectified Gradients and other modified saliency maps
introduce a strong input bias (e.g., brightness in the RGB space) because of
inappropriate uses of the input features. We demonstrate that dark areas of an
input image are not highlighted by a saliency map using Rectified Gradients,
even if it is relevant for the class or concept. Even in the scaled images, the
input bias exists around an artificial point in color spectrum. Our
modification, which simply eliminates multiplication with input features,
removes this bias. This showcases how a visual criteria may not align with true
explainability of deep learning models.
</p>
<a href="http://arxiv.org/abs/2011.05002" target="_blank">arXiv:2011.05002</a> [<a href="http://arxiv.org/pdf/2011.05002" target="_blank">pdf</a>]

<h2>Accounting for Human Learning when Inferring Human Preferences. (arXiv:2011.05596v2 [cs.LG] UPDATED)</h2>
<h3>Harry Giles, Lawrence Chan</h3>
<p>Inverse reinforcement learning (IRL) is a common technique for inferring
human preferences from data. Standard IRL techniques tend to assume that the
human demonstrator is stationary, that is that their policy $\pi$ doesn't
change over time. In practice, humans interacting with a novel environment or
performing well on a novel task will change their demonstrations as they learn
more about the environment or task. We investigate the consequences of relaxing
this assumption of stationarity, in particular by modelling the human as
learning. Surprisingly, we find in some small examples that this can lead to
better inference than if the human was stationary. That is, by observing a
demonstrator who is themselves learning, a machine can infer more than by
observing a demonstrator who is noisily rational. In addition, we find evidence
that misspecification can lead to poor inference, suggesting that modelling
human learning is important, especially when the human is facing an unfamiliar
environment.
</p>
<a href="http://arxiv.org/abs/2011.05596" target="_blank">arXiv:2011.05596</a> [<a href="http://arxiv.org/pdf/2011.05596" target="_blank">pdf</a>]

<h2>Investigating Learning in Deep Neural Networks using Layer-Wise Weight Change. (arXiv:2011.06735v2 [cs.LG] UPDATED)</h2>
<h3>Ayush Manish Agrawal, Atharva Tendle, Harshvardhan Sikka, Sahib Singh, Amr Kayid</h3>
<p>Understanding the per-layer learning dynamics of deep neural networks is of
significant interest as it may provide insights into how neural networks learn
and the potential for better training regimens. We investigate learning in Deep
Convolutional Neural Networks (CNNs) by measuring the relative weight change of
layers while training. Several interesting trends emerge in a variety of CNN
architectures across various computer vision classification tasks, including
the overall increase in relative weight change of later layers as compared to
earlier ones.
</p>
<a href="http://arxiv.org/abs/2011.06735" target="_blank">arXiv:2011.06735</a> [<a href="http://arxiv.org/pdf/2011.06735" target="_blank">pdf</a>]

<h2>Benchmarking Image Retrieval for Visual Localization. (arXiv:2011.11946v2 [cs.CV] UPDATED)</h2>
<h3>No&#xe9; Pion, Martin Humenberger, Gabriela Csurka, Yohann Cabon, Torsten Sattler</h3>
<p>Visual localization, i.e., camera pose estimation in a known scene, is a core
component of technologies such as autonomous driving and augmented reality.
State-of-the-art localization approaches often rely on image retrieval
techniques for one of two tasks: (1) provide an approximate pose estimate or
(2) determine which parts of the scene are potentially visible in a given query
image. It is common practice to use state-of-the-art image retrieval algorithms
for these tasks. These algorithms are often trained for the goal of retrieving
the same landmark under a large range of viewpoint changes. However, robustness
to viewpoint changes is not necessarily desirable in the context of visual
localization. This paper focuses on understanding the role of image retrieval
for multiple visual localization tasks. We introduce a benchmark setup and
compare state-of-the-art retrieval representations on multiple datasets. We
show that retrieval performance on classical landmark retrieval/recognition
tasks correlates only for some but not all tasks to localization performance.
This indicates a need for retrieval approaches specifically designed for
localization tasks. Our benchmark and evaluation protocols are available at
https://github.com/naver/kapture-localization.
</p>
<a href="http://arxiv.org/abs/2011.11946" target="_blank">arXiv:2011.11946</a> [<a href="http://arxiv.org/pdf/2011.11946" target="_blank">pdf</a>]

<h2>LiDAR-based Panoptic Segmentation via Dynamic Shifting Network. (arXiv:2011.11964v2 [cs.CV] UPDATED)</h2>
<h3>Fangzhou Hong, Hui Zhou, Xinge Zhu, Hongsheng Li, Ziwei Liu</h3>
<p>With the rapid advances of autonomous driving, it becomes critical to equip
its sensing system with more holistic 3D perception. However, existing works
focus on parsing either the objects (e.g. cars and pedestrians) or scenes (e.g.
trees and buildings) from the LiDAR sensor. In this work, we address the task
of LiDAR-based panoptic segmentation, which aims to parse both objects and
scenes in a unified manner. As one of the first endeavors towards this new
challenging task, we propose the Dynamic Shifting Network (DS-Net), which
serves as an effective panoptic segmentation framework in the point cloud
realm. In particular, DS-Net has three appealing properties: 1) strong backbone
design. DS-Net adopts the cylinder convolution that is specifically designed
for LiDAR point clouds. The extracted features are shared by the semantic
branch and the instance branch which operates in a bottom-up clustering style.
2) Dynamic Shifting for complex point distributions. We observe that
commonly-used clustering algorithms like BFS or DBSCAN are incapable of
handling complex autonomous driving scenes with non-uniform point cloud
distributions and varying instance sizes. Thus, we present an efficient
learnable clustering module, dynamic shifting, which adapts kernel functions
on-the-fly for different instances. 3) Consensus-driven Fusion. Finally,
consensus-driven fusion is used to deal with the disagreement between semantic
and instance predictions. To comprehensively evaluate the performance of
LiDAR-based panoptic segmentation, we construct and curate benchmarks from two
large-scale autonomous driving LiDAR datasets, SemanticKITTI and nuScenes.
Extensive experiments demonstrate that our proposed DS-Net achieves superior
accuracies over current state-of-the-art methods. Notably, we achieve 1st place
on the public leaderboard of SemanticKITTI, outperforming 2nd place by 2.6% in
terms of the PQ metric.
</p>
<a href="http://arxiv.org/abs/2011.11964" target="_blank">arXiv:2011.11964</a> [<a href="http://arxiv.org/pdf/2011.11964" target="_blank">pdf</a>]

<h2>Multiclass non-Adversarial Image Synthesis, with Application to Classification from Very Small Sample. (arXiv:2011.12942v2 [cs.CV] UPDATED)</h2>
<h3>Itamar Winter, Daphna Weinshall</h3>
<p>The generation of synthetic images is currently being dominated by Generative
Adversarial Networks (GANs). Despite their outstanding success in generating
realistic looking images, they still suffer from major drawbacks, including an
unstable and highly sensitive training procedure, mode-collapse and
mode-mixture, and dependency on large training sets. In this work we present a
novel non-adversarial generative method - Clustered Optimization of LAtent
space (COLA), which overcomes some of the limitations of GANs, and outperforms
GANs when training data is scarce. In the full data regime, our method is
capable of generating diverse multi-class images with no supervision,
surpassing previous non-adversarial methods in terms of image quality and
diversity. In the small-data regime, where only a small sample of labeled
images is available for training with no access to additional unlabeled data,
our results surpass state-of-the-art GAN models trained on the same amount of
data. Finally, when utilizing our model to augment small datasets, we surpass
the state-of-the-art performance in small-sample classification tasks on
challenging datasets, including CIFAR-10, CIFAR-100, STL-10 and Tiny-ImageNet.
A theoretical analysis supporting the essence of the method is presented.
</p>
<a href="http://arxiv.org/abs/2011.12942" target="_blank">arXiv:2011.12942</a> [<a href="http://arxiv.org/pdf/2011.12942" target="_blank">pdf</a>]

<h2>Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks. (arXiv:2011.13118v2 [cs.CV] UPDATED)</h2>
<h3>Xiaoxiao Long, Lingjie Liu, Wei Li, Christian Theobalt, Wenping Wang</h3>
<p>We present a novel method for multi-view depth estimation from a single
video, which is a critical task in various applications, such as perception,
reconstruction and robot navigation. Although previous learning-based methods
have demonstrated compelling results, most works estimate depth maps of
individual video frames independently, without taking into consideration the
strong geometric and temporal coherence among the frames. Moreover, current
state-of-the-art (SOTA) models mostly adopt a fully 3D convolution network for
cost regularization and therefore require high computational cost, thus
limiting their deployment in real-world applications. Our method achieves
temporally coherent depth estimation results by using a novel Epipolar
Spatio-Temporal (EST) transformer to explicitly associate geometric and
temporal correlation with multiple estimated depth maps. Furthermore, to reduce
the computational cost, inspired by recent Mixture-of-Experts models, we design
a compact hybrid network consisting of a 2D context-aware network and a 3D
matching network which learn 2D context information and 3D disparity cues
separately. Extensive experiments demonstrate that our method achieves higher
accuracy in depth estimation and significant speedup than the SOTA methods.
</p>
<a href="http://arxiv.org/abs/2011.13118" target="_blank">arXiv:2011.13118</a> [<a href="http://arxiv.org/pdf/2011.13118" target="_blank">pdf</a>]

<h2>Automatic coding of students' writing via Contrastive Representation Learning in the Wasserstein space. (arXiv:2011.13384v2 [cs.LG] UPDATED)</h2>
<h3>Ruijie Jiang, Julia Gouvea, David Hammer, Eric Miller, Shuchin Aeron</h3>
<p>Qualitative analysis of verbal data is of central importance in the learning
sciences. It is labor-intensive and time-consuming, however, which limits the
amount of data researchers can include in studies. This work is a step towards
building a statistical machine learning (ML) method for achieving an automated
support for qualitative analyses of students' writing, here specifically in
score laboratory reports in introductory biology for sophistication of
argumentation and reasoning. We start with a set of lab reports from an
undergraduate biology course, scored by a four-level scheme that considers the
complexity of argument structure, the scope of evidence, and the care and
nuance of conclusions. Using this set of labeled data, we show that a popular
natural language modeling processing pipeline, namely vector representation of
words, a.k.a word embeddings, followed by Long Short Term Memory (LSTM) model
for capturing language generation as a state-space model, is able to
quantitatively capture the scoring, with a high Quadratic Weighted Kappa (QWK)
prediction score, when trained in via a novel contrastive learning set-up. We
show that the ML algorithm approached the inter-rater reliability of human
analysis. Ultimately, we conclude, that machine learning (ML) for natural
language processing (NLP) holds promise for assisting learning sciences
researchers in conducting qualitative studies at much larger scales than is
currently possible.
</p>
<a href="http://arxiv.org/abs/2011.13384" target="_blank">arXiv:2011.13384</a> [<a href="http://arxiv.org/pdf/2011.13384" target="_blank">pdf</a>]

<h2>Frequency Domain Image Translation: More Photo-realistic, Better Identity-preserving. (arXiv:2011.13611v2 [cs.CV] UPDATED)</h2>
<h3>Mu Cai, Hong Zhang, Huijuan Huang, Qichuan Geng, Gao Huang</h3>
<p>Image-to-image translation aims at translating a particular style of an image
to another. The synthesized images can be more photo-realistic and
identity-preserving by decomposing the image into content and style in a
disentangled manner. While existing models focus on designing specialized
network architecture to separate the two components, this paper investigates
how to explicitly constrain the content and style statistics of images. We
achieve this goal by transforming the input image into high frequency and low
frequency information, which correspond to the content and style, respectively.
We regulate the frequency distribution from two aspects: a) a spatial level
restriction to locally restrict the frequency distribution of images; b) a
spectral level regulation to enhance the global consistency among images. On
multiple datasets we show that the proposed approach consistently leads to
significant improvements on top of various state-of-the-art image translation
models.
</p>
<a href="http://arxiv.org/abs/2011.13611" target="_blank">arXiv:2011.13611</a> [<a href="http://arxiv.org/pdf/2011.13611" target="_blank">pdf</a>]

<h2>Connecting Context-specific Adaptation in Humans to Meta-learning. (arXiv:2011.13782v2 [cs.AI] UPDATED)</h2>
<h3>Rachit Dubey, Erin Grant, Michael Luo, Karthik Narasimhan, Thomas Griffiths</h3>
<p>Cognitive control, the ability of a system to adapt to the demands of a task,
is an integral part of cognition. A widely accepted fact about cognitive
control is that it is context-sensitive: Adults and children alike infer
information about a task's demands from contextual cues and use these
inferences to learn from ambiguous cues. However, the precise way in which
people use contextual cues to guide adaptation to a new task remains poorly
understood. This work connects the context-sensitive nature of cognitive
control to a method for meta-learning with context-conditioned adaptation. We
begin by identifying an essential difference between human learning and current
approaches to meta-learning: In contrast to humans, existing meta-learning
algorithms do not make use of task-specific contextual cues but instead rely
exclusively on online feedback in the form of task-specific labels or rewards.
To remedy this, we introduce a framework for using contextual information about
a task to guide the initialization of task-specific models before adaptation to
online feedback. We show how context-conditioned meta-learning can capture
human behavior in a cognitive task and how it can be scaled to improve the
speed of learning in various settings, including few-shot classification and
low-sample reinforcement learning. Our work demonstrates that guiding
meta-learning with task information can capture complex, human-like behavior,
thereby deepening our understanding of cognitive control.
</p>
<a href="http://arxiv.org/abs/2011.13782" target="_blank">arXiv:2011.13782</a> [<a href="http://arxiv.org/pdf/2011.13782" target="_blank">pdf</a>]

<h2>Navigating the GAN Parameter Space for Semantic Image Editing. (arXiv:2011.13786v2 [cs.LG] UPDATED)</h2>
<h3>Anton Cherepkov, Andrey Voynov, Artem Babenko</h3>
<p>Generative Adversarial Networks (GANs) are currently an indispensable tool
for visual editing, being a standard component of image-to-image translation
and image restoration pipelines. Furthermore, GANs are especially useful for
controllable generation since their latent spaces contain a wide range of
interpretable directions, well suited for semantic editing operations. By
gradually changing latent codes along these directions, one can produce
impressive visual effects, unattainable without GANs.

In this paper, we significantly expand the range of visual effects achievable
with the state-of-the-art models, like StyleGAN2. In contrast to existing
works, which mostly operate by latent codes, we discover interpretable
directions in the space of the generator parameters. By several simple methods,
we explore this space and demonstrate that it also contains a plethora of
interpretable directions, which are an excellent source of non-trivial semantic
manipulations. The discovered manipulations cannot be achieved by transforming
the latent codes and can be used to edit both synthetic and real images. We
release our code and models and hope they will serve as a handy tool for
further efforts on GAN-based image editing.
</p>
<a href="http://arxiv.org/abs/2011.13786" target="_blank">arXiv:2011.13786</a> [<a href="http://arxiv.org/pdf/2011.13786" target="_blank">pdf</a>]

<h2>Inaccurate Supervision of Neural Networks with Incorrect Labels: Application to Epilepsy. (arXiv:2011.14101v2 [cs.CV] UPDATED)</h2>
<h3>Florian Dubost, Erin Hong, Daniel Y Fu, Nandita Bhaskhar, Siyi Tang, Khaled Saab, Jared Dunnmon, Daniel Rubin, Christopher Lee-Messer</h3>
<p>This work describes multiple weak supervision strategies for video processing
with neural networks in the context of seizure detection. To study seizure
onset, we have designed automated methods to detect seizures from
electroencephalography (EEG), a modality used for recording electrical brain
activity. However, the EEG signal alone is sometimes not enough for existing
detection methods to discriminate seizure from artifacts having a similar
signal on EEG. For example, such artifacts could be triggered by patting,
rocking or suctioning in the case of neonates. In this article, we addressed
this problem by automatically detecting an example artifact -- patting of
neonates -- from continuous video recordings of neonates acquired during
clinical routine. We computed frame-to-frame cross-correlation matrices to
isolate patterns showing repetitive movements indicative of patting of the
patient. Next, a convolutional neural network was trained to classify whether
these matrices contained patting events using weak training labels -- noisy
labels generated during daily clinical procedure. The labels were considered
weak as they were sometimes incorrect. We investigated whether networks trained
with more samples, containing more uncertain and weak labels, could achieve a
higher performance. Our results showed that, in the case of patting detection,
such networks could achieve a higher recall, without sacrificing precision.
These networks focused on areas of the cross-correlation matrices that were
more meaningful to the task. More generally, our work gives insights into
building more accurate models from weakly labelled time sequences.
</p>
<a href="http://arxiv.org/abs/2011.14101" target="_blank">arXiv:2011.14101</a> [<a href="http://arxiv.org/pdf/2011.14101" target="_blank">pdf</a>]

<h2>Fast and Uncertainty-Aware Directional Message Passing for Non-Equilibrium Molecules. (arXiv:2011.14115v2 [cs.LG] UPDATED)</h2>
<h3>Johannes Klicpera, Shankari Giri, Johannes T. Margraf, Stephan G&#xfc;nnemann</h3>
<p>Many important tasks in chemistry revolve around molecules during reactions.
This requires predictions far from the equilibrium, while most recent work in
machine learning for molecules has been focused on equilibrium or
near-equilibrium states. In this paper we aim to extend this scope in three
ways. First, we propose the DimeNet++ model, which is 8x faster and 10% more
accurate than the original DimeNet on the QM9 benchmark of equilibrium
molecules. Second, we validate DimeNet++ on highly reactive molecules by
developing the challenging COLL dataset, which contains distorted
configurations of small molecules during collisions. Finally, we investigate
ensembling and mean-variance estimation for uncertainty quantification with the
goal of accelerating the exploration of the vast space of non-equilibrium
structures. Our DimeNet++ implementation as well as the COLL dataset are
available online.
</p>
<a href="http://arxiv.org/abs/2011.14115" target="_blank">arXiv:2011.14115</a> [<a href="http://arxiv.org/pdf/2011.14115" target="_blank">pdf</a>]

<h2>Truly shift-invariant convolutional neural networks. (arXiv:2011.14214v2 [cs.CV] UPDATED)</h2>
<h3>Anadi Chaman (1), Ivan Dokmani&#x107; (2) ((1) University of Illinois at Urbana-Champaign, (2) University of Basel)</h3>
<p>Thanks to the use of convolution and pooling layers, convolutional neural
networks were for a long time thought to be shift-invariant. However, recent
works have shown that the output of a CNN can change significantly with small
shifts in input: a problem caused by the presence of downsampling (stride)
layers. The existing solutions rely either on data augmentation or on
anti-aliasing, both of which have limitations and neither of which enables
perfect shift invariance. Additionally, the gains obtained from these methods
do not extend to image patterns not seen during training. To address these
challenges, we propose adaptive polyphase sampling (APS), a simple sub-sampling
scheme that allows convolutional neural networks to achieve 100% consistency in
classification performance under shifts, without any loss in accuracy. With APS
the networks exhibit perfect consistency to shifts even before training, making
it the first approach that makes convolutional neural networks truly shift
invariant.
</p>
<a href="http://arxiv.org/abs/2011.14214" target="_blank">arXiv:2011.14214</a> [<a href="http://arxiv.org/pdf/2011.14214" target="_blank">pdf</a>]

<h2>Generalization and Memorization: The Bias Potential Model. (arXiv:2011.14269v2 [stat.ML] UPDATED)</h2>
<h3>Hongkang Yang, Weinan E</h3>
<p>Models for learning probability distributions such as generative models and
density estimators behave quite differently from models for learning functions.
One example is found in the memorization phenomenon, namely the ultimate
convergence to the empirical distribution, that occurs in generative
adversarial networks (GANs). For this reason, the issue of generalization is
more subtle than that for supervised learning. For the bias potential model, we
show that dimension-independent generalization accuracy is achievable if early
stopping is adopted, despite that in the long term, the model either memorizes
the samples or diverges.
</p>
<a href="http://arxiv.org/abs/2011.14269" target="_blank">arXiv:2011.14269</a> [<a href="http://arxiv.org/pdf/2011.14269" target="_blank">pdf</a>]

<h2>Multi-stage Attention ResU-Net for Semantic Segmentation of Fine-Resolution Remote Sensing Images. (arXiv:2011.14302v2 [cs.CV] UPDATED)</h2>
<h3>Rui Li, Shunyi Zheng, Chenxi Duan, Jianlin Su, Ce Zhang</h3>
<p>The attention mechanism can refine the extracted feature maps and boost the
classification performance of the deep network, which has become an essential
technique in computer vision and natural language processing. However, the
memory and computational costs of the dot-product attention mechanism increase
quadratically with the spatio-temporal size of the input. Such growth hinders
the usage of attention mechanisms considerably in application scenarios with
large-scale inputs. In this Letter, we propose a Linear Attention Mechanism
(LAM) to address this issue, which is approximately equivalent to dot-product
attention with computational efficiency. Such a design makes the incorporation
between attention mechanisms and deep networks much more flexible and
versatile. Based on the proposed LAM, we re-factor the skip connections in the
raw U-Net and design a Multi-stage Attention ResU-Net (MAResU-Net) for semantic
segmentation from fine-resolution remote sensing images. Experiments conducted
on the Vaihingen dataset demonstrated the effectiveness and efficiency of our
MAResU-Net. Open-source code is available at
https://github.com/lironui/Multistage-Attention-ResU-Net.
</p>
<a href="http://arxiv.org/abs/2011.14302" target="_blank">arXiv:2011.14302</a> [<a href="http://arxiv.org/pdf/2011.14302" target="_blank">pdf</a>]

<h2>An Artificial Consciousness Model and its relations with Philosophy of Mind. (arXiv:2011.14475v2 [cs.AI] UPDATED)</h2>
<h3>Eduardo C. Garrido-Merch&#xe1;n, Martin Molina, Francisco M. Mendoza</h3>
<p>This work seeks to study the beneficial properties that an autonomous agent
can obtain by implementing a cognitive architecture similar to the one of
conscious beings. Along this document, a conscious model of autonomous agent
based in a global workspace architecture is presented. We describe how this
agent is viewed from different perspectives of philosophy of mind, being
inspired by their ideas. The goal of this model is to create autonomous agents
able to navigate within an environment composed of multiple independent
magnitudes, adapting to its surroundings in order to find the best possible
position in base of its inner preferences. The purpose of the model is to test
the effectiveness of many cognitive mechanisms that are incorporated, such as
an attention mechanism for magnitude selection, pos-session of inner feelings
and preferences, usage of a memory system to storage beliefs and past
experiences, and incorporating a global workspace which controls and integrates
information processed by all the subsystem of the model. We show in a large
experiment set how an autonomous agent can benefit from having a cognitive
architecture such as the one described.
</p>
<a href="http://arxiv.org/abs/2011.14475" target="_blank">arXiv:2011.14475</a> [<a href="http://arxiv.org/pdf/2011.14475" target="_blank">pdf</a>]

<h2>DUT: Learning Video Stabilization by Simply Watching Unstable Videos. (arXiv:2011.14574v2 [cs.CV] UPDATED)</h2>
<h3>Yufei Xu, Jing Zhang, Stephen J. Maybank, Dacheng Tao</h3>
<p>We propose a Deep Unsupervised Trajectory-based stabilization framework (DUT)
in this paper. Traditional stabilizers focus on trajectory-based smoothing,
which is controllable but fragile in occluded and textureless cases regarding
the usage of hand-crafted features. On the other hand, previous deep video
stabilizers directly generate stable videos in a supervised manner without
explicit trajectory estimation, which is robust but less controllable and the
appropriate paired data are hard to obtain. To construct a controllable and
robust stabilizer, DUT makes the first attempt to stabilize unstable videos by
explicitly estimating and smoothing trajectories in an unsupervised deep
learning manner, which is composed of a DNN-based keypoint detector and motion
estimator to generate grid-based trajectories, and a DNN-based trajectory
smoother to stabilize videos. We exploit both the nature of continuity in
motion and the consistency of keypoints and grid vertices before and after
stabilization for unsupervised training. Experiment results on public
benchmarks show that DUT outperforms representative state-of-the-art methods
both qualitatively and quantitatively.
</p>
<a href="http://arxiv.org/abs/2011.14574" target="_blank">arXiv:2011.14574</a> [<a href="http://arxiv.org/pdf/2011.14574" target="_blank">pdf</a>]

<h2>Monocular 3D Object Detection with Sequential Feature Association and Depth Hint Augmentation. (arXiv:2011.14589v2 [cs.CV] UPDATED)</h2>
<h3>Tianze Gao, Huihui Pan, Huijun Gao</h3>
<p>Monocular 3D object detection is a promising research topic for the
intelligent perception systems of autonomous driving. In this work, a
single-stage keypoint-based network, named as FADNet, is presented to address
the task of monocular 3D object detection. In contrast to previous
keypoint-based methods which adopt identical layouts for output branches, we
propose to divide the output modalities into different groups according to the
estimating difficulty, whereby different groups are treated differently by
sequential feature association. Another contribution of this work is the
strategy of depth hint augmentation. To provide characterized depth patterns as
hints for depth estimation, a dedicated depth hint module is designed to
generate row-wise features named as depth hints, which are explicitly
supervised in a bin-wise manner. In the training stage, the regression outputs
are uniformly encoded to enable loss disentanglement. The 2D loss term is
further adapted to be depth-aware for improving the detection accuracy of small
objects. The contributions of this work are validated by conducting experiments
and ablation study on the KITTI benchmark. Without utilizing depth priors, post
optimization, or other refinement modules, our network performs competitively
against state-of-the-art methods while maintaining a decent running speed.
</p>
<a href="http://arxiv.org/abs/2011.14589" target="_blank">arXiv:2011.14589</a> [<a href="http://arxiv.org/pdf/2011.14589" target="_blank">pdf</a>]

<h2>Revisiting Unsupervised Meta-Learning: Amplifying or Compensating for the Characteristics of Few-Shot Tasks. (arXiv:2011.14663v2 [cs.CV] UPDATED)</h2>
<h3>Han-Jia Ye, Lu Han, De-Chuan Zhan</h3>
<p>Meta-learning becomes a practical approach towards few-shot image
classification, where a visual recognition system is constructed with limited
annotated data. Inductive bias such as embedding is learned from a base class
set with ample labeled examples and then generalizes to few-shot tasks with
novel classes. Surprisingly, we find that the base class set labels are not
necessary, and discriminative embeddings could be meta-learned in an
unsupervised manner. Comprehensive analyses indicate two modifications -- the
semi-normalized distance metric and the sufficient sampling -- improves
unsupervised meta-learning (UML) significantly. Based on the modified baseline,
we further amplify or compensate for the characteristic of tasks when training
a UML model. First, mixed embeddings are incorporated to increase the
difficulty of few-shot tasks. Next, we utilize a task-specific embedding
transformation to deal with the specific properties among tasks, maintaining
the generalization ability into the vanilla embeddings. Experiments on few-shot
learning benchmarks verify that our approaches outperform previous UML methods
by a 4-10% performance gap, and embeddings learned with our UML achieve
comparable or even better performance than its supervised variants.
</p>
<a href="http://arxiv.org/abs/2011.14663" target="_blank">arXiv:2011.14663</a> [<a href="http://arxiv.org/pdf/2011.14663" target="_blank">pdf</a>]

<h2>BOTD: Bold Outline Text Detector. (arXiv:2011.14714v2 [cs.CV] UPDATED)</h2>
<h3>Chuang Yang, Zhitong Xiong, Mulin Chen, Qi Wang, Xuelong Li</h3>
<p>Recently, text detection for arbitrary shape has attracted more and more
search attention. Although segmentation-based methods, which are not limited by
the text shape, have been studied to improve the performance, the slow
detection speed, complicated post-processing, and text adhesion problem are
still limitations for the practical application. In this paper, we propose a
simple yet effective arbitrary-shape text detector, named Bold Outline Text
Detector (BOTD). It is a novel one-stage detection framework with few
post-processing processes. At the same time, the text adhesion problem can also
be well alleviated. Specifically, BOTD first generates a center mask (CM) for
each text instance, which makes the adhesive text easy to distinguish. Base on
the CM, we further compute the polar minimum distance (PMD) for each text
instance. PMD denotes the shortest distance between the center point of CM and
the outline of the text instance. By dividing the text mask into CM and PMD,
the outline of arbitrary-shape text instance can be obtained by simply
predicting its CM and PMD. Without any bells and whistles, BOTD achieves an
F-measure of 80.1% on CTW1500 with 52 FPS. Note that the post-processing time
only accounts for 9% of the whole inference time. Code and trained models will
be publicly available soon.
</p>
<a href="http://arxiv.org/abs/2011.14714" target="_blank">arXiv:2011.14714</a> [<a href="http://arxiv.org/pdf/2011.14714" target="_blank">pdf</a>]

<h2>Outliers Detection in Networks with Missing Links. (arXiv:1911.13122v2 [stat.ML] CROSS LISTED)</h2>
<h3>Solenne Gaucher (LMO), Olga Klopp (CREST), Genevi&#xe8;ve Robin (ENPC, MATHERIALS)</h3>
<p>Outliers arise in networks due to different reasons such as fraudulent
behavior of malicious users or default in measurement instruments and can
significantly impair network analyses. In addition, real-life networks are
likely to be incompletely observed, with missing links due to individual
non-response or machine failures. Identifying outliers in the presence of
missing links is therefore a crucial problem in network analysis. In this work,
we introduce a new algorithm to detect outliers in a network that
simultaneously predicts the missing links. The proposed method is statistically
sound: we prove that, under fairly general assumptions, our algorithm exactly
detects the outliers, and achieves the best known error for the prediction of
missing links with polynomial computation cost. It is also computationally
efficient: we prove sub-linear convergence of our algorithm. We provide a
simulation study which demonstrates the good behavior of the algorithm in terms
of outliers detection and prediction of the missing links. We also illustrate
the method with an application in epidemiology, and with the analysis of a
political Twitter network. The method is freely available as an R package on
the Comprehensive R Archive Network.
</p>
<a href="http://arxiv.org/abs/1911.13122" target="_blank">arXiv:1911.13122</a> [<a href="http://arxiv.org/pdf/1911.13122" target="_blank">pdf</a>]

