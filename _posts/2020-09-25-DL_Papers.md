---
title: Latest Deep Learning Papers
date: 2021-02-27 21:00:01 +0800
featured-img: DL-logo
categories: [Deep_Learning]
tags: [Deep_Learning]
mathjax: Yes
---

<h1>Your arXiv Feed (67 Articles)</h1>
<h2>Kernel-based framework to estimate deformations of pneumothorax lung using relative position of anatomical landmarks. (arXiv:2102.12505v1 [cs.CV])</h2>
<h3>Utako Yamamoto, Megumi Nakao, Masayuki Ohzeki, Junko Tokuno, Toyofumi Fengshi Chen-Yoshikawa, Tetsuya Matsuda</h3>
<p>In video-assisted thoracoscopic surgeries, successful procedures of nodule
resection are highly dependent on the precise estimation of lung deformation
between the inflated lung in the computed tomography (CT) images during
preoperative planning and the deflated lung in the treatment views during
surgery. Lungs in the pneumothorax state during surgery have a large volume
change from normal lungs, making it difficult to build a mechanical model. The
purpose of this study is to develop a deformation estimation method of the 3D
surface of a deflated lung from a few partial observations. To estimate
deformations for a largely deformed lung, a kernel regression-based solution
was introduced. The proposed method used a few landmarks to capture the partial
deformation between the 3D surface mesh obtained from preoperative CT and the
intraoperative anatomical positions. The deformation for each vertex of the
entire mesh model was estimated per-vertex as a relative position from the
landmarks. The landmarks were placed in the anatomical position of the lung's
outer contour. The method was applied on nine datasets of the left lungs of
live Beagle dogs. Contrast-enhanced CT images of the lungs were acquired. The
proposed method achieved a local positional error of vertices of 2.74 mm,
Hausdorff distance of 6.11 mm, and Dice similarity coefficient of 0.94.
Moreover, the proposed method could estimate lung deformations from a small
number of training cases and a small observation area. This study contributes
to the data-driven modeling of pneumothorax deformation of the lung.
</p>
<a href="http://arxiv.org/abs/2102.12505" target="_blank">arXiv:2102.12505</a> [<a href="http://arxiv.org/pdf/2102.12505" target="_blank">pdf</a>]

<h2>The Catenary Robot: Design and Control of a Cable Propelled by Two Quadrotors. (arXiv:2102.12519v1 [cs.RO])</h2>
<h3>Diego S. D&#x27;antonio, Gustavo A. Cardona, David Salda&#xf1;a</h3>
<p>Transporting objects using aerial robots has been widely studied in the
literature. Still, those approaches always assume that the connection between
the quadrotor and the load is made in a previous stage. However, that previous
stage usually requires human intervention, and autonomous procedures to locate
and attach the object are not considered. Additionally, most of the approaches
assume cables as rigid links, but manipulating cables requires considering the
state when the cables are hanging. In this work, we design and control a
\emph{catenary robot}. Our robot is able to transport hook-shaped objects in
the environment. The robotic system is composed of two quadrotors attached to
the two ends of a cable. By defining the catenary curve with five degrees of
freedom, position in 3-D, orientation in the z-axis, and span, we can drive the
two quadrotors to track a given trajectory. We validate our approach with
simulations and real robots. We present four different scenarios of
experiments. Our numerical solution is computationally fast and can be executed
in real-time.
</p>
<a href="http://arxiv.org/abs/2102.12519" target="_blank">arXiv:2102.12519</a> [<a href="http://arxiv.org/pdf/2102.12519" target="_blank">pdf</a>]

<h2>Software Engineering for Robotic Systems:a systematic mapping study. (arXiv:2102.12520v1 [cs.RO])</h2>
<h3>Marcela G. dos Santos, Fabio Petrillo</h3>
<p>Robots are being applied in a vast range of fields, leading researchers and
practitioners to write tasks more complex than in the past. The robot software
complexity increases the difficulty of engineering the robot's software
components with quality requirements. Researchers and practitioners have
applied software engineering (SE) approaches and robotic domains to address
this issue in the last two decades. This study aims to identify, classify and
evaluate the current state-of-the-art Software Engineering for Robotic Systems
(SERS). We systematically selected and analyzed 50 primary studies extracted
from an automated search on Scopus digital library and manual search on the two
editions of the RoSE workshop. We present three main contributions. Firstly, we
provide an analysis from three following perspectives: demographics of
publication, SE areas applied in robotics domains, and RSE findings. Secondly,
we show a catalogue of research studies that apply software engineering
techniques in the robotic domain, classified with the SWEBOK guide. We have
identified 5 of 15 software engineering areas from the SWEBOK guide applied
explicitly in robotic domains. The majority of the studies focused on the
development phase (design, models and methods and construction). Testing and
quality software areas have little coverage in SERS. Finally, we identify
research opportunities and gaps in software engineering for robotic systems for
future studies.
</p>
<a href="http://arxiv.org/abs/2102.12520" target="_blank">arXiv:2102.12520</a> [<a href="http://arxiv.org/pdf/2102.12520" target="_blank">pdf</a>]

<h2>Auto-Detection of Tibial Plateau Angle in Canine Radiographs Using a Deep Learning Approach. (arXiv:2102.12544v1 [cs.CV])</h2>
<h3>Masuda Akter Tonima, F M Anim Hossain, Austin DeHart, Youmin Zhang</h3>
<p>Stifle joint issues are a major cause of lameness in dogs and it can be a
significant marker for various forms of diseases or injuries. A known Tibial
Plateau Angle (TPA) helps in the reduction of the diagnosis time of the cause.
With the state of the art object detection algorithm YOLO, and its variants,
this paper delves into identifying joints, their centroids and other regions of
interest to draw multiple line axes and finally calculating the TPA. The
methods investigated predicts successfully the TPA within the normal range for
80 percent of the images.
</p>
<a href="http://arxiv.org/abs/2102.12544" target="_blank">arXiv:2102.12544</a> [<a href="http://arxiv.org/pdf/2102.12544" target="_blank">pdf</a>]

<h2>Robust SleepNets. (arXiv:2102.12555v1 [cs.CV])</h2>
<h3>Yigit Alparslan, Edward Kim</h3>
<p>State-of-the-art convolutional neural networks excel in machine learning
tasks such as face recognition, and object classification but suffer
significantly when adversarial attacks are present. It is crucial that machine
critical systems, where machine learning models are deployed, utilize robust
models to handle a wide range of variability in the real world and malicious
actors that may use adversarial attacks. In this study, we investigate eye
closedness detection to prevent vehicle accidents related to driver
disengagements and driver drowsiness. Specifically, we focus on adversarial
attacks in this application domain, but emphasize that the methodology can be
applied to many other domains. We develop two models to detect eye closedness:
first model on eye images and a second model on face images. We adversarially
attack the models with Projected Gradient Descent, Fast Gradient Sign and
DeepFool methods and report adversarial success rate. We also study the effect
of training data augmentation. Finally, we adversarially train the same models
on perturbed images and report the success rate for the defense against these
attacks. We hope our study sets up the work to prevent potential vehicle
accidents by capturing drivers' face images and alerting them in case driver's
eyes are closed due to drowsiness.
</p>
<a href="http://arxiv.org/abs/2102.12555" target="_blank">arXiv:2102.12555</a> [<a href="http://arxiv.org/pdf/2102.12555" target="_blank">pdf</a>]

<h2>Deep Compact Polyhedral Conic Classifier for Open and Closed Set Recognition. (arXiv:2102.12570v1 [cs.CV])</h2>
<h3>Hakan Cevikalp, Bedirhan Uzun, Okan K&#xf6;p&#xfc;kl&#xfc;, Gurkan Ozturk</h3>
<p>In this paper, we propose a new deep neural network classifier that
simultaneously maximizes the inter-class separation and minimizes the
intra-class variation by using the polyhedral conic classification function.
The proposed method has one loss term that allows the margin maximization to
maximize the inter-class separation and another loss term that controls the
compactness of the class acceptance regions. Our proposed method has a nice
geometric interpretation using polyhedral conic function geometry. We tested
the proposed method on various visual classification problems including
closed/open set recognition and anomaly detection. The experimental results
show that the proposed method typically outperforms other state-of-the art
methods, and becomes a better choice compared to other tested methods
especially for open set recognition type problems.
</p>
<a href="http://arxiv.org/abs/2102.12570" target="_blank">arXiv:2102.12570</a> [<a href="http://arxiv.org/pdf/2102.12570" target="_blank">pdf</a>]

<h2>AniGAN: Style-Guided Generative Adversarial Networks for Unsupervised Anime Face Generation. (arXiv:2102.12593v1 [cs.CV])</h2>
<h3>Bing Li, Yuanlue Zhu, Yitong Wang, Chia-Wen Lin, Bernard Ghanem, Linlin Shen</h3>
<p>In this paper, we propose a novel framework to translate a portrait
photo-face into an anime appearance. Our aim is to synthesize anime-faces which
are style-consistent with a given reference anime-face. However, unlike typical
translation tasks, such anime-face translation is challenging due to complex
variations of appearances among anime-faces. Existing methods often fail to
transfer the styles of reference anime-faces, or introduce noticeable
artifacts/distortions in the local shapes of their generated faces. We propose
Ani- GAN, a novel GAN-based translator that synthesizes highquality
anime-faces. Specifically, a new generator architecture is proposed to
simultaneously transfer color/texture styles and transform local facial shapes
into anime-like counterparts based on the style of a reference anime-face,
while preserving the global structure of the source photoface. We propose a
double-branch discriminator to learn both domain-specific distributions and
domain-shared distributions, helping generate visually pleasing anime-faces and
effectively mitigate artifacts. Extensive experiments qualitatively and
quantitatively demonstrate the superiority of our method over state-of-the-art
methods.
</p>
<a href="http://arxiv.org/abs/2102.12593" target="_blank">arXiv:2102.12593</a> [<a href="http://arxiv.org/pdf/2102.12593" target="_blank">pdf</a>]

<h2>Railway Anomaly detection model using synthetic defect images generated by CycleGAN. (arXiv:2102.12595v1 [cs.CV])</h2>
<h3>Takuro Hoshi, Yohei Baba, Gaurang Gavai</h3>
<p>Although training data is essential for machine learning, railway companies
are facing difficulties in gathering adequate images of defective equipment due
to their proactive replacement of would be defective equipment. Nevertheless,
proactive replacement is indispensable for safe and undisturbed operation of
public transport. In this research, we have developed a model using CycleGAN to
generate artificial images of defective equipment instead of real images. By
adopting these generated images as training data, we verified that these images
are indistinguishable from real images and they play a vital role in enhancing
the accuracy of the defect detection models.
</p>
<a href="http://arxiv.org/abs/2102.12595" target="_blank">arXiv:2102.12595</a> [<a href="http://arxiv.org/pdf/2102.12595" target="_blank">pdf</a>]

<h2>How to represent part-whole hierarchies in a neural network. (arXiv:2102.12627v1 [cs.CV])</h2>
<h3>Geoffrey Hinton</h3>
<p>This paper does not describe a working system. Instead, it presents a single
idea about representation which allows advances made by several different
groups to be combined into an imaginary system called GLOM. The advances
include transformers, neural fields, contrastive representation learning,
distillation and capsules. GLOM answers the question: How can a neural network
with a fixed architecture parse an image into a part-whole hierarchy which has
a different structure for each image? The idea is simply to use islands of
identical vectors to represent the nodes in the parse tree. If GLOM can be made
to work, it should significantly improve the interpretability of the
representations produced by transformer-like systems when applied to vision or
language
</p>
<a href="http://arxiv.org/abs/2102.12627" target="_blank">arXiv:2102.12627</a> [<a href="http://arxiv.org/pdf/2102.12627" target="_blank">pdf</a>]

<h2>Theory and Analysis of Optimal Planning over Long and Infinite Horizons for Achieving Independent Partially-Observable Tasks that Evolve over Time. (arXiv:2102.12633v1 [cs.RO])</h2>
<h3>Anahita Mohseni-Kabir, Manuela Veloso, Maxim Likhachev</h3>
<p>We present the theoretical analysis and proofs of a recently developed
algorithm that allows for optimal planning over long and infinite horizons for
achieving multiple independent tasks that are partially observable and evolve
over time.
</p>
<a href="http://arxiv.org/abs/2102.12633" target="_blank">arXiv:2102.12633</a> [<a href="http://arxiv.org/pdf/2102.12633" target="_blank">pdf</a>]

<h2>CelebA-Spoof Challenge 2020 on Face Anti-Spoofing: Methods and Results. (arXiv:2102.12642v1 [cs.CV])</h2>
<h3>Yuanhan Zhang, Zhenfei Yin, Jing Shao, Ziwei Liu, Shuo Yang, Yuanjun Xiong, Wei Xia, Yan Xu, Man Luo, Jian Liu, Jianshu Li, Zhijun Chen, Mingyu Guo, Hui Li, Junfu Liu, Pengfei Gao, Tianqi Hong, Hao Han, Shijie Liu, Xinhua Chen, Di Qiu, Cheng Zhen, Dashuang Liang, Yufeng Jin, Zhanlong Hao</h3>
<p>As facial interaction systems are prevalently deployed, security and
reliability of these systems become a critical issue, with substantial research
efforts devoted. Among them, face anti-spoofing emerges as an important area,
whose objective is to identify whether a presented face is live or spoof.
Recently, a large-scale face anti-spoofing dataset, CelebA-Spoof which
comprised of 625,537 pictures of 10,177 subjects has been released. It is the
largest face anti-spoofing dataset in terms of the numbers of the data and the
subjects. This paper reports methods and results in the CelebA-Spoof Challenge
2020 on Face AntiSpoofing which employs the CelebA-Spoof dataset. The model
evaluation is conducted online on the hidden test set. A total of 134
participants registered for the competition, and 19 teams made valid
submissions. We will analyze the top ranked solutions and present some
discussion on future work directions.
</p>
<a href="http://arxiv.org/abs/2102.12642" target="_blank">arXiv:2102.12642</a> [<a href="http://arxiv.org/pdf/2102.12642" target="_blank">pdf</a>]

<h2>Adapting legacy robotic machinery to industry 4: a ciot experiment version 1. (arXiv:2102.12649v1 [cs.RO])</h2>
<h3>Hadi Alasti</h3>
<p>This paper presents an experimental adaptation of a non-collaborative robot
arm to collaborate with the environment, as one step towards adapting legacy
robotic machinery to fit in industry 4.0 requirements. A cloud-based internet
of things (CIoT) service is employed to connect, supervise and control a
robotic arm's motion using the added wireless sensing devices to the
environment. A programmable automation controller (PAC) unit, connected to the
robot arm receives the most recent changes and updates the motion of the robot
arm. The experimental results show that the proposed non-expensive service is
tractable and adaptable to higher level for machine to machine collaboration.
The proposed approach in this paper has industrial and educational
applications. In the proposed approach, the CIoT technology is added as a
technology interface between the sensors to the environment and the robotic
arm. The proposed approach is versatile and fits to variety of applications to
meet the flexible requirements of industry 4.0. The proposed approach has been
implemented in an experiment using MECA 500 robot arm and AMAX 5580
programmable automation controller and ultrasonic proximity wireless sensor.
</p>
<a href="http://arxiv.org/abs/2102.12649" target="_blank">arXiv:2102.12649</a> [<a href="http://arxiv.org/pdf/2102.12649" target="_blank">pdf</a>]

<h2>Learning Inverse Kinodynamics for Accurate High-Speed Off-Road Navigation on Unstructured Terrain. (arXiv:2102.12667v1 [cs.RO])</h2>
<h3>Xuesu Xiao, Joydeep Biswas, Peter Stone</h3>
<p>This paper presents a learning-based approach to consider the effect of
unobservable world states in kinodynamic motion planning in order to enable
accurate high-speed off-road navigation on unstructured terrain. Existing
kinodynamic motion planners either operate in structured and homogeneous
environments and thus do not need to explicitly account for terrain-vehicle
interaction, or assume a set of discrete terrain classes. However, when
operating on unstructured terrain, especially at high speeds, even small
variations in the environment will be magnified and cause inaccurate plan
execution. In this paper, to capture the complex kinodynamic model and
mathematically unknown world state, we learn a kinodynamic planner in a
data-driven manner with onboard inertial observations. Our approach is tested
on a physical robot in different indoor and outdoor environments, enables fast
and accurate off-road navigation, and outperforms environment-independent
alternatives, demonstrating 52.4% to 86.9% improvement in terms of plan
execution success rate while traveling at high speeds.
</p>
<a href="http://arxiv.org/abs/2102.12667" target="_blank">arXiv:2102.12667</a> [<a href="http://arxiv.org/pdf/2102.12667" target="_blank">pdf</a>]

<h2>Imitation Learning for Robust and Safe Real-time Motion Planning: A Contraction Theory Approach. (arXiv:2102.12668v1 [cs.RO])</h2>
<h3>Hiroyasu Tsukamoto, Soon-Jo Chung</h3>
<p>This paper presents Learning-based Autonomous Guidance with Robustness,
Optimality, and Safety guarantees (LAG-ROS), a real-time robust motion planning
algorithm for safety-critical nonlinear systems perturbed by bounded
disturbances. The LAG-ROS method consists of three phases: 1) Control Lyapunov
Function (CLF) construction via contraction theory; 2) imitation learning of
the CLF-based robust feedback motion planner; and 3) its real-time and
decentralized implementation with a learning-based model predictive safety
filter. For the CLF, we exploit a neural-network-based method of Neural
Contraction Metrics (NCMs), which provides a differential Lyapunov function to
minimize an upper bound of the steady-state Euclidean distance between
perturbed and unperturbed system trajectories. The NCM ensures the perturbed
state to stay in bounded error tubes around given desired trajectories, where
we sample training data for imitation learning of the NCM-CLF-based robust
centralized motion planner. Using local observations in training also enables
its decentralized implementation. Simulation results for perturbed nonlinear
systems show that the LAG-ROS achieves higher control performance and task
success rate with faster execution speed for real-time computation, when
compared with the existing real-time robust MPC and learning-based feedforward
motion planners.
</p>
<a href="http://arxiv.org/abs/2102.12668" target="_blank">arXiv:2102.12668</a> [<a href="http://arxiv.org/pdf/2102.12668" target="_blank">pdf</a>]

<h2>Real-Time Ellipse Detection for Robotics Applications. (arXiv:2102.12670v1 [cs.RO])</h2>
<h3>Azarakhsh Keipour, Guilherme A. S. Pereira, Sebastian Scherer</h3>
<p>We propose a new algorithm for real-time detection and tracking of elliptic
patterns suitable for real-world robotics applications. The method fits
ellipses to each contour in the image frame and rejects ellipses that do not
yield a good fit. It can detect complete, partial, and imperfect ellipses in
extreme weather and lighting conditions and is lightweight enough to be used on
robots' resource-limited onboard computers. The method is used on an example
application of autonomous UAV landing on a fast-moving vehicle to show its
performance indoors, outdoors, and in simulation on a real-world robotics task.
The comparison with other well-known ellipse detection methods shows that our
proposed algorithm outperforms other methods with the F1 score of 0.981 on a
dataset with over 1500 frames. The videos of experiments, the source codes, and
the collected dataset are provided with the paper.
</p>
<a href="http://arxiv.org/abs/2102.12670" target="_blank">arXiv:2102.12670</a> [<a href="http://arxiv.org/pdf/2102.12670" target="_blank">pdf</a>]

<h2>Ensuring Progress for Multiple Mobile Robots via Space Partitioning, Motion Rules, and Adaptively Centralized Conflict Resolution. (arXiv:2102.12684v1 [cs.RO])</h2>
<h3>Claire Liang (1), Wil Thomason (1), Elizabeth Ricci (1), Soham Sankaran (1, 2) ((1) Cornell University Department of Computer Science, (2) Pashi Corp.)</h3>
<p>In environments where multiple robots must coordinate in a shared space,
decentralized approaches allow for decoupled planning at the cost of global
guarantees, while centralized approaches make the opposite trade-off. These
solutions make a range of assumptions - commonly, that all the robots share the
same planning strategies. In this work, we present a framework that ensures
progress for all robots without assumptions on any robot's planning strategy by
(1) generating a partition of the environment into "flow", "open", and
"passage" regions and (2) imposing a set of rules for robot motion in these
regions. These rules for robot motion prevent deadlock through an adaptively
centralized protocol for resolving spatial conflicts between robots. Our
proposed framework ensures progress for all robots without a grid-like
discretization of the environment or strong requirements on robot
communication, coordination, or cooperation. Each robot can freely choose how
to plan and coordinate for itself, without being vulnerable to other robots or
groups of robots blocking them from their goals, as long as they follow the
rules when necessary. We describe our space partition and motion rules, prove
that the motion rules suffice to guarantee progress in partitioned
environments, and demonstrate several cases in simulated polygonal
environments. This work strikes a balance between each robot's planning
independence and a guarantee that each robot can always reach any goal in
finite time.
</p>
<a href="http://arxiv.org/abs/2102.12684" target="_blank">arXiv:2102.12684</a> [<a href="http://arxiv.org/pdf/2102.12684" target="_blank">pdf</a>]

<h2>Strapdown Inertial Navigation System Initial Alignment based on Group of Double Direct Spatial Isometries. (arXiv:2102.12697v1 [cs.RO])</h2>
<h3>Lubin Chang, Fangjun Qin, Jiangning Xu</h3>
<p>The task of strapdown inertial navigation system (SINS) initial alignment is
to calculate the attitude transformation matrix from body frame to navigation
frame. In this paper, such attitude transformation matrix is divided into two
parts through introducing the initial inertially fixed navigation frame as
inertial frame. The attitude changes of the navigation frame corresponding to
the defined inertial frame can be exactly calculated with known velocity and
position provided by GNSS. The attitude from body frame to the defined inertial
frame is estimated based on the SINS mechanization in inertial frame. The
attitude, velocity and position in inertial frame are formulated together as
element of the group of double direct spatial isometries.It is proven that the
group state model in inertial frame satisfies a particular "group affine"
property and the corresponding error model satisfies a "log linear" autonomous
differential equation on the Lie algebra. Based on such striking property, the
attitude from body frame to the defined inertial frame can be estimated based
on the linear error model with even extreme large misalignments. Two different
error state vectors are extracted based on right and left matrix
multiplications and the detailed linear state space models are derived based on
the right and left errors for SINS mechanization in inertial frame. With the
derived linear state space models, the explicit initial alignment procedures
have been presented. Extensive simulation and field tests indicate that the
initial alignment based on the left error model can perform quite well within a
wide range of initial attitude errors, although the used filter is still a type
of linear Kalman filter. This method is promising in practical products
abandoning the traditional coarse alignment stage.
</p>
<a href="http://arxiv.org/abs/2102.12697" target="_blank">arXiv:2102.12697</a> [<a href="http://arxiv.org/pdf/2102.12697" target="_blank">pdf</a>]

<h2>A Simulation-based End-to-End Learning Framework for Evidential Occupancy Grid Mapping. (arXiv:2102.12718v1 [cs.RO])</h2>
<h3>Raphael van Kempen, Bastian Lampe, Timo Woopen, Lutz Eckstein</h3>
<p>Evidential occupancy grid maps (OGMs) are a popular representation of the
environment of automated vehicles. Inverse sensor models (ISMs) are used to
compute OGMs from sensor data such as lidar point clouds. Geometric ISMs show a
limited performance when estimating states in unobserved but inferable areas
and have difficulties dealing with ambiguous input. Deep learning-based ISMs
face the challenge of limited training data and they often cannot handle
uncertainty quantification yet. We propose a deep learning-based framework for
learning an OGM algorithm which is both capable of quantifying uncertainty and
which does not rely on manually labeled data. Results on synthetic and on
real-world data show superiority over other approaches.
</p>
<a href="http://arxiv.org/abs/2102.12718" target="_blank">arXiv:2102.12718</a> [<a href="http://arxiv.org/pdf/2102.12718" target="_blank">pdf</a>]

<h2>Defining Preferred and Natural Robot Motions in Immersive Telepresence from a First-Person Perspective. (arXiv:2102.12719v1 [cs.RO])</h2>
<h3>Katherine J. Mimnaugh, Markku Suomalainen, Israel Becerra, Eliezer Lozano, Rafael Murrieta-Cid, Steven M. LaValle</h3>
<p>This paper presents some early work and future plans regarding how the
autonomous motions of a telepresence robot affect a person embodied in the
robot through a head-mounted display. We consider the preferences, comfort, and
the perceived naturalness of aspects of piecewise linear paths compared to the
same aspects on a smooth path. In a user study, thirty-six subjects (eighteen
females) watched panoramic videos of three different paths through a simulated
museum in virtual reality and responded to questionnaires regarding each path.
We found that comfort had a strong effect on path preference, and that the
subjective feeling of naturalness also had a strong effect on path preference,
even though people consider different things as natural. We describe a
categorization of the responses regarding the naturalness of the robot's motion
and provide a recommendation on how this can be applied more broadly. Although
immersive robotic telepresence is increasingly being used for remote education,
clinical care, and to assist people with disabilities or mobility
complications, the full potential of this technology is limited by issues
related to user experience. Our work addresses these shortcomings and will
enable the future personalization of telepresence experiences for the
improvement of overall remote communication and the enhancement of the feeling
of presence in a remote location.
</p>
<a href="http://arxiv.org/abs/2102.12719" target="_blank">arXiv:2102.12719</a> [<a href="http://arxiv.org/pdf/2102.12719" target="_blank">pdf</a>]

<h2>Design and Control of a Highly Redundant Rigid-Flexible Coupling Robot to Assist the COVID-19 Oropharyngeal-Swab Sampling. (arXiv:2102.12726v1 [cs.RO])</h2>
<h3>Yingbai Hu (3 and 2), Jian Li (1 and 2), Yongquan Chen (1 and 2), Qiwen Wang (2 and 1), Chuliang Chi (2 and 1), Heng Zhang (2 and 1), Qing Gao (2 and 1), Yuanmin Lan (6 and 2), Zheng Li (4 and 2), Zonggao Mu (5 and 2), Zhenglong Sun (1 and 2), Alois Knoll (3) ((1) Robotics and Intelligent Manufacturing &amp; School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen, China, (2) Shenzhen Institute of Artificial Intelligence and Robotics for Society, China, (3) Chair of Robotics, Artificial Intelligence and Real-time Systems, Technische Universit M&#xfc;nchen, M&#xfc;nchen, Germany, (4) Department of surgery, and Chow Yuk Ho Technology Centre for Innovative Medicine, The Chinese University of Hong Kong, Hong Kong, (5) School of Mechanical Engineering, Shandong University of Technology, Zibo, China, (6) Longgang District People&#x27;s Hospital of Shenzhen, China.)</h3>
<p>The outbreak of novel coronavirus pneumonia (COVID-19) has caused mortality
and morbidity worldwide. Oropharyngeal-swab (OP-swab) sampling is widely used
for the diagnosis of COVID-19 in the world. To avoid the clinical staff from
being affected by the virus, we developed a 9-degree-of-freedom (DOF)
rigid-flexible coupling (RFC) robot to assist the COVID-19 OP-swab sampling.
This robot is composed of a visual system, UR5 robot arm, micro-pneumatic
actuator and force-sensing system. The robot is expected to reduce risk and
free up the clinical staff from the long-term repetitive sampling work.
Compared with a rigid sampling robot, the developed force-sensing RFC robot can
facilitate OP-swab sampling procedures in a safer and softer way. In addition,
a varying-parameter zeroing neural network-based optimization method is also
proposed for motion planning of the 9-DOF redundant manipulator. The developed
robot system is validated by OP-swab sampling on both oral cavity phantoms and
volunteers.
</p>
<a href="http://arxiv.org/abs/2102.12726" target="_blank">arXiv:2102.12726</a> [<a href="http://arxiv.org/pdf/2102.12726" target="_blank">pdf</a>]

<h2>Scene Retrieval for Contextual Visual Mapping. (arXiv:2102.12728v1 [cs.CV])</h2>
<h3>William H. B. Smith, Michael Milford, Klaus D. McDonald-Maier, Shoaib Ehsan</h3>
<p>Visual navigation localizes a query place image against a reference database
of place images, also known as a `visual map'. Localization accuracy
requirements for specific areas of the visual map, `scene classes', vary
according to the context of the environment and task. State-of-the-art visual
mapping is unable to reflect these requirements by explicitly targetting scene
classes for inclusion in the map. Four different scene classes, including
pedestrian crossings and stations, are identified in each of the Nordland and
St. Lucia datasets. Instead of re-training separate scene classifiers which
struggle with these overlapping scene classes we make our first contribution:
defining the problem of `scene retrieval'. Scene retrieval extends image
retrieval to classification of scenes defined at test time by associating a
single query image to reference images of scene classes. Our second
contribution is a triplet-trained convolutional neural network (CNN) to address
this problem which increases scene classification accuracy by up to 7% against
state-of-the-art networks pre-trained for scene recognition. The second
contribution is an algorithm `DMC' that combines our scene classification with
distance and memorability for visual mapping. Our analysis shows that DMC
includes 64% more images of our chosen scene classes in a visual map than just
using distance interval mapping. State-of-the-art visual place descriptors
AMOS-Net, Hybrid-Net and NetVLAD are finally used to show that DMC improves
scene class localization accuracy by a mean of 3% and localization accuracy of
the remaining map images by a mean of 10% across both datasets.
</p>
<a href="http://arxiv.org/abs/2102.12728" target="_blank">arXiv:2102.12728</a> [<a href="http://arxiv.org/pdf/2102.12728" target="_blank">pdf</a>]

<h2>Active Modular Environment for Robot Navigation. (arXiv:2102.12748v1 [cs.RO])</h2>
<h3>Shota Kameyama, Keisuke Okumura, Yasumasa Tamura, Xavier D&#xe9;fago</h3>
<p>This paper presents a novel robot-environment interaction in navigation tasks
such that robots have neither a representation of their working space nor
planning function, instead, an active environment takes charge of these
aspects. This is realized by spatially deploying computing units, called cells,
and making cells manage traffic in their respective physical region. Different
from stigmegic approaches, cells interact with each other to manage
environmental information and to construct instructions on how robots move.

As a proof-of-concept, we present an architecture called AFADA and its
prototype, consisting of modular cells and robots moving on the cells. The
instructions from cells are based on a distributed routing algorithm and a
reservation protocol. We demonstrate that AFADA achieves efficient robot moves
for single-robot navigation in a dynamic environment changing its topology with
a stochastic model, comparing to self-navigation by a robot itself. This is
followed by several demos, including multi-robot navigation, highlighting the
power of offloading both representation and planning from robots to the
environment. We expect that the concept of AFADA contributes to developing the
infrastructure for multiple robots because it can engage online and lifelong
planning and execution.
</p>
<a href="http://arxiv.org/abs/2102.12748" target="_blank">arXiv:2102.12748</a> [<a href="http://arxiv.org/pdf/2102.12748" target="_blank">pdf</a>]

<h2>Domain Adaptation for Learning Generator from Paired Few-Shot Data. (arXiv:2102.12765v1 [cs.CV])</h2>
<h3>Chun-Chih Teng, Pin-Yu Chen, Wei-Chen Chiu</h3>
<p>We propose a Paired Few-shot GAN (PFS-GAN) model for learning generators with
sufficient source data and a few target data. While generative model learning
typically needs large-scale training data, our PFS-GAN not only uses the
concept of few-shot learning but also domain shift to transfer the knowledge
across domains, which alleviates the issue of obtaining low-quality generator
when only trained with target domain data. The cross-domain datasets are
assumed to have two properties: (1) each target-domain sample has its
source-domain correspondence and (2) two domains share similar content
information but different appearance. Our PFS-GAN aims to learn the
disentangled representation from images, which composed of domain-invariant
content features and domain-specific appearance features. Furthermore, a
relation loss is introduced on the content features while shifting the
appearance features to increase the structural diversity. Extensive experiments
show that our method has better quantitative and qualitative results on the
generated target-domain data with higher diversity in comparison to several
baselines.
</p>
<a href="http://arxiv.org/abs/2102.12765" target="_blank">arXiv:2102.12765</a> [<a href="http://arxiv.org/pdf/2102.12765" target="_blank">pdf</a>]

<h2>SCD: A Stacked Carton Dataset for Detection and Segmentation. (arXiv:2102.12808v1 [cs.CV])</h2>
<h3>Jinrong Yang, Shengkai Wu, Lijun Gou, Hangcheng Yu, Chenxi Lin, Jiazhuo Wang, Minxuan Li, Xiaoping Li</h3>
<p>Carton detection is an important technique in the automatic logistics system
and can be applied to many applications such as the stacking and unstacking of
cartons, the unloading of cartons in the containers. However, there is no
public large-scale carton dataset for the research community to train and
evaluate the carton detection models up to now, which hinders the development
of carton detection. In this paper, we present a large-scale carton dataset
named Stacked Carton Dataset(SCD) with the goal of advancing the
state-of-the-art in carton detection. Images are collected from the internet
and several warehourses, and objects are labeled using per-instance
segmentation for precise localization. There are totally 250,000 instance masks
from 16,136 images. In addition, we design a carton detector based on RetinaNet
by embedding Offset Prediction between Classification and Localization
module(OPCL) and Boundary Guided Supervision module(BGS). OPCL alleviates the
imbalance problem between classification and localization quality which boosts
AP by 3.1% - 4.7% on SCD while BGS guides the detector to pay more attention to
boundary information of cartons and decouple repeated carton textures. To
demonstrate the generalization of OPCL to other datasets, we conduct extensive
experiments on MS COCO and PASCAL VOC. The improvement of AP on MS COCO and
PASCAL VOC is 1.8% - 2.2% and 3.4% - 4.3% respectively.
</p>
<a href="http://arxiv.org/abs/2102.12808" target="_blank">arXiv:2102.12808</a> [<a href="http://arxiv.org/pdf/2102.12808" target="_blank">pdf</a>]

<h2>FAITH: Fast iterative half-plane focus of expansion estimation using event-based optic flow. (arXiv:2102.12823v1 [cs.RO])</h2>
<h3>Raoul Dinaux, Nikhil Wessendorp, Julien Dupeyroux, Guido de Croon</h3>
<p>Course estimation is a key component for the development of autonomous
navigation systems for robots. While state-of-the-art methods widely use
visual-based algorithms, it is worth noting that they all fail to deal with the
complexity of the real world by being computationally greedy and sometimes too
slow. They often require obstacles to be highly textured to improve the overall
performance, particularly when the obstacle is located within the focus of
expansion (FOE) where the optic flow (OF) is almost null. This study proposes
the FAst ITerative Half-plane (FAITH) method to determine the course of a micro
air vehicle (MAV). This is achieved by means of an event-based camera, along
with a fast RANSAC-based algorithm that uses event-based OF to determine the
FOE. The performance is validated by means of a benchmark on a simulated
environment and then tested on a dataset collected for indoor obstacle
avoidance. Our results show that the computational efficiency of our solution
outperforms state-of-the-art methods while keeping a high level of accuracy.
This has been further demonstrated onboard an MAV equipped with an event-based
camera, showing that our event-based FOE estimation can be achieved online
onboard tiny drones, thus opening the path towards fully neuromorphic solutions
for autonomous obstacle avoidance and navigation onboard MAVs.
</p>
<a href="http://arxiv.org/abs/2102.12823" target="_blank">arXiv:2102.12823</a> [<a href="http://arxiv.org/pdf/2102.12823" target="_blank">pdf</a>]

<h2>A deep perceptual metric for 3D point clouds. (arXiv:2102.12839v1 [cs.CV])</h2>
<h3>Maurice Quach, Aladine Chetouani, Giuseppe Valenzise, Frederic Dufaux</h3>
<p>Point clouds are essential for storage and transmission of 3D content. As
they can entail significant volumes of data, point cloud compression is crucial
for practical usage. Recently, point cloud geometry compression approaches
based on deep neural networks have been explored. In this paper, we evaluate
the ability to predict perceptual quality of typical voxel-based loss functions
employed to train these networks. We find that the commonly used focal loss and
weighted binary cross entropy are poorly correlated with human perception. We
thus propose a perceptual loss function for 3D point clouds which outperforms
existing loss functions on the ICIP2020 subjective dataset. In addition, we
propose a novel truncated distance field voxel grid representation and find
that it leads to sparser latent spaces and loss functions that are more
correlated with perceived visual quality compared to a binary representation.
The source code is available at
https://github.com/mauriceqch/2021_pc_perceptual_loss.
</p>
<a href="http://arxiv.org/abs/2102.12839" target="_blank">arXiv:2102.12839</a> [<a href="http://arxiv.org/pdf/2102.12839" target="_blank">pdf</a>]

<h2>CausalX: Causal Explanations and Block Multilinear Factor Analysis. (arXiv:2102.12853v1 [cs.CV])</h2>
<h3>M. Alex O. Vasilescu, Eric Kim, Xiao S. Zeng</h3>
<p>By adhering to the dictum, "No causation without manipulation (treatment,
intervention)", cause and effect data analysis represents changes in observed
data in terms of changes in the causal factors. When causal factors are not
amenable for active manipulation in the real world due to current technological
limitations or ethical considerations, a counterfactual approach performs an
intervention on the model of data formation. In the case of object
representation or activity (temporal object) representation, varying object
parts is generally unfeasible whether they be spatial and/or temporal.
Multilinear algebra, the algebra of higher-order tensors, is a suitable and
transparent framework for disentangling the causal factors of data formation.
Learning a part-based intrinsic causal factor representations in a multilinear
framework requires applying a set of interventions on a part-based multilinear
model. We propose a unified multilinear model of wholes and parts. We derive a
hierarchical block multilinear factorization, the M-mode Block SVD, that
computes a disentangled representation of the causal factors by optimizing
simultaneously across the entire object hierarchy. Given computational
efficiency considerations, we introduce an incremental bottom-up computational
alternative, the Incremental M-mode Block SVD, that employs the lower-level
abstractions, the part representations, to represent the higher level of
abstractions, the parent wholes. This incremental computational approach may
also be employed to update the causal model parameters when data becomes
available incrementally. The resulting object representation is an
interpretable combinatorial choice of intrinsic causal factor representations
related to an object's recursive hierarchy of wholes and parts that renders
object recognition robust to occlusion and reduces training data requirements.
</p>
<a href="http://arxiv.org/abs/2102.12853" target="_blank">arXiv:2102.12853</a> [<a href="http://arxiv.org/pdf/2102.12853" target="_blank">pdf</a>]

<h2>FASA: Feature Augmentation and Sampling Adaptation for Long-Tailed Instance Segmentation. (arXiv:2102.12867v1 [cs.CV])</h2>
<h3>Yuhang Zang, Chen Huang, Chen Change Loy</h3>
<p>Recent methods for long-tailed instance segmentation still struggle on rare
object classes with few training data. We propose a simple yet effective
method, Feature Augmentation and Sampling Adaptation (FASA), that addresses the
data scarcity issue by augmenting the feature space especially for rare
classes. Both the Feature Augmentation (FA) and feature sampling components are
adaptive to the actual training status -- FA is informed by the feature mean
and variance of observed real samples from past iterations, and we sample the
generated virtual features in a loss-adapted manner to avoid over-fitting. FASA
does not require any elaborate loss design, and removes the need for
inter-class transfer learning that often involves large cost and
manually-defined head/tail class groups. We show FASA is a fast, generic method
that can be easily plugged into standard or long-tailed segmentation
frameworks, with consistent performance gains and little added cost. FASA is
also applicable to other tasks like long-tailed classification with
state-of-the-art performance. Code will be released.
</p>
<a href="http://arxiv.org/abs/2102.12867" target="_blank">arXiv:2102.12867</a> [<a href="http://arxiv.org/pdf/2102.12867" target="_blank">pdf</a>]

<h2>CPG-ACTOR: Reinforcement Learning for Central Pattern Generators. (arXiv:2102.12891v1 [cs.RO])</h2>
<h3>Luigi Campanaro, Siddhant Gangapurwala, Daniele De Martini, Wolfgang Merkt, Ioannis Havoutis</h3>
<p>Central Pattern Generators (CPGs) have several properties desirable for
locomotion: they generate smooth trajectories, are robust to perturbations and
are simple to implement. Although conceptually promising, we argue that the
full potential of CPGs has so far been limited by insufficient sensory-feedback
information. This paper proposes a new methodology that allows tuning CPG
controllers through gradient-based optimization in a Reinforcement Learning
(RL) setting. To the best of our knowledge, this is the first time CPGs have
been trained in conjunction with a MultilayerPerceptron (MLP) network in a
Deep-RL context. In particular, we show how CPGs can directly be integrated as
the Actor in an Actor-Critic formulation. Additionally, we demonstrate how this
change permits us to integrate highly non-linear feedback directly from sensory
perception to reshape the oscillators' dynamics. Our results on a locomotion
task using a single-leg hopper demonstrate that explicitly using the CPG as the
Actor rather than as part of the environment results in a significant increase
in the reward gained over time (6x more) compared with previous approaches.
Furthermore, we show that our method without feedback reproduces results
similar to prior work with feedback. Finally, we demonstrate how our
closed-loop CPG progressively improves the hopping behaviour for longer
training epochs relying only on basic reward functions.
</p>
<a href="http://arxiv.org/abs/2102.12891" target="_blank">arXiv:2102.12891</a> [<a href="http://arxiv.org/pdf/2102.12891" target="_blank">pdf</a>]

<h2>$SE_2(3)$ based Extended Kalman Filter for Inertial-Integrated Navigation. (arXiv:2102.12897v1 [cs.RO])</h2>
<h3>Yarong Luo, Chi Guo, Shenyong You, Jianlang Hu, Jingnan Liu</h3>
<p>The error representation using the straight difference of two vectors in the
inertial navigation system may not be reasonable as it does not take the
direction difference into consideration. Therefore, we proposed to use the
$SE_2(3)$ matrix Lie group to represent the state of the inertial-integrated
navigation system which consequently leads to the common frame error
representation.

With the new velocity and position error definition, we leverage the group
affine dynamics with the autonomous error properties and derive the error state
differential equation for the inertial-integrated navigation on the
north-east-down local-level navigation frame and the earth-centered earth-fixed
frame, respectively, the corresponding extending Kalman filter (EKF), terms as
$SE_2(3)$-EKF has also been derived. It provides a new perspective on the
geometric EKF with a more sophisticated formula for the inertial navigation
system.
</p>
<a href="http://arxiv.org/abs/2102.12897" target="_blank">arXiv:2102.12897</a> [<a href="http://arxiv.org/pdf/2102.12897" target="_blank">pdf</a>]

<h2>Docking and Undocking a Modular Underactuated Oscillating Swimming Robot. (arXiv:2102.12909v1 [cs.RO])</h2>
<h3>Gedaliah Knizhnik, Mark Yim</h3>
<p>We describe a docking mechanism and strategy to allow modular self-assembly
for the Modboat: an inexpensive underactuated oscillating swimming robot
powered by a single motor. Because propulsion is achieved through oscillation,
orientation can be controlled only in the average; this complicates docking,
which requires precise position and orientation control. Given these
challenges, we present a docking strategy and a motion primitive for
controlling orientation, and show that this strategy allows successful docking
in multiple configurations. Moreover, we demonstrate that the Modboat is also
capable of undocking and changing its dock configuration, all without any
additional actuation. This is unique among similar modular robotic systems.
</p>
<a href="http://arxiv.org/abs/2102.12909" target="_blank">arXiv:2102.12909</a> [<a href="http://arxiv.org/pdf/2102.12909" target="_blank">pdf</a>]

<h2>Blocks World Revisited: The Effect of Self-Occlusion on Classification by Convolutional Neural Networks. (arXiv:2102.12911v1 [cs.CV])</h2>
<h3>Markus D. Solbach, John K. Tsotsos</h3>
<p>Despite the recent successes in computer vision, there remain new avenues to
explore. In this work, we propose a new dataset to investigate the effect of
self-occlusion on deep neural networks. With TEOS (The Effect of
Self-Occlusion), we propose a 3D blocks world dataset that focuses on the
geometric shape of 3D objects and their omnipresent challenge of
self-occlusion. We designed TEOS to investigate the role of self-occlusion in
the context of object classification. Even though remarkable progress has been
seen in object classification, self-occlusion is a challenge. In the
real-world, self-occlusion of 3D objects still presents significant challenges
for deep learning approaches. However, humans deal with this by deploying
complex strategies, for instance, by changing the viewpoint or manipulating the
scene to gather necessary information. With TEOS, we present a dataset of two
difficulty levels (L1 and L2 ), containing 36 and 12 objects, respectively. We
provide 738 uniformly sampled views of each object, their mask, object and
camera position, orientation, amount of self-occlusion, as well as the CAD
model of each object. We present baseline evaluations with five well-known
classification deep neural networks and show that TEOS poses a significant
challenge for all of them. The dataset, as well as the pre-trained models, are
made publicly available for the scientific community under
https://nvision2.data.eecs.yorku.ca/TEOS.
</p>
<a href="http://arxiv.org/abs/2102.12911" target="_blank">arXiv:2102.12911</a> [<a href="http://arxiv.org/pdf/2102.12911" target="_blank">pdf</a>]

<h2>Structured Prediction for CRiSP Inverse Kinematics Learning with Misspecified Robot Models. (arXiv:2102.12942v1 [cs.RO])</h2>
<h3>Gian Maria Marconi, Rafaello Camoriano, Lorenzo Rosasco, Carlo Ciliberto</h3>
<p>With the recent advances in machine learning, problems that traditionally
would require accurate modeling to be solved analytically can now be
successfully approached with data-driven strategies. Among these, computing the
inverse kinematics of a redundant robot arm poses a significant challenge due
to the non-linear structure of the robot, the hard joint constraints and the
non-invertible kinematics map. Moreover, most learning algorithms consider a
completely data-driven approach, while often useful information on the
structure of the robot is available and should be positively exploited. In this
work, we present a simple, yet effective, approach for learning the inverse
kinematics. We introduce a structured prediction algorithm that combines a
data-driven strategy with the model provided by a forward kinematics function
-- even when this function is misspeficied -- to accurately solve the problem.
The proposed approach ensures that predicted joint configurations are well
within the robot's constraints. We also provide statistical guarantees on the
generalization properties of our estimator as well as an empirical evaluation
of its performance on trajectory reconstruction tasks.
</p>
<a href="http://arxiv.org/abs/2102.12942" target="_blank">arXiv:2102.12942</a> [<a href="http://arxiv.org/pdf/2102.12942" target="_blank">pdf</a>]

<h2>Non-invasive Cognitive-level Human Interfacing for the Robotic Restoration of Reaching & Grasping. (arXiv:2102.12980v1 [cs.RO])</h2>
<h3>Ali Shafti, A. Aldo Faisal</h3>
<p>Assistive and Wearable Robotics have the potential to support humans with
different types of motor impairments to become independent and fulfil their
activities of daily living successfully. The success of these robot systems,
however, relies on the ability to meaningfully decode human action intentions
and carry them out appropriately. Neural interfaces have been explored for use
in such system with several successes, however, they tend to be invasive and
require training periods in the order of months. We present a robotic system
for human augmentation, capable of actuating the user's arm and fingers for
them, effectively restoring the capability of reaching, grasping and
manipulating objects; controlled solely through the user's eye movements. We
combine wearable eye tracking, the visual context of the environment and the
structural grammar of human actions to create a cognitive-level assistive
robotic setup that enables the users in fulfilling activities of daily living,
while conserving interpretability, and the agency of the user. The interface is
worn, calibrated and ready to use within 5 minutes. Users learn to control and
make successful use of the system with an additional 5 minutes of interaction.
The system is tested with 5 healthy participants, showing an average success
rate of $96.6\%$ on first attempt across 6 tasks.
</p>
<a href="http://arxiv.org/abs/2102.12980" target="_blank">arXiv:2102.12980</a> [<a href="http://arxiv.org/pdf/2102.12980" target="_blank">pdf</a>]

<h2>Maximizing Cosine Similarity Between Spatial Features for Unsupervised Domain Adaptation in Semantic Segmentation. (arXiv:2102.13002v1 [cs.CV])</h2>
<h3>Inseop Chung, Daesik Kim, Nojun Kwak</h3>
<p>We propose a novel method that tackles the problem of unsupervised domain
adaptation for semantic segmentation by maximizing the cosine similarity
between the source and the target domain at the feature level. A segmentation
network mainly consists of two parts, a feature extractor and a classification
head. We expect that if we can make the two domains have small domain gap at
the feature level, they would also have small domain discrepancy at the
classification head. Our method computes a cosine similarity matrix between the
source feature map and the target feature map, then we maximize the elements
exceeding a threshold to guide the target features to have high similarity with
the most similar source feature. Moreover, we use a class-wise source feature
dictionary which stores the latest features of the source domain to prevent the
unmatching problem when computing the cosine similarity matrix and be able to
compare a target feature with various source features from various images.
Through extensive experiments, we verify that our method gains performance on
two unsupervised domain adaptation tasks (GTA5$\to$ Cityscaspes and
SYNTHIA$\to$ Cityscapes).
</p>
<a href="http://arxiv.org/abs/2102.13002" target="_blank">arXiv:2102.13002</a> [<a href="http://arxiv.org/pdf/2102.13002" target="_blank">pdf</a>]

<h2>Learning for Unconstrained Space-Time Video Super-Resolution. (arXiv:2102.13011v1 [cs.CV])</h2>
<h3>Zhihao Shi, Chengqi Li, Linhui Dai, Xiaohong Liu, Jun Chen, Timothy N. Davidson</h3>
<p>Recent years have seen considerable research activities devoted to video
enhancement that simultaneously increases temporal frame rate and spatial
resolution. However, the existing methods either fail to explore the intrinsic
relationship between temporal and spatial information or lack flexibility in
the choice of final temporal/spatial resolution. In this work, we propose an
unconstrained space-time video super-resolution network, which can effectively
exploit space-time correlation to boost performance. Moreover, it has complete
freedom in adjusting the temporal frame rate and spatial resolution through the
use of the optical flow technique and a generalized pixelshuffle operation. Our
extensive experiments demonstrate that the proposed method not only outperforms
the state-of-the-art, but also requires far fewer parameters and less running
time.
</p>
<a href="http://arxiv.org/abs/2102.13011" target="_blank">arXiv:2102.13011</a> [<a href="http://arxiv.org/pdf/2102.13011" target="_blank">pdf</a>]

<h2>LES: Locally Exploitative Sampling for Robot Path Planning. (arXiv:2102.13064v1 [cs.RO])</h2>
<h3>Sagar Suhas Joshi, Seth Hutchinson, Panagiotis Tsiotras</h3>
<p>Sampling-based algorithms solve the path planning problem by generating
random samples in the search-space and incrementally growing a connectivity
graph or a tree. Conventionally, the sampling strategy used in these algorithms
is biased towards exploration to acquire information about the search-space. In
contrast, this work proposes an optimization-based procedure that generates new
samples to improve the cost-to-come value of vertices in a neighborhood. The
application of proposed algorithm adds an exploitative-bias to sampling and
results in a faster convergence to the optimal solution compared to other
state-of-the-art sampling techniques. This is demonstrated using benchmarking
experiments performed fora variety of higher dimensional robotic planning
tasks.
</p>
<a href="http://arxiv.org/abs/2102.13064" target="_blank">arXiv:2102.13064</a> [<a href="http://arxiv.org/pdf/2102.13064" target="_blank">pdf</a>]

<h2>Where to go next: Learning a Subgoal Recommendation Policy for Navigation Among Pedestrians. (arXiv:2102.13073v1 [cs.RO])</h2>
<h3>Bruno Brito, Michael Everett, Jonathan P. How, Javier Alonso-Mora</h3>
<p>Robotic navigation in environments shared with other robots or humans remains
challenging because the intentions of the surrounding agents are not directly
observable and the environment conditions are continuously changing. Local
trajectory optimization methods, such as model predictive control (MPC), can
deal with those changes but require global guidance, which is not trivial to
obtain in crowded scenarios. This paper proposes to learn, via deep
Reinforcement Learning (RL), an interaction-aware policy that provides
long-term guidance to the local planner. In particular, in simulations with
cooperative and non-cooperative agents, we train a deep network to recommend a
subgoal for the MPC planner. The recommended subgoal is expected to help the
robot in making progress towards its goal and accounts for the expected
interaction with other agents. Based on the recommended subgoal, the MPC
planner then optimizes the inputs for the robot satisfying its kinodynamic and
collision avoidance constraints. Our approach is shown to substantially improve
the navigation performance in terms of number of collisions as compared to
prior MPC frameworks, and in terms of both travel time and number of collisions
compared to deep RL methods in cooperative, competitive and mixed multiagent
scenarios.
</p>
<a href="http://arxiv.org/abs/2102.13073" target="_blank">arXiv:2102.13073</a> [<a href="http://arxiv.org/pdf/2102.13073" target="_blank">pdf</a>]

<h2>Simple multi-dataset detection. (arXiv:2102.13086v1 [cs.CV])</h2>
<h3>Xingyi Zhou, Vladlen Koltun, Philipp Kr&#xe4;henb&#xfc;hl</h3>
<p>How do we build a general and broad object detection system? We use all
labels of all concepts ever annotated. These labels span diverse datasets with
potentially inconsistent taxonomies. In this paper, we present a simple method
for training a unified detector on multiple large-scale datasets. We use
dataset-specific training protocols and losses, but share a common detection
architecture with dataset-specific outputs. We show how to automatically
integrate these dataset-specific outputs into a common semantic taxonomy. In
contrast to prior work, our approach does not require manual taxonomy
reconciliation. Our multi-dataset detector performs as well as dataset-specific
models on each training domain, but generalizes much better to new unseen
domains. Entries based on the presented methodology ranked first in the object
detection and instance segmentation tracks of the ECCV 2020 Robust Vision
Challenge.
</p>
<a href="http://arxiv.org/abs/2102.13086" target="_blank">arXiv:2102.13086</a> [<a href="http://arxiv.org/pdf/2102.13086" target="_blank">pdf</a>]

<h2>IBRNet: Learning Multi-View Image-Based Rendering. (arXiv:2102.13090v1 [cs.CV])</h2>
<h3>Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul Srinivasan, Howard Zhou, Jonathan T. Barron, Ricardo Martin-Brualla, Noah Snavely, Thomas Funkhouser</h3>
<p>We present a method that synthesizes novel views of complex scenes by
interpolating a sparse set of nearby views. The core of our method is a network
architecture that includes a multilayer perceptron and a ray transformer that
estimates radiance and volume density at continuous 5D locations (3D spatial
locations and 2D viewing directions), drawing appearance information on the fly
from multiple source views. By drawing on source views at render time, our
method hearkens back to classic work on image-based rendering (IBR), and allows
us to render high-resolution imagery. Unlike neural scene representation work
that optimizes per-scene functions for rendering, we learn a generic view
interpolation function that generalizes to novel scenes. We render images using
classic volume rendering, which is fully differentiable and allows us to train
using only multi-view posed images as supervision. Experiments show that our
method outperforms recent novel view synthesis methods that also seek to
generalize to novel scenes. Further, if fine-tuned on each scene, our method is
competitive with state-of-the-art single-scene neural rendering methods.
</p>
<a href="http://arxiv.org/abs/2102.13090" target="_blank">arXiv:2102.13090</a> [<a href="http://arxiv.org/pdf/2102.13090" target="_blank">pdf</a>]

<h2>Learning Generalizable Visual Representations via Interactive Gameplay. (arXiv:1912.08195v3 [cs.CV] UPDATED)</h2>
<h3>Luca Weihs, Aniruddha Kembhavi, Kiana Ehsani, Sarah M Pratt, Winson Han, Alvaro Herrasti, Eric Kolve, Dustin Schwenk, Roozbeh Mottaghi, Ali Farhadi</h3>
<p>A growing body of research suggests that embodied gameplay, prevalent not
just in human cultures but across a variety of animal species including turtles
and ravens, is critical in developing the neural flexibility for creative
problem solving, decision making, and socialization. Comparatively little is
known regarding the impact of embodied gameplay upon artificial agents. While
recent work has produced agents proficient in abstract games, these
environments are far removed from the real world and thus these agents can
provide little insight into the advantages of embodied play. Hiding games, such
as hide-and-seek, played universally, provide a rich ground for studying the
impact of embodied gameplay on representation learning in the context of
perspective taking, secret keeping, and false belief understanding. Here we are
the first to show that embodied adversarial reinforcement learning agents
playing Cache, a variant of hide-and-seek, in a high fidelity, interactive,
environment, learn generalizable representations of their observations encoding
information such as object permanence, free space, and containment. Moving
closer to biologically motivated learning strategies, our agents'
representations, enhanced by intentionality and memory, are developed through
interaction and play. These results serve as a model for studying how facets of
vision develop through interaction, provide an experimental framework for
assessing what is learned by artificial agents, and demonstrates the value of
moving from large, static, datasets towards experiential, interactive,
representation learning.
</p>
<a href="http://arxiv.org/abs/1912.08195" target="_blank">arXiv:1912.08195</a> [<a href="http://arxiv.org/pdf/1912.08195" target="_blank">pdf</a>]

<h2>Un-Mix: Rethinking Image Mixtures for Unsupervised Visual Representation Learning. (arXiv:2003.05438v3 [cs.CV] UPDATED)</h2>
<h3>Zhiqiang Shen, Zechun Liu, Zhuang Liu, Marios Savvides, Trevor Darrell, Eric Xing</h3>
<p>In supervised learning, smoothing label or prediction distribution in neural
network training has been proven useful in preventing the model from being
over-confident, and is crucial for learning more robust visual representations.
This observation motivates us to explore ways to make predictions flattened in
unsupervised learning. Considering that human-annotated labels are not adopted
in unsupervised learning, we introduce a straightforward approach to perturb
input image space in order to soften the output prediction space indirectly,
meanwhile, assigning new label values in the unsupervised frameworks
accordingly. Despite its conceptual simplicity, we show empirically that with
the simple solution -- Unsupervised image mixtures (Un-Mix), we can learn more
robust visual representations from the transformed input. Extensive experiments
are conducted on CIFAR-10, CIFAR-100, STL-10, Tiny ImageNet and standard
ImageNet with popular unsupervised methods SimCLR, BYOL, MoCo V1&amp;V2, etc. Our
proposed image mixture and label assignment strategy can obtain consistent
improvement by 1~3% following exactly the same hyperparameters and training
procedures of the base methods.
</p>
<a href="http://arxiv.org/abs/2003.05438" target="_blank">arXiv:2003.05438</a> [<a href="http://arxiv.org/pdf/2003.05438" target="_blank">pdf</a>]

<h2>Scenario-Transferable Semantic Graph Reasoning for Interaction-Aware Probabilistic Prediction. (arXiv:2004.03053v2 [cs.RO] UPDATED)</h2>
<h3>Yeping Hu, Wei Zhan, Masayoshi Tomizuka</h3>
<p>Accurately predicting the possible behaviors of traffic participants is an
essential capability for autonomous vehicles. Since autonomous vehicles need to
navigate in dynamically changing environments, they are expected to make
accurate predictions regardless of where they are and what driving
circumstances they encountered. A number of methodologies have been proposed to
solve prediction problems under different traffic situations. However, these
works either focus on one particular driving scenario (e.g. highway,
intersection, or roundabout) or do not take sufficient environment information
(e.g. road topology, traffic rules, and surrounding agents) into account. In
fact, the limitation to certain scenario is mainly due to the lackness of
generic representations of the environment. The insufficiency of environment
information further limits the flexibility and transferability of the
predictor. In this paper, we propose a scenario-transferable and
interaction-aware probabilistic prediction algorithm based on semantic graph
reasoning. We first introduce generic representations for both static and
dynamic elements in driving environments. Then these representations are
utilized to describe semantic goals for selected agents and incorporate them
into spatial-temporal structures. Finally, we reason internal relations among
these structured semantic representations using learning-based method and
obtain prediction results. The proposed algorithm is thoroughly examined under
several complicated real-world driving scenarios to demonstrate its flexibility
and transferability, where the predictor can be directly used under unforeseen
driving circumstances with different static and dynamic information.
</p>
<a href="http://arxiv.org/abs/2004.03053" target="_blank">arXiv:2004.03053</a> [<a href="http://arxiv.org/pdf/2004.03053" target="_blank">pdf</a>]

<h2>Anomaly Detection in Connected and Automated Vehicles using an Augmented State Formulation. (arXiv:2004.09496v2 [cs.RO] UPDATED)</h2>
<h3>Yiyang Wang, Neda Masoud, Anahita Khojandi</h3>
<p>In this paper we propose a novel observer-based method for anomaly detection
in connected and automated vehicles (CAVs). The proposed method utilizes an
augmented extended Kalman filter (AEKF) to smooth sensor readings of a CAV
based on a nonlinear car-following motion model with time delay, where the
leading vehicle's trajectory is used by the subject vehicle to detect sensor
anomalies. We use the classic $\chi^2$ fault detector in conjunction with the
proposed AEKF for anomaly detection. To make the proposed model more suitable
for real-world applications, we consider a stochastic communication time delay
in the car-following model. Our experiments conducted on real-world connected
vehicle data indicate that the AEKF with $\chi^2$-detector can achieve a high
anomaly detection performance.
</p>
<a href="http://arxiv.org/abs/2004.09496" target="_blank">arXiv:2004.09496</a> [<a href="http://arxiv.org/pdf/2004.09496" target="_blank">pdf</a>]

<h2>MakeItTalk: Speaker-Aware Talking-Head Animation. (arXiv:2004.12992v3 [cs.CV] UPDATED)</h2>
<h3>Yang Zhou, Xintong Han, Eli Shechtman, Jose Echevarria, Evangelos Kalogerakis, Dingzeyu Li</h3>
<p>We present a method that generates expressive talking heads from a single
facial image with audio as the only input. In contrast to previous approaches
that attempt to learn direct mappings from audio to raw pixels or points for
creating talking faces, our method first disentangles the content and speaker
information in the input audio signal. The audio content robustly controls the
motion of lips and nearby facial regions, while the speaker information
determines the specifics of facial expressions and the rest of the talking head
dynamics. Another key component of our method is the prediction of facial
landmarks reflecting speaker-aware dynamics. Based on this intermediate
representation, our method is able to synthesize photorealistic videos of
entire talking heads with full range of motion and also animate artistic
paintings, sketches, 2D cartoon characters, Japanese mangas, stylized
caricatures in a single unified framework. We present extensive quantitative
and qualitative evaluation of our method, in addition to user studies,
demonstrating generated talking heads of significantly higher quality compared
to prior state-of-the-art.
</p>
<a href="http://arxiv.org/abs/2004.12992" target="_blank">arXiv:2004.12992</a> [<a href="http://arxiv.org/pdf/2004.12992" target="_blank">pdf</a>]

<h2>DeepCorn: A Semi-Supervised Deep Learning Method for High-Throughput Image-Based Corn Kernel Counting and Yield Estimation. (arXiv:2007.10521v2 [cs.CV] UPDATED)</h2>
<h3>Saeed Khaki, Hieu Pham, Ye Han, Andy Kuhl, Wade Kent, Lizhi Wang</h3>
<p>The success of modern farming and plant breeding relies on accurate and
efficient collection of data. For a commercial organization that manages large
amounts of crops, collecting accurate and consistent data is a bottleneck. Due
to limited time and labor, accurately phenotyping crops to record color, head
count, height, weight, etc. is severely limited. However, this information,
combined with other genetic and environmental factors, is vital for developing
new superior crop species that help feed the world's growing population. Recent
advances in machine learning, in particular deep learning, have shown promise
in mitigating this bottleneck. In this paper, we propose a novel deep learning
method for counting on-ear corn kernels in-field to aid in the gathering of
real-time data and, ultimately, to improve decision making to maximize yield.
We name this approach DeepCorn, and show that this framework is robust under
various conditions. DeepCorn estimates the density of corn kernels in an image
of corn ears and predicts the number of kernels based on the estimated density
map. DeepCorn uses a truncated VGG-16 as a backbone for feature extraction and
merges feature maps from multiple scales of the network to make it robust
against image scale variations. We also adopt a semi-supervised learning
approach to further improve the performance of our proposed method. Our
proposed method achieves the MAE and RMSE of 41.36 and 60.27 in the corn kernel
counting task, respectively. Our experimental results demonstrate the
superiority and effectiveness of our proposed method compared to other
state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2007.10521" target="_blank">arXiv:2007.10521</a> [<a href="http://arxiv.org/pdf/2007.10521" target="_blank">pdf</a>]

<h2>Reliable Label Bootstrapping for Semi-Supervised Learning. (arXiv:2007.11866v2 [cs.CV] UPDATED)</h2>
<h3>Paul Albert, Diego Ortego, Eric Arazo, Noel E. O&#x27;Connor, Kevin McGuinness</h3>
<p>Reducing the amount of labels required to train convolutional neural networks
without performance degradation is key to effectively reduce human annotation
efforts. We propose Reliable Label Bootstrapping (ReLaB), an unsupervised
preprossessing algorithm which improves the performance of semi-supervised
algorithms in extremely low supervision settings. Given a dataset with few
labeled samples, we first learn meaningful self-supervised, latent features for
the data. Second, a label propagation algorithm propagates the known labels on
the unsupervised features, effectively labeling the full dataset in an
automatic fashion. Third, we select a subset of correctly labeled (reliable)
samples using a label noise detection algorithm. Finally, we train a
semi-supervised algorithm on the extended subset. We show that the selection of
the network architecture and the self-supervised algorithm are important
factors to achieve successful label propagation and demonstrate that ReLaB
substantially improves semi-supervised learning in scenarios of very limited
supervision on CIFAR-10, CIFAR-100 and mini-ImageNet. We reach average error
rates of $\boldsymbol{22.34}$ with 1 random labeled sample per class on
CIFAR-10 and lower this error to $\boldsymbol{8.46}$ when the labeled sample in
each class is highly representative. Our work is fully reproducible:
https://github.com/PaulAlbert31/ReLaB.
</p>
<a href="http://arxiv.org/abs/2007.11866" target="_blank">arXiv:2007.11866</a> [<a href="http://arxiv.org/pdf/2007.11866" target="_blank">pdf</a>]

<h2>Hearing What You Cannot See: Acoustic Vehicle Detection Around Corners. (arXiv:2007.15739v2 [cs.RO] UPDATED)</h2>
<h3>Yannick Schulz, Avinash Kini Mattar, Thomas M. Hehn, Julian F. P. Kooij</h3>
<p>This work proposes to use passive acoustic perception as an additional
sensing modality for intelligent vehicles. We demonstrate that approaching
vehicles behind blind corners can be detected by sound before such vehicles
enter in line-of-sight. We have equipped a research vehicle with a roof-mounted
microphone array, and show on data collected with this sensor setup that wall
reflections provide information on the presence and direction of occluded
approaching vehicles. A novel method is presented to classify if and from what
direction a vehicle is approaching before it is visible, using as input
Direction-of-Arrival features that can be efficiently computed from the
streaming microphone array data. Since the local geometry around the
ego-vehicle affects the perceived patterns, we systematically study several
environment types, and investigate generalization across these environments.
With a static ego-vehicle, an accuracy of 0.92 is achieved on the hidden
vehicle classification task. Compared to a state-of-the-art visual detector,
Faster R-CNN, our pipeline achieves the same accuracy more than one second
ahead, providing crucial reaction time for the situations we study. While the
ego-vehicle is driving, we demonstrate positive results on acoustic detection,
still achieving an accuracy of 0.84 within one environment type. We further
study failure cases across environments to identify future research directions.
</p>
<a href="http://arxiv.org/abs/2007.15739" target="_blank">arXiv:2007.15739</a> [<a href="http://arxiv.org/pdf/2007.15739" target="_blank">pdf</a>]

<h2>Teacher-Student Training and Triplet Loss for Facial Expression Recognition under Occlusion. (arXiv:2008.01003v2 [cs.CV] UPDATED)</h2>
<h3>Mariana-Iuliana Georgescu, Radu Tudor Ionescu</h3>
<p>In this paper, we study the task of facial expression recognition under
strong occlusion. We are particularly interested in cases where 50% of the face
is occluded, e.g. when the subject wears a Virtual Reality (VR) headset. While
previous studies show that pre-training convolutional neural networks (CNNs) on
fully-visible (non-occluded) faces improves the accuracy, we propose to employ
knowledge distillation to achieve further improvements. First of all, we employ
the classic teacher-student training strategy, in which the teacher is a CNN
trained on fully-visible faces and the student is a CNN trained on occluded
faces. Second of all, we propose a new approach for knowledge distillation
based on triplet loss. During training, the goal is to reduce the distance
between an anchor embedding, produced by a student CNN that takes occluded
faces as input, and a positive embedding (from the same class as the anchor),
produced by a teacher CNN trained on fully-visible faces, so that it becomes
smaller than the distance between the anchor and a negative embedding (from a
different class than the anchor), produced by the student CNN. Third of all, we
propose to combine the distilled embeddings obtained through the classic
teacher-student strategy and our novel teacher-student strategy based on
triplet loss into a single embedding vector. We conduct experiments on two
benchmarks, FER+ and AffectNet, with two CNN architectures, VGG-f and VGG-face,
showing that knowledge distillation can bring significant improvements over the
state-of-the-art methods designed for occluded faces in the VR setting.
</p>
<a href="http://arxiv.org/abs/2008.01003" target="_blank">arXiv:2008.01003</a> [<a href="http://arxiv.org/pdf/2008.01003" target="_blank">pdf</a>]

<h2>Can I Pour into It? Robot Imagining Open Containability Affordance of Previously Unseen Objects via Physical Simulations. (arXiv:2008.02321v2 [cs.RO] UPDATED)</h2>
<h3>Hongtao Wu, Gregory S. Chirikjian</h3>
<p>Open containers, i.e., containers without covers, are an important and
ubiquitous class of objects in human life. In this letter, we propose a novel
method for robots to "imagine" the open containability affordance of a
previously unseen object via physical simulations. We implement our imagination
method on a UR5 manipulator. The robot autonomously scans the object with an
RGB-D camera. The scanned 3D model is used for open containability imagination
which quantifies the open containability affordance by physically simulating
dropping particles onto the object and counting how many particles are retained
in it. This quantification is used for open-container vs. non-open-container
binary classification (hereafter referred to as open container classification).
If the object is classified as an open container, the robot further imagines
pouring into the object, again using physical simulations, to obtain the
pouring position and orientation for real robot autonomous pouring. We evaluate
our method on open container classification and autonomous pouring of granular
material on a dataset containing 130 previously unseen objects with 57 object
categories. Although our proposed method uses only 11 objects for simulation
calibration (training), its open container classification aligns well with
human judgements. In addition, our method endows the robot with the capability
to autonomously pour into the 55 containers in the dataset with a very high
success rate. We also compare to a deep learning method. Results show that our
method achieves the same performance as the deep learning method on open
container classification and outperforms it on autonomous pouring. Moreover,
our method is fully explainable.
</p>
<a href="http://arxiv.org/abs/2008.02321" target="_blank">arXiv:2008.02321</a> [<a href="http://arxiv.org/pdf/2008.02321" target="_blank">pdf</a>]

<h2>Noisy Student Training using Body Language Dataset Improves Facial Expression Recognition. (arXiv:2008.02655v2 [cs.CV] UPDATED)</h2>
<h3>Vikas Kumar, Shivansh Rao, Li Yu</h3>
<p>Facial expression recognition from videos in the wild is a challenging task
due to the lack of abundant labelled training data. Large DNN (deep neural
network) architectures and ensemble methods have resulted in better
performance, but soon reach saturation at some point due to data inadequacy. In
this paper, we use a self-training method that utilizes a combination of a
labelled dataset and an unlabelled dataset (Body Language Dataset - BoLD).
Experimental analysis shows that training a noisy student network iteratively
helps in achieving significantly better results. Additionally, our model
isolates different regions of the face and processes them independently using a
multi-level attention mechanism which further boosts the performance. Our
results show that the proposed method achieves state-of-the-art performance on
benchmark datasets CK+ and AFEW 8.0 when compared to other single models.
</p>
<a href="http://arxiv.org/abs/2008.02655" target="_blank">arXiv:2008.02655</a> [<a href="http://arxiv.org/pdf/2008.02655" target="_blank">pdf</a>]

<h2>LPMNet: Latent Part Modification and Generation for 3D Point Clouds. (arXiv:2008.03560v3 [cs.CV] UPDATED)</h2>
<h3>Cihan &#xd6;ng&#xfc;n, Alptekin Temizel</h3>
<p>In this paper, we focus on latent modification and generation of 3D point
cloud object models with respect to their semantic parts. Different to the
existing methods which use separate networks for part generation and assembly,
we propose a single end-to-end Autoencoder model that can handle generation and
modification of both semantic parts, and global shapes. The proposed method
supports part exchange between 3D point cloud models and composition by
different parts to form new models by directly editing latent representations.
This holistic approach does not need part-based training to learn part
representations and does not introduce any extra loss besides the standard
reconstruction loss. The experiments demonstrate the robustness of the proposed
method with different object categories and varying number of points. The
method can generate new models by integration of generative models such as GANs
and VAEs and can work with unannotated point clouds by integration of a
segmentation module.
</p>
<a href="http://arxiv.org/abs/2008.03560" target="_blank">arXiv:2008.03560</a> [<a href="http://arxiv.org/pdf/2008.03560" target="_blank">pdf</a>]

<h2>Dynamic Object Removal and Spatio-Temporal RGB-D Inpainting via Geometry-Aware Adversarial Learning. (arXiv:2008.05058v3 [cs.CV] UPDATED)</h2>
<h3>Borna Be&#x161;i&#x107;, Abhinav Valada</h3>
<p>Dynamic objects have a significant impact on the robot's perception of the
environment which degrades the performance of essential tasks such as
localization and mapping. In this work, we address this problem by synthesizing
plausible color, texture and geometry in regions occluded by dynamic objects.
We propose the novel geometry-aware DynaFill architecture that follows a
coarse-to-fine topology and incorporates our gated recurrent feedback mechanism
to adaptively fuse information from previous timesteps. We optimize our
architecture using adversarial training to synthesize fine realistic textures
which enables it to hallucinate color and depth structure in occluded regions
online in a spatially and temporally coherent manner, without relying on future
frame information. Casting our inpainting problem as an image-to-image
translation task, our model also corrects regions correlated with the presence
of dynamic objects in the scene, such as shadows or reflections. We introduce a
large-scale hyperrealistic dataset with RGB-D images, semantic segmentation
labels, camera poses as well as groundtruth RGB-D information of occluded
regions. Extensive quantitative and qualitative evaluations show that our
approach achieves state-of-the-art performance, even in challenging weather
conditions. Furthermore, we present results for retrieval-based visual
localization with the synthesized images that demonstrate the utility of our
approach.
</p>
<a href="http://arxiv.org/abs/2008.05058" target="_blank">arXiv:2008.05058</a> [<a href="http://arxiv.org/pdf/2008.05058" target="_blank">pdf</a>]

<h2>Align Deep Features for Oriented Object Detection. (arXiv:2008.09397v2 [cs.CV] UPDATED)</h2>
<h3>Jiaming Han, Jian Ding, Jie Li, Gui-Song Xia</h3>
<p>The past decade has witnessed significant progress on detecting objects in
aerial images that are often distributed with large scale variations and
arbitrary orientations. However most of existing methods rely on heuristically
defined anchors with different scales, angles and aspect ratios and usually
suffer from severe misalignment between anchor boxes and axis-aligned
convolutional features, which leads to the common inconsistency between the
classification score and localization accuracy. To address this issue, we
propose a Single-shot Alignment Network (S$^2$A-Net) consisting of two modules:
a Feature Alignment Module (FAM) and an Oriented Detection Module (ODM). The
FAM can generate high-quality anchors with an Anchor Refinement Network and
adaptively align the convolutional features according to the anchor boxes with
a novel Alignment Convolution. The ODM first adopts active rotating filters to
encode the orientation information and then produces orientation-sensitive and
orientation-invariant features to alleviate the inconsistency between
classification score and localization accuracy. Besides, we further explore the
approach to detect objects in large-size images, which leads to a better
trade-off between speed and accuracy. Extensive experiments demonstrate that
our method can achieve state-of-the-art performance on two commonly used aerial
objects datasets (i.e., DOTA and HRSC2016) while keeping high efficiency. The
code is available at https://github.com/csuhan/s2anet.
</p>
<a href="http://arxiv.org/abs/2008.09397" target="_blank">arXiv:2008.09397</a> [<a href="http://arxiv.org/pdf/2008.09397" target="_blank">pdf</a>]

<h2>Planning in Learned Latent Action Spaces for Generalizable Legged Locomotion. (arXiv:2008.11867v4 [cs.RO] UPDATED)</h2>
<h3>Tianyu Li, Roberto Calandra, Deepak Pathak, Yuandong Tian, Franziska Meier, Akshara Rai</h3>
<p>Hierarchical learning has been successful at learning generalizable
locomotion skills on walking robots in a sample-efficient manner. However, the
low-dimensional "latent" action used to communicate between two layers of the
hierarchy is typically user-designed. In this work, we present a fully-learned
hierarchical framework, that is capable of jointly learning the low-level
controller and the high-level latent action space. Once this latent space is
learned, we plan over continuous latent actions in a model-predictive control
fashion, using a learned high-level dynamics model. This framework generalizes
to multiple robots, and we present results on a Daisy hexapod simulation, A1
quadruped simulation, and Daisy robot hardware. We compare a range of learned
hierarchical approaches from literature, and show that our framework
outperforms baselines on multiple tasks and two simulations. In addition to
learning approaches, we also compare to inverse-kinematics (IK) acting on
desired robot motion, and show that our fully-learned framework outperforms IK
in adverse settings on both A1 and Daisy simulations. On hardware, we show the
Daisy hexapod achieve multiple locomotion tasks, in an unstructured outdoor
setting, with only 2000 hardware samples, reinforcing the robustness and
sample-efficiency of our approach.
</p>
<a href="http://arxiv.org/abs/2008.11867" target="_blank">arXiv:2008.11867</a> [<a href="http://arxiv.org/pdf/2008.11867" target="_blank">pdf</a>]

<h2>Group-Skeleton-Based Human Action Recognition in Complex Events. (arXiv:2011.13273v2 [cs.CV] UPDATED)</h2>
<h3>Tingtian Li, Zixun Sun, Xiao Chen</h3>
<p>Human action recognition as an important application of computer vision has
been studied for decades. Among various approaches, skeleton-based methods
recently attract increasing attention due to their robust and superior
performance. However, existing skeleton-based methods ignore the potential
action relationships between different persons, while the action of a person is
highly likely to be impacted by another person especially in complex events. In
this paper, we propose a novel group-skeleton-based human action recognition
method in complex events. This method first utilizes multi-scale
spatial-temporal graph convolutional networks (MS-G3Ds) to extract skeleton
features from multiple persons. In addition to the traditional key point
coordinates, we also input the key point speed values to the networks for
better performance. Then we use multilayer perceptrons (MLPs) to embed the
distance values between the reference person and other persons into the
extracted features. Lastly, all the features are fed into another MS-G3D for
feature fusion and classification. For avoiding class imbalance problems, the
networks are trained with a focal loss. The proposed algorithm is also our
solution for the Large-scale Human-centric Video Analysis in Complex Events
Challenge. Results on the HiEve dataset show that our method can give superior
performance compared to other state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2011.13273" target="_blank">arXiv:2011.13273</a> [<a href="http://arxiv.org/pdf/2011.13273" target="_blank">pdf</a>]

<h2>The Geometry and Kinematics of the Matrix Lie Group $SE_K(3)$. (arXiv:2012.00950v3 [cs.RO] UPDATED)</h2>
<h3>Yarong Luo, Mengyuan Wang, Chi Guo</h3>
<p>Currently state estimation is very important for the robotics, and the
uncertainty representation based Lie group is natural for the state estimation
problem. It is necessary to exploit the geometry and kinematic of matrix Lie
group sufficiently. Therefore, this note gives a detailed derivation of the
recently proposed matrix Lie group $SE_K(3)$ for the first time, our results
extend the results in Barfoot \cite{barfoot2017state}. Then we describe the
situations where this group is suitable for state representation. We also have
developed code based on Matlab framework for quickly implementing and testing.
</p>
<a href="http://arxiv.org/abs/2012.00950" target="_blank">arXiv:2012.00950</a> [<a href="http://arxiv.org/pdf/2012.00950" target="_blank">pdf</a>]

<h2>Generative Max-Mahalanobis Classifiers for Image Classification, Generation and More. (arXiv:2101.00122v2 [cs.CV] UPDATED)</h2>
<h3>Xiulong Yang, Hui Ye, Yang Ye, Xiang Li, Shihao Ji</h3>
<p>Joint Energy-based Model (JEM) of Grathwohl et al. shows that a standard
softmax classifier can be reinterpreted as an energy-based model (EBM) for the
joint distribution p(x,y); the resulting model can be optimized to improve
calibration, robustness, and out-of-distribution detection, while generating
samples rivaling the quality of recent GAN-based approaches. However, the
softmax classifier that JEM exploits is inherently discriminative and its
latent feature space is not well formulated as probabilistic distributions,
which may hinder its potential for image generation and incur training
instability. We hypothesize that generative classifiers, such as Linear
Discriminant Analysis (LDA), might be more suitable for image generation since
generative classifiers model the data generation process explicitly. This paper
therefore investigates an LDA classifier for image classification and
generation. In particular, the Max-Mahalanobis Classifier (MMC), a special case
of LDA, fits our goal very well. We show that our Generative MMC (GMMC) can be
trained discriminatively, generatively, or jointly for image classification and
generation. Extensive experiments on multiple datasets show that GMMC achieves
state-of-the-art discriminative and generative performances, while
outperforming JEM in calibration, adversarial robustness, and
out-of-distribution detection by a significant margin. Our source code is
available at https://github.com/sndnyang/GMMC.
</p>
<a href="http://arxiv.org/abs/2101.00122" target="_blank">arXiv:2101.00122</a> [<a href="http://arxiv.org/pdf/2101.00122" target="_blank">pdf</a>]

<h2>BSUV-Net 2.0: Spatio-Temporal Data Augmentations for Video-Agnostic Supervised Background Subtraction. (arXiv:2101.09585v2 [cs.CV] UPDATED)</h2>
<h3>M. Ozan Tezcan, Prakash Ishwar, Janusz Konrad</h3>
<p>Background subtraction (BGS) is a fundamental video processing task which is
a key component of many applications. Deep learning-based supervised algorithms
achieve very good perforamnce in BGS, however, most of these algorithms are
optimized for either a specific video or a group of videos, and their
performance decreases dramatically when applied to unseen videos. Recently,
several papers addressed this problem and proposed video-agnostic supervised
BGS algorithms. However, nearly all of the data augmentations used in these
algorithms are limited to the spatial domain and do not account for temporal
variations that naturally occur in video data. In this work, we introduce
spatio-temporal data augmentations and apply them to one of the leading
video-agnostic BGS algorithms, BSUV-Net. We also introduce a new
cross-validation training and evaluation strategy for the CDNet-2014 dataset
that makes it possible to fairly and easily compare the performance of various
video-agnostic supervised BGS algorithms. Our new model trained using the
proposed data augmentations, named BSUV-Net 2.0, significantly outperforms
state-of-the-art algorithms evaluated on unseen videos of CDNet-2014. We also
evaluate the cross-dataset generalization capacity of BSUV-Net 2.0 by training
it solely on CDNet-2014 videos and evaluating its performance on LASIESTA
dataset. Overall, BSUV-Net 2.0 provides a ~5% improvement in the F-score over
state-of-the-art methods on unseen videos of CDNet-2014 and LASIESTA datasets.
Furthermore, we develop a real-time variant of our model, that we call Fast
BSUV-Net 2.0, whose performance is close to the state of the art.
</p>
<a href="http://arxiv.org/abs/2101.09585" target="_blank">arXiv:2101.09585</a> [<a href="http://arxiv.org/pdf/2101.09585" target="_blank">pdf</a>]

<h2>Arbitrary-Oriented Ship Detection through Center-Head Point Extraction. (arXiv:2101.11189v2 [cs.CV] UPDATED)</h2>
<h3>Feng Zhang, Xueying Wang, Shilin Zhou, Yingqian Wang, Yi Hou</h3>
<p>Ship detection in remote sensing images plays a crucial role in various
applications and has drawn increasing attention in recent years. However,
existing multi-oriented ship detection methods are generally developed on a set
of predefined rotated anchor boxes. These predefined boxes not only lead to
inaccurate angle predictions but also introduce extra hyper-parameters and high
computational cost. Moreover, the prior knowledge of ship size has not been
fully exploited by existing methods, which hinders the improvement of their
detection accuracy. Aiming at solving the above issues, in this paper, we
propose a \emph{center-head point extraction based detector} (named CHPDet) to
achieve arbitrary-oriented ship detection in remote sensing images. Our CHPDet
formulates arbitrary-oriented ships as rotated boxes with head points which are
used to determine the direction. The orientation-invariant model (OIM) is used
to produce orientation-invariant feature maps. Keypoint estimation is performed
to find the center of ships. Then, the size and head point of the ships are
regressed. Finally, we use the target size as prior to finetune the results.
Moreover, we introduce a new dataset for multi-class arbitrary-oriented ship
detection in remote sensing images at a fixed ground sample distance (GSD)
which is named FGSD2021. Experimental results on two ship detection datasets
(i.e., FGSD2021 and HRSC2016) demonstrate that our CHPDet achieves
state-of-the-art performance and can well distinguish between bow and stern.
The code and dataset will be made publicly available.
</p>
<a href="http://arxiv.org/abs/2101.11189" target="_blank">arXiv:2101.11189</a> [<a href="http://arxiv.org/pdf/2101.11189" target="_blank">pdf</a>]

<h2>DRIV100: In-The-Wild Multi-Domain Dataset and Evaluation for Real-World Domain Adaptation of Semantic Segmentation. (arXiv:2102.00150v2 [cs.CV] UPDATED)</h2>
<h3>Haruya Sakashita, Christoph Flothow, Noriko Takemura, Yusuke Sugano</h3>
<p>Together with the recent advances in semantic segmentation, many domain
adaptation methods have been proposed to overcome the domain gap between
training and deployment environments. However, most previous studies use
limited combinations of source/target datasets, and domain adaptation
techniques have never been thoroughly evaluated in a more challenging and
diverse set of target domains. This work presents a new multi-domain dataset
DRIV100 for benchmarking domain adaptation techniques on in-the-wild road-scene
videos collected from the Internet. The dataset consists of pixel-level
annotations for 100 videos selected to cover diverse scenes/domains based on
two criteria; human subjective judgment and an anomaly score judged using an
existing road-scene dataset. We provide multiple manually labeled ground-truth
frames for each video, enabling a thorough evaluation of video-level domain
adaptation where each video independently serves as the target domain. Using
the dataset, we quantify domain adaptation performances of state-of-the-art
methods and clarify the potential and novel challenges of domain adaptation
techniques. The dataset is available at https://doi.org/10.5281/zenodo.4389243.
</p>
<a href="http://arxiv.org/abs/2102.00150" target="_blank">arXiv:2102.00150</a> [<a href="http://arxiv.org/pdf/2102.00150" target="_blank">pdf</a>]

<h2>Bridging Unpaired Facial Photos And Sketches By Line-drawings. (arXiv:2102.00635v3 [cs.CV] UPDATED)</h2>
<h3>Meimei Shang, Fei Gao, Xiang Li, Jingjie Zhu, Lingna Dai</h3>
<p>In this paper, we propose a novel method to learn face sketch synthesis
models by using unpaired data. Our main idea is bridging the photo domain
$\mathcal{X}$ and the sketch domain $Y$ by using the line-drawing domain
$\mathcal{Z}$. Specially, we map both photos and sketches to line-drawings by
using a neural style transfer method, i.e. $F: \mathcal{X}/\mathcal{Y} \mapsto
\mathcal{Z}$. Consequently, we obtain \textit{pseudo paired data}
$(\mathcal{Z}, \mathcal{Y})$, and can learn the mapping $G:\mathcal{Z} \mapsto
\mathcal{Y}$ in a supervised learning manner. In the inference stage, given a
facial photo, we can first transfer it to a line-drawing and then to a sketch
by $G \circ F$. Additionally, we propose a novel stroke loss for generating
different types of strokes. Our method, termed sRender, accords well with human
artists' rendering process. Experimental results demonstrate that sRender can
generate multi-style sketches, and significantly outperforms existing unpaired
image-to-image translation methods.
</p>
<a href="http://arxiv.org/abs/2102.00635" target="_blank">arXiv:2102.00635</a> [<a href="http://arxiv.org/pdf/2102.00635" target="_blank">pdf</a>]

<h2>Minimizing false negative rate in melanoma detection and providing insight into the causes of classification. (arXiv:2102.09199v2 [cs.CV] UPDATED)</h2>
<h3>Ell&#xe1;k Somfai, Benj&#xe1;min Baffy, Kristian Fenech, Changlu Guo, Rita Hossz&#xfa;, Dorina Kor&#xf3;zs, Fabrizio Nunnari, Marcell P&#xf3;lik, Daniel Sonntag, Attila Ulbert, Andr&#xe1;s L&#x151;rincz</h3>
<p>Our goal is to bridge human and machine intelligence in melanoma detection.
We develop a classification system exploiting a combination of visual
pre-processing, deep learning, and ensembling for providing explanations to
experts and to minimize false negative rate while maintaining high accuracy in
melanoma detection. Source images are first automatically segmented using a
U-net CNN. The result of the segmentation is then used to extract image
sub-areas and specific parameters relevant in human evaluation, namely center,
border, and asymmetry measures. These data are then processed by tailored
neural networks which include structure searching algorithms. Partial results
are then ensembled by a committee machine. Our evaluation on the largest skin
lesion dataset which is publicly available today, ISIC-2019, shows improvement
in all evaluated metrics over a baseline using the original images only. We
also showed that indicative scores computed by the feature classifiers can
provide useful insight into the various features on which the decision can be
based.
</p>
<a href="http://arxiv.org/abs/2102.09199" target="_blank">arXiv:2102.09199</a> [<a href="http://arxiv.org/pdf/2102.09199" target="_blank">pdf</a>]

<h2>Adversarial Shape Learning for Building Extraction in VHR Remote Sensing Images. (arXiv:2102.11262v2 [cs.CV] UPDATED)</h2>
<h3>Lei Ding, Hao Tang, Yahui Liu, Yilei Shi, Lorenzo Bruzzone</h3>
<p>Building extraction in VHR RSIs remains to be a challenging task due to
occlusion and boundary ambiguity problems. Although conventional convolutional
neural networks (CNNs) based methods are capable of exploiting local texture
and context information, they fail to capture the shape patterns of buildings,
which is a necessary constraint in the human recognition. In this context, we
propose an adversarial shape learning network (ASLNet) to model the building
shape patterns, thus improving the accuracy of building segmentation. In the
proposed ASLNet, we introduce the adversarial learning strategy to explicitly
model the shape constraints, as well as a CNN shape regularizer to strengthen
the embedding of shape features. To assess the geometric accuracy of building
segmentation results, we further introduced several object-based assessment
metrics. Experiments on two open benchmark datasets show that the proposed
ASLNet improves both the pixel-based accuracy and the object-based measurements
by a large margin. The code is available at: https://github.com/ggsDing/ASLNet
</p>
<a href="http://arxiv.org/abs/2102.11262" target="_blank">arXiv:2102.11262</a> [<a href="http://arxiv.org/pdf/2102.11262" target="_blank">pdf</a>]

<h2>ROAD: The ROad event Awareness Dataset for Autonomous Driving. (arXiv:2102.11585v2 [cs.CV] UPDATED)</h2>
<h3>Gurkirt Singh, Stephen Akrigg, Manuele Di Maio, Valentina Fontana, Reza Javanmard Alitappeh, Suman Saha, Kossar Jeddisaravi, Farzad Yousefi, Jacob Culley, Tom Nicholson, Jordan Omokeowa, Salman Khan, Stanislao Grazioso, Andrew Bradley, Giuseppe Di Gironimo, Fabio Cuzzolin</h3>
<p>Humans approach driving in a holistic fashion which entails, in particular,
understanding road events and their evolution. Injecting these capabilities in
an autonomous vehicle has thus the potential to take situational awareness and
decision making closer to human-level performance. To this purpose, we
introduce the ROad event Awareness Dataset (ROAD) for Autonomous Driving, to
our knowledge the first of its kind. ROAD is designed to test an autonomous
vehicle's ability to detect road events, defined as triplets composed by a
moving agent, the action(s) it performs and the corresponding scene locations.
ROAD comprises 22 videos, originally from the Oxford RobotCar Dataset,
annotated with bounding boxes showing the location in the image plane of each
road event. We also provide as baseline a new incremental algorithm for online
road event awareness, based on inflating RetinaNet along time, which achieves a
mean average precision of 16.8% and 6.1% for frame-level and video-level event
detection, respectively, at 50% overlap. Though promising, these figures
highlight the challenges faced by situation awareness in autonomous driving.
Finally, ROAD allows scholars to investigate exciting tasks such as complex
(road) activity detection, future road event anticipation and the modelling of
sentient road agents in terms of mental states. Dataset can be obtained from
https://github.com/gurkirt/road-dataset and baseline code from
https://github.com/gurkirt/3D-RetinaNet.
</p>
<a href="http://arxiv.org/abs/2102.11585" target="_blank">arXiv:2102.11585</a> [<a href="http://arxiv.org/pdf/2102.11585" target="_blank">pdf</a>]

<h2>Localization Distillation for Object Detection. (arXiv:2102.12252v2 [cs.CV] UPDATED)</h2>
<h3>Zhaohui Zheng, Rongguang Ye, Ping Wang, Jun Wang, Dongwei Ren, Wangmeng Zuo</h3>
<p>Knowledge distillation (KD) has witnessed its powerful ability in learning
compact models in deep learning field, but it is still limited in distilling
localization information for object detection. Existing KD methods for object
detection mainly focus on mimicking deep features between teacher model and
student model, which not only is restricted by specific model architectures,
but also cannot distill localization ambiguity. In this paper, we first propose
localization distillation (LD) for object detection. In particular, our LD can
be formulated as standard KD by adopting the general localization
representation of bounding box. Our LD is very flexible, and is applicable to
distill localization ambiguity for arbitrary architecture of teacher model and
student model. Moreover, it is interesting to find that Self-LD, i.e.,
distilling teacher model itself, can further boost state-of-the-art
performance. Second, we suggest a teacher assistant (TA) strategy to fill the
possible gap between teacher model and student model, by which the distillation
effectiveness can be guaranteed even the selected teacher model is not optimal.
On benchmark datasets PASCAL VOC and MS COCO, our LD can consistently improve
the performance for student detectors, and also boosts state-of-the-art
detectors notably. Our source code and trained models are publicly available at
https://github.com/HikariTJU/LD
</p>
<a href="http://arxiv.org/abs/2102.12252" target="_blank">arXiv:2102.12252</a> [<a href="http://arxiv.org/pdf/2102.12252" target="_blank">pdf</a>]

<h2>Compositional Hierarchical Tensor Factorization: Representing Hierarchical Intrinsic and Extrinsic Causal Factors. (arXiv:1911.04180v2 [cs.CV] CROSS LISTED)</h2>
<h3>M. Alex O. Vasilescu, Eric Kim</h3>
<p>Visual objects are composed of a recursive hierarchy of perceptual wholes and
parts, whose properties, such as shape, reflectance, and color, constitute a
hierarchy of intrinsic causal factors of object appearance. However, object
appearance is the compositional consequence of both an object's intrinsic and
extrinsic causal factors, where the extrinsic causal factors are related to
illumination, and imaging conditions. Therefore, this paper proposes a unified
tensor model of wholes and parts, and introduces a compositional hierarchical
tensor factorization that disentangles the hierarchical causal structure of
object image formation, and subsumes multilinear block tensor decomposition as
a special case. The resulting object representation is an interpretable
combinatorial choice of wholes' and parts' representations that renders object
recognition robust to occlusion and reduces training data requirements. We
demonstrate ourapproach in the context of face recognition by training on an
extremely reduced dataset of synthetic images, and report encouragingface
verification results on two datasets - the Freiburg dataset, andthe Labeled
Face in the Wild (LFW) dataset consisting of real world images, thus,
substantiating the suitability of our approach for data starved domains.
</p>
<a href="http://arxiv.org/abs/1911.04180" target="_blank">arXiv:1911.04180</a> [<a href="http://arxiv.org/pdf/1911.04180" target="_blank">pdf</a>]

