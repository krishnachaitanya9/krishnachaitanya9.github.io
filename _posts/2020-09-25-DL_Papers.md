---
title: Latest Deep Learning Papers
date: 2020-09-29 21:00:01 +0800
featured-img: DL-logo
categories: [Deep_Learning]
tags: [Deep_Learning]
mathjax: Yes
---

<h1>Your arXiv Feed</h1>
<h2>Recursive CSI Quantization of Time-Correlated MIMO Channels by Deep Learning Classification. (arXiv:2009.13560v1 [cs.IT])</h2>
<h3>Stefan Schwarz</h3>
<p>In frequency division duplex (FDD) multiple-input multiple-output (MIMO)
wireless communications, limited channel state information (CSI) feedback is a
central tool to support advanced single- and multi-user MIMO
beamforming/precoding. To achieve a given CSI quality, the CSI quantization
codebook size has to grow exponentially with the number of antennas, leading to
quantization complexity, as well as, feedback overhead issues for larger MIMO
systems. We have recently proposed a multi-stage recursive Grassmannian
quantizer that enables a significant complexity reduction of CSI quantization.
In this paper, we show that this recursive quantizer can effectively be
combined with deep learning classification to further reduce the complexity,
and that it can exploit temporal channel correlations to reduce the CSI
feedback overhead.
</p>
<a href="http://arxiv.org/abs/2009.13560">arXiv:2009.13560</a> [<a href="http://arxiv.org/pdf/2009.13560">pdf</a>]

<h2>Computing robust control invariant sets of constrained nonlinear systems: A graph algorithm approach. (arXiv:2009.13581v1 [eess.SY])</h2>
<h3>Benjamin Decardi-Nelson, Jinfeng Liu</h3>
<p>This paper deals with the computation of the largest robust control invariant
sets (RCISs) of constrained nonlinear systems. The proposed approach is based
on casting the search for the invariant set as a graph theoretical problem.
Specifically, a general class of discrete-time time-invariant nonlinear systems
is considered. First, the dynamics of a nonlinear system is approximated with a
directed graph. Subsequently, the condition for robust control invariance is
derived and an algorithm for computing the robust control invariant set is
presented. The algorithm combines the iterative subdivision technique with the
robust control invariance condition to produce outer approximations of the
largest robust control invariant set at each iteration. Following this, we
prove convergence of the algorithm to the largest RCIS as the iterations
proceed to infinity. Based on the developed algorithms, an algorithm to compute
inner approximations of the RCIS is also presented. A special case of input
affine and disturbance affine systems is also considered. Finally, two
numerical examples are presented to demonstrate the efficacy of the proposed
method.
</p>
<a href="http://arxiv.org/abs/2009.13581">arXiv:2009.13581</a> [<a href="http://arxiv.org/pdf/2009.13581">pdf</a>]

<h2>On Robustness of the Normalized Subgradient Method with Randomly Corrupted Subgradients. (arXiv:2009.13725v1 [math.OC])</h2>
<h3>Berkay Turan, Cesar A. Uribe, Hoi-To Wai, Mahnoosh Alizadeh</h3>
<p>Numerous modern optimization and machine learning algorithms rely on
subgradient information being trustworthy and hence, they may fail to converge
when such information is corrupted. In this paper, we consider the setting
where subgradient information may be arbitrarily corrupted (with a given
probability) and study the robustness properties of the normalized subgradient
method. Under the probabilistic corruption scenario, we prove that the
normalized subgradient method, whose updates rely solely on directional
information of the subgradient, converges to a minimizer for convex, strongly
convex, and weakly-pseudo convex functions satisfying certain conditions.
Numerical evidence on linear regression and logistic classification problems
support our results.
</p>
<a href="http://arxiv.org/abs/2009.13725">arXiv:2009.13725</a> [<a href="http://arxiv.org/pdf/2009.13725">pdf</a>]

<h2>Distributed learning for optimal allocation in radial power systems. (arXiv:2009.13857v1 [math.OC])</h2>
<h3>Taouba Jouini, Zhiyong Sun</h3>
<p>We revisit the classical log-linear learning algorithm for optimal allocation
of DC/AC converters and synchronous machines in radial (no loops) power
systems. The objective is to assign to each generator node a type; either a
synchronous machine or a DC/AC converter in closed-loop with droop control,
while minimizing the steady state angle deviation relative to an optimum
associated with unknown optimal configuration of synchronous machines and DC/AC
converters. Additionally, we study the robustness of the learning algorithm
against uniform drop in the line susceptances and with respect to well-defined
feasibility region describing admissible power deviations. We show guaranteed
probabilistic convergence to maximizers of the perturbed potential function
with feasible power flows and demonstrate our theoretical findings via
simulative example of power network with six generation units.
</p>
<a href="http://arxiv.org/abs/2009.13857">arXiv:2009.13857</a> [<a href="http://arxiv.org/pdf/2009.13857">pdf</a>]

<h2>Geometric Disentanglement by Random Convex Polytopes. (arXiv:2009.13987v1 [cs.LG])</h2>
<h3>Michael Joswig, Marek Kaluba, Lukas Ruff</h3>
<p>Finding and analyzing meaningful representations of data is the purpose of
machine learning. The idea of representation learning is to extract
representations from the data itself, e.g., by utilizing deep neural networks.
In this work, we examine representation learning from a geometric perspective.
Especially, we focus on the convexity of classes and clusters as a natural and
desirable representation property, for which robust and scalable measures are
still lacking. To address this, we propose a new approach called Random
Polytope Descriptor that allows a convex description of data points based on
the construction of random convex polytopes. This ties in with current methods
for statistical disentanglement. We demonstrate the use of our technique on
well-known deep learning methods for representation learning. Specifically we
find that popular regularization variants such as the Variational Autoencoder
can destroy crucial information that is relevant for tasks such as
out-of-distribution detection.
</p>
<a href="http://arxiv.org/abs/2009.13987">arXiv:2009.13987</a> [<a href="http://arxiv.org/pdf/2009.13987">pdf</a>]

<h2>Deep Learning-based Phase Reconfiguration for Intelligent Reflecting Surfaces. (arXiv:2009.13988v1 [eess.SP])</h2>
<h3>&#xd6;zgecan &#xd6;zdogan, Emil Bj&#xf6;rnson</h3>
<p>Intelligent reflecting surfaces (IRSs), consisting of reconfigurable
metamaterials, have recently attracted attention as a promising cost-effective
technology that can bring new features to wireless communications. These
surfaces can be used to partially control the propagation environment and can
potentially provide a power gain that is proportional to the square of the
number of IRS elements when configured in a proper way. However, the
configuration of the local phase matrix at the IRSs can be quite a challenging
task since they are purposely designed to not have any active components,
therefore, they are not able to process any pilot signal. In addition, a large
number of elements at the IRS may create a huge training overhead. In this
paper, we present a deep learning (DL) approach for phase reconfiguration at an
IRS in order to learn and make use of the local propagation environment. The
proposed method uses the received pilot signals reflected through the IRS to
train the deep feedforward network. The performance of the proposed approach is
evaluated and the numerical results are presented.
</p>
<a href="http://arxiv.org/abs/2009.13988">arXiv:2009.13988</a> [<a href="http://arxiv.org/pdf/2009.13988">pdf</a>]

<h2>Elementary Properties of Positive Concave Mappings with Applications to Network Planning and Optimization. (arXiv:1505.03006v7 [cs.IT] UPDATED)</h2>
<h3>Renato L. G. Cavalcante, Yuxiang Shen, Slawomir Sta&#x144;czak</h3>
<p>This study presents novel methods for computing fixed points of positive
concave mappings and for characterizing the existence of fixed points. These
methods are particularly important in planning and optimization tasks in
wireless networks. For example, previous studies have shown that the
feasibility of a network design can be quickly evaluated by computing the fixed
point of a concave mapping that is constructed based on many environmental and
network control parameters such as the position of base stations, channel
conditions, and antenna tilts. To address this and more general problems, given
a positive concave mapping, we show two alternative but equivalent ways to
construct a matrix that is guaranteed to have spectral radius strictly smaller
than one if the mapping has a fixed point. This matrix is then used to build a
new mapping that preserves the fixed point of the original positive concave
mapping. We show that the standard fixed point iterations using the new mapping
converges faster than the standard iterations applied to the original concave
mapping. As exemplary applications of the proposed methods, we consider the
problems of power and load planning in networks based on the orthogonal
frequency division multiple access (OFDMA) technology.
</p>
<a href="http://arxiv.org/abs/1505.03006">arXiv:1505.03006</a> [<a href="http://arxiv.org/pdf/1505.03006">pdf</a>]

<h2>Quasi-Newton Methods for Deep Learning: Forget the Past, Just Sample. (arXiv:1901.09997v4 [math.OC] UPDATED)</h2>
<h3>Albert S. Berahas, Majid Jahani, Peter Richt&#xe1;rik, Martin Tak&#xe1;&#x10d;</h3>
<p>We present two sampled quasi-Newton methods for deep learning: sampled LBFGS
(S-LBFGS) and sampled LSR1 (S-LSR1). Contrary to the classical variants of
these methods that sequentially build Hessian or inverse Hessian approximations
as the optimization progresses, our proposed methods sample points randomly
around the current iterate at every iteration to produce these approximations.
As a result, the approximations constructed make use of more reliable (recent
and local) information, and do not depend on past iterate information that
could be significantly stale. Our proposed algorithms are efficient in terms of
accessed data points (epochs) and have enough concurrency to take advantage of
parallel/distributed computing environments. We provide convergence guarantees
for our proposed methods. Numerical tests on a toy classification problem as
well as on popular benchmarking neural network training tasks reveal that the
methods outperform their classical variants.
</p>
<a href="http://arxiv.org/abs/1901.09997">arXiv:1901.09997</a> [<a href="http://arxiv.org/pdf/1901.09997">pdf</a>]

<h2>Monte Carlo Gradient Estimation in Machine Learning. (arXiv:1906.10652v2 [stat.ML] UPDATED)</h2>
<h3>Shakir Mohamed, Mihaela Rosca, Michael Figurnov, Andriy Mnih</h3>
<p>This paper is a broad and accessible survey of the methods we have at our
disposal for Monte Carlo gradient estimation in machine learning and across the
statistical sciences: the problem of computing the gradient of an expectation
of a function with respect to parameters defining the distribution that is
integrated; the problem of sensitivity analysis. In machine learning research,
this gradient problem lies at the core of many learning problems, in
supervised, unsupervised and reinforcement learning. We will generally seek to
rewrite such gradients in a form that allows for Monte Carlo estimation,
allowing them to be easily and efficiently used and analysed. We explore three
strategies--the pathwise, score function, and measure-valued gradient
estimators--exploring their historical development, derivation, and underlying
assumptions. We describe their use in other fields, show how they are related
and can be combined, and expand on their possible generalisations. Wherever
Monte Carlo gradient estimators have been derived and deployed in the past,
important advances have followed. A deeper and more widely-held understanding
of this problem will lead to further advances, and it is these advances that we
wish to support.
</p>
<a href="http://arxiv.org/abs/1906.10652">arXiv:1906.10652</a> [<a href="http://arxiv.org/pdf/1906.10652">pdf</a>]

<h2>SIRUS: Stable and Interpretable RUle Set. (arXiv:1908.06852v4 [stat.ML] UPDATED)</h2>
<h3>Cl&#xe9;ment B&#xe9;nard (LPSM (UMR\_8001)), G&#xe9;rard Biau (LPSM (UMR\_8001)), S&#xe9;bastien da Veiga, Erwan Scornet (CMAP)</h3>
<p>State-of-the-art learning algorithms, such as random forests or neural
networks, are often qualified as "black-boxes" because of the high number and
complexity of operations involved in their prediction mechanism. This lack of
interpretability is a strong limitation for applications involving critical
decisions, typically the analysis of production processes in the manufacturing
industry. In such critical contexts, models have to be interpretable, i.e.,
simple, stable, and predictive. To address this issue, we design SIRUS (Stable
and Interpretable RUle Set), a new classification algorithm based on random
forests, which takes the form of a short list of rules. While simple models are
usually unstable with respect to data perturbation, SIRUS achieves a remarkable
stability improvement over cutting-edge methods. Furthermore, SIRUS inherits a
predictive accuracy close to random forests, combined with the simplicity of
decision trees. These properties are assessed both from a theoretical and
empirical point of view, through extensive numerical experiments based on our
R/C++ software implementation sirus available from CRAN.
</p>
<a href="http://arxiv.org/abs/1908.06852">arXiv:1908.06852</a> [<a href="http://arxiv.org/pdf/1908.06852">pdf</a>]

<h2>On the rate of convergence of fully connected very deep neural network regression estimates. (arXiv:1908.11133v5 [stat.ML] UPDATED)</h2>
<h3>Michael Kohler, Sophie Langer</h3>
<p>Recent results in nonparametric regression show that deep learning, i.e.,
neural network estimates with many hidden layers, are able to circumvent the
so-called curse of dimensionality in case that suitable restrictions on the
structure of the regression function hold. One key feature of the neural
networks used in these results is that their network architecture has a further
constraint, namely the network sparsity. In this paper we show that we can get
similar results also for least squares estimates based on simple fully
connected neural networks with ReLU activation functions. Here either the
number of neurons per hidden layer is fixed and the number of hidden layers
tends to infinity suitably fast for sample size tending to infinity, or the
number of hidden layers is bounded by some logarithmic factor in the sample
size and the number of neurons per hidden layer tends to infinity suitably fast
for sample size tending to infinity. The proof is based on new approximation
results concerning deep neural networks.
</p>
<a href="http://arxiv.org/abs/1908.11133">arXiv:1908.11133</a> [<a href="http://arxiv.org/pdf/1908.11133">pdf</a>]

<h2>Borrowing From the Future: An Attempt to Address Double Sampling. (arXiv:1912.00304v2 [math.OC] UPDATED)</h2>
<h3>Yuhua Zhu, Lexing Ying</h3>
<p>For model-free reinforcement learning, one of the main difficulty of
stochastic Bellman residual minimization is the double sampling problem, i.e.,
while only one single sample for the next state is available in the model-free
setting, two independent samples for the next state are required in order to
perform unbiased stochastic gradient descent. We propose new algorithms for
addressing this problem based on the idea of borrowing extra randomness from
the future. When the transition kernel varies slowly with respect to the state,
it is shown that the training trajectory of new algorithms is close to the one
of unbiased stochastic gradient descent. Numerical results for policy
evaluation in both tabular and neural network settings are provided to confirm
the theoretical findings.
</p>
<a href="http://arxiv.org/abs/1912.00304">arXiv:1912.00304</a> [<a href="http://arxiv.org/pdf/1912.00304">pdf</a>]

<h2>Dynamic Anomaly Detection with High-fidelity Simulators: A Convex Optimization Approach. (arXiv:2004.13927v2 [math.OC] UPDATED)</h2>
<h3>Kaikai Pan, Peter Palensky, Peyman Mohajerin Esfahani</h3>
<p>The main objective of this article is to develop scalable dynamic anomaly
detectors when high-fidelity simulators of power systems are at our disposal.
On the one hand, mathematical models of these high-fidelity simulators are
typically "intractable" to apply existing model-based approaches. On the other
hand, pure data-driven methods developed primarily in the machine learning
literature neglect our knowledge about the underlying dynamics of the systems.
In this study, we combine tools from these two mainstream approaches to develop
a diagnosis filter that utilizes the knowledge of both the dynamical system as
well as the simulation data of the high-fidelity simulators. The proposed
diagnosis filter aims to achieve two desired features: (i) performance
robustness with respect to model mismatch; (ii) high scalability. To this end,
we propose a tractable (convex) optimization-based reformulation in which
decisions are the filter parameters, the model-based information introduces
feasible sets, and the data from the simulator forms the objective function
to-be-minimized regarding the effect of model mismatch on the filter
performance. To validate the theoretical results, we implement the developed
diagnosis filter in DIgSILENT PowerFactory to detect false data injection
attacks on the Automatic Generation Control measurements in the three-area IEEE
39-bus system.
</p>
<a href="http://arxiv.org/abs/2004.13927">arXiv:2004.13927</a> [<a href="http://arxiv.org/pdf/2004.13927">pdf</a>]

<h2>A generative adversarial network approach to calibration of local stochastic volatility models. (arXiv:2005.02505v3 [q-fin.CP] UPDATED)</h2>
<h3>Christa Cuchiero, Wahid Khosrawi, Josef Teichmann</h3>
<p>We propose a fully data-driven approach to calibrate local stochastic
volatility (LSV) models, circumventing in particular the ad hoc interpolation
of the volatility surface. To achieve this, we parametrize the leverage
function by a family of feed-forward neural networks and learn their parameters
directly from the available market option prices. This should be seen in the
context of neural SDEs and (causal) generative adversarial networks: we
generate volatility surfaces by specific neural SDEs, whose quality is assessed
by quantifying, possibly in an adversarial manner, distances to market prices.
The minimization of the calibration functional relies strongly on a variance
reduction technique based on hedging and deep hedging, which is interesting in
its own right: it allows the calculation of model prices and model implied
volatilities in an accurate way using only small sets of sample paths. For
numerical illustration we implement a SABR-type LSV model and conduct a
thorough statistical performance analysis on many samples of implied volatility
smiles, showing the accuracy and stability of the method.
</p>
<a href="http://arxiv.org/abs/2005.02505">arXiv:2005.02505</a> [<a href="http://arxiv.org/pdf/2005.02505">pdf</a>]

<h2>Accelerating Ill-Conditioned Low-Rank Matrix Estimation via Scaled Gradient Descent. (arXiv:2005.08898v3 [cs.LG] UPDATED)</h2>
<h3>Tian Tong, Cong Ma, Yuejie Chi</h3>
<p>Low-rank matrix estimation is a canonical problem that finds numerous
applications in signal processing, machine learning and imaging science. A
popular approach in practice is to factorize the matrix into two compact
low-rank factors, and then optimize these factors directly via simple iterative
methods such as gradient descent and alternating minimization. Despite
nonconvexity, recent literatures have shown that these simple heuristics in
fact achieve linear convergence when initialized properly for a growing number
of problems of interest. However, upon closer examination, existing approaches
can still be computationally expensive especially for ill-conditioned matrices:
the convergence rate of gradient descent depends linearly on the condition
number of the low-rank matrix, while the per-iteration cost of alternating
minimization is often prohibitive for large matrices. The goal of this paper is
to set forth a competitive algorithmic approach dubbed Scaled Gradient Descent
(ScaledGD) which can be viewed as pre-conditioned or diagonally-scaled gradient
descent, where the pre-conditioners are adaptive and iteration-varying with a
minimal computational overhead. With tailored variants for low-rank matrix
sensing, robust principal component analysis and matrix completion, we
theoretically show that ScaledGD achieves the best of both worlds: it converges
linearly at a rate independent of the condition number of the low-rank matrix
similar as alternating minimization, while maintaining the low per-iteration
cost of gradient descent. Our analysis is also applicable to general loss
functions that are restricted strongly convex and smooth over low-rank
matrices. To the best of our knowledge, ScaledGD is the first algorithm that
provably has such properties over a wide range of low-rank matrix estimation
tasks.
</p>
<a href="http://arxiv.org/abs/2005.08898">arXiv:2005.08898</a> [<a href="http://arxiv.org/pdf/2005.08898">pdf</a>]

<h2>An Incremental Gradient Method for Large-scale Distributed Nonlinearly Constrained Optimization. (arXiv:2006.07956v2 [math.OC] UPDATED)</h2>
<h3>Harshal D. Kaushik, Farzad Yousefian</h3>
<p>Motivated by applications arising from sensor networks and machine learning,
we consider the problem of minimizing a finite sum of nondifferentiable convex
functions where each component function is associated with an agent and a
hard-to-project constraint set. Among well-known avenues to address finite sum
problems is the class of incremental gradient (IG) methods where a single
component function is selected at each iteration in a cyclic or randomized
manner. When the problem is constrained, the existing IG schemes (including
projected IG, proximal IAG, and SAGA) require a projection step onto the
feasible set at each iteration. Consequently, the performance of these schemes
is afflicted with costly projections when the problem includes: (1) nonlinear
constraints, or (2) a large number of linear constraints. Our focus in this
paper lies in addressing both of these challenges. We develop an algorithm
called averaged iteratively regularized incremental gradient (aIR-IG) that does
not involve any hard-to-project computation. Under mild assumptions, we derive
non-asymptotic rates of convergence for both suboptimality and infeasibility
metrics. Numerically, we show that the proposed scheme outperforms the standard
IG methods on distributed soft-margin support vector machine problems.
</p>
<a href="http://arxiv.org/abs/2006.07956">arXiv:2006.07956</a> [<a href="http://arxiv.org/pdf/2006.07956">pdf</a>]

<h2>On generic $G$-graded Azumaya algebras. (arXiv:2008.00976v2 [math.RA] UPDATED)</h2>
<h3>Eli Aljadeff, Yakov Karasik</h3>
<p>Let $F$ be an algebraically closed field of characteristic zero and let $G$
be a finite group. Consider $G$-graded simple algebras $A$ which are finite
dimensional and $e$-central over $F$, i.e. $Z(A)_{e} := Z(A)\cap A_{e} = F$.
For any such algebra we construct a \textit{generic} $G$-graded algebra
$\mathcal{U}$ which is \textit{Azumaya} in the following sense. $(1)$
\textit{$($Correspondence of ideals$)$}: There is one to one correspondence
between the $G$-graded ideals of $\mathcal{U}$ and the ideals of the ring $R$,
the $e$-center of $\mathcal{U}$. $(2)$ \textit{Artin-Procesi condition}:
$\mathcal{U}$ satisfies the $G$-graded identities of $A$ and no nonzero
$G$-graded homomorphic image of $\mathcal{U}$ satisfies properly more
identities. $(3)$ \textit{Generic}: If $B$ is a $G$-graded algebra over a field
then it is a specialization of $\mathcal{U}$ along an ideal $\mathfrak{a} \in
spec(Z(\mathcal{U})_{e})$ if and only if it is a $G$-graded form of $A$ over
its $e$-center.

We apply this to characterize finite dimensional $G$-graded simple algebras
over $F$ that admit a $G$-graded division algebra form over their $e$-center.
</p>
<a href="http://arxiv.org/abs/2008.00976">arXiv:2008.00976</a> [<a href="http://arxiv.org/pdf/2008.00976">pdf</a>]

<h2>Are Deep Neural Architectures Losing Information? Invertibility Is Indispensable. (arXiv:2009.03173v2 [cs.CV] UPDATED)</h2>
<h3>Yang Liu, Zhenyue Qin, Saeed Anwar, Sabrina Caldwell, Tom Gedeon</h3>
<p>Ever since the advent of AlexNet, designing novel deep neural architectures
for different tasks has consistently been a productive research direction.
Despite the exceptional performance of various architectures in practice, we
study a theoretical question: what is the condition for deep neural
architectures to preserve all the information of the input data? Identifying
the information lossless condition for deep neural architectures is important,
because tasks such as image restoration require keep the detailed information
of the input data as much as possible. Using the definition of mutual
information, we show that: a deep neural architecture can preserve maximum
details about the given data if and only if the architecture is invertible. We
verify the advantages of our Invertible Restoring Autoencoder (IRAE) network by
comparing it with competitive models on three perturbed image restoration
tasks: image denoising, jpeg image decompression and image inpainting.
Experimental results show that IRAE consistently outperforms non-invertible
ones. Our model even contains far fewer parameters. Thus, it may be worthwhile
to try replacing standard components of deep neural architectures, such as
residual blocks and ReLU, with their invertible counterparts. We believe our
work provides a unique perspective and direction for future deep learning
research.
</p>
<a href="http://arxiv.org/abs/2009.03173">arXiv:2009.03173</a> [<a href="http://arxiv.org/pdf/2009.03173">pdf</a>]

<h2>Mez: A Messaging System for Latency-Sensitive Multi-Camera Machine Vision at the IoT Edge. (arXiv:2009.13549v1 [cs.NI])</h2>
<h3>Anjus George, Arun Ravindran, Mattias Mendieta, Hamed Tabkhi</h3>
<p>Mez is a publish-subscribe messaging system for latency sensitive
multi-camera machine vision at the IoT Edge. Unlike existing messaging systems,
Mez allows applications to specify latency, and application accuracy bounds.
Mez implements a network latency controller that dynamically adjusts the video
frame quality to satisfy latency, and application accuracy requirements.
Additionally, the design of Mez utilizes application domain specific features
to provide low latency operations. Experimental evaluation on an IoT Edge
testbed with a pedestrian detection machine vision application indicates that
Mez is able to tolerate latency variations of up to 10x with a worst-case
reduction of 4.2\% in the application accuracy F1 score metric.
</p>
<a href="http://arxiv.org/abs/2009.13549">arXiv:2009.13549</a> [<a href="http://arxiv.org/pdf/2009.13549">pdf</a>]

<h2>DialoGLUE: A Natural Language Understanding Benchmark for Task-Oriented Dialogue. (arXiv:2009.13570v1 [cs.CL])</h2>
<h3>Shikib Mehri, Mihail Eric, Dilek Hakkani-Tur</h3>
<p>A long-standing goal of task-oriented dialogue research is the ability to
flexibly adapt dialogue models to new domains. To progress research in this
direction, we introduce \textbf{DialoGLUE} (Dialogue Language Understanding
Evaluation), a public benchmark consisting of 7 task-oriented dialogue datasets
covering 4 distinct natural language understanding tasks, designed to encourage
dialogue research in representation-based transfer, domain adaptation, and
sample-efficient task learning. We release several strong baseline models,
demonstrating performance improvements over a vanilla BERT architecture and
state-of-the-art results on 5 out of 7 tasks, by pre-training on a large
open-domain dialogue corpus and task-adaptive self-supervised training. Through
the DialoGLUE benchmark, the baseline methods, and our evaluation scripts, we
hope to facilitate progress towards the goal of developing more general
task-oriented dialogue models.
</p>
<a href="http://arxiv.org/abs/2009.13570">arXiv:2009.13570</a> [<a href="http://arxiv.org/pdf/2009.13570">pdf</a>]

<h2>Robust Monotonic Convergent Iterative Learning Control Design: an LMI-based Method. (arXiv:2009.13574v1 [eess.SY])</h2>
<h3>Lanlan Su</h3>
<p>This work investigates robust monotonic convergent iterative learning control
(ILC) for uncertain linear systems in both time and frequency domains, and the
ILC algorithm optimizing the convergence speed in terms of $l_{2}$ norm of
error signals is derived. Firstly, it is shown that the robust monotonic
convergence of the ILC system can be established equivalently by the positive
definiteness of a matrix polynomial over some set. Then, a necessary and
sufficient condition in the form of sum of squares (SOS) for the positive
definiteness is proposed, which is amendable to the feasibility of linear
matrix inequalities (LMIs). Based on such a condition, the optimal ILC
algorithm that maximizes the convergence speed is obtained by solving a set of
convex optimization problems. Moreover, the order of the learning function can
be chosen arbitrarily so that the designers have the flexibility to decide the
complexity of the learning algorithm.
</p>
<a href="http://arxiv.org/abs/2009.13574">arXiv:2009.13574</a> [<a href="http://arxiv.org/pdf/2009.13574">pdf</a>]

<h2>Deep Learning-Based Automatic Detection of Poorly Positioned Mammograms to Minimize Patient Return Visits for Repeat Imaging: A Real-World Application. (arXiv:2009.13580v1 [eess.IV])</h2>
<h3>Vikash Gupta, Clayton Taylor, Sarah Bonnet, Luciano M. Prevedello, Jeffrey Hawley, Richard D White, Mona G Flores, Barbaros Selnur Erdal</h3>
<p>Screening mammograms are a routine imaging exam performed to detect breast
cancer in its early stages to reduce morbidity and mortality attributed to this
disease. In order to maximize the efficacy of breast cancer screening programs,
proper mammographic positioning is paramount. Proper positioning ensures
adequate visualization of breast tissue and is necessary for effective breast
cancer detection. Therefore, breast-imaging radiologists must assess each
mammogram for the adequacy of positioning before providing a final
interpretation of the examination; this often necessitates return patient
visits for additional imaging. In this paper, we propose a deep
learning-algorithm method that mimics and automates this decision-making
process to identify poorly positioned mammograms. Our objective for this
algorithm is to assist mammography technologists in recognizing inadequately
positioned mammograms real-time, improve the quality of mammographic
positioning and performance, and ultimately reducing repeat visits for patients
with initially inadequate imaging. The proposed model showed a true positive
rate for detecting correct positioning of 91.35% in the mediolateral oblique
view and 95.11% in the craniocaudal view. In addition to these results, we also
present an automatically generated report which can aid the mammography
technologist in taking corrective measures during the patient visit.
</p>
<a href="http://arxiv.org/abs/2009.13580">arXiv:2009.13580</a> [<a href="http://arxiv.org/pdf/2009.13580">pdf</a>]

<h2>Fully Automatic Intervertebral Disc Segmentation Using Multimodal 3D U-Net. (arXiv:2009.13583v1 [eess.IV])</h2>
<h3>Chuanbo Wang, Ye Guo, Wei Chen, Zeyun Yu</h3>
<p>Intervertebral discs (IVDs), as small joints lying between adjacent
vertebrae, have played an important role in pressure buffering and tissue
protection. The fully-automatic localization and segmentation of IVDs have been
discussed in the literature for many years since they are crucial to spine
disease diagnosis and provide quantitative parameters in the treatment.
Traditionally hand-crafted features are derived based on image intensities and
shape priors to localize and segment IVDs. With the advance of deep learning,
various neural network models have gained great success in image analysis
including the recognition of intervertebral discs. Particularly, U-Net stands
out among other approaches due to its outstanding performance on biomedical
images with a relatively small set of training data. This paper proposes a
novel convolutional framework based on 3D U-Net to segment IVDs from
multi-modality MRI images. We first localize the centers of intervertebral
discs in each spine sample and then train the network based on the cropped
small volumes centered at the localized intervertebral discs. A detailed
comprehensive analysis of the results using various combinations of
multi-modalities is presented. Furthermore, experiments conducted on 2D and 3D
U-Nets with augmented and non-augmented datasets are demonstrated and compared
in terms of Dice coefficient and Hausdorff distance. Our method has proved to
be effective with a mean segmentation Dice coefficient of 89.0% and a standard
deviation of 1.4%.
</p>
<a href="http://arxiv.org/abs/2009.13583">arXiv:2009.13583</a> [<a href="http://arxiv.org/pdf/2009.13583">pdf</a>]

<h2>Machine Learning-powered Iterative Combinatorial Auctions with Interval Bidding. (arXiv:2009.13605v1 [cs.GT])</h2>
<h3>Manuel Beyeler, Gianluca Brero, Benjamin Lubin, Sven Seuken</h3>
<p>We study the design of iterative combinatorial auctions for domains with a
large number of items. In such domains, preference elicitation is a major
challenge because the bundle space grows exponentially in the number of items.
To keep preference elicitation manageable, recent work has employed a machine
learning (ML) algorithm that identifies a small set of bundles to query from
each bidder. However, a major limitation of this prior work is that bidders
must submit exact values for the queried bundles, which can be quite costly for
them. To address this, we propose a new ML-powered auction with interval
bidding (i.e., where bidders submit upper and lower bounds for the queried
bundles). To steer the auction towards an efficient allocation, we design a
price-based refinement process asking bidders to tighten bounds on relevant
bundles only, and we carefully integrate this refinement process into the
ML-based query module. Our experiments show that our new auction with interval
bidding achieves almost the same allocative efficiency as the prior auction
design that required bidders to submit exact values. Despite only eliciting
interval bids, our auction beats the well-known combinatorial clock auction in
a realistically-sized domain.
</p>
<a href="http://arxiv.org/abs/2009.13605">arXiv:2009.13605</a> [<a href="http://arxiv.org/pdf/2009.13605">pdf</a>]

<h2>Monitoring My Dehydration: A Non-Invasive Dehydration Alert System Using Electrodermal Activity. (arXiv:2009.13626v1 [cs.CY])</h2>
<h3>Nandan Kulkarni, Christopher Compton, Jooseppi Luna, Mohammad Arif Ul Alam</h3>
<p>Staying hydrated and drinking fluids is extremely crucial to stay healthy and
maintaining even basic bodily functions. Studies have shown that dehydration
leads to loss of productivity, cognitive impairment and mood in both men and
women. However, there are no such an existing tool that can monitor dehydration
continuously and provide alert to users before it affects on their health. In
this paper, we propose to utilize wearable Electrodermal Activity (EDA) sensors
in conjunction with signal processing and machine learning techniques to
develop first time ever a dehydration self-monitoring tool, \emph{Monitoring My
Dehydration} (MMD), that can instantly detect the hydration level of human
skin. Moreover, we develop an Android application over Bluetooth to connect
with wearable EDA sensor integrated wristband to track hydration levels of the
users real-time and instantly alert to the users when the hydration level goes
beyond the danger level. To validate our developed tool's performance, we
recruit 5 users, carefully designed the water intake routines to annotate the
dehydration ground truth and trained state-of-art machine learning models to
predict instant hydration level i.e., well-hydrated, hydrated, dehydrated and
very dehydrated. Our system provides an accuracy of 84.5% in estimating
dehydration level with an sensitivity of 87.5% and a specificity of 90.3% which
provides us confidence of moving forward with our method for larger
longitudinal study.
</p>
<a href="http://arxiv.org/abs/2009.13626">arXiv:2009.13626</a> [<a href="http://arxiv.org/pdf/2009.13626">pdf</a>]

<h2>Fully Automated Left Atrium Segmentation from Anatomical Cine Long-axis MRI Sequences using Deep Convolutional Neural Network with Unscented Kalman Filter. (arXiv:2009.13627v1 [cs.CV])</h2>
<h3>Xiaoran Zhang, Michelle Noga, David Glynn Martin, Kumaradevan Punithakumar</h3>
<p>This study proposes a fully automated approach for the left atrial
segmentation from routine cine long-axis cardiac magnetic resonance image
sequences using deep convolutional neural networks and Bayesian filtering. The
proposed approach consists of a classification network that automatically
detects the type of long-axis sequence and three different convolutional neural
network models followed by unscented Kalman filtering (UKF) that delineates the
left atrium. Instead of training and predicting all long-axis sequence types
together, the proposed approach first identifies the image sequence type as to
2, 3 and 4 chamber views, and then performs prediction based on neural nets
trained for that particular sequence type. The datasets were acquired
retrospectively and ground truth manual segmentation was provided by an expert
radiologist. In addition to neural net based classification and segmentation,
another neural net is trained and utilized to select image sequences for
further processing using UKF to impose temporal consistency over cardiac cycle.
A cyclic dynamic model with time-varying angular frequency is introduced in UKF
to characterize the variations in cardiac motion during image scanning. The
proposed approach was trained and evaluated separately with varying amount of
training data with images acquired from 20, 40, 60 and 80 patients. Evaluations
over 1515 images with equal number of images from each chamber group acquired
from an additional 20 patients demonstrated that the proposed model
outperformed state-of-the-art and yielded a mean Dice coefficient value of
94.1%, 93.7% and 90.1% for 2, 3 and 4-chamber sequences, respectively, when
trained with datasets from 80 patients.
</p>
<a href="http://arxiv.org/abs/2009.13627">arXiv:2009.13627</a> [<a href="http://arxiv.org/pdf/2009.13627">pdf</a>]

<h2>MPG-Net: Multi-Prediction Guided Network for Segmentation of Retinal Layers in OCT Images. (arXiv:2009.13634v1 [eess.IV])</h2>
<h3>Zeyu Fu, Yang Sun, Xiangyu Zhang, Scott Stainton, Shaun Barney, Jeffry Hogg, William Innes, Satnam Dlay</h3>
<p>Optical coherence tomography (OCT) is a commonly-used method of extracting
high resolution retinal information. Moreover there is an increasing demand for
the automated retinal layer segmentation which facilitates the retinal disease
diagnosis. In this paper, we propose a novel multiprediction guided attention
network (MPG-Net) for automated retinal layer segmentation in OCT images. The
proposed method consists of two major steps to strengthen the discriminative
power of a U-shape Fully convolutional network (FCN) for reliable automated
segmentation. Firstly, the feature refinement module which adaptively
re-weights the feature channels is exploited in the encoder to capture more
informative features and discard information in irrelevant regions.
Furthermore, we propose a multi-prediction guided attention mechanism which
provides pixel-wise semantic prediction guidance to better recover the
segmentation mask at each scale. This mechanism which transforms the deep
supervision to supervised attention is able to guide feature aggregation with
more semantic information between intermediate layers. Experiments on the
publicly available Duke OCT dataset confirm the effectiveness of the proposed
method as well as an improved performance over other state-of-the-art
approaches.
</p>
<a href="http://arxiv.org/abs/2009.13634">arXiv:2009.13634</a> [<a href="http://arxiv.org/pdf/2009.13634">pdf</a>]

<h2>Cross-Task Representation Learning for Anatomical Landmark Detection. (arXiv:2009.13635v1 [cs.CV])</h2>
<h3>Zeyu Fu, Jianbo Jiao, Michael Suttie, J. Alison Noble</h3>
<p>Recently, there is an increasing demand for automatically detecting
anatomical landmarks which provide rich structural information to facilitate
subsequent medical image analysis. Current methods related to this task often
leverage the power of deep neural networks, while a major challenge in fine
tuning such models in medical applications arises from insufficient number of
labeled samples. To address this, we propose to regularize the knowledge
transfer across source and target tasks through cross-task representation
learning. The proposed method is demonstrated for extracting facial anatomical
landmarks which facilitate the diagnosis of fetal alcohol syndrome. The source
and target tasks in this work are face recognition and landmark detection,
respectively. The main idea of the proposed method is to retain the feature
representations of the source model on the target task data, and to leverage
them as an additional source of supervisory signals for regularizing the target
model learning, thereby improving its performance under limited training
samples. Concretely, we present two approaches for the proposed representation
learning by constraining either final or intermediate model features on the
target model. Experimental results on a clinical face image dataset demonstrate
that the proposed approach works well with few labeled data, and outperforms
other compared approaches.
</p>
<a href="http://arxiv.org/abs/2009.13635">arXiv:2009.13635</a> [<a href="http://arxiv.org/pdf/2009.13635">pdf</a>]

<h2>TEL: Low-Latency Failover Traffic Engineering in Data Plane. (arXiv:2009.13640v1 [cs.NI])</h2>
<h3>Habib Mostafaei, Mohammad Shojafar, Mauro Conti</h3>
<p>Modern network applications demand low-latency traffic engineering in the
presence of network failure while preserving the quality of service constraints
like delay, and capacity. The control plane reactions to the failure can be
slow compared to the data plane while supporting traffic demands for highly
sensitive applications. The control plane interaction requires to find an
alternative path for the failed one in the legacy approaches. In this paper, we
formulate failover traffic engineering as a max-min fair allocation problem
that maximizes the number of flows while minimizing their costs. We also
present TEL, a system with a linear algorithm, that uses the idea of backup
paths to avoid the control plane interaction to compute new paths. We use a
reinforcement learning-based algorithm to explore paths in the network. In
particular, our solution performs traffic engineering in the data plane. We
implement our approach in P4 and evaluate it on two real-world topologies,
namely, Goodnet and AttMpls. The simulation results confirm that TEL has
significant throughput improvement and lower flow completion time compared to
Open Shortest Path First (OSPF). Finally, we state the applicability of TEL in
the different modern network applications.
</p>
<a href="http://arxiv.org/abs/2009.13640">arXiv:2009.13640</a> [<a href="http://arxiv.org/pdf/2009.13640">pdf</a>]

<h2>A Distributed Computing Perspective of Unconditionally Secure Information Transmission in Russian Cards Problems. (arXiv:2009.13644v1 [cs.CR])</h2>
<h3>Sergio Rajsbaum</h3>
<p>The problem of $A$ privately transmitting information to $B$ by a public
announcement overheard by an eavesdropper $C$ is considered. To do so by a
deterministic protocol, their inputs must be correlated. Dependent inputs are
represented using a deck of cards. There is a publicly known signature
$(a,b,c)$, where $n = a + b + c + r$, and $A$ gets $a$ cards, $B$ gets $b$
cards, and $C$ gets $c$ cards, out of the deck of $n$ cards.

Using a deterministic protocol, $A$ decides its announcement based on her
hand. Using techniques from coding theory, Johnson graphs, and additive number
theory, a novel perspective inspired by distributed computing theory is
provided, to analyze the amount of information that $A$ needs to send, while
preventing $C$ from learning a single card of her hand. In one extreme, the
generalized Russian cards problem, $B$ wants to learn all of $A$'s cards, and
in the other, $B$ wishes to learn something about $A$'s hand.
</p>
<a href="http://arxiv.org/abs/2009.13644">arXiv:2009.13644</a> [<a href="http://arxiv.org/pdf/2009.13644">pdf</a>]

<h2>The EMPATHIC Framework for Task Learning from Implicit Human Feedback. (arXiv:2009.13649v1 [cs.HC])</h2>
<h3>Yuchen Cui, Qiping Zhang, Alessandro Allievi, Peter Stone, Scott Niekum, W. Bradley Knox</h3>
<p>Reactions such as gestures, facial expressions, and vocalizations are an
abundant, naturally occurring channel of information that humans provide during
interactions. A robot or other agent could leverage an understanding of such
implicit human feedback to improve its task performance at no cost to the
human. This approach contrasts with common agent teaching methods based on
demonstrations, critiques, or other guidance that need to be attentively and
intentionally provided. In this paper, we first define the general problem of
learning from implicit human feedback and then propose to address this problem
through a novel data-driven framework, EMPATHIC. This two-stage method consists
of (1) mapping implicit human feedback to relevant task statistics such as
rewards, optimality, and advantage; and (2) using such a mapping to learn a
task. We instantiate the first stage and three second-stage evaluations of the
learned mapping. To do so, we collect a dataset of human facial reactions while
participants observe an agent execute a sub-optimal policy for a prescribed
training task. We train a deep neural network on this data and demonstrate its
ability to (1) infer relative reward ranking of events in the training task
from prerecorded human facial reactions; (2) improve the policy of an agent in
the training task using live human facial reactions; and (3) transfer to a
novel domain in which it evaluates robot manipulation trajectories.
</p>
<a href="http://arxiv.org/abs/2009.13649">arXiv:2009.13649</a> [<a href="http://arxiv.org/pdf/2009.13649">pdf</a>]

<h2>Towards a Measure of Individual Fairness for Deep Learning. (arXiv:2009.13650v1 [cs.AI])</h2>
<h3>Krystal Maughan, Joseph P. Near</h3>
<p>Deep learning has produced big advances in artificial intelligence, but
trained neural networks often reflect and amplify bias in their training data,
and thus produce unfair predictions. We propose a novel measure of individual
fairness, called prediction sensitivity, that approximates the extent to which
a particular prediction is dependent on a protected attribute. We show how to
compute prediction sensitivity using standard automatic differentiation
capabilities present in modern deep learning frameworks, and present
preliminary empirical results suggesting that prediction sensitivity may be
effective for measuring bias in individual predictions.
</p>
<a href="http://arxiv.org/abs/2009.13650">arXiv:2009.13650</a> [<a href="http://arxiv.org/pdf/2009.13650">pdf</a>]

<h2>Learning Knowledge Bases with Parameters for Task-Oriented Dialogue Systems. (arXiv:2009.13656v1 [cs.CL])</h2>
<h3>Andrea Madotto, Samuel Cahyawijaya, Genta Indra Winata, Yan Xu, Zihan Liu, Zhaojiang Lin, Pascale Fung</h3>
<p>Task-oriented dialogue systems are either modularized with separate dialogue
state tracking (DST) and management steps or end-to-end trainable. In either
case, the knowledge base (KB) plays an essential role in fulfilling user
requests. Modularized systems rely on DST to interact with the KB, which is
expensive in terms of annotation and inference time. End-to-end systems use the
KB directly as input, but they cannot scale when the KB is larger than a few
hundred entries. In this paper, we propose a method to embed the KB, of any
size, directly into the model parameters. The resulting model does not require
any DST or template responses, nor the KB as input, and it can dynamically
update its KB via fine-tuning. We evaluate our solution in five task-oriented
dialogue datasets with small, medium, and large KB size. Our experiments show
that end-to-end models can effectively embed knowledge bases in their
parameters and achieve competitive performance in all evaluated datasets.
</p>
<a href="http://arxiv.org/abs/2009.13656">arXiv:2009.13656</a> [<a href="http://arxiv.org/pdf/2009.13656">pdf</a>]

<h2>Breaking the Memory Wall for AI Chip with a New Dimension. (arXiv:2009.13664v1 [cs.AR])</h2>
<h3>Eugene Tam, Shenfei Jiang, Paul Duan, Shawn Meng, Yue Pang, Cayden Huang, Yi Han, Jacke Xie, Yuanjun Cui, Jinsong Yu, Minggui Lu</h3>
<p>Recent advancements in deep learning have led to the widespread adoption of
artificial intelligence (AI) in applications such as computer vision and
natural language processing. As neural networks become deeper and larger, AI
modeling demands outstrip the capabilities of conventional chip architectures.
Memory bandwidth falls behind processing power. Energy consumption comes to
dominate the total cost of ownership. Currently, memory capacity is
insufficient to support the most advanced NLP models. In this work, we present
a 3D AI chip, called Sunrise, with near-memory computing architecture to
address these three challenges. This distributed, near-memory computing
architecture allows us to tear down the performance-limiting memory wall with
an abundance of data bandwidth. We achieve the same level of energy efficiency
on 40nm technology as competing chips on 7nm technology. By moving to similar
technologies as other AI chips, we project to achieve more than ten times the
energy efficiency, seven times the performance of the current state-of-the-art
chips, and twenty times of memory capacity as compared with the best chip in
each benchmark.
</p>
<a href="http://arxiv.org/abs/2009.13664">arXiv:2009.13664</a> [<a href="http://arxiv.org/pdf/2009.13664">pdf</a>]

<h2>Parameter Critic: a Model Free Variance Reduction Method Through Imperishable Samples. (arXiv:2009.13668v1 [eess.SY])</h2>
<h3>Juan Cervino, Harshat Kumar, Alejandro Ribeiro</h3>
<p>We consider the problem of finding a policy that maximizes an expected reward
throughout the trajectory of an agent that interacts with an unknown
environment. Frequently denoted Reinforcement Learning, this framework suffers
from the need of large amount of samples in each step of the learning process.
To this end, we introduce parameter critic, a formulation that allows samples
to keep their validity even when the parameters of the policy change. In
particular, we propose the use of a function approximator to directly learn the
relationship between the parameters and the expected cumulative reward. Through
convergence analysis, we demonstrate the parameter critic outperforms
gradient-free parameter space exploration techniques as it is robust to noise.
Empirically, we show that our method solves the cartpole problem which
corroborates our claim as the agent can successfully learn an optimal policy
while learning the relationship between the parameters and the cumulative
reward.
</p>
<a href="http://arxiv.org/abs/2009.13668">arXiv:2009.13668</a> [<a href="http://arxiv.org/pdf/2009.13668">pdf</a>]

<h2>Vaccination strategies against COVID-19 and the diffusion of anti-vaccination views. (arXiv:2009.13674v1 [physics.soc-ph])</h2>
<h3>Rafael Prieto Curiel, Humberto Gonz&#xe1;lez Ram&#xed;rez</h3>
<p>Miss-information is usually adjusted to fit distinct narratives and can
propagate rapidly through communities of interest, which work as echo chambers,
cause reinforcement and foster confirmation bias. False beliefs, once adopted,
are rarely corrected. Amidst the COVID-19 crisis, pandemic-deniers and people
who oppose wearing face masks or quarantines have already been a substantial
aspect of the development of the pandemic. With a potential vaccine for
COVID-19, different anti-vaccine narratives will be created and, likely,
adopted by large population groups, with critical consequences. Here, we
analyse epidemic spreading and optimal vaccination strategies, measured with
the average years of life lost, in two network topologies (scale-free and
small-world) assuming full adherence to vaccine administration. We consider the
spread of anti-vaccine views in the network, using a similar diffusion model as
the one used in epidemics, which are adopted based on a persuasiveness
parameter of anti-vaccine views. Results show that even if an anti-vaccine
narrative has a small persuasiveness, a large part of the population will be
rapidly exposed to them. Assuming that all individuals are equally likely to
adopt anti-vaccine views after being exposed, more central nodes in the network
are more exposed and therefore are more likely to adopt them. Comparing years
of life lost, anti-vaccine views could have a significant cost not only on
those who share them, since the core social benefits of a limited vaccination
strategy (reduction of susceptible hosts, network disruptions and slowing the
spread of the disease) are substantially shortened.
</p>
<a href="http://arxiv.org/abs/2009.13674">arXiv:2009.13674</a> [<a href="http://arxiv.org/pdf/2009.13674">pdf</a>]

<h2>Deep Learning-based Symbolic Indoor Positioning using the Serving eNodeB. (arXiv:2009.13675v1 [eess.SP])</h2>
<h3>Fahad Alhomayani, Mohammad Mahoor</h3>
<p>This paper presents a novel indoor positioning method designed for
residential apartments. The proposed method makes use of cellular signals
emitting from a serving eNodeB which eliminates the need for specialized
positioning infrastructure. Additionally, it utilizes Denoising Autoencoders to
mitigate the effects of cellular signal loss. We evaluated the proposed method
using real-world data collected from two different smartphones inside a
representative apartment of eight symbolic spaces. Experimental results verify
that the proposed method outperforms conventional symbolic indoor positioning
techniques in various performance metrics. To promote reproducibility and
foster new research efforts, we made all the data and codes associated with
this work publicly available.
</p>
<a href="http://arxiv.org/abs/2009.13675">arXiv:2009.13675</a> [<a href="http://arxiv.org/pdf/2009.13675">pdf</a>]

<h2>The Grey Hoodie Project: Big Tobacco, Big Tech, and the threat on academic integrity. (arXiv:2009.13676v1 [cs.CY])</h2>
<h3>Mohamed Abdalla, Moustafa Abdalla</h3>
<p>As governmental bodies rely on academics' expert advice to shape policy
regarding Artificial Intelligence, it is important that these academics not
have conflicts of interests that may cloud or bias their judgement. Our work
explores how Big Tech is actively distorting the academic landscape to suit its
needs. By comparing the well-studied actions of another industry, that of Big
Tobacco, to the current actions of Big Tech we see similar strategies employed
by both industries to sway and influence academic and public discourse. We
examine the funding of academic research as a tool used by Big Tech to put
forward a socially responsible public image, influence events hosted by and
decisions made by funded universities, influence the research questions and
plans of individual scientists, and discover receptive academics who can be
leveraged. We demonstrate, in a rigorous manner, how Big Tech can affect
academia from the institutional level down to individual researchers. Thus, we
believe that it is vital, particularly for universities and other institutions
of higher learning, to discuss the appropriateness and the tradeoffs of
accepting funding from Big Tech, and what limitations or conditions should be
put in place.
</p>
<a href="http://arxiv.org/abs/2009.13676">arXiv:2009.13676</a> [<a href="http://arxiv.org/pdf/2009.13676">pdf</a>]

<h2>VIVO: Surpassing Human Performance in Novel Object Captioning with Visual Vocabulary Pre-Training. (arXiv:2009.13682v1 [cs.CV])</h2>
<h3>Xiaowei Hu, Xi Yin, Kevin Lin, Lijuan Wang, Lei Zhang, Jianfeng Gao, Zicheng Liu</h3>
<p>It is highly desirable yet challenging to generate image captions that can
describe novel objects which are unseen in caption-labeled training data, a
capability that is evaluated in the novel object captioning challenge (nocaps).
In this challenge, no additional image-caption training data, other than COCO
Captions, is allowed for model training. Thus, conventional Vision-Language
Pre-training (VLP) methods cannot be applied. This paper presents VIsual
VOcabulary pre-training (VIVO) that performs pre-training in the absence of
caption annotations. By breaking the dependency of paired image-caption
training data in VLP, VIVO can leverage large amounts of paired image-tag data
to learn a visual vocabulary. This is done by pre-training a multi-layer
Transformer model that learns to align image-level tags with their
corresponding image region features. To address the unordered nature of image
tags, VIVO uses a Hungarian matching loss with masked tag prediction to conduct
pre-training.

We validate the effectiveness of VIVO by fine-tuning the pre-trained model
for image captioning. In addition, we perform an analysis of the visual-text
alignment inferred by our model. The results show that our model can not only
generate fluent image captions that describe novel objects, but also identify
the locations of these objects. Our single model has achieved new
state-of-the-art results on nocaps and surpassed the human CIDEr score.
</p>
<a href="http://arxiv.org/abs/2009.13682">arXiv:2009.13682</a> [<a href="http://arxiv.org/pdf/2009.13682">pdf</a>]

<h2>A Fast Graph Neural Network-Based Method for Winner Determination in Multi-Unit Combinatorial Auctions. (arXiv:2009.13697v1 [cs.LG])</h2>
<h3>Mengyuan Lee, Seyyedali Hosseinalipour, Christopher G. Brinton, Guanding Yu, Huaiyu Dai</h3>
<p>The combinatorial auction (CA) is an efficient mechanism for resource
allocation in different fields, including cloud computing. It can obtain high
economic efficiency and user flexibility by allowing bidders to submit bids for
combinations of different items instead of only for individual items. However,
the problem of allocating items among the bidders to maximize the auctioneers"
revenue, i.e., the winner determination problem (WDP), is NP-complete to solve
and inapproximable. Existing works for WDPs are generally based on mathematical
optimization techniques and most of them focus on the single-unit WDP, where
each item only has one unit. On the contrary, few works consider the multi-unit
WDP in which each item may have multiple units. Given that the multi-unit WDP
is more complicated but prevalent in cloud computing, we propose leveraging
machine learning (ML) techniques to develop a novel low-complexity algorithm
for solving this problem with negligible revenue loss. Specifically, we model
the multi-unit WDP as an augmented bipartite bid-item graph and use a graph
neural network (GNN) with half-convolution operations to learn the probability
of each bid belonging to the optimal allocation. To improve the sample
generation efficiency and decrease the number of needed labeled instances, we
propose two different sample generation processes. We also develop two novel
graph-based post-processing algorithms to transform the outputs of the GNN into
feasible solutions. Through simulations on both synthetic instances and a
specific virtual machine (VM) allocation problem in a cloud computing platform,
we validate that our proposed method can approach optimal performance with low
complexity and has good generalization ability in terms of problem size and
user-type distribution.
</p>
<a href="http://arxiv.org/abs/2009.13697">arXiv:2009.13697</a> [<a href="http://arxiv.org/pdf/2009.13697">pdf</a>]

<h2>Learn like a Pathologist: Curriculum Learning by Annotator Agreement for Histopathology Image Classification. (arXiv:2009.13698v1 [cs.CV])</h2>
<h3>Jerry Wei, Arief Suriawinata, Bing Ren, Xiaoying Liu, Mikhail Lisovsky, Louis Vaickus, Charles Brown, Michael Baker, Mustafa Nasir-Moin, Naofumi Tomita, Lorenzo Torresani, Jason Wei, Saeed Hassanpour</h3>
<p>Applying curriculum learning requires both a range of difficulty in data and
a method for determining the difficulty of examples. In many tasks, however,
satisfying these requirements can be a formidable challenge. In this paper, we
contend that histopathology image classification is a compelling use case for
curriculum learning. Based on the nature of histopathology images, a range of
difficulty inherently exists among examples, and, since medical datasets are
often labeled by multiple annotators, annotator agreement can be used as a
natural proxy for the difficulty of a given example. Hence, we propose a simple
curriculum learning method that trains on progressively-harder images as
determined by annotator agreement. We evaluate our hypothesis on the
challenging and clinically-important task of colorectal polyp classification.
Whereas vanilla training achieves an AUC of 83.7% for this task, a model
trained with our proposed curriculum learning approach achieves an AUC of
88.2%, an improvement of 4.5%. Our work aims to inspire researchers to think
more creatively and rigorously when choosing contexts for applying curriculum
learning.
</p>
<a href="http://arxiv.org/abs/2009.13698">arXiv:2009.13698</a> [<a href="http://arxiv.org/pdf/2009.13698">pdf</a>]

<h2>A Comprehensive Survey of Machine Learning Applied to Radar Signal Processing. (arXiv:2009.13702v1 [eess.SP])</h2>
<h3>Ping Lang, Xiongjun Fu, Marco Martorella, Jian Dong, Rui Qin, Xianpeng Meng, Min Xie</h3>
<p>Modern radar systems have high requirements in terms of accuracy, robustness
and real-time capability when operating on increasingly complex electromagnetic
environments. Traditional radar signal processing (RSP) methods have shown some
limitations when meeting such requirements, particularly in matters of target
classification. With the rapid development of machine learning (ML), especially
deep learning, radar researchers have started integrating these new methods
when solving RSP-related problems. This paper aims at helping researchers and
practitioners to better understand the application of ML techniques to
RSP-related problems by providing a comprehensive, structured and reasoned
literature overview of ML-based RSP techniques. This work is amply introduced
by providing general elements of ML-based RSP and by stating the motivations
behind them. The main applications of ML-based RSP are then analysed and
structured based on the application field. This paper then concludes with a
series of open questions and proposed research directions, in order to indicate
current gaps and potential future solutions and trends.
</p>
<a href="http://arxiv.org/abs/2009.13702">arXiv:2009.13702</a> [<a href="http://arxiv.org/pdf/2009.13702">pdf</a>]

<h2>Cranial Implant Design via Virtual Craniectomy with Shape Priors. (arXiv:2009.13704v1 [eess.IV])</h2>
<h3>Franco Matzkin, Virginia Newcombe, Ben Glocker, Enzo Ferrante</h3>
<p>Cranial implant design is a challenging task, whose accuracy is crucial in
the context of cranioplasty procedures. This task is usually performed manually
by experts using computer-assisted design software. In this work, we propose
and evaluate alternative automatic deep learning models for cranial implant
reconstruction from CT images. The models are trained and evaluated using the
database released by the AutoImplant challenge, and compared to a baseline
implemented by the organizers. We employ a simulated virtual craniectomy to
train our models using complete skulls, and compare two different approaches
trained with this procedure. The first one is a direct estimation method based
on the UNet architecture. The second method incorporates shape priors to
increase the robustness when dealing with out-of-distribution implant shapes.
Our direct estimation method outperforms the baselines provided by the
organizers, while the model with shape priors shows superior performance when
dealing with out-of-distribution cases. Overall, our methods show promising
results in the difficult task of cranial implant design.
</p>
<a href="http://arxiv.org/abs/2009.13704">arXiv:2009.13704</a> [<a href="http://arxiv.org/pdf/2009.13704">pdf</a>]

<h2>PDLight: A Deep Reinforcement Learning Traffic Light Control Algorithm with Pressure and Dynamic Light Duration. (arXiv:2009.13711v1 [cs.LG])</h2>
<h3>Chenguang Zhao, Xiaorong Hu, Gang Wang</h3>
<p>Existing ineffective and inflexible traffic light control at urban
intersections can often lead to congestion in traffic flows and cause numerous
problems, such as long delay and waste of energy. How to find the optimal
signal timing strategy is a significant challenge in urban traffic management.
In this paper, we propose PDlight, a deep reinforcement learning (DRL) traffic
light control algorithm with a novel reward as PRCOL (Pressure with Remaining
Capacity of Outgoing Lane). Serving as an improvement over the pressure used in
traffic control algorithms, PRCOL considers not only the number of vehicles on
the incoming lane but also the remaining capacity of the outgoing lane.
Simulation results using both synthetic and real-world data-sets show that the
proposed PDlight yields lower average travel time compared with several
state-of-the-art algorithms, PressLight and Colight, under both fixed and
dynamic green light duration.
</p>
<a href="http://arxiv.org/abs/2009.13711">arXiv:2009.13711</a> [<a href="http://arxiv.org/pdf/2009.13711">pdf</a>]

<h2>Learned Fine-Tuner for Incongruous Few-Shot Learning. (arXiv:2009.13714v1 [cs.LG])</h2>
<h3>Pu Zhao, Sijia Liu, Parikshit Ram, Songtao Lu, Djallel Bouneffouf, Xue Lin</h3>
<p>Model-agnostic meta-learning (MAML) effectively meta-learns an initialization
of model parameters for few-shot learning where all learning problems share the
same format of model parameters -- congruous meta-learning. We extend MAML to
incongruous meta-learning where different yet related few-shot learning
problems may not share any model parameters. In this setup, we propose the use
of a Learned Fine Tuner (LFT) to replace hand-designed optimizers (such as SGD)
for the task-specific fine-tuning. The meta-learned initialization in MAML is
replaced by learned optimizers based on the learning-to-optimize (L2O)
framework to meta-learn across incongruous tasks such that models fine-tuned
with LFT (even from random initializations) adapt quickly to new tasks. The
introduction of LFT within MAML (i) offers the capability to tackle few-shot
learning tasks by meta-learning across incongruous yet related problems (e.g.,
classification over images of different sizes and model architectures), and
(ii) can {efficiently} work with first-order and derivative-free few-shot
learning problems. Theoretically, we quantify the difference between LFT (for
MAML) and L2O. Empirically, we demonstrate the effectiveness of LFT through
both synthetic and real problems and a novel application of generating
universal adversarial attacks across different image sources in the few-shot
learning regime.
</p>
<a href="http://arxiv.org/abs/2009.13714">arXiv:2009.13714</a> [<a href="http://arxiv.org/pdf/2009.13714">pdf</a>]

<h2>Deep discriminant analysis for task-dependent compact network search. (arXiv:2009.13716v1 [cs.CV])</h2>
<h3>Qing Tian, Tal Arbel, James J. Clark</h3>
<p>Most of today's popular deep architectures are hand-engineered for general
purpose applications. However, this design procedure usually leads to massive
redundant, useless, or even harmful features for specific tasks. Such
unnecessarily high complexities render deep nets impractical for many
real-world applications, especially those without powerful GPU support. In this
paper, we attempt to derive task-dependent compact models from a deep
discriminant analysis perspective. We propose an iterative and proactive
approach for classification tasks which alternates between (1) a pushing step,
with an objective to simultaneously maximize class separation, penalize
co-variances, and push deep discriminants into alignment with a compact set of
neurons, and (2) a pruning step, which discards less useful or even interfering
neurons. Deconvolution is adopted to reverse `unimportant' filters' effects and
recover useful contributing sources. A simple network growing strategy based on
the basic Inception module is proposed for challenging tasks requiring larger
capacity than what the base net can offer. Experiments on the MNIST, CIFAR10,
and ImageNet datasets demonstrate our approach's efficacy. On ImageNet, by
pushing and pruning our grown Inception-88 model, we achieve better-performing
models than smaller deep Inception nets grown, residual nets, and famous
compact nets at similar sizes. We also show that our grown deep Inception nets
(without hard-coded dimension alignment) can beat residual nets of similar
complexities.
</p>
<a href="http://arxiv.org/abs/2009.13716">arXiv:2009.13716</a> [<a href="http://arxiv.org/pdf/2009.13716">pdf</a>]

<h2>Adversarial Attacks Against Deep Learning Systems for ICD-9 Code Assignment. (arXiv:2009.13720v1 [cs.LG])</h2>
<h3>Sharan Raja, Rudraksh Tuwani</h3>
<p>Manual annotation of ICD-9 codes is a time consuming and error-prone process.
Deep learning based systems tackling the problem of automated ICD-9 coding have
achieved competitive performance. Given the increased proliferation of
electronic medical records, such automated systems are expected to eventually
replace human coders. In this work, we investigate how a simple typo-based
adversarial attack strategy can impact the performance of state-of-the-art
models for the task of predicting the top 50 most frequent ICD-9 codes from
discharge summaries. Preliminary results indicate that a malicious adversary,
using gradient information, can craft specific perturbations, that appear as
regular human typos, for less than 3% of words in the discharge summary to
significantly affect the performance of the baseline model.
</p>
<a href="http://arxiv.org/abs/2009.13720">arXiv:2009.13720</a> [<a href="http://arxiv.org/pdf/2009.13720">pdf</a>]

<h2>A Flow Base Bi-path Network for Cross-scene Video Crowd Understanding in Aerial View. (arXiv:2009.13723v1 [cs.CV])</h2>
<h3>Zhiyuan Zhao, Tao Han, Junyu Gao, Qi Wang, Xuelong Li</h3>
<p>Drones shooting can be applied in dynamic traffic monitoring, object
detecting and tracking, and other vision tasks. The variability of the shooting
location adds some intractable challenges to these missions, such as varying
scale, unstable exposure, and scene migration. In this paper, we strive to
tackle the above challenges and automatically understand the crowd from the
visual data collected from drones. First, to alleviate the background noise
generated in cross-scene testing, a double-stream crowd counting model is
proposed, which extracts optical flow and frame difference information as an
additional branch. Besides, to improve the model's generalization ability at
different scales and time, we randomly combine a variety of data transformation
methods to simulate some unseen environments. To tackle the crowd density
estimation problem under extreme dark environments, we introduce synthetic data
generated by game Grand Theft Auto V(GTAV). Experiment results show the
effectiveness of the virtual data. Our method wins the challenge with a mean
absolute error (MAE) of 12.70. Moreover, a comprehensive ablation study is
conducted to explore each component's contribution.
</p>
<a href="http://arxiv.org/abs/2009.13723">arXiv:2009.13723</a> [<a href="http://arxiv.org/pdf/2009.13723">pdf</a>]

<h2>One Person, One Model, One World: Learning Continual User Representation without Forgetting. (arXiv:2009.13724v1 [cs.IR])</h2>
<h3>Fajie Yuan, Guoxiao Zhang, Alexandros Karatzoglou, Xiangnan He, Joemon Jose, Beibei Kong, Yudong Li</h3>
<p>Learning generic user representations which can then be applied to other
user-related tasks (e.g., profile prediction and recommendation) has recently
attracted much attention. Existing approaches often derive an individual set of
model parameters for each task by training their own data. However, the
representation of a user usually has some potential commonalities. As such,
these separately trained representations could be suboptimal in performance as
well as inefficient in terms of parameter sharing. In this paper, we delve on
the research to continually learn user representations task by task, whereby
new tasks are learned while using parameters from old ones. A new problem
arises since when new tasks are trained, previously learned parameters are very
likely to be modified, and thus, an artificial neural network (ANN)-based model
may lose its capacity to serve for well-trained previous tasks forever, termed
as catastrophic forgetting. To address this issue, we present Conure which is
the first continual, or lifelong, user representation learner -- i.e., learning
new tasks over time without forgetting old ones. Specifically, we propose
iteratively removing unimportant weights by pruning on a well-optimized
backbone representation model, enlightened by fact that neural network models
are highly over-parameterized. Then, we are able to learn a coming task by
sharing previous parameters and training new ones only on the empty space after
pruning. We conduct extensive experiments on two real-world datasets across
nine tasks and demonstrate that Conure performs largely better than common
models without purposely preserving such old "knowledge", and is competitive or
sometimes better than models which are trained either individually for each
task or simultaneously by preparing all task data together.
</p>
<a href="http://arxiv.org/abs/2009.13724">arXiv:2009.13724</a> [<a href="http://arxiv.org/pdf/2009.13724">pdf</a>]

<h2>Learning Skills to Patch Plans Based on Inaccurate Models. (arXiv:2009.13732v1 [cs.RO])</h2>
<h3>Alex LaGrassa, Steven Lee, Oliver Kroemer</h3>
<p>Planners using accurate models can be effective for accomplishing
manipulation tasks in the real world, but are typically highly specialized and
require significant fine-tuning to be reliable. Meanwhile, learning is useful
for adaptation, but can require a substantial amount of data collection. In
this paper, we propose a method that improves the efficiency of sub-optimal
planners with approximate but simple and fast models by switching to a
model-free policy when unexpected transitions are observed. Unlike previous
work, our method specifically addresses when the planner fails due to
transition model error by patching with a local policy only where needed.
First, we use a sub-optimal model-based planner to perform a task until model
failure is detected. Next, we learn a local model-free policy from expert
demonstrations to complete the task in regions where the model failed. To show
the efficacy of our method, we perform experiments with a shape insertion
puzzle and compare our results to both pure planning and imitation learning
approaches. We then apply our method to a door opening task. Our experiments
demonstrate that our patch-enhanced planner performs more reliably than pure
planning and with lower overall sample complexity than pure imitation learning.
</p>
<a href="http://arxiv.org/abs/2009.13732">arXiv:2009.13732</a> [<a href="http://arxiv.org/pdf/2009.13732">pdf</a>]

<h2>MetaMix: Improved Meta-Learning with Interpolation-based Consistency Regularization. (arXiv:2009.13735v1 [cs.CV])</h2>
<h3>Yangbin Chen, Yun Ma, Tom Ko, Jianping Wang, Qing Li</h3>
<p>Model-Agnostic Meta-Learning (MAML) and its variants are popular few-shot
classification methods. They train an initializer across a variety of sampled
learning tasks (also known as episodes) such that the initialized model can
adapt quickly to new tasks. However, current MAML-based algorithms have
limitations in forming generalizable decision boundaries. In this paper, we
propose an approach called MetaMix. It generates virtual feature-target pairs
within each episode to regularize the backbone models. MetaMix can be
integrated with any of the MAML-based algorithms and learn the decision
boundaries generalizing better to new tasks. Experiments on the mini-ImageNet,
CUB, and FC100 datasets show that MetaMix improves the performance of
MAML-based algorithms and achieves state-of-the-art result when integrated with
Meta-Transfer Learning.
</p>
<a href="http://arxiv.org/abs/2009.13735">arXiv:2009.13735</a> [<a href="http://arxiv.org/pdf/2009.13735">pdf</a>]

<h2>Lucid Dreaming for Experience Replay: Refreshing Past States with the Current Policy. (arXiv:2009.13736v1 [cs.LG])</h2>
<h3>Yunshu Du, Garrett Warnell, Assefaw Gebremedhin, Peter Stone, Matthew E. Taylor</h3>
<p>Experience replay (ER) improves the data efficiency of off-policy
reinforcement learning (RL) algorithms by allowing an agent to store and reuse
its past experiences in a replay buffer. While many techniques have been
proposed to enhance ER by biasing how experiences are sampled from the buffer,
thus far they have not considered strategies for refreshing experiences inside
the buffer. In this work, we introduce Lucid Dreaming for Experience Replay
(LiDER), a conceptually new framework that allows replay experiences to be
refreshed by leveraging the agent's current policy. LiDER 1) moves an agent
back to a past state; 2) lets the agent try following its current policy to
execute different actions---as if the agent were "dreaming" about the past, but
is aware of the situation and can control the dream to encounter new
experiences; and 3) stores and reuses the new experience if it turned out
better than what the agent previously experienced, i.e., to refresh its
memories. LiDER is designed to be easily incorporated into off-policy,
multi-worker RL algorithms that use ER; we present in this work a case study of
applying LiDER to an actor-critic based algorithm. Results show LiDER
consistently improves performance over the baseline in four Atari 2600 games.
Our open-source implementation of LiDER and the data used to generate all plots
in this paper are available at
github.com/duyunshu/lucid-dreaming-for-exp-replay.
</p>
<a href="http://arxiv.org/abs/2009.13736">arXiv:2009.13736</a> [<a href="http://arxiv.org/pdf/2009.13736">pdf</a>]

<h2>Sequential Reinforced 360-Degree Video Adaptive Streaming with Cross-user Attentive Network. (arXiv:2009.13737v1 [cs.MM])</h2>
<h3>Jun Fu, Zhibo Chen, Xiaoming Chen, Weiping Li</h3>
<p>In the tile-based 360-degree video streaming, predicting user's future
viewpoints and developing adaptive bitrate (ABR) algorithms are essential for
optimizing user's quality of experience (QoE). Traditional single-user based
viewpoint prediction methods fail to achieve good performance in long-term
prediction, and the recently proposed reinforcement learning (RL) based ABR
schemes applied in traditional video streaming can not be directly applied in
the tile-based 360-degree video streaming due to the exponential action space.
Therefore, we propose a sequential reinforced 360-degree video streaming scheme
with cross-user attentive network. Firstly, considering different users may
have the similar viewing preference on the same video, we propose a cross-user
attentive network (CUAN), boosting the performance of long-term viewpoint
prediction by selectively utilizing cross-user information. Secondly, we
propose a sequential RL-based (360SRL) ABR approach, transforming action space
size of each decision step from exponential to linear via introducing a
sequential decision structure. We evaluate the proposed CUAN and 360SRL using
trace-driven experiments and experimental results demonstrate that CUAN and
360SRL outperform existing viewpoint prediction and ABR approaches with a
noticeable margin.
</p>
<a href="http://arxiv.org/abs/2009.13737">arXiv:2009.13737</a> [<a href="http://arxiv.org/pdf/2009.13737">pdf</a>]

<h2>SwiftFace: Real-Time Face Detection. (arXiv:2009.13743v1 [cs.CV])</h2>
<h3>Leonardo Ramos, Bernardo Morales</h3>
<p>Computer vision is a field of artificial intelligence that trains computers
to interpret the visual world in a way similar to that of humans. Due to the
rapid advancements in technology and the increasing availability of
sufficiently large training datasets, the topics within computer vision have
experienced a steep growth in the last decade. Among them, one of the most
promising fields is face detection. Being used daily in a wide variety of
fields; from mobile apps and augmented reality for entertainment purposes, to
social studies and security cameras; designing high-performance models for face
detection is crucial. On top of that, with the aforementioned growth in face
detection technologies, precision and accuracy are no longer the only relevant
factors: for real-time face detection, speed of detection is essential.
SwiftFace is a novel deep learning model created solely to be a fast face
detection model. By focusing only on detecting faces, SwiftFace performs 30%
faster than current state-of-the-art face detection models. Code available at
https://github.com/leo7r/swiftface
</p>
<a href="http://arxiv.org/abs/2009.13743">arXiv:2009.13743</a> [<a href="http://arxiv.org/pdf/2009.13743">pdf</a>]

<h2>Dynamic Slicing for Deep Neural Networks. (arXiv:2009.13747v1 [cs.SE])</h2>
<h3>Ziqi Zhang, Yuanchun Li, Yao Guo, Xiangqun Chen, Yunxin Liu</h3>
<p>Program slicing has been widely applied in a variety of software engineering
tasks. However, existing program slicing techniques only deal with traditional
programs that are constructed with instructions and variables, rather than
neural networks that are composed of neurons and synapses. In this paper, we
propose NNSlicer, the first approach for slicing deep neural networks based on
data flow analysis. Our method understands the reaction of each neuron to an
input based on the difference between its behavior activated by the input and
the average behavior over the whole dataset. Then we quantify the neuron
contributions to the slicing criterion by recursively backtracking from the
output neurons, and calculate the slice as the neurons and the synapses with
larger contributions. We demonstrate the usefulness and effectiveness of
NNSlicer with three applications, including adversarial input detection, model
pruning, and selective model protection. In all applications, NNSlicer
significantly outperforms other baselines that do not rely on data flow
analysis.
</p>
<a href="http://arxiv.org/abs/2009.13747">arXiv:2009.13747</a> [<a href="http://arxiv.org/pdf/2009.13747">pdf</a>]

<h2>Geometric Loss for Deep Multiple Sclerosis lesion Segmentation. (arXiv:2009.13755v1 [cs.CV])</h2>
<h3>Hang Zhang, Jinwei Zhang, Rongguang Wang, Qihao Zhang, Susan A. Gauthier, Pascal Spincemaille, Thanh D. Nguyen, Yi Wang</h3>
<p>Multiple sclerosis (MS) lesions occupy a small fraction of the brain volume,
and are heterogeneous with regards to shape, size and locations, which poses a
great challenge for training deep learning based segmentation models. We
proposed a new geometric loss formula to address the data imbalance and exploit
the geometric property of MS lesions. We showed that traditional region-based
and boundary-aware loss functions can be associated with the formula. We
further develop and instantiate two loss functions containing first- and
second-order geometric information of lesion regions to enforce regularization
on optimizing deep segmentation models. Experimental results on two MS lesion
datasets with different scales, acquisition protocols and resolutions
demonstrated the superiority of our proposed methods compared to other
state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2009.13755">arXiv:2009.13755</a> [<a href="http://arxiv.org/pdf/2009.13755">pdf</a>]

<h2>Fast Design Space Adaptation with Deep Reinforcement Learning for Analog Circuit Sizing. (arXiv:2009.13772v1 [cs.LG])</h2>
<h3>Kevin-CY Tsai, Kai-En Yang, Hung-Hao Shen, Mike Jiang, Fammy Tsai, CA Wang, Yiju Ting, Jason Yeh, Citi Lai</h3>
<p>We present a novel framework for design space search on analog circuit sizing
using deep reinforcement learning (DRL). Nowadays, analog circuit design is a
manual routine that requires heavy design efforts due to the absence of
automation tools, motivating the urge to develop one. Prior approaches cast
this process as an optimization problem. They use global search strategies
based on DRL with complex network architectures. Nonetheless, the models are
hard to converge and neglected various working conditions of PVT (process,
voltage, temperature).In this work, we reduce the problem to a constraint
satisfaction problem, where a local strategy is adopted. Thus, a simple
feed-forward network with few layers can be used to implement a model-based
reinforcement learning agent. To evaluate the value of the our framework in
production, we cooperate with R&amp;Ds in an IC design company. On circuits with
TSMC advanced 5 and 6nm process, our agents can deliver PPA (performance,
power, area) beyond human level. Furthermore, the product will be taped out in
the near future.
</p>
<a href="http://arxiv.org/abs/2009.13772">arXiv:2009.13772</a> [<a href="http://arxiv.org/pdf/2009.13772">pdf</a>]

<h2>Cross Learning in Deep Q-Networks. (arXiv:2009.13780v1 [cs.AI])</h2>
<h3>Xing Wang, Alexander Vinel</h3>
<p>In this work, we propose a novel cross Q-learning algorithm, aim at
alleviating the well-known overestimation problem in value-based reinforcement
learning methods, particularly in the deep Q-networks where the overestimation
is exaggerated by function approximation errors. Our algorithm builds on double
Q-learning, by maintaining a set of parallel models and estimate the Q-value
based on a randomly selected network, which leads to reduced overestimation
bias as well as the variance. We provide empirical evidence on the advantages
of our method by evaluating on some benchmark environment, the experimental
results demonstrate significant improvement of performance in reducing the
overestimation bias and stabilizing the training, further leading to better
derived policies.
</p>
<a href="http://arxiv.org/abs/2009.13780">arXiv:2009.13780</a> [<a href="http://arxiv.org/pdf/2009.13780">pdf</a>]

<h2>Micro-Facial Expression Recognition in Video Based on Optimal Convolutional Neural Network (MFEOCNN) Algorithm. (arXiv:2009.13792v1 [cs.CV])</h2>
<h3>S. D. Lalitha, K. K. Thyagharajan</h3>
<p>Facial expression is a standout amongst the most imperative features of human
emotion recognition. For demonstrating the emotional states facial expressions
are utilized by the people. In any case, recognition of facial expressions has
persisted a testing and intriguing issue with regards to PC vision. Recognizing
the Micro-Facial expression in video sequence is the main objective of the
proposed approach. For efficient recognition, the proposed method utilizes the
optimal convolution neural network. Here the proposed method considering the
input dataset is the CK+ dataset. At first, by means of Adaptive median
filtering preprocessing is performed in the input image. From the preprocessed
output, the extracted features are Geometric features, Histogram of Oriented
Gradients features and Local binary pattern features. The novelty of the
proposed method is, with the help of Modified Lion Optimization (MLO)
algorithm, the optimal features are selected from the extracted features. In a
shorter computational time, it has the benefits of rapidly focalizing and
effectively acknowledging with the aim of getting an overall arrangement or
idea. Finally, the recognition is done by Convolution Neural network (CNN).
Then the performance of the proposed MFEOCNN method is analysed in terms of
false measures and recognition accuracy. This kind of emotion recognition is
mainly used in medicine, marketing, E-learning, entertainment, law and
monitoring. From the simulation, we know that the proposed approach achieves
maximum recognition accuracy of 99.2% with minimum Mean Absolute Error (MAE)
value. These results are compared with the existing for MicroFacial Expression
Based Deep-Rooted Learning (MFEDRL), Convolutional Neural Network with Lion
Optimization (CNN+LO) and Convolutional Neural Network (CNN) without
optimization. The simulation of the proposed method is done in the working
platform of MATLAB.
</p>
<a href="http://arxiv.org/abs/2009.13792">arXiv:2009.13792</a> [<a href="http://arxiv.org/pdf/2009.13792">pdf</a>]

<h2>BAMSProd: A Step towards Generalizing the Adaptive Optimization Methods to Deep Binary Model. (arXiv:2009.13799v1 [cs.CV])</h2>
<h3>Junjie Liu, Dongchao Wen, Deyu Wang, Wei Tao, Tse-Wei Chen, Kinya Osa, Masami Kato</h3>
<p>Recent methods have significantly reduced the performance degradation of
Binary Neural Networks (BNNs), but guaranteeing the effective and efficient
training of BNNs is an unsolved problem. The main reason is that the estimated
gradients produced by the Straight-Through-Estimator (STE) mismatches with the
gradients of the real derivatives. In this paper, we provide an explicit convex
optimization example where training the BNNs with the traditionally adaptive
optimization methods still faces the risk of non-convergence, and identify that
constraining the range of gradients is critical for optimizing the deep binary
model to avoid highly suboptimal solutions. For solving above issues, we
propose a BAMSProd algorithm with a key observation that the convergence
property of optimizing deep binary model is strongly related to the
quantization errors. In brief, it employs an adaptive range constraint via an
errors measurement for smoothing the gradients transition while follows the
exponential moving strategy from AMSGrad to avoid errors accumulation during
the optimization. The experiments verify the corollary of theoretical
convergence analysis, and further demonstrate that our optimization method can
speed up the convergence about 1:2x and boost the performance of BNNs to a
significant level than the specific binary optimizer about 3:7%, even in a
highly non-convex optimization problem.
</p>
<a href="http://arxiv.org/abs/2009.13799">arXiv:2009.13799</a> [<a href="http://arxiv.org/pdf/2009.13799">pdf</a>]

<h2>Framework for Designing Filters of Spectral Graph Convolutional Neural Networks in the Context of Regularization Theory. (arXiv:2009.13801v1 [cs.LG])</h2>
<h3>Asif Salim, Sumitra S</h3>
<p>Graph convolutional neural networks (GCNNs) have been widely used in graph
learning. It has been observed that the smoothness functional on graphs can be
defined in terms of the graph Laplacian. This fact points out in the direction
of using Laplacian in deriving regularization operators on graphs and its
consequent use with spectral GCNN filter designs. In this work, we explore the
regularization properties of graph Laplacian and proposed a generalized
framework for regularized filter designs in spectral GCNNs. We found that the
filters used in many state-of-the-art GCNNs can be derived as a special case of
the framework we developed. We designed new filters that are associated with
well-defined regularization behavior and tested their performance on
semi-supervised node classification tasks. Their performance was found to be
superior to that of the other state-of-the-art techniques.
</p>
<a href="http://arxiv.org/abs/2009.13801">arXiv:2009.13801</a> [<a href="http://arxiv.org/pdf/2009.13801">pdf</a>]

<h2>Self-grouping Convolutional Neural Networks. (arXiv:2009.13803v1 [cs.LG])</h2>
<h3>Qingbei Guo, Xiao-Jun Wu, Josef Kittler, Zhiquan Feng</h3>
<p>Although group convolution operators are increasingly used in deep
convolutional neural networks to improve the computational efficiency and to
reduce the number of parameters, most existing methods construct their group
convolution architectures by a predefined partitioning of the filters of each
convolutional layer into multiple regular filter groups with an equal spatial
group size and data-independence, which prevents a full exploitation of their
potential. To tackle this issue, we propose a novel method of designing
self-grouping convolutional neural networks, called SG-CNN, in which the
filters of each convolutional layer group themselves based on the similarity of
their importance vectors. Concretely, for each filter, we first evaluate the
importance value of their input channels to identify the importance vectors,
and then group these vectors by clustering. Using the resulting
\emph{data-dependent} centroids, we prune the less important connections, which
implicitly minimizes the accuracy loss of the pruning, thus yielding a set of
\emph{diverse} group convolution filters. Subsequently, we develop two
fine-tuning schemes, i.e. (1) both local and global fine-tuning and (2) global
only fine-tuning, which experimentally deliver comparable results, to recover
the recognition capacity of the pruned network. Comprehensive experiments
carried out on the CIFAR-10/100 and ImageNet datasets demonstrate that our
self-grouping convolution method adapts to various state-of-the-art CNN
architectures, such as ResNet and DenseNet, and delivers superior performance
in terms of compression ratio, speedup and recognition accuracy. We demonstrate
the ability of SG-CNN to generalise by transfer learning, including domain
adaption and object detection, showing competitive results. Our source code is
available at https://github.com/QingbeiGuo/SG-CNN.git.
</p>
<a href="http://arxiv.org/abs/2009.13803">arXiv:2009.13803</a> [<a href="http://arxiv.org/pdf/2009.13803">pdf</a>]

<h2>Current Time Series Anomaly Detection Benchmarks are Flawed and are Creating the Illusion of Progress. (arXiv:2009.13807v1 [cs.LG])</h2>
<h3>Renjie Wu, Eamonn J. Keogh</h3>
<p>Time series anomaly detection has been a perennially important topic in data
science, with papers dating back to the 1950s. However, in recent years there
has been an explosion of interest in this topic, much of it driven by the
success of deep learning in other domains and for other time series tasks. Most
of these papers test on one or more of a handful of popular benchmark datasets,
created by Yahoo, Numenta, NASA, etc. In this work we make a surprising claim.
The majority of the individual exemplars in these datasets suffer from one or
more of four flaws. Because of these four flaws, we believe that many published
comparisons of anomaly detection algorithms may be unreliable, and more
importantly, much of the apparent progress in recent years may be illusionary.
In addition to demonstrating these claims, with this paper we introduce the UCR
Time Series Anomaly Datasets. We believe that this resource will perform a
similar role as the UCR Time Series Classification Archive, by providing the
community with a benchmark that allows meaningful comparisons between
approaches and a meaningful gauge of overall progress.
</p>
<a href="http://arxiv.org/abs/2009.13807">arXiv:2009.13807</a> [<a href="http://arxiv.org/pdf/2009.13807">pdf</a>]

<h2>An Image Processing Pipeline for Automated Packaging Structure Recognition. (arXiv:2009.13824v1 [cs.CV])</h2>
<h3>Laura D&#xf6;rr, Felix Brandt, Martin Pouls, Alexander Naumann</h3>
<p>Dispatching and receiving logistics goods, as well as transportation itself,
involve a high amount of manual efforts. The transported goods, including their
packaging and labeling, need to be double-checked, verified or recognized at
many supply chain network points. These processes hold automation potentials,
which we aim to exploit using computer vision techniques. More precisely, we
propose a cognitive system for the fully automated recognition of packaging
structures for standardized logistics shipments based on single RGB images. Our
contribution contains descriptions of a suitable system design and its
evaluation on relevant real-world data. Further, we discuss our algorithmic
choices.
</p>
<a href="http://arxiv.org/abs/2009.13824">arXiv:2009.13824</a> [<a href="http://arxiv.org/pdf/2009.13824">pdf</a>]

<h2>An information theoretic network approach to socioeconomic correlations. (arXiv:2009.13825v1 [physics.soc-ph])</h2>
<h3>Alec Kirkley</h3>
<p>Due to its wide reaching implications for everything from identifying
hotspots of income inequality to political redistricting, there is a rich body
of literature across the sciences quantifying spatial patterns in socioeconomic
data. In particular, the variability of indicators relevant to social and
economic well-being between localized populations is of great interest, as it
pertains to the spatial manifestations of inequality and segregation. However,
heterogeneity in population density, sensitivity of statistical analyses to
spatial aggregation, and the importance of pre-drawn political boundaries for
policy intervention may decrease the efficacy and relevance of existing methods
for analyzing spatial socioeconomic data. Additionally, these measures commonly
lack either a framework for comparing results for qualitative and quantitative
data on the same scale, or a mechanism for generalization to multi-region
correlations. To mitigate these issues associated with traditional spatial
measures, here we view local deviations in socioeconomic variables from a
topological lens rather than a spatial one, and use a novel information
theoretic network approach based on the Generalized Jensen Shannon Divergence
to distinguish distributional quantities across adjacent regions. We apply our
methodology in a series of experiments to study the network of neighboring
census tracts in the continental US, quantifying the decay in two-point
distributional correlations across the network, examining the county-level
socioeconomic disparities induced from the aggregation of tracts, and
constructing an algorithm for the division of a city into homogeneous clusters.
These results provide a new framework for analyzing the variation of attributes
across regional populations, and shed light on new, universal patterns in
socioeconomic attributes.
</p>
<a href="http://arxiv.org/abs/2009.13825">arXiv:2009.13825</a> [<a href="http://arxiv.org/pdf/2009.13825">pdf</a>]

<h2>EEMC: Embedding Enhanced Multi-tag Classification. (arXiv:2009.13826v1 [cs.LG])</h2>
<h3>Yanlin Li, Shi An, Ruisheng Zhang</h3>
<p>The recently occurred representation learning make an attractive performance
in NLP and complex network, it is becoming a fundamental technology in machine
learning and data mining. How to use representation learning to improve the
performance of classifiers is a very significance research direction. We using
representation learning technology to map raw data(node of graph) to a
low-dimensional feature space. In this space, each raw data obtained a lower
dimensional vector representation, we do some simple linear operations for
those vectors to produce some virtual data, using those vectors and virtual
data to training multi-tag classifier. After that we measured the performance
of classifier by F1 score(Macro% F1 and Micro% F1). Our method make Macro F1
rise from 28 % - 450% and make average F1 score rise from 12 % - 224%. By
contrast, we trained the classifier directly with the lower dimensional vector,
and measured the performance of classifiers. We validate our algorithm on three
public data sets, we found that the virtual data helped the classifier greatly
improve the F1 score. Therefore, our algorithm is a effective way to improve
the performance of classifier. These result suggest that the virtual data
generated by simple linear operation, in representation space, still retains
the information of the raw data. It's also have great significance to the
learning of small sample data sets.
</p>
<a href="http://arxiv.org/abs/2009.13826">arXiv:2009.13826</a> [<a href="http://arxiv.org/pdf/2009.13826">pdf</a>]

<h2>Testing for Normality with Neural Networks. (arXiv:2009.13831v1 [stat.ML])</h2>
<h3>Milos Simic</h3>
<p>In this paper, we treat the problem of testing for normality as a binary
classification problem and construct a feedforward neural network that can
successfully detect normal distributions by inspecting small samples from them.
The numerical experiments conducted on small samples with no more than 100
elements indicated that the neural network which we trained was more accurate
and far more powerful than the most frequently used and most powerful standard
tests of normality: Shapiro-Wilk, Anderson-Darling, Lilliefors and
Jarque-Berra, as well as the kernel tests of goodness-of-fit. The neural
network had the AUROC score of almost 1, which corresponds to the perfect
binary classifier. Additionally, the network's accuracy was higher than 96% on
a set of larger samples with 250-1000 elements. Since the normality of data is
an assumption of numerous techniques for analysis and inference, the neural
network constructed in this study has a very high potential for use in everyday
practice of statistics, data analysis and machine learning in both science and
industry.
</p>
<a href="http://arxiv.org/abs/2009.13831">arXiv:2009.13831</a> [<a href="http://arxiv.org/pdf/2009.13831">pdf</a>]

<h2>imdpGAN: Generating Private and Specific Data with Generative Adversarial Networks. (arXiv:2009.13839v1 [cs.CV])</h2>
<h3>Saurabh Gupta, Arun Balaji Buduru, Ponnurangam Kumaraguru</h3>
<p>Generative Adversarial Network (GAN) and its variants have shown promising
results in generating synthetic data. However, the issues with GANs are: (i)
the learning happens around the training samples and the model often ends up
remembering them, consequently, compromising the privacy of individual samples
- this becomes a major concern when GANs are applied to training data including
personally identifiable information, (ii) the randomness in generated data -
there is no control over the specificity of generated samples. To address these
issues, we propose imdpGAN - an information maximizing differentially private
Generative Adversarial Network. It is an end-to-end framework that
simultaneously achieves privacy protection and learns latent representations.
With experiments on MNIST dataset, we show that imdpGAN preserves the privacy
of the individual data point, and learns latent codes to control the
specificity of the generated samples. We perform binary classification on digit
pairs to show the utility versus privacy trade-off. The classification accuracy
decreases as we increase privacy levels in the framework. We also
experimentally show that the training process of imdpGAN is stable but
experience a 10-fold time increase as compared with other GAN frameworks.
Finally, we extend imdpGAN framework to CelebA dataset to show how the privacy
and learned representations can be used to control the specificity of the
output.
</p>
<a href="http://arxiv.org/abs/2009.13839">arXiv:2009.13839</a> [<a href="http://arxiv.org/pdf/2009.13839">pdf</a>]

<h2>Research and Education Towards Smart and Sustainable World. (arXiv:2009.13849v1 [cs.AI])</h2>
<h3>Jukka Riekki, Aarne M&#xe4;mmel&#xe4;</h3>
<p>We propose a vision for directing research and education in the ICT field.
Our Smart and Sustainable World vision targets at prosperity for the people and
the planet through better awareness and control of both human-made and natural
environment. The needs of the society, individuals, and industries are
fulfilled with intelligent systems that sense their environment, make proactive
decisions on actions advancing their goals, and perform the actions on the
environment. We emphasize artificial intelligence, feedback loops, human
acceptance and control, intelligent use of basic resources, performance
parameters, mission-oriented interdisciplinary research, and a holistic systems
view complementing the conventional analytical reductive view as a research
paradigm especially for complex problems. To serve a broad audience, we explain
these concepts and list the essential literature. We suggest planning research
and education by specifying, in a step-wise manner, scenarios, performance
criteria, system models, research problems and education content, resulting in
common goals and a coherent project portfolio as well as education curricula.
Research and education produce feedback to support evolutionary development and
encourage creativity in research. Finally, we propose concrete actions for
realizing this approach.
</p>
<a href="http://arxiv.org/abs/2009.13849">arXiv:2009.13849</a> [<a href="http://arxiv.org/pdf/2009.13849">pdf</a>]

<h2>Loop-box: Multi-Agent Direct SLAM Triggered by Single Loop Closure for Large-Scale Mapping. (arXiv:2009.13851v1 [cs.RO])</h2>
<h3>M Usman Maqbool Bhutta, Manohar Kuse, Rui Fan, Yanan Liu, Ming Liu</h3>
<p>In this paper, we present a multi-agent framework for real-time large-scale
3D reconstruction applications. In SLAM, researchers usually build and update a
3D map after applying non-linear pose graph optimization techniques. Moreover,
many multi-agent systems are prevalently using odometry information from
additional sensors. These methods generally involve intensive computer vision
algorithms and are tightly coupled with various sensors. We develop a generic
method for the keychallenging scenarios in multi-agent 3D mapping based on
different camera systems. The proposed framework performs actively in terms of
localizing each agent after the first loop closure between them. It is shown
that the proposed system only uses monocular cameras to yield real-time
multi-agent large-scale localization and 3D global mapping. Based on the
initial matching, our system can calculate the optimal scale difference between
multiple 3D maps and then estimate an accurate relative pose transformation for
large-scale global mapping.
</p>
<a href="http://arxiv.org/abs/2009.13851">arXiv:2009.13851</a> [<a href="http://arxiv.org/pdf/2009.13851">pdf</a>]

<h2>Machine Learning for Semi-Automated Meteorite Recovery. (arXiv:2009.13852v1 [astro-ph.EP])</h2>
<h3>Seamus Anderson, Martin Towner, Phil Bland, Christopher Haikings, William Volante, Eleanor Sansom, Hadrien Devillepoix, Patrick Shober, Benjamin Hartig, Martin Cupak, Trent Jansen-Sturgeon, Robert Howie, Gretchen Benedix, Geoff Deacon</h3>
<p>We present a novel methodology for recovering meteorite falls observed and
constrained by fireball networks, using drones and machine learning algorithms.
This approach uses images of the local terrain for a given fall site to train
an artificial neural network, designed to detect meteorite candidates. We have
field tested our methodology to show a meteorite detection rate between 75-97%,
while also providing an efficient mechanism to eliminate false-positives. Our
tests at a number of locations within Western Australia also showcase the
ability for this training scheme to generalize a model to learn localized
terrain features. Our model-training approach was also able to correctly
identify 3 meteorites in their native fall sites, that were found using
traditional searching techniques. Our methodology will be used to recover
meteorite falls in a wide range of locations within globe-spanning fireball
networks.
</p>
<a href="http://arxiv.org/abs/2009.13852">arXiv:2009.13852</a> [<a href="http://arxiv.org/pdf/2009.13852">pdf</a>]

<h2>Multi-objective Reinforcement Learning based approach for User-Centric Power Optimization in Smart Home Environments. (arXiv:2009.13854v1 [cs.AI])</h2>
<h3>Saurabh Gupta, Siddhant Bhambri, Karan Dhingra, Arun Balaji Buduru, Ponnurangam Kumaraguru</h3>
<p>Smart homes require every device inside them to be connected with each other
at all times, which leads to a lot of power wastage on a daily basis. As the
devices inside a smart home increase, it becomes difficult for the user to
control or operate every individual device optimally. Therefore, users
generally rely on power management systems for such optimization but often are
not satisfied with the results. In this paper, we present a novel
multi-objective reinforcement learning framework with two-fold objectives of
minimizing power consumption and maximizing user satisfaction. The framework
explores the trade-off between the two objectives and converges to a better
power management policy when both objectives are considered while finding an
optimal policy. We experiment on real-world smart home data, and show that the
multi-objective approaches: i) establish trade-off between the two objectives,
ii) achieve better combined user satisfaction and power consumption than
single-objective approaches. We also show that the devices that are used
regularly and have several fluctuations in device modes at regular intervals
should be targeted for optimization, and the experiments on data from other
smart homes fetch similar results, hence ensuring transfer-ability of the
proposed framework.
</p>
<a href="http://arxiv.org/abs/2009.13854">arXiv:2009.13854</a> [<a href="http://arxiv.org/pdf/2009.13854">pdf</a>]

<h2>Fake News Spreader Detection on Twitter using Character N-Grams. Notebook for PAN at CLEF 2020. (arXiv:2009.13859v1 [cs.CL])</h2>
<h3>Inna Vogel, Meghana Meghana</h3>
<p>The authors of fake news often use facts from verified news sources and mix
them with misinformation to create confusion and provoke unrest among the
readers. The spread of fake news can thereby have serious implications on our
society. They can sway political elections, push down the stock price or crush
reputations of corporations or public figures. Several websites have taken on
the mission of checking rumors and allegations, but are often not fast enough
to check the content of all the news being disseminated. Especially social
media websites have offered an easy platform for the fast propagation of
information. Towards limiting fake news from being propagated among social
media users, the task of this year's PAN 2020 challenge lays the focus on the
fake news spreaders. The aim of the task is to determine whether it is possible
to discriminate authors that have shared fake news in the past from those that
have never done it. In this notebook, we describe our profiling system for the
fake news detection task on Twitter. For this, we conduct different feature
extraction techniques and learning experiments from a multilingual perspective,
namely English and Spanish. Our final submitted systems use character n-grams
as features in combination with a linear SVM for English and Logistic
Regression for the Spanish language. Our submitted models achieve an overall
accuracy of 73% and 79% on the English and Spanish official test set,
respectively. Our experiments show that it is difficult to differentiate
solidly fake news spreaders on Twitter from users who share credible
information leaving room for further investigations. Our model ranked 3rd out
of 72 competitors.
</p>
<a href="http://arxiv.org/abs/2009.13859">arXiv:2009.13859</a> [<a href="http://arxiv.org/pdf/2009.13859">pdf</a>]

<h2>Where is the Model Looking At?--Concentrate and Explain the Network Attention. (arXiv:2009.13862v1 [cs.CV])</h2>
<h3>Wenjia Xu, Jiuniu Wang, Yang Wang, Guangluan Xu, Wei Dai, Yirong Wu</h3>
<p>Image classification models have achieved satisfactory performance on many
datasets, sometimes even better than human. However, The model attention is
unclear since the lack of interpretability. This paper investigates the
fidelity and interpretability of model attention. We propose an Explainable
Attribute-based Multi-task (EAT) framework to concentrate the model attention
on the discriminative image area and make the attention interpretable. We
introduce attributes prediction to the multi-task learning network, helping the
network to concentrate attention on the foreground objects. We generate
attribute-based textual explanations for the network and ground the attributes
on the image to show visual explanations. The multi-model explanation can not
only improve user trust but also help to find the weakness of network and
dataset. Our framework can be generalized to any basic model. We perform
experiments on three datasets and five basic models. Results indicate that the
EAT framework can give multi-modal explanations that interpret the network
decision. The performance of several recognition approaches is improved by
guiding network attention.
</p>
<a href="http://arxiv.org/abs/2009.13862">arXiv:2009.13862</a> [<a href="http://arxiv.org/pdf/2009.13862">pdf</a>]

<h2>Online Trainable Wireless Link Quality Prediction System using Camera Imagery. (arXiv:2009.13864v1 [cs.NI])</h2>
<h3>Sohei Itahara, Takayuki Nishio, Masahiro Morikura, Koji Yamamoto</h3>
<p>Machine-learning-based prediction of future wireless link quality is an
emerging technique that can potentially improve the reliability of wireless
communications, especially at higher frequencies (e.g., millimeter-wave and
terahertz technologies), through predictive handover and beamforming to solve
line-of-sight (LOS) blockage problem. In this study, a real-time online
trainable wireless link quality prediction system was proposed; the system was
implemented with commercially available laptops. The proposed system collects
datasets, updates a model, and infers the received power in real-time. The
experimental evaluation was conducted using 5 GHz Wi-Fi, where received signal
strength could be degraded by 10 dB when the LOS path was blocked by large
obstacles. The experimental results demonstrate that the prediction model is
updated in real-time, adapts to the change in environment, and predicts the
time-varying Wi-Fi received power accurately.
</p>
<a href="http://arxiv.org/abs/2009.13864">arXiv:2009.13864</a> [<a href="http://arxiv.org/pdf/2009.13864">pdf</a>]

<h2>MAB-based Client Selection for Federated Learning with Uncertain Resources in Mobile Networks. (arXiv:2009.13879v1 [cs.NI])</h2>
<h3>Naoya Yoshida, Takayuki Nishio, Masahiro Morikura, Koji Yamamoto</h3>
<p>This paper proposes a client selection method for federated learning (FL)
when the computation and communication resource of clients cannot be estimated;
the method trains a machine learning (ML) model using the rich data and
computational resources of mobile clients without collecting their data in
central systems. Conventional FL with client selection estimates the required
time for an FL round from a given clients' computation power and throughput and
determines a client set to reduce time consumption in FL rounds. However, it is
difficult to obtain accurate resource information for all clients before the FL
process is conducted because the available computation and communication
resources change easily based on background computation tasks, background
traffic, bottleneck links, etc. Consequently, the FL operator must select
clients through exploration and exploitation processes. This paper proposes a
multi-armed bandit (MAB)-based client selection method to solve the exploration
and exploitation trade-off and reduce the time consumption for FL in mobile
networks. The proposed method balances the selection of clients for which the
amount of resources is uncertain and those known to have a large amount of
resources. The simulation evaluation demonstrated that the proposed scheme
requires less learning time than the conventional method in the resource
fluctuating scenario.
</p>
<a href="http://arxiv.org/abs/2009.13879">arXiv:2009.13879</a> [<a href="http://arxiv.org/pdf/2009.13879">pdf</a>]

<h2>Sequence-to-Sequence Learning for Indonesian Automatic Question Generator. (arXiv:2009.13889v1 [cs.CL])</h2>
<h3>Ferdiant Joshua Muis (1), Ayu Purwarianti (1 and 2) ((1) Institut Teknologi Bandung, (2) U-CoE AI-VLB)</h3>
<p>Automatic question generation is defined as the task of automating the
creation of question given a various of textual data. Research in automatic
question generator (AQG) has been conducted for more than 10 years, mainly
focused on factoid question. In all these studies, the state-of-the-art is
attained using sequence-to-sequence approach. However, AQG system for
Indonesian has not ever been researched intensely. In this work we construct an
Indonesian automatic question generator, adapting the architecture from some
previous works. In summary, we used sequence-to-sequence approach using BiGRU,
BiLSTM, and Transformer with additional linguistic features, copy mechanism,
and coverage mechanism. Since there is no public large dan popular Indonesian
dataset for question generation, we translated SQuAD v2.0 factoid question
answering dataset, with additional Indonesian TyDiQA dev set for testing. The
system achieved BLEU1, BLEU2, BLEU3, BLEU4, and ROUGE-L score at 38,35, 20,96,
10,68, 5,78, and 43,4 for SQuAD, and 39.9, 20.78, 10.26, 6.31, 44.13 for
TyDiQA, respectively. The system performed well when the expected answers are
named entities and are syntactically close with the context explaining them.
Additionally, from native Indonesian perspective, the best questions generated
by our best models on their best cases are acceptable and reasonably useful.
</p>
<a href="http://arxiv.org/abs/2009.13889">arXiv:2009.13889</a> [<a href="http://arxiv.org/pdf/2009.13889">pdf</a>]

<h2>Towards Effective Context for Meta-Reinforcement Learning: an Approach based on Contrastive Learning. (arXiv:2009.13891v1 [cs.LG])</h2>
<h3>Haotian Fu, Hongyao Tang, Jianye Hao, Chen Chen, Xidong Feng, Dong Li, Wulong Liu</h3>
<p>Context, the embedding of previous collected trajectories, is a powerful
construct for Meta-Reinforcement Learning (Meta-RL) algorithms. By conditioning
on an effective context, Meta-RL policies can easily generalize to new tasks
within a few adaptation steps. We argue that improving the quality of context
involves answering two questions: 1. How to train a compact and sufficient
encoder that can embed the task-specific information contained in prior
trajectories? 2. How to collect informative trajectories of which the
corresponding context reflects the specification of tasks? To this end, we
propose a novel Meta-RL framework called CCM (Contrastive learning augmented
Context-based Meta-RL). We first focus on the contrastive nature behind
different tasks and leverage it to train a compact and sufficient context
encoder. Further, we train a separate exploration policy and theoretically
derive a new information-gain-based objective which aims to collect informative
trajectories in a few steps. Empirically, we evaluate our approaches on common
benchmarks as well as several complex sparse-reward environments. The
experimental results show that CCM outperforms state-of-the-art algorithms by
addressing previously mentioned problems respectively.
</p>
<a href="http://arxiv.org/abs/2009.13891">arXiv:2009.13891</a> [<a href="http://arxiv.org/pdf/2009.13891">pdf</a>]

<h2>Message Passing Neural Processes. (arXiv:2009.13895v1 [cs.LG])</h2>
<h3>Ben Day, C&#x103;t&#x103;lina Cangea, Arian R. Jamasb, Pietro Li&#xf2;</h3>
<p>Neural Processes (NPs) are powerful and flexible models able to incorporate
uncertainty when representing stochastic processes, while maintaining a linear
time complexity. However, NPs produce a latent description by aggregating
independent representations of context points and lack the ability to exploit
relational information present in many datasets. This renders NPs ineffective
in settings where the stochastic process is primarily governed by neighbourhood
rules, such as cellular automata (CA), and limits performance for any task
where relational information remains unused. We address this shortcoming by
introducing Message Passing Neural Processes (MPNPs), the first class of NPs
that explicitly makes use of relational structure within the model. Our
evaluation shows that MPNPs thrive at lower sampling rates, on existing
benchmarks and newly-proposed CA and Cora-Branched tasks. We further report
strong generalisation over density-based CA rule-sets and significant gains in
challenging arbitrary-labelling and few-shot learning setups.
</p>
<a href="http://arxiv.org/abs/2009.13895">arXiv:2009.13895</a> [<a href="http://arxiv.org/pdf/2009.13895">pdf</a>]

<h2>Weakly-supervised Salient Instance Detection. (arXiv:2009.13898v1 [cs.CV])</h2>
<h3>Xin Tian, Ke Xu, Xin Yang, Baocai Yin, Rynson W.H. Lau</h3>
<p>Existing salient instance detection (SID) methods typically learn from
pixel-level annotated datasets. In this paper, we present the first
weakly-supervised approach to the SID problem. Although weak supervision has
been considered in general saliency detection, it is mainly based on using
class labels for object localization. However, it is non-trivial to use only
class labels to learn instance-aware saliency information, as salient instances
with high semantic affinities may not be easily separated by the labels. We
note that subitizing information provides an instant judgement on the number of
salient items, which naturally relates to detecting salient instances and may
help separate instances of the same class while grouping different parts of the
same instance. Inspired by this insight, we propose to use class and subitizing
labels as weak supervision for the SID problem. We propose a novel
weakly-supervised network with three branches: a Saliency Detection Branch
leveraging class consistency information to locate candidate objects; a
Boundary Detection Branch exploiting class discrepancy information to delineate
object boundaries; and a Centroid Detection Branch using subitizing information
to detect salient instance centroids. This complementary information is further
fused to produce salient instance maps. We conduct extensive experiments to
demonstrate that the proposed method plays favorably against carefully designed
baseline methods adapted from related tasks.
</p>
<a href="http://arxiv.org/abs/2009.13898">arXiv:2009.13898</a> [<a href="http://arxiv.org/pdf/2009.13898">pdf</a>]

<h2>SoK: On the Security Challenges and Risks of Multi-Tenant FPGAs in the Cloud. (arXiv:2009.13914v1 [cs.CR])</h2>
<h3>Shaza Zeitouni, Ghada Dessouky, Ahmad-Reza Sadeghi</h3>
<p>In their continuous growth and penetration into new markets, Field
Programmable Gate Arrays (FPGAs) have recently made their way into hardware
acceleration of machine learning among other specialized compute-intensive
services in cloud data centers, such as Amazon and Microsoft. To further
maximize their utilization in the cloud, several academic works propose the
spatial multi-tenant deployment model, where the FPGA fabric is simultaneously
shared among mutually mistrusting clients. This is enabled by leveraging the
partial reconfiguration property of FPGAs, which allows to split the FPGA
fabric into several logically isolated regions and reconfigure the
functionality of each region independently at runtime. In this paper, we survey
industrial and academic deployment models of multi-tenant FPGAs in the cloud
computing settings, and highlight their different adversary models and security
guarantees, while shedding light on their fundamental shortcomings from a
security standpoint. We further survey and classify existing academic works
that demonstrate a new class of remotely exploitable physical attacks on
multi-tenant FPGA devices, where these attacks are launched remotely by
malicious clients sharing physical resources with victim users. Through
investigating the problem of end-to-end multi-tenant FPGA deployment more
comprehensively, we reveal how these attacks actually represent only one
dimension of the problem, while various open security and privacy challenges
remain unaddressed. We conclude with our insights and a call for future
research to tackle these challenges.
</p>
<a href="http://arxiv.org/abs/2009.13914">arXiv:2009.13914</a> [<a href="http://arxiv.org/pdf/2009.13914">pdf</a>]

<h2>Mobility Management in Emerging Ultra-Dense Cellular Networks: A Survey, Outlook, and Future Research Directions. (arXiv:2009.13922v1 [cs.NI])</h2>
<h3>Syed Muhammad Asad Zaidi, Marvin Manalastas, Hasan Farooq, Ali Imran</h3>
<p>The exponential rise in mobile traffic originating from mobile devices
highlights the need for making mobility management in future networks even more
efficient and seamless than ever before. Ultra-Dense Cellular Network vision
consisting of cells of varying sizes with conventional and mmWave bands is
being perceived as the panacea for the eminent capacity crunch. However,
mobility challenges in an ultra-dense heterogeneous network with motley of high
frequency and mmWave band cells will be unprecedented due to plurality of
handover instances, and the resulting signaling overhead and data interruptions
for miscellany of devices. Similarly, issues like user tracking and cell
discovery for mmWave with narrow beams need to be addressed before the
ambitious gains of emerging mobile networks can be realized. Mobility
challenges are further highlighted when considering the 5G deliverables of
multi-Gbps wireless connectivity, &lt;1ms latency and support for devices moving
at maximum speed of 500km/h, to name a few. Despite its significance, few
mobility surveys exist with the majority focused on adhoc networks. This paper
is the first to provide a comprehensive survey on the panorama of mobility
challenges in the emerging ultra-dense mobile networks. We not only present a
detailed tutorial on 5G mobility approaches and highlight key mobility risks of
legacy networks, but also review key findings from recent studies and highlight
the technical challenges and potential opportunities related to mobility from
the perspective of emerging ultra-dense cellular networks.
</p>
<a href="http://arxiv.org/abs/2009.13922">arXiv:2009.13922</a> [<a href="http://arxiv.org/pdf/2009.13922">pdf</a>]

<h2>Residual acoustic echo suppression based on efficient multi-task convolutional neural network. (arXiv:2009.13931v1 [cs.SD])</h2>
<h3>Xinquan Zhou, Yanhong Leng</h3>
<p>Acoustic echo degrades the user experience in voice communication systems
thus needs to be suppressed completely. We propose a real-time residual
acoustic echo suppression (RAES) method using an efficient convolutional neural
network. The double talk detector is used as an auxiliary task to improve the
performance of RAES in the context of multi-task learning. The training
criterion is based on a novel loss function, which we call as the suppression
loss, to balance the suppression of residual echo and the distortion of
near-end signals. The experimental results show that the proposed method can
efficiently suppress the residual echo under different circumstances.
</p>
<a href="http://arxiv.org/abs/2009.13931">arXiv:2009.13931</a> [<a href="http://arxiv.org/pdf/2009.13931">pdf</a>]

<h2>A Comparative Study of Deep Learning Loss Functions for Multi-Label Remote Sensing Image Classification. (arXiv:2009.13935v1 [cs.CV])</h2>
<h3>Hichame Yessou, Gencer Sumbul, Beg&#xfc;m Demir</h3>
<p>This paper analyzes and compares different deep learning loss functions in
the framework of multi-label remote sensing (RS) image scene classification
problems. We consider seven loss functions: 1) cross-entropy loss; 2) focal
loss; 3) weighted cross-entropy loss; 4) Hamming loss; 5) Huber loss; 6)
ranking loss; and 7) sparseMax loss. All the considered loss functions are
analyzed for the first time in RS. After a theoretical analysis, an
experimental analysis is carried out to compare the considered loss functions
in terms of their: 1) overall accuracy; 2) class imbalance awareness (for which
the number of samples associated to each class significantly varies); 3)
convexibility and differentiability; and 4) learning efficiency (i.e.,
convergence speed). On the basis of our analysis, some guidelines are derived
for a proper selection of a loss function in multi-label RS scene
classification problems.
</p>
<a href="http://arxiv.org/abs/2009.13935">arXiv:2009.13935</a> [<a href="http://arxiv.org/pdf/2009.13935">pdf</a>]

<h2>MS-RANAS: Multi-Scale Resource-Aware Neural Architecture Search. (arXiv:2009.13940v1 [cs.CV])</h2>
<h3>Cristian Cioflan, Radu Timofte</h3>
<p>Neural Architecture Search (NAS) has proved effective in offering
outperforming alternatives to handcrafted neural networks. In this paper we
analyse the benefits of NAS for image classification tasks under strict
computational constraints. Our aim is to automate the design of highly
efficient deep neural networks, capable of offering fast and accurate
predictions and that could be deployed on a low-memory, low-power
system-on-chip. The task thus becomes a three-party trade-off between accuracy,
computational complexity, and memory requirements. To address this concern, we
propose Multi-Scale Resource-Aware Neural Architecture Search (MS-RANAS). We
employ a one-shot architecture search approach in order to obtain a reduced
search cost and we focus on an anytime prediction setting. Through the usage of
multiple-scaled features and early classifiers, we achieved state-of-the-art
results in terms of accuracy-speed trade-off.
</p>
<a href="http://arxiv.org/abs/2009.13940">arXiv:2009.13940</a> [<a href="http://arxiv.org/pdf/2009.13940">pdf</a>]

<h2>ChemoVerse: Manifold traversal of latent spaces for novel molecule discovery. (arXiv:2009.13946v1 [cs.LG])</h2>
<h3>Harshdeep Singh, Nicholas McCarthy, Qurrat Ul Ain, Jeremiah Hayes</h3>
<p>In order to design a more potent and effective chemical entity, it is
essential to identify molecular structures with the desired chemical
properties. Recent advances in generative models using neural networks and
machine learning are being widely used by many emerging startups and
researchers in this domain to design virtual libraries of drug-like compounds.
Although these models can help a scientist to produce novel molecular
structures rapidly, the challenge still exists in the intelligent exploration
of the latent spaces of generative models, thereby reducing the randomness in
the generative procedure. In this work we present a manifold traversal with
heuristic search to explore the latent chemical space. Different heuristics and
scores such as the Tanimoto coefficient, synthetic accessibility, binding
activity, and QED drug-likeness can be incorporated to increase the validity
and proximity for desired molecular properties of the generated molecules. For
evaluating the manifold traversal exploration, we produce the latent chemical
space using various generative models such as grammar variational autoencoders
(with and without attention) as they deal with the randomized generation and
validity of compounds. With this novel traversal method, we are able to find
more unseen compounds and more specific regions to mine in the latent space.
Finally, these components are brought together in a simple platform allowing
users to perform search, visualization and selection of novel generated
compounds.
</p>
<a href="http://arxiv.org/abs/2009.13946">arXiv:2009.13946</a> [<a href="http://arxiv.org/pdf/2009.13946">pdf</a>]

<h2>One-Shot learning based classification for segregation of plastic waste. (arXiv:2009.13953v1 [cs.CV])</h2>
<h3>Shivaank Agarwal, Ravindra Gudi, Paresh Saxena</h3>
<p>The problem of segregating recyclable waste is fairly daunting for many
countries. This article presents an approach for image based classification of
plastic waste using one-shot learning techniques. The proposed approach
exploits discriminative features generated via the siamese and triplet loss
convolutional neural networks to help differentiate between 5 types of plastic
waste based on their resin codes. The approach achieves an accuracy of 99.74%
on the WaDaBa Database
</p>
<a href="http://arxiv.org/abs/2009.13953">arXiv:2009.13953</a> [<a href="http://arxiv.org/pdf/2009.13953">pdf</a>]

<h2>Beneficial Perturbation Network for designing general adaptive artificial intelligence systems. (arXiv:2009.13954v1 [cs.CV])</h2>
<h3>Shixian Wen, Amanda Rios, Yunhao Ge, Laurent Itti</h3>
<p>The human brain is the gold standard of adaptive learning. It not only can
learn and benefit from experience, but also can adapt to new situations. In
contrast, deep neural networks only learn one sophisticated but fixed mapping
from inputs to outputs. This limits their applicability to more dynamic
situations, where input to output mapping may change with different contexts. A
salient example is continual learning - learning new independent tasks
sequentially without forgetting previous tasks. Continual learning of multiple
tasks in artificial neural networks using gradient descent leads to
catastrophic forgetting, whereby a previously learned mapping of an old task is
erased when learning new mappings for new tasks. Here, we propose a new
biologically plausible type of deep neural network with extra, out-of-network,
task-dependent biasing units to accommodate these dynamic situations. This
allows, for the first time, a single network to learn potentially unlimited
parallel input to output mappings, and to switch on the fly between them at
runtime. Biasing units are programmed by leveraging beneficial perturbations
(opposite to well-known adversarial perturbations) for each task. Beneficial
perturbations for a given task bias the network toward that task, essentially
switching the network into a different mode to process that task. This largely
eliminates catastrophic interference between tasks. Our approach is
memory-efficient and parameter-efficient, can accommodate many tasks, and
achieves state-of-the-art performance across different tasks and domains.
</p>
<a href="http://arxiv.org/abs/2009.13954">arXiv:2009.13954</a> [<a href="http://arxiv.org/pdf/2009.13954">pdf</a>]

<h2>A Prototype-Based Generalized Zero-Shot Learning Framework for Hand Gesture Recognition. (arXiv:2009.13957v1 [cs.CV])</h2>
<h3>Jinting Wu, Yujia Zhang, Xiaoguang Zhao</h3>
<p>Hand gesture recognition plays a significant role in human-computer
interaction for understanding various human gestures and their intent. However,
most prior works can only recognize gestures of limited labeled classes and
fail to adapt to new categories. The task of Generalized Zero-Shot Learning
(GZSL) for hand gesture recognition aims to address the above issue by
leveraging semantic representations and detecting both seen and unseen class
samples. In this paper, we propose an end-to-end prototype-based GZSL framework
for hand gesture recognition which consists of two branches. The first branch
is a prototype-based detector that learns gesture representations and
determines whether an input sample belongs to a seen or unseen category. The
second branch is a zero-shot label predictor which takes the features of unseen
classes as input and outputs predictions through a learned mapping mechanism
between the feature and the semantic space. We further establish a hand gesture
dataset that specifically targets this GZSL task, and comprehensive experiments
on this dataset demonstrate the effectiveness of our proposed approach on
recognizing both seen and unseen gestures.
</p>
<a href="http://arxiv.org/abs/2009.13957">arXiv:2009.13957</a> [<a href="http://arxiv.org/pdf/2009.13957">pdf</a>]

<h2>Online Action Learning in High Dimensions: A New Exploration Rule for Contextual $\epsilon_t$-Greedy Heuristics. (arXiv:2009.13961v1 [stat.ML])</h2>
<h3>Claudio C. Flores, Marcelo C. Medeiros</h3>
<p>Bandit problems are pervasive in various fields of research and are also
present in several practical applications. Examples, including dynamic pricing
and assortment and the design of auctions and incentives, permeate a large
number of sequential treatment experiments. Different applications impose
distinct levels of restrictions on viable actions. Some favor diversity of
outcomes, while others require harmful actions to be closely monitored or
mainly avoided. In this paper, we extend one of the most popular bandit
solutions, the original $\epsilon_t$-greedy heuristics, to high-dimensional
contexts. Moreover, we introduce a competing exploration mechanism that counts
with searching sets based on order statistics. We view our proposals as
alternatives for cases where pluralism is valued or, in the opposite direction,
cases where the end-user should carefully tune the range of exploration of new
actions. We find reasonable bounds for the cumulative regret of a decaying
$\epsilon_t$-greedy heuristic in both cases and we provide an upper bound for
the initialization phase that implies the regret bounds when order statistics
are considered to be at most equal but mostly better than the case when random
searching is the sole exploration mechanism. Additionally, we show that
end-users have sufficient flexibility to avoid harmful actions since any
cardinality for the higher-order statistics can be used to achieve an stricter
upper bound. In a simulation exercise, we show that the algorithms proposed in
this paper outperform simple and adapted counterparts.
</p>
<a href="http://arxiv.org/abs/2009.13961">arXiv:2009.13961</a> [<a href="http://arxiv.org/pdf/2009.13961">pdf</a>]

<h2>Neural Topic Modeling with Cycle-Consistent Adversarial Training. (arXiv:2009.13971v1 [cs.CL])</h2>
<h3>Xuemeng Hu, Rui Wang, Deyu Zhou, Yuxuan Xiong</h3>
<p>Advances on deep generative models have attracted significant research
interest in neural topic modeling. The recently proposed Adversarial-neural
Topic Model models topics with an adversarially trained generator network and
employs Dirichlet prior to capture the semantic patterns in latent topics. It
is effective in discovering coherent topics but unable to infer topic
distributions for given documents or utilize available document labels. To
overcome such limitations, we propose Topic Modeling with Cycle-consistent
Adversarial Training (ToMCAT) and its supervised version sToMCAT. ToMCAT
employs a generator network to interpret topics and an encoder network to infer
document topics. Adversarial training and cycle-consistent constraints are used
to encourage the generator and the encoder to produce realistic samples that
coordinate with each other. sToMCAT extends ToMCAT by incorporating document
labels into the topic modeling process to help discover more coherent topics.
The effectiveness of the proposed models is evaluated on
unsupervised/supervised topic modeling and text classification. The
experimental results show that our models can produce both coherent and
informative topics, outperforming a number of competitive baselines.
</p>
<a href="http://arxiv.org/abs/2009.13971">arXiv:2009.13971</a> [<a href="http://arxiv.org/pdf/2009.13971">pdf</a>]

<h2>Identification of Probability weighted ARX models with arbitrary domains. (arXiv:2009.13975v1 [cs.LG])</h2>
<h3>Alessandro Brusaferri, Matteo Matteucci, Stefano Spinelli</h3>
<p>Hybrid system identification is a key tool to achieve reliable models of
Cyber-Physical Systems from data. PieceWise Affine models guarantees universal
approximation, local linearity and equivalence to other classes of hybrid
system. Still, PWA identification is a challenging problem, requiring the
concurrent solution of regression and classification tasks. In this work, we
focus on the identification of PieceWise Auto Regressive with eXogenous input
models with arbitrary regions (NPWARX), thus not restricted to polyhedral
domains, and characterized by discontinuous maps. To this end, we propose a
method based on a probabilistic mixture model, where the discrete state is
represented through a multinomial distribution conditioned by the input
regressors. The architecture is conceived following the Mixture of Expert
concept, developed within the machine learning field. To achieve nonlinear
partitioning, we parametrize the discriminant function using a neural network.
Then, the parameters of both the ARX submodels and the classifier are
concurrently estimated by maximizing the likelihood of the overall model using
Expectation Maximization. The proposed method is demonstrated on a nonlinear
piece-wise problem with discontinuous maps.
</p>
<a href="http://arxiv.org/abs/2009.13975">arXiv:2009.13975</a> [<a href="http://arxiv.org/pdf/2009.13975">pdf</a>]

<h2>A Low Complexity Decentralized Neural Net with Centralized Equivalence using Layer-wise Learning. (arXiv:2009.13982v1 [cs.LG])</h2>
<h3>Xinyue Liang, Alireza M. Javid, Mikael Skoglund, Saikat Chatterjee</h3>
<p>We design a low complexity decentralized learning algorithm to train a
recently proposed large neural network in distributed processing nodes
(workers). We assume the communication network between the workers is
synchronized and can be modeled as a doubly-stochastic mixing matrix without
having any master node. In our setup, the training data is distributed among
the workers but is not shared in the training process due to privacy and
security concerns. Using alternating-direction-method-of-multipliers (ADMM)
along with a layerwise convex optimization approach, we propose a decentralized
learning algorithm which enjoys low computational complexity and communication
cost among the workers. We show that it is possible to achieve equivalent
learning performance as if the data is available in a single place. Finally, we
experimentally illustrate the time complexity and convergence behavior of the
algorithm.
</p>
<a href="http://arxiv.org/abs/2009.13982">arXiv:2009.13982</a> [<a href="http://arxiv.org/pdf/2009.13982">pdf</a>]

<h2>The design and implementation of Language Learning Chatbot with XAI using Ontology and Transfer Learning. (arXiv:2009.13984v1 [cs.AI])</h2>
<h3>Nuobei Shi, Qin Zeng, Raymond Lee</h3>
<p>In this paper, we proposed a transfer learning-based English language
learning chatbot, whose output generated by GPT-2 can be explained by
corresponding ontology graph rooted by fine-tuning dataset. We design three
levels for systematically English learning, including phonetics level for
speech recognition and pronunciation correction, semantic level for specific
domain conversation, and the simulation of free-style conversation in English -
the highest level of language chatbot communication as free-style conversation
agent. For academic contribution, we implement the ontology graph to explain
the performance of free-style conversation, following the concept of XAI
(Explainable Artificial Intelligence) to visualize the connections of neural
network in bionics, and explain the output sentence from language model. From
implementation perspective, our Language Learning agent integrated the
mini-program in WeChat as front-end, and fine-tuned GPT-2 model of transfer
learning as back-end to interpret the responses by ontology graph.
</p>
<a href="http://arxiv.org/abs/2009.13984">arXiv:2009.13984</a> [<a href="http://arxiv.org/pdf/2009.13984">pdf</a>]

<h2>Deep Image Reconstruction using Unregistered Measurements without Groundtruth. (arXiv:2009.13986v1 [eess.IV])</h2>
<h3>Weijie Gan, Yu Sun, Cihat Eldeniz, Jiaming Liu, Hongyu An, Ulugbek S. Kamilov</h3>
<p>One of the key limitations in conventional deep learning based image
reconstruction is the need for registered pairs of training images containing a
set of high-quality groundtruth images. This paper addresses this limitation by
proposing a novel unsupervised deep registration-augmented reconstruction
method (U-Dream) for training deep neural nets to reconstruct high-quality
images by directly mapping pairs of unregistered and artifact-corrupted images.
The ability of U-Dream to circumvent the need for accurately registered data
makes it widely applicable to many biomedical image reconstruction tasks. We
validate it in accelerated magnetic resonance imaging (MRI) by training an
image reconstruction model directly on pairs of undersampled measurements from
images that have undergone nonrigid deformations.
</p>
<a href="http://arxiv.org/abs/2009.13986">arXiv:2009.13986</a> [<a href="http://arxiv.org/pdf/2009.13986">pdf</a>]

<h2>MARA-Net: Single Image Deraining Network with Multi-level connection and Adaptive Regional Attention. (arXiv:2009.13990v1 [cs.CV])</h2>
<h3>Yeachan Park, Myeongho Jeon, Junho Lee, Myungjoo Kan</h3>
<p>Removing rain streaks from single images is an important problem in various
computer vision tasks because rain streaks can degrade outdoor images and
reduce their visibility. While recent convolutional neural network-based
deraining models have succeeded in capturing rain streaks effectively,
difficulties in recovering the details in rain-free images still remain. In
this paper, we present a multi-level connection and adaptive regional attention
network (MARA-Net) to properly restore the original background textures in
rainy images. The first main idea is a multi-level connection design that
repeatedly connects multi-level features of the encoder network to the decoder
network. Multi-level connections encourage the decoding process to use the
feature information of all levels. Channel attention is considered in
multi-level connections to learn which level of features is important in the
decoding process of the current level. The second main idea is a wide regional
non-local block (WRNL). As rain streaks primarily exhibit a vertical
distribution, we divide the grid of the image into horizontally-wide patches
and apply a non-local operation to each region to explore the rich rain-free
background information. Experimental results on both synthetic and real-world
rainy datasets demonstrate that the proposed model significantly outperforms
existing state-of-the-art models. Furthermore, the results of the joint
deraining and segmentation experiment prove that our model contributes
effectively to other vision tasks.
</p>
<a href="http://arxiv.org/abs/2009.13990">arXiv:2009.13990</a> [<a href="http://arxiv.org/pdf/2009.13990">pdf</a>]

<h2>Explainable AI without Interpretable Model. (arXiv:2009.13996v1 [cs.AI])</h2>
<h3>Kary Fr&#xe4;mling</h3>
<p>Explainability has been a challenge in AI for as long as AI has existed. With
the recently increased use of AI in society, it has become more important than
ever that AI systems would be able to explain the reasoning behind their
results also to end-users in situations such as being eliminated from a
recruitment process or having a bank loan application refused by an AI system.
Especially if the AI system has been trained using Machine Learning, it tends
to contain too many parameters for them to be analysed and understood, which
has caused them to be called `black-box' systems. Most Explainable AI (XAI)
methods are based on extracting an interpretable model that can be used for
producing explanations. However, the interpretable model does not necessarily
map accurately to the original black-box model. Furthermore, the
understandability of interpretable models for an end-user remains questionable.
The notions of Contextual Importance and Utility (CIU) presented in this paper
make it possible to produce human-like explanations of black-box outcomes
directly, without creating an interpretable model. Therefore, CIU explanations
map accurately to the black-box model itself. CIU is completely model-agnostic
and can be used with any black-box system. In addition to feature importance,
the utility concept that is well-known in Decision Theory provides a new
dimension to explanations compared to most existing XAI methods. Finally, CIU
can produce explanations at any level of abstraction and using different
vocabularies and other means of interaction, which makes it possible to adjust
explanations and interaction according to the context and to the target users.
</p>
<a href="http://arxiv.org/abs/2009.13996">arXiv:2009.13996</a> [<a href="http://arxiv.org/pdf/2009.13996">pdf</a>]

<h2>Improving Interpretability for Computer-aided Diagnosis tools on Whole Slide Imaging with Multiple Instance Learning and Gradient-based Explanations. (arXiv:2009.14001v1 [cs.CV])</h2>
<h3>Antoine Pirovano, Hippolyte Heuberger, Sylvain Berlemont, Sa&#xef;d Ladjal, Isabelle Bloch</h3>
<p>Deep learning methods are widely used for medical applications to assist
medical doctors in their daily routines. While performances reach expert's
level, interpretability (highlight how and what a trained model learned and why
it makes a specific decision) is the next important challenge that deep
learning methods need to answer to be fully integrated in the medical field. In
this paper, we address the question of interpretability in the context of whole
slide images (WSI) classification. We formalize the design of WSI
classification architectures and propose a piece-wise interpretability
approach, relying on gradient-based methods, feature visualization and multiple
instance learning context. We aim at explaining how the decision is made based
on tile level scoring, how these tile scores are decided and which features are
used and relevant for the task. After training two WSI classification
architectures on Camelyon-16 WSI dataset, highlighting discriminative features
learned, and validating our approach with pathologists, we propose a novel
manner of computing interpretability slide-level heat-maps, based on the
extracted features, that improves tile-level classification performances by
more than 29% for AUC.
</p>
<a href="http://arxiv.org/abs/2009.14001">arXiv:2009.14001</a> [<a href="http://arxiv.org/pdf/2009.14001">pdf</a>]

<h2>Fast Gravitational Approach for Rigid Point Set Registration with Ordinary Differential Equations. (arXiv:2009.14005v1 [cs.CV])</h2>
<h3>Sk Aziz Ali, Kerem Kahraman, Christian Theobalt, Didier Stricker, Vladislav Golyanik</h3>
<p>This article introduces a new physics-based method for rigid point set
alignment called Fast Gravitational Approach (FGA). In FGA, the source and
target point sets are interpreted as rigid particle swarms with masses
interacting in a globally multiply-linked manner while moving in a simulated
gravitational force field. The optimal alignment is obtained by explicit
modeling of forces acting on the particles as well as their velocities and
displacements with second-order ordinary differential equations of motion.
Additional alignment cues (point-based or geometric features, and other
boundary conditions) can be integrated into FGA through particle masses. We
propose a smooth-particle mass function for point mass initialization, which
improves robustness to noise and structural discontinuities. To avoid
prohibitive quadratic complexity of all-to-all point interactions, we adapt a
Barnes-Hut tree for accelerated force computation and achieve quasilinear
computational complexity. We show that the new method class has characteristics
not found in previous alignment methods such as efficient handling of partial
overlaps, inhomogeneous point sampling densities, and coping with large point
clouds with reduced runtime compared to the state of the art. Experiments show
that our method performs on par with or outperforms all compared competing
non-deep-learning-based and general-purpose techniques (which do not assume the
availability of training data and a scene prior) in resolving transformations
for LiDAR data and gains state-of-the-art accuracy and speed when coping with
different types of data disturbances.
</p>
<a href="http://arxiv.org/abs/2009.14005">arXiv:2009.14005</a> [<a href="http://arxiv.org/pdf/2009.14005">pdf</a>]

<h2>Realistic Image Normalization for Multi-Domain Segmentation. (arXiv:2009.14024v1 [cs.LG])</h2>
<h3>Pierre-Luc Delisle, Benoit Anctil-Robitaille, Christian Desrosiers, Herve Lombaert</h3>
<p>Image normalization is a building block in medical image analysis.
Conventional approaches are customarily utilized on a per-dataset basis. This
strategy, however, prevents the current normalization algorithms from fully
exploiting the complex joint information available across multiple datasets.
Consequently, ignoring such joint information has a direct impact on the
performance of segmentation algorithms. This paper proposes to revisit the
conventional image normalization approach by instead learning a common
normalizing function across multiple datasets. Jointly normalizing multiple
datasets is shown to yield consistent normalized images as well as an improved
image segmentation. To do so, a fully automated adversarial and task-driven
normalization approach is employed as it facilitates the training of realistic
and interpretable images while keeping performance on-par with the
state-of-the-art. The adversarial training of our network aims at finding the
optimal transfer function to improve both the segmentation accuracy and the
generation of realistic images. We evaluated the performance of our normalizer
on both infant and adult brains images from the iSEG, MRBrainS and ABIDE
datasets. Results reveal the potential of our normalization approach for
segmentation, with Dice improvements of up to 57.5% over our baseline. Our
method can also enhance data availability by increasing the number of samples
available when learning from multiple imaging domains.
</p>
<a href="http://arxiv.org/abs/2009.14024">arXiv:2009.14024</a> [<a href="http://arxiv.org/pdf/2009.14024">pdf</a>]

<h2>Machine-Learning Approach to Analyze the Status of Forklift Vehicles with Irregular Movement in a Shipyard. (arXiv:2009.14025v1 [cs.LG])</h2>
<h3>Hyeonju Lee, Jongho Lee, Minji An, Gunil Park, Sungchul Choi</h3>
<p>In large shipyards, the management of equipment, which are used for building
a variety of ships, is critical. Because orders vary year to year, shipyard
managers are required to determine methods to make the most of their limited
resources. A particular difficulty that arises because of the nature and size
of shipyards is the management of moving vehicles. In recent years,
shipbuilding companies have attempted to manage and track the locations and
movements of vehicles using Global Positioning System (GPS) modules. However,
because certain vehicles, such as forklifts, roam irregularly around a yard,
identifying their working status without being onsite is difficult. Location
information alone is not sufficient to determine whether a vehicle is working,
moving, waiting, or resting. This study proposes an approach based on machine
learning to identify the work status of each forklift. We use the DBSCAN and
k-means algorithms to identify the area in which a particular forklift is
operating and the type of work it is performing. We developed a business
intelligence system to collect information from forklifts equipped with GPS and
Internet of Things (IoT) devices. The system provides visual information on the
status of individual forklifts and helps in the efficient management of their
movements within large shipyards.
</p>
<a href="http://arxiv.org/abs/2009.14025">arXiv:2009.14025</a> [<a href="http://arxiv.org/pdf/2009.14025">pdf</a>]

<h2>Understanding Human Intelligence through Human Limitations. (arXiv:2009.14050v1 [cs.AI])</h2>
<h3>Thomas L. Griffiths</h3>
<p>Recent progress in artificial intelligence provides the opportunity to ask
the question of what is unique about human intelligence, but with a new
comparison class. I argue that we can understand human intelligence, and the
ways in which it may differ from artificial intelligence, by considering the
characteristics of the kind of computational problems that human minds have to
solve. I claim that these problems acquire their structure from three
fundamental limitations that apply to human beings: limited time, limited
computation, and limited communication. From these limitations we can derive
many of the properties we associate with human intelligence, such as rapid
learning, the ability to break down problems into parts, and the capacity for
cumulative cultural evolution.
</p>
<a href="http://arxiv.org/abs/2009.14050">arXiv:2009.14050</a> [<a href="http://arxiv.org/pdf/2009.14050">pdf</a>]

<h2>Graph convolutional regression of cardiac depolarization from sparse endocardial maps. (arXiv:2009.14068v1 [physics.med-ph])</h2>
<h3>Felix Meister, Tiziano Passerini, Chlo&#xe9; Audigier, &#xc8;ric Lluch, Viorel Mihalef, Hiroshi Ashikaga, Andreas Maier, Henry Halperin, Tommaso Mansi</h3>
<p>Electroanatomic mapping as routinely acquired in ablation therapy of
ventricular tachycardia is the gold standard method to identify the
arrhythmogenic substrate. To reduce the acquisition time and still provide maps
with high spatial resolution, we propose a novel deep learning method based on
graph convolutional neural networks to estimate the depolarization time in the
myocardium, given sparse catheter data on the left ventricular endocardium,
ECG, and magnetic resonance images. The training set consists of data produced
by a computational model of cardiac electrophysiology on a large cohort of
synthetically generated geometries of ischemic hearts. The predicted
depolarization pattern has good agreement with activation times computed by the
cardiac electrophysiology model in a validation set of five swine heart
geometries with complex scar and border zone morphologies. The mean absolute
error hereby measures 8 ms on the entire myocardium when providing 50\% of the
endocardial ground truth in over 500 computed depolarization patterns.
Furthermore, when considering a complete animal data set with high density
electroanatomic mapping data as reference, the neural network can accurately
reproduce the endocardial depolarization pattern, even when a small percentage
of measurements are provided as input features (mean absolute error of 7 ms
with 50\% of input samples). The results show that the proposed method, trained
on synthetically generated data, may generalize to real data.
</p>
<a href="http://arxiv.org/abs/2009.14068">arXiv:2009.14068</a> [<a href="http://arxiv.org/pdf/2009.14068">pdf</a>]

<h2>Localize to Classify and Classify to Localize: Mutual Guidance in Object Detection. (arXiv:2009.14085v1 [cs.CV])</h2>
<h3>Heng Zhang, Elisa Fromont, S&#xe9;bastien Lefevre, Bruno Avignon</h3>
<p>Most deep learning object detectors are based on the anchor mechanism and
resort to the Intersection over Union (IoU) between predefined anchor boxes and
ground truth boxes to evaluate the matching quality between anchors and
objects. In this paper, we question this use of IoU and propose a new anchor
matching criterion guided, during the training phase, by the optimization of
both the localization and the classification tasks: the predictions related to
one task are used to dynamically assign sample anchors and improve the model on
the other task, and vice versa. Despite the simplicity of the proposed method,
our experiments with different state-of-the-art deep learning architectures on
PASCAL VOC and MS COCO datasets demonstrate the effectiveness and generality of
our Mutual Guidance strategy.
</p>
<a href="http://arxiv.org/abs/2009.14085">arXiv:2009.14085</a> [<a href="http://arxiv.org/pdf/2009.14085">pdf</a>]

<h2>Weakly Supervised-Based Oversampling for High Imbalance and High Dimensionality Data Classification. (arXiv:2009.14096v1 [cs.LG])</h2>
<h3>Min Qian, Yan-Fu Li</h3>
<p>With the abundance of industrial datasets, imbalanced classification has
become a common problem in several application domains. Oversampling is an
effective method to solve imbalanced classification. One of the main challenges
of existing oversampling methods is to accurately label the new synthetic
samples. Inaccurate labels of synthetic samples would distort the distribution
of the dataset and possibly worsen the classification performance. This paper
introduces the idea of weakly supervised learning to handle the inaccurate
labeling of synthetic samples by traditional oversampling methods. Graph
semi-supervised SMOTE is developed to improve the credibility of the synthetic
samples' labels. In addition, we propose cost-sensitive neighborhood components
analysis for high dimensionality datasets and bootstrap based ensemble
framework for high imbalance datasets. The proposed method has achieved good
classification performance on 8 synthetic datasets and 3 real-world datasets,
especially for high imbalance and high dimensionality problems. The average
performances and robustness are better than the benchmark methods.
</p>
<a href="http://arxiv.org/abs/2009.14096">arXiv:2009.14096</a> [<a href="http://arxiv.org/pdf/2009.14096">pdf</a>]

<h2>Align-RUDDER: Learning From Few Demonstrations by Reward Redistribution. (arXiv:2009.14108v1 [cs.LG])</h2>
<h3>Vihang P. Patil, Markus Hofmarcher, Marius-Constantin Dinu, Matthias Dorfer, Patrick M. Blies, Johannes Brandstetter, Jose A. Arjona-Medina, Sepp Hochreiter</h3>
<p>Reinforcement Learning algorithms require a large number of samples to solve
complex tasks with sparse and delayed rewards. Complex tasks can often be
hierarchically decomposed into sub-tasks. A step in the Q-function can be
associated with solving a sub-task, where the expectation of the return
increases. RUDDER has been introduced to identify these steps and then
redistribute reward to them, thus immediately giving reward if sub-tasks are
solved. Since the problem of delayed rewards is mitigated, learning is
considerably sped up. However, for complex tasks, current exploration
strategies as deployed in RUDDER struggle with discovering episodes with high
rewards. Therefore, we assume that episodes with high rewards are given as
demonstrations and do not have to be discovered by exploration. Typically the
number of demonstrations is small and RUDDER's LSTM model as a deep learning
method does not learn well. Hence, we introduce Align-RUDDER, which is RUDDER
with two major modifications. First, Align-RUDDER assumes that episodes with
high rewards are given as demonstrations, replacing RUDDER's safe exploration
and lessons replay buffer. Second, we replace RUDDER's LSTM model by a profile
model that is obtained from multiple sequence alignment of demonstrations.
Profile models can be constructed from as few as two demonstrations as known
from bioinformatics. Align-RUDDER inherits the concept of reward
redistribution, which considerably reduces the delay of rewards, thus speeding
up learning. Align-RUDDER outperforms competitors on complex artificial tasks
with delayed reward and few demonstrations. On the MineCraft ObtainDiamond
task, Align-RUDDER is able to mine a diamond, though not frequently. Github:
https://github.com/ml-jku/align-rudder, YouTube: https://youtu.be/HO-_8ZUl-UY
</p>
<a href="http://arxiv.org/abs/2009.14108">arXiv:2009.14108</a> [<a href="http://arxiv.org/pdf/2009.14108">pdf</a>]

<h2>Improving Low Compute Language Modeling with In-Domain Embedding Initialisation. (arXiv:2009.14109v1 [cs.CL])</h2>
<h3>Charles Welch, Rada Mihalcea, Jonathan K. Kummerfeld</h3>
<p>Many NLP applications, such as biomedical data and technical support, have
10-100 million tokens of in-domain data and limited computational resources for
learning from it. How should we train a language model in this scenario? Most
language modeling research considers either a small dataset with a closed
vocabulary (like the standard 1 million token Penn Treebank), or the whole web
with byte-pair encoding. We show that for our target setting in English,
initialising and freezing input embeddings using in-domain data can improve
language model performance by providing a useful representation of rare words,
and this pattern holds across several different domains. In the process, we
show that the standard convention of tying input and output embeddings does not
improve perplexity when initializing with embeddings trained on in-domain data.
</p>
<a href="http://arxiv.org/abs/2009.14109">arXiv:2009.14109</a> [<a href="http://arxiv.org/pdf/2009.14109">pdf</a>]

<h2>Learning to Compress Videos without Computing Motion. (arXiv:2009.14110v1 [eess.IV])</h2>
<h3>Meixu Chen, Todd Goodall, Anjul Patney, Alan C. Bovik</h3>
<p>With the development of higher resolution contents and displays, its
significant volume poses significant challenges to the goals of acquiring,
transmitting, compressing and displaying high quality video content. In this
paper, we propose a new deep learning video compression architecture that does
not require motion estimation, which is the most expensive element of modern
hybrid video compression codecs like H.264 and HEVC. Our framework exploits the
regularities inherent to video motion, which we capture by using displaced
frame differences as video representations to train the neural network. In
addition, we propose a new space-time reconstruction network based on both an
LSTM model and a UNet model, which we call LSTM-UNet. The combined network is
able to efficiently capture both temporal and spatial video information, making
it highly amenable for our purposes. The new video compression framework has
three components: a Displacement Calculation Unit (DCU), a Displacement
Compression Network (DCN), and a Frame Reconstruction Network (FRN), all of
which are jointly optimized against a single perceptual loss function. The DCU
obviates the need for motion estimation as in hybrid codecs, and is less
expensive. In the DCN, an RNN-based network is utilized to conduct variable
bit-rate encoding based on a single round of training. The LSTM-UNet is used in
the FRN to learn space time differential representations of videos. Our
experimental results show that our compression model, which we call the
MOtionless VIdeo Codec (MOVI-Codec), learns how to efficiently compress videos
without computing motion. Our experiments show that MOVI-Codec outperforms the
video coding standard H.264, and is highly competitive with, and sometimes
exceeds the performance of the modern global standard HEVC codec, as measured
by MS-SSIM.
</p>
<a href="http://arxiv.org/abs/2009.14110">arXiv:2009.14110</a> [<a href="http://arxiv.org/pdf/2009.14110">pdf</a>]

<h2>Inverse Classification with Limited Budget and Maximum Number of Perturbed Samples. (arXiv:2009.14111v1 [cs.LG])</h2>
<h3>Jaehoon Koo, Diego Klabjan, Jean Utke</h3>
<p>Most recent machine learning research focuses on developing new classifiers
for the sake of improving classification accuracy. With many well-performing
state-of-the-art classifiers available, there is a growing need for
understanding interpretability of a classifier necessitated by practical
purposes such as to find the best diet recommendation for a diabetes patient.
Inverse classification is a post modeling process to find changes in input
features of samples to alter the initially predicted class. It is useful in
many business applications to determine how to adjust a sample input data such
that the classifier predicts it to be in a desired class. In real world
applications, a budget on perturbations of samples corresponding to customers
or patients is usually considered, and in this setting, the number of
successfully perturbed samples is key to increase benefits. In this study, we
propose a new framework to solve inverse classification that maximizes the
number of perturbed samples subject to a per-feature-budget limits and
favorable classification classes of the perturbed samples. We design algorithms
to solve this optimization problem based on gradient methods, stochastic
processes, Lagrangian relaxations, and the Gumbel trick. In experiments, we
find that our algorithms based on stochastic processes exhibit an excellent
performance in different budget settings and they scale well.
</p>
<a href="http://arxiv.org/abs/2009.14111">arXiv:2009.14111</a> [<a href="http://arxiv.org/pdf/2009.14111">pdf</a>]

<h2>CoKe: Localized Contrastive Learning for Robust Keypoint Detection. (arXiv:2009.14115v1 [cs.CV])</h2>
<h3>Yutong Bai, Angtian Wang, Adam Kortylewski, Alan Yuille</h3>
<p>Today's most popular approaches to keypoint detection learn a holistic
representation of all keypoints. This enables them to implicitly leverage the
relative spatial geometry between keypoints and thus to prevent false-positive
detections due to local ambiguities. However, our experiments show that such
holistic representations do not generalize well when the 3D pose of objects
varies strongly, or when objects are partially occluded. In this paper, we
propose CoKe, a framework for the supervised contrastive learning of distinct
local feature representations for robust keypoint detection. In particular, we
introduce a feature bank mechanism and update rules for keypoint and
non-keypoint features which make possible to learn local keypoint detectors
that are accurate and robust to local ambiguities. Our experiments show that
CoKe achieves state-of-the-art results compared to approaches that jointly
represent all keypoints holistically (Stacked Hourglass Networks, MSS-Net) as
well as to approaches that are supervised with the detailed 3D object geometry
(StarMap). Notably, CoKe performs exceptionally well when objects are partially
occluded and outperforms related work on a range of diverse datasets
(PASCAL3D+,ObjectNet, MPII).
</p>
<a href="http://arxiv.org/abs/2009.14115">arXiv:2009.14115</a> [<a href="http://arxiv.org/pdf/2009.14115">pdf</a>]

<h2>EEG to fMRI Synthesis: Is Deep Learning a candidate?. (arXiv:2009.14133v1 [cs.LG])</h2>
<h3>David Calhas, Rui Henriques</h3>
<p>Advances on signal, image and video generation underly major breakthroughs on
generative medical imaging tasks, including Brain Image Synthesis. Still, the
extent to which functional Magnetic Ressonance Imaging (fMRI) can be mapped
from the brain electrophysiology remains largely unexplored. This work provides
the first comprehensive view on how to use state-of-the-art principles from
Neural Processing to synthesize fMRI data from electroencephalographic (EEG)
data. Given the distinct spatiotemporal nature of haemodynamic and
electrophysiological signals, this problem is formulated as the task of
learning a mapping function between multivariate time series with highly
dissimilar structures. A comparison of state-of-the-art synthesis approaches,
including Autoencoders, Generative Adversarial Networks and Pairwise Learning,
is undertaken. Results highlight the feasibility of EEG to fMRI brain image
mappings, pinpointing the role of current advances in Machine Learning and
showing the relevance of upcoming contributions to further improve performance.
EEG to fMRI synthesis offers a way to enhance and augment brain image data, and
guarantee access to more affordable, portable and long-lasting protocols of
brain activity monitoring. The code used in this manuscript is available in
Github and the datasets are open source.
</p>
<a href="http://arxiv.org/abs/2009.14133">arXiv:2009.14133</a> [<a href="http://arxiv.org/pdf/2009.14133">pdf</a>]

<h2>Time your hedge with Deep Reinforcement Learning. (arXiv:2009.14136v1 [q-fin.PM])</h2>
<h3>Eric Benhamou, David Saltiel, Sandrine Ungari, Abhishek Mukhopadhyay</h3>
<p>Can an asset manager plan the optimal timing for her/his hedging strategies
given market conditions? The standard approach based on Markowitz or other more
or less sophisticated financial rules aims to find the best portfolio
allocation thanks to forecasted expected returns and risk but fails to fully
relate market conditions to hedging strategies decision. In contrast, Deep
Reinforcement Learning (DRL) can tackle this challenge by creating a dynamic
dependency between market information and hedging strategies allocation
decisions. In this paper, we present a realistic and augmented DRL framework
that: (i) uses additional contextual information to decide an action, (ii) has
a one period lag between observations and actions to account for one day lag
turnover of common asset managers to rebalance their hedge, (iii) is fully
tested in terms of stability and robustness thanks to a repetitive train test
method called anchored walk forward training, similar in spirit to k fold cross
validation for time series and (iv) allows managing leverage of our hedging
strategy. Our experiment for an augmented asset manager interested in sizing
and timing his hedges shows that our approach achieves superior returns and
lower risk.
</p>
<a href="http://arxiv.org/abs/2009.14136">arXiv:2009.14136</a> [<a href="http://arxiv.org/pdf/2009.14136">pdf</a>]

<h2>Selective Cascade of Residual ExtraTrees. (arXiv:2009.14138v1 [cs.LG])</h2>
<h3>Qimin Liu, Fang Liu</h3>
<p>We propose a novel tree-based ensemble method named Selective Cascade of
Residual ExtraTrees (SCORE). SCORE draws inspiration from representation
learning, incorporates regularized regression with variable selection features,
and utilizes boosting to improve prediction and reduce generalization errors.
We also develop a variable importance measure to increase the explainability of
SCORE. Our computer experiments show that SCORE provides comparable or superior
performance in prediction against ExtraTrees, random forest, gradient boosting
machine, and neural networks; and the proposed variable importance measure for
SCORE is comparable to studied benchmark methods. Finally, the predictive
performance of SCORE remains stable across hyper-parameter values, suggesting
potential robustness to hyperparameter specification.
</p>
<a href="http://arxiv.org/abs/2009.14138">arXiv:2009.14138</a> [<a href="http://arxiv.org/pdf/2009.14138">pdf</a>]

<h2>A Survey on Deep Learning Techniques for Video Anomaly Detection. (arXiv:2009.14146v1 [cs.CV])</h2>
<h3>Jessie James P. Suarez, Prospero C. Naval Jr</h3>
<p>Anomaly detection in videos is a problem that has been studied for more than
a decade. This area has piqued the interest of researchers due to its wide
applicability. Because of this, there has been a wide array of approaches that
have been proposed throughout the years and these approaches range from
statistical-based approaches to machine learning-based approaches. Numerous
surveys have already been conducted on this area but this paper focuses on
providing an overview on the recent advances in the field of anomaly detection
using Deep Learning. Deep Learning has been applied successfully in many fields
of artificial intelligence such as computer vision, natural language processing
and more. This survey, however, focuses on how Deep Learning has improved and
provided more insights to the area of video anomaly detection. This paper
provides a categorization of the different Deep Learning approaches with
respect to their objectives. Additionally, it also discusses the commonly used
datasets along with the common evaluation metrics. Afterwards, a discussion
synthesizing all of the recent approaches is made to provide direction and
possible areas for future research.
</p>
<a href="http://arxiv.org/abs/2009.14146">arXiv:2009.14146</a> [<a href="http://arxiv.org/pdf/2009.14146">pdf</a>]

<h2>Enforcing nonholonomic constraints in Aerobat, a roosting flapping wing model. (arXiv:2009.14156v1 [cs.RO])</h2>
<h3>Eric Sihite, Alireza Ramezani</h3>
<p>Flapping wing flight is a challenging dynamical problem and is also a very
fascinating subject to study in the field of biomimetic robotics. A Bat, in
particular, has a very articulated armwing mechanism with high
degrees-of-freedom and flexibility which allows the animal to perform highly
dynamic and complex maneuvers, such as upside-down perching. This paper
presents the derivation of a multi-body dynamical system of a bio-inspired bat
robot called Aerobat which captures multiple biologically meaningful
degrees-of-freedom for flapping flight that is present in biological bats.
Then, the work attempts to manifest closed-loop aerial body reorientation and
preparation for landing through the manipulation of inertial dynamics and
aerodynamics by enforcing nonholonomic constraints onto the system. The
proposed design paradigm assumes for rapidly exponentially stable controllers
that enforce holonomic constraints in the joint space of the model. A model and
optimization-based nonlinear controller is applied to resolve the joint
trajectories such that the desired angular momentum about the roll axis is
achieved.
</p>
<a href="http://arxiv.org/abs/2009.14156">arXiv:2009.14156</a> [<a href="http://arxiv.org/pdf/2009.14156">pdf</a>]

<h2>Multi-View Consistency Loss for Improved Single-Image 3D Reconstruction of Clothed People. (arXiv:2009.14162v1 [cs.CV])</h2>
<h3>Akin Caliskan, Armin Mustafa, Evren Imre, Adrian Hilton</h3>
<p>We present a novel method to improve the accuracy of the 3D reconstruction of
clothed human shape from a single image. Recent work has introduced volumetric,
implicit and model-based shape learning frameworks for reconstruction of
objects and people from one or more images. However, the accuracy and
completeness for reconstruction of clothed people is limited due to the large
variation in shape resulting from clothing, hair, body size, pose and camera
viewpoint. This paper introduces two advances to overcome this limitation:
firstly a new synthetic dataset of realistic clothed people, 3DVH; and
secondly, a novel multiple-view loss function for training of monocular
volumetric shape estimation, which is demonstrated to significantly improve
generalisation and reconstruction accuracy. The 3DVH dataset of realistic
clothed 3D human models rendered with diverse natural backgrounds is
demonstrated to allows transfer to reconstruction from real images of people.
Comprehensive comparative performance evaluation on both synthetic and real
images of people demonstrates that the proposed method significantly
outperforms the previous state-of-the-art learning-based single image 3D human
shape estimation approaches achieving significant improvement of reconstruction
accuracy, completeness, and quality. An ablation study shows that this is due
to both the proposed multiple-view training and the new 3DVH dataset. The code
and the dataset can be found at the project website:
https://akincaliskan3d.github.io/MV3DH/.
</p>
<a href="http://arxiv.org/abs/2009.14162">arXiv:2009.14162</a> [<a href="http://arxiv.org/pdf/2009.14162">pdf</a>]

<h2>Contrastive Distillation on Intermediate Representations for Language Model Compression. (arXiv:2009.14167v1 [cs.CL])</h2>
<h3>Siqi Sun, Zhe Gan, Yu Cheng, Yuwei Fang, Shuohang Wang, Jingjing Liu</h3>
<p>Existing language model compression methods mostly use a simple L2 loss to
distill knowledge in the intermediate representations of a large BERT model to
a smaller one. Although widely used, this objective by design assumes that all
the dimensions of hidden representations are independent, failing to capture
important structural knowledge in the intermediate layers of the teacher
network. To achieve better distillation efficacy, we propose Contrastive
Distillation on Intermediate Representations (CoDIR), a principled knowledge
distillation framework where the student is trained to distill knowledge
through intermediate layers of the teacher via a contrastive objective. By
learning to distinguish positive sample from a large set of negative samples,
CoDIR facilitates the student's exploitation of rich information in teacher's
hidden layers. CoDIR can be readily applied to compress large-scale language
models in both pre-training and finetuning stages, and achieves superb
performance on the GLUE benchmark, outperforming state-of-the-art compression
methods.
</p>
<a href="http://arxiv.org/abs/2009.14167">arXiv:2009.14167</a> [<a href="http://arxiv.org/pdf/2009.14167">pdf</a>]

<h2>Self-Supervised Few-Shot Learning on Point Clouds. (arXiv:2009.14168v1 [cs.LG])</h2>
<h3>Charu Sharma, Manohar Kaul</h3>
<p>The increased availability of massive point clouds coupled with their utility
in a wide variety of applications such as robotics, shape synthesis, and
self-driving cars has attracted increased attention from both industry and
academia. Recently, deep neural networks operating on labeled point clouds have
shown promising results on supervised learning tasks like classification and
segmentation. However, supervised learning leads to the cumbersome task of
annotating the point clouds. To combat this problem, we propose two novel
self-supervised pre-training tasks that encode a hierarchical partitioning of
the point clouds using a cover-tree, where point cloud subsets lie within balls
of varying radii at each level of the cover-tree. Furthermore, our
self-supervised learning network is restricted to pre-train on the support set
(comprising of scarce training examples) used to train the downstream network
in a few-shot learning (FSL) setting. Finally, the fully-trained
self-supervised network's point embeddings are input to the downstream task's
network. We present a comprehensive empirical evaluation of our method on both
downstream classification and segmentation tasks and show that supervised
methods pre-trained with our self-supervised learning method significantly
improve the accuracy of state-of-the-art methods. Additionally, our method also
outperforms previous unsupervised methods in downstream classification tasks.
</p>
<a href="http://arxiv.org/abs/2009.14168">arXiv:2009.14168</a> [<a href="http://arxiv.org/pdf/2009.14168">pdf</a>]

<h2>Robust Detection of Objects under Periodic Motion with Gaussian Process Filtering. (arXiv:2009.14178v1 [cs.CV])</h2>
<h3>Joris Guerin, Anne Magaly de Paula Canuto, Luiz Marcos Garcia Goncalves</h3>
<p>Object Detection (OD) is an important task in Computer Vision with many
practical applications. For some use cases, OD must be done on videos, where
the object of interest has a periodic motion. In this paper, we formalize the
problem of periodic OD, which consists in improving the performance of an OD
model in the specific case where the object of interest is repeating similar
spatio-temporal trajectories with respect to the video frames. The proposed
approach is based on training a Gaussian Process to model the periodic motion,
and use it to filter out the erroneous predictions of the OD model. By
simulating various OD models and periodic trajectories, we demonstrate that
this filtering approach, which is entirely data-driven, improves the detection
performance by a large margin.
</p>
<a href="http://arxiv.org/abs/2009.14178">arXiv:2009.14178</a> [<a href="http://arxiv.org/pdf/2009.14178">pdf</a>]

<h2>Learning to Play against Any Mixture of Opponents. (arXiv:2009.14180v1 [cs.MA])</h2>
<h3>Max Olan Smith, Thomas Anthony, Yongzhao Wang, Michael P. Wellman</h3>
<p>Intuitively, experience playing against one mixture of opponents in a given
domain should be relevant for a different mixture in the same domain. We
propose a transfer learning method, Q-Mixing, that starts by learning Q-values
against each pure-strategy opponent. Then a Q-value for any distribution of
opponent strategies is approximated by appropriately averaging the separately
learned Q-values. From these components, we construct policies against all
opponent mixtures without any further training. We empirically validate
Q-Mixing in two environments: a simple grid-world soccer environment, and a
complicated cyber-security game. We find that Q-Mixing is able to successfully
transfer knowledge across any mixture of opponents. We next consider the use of
observations during play to update the believed distribution of opponents. We
introduce an opponent classifier---trained in parallel to Q-learning, using the
same data---and use the classifier results to refine the mixing of Q-values. We
find that Q-Mixing augmented with the opponent classifier function performs
comparably, and with lower variance, than training directly against a
mixed-strategy opponent.
</p>
<a href="http://arxiv.org/abs/2009.14180">arXiv:2009.14180</a> [<a href="http://arxiv.org/pdf/2009.14180">pdf</a>]

<h2>Deep Evolution for Facial Emotion Recognition. (arXiv:2009.14194v1 [cs.NE])</h2>
<h3>Emmanuel Dufourq, Bruce A. Bassett</h3>
<p>Deep facial expression recognition faces two challenges that both stem from
the large number of trainable parameters: long training times and a lack of
interpretability. We propose a novel method based on evolutionary algorithms,
that deals with both challenges by massively reducing the number of trainable
parameters, whilst simultaneously retaining classification performance, and in
some cases achieving superior performance. We are robustly able to reduce the
number of parameters on average by 95% (e.g. from 2M to 100k parameters) with
no loss in classification accuracy. The algorithm learns to choose small
patches from the image, relative to the nose, which carry the most important
information about emotion, and which coincide with typical human choices of
important features. Our work implements a novel form attention and shows that
evolutionary algorithms are a valuable addition to machine learning in the deep
learning era, both for reducing the number of parameters for facial expression
recognition and for providing interpretable features that can help reduce bias.
</p>
<a href="http://arxiv.org/abs/2009.14194">arXiv:2009.14194</a> [<a href="http://arxiv.org/pdf/2009.14194">pdf</a>]

<h2>A universal framework for learning the elliptical mixture model. (arXiv:1805.08045v5 [cs.LG] UPDATED)</h2>
<h3>Shengxi Li, Zeyang Yu, Danilo Mandic</h3>
<p>Mixture modelling using elliptical distributions promises enhanced
robustness, flexibility and stability over the widely employed Gaussian mixture
model (GMM). However, existing studies based on the elliptical mixture model
(EMM) are restricted to several specific types of elliptical probability
density functions, which are not supported by general solutions or systematic
analysis frameworks; this significantly limits the rigour and the power of EMMs
in applications. To this end, we propose a novel general framework for
estimating and analysing the EMMs, achieved through Riemannian manifold
optimisation. First, we investigate the relationships between Riemannian
manifolds and elliptical distributions, and the so established connection
between the original manifold and a reformulated one indicates a mismatch
between those manifolds, the major cause of failure of the existing
optimisation for solving general EMMs. We next propose a universal solver which
is based on the optimisation of a re-designed cost and prove the existence of
the same optimum as in the original problem; this is achieved in a simple, fast
and stable way. We further calculate the influence functions of the EMM as
theoretical bounds to quantify robustness to outliers. Comprehensive numerical
results demonstrate the ability of the proposed framework to accommodate EMMs
with different properties of individual functions in a stable way and with fast
convergence speed. Finally, the enhanced robustness and flexibility of the
proposed framework over the standard GMM are demonstrated both analytically and
through comprehensive simulations.
</p>
<a href="http://arxiv.org/abs/1805.08045">arXiv:1805.08045</a> [<a href="http://arxiv.org/pdf/1805.08045">pdf</a>]

<h2>Closed-Loop Memory GAN for Continual Learning. (arXiv:1811.01146v3 [cs.LG] UPDATED)</h2>
<h3>Amanda Rios, Laurent Itti</h3>
<p>Sequential learning of tasks using gradient descent leads to an unremitting
decline in the accuracy of tasks for which training data is no longer
available, termed catastrophic forgetting. Generative models have been explored
as a means to approximate the distribution of old tasks and bypass storage of
real data. Here we propose a cumulative closed-loop memory replay GAN (CloGAN)
provided with external regularization by a small memory unit selected for
maximum sample diversity. We evaluate incremental class learning using a
notoriously hard paradigm, single-headed learning, in which each task is a
disjoint subset of classes in the overall dataset, and performance is evaluated
on all previous classes. First, we show that when constructing a dynamic memory
unit to preserve sample heterogeneity, model performance asymptotically
approaches training on the full dataset. We then show that using a stochastic
generator to continuously output fresh new images during training increases
performance significantly further meanwhile generating quality images. We
compare our approach to several baselines including fine-tuning by gradient
descent (FGD), Elastic Weight Consolidation (EWC), Deep Generative Replay (DGR)
and Memory Replay GAN (MeRGAN). Our method has very low long-term memory cost,
the memory unit, as well as negligible intermediate memory storage.
</p>
<a href="http://arxiv.org/abs/1811.01146">arXiv:1811.01146</a> [<a href="http://arxiv.org/pdf/1811.01146">pdf</a>]

<h2>Learning Space Partitions for Nearest Neighbor Search. (arXiv:1901.08544v4 [cs.LG] UPDATED)</h2>
<h3>Yihe Dong, Piotr Indyk, Ilya Razenshteyn, Tal Wagner</h3>
<p>Space partitions of $\mathbb{R}^d$ underlie a vast and important class of
fast nearest neighbor search (NNS) algorithms. Inspired by recent theoretical
work on NNS for general metric spaces [Andoni, Naor, Nikolov, Razenshteyn,
Waingarten STOC 2018, FOCS 2018], we develop a new framework for building space
partitions reducing the problem to balanced graph partitioning followed by
supervised classification. We instantiate this general approach with the KaHIP
graph partitioner [Sanders, Schulz SEA 2013] and neural networks, respectively,
to obtain a new partitioning procedure called Neural Locality-Sensitive Hashing
(Neural LSH). On several standard benchmarks for NNS, our experiments show that
the partitions obtained by Neural LSH consistently outperform partitions found
by quantization-based and tree-based methods as well as classic, data-oblivious
LSH.
</p>
<a href="http://arxiv.org/abs/1901.08544">arXiv:1901.08544</a> [<a href="http://arxiv.org/pdf/1901.08544">pdf</a>]

<h2>An Attention-Guided Deep Regression Model for Landmark Detection in Cephalograms. (arXiv:1906.07549v3 [eess.IV] UPDATED)</h2>
<h3>Zhusi Zhong, Jie Li, Zhenxi Zhang, Zhicheng Jiao, Xinbo Gao</h3>
<p>Cephalometric tracing method is usually used in orthodontic diagnosis and
treatment planning. In this paper, we propose a deep learning based framework
to automatically detect anatomical landmarks in cephalometric X-ray images. We
train the deep encoder-decoder for landmark detection, and combine global
landmark configuration with local high-resolution feature responses. The
proposed frame-work is based on 2-stage u-net, regressing the multi-channel
heatmaps for land-mark detection. In this framework, we embed attention
mechanism with global stage heatmaps, guiding the local stage inferring, to
regress the local heatmap patches in a high resolution. Besides, the Expansive
Exploration strategy improves robustness while inferring, expanding the
searching scope without increasing model complexity. We have evaluated our
framework in the most widely-used public dataset of landmark detection in
cephalometric X-ray images. With less computation and manually tuning, our
framework achieves state-of-the-art results.
</p>
<a href="http://arxiv.org/abs/1906.07549">arXiv:1906.07549</a> [<a href="http://arxiv.org/pdf/1906.07549">pdf</a>]

<h2>A Dictionary Approach to Domain-Invariant Learning in Deep Networks. (arXiv:1909.11285v2 [cs.LG] UPDATED)</h2>
<h3>Ze Wang, Xiuyuan Cheng, Guillermo Sapiro, Qiang Qiu</h3>
<p>In this paper, we consider domain-invariant deep learning by explicitly
modeling domain shifts with only a small amount of domain-specific parameters
in a Convolutional Neural Network (CNN). By exploiting the observation that a
convolutional filter can be well approximated as a linear combination of a
small set of dictionary atoms, we show for the first time, both empirically and
theoretically, that domain shifts can be effectively handled by decomposing a
convolutional layer into a domain-specific atom layer and a domain-shared
coefficient layer, while both remain convolutional. An input channel will now
first convolve spatially only with each respective domain-specific dictionary
atom to "absorb" domain variations, and then output channels are linearly
combined using common decomposition coefficients trained to promote shared
semantics across domains. We use toy examples, rigorous analysis, and
real-world examples with diverse datasets and architectures, to show the
proposed plug-in framework's effectiveness in cross and joint domain
performance and domain adaptation. With the proposed architecture, we need only
a small set of dictionary atoms to model each additional domain, which brings a
negligible amount of additional parameters, typically a few hundred.
</p>
<a href="http://arxiv.org/abs/1909.11285">arXiv:1909.11285</a> [<a href="http://arxiv.org/pdf/1909.11285">pdf</a>]

<h2>Multi-task Batch Reinforcement Learning with Metric Learning. (arXiv:1909.11373v4 [cs.LG] UPDATED)</h2>
<h3>Jiachen Li, Quan Vuong, Shuang Liu, Minghua Liu, Kamil Ciosek, Henrik Iskov Christensen, Hao Su</h3>
<p>We tackle the Multi-task Batch Reinforcement Learning problem. Given multiple
datasets collected from different tasks, we train a multi-task policy to
perform well in unseen tasks sampled from the same distribution. The task
identities of the unseen tasks are not provided. To perform well, the policy
must infer the task identity from collected transitions by modelling its
dependency on states, actions and rewards. Because the different datasets may
have state-action distributions with large divergence, the task inference
module can learn to ignore the rewards and spuriously correlate $\textit{only}$
state-action pairs to the task identity, leading to poor test time performance.
To robustify task inference, we propose a novel application of the triplet
loss. To mine hard negative examples, we relabel the transitions from the
training tasks by approximating their reward functions. When we allow further
training on the unseen tasks, using the trained policy as an initialization
leads to significantly faster convergence compared to randomly initialized
policies (up to $80\%$ improvement and across 5 different Mujoco task
distributions). We name our method $\textbf{MBML}$
($\textbf{M}\text{ulti-task}$ $\textbf{B}\text{atch}$ RL with
$\textbf{M}\text{etric}$ $\textbf{L}\text{earning}$).
</p>
<a href="http://arxiv.org/abs/1909.11373">arXiv:1909.11373</a> [<a href="http://arxiv.org/pdf/1909.11373">pdf</a>]

<h2>Differentiable Sparsification for Deep Neural Networks. (arXiv:1910.03201v4 [cs.LG] UPDATED)</h2>
<h3>Yognjin Lee</h3>
<p>Deep neural networks have relieved the feature engineering burden on human
experts. However, comparable efforts are instead required to determine an
effective architecture. In addition, as the sizes of networks have over-grown,
a considerable amount of resources is also invested in reducing the sizes. The
sparsification of an over-complete model addresses these problems as it removes
redundant parameters or connections by pruning them away after training or
encouraging them to become zero during training. However, these approaches are
not fully differentiable and interrupt an end-to-end training process with the
stochastic gradient descent in that they require either a parameter selection
or soft-thresholding step. In this study, we proposed a fully differentiable
sparsification method for deep neural networks, which allows parameters to be
exactly zero during training. The proposed method can simultaneously learn the
sparsified structure and weights of networks using the stochastic gradient
descent.
</p>
<a href="http://arxiv.org/abs/1910.03201">arXiv:1910.03201</a> [<a href="http://arxiv.org/pdf/1910.03201">pdf</a>]

<h2>Find or Classify? Dual Strategy for Slot-Value Predictions on Multi-Domain Dialog State Tracking. (arXiv:1910.03544v3 [cs.CL] UPDATED)</h2>
<h3>Jian-Guo Zhang, Kazuma Hashimoto, Chien-Sheng Wu, Yao Wan, Philip S. Yu, Richard Socher, Caiming Xiong</h3>
<p>Dialog state tracking (DST) is a core component in task-oriented dialog
systems. Existing approaches for DST mainly fall into one of two categories,
namely, ontology-based and ontology-free methods. An ontology-based method
selects a value from a candidate-value list for each target slot, while an
ontology-free method extracts spans from dialog contexts. Recent work
introduced a BERT-based model to strike a balance between the two methods by
pre-defining categorical and non-categorical slots. However, it is not clear
enough which slots are better handled by either of the two slot types, and the
way to use the pre-trained model has not been well investigated. In this paper,
we propose a simple yet effective dual-strategy model for DST, by adapting a
single BERT-style reading comprehension model to jointly handle both the
categorical and non-categorical slots. Our experiments on the MultiWOZ datasets
show that our method significantly outperforms the BERT-based counterpart,
finding that the key is a deep interaction between the domain-slot and context
information. When evaluated on noisy (MultiWOZ 2.0) and cleaner (MultiWOZ 2.1)
settings, our method performs competitively and robustly across the two
different settings. Our method sets the new state of the art in the noisy
setting, while performing more robustly than the best model in the cleaner
setting. We also conduct a comprehensive error analysis on the dataset,
including the effects of the dual strategy for each slot, to facilitate future
research.
</p>
<a href="http://arxiv.org/abs/1910.03544">arXiv:1910.03544</a> [<a href="http://arxiv.org/pdf/1910.03544">pdf</a>]

<h2>Words of Estimative Correlation: Studying Verbalizations of Scatterplots. (arXiv:1911.12793v4 [cs.HC] UPDATED)</h2>
<h3>Rafael Henkin, Cagatay Turkay</h3>
<p>Natural language and visualization are being increasingly deployed together
for supporting data analysis in different ways, from multimodal interaction to
enriched data summaries and insights. Yet, researchers still lack systematic
knowledge on how viewers verbalize their interpretations of visualizations, and
how they interpret verbalizations of visualizations in such contexts. We
describe two studies aimed at identifying characteristics of data and charts
that are relevant in such tasks. The first study asks participants to verbalize
what they see in scatterplots that depict various levels of correlations. The
second study then asks participants to choose visualizations that match a given
verbal description of correlation. We extract key concepts from responses,
organize them in a taxonomy and analyze the categorized responses. We observe
that participants use a wide range of vocabulary across all scatterplots, but
particular concepts are preferred for higher levels of correlation. A
comparison between the studies reveals the ambiguity of some of the concepts.
We discuss how the results could inform the design of multimodal
representations aligned with the data and analytical tasks, and present a
research roadmap to deepen the understanding about visualizations and natural
language.
</p>
<a href="http://arxiv.org/abs/1911.12793">arXiv:1911.12793</a> [<a href="http://arxiv.org/pdf/1911.12793">pdf</a>]

<h2>Pixel-Semantic Revise of Position Learning A One-Stage Object Detector with A Shared Encoder-Decoder. (arXiv:2001.01057v2 [cs.CV] UPDATED)</h2>
<h3>Qian Li, Nan Guo, Xiaochun Ye, Dongrui Fan, Zhimin Tang</h3>
<p>Recently, many methods have been proposed for object detection. They cannot
detect objects by semantic features, adaptively. In this work, according to
channel and spatial attention mechanisms, we mainly analyze that different
methods detect objects adaptively. Some state-of-the-art detectors combine
different feature pyramids with many mechanisms to enhance multi-level semantic
information. However, they require more cost. This work addresses that by an
anchor-free detector with shared encoder-decoder with attention mechanism,
extracting shared features. We consider features of different levels from
backbone (e.g., ResNet-50) as the basis features. Then, we feed the features
into a simple module, followed by a detector header to detect objects.
Meantime, we use the semantic features to revise geometric locations, and the
detector is a pixel-semantic revising of position. More importantly, this work
analyzes the impact of different pooling strategies (e.g., mean, maximum or
minimum) on multi-scale objects, and finds the minimum pooling improve
detection performance on small objects better. Compared with state-of-the-art
MNC based on ResNet-101 for the standard MSCOCO 2014 baseline, our method
improves detection AP of 3.8%.
</p>
<a href="http://arxiv.org/abs/2001.01057">arXiv:2001.01057</a> [<a href="http://arxiv.org/pdf/2001.01057">pdf</a>]

<h2>FsNet: Feature Selection Network on High-dimensional Biological Data. (arXiv:2001.08322v2 [cs.LG] UPDATED)</h2>
<h3>Dinesh Singh, H&#xe9;ctor Climente-Gonz&#xe1;lez, Mathis Petrovich, Eiryo Kawakami, Makoto Yamada</h3>
<p>Biological data including gene expression data are generally high-dimensional
and require efficient, generalizable, and scalable machine-learning methods to
discover their complex nonlinear patterns. The recent advances in machine
learning can be attributed to deep neural networks (DNNs), which excel in
various tasks in terms of computer vision and natural language processing.
However, standard DNNs are not appropriate for high-dimensional datasets
generated in biology because they have many parameters, which in turn require
many samples. In this paper, we propose a DNN-based, nonlinear feature
selection method, called the feature selection network (FsNet), for
high-dimensional and small number of sample data. Specifically, FsNet comprises
a selection layer that selects features and a reconstruction layer that
stabilizes the training. Because a large number of parameters in the selection
and reconstruction layers can easily result in overfitting under a limited
number of samples, we use two tiny networks to predict the large, virtual
weight matrices of the selection and reconstruction layers. Experimental
results on several real-world, high-dimensional biological datasets demonstrate
the efficacy of the proposed method.
</p>
<a href="http://arxiv.org/abs/2001.08322">arXiv:2001.08322</a> [<a href="http://arxiv.org/pdf/2001.08322">pdf</a>]

<h2>Spiking Inception Module for Multi-layer Unsupervised Spiking Neural Networks. (arXiv:2001.10696v5 [cs.NE] UPDATED)</h2>
<h3>Mingyuan Meng, Xingyu Yang, Shanlin Xiao, Zhiyi Yu</h3>
<p>Spiking Neural Network (SNN), as a brain-inspired approach, is attracting
attention due to its potential to produce ultra-high-energy-efficient hardware.
Competitive learning based on Spike-Timing-Dependent Plasticity (STDP) is a
popular method to train an unsupervised SNN. However, previous unsupervised
SNNs trained through this method are limited to a shallow network with only one
learnable layer and cannot achieve satisfactory results when compared with
multi-layer SNNs. In this paper, we eased this limitation by: 1)We proposed a
Spiking Inception (Sp-Inception) module, inspired by the Inception module in
the Artificial Neural Network (ANN) literature. This module is trained through
STDP-based competitive learning and outperforms the baseline modules on
learning capability, learning efficiency, and robustness. 2)We proposed a
Pooling-Reshape-Activate (PRA) layer to make the Sp-Inception module stackable.
3)We stacked multiple Sp-Inception modules to construct multi-layer SNNs. Our
algorithm outperforms the baseline algorithms on the hand-written digit
classification task, and reaches state-of-the-art results on the MNIST dataset
among the existing unsupervised SNNs.
</p>
<a href="http://arxiv.org/abs/2001.10696">arXiv:2001.10696</a> [<a href="http://arxiv.org/pdf/2001.10696">pdf</a>]

<h2>Concept Whitening for Interpretable Image Recognition. (arXiv:2002.01650v2 [cs.LG] UPDATED)</h2>
<h3>Zhi Chen, Yijie Bei, Cynthia Rudin</h3>
<p>What does a neural network encode about a concept as we traverse through the
layers? Interpretability in machine learning is undoubtedly important, but the
calculations of neural networks are very challenging to understand. Attempts to
see inside their hidden layers can either be misleading, unusable, or rely on
the latent space to possess properties that it may not have. In this work,
rather than attempting to analyze a neural network posthoc, we introduce a
mechanism, called concept whitening (CW), to alter a given layer of the network
to allow us to better understand the computation leading up to that layer. When
a concept whitening module is added to a CNN, the axes of the latent space can
be aligned with concepts of interest. By experiment, we show that CW can
provide us a much clearer understanding for how the network gradually learns
concepts over layers without hurting predictive performance.
</p>
<a href="http://arxiv.org/abs/2002.01650">arXiv:2002.01650</a> [<a href="http://arxiv.org/pdf/2002.01650">pdf</a>]

<h2>BARK: Open Behavior Benchmarking in Multi-Agent Environments. (arXiv:2003.02604v2 [cs.MA] UPDATED)</h2>
<h3>Julian Bernhard, Klemens Esterle, Patrick Hart, Tobias Kessler</h3>
<p>Predicting and planning interactive behaviors in complex traffic situations
presents a challenging task. Especially in scenarios involving multiple traffic
participants that interact densely, autonomous vehicles still struggle to
interpret situations and to eventually achieve their own mission goal. As
driving tests are costly and challenging scenarios are hard to find and
reproduce, simulation is widely used to develop, test, and benchmark behavior
models. However, most simulations rely on datasets and simplistic behavior
models for traffic participants and do not cover the full variety of
real-world, interactive human behaviors. In this work, we introduce BARK, an
open-source behavior benchmarking environment designed to mitigate the
shortcomings stated above. In BARK, behavior models are (re-)used for planning,
prediction, and simulation. A range of models is currently available, such as
Monte-Carlo Tree Search and Reinforcement Learning-based behavior models. We
use a public dataset and sampling-based scenario generation to show the
inter-exchangeability of behavior models in BARK. We evaluate how well the
models used cope with interactions and how robust they are towards exchanging
behavior models. Our evaluation shows that BARK provides a suitable framework
for a systematic development of behavior models.
</p>
<a href="http://arxiv.org/abs/2003.02604">arXiv:2003.02604</a> [<a href="http://arxiv.org/pdf/2003.02604">pdf</a>]

<h2>DPANet: Depth Potentiality-Aware Gated Attention Network for RGB-D Salient Object Detection. (arXiv:2003.08608v4 [cs.CV] UPDATED)</h2>
<h3>Zuyao Chen, Runmin Cong, Qianqian Xu, Qingming Huang</h3>
<p>There are two main issues in RGB-D salient object detection: (1) how to
effectively integrate the complementarity from the cross-modal RGB-D data; (2)
how to prevent the contamination effect from the unreliable depth map. In fact,
these two problems are linked and intertwined, but the previous methods tend to
focus only on the first problem and ignore the consideration of depth map
quality, which may yield the model fall into the sub-optimal state. In this
paper, we address these two issues in a holistic model synergistically, and
propose a novel network named DPANet to explicitly model the potentiality of
the depth map and effectively integrate the cross-modal complementarity. By
introducing the depth potentiality perception, the network can perceive the
potentiality of depth information in a learning-based manner, and guide the
fusion process of two modal data to prevent the contamination occurred. The
gated multi-modality attention module in the fusion process exploits the
attention mechanism with a gate controller to capture long-range dependencies
from a cross-modal perspective. Experimental results compared with 15
state-of-the-art methods on 8 datasets demonstrate the validity of the proposed
approach both quantitatively and qualitatively.
</p>
<a href="http://arxiv.org/abs/2003.08608">arXiv:2003.08608</a> [<a href="http://arxiv.org/pdf/2003.08608">pdf</a>]

<h2>SAC: Accelerating and Structuring Self-Attention via Sparse Adaptive Connection. (arXiv:2003.09833v3 [cs.CL] UPDATED)</h2>
<h3>Xiaoya Li, Yuxian Meng, Mingxin Zhou, Qinghong Han, Fei Wu, Jiwei Li</h3>
<p>While the self-attention mechanism has been widely used in a wide variety of
tasks, it has the unfortunate property of a quadratic cost with respect to the
input length, which makes it difficult to deal with long inputs. In this paper,
we present a method for accelerating and structuring self-attentions: Sparse
Adaptive Connection (SAC). In SAC, we regard the input sequence as a graph and
attention operations are performed between linked nodes. In contrast with
previous self-attention models with pre-defined structures (edges), the model
learns to construct attention edges to improve task-specific performances. In
this way, the model is able to select the most salient nodes and reduce the
quadratic complexity regardless of the sequence length. Based on SAC, we show
that previous variants of self-attention models are its special cases. Through
extensive experiments on neural machine translation, language modeling, graph
representation learning and image classification, we demonstrate SAC is
competitive with state-of-the-art models while significantly reducing memory
cost.
</p>
<a href="http://arxiv.org/abs/2003.09833">arXiv:2003.09833</a> [<a href="http://arxiv.org/pdf/2003.09833">pdf</a>]

<h2>Steepest Descent Neural Architecture Optimization: Escaping Local Optimum with Signed Neural Splitting. (arXiv:2003.10392v4 [cs.LG] UPDATED)</h2>
<h3>Lemeng Wu, Mao Ye, Qi Lei, Jason D. Lee, Qiang Liu</h3>
<p>Developing efficient and principled neural architecture optimization methods
is a critical challenge of modern deep learning. Recently, Liu et al.[19]
proposed a splitting steepest descent (S2D) method that jointly optimizes the
neural parameters and architectures based on progressively growing network
structures by splitting neurons into multiple copies in a steepest descent
fashion. However, S2D suffers from a local optimality issue when all the
neurons become "splitting stable", a concept akin to local stability in
parametric optimization. In this work, we develop a significant and surprising
extension of the splitting descent framework that addresses the local
optimality issue. The idea is to observe that the original S2D is unnecessarily
restricted to splitting neurons into positive weighted copies. By simply
allowing both positive and negative weights during splitting, we can eliminate
the appearance of splitting stability in S2D and hence escape the local optima
to obtain better performance. By incorporating signed splittings, we
significantly extend the optimization power of splitting steepest descent both
theoretically and empirically. We verify our method on various challenging
benchmarks such as CIFAR-100, ImageNet and ModelNet40, on which we outperform
S2D and other advanced methods on learning accurate and energy-efficient neural
networks.
</p>
<a href="http://arxiv.org/abs/2003.10392">arXiv:2003.10392</a> [<a href="http://arxiv.org/pdf/2003.10392">pdf</a>]

<h2>A Collective Learning Framework to Boost GNN Expressiveness. (arXiv:2003.12169v2 [cs.LG] UPDATED)</h2>
<h3>Mengyue Hang, Jennifer Neville, Bruno Ribeiro</h3>
<p>Graph Neural Networks (GNNs) have recently been used for node and graph
classification tasks with great success, but GNNs model dependencies among the
attributes of nearby neighboring nodes rather than dependencies among observed
node labels. In this work, we consider the task of inductive node
classification using GNNs in supervised and semi-supervised settings, with the
goal of incorporating label dependencies. Because current GNNs are not
universal (i.e., most-expressive) graph representations, we propose a general
collective learning approach to increase the representation power of any
existing GNN. Our framework combines ideas from collective classification with
self-supervised learning, and uses a Monte Carlo approach to sampling
embeddings for inductive learning across graphs. We evaluate performance on
five real-world network datasets and demonstrate consistent, significant
improvement in node classification accuracy, for a variety of state-of-the-art
GNNs.
</p>
<a href="http://arxiv.org/abs/2003.12169">arXiv:2003.12169</a> [<a href="http://arxiv.org/pdf/2003.12169">pdf</a>]

<h2>Spectrally Consistent UNet for High Fidelity Image Transformations. (arXiv:2004.10696v2 [eess.IV] UPDATED)</h2>
<h3>Demetris Marnerides, Thomas Bashford-Rogers, Kurt Debattista</h3>
<p>Convolutional Neural Networks (CNNs) are the current de-facto models used for
many imaging tasks due to their high learning capacity as well as their
architectural qualities. The ubiquitous UNet architecture provides an efficient
and multi-scale solution that combines local and global information. Despite
the success of UNet architectures, the use of upsampling layers can cause
artefacts. In this work, a method for assessing the structural biases of UNets
and the effects these have on the outputs is presented, characterising their
impact in the Fourier domain. A new upsampling module is proposed, based on a
novel use of the Guided Image Filter, that provides spectrally consistent
outputs when used in a UNet architecture, forming the Guided UNet (GUNet). The
GUNet architecture is applied and evaluated for example applications of inverse
tone mapping/dynamic range expansion and colourisation from grey-scale images
and is shown to provide higher fidelity outputs.
</p>
<a href="http://arxiv.org/abs/2004.10696">arXiv:2004.10696</a> [<a href="http://arxiv.org/pdf/2004.10696">pdf</a>]

<h2>Towards Reasonably-Sized Character-Level Transformer NMT by Finetuning Subword Systems. (arXiv:2004.14280v2 [cs.CL] UPDATED)</h2>
<h3>Jind&#x159;ich Libovick&#xfd;, Alexander Fraser</h3>
<p>Applying the Transformer architecture on the character level usually requires
very deep architectures that are difficult and slow to train. These problems
can be partially overcome by incorporating a segmentation into tokens in the
model. We show that by initially training a subword model and then finetuning
it on characters, we can obtain a neural machine translation model that works
at the character level without requiring token segmentation. We use only the
vanilla 6-layer Transformer Base architecture. Our character-level models
better capture morphological phenomena and show more robustness to noise at the
expense of somewhat worse overall translation quality. Our study is a
significant step towards high-performance and easy to train character-based
models that are not extremely large.
</p>
<a href="http://arxiv.org/abs/2004.14280">arXiv:2004.14280</a> [<a href="http://arxiv.org/pdf/2004.14280">pdf</a>]

<h2>Active Preference Learning using Maximum Regret. (arXiv:2005.04067v2 [cs.RO] UPDATED)</h2>
<h3>Nils Wilde, Dana Kulic, Stephen L. Smith</h3>
<p>We study active preference learning as a framework for intuitively specifying
the behaviour of autonomous robots. In active preference learning, a user
chooses the preferred behaviour from a set of alternatives, from which the
robot learns the user's preferences, modeled as a parameterized cost function.
Previous approaches present users with alternatives that minimize the
uncertainty over the parameters of the cost function. However, different
parameters might lead to the same optimal behaviour; as a consequence the
solution space is more structured than the parameter space. We exploit this by
proposing a query selection that greedily reduces the maximum error ratio over
the solution space. In simulations we demonstrate that the proposed approach
outperforms other state of the art techniques in both learning efficiency and
ease of queries for the user. Finally, we show that evaluating the learning
based on the similarities of solutions instead of the similarities of weights
allows for better predictions for different scenarios.
</p>
<a href="http://arxiv.org/abs/2005.04067">arXiv:2005.04067</a> [<a href="http://arxiv.org/pdf/2005.04067">pdf</a>]

<h2>A Robust Interpretable Deep Learning Classifier for Heart Anomaly Detection Without Segmentation. (arXiv:2005.10480v2 [cs.SD] UPDATED)</h2>
<h3>Theekshana Dissanayake, Tharindu Fernando, Simon Denman, Sridha Sridharan, Houman Ghaemmaghami, Clinton Fookes</h3>
<p>Traditionally, abnormal heart sound classification is framed as a three-stage
process. The first stage involves segmenting the phonocardiogram to detect
fundamental heart sounds; after which features are extracted and classification
is performed. Some researchers in the field argue the segmentation step is an
unwanted computational burden, whereas others embrace it as a prior step to
feature extraction. When comparing accuracies achieved by studies that have
segmented heart sounds before analysis with those who have overlooked that
step, the question of whether to segment heart sounds before feature extraction
is still open. In this study, we explicitly examine the importance of heart
sound segmentation as a prior step for heart sound classification, and then
seek to apply the obtained insights to propose a robust classifier for abnormal
heart sound detection. Furthermore, recognizing the pressing need for
explainable Artificial Intelligence (AI) models in the medical domain, we also
unveil hidden representations learned by the classifier using model
interpretation techniques. Experimental results demonstrate that the
segmentation plays an essential role in abnormal heart sound classification.
Our new classifier is also shown to be robust, stable and most importantly,
explainable, with an accuracy of almost 100% on the widely used PhysioNet
dataset.
</p>
<a href="http://arxiv.org/abs/2005.10480">arXiv:2005.10480</a> [<a href="http://arxiv.org/pdf/2005.10480">pdf</a>]

<h2>Feature Robust Optimal Transport for High-dimensional Data. (arXiv:2005.12123v4 [stat.ML] UPDATED)</h2>
<h3>Mathis Petrovich, Chao Liang, Ryoma Sato, Yanbin Liu, Yao-Hung Hubert Tsai, Linchao Zhu, Yi Yang, Ruslan Salakhutdinov, Makoto Yamada</h3>
<p>Optimal transport is a machine learning problem with applications including
distribution comparison, feature selection, and generative adversarial
networks. In this paper, we propose feature-robust optimal transport (FROT) for
high-dimensional data, which solves high-dimensional OT problems using feature
selection to avoid the curse of dimensionality. Specifically, we find a
transport plan with discriminative features. To this end, we formulate the FROT
problem as a min--max optimization problem. We then propose a convex
formulation of the FROT problem and solve it using a Frank--Wolfe-based
optimization algorithm, whereby the subproblem can be efficiently solved using
the Sinkhorn algorithm. Since FROT finds the transport plan from selected
features, it is robust to noise features. To show the effectiveness of FROT, we
propose using the FROT algorithm for the layer selection problem in deep neural
networks for semantic correspondence. By conducting synthetic and benchmark
experiments, we demonstrate that the proposed method can find a strong
correspondence by determining important layers. We show that the FROT algorithm
achieves state-of-the-art performance in real-world semantic correspondence
datasets.
</p>
<a href="http://arxiv.org/abs/2005.12123">arXiv:2005.12123</a> [<a href="http://arxiv.org/pdf/2005.12123">pdf</a>]

<h2>Federated Face Presentation Attack Detection. (arXiv:2005.14638v2 [cs.CV] UPDATED)</h2>
<h3>Rui Shao, Pramuditha Perera, Pong C. Yuen, Vishal M. Patel</h3>
<p>Face presentation attack detection (fPAD) plays a critical role in the modern
face recognition pipeline. A face presentation attack detection model with good
generalization can be obtained when it is trained with face images from
different input distributions and different types of spoof attacks. In reality,
training data (both real face images and spoof images) are not directly shared
between data owners due to legal and privacy issues. In this paper, with the
motivation of circumventing this challenge, we propose Federated Face
Presentation Attack Detection (FedPAD) framework. FedPAD simultaneously takes
advantage of rich fPAD information available at different data owners while
preserving data privacy. In the proposed framework, each data owner (referred
to as \textit{data centers}) locally trains its own fPAD model. A server learns
a global fPAD model by iteratively aggregating model updates from all data
centers without accessing private data in each of them. Once the learned global
model converges, it is used for fPAD inference. We introduce the experimental
setting to evaluate the proposed FedPAD framework and carry out extensive
experiments to provide various insights about federated learning for fPAD.
</p>
<a href="http://arxiv.org/abs/2005.14638">arXiv:2005.14638</a> [<a href="http://arxiv.org/pdf/2005.14638">pdf</a>]

<h2>RISCuer: A Reliable Multi-UAV Search and Rescue Testbed. (arXiv:2006.06966v3 [cs.RO] UPDATED)</h2>
<h3>Mohamed Abdelkader, Usman A. Fiaz, Noureddine Toumi, Mohamed A. Mabrok, Jeff S. Shamma</h3>
<p>We present the Robotics Intelligent Systems &amp; Control (RISC) Lab multiagent
testbed for reliable search and rescue and aerial transport in outdoor
environments. The system consists of a team of three multirotor unmanned aerial
vehicles (UAVs), which are capable of autonomously searching, picking up, and
transporting randomly distributed objects in an outdoor field. The method
involves vision based object detection and localization, passive aerial
grasping with our novel design, GPS based UAV navigation, and safe release of
the objects at the drop zone. Our cooperative strategy ensures safe spatial
separation between UAVs at all times and we prevent any conflicts at the drop
zone using communication enabled consensus. All computation is performed
onboard each UAV. We describe the complete software and hardware architecture
for the system and demonstrate its reliable performance using comprehensive
outdoor experiments, and by comparing our results with some recent, similar
works.
</p>
<a href="http://arxiv.org/abs/2006.06966">arXiv:2006.06966</a> [<a href="http://arxiv.org/pdf/2006.06966">pdf</a>]

<h2>Bidirectional Model-based Policy Optimization. (arXiv:2007.01995v2 [cs.LG] UPDATED)</h2>
<h3>Hang Lai, Jian Shen, Weinan Zhang, Yong Yu</h3>
<p>Model-based reinforcement learning approaches leverage a forward dynamics
model to support planning and decision making, which, however, may fail
catastrophically if the model is inaccurate. Although there are several
existing methods dedicated to combating the model error, the potential of the
single forward model is still limited. In this paper, we propose to
additionally construct a backward dynamics model to reduce the reliance on
accuracy in forward model predictions. We develop a novel method, called
Bidirectional Model-based Policy Optimization (BMPO) to utilize both the
forward model and backward model to generate short branched rollouts for policy
optimization. Furthermore, we theoretically derive a tighter bound of return
discrepancy, which shows the superiority of BMPO against the one using merely
the forward model. Extensive experiments demonstrate that BMPO outperforms
state-of-the-art model-based methods in terms of sample efficiency and
asymptotic performance.
</p>
<a href="http://arxiv.org/abs/2007.01995">arXiv:2007.01995</a> [<a href="http://arxiv.org/pdf/2007.01995">pdf</a>]

<h2>Data-driven geophysics: from dictionary learning to deep learning. (arXiv:2007.06183v2 [physics.geo-ph] UPDATED)</h2>
<h3>Siwei Yu, Jianwei Ma</h3>
<p>Understanding the principles of geophysical phenomena is an essential and
challenging task. "Model-driven" approaches have supported the development of
geophysics for a long time; however, such methods suffer from the curse of
dimensionality and may inaccurately model the subsurface. "Data-driven"
techniques may overcome these issues with increasingly available geophysical
data. In this article, we review the basic concepts of and recent advances in
data-driven approaches from dictionary learning to deep learning in a variety
of geophysical scenarios. Explorational geophysics including data processing,
inversion and interpretation will be mainly focused. Artificial intelligence
applications on geoscience involving deep Earth, earthquake, water resource,
atmospheric science, satellite remoe sensing and space sciences are also
reviewed. We present a coding tutorial and a summary of tips for beginners and
interested geophysical readers to rapidly explore deep learning. Some promising
directions are provided for future research involving deep learning in
geophysics, such as unsupervised learning, transfer learning, multimodal deep
learning, federated learning, uncertainty estimation, and activate learning.
</p>
<a href="http://arxiv.org/abs/2007.06183">arXiv:2007.06183</a> [<a href="http://arxiv.org/pdf/2007.06183">pdf</a>]

<h2>Competing Bandits: The Perils of Exploration Under Competition. (arXiv:2007.10144v2 [cs.GT] UPDATED)</h2>
<h3>Guy Aridor, Yishay Mansour, Aleksandrs Slivkins, Zhiwei Steven Wu</h3>
<p>Most online platforms strive to learn from interactions with consumers, and
many engage in exploration: making potentially suboptimal choices for the sake
of acquiring new information. We initiate a study of the interplay between
exploration and competition: how such platforms balance the exploration for
learning and the competition for consumers. Here consumers play three distinct
roles: they are customers that generate revenue, they are sources of data for
learning, and they are self-interested agents which choose among the competing
platforms.

We consider a stylized duopoly model in which two firms face the same problem
instance of multi-armed bandits. Users arrive one by one and choose between the
two firms, so that each firm makes progress on its bandit instance only if it
is chosen. We study whether and to what extent competition incentivizes the
adoption of better bandit algorithms, and whether it leads to welfare increases
for consumers. We find that stark competition induces firms to commit to a
"greedy" bandit algorithm that leads to low consumer welfare. However, we find
that weakening competition by providing firms with some "free" consumers
incentivizes better exploration strategies and increases consumer welfare. We
investigate two channels for weakening the competition: relaxing the
rationality of consumers and giving one firm a first-mover advantage. We
provide a mix of theoretical results and numerical simulations. Our findings
are closely related to the "competition vs. innovation" relationship, a
well-studied theme in economics. They also elucidate the first-mover advantage
in the digital economy by exploring the role that data can play as a barrier to
entry in online markets.
</p>
<a href="http://arxiv.org/abs/2007.10144">arXiv:2007.10144</a> [<a href="http://arxiv.org/pdf/2007.10144">pdf</a>]

<h2>RoboTed: a case study in Ethical Risk Assessment. (arXiv:2007.15864v2 [cs.RO] UPDATED)</h2>
<h3>Alan F.T. Winfield, Katie Winkle</h3>
<p>Risk Assessment is a well known and powerful method for discovering and
mitigating risks, and hence improving safety. Ethical Risk Assessment uses the
same approach but extends the envelope of risk to cover ethical risks in
addition to safety risks. In this paper we outline Ethical Risk Assessment
(ERA) and set ERA within the broader framework of Responsible Robotics. We then
illustrate ERA with a case study of a hypothetical smart robot toy teddy bear:
RoboTed. The case study shows the value of ERA and how consideration of ethical
risks can prompt design changes, resulting in a more ethical and sustainable
robot.
</p>
<a href="http://arxiv.org/abs/2007.15864">arXiv:2007.15864</a> [<a href="http://arxiv.org/pdf/2007.15864">pdf</a>]

<h2>Accurate and Efficient Intracranial Hemorrhage Detection and Subtype Classification in 3D CT Scans with Convolutional and Long Short-Term Memory Neural Networks. (arXiv:2008.00302v3 [cs.CV] UPDATED)</h2>
<h3>Mihail Burduja, Radu Tudor Ionescu, Nicolae Verga</h3>
<p>In this paper, we present our system for the RSNA Intracranial Hemorrhage
Detection challenge. The proposed system is based on a lightweight deep neural
network architecture composed of a convolutional neural network (CNN) that
takes as input individual CT slices, and a Long Short-Term Memory (LSTM)
network that takes as input feature embeddings provided by the CNN. For
efficient processing, we consider various feature selection methods to produce
a subset of useful CNN features for the LSTM. Furthermore, we reduce the CT
slices by a factor of 2x, allowing ourselves to train the model faster. Even if
our model is designed to balance speed and accuracy, we report a weighted mean
log loss of 0.04989 on the final test set, which places us in the top 30
ranking (2%) from a total of 1345 participants. Although our computing
infrastructure does not allow it, processing CT slices at their original scale
is likely to improve performance. In order to enable others to reproduce our
results, we provide our code as open source at
https://github.com/warchildmd/ihd. After the challenge, we conducted a
subjective intracranial hemorrhage detection assessment by radiologists,
indicating that the performance of our deep model is on par with that of
doctors specialized in reading CT scans. Another contribution of our work is to
integrate Grad-CAM visualizations in our system, providing useful explanations
for its predictions. We therefore consider our system as a viable option when a
fast diagnosis or a second opinion on intracranial hemorrhage detection are
needed.
</p>
<a href="http://arxiv.org/abs/2008.00302">arXiv:2008.00302</a> [<a href="http://arxiv.org/pdf/2008.00302">pdf</a>]

<h2>Optimizing Information Loss Towards Robust Neural Networks. (arXiv:2008.03072v2 [cs.CR] UPDATED)</h2>
<h3>Philip Sperl, Konstantin B&#xf6;ttinger</h3>
<p>Neural Networks (NNs) are vulnerable to adversarial examples. Such inputs
differ only slightly from their benign counterparts yet provoke
misclassifications of the attacked NNs. The required perturbations to craft the
examples are often negligible and even human imperceptible. To protect deep
learning-based systems from such attacks, several countermeasures have been
proposed with adversarial training still being considered the most effective.
Here, NNs are iteratively retrained using adversarial examples forming a
computational expensive and time consuming process often leading to a
performance decrease. To overcome the downsides of adversarial training while
still providing a high level of security, we present a new training approach we
call \textit{entropic retraining}. Based on an information-theoretic-inspired
analysis, entropic retraining mimics the effects of adversarial training
without the need of the laborious generation of adversarial examples. We
empirically show that entropic retraining leads to a significant increase in
NNs' security and robustness while only relying on the given original data.
With our prototype implementation we validate and show the effectiveness of our
approach for various NN architectures and data sets.
</p>
<a href="http://arxiv.org/abs/2008.03072">arXiv:2008.03072</a> [<a href="http://arxiv.org/pdf/2008.03072">pdf</a>]

<h2>Hierarchical Reinforcement Learning in StarCraft II with Human Expertise in Subgoals Selection. (arXiv:2008.03444v3 [cs.AI] UPDATED)</h2>
<h3>Xinyi Xu, Tiancheng Huang, Pengfei Wei, Akshay Narayan, Tze-Yun Leong</h3>
<p>This work is inspired by recent advances in hierarchical reinforcement
learning (HRL) (Barto and Mahadevan 2003; Hengst 2010), and improvements in
learning efficiency from heuristic-based subgoal selection, experience replay
(Lin 1993; Andrychowicz et al. 2017), and task-based curriculum learning
(Bengio et al. 2009; Zaremba and Sutskever 2014). We propose a new method to
integrate HRL, experience replay and effective subgoal selection through an
implicit curriculum design based on human expertise to support sample-efficient
learning and enhance interpretability of the agent's behavior. Human expertise
remains indispensable in many areas such as medicine (Buch, Ahmed, and
Maruthappu 2018) and law (Cath 2018), where interpretability, explainability
and transparency are crucial in the decision making process, for ethical and
legal reasons. Our method simplifies the complex task sets for achieving the
overall objectives by decomposing them into subgoals at different levels of
abstraction. Incorporating relevant subjective knowledge also significantly
reduces the computational resources spent in exploration for RL, especially in
high speed, changing, and complex environments where the transition dynamics
cannot be effectively learned and modelled in a short time. Experimental
results in two StarCraft II (SC2) (Vinyals et al. 2017) minigames demonstrate
that our method can achieve better sample efficiency than flat and end-to-end
RL methods, and provides an effective method for explaining the agent's
performance.
</p>
<a href="http://arxiv.org/abs/2008.03444">arXiv:2008.03444</a> [<a href="http://arxiv.org/pdf/2008.03444">pdf</a>]

<h2>Assigning function to protein-protein interactions: a weakly supervised BioBERT based approach using PubMed abstracts. (arXiv:2008.08727v2 [cs.CL] UPDATED)</h2>
<h3>Aparna Elangovan, Melissa Davis, Karin Verspoor</h3>
<p>Motivation: Protein-protein interactions (PPI) are critical to the function
of proteins in both normal and diseased cells, and many critical protein
functions are mediated by interactions.Knowledge of the nature of these
interactions is important for the construction of networks to analyse
biological data. However, only a small percentage of PPIs captured in protein
interaction databases have annotations of function available, e.g. only 4% of
PPI are functionally annotated in the IntAct database. Here, we aim to label
the function type of PPIs by extracting relationships described in PubMed
abstracts.

Method: We create a weakly supervised dataset from the IntAct PPI database
containing interacting protein pairs with annotated function and associated
abstracts from the PubMed database. We apply a state-of-the-art deep learning
technique for biomedical natural language processing tasks, BioBERT, to build a
model - dubbed PPI-BioBERT - for identifying the function of PPIs. In order to
extract high quality PPI functions at large scale, we use an ensemble of
PPI-BioBERT models to improve uncertainty estimation and apply an interaction
type-specific threshold to counteract the effects of variations in the number
of training samples per interaction type.

Results: We scan 18 million PubMed abstracts to automatically identify 3253
new typed PPIs, including phosphorylation and acetylation interactions, with an
overall precision of 46% (87% for acetylation) based on a human-reviewed
sample. This work demonstrates that analysis of biomedical abstracts for PPI
function extraction is a feasible approach to substantially increasing the
number of interactions annotated with function captured in online databases.
</p>
<a href="http://arxiv.org/abs/2008.08727">arXiv:2008.08727</a> [<a href="http://arxiv.org/pdf/2008.08727">pdf</a>]

<h2>FeatGraph: A Flexible and Efficient Backend for Graph Neural Network Systems. (arXiv:2008.11359v2 [cs.LG] UPDATED)</h2>
<h3>Yuwei Hu, Zihao Ye, Minjie Wang, Jiali Yu, Da Zheng, Mu Li, Zheng Zhang, Zhiru Zhang, Yida Wang</h3>
<p>Graph neural networks (GNNs) are gaining increasing popularity as a promising
approach to machine learning on graphs. Unlike traditional graph workloads
where each vertex/edge is associated with a scalar, GNNs attach a feature
tensor to each vertex/edge. This additional feature dimension, along with
consequently more complex vertex- and edge-wise computations, has enormous
implications on locality and parallelism, which existing graph processing
systems fail to exploit.

This paper proposes FeatGraph to accelerate GNN workloads by co-optimizing
graph traversal and feature dimension computation. FeatGraph provides a
flexible programming interface to express diverse GNN models by composing
coarse-grained sparse templates with fine-grained user-defined functions (UDFs)
on each vertex/edge. FeatGraph incorporates optimizations for graph traversal
into the sparse templates and allows users to specify optimizations for UDFs
with a feature dimension schedule (FDS). FeatGraph speeds up end-to-end GNN
training and inference by up to 32x on CPU and 7x on GPU.
</p>
<a href="http://arxiv.org/abs/2008.11359">arXiv:2008.11359</a> [<a href="http://arxiv.org/pdf/2008.11359">pdf</a>]

<h2>Towards Demystifying Dimensions of Source Code Embeddings. (arXiv:2008.13064v3 [cs.LG] UPDATED)</h2>
<h3>Md Rafiqul Islam Rabin, Arjun Mukherjee, Omprakash Gnawali, Mohammad Amin Alipour</h3>
<p>Source code representations are key in applying machine learning techniques
for processing and analyzing programs. A popular approach in representing
source code is neural source code embeddings that represents programs with
high-dimensional vectors computed by training deep neural networks on a large
volume of programs. Although successful, there is little known about the
contents of these vectors and their characteristics. In this paper, we present
our preliminary results towards better understanding the contents of code2vec
neural source code embeddings. In particular, in a small case study, we use the
code2vec embeddings to create binary SVM classifiers and compare their
performance with the handcrafted features. Our results suggest that the
handcrafted features can perform very close to the highly-dimensional code2vec
embeddings, and the information gains are more evenly distributed in the
code2vec embeddings compared to the handcrafted features. We also find that the
code2vec embeddings are more resilient to the removal of dimensions with low
information gains than the handcrafted features. We hope our results serve a
stepping stone toward principled analysis and evaluation of these code
representations.
</p>
<a href="http://arxiv.org/abs/2008.13064">arXiv:2008.13064</a> [<a href="http://arxiv.org/pdf/2008.13064">pdf</a>]

<h2>Variational Deep Learning for the Identification and Reconstruction of Chaotic and Stochastic Dynamical Systems from Noisy and Partial Observations. (arXiv:2009.02296v3 [cs.LG] UPDATED)</h2>
<h3>Duong Nguyen, Said Ouala, Lucas Drumetz, Ronan Fablet</h3>
<p>The data-driven recovery of the unknown governing equations of dynamical
systems has recently received an increasing interest. However, the
identification of the governing equations remains challenging when dealing with
noisy and partial observations. Here, we address this challenge and investigate
variational deep learning schemes. Within the proposed framework, we jointly
learn an inference model to reconstruct the true states of the system from
series of noisy and partial data and the governing equations of these states.
In doing so, this framework bridges classical data assimilation and
state-of-the-art machine learning techniques and we show that it generalizes
state-of-the-art methods. Importantly, both the inference model and the
governing equations embed stochastic components to account for stochastic
variabilities, model errors and reconstruction uncertainties. Various
experiments on chaotic and stochastic dynamical systems support the relevance
of our scheme w.r.t. state-of-the-art approaches.
</p>
<a href="http://arxiv.org/abs/2009.02296">arXiv:2009.02296</a> [<a href="http://arxiv.org/pdf/2009.02296">pdf</a>]

<h2>A Deep Learning Approach to Tongue Detection for Pediatric Population. (arXiv:2009.02397v3 [cs.CV] UPDATED)</h2>
<h3>Javad Rahimipour Anaraki, Silvia Orlandi, Tom Chau</h3>
<p>Children with severe disabilities and complex communication needs face
limitations in the usage of access technology (AT) devices. Conventional ATs
(e.g., mechanical switches) can be insufficient for nonverbal children and
those with limited voluntary motion control. Automatic techniques for the
detection of tongue gestures represent a promising pathway. Previous studies
have shown the robustness of tongue detection algorithms on adult participants,
but further research is needed to use these methods with children. In this
study, a network architecture for tongue-out gesture recognition was
implemented and evaluated on videos recorded in a naturalistic setting when
children were playing a video-game. A cascade object detector algorithm was
used to detect the participants' faces, and an automated classification scheme
for tongue gesture detection was developed using a convolutional neural network
(CNN). In evaluation experiments conducted, the network was trained using
adults and children's images. The network classification accuracy was evaluated
using leave-one-subject-out cross-validation. Preliminary classification
results obtained from the analysis of videos of five typically developing
children showed an accuracy of up to 99% in predicting tongue-out gestures.
Moreover, we demonstrated that using only children data for training the
classifier yielded better performance than adult's one supporting the need for
pediatric tongue gesture datasets.
</p>
<a href="http://arxiv.org/abs/2009.02397">arXiv:2009.02397</a> [<a href="http://arxiv.org/pdf/2009.02397">pdf</a>]

<h2>Activation Relaxation: A Local Dynamical Approximation to Backpropagation in the Brain. (arXiv:2009.05359v3 [cs.NE] UPDATED)</h2>
<h3>Beren Millidge, Alexander Tschantz, Anil K Seth, Christopher L Buckley</h3>
<p>Can the powerful backpropagation of error (backprop) learning algorithm be
formulated in a manner suitable for implementation in neural circuitry? The
primary challenge is to ensure that any candidate formulation uses only local
information, rather than relying on global (error) signals, as in orthodox
backprop. Recently several algorithms for approximating backprop using only
local signals have been proposed. However, these algorithms typically impose
other requirements which challenge biological plausibility: for example,
requiring complex and precise connectivity schemes (predictive coding), or
multiple sequential backwards phases with information being stored across
phases (equilibrium-prop). Here, we propose a novel local algorithm, Activation
Relaxation (AR), which is motivated by constructing the backpropagation
gradient as the equilibrium point of a dynamical system. Our algorithm
converges robustly and exactly to the correct backpropagation gradients,
requires only a single type of computational unit, utilises only a single
backwards phase, and can perform credit assignment on arbitrary computation
graphs. We illustrate these properties by training deep neural networks on
visual classification tasks, and we describe simplifications to the algorithm
which remove further obstacles to neurobiological implementation (for example,
the weight-transport problem, and the use of nonlinear derivatives), while
preserving performance.
</p>
<a href="http://arxiv.org/abs/2009.05359">arXiv:2009.05359</a> [<a href="http://arxiv.org/pdf/2009.05359">pdf</a>]

<h2>Grassmannian diffusion maps based dimension reduction and classification for high-dimensional data. (arXiv:2009.07547v2 [cs.LG] UPDATED)</h2>
<h3>K. R. M. dos Santos, D. G. Giovanis, M. D. Shields</h3>
<p>Diffusion Maps is a nonlinear dimensionality reduction technique used to
embed high-dimensional data in a low-dimensional Euclidean space, where the
notion of distance is due to the transition probability of a random walk over
the dataset. However, the conventional approach is not capable to reveal the
dataset underlying subspace structure, a useful information for machine
learning applications such as object classification and face recognition. To
circumvent this limitation, a novel nonlinear dimensionality reduction
technique, referred to as Grassmannian Diffusion Maps, is developed herein
relying on the affinity between subspaces represented by points on the
Grassmann manifold. To this aim, a kernel matrix is used to construct the
transition matrix of a random walk on a graph connecting points on the
Grassmann manifold for posterior determination of the diffusion coordinates
embedding the data in a low-dimensional Euclidean space. In this paper, three
examples are considered to evaluate the performance of both conventional and
Grassmannian Diffusion Maps. First, a "toy" example shows that the Grassmannian
Diffusion Maps can identify a well-defined parametrization of points on the
unit sphere, representing a Grassmann manifold. The second example shows that
the Grassmannian Diffusion Maps outperforms the conventional Diffusion Maps in
classifying elements, later recovered by a conventional clustering, of a
dataset by their intrinsic characteristics. In the last example, a novel data
classification/recognition technique is developed based on the construction of
an overcomplete dictionary of reduced dimension whose atoms are given by the
diffusion coordinates. A face recognition problem is solved and high
recognition rates (i.e., 95% in the best-case scenario) are obtained using a
fraction of the data required by conventional methods.
</p>
<a href="http://arxiv.org/abs/2009.07547">arXiv:2009.07547</a> [<a href="http://arxiv.org/pdf/2009.07547">pdf</a>]

<h2>GraphCodeBERT: Pre-training Code Representations with Data Flow. (arXiv:2009.08366v2 [cs.SE] UPDATED)</h2>
<h3>Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, Michele Tufano, Shao Kun Deng, Colin Clement, Dawn Drain, Neel Sundaresan, Jian Yin, Daxin Jiang, Ming Zhou</h3>
<p>Pre-trained models for programming language have achieved dramatic empirical
improvements on a variety of code-related tasks such as code search, code
completion, code summarization, etc. However, existing pre-trained models
regard a code snippet as a sequence of tokens, while ignoring the inherent
structure of code, which provides crucial code semantics and would enhance the
code understanding process. We present GraphCodeBERT, a pre-trained model for
programming language that considers the inherent structure of code. Instead of
taking syntactic-level structure of code like abstract syntax tree (AST), we
use data flow in the pre-training stage, which is a semantic-level structure of
code that encodes the relation of "where-the-value-comes-from" between
variables. Such a semantic-level structure is neat and does not bring an
unnecessarily deep hierarchy of AST, the property of which makes the model more
efficient. We develop GraphCodeBERT based on Transformer. In addition to using
the task of masked language modeling, we introduce two structure-aware
pre-training tasks. One is to predict code structure edges, and the other is to
align representations between source code and code structure. We implement the
model in an efficient way with a graph-guided masked attention function to
incorporate the code structure. We evaluate our model on four tasks, including
code search, clone detection, code translation, and code refinement. Results
show that code structure and newly introduced pre-training tasks can improve
GraphCodeBERT and achieves state-of-the-art performance on the four downstream
tasks. We further show that the model prefers structure-level attentions over
token-level attentions in the task of code search.
</p>
<a href="http://arxiv.org/abs/2009.08366">arXiv:2009.08366</a> [<a href="http://arxiv.org/pdf/2009.08366">pdf</a>]

<h2>Psoriasis Severity Assessment with a Similarity-Clustering Machine Learning Approach Reduces Intra- and Inter-observation variation. (arXiv:2009.08997v2 [cs.CV] UPDATED)</h2>
<h3>Arman Garakani, Martin Malmstedt-Miller, Ionela Manole, Adrian Y. Rossler, John R. Zibert</h3>
<p>Psoriasis is a complex disease with many variations in genotype and
phenotype. General advancements in medicine has further complicated both
assessments and treatment for both physicians and dermatologist alike. Even
with all of our technological progress we still primarily use the assessment
tool Psoriasis Area and Severity Index (PASI) for severity assessments which
was developed in the 1970s. In this study we evaluate a method involving
digital images, a comparison web application and similarity clustering,
developed to improve the assessment tool in terms of intra- and inter-observer
variation. Images of patients was collected from a mobile device. Images were
captured of the same lesion area taken approximately 1 week apart. Five
dermatologists evaluated the severity of psoriasis by modified-PASI, absolute
scoring and a relative pairwise PASI scoring using similarity-clustering and
conducted using a web-program displaying two images at a time. mPASI scoring of
single photos by the same or different dermatologist showed mPASI ratings of
50% to 80%, respectively. Repeated mPASI comparison using similarity clustering
showed consistent mPASI ratings &gt; 95%. Pearson correlation between absolute
scoring and pairwise scoring progression was 0.72.
</p>
<a href="http://arxiv.org/abs/2009.08997">arXiv:2009.08997</a> [<a href="http://arxiv.org/pdf/2009.08997">pdf</a>]

<h2>Deep Clustering and Representation Learning that Preserves Geometric Structures. (arXiv:2009.09590v2 [cs.LG] UPDATED)</h2>
<h3>Lirong Wu, Zicheng Liu, Zelin Zang, Jun Xia, Siyuan Li, Stan. Z Li</h3>
<p>In this paper, we propose a novel framework for Deep Clustering and
Representation Learning (DCRL) that preserves the geometric structure of data.
In the proposed DCRL framework, the clustering of data points from different
manifolds is done in the latent space guided by a clustering loss. To overcome
the problem that clustering-oriented losses may deteriorate the geometric
structure of embeddings in the latent space, two structure-oriented losses,
namely an isometric loss and a ranking loss, are proposed to preserve
intra-manifold structure locally and inter-manifold structure globally.
Experimental results on various datasets show that the DCRL framework leads to
comparable performances to current state-of-the-art deep clustering algorithms,
yet exhibits superior performance for downstream tasks. Our results also
demonstrate the importance and effectiveness of the proposed method in
preserving geometric structure in terms of visualization and performance
metrics.
</p>
<a href="http://arxiv.org/abs/2009.09590">arXiv:2009.09590</a> [<a href="http://arxiv.org/pdf/2009.09590">pdf</a>]

<h2>Discriminative Segmentation Tracking Using Dual Memory Banks. (arXiv:2009.09669v3 [cs.CV] UPDATED)</h2>
<h3>Fei Xie, Wankou Yang, Bo Liu, Kaihua Zhang, Wanli Xue, Wangmeng Zuo</h3>
<p>Existing template-based trackers usually localize the target in each frame
with bounding box, thereby being limited in learning pixel-wise representation
and handling complex and non-rigid transformation of the target. Further,
existing segmentation tracking methods are still insufficient in modeling and
exploiting dense correspondence of target pixels across frames. To overcome
these limitations, this work presents a novel discriminative segmentation
tracking architecture equipped with dual memory banks, i.e., appearance memory
bank and spatial memory bank. In particular, the appearance memory bank
utilizes spatial and temporal non-local similarity to propagate segmentation
mask to the current frame, and we further treat discriminative correlation
filter as spatial memory bank to store the mapping between feature map and
spatial map. Without bells and whistles, our simple-yet-effective tracking
architecture sets a new state-of-the-art on the VOT2016, VOT2018, VOT2019,
GOT-10K and TrackingNet benchmarks, especially achieving the EAO of 0.535 and
0.506 respectively on VOT2016 and VOT2018. Moreover, our approach outperforms
the leading segmentation tracker D3S on two video object segmentation
benchmarks DAVIS16 and DAVIS17. The source code will be released at
https://github.com/phiphiphi31/DMB.
</p>
<a href="http://arxiv.org/abs/2009.09669">arXiv:2009.09669</a> [<a href="http://arxiv.org/pdf/2009.09669">pdf</a>]

<h2>Modeling Text with Decision Forests using Categorical-Set Splits. (arXiv:2009.09991v2 [cs.LG] UPDATED)</h2>
<h3>Mathieu Guillame-Bert, Sebastian Bruch, Petr Mitrichev, Petr Mikheev, Jan Pfeifer</h3>
<p>Decision forest algorithms model data by learning a binary tree structure
recursively where every node splits the feature space into two regions, sending
examples into the left or right branches. This "decision" is the result of the
evaluation of a condition. For example, a node may split input data by applying
a threshold to a numerical feature value. Such decisions are learned using
(often greedy) algorithms that attempt to optimize a local loss function.
Crucially, whether an algorithm exists to find and evaluate splits for a
feature type (e.g., text) determines whether a decision forest algorithm can
model that feature type at all. In this work, we set out to devise such an
algorithm for textual features, thereby equipping decision forests with the
ability to directly model text without the need for feature transformation. Our
algorithm is efficient during training and the resulting splits are fast to
evaluate with our extension of the QuickScorer inference algorithm. Experiments
on benchmark text classification datasets demonstrate the utility and
effectiveness of our proposal.
</p>
<a href="http://arxiv.org/abs/2009.09991">arXiv:2009.09991</a> [<a href="http://arxiv.org/pdf/2009.09991">pdf</a>]

<h2>Learning a Contact-Adaptive Controller for Robust, Efficient Legged Locomotion. (arXiv:2009.10019v2 [cs.RO] UPDATED)</h2>
<h3>Xingye Da, Zhaoming Xie, David Hoeller, Byron Boots, Animashree Anandkumar, Yuke Zhu, Buck Babich, Animesh Garg</h3>
<p>We present a hierarchical framework that combines model-based control and
reinforcement learning (RL) to synthesize robust controllers for a quadruped
(the Unitree Laikago). The system consists of a high-level controller that
learns to choose from a set of primitives in response to changes in the
environment and a low-level controller that utilizes an established control
method to robustly execute the primitives. Our framework learns a controller
that can adapt to challenging environmental changes on the fly, including novel
scenarios not seen during training. The learned controller is up to 85~percent
more energy efficient and is more robust compared to baseline methods. We also
deploy the controller on a physical robot without any randomization or
adaptation scheme.
</p>
<a href="http://arxiv.org/abs/2009.10019">arXiv:2009.10019</a> [<a href="http://arxiv.org/pdf/2009.10019">pdf</a>]

<h2>Online Structural Change-point Detection of High-dimensional Streaming Data via Dynamic Sparse Subspace Learning. (arXiv:2009.11713v2 [stat.ML] UPDATED)</h2>
<h3>Ruiyu Xu, Jianguo Wu, Xiaowei Yue, Yongxiang Li</h3>
<p>High-dimensional streaming data are becoming increasingly ubiquitous in many
fields. They often lie in multiple low-dimensional subspaces, and the manifold
structures may change abruptly on the time scale due to pattern shift or
occurrence of anomalies. However, the problem of detecting the structural
changes in a real-time manner has not been well studied. To fill this gap, we
propose a dynamic sparse subspace learning (DSSL) approach for online
structural change-point detection of high-dimensional streaming data. A novel
multiple structural change-point model is proposed and it is shown to be
equivalent to maximizing a posterior under certain conditions. The asymptotic
properties of the estimators are investigated. The penalty coefficients in our
model can be selected by AMDL criterion based on some historical data. An
efficient Pruned Exact Linear Time (PELT) based method is proposed for online
optimization and change-point detection. The effectiveness of the proposed
method is demonstrated through a simulation study and a real case study using
gesture data for motion tracking.
</p>
<a href="http://arxiv.org/abs/2009.11713">arXiv:2009.11713</a> [<a href="http://arxiv.org/pdf/2009.11713">pdf</a>]

<h2>Revealing the Myth of Higher-Order Inference in Coreference Resolution. (arXiv:2009.12013v2 [cs.CL] UPDATED)</h2>
<h3>Liyan Xu, Jinho D. Choi</h3>
<p>This paper analyzes the impact of higher-order inference (HOI) on the task of
coreference resolution. HOI has been adapted by almost all recent coreference
resolution models without taking much investigation on its true effectiveness
over representation learning. To make a comprehensive analysis, we implement an
end-to-end coreference system as well as four HOI approaches, attended
antecedent, entity equalization, span clustering, and cluster merging, where
the latter two are our original methods. We find that given a high-performing
encoder such as SpanBERT, the impact of HOI is negative to marginal, providing
a new perspective of HOI to this task. Our best model using cluster merging
shows the Avg-F1 of 80.2 on the CoNLL 2012 shared task dataset in English.
</p>
<a href="http://arxiv.org/abs/2009.12013">arXiv:2009.12013</a> [<a href="http://arxiv.org/pdf/2009.12013">pdf</a>]

<h2>From Pixel to Patch: Synthesize Context-aware Features for Zero-shot Semantic Segmentation. (arXiv:2009.12232v2 [cs.CV] UPDATED)</h2>
<h3>Zhangxuan Gu, Siyuan Zhou, Li Niu, Zihan Zhao, Liqing Zhang</h3>
<p>Zero-shot learning has been actively studied for image classification task to
relieve the burden of annotating image labels. Interestingly, semantic
segmentation task requires more labor-intensive pixel-wise annotation, but
zero-shot semantic segmentation has only attracted limited research interest.
Thus, we focus on zero-shot semantic segmentation, which aims to segment unseen
objects with only category-level semantic representations provided for unseen
categories. In this paper, we propose a novel Context-aware feature Generation
Network (CaGNet), which can synthesize context-aware pixel-wise visual features
for unseen categories based on category-level semantic representations and
pixel-wise contextual information. The synthesized features are used to
finetune the classifier to enable segmenting unseen objects. Furthermore, we
extend pixel-wise feature generation and finetuning to patch-wise feature
generation and finetuning, which additionally considers inter-pixel
relationship. Experimental results on Pascal-VOC, Pascal-Context, and
COCO-stuff show that our method significantly outperforms the existing
zero-shot semantic segmentation methods. Code is available at
https://github.com/bcmi/CaGNetv2-Zero-Shot-Semantic-Segmentation.
</p>
<a href="http://arxiv.org/abs/2009.12232">arXiv:2009.12232</a> [<a href="http://arxiv.org/pdf/2009.12232">pdf</a>]

<h2>Learning to Improve Image Compression without Changing the Standard Decoder. (arXiv:2009.12927v2 [eess.IV] UPDATED)</h2>
<h3>Yannick Str&#xfc;mpler, Ren Yang, Radu Timofte</h3>
<p>In recent years we have witnessed an increasing interest in applying Deep
Neural Networks (DNNs) to improve the rate-distortion performance in image
compression. However, the existing approaches either train a post-processing
DNN on the decoder side, or propose learning for image compression in an
end-to-end manner. This way, the trained DNNs are required in the decoder,
leading to the incompatibility to the standard image decoders (e.g., JPEG) in
personal computers and mobiles. Therefore, we propose learning to improve the
encoding performance with the standard decoder. In this paper, We work on JPEG
as an example. Specifically, a frequency-domain pre-editing method is proposed
to optimize the distribution of DCT coefficients, aiming at facilitating the
JPEG compression. Moreover, we propose learning the JPEG quantization table
jointly with the pre-editing network. Most importantly, we do not modify the
JPEG decoder and therefore our approach is applicable when viewing images with
the widely used standard JPEG decoder. The experiments validate that our
approach successfully improves the rate-distortion performance of JPEG in terms
of various quality metrics, such as PSNR, MS-SSIM and LPIPS. Visually, this
translates to better overall color retention especially when strong compression
is applied. The codes are available at
https://github.com/YannickStruempler/LearnedJPEG.
</p>
<a href="http://arxiv.org/abs/2009.12927">arXiv:2009.12927</a> [<a href="http://arxiv.org/pdf/2009.12927">pdf</a>]

<h2>Parametric UMAP: learning embeddings with deep neural networks for representation and semi-supervised learning. (arXiv:2009.12981v2 [cs.LG] UPDATED)</h2>
<h3>Tim Sainburg, Leland McInnes, Timothy Q Gentner</h3>
<p>We propose Parametric UMAP, a parametric variation of the UMAP (Uniform
Manifold Approximation and Projection) algorithm. UMAP is a non-parametric
graph-based dimensionality reduction algorithm using applied Riemannian
geometry and algebraic topology to find low-dimensional embeddings of
structured data. The UMAP algorithm consists of two steps: (1) Compute a
graphical representation of a dataset (fuzzy simplicial complex), and (2)
Through stochastic gradient descent, optimize a low-dimensional embedding of
the graph. Here, we replace the second step of UMAP with a deep neural network
that learns a parametric relationship between data and embedding. We
demonstrate that our method performs similarly to its non-parametric
counterpart while conferring the benefit of a learned parametric mapping (e.g.
fast online embeddings for new data). We then show that UMAP loss can be
extended to arbitrary deep learning applications, for example constraining the
latent distribution of autoencoders, and improving classifier accuracy for
semi-supervised learning by capturing structure in unlabeled data. Our code is
available at https://github.com/timsainb/ParametricUMAP_paper.
</p>
<a href="http://arxiv.org/abs/2009.12981">arXiv:2009.12981</a> [<a href="http://arxiv.org/pdf/2009.12981">pdf</a>]

<h2>Long-Tailed Classification by Keeping the Good and Removing the Bad Momentum Causal Effect. (arXiv:2009.12991v2 [cs.CV] UPDATED)</h2>
<h3>Kaihua Tang, Jianqiang Huang, Hanwang Zhang</h3>
<p>As the class size grows, maintaining a balanced dataset across many classes
is challenging because the data are long-tailed in nature; it is even
impossible when the sample-of-interest co-exists with each other in one
collectable unit, e.g., multiple visual instances in one image. Therefore,
long-tailed classification is the key to deep learning at scale. However,
existing methods are mainly based on re-weighting/re-sampling heuristics that
lack a fundamental theory. In this paper, we establish a causal inference
framework, which not only unravels the whys of previous methods, but also
derives a new principled solution. Specifically, our theory shows that the SGD
momentum is essentially a confounder in long-tailed classification. On one
hand, it has a harmful causal effect that misleads the tail prediction biased
towards the head. On the other hand, its induced mediation also benefits the
representation learning and head prediction. Our framework elegantly
disentangles the paradoxical effects of the momentum, by pursuing the direct
causal effect caused by an input sample. In particular, we use causal
intervention in training, and counterfactual reasoning in inference, to remove
the "bad" while keep the "good". We achieve new state-of-the-arts on three
long-tailed visual recognition benchmarks: Long-tailed CIFAR-10/-100,
ImageNet-LT for image classification and LVIS for instance segmentation.
</p>
<a href="http://arxiv.org/abs/2009.12991">arXiv:2009.12991</a> [<a href="http://arxiv.org/pdf/2009.12991">pdf</a>]

<h2>Kernel Based Progressive Distillation for Adder Neural Networks. (arXiv:2009.13044v2 [cs.CV] UPDATED)</h2>
<h3>Yixing Xu, Chang Xu, Xinghao Chen, Wei Zhang, Chunjing Xu, Yunhe Wang</h3>
<p>Adder Neural Networks (ANNs) which only contain additions bring us a new way
of developing deep neural networks with low energy consumption. Unfortunately,
there is an accuracy drop when replacing all convolution filters by adder
filters. The main reason here is the optimization difficulty of ANNs using
$\ell_1$-norm, in which the estimation of gradient in back propagation is
inaccurate. In this paper, we present a novel method for further improving the
performance of ANNs without increasing the trainable parameters via a
progressive kernel based knowledge distillation (PKKD) method. A convolutional
neural network (CNN) with the same architecture is simultaneously initialized
and trained as a teacher network, features and weights of ANN and CNN will be
transformed to a new space to eliminate the accuracy drop. The similarity is
conducted in a higher-dimensional space to disentangle the difference of their
distributions using a kernel based method. Finally, the desired ANN is learned
based on the information from both the ground-truth and teacher, progressively.
The effectiveness of the proposed method for learning ANN with higher
performance is then well-verified on several benchmarks. For instance, the
ANN-50 trained using the proposed PKKD method obtains a 76.8\% top-1 accuracy
on ImageNet dataset, which is 0.6\% higher than that of the ResNet-50.
</p>
<a href="http://arxiv.org/abs/2009.13044">arXiv:2009.13044</a> [<a href="http://arxiv.org/pdf/2009.13044">pdf</a>]

<h2>Rotated Binary Neural Network. (arXiv:2009.13055v2 [cs.CV] UPDATED)</h2>
<h3>Mingbao Lin, Rongrong Ji, Zihan Xu, Baochang Zhang, Yan Wang, Yongjian Wu, Feiyue Huang, Chia-Wen Lin</h3>
<p>Binary Neural Network (BNN) shows its predominance in reducing the complexity
of deep neural networks. However, it suffers severe performance degradation.
One of the major impediments is the large quantization error between the
full-precision weight vector and its binary vector. Previous works focus on
compensating for the norm gap while leaving the angular bias hardly touched. In
this paper, for the first time, we explore the influence of angular bias on the
quantization error and then introduce a Rotated Binary Neural Network (RBNN),
which considers the angle alignment between the full-precision weight vector
and its binarized version. At the beginning of each training epoch, we propose
to rotate the full-precision weight vector to its binary vector to reduce the
angular bias. To avoid the high complexity of learning a large rotation matrix,
we further introduce a bi-rotation formulation that learns two smaller rotation
matrices. In the training stage, we devise an adjustable rotated weight vector
for binarization to escape the potential local optimum. Our rotation leads to
around 50% weight flips which maximize the information gain. Finally, we
propose a training-aware approximation of the sign function for the gradient
backward. Experiments on CIFAR-10 and ImageNet demonstrate the superiorities of
RBNN over many state-of-the-arts. Our source code, experimental settings,
training logs and binary models are available at
https://github.com/lmbxmu/RBNN.
</p>
<a href="http://arxiv.org/abs/2009.13055">arXiv:2009.13055</a> [<a href="http://arxiv.org/pdf/2009.13055">pdf</a>]

<h2>A Simple and Efficient Ensemble Classifier Combining Multiple Neural Network Models on Social Media Datasets in Vietnamese. (arXiv:2009.13060v2 [cs.CL] UPDATED)</h2>
<h3>Huy Duc Huynh, Hang Thi-Thuy Do, Kiet Van Nguyen, Ngan Luu-Thuy Nguyen</h3>
<p>Text classification is a popular topic of natural language processing, which
has currently attracted numerous research efforts worldwide. The significant
increase of data in social media requires the vast attention of researchers to
analyze such data. There are various studies in this field in many languages
but limited to the Vietnamese language. Therefore, this study aims to classify
Vietnamese texts on social media from three different Vietnamese benchmark
datasets. Advanced deep learning models are used and optimized in this study,
including CNN, LSTM, and their variants. We also implement the BERT, which has
never been applied to the datasets. Our experiments find a suitable model for
classification tasks on each specific dataset. To take advantage of single
models, we propose an ensemble model, combining the highest-performance models.
Our single models reach positive results on each dataset. Moreover, our
ensemble model achieves the best performance on all three datasets. We reach
86.96% of F1- score for the HSD-VLSP dataset, 65.79% of F1-score for the
UIT-VSMEC dataset, 92.79% and 89.70% for sentiments and topics on the UIT-VSFC
dataset, respectively. Therefore, our models achieve better performances as
compared to previous studies on these datasets.
</p>
<a href="http://arxiv.org/abs/2009.13060">arXiv:2009.13060</a> [<a href="http://arxiv.org/pdf/2009.13060">pdf</a>]

<h2>Learning to Stop: A Simple yet Effective Approach to Urban Vision-Language Navigation. (arXiv:2009.13112v2 [cs.CV] UPDATED)</h2>
<h3>Jiannan Xiang, Xin Eric Wang, William Yang Wang</h3>
<p>Vision-and-Language Navigation (VLN) is a natural language grounding task
where an agent learns to follow language instructions and navigate to specified
destinations in real-world environments. A key challenge is to recognize and
stop at the correct location, especially for complicated outdoor environments.
Existing methods treat the STOP action equally as other actions, which results
in undesirable behaviors that the agent often fails to stop at the destination
even though it might be on the right path. Therefore, we propose Learning to
Stop (L2Stop), a simple yet effective policy module that differentiates STOP
and other actions. Our approach achieves the new state of the art on a
challenging urban VLN dataset Touchdown, outperforming the baseline by 6.89%
(absolute improvement) on Success weighted by Edit Distance (SED).
</p>
<a href="http://arxiv.org/abs/2009.13112">arXiv:2009.13112</a> [<a href="http://arxiv.org/pdf/2009.13112">pdf</a>]

<h2>Transformers Are Better Than Humans at Identifying Generated Text. (arXiv:2009.13375v2 [cs.CL] UPDATED)</h2>
<h3>Antonis Maronikolakis, Mark Stevenson, Hinrich Schutze</h3>
<p>Fake information spread via the internet and social media influences public
opinion and user activity. Generative models enable fake content to be
generated faster and more cheaply than had previously been possible. This paper
examines the problem of identifying fake content generated by lightweight deep
learning models. A dataset containing human and machine-generated headlines was
created and a user study indicated that humans were only able to identify the
fake headlines in 45.3% of the cases. However, the most accurate automatic
approach, transformers, achieved an accuracy of 94%, indicating that content
generated from language models can be filtered out accurately.
</p>
<a href="http://arxiv.org/abs/2009.13375">arXiv:2009.13375</a> [<a href="http://arxiv.org/pdf/2009.13375">pdf</a>]

<h2>Parameterized Synthesis with Safety Properties. (arXiv:2009.13459v1 [cs.LO] CROSS LISTED)</h2>
<h3>Oliver Markgraf, Chih-Duo Hong, Anthony W. Lin, Muhammad Najib, Daniel Neider</h3>
<p>Parameterized synthesis offers a solution to the problem of constructing
correct and verified controllers for parameterized systems. Such systems occur
naturally in practice (e.g., in the form of distributed protocols where the
amount of processes is often unknown at design time and the protocol must work
regardless of the number of processes). In this paper, we present a novel
learning based approach to the synthesis of reactive controllers for
parameterized systems from safety specifications. We use the framework of
regular model checking to model the synthesis problem as an infinite-duration
two-player game and show how one can utilize Angluin's well-known L* algorithm
to learn correct-by-design controllers. This approach results in a synthesis
procedure that is conceptually simpler than existing synthesis methods with a
completeness guarantee, whenever a winning strategy can be expressed by a
regular set. We have implemented our algorithm in a tool called L*-PSynth and
have demonstrated its performance on a range of benchmarks, including robotic
motion planning and distributed protocols. Despite the simplicity of L*-PSynth
it competes well against (and in many cases even outperforms) the
state-of-the-art tools for synthesizing parameterized systems.
</p>
<a href="http://arxiv.org/abs/2009.13459">arXiv:2009.13459</a> [<a href="http://arxiv.org/pdf/2009.13459">pdf</a>]

<h2>A General Bayesian Model for Heteroskedastic Data with Fully Conjugate Full-Conditional Distributions. (arXiv:2009.13636v1 [stat.ME])</h2>
<h3>Paul A. Parker, Scott H. Holan, Skye A. Wills</h3>
<p>Models for heteroskedastic data are relevant in a wide variety of
applications ranging from financial time series to environmental statistics.
However, the topic of modeling the variance function conditionally has not seen
near as much attention as modeling the mean. Volatility models have been used
in specific applications, but these models can be difficult to fit in a
Bayesian setting due to posterior distributions that are challenging to sample
from efficiently. In this work, we introduce a general model for
heteroskedastic data. This approach models the conditional variance in a mixed
model approach as a function of any desired covariates or random effects. We
rely on new distribution theory in order to construct priors that yield fully
conjugate full conditional distributions. Thus, our approach can easily be fit
via Gibbs sampling. Furthermore, we extend the model to a deep learning
approach that can provide highly accurate estimates for time dependent data. We
also provide an extension for heavy-tailed data. We illustrate our methodology
via three applications. The first application utilizes a high dimensional soil
dataset with inherent spatial dependence. The second application involves
modeling of asset volatility. The third application focuses on clinical trial
data for creatinine.
</p>
<a href="http://arxiv.org/abs/2009.13636">arXiv:2009.13636</a> [<a href="http://arxiv.org/pdf/2009.13636">pdf</a>]

<h2>Physics-Constrained Predictive Molecular Latent Space Discovery with Graph Scattering Variational Autoencoder. (arXiv:2009.13878v1 [physics.chem-ph])</h2>
<h3>Navid Shervani-Tabar, Nicholas Zabaras</h3>
<p>Recent advances in artificial intelligence have propelled the development of
innovative computational materials modeling and design techniques. In
particular, generative deep learning models have been used for molecular
representation, discovery and design with applications ranging from drug
discovery to solar cell development. In this work, we assess the predictive
capabilities of a molecular generative model developed based on variational
inference and graph theory. The encoder network is based on the scattering
transform, which allows for a better generalization of the model in the
presence of limited training data. The scattering layers incorporate adaptive
spectral filters which are tailored to the training dataset based on the
molecular graphs' spectra. The decoding network is a one-shot graph generative
model that conditions atom types on molecular topology. We present a
quantitative assessment of the latent space in terms of its predictive ability
for organic molecules in the QM9 dataset. To account for the limited size
training data set, a Bayesian formalism is considered that allows us capturing
the uncertainties in the predicted properties.
</p>
<a href="http://arxiv.org/abs/2009.13878">arXiv:2009.13878</a> [<a href="http://arxiv.org/pdf/2009.13878">pdf</a>]

<h2>Variational Autoencoders for Anomalous Jet Tagging. (arXiv:2007.01850v2 [hep-ph] UPDATED)</h2>
<h3>Taoli Cheng, Jean-Fran&#xe7;ois Arguin, Julien Leissner-Martin, Jacinthe Pilette, Tobias Golling</h3>
<p>We present a detailed study on Variational Autoencoders (VAEs) for anomalous
jet tagging at the Large Hadron Collider. By taking in low-level jet
constituents' information, and only training with background QCD jets in an
unsupervised manner, the VAE is able to encode important information for
reconstructing jets, while learning an expressive posterior distribution in the
latent space. When using the VAE as an anomaly detector, we present different
approaches to detect anomalies: directly comparing in the input space or,
instead, working in the latent space. Different anomaly metrics are examined. A
comprehensive series of test sets are generated to fully examine the anomalous
tagging performance in different jet types. In order to facilitate general
search approaches such as bump-hunt, mass-decorrelated VAEs based on distance
correlation regularization are also studied. Confronted with the problem of
mis-assigning higher probability to out-of-distribution samples, we explore one
potential solution -- Outlier Exposure (OE), in which outlier samples are
utilized to guide the learning heuristics. OE, in the context of jet tagging,
is employed to achieve two goals at the same time: increasing sensitivity of
outlier detection and decorrelating jet mass from the anomaly score. We observe
excellent results from both aspects. Code implementation can be found at
https://github.com/taolicheng/VAE-Jet
</p>
<a href="http://arxiv.org/abs/2007.01850">arXiv:2007.01850</a> [<a href="http://arxiv.org/pdf/2007.01850">pdf</a>]

<h2>Doubly Robust Nonparametric Instrumental Variable Estimators for Survival Outcomes. (arXiv:2007.12973v2 [stat.ME] UPDATED)</h2>
<h3>Youjin Lee, Edward H. Kennedy, Nandita Mitra</h3>
<p>Instrumental variable (IV) methods allow us the opportunity to address
unmeasured confounding in causal inference. However, most IV methods are only
applicable to discrete or continuous outcomes with very few IV methods for
censored survival outcomes. In this work we propose nonparametric estimators
for the local average treatment effect on survival probabilities under both
nonignorable and ignorable censoring. We provide an efficient influence
function-based estimator and a simple estimation procedure when the IV is
either binary or continuous. The proposed estimators possess double-robustness
properties and can easily incorporate nonparametric estimation using machine
learning tools. In simulation studies, we demonstrate the flexibility and
efficiency of our proposed estimators under various plausible scenarios. We
apply our method to the Prostate, Lung, Colorectal, and Ovarian Cancer
Screening Trial for estimating the causal effect of screening on survival
probabilities and investigate the causal contrasts between the two
interventions under different censoring assumptions.
</p>
<a href="http://arxiv.org/abs/2007.12973">arXiv:2007.12973</a> [<a href="http://arxiv.org/pdf/2007.12973">pdf</a>]

