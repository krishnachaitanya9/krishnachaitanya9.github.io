---
title: Latest Deep Learning Papers
date: 2021-03-01 21:00:01 +0800
featured-img: DL-logo
categories: [Deep_Learning]
tags: [Deep_Learning]
mathjax: Yes
---

<h1>Your arXiv Feed (95 Articles)</h1>
<h2>Learning Transferable Visual Models From Natural Language Supervision. (arXiv:2103.00020v1 [cs.CV])</h2>
<h3>Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever</h3>
<p>State-of-the-art computer vision systems are trained to predict a fixed set
of predetermined object categories. This restricted form of supervision limits
their generality and usability since additional labeled data is needed to
specify any other visual concept. Learning directly from raw text about images
is a promising alternative which leverages a much broader source of
supervision. We demonstrate that the simple pre-training task of predicting
which caption goes with which image is an efficient and scalable way to learn
SOTA image representations from scratch on a dataset of 400 million (image,
text) pairs collected from the internet. After pre-training, natural language
is used to reference learned visual concepts (or describe new ones) enabling
zero-shot transfer of the model to downstream tasks. We study the performance
of this approach by benchmarking on over 30 different existing computer vision
datasets, spanning tasks such as OCR, action recognition in videos,
geo-localization, and many types of fine-grained object classification. The
model transfers non-trivially to most tasks and is often competitive with a
fully supervised baseline without the need for any dataset specific training.
For instance, we match the accuracy of the original ResNet-50 on ImageNet
zero-shot without needing to use any of the 1.28 million training examples it
was trained on. We release our code and pre-trained model weights at
https://github.com/OpenAI/CLIP.
</p>
<a href="http://arxiv.org/abs/2103.00020" target="_blank">arXiv:2103.00020</a> [<a href="http://arxiv.org/pdf/2103.00020" target="_blank">pdf</a>]

<h2>SocNavBench: A Grounded Simulation Testing Framework for Evaluating Social Navigation. (arXiv:2103.00047v1 [cs.RO])</h2>
<h3>Abhijat Biswas, Allan Wang, Gustavo Silvera, Aaron Steinfeld, Henny Admoni</h3>
<p>The human-robot interaction (HRI) community has developed many methods for
robots to navigate safely and socially alongside humans. However, experimental
procedures to evaluate these works are usually constructed on a per-method
basis. Such disparate evaluations make it difficult to compare the performance
of such methods across the literature. To bridge this gap, we introduce
SocNavBench, a simulation framework for evaluating social navigation
algorithms. SocNavBench comprises a simulator with photo-realistic capabilities
and curated social navigation scenarios grounded in real-world pedestrian data.
We also provide an implementation of a suite of metrics to quantify the
performance of navigation algorithms on these scenarios. Altogether,
SocNavBench provides a test framework for evaluating disparate social
navigation methods in a consistent and interpretable manner. To illustrate its
use, we demonstrate testing three existing social navigation methods and a
baseline method on SocNavBench, showing how the suite of metrics helps infer
their performance trade-offs. Our code is open-source, allowing the addition of
new scenarios and metrics by the community to help evolve SocNavBench to
reflect advancements in our understanding of social navigation.
</p>
<a href="http://arxiv.org/abs/2103.00047" target="_blank">arXiv:2103.00047</a> [<a href="http://arxiv.org/pdf/2103.00047" target="_blank">pdf</a>]

<h2>Trajectory Servoing: Image-Based Trajectory Tracking Using SLAM. (arXiv:2103.00055v1 [cs.RO])</h2>
<h3>Shiyu Feng, Zixuan Wu, Patricio A. Vela</h3>
<p>This paper describes an image based visual servoing (IBVS) system for a
nonholonomic robot to achieve good trajectory following without real-time robot
pose information and without a known visual map of the environment. We call it
trajectory servoing. The critical component is a feature-based, indirect SLAM
method to provide a pool of available features with estimated depth, so that
they may be propagated forward in time to generate image feature trajectories
for visual servoing. Short and long distance experiments show the benefits of
trajectory servoing for navigating unknown areas without absolute positioning.
Trajectory servoing is shown to be more accurate than pose-based feedback when
both rely on the same underlying SLAM system.
</p>
<a href="http://arxiv.org/abs/2103.00055" target="_blank">arXiv:2103.00055</a> [<a href="http://arxiv.org/pdf/2103.00055" target="_blank">pdf</a>]

<h2>Recursive Training for Zero-Shot Semantic Segmentation. (arXiv:2103.00086v1 [cs.CV])</h2>
<h3>Ce Wang, Moshiur Farazi, Nick Barnes</h3>
<p>General purpose semantic segmentation relies on a backbone CNN network to
extract discriminative features that help classify each image pixel into a
'seen' object class (ie., the object classes available during training) or a
background class. Zero-shot semantic segmentation is a challenging task that
requires a computer vision model to identify image pixels belonging to an
object class which it has never seen before. Equipping a general purpose
semantic segmentation model to separate image pixels of 'unseen' classes from
the background remains an open challenge. Some recent models have approached
this problem by fine-tuning the final pixel classification layer of a semantic
segmentation model for a Zero-Shot setting, but struggle to learn
discriminative features due to the lack of supervision. We propose a recursive
training scheme to supervise the retraining of a semantic segmentation model
for a zero-shot setting using a pseudo-feature representation. To this end, we
propose a Zero-Shot Maximum Mean Discrepancy (ZS-MMD) loss that weighs high
confidence outputs of the pixel classification layer as a pseudo-feature
representation, and feeds it back to the generator. By closing-the-loop on the
generator end, we provide supervision during retraining that in turn helps the
model learn a more discriminative feature representation for 'unseen' classes.
We show that using our recursive training and ZS-MMD loss, our proposed model
achieves state-of-the-art performance on the Pascal-VOC 2012 dataset and
Pascal-Context dataset.
</p>
<a href="http://arxiv.org/abs/2103.00086" target="_blank">arXiv:2103.00086</a> [<a href="http://arxiv.org/pdf/2103.00086" target="_blank">pdf</a>]

<h2>Transformer in Transformer. (arXiv:2103.00112v1 [cs.CV])</h2>
<h3>Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, Yunhe Wang</h3>
<p>Transformer is a type of self-attention-based neural networks originally
applied for NLP tasks. Recently, pure transformer-based models are proposed to
solve computer vision problems. These visual transformers usually view an image
as a sequence of patches while they ignore the intrinsic structure information
inside each patch. In this paper, we propose a novel Transformer-iN-Transformer
(TNT) model for modeling both patch-level and pixel-level representation. In
each TNT block, an outer transformer block is utilized to process patch
embeddings, and an inner transformer block extracts local features from pixel
embeddings. The pixel-level feature is projected to the space of patch
embedding by a linear transformation layer and then added into the patch. By
stacking the TNT blocks, we build the TNT model for image recognition.
Experiments on ImageNet benchmark and downstream tasks demonstrate the
superiority and efficiency of the proposed TNT architecture. For example, our
TNT achieves $81.3\%$ top-1 accuracy on ImageNet which is $1.5\%$ higher than
that of DeiT with similar computational cost. The code will be available at
https://github.com/huawei-noah/noah-research/tree/master/TNT.
</p>
<a href="http://arxiv.org/abs/2103.00112" target="_blank">arXiv:2103.00112</a> [<a href="http://arxiv.org/pdf/2103.00112" target="_blank">pdf</a>]

<h2>Deep Active Shape Model for Face Alignment and Pose Estimation. (arXiv:2103.00119v1 [cs.CV])</h2>
<h3>Ali Pourramezan Fard, Hojjat Abdollahi, Mohammad Mahoor</h3>
<p>Active Shape Model (ASM) is a statistical model of object shapes that
represents a target structure. ASM can guide machine learning algorithms to fit
a set of points representing an object (e.g., face) onto an image. This paper
presents a lightweight Convolutional Neural Network (CNN) architecture with a
loss function regularized by ASM for face alignment and estimating head pose in
the wild. The ASM-based regularization term in the loss function would guide
the network to learn faster, generalize better, and hence handle challenging
examples even with light-weight network architecture. We define multi-tasks in
our loss function that are responsible for detecting facial landmark points, as
well as estimating face pose. Learning multiple correlated tasks simultaneously
builds synergy and improves the performance of individual tasks. Experimental
results on challenging datasets show that our proposed ASM regularized loss
function achieves competitive performance for facial landmark points detection
and pose estimation using a very light-weight CNN architecture.
</p>
<a href="http://arxiv.org/abs/2103.00119" target="_blank">arXiv:2103.00119</a> [<a href="http://arxiv.org/pdf/2103.00119" target="_blank">pdf</a>]

<h2>Successive Subspace Learning: An Overview. (arXiv:2103.00121v1 [cs.CV])</h2>
<h3>Mozhdeh Rouhsedaghat, Masoud Monajatipoor, Zohreh Azizi, C.-C. Jay Kuo</h3>
<p>Successive Subspace Learning (SSL) offers a light-weight unsupervised feature
learning method based on inherent statistical properties of data units (e.g.
image pixels and points in point cloud sets). It has shown promising results,
especially on small datasets. In this paper, we intuitively explain this
method, provide an overview of its development, and point out some open
questions and challenges for future research.
</p>
<a href="http://arxiv.org/abs/2103.00121" target="_blank">arXiv:2103.00121</a> [<a href="http://arxiv.org/pdf/2103.00121" target="_blank">pdf</a>]

<h2>PRISM: A Unified Framework of Parameterized Submodular Information Measures for Targeted Data Subset Selection and Summarization. (arXiv:2103.00128v1 [cs.CV])</h2>
<h3>Vishal Kaushal, Suraj Kothawade, Ganesh Ramakrishnan, Jeff Bilmes, Rishabh Iyer</h3>
<p>With increasing data, techniques for finding smaller, yet effective subsets
with specific characteristics become important. Motivated by this, we present
PRISM, a rich class of Parameterized Submodular Information Measures, that can
be used in applications where such targeted subsets are desired. We demonstrate
the utility of PRISM in two such applications. First, we apply PRISM to improve
a supervised model's performance at a given additional labeling cost by
targeted subset selection (PRISM-TSS) where a subset of unlabeled points
matching a target set are added to the training set. We show that PRISM-TSS
generalizes and is connected to several existing approaches to targeted data
subset selection. Second, we apply PRISM to a more nuanced targeted
summarization (PRISM-TSUM) where data (e.g., image collections, text or videos)
is summarized for quicker human consumption with additional user intent.
PRISM-TSUM handles multiple flavors of targeted summarization such as
query-focused, topic-irrelevant, privacy-preserving and update summarization in
a unified way. We show that PRISM-TSUM also generalizes and unifies several
existing past work on targeted summarization. Through extensive experiments on
image classification and image-collection summarization we empirically verify
the superiority of PRISM-TSS and PRISM-TSUM over the state-of-the-art.
</p>
<a href="http://arxiv.org/abs/2103.00128" target="_blank">arXiv:2103.00128</a> [<a href="http://arxiv.org/pdf/2103.00128" target="_blank">pdf</a>]

<h2>Open-set Intersection Intention Prediction for Autonomous Driving. (arXiv:2103.00140v1 [cs.RO])</h2>
<h3>Fei Li, Xiangxu Li, Shiwei Fan, Hongbo Zhang, Jun Luo</h3>
<p>Intention prediction is a crucial task for Autonomous Driving (AD). Due to
the variety of size and layout of intersections, it is challenging to predict
intention of human driver at different intersections, especially unseen and
irregular intersections. In this paper, we formulate the prediction of
intention at intersections as an open-set prediction problem that requires
context specific matching of the target vehicle state and the diverse
intersection configurations that are in principle unbounded. We capture
map-centric features that correspond to intersection structures under a
spatial-temporal graph representation, and use two MAAMs (mutually auxiliary
attention module) that cover respectively lane-level and exitlevel intentions
to predict a target that best matches intersection elements in map-centric
feature space. Under our model, attention scores estimate the probability
distribution of the openset intentions that are contextually defined by the
structure of the current intersection. The proposed model is trained and
evaluated on simulated dataset. Furthermore, the model, trained on simulated
dataset and without any fine tuning, is directly validated on in-house
real-world dataset collected at 98 realworld intersections and exhibits
satisfactory performance,demonstrating the practical viability of our approach.
</p>
<a href="http://arxiv.org/abs/2103.00140" target="_blank">arXiv:2103.00140</a> [<a href="http://arxiv.org/pdf/2103.00140" target="_blank">pdf</a>]

<h2>Disentangling Geometric Deformation Spaces in Generative Latent Shape Models. (arXiv:2103.00142v1 [cs.CV])</h2>
<h3>Tristan Aumentado-Armstrong, Stavros Tsogkas, Sven Dickinson, Allan Jepson</h3>
<p>A complete representation of 3D objects requires characterizing the space of
deformations in an interpretable manner, from articulations of a single
instance to changes in shape across categories. In this work, we improve on a
prior generative model of geometric disentanglement for 3D shapes, wherein the
space of object geometry is factorized into rigid orientation, non-rigid pose,
and intrinsic shape. The resulting model can be trained from raw 3D shapes,
without correspondences, labels, or even rigid alignment, using a combination
of classical spectral geometry and probabilistic disentanglement of a
structured latent representation space. Our improvements include more
sophisticated handling of rotational invariance and the use of a diffeomorphic
flow network to bridge latent and spectral space. The geometric structuring of
the latent space imparts an interpretable characterization of the deformation
space of an object. Furthermore, it enables tasks like pose transfer and
pose-aware retrieval without requiring supervision. We evaluate our model on
its generative modelling, representation learning, and disentanglement
performance, showing improved rotation invariance and intrinsic-extrinsic
factorization quality over the prior model.
</p>
<a href="http://arxiv.org/abs/2103.00142" target="_blank">arXiv:2103.00142</a> [<a href="http://arxiv.org/pdf/2103.00142" target="_blank">pdf</a>]

<h2>Pedestrian Motion State Estimation From 2D Pose. (arXiv:2103.00145v1 [cs.CV])</h2>
<h3>Fei Li, Shiwei Fan, Pengzhen Chen, Xiangxu Li</h3>
<p>Traffic violation and the flexible and changeable nature of pedestrians make
it more difficult to predict pedestrian behavior or intention, which might be a
potential safety hazard on the road. Pedestrian motion state (such as walking
and standing) directly affects or reflects its intention. In combination with
pedestrian motion state and other influencing factors, pedestrian intention can
be predicted to avoid unnecessary accidents. In this paper, pedestrian is
treated as non-rigid object, which can be represented by a set of
two-dimensional key points, and the movement of key point relative to the torso
is introduced as micro motion. Static and dynamic micro motion features, such
as position, angle and distance, and their differential calculations in time
domain, are used to describe its motion pattern. Gated recurrent neural network
based seq2seq model is used to learn the dependence of motion state transition
on previous information, finally the pedestrian motion state is estimated via a
softmax classifier. The proposed method only needs the previous hidden state of
GRU and current feature to evaluate the probability of current motion state,
and it is computation efficient to deploy on vehicles. This paper verifies the
proposed algorithm on the JAAD public dataset, and the accuracy is improved by
11.6% compared with the existing method.
</p>
<a href="http://arxiv.org/abs/2103.00145" target="_blank">arXiv:2103.00145</a> [<a href="http://arxiv.org/pdf/2103.00145" target="_blank">pdf</a>]

<h2>Geometrically Constrained Trajectory Optimization for Multicopters. (arXiv:2103.00190v1 [cs.RO])</h2>
<h3>Zhepei Wang, Xin Zhou, Chao Xu, Fei Gao</h3>
<p>We present an optimization-based framework for multicopter trajectory
planning subject to geometrical spatial constraints and user-defined dynamic
constraints. The basis of the framework is a novel trajectory representation
built upon our novel optimality conditions for unconstrained control effort
minimization. We design linear-complexity operations on this representation to
conduct spatial-temporal deformation under various planning requirements.
Smooth maps are utilized to exactly eliminate geometrical constraints in a
lightweight fashion. A wide range of state-input constraints are supported by
the decoupling of dense constraint evaluation from sparse parameterization, and
backward differentiation of flatness map. As a result, the proposed framework
transforms a generally constrained multicopter planning problem into an
unconstrained optimization that can be solved reliably and efficiently. Our
framework bridges the gaps among solution quality, planning frequency and
constraint fidelity for a multicopter with limited resources and maneuvering
capability. Its generality and robustness are both demonstrated by applications
and experiments for different tasks. Extensive simulations and benchmarks are
also conducted to show its capability of generating high-quality solutions
while retaining the computation speed against other specialized methods by
orders of magnitudes. Details and source code of our framework will be freely
available at: this http URL
</p>
<a href="http://arxiv.org/abs/2103.00190" target="_blank">arXiv:2103.00190</a> [<a href="http://arxiv.org/pdf/2103.00190" target="_blank">pdf</a>]

<h2>FisheyeSuperPoint: Keypoint Detection and Description Network for Fisheye Images. (arXiv:2103.00191v1 [cs.CV])</h2>
<h3>Anna Konrad, Ciar&#xe1;n Eising, Ganesh Sistu, John McDonald, Rudi Villing, Senthil Yogamani</h3>
<p>Keypoint detection and description is a commonly used building block in
computer vision systems particularly for robotics and autonomous driving.
Recently CNN based approaches have surpassed classical methods in a number of
perception tasks. However, the majority of techniques to date have focused on
standard cameras with little consideration given to fisheye cameras which are
commonly used in autonomous driving. In this paper, we propose a novel training
and evaluation pipeline for fisheye images. We make use of SuperPoint as our
baseline which is a self-supervised keypoint detector and descriptor that has
achieved state-of-the-art results on homography estimation. We introduce a
fisheye adaptation pipeline to enable training on undistorted fisheye images.
We evaluate the performance on the HPatches benchmark, and, by introducing a
fisheye based evaluation methods for detection repeatability and descriptor
matching correctness on the Oxford RobotCar datasets.
</p>
<a href="http://arxiv.org/abs/2103.00191" target="_blank">arXiv:2103.00191</a> [<a href="http://arxiv.org/pdf/2103.00191" target="_blank">pdf</a>]

<h2>Efficient Transformer based Method for Remote Sensing Image Change Detection. (arXiv:2103.00208v1 [cs.CV])</h2>
<h3>Hao Chen, Zipeng Qi, Zhenwei Shi</h3>
<p>Modern change detection (CD) has achieved remarkable success by the powerful
discriminative ability of deep convolutions. However, high-resolution remote
sensing CD remains challenging due to the complexity of objects in the scene.
The objects with the same semantic concept show distinct spectral behaviors at
different times and different spatial locations. Modeling interactions between
global semantic concepts is critical for change recognition. Most recent change
detection pipelines using pure convolutions are still struggling to relate
long-range concepts in space-time. Non-local self-attention approaches show
promising performance via modeling dense relations among pixels, yet are
computationally inefficient. In this paper, we propose a bitemporal image
transformer (BiT) to efficiently and effectively model contexts within the
spatial-temporal domain. Our intuition is that the high-level concepts of the
change of interest can be represented by a few visual words, i.e., semantic
tokens. To achieve this, we express the bitemporal image into a few tokens, and
use a transformer encoder to model contexts in the compact token-based
space-time. The learned context-rich tokens are then feedback to the
pixel-space for refining the original features via a transformer decoder. We
incorporate BiT in a deep feature differencing-based CD framework. Extensive
experiments on three public CD datasets demonstrate the effectiveness and
efficiency of the proposed method. Notably, our BiT-based model significantly
outperforms the purely convolutional baseline using only 3 times lower
computational costs and model parameters. Based on a naive backbone (ResNet18)
without sophisticated structures (e.g., FPN, UNet), our model surpasses several
state-of-the-art CD methods, including better than two recent attention-based
methods in terms of efficiency and accuracy. Our code will be made public.
</p>
<a href="http://arxiv.org/abs/2103.00208" target="_blank">arXiv:2103.00208</a> [<a href="http://arxiv.org/pdf/2103.00208" target="_blank">pdf</a>]

<h2>Countering Malicious DeepFakes: Survey, Battleground, and Horizon. (arXiv:2103.00218v1 [cs.CV])</h2>
<h3>Felix Juefei-Xu, Run Wang, Yihao Huang, Qing Guo, Lei Ma, Yang Liu</h3>
<p>The creation and the manipulation of facial appearance via deep generative
approaches, known as DeepFake, have achieved significant progress and promoted
a wide range of benign and malicious applications. The evil side of this new
technique poses another popular study, i.e., DeepFake detection aiming to
identify the fake faces from the real ones. With the rapid development of the
DeepFake-related studies in the community, both sides (i.e., DeepFake
generation and detection) have formed the relationship of the battleground,
pushing the improvements of each other and inspiring new directions, e.g., the
evasion of DeepFake detection. Nevertheless, the overview of such battleground
and the new direction is unclear and neglected by recent surveys due to the
rapid increase of related publications, limiting the in-depth understanding of
the tendency and future works.

To fill this gap, in this paper, we provide a comprehensive overview and
detailed analysis of the research work on the topic of DeepFake generation,
DeepFake detection as well as evasion of DeepFake detection, with more than 191
research papers carefully surveyed. We present the taxonomy of various DeepFake
generation methods and the categorization of various DeepFake detection
methods, and more importantly, we showcase the battleground between the two
parties with detailed interactions between the adversaries (DeepFake
generation) and the defenders (DeepFake detection). The battleground allows
fresh perspective into the latest landscape of the DeepFake research and can
provide valuable analysis towards the research challenges and opportunities as
well as research trends and directions in the field of DeepFake generation and
detection. We also elaborately design interactive diagrams
(this http URL) to allow researchers to explore their own
interests on popular DeepFake generators or detectors.
</p>
<a href="http://arxiv.org/abs/2103.00218" target="_blank">arXiv:2103.00218</a> [<a href="http://arxiv.org/pdf/2103.00218" target="_blank">pdf</a>]

<h2>Uncertainty-Aware Unsupervised Domain Adaptation in Object Detection. (arXiv:2103.00236v1 [cs.CV])</h2>
<h3>Dayan Guan, Jiaxing Huang, Aoran Xiao, Shijian Lu, Yanpeng Cao</h3>
<p>Unsupervised domain adaptive object detection aims to adapt detectors from a
labelled source domain to an unlabelled target domain. Most existing works take
a two-stage strategy that first generates region proposals and then detects
objects of interest, where adversarial learning is widely adopted to mitigate
the inter-domain discrepancy in both stages. However, adversarial learning may
impair the alignment of well-aligned samples as it merely aligns the global
distributions across domains. To address this issue, we design an
uncertainty-aware domain adaptation network (UaDAN) that introduces conditional
adversarial learning to align well-aligned and poorly-aligned samples
separately in different manners. Specifically, we design an uncertainty metric
that assesses the alignment of each sample and adjusts the strength of
adversarial learning for well-aligned and poorly-aligned samples adaptively. In
addition, we exploit the uncertainty metric to achieve curriculum learning that
first performs easier image-level alignment and then more difficult
instance-level alignment progressively. Extensive experiments over four
challenging domain adaptive object detection datasets show that UaDAN achieves
superior performance as compared with state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2103.00236" target="_blank">arXiv:2103.00236</a> [<a href="http://arxiv.org/pdf/2103.00236" target="_blank">pdf</a>]

<h2>Color-Coded Symbology and New Computer Vision Tool to Predict the Historical Color Pallets of the Renaissance Oil Artworks. (arXiv:2103.00238v1 [cs.CV])</h2>
<h3>Artyom M. Grigoryan, Sos S. Agaian</h3>
<p>In this paper, we discuss possible color palletes, prediction and analysis of
originality of the colors that Artists used on the Renaissance oil paintings.
This framework goal is to help to use the color symbology and image enhancement
tools, to predict the historical color palletes of the Renaissance oil
artworks. This work is only the start of a development to explore the
possibilities of prediction of color palletes of the Renaissance oil artworks.
We believe that framework might be very useful in the prediction of color
palletes of the Renaissance oil artworks and other artworks. The images in
number 105 have been taken from the paintings of three well-known artists,
Rafael, Leonardo Da Vinci, and Rembrandt that are available in the Olga's
Gallery. Images are processed in the frequency domain to enhance a quality of
images and ratios of primary colors are calculated and analyzed by using new
measurements of color-ratios.
</p>
<a href="http://arxiv.org/abs/2103.00238" target="_blank">arXiv:2103.00238</a> [<a href="http://arxiv.org/pdf/2103.00238" target="_blank">pdf</a>]

<h2>Exposing Semantic Segmentation Failures via Maximum Discrepancy Competition. (arXiv:2103.00259v1 [cs.CV])</h2>
<h3>Jiebin Yan, Yu Zhong, Yuming Fang, Zhangyang Wang, Kede Ma</h3>
<p>Semantic segmentation is an extensively studied task in computer vision, with
numerous methods proposed every year. Thanks to the advent of deep learning in
semantic segmentation, the performance on existing benchmarks is close to
saturation. A natural question then arises: Does the superior performance on
the closed (and frequently re-used) test sets transfer to the open visual world
with unconstrained variations? In this paper, we take steps toward answering
the question by exposing failures of existing semantic segmentation methods in
the open visual world under the constraint of very limited human labeling
effort. Inspired by previous research on model falsification, we start from an
arbitrarily large image set, and automatically sample a small image set by
MAximizing the Discrepancy (MAD) between two segmentation methods. The selected
images have the greatest potential in falsifying either (or both) of the two
methods. We also explicitly enforce several conditions to diversify the exposed
failures, corresponding to different underlying root causes. A segmentation
method, whose failures are more difficult to be exposed in the MAD competition,
is considered better. We conduct a thorough MAD diagnosis of ten PASCAL VOC
semantic segmentation algorithms. With detailed analysis of experimental
results, we point out strengths and weaknesses of the competing algorithms, as
well as potential research directions for further advancement in semantic
segmentation. The codes are publicly available at
\url{https://github.com/QTJiebin/MAD_Segmentation}.
</p>
<a href="http://arxiv.org/abs/2103.00259" target="_blank">arXiv:2103.00259</a> [<a href="http://arxiv.org/pdf/2103.00259" target="_blank">pdf</a>]

<h2>Walk2Map: Extracting Floor Plans from Indoor Walk Trajectories. (arXiv:2103.00262v1 [cs.CV])</h2>
<h3>Claudio Mura, Renato Pajarola, Konrad Schindler, Niloy Mitra</h3>
<p>Recent years have seen a proliferation of new digital products for the
efficient management of indoor spaces, with important applications like
emergency management, virtual property showcasing and interior design. These
products rely on accurate 3D models of the environments considered, including
information on both architectural and non-permanent elements. These models must
be created from measured data such as RGB-D images or 3D point clouds, whose
capture and consolidation involves lengthy data workflows. This strongly limits
the rate at which 3D models can be produced, preventing the adoption of many
digital services for indoor space management. We provide an alternative to such
data-intensive procedures by presenting Walk2Map, a data-driven approach to
generate floor plans only from trajectories of a person walking inside the
rooms. Thanks to recent advances in data-driven inertial odometry, such
minimalistic input data can be acquired from the IMU readings of consumer-level
smartphones, which allows for an effortless and scalable mapping of real-world
indoor spaces. Our work is based on learning the latent relation between an
indoor walk trajectory and the information represented in a floor plan:
interior space footprint, portals, and furniture. We distinguish between
recovering area-related (interior footprint, furniture) and wall-related
(doors) information and use two different neural architectures for the two
tasks: an image-based Encoder-Decoder and a Graph Convolutional Network,
respectively. We train our networks using scanned 3D indoor models and apply
them in a cascaded fashion on an indoor walk trajectory at inference time. We
perform a qualitative and quantitative evaluation using both simulated and
measured, real-world trajectories, and compare against a baseline method for
image-to-image translation. The experiments confirm the feasibility of our
approach.
</p>
<a href="http://arxiv.org/abs/2103.00262" target="_blank">arXiv:2103.00262</a> [<a href="http://arxiv.org/pdf/2103.00262" target="_blank">pdf</a>]

<h2>Object affordance as a guide for grasp-type recognition. (arXiv:2103.00268v1 [cs.RO])</h2>
<h3>Naoki Wake, Daichi Saito, Kazuhiro Sasabuchi, Hideki Koike, Katsushi Ikeuchi</h3>
<p>Recognizing human grasping strategies is an important factor in robot
teaching as these strategies contain the implicit knowledge necessary to
perform a series of manipulations smoothly. This study analyzed the effects of
object affordance-a prior distribution of grasp types for each object-on
convolutional neural network (CNN)-based grasp-type recognition. To this end,
we created datasets of first-person grasping-hand images labeled with grasp
types and object names, and tested a recognition pipeline leveraging object
affordance. We evaluated scenarios with real and illusory objects to be
grasped, to consider a teaching condition in mixed reality where the lack of
visual object information can make the CNN recognition challenging. The results
show that object affordance guided the CNN in both scenarios, increasing the
accuracy by 1) excluding unlikely grasp types from the candidates and 2)
enhancing likely grasp types. In addition, the "enhancing effect" was more
pronounced with high degrees of grasp-type heterogeneity. These results
indicate the effectiveness of object affordance for guiding grasp-type
recognition in robot teaching applications.
</p>
<a href="http://arxiv.org/abs/2103.00268" target="_blank">arXiv:2103.00268</a> [<a href="http://arxiv.org/pdf/2103.00268" target="_blank">pdf</a>]

<h2>Singularity-aware motion planning for multi-axis additive manufacturing. (arXiv:2103.00273v1 [cs.RO])</h2>
<h3>Tianyu Zhang, Xiangjia Chen, Guoxin Fang, Yingjun Tian, Charlie C.L. Wang</h3>
<p>Multi-axis additive manufacturing enables high flexibility of material
deposition along dynamically varied directions. The Cartesian motion platforms
of these machines include three parallel axes and two rotational axes.
Singularity on rotational axes is a critical issue to be tackled in motion
planning for ensuring high quality of manufacturing results. The highly
nonlinear mapping in the singular region can convert a smooth toolpath with
uniformly sampled waypoints defined in the model coordinate system into a
highly discontinuous motion in the machine coordinate system, which leads to
over-extrusion / under-extrusion of materials in filament-based additive
manufacturing. Moreover, collision may occur when sampling-based collision
avoidance is employed. In this paper, we present a motion planning method to
support the manufacturing realization of designed toolpaths for multi-axis
additive manufacturing. Problems of singularity and collision are considered in
an integrated manner to improve the motion therefore the quality of
fabrication. Experiments are conducted to demonstrate the performance of our
method.
</p>
<a href="http://arxiv.org/abs/2103.00273" target="_blank">arXiv:2103.00273</a> [<a href="http://arxiv.org/pdf/2103.00273" target="_blank">pdf</a>]

<h2>A Novel Adaptive Deep Network for Building Footprint Segmentation. (arXiv:2103.00286v1 [cs.CV])</h2>
<h3>A. Ziaee, R. Dehbozorgi, M. D&#xf6;ller</h3>
<p>Building footprint segmentations for high resolution images are increasingly
demanded for many remote sensing applications. By the emerging deep learning
approaches, segmentation networks have made significant advances in the
semantic segmentation of objects. However, these advances and the increased
access to satellite images require the generation of accurate object boundaries
in satellite images. In the current paper, we propose a novel network-based on
Pix2Pix methodology to solve the problem of inaccurate boundaries obtained by
converting satellite images into maps using segmentation networks in order to
segment building footprints. To define the new network named G2G, our framework
includes two generators where the first generator extracts localization
features in order to merge them with the boundary features extracted from the
second generator to segment all detailed building edges. Moreover, different
strategies are implemented to enhance the quality of the proposed networks'
results, implying that the proposed network outperforms state-of-the-art
networks in segmentation accuracy with a large margin for all evaluation
metrics. The implementation is available at
https://github.com/A2Amir/A-Novel-Adaptive-Deep-Network-for-Building-Footprint-Segmentation.
</p>
<a href="http://arxiv.org/abs/2103.00286" target="_blank">arXiv:2103.00286</a> [<a href="http://arxiv.org/pdf/2103.00286" target="_blank">pdf</a>]

<h2>BiconNet: An Edge-preserved Connectivity-based Approach for Salient Object Detection. (arXiv:2103.00334v1 [cs.CV])</h2>
<h3>Ziyun Yang, Somayyeh Soltanian-Zadeh, Sina Farsiu</h3>
<p>Salient object detection (SOD) is viewed as a pixel-wise saliency modeling
task by traditional deep learning-based methods. Although great progress has
been made, a challenge of modern SOD models is the insufficient utilization of
inter-pixel information, which usually results in imperfect segmentations near
the edge regions. As we demonstrate, using a saliency map as the network output
is a sub-optimal choice. To address this problem, we propose a
connectivity-based approach named bilateral connectivity network (BiconNet),
which uses a connectivity map instead of a saliency map as the network output
for effective modeling of inter-pixel relationships and object saliency.
Moreover, we propose a bilateral voting module to enhance the output
connectivity map and a novel edge feature enhancement method that efficiently
utilizes edge-specific features with negligible parameter increase. We show
that our model can use any existing saliency-based SOD framework as its
backbone. Through comprehensive experiments on five benchmark datasets, we
demonstrate that our proposed method outperforms state-of-the-art SOD
approaches.
</p>
<a href="http://arxiv.org/abs/2103.00334" target="_blank">arXiv:2103.00334</a> [<a href="http://arxiv.org/pdf/2103.00334" target="_blank">pdf</a>]

<h2>End-to-end Uncertainty-based Mitigation of Adversarial Attacks to Automated Lane Centering. (arXiv:2103.00345v1 [cs.RO])</h2>
<h3>Ruochen Jiao, Hengyi Liang, Takami Sato, Junjie Shen, Qi Alfred Chen, Qi Zhu</h3>
<p>In the development of advanced driver-assistance systems (ADAS) and
autonomous vehicles, machine learning techniques that are based on deep neural
networks (DNNs) have been widely used for vehicle perception. These techniques
offer significant improvement on average perception accuracy over traditional
methods, however, have been shown to be susceptible to adversarial attacks,
where small perturbations in the input may cause significant errors in the
perception results and lead to system failure. Most prior works addressing such
adversarial attacks focus only on the sensing and perception modules. In this
work, we propose an end-to-end approach that addresses the impact of
adversarial attacks throughout perception, planning, and control modules. In
particular, we choose a target ADAS application, the automated lane centering
system in OpenPilot, quantify the perception uncertainty under adversarial
attacks, and design a robust planning and control module accordingly based on
the uncertainty analysis. We evaluate our proposed approach using both the
public dataset and production-grade autonomous driving simulator. The
experiment results demonstrate that our approach can effectively mitigate the
impact of adversarial attacks and can achieve 55% to 90% improvement over the
original OpenPilot.
</p>
<a href="http://arxiv.org/abs/2103.00345" target="_blank">arXiv:2103.00345</a> [<a href="http://arxiv.org/pdf/2103.00345" target="_blank">pdf</a>]

<h2>SUM: A Benchmark Dataset of Semantic Urban Meshes. (arXiv:2103.00355v1 [cs.CV])</h2>
<h3>Weixiao Gao, Liangliang Nan, Bas Boom, Hugo Ledoux</h3>
<p>Recent developments in data acquisition technology allow us to collect 3D
texture meshes quickly. Those can help us understand and analyse the urban
environment, and as a consequence are useful for several applications like
spatial analysis and urban planning. Semantic segmentation of texture meshes
through deep learning methods can enhance this understanding, but it requires a
lot of labelled data. This paper introduces a new benchmark dataset of semantic
urban meshes, a novel semi-automatic annotation framework, and an open-source
annotation tool for 3D meshes. In particular, our dataset covers about 4 km2 in
Helsinki (Finland), with six classes, and we estimate that we save about 600
hours of labelling work using our annotation framework, which includes initial
segmentation and interactive refinement. Furthermore, we compare the
performance of several representative 3D semantic segmentation methods on our
annotated dataset. The results show our initial segmentation outperforms other
methods and achieves an overall accuracy of 93.0% and mIoU of 66.2% with less
training time compared to other deep learning methods. We also evaluate the
effect of the input training data, which shows that our method only requires
about 7% (which covers about 0.23 km2) to approach robust and adequate results
whereas KPConv needs at least 33% (which covers about 1.0 km2).
</p>
<a href="http://arxiv.org/abs/2103.00355" target="_blank">arXiv:2103.00355</a> [<a href="http://arxiv.org/pdf/2103.00355" target="_blank">pdf</a>]

<h2>Online Behavioral Analysis with Application to Emotion State Identification. (arXiv:2103.00356v1 [cs.CV])</h2>
<h3>Lei Gao, Lin Qi, Ling Guan</h3>
<p>In this paper, we propose a novel discriminative model for online behavioral
analysis with application to emotion state identification. The proposed model
is able to extract more discriminative characteristics from behavioral data
effectively and find the direction of optimal projection efficiently to satisfy
requirements of online data analysis, leading to better utilization of the
behavioral information to produce more accurate recognition results.
</p>
<a href="http://arxiv.org/abs/2103.00356" target="_blank">arXiv:2103.00356</a> [<a href="http://arxiv.org/pdf/2103.00356" target="_blank">pdf</a>]

<h2>A Holistic Motion Planning and Control Solution to Challenge a Professional Racecar Driver. (arXiv:2103.00358v1 [cs.RO])</h2>
<h3>Sirish Srinivasan, Sebastian Nicolas Giles, Alexander Liniger</h3>
<p>We present a holistically designed three layer control architecture capable
of outperforming a professional driver racing the same car. Our approach
focuses on the co-design of the motion planning and control layers, extracting
the full potential of the connected system. First, a high-level planner
computes an optimal trajectory around the track, then in real-time the
mid-level nonlinear model predictive controller follows this path using the
high-level information as guidance. Finally a high frequency, low-level
controller tracks the states predicted by the mid-level controller. Tracking
the predicted behavior has two advantages: it reduces the mismatch between the
model used in the upper layers and the real car, and allows for a torque
vectoring command to be optimized by the higher level motion planners. The
tailored design of the low-level controller proved to be crucial for bridging
the gap between planning and control, unlocking unseen performance in
autonomous racing. The proposed approach was verified on a full size racecar,
resulting in a considerable improvement over the state-of-the-art results
achieved on the same vehicle. Finally, we also show that the proposed co-design
approach outperforms a professional racecar driver.
</p>
<a href="http://arxiv.org/abs/2103.00358" target="_blank">arXiv:2103.00358</a> [<a href="http://arxiv.org/pdf/2103.00358" target="_blank">pdf</a>]

<h2>The Labeled Multiple Canonical Correlation Analysis for Information Fusion. (arXiv:2103.00359v1 [cs.CV])</h2>
<h3>Lei Gao, Rui Zhang, Lin Qi, Enqing Chen, Ling Guan</h3>
<p>The objective of multimodal information fusion is to mathematically analyze
information carried in different sources and create a new representation which
will be more effectively utilized in pattern recognition and other multimedia
information processing tasks. In this paper, we introduce a new method for
multimodal information fusion and representation based on the Labeled Multiple
Canonical Correlation Analysis (LMCCA). By incorporating class label
information of the training samples,the proposed LMCCA ensures that the fused
features carry discriminative characteristics of the multimodal information
representations, and are capable of providing superior recognition performance.
We implement a prototype of LMCCA to demonstrate its effectiveness on
handwritten digit recognition,face recognition and object recognition utilizing
multiple features,bimodal human emotion recognition involving information from
both audio and visual domains. The generic nature of LMCCA allows it to take as
input features extracted by any means,including those by deep learning (DL)
methods. Experimental results show that the proposed method enhanced the
performance of both statistical machine learning (SML) methods, and methods
based on DL.
</p>
<a href="http://arxiv.org/abs/2103.00359" target="_blank">arXiv:2103.00359</a> [<a href="http://arxiv.org/pdf/2103.00359" target="_blank">pdf</a>]

<h2>Predicting post-operative right ventricular failure using video-based deep learning. (arXiv:2103.00364v1 [cs.CV])</h2>
<h3>Rohan Shad, Nicolas Quach, Robyn Fong, Patpilai Kasinpila, Cayley Bowles, Miguel Castro, Ashrith Guha, Eddie Suarez, Stefan Jovinge, Sangjin Lee, Theodore Boeve, Myriam Amsallem, Xiu Tang, Francois Haddad, Yasuhiro Shudo, Y. Joseph Woo, Jeffrey Teuteberg, John P. Cunningham, Curt P. Langlotz, William Hiesinger</h3>
<p>Non-invasive and cost effective in nature, the echocardiogram allows for a
comprehensive assessment of the cardiac musculature and valves. Despite
progressive improvements over the decades, the rich temporally resolved data in
echocardiography videos remain underutilized. Human reads of echocardiograms
reduce the complex patterns of cardiac wall motion, to a small list of
measurements of heart function. Furthermore, all modern echocardiography
artificial intelligence (AI) systems are similarly limited by design -
automating measurements of the same reductionist metrics rather than utilizing
the wealth of data embedded within each echo study. This underutilization is
most evident in situations where clinical decision making is guided by
subjective assessments of disease acuity, and tools that predict disease onset
within clinically actionable timeframes are unavailable. Predicting the
likelihood of developing post-operative right ventricular failure (RV failure)
in the setting of mechanical circulatory support is one such clinical example.
To address this, we developed a novel video AI system trained to predict
post-operative right ventricular failure (RV failure), using the full
spatiotemporal density of information from pre-operative echocardiography
scans. We achieve an AUC of 0.729, specificity of 52% at 80% sensitivity and
46% sensitivity at 80% specificity. Furthermore, we show that our ML system
significantly outperforms a team of human experts tasked with predicting RV
failure on independent clinical evaluation. Finally, the methods we describe
are generalizable to any cardiac clinical decision support application where
treatment or patient selection is guided by qualitative echocardiography
assessments.
</p>
<a href="http://arxiv.org/abs/2103.00364" target="_blank">arXiv:2103.00364</a> [<a href="http://arxiv.org/pdf/2103.00364" target="_blank">pdf</a>]

<h2>Towards Continual, Online, Unsupervised Depth. (arXiv:2103.00369v1 [cs.CV])</h2>
<h3>Muhammad Umar Karim Khan</h3>
<p>Although depth extraction with passive sensors has seen remarkable
improvement with deep learning, these approaches may fail to obtain correct
depth if they are exposed to environments not observed during training. Online
adaptation, where the neural network trains while deployed, with unsupervised
learning provides a convenient solution. However, online adaptation causes a
neural network to forget the past. Thus, past training is wasted and the
network is not able to provide good results if it observes past scenes. This
work deals with practical online-adaptation where the input is online and
temporally-correlated, and training is completely unsupervised. Regularization
and replay-based methods without task boundaries are proposed to avoid
catastrophic forgetting while adapting to online data. Experiments are
performed on different datasets with both structure-from-motion and stereo.
Results of forgetting as well as adaptation are provided, which are superior to
recent methods. The proposed approach is more inline with the artificial
general intelligence paradigm as the neural network learns the scene where it
is deployed without any supervision (target labels and tasks) and without
forgetting about the past. Code is available at github.com/umarKarim/cou-stereo
and github.com/umarKarim/cou-sfm.
</p>
<a href="http://arxiv.org/abs/2103.00369" target="_blank">arXiv:2103.00369</a> [<a href="http://arxiv.org/pdf/2103.00369" target="_blank">pdf</a>]

<h2>Generalization Through Hand-Eye Coordination: An Action Space for Learning Spatially-Invariant Visuomotor Control. (arXiv:2103.00375v1 [cs.RO])</h2>
<h3>Chen Wang, Rui Wang, Danfei Xu, Ajay Mandlekar, Li Fei-Fei, Silvio Savarese</h3>
<p>Imitation Learning (IL) is an effective framework to learn visuomotor skills
from offline demonstration data. However, IL methods often fail to generalize
to new scene configurations not covered by training data. On the other hand,
humans can manipulate objects in varying conditions. Key to such capability is
hand-eye coordination, a cognitive ability that enables humans to adaptively
direct their movements at task-relevant objects and be invariant to the
objects' absolute spatial location. In this work, we present a learnable action
space, Hand-eye Action Networks (HAN), that can approximate human's hand-eye
coordination behaviors by learning from human teleoperated demonstrations.
Through a set of challenging multi-stage manipulation tasks, we show that a
visuomotor policy equipped with HAN is able to inherit the key spatial
invariance property of hand-eye coordination and achieve zero-shot
generalization to new scene configurations. Additional materials available at
https://sites.google.com/stanford.edu/han
</p>
<a href="http://arxiv.org/abs/2103.00375" target="_blank">arXiv:2103.00375</a> [<a href="http://arxiv.org/pdf/2103.00375" target="_blank">pdf</a>]

<h2>Avoiding dynamic small obstacles with onboard sensing and computating on aerial robots. (arXiv:2103.00406v1 [cs.RO])</h2>
<h3>Fanze Kong, Wei Xu, Fu Zhang</h3>
<p>In practical applications, autonomous quadrotors are still facing significant
challenges, such as the detection and avoidance of very small and even dynamic
obstacles (e.g., tree branches, power lines). In this paper, we propose a
compact, integrated, and fully autonomous quadrotor system, which can fly
safely in cluttered environments while avoiding dynamic small obstacles. Our
quadrotor platform is equipped with a forward-looking three-dimensional (3D)
light detection and ranging (lidar) sensor to perceive the environment and an
onboard embedded computer to perform all the estimation, mapping, and planning
tasks. Specifically, the computer estimates the current pose of the UAV,
maintains a local map (time-accumulated point clouds KD-Trees), and computes a
safe trajectory using kinodynamic A* search to the goal point. The whole
perception and planning system can run onboard at 50Hz with careful
optimization. Various indoor and outdoor experiments show that the system can
avoid dynamic small obstacles (down to 20mm diameter bar) while flying at 2m/s
in cluttered environments. Our codes and hardware design are open-sourced on
Github.
</p>
<a href="http://arxiv.org/abs/2103.00406" target="_blank">arXiv:2103.00406</a> [<a href="http://arxiv.org/pdf/2103.00406" target="_blank">pdf</a>]

<h2>Sim-to-Real Transfer for Robotic Manipulation with Tactile Sensory. (arXiv:2103.00410v1 [cs.RO])</h2>
<h3>Zihan Ding, Ya-Yen Tsai, Wang Wei Lee, Bidan Huang</h3>
<p>Reinforcement Learning (RL) methods have been widely applied for robotic
manipulations via sim-to-real transfer, typically with proprioceptive and
visual information. However, the incorporation of tactile sensing into RL for
contact-rich tasks lacks investigation. In this paper, we model a tactile
sensor in simulation and study the effects of its feedback in RL-based robotic
control via a zero-shot sim-to-real approach with domain randomization. We
demonstrate that learning and controlling with feedback from tactile sensor
arrays at the gripper, both in simulation and reality, can enhance grasping
stability, which leads to a significant improvement in robotic manipulation
performance for a door opening task. In real-world experiments, the door open
angle was increased by 45% on average for transferred policies with tactile
sensing over those without it.
</p>
<a href="http://arxiv.org/abs/2103.00410" target="_blank">arXiv:2103.00410</a> [<a href="http://arxiv.org/pdf/2103.00410" target="_blank">pdf</a>]

<h2>Medical Image Segmentation with Limited Supervision: A Review of Deep Network Models. (arXiv:2103.00429v1 [cs.CV])</h2>
<h3>Jialin Peng, Ye Wang</h3>
<p>Despite the remarkable performance of deep learning methods on various tasks,
most cutting-edge models rely heavily on large-scale annotated training
examples, which are often unavailable for clinical and health care tasks. The
labeling costs for medical images are very high, especially in medical image
segmentation, which typically requires intensive pixel/voxel-wise labeling.
Therefore, the strong capability of learning and generalizing from limited
supervision, including a limited amount of annotations, sparse annotations, and
inaccurate annotations, is crucial for the successful application of deep
learning models in medical image segmentation. However, due to its intrinsic
difficulty, segmentation with limited supervision is challenging and specific
model design and/or learning strategies are needed. In this paper, we provide a
systematic and up-to-date review of the solutions above, with summaries and
comments about the methodologies. We also highlight several problems in this
field, discussed future directions observing further investigations.
</p>
<a href="http://arxiv.org/abs/2103.00429" target="_blank">arXiv:2103.00429</a> [<a href="http://arxiv.org/pdf/2103.00429" target="_blank">pdf</a>]

<h2>Training Generative Adversarial Networks in One Stage. (arXiv:2103.00430v1 [cs.CV])</h2>
<h3>Chengchao Shen, Youtan Yin, Xinchao Wang, Xubin LI, Jie Song, Mingli Song</h3>
<p>Generative Adversarial Networks (GANs) have demonstrated unprecedented
success in various image generation tasks. The encouraging results, however,
come at the price of a cumbersome training process, during which the generator
and discriminator are alternately updated in two stages. In this paper, we
investigate a general training scheme that enables training GANs efficiently in
only one stage. Based on the adversarial losses of the generator and
discriminator, we categorize GANs into two classes, Symmetric GANs and
Asymmetric GANs, and introduce a novel gradient decomposition method to unify
the two, allowing us to train both classes in one stage and hence alleviate the
training effort. Computational analysis and experimental results on several
datasets and various network architectures demonstrate that, the proposed
one-stage training scheme yields a solid 1.5$\times$ acceleration over
conventional training schemes, regardless of the network architectures of the
generator and discriminator. Furthermore, we show that the proposed method is
readily applicable to other adversarial-training scenarios, such as data-free
knowledge distillation. Our source code will be published soon.
</p>
<a href="http://arxiv.org/abs/2103.00430" target="_blank">arXiv:2103.00430</a> [<a href="http://arxiv.org/pdf/2103.00430" target="_blank">pdf</a>]

<h2>Learning for Visual Navigation by Imagining the Success. (arXiv:2103.00446v1 [cs.CV])</h2>
<h3>Mahdi Kazemi Moghaddam, Ehsan Abbasnejad, Qi Wu, Javen Shi, Anton Van Den Hengel</h3>
<p>Visual navigation is often cast as a reinforcement learning (RL) problem.
Current methods typically result in a suboptimal policy that learns general
obstacle avoidance and search behaviours. For example, in the target-object
navigation setting, the policies learnt by traditional methods often fail to
complete the task, even when the target is clearly within reach from a human
perspective. In order to address this issue, we propose to learn to imagine a
latent representation of the successful (sub-)goal state. To do so, we have
developed a module which we call Foresight Imagination (ForeSIT). ForeSIT is
trained to imagine the recurrent latent representation of a future state that
leads to success, e.g. either a sub-goal state that is important to reach
before the target, or the goal state itself. By conditioning the policy on the
generated imagination during training, our agent learns how to use this
imagination to achieve its goal robustly. Our agent is able to imagine what the
(sub-)goal state may look like (in the latent space) and can learn to navigate
towards that state. We develop an efficient learning algorithm to train ForeSIT
in an on-policy manner and integrate it into our RL objective. The integration
is not trivial due to the constantly evolving state representation shared
between both the imagination and the policy. We, empirically, observe that our
method outperforms the state-of-the-art methods by a large margin in the
commonly accepted benchmark AI2THOR environment. Our method can be readily
integrated or added to other model-free RL navigation frameworks.
</p>
<a href="http://arxiv.org/abs/2103.00446" target="_blank">arXiv:2103.00446</a> [<a href="http://arxiv.org/pdf/2103.00446" target="_blank">pdf</a>]

<h2>Path Planning for Manipulation using Experience-driven Random Trees. (arXiv:2103.00448v1 [cs.RO])</h2>
<h3>&#xc8;ric Pairet, Constantinos Chamzas, Yvan Petillot, Lydia E. Kavraki</h3>
<p>Robotic systems may frequently come across similar manipulation planning
problems that result in similar motion plans. Instead of planning each problem
from scratch, it is preferable to leverage previously computed motion plans,
i.e., experiences, to ease the planning. Different approaches have been
proposed to exploit prior information on novel task instances. These methods,
however, rely on a vast repertoire of experiences and fail when none relates
closely to the current problem. Thus, an open challenge is the ability to
generalise prior experiences to task instances that do not necessarily resemble
the prior. This work tackles the above challenge with the proposition that
experiences are "decomposable" and "malleable", i.e., parts of an experience
are suitable to relevantly explore the connectivity of the robot-task space
even in non-experienced regions. Two new planners result from this insight:
experience-driven random trees (ERT) and its bi-directional version ERTConnect.
These planners adopt a tree sampling-based strategy that incrementally extracts
and modulates parts of a single path experience to compose a valid motion plan.
We demonstrate our method on task instances that significantly differ from the
prior experiences, and compare with related state-of-the-art experience-based
planners. While their repairing strategies fail to generalise priors of tens of
experiences, our planner, with a single experience, significantly outperforms
them in both success rate and planning time. Our planners are implemented and
freely available in the Open Motion Planning Library.
</p>
<a href="http://arxiv.org/abs/2103.00448" target="_blank">arXiv:2103.00448</a> [<a href="http://arxiv.org/pdf/2103.00448" target="_blank">pdf</a>]

<h2>EKMP: Generalized Imitation Learning with Adaptation, Nonlinear Hard Constraints and Obstacle Avoidance. (arXiv:2103.00452v1 [cs.RO])</h2>
<h3>Yanlong Huang</h3>
<p>As a user-friendly and straightforward solution for robot trajectory
generation, imitation learning has been viewed as a vital direction in the
context of robot skill learning. In contrast to unconstrained imitation
learning which ignores possible internal and external constraints arising from
environments and robot kinematics/dynamics, recent works on constrained
imitation learning allow for transferring human skills to unstructured
scenarios, further enlarging the application domain of imitation learning.
While various constraints have been studied, e.g., joint limits, obstacle
avoidance and plane constraints, the problem of nonlinear hard constraints has
not been well-addressed. In this paper, we propose extended kernelized movement
primitives (EKMP) to cope with most of the key problems in imitation learning,
including nonlinear hard constraints. Specifically, EKMP is capable of learning
the probabilistic features of multiple demonstrations, adapting the learned
skills towards arbitrary desired points in terms of joint position and
velocity, avoiding obstacles at the level of robot links, as well as satisfying
arbitrary linear and nonlinear, equality and inequality hard constraints.
Besides, the connections between EKMP and state-of-the-art motion planning
approaches are discussed. Several evaluations including the planning of joint
trajectories for a 7-DoF robotic arm are provided to verify the effectiveness
of our framework.
</p>
<a href="http://arxiv.org/abs/2103.00452" target="_blank">arXiv:2103.00452</a> [<a href="http://arxiv.org/pdf/2103.00452" target="_blank">pdf</a>]

<h2>NLP-CUET@DravidianLangTech-EACL2021: Investigating Visual and Textual Features to Identify Trolls from Multimodal Social Media Memes. (arXiv:2103.00466v1 [cs.CV])</h2>
<h3>Eftekhar Hossain, Omar Sharif, Mohammed Moshiul Hoque</h3>
<p>In the past few years, the meme has become a new way of communication on the
Internet. As memes are the images with embedded text, it can quickly spread
hate, offence and violence. Classifying memes are very challenging because of
their multimodal nature and region-specific interpretation. A shared task is
organized to develop models that can identify trolls from multimodal social
media memes. This work presents a computational model that we have developed as
part of our participation in the task. Training data comes in two forms: an
image with embedded Tamil code-mixed text and an associated caption given in
English. We investigated the visual and textual features using CNN, VGG16,
Inception, Multilingual-BERT, XLM-Roberta, XLNet models. Multimodal features
are extracted by combining image (CNN, ResNet50, Inception) and text (Long
short term memory network) features via early fusion approach. Results indicate
that the textual approach with XLNet achieved the highest weighted $f_1$-score
of $0.58$, which enabled our model to secure $3^{rd}$ rank in this task.
</p>
<a href="http://arxiv.org/abs/2103.00466" target="_blank">arXiv:2103.00466</a> [<a href="http://arxiv.org/pdf/2103.00466" target="_blank">pdf</a>]

<h2>Face Mask Extraction in Video Sequence. (arXiv:1807.09207v3 [cs.CV] UPDATED)</h2>
<h3>Yujiang Wang, Bingnan Luo, Jie Shen, Maja Pantic</h3>
<p>Inspired by the recent development of deep network-based methods in semantic
image segmentation, we introduce an end-to-end trainable model for face mask
extraction in video sequence. Comparing to landmark-based sparse face shape
representation, our method can produce the segmentation masks of individual
facial components, which can better reflect their detailed shape variations. By
integrating Convolutional LSTM (ConvLSTM) algorithm with Fully Convolutional
Networks (FCN), our new ConvLSTM-FCN model works on a per-sequence basis and
takes advantage of the temporal correlation in video clips. In addition, we
also propose a novel loss function, called Segmentation Loss, to directly
optimise the Intersection over Union (IoU) performances. In practice, to
further increase segmentation accuracy, one primary model and two additional
models were trained to focus on the face, eyes, and mouth regions,
respectively. Our experiment shows the proposed method has achieved a 16.99%
relative improvement (from 54.50% to 63.76% mean IoU) over the baseline FCN
model on the 300 Videos in the Wild (300VW) dataset.
</p>
<a href="http://arxiv.org/abs/1807.09207" target="_blank">arXiv:1807.09207</a> [<a href="http://arxiv.org/pdf/1807.09207" target="_blank">pdf</a>]

<h2>Deep Shape-from-Template: Wide-Baseline, Dense and Fast Registration and Deformable Reconstruction from a Single Image. (arXiv:1811.07791v3 [cs.CV] UPDATED)</h2>
<h3>David Fuentes-Jimenez, David Casillas-Perez, Daniel Pizarro, Toby Collins, Adrien Bartoli</h3>
<p>We present Deep Shape-from-Template (DeepSfT), a novel Deep Neural Network
(DNN) method for solving real-time automatic registration and 3D reconstruction
of a deformable object viewed in a single monocular image.DeepSfT advances the
state-of-the-art in various aspects. Compared to existing DNN SfT methods, it
is the first fully convolutional real-time approach that handles an arbitrary
object geometry, topology and surface representation. It also does not require
ground truth registration with real data and scales well to very complex object
models with large numbers of elements. Compared to previous non-DNN SfT
methods, it does not involve numerical optimization at run-time, and is a
dense, wide-baseline solution that does not demand, and does not suffer from,
feature-based matching. It is able to process a single image with significant
deformation and viewpoint changes, and handles well the core challenges of
occlusions, weak texture and blur. DeepSfT is based on residual encoder-decoder
structures and refining blocks. It is trained end-to-end with a novel
combination of supervised learning from simulated renderings of the object
model and semi-supervised automatic fine-tuning using real data captured with a
standard RGB-D camera. The cameras used for fine-tuning and run-time can be
different, making DeepSfT practical for real-world use. We show that DeepSfT
significantly outperforms state-of-the-art wide-baseline approaches for
non-trivial templates, with quantitative and qualitative evaluation.
</p>
<a href="http://arxiv.org/abs/1811.07791" target="_blank">arXiv:1811.07791</a> [<a href="http://arxiv.org/pdf/1811.07791" target="_blank">pdf</a>]

<h2>Geometry-aware Manipulability Learning, Tracking and Transfer. (arXiv:1811.11050v5 [cs.RO] UPDATED)</h2>
<h3>No&#xe9;mie Jaquier, Leonel Rozo, Darwin G. Caldwell, Sylvain Calinon</h3>
<p>Body posture influences human and robots performance in manipulation tasks,
as appropriate poses facilitate motion or force exertion along different axes.
In robotics, manipulability ellipsoids arise as a powerful descriptor to
analyze, control and design the robot dexterity as a function of the
articulatory joint configuration. This descriptor can be designed according to
different task requirements, such as tracking a desired position or apply a
specific force. In this context, this paper presents a novel
\emph{manipulability transfer} framework, a method that allows robots to learn
and reproduce manipulability ellipsoids from expert demonstrations. The
proposed learning scheme is built on a tensor-based formulation of a Gaussian
mixture model that takes into account that manipulability ellipsoids lie on the
manifold of symmetric positive definite matrices. Learning is coupled with a
geometry-aware tracking controller allowing robots to follow a desired profile
of manipulability ellipsoids. Extensive evaluations in simulation with
redundant manipulators, a robotic hand and humanoids agents, as well as an
experiment with two real dual-arm systems validate the feasibility of the
approach.
</p>
<a href="http://arxiv.org/abs/1811.11050" target="_blank">arXiv:1811.11050</a> [<a href="http://arxiv.org/pdf/1811.11050" target="_blank">pdf</a>]

<h2>Semantics-Aware Image to Image Translation and Domain Transfer. (arXiv:1904.02203v2 [cs.CV] UPDATED)</h2>
<h3>Pravakar Roy, Nicolai H&#xe4;ni, Jun-Jee Chao, Volkan Isler</h3>
<p>Image to image translation is the problem of transferring an image from a
source domain to a different (but related) target domain. We present a new
unsupervised image to image translation technique that leverages the underlying
semantic information for object transfiguration and domain transfer tasks.
Specifically, we present a generative adversarial learning approach that
jointly translates images and labels from a source domain to a target domain.
Our main technical contribution is an encoder-decoder based network
architecture that jointly encodes the image and its underlying semantics and
translates both individually to the target domain. Additionally, we propose
object transfiguration and cross-domain semantic consistency losses that
preserve semantic labels. Through extensive experimental evaluation, we
demonstrate the effectiveness of our approach as compared to the
state-of-the-art methods on unsupervised image-to-image translation, domain
adaptation, and object transfiguration.
</p>
<a href="http://arxiv.org/abs/1904.02203" target="_blank">arXiv:1904.02203</a> [<a href="http://arxiv.org/pdf/1904.02203" target="_blank">pdf</a>]

<h2>Composition of Safety Constraints For Fixed-Wing Collision Avoidance Amidst Limited Communications. (arXiv:1906.03771v4 [cs.RO] UPDATED)</h2>
<h3>Eric Squires, Pietro Pierpaoli, Rohit Konda, Samuel Coogan, Magnus Egerstedt</h3>
<p>This paper considers how to ensure that a system of fixed wing Unmanned
Aerial Vehicles (UAVs) can avoid collisions. To do so we develop a novel method
for creating a barrier function, which is similar to a Lyapunov function and
can be used to ensure that a system can stay safe for all future times. After
introducing the general approach, it is shown how to ensure that collision
avoidance for two vehicles can be guaranteed for all future times. The
construction is then extended to the case of arbitrarily many vehicles by
addressing how to satisfy multiple safety objectives simultaneously. We do this
while ensuring output actuator commands are within specified limits. Because
this formulation requires communication of control values and may therefore
reduce throughput of other important messages, we then show how to reformulate
the solution without this significant communication overhead while still
ensuring safety is maintained and actuator limits are respected. We validate
the theoretical developments of this paper in the simulator SCRIMMAGE with a
simulation of 20 UAVs that maintain safe distances from each other even though
their nominal paths would otherwise cause a collision.
</p>
<a href="http://arxiv.org/abs/1906.03771" target="_blank">arXiv:1906.03771</a> [<a href="http://arxiv.org/pdf/1906.03771" target="_blank">pdf</a>]

<h2>Dynamic Face Video Segmentation via Reinforcement Learning. (arXiv:1907.01296v4 [cs.CV] UPDATED)</h2>
<h3>Yujiang Wang, Mingzhi Dong, Jie Shen, Yang Wu, Shiyang Cheng, Maja Pantic</h3>
<p>For real-time semantic video segmentation, most recent works utilised a
dynamic framework with a key scheduler to make online key/non-key decisions.
Some works used a fixed key scheduling policy, while others proposed adaptive
key scheduling methods based on heuristic strategies, both of which may lead to
suboptimal global performance. To overcome this limitation, we model the online
key decision process in dynamic video segmentation as a deep reinforcement
learning problem and learn an efficient and effective scheduling policy from
expert information about decision history and from the process of maximising
global return. Moreover, we study the application of dynamic video segmentation
on face videos, a field that has not been investigated before. By evaluating on
the 300VW dataset, we show that the performance of our reinforcement key
scheduler outperforms that of various baselines in terms of both effective key
selections and running speed. Further results on the Cityscapes dataset
demonstrate that our proposed method can also generalise to other scenarios. To
the best of our knowledge, this is the first work to use reinforcement learning
for online key-frame decision in dynamic video segmentation, and also the first
work on its application on face videos.
</p>
<a href="http://arxiv.org/abs/1907.01296" target="_blank">arXiv:1907.01296</a> [<a href="http://arxiv.org/pdf/1907.01296" target="_blank">pdf</a>]

<h2>Learning Deep Parameterized Skills from Demonstration for Re-targetable Visuomotor Control. (arXiv:1910.10628v2 [cs.RO] UPDATED)</h2>
<h3>Jonathan Chang, Nishanth Kumar, Sean Hastings, Aaron Gokaslan, Diego Romeres, Devesh Jha, Daniel Nikovski, George Konidaris, Stefanie Tellex</h3>
<p>Robots need to learn skills that can not only generalize across similar
problems but also be directed to a specific goal. Previous methods either train
a new skill for every different goal or do not infer the specific target in the
presence of multiple goals from visual data. We introduce an end-to-end method
that represents targetable visuomotor skills as a goal-parameterized neural
network policy. By training on an informative subset of available goals with
the associated target parameters, we are able to learn a policy that can
zero-shot generalize to previously unseen goals. We evaluate our method in a
representative 2D simulation of a button-grid and on both button-pressing and
peg-insertion tasks on two different physical arms. We demonstrate that our
model trained on 33% of the possible goals is able to generalize to more than
90% of the targets in the scene for both simulation and robot experiments. We
also successfully learn a mapping from target pixel coordinates to a robot
policy to complete a specified goal.
</p>
<a href="http://arxiv.org/abs/1910.10628" target="_blank">arXiv:1910.10628</a> [<a href="http://arxiv.org/pdf/1910.10628" target="_blank">pdf</a>]

<h2>Video action detection by learning graph-based spatio-temporal interactions. (arXiv:1912.04316v3 [cs.CV] UPDATED)</h2>
<h3>Matteo Tomei, Lorenzo Baraldi, Simone Calderara, Simone Bronzin, Rita Cucchiara</h3>
<p>Action Detection is a complex task that aims to detect and classify human
actions in video clips. Typically, it has been addressed by processing
fine-grained features extracted from a video classification backbone. Recently,
thanks to the robustness of object and people detectors, a deeper focus has
been added on relationship modelling. Following this line, we propose a
graph-based framework to learn high-level interactions between people and
objects, in both space and time. In our formulation, spatio-temporal
relationships are learned through self-attention on a multi-layer graph
structure which can connect entities from consecutive clips, thus considering
long-range spatial and temporal dependencies. The proposed module is backbone
independent by design and does not require end-to-end training. Extensive
experiments are conducted on the AVA dataset, where our model demonstrates
state-of-the-art results and consistent improvements over baselines built with
different backbones. Code is publicly available at
https://github.com/aimagelab/STAGE_action_detection.
</p>
<a href="http://arxiv.org/abs/1912.04316" target="_blank">arXiv:1912.04316</a> [<a href="http://arxiv.org/pdf/1912.04316" target="_blank">pdf</a>]

<h2>SKD: Keypoint Detection for Point Clouds using Saliency Estimation. (arXiv:1912.04943v3 [cs.CV] UPDATED)</h2>
<h3>Georgi Tinchev, Adrian Penate-Sanchez, Maurice Fallon</h3>
<p>We present SKD, a novel keypoint detector that uses saliency to determine the
best candidates from a point cloud for tasks such as registration and
reconstruction. The approach can be applied to any differentiable deep learning
descriptor by using the gradients of that descriptor with respect to the 3D
position of the input points as a measure of their saliency. The saliency is
combined with the original descriptor and context information in a neural
network, which is trained to learn robust keypoint candidates. The key
intuition behind this approach is that keypoints are not extracted solely as a
result of the geometry surrounding a point, but also take into account the
descriptor's response. The approach was evaluated on two large LIDAR datasets -
the Oxford RobotCar dataset and the KITTI dataset, where we obtain up to 50%
improvement over the state-of-the-art in both matchability and repeatability.
When performing sparse matching with the keypoints computed by our method we
achieve a higher inlier ratio and faster convergence.
</p>
<a href="http://arxiv.org/abs/1912.04943" target="_blank">arXiv:1912.04943</a> [<a href="http://arxiv.org/pdf/1912.04943" target="_blank">pdf</a>]

<h2>Evaluating the Progress of Deep Learning for Visual Relational Concepts. (arXiv:2001.10857v2 [cs.CV] UPDATED)</h2>
<h3>Sebastian Stabinger, Peer David, Justus Piater, Antonio Rodr&#xed;guez-S&#xe1;nchez</h3>
<p>Convolutional Neural Networks (CNNs) have become the state of the art method
for image classification in the last ten years. Despite the fact that they
achieve superhuman classification accuracy on many popular datasets, they often
perform much worse on more abstract image classification tasks. We will show
that these difficult tasks are linked to relational concepts from the field of
concept learning.

We will review deep learning research that is linked to this area, even if it
was not originally presented from this angle. Reviewing the current literature,
we will argue that used datasets lead to an overestimate of system performance
by providing data in a pre-attended form, by overestimating the true
variability and complexity of the given tasks, and other shortcomings.

We will hypothesise that iterative processing of the input, together with
attentional shifts, will be needed to efficiently and reliably solve relational
reasoning tasks with deep learning methods.
</p>
<a href="http://arxiv.org/abs/2001.10857" target="_blank">arXiv:2001.10857</a> [<a href="http://arxiv.org/pdf/2001.10857" target="_blank">pdf</a>]

<h2>Fast Symmetric Diffeomorphic Image Registration with Convolutional Neural Networks. (arXiv:2003.09514v3 [cs.CV] UPDATED)</h2>
<h3>Tony C.W. Mok, Albert C.S. Chung</h3>
<p>Diffeomorphic deformable image registration is crucial in many medical image
studies, as it offers unique, special properties including topology
preservation and invertibility of the transformation. Recent deep
learning-based deformable image registration methods achieve fast image
registration by leveraging a convolutional neural network (CNN) to learn the
spatial transformation from the synthetic ground truth or the similarity
metric. However, these approaches often ignore the topology preservation of the
transformation and the smoothness of the transformation which is enforced by a
global smoothing energy function alone. Moreover, deep learning-based
approaches often estimate the displacement field directly, which cannot
guarantee the existence of the inverse transformation. In this paper, we
present a novel, efficient unsupervised symmetric image registration method
which maximizes the similarity between images within the space of diffeomorphic
maps and estimates both forward and inverse transformations simultaneously. We
evaluate our method on 3D image registration with a large scale brain image
dataset. Our method achieves state-of-the-art registration accuracy and running
time while maintaining desirable diffeomorphic properties.
</p>
<a href="http://arxiv.org/abs/2003.09514" target="_blank">arXiv:2003.09514</a> [<a href="http://arxiv.org/pdf/2003.09514" target="_blank">pdf</a>]

<h2>OF-VO: Reliable Navigation among Pedestrians Using Commodity Sensors. (arXiv:2004.10976v5 [cs.RO] UPDATED)</h2>
<h3>Jing Liang, Yi-Ling Qiao, Tianrui Guan, Dinesh Manocha</h3>
<p>We present a modified velocity-obstacle (VO) algorithm that uses
probabilistic partial observations of the environment to compute velocities and
navigate a robot to a target. Our system uses commodity visual sensors,
including a mono-camera and a 2D Lidar, to explicitly predict the velocities
and positions of surrounding obstacles through optical flow estimation, object
detection, and sensor fusion. A key aspect of our work is coupling the
perception (OF: optical flow) and planning (VO) components for reliable
navigation. Overall, our OF-VO algorithm using learning-based perception and
model-based planning methods offers better performance than prior algorithms in
terms of navigation time and success rate of collision avoidance. Our method
also provides bounds on the probabilistic collision avoidance algorithm. We
highlight the realtime performance of OF-VO on a Turtlebot navigating among
pedestrians in both simulated and real-world scenes. A demo video is available
at https://gamma.umd.edu/ofvo
</p>
<a href="http://arxiv.org/abs/2004.10976" target="_blank">arXiv:2004.10976</a> [<a href="http://arxiv.org/pdf/2004.10976" target="_blank">pdf</a>]

<h2>Biometric Quality: Review and Application to Face Recognition with FaceQnet. (arXiv:2006.03298v3 [cs.CV] UPDATED)</h2>
<h3>Javier Hernandez-Ortega, Javier Galbally, Julian Fierrez, Laurent Beslay</h3>
<p>"The output of a computerised system can only be as accurate as the
information entered into it." This rather trivial statement is the basis behind
one of the driving concepts in biometric recognition: biometric quality.
Quality is nowadays widely regarded as the number one factor responsible for
the good or bad performance of automated biometric systems. It refers to the
ability of a biometric sample to be used for recognition purposes and produce
consistent, accurate, and reliable results. Such a subjective term is
objectively estimated by the so-called biometric quality metrics. These
algorithms play nowadays a pivotal role in the correct functioning of systems,
providing feedback to the users and working as invaluable audit tools. In spite
of their unanimously accepted relevance, some of the most used and deployed
biometric characteristics are lacking behind in the development of these
methods. This is the case of face recognition. After a gentle introduction to
the general topic of biometric quality and a review of past efforts in face
quality metrics, in the present work, we address the need for better face
quality metrics by developing FaceQnet. FaceQnet is a novel open-source face
quality assessment tool, inspired and powered by deep learning technology,
which assigns a scalar quality measure to facial images, as prediction of their
recognition accuracy. Two versions of FaceQnet have been thoroughly evaluated
both in this work and also independently by NIST, showing the soundness of the
approach and its competitiveness with respect to current state-of-the-art
metrics. Even though our work is presented here particularly in the framework
of face biometrics, the proposed methodology for building a fully automated
quality metric can be very useful and easily adapted to other artificial
intelligence tasks.
</p>
<a href="http://arxiv.org/abs/2006.03298" target="_blank">arXiv:2006.03298</a> [<a href="http://arxiv.org/pdf/2006.03298" target="_blank">pdf</a>]

<h2>Fully Unsupervised Diversity Denoising with Convolutional Variational Autoencoders. (arXiv:2006.06072v2 [cs.CV] UPDATED)</h2>
<h3>Mangal Prakash, Alexander Krull, Florian Jug</h3>
<p>Deep Learning based methods have emerged as the indisputable leaders for
virtually all image restoration tasks. Especially in the domain of microscopy
images, various content-aware image restoration (CARE) approaches are now used
to improve the interpretability of acquired data. Naturally, there are
limitations to what can be restored in corrupted images, and like for all
inverse problems, many potential solutions exist, and one of them must be
chosen. Here, we propose DivNoising, a denoising approach based on fully
convolutional variational autoencoders (VAEs), overcoming the problem of having
to choose a single solution by predicting a whole distribution of denoised
images. First we introduce a principled way of formulating the unsupervised
denoising problem within the VAE framework by explicitly incorporating imaging
noise models into the decoder. Our approach is fully unsupervised, only
requiring noisy images and a suitable description of the imaging noise
distribution. We show that such a noise model can either be measured,
bootstrapped from noisy data, or co-learned during training. If desired,
consensus predictions can be inferred from a set of DivNoising predictions,
leading to competitive results with other unsupervised methods and, on
occasion, even with the supervised state-of-the-art. DivNoising samples from
the posterior enable a plethora of useful applications. We are (i) showing
denoising results for 13 datasets, (ii) discussing how optical character
recognition (OCR) applications can benefit from diverse predictions, and are
(iii) demonstrating how instance cell segmentation improves when using diverse
DivNoising predictions.
</p>
<a href="http://arxiv.org/abs/2006.06072" target="_blank">arXiv:2006.06072</a> [<a href="http://arxiv.org/pdf/2006.06072" target="_blank">pdf</a>]

<h2>Predicting Livelihood Indicators from Community-Generated Street-Level Imagery. (arXiv:2006.08661v6 [cs.CV] UPDATED)</h2>
<h3>Jihyeon Lee, Dylan Grosz, Burak Uzkent, Sicheng Zeng, Marshall Burke, David Lobell, Stefano Ermon</h3>
<p>Major decisions from governments and other large organizations rely on
measurements of the populace's well-being, but making such measurements at a
broad scale is expensive and thus infrequent in much of the developing world.
We propose an inexpensive, scalable, and interpretable approach to predict key
livelihood indicators from public crowd-sourced street-level imagery. Such
imagery can be cheaply collected and more frequently updated compared to
traditional surveying methods, while containing plausibly relevant information
for a range of livelihood indicators. We propose two approaches to learn from
the street-level imagery: (1) a method that creates multi-household cluster
representations by detecting informative objects and (2) a graph-based approach
that captures the relationships between images. By visualizing what features
are important to a model and how they are used, we can help end-user
organizations understand the models and offer an alternate approach for index
estimation that uses cheaply obtained roadway features. By comparing our
results against ground data collected in nationally-representative household
surveys, we demonstrate the performance of our approach in accurately
predicting indicators of poverty, population, and health and its scalability by
testing in two different countries, India and Kenya. Our code is available at
https://github.com/sustainlab-group/mapillarygcn.
</p>
<a href="http://arxiv.org/abs/2006.08661" target="_blank">arXiv:2006.08661</a> [<a href="http://arxiv.org/pdf/2006.08661" target="_blank">pdf</a>]

<h2>Labelling unlabelled videos from scratch with multi-modal self-supervision. (arXiv:2006.13662v3 [cs.CV] UPDATED)</h2>
<h3>Yuki M. Asano, Mandela Patrick, Christian Rupprecht, Andrea Vedaldi</h3>
<p>A large part of the current success of deep learning lies in the
effectiveness of data -- more precisely: labelled data. Yet, labelling a
dataset with human annotation continues to carry high costs, especially for
videos. While in the image domain, recent methods have allowed to generate
meaningful (pseudo-) labels for unlabelled datasets without supervision, this
development is missing for the video domain where learning feature
representations is the current focus. In this work, we a) show that
unsupervised labelling of a video dataset does not come for free from strong
feature encoders and b) propose a novel clustering method that allows
pseudo-labelling of a video dataset without any human annotations, by
leveraging the natural correspondence between the audio and visual modalities.
An extensive analysis shows that the resulting clusters have high semantic
overlap to ground truth human labels. We further introduce the first
benchmarking results on unsupervised labelling of common video datasets
Kinetics, Kinetics-Sound, VGG-Sound and AVE.
</p>
<a href="http://arxiv.org/abs/2006.13662" target="_blank">arXiv:2006.13662</a> [<a href="http://arxiv.org/pdf/2006.13662" target="_blank">pdf</a>]

<h2>Lane Detection Model Based on Spatio-Temporal Network With Double Convolutional Gated Recurrent Units. (arXiv:2008.03922v2 [cs.CV] UPDATED)</h2>
<h3>Jiyong Zhang, Tao Deng, Fei Yan, Wenbo Liu</h3>
<p>Lane detection is one of the indispensable and key elements of self-driving
environmental perception. Many lane detection models have been proposed,
solving lane detection under challenging conditions, including intersection
merging and splitting, curves, boundaries, occlusions and combinations of scene
types. Nevertheless, lane detection will remain an open problem for some time
to come. The ability to cope well with those challenging scenes impacts greatly
the applications of lane detection on advanced driver assistance systems
(ADASs). In this paper, a spatio-temporal network with double Convolutional
Gated Recurrent Units (ConvGRUs) is proposed to address lane detection in
challenging scenes. Both of ConvGRUs have the same structures, but different
locations and functions in our network. One is used to extract the information
of the most likely low-level features of lane markings. The extracted features
are input into the next layer of the end-to-end network after concatenating
them with the outputs of some blocks. The other one takes some continuous
frames as its input to process the spatio-temporal driving information.
Extensive experiments on the large-scale TuSimple lane marking challenge
dataset and Unsupervised LLAMAS dataset demonstrate that the proposed model can
effectively detect lanes in the challenging driving scenes. Our model can
outperform the state-of-the-art lane detection models.
</p>
<a href="http://arxiv.org/abs/2008.03922" target="_blank">arXiv:2008.03922</a> [<a href="http://arxiv.org/pdf/2008.03922" target="_blank">pdf</a>]

<h2>Critical analysis on the reproducibility of visual quality assessment using deep features. (arXiv:2009.05369v3 [cs.CV] UPDATED)</h2>
<h3>Franz G&#xf6;tz-Hahn, Vlad Hosu, Dietmar Saupe</h3>
<p>Data used to train supervised machine learning models are commonly split into
independent training, validation, and test sets. This paper illustrates that
complex data leakage cases have occurred in the no-reference image and video
quality assessment literature. Recently, papers in several journals reported
performance results well above the best in the field. However, our analysis
shows that information from the test set was inappropriately used in the
training process in different ways and that the claimed performance results
cannot be achieved. When correcting for the data leakage, the performances of
the approaches drop even below the state-of-the-art by a large margin.
Additionally, we investigate end-to-end variations to the discussed approaches,
which do not improve upon the original.
</p>
<a href="http://arxiv.org/abs/2009.05369" target="_blank">arXiv:2009.05369</a> [<a href="http://arxiv.org/pdf/2009.05369" target="_blank">pdf</a>]

<h2>Semi-Supervised Active Learning for COVID-19 Lung Ultrasound Multi-symptom Classification. (arXiv:2009.05436v2 [cs.CV] UPDATED)</h2>
<h3>Lei Liu, Wentao Lei, Yongfang Luo, Cheng Feng, Xiang Wan, Li Liu</h3>
<p>Ultrasound (US) is a non-invasive yet effective medical diagnostic imaging
technique for the COVID-19 global pandemic. However, due to complex feature
behaviors and expensive annotations of US images, it is difficult to apply
Artificial Intelligence (AI) assisting approaches for lung's multi-symptom
(multi-label) classification. To overcome these difficulties, we propose a
novel semi-supervised Two-Stream Active Learning (TSAL) method to model
complicated features and reduce labeling costs in an iterative procedure. The
core component of TSAL is the multi-label learning mechanism, in which label
correlations information is used to design multi-label margin (MLM) strategy
and confidence validation for automatically selecting informative samples and
confident labels. On this basis, a multi-symptom multi-label (MSML)
classification network is proposed to learn discriminative features of lung
symptoms, and a human-machine interaction is exploited to confirm the final
annotations that are used to fine-tune MSML with progressively labeled data.
Moreover, a novel lung US dataset named COVID19-LUSMS is built, currently
containing 71 clinical patients with 6,836 images sampled from 678 videos.
Experimental evaluations show that TSAL using only 20% data can achieve
superior performance to the baseline and the state-of-the-art. Qualitatively,
visualization of both attention map and sample distribution confirms the good
consistency with the clinic knowledge.
</p>
<a href="http://arxiv.org/abs/2009.05436" target="_blank">arXiv:2009.05436</a> [<a href="http://arxiv.org/pdf/2009.05436" target="_blank">pdf</a>]

<h2>BSN++: Complementary Boundary Regressor with Scale-Balanced Relation Modeling for Temporal Action Proposal Generation. (arXiv:2009.07641v5 [cs.CV] UPDATED)</h2>
<h3>Haisheng Su, Weihao Gan, Wei Wu, Yu Qiao, Junjie Yan</h3>
<p>Generating human action proposals in untrimmed videos is an important yet
challenging task with wide applications. Current methods often suffer from the
noisy boundary locations and the inferior quality of confidence scores used for
proposal retrieving. In this paper, we present BSN++, a new framework which
exploits complementary boundary regressor and relation modeling for temporal
proposal generation. First, we propose a novel boundary regressor based on the
complementary characteristics of both starting and ending boundary classifiers.
Specifically, we utilize the U-shaped architecture with nested skip connections
to capture rich contexts and introduce bi-directional boundary matching
mechanism to improve boundary precision. Second, to account for the
proposal-proposal relations ignored in previous methods, we devise a proposal
relation block to which includes two self-attention modules from the aspects of
position and channel. Furthermore, we find that there inevitably exists data
imbalanced problems in the positive/negative proposals and temporal durations,
which harm the model performance on tail distributions. To relieve this issue,
we introduce the scale-balanced re-sampling strategy. Extensive experiments are
conducted on two popular benchmarks: ActivityNet-1.3 and THUMOS14, which
demonstrate that BSN++ achieves the state-of-the-art performance. Not
surprisingly, the proposed BSN++ ranked 1st place in the CVPR19 - ActivityNet
challenge leaderboard on temporal action localization task.
</p>
<a href="http://arxiv.org/abs/2009.07641" target="_blank">arXiv:2009.07641</a> [<a href="http://arxiv.org/pdf/2009.07641" target="_blank">pdf</a>]

<h2>Regularizing Attention Networks for Anomaly Detection in Visual Question Answering. (arXiv:2009.10054v2 [cs.CV] UPDATED)</h2>
<h3>Doyup Lee, Yeongjae Cheon, Wook-Shin Han</h3>
<p>For stability and reliability of real-world applications, the robustness of
DNNs in unimodal tasks has been evaluated. However, few studies consider
abnormal situations that a visual question answering (VQA) model might
encounter at test time after deployment in the real-world. In this study, we
evaluate the robustness of state-of-the-art VQA models to five different
anomalies, including worst-case scenarios, the most frequent scenarios, and the
current limitation of VQA models. Different from the results in unimodal tasks,
the maximum confidence of answers in VQA models cannot detect anomalous inputs,
and post-training of the outputs, such as outlier exposure, is ineffective for
VQA models. Thus, we propose an attention-based method, which uses confidence
of reasoning between input images and questions and shows much more promising
results than the previous methods in unimodal tasks. In addition, we show that
a maximum entropy regularization of attention networks can significantly
improve the attention-based anomaly detection of the VQA models. Thanks to the
simplicity, attention-based anomaly detection and the regularization are
model-agnostic methods, which can be used for various cross-modal attentions in
the state-of-the-art VQA models. The results imply that cross-modal attention
in VQA is important to improve not only VQA accuracy, but also the robustness
to various anomalies.
</p>
<a href="http://arxiv.org/abs/2009.10054" target="_blank">arXiv:2009.10054</a> [<a href="http://arxiv.org/pdf/2009.10054" target="_blank">pdf</a>]

<h2>TDR-OBCA: A Reliable Planner for Autonomous Driving in Free-Space Environment. (arXiv:2009.11345v2 [cs.RO] UPDATED)</h2>
<h3>Runxin He, Jinyun Zhou, Shu Jiang, Yu Wang, Jiaming Tao, Shiyu Song, Jiangtao Hu, Jinghao Miao, Qi Luo</h3>
<p>This paper presents an optimization-based collision avoidance trajectory
generation method for autonomous driving in free-space environments, with
enhanced robustness, driving comfort and efficiency. Starting from the hybrid
optimization-based framework, we introduces two warm start methods, temporal
and dual variable warm starts, to improve the efficiency. We also reformulate
the problem to improve the robustness and efficiency. We name this new
algorithm TDR-OBCA. With these changes, compared with original hybrid
optimization we achieve a 96.67% failure rate decrease with respect to initial
conditions, 13.53% increase in driving comforts and 3.33% to 44.82% increase in
planner efficiency as obstacles number scales. We validate our results in
hundreds of simulation scenarios and hundreds of hours of public road tests in
both U.S. and China. Our source code is available at
https://github.com/ApolloAuto/apollo.
</p>
<a href="http://arxiv.org/abs/2009.11345" target="_blank">arXiv:2009.11345</a> [<a href="http://arxiv.org/pdf/2009.11345" target="_blank">pdf</a>]

<h2>Dictionary Learning with Low-rank Coding Coefficients for Tensor Completion. (arXiv:2009.12507v2 [cs.CV] UPDATED)</h2>
<h3>Tai-Xiang Jiang, Xi-Le Zhao, Hao Zhang, Michael K. Ng</h3>
<p>In this paper, we propose a novel tensor learning and coding model for
third-order data completion. Our model is to learn a data-adaptive dictionary
from the given observations, and determine the coding coefficients of
third-order tensor tubes. In the completion process, we minimize the
low-rankness of each tensor slice containing the coding coefficients. By
comparison with the traditional pre-defined transform basis, the advantages of
the proposed model are that (i) the dictionary can be learned based on the
given data observations so that the basis can be more adaptively and accurately
constructed, and (ii) the low-rankness of the coding coefficients can allow the
linear combination of dictionary features more effectively. Also we develop a
multi-block proximal alternating minimization algorithm for solving such tensor
learning and coding model, and show that the sequence generated by the
algorithm can globally converge to a critical point. Extensive experimental
results for real data sets such as videos, hyperspectral images, and traffic
data are reported to demonstrate these advantages and show the performance of
the proposed tensor learning and coding method is significantly better than the
other tensor completion methods in terms of several evaluation metrics.
</p>
<a href="http://arxiv.org/abs/2009.12507" target="_blank">arXiv:2009.12507</a> [<a href="http://arxiv.org/pdf/2009.12507" target="_blank">pdf</a>]

<h2>MetaPhys: Few-Shot Adaptation for Non-Contact Physiological Measurement. (arXiv:2010.01773v2 [cs.CV] UPDATED)</h2>
<h3>Xin Liu, Ziheng Jiang, Josh Fromm, Xuhai Xu, Shwetak Patel, Daniel McDuff</h3>
<p>There are large individual differences in physiological processes, making
designing personalized health sensing algorithms challenging. Existing machine
learning systems struggle to generalize well to unseen subjects or contexts and
can often contain problematic biases. Video-based physiological measurement is
not an exception. Therefore, learning personalized or customized models from a
small number of unlabeled samples is very attractive as it would allow fast
calibrations to improve generalization and help correct biases. In this paper,
we present a novel meta-learning approach called MetaPhys for personalized
video-based cardiac measurement for contactless pulse and heart rate
monitoring. Our method uses only 18-seconds of video for customization and
works effectively in both supervised and unsupervised manners. We evaluate our
proposed approach on two benchmark datasets and demonstrate superior
performance in cross-dataset evaluation with substantial reductions (42% to
44%) in errors compared with state-of-the-art approaches. We have also
demonstrated our proposed method significantly helps reduce the bias in skin
type.
</p>
<a href="http://arxiv.org/abs/2010.01773" target="_blank">arXiv:2010.01773</a> [<a href="http://arxiv.org/pdf/2010.01773" target="_blank">pdf</a>]

<h2>Automotive Radar Data Acquisition using Object Detection. (arXiv:2010.02367v2 [cs.CV] UPDATED)</h2>
<h3>Madhumitha Sakthi, Ahmed Tewfik</h3>
<p>The growing urban complexity demands an efficient algorithm to acquire and
process various sensor information from autonomous vehicles. In this paper, we
introduce an algorithm to utilize object detection results from the image to
adaptively sample and acquire radar data using Compressed Sensing (CS). This
novel algorithm is motivated by the hypothesis that with a limited sampling
budget, allocating more sampling budget to areas with the object as opposed to
a uniform sampling ultimately improves relevant object detection performance.
We improve detection performance by dynamically allocating a lower sampling
rate to objects such as buses than pedestrians leading to better reconstruction
than baseline across areas with objects of interest. We automate the sampling
rate allocation using linear programming and show significant time savings
while reducing the radar block size by a factor of 2. We also analyze a Binary
Permuted Diagonal measurement matrix for radar acquisition which is
hardware-efficient and show its performance is similar to Gaussian and Binary
Permuted Block Diagonal matrix. Our experiments on the Oxford radar dataset
show an effective reconstruction of objects of interest with 10% sampling rate.
Finally, we develop a transformer-based 2D object detection network using the
NuScenes radar and image data.
</p>
<a href="http://arxiv.org/abs/2010.02367" target="_blank">arXiv:2010.02367</a> [<a href="http://arxiv.org/pdf/2010.02367" target="_blank">pdf</a>]

<h2>Real-Time Deep Learning Approach to Visual Servo Control and Grasp Detection for Autonomous Robotic Manipulation. (arXiv:2010.06544v2 [cs.RO] UPDATED)</h2>
<h3>Eduardo Godinho Ribeiro, Raul de Queiroz Mendes, Valdir Grassi Jr</h3>
<p>In order to explore robotic grasping in unstructured and dynamic
environments, this work addresses the visual perception phase involved in the
task. This phase involves the processing of visual data to obtain the location
of the object to be grasped, its pose and the points at which the robot`s
grippers must make contact to ensure a stable grasp. For this, the Cornell
Grasping dataset is used to train a convolutional neural network that, having
an image of the robot`s workspace, with a certain object, is able to predict a
grasp rectangle that symbolizes the position, orientation and opening of the
robot`s grippers before its closing. In addition to this network, which runs in
real-time, another one is designed to deal with situations in which the object
moves in the environment. Therefore, the second network is trained to perform a
visual servo control, ensuring that the object remains in the robot`s field of
view. This network predicts the proportional values of the linear and angular
velocities that the camera must have so that the object is always in the image
processed by the grasp network. The dataset used for training was automatically
generated by a Kinova Gen3 manipulator. The robot is also used to evaluate the
applicability in real-time and obtain practical results from the designed
algorithms. Moreover, the offline results obtained through validation sets are
also analyzed and discussed regarding their efficiency and processing speed.
The developed controller was able to achieve a millimeter accuracy in the final
position considering a target object seen for the first time. To the best of
our knowledge, we have not found in the literature other works that achieve
such precision with a controller learned from scratch. Thus, this work presents
a new system for autonomous robotic manipulation with high processing speed and
the ability to generalize to several different objects.
</p>
<a href="http://arxiv.org/abs/2010.06544" target="_blank">arXiv:2010.06544</a> [<a href="http://arxiv.org/pdf/2010.06544" target="_blank">pdf</a>]

<h2>An Empowerment-based Solution to Robotic Manipulation Tasks with Sparse Rewards. (arXiv:2010.07986v2 [cs.RO] UPDATED)</h2>
<h3>Siyu Dai, Wei Xu, Andreas Hofmann, Brian Williams</h3>
<p>In order to provide adaptive and user-friendly solutions to robotic
manipulation, it is important that the agent can learn to accomplish tasks even
if they are only provided with very sparse instruction signals. To address the
issues reinforcement learning algorithms face when task rewards are sparse,
this paper proposes an intrinsic motivation approach that can be easily
integrated into any standard reinforcement learning algorithm and can allow
robotic manipulators to learn useful manipulation skills with only sparse
extrinsic rewards. Through integrating and balancing empowerment and curiosity,
this approach shows superior performance compared to other state-of-the-art
intrinsic exploration approaches during extensive empirical testing.
Qualitative analysis also shows that when combined with diversity-driven
intrinsic motivations, this approach can help manipulators learn a set of
diverse skills which could potentially be applied to other more complicated
manipulation tasks and accelerate their learning process.
</p>
<a href="http://arxiv.org/abs/2010.07986" target="_blank">arXiv:2010.07986</a> [<a href="http://arxiv.org/pdf/2010.07986" target="_blank">pdf</a>]

<h2>Planning with Learned Dynamics: Probabilistic Guarantees on Safety and Reachability via Lipschitz Constants. (arXiv:2010.08993v3 [cs.RO] UPDATED)</h2>
<h3>Craig Knuth, Glen Chou, Necmiye Ozay, Dmitry Berenson</h3>
<p>We present a method for feedback motion planning of systems with unknown
dynamics which provides probabilistic guarantees on safety, reachability, and
goal stability. To find a domain in which a learned control-affine
approximation of the true dynamics can be trusted, we estimate the Lipschitz
constant of the difference between the true and learned dynamics, and ensure
the estimate is valid with a given probability. Provided the system has at
least as many controls as states, we also derive existence conditions for a
one-step feedback law which can keep the real system within a small bound of a
nominal trajectory planned with the learned dynamics. Our method imposes the
feedback law existence as a constraint in a sampling-based planner, which
returns a feedback policy around a nominal plan ensuring that, if the Lipschitz
constant estimate is valid, the true system is safe during plan execution,
reaches the goal, and is ultimately invariant in a small set about the goal. We
demonstrate our approach by planning using learned models of a 6D quadrotor and
a 7DOF Kuka arm. We show that a baseline which plans using the same learned
dynamics without considering the error bound or the existence of the feedback
law can fail to stabilize around the plan and become unsafe.
</p>
<a href="http://arxiv.org/abs/2010.08993" target="_blank">arXiv:2010.08993</a> [<a href="http://arxiv.org/pdf/2010.08993" target="_blank">pdf</a>]

<h2>A 2D Surgical Simulation Framework for Tool-Tissue Interaction. (arXiv:2010.13936v2 [cs.RO] UPDATED)</h2>
<h3>Yunhai Han, Fei Liu, Michael C. Yip</h3>
<p>The control and task automation of robotic surgical system is very
challenging, especially in soft tissue manipulation, due to the unpredictable
deformations. Thus, an accurate simulator of soft tissues with the ability of
interacting with robot manipulators is necessary. In this work, we propose a
novel 2D simulation framework for tool-tissue interaction. This framework
continuously tracks the motion of manipulator and simulates the tissue
deformation in presence of collision detection. The deformation energy can be
computed for the control and planning task.
</p>
<a href="http://arxiv.org/abs/2010.13936" target="_blank">arXiv:2010.13936</a> [<a href="http://arxiv.org/pdf/2010.13936" target="_blank">pdf</a>]

<h2>Learning Constrained Distributions of Robot Configurations with Generative Adversarial Network. (arXiv:2011.05717v2 [cs.RO] UPDATED)</h2>
<h3>Teguh Santoso Lembono, Emmanuel Pignat, Julius Jankowski, Sylvain Calinon</h3>
<p>In high dimensional robotic system, the manifold of the valid configuration
space often has a complex shape, especially under constraints such as
end-effector orientation or static stability. We propose a generative
adversarial network approach to learn the distribution of valid robot
configurations under such constraints. It can generate configurations that are
close to the constraint manifold. We present two applications of this method.
First, by learning the conditional distribution with respect to the desired
end-effector position, we can do fast inverse kinematics even for very high
degrees of freedom (DoF) systems. Then, we use it to generate samples in
sampling-based constrained motion planning algorithms to reduce the necessary
projection steps, speeding up the computation. We validate the approach in
simulation using the 7-DoF Panda manipulator and the 28-DoF humanoid robot
Talos.
</p>
<a href="http://arxiv.org/abs/2011.05717" target="_blank">arXiv:2011.05717</a> [<a href="http://arxiv.org/pdf/2011.05717" target="_blank">pdf</a>]

<h2>Neural Scene Graphs for Dynamic Scenes. (arXiv:2011.10379v2 [cs.CV] UPDATED)</h2>
<h3>Julian Ost, Fahim Mannan, Nils Thuerey, Julian Knodt, Felix Heide</h3>
<p>Recent implicit neural rendering methods have demonstrated that it is
possible to learn accurate view synthesis for complex scenes by predicting
their volumetric density and color supervised solely by a set of RGB images.
However, existing methods are restricted to learning efficient representations
of static scenes that encode all scene objects into a single neural network,
and lack the ability to represent dynamic scenes and decompositions into
individual scene objects. In this work, we present the first neural rendering
method that decomposes dynamic scenes into scene graphs. We propose a learned
scene graph representation, which encodes object transformation and radiance,
to efficiently render novel arrangements and views of the scene. To this end,
we learn implicitly encoded scenes, combined with a jointly learned latent
representation to describe objects with a single implicit function. We assess
the proposed method on synthetic and real automotive data, validating that our
approach learns dynamic scenes -- only by observing a video of this scene --
and allows for rendering novel photo-realistic views of novel scene
compositions with unseen sets of objects at unseen poses.
</p>
<a href="http://arxiv.org/abs/2011.10379" target="_blank">arXiv:2011.10379</a> [<a href="http://arxiv.org/pdf/2011.10379" target="_blank">pdf</a>]

<h2>Learning to Assist Drone Landings. (arXiv:2011.13146v3 [cs.RO] UPDATED)</h2>
<h3>Kal Backman, Dana Kuli&#x107;, Hoam Chung</h3>
<p>Unmanned aerial vehicles (UAVs) are often used for navigating dangerous
terrains, however they are difficult to pilot. Due to complex input-output
mapping schemes, limited perception, the complex system dynamics and the need
to maintain a safe operation distance, novice pilots experience difficulties in
performing safe landings in obstacle filled environments. In this work we
propose a shared autonomy approach that assists novice pilots to perform safe
landings on one of several elevated platforms at a proficiency equal to or
greater than experienced pilots. Our approach consists of two modules, a
perceptual module and a policy module. The perceptual module compresses high
dimensionality RGB-D images into a latent vector trained with a cross-modal
variational auto-encoder. The policy module provides assistive control inputs
trained with the reinforcement algorithm TD3. We conduct a user study (n=33)
where participants land a simulated drone with and without the use of the
assistant. Despite the goal platform not being known to the assistant,
participants of all skill levels were able to outperform experienced
participants while assisted in the task.
</p>
<a href="http://arxiv.org/abs/2011.13146" target="_blank">arXiv:2011.13146</a> [<a href="http://arxiv.org/pdf/2011.13146" target="_blank">pdf</a>]

<h2>Human-in-the-loop Auditory Cueing Strategy for Gait Modification. (arXiv:2011.13516v2 [cs.RO] UPDATED)</h2>
<h3>Tina LY Wu, Anna Murphy, Chao Chen, Dana Kulic</h3>
<p>External feedback in the form of visual, auditory and tactile cues has been
used to assist patients to overcome mobility challenges. However, these cues
can become less effective over time. There is limited research on adapting cues
to account for inter and intra-personal variations in cue responsiveness. We
propose a cue-provision framework that consists of a gait performance
monitoring algorithm and an adaptive cueing strategy to improve gait
performance. The proposed approach learns a model of the person's response to
cues using Gaussian Process regression. The model is then used within an
on-line optimization algorithm to generate cues to improve gait performance. We
conduct a study with healthy participants to evaluate the ability of the
adaptive cueing strategy to influence human gait, and compare its effectiveness
to two other cueing approaches: the standard fixed cue approach and a
proportional cue approach. The results show that adaptive cueing is more
effective in changing the person's gait state once the response model is
learned compared to the other methods.
</p>
<a href="http://arxiv.org/abs/2011.13516" target="_blank">arXiv:2011.13516</a> [<a href="http://arxiv.org/pdf/2011.13516" target="_blank">pdf</a>]

<h2>Robust Instance Segmentation through Reasoning about Multi-Object Occlusion. (arXiv:2012.02107v2 [cs.CV] UPDATED)</h2>
<h3>Xiaoding Yuan, Adam Kortylewski, Yihong Sun, Alan Yuille</h3>
<p>Analyzing complex scenes with Deep Neural Networks is a challenging task,
particularly when images contain multiple objects that partially occlude each
other. Existing approaches to image analysis mostly process objects
independently and do not take into account the relative occlusion of nearby
objects. In this paper, we propose a deep network for multi-object instance
segmentation that is robust to occlusion and can be trained from bounding box
supervision only. Our work builds on Compositional Networks, which learn a
generative model of neural feature activations to locate occluders and to
classify objects based on their non-occluded parts. We extend their generative
model to include multiple objects and introduce a framework for the efficient
inference in challenging occlusion scenarios. In particular, we obtain
feed-forward predictions of the object classes and their instance and occluder
segmentations. We introduce an Occlusion Reasoning Module (ORM) that locates
erroneous segmentations and estimates the occlusion ordering to correct them.
The improved segmentation masks are, in turn, integrated into the network in a
top-down manner to improve the image classification. Our experiments on the
KITTI INStance dataset (KINS) and a synthetic occlusion dataset demonstrate the
effectiveness and robustness of our model at multi-object instance segmentation
under occlusion.
</p>
<a href="http://arxiv.org/abs/2012.02107" target="_blank">arXiv:2012.02107</a> [<a href="http://arxiv.org/pdf/2012.02107" target="_blank">pdf</a>]

<h2>Efficient Gross Target Volume of Nasopharyngeal Carcinoma Segmentation via Uncertainty Rectified Pyramid Consistency. (arXiv:2012.07042v2 [cs.CV] UPDATED)</h2>
<h3>Xiangde Luo, Wenjun Liao, Jieneng Chen, Tao Song, Yinan Chen, Shichuan Zhang, Nianyong Chen, Guotai Wang, Shaoting Zhang</h3>
<p>Gross Target Volume (GTV) segmentation plays an irreplaceable role in
radiotherapy planning for Nasopharyngeal Carcinoma (NPC). Despite that
Convolutional Neural Networks (CNN) have achieved good performance for this
task, they rely on a large set of labeled images for training, which is
expensive and time-consuming to acquire. In this paper, we propose a novel
framework with Uncertainty Rectified Pyramid Consistency (URPC) regularization
for semi-supervised NPC GTV segmentation. Concretely, we extend a backbone
segmentation network to produce pyramid predictions at different scales, the
pyramid predictions network (PPNet) is supervised by the ground truth of
labeled images and a multi-scale consistency loss for unlabeled images,
motivated by the fact that prediction at different scales for the same input
should be similar and consistent. However, due to the different resolution of
these predictions, encouraging them to be consistent at each pixel directly has
low robustness and may lose details. To address this problem, we further design
a novel uncertainty rectifying module to enable the framework to gradually
learn from meaningful and reliable consensual regions at different scales.
Extensive experiments on our collected NPC dataset with 258 volumes show that
our method outperforms other state-of-the-art semi-supervised methods on 10%
and 20% labeled data. Moreover, when increasing the labeled data to 50%, our
method achieves a comparable result compared with a fully supervised baseline
(the mean DSC 82.74% vs 83.51%, p &gt; 0.05).
</p>
<a href="http://arxiv.org/abs/2012.07042" target="_blank">arXiv:2012.07042</a> [<a href="http://arxiv.org/pdf/2012.07042" target="_blank">pdf</a>]

<h2>Flow-based Generative Models for Learning Manifold to Manifold Mappings. (arXiv:2012.10013v2 [cs.CV] UPDATED)</h2>
<h3>Xingjian Zhen, Rudrasis Chakraborty, Liu Yang, Vikas Singh</h3>
<p>Many measurements or observations in computer vision and machine learning
manifest as non-Euclidean data. While recent proposals (like spherical CNN)
have extended a number of deep neural network architectures to manifold-valued
data, and this has often provided strong improvements in performance, the
literature on generative models for manifold data is quite sparse. Partly due
to this gap, there are also no modality transfer/translation models for
manifold-valued data whereas numerous such methods based on generative models
are available for natural images. This paper addresses this gap, motivated by a
need in brain imaging -- in doing so, we expand the operating range of certain
generative models (as well as generative models for modality transfer) from
natural images to images with manifold-valued measurements. Our main result is
the design of a two-stream version of GLOW (flow-based invertible generative
models) that can synthesize information of a field of one type of
manifold-valued measurements given another. On the theoretical side, we
introduce three kinds of invertible layers for manifold-valued data, which are
not only analogous to their functionality in flow-based generative models
(e.g., GLOW) but also preserve the key benefits (determinants of the Jacobian
are easy to calculate). For experiments, on a large dataset from the Human
Connectome Project (HCP), we show promising results where we can reliably and
accurately reconstruct brain images of a field of orientation distribution
functions (ODF) from diffusion tensor images (DTI), where the latter has a
$5\times$ faster acquisition time but at the expense of worse angular
resolution.
</p>
<a href="http://arxiv.org/abs/2012.10013" target="_blank">arXiv:2012.10013</a> [<a href="http://arxiv.org/pdf/2012.10013" target="_blank">pdf</a>]

<h2>Content Masked Loss: Human-Like Brush Stroke Planning in a Reinforcement Learning Painting Agent. (arXiv:2012.10043v2 [cs.CV] UPDATED)</h2>
<h3>Peter Schaldenbrand, Jean Oh</h3>
<p>The objective of most Reinforcement Learning painting agents is to minimize
the loss between a target image and the paint canvas. Human painter artistry
emphasizes important features of the target image rather than simply
reproducing it (DiPaola 2007). Using adversarial or L2 losses in the RL
painting models, although its final output is generally a work of finesse,
produces a stroke sequence that is vastly different from that which a human
would produce since the model does not have knowledge about the abstract
features in the target image. In order to increase the human-like planning of
the model without the use of expensive human data, we introduce a new loss
function for use with the model's reward function: Content Masked Loss. In the
context of robot painting, Content Masked Loss employs an object detection
model to extract features which are used to assign higher weight to regions of
the canvas that a human would find important for recognizing content. The
results, based on 332 human evaluators, show that the digital paintings
produced by our Content Masked model show detectable subject matter earlier in
the stroke sequence than existing methods without compromising on the quality
of the final painting. Our code is available at
https://github.com/pschaldenbrand/ContentMaskedLoss.
</p>
<a href="http://arxiv.org/abs/2012.10043" target="_blank">arXiv:2012.10043</a> [<a href="http://arxiv.org/pdf/2012.10043" target="_blank">pdf</a>]

<h2>Temporal Contrastive Graph for Self-supervised Video Representation Learning. (arXiv:2101.00820v5 [cs.CV] UPDATED)</h2>
<h3>Yang Liu, Keze Wang, Haoyuan Lan, Liang Lin</h3>
<p>Attempt to fully discover the temporal diversity and global-local
chronological characteristics for self-supervised video representation
learning, this work takes advantage of the temporal structure of videos and
further proposes a novel self-supervised method named Temporal Contrastive
Graph (TCG). In contrast to the existing methods that randomly shuffle the
video frames or video snippets within a video, the TCG roots in a hybrid graph
contrastive learning strategy to regard the inter-snippet and intra-snippet
temporal relationships as self-supervision signals for temporal representation
learning. To increase the temporal diversity of features, the TCG integrates
the prior knowledge about the frame and snippet orders into temporal
contrastive graphs, i.e., the intra-/inter- snippet temporal contrastive graph
modules. By randomly removing edges and masking node features of the
intra-snippet graphs or inter-snippet graphs, the TCG can generate different
correlated graph views. Then, specific contrastive losses are designed to
maximize the agreement between node embeddings in different views. To learn the
global context representation and recalibrate the channel-wise features
adaptively, we introduce an adaptive video snippet order prediction module,
which leverages the relational knowledge among video snippets to predict the
actual snippet orders. Experimental results demonstrate the superiority of our
TCG over the state-of-the-art methods on large-scale action recognition and
video retrieval benchmarks.
</p>
<a href="http://arxiv.org/abs/2101.00820" target="_blank">arXiv:2101.00820</a> [<a href="http://arxiv.org/pdf/2101.00820" target="_blank">pdf</a>]

<h2>Road Surface Translation Under Snow-covered and Semantic Segmentation for Snow Hazard Index. (arXiv:2101.05616v4 [cs.CV] UPDATED)</h2>
<h3>Takato Yasuno, Junichiro Fujii, Hiroaki Sugawara, Masazumi Amakata</h3>
<p>In 2020, there was a record heavy snowfall owing to climate change. In
reality, 2,000 vehicles were stuck on the highway for three days. Because of
the freezing of the road surface, 10 vehicles had a billiard accident. Road
managers are required to provide indicators to alert drivers regarding snow
cover at hazardous locations. This study proposes a deep learning application
with live image post-processing to automatically calculate a snow hazard ratio
indicator. First, the road surface hidden under snow is translated using a
generative adversarial network, pix2pix. Second, snow-covered and road surface
classes are detected by semantic segmentation using DeepLabv3+ with MobileNet
as a backbone. Based on these trained networks, we automatically compute the
road to snow rate hazard index, indicating the amount of snow covered on the
road surface. We demonstrate the applied results to 1,155 live snow images of
the cold region in Japan. We mention the usefulness and the practical
robustness of our study.
</p>
<a href="http://arxiv.org/abs/2101.05616" target="_blank">arXiv:2101.05616</a> [<a href="http://arxiv.org/pdf/2101.05616" target="_blank">pdf</a>]

<h2>Online Robust Sliding-Windowed LiDAR SLAM in Natural Environments. (arXiv:2101.06615v2 [cs.RO] UPDATED)</h2>
<h3>Quang-Ha Pham, Ngoc-Huy Tran, Tran-Minh-Duc Ho</h3>
<p>Despite the growing interest for autonomous environmental monitoring,
effective SLAM realization in native habitats remains largely unsolved. In this
paper, we fill this gap by presenting a novel online graph-based SLAM system
for 2D LiDAR sensor in natural environments. By taking advantage of robust
weighting scheme, sliding-windowed optimization, fast scan-matcher and parallel
computing, our system not only delivers stable performance in cluttered
surroudings but also meets real-time constraint. Simulated and experimental
results confirm the feasibility and efficiency in the overall design of the
proposed system.
</p>
<a href="http://arxiv.org/abs/2101.06615" target="_blank">arXiv:2101.06615</a> [<a href="http://arxiv.org/pdf/2101.06615" target="_blank">pdf</a>]

<h2>Transferable Interactiveness Knowledge for Human-Object Interaction Detection. (arXiv:2101.10292v2 [cs.CV] UPDATED)</h2>
<h3>Yong-Lu Li, Xinpeng Liu, Xiaoqian Wu, Xijie Huang, Liang Xu, Cewu Lu</h3>
<p>Human-Object Interaction (HOI) detection is an important problem to
understand how humans interact with objects. In this paper, we explore
interactiveness knowledge which indicates whether a human and an object
interact with each other or not. We found that interactiveness knowledge can be
learned across HOI datasets and bridge the gap between diverse HOI category
settings. Our core idea is to exploit an interactiveness network to learn the
general interactiveness knowledge from multiple HOI datasets and perform
Non-Interaction Suppression (NIS) before HOI classification in inference. On
account of the generalization ability of interactiveness, interactiveness
network is a transferable knowledge learner and can be cooperated with any HOI
detection models to achieve desirable results. We utilize the human instance
and body part features together to learn the interactiveness in hierarchical
paradigm, i.e., instance-level and body part-level interactivenesses.
Thereafter, a consistency task is proposed to guide the learning and extract
deeper interactive visual clues. We extensively evaluate the proposed method on
HICO-DET, V-COCO, and a newly constructed PaStaNet-HOI dataset. With the
learned interactiveness, our method outperforms state-of-the-art HOI detection
methods, verifying its efficacy and flexibility. Code is available at
https://github.com/DirtyHarryLYL/Transferable-Interactiveness-Network.
</p>
<a href="http://arxiv.org/abs/2101.10292" target="_blank">arXiv:2101.10292</a> [<a href="http://arxiv.org/pdf/2101.10292" target="_blank">pdf</a>]

<h2>Online Extrinsic Calibration based on Per-Sensor Ego-Motion Using Dual Quaternions. (arXiv:2101.11440v2 [cs.RO] UPDATED)</h2>
<h3>Markus Horn, Thomas Wodtko, Michael Buchholz, Klaus Dietmayer</h3>
<p>In this work, we propose an approach for extrinsic sensor calibration from
per-sensor ego-motion estimates. Our problem formulation is based on dual
quaternions, enabling two different online capable solving approaches. We
provide a certifiable globally optimal and a fast local approach along with a
method to verify the globality of the local approach. Additionally, means for
integrating previous knowledge, for example, a common ground plane for planar
sensor motion, are described. Our algorithms are evaluated on simulated data
and on a publicly available dataset containing RGB-D camera images. Further,
our online calibration approach is tested on the KITTI odometry dataset, which
provides data of a lidar and two stereo camera systems mounted on a vehicle.
Our evaluation confirms the short run time, state-of-the-art accuracy, as well
as online capability of our approach while retaining the global optimality of
the solution at any time.
</p>
<a href="http://arxiv.org/abs/2101.11440" target="_blank">arXiv:2101.11440</a> [<a href="http://arxiv.org/pdf/2101.11440" target="_blank">pdf</a>]

<h2>Towards Hierarchical Task Decomposition using Deep Reinforcement Learning for Pick and Place Subtasks. (arXiv:2102.04022v2 [cs.RO] UPDATED)</h2>
<h3>Luca Marzari, Ameya Pore, Diego Dall&#x27;Alba, Gerardo Aragon-Camarasa, Alessandro Farinelli, Paolo Fiorini</h3>
<p>Deep Reinforcement Learning (DRL) is emerging as a promising approach to
generate adaptive behaviors for robotic platforms. However, a major drawback of
using DRL is the data-hungry training regime that requires millions of trial
and error attempts, which is impractical when running experiments on robotic
systems. To address this issue, we propose a multi-subtask reinforcement
learning method where complex tasks are decomposed manually into low-level
subtasks by leveraging human domain knowledge. These subtasks can be
parametrized as expert networks and learned via existing DRL methods. Trained
subtasks can then be composed with a high-level choreographer. As a testbed, we
use a pick and place robotic simulator to demonstrate our methodology, and show
that our method outperforms an imitation learning-based method and reaches a
high success rate compared to an end-to-end learning approach. Moreover, we
transfer the learned behavior in a different robotic environment that allows us
to exploit sim-to-real transfer and demonstrate the trajectories in a real
robotic system. Our training regime is carried out using a central processing
unit (CPU)-based system, which demonstrates the data-efficient properties of
our approach.
</p>
<a href="http://arxiv.org/abs/2102.04022" target="_blank">arXiv:2102.04022</a> [<a href="http://arxiv.org/pdf/2102.04022" target="_blank">pdf</a>]

<h2>Learned Camera Gain and Exposure Control for Improved Visual Feature Detection and Matching. (arXiv:2102.04341v2 [cs.RO] UPDATED)</h2>
<h3>Justin Tomasi, Brandon Wagstaff, Steven L. Waslander, Jonathan Kelly</h3>
<p>Successful visual navigation depends upon capturing images that contain
sufficient useful information. In this paper, we explore a data-driven approach
to account for environmental lighting changes, improving the quality of images
for use in visual odometry (VO) or visual simultaneous localization and mapping
(SLAM). We train a deep convolutional neural network model to predictively
adjust camera gain and exposure time parameters such that consecutive images
contain a maximal number of matchable features. The training process is fully
self-supervised: our training signal is derived from an underlying VO or SLAM
pipeline and, as a result, the model is optimized to perform well with that
specific pipeline. We demonstrate through extensive real-world experiments that
our network can anticipate and compensate for dramatic lighting changes (e.g.,
transitions into and out of road tunnels), maintaining a substantially higher
number of inlier feature matches than competing camera parameter control
algorithms.
</p>
<a href="http://arxiv.org/abs/2102.04341" target="_blank">arXiv:2102.04341</a> [<a href="http://arxiv.org/pdf/2102.04341" target="_blank">pdf</a>]

<h2>iX-BSP: Incremental Belief Space Planning. (arXiv:2102.09539v2 [cs.RO] UPDATED)</h2>
<h3>Elad I. Farhi, Vadim Indelman</h3>
<p>Deciding what's next? is a fundamental problem in robotics and Artificial
Intelligence. Under belief space planning (BSP), in a partially observable
setting, it involves calculating the expected accumulated belief-dependent
reward, where the expectation is with respect to all future measurements. Since
solving this general un-approximated problem quickly becomes intractable, state
of the art approaches turn to approximations while still calculating planning
sessions from scratch. In this work we propose a novel paradigm, Incremental
BSP (iX-BSP), based on the key insight that calculations across planning
sessions are similar in nature and can be appropriately re-used. We calculate
the expectation incrementally by utilizing Multiple Importance Sampling
techniques for selective re-sampling and re-use of measurement from previous
planning sessions. The formulation of our approach considers general
distributions and accounts for data association aspects. We demonstrate how
iX-BSP could benefit existing approximations of the general problem,
introducing iML-BSP, which re-uses calculations across planning sessions under
the common Maximum Likelihood assumption. We evaluate both methods and
demonstrate a substantial reduction in computation time while statistically
preserving accuracy. The evaluation includes both simulation and real-world
experiments considering autonomous vision-based navigation and SLAM. As a
further contribution, we introduce to iX-BSP the non-integral wildfire
approximation, allowing one to trade accuracy for computational performance by
averting from updating re-used beliefs when they are "close enough". We
evaluate iX-BSP under wildfire demonstrating a substantial reduction in
computation time while controlling the accuracy sacrifice. We also provide
analytical and empirical bounds of the effect wildfire holds over the objective
value.
</p>
<a href="http://arxiv.org/abs/2102.09539" target="_blank">arXiv:2102.09539</a> [<a href="http://arxiv.org/pdf/2102.09539" target="_blank">pdf</a>]

<h2>Training custom modality-specific U-Net models with weak localizations for improved Tuberculosis segmentation and localization. (arXiv:2102.10607v2 [cs.CV] UPDATED)</h2>
<h3>Sivaramakrishnan Rajaraman, Les Folio, Jane Dimperio, Philip Alderson, Sameer Antani</h3>
<p>Deep learning (DL) has drawn tremendous attention in object localization and
recognition for both natural and medical images. U-Net segmentation models have
demonstrated superior performance compared to conventional hand-crafted
feature-based methods. Medical image modality-specific DL models are better at
transferring domain knowledge to a relevant target task than those that are
pretrained on stock photography images. This helps improve model adaptation,
generalization, and class-specific region of interest (ROI) localization. In
this study, we train chest X-ray (CXR) modality-specific U-Nets and other
state-of-the-art U-Net models for semantic segmentation of tuberculosis
(TB)-consistent findings. Automated segmentation of such manifestations could
help radiologists reduce errors and supplement decision-making while improving
patient care and productivity. Our approach uses the publicly available TBX11K
CXR dataset with weak TB annotations, typically provided as bounding boxes, to
train a set of U-Net models. Next, we improve the results by augmenting the
training data with weak localizations, post-processed into an ROI mask, from a
DL classifier that is trained to classify CXRs as showing normal lungs or
suspected TB manifestations. Test data are individually derived from the TBX11K
CXR training distribution and other cross-institutional collections including
the Shenzhen TB and Montgomery TB CXR datasets. We observe that our augmented
training strategy helped the CXR modality-specific U-Net models achieve
superior performance with test data derived from the TBX11K CXR training
distribution as well as from cross-institutional collections (p &lt; 0.05).
</p>
<a href="http://arxiv.org/abs/2102.10607" target="_blank">arXiv:2102.10607</a> [<a href="http://arxiv.org/pdf/2102.10607" target="_blank">pdf</a>]

<h2>Deepfake Video Detection Using Convolutional Vision Transformer. (arXiv:2102.11126v2 [cs.CV] UPDATED)</h2>
<h3>Deressa Wodajo, Solomon Atnafu</h3>
<p>The rapid advancement of deep learning models that can generate and synthesis
hyper-realistic videos known as Deepfakes and their ease of access to the
general public have raised concern from all concerned bodies to their possible
malicious intent use. Deep learning techniques can now generate faces, swap
faces between two subjects in a video, alter facial expressions, change gender,
and alter facial features, to list a few. These powerful video manipulation
methods have potential use in many fields. However, they also pose a looming
threat to everyone if used for harmful purposes such as identity theft,
phishing, and scam. In this work, we propose a Convolutional Vision Transformer
for the detection of Deepfakes. The Convolutional Vision Transformer has two
components: Convolutional Neural Network (CNN) and Vision Transformer (ViT).
The CNN extracts learnable features while the ViT takes in the learned features
as input and categorizes them using an attention mechanism. We trained our
model on the DeepFake Detection Challenge Dataset (DFDC) and have achieved 91.5
percent accuracy, an AUC value of 0.91, and a loss value of 0.32. Our
contribution is that we have added a CNN module to the ViT architecture and
have achieved a competitive result on the DFDC dataset.
</p>
<a href="http://arxiv.org/abs/2102.11126" target="_blank">arXiv:2102.11126</a> [<a href="http://arxiv.org/pdf/2102.11126" target="_blank">pdf</a>]

<h2>Generator Surgery for Compressed Sensing. (arXiv:2102.11163v2 [cs.CV] UPDATED)</h2>
<h3>Niklas Smedemark-Margulies, Jung Yeon Park, Max Daniels, Rose Yu, Jan-Willem van de Meent, Paul Hand</h3>
<p>Image recovery from compressive measurements requires a signal prior for the
images being reconstructed. Recent work has explored the use of deep generative
models with low latent dimension as signal priors for such problems. However,
their recovery performance is limited by high representation error. We
introduce a method for achieving low representation error using generators as
signal priors. Using a pre-trained generator, we remove one or more initial
blocks at test time and optimize over the new, higher-dimensional latent space
to recover a target image. Experiments demonstrate significantly improved
reconstruction quality for a variety of network architectures. This approach
also works well for out-of-training-distribution images and is competitive with
other state-of-the-art methods. Our experiments show that test-time
architectural modifications can greatly improve the recovery quality of
generator signal priors for compressed sensing.
</p>
<a href="http://arxiv.org/abs/2102.11163" target="_blank">arXiv:2102.11163</a> [<a href="http://arxiv.org/pdf/2102.11163" target="_blank">pdf</a>]

<h2>Neural Architecture Search on ImageNet in Four GPU Hours: A Theoretically Inspired Perspective. (arXiv:2102.11535v2 [cs.CV] UPDATED)</h2>
<h3>Wuyang Chen, Xinyu Gong, Zhangyang Wang</h3>
<p>Neural Architecture Search (NAS) has been explosively studied to automate the
discovery of top-performer neural networks. Current works require heavy
training of supernet or intensive architecture evaluations, thus suffering from
heavy resource consumption and often incurring search bias due to truncated
training or approximations. Can we select the best neural architectures without
involving any training and eliminate a drastic portion of the search cost? We
provide an affirmative answer, by proposing a novel framework called
training-free neural architecture search (TE-NAS). TE-NAS ranks architectures
by analyzing the spectrum of the neural tangent kernel (NTK) and the number of
linear regions in the input space. Both are motivated by recent theory advances
in deep networks and can be computed without any training and any label. We
show that: (1) these two measurements imply the trainability and expressivity
of a neural network; (2) they strongly correlate with the network's test
accuracy. Further on, we design a pruning-based NAS mechanism to achieve a more
flexible and superior trade-off between the trainability and expressivity
during the search. In NAS-Bench-201 and DARTS search spaces, TE-NAS completes
high-quality search but only costs 0.5 and 4 GPU hours with one 1080Ti on
CIFAR-10 and ImageNet, respectively. We hope our work inspires more attempts in
bridging the theoretical findings of deep networks and practical impacts in
real NAS applications. Code is available at:
https://github.com/VITA-Group/TENAS.
</p>
<a href="http://arxiv.org/abs/2102.11535" target="_blank">arXiv:2102.11535</a> [<a href="http://arxiv.org/pdf/2102.11535" target="_blank">pdf</a>]

<h2>Resilient Path Planning of UAVs against Covert Attacks on UWB Sensors. (arXiv:2102.11696v2 [cs.RO] UPDATED)</h2>
<h3>Jiayi He, Xin Gong, Yukang Cui, Tingwen Huang</h3>
<p>In this letter, a resilient path planning scheme is proposed to navigate a
UAV to the planned (nominal) destination with minimum energy-consumption in the
presence of a smart attacker. The UAV is equipped with two sensors, a GPS
sensor, which is vulnerable to the spoofing attacker, and a well-functioning
Ultra-Wideband (UWB) sensor, which is possible to be fooled. We show that a
covert attacker can significantly deviate the UAV's path by simultaneously
corrupting the GPS signals and forging control inputs without being detected by
the UWB sensor. The prerequisite for the attack occurrence is first discussed.
Based on this prerequisite, the optimal attack scheme is proposed, which
maximizes the deviation between the nominal destination and the real one.
Correspondingly, an energy-efficient and resilient navigation scheme based on
Pontryagin's maximum principle \cite{gelfand2000calculus} is formulated, which
depresses the above covert attacker effectively. To sum up, this problem can be
seen as a Stackelberg game \cite{bacsar1998dynamic} between a secure path
planner (defender) and a covert attacker. The effectiveness and practicality of
our theoretical results are illustrated via two series of simulation examples
and a UAV experiment.
</p>
<a href="http://arxiv.org/abs/2102.11696" target="_blank">arXiv:2102.11696</a> [<a href="http://arxiv.org/pdf/2102.11696" target="_blank">pdf</a>]

<h2>Zero-Shot Text-to-Image Generation. (arXiv:2102.12092v2 [cs.CV] UPDATED)</h2>
<h3>Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever</h3>
<p>Text-to-image generation has traditionally focused on finding better modeling
assumptions for training on a fixed dataset. These assumptions might involve
complex architectures, auxiliary losses, or side information such as object
part labels or segmentation masks supplied during training. We describe a
simple approach for this task based on a transformer that autoregressively
models the text and image tokens as a single stream of data. With sufficient
data and scale, our approach is competitive with previous domain-specific
models when evaluated in a zero-shot fashion.
</p>
<a href="http://arxiv.org/abs/2102.12092" target="_blank">arXiv:2102.12092</a> [<a href="http://arxiv.org/pdf/2102.12092" target="_blank">pdf</a>]

<h2>Safe Learning-based Gradient-free Model Predictive Control Based on Cross-entropy Method. (arXiv:2102.12124v2 [cs.RO] UPDATED)</h2>
<h3>Lei Zheng, Rui Yang, Zhixuan Wu, Jiesen Pan, Hui Cheng</h3>
<p>In this paper, a safe and learning-based control framework for model
predictive control (MPC) is proposed to optimize nonlinear systems with a
gradient-free objective function under uncertain environmental disturbances.
The control framework integrates a learning-based MPC with an auxiliary
controller in a way of minimal intervention. The learning-based MPC augments
the prior nominal model with incremental Gaussian Processes to learn the
uncertain disturbances. The cross-entropy method (CEM) is utilized as the
sampling-based optimizer for the MPC with a gradient-free objective function. A
minimal intervention controller is devised with a control Lyapunov function and
a control barrier function to guide the sampling process and endow the system
with high probabilistic safety. The proposed algorithm shows a safe and
adaptive control performance on a simulated quadrotor in the tasks of
trajectory tracking and obstacle avoidance under uncertain wind disturbances.
</p>
<a href="http://arxiv.org/abs/2102.12124" target="_blank">arXiv:2102.12124</a> [<a href="http://arxiv.org/pdf/2102.12124" target="_blank">pdf</a>]

<h2>CausalX: Causal Explanations and Block Multilinear Factor Analysis. (arXiv:2102.12853v2 [cs.CV] UPDATED)</h2>
<h3>M. Alex O. Vasilescu, Eric Kim, Xiao S. Zeng</h3>
<p>By adhering to the dictum, "No causation without manipulation (treatment,
intervention)", cause and effect data analysis represents changes in observed
data in terms of changes in the causal factors. When causal factors are not
amenable for active manipulation in the real world due to current technological
limitations or ethical considerations, a counterfactual approach performs an
intervention on the model of data formation. In the case of object
representation or activity (temporal object) representation, varying object
parts is generally unfeasible whether they be spatial and/or temporal.
Multilinear algebra, the algebra of higher-order tensors, is a suitable and
transparent framework for disentangling the causal factors of data formation.
Learning a part-based intrinsic causal factor representations in a multilinear
framework requires applying a set of interventions on a part-based multilinear
model. We propose a unified multilinear model of wholes and parts. We derive a
hierarchical block multilinear factorization, the M-mode Block SVD, that
computes a disentangled representation of the causal factors by optimizing
simultaneously across the entire object hierarchy. Given computational
efficiency considerations, we introduce an incremental bottom-up computational
alternative, the Incremental M-mode Block SVD, that employs the lower-level
abstractions, the part representations, to represent the higher level of
abstractions, the parent wholes. This incremental computational approach may
also be employed to update the causal model parameters when data becomes
available incrementally. The resulting object representation is an
interpretable combinatorial choice of intrinsic causal factor representations
related to an object's recursive hierarchy of wholes and parts that renders
object recognition robust to occlusion and reduces training data requirements.
</p>
<a href="http://arxiv.org/abs/2102.12853" target="_blank">arXiv:2102.12853</a> [<a href="http://arxiv.org/pdf/2102.12853" target="_blank">pdf</a>]

<h2>Structured Prediction for CRiSP Inverse Kinematics Learning with Misspecified Robot Models. (arXiv:2102.12942v2 [cs.RO] UPDATED)</h2>
<h3>Gian Maria Marconi, Raffaello Camoriano, Lorenzo Rosasco, Carlo Ciliberto</h3>
<p>With the recent advances in machine learning, problems that traditionally
would require accurate modeling to be solved analytically can now be
successfully approached with data-driven strategies. Among these, computing the
inverse kinematics of a redundant robot arm poses a significant challenge due
to the non-linear structure of the robot, the hard joint constraints and the
non-invertible kinematics map. Moreover, most learning algorithms consider a
completely data-driven approach, while often useful information on the
structure of the robot is available and should be positively exploited. In this
work, we present a simple, yet effective, approach for learning the inverse
kinematics. We introduce a structured prediction algorithm that combines a
data-driven strategy with the model provided by a forward kinematics function
-- even when this function is misspeficied -- to accurately solve the problem.
The proposed approach ensures that predicted joint configurations are well
within the robot's constraints. We also provide statistical guarantees on the
generalization properties of our estimator as well as an empirical evaluation
of its performance on trajectory reconstruction tasks.
</p>
<a href="http://arxiv.org/abs/2102.12942" target="_blank">arXiv:2102.12942</a> [<a href="http://arxiv.org/pdf/2102.12942" target="_blank">pdf</a>]

<h2>Hyperspectral Image Classification: Artifacts of Dimension Reduction on Hybrid CNN. (arXiv:2101.10532v1 [cs.CV] CROSS LISTED)</h2>
<h3>Muhammad Ahmad, Sidrah Shabbir, Rana Aamir Raza, Manuel Mazzara, Salvatore Distefano, Adil Mehmood Khan</h3>
<p>Convolutional Neural Networks (CNN) has been extensively studied for
Hyperspectral Image Classification (HSIC) more specifically, 2D and 3D CNN
models have proved highly efficient in exploiting the spatial and spectral
information of Hyperspectral Images. However, 2D CNN only considers the spatial
information and ignores the spectral information whereas 3D CNN jointly
exploits spatial-spectral information at a high computational cost. Therefore,
this work proposed a lightweight CNN (3D followed by 2D-CNN) model which
significantly reduces the computational cost by distributing spatial-spectral
feature extraction across a lighter model alongside a preprocessing that has
been carried out to improve the classification results. Five benchmark
Hyperspectral datasets (i.e., SalinasA, Salinas, Indian Pines, Pavia
University, Pavia Center, and Botswana) are used for experimental evaluation.
The experimental results show that the proposed pipeline outperformed in terms
of generalization performance, statistical significance, and computational
complexity, as compared to the state-of-the-art 2D/3D CNN models except
commonly used computationally expensive design choices.
</p>
<a href="http://arxiv.org/abs/2101.10532" target="_blank">arXiv:2101.10532</a> [<a href="http://arxiv.org/pdf/2101.10532" target="_blank">pdf</a>]

<h2>Emotion pattern detection on facial videos using functional statistics. (arXiv:2103.00844v1 [cs.CV])</h2>
<h3>Rongjiao Ji, Alessandra Micheletti, Natasa Krklec Jerinkic, Zoranka Desnica</h3>
<p>There is an increasing scientific interest in automatically analysing and
understanding human behavior, with particular reference to the evolution of
facial expressions and the recognition of the corresponding emotions. In this
paper we propose a technique based on Functional ANOVA to extract significant
patterns of face muscles movements, in order to identify the emotions expressed
by actors in recorded videos. We determine if there are time-related
differences on expressions among emotional groups by using a functional F-test.
Such results are the first step towards the construction of a reliable
automatic emotion recognition system
</p>
<a href="http://arxiv.org/abs/2103.00844" target="_blank">arXiv:2103.00844</a> [<a href="http://arxiv.org/pdf/2103.00844" target="_blank">pdf</a>]

