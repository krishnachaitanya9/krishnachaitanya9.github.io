---
title: Latest Deep Learning Papers
date: 2021-01-06 21:00:01 +0800
featured-img: DL-logo
categories: [Deep_Learning]
tags: [Deep_Learning]
mathjax: Yes
---

<h1>Your arXiv Feed (125 Articles)</h1>
<h2>Online Multivalid Learning: Means, Moments, and Prediction Intervals. (arXiv:2101.01739v1 [cs.LG])</h2>
<h3>Varun Gupta, Christopher Jung, Georgy Noarov, Mallesh M. Pai, Aaron Roth</h3>
<p>We present a general, efficient technique for providing contextual
predictions that are "multivalid" in various senses, against an online sequence
of adversarially chosen examples $(x,y)$. This means that the resulting
estimates correctly predict various statistics of the labels $y$ not just
marginally -- as averaged over the sequence of examples -- but also
conditionally on $x \in G$ for any $G$ belonging to an arbitrary intersecting
collection of groups $\mathcal{G}$.

We provide three instantiations of this framework. The first is mean
prediction, which corresponds to an online algorithm satisfying the notion of
multicalibration from Hebert-Johnson et al. The second is variance and higher
moment prediction, which corresponds to an online algorithm satisfying the
notion of mean-conditioned moment multicalibration from Jung et al. Finally, we
define a new notion of prediction interval multivalidity, and give an algorithm
for finding prediction intervals which satisfy it. Because our algorithms
handle adversarially chosen examples, they can equally well be used to predict
statistics of the residuals of arbitrary point prediction methods, giving rise
to very general techniques for quantifying the uncertainty of predictions of
black box algorithms, even in an online adversarial setting. When instantiated
for prediction intervals, this solves a similar problem as conformal
prediction, but in an adversarial environment and with multivalidity guarantees
stronger than simple marginal coverage guarantees.
</p>
<a href="http://arxiv.org/abs/2101.01739" target="_blank">arXiv:2101.01739</a> [<a href="http://arxiv.org/pdf/2101.01739" target="_blank">pdf</a>]

<h2>AutoDropout: Learning Dropout Patterns to Regularize Deep Networks. (arXiv:2101.01761v1 [cs.LG])</h2>
<h3>Hieu Pham, Quoc V. Le</h3>
<p>Neural networks are often over-parameterized and hence benefit from
aggressive regularization. Conventional regularization methods, such as Dropout
or weight decay, do not leverage the structures of the network's inputs and
hidden states. As a result, these conventional methods are less effective than
methods that leverage the structures, such as SpatialDropout and DropBlock,
which randomly drop the values at certain contiguous areas in the hidden states
and setting them to zero. Although the locations of dropout areas random, the
patterns of SpatialDropout and DropBlock are manually designed and fixed. Here
we propose to learn the dropout patterns. In our method, a controller learns to
generate a dropout pattern at every channel and layer of a target network, such
as a ConvNet or a Transformer. The target network is then trained with the
dropout pattern, and its resulting validation performance is used as a signal
for the controller to learn from. We show that this method works well for both
image recognition on CIFAR-10 and ImageNet, as well as language modeling on
Penn Treebank and WikiText-2. The learned dropout patterns also transfers to
different tasks and datasets, such as from language model on Penn Treebank to
Engligh-French translation on WMT 2014. Our code will be available.
</p>
<a href="http://arxiv.org/abs/2101.01761" target="_blank">arXiv:2101.01761</a> [<a href="http://arxiv.org/pdf/2101.01761" target="_blank">pdf</a>]

<h2>A unifying approach on bias and variance analysis for classification. (arXiv:2101.01765v1 [cs.LG])</h2>
<h3>Cemre Zor, Terry Windeatt</h3>
<p>Bias and variance (B&amp;V) decomposition is frequently used as a tool for
analysing classification performance. However, the standard B&amp;V terminologies
were originally defined for the regression setting and their extensions to
classification has led to several different models / definitions in the
literature. Although the relation between some of these models has previously
been explored, their links to the standard terminology in terms of the Bayesian
statistics has not been established. In this paper, we aim to provide this
missing link via employing the frameworks of Tumer &amp; Ghosh (T&amp;G) and James. By
unifying the two approaches, we relate the classification B&amp;V defined for the
0/1 loss to the standard B&amp;V of the boundary distributions given for the
squared error loss. The closed form relationships derived in this study provide
deeper understanding of the classification performance, and their example uses
on predictor design and analysis are demonstrated in two case studies.
</p>
<a href="http://arxiv.org/abs/2101.01765" target="_blank">arXiv:2101.01765</a> [<a href="http://arxiv.org/pdf/2101.01765" target="_blank">pdf</a>]

<h2>An A* Curriculum Approach to Reinforcement Learning for RGBD Indoor Robot Navigation. (arXiv:2101.01774v1 [cs.RO])</h2>
<h3>Kaushik Balakrishnan, Punarjay Chakravarty, Shubham Shrivastava</h3>
<p>Training robots to navigate diverse environments is a challenging problem as
it involves the confluence of several different perception tasks such as
mapping and localization, followed by optimal path-planning and control.
Recently released photo-realistic simulators such as Habitat allow for the
training of networks that output control actions directly from perception:
agents use Deep Reinforcement Learning (DRL) to regress directly from the
camera image to a control output in an end-to-end fashion. This is
data-inefficient and can take several days to train on a GPU. Our paper tries
to overcome this problem by separating the training of the perception and
control neural nets and increasing the path complexity gradually using a
curriculum approach. Specifically, a pre-trained twin Variational AutoEncoder
(VAE) is used to compress RGBD (RGB &amp; depth) sensing from an environment into a
latent embedding, which is then used to train a DRL-based control policy. A*, a
traditional path-planner is used as a guide for the policy and the distance
between start and target locations is incrementally increased along the A*
route, as training progresses. We demonstrate the efficacy of the proposed
approach, both in terms of increased performance and decreased training times
for the PointNav task in the Habitat simulation environment. This strategy of
improving the training of direct-perception based DRL navigation policies is
expected to hasten the deployment of robots of particular interest to industry
such as co-bots on the factory floor and last-mile delivery robots.
</p>
<a href="http://arxiv.org/abs/2101.01774" target="_blank">arXiv:2101.01774</a> [<a href="http://arxiv.org/pdf/2101.01774" target="_blank">pdf</a>]

<h2>Minibatch optimal transport distances; analysis and applications. (arXiv:2101.01792v1 [stat.ML])</h2>
<h3>Kilian Fatras, Younes Zine, Szymon Majewski, R&#xe9;mi Flamary, R&#xe9;mi Gribonval, Nicolas Courty</h3>
<p>Optimal transport distances have become a classic tool to compare probability
distributions and have found many applications in machine learning. Yet,
despite recent algorithmic developments, their complexity prevents their direct
use on large scale datasets. To overcome this challenge, a common workaround is
to compute these distances on minibatches i.e. to average the outcome of
several smaller optimal transport problems. We propose in this paper an
extended analysis of this practice, which effects were previously studied in
restricted cases. We first consider a large variety of Optimal Transport
kernels. We notably argue that the minibatch strategy comes with appealing
properties such as unbiased estimators, gradients and a concentration bound
around the expectation, but also with limits: the minibatch OT is not a
distance. To recover some of the lost distance axioms, we introduce a debiased
minibatch OT function and study its statistical and optimisation properties.
Along with this theoretical analysis, we also conduct empirical experiments on
gradient flows, generative adversarial networks (GANs) or color transfer that
highlight the practical interest of this strategy.
</p>
<a href="http://arxiv.org/abs/2101.01792" target="_blank">arXiv:2101.01792</a> [<a href="http://arxiv.org/pdf/2101.01792" target="_blank">pdf</a>]

<h2>Active Bayesian Multi-class Mapping from Range and Semantic Segmentation Observation. (arXiv:2101.01831v1 [cs.RO])</h2>
<h3>Arash Asgharivaskasi, Nikolay Atanasov</h3>
<p>Many robot applications call for autonomous exploration and mapping of
unknown and unstructured environments. Information-based exploration
techniques, such as Cauchy-Schwarz quadratic mutual information (CSQMI) and
fast Shannon mutual information (FSMI), have successfully achieved active
binary occupancy mapping with range measurements. However, as we envision
robots performing complex tasks specified with semantically meaningful objects,
it is necessary to capture semantic categories in the measurements, map
representation, and exploration objective. This work develops a Bayesian
multi-class mapping algorithm utilizing range-category measurements. We derive
a closed-form efficiently computable lower bound for the Shannon mutual
information between the multi-class map and the measurements. The bound allows
rapid evaluation of many potential robot trajectories for autonomous
exploration and mapping. We compare our method against frontier-based and FSMI
exploration and apply it in a 3-D photo-realistic simulation environment.
</p>
<a href="http://arxiv.org/abs/2101.01831" target="_blank">arXiv:2101.01831</a> [<a href="http://arxiv.org/pdf/2101.01831" target="_blank">pdf</a>]

<h2>Large-Scale Extended Granger Causality for Classification of Marijuana Users From Functional MRI. (arXiv:2101.01832v1 [cs.LG])</h2>
<h3>M. Ali Vosoughi, Axel Wismuller</h3>
<p>It has been shown in the literature that marijuana use is associated with
changes in brain network connectivity. We propose large-scale Extended Granger
Causality (lsXGC) and investigate whether it can capture such changes using
resting-state fMRI. This method combines dimension reduction with source
time-series augmentation and uses predictive time-series modeling for
estimating directed causal relationships among fMRI time-series. It is a
multivariate approach, since it is capable of identifying the interdependence
of time-series in the presence of all other time-series of the underlying
dynamic system. Here, we investigate whether this model can serve as a
biomarker for classifying marijuana users from typical controls using 126 adult
subjects with a childhood diagnosis of ADHD from the Addiction Connectome
Preprocessed Initiative (ACPI) database. We use brain connections estimated by
lsXGC as features for classification. After feature extraction, we perform
feature selection by Kendall's-tau rank correlation coefficient followed by
classification using a support vector machine. As a reference method, we
compare our results with cross-correlation, which is typically used in the
literature as a standard measure of functional connectivity. Within a
cross-validation scheme of 100 different training/test (90%/10%) data splits,
we obtain a mean accuracy range of [0.714, 0.985] and a mean Area Under the
receiver operating characteristic Curve (AUC) range of [0.779, 0.999] across
all tested numbers of features for lsXGC, which is significantly better than
results obtained with cross-correlation, namely mean accuracy of [0.728, 0.912]
and mean AUC of [0.825, 0.969]. Our results suggest the applicability of lsXGC
as a potential biomarker for marijuana use.
</p>
<a href="http://arxiv.org/abs/2101.01832" target="_blank">arXiv:2101.01832</a> [<a href="http://arxiv.org/pdf/2101.01832" target="_blank">pdf</a>]

<h2>Risk markers by sex and age group for in-hospital mortality in patients with STEMI or NSTEMI: an approach based on machine learning. (arXiv:2101.01835v1 [cs.LG])</h2>
<h3>Blanca Vazquez, Gibran Fuentes, Fabian Garcia, Gabriela Borrayo, Juan Prohias</h3>
<p>Machine learning (ML) has demonstrated promising results in the
identification of clinical markers for Acute Coronary Syndrome (ACS) from
electronic health records (EHR). In the past, the ACS was perceived as a health
problem mainly for men and women were under-represented in clinical trials,
which led to both sexes receiving the same clinical attention. Although some
approaches have emphasized the importance of distinguishing markers, these
distinctions remain unclear. This study aims at exploiting ML methods for
identifying in-hospital mortality markers by sex and age-group for patients
with ST-elevation myocardial infarction (STEMI) and the Non-ST-elevation
myocardial infarction (NSTEMI) from EHR. From the MIMIC-III database, we
extracted 1,299 patients with STEMI and 2,820 patients with NSTEMI. We trained
and validated mortality prediction models with different hyperparameters,
clinical sets, and ML methods. Using the best performing model and a
game-theoretic approach to interpret predictions, we identified risk markers
for patients with STEMI and NSTEMI separately. The models based on Extreme
Gradient Boosting achieved the highest performance: AUC=0.92 (95\%
CI:0.87-0.98) for STEMI and AUC=0.87 (95\% CI:0.80-0.93) for NSTEMI. For STEMI,
the top markers for both sexes are the presence of hyponatremia, and metabolic
acidosis. More specific markers for women are acute kidney failure, and age&gt;75
years, while for men are chronic kidney failure, and age&gt;70 years. In contrast,
for NSTEMI, the top markers for both sexes are advanced age, and intubation
procedures. The specific markers for women are low creatinine levels and age&gt;60
years, whilst, for men are damage to the left atrium and age&gt;70 years. We
consider that distinguishing markers for sexes could lead to more appropriate
treatment strategies, thus improving clinical outcomes.
</p>
<a href="http://arxiv.org/abs/2101.01835" target="_blank">arXiv:2101.01835</a> [<a href="http://arxiv.org/pdf/2101.01835" target="_blank">pdf</a>]

<h2>CNN-based Visual Ego-Motion Estimation for Fast MAV Maneuvers. (arXiv:2101.01841v1 [cs.CV])</h2>
<h3>Yingfu Xu, Guido C. H. E. de Croon</h3>
<p>In the field of visual ego-motion estimation for Micro Air Vehicles (MAVs),
fast maneuvers stay challenging mainly because of the big visual disparity and
motion blur. In the pursuit of higher robustness, we study convolutional neural
networks (CNNs) that predict the relative pose between subsequent images from a
fast-moving monocular camera facing a planar scene. Aided by the Inertial
Measurement Unit (IMU), we mainly focus on the translational motion. The
networks we study have similar small model sizes (around 1.35MB) and high
inference speeds (around 100Hz on a mobile GPU). Images for training and
testing have realistic motion blur. Departing from a network framework that
iteratively warps the first image to match the second with cascaded network
blocks, we study different network architectures and training strategies.
Simulated datasets and MAV flight datasets are used for evaluation. The
proposed setup shows better accuracy over existing networks and traditional
feature-point-based methods during fast maneuvers. Moreover, self-supervised
learning outperforms supervised learning. The code developed for this paper
will be open-source upon publication at https://github.com/tudelft/.
</p>
<a href="http://arxiv.org/abs/2101.01841" target="_blank">arXiv:2101.01841</a> [<a href="http://arxiv.org/pdf/2101.01841" target="_blank">pdf</a>]

<h2>ISETAuto: Detecting vehicles with depth and radiance information. (arXiv:2101.01843v1 [cs.CV])</h2>
<h3>Zhenyi Liu, Joyce Farrell, Brian Wandell</h3>
<p>Autonomous driving applications use two types of sensor systems to identify
vehicles - depth sensing LiDAR and radiance sensing cameras. We compare the
performance (average precision) of a ResNet for vehicle detection in complex,
daytime, driving scenes when the input is a depth map (D = d(x,y)), a radiance
image (L = r(x,y)), or both [D,L]. (1) When the spatial sampling resolution of
the depth map and radiance image are equal to typical camera resolutions, a
ResNet detects vehicles at higher average precision from depth than radiance.
(2) As the spatial sampling of the depth map declines to the range of current
LiDAR devices, the ResNet average precision is higher for radiance than depth.
(3) For a hybrid system that combines a depth map and radiance image, the
average precision is higher than using depth or radiance alone. We established
these observations in simulation and then confirmed them using realworld data.
The advantage of combining depth and radiance can be explained by noting that
the two type of information have complementary weaknesses. The radiance data
are limited by dynamic range and motion blur. The LiDAR data have relatively
low spatial resolution. The ResNet combines the two data sources effectively to
improve overall vehicle detection.
</p>
<a href="http://arxiv.org/abs/2101.01843" target="_blank">arXiv:2101.01843</a> [<a href="http://arxiv.org/pdf/2101.01843" target="_blank">pdf</a>]

<h2>Mesh Reconstruction from Aerial Images for Outdoor Terrain Mapping Using Joint 2D-3D Learning. (arXiv:2101.01844v1 [cs.CV])</h2>
<h3>Qiaojun Feng, Nikolay Atanasov</h3>
<p>This paper addresses outdoor terrain mapping using overhead images obtained
from an unmanned aerial vehicle. Dense depth estimation from aerial images
during flight is challenging. While feature-based localization and mapping
techniques can deliver real-time odometry and sparse points reconstruction, a
dense environment model is generally recovered offline with significant
computation and storage. This paper develops a joint 2D-3D learning approach to
reconstruct local meshes at each camera keyframe, which can be assembled into a
global environment model. Each local mesh is initialized from sparse depth
measurements. We associate image features with the mesh vertices through camera
projection and apply graph convolution to refine the mesh vertices based on
joint 2-D reprojected depth and 3-D mesh supervision. Quantitative and
qualitative evaluations using real aerial images show the potential of our
method to support environmental monitoring and surveillance applications.
</p>
<a href="http://arxiv.org/abs/2101.01844" target="_blank">arXiv:2101.01844</a> [<a href="http://arxiv.org/pdf/2101.01844" target="_blank">pdf</a>]

<h2>Node2Seq: Towards Trainable Convolutions in Graph Neural Networks. (arXiv:2101.01849v1 [cs.LG])</h2>
<h3>Hao Yuan, Shuiwang Ji</h3>
<p>Investigating graph feature learning becomes essentially important with the
emergence of graph data in many real-world applications. Several graph neural
network approaches are proposed for node feature learning and they generally
follow a neighboring information aggregation scheme to learn node features.
While great performance has been achieved, the weights learning for different
neighboring nodes is still less explored. In this work, we propose a novel
graph network layer, known as Node2Seq, to learn node embeddings with
explicitly trainable weights for different neighboring nodes. For a target
node, our method sorts its neighboring nodes via attention mechanism and then
employs 1D convolutional neural networks (CNNs) to enable explicit weights for
information aggregation. In addition, we propose to incorporate non-local
information for feature learning in an adaptive manner based on the attention
scores. Experimental results demonstrate the effectiveness of our proposed
Node2Seq layer and show that the proposed adaptively non-local information
learning can improve the performance of feature learning.
</p>
<a href="http://arxiv.org/abs/2101.01849" target="_blank">arXiv:2101.01849</a> [<a href="http://arxiv.org/pdf/2101.01849" target="_blank">pdf</a>]

<h2>Reinforcement Learning with Latent Flow. (arXiv:2101.01857v1 [cs.LG])</h2>
<h3>Wenling Shang, Xiaofei Wang, Aravind Srinivas, Aravind Rajeswaran, Yang Gao, Pieter Abbeel, Michael Laskin</h3>
<p>Temporal information is essential to learning effective policies with
Reinforcement Learning (RL). However, current state-of-the-art RL algorithms
either assume that such information is given as part of the state space or,
when learning from pixels, use the simple heuristic of frame-stacking to
implicitly capture temporal information present in the image observations. This
heuristic is in contrast to the current paradigm in video classification
architectures, which utilize explicit encodings of temporal information through
methods such as optical flow and two-stream architectures to achieve
state-of-the-art performance. Inspired by leading video classification
architectures, we introduce the Flow of Latents for Reinforcement Learning
(Flare), a network architecture for RL that explicitly encodes temporal
information through latent vector differences. We show that Flare (i) recovers
optimal performance in state-based RL without explicit access to the state
velocity, solely with positional state information, (ii) achieves
state-of-the-art performance on pixel-based challenging continuous control
tasks within the DeepMind control benchmark suite, namely quadruped walk,
hopper hop, finger turn hard, pendulum swing, and walker run, and is the most
sample efficient model-free pixel-based RL algorithm, outperforming the prior
model-free state-of-the-art by 1.9X and 1.5X on the 500k and 1M step
benchmarks, respectively, and (iv), when augmented over rainbow DQN,
outperforms this state-of-the-art level baseline on 5 of 8 challenging Atari
games at 100M time step benchmark.
</p>
<a href="http://arxiv.org/abs/2101.01857" target="_blank">arXiv:2101.01857</a> [<a href="http://arxiv.org/pdf/2101.01857" target="_blank">pdf</a>]

<h2>One-shot Policy Elicitation via Semantic Reward Manipulation. (arXiv:2101.01860v1 [cs.RO])</h2>
<h3>Aaquib Tabrez, Ryan Leonard, Bradley Hayes</h3>
<p>Synchronizing expectations and knowledge about the state of the world is an
essential capability for effective collaboration. For robots to effectively
collaborate with humans and other autonomous agents, it is critical that they
be able to generate intelligible explanations to reconcile differences between
their understanding of the world and that of their collaborators. In this work
we present Single-shot Policy Explanation for Augmenting Rewards (SPEAR), a
novel sequential optimization algorithm that uses semantic explanations derived
from combinations of planning predicates to augment agents' reward functions,
driving their policies to exhibit more optimal behavior. We provide an
experimental validation of our algorithm's policy manipulation capabilities in
two practically grounded applications and conclude with a performance analysis
of SPEAR on domains of increasingly complex state space and predicate counts.
We demonstrate that our method makes substantial improvements over the
state-of-the-art in terms of runtime and addressable problem size, enabling an
agent to leverage its own expertise to communicate actionable information to
improve another's performance.
</p>
<a href="http://arxiv.org/abs/2101.01860" target="_blank">arXiv:2101.01860</a> [<a href="http://arxiv.org/pdf/2101.01860" target="_blank">pdf</a>]

<h2>TGCN: Time Domain Graph Convolutional Network for Multiple Objects Tracking. (arXiv:2101.01861v1 [cs.CV])</h2>
<h3>Jie Zhang</h3>
<p>Multiple object tracking is to give each object an id in the video. The
difficulty is how to match the predicted objects and detected objects in same
frames. Matching features include appearance features, location features, etc.
These features of the predicted object are basically based on some previous
frames. However, few papers describe the relationship in the time domain
between the previous frame features and the current frame features.In this
paper, we proposed a time domain graph convolutional network for multiple
objects tracking.The model is mainly divided into two parts, we first use
convolutional neural network (CNN) to extract pedestrian appearance feature,
which is a normal operation processing image in deep learning, then we use GCN
to model some past frames' appearance feature to get the prediction appearance
feature of the current frame. Due to this extension, we can get the pose
features of the current frame according to the relationship between some frames
in the past. Experimental evaluation shows that our extensions improve the MOTA
by 1.3 on the MOT16, achieving overall competitive performance at high frame
rates.
</p>
<a href="http://arxiv.org/abs/2101.01861" target="_blank">arXiv:2101.01861</a> [<a href="http://arxiv.org/pdf/2101.01861" target="_blank">pdf</a>]

<h2>dame-flame: A Python Library Providing Fast Interpretable Matching for Causal Inference. (arXiv:2101.01867v1 [cs.LG])</h2>
<h3>Neha R. Gupta (1), Vittorio Orlandi (1), Chia-Rui Chang (2), Tianyu Wang (1), Marco Morucci (1), Pritam Dey (1), Thomas J. Howell (1), Xian Sun (1), Angikar Ghosal (1), Sudeepa Roy (1), Cynthia Rudin (1), Alexander Volfovsky (1) ((1) Duke University, (2) Harvard University)</h3>
<p>dame-flame is a Python package for performing matching for observational
causal inference on datasets containing discrete covariates. This package
implements the Dynamic Almost Matching Exactly (DAME) and Fast Large-Scale
Almost Matching Exactly (FLAME) algorithms, which match treatment and control
units on subsets of the covariates. The resulting matched groups are
interpretable, because the matches are made on covariates (rather than, for
instance, propensity scores), and high-quality, because machine learning is
used to determine which covariates are important to match on. DAME solves an
optimization problem that matches units on as many covariates as possible,
prioritizing matches on important covariates. FLAME approximates the solution
found by DAME via a much faster backward feature selection procedure. The
package provides several adjustable parameters to adapt the algorithms to
specific applications, and can calculate treatment effects after matching.
Descriptions of these parameters, details on estimating treatment effects, and
further examples, can be found in the documentation at
https://almost-matching-exactly.github.io/DAME-FLAME-Python-Package/
</p>
<a href="http://arxiv.org/abs/2101.01867" target="_blank">arXiv:2101.01867</a> [<a href="http://arxiv.org/pdf/2101.01867" target="_blank">pdf</a>]

<h2>Multi-Stage Residual Hiding for Image-into-Audio Steganography. (arXiv:2101.01872v1 [cs.CV])</h2>
<h3>Wenxue Cui, Shaohui Liu, Feng Jiang, Yongliang Liu, Debin Zhao</h3>
<p>The widespread application of audio communication technologies has speeded up
audio data flowing across the Internet, which made it a popular carrier for
covert communication. In this paper, we present a cross-modal steganography
method for hiding image content into audio carriers while preserving the
perceptual fidelity of the cover audio. In our framework, two multi-stage
networks are designed: the first network encodes the decreasing multilevel
residual errors inside different audio subsequences with the corresponding
stage sub-networks, while the second network decodes the residual errors from
the modified carrier with the corresponding stage sub-networks to produce the
final revealed results. The multi-stage design of proposed framework not only
make the controlling of payload capacity more flexible, but also make hiding
easier because of the gradual sparse characteristic of residual errors.
Qualitative experiments suggest that modifications to the carrier are
unnoticeable by human listeners and that the decoded images are highly
intelligible.
</p>
<a href="http://arxiv.org/abs/2101.01872" target="_blank">arXiv:2101.01872</a> [<a href="http://arxiv.org/pdf/2101.01872" target="_blank">pdf</a>]

<h2>Smile and Laugh Expressions Detection Based on Local Minimum Key Points. (arXiv:2101.01874v1 [cs.CV])</h2>
<h3>Mina Mohammadi Dashti, Majid Harouni</h3>
<p>In this paper, a smile and laugh facial expression is presented based on
dimension reduction and description process of the key points. The paper has
two main objectives; the first is to extract the local critical points in terms
of their apparent features, and the second is to reduce the system's dependence
on training inputs. To achieve these objectives, three different scenarios on
extracting the features are proposed. First of all, the discrete parts of a
face are detected by local binary pattern method that is used to extract a set
of global feature vectors for texture classification considering various
regions of an input-image face. Then, in the first scenario and with respect to
the correlation changes of adjacent pixels on the texture of a mouth area, a
set of local key points are extracted using the Harris corner detector. In the
second scenario, the dimension reduction of the extracted points of first
scenario provided by principal component analysis algorithm leading to
reduction in computational costs and overall complexity without loss of
performance and flexibility, etc.
</p>
<a href="http://arxiv.org/abs/2101.01874" target="_blank">arXiv:2101.01874</a> [<a href="http://arxiv.org/pdf/2101.01874" target="_blank">pdf</a>]

<h2>The data synergy effects of time-series deep learning models in hydrology. (arXiv:2101.01876v1 [cs.LG])</h2>
<h3>Kuai Fang, Daniel Kifer, Kathryn Lawson, Dapeng Feng, Chaopeng Shen</h3>
<p>When fitting statistical models to variables in geoscientific disciplines
such as hydrology, it is a customary practice to regionalize - to divide a
large spatial domain into multiple regions and study each region separately -
instead of fitting a single model on the entire data (also known as
unification). Traditional wisdom in these fields suggests that models built for
each region separately will have higher performance because of homogeneity
within each region. However, by partitioning the training data, each model has
access to fewer data points and cannot learn from commonalities between
regions. Here, through two hydrologic examples (soil moisture and streamflow),
we argue that unification can often significantly outperform regionalization in
the era of big data and deep learning (DL). Common DL architectures, even
without bespoke customization, can automatically build models that benefit from
regional commonality while accurately learning region-specific differences. We
highlight an effect we call data synergy, where the results of the DL models
improved when data were pooled together from characteristically different
regions. In fact, the performance of the DL models benefited from more diverse
rather than more homogeneous training data. We hypothesize that DL models
automatically adjust their internal representations to identify commonalities
while also providing sufficient discriminatory information to the model. The
results here advocate for pooling together larger datasets, and suggest the
academic community should place greater emphasis on data sharing and
compilation.
</p>
<a href="http://arxiv.org/abs/2101.01876" target="_blank">arXiv:2101.01876</a> [<a href="http://arxiv.org/pdf/2101.01876" target="_blank">pdf</a>]

<h2>3D Convolutional Selective Autoencoder For Instability Detection in Combustion Systems. (arXiv:2101.01877v1 [cs.LG])</h2>
<h3>Tryambak Gangopadhyay, Vikram Ramanan, Adedotun Akintayo, Paige K Boor, Soumalya Sarkar, Satyanarayanan R Chakravarthy, Soumik Sarkar</h3>
<p>While analytical solutions of critical (phase) transitions in physical
systems are abundant for simple nonlinear systems, such analysis remains
intractable for real-life dynamical systems. A key example of such a physical
system is thermoacoustic instability in combustion, where prediction or early
detection of an onset of instability is a hard technical challenge, which needs
to be addressed to build safer and more energy-efficient gas turbine engines
powering aerospace and energy industries. The instabilities arising in
combustion chambers of engines are mathematically too complex to model. To
address this issue in a data-driven manner instead, we propose a novel deep
learning architecture called 3D convolutional selective autoencoder (3D-CSAE)
to detect the evolution of self-excited oscillations using spatiotemporal data,
i.e., hi-speed videos taken from a swirl-stabilized combustor (laboratory
surrogate of gas turbine engine combustor). 3D-CSAE consists of filters to
learn, in a hierarchical fashion, the complex visual and dynamic features
related to combustion instability. We train the 3D-CSAE on frames of videos
obtained from a limited set of operating conditions. We select the 3D-CSAE
hyper-parameters that are effective for characterizing hierarchical and
multiscale instability structure evolution by utilizing the dynamic information
available in the video. The proposed model clearly shows performance
improvement in detecting the precursors of instability. The machine
learning-driven results are verified with physics-based off-line measures.
Advanced active control mechanisms can directly leverage the proposed online
detection capability of 3D-CSAE to mitigate the adverse effects of combustion
instabilities on the engine operating under various stringent requirements and
conditions.
</p>
<a href="http://arxiv.org/abs/2101.01877" target="_blank">arXiv:2101.01877</a> [<a href="http://arxiv.org/pdf/2101.01877" target="_blank">pdf</a>]

<h2>On-Device Document Classification using multimodal features. (arXiv:2101.01880v1 [cs.CV])</h2>
<h3>Sugam Garg, Harichandana, Sumit Kumar</h3>
<p>From small screenshots to large videos, documents take up a bulk of space in
a modern smartphone. Documents in a phone can accumulate from various sources,
and with the high storage capacity of mobiles, hundreds of documents are
accumulated in a short period. However, searching or managing documents remains
an onerous task, since most search methods depend on meta-information or only
text in a document. In this paper, we showcase that a single modality is
insufficient for classification and present a novel pipeline to classify
documents on-device, thus preventing any private user data transfer to server.
For this task, we integrate an open-source library for Optical Character
Recognition (OCR) and our novel model architecture in the pipeline. We optimise
the model for size, a necessary metric for on-device inference. We benchmark
our classification model with a standard multimodal dataset FOOD-101 and
showcase competitive results with the previous State of the Art with 30% model
compression.
</p>
<a href="http://arxiv.org/abs/2101.01880" target="_blank">arXiv:2101.01880</a> [<a href="http://arxiv.org/pdf/2101.01880" target="_blank">pdf</a>]

<h2>Modality-specific Distillation. (arXiv:2101.01881v1 [cs.CV])</h2>
<h3>Woojeong Jin, Maziar Sanjabi, Shaoliang Nie, Liang Tan, Xiang Ren, Hamed Firooz</h3>
<p>Large neural networks are impractical to deploy on mobile devices due to
their heavy computational cost and slow inference. Knowledge distillation (KD)
is a technique to reduce the model size while retaining performance by
transferring knowledge from a large "teacher" model to a smaller "student"
model. However, KD on multimodal datasets such as vision-language datasets is
relatively unexplored and digesting such multimodal information is challenging
since different modalities present different types of information. In this
paper, we propose modality-specific distillation (MSD) to effectively transfer
knowledge from a teacher on multimodal datasets. Existing KD approaches can be
applied to multimodal setup, but a student doesn't have access to
modality-specific predictions. Our idea aims at mimicking a teacher's
modality-specific predictions by introducing an auxiliary loss term for each
modality. Because each modality has different importance for predictions, we
also propose weighting approaches for the auxiliary losses; a meta-learning
approach to learn the optimal weights on these loss terms. In our experiments,
we demonstrate the effectiveness of our MSD and the weighting scheme and show
that it achieves better performance than KD.
</p>
<a href="http://arxiv.org/abs/2101.01881" target="_blank">arXiv:2101.01881</a> [<a href="http://arxiv.org/pdf/2101.01881" target="_blank">pdf</a>]

<h2>Off-Policy Meta-Reinforcement Learning Based on Feature Embedding Spaces. (arXiv:2101.01883v1 [cs.AI])</h2>
<h3>Takahisa Imagawa, Takuya Hiraoka, Yoshimasa Tsuruoka</h3>
<p>Meta-reinforcement learning (RL) addresses the problem of sample inefficiency
in deep RL by using experience obtained in past tasks for a new task to be
solved.

However, most meta-RL methods require partially or fully on-policy data,
i.e., they cannot reuse the data collected by past policies, which hinders the
improvement of sample efficiency.

To alleviate this problem, we propose a novel off-policy meta-RL method,
embedding learning and evaluation of uncertainty (ELUE).

An ELUE agent is characterized by the learning of a feature embedding space
shared among tasks.

It learns a belief model over the embedding space and a belief-conditional
policy and Q-function.

Then, for a new task, it collects data by the pretrained policy, and updates
its belief based on the belief model.

Thanks to the belief update, the performance can be improved with a small
amount of data.

In addition, it updates the parameters of the neural networks to adjust the
pretrained relationships when there are enough data.

We demonstrate that ELUE outperforms state-of-the-art meta RL methods through
experiments on meta-RL benchmarks.
</p>
<a href="http://arxiv.org/abs/2101.01883" target="_blank">arXiv:2101.01883</a> [<a href="http://arxiv.org/pdf/2101.01883" target="_blank">pdf</a>]

<h2>Statistical learning for accurate and interpretable battery lifetime prediction. (arXiv:2101.01885v1 [cs.LG])</h2>
<h3>Peter M. Attia, Kristen A. Severson, Jeremy D. Witmer</h3>
<p>Data-driven methods for battery lifetime prediction are attracting increasing
attention for applications in which the degradation mechanisms are poorly
understood and suitable training sets are available. However, while advanced
machine learning and deep learning methods offer high performance with minimal
feature engineering, simpler "statistical learning" methods often achieve
comparable performance, especially for small training sets, while also
providing physical and statistical interpretability. In this work, we use a
previously published dataset to develop simple, accurate, and interpretable
data-driven models for battery lifetime prediction. We first present the
"capacity matrix" concept as a compact representation of battery
electrochemical cycling data, along with a series of feature representations.
We then create a number of univariate and multivariate models, many of which
achieve comparable performance to the highest-performing models previously
published for this dataset. These models also provide insights into the
degradation of these cells. Our approaches can be used both to quickly train
models for a new dataset and to benchmark the performance of more advanced
machine learning methods.
</p>
<a href="http://arxiv.org/abs/2101.01885" target="_blank">arXiv:2101.01885</a> [<a href="http://arxiv.org/pdf/2101.01885" target="_blank">pdf</a>]

<h2>Shape Control of Elastic Objects Based on Implicit Sensorimotor Models and Data-Driven Geometric Features. (arXiv:2101.01889v1 [cs.RO])</h2>
<h3>Wanyu Ma, Jihong Zhu, David Navarro-Alarcon</h3>
<p>This paper proposes a general approach to design automatic controls to
manipulate elastic objects into desired shapes. The object's geometric model is
defined as the shape feature based on the specific task to globally describe
the deformation. Raw visual feedback data is processed using classic regression
methods to identify parameters of data-driven geometric models in real-time.
Our proposed method is able to analytically compute a pose-shape Jacobian
matrix based on implicit functions. This model is then used to derive a shape
servoing controller. To validate the proposed method, we report a detailed
experimental study with robotic manipulators deforming an elastic rod.
</p>
<a href="http://arxiv.org/abs/2101.01889" target="_blank">arXiv:2101.01889</a> [<a href="http://arxiv.org/pdf/2101.01889" target="_blank">pdf</a>]

<h2>Line Segment Detection Using Transformers without Edges. (arXiv:2101.01909v1 [cs.CV])</h2>
<h3>Yifan Xu, Weijian Xu, David Cheung, Zhuowen Tu</h3>
<p>In this paper, we present a holistically end-to-end algorithm for line
segment detection with transformers that is post-processing and
heuristics-guided intermediate processing (edge/junction/region detection)
free. Our method, named LinE segment TRansformers (LETR), tackles the three
main problems in this domain, namely edge element detection, perceptual
grouping, and holistic inference by three highlights in detection transformers
(DETR) including tokenized queries with integrated encoding and decoding,
self-attention, and joint queries respectively. The transformers learn to
progressively refine line segments through layers of self-attention mechanism
skipping the heuristic design in the previous line segmentation algorithms. We
equip multi-scale encoder/decoder in the transformers to perform fine-grained
line segment detection under a direct end-point distance loss that is
particularly suitable for entities such as line segments that are not
conveniently represented by bounding boxes. In the experiments, we show
state-of-the-art results on Wireframe and YorkUrban benchmarks. LETR points to
a promising direction for joint end-to-end detection of general entities beyond
the standard object bounding box representation.
</p>
<a href="http://arxiv.org/abs/2101.01909" target="_blank">arXiv:2101.01909</a> [<a href="http://arxiv.org/pdf/2101.01909" target="_blank">pdf</a>]

<h2>Phase Transitions in Transfer Learning for High-Dimensional Perceptrons. (arXiv:2101.01918v1 [cs.LG])</h2>
<h3>Oussama Dhifallah, Yue M. Lu</h3>
<p>Transfer learning seeks to improve the generalization performance of a target
task by exploiting the knowledge learned from a related source task. Central
questions include deciding what information one should transfer and when
transfer can be beneficial. The latter question is related to the so-called
negative transfer phenomenon, where the transferred source information actually
reduces the generalization performance of the target task. This happens when
the two tasks are sufficiently dissimilar. In this paper, we present a
theoretical analysis of transfer learning by studying a pair of related
perceptron learning tasks. Despite the simplicity of our model, it reproduces
several key phenomena observed in practice. Specifically, our asymptotic
analysis reveals a phase transition from negative transfer to positive transfer
as the similarity of the two tasks moves past a well-defined threshold.
</p>
<a href="http://arxiv.org/abs/2101.01918" target="_blank">arXiv:2101.01918</a> [<a href="http://arxiv.org/pdf/2101.01918" target="_blank">pdf</a>]

<h2>A Lower Bound on DNNF Encodings of Pseudo-Boolean Constraints. (arXiv:2101.01953v1 [cs.AI])</h2>
<h3>Alexis de Colnet</h3>
<p>Two major considerations when encoding pseudo-Boolean (PB) constraints into
SAT are the size of the encoding and its propagation strength, that is, the
guarantee that it has a good behaviour under unit propagation. Several
encodings with propagation strength guarantees rely upon prior compilation of
the constraints into DNNF (decomposable negation normal form), BDD (binary
decision diagram), or some other sub-variants. However it has been shown that
there exist PB-constraints whose ordered BDD (OBDD) representations, and thus
the inferred CNF encodings, all have exponential size. Since DNNFs are more
succinct than OBDDs, preferring encodings via DNNF to avoid size explosion
seems a legitimate choice. Yet in this paper, we prove the existence of
PB-constraints whose DNNFs all require exponential size.
</p>
<a href="http://arxiv.org/abs/2101.01953" target="_blank">arXiv:2101.01953</a> [<a href="http://arxiv.org/pdf/2101.01953" target="_blank">pdf</a>]

<h2>Optimized Execution of PDDL Plans using Behavior Trees. (arXiv:2101.01964v1 [cs.RO])</h2>
<h3>Francisco Mart&#xed;n, Matteo Morelli, Huascar Espinoza, Francisco J. G. Lera, Vicente Matell&#xe1;n</h3>
<p>Robots need task planning to sequence and execute actions toward achieving
their goals. On the other hand, Behavior Trees provide a mathematical model for
specifying plan execution in an intrinsically composable, reactive, and robust
way. PDDL (Planning Domain Definition Language) has become the standard
description language for most planners. In this paper, we present a novel
algorithm to systematically create behavior trees from PDDL plans to execute
them. This approach uses the execution graph of the plan to generate a behavior
tree. The most remarkable contribution of this approach is the algorithm to
build a Behavior Tree that optimizes its execution by paralyzing actions,
applicable to any plan, taking into account the actions' causal relationships.
We demonstrate the improvement in the execution of plans in mobile robots using
the ROS2 Planning System framework.
</p>
<a href="http://arxiv.org/abs/2101.01964" target="_blank">arXiv:2101.01964</a> [<a href="http://arxiv.org/pdf/2101.01964" target="_blank">pdf</a>]

<h2>Weighted Ensemble-model and Network Analysis: A method to predict fluid intelligence via naturalistic functional connectivity. (arXiv:2101.01973v1 [cs.AI])</h2>
<h3>Xiaobo Liu, Su Yang</h3>
<p>Objectives: Functional connectivity triggered by naturalistic stimulus (e.g.,
movies) and machine learning techniques provide a great insight in exploring
the brain functions such as fluid intelligence. However, functional
connectivity are considered to be multi-layered, while traditional machine
learning based on individual models not only are limited in performance, but
also fail to extract multi-dimensional and multi-layered information from brain
network. Methods: In this study, inspired by multi-layer brain network
structure, we propose a new method namely Weighted Ensemble-model and Network
Analysis, which combines the machine learning and graph theory for improved
fluid intelligence prediction. Firstly, functional connectivity analysis and
graphical theory were jointly employed. The functional connectivity and
graphical indices computed using the preprocessed fMRI data were then all fed
into auto-encoder parallelly for feature extraction to predict the fluid
intelligence. In order to improve the performance, tree regression and ridge
regression model were automatically stacked and fused with weighted values.
Finally, layers of auto-encoder were visualized to better illustrate the
connectome patterns, followed by the evaluation of the performance to justify
the mechanism of brain functions. Results: Our proposed methods achieved best
performance with 3.85 mean absolute deviation, 0.66 correlation coefficient and
0.42 R-squared coefficient, outperformed other state-of-the-art methods. It is
also worth noting that, the optimization of the biological pattern extraction
was automated though the auto-encoder algorithm. Conclusion: The proposed
method not only outperforming the state-of-the-art reports, but also able to
effectively capturing the biological patterns from functional connectivity
during naturalistic movies state for potential clinical explorations.
</p>
<a href="http://arxiv.org/abs/2101.01973" target="_blank">arXiv:2101.01973</a> [<a href="http://arxiv.org/pdf/2101.01973" target="_blank">pdf</a>]

<h2>Predicting Forest Fire Using Remote Sensing Data And Machine Learning. (arXiv:2101.01975v1 [cs.CV])</h2>
<h3>Suwei Yang, Massimo Lupascu, Kuldeep S. Meel</h3>
<p>Over the last few decades, deforestation and climate change have caused
increasing number of forest fires. In Southeast Asia, Indonesia has been the
most affected country by tropical peatland forest fires. These fires have a
significant impact on the climate resulting in extensive health, social and
economic issues. Existing forest fire prediction systems, such as the Canadian
Forest Fire Danger Rating System, are based on handcrafted features and require
installation and maintenance of expensive instruments on the ground, which can
be a challenge for developing countries such as Indonesia. We propose a novel,
cost-effective, machine-learning based approach that uses remote sensing data
to predict forest fires in Indonesia. Our prediction model achieves more than
0.81 area under the receiver operator characteristic (ROC) curve, performing
significantly better than the baseline approach which never exceeds 0.70 area
under ROC curve on the same tasks. Our model's performance remained above 0.81
area under ROC curve even when evaluated with reduced data. The results support
our claim that machine-learning based approaches can lead to reliable and
cost-effective forest fire prediction systems.
</p>
<a href="http://arxiv.org/abs/2101.01975" target="_blank">arXiv:2101.01975</a> [<a href="http://arxiv.org/pdf/2101.01975" target="_blank">pdf</a>]

<h2>Dynamic Prioritization for Conflict-Free Path Planning of Multi-Robot Systems. (arXiv:2101.01978v1 [cs.RO])</h2>
<h3>Aditya Rathi, Rohith G, Madhu Vadali</h3>
<p>Planning collision-free paths for multi-robot systems (MRS) is a challenging
problem because of the safety and efficiency constraints required for
real-world solutions. Even though coupled path planning approaches provide
optimal collision-free paths for each agent of the MRS, they search the
composite space of all the agents and therefore, suffer from exponential
increase in computation with the number of robots. On the other hand,
prioritized approaches provide a practical solution to applications with large
number of robots, especially when path computation time and collision avoidance
take precedence over guaranteed globally optimal solution. While most
centrally-planned algorithms use static prioritization, a dynamic
prioritization algorithm, PD*, is proposed that employs a novel metric, called
freedom index, to decide the priority order of the robots at each time step.
This allows the PD* algorithm to simultaneously plan the next step for all
robots while ensuring collision-free operation in obstacle ridden environments.
Extensive simulations were performed to test and compare the performance of the
proposed PD* scheme with other state-of-the-art algorithms. It was found that
PD* improves upon the computational time by 25% while providing solutions of
similar path lengths. Increase in efficiency was particularly prominent in
scenarios with large number of robots and/or higher obstacle densities, where
the probability of collisions is higher, suggesting the suitability of PD* in
solving such problems.
</p>
<a href="http://arxiv.org/abs/2101.01978" target="_blank">arXiv:2101.01978</a> [<a href="http://arxiv.org/pdf/2101.01978" target="_blank">pdf</a>]

<h2>Multi-object Tracking with a Hierarchical Single-branch Network. (arXiv:2101.01984v1 [cs.CV])</h2>
<h3>Fan Wang, Lei Luo, En Zhu, Siwei Wang, Jun Long</h3>
<p>Recent Multiple Object Tracking (MOT) methods have gradually attempted to
integrate object detection and instance re-identification (Re-ID) into a united
network to form a one-stage solution. Typically, these methods use two
separated branches within a single network to accomplish detection and Re-ID
respectively without studying the inter-relationship between them, which
inevitably impedes the tracking performance. In this paper, we propose an
online multi-object tracking framework based on a hierarchical single-branch
network to solve this problem. Specifically, the proposed single-branch network
utilizes an improved Hierarchical Online In-stance Matching (iHOIM) loss to
explicitly model the inter-relationship between object detection and Re-ID. Our
novel iHOIM loss function unifies the objectives of the two sub-tasks and
encourages better detection performance and feature learning even in extremely
crowded scenes. Moreover, we propose to introduce the object positions,
predicted by a motion model, as region proposals for subsequent object
detection, where the intuition is that detection results and motion predictions
can complement each other in different scenarios. Experimental results on MOT16
and MOT20 datasets show that we can achieve state-of-the-art tracking
performance, and the ablation study verifies the effectiveness of each proposed
component.
</p>
<a href="http://arxiv.org/abs/2101.01984" target="_blank">arXiv:2101.01984</a> [<a href="http://arxiv.org/pdf/2101.01984" target="_blank">pdf</a>]

<h2>Cross-Validation and Uncertainty Determination for Randomized Neural Networks with Applications to Mobile Sensors. (arXiv:2101.01990v1 [stat.ML])</h2>
<h3>Ansgar Steland, Bart E. Pieters</h3>
<p>Randomized artificial neural networks such as extreme learning machines
provide an attractive and efficient method for supervised learning under
limited computing ressources and green machine learning. This especially
applies when equipping mobile devices (sensors) with weak artificial
intelligence. Results are discussed about supervised learning with such
networks and regression methods in terms of consistency and bounds for the
generalization and prediction error. Especially, some recent results are
reviewed addressing learning with data sampled by moving sensors leading to
non-stationary and dependent samples.

As randomized networks lead to random out-of-sample performance measures, we
study a cross-validation approach to handle the randomness and make use of it
to improve out-of-sample performance. Additionally, a computationally efficient
approach to determine the resulting uncertainty in terms of a confidence
interval for the mean out-of-sample prediction error is discussed based on
two-stage estimation. The approach is applied to a prediction problem arising
in vehicle integrated photovoltaics.
</p>
<a href="http://arxiv.org/abs/2101.01990" target="_blank">arXiv:2101.01990</a> [<a href="http://arxiv.org/pdf/2101.01990" target="_blank">pdf</a>]

<h2>A Survey of Deep RL and IL for Autonomous Driving Policy Learning. (arXiv:2101.01993v1 [cs.RO])</h2>
<h3>Zeyu Zhu, Huijing Zhao</h3>
<p>Autonomous driving (AD) agents generate driving policies based on online
perception results, which are obtained at multiple levels of abstraction, e.g.,
behavior planning, motion planning and control. Driving policies are crucial to
the realization of safe, efficient and harmonious driving behaviors, where AD
agents still face substantial challenges in complex scenarios. Due to their
successful application in fields such as robotics and video games, the use of
deep reinforcement learning (DRL) and deep imitation learning (DIL) techniques
to derive AD policies have witnessed vast research efforts in recent years.
This paper is a comprehensive survey of this body of work, which is conducted
at three levels: First, a taxonomy of the literature studies is constructed
from the system perspective, among which five modes of integration of DRL/DIL
models into an AD architecture are identified. Second, the formulations of
DRL/DIL models for conducting specified AD tasks are comprehensively reviewed,
where various designs on the model state and action spaces and the
reinforcement learning rewards are covered. Finally, an in-depth review is
conducted on how the critical issues of AD applications regarding driving
safety, interaction with other traffic participants and uncertainty of the
environment are addressed by the DRL/DIL models. To the best of our knowledge,
this is the first survey to focus on AD policy learning using DRL/DIL, which is
addressed simultaneously from the system, task-driven and problem-driven
perspectives. We share and discuss findings, which may lead to the
investigation of various topics in the future.
</p>
<a href="http://arxiv.org/abs/2101.01993" target="_blank">arXiv:2101.01993</a> [<a href="http://arxiv.org/pdf/2101.01993" target="_blank">pdf</a>]

<h2>Weakly-Supervised Multi-Face 3D Reconstruction. (arXiv:2101.02000v1 [cs.CV])</h2>
<h3>Jialiang Zhang, Lixiang Lin, Jianke Zhu, Steven C.H. Hoi</h3>
<p>3D face reconstruction plays a very important role in many real-world
multimedia applications, including digital entertainment, social media,
affection analysis, and person identification. The de-facto pipeline for
estimating the parametric face model from an image requires to firstly detect
the facial regions with landmarks, and then crop each face to feed the deep
learning-based regressor. Comparing to the conventional methods performing
forward inference for each detected instance independently, we suggest an
effective end-to-end framework for multi-face 3D reconstruction, which is able
to predict the model parameters of multiple instances simultaneously using
single network inference. Our proposed approach not only greatly reduces the
computational redundancy in feature extraction but also makes the deployment
procedure much easier using the single network model. More importantly, we
employ the same global camera model for the reconstructed faces in each image,
which makes it possible to recover the relative head positions and orientations
in the 3D scene. We have conducted extensive experiments to evaluate our
proposed approach on the sparse and dense face alignment tasks. The
experimental results indicate that our proposed approach is very promising on
face alignment tasks without fully-supervision and pre-processing like
detection and crop. Our implementation is publicly available at
\url{https://github.com/kalyo-zjl/WM3DR}.
</p>
<a href="http://arxiv.org/abs/2101.02000" target="_blank">arXiv:2101.02000</a> [<a href="http://arxiv.org/pdf/2101.02000" target="_blank">pdf</a>]

<h2>Deep Reinforcement Learning with Quantum-inspired Experience Replay. (arXiv:2101.02034v1 [cs.LG])</h2>
<h3>Qing Wei, Hailan Ma, Chunlin Chen, Daoyi Dong</h3>
<p>In this paper, a novel training paradigm inspired by quantum computation is
proposed for deep reinforcement learning (DRL) with experience replay. In
contrast to traditional experience replay mechanism in DRL, the proposed deep
reinforcement learning with quantum-inspired experience replay (DRL-QER)
adaptively chooses experiences from the replay buffer according to the
complexity and the replayed times of each experience (also called transition),
to achieve a balance between exploration and exploitation. In DRL-QER,
transitions are first formulated in quantum representations, and then the
preparation operation and the depreciation operation are performed on the
transitions. In this progress, the preparation operation reflects the
relationship between the temporal difference errors (TD-errors) and the
importance of the experiences, while the depreciation operation is taken into
account to ensure the diversity of the transitions. The experimental results on
Atari 2600 games show that DRL-QER outperforms state-of-the-art algorithms such
as DRL-PER and DCRL on most of these games with improved training efficiency,
and is also applicable to such memory-based DRL approaches as double network
and dueling network.
</p>
<a href="http://arxiv.org/abs/2101.02034" target="_blank">arXiv:2101.02034</a> [<a href="http://arxiv.org/pdf/2101.02034" target="_blank">pdf</a>]

<h2>TextBox: A Unified, Modularized, and Extensible Framework for Text Generation. (arXiv:2101.02046v1 [cs.AI])</h2>
<h3>Junyi Li, Tianyi Tang, Gaole He, Jinhao Jiang, Xiaoxuan Hu, Puzhao Xie, Wayne Xin Zhao, Ji-Rong Wen</h3>
<p>We release an open library, called TextBox, which provides a unified,
modularized, and extensible text generation framework. TextBox aims to support
a broad set of text generation tasks and models. In TextBox, we implements
several text generation models on benchmark datasets, covering the categories
of VAE, GAN, pre-trained language models, etc. Meanwhile, our library maintains
sufficient modularity and extensibility by properly decomposing the model
architecture, inference, learning process into highly reusable modules, which
allows easily incorporating new models into our framework. It is specially
suitable for researchers and practitioners to efficiently reproduce baseline
models and develop new models. TextBox is implemented based on PyTorch, and
released under Apache License 2.0 at the link
\url{https://github.com/RUCAIBox/TextBox}.
</p>
<a href="http://arxiv.org/abs/2101.02046" target="_blank">arXiv:2101.02046</a> [<a href="http://arxiv.org/pdf/2101.02046" target="_blank">pdf</a>]

<h2>A Unified Learning Approach for Hand Gesture Recognition and Fingertip Detection. (arXiv:2101.02047v1 [cs.CV])</h2>
<h3>Mohammad Mahmudul Alam, Mohammad Tariqul Islam, S. M. Mahbubur Rahman</h3>
<p>In human-computer interaction or sign language interpretation, recognizing
hand gestures and detecting fingertips become ubiquitous in computer vision
research. In this paper, a unified approach of convolutional neural network for
both hand gesture recognition and fingertip detection is introduced. The
proposed algorithm uses a single network to predict the probabilities of finger
class and positions of fingertips in one forward propagation of the network.
Instead of directly regressing the positions of fingertips from the fully
connected layer, the ensemble of the position of fingertips is regressed from
the fully convolutional network. Subsequently, the ensemble average is taken to
regress the final position of fingertips. Since the whole pipeline uses a
single network, it is significantly fast in computation. The proposed method
results in remarkably less pixel error as compared to that in the direct
regression approach and it outperforms the existing fingertip detection
approaches including the Heatmap-based framework.
</p>
<a href="http://arxiv.org/abs/2101.02047" target="_blank">arXiv:2101.02047</a> [<a href="http://arxiv.org/pdf/2101.02047" target="_blank">pdf</a>]

<h2>Geometric Entropic Exploration. (arXiv:2101.02055v1 [cs.LG])</h2>
<h3>Zhaohan Daniel Guo, Mohammad Gheshlagi Azar, Alaa Saade, Shantanu Thakoor, Bilal Piot, Bernardo Avila Pires, Michal Valko, Thomas Mesnard, Tor Lattimore, R&#xe9;mi Munos</h3>
<p>Exploration is essential for solving complex Reinforcement Learning (RL)
tasks. Maximum State-Visitation Entropy (MSVE) formulates the exploration
problem as a well-defined policy optimization problem whose solution aims at
visiting all states as uniformly as possible. This is in contrast to standard
uncertainty-based approaches where exploration is transient and eventually
vanishes. However, existing approaches to MSVE are theoretically justified only
for discrete state-spaces as they are oblivious to the geometry of continuous
domains. We address this challenge by introducing Geometric Entropy
Maximisation (GEM), a new algorithm that maximises the geometry-aware Shannon
entropy of state-visits in both discrete and continuous domains. Our key
theoretical contribution is casting geometry-aware MSVE exploration as a
tractable problem of optimising a simple and novel noise-contrastive objective
function. In our experiments, we show the efficiency of GEM in solving several
RL problems with sparse rewards, compared against other deep RL exploration
approaches.
</p>
<a href="http://arxiv.org/abs/2101.02055" target="_blank">arXiv:2101.02055</a> [<a href="http://arxiv.org/pdf/2101.02055" target="_blank">pdf</a>]

<h2>Shallow-UWnet : Compressed Model for Underwater Image Enhancement. (arXiv:2101.02073v1 [cs.CV])</h2>
<h3>Ankita Naik (1), Apurva Swarnakar (1), Kartik Mittal (1) ((1) University of Massachusetts Amherst)</h3>
<p>Over the past few decades, underwater image enhancement has attracted
increasing amount of research effort due to its significance in underwater
robotics and ocean engineering. Research has evolved from implementing
physics-based solutions to using very deep CNNs and GANs. However, these
state-of-art algorithms are computationally expensive and memory intensive.
This hinders their deployment on portable devices for underwater exploration
tasks. These models are trained on either synthetic or limited real world
datasets making them less practical in real-world scenarios. In this paper we
propose a shallow neural network architecture, \textbf{Shallow-UWnet} which
maintains performance and has fewer parameters than the state-of-art models. We
also demonstrated the generalization of our model by benchmarking its
performance on combination of synthetic and real-world datasets.
</p>
<a href="http://arxiv.org/abs/2101.02073" target="_blank">arXiv:2101.02073</a> [<a href="http://arxiv.org/pdf/2101.02073" target="_blank">pdf</a>]

<h2>Latency Overhead of ROS2 for Modular Time-Critical Systems. (arXiv:2101.02074v1 [cs.RO])</h2>
<h3>Tobias Kronauer, Joshwa Pohlmann, Maximilian Matthe, Till Smejkal, Gerhard Fettweis</h3>
<p>Robot Operating System 2 (ROS2) targets distributed real-time systems.
Especially in tight real-time control loops, latency in data processing and
communication can lead to instabilities. As ROS2 encourages splitting of the
data-processing pipelines into several modules, it is important to understand
the latency implications of such modularization. In this paper, we investigate
the end-to-end latency of ROS2 data-processing pipeline with different Data
Distribution Service (DDS) middlewares. In addition, we profile the ROS2 stack
and point out latency bottlenecks. Our findings indicate that end-to-end
latency strongly depends on the used DDS middleware. Moreover, we show that
ROS2 can lead to 50 % latency overhead compared to using low-level DDS
communications. Our results imply guidelines for designing modular ROS2
architectures and indicate possibilities for reducing the ROS2 overhead.
</p>
<a href="http://arxiv.org/abs/2101.02074" target="_blank">arXiv:2101.02074</a> [<a href="http://arxiv.org/pdf/2101.02074" target="_blank">pdf</a>]

<h2>Artificial Intelligence Methods in In-Cabin Use Cases: A Survey. (arXiv:2101.02082v1 [cs.AI])</h2>
<h3>Yao Rong, Chao Han, Christian Hellert, Antje Loyal, Enkelejda Kasneci</h3>
<p>As interest in autonomous driving increases, efforts are being made to meet
requirements for the high-level automation of vehicles. In this context, the
functionality inside the vehicle cabin plays a key role in ensuring a safe and
pleasant journey for driver and passenger alike. At the same time, recent
advances in the field of artificial intelligence (AI) have enabled a whole
range of new applications and assistance systems to solve automated problems in
the vehicle cabin. This paper presents a thorough survey on existing work that
utilizes AI methods for use-cases inside the driving cabin, focusing, in
particular, on application scenarios related to (1) driving safety and (2)
driving comfort. Results from the surveyed works show that AI technology has a
promising future in tackling in-cabin tasks within the autonomous driving
aspect.
</p>
<a href="http://arxiv.org/abs/2101.02082" target="_blank">arXiv:2101.02082</a> [<a href="http://arxiv.org/pdf/2101.02082" target="_blank">pdf</a>]

<h2>A unified view for unsupervised representation learning with density ratio estimation: Maximization of mutual information, nonlinear ICA and nonlinear subspace estimation. (arXiv:2101.02083v1 [cs.LG])</h2>
<h3>Hiroaki Sasaki, Takashi Takenouchi</h3>
<p>Unsupervised representation learning is one of the most important problems in
machine learning. Recent promising methods are based on contrastive learning.
However, contrastive learning often relies on heuristic ideas, and therefore it
is not easy to understand what contrastive learning is doing. This paper
emphasizes that density ratio estimation is a promising goal for unsupervised
representation learning, and promotes understanding to contrastive learning.
Our primal contribution is to theoretically show that density ratio estimation
unifies three frameworks for unsupervised representation learning: Maximization
of mutual information (MI), nonlinear independent component analysis (ICA) and
a novel framework for estimation of a lower-dimensional nonlinear subspace
proposed in this paper. This unified view clarifies under what conditions
contrastive learning can be regarded as maximizing MI, performing nonlinear ICA
or estimating the lower-dimensional nonlinear subspace in the proposed
framework. Furthermore, we also make theoretical contributions in each of the
three frameworks: We show that MI can be maximized through density ratio
estimation under certain conditions, while our analysis for nonlinear ICA
reveals a novel insight for recovery of the latent source components, which is
clearly supported by numerical experiments. In addition, some theoretical
conditions are also established to estimate a nonlinear subspace in the
proposed framework. Based on the unified view, we propose two practical methods
for unsupervised representation learning through density ratio estimation: The
first method is an outlier-robust method for representation learning, while the
second one is a sample-efficient nonlinear ICA method. Finally, we numerically
demonstrate usefulness of the proposed methods in nonlinear ICA and through
application to a downstream task for classification.
</p>
<a href="http://arxiv.org/abs/2101.02083" target="_blank">arXiv:2101.02083</a> [<a href="http://arxiv.org/pdf/2101.02083" target="_blank">pdf</a>]

<h2>Fairness with Continuous Optimal Transport. (arXiv:2101.02084v1 [cs.LG])</h2>
<h3>Silvia Chiappa, Aldo Pacchiano</h3>
<p>Whilst optimal transport (OT) is increasingly being recognized as a powerful
and flexible approach for dealing with fairness issues, current OT fairness
methods are confined to the use of discrete OT. In this paper, we leverage
recent advances from the OT literature to introduce a stochastic-gradient
fairness method based on a dual formulation of continuous OT. We show that this
method gives superior performance to discrete OT methods when little data is
available to solve the OT problem, and similar performance otherwise. We also
show that both continuous and discrete OT methods are able to continually
adjust the model parameters to adapt to different levels of unfairness that
might occur in real-world applications of ML systems.
</p>
<a href="http://arxiv.org/abs/2101.02084" target="_blank">arXiv:2101.02084</a> [<a href="http://arxiv.org/pdf/2101.02084" target="_blank">pdf</a>]

<h2>On the Tightness of Semidefinite Relaxations for Rotation Estimation. (arXiv:2101.02099v1 [cs.CV])</h2>
<h3>Lucas Brynte, Viktor Larsson, Jos&#xe9; Pedro Iglesias, Carl Olsson, Fredrik Kahl</h3>
<p>Why is it that semidefinite relaxations have been so successful in numerous
applications in computer vision and robotics for solving non-convex
optimization problems involving rotations? In studying the empirical
performance, we note that there are hardly any failure cases reported in the
literature, motivating us to approach these problems from a theoretical
perspective.

A general framework based on tools from algebraic geometry is introduced for
analyzing the power of semidefinite relaxations of problems with quadratic
objective functions and rotational constraints. Applications include
registration, hand-eye calibration, camera resectioning and rotation averaging.
We characterize the extreme points, and show that there are plenty of failure
cases for which the relaxation is not tight, even in the case of a single
rotation. We also show that for some problem classes, an appropriate rotation
parametrization guarantees tight relaxations. Our theoretical findings are
accompanied with numerical simulations, providing further evidence and
understanding of the results.
</p>
<a href="http://arxiv.org/abs/2101.02099" target="_blank">arXiv:2101.02099</a> [<a href="http://arxiv.org/pdf/2101.02099" target="_blank">pdf</a>]

<h2>Adversarial Robustness by Design through Analog Computing and Synthetic Gradients. (arXiv:2101.02115v1 [cs.CV])</h2>
<h3>Alessandro Cappelli, Ruben Ohana, Julien Launay, Laurent Meunier, Iacopo Poli, Florent Krzakala</h3>
<p>We propose a new defense mechanism against adversarial attacks inspired by an
optical co-processor, providing robustness without compromising natural
accuracy in both white-box and black-box settings. This hardware co-processor
performs a nonlinear fixed random transformation, where the parameters are
unknown and impossible to retrieve with sufficient precision for large enough
dimensions. In the white-box setting, our defense works by obfuscating the
parameters of the random projection. Unlike other defenses relying on
obfuscated gradients, we find we are unable to build a reliable backward
differentiable approximation for obfuscated parameters. Moreover, while our
model reaches a good natural accuracy with a hybrid backpropagation - synthetic
gradient method, the same approach is suboptimal if employed to generate
adversarial examples. We find the combination of a random projection and
binarization in the optical system also improves robustness against various
types of black-box attacks. Finally, our hybrid training method builds robust
features against transfer attacks. We demonstrate our approach on a VGG-like
architecture, placing the defense on top of the convolutional features, on
CIFAR-10 and CIFAR-100. Code is available at
https://github.com/lightonai/adversarial-robustness-by-design.
</p>
<a href="http://arxiv.org/abs/2101.02115" target="_blank">arXiv:2101.02115</a> [<a href="http://arxiv.org/pdf/2101.02115" target="_blank">pdf</a>]

<h2>Do We Really Need Deep Learning Models for Time Series Forecasting?. (arXiv:2101.02118v1 [cs.LG])</h2>
<h3>Shereen Elsayed, Daniela Thyssens, Ahmed Rashed, Lars Schmidt-Thieme, Hadi Samer Jomaa</h3>
<p>Time series forecasting is a crucial task in machine learning, as it has a
wide range of applications including but not limited to forecasting electricity
consumption, traffic, and air quality. Traditional forecasting models relied on
rolling averages, vector auto-regression and auto-regressive integrated moving
averages. On the other hand, deep learning and matrix factorization models have
been recently proposed to tackle the same problem with more competitive
performance. However, one major drawback of such models is that they tend to be
overly complex in comparison to traditional techniques. In this paper, we try
to answer whether these highly complex deep learning models are without
alternative. We aim to enrich the pool of simple but powerful baselines by
revisiting the gradient boosting regression trees for time series forecasting.
Specifically, we reconfigure the way time series data is handled by Gradient
Tree Boosting models in a windowed fashion that is similar to the deep learning
models. For each training window, the target values are concatenated with
external features, and then flattened to form one input instance for a
multi-output gradient boosting regression tree model. We conducted a
comparative study on nine datasets for eight state-of-the-art deep-learning
models that were presented at top-level conferences in the last years. The
results demonstrated that the proposed approach outperforms all of the
state-of-the-art models.
</p>
<a href="http://arxiv.org/abs/2101.02118" target="_blank">arXiv:2101.02118</a> [<a href="http://arxiv.org/pdf/2101.02118" target="_blank">pdf</a>]

<h2>Ludii Game Logic Guide. (arXiv:2101.02120v1 [cs.AI])</h2>
<h3>Eric Piette, Cameron Browne, Dennis J. N. J. Soemers</h3>
<p>This technical report outlines the fundamental workings of the game logic
behind Ludii, a general game system, that can be used to play a wide variety of
games. Ludii is a program developed for the ERC-funded Digital Ludeme Project,
in which mathematical and computational approaches are used to study how games
were played, and spread, throughout history. This report explains how general
game states and equipment are represented in Ludii, and how the rule ludemes
dictating play are implemented behind the scenes, giving some insight into the
core game logic behind the Ludii general game player. This guide is intended to
help game designers using the Ludii game description language to understand it
more completely and make fuller use of its features when describing their
games.
</p>
<a href="http://arxiv.org/abs/2101.02120" target="_blank">arXiv:2101.02120</a> [<a href="http://arxiv.org/pdf/2101.02120" target="_blank">pdf</a>]

<h2>Attention-based Convolutional Autoencoders for 3D-Variational Data Assimilation. (arXiv:2101.02121v1 [cs.LG])</h2>
<h3>Julian Mack, Rossella Arcucci, Miguel Molina-Solana, Yi-Ke Guo</h3>
<p>We propose a new 'Bi-Reduced Space' approach to solving 3D Variational Data
Assimilation using Convolutional Autoencoders. We prove that our approach has
the same solution as previous methods but has significantly lower computational
complexity; in other words, we reduce the computational cost without affecting
the data assimilation accuracy. We tested the new method with data from a
real-world application: a pollution model of a site in Elephant and Castle,
London and found that we could reduce the size of the background covariance
matrix representation by O(10^3) and, at the same time, increase our data
assimilation accuracy with respect to existing reduced space methods.
</p>
<a href="http://arxiv.org/abs/2101.02121" target="_blank">arXiv:2101.02121</a> [<a href="http://arxiv.org/pdf/2101.02121" target="_blank">pdf</a>]

<h2>RethNet: Object-by-Object Learning for Detecting Facial Skin Problems. (arXiv:2101.02127v1 [cs.CV])</h2>
<h3>Shohrukh Bekmirzaev</h3>
<p>Semantic segmentation is a hot topic in computer vision where the most
challenging tasks of object detection and recognition have been handling by the
success of semantic segmentation approaches. We propose a concept of
object-by-object learning technique to detect 11 types of facial skin lesions
using semantic segmentation methods. Detecting individual skin lesion in a
dense group is a challenging task, because of ambiguities in the appearance of
the visual data. We observe that there exist co-occurrent visual relations
between object classes (e.g., wrinkle and age spot, or papule and whitehead,
etc.). In fact, rich contextual information significantly helps to handle the
issue. Therefore, we propose REthinker blocks that are composed of the locally
constructed convLSTM/Conv3D layers and SE module as a one-shot attention
mechanism whose responsibility is to increase network's sensitivity in the
local and global contextual representation that supports to capture ambiguously
appeared objects and co-occurrence interactions between object classes.
Experiments show that our proposed model reached MIoU of 79.46% on the test of
a prepared dataset, representing a 15.34% improvement over Deeplab v3+ (MIoU of
64.12%).
</p>
<a href="http://arxiv.org/abs/2101.02127" target="_blank">arXiv:2101.02127</a> [<a href="http://arxiv.org/pdf/2101.02127" target="_blank">pdf</a>]

<h2>LAEO-Net++: revisiting people Looking At Each Other in videos. (arXiv:2101.02136v1 [cs.CV])</h2>
<h3>Manuel J. Marin-Jimenez, Vicky Kalogeiton, Pablo Medina-Suarez, Andrew Zisserman</h3>
<p>Capturing the 'mutual gaze' of people is essential for understanding and
interpreting the social interactions between them. To this end, this paper
addresses the problem of detecting people Looking At Each Other (LAEO) in video
sequences. For this purpose, we propose LAEO-Net++, a new deep CNN for
determining LAEO in videos. In contrast to previous works, LAEO-Net++ takes
spatio-temporal tracks as input and reasons about the whole track. It consists
of three branches, one for each character's tracked head and one for their
relative position. Moreover, we introduce two new LAEO datasets: UCO-LAEO and
AVA-LAEO. A thorough experimental evaluation demonstrates the ability of
LAEO-Net++ to successfully determine if two people are LAEO and the temporal
window where it happens. Our model achieves state-of-the-art results on the
existing TVHID-LAEO video dataset, significantly outperforming previous
approaches. Finally, we apply LAEO-Net++ to a social network, where we
automatically infer the social relationship between pairs of people based on
the frequency and duration that they LAEO, and show that LAEO can be a useful
tool for guided search of human interactions in videos. The code is available
at https://github.com/AVAuco/laeonetplus.
</p>
<a href="http://arxiv.org/abs/2101.02136" target="_blank">arXiv:2101.02136</a> [<a href="http://arxiv.org/pdf/2101.02136" target="_blank">pdf</a>]

<h2>Smoothed functional-based gradient algorithms for off-policy reinforcement learning. (arXiv:2101.02137v1 [cs.LG])</h2>
<h3>Nithia Vijayan, Prashanth L. A</h3>
<p>We consider the problem of control in an off-policy reinforcement learning
(RL) context. We propose a policy gradient scheme that incorporates a smoothed
functional-based gradient estimation scheme. We provide an asymptotic
convergence guarantee for the proposed algorithm using the ordinary
differential equation (ODE) approach. Further, we derive a non-asymptotic bound
that quantifies the rate of convergence of the proposed algorithm.
</p>
<a href="http://arxiv.org/abs/2101.02137" target="_blank">arXiv:2101.02137</a> [<a href="http://arxiv.org/pdf/2101.02137" target="_blank">pdf</a>]

<h2>An Integrated Attribute Guided Dense Attention Model for Fine-Grained Generalized Zero-Shot Learning. (arXiv:2101.02141v1 [cs.CV])</h2>
<h3>Tasfia Shermin, Shyh Wei Teng, Ferdous Sohel, Manzur Murshed, Guojun Lu</h3>
<p>Fine-grained generalized zero-shot learning (GZSL) tasks require exploration
of relevance between local visual features and attributes to discover fine
distinctive information for satisfactory performance. Embedding learning and
feature synthesizing are two of the popular categories of GZSL methods.
However, these methods do not explore fine discriminative information as they
ignore either the local features or direct guidance from the attributes.
Consequently, they do not perform well. We propose a novel embedding learning
network with a two-step dense attention mechanism, which uses direct attribute
supervision to explore fine distinctive local visual features for fine-grained
GZSL tasks. We further incorporate a feature synthesizing network, which uses
the attribute-weighted visual features from the embedding learning network.
Both networks are mutually trained in an end-to-end fashion to exploit mutually
beneficial information. Consequently, the proposed method can test both
scenarios: when only the images of unseen classes are available (using the
feature synthesizing network) or when both images and semantic descriptors of
the unseen classes are available (via the embedding learning network).
Moreover, to reduce bias towards the source domain during testing, we compute
source-target class similarity based on mutual information and transfer-learn
the target classes. We demonstrate that our proposed method outperforms
contemporary methods on benchmark datasets.
</p>
<a href="http://arxiv.org/abs/2101.02141" target="_blank">arXiv:2101.02141</a> [<a href="http://arxiv.org/pdf/2101.02141" target="_blank">pdf</a>]

<h2>Transformer Guided Geometry Model for Flow-Based Unsupervised Visual Odometry. (arXiv:2101.02143v1 [cs.CV])</h2>
<h3>Xiangyu Li, Yonghong Hou, Pichao Wang, Zhimin Gao, Mingliang Xu, Wanqing Li</h3>
<p>Existing unsupervised visual odometry (VO) methods either match pairwise
images or integrate the temporal information using recurrent neural networks
over a long sequence of images. They are either not accurate, time-consuming in
training or error accumulative. In this paper, we propose a method consisting
of two camera pose estimators that deal with the information from pairwise
images and a short sequence of images respectively. For image sequences, a
Transformer-like structure is adopted to build a geometry model over a local
temporal window, referred to as Transformer-based Auxiliary Pose Estimator
(TAPE). Meanwhile, a Flow-to-Flow Pose Estimator (F2FPE) is proposed to exploit
the relationship between pairwise images. The two estimators are constrained
through a simple yet effective consistency loss in training. Empirical
evaluation has shown that the proposed method outperforms the state-of-the-art
unsupervised learning-based methods by a large margin and performs comparably
to supervised and traditional ones on the KITTI and Malaga dataset.
</p>
<a href="http://arxiv.org/abs/2101.02143" target="_blank">arXiv:2101.02143</a> [<a href="http://arxiv.org/pdf/2101.02143" target="_blank">pdf</a>]

<h2>Combining Deep Learning and Mathematical Morphology for Historical Map Segmentation. (arXiv:2101.02144v1 [cs.CV])</h2>
<h3>Yizi Chen (1,2), Edwin Carlinet (1), Joseph Chazalon (1), Cl&#xe9;ment Mallet (2), Bertrand Dum&#xe9;nieu (3), Julien Perret (2,3) ((1) EPITA Research and Development Lab. (LRDE), EPITA, France, (2) Univ. Gustave Eiffel, IGN-ENSG, LaSTIG, (3) LaD&#xe9;HiS, CRH, EHESS)</h3>
<p>The digitization of historical maps enables the study of ancient, fragile,
unique, and hardly accessible information sources. Main map features can be
retrieved and tracked through the time for subsequent thematic analysis. The
goal of this work is the vectorization step, i.e., the extraction of vector
shapes of the objects of interest from raster images of maps. We are
particularly interested in closed shape detection such as buildings, building
blocks, gardens, rivers, etc. in order to monitor their temporal evolution.
Historical map images present significant pattern recognition challenges. The
extraction of closed shapes by using traditional Mathematical Morphology (MM)
is highly challenging due to the overlapping of multiple map features and
texts. Moreover, state-of-the-art Convolutional Neural Networks (CNN) are
perfectly designed for content image filtering but provide no guarantee about
closed shape detection. Also, the lack of textural and color information of
historical maps makes it hard for CNN to detect shapes that are represented by
only their boundaries. Our contribution is a pipeline that combines the
strengths of CNN (efficient edge detection and filtering) and MM (guaranteed
extraction of closed shapes) in order to achieve such a task. The evaluation of
our approach on a public dataset shows its effectiveness for extracting the
closed boundaries of objects in historical maps.
</p>
<a href="http://arxiv.org/abs/2101.02144" target="_blank">arXiv:2101.02144</a> [<a href="http://arxiv.org/pdf/2101.02144" target="_blank">pdf</a>]

<h2>Cauchy-Schwarz Regularized Autoencoder. (arXiv:2101.02149v1 [cs.LG])</h2>
<h3>Linh Tran, Maja Pantic, Marc Peter Deisenroth</h3>
<p>Recent work in unsupervised learning has focused on efficient inference and
learning in latent variables models. Training these models by maximizing the
evidence (marginal likelihood) is typically intractable. Thus, a common
approximation is to maximize the Evidence Lower BOund (ELBO) instead.
Variational autoencoders (VAE) are a powerful and widely-used class of
generative models that optimize the ELBO efficiently for large datasets.
However, the VAE's default Gaussian choice for the prior imposes a strong
constraint on its ability to represent the true posterior, thereby degrading
overall performance. A Gaussian mixture model (GMM) would be a richer prior,
but cannot be handled efficiently within the VAE framework because of the
intractability of the Kullback-Leibler divergence for GMMs. We challenge the
adoption of the VAE framework on this specific point in favor of one with an
analytical solution for Gaussian mixture prior. To perform efficient inference
for GMM priors, we introduce a new constrained objective based on the
Cauchy-Schwarz divergence, which can be computed analytically for GMMs. This
new objective allows us to incorporate richer, multi-modal priors into the
auto-encoding framework.We provide empirical studies on a range of datasets and
show that our objective improves upon variational auto-encoding models in
density estimation, unsupervised clustering, semi-supervised learning, and face
analysis.
</p>
<a href="http://arxiv.org/abs/2101.02149" target="_blank">arXiv:2101.02149</a> [<a href="http://arxiv.org/pdf/2101.02149" target="_blank">pdf</a>]

<h2>The Shapley Value of Classifiers in Ensemble Games. (arXiv:2101.02153v1 [cs.LG])</h2>
<h3>Benedek Rozemberczki, Rik Sarkar</h3>
<p>How do we decide the fair value of individual classifiers in an ensemble
model? We introduce a new class of transferable utility cooperative games to
answer this question. The players in ensemble games are pre-trained binary
classifiers that collaborate in an ensemble to correctly label points from a
dataset. We design Troupe a scalable algorithm that designates payoffs to
individual models based on the Shapley value of those in the ensemble game. We
show that the approximate Shapley value of classifiers in these games is an
adequate measure for selecting a subgroup of highly predictive models. In
addition, we introduce the Shapley entropy a new metric to quantify the
heterogeneity of machine learning ensembles when it comes to model quality. We
analytically prove that our Shapley value approximation algorithm is accurate
and scales to large ensembles and big data. Experimental results on graph
classification tasks establish that Troupe gives precise estimates of the
Shapley value in ensemble games. We demonstrate that the Shapley value can be
used for pruning large ensembles, show that complex classifiers have a prime
role in correct and incorrect classification decisions, and provide evidence
that adversarial models receive a low valuation.
</p>
<a href="http://arxiv.org/abs/2101.02153" target="_blank">arXiv:2101.02153</a> [<a href="http://arxiv.org/pdf/2101.02153" target="_blank">pdf</a>]

<h2>Improving Training Result of Partially Observable Markov Decision Process by Filtering Beliefs. (arXiv:2101.02178v1 [cs.AI])</h2>
<h3>Oscar LiJen Hsu</h3>
<p>In this study I proposed a filtering beliefs method for improving performance
of Partially Observable Markov Decision Processes(POMDPs), which is a method
wildly used in autonomous robot and many other domains concerning control
policy. My method search and compare every similar belief pair. Because a
similar belief have insignificant influence on control policy, the belief is
filtered out for reducing training time. The empirical results show that the
proposed method outperforms the point-based approximate POMDPs in terms of the
quality of training results as well as the efficiency of the method.
</p>
<a href="http://arxiv.org/abs/2101.02178" target="_blank">arXiv:2101.02178</a> [<a href="http://arxiv.org/pdf/2101.02178" target="_blank">pdf</a>]

<h2>The case for psychometric artificial general intelligence. (arXiv:2101.02179v1 [cs.AI])</h2>
<h3>Mark McPherson</h3>
<p>A short review of the literature on measurement and detection of artificial
general intelligence is made. Proposed benchmarks and tests for artificial
general intelligence are critically evaluated against multiple criteria. Based
on the findings, the most promising approaches are identified and some useful
directions for future work are proposed.
</p>
<a href="http://arxiv.org/abs/2101.02179" target="_blank">arXiv:2101.02179</a> [<a href="http://arxiv.org/pdf/2101.02179" target="_blank">pdf</a>]

<h2>Bayesian Inference of Random Dot Product Graphs via Conic Programming. (arXiv:2101.02180v1 [cs.LG])</h2>
<h3>David Wu, David R. Palmer, Daryl R. Deford</h3>
<p>We present a convex cone program to infer the latent probability matrix of a
random dot product graph (RDPG). The optimization problem maximizes the
Bernoulli maximum likelihood function with an added nuclear norm regularization
term. The dual problem has a particularly nice form, related to the well-known
semidefinite program relaxation of the MaxCut problem. Using the primal-dual
optimality conditions, we bound the entries and rank of the primal and dual
solutions. Furthermore, we bound the optimal objective value and prove
asymptotic consistency of the probability estimates of a slightly modified
model under mild technical assumptions. Our experiments on synthetic RDPGs not
only recover natural clusters, but also reveal the underlying low-dimensional
geometry of the original data. We also demonstrate that the method recovers
latent structure in the Karate Club Graph and synthetic U.S. Senate vote graphs
and is scalable to graphs with up to a few hundred nodes.
</p>
<a href="http://arxiv.org/abs/2101.02180" target="_blank">arXiv:2101.02180</a> [<a href="http://arxiv.org/pdf/2101.02180" target="_blank">pdf</a>]

<h2>Adaptive Synthetic Characters for Military Training. (arXiv:2101.02185v1 [cs.AI])</h2>
<h3>Volkan Ustun, Rajay Kumar, Adam Reilly, Seyed Sajjadi, Andrew Miller</h3>
<p>Behaviors of the synthetic characters in current military simulations are
limited since they are generally generated by rule-based and reactive
computational models with minimal intelligence. Such computational models
cannot adapt to reflect the experience of the characters, resulting in brittle
intelligence for even the most effective behavior models devised via costly and
labor-intensive processes. Observation-based behavior model adaptation that
leverages machine learning and the experience of synthetic entities in
combination with appropriate prior knowledge can address the issues in the
existing computational behavior models to create a better training experience
in military training simulations. In this paper, we introduce a framework that
aims to create autonomous synthetic characters that can perform coherent
sequences of believable behavior while being aware of human trainees and their
needs within a training simulation. This framework brings together three
mutually complementary components. The first component is a Unity-based
simulation environment - Rapid Integration and Development Environment (RIDE) -
supporting One World Terrain (OWT) models and capable of running and supporting
machine learning experiments. The second is Shiva, a novel multi-agent
reinforcement and imitation learning framework that can interface with a
variety of simulation environments, and that can additionally utilize a variety
of learning algorithms. The final component is the Sigma Cognitive Architecture
that will augment the behavior models with symbolic and probabilistic reasoning
capabilities. We have successfully created proof-of-concept behavior models
leveraging this framework on realistic terrain as an essential step towards
bringing machine learning into military simulations.
</p>
<a href="http://arxiv.org/abs/2101.02185" target="_blank">arXiv:2101.02185</a> [<a href="http://arxiv.org/pdf/2101.02185" target="_blank">pdf</a>]

<h2>Predicting Illness for a Sustainable Dairy Agriculture: Predicting and Explaining the Onset of Mastitis in Dairy Cows. (arXiv:2101.02188v1 [cs.LG])</h2>
<h3>Cathal Ryan, Christophe G&#xfa;eret, Donagh Berry, Medb Corcoran, Mark T. Keane, Brian Mac Namee</h3>
<p>Mastitis is a billion dollar health problem for the modern dairy industry,
with implications for antibiotic resistance. The use of AI techniques to
identify the early onset of this disease, thus has significant implications for
the sustainability of this agricultural sector. Current approaches to treating
mastitis involve antibiotics and this practice is coming under ever increasing
scrutiny. Using machine learning models to identify cows at risk of developing
mastitis and applying targeted treatment regimes to only those animals promotes
a more sustainable approach. Incorrect predictions from such models, however,
can lead to monetary losses, unnecessary use of antibiotics, and even the
premature death of animals, so it is important to generate compelling
explanations for predictions to build trust with users and to better support
their decision making. In this paper we demonstrate a system developed to
predict mastitis infections in cows and provide explanations of these
predictions using counterfactuals. We demonstrate the system and describe the
engagement with farmers undertaken to build it.
</p>
<a href="http://arxiv.org/abs/2101.02188" target="_blank">arXiv:2101.02188</a> [<a href="http://arxiv.org/pdf/2101.02188" target="_blank">pdf</a>]

<h2>A Robust Illumination-Invariant Camera System for Agricultural Applications. (arXiv:2101.02190v1 [cs.CV])</h2>
<h3>Abhisesh Silwal, Tanvir Parhar, Francisco Yandun, George Kantor</h3>
<p>Object detection and semantic segmentation are two of the most widely adopted
deep learning algorithms in agricultural applications. One of the major sources
of variability in image quality acquired in the outdoors for such tasks is
changing lighting condition that can alter the appearance of the objects or the
contents of the entire image. While transfer learning and data augmentation to
some extent reduce the need for large amount of data to train deep neural
networks, the large variety of cultivars and the lack of shared datasets in
agriculture makes wide-scale field deployments difficult. In this paper, we
present a high throughput robust active lighting-based camera system that
generates consistent images in all lighting conditions. We detail experiments
that show the consistency in images quality leading to relatively fewer images
to train deep neural networks for the task of object detection. We further
present results from field experiment under extreme lighting conditions where
images without active lighting significantly lack to provide consistent
results. The experimental results show that on average, deep nets for object
detection trained on consistent data required nearly four times less data to
achieve similar level of accuracy. This proposed work could potentially provide
pragmatic solutions to computer vision needs in agriculture.
</p>
<a href="http://arxiv.org/abs/2101.02190" target="_blank">arXiv:2101.02190</a> [<a href="http://arxiv.org/pdf/2101.02190" target="_blank">pdf</a>]

<h2>Provably Efficient Reinforcement Learning with Linear Function Approximation Under Adaptivity Constraints. (arXiv:2101.02195v1 [cs.LG])</h2>
<h3>Tianhao Wang, Dongruo Zhou, Quanquan Gu</h3>
<p>We study reinforcement learning (RL) with linear function approximation under
the adaptivity constraint. We consider two popular limited adaptivity models:
batch learning model and rare policy switch model, and propose two efficient
online RL algorithms for linear Markov decision processes. In specific, for the
batch learning model, our proposed LSVI-UCB-Batch algorithm achieves an $\tilde
O(\sqrt{d^3H^3T} + dHT/B)$ regret, where $d$ is the dimension of the feature
mapping, $H$ is the episode length, $T$ is the number of interactions and $B$
is the number of batches. Our result suggests that it suffices to use only
$\sqrt{T/dH}$ batches to obtain $\tilde O(\sqrt{d^3H^3T})$ regret. For the rare
policy switch model, our proposed LSVI-UCB-RareSwitch algorithm enjoys an
$\tilde O(\sqrt{d^3H^3T[1+T/(dH)]^{dH/B}})$ regret, which implies that $dH\log
T$ policy switches suffice to obtain the $\tilde O(\sqrt{d^3H^3T})$ regret. Our
algorithms achieve the same regret as the LSVI-UCB algorithm (Jin et al.,
2019), yet with a substantially smaller amount of adaptivity.
</p>
<a href="http://arxiv.org/abs/2101.02195" target="_blank">arXiv:2101.02195</a> [<a href="http://arxiv.org/pdf/2101.02195" target="_blank">pdf</a>]

<h2>Generating Masks from Boxes by Mining Spatio-Temporal Consistencies in Videos. (arXiv:2101.02196v1 [cs.CV])</h2>
<h3>Bin Zhao, Goutam Bhat, Martin Danelljan, Luc Van Gool, Radu Timofte</h3>
<p>Segmenting objects in videos is a fundamental computer vision task. The
current deep learning based paradigm offers a powerful, but data-hungry
solution. However, current datasets are limited by the cost and human effort of
annotating object masks in videos. This effectively limits the performance and
generalization capabilities of existing video segmentation methods. To address
this issue, we explore weaker form of bounding box annotations.

We introduce a method for generating segmentation masks from per-frame
bounding box annotations in videos. To this end, we propose a spatio-temporal
aggregation module that effectively mines consistencies in the object and
background appearance across multiple frames. We use our resulting accurate
masks for weakly supervised training of video object segmentation (VOS)
networks. We generate segmentation masks for large scale tracking datasets,
using only their bounding box annotations. The additional data provides
substantially better generalization performance leading to state-of-the-art
results in both the VOS and more challenging tracking domain.
</p>
<a href="http://arxiv.org/abs/2101.02196" target="_blank">arXiv:2101.02196</a> [<a href="http://arxiv.org/pdf/2101.02196" target="_blank">pdf</a>]

<h2>A Survey on Bayesian Deep Learning. (arXiv:1604.01662v4 [stat.ML] UPDATED)</h2>
<h3>Hao Wang, Dit-Yan Yeung</h3>
<p>A comprehensive artificial intelligence system needs to not only perceive the
environment with different `senses' (e.g., seeing and hearing) but also infer
the world's conditional (or even causal) relations and corresponding
uncertainty. The past decade has seen major advances in many perception tasks
such as visual object recognition and speech recognition using deep learning
models. For higher-level inference, however, probabilistic graphical models
with their Bayesian nature are still more powerful and flexible. In recent
years, Bayesian deep learning has emerged as a unified probabilistic framework
to tightly integrate deep learning and Bayesian models. In this general
framework, the perception of text or images using deep learning can boost the
performance of higher-level inference and in turn, the feedback from the
inference process is able to enhance the perception of text or images. This
survey provides a comprehensive introduction to Bayesian deep learning and
reviews its recent applications on recommender systems, topic models, control,
etc. Besides, we also discuss the relationship and differences between Bayesian
deep learning and other related topics such as Bayesian treatment of neural
networks. For a constantly updating project page, please refer to
https://github.com/js05212/BayesianDeepLearning-Survey.
</p>
<a href="http://arxiv.org/abs/1604.01662" target="_blank">arXiv:1604.01662</a> [<a href="http://arxiv.org/pdf/1604.01662" target="_blank">pdf</a>]

<h2>Meta Dynamic Pricing: Transfer Learning Across Experiments. (arXiv:1902.10918v4 [cs.LG] UPDATED)</h2>
<h3>Hamsa Bastani, David Simchi-Levi, Ruihao Zhu</h3>
<p>We study the problem of learning shared structure \emph{across} a sequence of
dynamic pricing experiments for related products. We consider a practical
formulation where the unknown demand parameters for each product come from an
unknown distribution (prior) that is shared across products. We then propose a
meta dynamic pricing algorithm that learns this prior online while solving a
sequence of Thompson sampling pricing experiments (each with horizon $T$) for
$N$ different products. Our algorithm addresses two challenges: (i) balancing
the need to learn the prior (\emph{meta-exploration}) with the need to leverage
the estimated prior to achieve good performance (\emph{meta-exploitation}), and
(ii) accounting for uncertainty in the estimated prior by appropriately
"widening" the estimated prior as a function of its estimation error. We
introduce a novel prior alignment technique to analyze the regret of Thompson
sampling with a mis-specified prior, which may be of independent interest.
Unlike prior-independent approaches, our algorithm's meta regret grows
sublinearly in $N$, demonstrating that the price of an unknown prior in
Thompson sampling can be negligible in experiment-rich environments (large
$N$). Numerical experiments on synthetic and real auto loan data demonstrate
that our algorithm significantly speeds up learning compared to
prior-independent algorithms.
</p>
<a href="http://arxiv.org/abs/1902.10918" target="_blank">arXiv:1902.10918</a> [<a href="http://arxiv.org/pdf/1902.10918" target="_blank">pdf</a>]

<h2>Expert-Augmented Machine Learning. (arXiv:1903.09731v3 [stat.ML] UPDATED)</h2>
<h3>E.D. Gennatas, J.H. Friedman, L.H. Ungar, R. Pirracchio, E. Eaton, L. Reichman, Y. Interian, C.B. Simone, A. Auerbach, E. Delgado, M.J. Van der Laan, T.D. Solberg, G. Valdes</h3>
<p>Machine Learning is proving invaluable across disciplines. However, its
success is often limited by the quality and quantity of available data, while
its adoption by the level of trust that models afford users. Human vs. machine
performance is commonly compared empirically to decide whether a certain task
should be performed by a computer or an expert. In reality, the optimal
learning strategy may involve combining the complementary strengths of man and
machine. Here we present Expert-Augmented Machine Learning (EAML), an automated
method that guides the extraction of expert knowledge and its integration into
machine-learned models. We use a large dataset of intensive care patient data
to predict mortality and show that we can extract expert knowledge using an
online platform, help reveal hidden confounders, improve generalizability on a
different population and learn using less data. EAML presents a novel framework
for high performance and dependable machine learning in critical applications.
</p>
<a href="http://arxiv.org/abs/1903.09731" target="_blank">arXiv:1903.09731</a> [<a href="http://arxiv.org/pdf/1903.09731" target="_blank">pdf</a>]

<h2>Approximation and Non-parametric Estimation of ResNet-type Convolutional Neural Networks. (arXiv:1903.10047v3 [stat.ML] UPDATED)</h2>
<h3>Kenta Oono, Taiji Suzuki</h3>
<p>Convolutional neural networks (CNNs) have been shown to achieve optimal
approximation and estimation error rates (in minimax sense) in several function
classes. However, previous analyzed optimal CNNs are unrealistically wide and
difficult to obtain via optimization due to sparse constraints in important
function classes, including the H\"older class. We show a ResNet-type CNN can
attain the minimax optimal error rates in these classes in more plausible
situations -- it can be dense, and its width, channel size, and filter size are
constant with respect to sample size. The key idea is that we can replicate the
learning ability of Fully-connected neural networks (FNNs) by tailored CNNs, as
long as the FNNs have \textit{block-sparse} structures. Our theory is general
in a sense that we can automatically translate any approximation rate achieved
by block-sparse FNNs into that by CNNs. As an application, we derive
approximation and estimation error rates of the aformentioned type of CNNs for
the Barron and H\"older classes with the same strategy.
</p>
<a href="http://arxiv.org/abs/1903.10047" target="_blank">arXiv:1903.10047</a> [<a href="http://arxiv.org/pdf/1903.10047" target="_blank">pdf</a>]

<h2>Metric-Learning based Deep Hashing Network for Content Based Retrieval of Remote Sensing Images. (arXiv:1904.01258v3 [cs.CV] UPDATED)</h2>
<h3>Subhankar Roy, Enver Sangineto, Beg&#xfc;m Demir, Nicu Sebe</h3>
<p>Hashing methods have been recently found very effective in retrieval of
remote sensing (RS) images due to their computational efficiency and fast
search speed. The traditional hashing methods in RS usually exploit
hand-crafted features to learn hash functions to obtain binary codes, which can
be insufficient to optimally represent the information content of RS images. To
overcome this problem, in this paper we introduce a metric-learning based
hashing network, which learns: 1) a semantic-based metric space for effective
feature representation; and 2) compact binary hash codes for fast archive
search. Our network considers an interplay of multiple loss functions that
allows to jointly learn a metric based semantic space facilitating similar
images to be clustered together in that target space and at the same time
producing compact final activations that lose negligible information when
binarized. Experiments carried out on two benchmark RS archives point out that
the proposed network significantly improves the retrieval performance under the
same retrieval time when compared to the state-of-the-art hashing methods in
RS.
</p>
<a href="http://arxiv.org/abs/1904.01258" target="_blank">arXiv:1904.01258</a> [<a href="http://arxiv.org/pdf/1904.01258" target="_blank">pdf</a>]

<h2>Graph Neural Networks Exponentially Lose Expressive Power for Node Classification. (arXiv:1905.10947v5 [cs.LG] UPDATED)</h2>
<h3>Kenta Oono, Taiji Suzuki</h3>
<p>Graph Neural Networks (graph NNs) are a promising deep learning approach for
analyzing graph-structured data. However, it is known that they do not improve
(or sometimes worsen) their predictive performance as we pile up many layers
and add non-lineality. To tackle this problem, we investigate the expressive
power of graph NNs via their asymptotic behaviors as the layer size tends to
infinity. Our strategy is to generalize the forward propagation of a Graph
Convolutional Network (GCN), which is a popular graph NN variant, as a specific
dynamical system. In the case of a GCN, we show that when its weights satisfy
the conditions determined by the spectra of the (augmented) normalized
Laplacian, its output exponentially approaches the set of signals that carry
information of the connected components and node degrees only for
distinguishing nodes. Our theory enables us to relate the expressive power of
GCNs with the topological information of the underlying graphs inherent in the
graph spectra. To demonstrate this, we characterize the asymptotic behavior of
GCNs on the Erd\H{o}s -- R\'{e}nyi graph. We show that when the Erd\H{o}s --
R\'{e}nyi graph is sufficiently dense and large, a broad range of GCNs on it
suffers from the "information loss" in the limit of infinite layers with high
probability. Based on the theory, we provide a principled guideline for weight
normalization of graph NNs. We experimentally confirm that the proposed weight
scaling enhances the predictive performance of GCNs in real data. Code is
available at https://github.com/delta2323/gnn-asymptotics.
</p>
<a href="http://arxiv.org/abs/1905.10947" target="_blank">arXiv:1905.10947</a> [<a href="http://arxiv.org/pdf/1905.10947" target="_blank">pdf</a>]

<h2>Identifying Linear Models in Multi-Resolution Population Data using Minimum Description Length Principle to Predict Household Income. (arXiv:1907.05234v3 [cs.LG] UPDATED)</h2>
<h3>Chainarong Amornbunchornvej, Navaporn Surasvadi, Anon Plangprasopchok, Suttipong Thajchayapong</h3>
<p>One shirt size cannot fit everybody, while we cannot make a unique shirt that
fits perfectly for everyone because of resource limitation. This analogy is
true for the policy making. Policy makers cannot establish a single policy to
solve all problems for all regions because each region has its own unique
issue. In the other extreme, policy makers also cannot create a policy for each
small village due to the resource limitation. Would it be better if we can find
a set of largest regions such that the population of each region within this
set has common issues and we can establish a single policy for them? In this
work, we propose a framework using regression analysis and minimum description
length (MDL) to find a set of largest areas that have common indicators, which
can be used to predict household incomes efficiently. Given a set of household
features, and a multi-resolution partition that represents administrative
divisions, our framework reports a set C* of largest subdivisions that have a
common model for population-income prediction. We formalize a problem of
finding C* and propose the algorithm as a solution. We use both simulation
datasets as well as a real-world dataset of Thailand's population household
information to demonstrate our framework performance and application. The
results show that our framework performance is better than the baseline
methods. We show the results of our method can be used to find indicators of
income prediction for many areas in Thailand. By increasing these indicator
values, we expect people in these areas to gain more incomes. Hence, the policy
makers can plan to establish the policies by using these indicators in our
results as a guideline to solve low-income issues. Our framework can be used to
support policy makers to establish policies regarding any other dependent
variable beyond incomes in order to combat poverty and other issues.
</p>
<a href="http://arxiv.org/abs/1907.05234" target="_blank">arXiv:1907.05234</a> [<a href="http://arxiv.org/pdf/1907.05234" target="_blank">pdf</a>]

<h2>Organization of machine learning based product development as per ISO 26262 and ISO/PAS 21448. (arXiv:1910.05112v2 [cs.LG] UPDATED)</h2>
<h3>Krystian Radlak, Micha&#x142; Szczepankiewicz, Tim Jones, Piotr Serwa</h3>
<p>Machine learning (ML) algorithms generate a continuous stream of success
stories from various domains and enable many novel applications in
safety-critical systems. With the advent of autonomous driving, ML algorithms
are being used in the automotive domain, where the applicable functional safety
standard is ISO 26262. However, requirements and recommendations provided by
ISO 26262 do not cover specific properties of machine learning algorithms.
Therefore, specific aspects of ML (e.g., dataset requirements, performance
evaluation metrics, lack of interpretability) must be addressed within some
work products, which collect documentation resulting from one or more
associated requirements and recommendations of ISO 26262. In this paper, we
propose how key technical aspects and supporting processes related to
development of ML-based systems can be organized according to ISO 26262 phases,
sub-phases, and work products. We follow the same approach as in the ISO/PAS
21448 standard, which complements ISO 26262, in order to account for edge cases
that can lead to hazards not directly caused by system failure.%, but resulting
from functional insufficiencies of the intended functionality or by reasonably
foreseeable misuse by persons.
</p>
<a href="http://arxiv.org/abs/1910.05112" target="_blank">arXiv:1910.05112</a> [<a href="http://arxiv.org/pdf/1910.05112" target="_blank">pdf</a>]

<h2>Histogram Layers for Texture Analysis. (arXiv:2001.00215v9 [cs.LG] UPDATED)</h2>
<h3>Joshua Peeples, Weihuang Xu, Alina Zare</h3>
<p>We present a histogram layer for artificial neural networks (ANNs). An
essential aspect of texture analysis is the extraction of features that
describe the distribution of values in local spatial regions. The proposed
histogram layer directly computes the spatial distribution of features for
texture analysis and parameters for the layer are estimated during
backpropagation. We compare our method with state-of-the-art texture encoding
methods such as the Deep Encoding Network Pooling (DEP), Deep Texture Encoding
Network (DeepTEN), Fisher Vector convolutional neural network (FV-CNN), and
Multi-level Texture Encoding and Representation (MuLTER) on three
material/texture datasets: (1) the Describable Texture Dataset (DTD); (2) an
extension of the ground terrain in outdoor scenes (GTOS-mobile); (3) and a
subset of the Materials in Context (MINC-2500) dataset. Results indicate that
the inclusion of the proposed histogram layer improves performance. The source
code for the histogram layer is publicly available.
</p>
<a href="http://arxiv.org/abs/2001.00215" target="_blank">arXiv:2001.00215</a> [<a href="http://arxiv.org/pdf/2001.00215" target="_blank">pdf</a>]

<h2>Deep Learning for Person Re-identification: A Survey and Outlook. (arXiv:2001.04193v2 [cs.CV] UPDATED)</h2>
<h3>Mang Ye, Jianbing Shen, Gaojie Lin, Tao Xiang, Ling Shao, Steven C. H. Hoi</h3>
<p>Person re-identification (Re-ID) aims at retrieving a person of interest
across multiple non-overlapping cameras. With the advancement of deep neural
networks and increasing demand of intelligent video surveillance, it has gained
significantly increased interest in the computer vision community. By
dissecting the involved components in developing a person Re-ID system, we
categorize it into the closed-world and open-world settings. The widely studied
closed-world setting is usually applied under various research-oriented
assumptions, and has achieved inspiring success using deep learning techniques
on a number of datasets. We first conduct a comprehensive overview with
in-depth analysis for closed-world person Re-ID from three different
perspectives, including deep feature representation learning, deep metric
learning and ranking optimization. With the performance saturation under
closed-world setting, the research focus for person Re-ID has recently shifted
to the open-world setting, facing more challenging issues. This setting is
closer to practical applications under specific scenarios. We summarize the
open-world Re-ID in terms of five different aspects. By analyzing the
advantages of existing methods, we design a powerful AGW baseline, achieving
state-of-the-art or at least comparable performance on twelve datasets for FOUR
different Re-ID tasks. Meanwhile, we introduce a new evaluation metric (mINP)
for person Re-ID, indicating the cost for finding all the correct matches,
which provides an additional criteria to evaluate the Re-ID system for real
applications. Finally, some important yet under-investigated open issues are
discussed.
</p>
<a href="http://arxiv.org/abs/2001.04193" target="_blank">arXiv:2001.04193</a> [<a href="http://arxiv.org/pdf/2001.04193" target="_blank">pdf</a>]

<h2>Physics-Guided Deep Neural Networks for Power Flow Analysis. (arXiv:2002.00097v2 [cs.LG] UPDATED)</h2>
<h3>Xinyue Hu, Haoji Hu, Saurabh Verma, Zhi-Li Zhang</h3>
<p>Solving power flow (PF) equations is the basis of power flow analysis, which
is important in determining the best operation of existing systems, performing
security analysis, etc. However, PF equations can be out-of-date or even
unavailable due to system dynamics and uncertainties, making traditional
numerical approaches infeasible. To address these concerns, researchers have
proposed data-driven approaches to solve the PF problem by learning the mapping
rules from historical system operation data. Nevertheless, prior data-driven
approaches suffer from poor performance and generalizability, due to overly
simplified assumptions of the PF problem or ignorance of physical laws
governing power systems. In this paper, we propose a physics-guided neural
network to solve the PF problem, with an auxiliary task to rebuild the PF
model. By encoding different granularity of Kirchhoff's laws and system
topology into the rebuilt PF model, our neural-network based PF solver is
regularized by the auxiliary task and constrained by the physical laws. The
simulation results show that our physics-guided neural network methods achieve
better performance and generalizability compared to existing unconstrained
data-driven approaches. Furthermore, we demonstrate that the weight matrices of
our physics-guided neural networks embody power system physics by showing their
similarities with the bus admittance matrices.
</p>
<a href="http://arxiv.org/abs/2002.00097" target="_blank">arXiv:2002.00097</a> [<a href="http://arxiv.org/pdf/2002.00097" target="_blank">pdf</a>]

<h2>Randomization matters. How to defend against strong adversarial attacks. (arXiv:2002.11565v5 [cs.LG] UPDATED)</h2>
<h3>Rafael Pinot, Raphael Ettedgui, Geovani Rizk, Yann Chevaleyre, Jamal Atif</h3>
<p>Is there a classifier that ensures optimal robustness against all adversarial
attacks? This paper answers this question by adopting a game-theoretic point of
view. We show that adversarial attacks and defenses form an infinite zero-sum
game where classical results (e.g. Sion theorem) do not apply. We demonstrate
the non-existence of a Nash equilibrium in our game when the classifier and the
Adversary are both deterministic, hence giving a negative answer to the above
question in the deterministic regime. Nonetheless, the question remains open in
the randomized regime. We tackle this problem by showing that, undermild
conditions on the dataset distribution, any deterministic classifier can be
outperformed by a randomized one. This gives arguments for using randomization,
and leads us to a new algorithm for building randomized classifiers that are
robust to strong adversarial attacks. Empirical results validate our
theoretical analysis, and show that our defense method considerably outperforms
Adversarial Training against state-of-the-art attacks.
</p>
<a href="http://arxiv.org/abs/2002.11565" target="_blank">arXiv:2002.11565</a> [<a href="http://arxiv.org/pdf/2002.11565" target="_blank">pdf</a>]

<h2>Automatic Signboard Detection and Localization in Densely Populated Developing Cities. (arXiv:2003.01936v3 [cs.CV] UPDATED)</h2>
<h3>Md. Sadrul Islam Toaha, Sakib Bin Asad, Chowdhury Rafeed Rahman, S.M. Shahriar Haque, Mahfuz Ara Proma, Md. Ahsan Habib Shuvo, Tashin Ahmed, Md. Amimul Basher</h3>
<p>Most city establishments of developing cities are unlabeled because of the
necessity of manual annotation. Hence location and trajectory services remain
under utilized in such cities. Accurate signboard detection and localization in
natural scene images is the foremost task for accurate information retrieval
from such city streets. We develop an automated signboard detection system
suitable for such cities using Faster R-CNN based localization by incorporating
two specialized pretraining methods and a run time efficient hyperparameter
value selection algorithm. We have taken an incremental approach in reaching
our final proposed model through detailed evaluation and comparison with
baselines using our constructed SVSO signboard dataset containing signboard
natural scene images of six developing countries. Our proposed method can
detect signboards accurately, even though images contain multiple signboards
with diverse shapes and colours in a noisy background. Our proposed model
achieves 0.91 mAP score on validation set and 0.90 mAP score on an independent
test set.
</p>
<a href="http://arxiv.org/abs/2003.01936" target="_blank">arXiv:2003.01936</a> [<a href="http://arxiv.org/pdf/2003.01936" target="_blank">pdf</a>]

<h2>Unsupervised Learning of Category-Specific Symmetric 3D Keypoints from Point Sets. (arXiv:2003.07619v3 [cs.CV] UPDATED)</h2>
<h3>Clara Fernandez-Labrador, Ajad Chhatkuli, Danda Pani Paudel, Jose J. Guerrero, C&#xe9;dric Demonceaux, Luc Van Gool</h3>
<p>Automatic discovery of category-specific 3D keypoints from a collection of
objects of some category is a challenging problem. One reason is that not all
objects in a category necessarily have the same semantic parts. The level of
difficulty adds up further when objects are represented by 3D point clouds,
with variations in shape and unknown coordinate frames. We define keypoints to
be category-specific, if they meaningfully represent objects' shape and their
correspondences can be simply established order-wise across all objects. This
paper aims at learning category-specific 3D keypoints, in an unsupervised
manner, using a collection of misaligned 3D point clouds of objects from an
unknown category. In order to do so, we model shapes defined by the keypoints,
within a category, using the symmetric linear basis shapes without assuming the
plane of symmetry to be known. The usage of symmetry prior leads us to learn
stable keypoints suitable for higher misalignments. To the best of our
knowledge, this is the first work on learning such keypoints directly from 3D
point clouds. Using categories from four benchmark datasets, we demonstrate the
quality of our learned keypoints by quantitative and qualitative evaluations.
Our experiments also show that the keypoints discovered by our method are
geometrically and semantically consistent.
</p>
<a href="http://arxiv.org/abs/2003.07619" target="_blank">arXiv:2003.07619</a> [<a href="http://arxiv.org/pdf/2003.07619" target="_blank">pdf</a>]

<h2>Meta Pseudo Labels. (arXiv:2003.10580v3 [cs.LG] UPDATED)</h2>
<h3>Hieu Pham, Zihang Dai, Qizhe Xie, Minh-Thang Luong, Quoc V. Le</h3>
<p>We present Meta Pseudo Labels, a semi-supervised learning method that
achieves a new state-of-the-art top-1 accuracy of 90.2% on ImageNet, which is
1.6% better than the existing state-of-the-art. Like Pseudo Labels, Meta Pseudo
Labels has a teacher network to generate pseudo labels on unlabeled data to
teach a student network. However, unlike Pseudo Labels where the teacher is
fixed, the teacher in Meta Pseudo Labels is constantly adapted by the feedback
of the student's performance on the labeled dataset. As a result, the teacher
generates better pseudo labels to teach the student. Our code will be available
at
https://github.com/google-research/google-research/tree/master/meta_pseudo_labels.
</p>
<a href="http://arxiv.org/abs/2003.10580" target="_blank">arXiv:2003.10580</a> [<a href="http://arxiv.org/pdf/2003.10580" target="_blank">pdf</a>]

<h2>Towards Backward-Compatible Representation Learning. (arXiv:2003.11942v3 [cs.CV] UPDATED)</h2>
<h3>Yantao Shen, Yuanjun Xiong, Wei Xia, Stefano Soatto</h3>
<p>We propose a way to learn visual features that are compatible with previously
computed ones even when they have different dimensions and are learned via
different neural network architectures and loss functions. Compatible means
that, if such features are used to compare images, then "new" features can be
compared directly to "old" features, so they can be used interchangeably. This
enables visual search systems to bypass computing new features for all
previously seen images when updating the embedding models, a process known as
backfilling. Backward compatibility is critical to quickly deploy new embedding
models that leverage ever-growing large-scale training datasets and
improvements in deep learning architectures and training methods. We propose a
framework to train embedding models, called backward-compatible training (BCT),
as a first step towards backward compatible representation learning. In
experiments on learning embeddings for face recognition, models trained with
BCT successfully achieve backward compatibility without sacrificing accuracy,
thus enabling backfill-free model updates of visual embeddings.
</p>
<a href="http://arxiv.org/abs/2003.11942" target="_blank">arXiv:2003.11942</a> [<a href="http://arxiv.org/pdf/2003.11942" target="_blank">pdf</a>]

<h2>Leverage the Average: an Analysis of KL Regularization in RL. (arXiv:2003.14089v5 [cs.LG] UPDATED)</h2>
<h3>Nino Vieillard, Tadashi Kozuno, Bruno Scherrer, Olivier Pietquin, R&#xe9;mi Munos, Matthieu Geist</h3>
<p>Recent Reinforcement Learning (RL) algorithms making use of Kullback-Leibler
(KL) regularization as a core component have shown outstanding performance.
Yet, only little is understood theoretically about why KL regularization helps,
so far. We study KL regularization within an approximate value iteration scheme
and show that it implicitly averages q-values. Leveraging this insight, we
provide a very strong performance bound, the very first to combine two
desirable aspects: a linear dependency to the horizon (instead of quadratic)
and an error propagation term involving an averaging effect of the estimation
errors (instead of an accumulation effect). We also study the more general case
of an additional entropy regularizer. The resulting abstract scheme encompasses
many existing RL algorithms. Some of our assumptions do not hold with neural
networks, so we complement this theoretical analysis with an extensive
empirical study.
</p>
<a href="http://arxiv.org/abs/2003.14089" target="_blank">arXiv:2003.14089</a> [<a href="http://arxiv.org/pdf/2003.14089" target="_blank">pdf</a>]

<h2>Structural-analogy from a Single Image Pair. (arXiv:2004.02222v3 [cs.CV] UPDATED)</h2>
<h3>Sagie Benaim, Ron Mokady, Amit Bermano, Daniel Cohen-Or, Lior Wolf</h3>
<p>The task of unsupervised image-to-image translation has seen substantial
advancements in recent years through the use of deep neural networks.
Typically, the proposed solutions learn the characterizing distribution of two
large, unpaired collections of images, and are able to alter the appearance of
a given image, while keeping its geometry intact. In this paper, we explore the
capabilities of neural networks to understand image structure given only a
single pair of images, A and B. We seek to generate images that are
structurally aligned: that is, to generate an image that keeps the appearance
and style of B, but has a structural arrangement that corresponds to A. The key
idea is to map between image patches at different scales. This enables
controlling the granularity at which analogies are produced, which determines
the conceptual distinction between style and content. In addition to structural
alignment, our method can be used to generate high quality imagery in other
conditional generation tasks utilizing images A and B only: guided image
synthesis, style and texture transfer, text translation as well as video
translation. Our code and additional results are available in
https://github.com/rmokady/structural-analogy/.
</p>
<a href="http://arxiv.org/abs/2004.02222" target="_blank">arXiv:2004.02222</a> [<a href="http://arxiv.org/pdf/2004.02222" target="_blank">pdf</a>]

<h2>Capsule Networks -- A Probabilistic Perspective. (arXiv:2004.03553v3 [cs.LG] UPDATED)</h2>
<h3>Lewis Smith, Lisa Schut, Yarin Gal, Mark van der Wilk</h3>
<p>'Capsule' models try to explicitly represent the poses of objects, enforcing
a linear relationship between an object's pose and that of its constituent
parts. This modelling assumption should lead to robustness to viewpoint changes
since the sub-object/super-object relationships are invariant to the poses of
the object. We describe a probabilistic generative model which encodes such
capsule assumptions, clearly separating the generative parts of the model from
the inference mechanisms. With a variational bound we explore the properties of
the generative model independently of the approximate inference scheme, and
gain insights into failures of the capsule assumptions and inference
amortisation. We experimentally demonstrate the applicability of our unified
objective, and demonstrate the use of test time optimisation to solve problems
inherent to amortised inference in our model.
</p>
<a href="http://arxiv.org/abs/2004.03553" target="_blank">arXiv:2004.03553</a> [<a href="http://arxiv.org/pdf/2004.03553" target="_blank">pdf</a>]

<h2>Knowledge Distillation and Student-Teacher Learning for Visual Intelligence: A Review and New Outlooks. (arXiv:2004.05937v5 [cs.CV] UPDATED)</h2>
<h3>Lin Wang, Kuk-Jin Yoon</h3>
<p>Deep neural models in recent years have been successful in almost every
field, including extremely complex problem statements. However, these models
are huge in size, with millions (and even billions) of parameters, thus
demanding more heavy computation power and failing to be deployed on edge
devices. Besides, the performance boost is highly dependent on redundant
labeled data. To achieve faster speeds and to handle the problems caused by the
lack of data, knowledge distillation (KD) has been proposed to transfer
information learned from one model to another. KD is often characterized by the
so-called `Student-Teacher' (S-T) learning framework and has been broadly
applied in model compression and knowledge transfer. This paper is about KD and
S-T learning, which are being actively studied in recent years. First, we aim
to provide explanations of what KD is and how/why it works. Then, we provide a
comprehensive survey on the recent progress of KD methods together with S-T
frameworks typically for vision tasks. In general, we consider some fundamental
questions that have been driving this research area and thoroughly generalize
the research progress and technical details. Additionally, we systematically
analyze the research status of KD in vision applications. Finally, we discuss
the potentials and open challenges of existing methods and prospect the future
directions of KD and S-T learning.
</p>
<a href="http://arxiv.org/abs/2004.05937" target="_blank">arXiv:2004.05937</a> [<a href="http://arxiv.org/pdf/2004.05937" target="_blank">pdf</a>]

<h2>A Study of Neural Training with Iterative Non-Gradient Methods. (arXiv:2005.04211v3 [cs.LG] UPDATED)</h2>
<h3>Sayar Karmakar, Anirbit Mukherjee, Ramchandran Muthukumar</h3>
<p>In this work we demonstrate provable guarantees on the training of depth-2
neural networks in new regimes than previously explored. (1) First we give a
simple stochastic algorithm that can train a ReLU gate in the realizable
setting in linear time while using significantly milder conditions on the data
distribution than previous results. Leveraging some additional distributional
assumptions we also show approximate recovery of the true label generating
parameters when training a ReLU gate while a probabilistic adversary is allowed
to corrupt the true labels of the training data. Our guarantee on recovering
the true weight degrades gracefully with increasing probability of attack and
its nearly optimal in the worst case. Additionally our analysis allows for
mini-batching and computes how the convergence time scales with the mini-batch
size. (2) Secondly, we exhibit a non-gradient iterative algorithm "Neuro-Tron"
which gives a first-of-its-kind poly-time approximate solving of a neural
regression (here in the infinity-norm) problem at finite net widths and for
non-realizable data.
</p>
<a href="http://arxiv.org/abs/2005.04211" target="_blank">arXiv:2005.04211</a> [<a href="http://arxiv.org/pdf/2005.04211" target="_blank">pdf</a>]

<h2>Global Multiclass Classification and Dataset Construction via Heterogeneous Local Experts. (arXiv:2005.10848v3 [cs.LG] UPDATED)</h2>
<h3>Surin Ahn, Ayfer Ozgur, Mert Pilanci</h3>
<p>In the domains of dataset construction and crowdsourcing, a notable challenge
is to aggregate labels from a heterogeneous set of labelers, each of whom is
potentially an expert in some subset of tasks (and less reliable in others). To
reduce costs of hiring human labelers or training automated labeling systems,
it is of interest to minimize the number of labelers while ensuring the
reliability of the resulting dataset. We model this as the problem of
performing $K$-class classification using the predictions of smaller
classifiers, each trained on a subset of $[K]$, and derive bounds on the number
of classifiers needed to accurately infer the true class of an unlabeled sample
under both adversarial and stochastic assumptions. By exploiting a connection
to the classical set cover problem, we produce a near-optimal scheme for
designing such configurations of classifiers which recovers the well known
one-vs.-one classification approach as a special case. Experiments with the
MNIST and CIFAR-10 datasets demonstrate the favorable accuracy (compared to a
centralized classifier) of our aggregation scheme applied to classifiers
trained on subsets of the data. These results suggest a new way to
automatically label data or adapt an existing set of local classifiers to
larger-scale multiclass problems.
</p>
<a href="http://arxiv.org/abs/2005.10848" target="_blank">arXiv:2005.10848</a> [<a href="http://arxiv.org/pdf/2005.10848" target="_blank">pdf</a>]

<h2>Meta-Learning Bandit Policies by Gradient Ascent. (arXiv:2006.05094v2 [cs.LG] UPDATED)</h2>
<h3>Branislav Kveton, Martin Mladenov, Chih-Wei Hsu, Manzil Zaheer, Csaba Szepesvari, Craig Boutilier</h3>
<p>Most bandit policies are designed to either minimize regret in any problem
instance, making very few assumptions about the underlying environment, or in a
Bayesian sense, assuming a prior distribution over environment parameters. The
former are often too conservative in practical settings, while the latter
require assumptions that are hard to verify in practice. We study bandit
problems that fall between these two extremes, where the learning agent has
access to sampled bandit instances from an unknown prior distribution
$\mathcal{P}$ and aims to achieve high reward on average over the bandit
instances drawn from $\mathcal{P}$. This setting is of a particular importance
because it lays foundations for meta-learning of bandit policies and reflects
more realistic assumptions in many practical domains. We propose the use of
parameterized bandit policies that are differentiable and can be optimized
using policy gradients. This provides a broadly applicable framework that is
easy to implement. We derive reward gradients that reflect the structure of
bandit problems and policies, for both non-contextual and contextual settings,
and propose a number of interesting policies that are both differentiable and
have low regret. Our algorithmic and theoretical contributions are supported by
extensive experiments that show the importance of baseline subtraction, learned
biases, and the practicality of our approach on a range problems.
</p>
<a href="http://arxiv.org/abs/2006.05094" target="_blank">arXiv:2006.05094</a> [<a href="http://arxiv.org/pdf/2006.05094" target="_blank">pdf</a>]

<h2>Optimization and Generalization Analysis of Transduction through Gradient Boosting and Application to Multi-scale Graph Neural Networks. (arXiv:2006.08550v3 [cs.LG] UPDATED)</h2>
<h3>Kenta Oono, Taiji Suzuki</h3>
<p>It is known that the current graph neural networks (GNNs) are difficult to
make themselves deep due to the problem known as over-smoothing. Multi-scale
GNNs are a promising approach for mitigating the over-smoothing problem.
However, there is little explanation of why it works empirically from the
viewpoint of learning theory. In this study, we derive the optimization and
generalization guarantees of transductive learning algorithms that include
multi-scale GNNs. Using the boosting theory, we prove the convergence of the
training error under weak learning-type conditions. By combining it with
generalization gap bounds in terms of transductive Rademacher complexity, we
show that a test error bound of a specific type of multi-scale GNNs that
decreases corresponding to the number of node aggregations under some
conditions. Our results offer theoretical explanations for the effectiveness of
the multi-scale structure against the over-smoothing problem. We apply boosting
algorithms to the training of multi-scale GNNs for real-world node prediction
tasks. We confirm that its performance is comparable to existing GNNs, and the
practical behaviors are consistent with theoretical observations. Code is
available at https://github.com/delta2323/GB-GNN.
</p>
<a href="http://arxiv.org/abs/2006.08550" target="_blank">arXiv:2006.08550</a> [<a href="http://arxiv.org/pdf/2006.08550" target="_blank">pdf</a>]

<h2>A Survey of Constrained Gaussian Process Regression: Approaches and Implementation Challenges. (arXiv:2006.09319v3 [cs.LG] UPDATED)</h2>
<h3>Laura Swiler, Mamikon Gulian, Ari Frankel, Cosmin Safta, John Jakeman</h3>
<p>Gaussian process regression is a popular Bayesian framework for surrogate
modeling of expensive data sources. As part of a broader effort in scientific
machine learning, many recent works have incorporated physical constraints or
other a priori information within Gaussian process regression to supplement
limited data and regularize the behavior of the model. We provide an overview
and survey of several classes of Gaussian process constraints, including
positivity or bound constraints, monotonicity and convexity constraints,
differential equation constraints provided by linear PDEs, and boundary
condition constraints. We compare the strategies behind each approach as well
as the differences in implementation, concluding with a discussion of the
computational challenges introduced by constraints.
</p>
<a href="http://arxiv.org/abs/2006.09319" target="_blank">arXiv:2006.09319</a> [<a href="http://arxiv.org/pdf/2006.09319" target="_blank">pdf</a>]

<h2>Center-based 3D Object Detection and Tracking. (arXiv:2006.11275v2 [cs.CV] UPDATED)</h2>
<h3>Tianwei Yin, Xingyi Zhou, Philipp Kr&#xe4;henb&#xfc;hl</h3>
<p>Three-dimensional objects are commonly represented as 3D boxes in a
point-cloud. This representation mimics the well-studied image-based 2D
bounding-box detection but comes with additional challenges. Objects in a 3D
world do not follow any particular orientation, and box-based detectors have
difficulties enumerating all orientations or fitting an axis-aligned bounding
box to rotated objects. In this paper, we instead propose to represent, detect,
and track 3D objects as points. Our framework, CenterPoint, first detects
centers of objects using a keypoint detector and regresses to other attributes,
including 3D size, 3D orientation, and velocity. In a second stage, it refines
these estimates using additional point features on the object. In CenterPoint,
3D object tracking simplifies to greedy closest-point matching. The resulting
detection and tracking algorithm is simple, efficient, and effective.
CenterPoint achieved state-of-the-art performance on the nuScenes benchmark for
both 3D detection and tracking, with 65.5 NDS and 63.8 AMOTA for a single
model. On the Waymo Open Dataset, CenterPoint outperforms all previous single
model method by a large margin and ranks first among all Lidar-only
submissions. The code and pretrained models are available at
https://github.com/tianweiy/CenterPoint.
</p>
<a href="http://arxiv.org/abs/2006.11275" target="_blank">arXiv:2006.11275</a> [<a href="http://arxiv.org/pdf/2006.11275" target="_blank">pdf</a>]

<h2>A Loss Function for Generative Neural Networks Based on Watson's Perceptual Model. (arXiv:2006.15057v3 [cs.LG] UPDATED)</h2>
<h3>Steffen Czolbe, Oswin Krause, Ingemar Cox, Christian Igel</h3>
<p>To train Variational Autoencoders (VAEs) to generate realistic imagery
requires a loss function that reflects human perception of image similarity. We
propose such a loss function based on Watson's perceptual model, which computes
a weighted distance in frequency space and accounts for luminance and contrast
masking. We extend the model to color images, increase its robustness to
translation by using the Fourier Transform, remove artifacts due to splitting
the image into blocks, and make it differentiable. In experiments, VAEs trained
with the new loss function generated realistic, high-quality image samples.
Compared to using the Euclidean distance and the Structural Similarity Index,
the images were less blurry; compared to deep neural network based losses, the
new approach required less computational resources and generated images with
less artifacts.
</p>
<a href="http://arxiv.org/abs/2006.15057" target="_blank">arXiv:2006.15057</a> [<a href="http://arxiv.org/pdf/2006.15057" target="_blank">pdf</a>]

<h2>Towards Robust Deep Learning with Ensemble Networks and Noisy Layers. (arXiv:2007.01507v2 [cs.LG] UPDATED)</h2>
<h3>Yuting Liang, Reza Samavi</h3>
<p>In this paper we provide an approach for deep learning that protects against
adversarial examples in image classification-type networks. The approach relies
on two mechanisms:1) a mechanism that increases robustness at the expense of
accuracy, and, 2) a mechanism that improves accuracy but does not always
increase robustness. We show that an approach combining the two mechanisms can
provide protection against adversarial examples while retaining accuracy. We
formulate potential attacks on our approach with experimental results to
demonstrate its effectiveness. We also provide a robustness guarantee for our
approach along with an interpretation for the guarantee.
</p>
<a href="http://arxiv.org/abs/2007.01507" target="_blank">arXiv:2007.01507</a> [<a href="http://arxiv.org/pdf/2007.01507" target="_blank">pdf</a>]

<h2>Monitoring Robotic Systems using CSP: From Safety Designs to Safety Monitors. (arXiv:2007.03522v2 [cs.RO] UPDATED)</h2>
<h3>Matt Luckcuck</h3>
<p>Runtime Verification (RV) involves monitoring a system to check if it
satisfies or violates a property. It is effective at bridging the reality gap
between design-time assumptions and run-time environments; which is especially
useful for robotic systems, because they operate in the real-world. This paper
presents an RV approach that uses a Communicating Sequential Processes (CSP)
model, derived from natural-language safety documents, as a runtime monitor. We
describe our modelling process and monitoring toolchain, Varanus. The approach
is demonstrated on a teleoperated robotic system, called MASCOT, which enables
remote operations inside a nuclear reactor. We show how the safety design
documents for the MASCOT system were modelled (including how modelling revealed
an underspecification in the document) and evaluate the utility of the Varanus
toolchain. As far as we know, this is the first RV approach to directly use a
CSP model. This approach provides traceability of the safety properties from
the documentation to the system, a verified monitor for RV, and validation of
the safety documents themselves.
</p>
<a href="http://arxiv.org/abs/2007.03522" target="_blank">arXiv:2007.03522</a> [<a href="http://arxiv.org/pdf/2007.03522" target="_blank">pdf</a>]

<h2>SLAP: Improving Physical Adversarial Examples with Short-Lived Adversarial Perturbations. (arXiv:2007.04137v3 [cs.CV] UPDATED)</h2>
<h3>Giulio Lovisotto, Henry Turner, Ivo Sluganovic, Martin Strohmeier, Ivan Martinovic</h3>
<p>Research into adversarial examples (AE) has developed rapidly, yet static
adversarial patches are still the main technique for conducting attacks in the
real world, despite being obvious, semi-permanent and unmodifiable once
deployed.

In this paper, we propose Short-Lived Adversarial Perturbations (SLAP), a
novel technique that allows adversaries to realize physically robust real-world
AE by using a light projector. Attackers can project a specifically crafted
adversarial perturbation onto a real-world object, transforming it into an AE.
This allows the adversary greater control over the attack compared to
adversarial patches: (i) projections can be dynamically turned on and off or
modified at will, (ii) projections do not suffer from the locality constraint
imposed by patches, making them harder to detect.

We study the feasibility of SLAP in the self-driving scenario, targeting both
object detector and traffic sign recognition tasks, focusing on the detection
of stop signs. We conduct experiments in a variety of ambient light conditions,
including outdoors, showing how in non-bright settings the proposed method
generates AE that are extremely robust, causing misclassifications on
state-of-the-art networks with up to 99% success rate for a variety of angles
and distances. We also demostrate that SLAP-generated AE do not present
detectable behaviours seen in adversarial patches and therefore bypass
SentiNet, a physical AE detection method. We evaluate other defences including
an adaptive defender using adversarial learning which is able to thwart the
attack effectiveness up to 80% even in favourable attacker conditions.
</p>
<a href="http://arxiv.org/abs/2007.04137" target="_blank">arXiv:2007.04137</a> [<a href="http://arxiv.org/pdf/2007.04137" target="_blank">pdf</a>]

<h2>A Canonical Architecture For Predictive Analytics on Longitudinal Patient Records. (arXiv:2007.12780v2 [cs.LG] UPDATED)</h2>
<h3>Parthasarathy Suryanarayanan, Bhavani Iyer, Prithwish Chakraborty, Bibo Hao, Italo Buleje, Piyush Madan, James Codella, Antonio Foncubierta, Divya Pathak, Sarah Miller, Amol Rajmane, Shannon Harrer, Gigi Yuan-Reed, Daby Sow</h3>
<p>Many institutions within the healthcare ecosystem are making significant
investments in AI technologies to optimize their business operations at lower
cost with improved patient outcomes. Despite the hype with AI, the full
realization of this potential is seriously hindered by several systemic
problems, including data privacy, security, bias, fairness, and explainability.
In this paper, we propose a novel canonical architecture for the development of
AI models in healthcare that addresses these challenges. This system enables
the creation and management of AI predictive models throughout all the phases
of their life cycle, including data ingestion, model building, and model
promotion in production environments. This paper describes this architecture in
detail, along with a qualitative evaluation of our experience of using it on
real world problems.
</p>
<a href="http://arxiv.org/abs/2007.12780" target="_blank">arXiv:2007.12780</a> [<a href="http://arxiv.org/pdf/2007.12780" target="_blank">pdf</a>]

<h2>NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections. (arXiv:2008.02268v3 [cs.CV] UPDATED)</h2>
<h3>Ricardo Martin-Brualla, Noha Radwan, Mehdi S. M. Sajjadi, Jonathan T. Barron, Alexey Dosovitskiy, Daniel Duckworth</h3>
<p>We present a learning-based method for synthesizing novel views of complex
scenes using only unstructured collections of in-the-wild photographs. We build
on Neural Radiance Fields (NeRF), which uses the weights of a multilayer
perceptron to model the density and color of a scene as a function of 3D
coordinates. While NeRF works well on images of static subjects captured under
controlled settings, it is incapable of modeling many ubiquitous, real-world
phenomena in uncontrolled images, such as variable illumination or transient
occluders. We introduce a series of extensions to NeRF to address these issues,
thereby enabling accurate reconstructions from unstructured image collections
taken from the internet. We apply our system, dubbed NeRF-W, to internet photo
collections of famous landmarks, and demonstrate temporally consistent novel
view renderings that are significantly closer to photorealism than the prior
state of the art.
</p>
<a href="http://arxiv.org/abs/2008.02268" target="_blank">arXiv:2008.02268</a> [<a href="http://arxiv.org/pdf/2008.02268" target="_blank">pdf</a>]

<h2>Common pitfalls and recommendations for using machine learning to detect and prognosticate for COVID-19 using chest radiographs and CT scans. (arXiv:2008.06388v4 [cs.LG] UPDATED)</h2>
<h3>Michael Roberts, Derek Driggs, Matthew Thorpe, Julian Gilbey, Michael Yeung, Stephan Ursprung, Angelica I. Aviles-Rivero, Christian Etmann, Cathal McCague, Lucian Beer, Jonathan R. Weir-McCall, Zhongzhao Teng, Effrossyni Gkrania-Klotsas, James H.F. Rudd, Evis Sala, Carola-Bibiane Sch&#xf6;nlieb (on behalf of the AIX-COVNET collaboration)</h3>
<p>Machine learning methods offer great promise for fast and accurate detection
and prognostication of COVID-19 from standard-of-care chest radiographs (CXR)
and computed tomography (CT) images. Many articles have been published in 2020
describing new machine learning-based models for both of these tasks, but it is
unclear which are of potential clinical utility. In this systematic review, we
search EMBASE via OVID, MEDLINE via PubMed, bioRxiv, medRxiv and arXiv for
published papers and preprints uploaded from January 1, 2020 to October 3, 2020
which describe new machine learning models for the diagnosis or prognosis of
COVID-19 from CXR or CT images. Our search identified 2,212 studies, of which
415 were included after initial screening and, after quality screening, 61
studies were included in this systematic review. Our review finds that none of
the models identified are of potential clinical use due to methodological flaws
and/or underlying biases. This is a major weakness, given the urgency with
which validated COVID-19 models are needed. To address this, we give many
recommendations which, if followed, will solve these issues and lead to higher
quality model development and well documented manuscripts.
</p>
<a href="http://arxiv.org/abs/2008.06388" target="_blank">arXiv:2008.06388</a> [<a href="http://arxiv.org/pdf/2008.06388" target="_blank">pdf</a>]

<h2>The MRS UAV System: Pushing the Frontiers of Reproducible Research, Real-world Deployment, and Education with Autonomous Unmanned Aerial Vehicles. (arXiv:2008.08050v3 [cs.RO] UPDATED)</h2>
<h3>Tomas Baca, Matej Petrlik, Matous Vrba, Vojtech Spurny, Robert Penicka, Daniel Hert, Martin Saska</h3>
<p>We present a multirotor Unmanned Aerial Vehicle control (UAV) and estimation
system for supporting replicable research through realistic simulations and
real-world experiments. We propose a unique multi-frame localization paradigm
for estimating the states of a UAV in various frames of reference using
multiple sensors simultaneously. The system enables complex missions in GNSS
and GNSS-denied environments, including outdoor-indoor transitions and the
execution of redundant estimators for backing up unreliable localization
sources. Two feedback control designs are presented: one for precise and
aggressive maneuvers, and the other for stable and smooth flight with a noisy
state estimate. The proposed control and estimation pipeline are constructed
without using the Euler/Tait-Bryan angle representation of orientation in 3D.
Instead, we rely on rotation matrices and a novel heading-based convention to
represent the one free rotational degree-of-freedom in 3D of a standard
multirotor helicopter. We provide an actively maintained and well-documented
open-source implementation, including realistic simulation of UAV, sensors, and
localization systems. The proposed system is the product of years of applied
research on multi-robot systems, aerial swarms, aerial manipulation, motion
planning, and remote sensing. All our results have been supported by real-world
system deployment that shaped the system into the form presented here. In
addition, the system was utilized during the participation of our team from the
CTU in Prague in the prestigious MBZIRC 2017 and 2020 robotics competitions,
and also in the DARPA SubT challenge. Each time, our team was able to secure
top places among the best competitors from all over the world. On each
occasion, the challenges has motivated the team to improve the system and to
gain a great amount of high-quality experience within tight deadlines.
</p>
<a href="http://arxiv.org/abs/2008.08050" target="_blank">arXiv:2008.08050</a> [<a href="http://arxiv.org/pdf/2008.08050" target="_blank">pdf</a>]

<h2>Embodied Visual Navigation with Automatic Curriculum Learning in Real Environments. (arXiv:2009.05429v2 [cs.RO] UPDATED)</h2>
<h3>Steven D. Morad, Roberto Mecca, Rudra P.K. Poudel, Stephan Liwicki, Roberto Cipolla</h3>
<p>We present NavACL, a method of automatic curriculum learning tailored to the
navigation task. NavACL is simple to train and efficiently selects relevant
tasks using geometric features. In our experiments, deep reinforcement learning
agents trained using NavACL significantly outperform state-of-the-art agents
trained with uniform sampling -- the current standard. Furthermore, our agents
can navigate through unknown cluttered indoor environments to
semantically-specified targets using only RGB images. Obstacle-avoiding
policies and frozen feature networks support transfer to unseen real-world
environments, without any modification or retraining requirements. We evaluate
our policies in simulation, and in the real world on a ground robot and a
quadrotor drone. Videos of real-world results are available in the
supplementary material.
</p>
<a href="http://arxiv.org/abs/2009.05429" target="_blank">arXiv:2009.05429</a> [<a href="http://arxiv.org/pdf/2009.05429" target="_blank">pdf</a>]

<h2>SwarmCCO: Probabilistic Reactive Collision Avoidance for Quadrotor Swarms under Uncertainty. (arXiv:2009.07894v2 [cs.RO] UPDATED)</h2>
<h3>Senthil Hariharan Arul, Dinesh Manocha</h3>
<p>We present decentralized collision avoidance algorithms for quadrotor swarms
operating under uncertain state estimation. Our approach exploits the
differential flatness property and feedforward linearization to approximate the
quadrotor dynamics and performs reciprocal collision avoidance. We account for
the uncertainty in position and velocity by formulating the collision
constraints as chance constraints, which describe a set of velocities that
avoid collisions with a specified confidence level. We present two different
methods for formulating and solving the chance constraints: our first method
assumes a Gaussian noise distribution. Our second method is its extension to
the non-Gaussian case by using a Gaussian Mixture Model (GMM). We reformulate
the linear chance constraints into equivalent deterministic constraints, which
are used with an MPC framework to compute a local collision-free trajectory for
each quadrotor. We evaluate the proposed algorithm in simulations on benchmark
scenarios and highlight its benefits over prior methods. We observe that both
the Gaussian and non-Gaussian methods provide improved collision avoidance
performance over the deterministic method. On average, the Gaussian method
requires ~5ms to compute a local collision-free trajectory, while our
non-Gaussian method is computationally more expensive and requires ~9ms on
average in scenarios with 4 agents.
</p>
<a href="http://arxiv.org/abs/2009.07894" target="_blank">arXiv:2009.07894</a> [<a href="http://arxiv.org/pdf/2009.07894" target="_blank">pdf</a>]

<h2>Learn to Synchronize, Synchronize to Learn. (arXiv:2010.02860v2 [cs.LG] UPDATED)</h2>
<h3>Pietro Verzelli, Cesare Alippi, Lorenzo Livi</h3>
<p>In recent years, the machine learning community has seen a continuous growing
interest in research aimed at investigating dynamical aspects of both training
procedures and perfected recurrent models. Of particular interest among
recurrent neural networks we have the Reservoir Computing (RC) paradigm
characterized by conceptual simplicity and fast training scheme. Yet, the
guiding principles under which RC operates are only partially understood. In
this work, we study the properties behind learning dynamical systems and
propose a new guiding principle based on Generalized Synchronization (GS),
granting to learn a generic task with RC architectures. We show that the
well-known Echo State Property (ESP) implies and is implied by GS, so that
theoretical results derived from the ESP still hold when GS does. However, by
using GS one can profitably study the RC learning procedure by linking the
reservoir dynamics with the readout training. Notably, this allows us to shed
light on the interplay between the input encoding performed by the reservoir
and the output produced by the readout optimized for the task at hand. In
addition, we show that - as opposed to the ESP - satisfaction of the GS can be
measured by means of the Mutual False Nearest Neighbors index, which makes
effective to practitioners theoretical derivations.
</p>
<a href="http://arxiv.org/abs/2010.02860" target="_blank">arXiv:2010.02860</a> [<a href="http://arxiv.org/pdf/2010.02860" target="_blank">pdf</a>]

<h2>Towards Maximizing the Representation Gap between In-Domain & Out-of-Distribution Examples. (arXiv:2010.10474v2 [cs.LG] UPDATED)</h2>
<h3>Jay Nandy, Wynne Hsu, Mong Li Lee</h3>
<p>Among existing uncertainty estimation approaches, Dirichlet Prior Network
(DPN) distinctly models different predictive uncertainty types. However, for
in-domain examples with high data uncertainties among multiple classes, even a
DPN model often produces indistinguishable representations from the
out-of-distribution (OOD) examples, compromising their OOD detection
performance. We address this shortcoming by proposing a novel loss function for
DPN to maximize the \textit{representation gap} between in-domain and OOD
examples. Experimental results demonstrate that our proposed approach
consistently improves OOD detection performance.
</p>
<a href="http://arxiv.org/abs/2010.10474" target="_blank">arXiv:2010.10474</a> [<a href="http://arxiv.org/pdf/2010.10474" target="_blank">pdf</a>]

<h2>Incorporating Interpretable Output Constraints in Bayesian Neural Networks. (arXiv:2010.10969v2 [cs.LG] UPDATED)</h2>
<h3>Wanqian Yang, Lars Lorch, Moritz A. Graule, Himabindu Lakkaraju, Finale Doshi-Velez</h3>
<p>Domains where supervised models are deployed often come with task-specific
constraints, such as prior expert knowledge on the ground-truth function, or
desiderata like safety and fairness. We introduce a novel probabilistic
framework for reasoning with such constraints and formulate a prior that
enables us to effectively incorporate them into Bayesian neural networks
(BNNs), including a variant that can be amortized over tasks. The resulting
Output-Constrained BNN (OC-BNN) is fully consistent with the Bayesian framework
for uncertainty quantification and is amenable to black-box inference. Unlike
typical BNN inference in uninterpretable parameter space, OC-BNNs widen the
range of functional knowledge that can be incorporated, especially for model
users without expertise in machine learning. We demonstrate the efficacy of
OC-BNNs on real-world datasets, spanning multiple domains such as healthcare,
criminal justice, and credit scoring.
</p>
<a href="http://arxiv.org/abs/2010.10969" target="_blank">arXiv:2010.10969</a> [<a href="http://arxiv.org/pdf/2010.10969" target="_blank">pdf</a>]

<h2>On the Privacy Risks of Algorithmic Fairness. (arXiv:2011.03731v2 [stat.ML] UPDATED)</h2>
<h3>Hongyan Chang, Reza Shokri</h3>
<p>Algorithmic fairness and privacy are essential elements of trustworthy
machine learning for critical decision making processes. Fair machine learning
algorithms are developed to minimize discrimination against protected groups in
machine learning. This is achieved, for example, by imposing a constraint on
the model to equalize its behavior across different groups. This can
significantly increase the influence of some training data points on the fair
model. We study how this change in influence can change the information leakage
of the model about its training data. We analyze the privacy risks of
statistical notions of fairness (i.e., equalized odds) through the lens of
membership inference attacks: inferring whether a data point was used for
training a model. We show that fairness comes at the cost of privacy. However,
this privacy cost is not distributed equally: the information leakage of fair
models increases significantly on the unprivileged subgroups, which suffer from
the discrimination in regular models. Furthermore, the more biased the
underlying data is, the higher the privacy cost of achieving fairness for the
unprivileged subgroups is. We demonstrate this effect on multiple datasets and
explain how fairness-aware learning impacts privacy.
</p>
<a href="http://arxiv.org/abs/2011.03731" target="_blank">arXiv:2011.03731</a> [<a href="http://arxiv.org/pdf/2011.03731" target="_blank">pdf</a>]

<h2>A Review of Uncertainty Quantification in Deep Learning: Techniques, Applications and Challenges. (arXiv:2011.06225v4 [cs.LG] UPDATED)</h2>
<h3>Moloud Abdar, Farhad Pourpanah, Sadiq Hussain, Dana Rezazadegan, Li Liu, Mohammad Ghavamzadeh, Paul Fieguth, Xiaochun Cao, Abbas Khosravi, U Rajendra Acharya, Vladimir Makarenkov, Saeid Nahavandi</h3>
<p>Uncertainty quantification (UQ) plays a pivotal role in reduction of
uncertainties during both optimization and decision making processes. It can be
applied to solve a variety of real-world applications in science and
engineering. Bayesian approximation and ensemble learning techniques are two
most widely-used UQ methods in the literature. In this regard, researchers have
proposed different UQ methods and examined their performance in a variety of
applications such as computer vision (e.g., self-driving cars and object
detection), image processing (e.g., image restoration), medical image analysis
(e.g., medical image classification and segmentation), natural language
processing (e.g., text classification, social media texts and recidivism
risk-scoring), bioinformatics, etc. This study reviews recent advances in UQ
methods used in deep learning. Moreover, we also investigate the application of
these methods in reinforcement learning (RL). Then, we outline a few important
applications of UQ methods. Finally, we briefly highlight the fundamental
research challenges faced by UQ methods and discuss the future research
directions in this field.
</p>
<a href="http://arxiv.org/abs/2011.06225" target="_blank">arXiv:2011.06225</a> [<a href="http://arxiv.org/pdf/2011.06225" target="_blank">pdf</a>]

<h2>Born Identity Network: Multi-way Counterfactual Map Generation to Explain a Classifier's Decision. (arXiv:2011.10381v3 [cs.CV] UPDATED)</h2>
<h3>Kwanseok Oh, Jee Seok Yoon, Heung-Il Suk</h3>
<p>There exists an apparent negative correlation between performance and
interpretability of deep learning models. In an effort to reduce this negative
correlation, we propose Born Identity Network (BIN), which is a post-hoc
approach for producing multi-way counterfactual maps. A counterfactual map
transforms an input sample to be classified as a target label, which is similar
to how humans process knowledge through counterfactual thinking. Thus,
producing a better counterfactual map may be a step towards explanation at the
level of human knowledge. For example, a counterfactual map can localize
hypothetical abnormalities from a normal brain image that may cause it to be
diagnosed with a disease. Specifically, our proposed BIN consists of two core
components: Counterfactual Map Generator and Target Attribution Network. The
Counterfactual Map Generator is a variation of conditional GAN which can
synthesize a counterfactual map conditioned on an arbitrary target label. The
Target Attribution Network works in a complementary manner to enforce target
label attribution to the synthesized map. We have validated our proposed BIN in
qualitative, quantitative analysis on MNIST, 3D Shapes, and ADNI datasets, and
show the comprehensibility and fidelity of our method from various ablation
studies.
</p>
<a href="http://arxiv.org/abs/2011.10381" target="_blank">arXiv:2011.10381</a> [<a href="http://arxiv.org/pdf/2011.10381" target="_blank">pdf</a>]

<h2>They are Not Completely Useless: Towards Recycling Transferable Unlabeled Data for Class-Mismatched Semi-Supervised Learning. (arXiv:2011.13529v2 [cs.LG] UPDATED)</h2>
<h3>Zhuo Huang, Ying Tai, Chengjie Wang, Jian Yang, Chen Gong</h3>
<p>Semi-Supervised Learning (SSL) with mismatched classes deals with the problem
that the classes-of-interests in the limited labeled data is only a subset of
the classes in massive unlabeled data. As a result, the classes only possessed
by the unlabeled data may mislead the classifier training and thus hindering
the realistic landing of various SSL methods. To solve this problem, existing
methods usually divide unlabeled data to in-distribution (ID) data and
out-of-distribution (OOD) data, and directly discard or weaken the OOD data to
avoid their adverse impact. In other words, they treat OOD data as completely
useless and thus the potential valuable information for classification
contained by them is totally ignored. To remedy this defect, this paper
proposes a "Transferable OOD data Recycling" (TOOR) method which properly
utilizes ID data as well as the "recyclable" OOD data to enrich the information
for conducting class-mismatched SSL. Specifically, TOOR firstly attributes all
unlabeled data to ID data or OOD data, among which the ID data are directly
used for training. Then we treat the OOD data that have a close relationship
with ID data and labeled data as recyclable, and employ adversarial domain
adaptation to project them to the space of ID data and labeled data. In other
words, the recyclability of an OOD datum is evaluated by its transferability,
and the recyclable OOD data are transferred so that they are compatible with
the distribution of known classes-of-interests. Consequently, our TOOR method
extracts more information from unlabeled data than existing approaches, so it
can achieve the improved performance which is demonstrated by the experiments
on typical benchmark datasets.
</p>
<a href="http://arxiv.org/abs/2011.13529" target="_blank">arXiv:2011.13529</a> [<a href="http://arxiv.org/pdf/2011.13529" target="_blank">pdf</a>]

<h2>Generalization and Memorization: The Bias Potential Model. (arXiv:2011.14269v3 [stat.ML] UPDATED)</h2>
<h3>Hongkang Yang, Weinan E</h3>
<p>Models for learning probability distributions such as generative models and
density estimators behave quite differently from models for learning functions.
One example is found in the memorization phenomenon, namely the ultimate
convergence to the empirical distribution, that occurs in generative
adversarial networks (GANs). For this reason, the issue of generalization is
more subtle than that for supervised learning. For the bias potential model, we
show that dimension-independent generalization accuracy is achievable if early
stopping is adopted, despite that in the long term, the model either memorizes
the samples or diverges.
</p>
<a href="http://arxiv.org/abs/2011.14269" target="_blank">arXiv:2011.14269</a> [<a href="http://arxiv.org/pdf/2011.14269" target="_blank">pdf</a>]

<h2>A Survey on Principles, Models and Methods for Learning from Irregularly Sampled Time Series. (arXiv:2012.00168v2 [cs.LG] UPDATED)</h2>
<h3>Satya Narayan Shukla, Benjamin M. Marlin</h3>
<p>Irregularly sampled time series data arise naturally in many application
domains including biology, ecology, climate science, astronomy, and health.
Such data represent fundamental challenges to many classical models from
machine learning and statistics due to the presence of non-uniform intervals
between observations. However, there has been significant progress within the
machine learning community over the last decade on developing specialized
models and architectures for learning from irregularly sampled univariate and
multivariate time series data. In this survey, we first describe several axes
along which approaches to learning from irregularly sampled time series differ
including what data representations they are based on, what modeling primitives
they leverage to deal with the fundamental problem of irregular sampling, and
what inference tasks they are designed to perform. We then survey the recent
literature organized primarily along the axis of modeling primitives. We
describe approaches based on temporal discretization, interpolation,
recurrence, attention and structural invariance. We discuss similarities and
differences between approaches and highlight primary strengths and weaknesses.
</p>
<a href="http://arxiv.org/abs/2012.00168" target="_blank">arXiv:2012.00168</a> [<a href="http://arxiv.org/pdf/2012.00168" target="_blank">pdf</a>]

<h2>Reprogramming Language Models for Molecular Representation Learning. (arXiv:2012.03460v2 [cs.LG] UPDATED)</h2>
<h3>Ria Vinod, Pin-Yu Chen, Payel Das</h3>
<p>Recent advancements in transfer learning have made it a promising approach
for domain adaptation via transfer of learned representations. This is
especially when relevant when alternate tasks have limited samples of
well-defined and labeled data, which is common in the molecule data domain.
This makes transfer learning an ideal approach to solve molecular learning
tasks. While Adversarial reprogramming has proven to be a successful method to
repurpose neural networks for alternate tasks, most works consider source and
alternate tasks within the same domain. In this work, we propose a new
algorithm, Representation Reprogramming via Dictionary Learning (R2DL), for
adversarially reprogramming pretrained language models for molecular learning
tasks, motivated by leveraging learned representations in massive state of the
art language models. The adversarial program learns a linear transformation
between a dense source model input space (language data) and a sparse target
model input space (e.g., chemical and biological molecule data) using a k-SVD
solver to approximate a sparse representation of the encoded data, via
dictionary learning. R2DL achieves the baseline established by state of the art
toxicity prediction models trained on domain-specific data and outperforms the
baseline in a limited training-data setting, thereby establishing avenues for
domain-agnostic transfer learning for tasks with molecule data.
</p>
<a href="http://arxiv.org/abs/2012.03460" target="_blank">arXiv:2012.03460</a> [<a href="http://arxiv.org/pdf/2012.03460" target="_blank">pdf</a>]

<h2>An Efficient Asynchronous Method for Integrating Evolutionary and Gradient-based Policy Search. (arXiv:2012.05417v2 [cs.LG] UPDATED)</h2>
<h3>Kyunghyun Lee, Byeong-Uk Lee, Ukcheol Shin, In So Kweon</h3>
<p>Deep reinforcement learning (DRL) algorithms and evolution strategies (ES)
have been applied to various tasks, showing excellent performances. These have
the opposite properties, with DRL having good sample efficiency and poor
stability, while ES being vice versa. Recently, there have been attempts to
combine these algorithms, but these methods fully rely on synchronous update
scheme, making it not ideal to maximize the benefits of the parallelism in ES.
To solve this challenge, asynchronous update scheme was introduced, which is
capable of good time-efficiency and diverse policy exploration. In this paper,
we introduce an Asynchronous Evolution Strategy-Reinforcement Learning (AES-RL)
that maximizes the parallel efficiency of ES and integrates it with policy
gradient methods. Specifically, we propose 1) a novel framework to merge ES and
DRL asynchronously and 2) various asynchronous update methods that can take all
advantages of asynchronism, ES, and DRL, which are exploration and time
efficiency, stability, and sample efficiency, respectively. The proposed
framework and update methods are evaluated in continuous control benchmark
work, showing superior performance as well as time efficiency compared to the
previous methods.
</p>
<a href="http://arxiv.org/abs/2012.05417" target="_blank">arXiv:2012.05417</a> [<a href="http://arxiv.org/pdf/2012.05417" target="_blank">pdf</a>]

<h2>Demystifying Deep Neural Networks Through Interpretation: A Survey. (arXiv:2012.07119v2 [cs.LG] UPDATED)</h2>
<h3>Giang Dao, Minwoo Lee</h3>
<p>Modern deep learning algorithms tend to optimize an objective metric, such as
minimize a cross entropy loss on a training dataset, to be able to learn. The
problem is that the single metric is an incomplete description of the real
world tasks. The single metric cannot explain why the algorithm learn. When an
erroneous happens, the lack of interpretability causes a hardness of
understanding and fixing the error. Recently, there are works done to tackle
the problem of interpretability to provide insights into neural networks
behavior and thought process. The works are important to identify potential
bias and to ensure algorithm fairness as well as expected performance.
</p>
<a href="http://arxiv.org/abs/2012.07119" target="_blank">arXiv:2012.07119</a> [<a href="http://arxiv.org/pdf/2012.07119" target="_blank">pdf</a>]

<h2>Fine-Grained Vehicle Perception via 3D Part-Guided Visual Data Augmentation. (arXiv:2012.08055v2 [cs.CV] UPDATED)</h2>
<h3>Feixiang Lu, Zongdai Liu, Hui Miao, Peng Wang, Liangjun Zhang, Ruigang Yang, Dinesh Manocha, Bin Zhou</h3>
<p>Holistically understanding an object and its 3D movable parts through visual
perception models is essential for enabling an autonomous agent to interact
with the world. For autonomous driving, the dynamics and states of vehicle
parts such as doors, the trunk, and the bonnet can provide meaningful semantic
information and interaction states, which are essential to ensuring the safety
of the self-driving vehicle. Existing visual perception models mainly focus on
coarse parsing such as object bounding box detection or pose estimation and
rarely tackle these situations. In this paper, we address this important
autonomous driving problem by solving three critical issues. First, to deal
with data scarcity, we propose an effective training data generation process by
fitting a 3D car model with dynamic parts to vehicles in real images before
reconstructing human-vehicle interaction (VHI) scenarios. Our approach is fully
automatic without any human interaction, which can generate a large number of
vehicles in uncommon states (VUS) for training deep neural networks (DNNs).
Second, to perform fine-grained vehicle perception, we present a multi-task
network for VUS parsing and a multi-stream network for VHI parsing. Third, to
quantitatively evaluate the effectiveness of our data augmentation approach, we
build the first VUS dataset in real traffic scenarios (e.g., getting on/out or
placing/removing luggage). Experimental results show that our approach advances
other baseline methods in 2D detection and instance segmentation by a big
margin (over 8%). In addition, our network yields large improvements in
discovering and understanding these uncommon cases. Moreover, we have released
the source code, the dataset, and the trained model on Github
(https://github.com/zongdai/EditingForDNN).
</p>
<a href="http://arxiv.org/abs/2012.08055" target="_blank">arXiv:2012.08055</a> [<a href="http://arxiv.org/pdf/2012.08055" target="_blank">pdf</a>]

<h2>Measuring Disentanglement: A Review of Metrics. (arXiv:2012.09276v2 [cs.LG] UPDATED)</h2>
<h3>Julian Zaidi, Jonathan Boilard, Ghyslain Gagnon, Marc-Andr&#xe9; Carbonneau</h3>
<p>Learning to disentangle and represent factors of variation in data is an
important problem in AI. While many advances are made to learn these
representations, it is still unclear how to quantify disentanglement. Several
metrics exist, however little is known on their implicit assumptions, what they
truly measure and their limits. As a result, it is difficult to interpret
results when comparing different representations. In this work, we survey
supervised disentanglement metrics and thoroughly analyze them. We propose a
new taxonomy in which all metrics fall into one of three families:
intervention-based, predictor-based and information-based. We conduct extensive
experiments, where we isolate representation properties to compare all metrics
on many aspects. From experiment results and analysis, we provide insights on
relations between disentangled representation properties. Finally, we provide
guidelines on how to measure disentanglement and report the results.
</p>
<a href="http://arxiv.org/abs/2012.09276" target="_blank">arXiv:2012.09276</a> [<a href="http://arxiv.org/pdf/2012.09276" target="_blank">pdf</a>]

<h2>BKT-LSTM: Efficient Student Modeling for knowledge tracing and student performance prediction. (arXiv:2012.12218v2 [cs.AI] UPDATED)</h2>
<h3>Sein Minn</h3>
<p>Recently, we have seen a rapid rise in usage of online educational platforms.
The personalized education became crucially important in future learning
environments. Knowledge tracing (KT) refers to the detection of students'
knowledge states and predict future performance given their past outcomes for
providing adaptive solution to Intelligent Tutoring Systems (ITS). Bayesian
Knowledge Tracing (BKT) is a model to capture mastery level of each skill with
psychologically meaningful parameters and widely used in successful tutoring
systems. However, it is unable to detect learning transfer across skills
because each skill model is learned independently and shows lower efficiency in
student performance prediction. While recent KT models based on deep neural
networks shows impressive predictive power but it came with a price. Ten of
thousands of parameters in neural networks are unable to provide
psychologically meaningful interpretation that reflect to cognitive theory. In
this paper, we proposed an efficient student model called BKT-LSTM. It contains
three meaningful components: individual \textit{skill mastery} assessed by BKT,
\textit{ability profile} (learning transfer across skills) detected by k-means
clustering and \textit{problem difficulty}. All these components are taken into
account in student's future performance prediction by leveraging predictive
power of LSTM. BKT-LSTM outperforms state-of-the-art student models in
student's performance prediction by considering these meaningful features
instead of using binary values of student's past interaction in DKT. We also
conduct ablation studies on each of BKT-LSTM model components to examine their
value and each component shows significant contribution in student's
performance prediction. Thus, it has potential for providing adaptive and
personalized instruction in real-world educational systems.
</p>
<a href="http://arxiv.org/abs/2012.12218" target="_blank">arXiv:2012.12218</a> [<a href="http://arxiv.org/pdf/2012.12218" target="_blank">pdf</a>]

<h2>Weighting-Based Treatment Effect Estimation via Distribution Learning. (arXiv:2012.13805v3 [cs.LG] UPDATED)</h2>
<h3>Dongcheng Zhang, Kunpeng Zhang</h3>
<p>Existing weighting methods for treatment effect estimation are often built
upon the idea of propensity scores or covariate balance. They usually impose
strong assumptions on treatment assignment or outcome model to obtain unbiased
estimation, such as linearity or specific functional forms, which easily leads
to the major drawback of model mis-specification. In this paper, we aim to
alleviate these issues by developing a distribution learning-based weighting
method. We first learn the true underlying distribution of covariates
conditioned on treatment assignment, then leverage the ratio of covariates'
density in the treatment group to that of the control group as the weight for
estimating treatment effects. Specifically, we propose to approximate the
distribution of covariates in both treatment and control groups through
invertible transformations via change of variables. To demonstrate the
superiority, robustness, and generalizability of our method, we conduct
extensive experiments using synthetic and real data. From the experiment
results, we find that our method for estimating average treatment effect on
treated (ATT) with observational data outperforms several cutting-edge
weighting-only benchmarking methods, and it maintains its advantage under a
doubly-robust estimation framework that combines weighting with some advanced
outcome modeling methods.
</p>
<a href="http://arxiv.org/abs/2012.13805" target="_blank">arXiv:2012.13805</a> [<a href="http://arxiv.org/pdf/2012.13805" target="_blank">pdf</a>]

<h2>Good practices for Bayesian Optimization of high dimensional structured spaces. (arXiv:2012.15471v2 [cs.LG] UPDATED)</h2>
<h3>Eero Siivola, Javier Gonzalez, Andrei Paleyes, Aki Vehtari</h3>
<p>The increasing availability of structured but high dimensional data has
opened new opportunities for optimization. One emerging and promising avenue is
the exploration of unsupervised methods for projecting structured high
dimensional data into low dimensional continuous representations, simplifying
the optimization problem and enabling the application of traditional
optimization methods. However, this line of research has been purely
methodological with little connection to the needs of practitioners so far. In
this paper, we study the effect of different search space design choices for
performing Bayesian Optimization in high dimensional structured datasets. In
particular, we analyse the influence of the dimensionality of the latent space,
the role of the acquisition function and evaluate new methods to automatically
define the optimization bounds in the latent space. Finally, based on
experimental results using synthetic and real datasets, we provide
recommendations for the practitioners.
</p>
<a href="http://arxiv.org/abs/2012.15471" target="_blank">arXiv:2012.15471</a> [<a href="http://arxiv.org/pdf/2012.15471" target="_blank">pdf</a>]

<h2>Meta Variationally Intrinsic Motivated Reinforcement Learning for Decentralized Traffic Signal Control. (arXiv:2101.00746v3 [cs.LG] UPDATED)</h2>
<h3>Liwen Zhu, Peixi Peng, Zongqing Lu, Xiangqian Wang, Yonghong Tian</h3>
<p>The goal of traffic signal control is to coordinate multiple traffic signals
to improve the traffic efficiency of a district or a city. In this work, we
propose a novel Meta Variationally Intrinsic Motivated (MetaVIM) RL method, and
aim to learn the decentralized polices of each traffic signal only conditioned
on its local observation. MetaVIM makes three novel contributions. Firstly, to
make the model available to new unseen target scenarios, we formulate the
traffic signal control as a meta-learning problem over a set of related tasks.
The train scenario is divided as multiple partially observable Markov decision
process (POMDP) tasks, and each task corresponds to a traffic light. In each
task, the neighbours are regarded as an unobserved part of the state.

Secondly, we assume that the reward, transition and policy functions vary
across different tasks but share a common structure, and a learned latent
variable conditioned on the past trajectories is proposed for each task to
represent the specific information of the current task in these functions, then
is further brought into policy for automatically trade off between exploration
and exploitation to induce the RL agent to choose the reasonable action. In
addition, to make the policy learning stable, four decoders are introduced to
predict the received observations and rewards of the current agent with/without
neighbour agents' policies, and a novel intrinsic reward is designed to
encourage the received observation and reward invariant to the neighbour
agents. Empirically, extensive experiments conducted on CityFlow demonstrate
that the proposed method substantially outperforms existing methods and shows
superior generalizability.
</p>
<a href="http://arxiv.org/abs/2101.00746" target="_blank">arXiv:2101.00746</a> [<a href="http://arxiv.org/pdf/2101.00746" target="_blank">pdf</a>]

<h2>AutoEncoder for Interpolation. (arXiv:2101.00853v2 [stat.ML] UPDATED)</h2>
<h3>Rahul Bhadani</h3>
<p>In physical science, sensor data are collected over time to produce
timeseries data. However, depending on the real-world condition and underlying
physics of the sensor, data might be noisy. Besides, the limitation of
sample-time on sensors may not allow collecting data over all the timepoints,
may require some form of interpolation. Interpolation may not be smooth enough,
fail to denoise data, and derivative operation on noisy sensor data may be poor
that do not reveal any high order dynamics. In this article, we propose to use
AutoEncoder to perform interpolation that also denoise data simultaneously. A
brief example using a real-world is also provided.
</p>
<a href="http://arxiv.org/abs/2101.00853" target="_blank">arXiv:2101.00853</a> [<a href="http://arxiv.org/pdf/2101.00853" target="_blank">pdf</a>]

<h2>VIS30K: A Collection of Figures and Tables from {IEEE} Visualization Conference Publications. (arXiv:2101.01036v2 [cs.CV] UPDATED)</h2>
<h3>Jian Chen, Meng Ling, Rui Li, Petra Isenberg, Tobias Isenberg, Michael Sedlmair, Torsten M&#xf6;ller, Robert S. Laramee, Han-Wei Shen, Katharina W&#xfc;nsche, Qiru Wang</h3>
<p>We present the VIS30K dataset, a collection of 29,689 images that represents
30 years of figures and tables from each track of the IEEE Visualization
conference series (Vis, SciVis, InfoVis, VAST). VIS30K's comprehensive coverage
of the scientific literature in visualization not only reflects the progress of
the field but also enables researchers to study the evolution of the
state-of-the-art and to find relevant work based on graphical content. We
describe the dataset and our semi-automatic collection process, which couples
convolutional neural networks (CNN) with curation. Extracting figures and
tables semi-automatically allows us to verify that no images are overlooked or
extracted erroneously. To improve quality further, we engaged in a peer-search
process for high-quality figures from early IEEE Visualization papers. With the
resulting data, we also contribute VISImageNavigator (VIN,
visimagenavigator.github.io), a web-based tool that facilitates searching and
exploring VIS30K by author names, paper keywords, title and abstract, and
years.
</p>
<a href="http://arxiv.org/abs/2101.01036" target="_blank">arXiv:2101.01036</a> [<a href="http://arxiv.org/pdf/2101.01036" target="_blank">pdf</a>]

<h2>Dynamic Knowledge Graphs as Semantic Memory Model for Industrial Robots. (arXiv:2101.01099v2 [cs.RO] UPDATED)</h2>
<h3>Mohak Sukhwani, Vishakh Duggal, Said Zahrai</h3>
<p>In this paper, we present a model for semantic memory that allows machines to
collect information and experiences to become more proficient with time. After
a semantic analysis of the data, information is stored in a knowledge graph
which is used to comprehend instructions, expressed in natural language, and
execute the required tasks in a deterministic manner. This imparts industrial
robots cognitive behavior and an intuitive user interface, which is most
appreciated in an era, when collaborative robots are to work alongside humans.
The paper outlines the architecture of the system together with a practical
implementation of the proposal.
</p>
<a href="http://arxiv.org/abs/2101.01099" target="_blank">arXiv:2101.01099</a> [<a href="http://arxiv.org/pdf/2101.01099" target="_blank">pdf</a>]

<h2>Human Activity Recognition using Wearable Sensors: Review, Challenges, Evaluation Benchmark. (arXiv:2101.01665v2 [cs.CV] UPDATED)</h2>
<h3>Reem Abdel-Salam, Rana Mostafa, Mayada Hadhood</h3>
<p>Recognizing human activity plays a significant role in the advancements of
human-interaction applications in healthcare, personal fitness, and smart
devices. Many papers presented various techniques for human activity
representation that resulted in distinguishable progress. In this study, we
conduct an extensive literature review on recent, top-performing techniques in
human activity recognition based on wearable sensors. Due to the lack of
standardized evaluation and to assess and ensure a fair comparison between the
state-of-the-art techniques, we applied a standardized evaluation benchmark on
the state-of-the-art techniques using six publicly available data-sets:
MHealth, USCHAD, UTD-MHAD, WISDM, WHARF, and OPPORTUNITY. Also, we propose an
experimental, improved approach that is a hybrid of enhanced handcrafted
features and a neural network architecture which outperformed top-performing
techniques with the same standardized evaluation benchmark applied concerning
MHealth, USCHAD, UTD-MHAD data-sets.
</p>
<a href="http://arxiv.org/abs/2101.01665" target="_blank">arXiv:2101.01665</a> [<a href="http://arxiv.org/pdf/2101.01665" target="_blank">pdf</a>]

<h2>An Odor Labeling Convolutional Encoder-Decoder for Odor Sensing in Machine Olfaction. (arXiv:2011.12538v2 [cs.LG] CROSS LISTED)</h2>
<h3>Tengteng Wen, Zhuofeng Mo, Jingshan Li, Qi Liu, Liming Wu, Dehan Luo</h3>
<p>Deep learning methods have been widely applied to visual and acoustic
technology. In this paper, we proposed an odor labeling convolutional
encoder-decoder (OLCE) for odor identification in machine olfaction. OLCE
composes a convolutional neural network encoder and decoder where the encoder
output is constrained to odor labels. An electronic nose was used for the data
collection of gas responses followed by a normative experimental procedure.
Several evaluation indexes were calculated to evaluate the algorithm
effectiveness: accuracy 92.57%, precision 92.29%, recall rate 92.06%, F1-Score
91.96%, and Kappa coefficient 90.76%. We also compared the model with some
algorithms used in machine olfaction. The comparison result demonstrated that
OLCE had the best performance among these algorithms. In the paper, some
perspectives of machine olfactions have been also discussed.
</p>
<a href="http://arxiv.org/abs/2011.12538" target="_blank">arXiv:2011.12538</a> [<a href="http://arxiv.org/pdf/2011.12538" target="_blank">pdf</a>]

