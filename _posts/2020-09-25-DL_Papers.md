---
title: Latest Deep Learning Papers
date: 2020-10-01 21:00:01 +0800
featured-img: DL-logo
categories: [Deep_Learning]
tags: [Deep_Learning]
mathjax: Yes
---

<h1>Your arXiv Feed</h1>
<h2>Linear-Sample Learning of Low-Rank Distributions. (arXiv:2010.00064v1 [cs.LG])</h2>
<h3>Ayush Jain, Alon Orlitsky</h3>
<p>Many latent-variable applications, including community detection,
collaborative filtering, genomic analysis, and NLP, model data as generated by
low-rank matrices. Yet despite considerable research, except for very special
cases, the number of samples required to efficiently recover the underlying
matrices has not been known. We determine the onset of learning in several
common latent-variable settings. For all of them, we show that learning
$k\times k$, rank-$r$, matrices to normalized $L_{1}$ distance $\epsilon$
requires $\Omega(\frac{kr}{\epsilon^2})$ samples, and propose an algorithm that
uses ${\cal O}(\frac{kr}{\epsilon^2}\log^2\frac r\epsilon)$ samples, a number
linear in the high dimension, and nearly linear in the, typically low, rank.
The algorithm improves on existing spectral techniques and runs in polynomial
time. The proofs establish new results on the rapid convergence of the spectral
distance between the model and observation matrices, and may be of independent
interest.
</p>
<a href="http://arxiv.org/abs/2010.00064">arXiv:2010.00064</a> [<a href="http://arxiv.org/pdf/2010.00064">pdf</a>]

<h2>Error Compensated Distributed SGD Can Be Accelerated. (arXiv:2010.00091v1 [math.OC])</h2>
<h3>Xun Qian, Peter Richt&#xe1;rik, Tong Zhang</h3>
<p>Gradient compression is a recent and increasingly popular technique for
reducing the communication cost in distributed training of large-scale machine
learning models. In this work we focus on developing efficient distributed
methods that can work for any compressor satisfying a certain contraction
property, which includes both unbiased (after appropriate scaling) and biased
compressors such as RandK and TopK. Applied naively, gradient compression
introduces errors that either slow down convergence or lead to divergence. A
popular technique designed to tackle this issue is error compensation/error
feedback. Due to the difficulties associated with analyzing biased compressors,
it is not known whether gradient compression with error compensation can be
combined with Nesterov's acceleration. In this work, we show for the first time
that error compensated gradient compression methods can be accelerated. In
particular, we propose and study the error compensated loopless Katyusha
method, and establish an accelerated linear convergence rate under standard
assumptions. We show through numerical experiments that the proposed method
converges with substantially fewer communication rounds than previous error
compensated algorithms.
</p>
<a href="http://arxiv.org/abs/2010.00091">arXiv:2010.00091</a> [<a href="http://arxiv.org/pdf/2010.00091">pdf</a>]

<h2>Massive Uncoordinated Multiple Access for Beyond 5G. (arXiv:2010.00098v1 [cs.IT])</h2>
<h3>Mostafa Mohammadkarimi, Octavia A. Dobre, Moe Z. Win</h3>
<p>Existing wireless communication systems have been mainly designed to provide
substantial gain in terms of data rates. However, 5G and Beyond will depart
from this scheme, with the objective not only to provide services with higher
data rates. One of the main goals is to support massive machine-type
communications (mMTC) in the IoT applications. Supporting massive uplink (UP)
communications for devices with sporadic traffic pattern and short-packet size,
as it is in many mMTC use cases, is a challenging task, particularly when the
control signaling is not negligible in size compared to the payload. Also,
channel estimation is challenging for sporadic and short-packet transmission
due to the limited number of employed pilots. In this paper, a new UP multiple
access (MA) scheme is proposed for mMTC, which can support a large number of
uncoordinated IoT devices with short-packet and sporadic traffic. The proposed
UP MA scheme removes the overheads associated with the device identifier as
well as pilots related to channel estimation. An alternative mechanism for
device identification is proposed, where a unique spreading code is dedicated
to each IoT device. This unique code is simultaneously used for the spreading
purpose and device identification. Two IoT device identification algorithms
which employ sparse signal reconstruction methods are proposed to determine the
active IoT devices prior to data detection. Specifically, the BIC model order
selection method is employed to develop an IoT device identification algorithm
for unknown and time-varying probability of device activity. Our proposed MA
scheme benefits from a non-coherent multiuser detection algorithm based on
machine learning to enable data detection without a priori knowledge on channel
state information. The effectiveness of the proposed MA scheme for known and
unknown probability of activity is supported by simulation results.
</p>
<a href="http://arxiv.org/abs/2010.00098">arXiv:2010.00098</a> [<a href="http://arxiv.org/pdf/2010.00098">pdf</a>]

<h2>Machine Learning in Airline Crew Pairing to Construct Initial Clusters for Dynamic Constraint Aggregation. (arXiv:2010.00134v1 [cs.AI])</h2>
<h3>Yassine Yaakoubi, Fran&#xe7;ois Soumis, Simon Lacoste-Julien</h3>
<p>The crew pairing problem (CPP) is generally modelled as a set partitioning
problem where the flights have to be partitioned in pairings. A pairing is a
sequence of flight legs separated by connection time and rest periods that
starts and ends at the same base. Because of the extensive list of complex
rules and regulations, determining whether a sequence of flights constitutes a
feasible pairing can be quite difficult by itself, making CPP one of the
hardest of the airline planning problems. In this paper, we first propose to
improve the prototype Baseline solver of Desaulniers et al. (2020) by adding
dynamic control strategies to obtain an efficient solver for large-scale CPPs:
Commercial-GENCOL-DCA. These solvers are designed to aggregate the flights
covering constraints to reduce the size of the problem. Then, we use machine
learning (ML) to produce clusters of flights having a high probability of being
performed consecutively by the same crew. The solver combines several advanced
Operations Research techniques to assemble and modify these clusters, when
necessary, to produce a good solution. We show, on monthly CPPs with up to 50
000 flights, that Commercial-GENCOL-DCA with clusters produced by ML-based
heuristics outperforms Baseline fed by initial clusters that are pairings of a
solution obtained by rolling horizon with GENCOL. The reduction of solution
cost averages between 6.8% and 8.52%, which is mainly due to the reduction in
the cost of global constraints between 69.79% and 78.11%.
</p>
<a href="http://arxiv.org/abs/2010.00134">arXiv:2010.00134</a> [<a href="http://arxiv.org/pdf/2010.00134">pdf</a>]

<h2>Entropy Regularization for Mean Field Games with Learning. (arXiv:2010.00145v1 [math.OC])</h2>
<h3>Xin Guo, Renyuan Xu, Thaleia Zariphopoulou</h3>
<p>Entropy regularization has been extensively adopted to improve the
efficiency, the stability, and the convergence of algorithms in reinforcement
learning. This paper analyzes both quantitatively and qualitatively the impact
of entropy regularization for Mean Field Game (MFG) with learning in a finite
time horizon. Our study provides a theoretical justification that entropy
regularization yields time-dependent policies and, furthermore, helps
stabilizing and accelerating convergence to the game equilibrium. In addition,
this study leads to a policy-gradient algorithm for exploration in MFG. Under
this algorithm, agents are able to learn the optimal exploration scheduling,
with stable and fast convergence to the game equilibrium.
</p>
<a href="http://arxiv.org/abs/2010.00145">arXiv:2010.00145</a> [<a href="http://arxiv.org/pdf/2010.00145">pdf</a>]

<h2>Robust Model-Free Learning and Control without Prior Knowledge. (arXiv:2010.00204v1 [math.OC])</h2>
<h3>Dimitar Ho, John Doyle</h3>
<p>We present a simple model-free control algorithm that is able to robustly
learn and stabilize an unknown discrete-time linear system with full control
and state feedback subject to arbitrary bounded disturbance and noise
sequences. The controller does not require any prior knowledge of the system
dynamics, disturbances, or noise, yet it can guarantee robust stability and
provides asymptotic and worst-case bounds on the state and input trajectories.
To the best of our knowledge, this is the first model-free algorithm that comes
with such robust stability guarantees without the need to make any prior
assumptions about the system. We would like to highlight the new convex
geometry-based approach taken towards robust stability analysis which served as
a key enabler in our results. We will conclude with simulation results that
show that despite the generality and simplicity, the controller demonstrates
good closed-loop performance.
</p>
<a href="http://arxiv.org/abs/2010.00204">arXiv:2010.00204</a> [<a href="http://arxiv.org/pdf/2010.00204">pdf</a>]

<h2>Universal time-series forecasting with mixture predictors. (arXiv:2010.00297v1 [cs.LG])</h2>
<h3>Daniil Ryabko</h3>
<p>This book is devoted to the problem of sequential probability forecasting,
that is, predicting the probabilities of the next outcome of a growing sequence
of observations given the past. This problem is considered in a very general
setting that unifies commonly used probabilistic and non-probabilistic
settings, trying to make as few as possible assumptions on the mechanism
generating the observations. A common form that arises in various formulations
of this problem is that of mixture predictors, which are formed as a
combination of a finite or infinite set of other predictors attempting to
combine their predictive powers. The main subject of this book are such mixture
predictors, and the main results demonstrate the universality of this method in
a very general probabilistic setting, but also show some of its limitations.
While the problems considered are motivated by practical applications,
involving, for example, financial, biological or behavioural data, this
motivation is left implicit and all the results exposed are theoretical.

The book targets graduate students and researchers interested in the problem
of sequential prediction, and, more generally, in theoretical analysis of
problems in machine learning and non-parametric statistics, as well as
mathematical and philosophical foundations of these fields.

The material in this volume is presented in a way that presumes familiarity
with basic concepts of probability and statistics, up to and including
probability distributions over spaces of infinite sequences. Familiarity with
the literature on learning or stochastic processes is not required.
</p>
<a href="http://arxiv.org/abs/2010.00297">arXiv:2010.00297</a> [<a href="http://arxiv.org/pdf/2010.00297">pdf</a>]

<h2>Machine Learning at Wireless Edge with OFDM and Low Resolution ADC and DAC. (arXiv:2010.00350v1 [cs.IT])</h2>
<h3>Busra Tegin, Tolga M. Duman</h3>
<p>We study collaborative machine learning (ML) systems where a massive dataset
is distributed across independent workers which compute their local gradient
estimates based on their own datasets. Workers send their estimates through a
multipath fading multiple access channel (MAC) with orthogonal frequency
division multiplexing (OFDM) to mitigate the frequency selectivity of the
channel. We assume that the parameter server (PS) employs multiple antennas to
align the received signals with no channel state information (CSI) at the
workers. To reduce the power consumption and the hardware costs, we employ
complex-valued low resolution digital to analog converters (DACs) and analog to
digital converters (ADCs), respectively, at the transmitter and the receiver
sides to study the effects of practical low cost DACs and ADCs on the learning
performance of the system. Our theoretical analysis shows that the impairments
caused by low-resolution DACs and ADCs, including the extreme case of one-bit
DACs and ADCs, do not prevent the convergence of the learning algorithm, and
the multipath channel effects vanish when a sufficient number of antennas are
used at the PS. We also validate our theoretical results via simulations, and
demonstrate that using low-resolution, even one-bit, DACs and ADCs causes only
a slight decrease in the learning accuracy.
</p>
<a href="http://arxiv.org/abs/2010.00350">arXiv:2010.00350</a> [<a href="http://arxiv.org/pdf/2010.00350">pdf</a>]

<h2>Low-Rank and Sparse Enhanced Tucker Decomposition for Tensor Completion. (arXiv:2010.00359v1 [cs.LG])</h2>
<h3>Chenjian Pan, Chen Ling, Hongjin He, Liqun Qi, Yanwei Xu</h3>
<p>Tensor completion refers to the task of estimating the missing data from an
incomplete measurement or observation, which is a core problem frequently
arising from the areas of big data analysis, computer vision, and network
engineering. Due to the multidimensional nature of high-order tensors, the
matrix approaches, e.g., matrix factorization and direct matricization of
tensors, are often not ideal for tensor completion and recovery. Exploiting the
potential periodicity and inherent correlation properties appeared in
real-world tensor data, in this paper, we shall incorporate the low-rank and
sparse regularization technique to enhance Tucker decomposition for tensor
completion. A series of computational experiments on real-world datasets,
including internet traffic data, color images, and face recognition, show that
our model performs better than many existing state-of-the-art matricization and
tensorization approaches in terms of achieving higher recovery accuracy.
</p>
<a href="http://arxiv.org/abs/2010.00359">arXiv:2010.00359</a> [<a href="http://arxiv.org/pdf/2010.00359">pdf</a>]

<h2>Understanding the Role of Momentum in Non-Convex Optimization: Practical Insights from a Lyapunov Analysis. (arXiv:2010.00406v1 [cs.LG])</h2>
<h3>Aaron Defazio</h3>
<p>Momentum methods are now used pervasively within the machine learning
community for training non-convex models such as deep neural networks.
Empirically, they out perform traditional stochastic gradient descent (SGD)
approaches. In this work we develop a Lyapunov analysis of SGD with momentum
(SGD+M), by utilizing a equivalent rewriting of the method known as the
stochastic primal averaging (SPA) form. This analysis is much tighter than
previous theory in the non-convex case, and due to this we are able to give
precise insights into when SGD+M may out-perform SGD, and what hyper-parameter
schedules will work and why.
</p>
<a href="http://arxiv.org/abs/2010.00406">arXiv:2010.00406</a> [<a href="http://arxiv.org/pdf/2010.00406">pdf</a>]

<h2>Understanding the Role of Adversarial Regularization in Supervised Learning. (arXiv:2010.00522v1 [cs.LG])</h2>
<h3>Litu Rout</h3>
<p>Despite numerous attempts sought to provide empirical evidence of adversarial
regularization outperforming sole supervision, the theoretical understanding of
such phenomena remains elusive. In this study, we aim to resolve whether
adversarial regularization indeed performs better than sole supervision at a
fundamental level. To bring this insight into fruition, we study vanishing
gradient issue, asymptotic iteration complexity, gradient flow and provable
convergence in the context of sole supervision and adversarial regularization.
The key ingredient is a theoretical justification supported by empirical
evidence of adversarial acceleration in gradient descent. In addition,
motivated by a recently introduced unit-wise capacity based generalization
bound, we analyze the generalization error in adversarial framework. Guided by
our observation, we cast doubts on the ability of this measure to explain
generalization. We therefore leave as open questions to explore new measures
that can explain generalization behavior in adversarial learning. Furthermore,
we observe an intriguing phenomenon in the neural embedded vector space while
contrasting adversarial learning with sole supervision.
</p>
<a href="http://arxiv.org/abs/2010.00522">arXiv:2010.00522</a> [<a href="http://arxiv.org/pdf/2010.00522">pdf</a>]

<h2>Persistent homology advances interpretable machine learning for nanoporous materials. (arXiv:2010.00532v1 [cond-mat.mtrl-sci])</h2>
<h3>Aditi S. Krishnapriyan, Joseph Montoya, Jens Hummelsh&#xf8;j, Dmitriy Morozov</h3>
<p>Machine learning for nanoporous materials design and discovery has emerged as
a promising alternative to more time-consuming experiments and simulations. The
challenge with this approach is the selection of features that enable universal
and interpretable materials representations across multiple prediction tasks.
We use persistent homology to construct holistic representations of the
materials structure. We show that these representations can also be augmented
with other generic features such as word embeddings from natural language
processing to capture chemical information. We demonstrate our approach on
multiple metal-organic framework datasets by predicting a variety of gas
adsorption targets. Our results show considerable improvement in both accuracy
and transferability across targets compared to models constructed from commonly
used manually curated features. Persistent homology features allow us to locate
the pores that correlate best to adsorption at different pressures,
contributing to understanding atomic level structure-property relationships for
materials design.
</p>
<a href="http://arxiv.org/abs/2010.00532">arXiv:2010.00532</a> [<a href="http://arxiv.org/pdf/2010.00532">pdf</a>]

<h2>Agnostic Learning of Halfspaces with Gradient Descent via Soft Margins. (arXiv:2010.00539v1 [cs.LG])</h2>
<h3>Spencer Frei, Yuan Cao, Quanquan Gu</h3>
<p>We analyze the properties of gradient descent on convex surrogates for the
zero-one loss for the agnostic learning of linear halfspaces. If $\mathsf{OPT}$
is the best classification error achieved by a halfspace, by appealing to the
notion of soft margins we are able to show that gradient descent finds
halfspaces with classification error $\tilde O(\mathsf{OPT}^{1/2}) +
\varepsilon$ in $\mathrm{poly}(d,1/\varepsilon)$ time and sample complexity for
a broad class of distributions that includes log-concave isotropic
distributions as a subclass. Along the way we answer a question recently posed
by Ji et al. (2020) on how the tail behavior of a loss function can affect
sample complexity and runtime guarantees for gradient descent.
</p>
<a href="http://arxiv.org/abs/2010.00539">arXiv:2010.00539</a> [<a href="http://arxiv.org/pdf/2010.00539">pdf</a>]

<h2>Minimax Optimal Reinforcement Learning for Discounted MDPs. (arXiv:2010.00587v1 [cs.LG])</h2>
<h3>Jiafan He, Dongruo Zhou, Quanquan Gu</h3>
<p>We study the reinforcement learning problem for discounted Markov Decision
Processes (MDPs) in the tabular setting. We propose a model-based algorithm
named UCBVI-$\gamma$, which is based on the optimism in the face of uncertainty
principle and the Bernstein-type bonus. It achieves
$\tilde{O}\big({\sqrt{SAT}}/{(1-\gamma)^{1.5}}\big)$ regret, where $S$ is the
number of states, $A$ is the number of actions, $\gamma$ is the discount factor
and $T$ is the number of steps. In addition, we construct a class of hard MDPs
and show that for any algorithm, the expected regret is at least
$\tilde{\Omega}\big({\sqrt{SAT}}/{(1-\gamma)^{1.5}}\big)$. Our upper bound
matches the minimax lower bound up to logarithmic factors, which suggests that
UCBVI-$\gamma$ is near optimal for discounted MDPs.
</p>
<a href="http://arxiv.org/abs/2010.00587">arXiv:2010.00587</a> [<a href="http://arxiv.org/pdf/2010.00587">pdf</a>]

<h2>Inference under Information Constraints I: Lower Bounds from Chi-Square Contraction. (arXiv:1812.11476v4 [cs.DS] UPDATED)</h2>
<h3>Jayadev Acharya, Cl&#xe9;ment L. Canonne, Himanshu Tyagi</h3>
<p>Multiple players are each given one independent sample, about which they can
only provide limited information to a central referee. Each player is allowed
to describe its observed sample to the referee using a channel from a family of
channels $\mathcal{W}$, which can be instantiated to capture both the
communication- and privacy-constrained settings and beyond. The referee uses
the messages from players to solve an inference problem for the unknown
distribution that generated the samples. We derive lower bounds for sample
complexity of learning and testing discrete distributions in this
information-constrained setting.

Underlying our bounds is a characterization of the contraction in chi-square
distances between the observed distributions of the samples when information
constraints are placed. This contraction is captured in a local neighborhood in
terms of chi-square and decoupled chi-square fluctuations of a given channel,
two quantities we introduce. The former captures the average distance between
distributions of channel output for two product distributions on the input, and
the latter for a product distribution and a mixture of product distribution on
the input. Our bounds are tight for both public- and private-coin protocols.
Interestingly, the sample complexity of testing is order-wise higher when
restricted to private-coin protocols.
</p>
<a href="http://arxiv.org/abs/1812.11476">arXiv:1812.11476</a> [<a href="http://arxiv.org/pdf/1812.11476">pdf</a>]

<h2>Inference under Information Constraints II: Communication Constraints and Shared Randomness. (arXiv:1905.08302v2 [cs.DS] UPDATED)</h2>
<h3>Jayadev Acharya, Cl&#xe9;ment L. Canonne, Himanshu Tyagi</h3>
<p>A central server needs to perform statistical inference based on samples that
are distributed over multiple users who can each send a message of limited
length to the center. We study problems of distribution learning and identity
testing in this distributed inference setting and examine the role of shared
randomness as a resource. We propose a general-purpose simulate-and-infer
strategy that uses only private-coin communication protocols and is
sample-optimal for distribution learning. This general strategy turns out to be
sample-optimal even for distribution testing among private-coin protocols.
Interestingly, we propose a public-coin protocol that outperforms
simulate-and-infer for distribution testing and is, in fact, sample-optimal.
Underlying our public-coin protocol is a random hash that when applied to the
samples minimally contracts the chi-squared distance of their distribution to
the uniform distribution.
</p>
<a href="http://arxiv.org/abs/1905.08302">arXiv:1905.08302</a> [<a href="http://arxiv.org/pdf/1905.08302">pdf</a>]

<h2>Learning to Discretize: Solving 1D Scalar Conservation Laws via Deep Reinforcement Learning. (arXiv:1905.11079v3 [cs.LG] UPDATED)</h2>
<h3>Yufei Wang, Ziju Shen, Zichao Long, Bin Dong</h3>
<p>Conservation laws are considered to be fundamental laws of nature. It has
broad applications in many fields, including physics, chemistry, biology,
geology, and engineering. Solving the differential equations associated with
conservation laws is a major branch in computational mathematics. The recent
success of machine learning, especially deep learning in areas such as computer
vision and natural language processing, has attracted a lot of attention from
the community of computational mathematics and inspired many intriguing works
in combining machine learning with traditional methods. In this paper, we are
the first to view numerical PDE solvers as an MDP and to use (deep) RL to learn
new solvers. As proof of concept, we focus on 1-dimensional scalar conservation
laws. We deploy the machinery of deep reinforcement learning to train a policy
network that can decide on how the numerical solutions should be approximated
in a sequential and spatial-temporal adaptive manner. We will show that the
problem of solving conservation laws can be naturally viewed as a sequential
decision-making process, and the numerical schemes learned in such a way can
easily enforce long-term accuracy. Furthermore, the learned policy network is
carefully designed to determine a good local discrete approximation based on
the current state of the solution, which essentially makes the proposed method
a meta-learning approach. In other words, the proposed method is capable of
learning how to discretize for a given situation mimicking human experts.
Finally, we will provide details on how the policy network is trained, how well
it performs compared with some state-of-the-art numerical solvers such as WENO
schemes, and supervised learning based approach L3D and PINN, and how well it
generalizes.
</p>
<a href="http://arxiv.org/abs/1905.11079">arXiv:1905.11079</a> [<a href="http://arxiv.org/pdf/1905.11079">pdf</a>]

<h2>Asymptotic Guarantees for Generative Modeling based on the Smooth Wasserstein Distance. (arXiv:2002.01012v2 [math.ST] UPDATED)</h2>
<h3>Ziv Goldfeld, Kengo Kato, Kristjan Greenewald</h3>
<p>Minimum distance estimation (MDE) gained recent attention as a formulation of
(implicit) generative modeling. It considers minimizing, over model parameters,
a statistical distance between the empirical data distribution and the model.
This formulation lends itself well to theoretical analysis, but typical results
are hindered by the curse of dimensionality. To overcome this and devise a
scalable finite-sample statistical MDE theory, we adopt the framework of smooth
1-Wasserstein distance (SWD) $\mathsf{W}_1^{(\sigma)}$. The SWD was recently
shown to preserve the metric and topological structure of classic Wasserstein
distances, while enjoying dimension-free empirical convergence rates. In this
work, we conduct a thorough statistical study of the minimum smooth Wasserstein
estimators (MSWEs), first proving the estimator's measurability and asymptotic
consistency. We then characterize the limit distribution of the optimal model
parameters and their associated minimal SWD. These results imply an
$O(n^{-1/2})$ generalization bound for generative modeling based on MSWE, which
holds in arbitrary dimension. Our main technical tool is a novel
high-dimensional limit distribution result for empirical
$\mathsf{W}_1^{(\sigma)}$. The characterization of a nondegenerate limit stands
in sharp contrast with the classic unsmooth empirical 1-Wasserstein distance,
for which a similar result is known only in the one-dimensional case. The
validity of our theory is supported by empirical results, posing the SWD as a
potent tool for learning and inference in high dimensions.
</p>
<a href="http://arxiv.org/abs/2002.01012">arXiv:2002.01012</a> [<a href="http://arxiv.org/pdf/2002.01012">pdf</a>]

<h2>Learning Min-norm Stabilizing Control Laws for Systems with Unknown Dynamics. (arXiv:2004.10331v2 [math.OC] UPDATED)</h2>
<h3>Tyler Westenbroek, Fernando Castaneda, Ayush Agrawal, S. Shankar Sastry, Koushil Sreenath</h3>
<p>This paper introduces a framework for learning a minimum-norm stabilizing
controller for a system with unknown dynamics using model-free policy
optimization methods. The approach begins by first designing a Control Lyapunov
Function (CLF) for a (possibly inaccurate) dynamics model for the system, along
with a function which specifies a minimum acceptable rate of energy dissipation
for the CLF at different points in the state-space. Treating the energy
dissipation condition as a constraint on the desired closed-loop behavior of
the real-world system, we use penalty methods to formulate an unconstrained
optimization problem over the parameters of a learned controller, which can be
solved using model-free policy optimization algorithms using data collected
from the plant. We discuss when the optimization learns a stabilizing
controller for the real world system and derive conditions on the structure of
the learned controller which ensure that the optimization is strongly convex,
meaning the globally optimal solution can be found reliably. We validate the
approach in simulation, first for a double pendulum, and then generalize the
framework to learn stable walking controllers for underactuated bipedal robots
using the Hybrid Zero Dynamics framework. By encoding a large amount of
structure into the learning problem, we are able to learn stabilizing
controllers for both systems with only minutes or even seconds of training
data.
</p>
<a href="http://arxiv.org/abs/2004.10331">arXiv:2004.10331</a> [<a href="http://arxiv.org/pdf/2004.10331">pdf</a>]

<h2>Understanding the dynamics emerging from infodemics: A call to action for interdisciplinary research. (arXiv:2007.12226v2 [physics.soc-ph] UPDATED)</h2>
<h3>Stephan Leitner, Bartosz Gula, Dietmar Jannach, Ulrike Krieg-Holz, Friederike Wall</h3>
<p>Research on infodemics, i.e., the rapid spread of (mis)information related to
a hazardous event, such as the COVID-19 pandemic, requires the integration of a
multiplicity of scientific disciplines. The dynamics emerging from infodemics
have the potential to generate complex behavioral patterns. In order to react
appropriately, it is of ultimate importance for the fields of Business and
Economics to understand the dynamics emerging from it. In the short run,
dynamics might lead to an adaptation in household spending or to a shift in
buying behavior towards online providers. In the long run, changes in
investments, consumer behavior, and markets are to be expected. We argue that
the dynamics emerge from complex interactions among multiple factors, such as
information and misinformation accessible for individuals and the formation and
revision of beliefs. (Mis)information accessible to individuals is, amongst
others, affected by algorithms specifically designed to provide personalized
information, while automated fact-checking algorithms can help reduce the
amount of circulating misinformation. The formation and revision of individual
(and probably false) beliefs and individual fact-checking and interpretation of
information are heavily affected by linguistic patterns inherent to information
during pandemics and infodemics and further factors, such as affect, intuition
and motives. We argue that, in order to get a deep(er) understanding of the
dynamics emerging from infodemics, the fields of Business and Economics should
integrate the perspectives of Computer Science and Information Systems,
(Computational) Linguistics, and Cognitive Science into the wider context of
economic systems (e.g., organizations, markets or industries) and propose a way
to do so.
</p>
<a href="http://arxiv.org/abs/2007.12226">arXiv:2007.12226</a> [<a href="http://arxiv.org/pdf/2007.12226">pdf</a>]

<h2>Learning-Based Distributionally Robust Model Predictive Control of Markovian Switching Systems with Guaranteed Stability and Recursive Feasibility. (arXiv:2009.04422v2 [math.OC] UPDATED)</h2>
<h3>Mathijs Schuurmans, Panagiotis Patrinos</h3>
<p>We present a data-driven model predictive control scheme for
chance-constrained Markovian switching systems with unknown switching
probabilities. Using samples of the underlying Markov chain, ambiguity sets of
transition probabilities are estimated which include the true conditional
probability distributions with high probability. These sets are updated online
and used to formulate a time-varying, risk-averse optimal control problem. We
prove recursive feasibility of the resulting MPC scheme and show that the
original chance constraints remain satisfied at every time step. Furthermore,
we show that under sufficient decrease of the confidence levels, the resulting
MPC scheme renders the closed-loop system mean-square stable with respect to
the true-but-unknown distributions, while remaining less conservative than a
fully robust approach.
</p>
<a href="http://arxiv.org/abs/2009.04422">arXiv:2009.04422</a> [<a href="http://arxiv.org/pdf/2009.04422">pdf</a>]

<h2>Weber's class number problem and $p$-rationality in the cyclotomic $\widehat{\mathbb{Z}}$-extension of $\mathbb{Q}$. (arXiv:2009.05278v2 [math.NT] UPDATED)</h2>
<h3>Georges Gras (LMB)</h3>
<p>Let $K:=\mathbb{Q}(\ell^n)$, $n \geq 0$, be the $n$th layer in the cyclotomic
$\mathbb{Z}_\ell$-extension of $\mathbb{Q}$. It is conjectured that, for all
$\ell$ and $n$, $K$ is principal (especially for $\ell=2$, a conjecture due to
Weber). Many studies (Ichimura--Morisawa--Nakajima--Okazaki$\,\ldots$) go in
this direction, as the Miller use of the Cohen--Lenstra--Martinet heuristics.
Nevertheless, we examine in what circumstances a counterexample may be
possible. For this, computations show that the $p$-torsion group ${\mathcal
T}_K$ of the Galois group of the maximal abelian $p$-ramified pro-$p$-extension
of $K$ is not always trivial. This questions the relevance of the conjecture
since $\# {\mathcal T}_K = \# {\mathcal C}_K \cdot \# {\mathcal R}_K \cdot \#
{\mathcal W}_K$, where ${\mathcal C}_K$ is the $p$-class group of $K$,
${\mathcal R}_K$ its normalized $p$-adic regulator, $\# {\mathcal W}_K = 1$ for
$p&gt;2$, $\# {\mathcal W}_K = 2^{\# \{v, \,v \mid 2\}-1}$ for $p=2$;
nevertheless, no counterexample has been found so far, even using the
reflection theorem giving $p$-ranks equalities between ${\mathcal C}_K$ and a
suitable component of ${\mathcal T}_{K(\mu_p)}$. When $n$ increases, some
relative components ${\mathcal T}_K^*$ may appear for large $p$. We give a
method (Theorem 4.6), for testing $\# {\mathcal T}_K \ne 1$, allowing larger
values of $\ell^n$ than those of the literature. Finally, we consider the
subfields $K$ of the composite $\widehat{\mathbb{Q}}$ of the
$\mathbb{Z}_\ell$-extension and give programs finding again some rare cases of
non-trivial class groups (Fukuda--Komatsu--Horie) due to genus theory in
connection with a deep link involving ${\mathcal R}_K$ (Theorem 6.2) in
relation with Greenberg's conjecture as initiated, via $p$-adic zeta-functions,
by Taya. In all attempts, Jaulent's logarithmic class group
$\widetilde{{\mathcal C}}_K$, $K \subset \widehat{\mathbb{Q}}$, governing
Greenberg's conjecture for $K$ and $p$, was trivial.
</p>
<a href="http://arxiv.org/abs/2009.05278">arXiv:2009.05278</a> [<a href="http://arxiv.org/pdf/2009.05278">pdf</a>]

<h2>Deep Neural Tangent Kernel and Laplace Kernel Have the Same RKHS. (arXiv:2009.10683v4 [cs.LG] UPDATED)</h2>
<h3>Lin Chen, Sheng Xu</h3>
<p>We prove that the reproducing kernel Hilbert spaces (RKHS) of a deep neural
tangent kernel and the Laplace kernel include the same set of functions, when
both kernels are restricted to the sphere $\mathbb{S}^{d-1}$. Additionally, we
prove that the exponential power kernel with a smaller power (making the kernel
more non-smooth) leads to a larger RKHS, when it is restricted to the sphere
$\mathbb{S}^{d-1}$ and when it is defined on the entire $\mathbb{R}^d$.
</p>
<a href="http://arxiv.org/abs/2009.10683">arXiv:2009.10683</a> [<a href="http://arxiv.org/pdf/2009.10683">pdf</a>]

<h2>Towards a Mathematical Understanding of Neural Network-Based Machine Learning: what we know and what we don't. (arXiv:2009.10713v2 [cs.LG] UPDATED)</h2>
<h3>Weinan E, Chao Ma, Stephan Wojtowytsch, Lei Wu</h3>
<p>The purpose of this article is to review the achievements made in the last
few years towards the understanding of the reasons behind the success and
subtleties of neural network-based machine learning. In the tradition of good
old applied mathematics, we will not only give attention to rigorous
mathematical results, but also the insight we have gained from careful
numerical experiments as well as the analysis of simplified models. Along the
way, we also list the open problems which we believe to be the most important
topics for further study. This is not a complete overview over this quickly
moving field, but we hope to provide a perspective which may be helpful
especially to new researchers in the area.
</p>
<a href="http://arxiv.org/abs/2009.10713">arXiv:2009.10713</a> [<a href="http://arxiv.org/pdf/2009.10713">pdf</a>]

<h2>Accelerating Optimization and Reinforcement Learning with Quasi-Stochastic Approximation. (arXiv:2009.14431v2 [math.OC] UPDATED)</h2>
<h3>Shuhang Chen, Adithya Devraj, Andrey Bernstein, Sean Meyn</h3>
<p>The ODE method has been a workhorse for algorithm design and analysis since
the introduction of the stochastic approximation. It is now understood that
convergence theory amounts to establishing robustness of Euler approximations
for ODEs, while theory of rates of convergence requires finer analysis. This
paper sets out to extend this theory to quasi-stochastic approximation, based
on algorithms in which the "noise" is based on deterministic signals. The main
results are obtained under minimal assumptions: the usual Lipschitz conditions
for ODE vector fields, and it is assumed that there is a well defined
linearization near the optimal parameter $\theta^*$, with Hurwitz linearization
matrix $A^*$.

The main contributions are summarized as follows:

(i) If the algorithm gain is $a_t=g/(1+t)^\rho$ with $g&gt;0$ and
$\rho\in(0,1)$, then the rate of convergence of the algorithm is $1/t^\rho$.
There is also a well defined "finite-$t$" approximation: \[
a_t^{-1}\{\Theta_t-\theta^*\}=\bar{Y}+\Xi^{\mathrm{I}}_t+o(1) \] where
$\bar{Y}\in\mathbb{R}^d$ is a vector identified in the paper, and
$\{\Xi^{\mathrm{I}}_t\}$ is bounded with zero temporal mean.

(ii) With gain $a_t = g/(1+t)$ the results are not as sharp: the rate of
convergence $1/t$ holds only if $I + g A^*$ is Hurwitz.

(iii) Based on the Ruppert-Polyak averaging of stochastic approximation, one
would expect that a convergence rate of $1/t$ can be obtained by averaging: \[
\Theta^{\text{RP}}_T=\frac{1}{T}\int_{0}^T \Theta_t\,dt \] where the estimates
$\{\Theta_t\}$ are obtained using the gain in (i). The preceding sharp bounds
imply that averaging results in $1/t$ convergence rate if and only if
$\bar{Y}=\sf 0$. This condition holds if the noise is additive, but appears to
fail in general.

(iv) The theory is illustrated with applications to gradient-free
optimization and policy gradient algorithms for reinforcement learning.
</p>
<a href="http://arxiv.org/abs/2009.14431">arXiv:2009.14431</a> [<a href="http://arxiv.org/pdf/2009.14431">pdf</a>]

<h2>First-order Optimization for Superquantile-based Supervised Learning. (arXiv:2009.14575v2 [math.OC] UPDATED)</h2>
<h3>Yassine Laguel, J&#xe9;r&#xf4;me Malick, Zaid Harchaoui</h3>
<p>Classical supervised learning via empirical risk (or negative log-likelihood)
minimization hinges upon the assumption that the testing distribution coincides
with the training distribution. This assumption can be challenged in modern
applications of machine learning in which learning machines may operate at
prediction time with testing data whose distribution departs from the one of
the training data. We revisit the superquantile regression method by proposing
a first-order optimization algorithm to minimize a superquantile-based learning
objective. The proposed algorithm is based on smoothing the superquantile
function by infimal convolution. Promising numerical results illustrate the
interest of the approach towards safer supervised learning.
</p>
<a href="http://arxiv.org/abs/2009.14575">arXiv:2009.14575</a> [<a href="http://arxiv.org/pdf/2009.14575">pdf</a>]

<h2>Spectral Decomposition in Deep Networks for Segmentation of Dynamic Medical Images. (arXiv:2010.00003v1 [physics.med-ph])</h2>
<h3>Edgar A. Rios Piedra, Morteza Mardani, Frank Ong, Ukash Nakarmi, Joseph Y. Cheng, Shreyas Vasanawala</h3>
<p>Dynamic contrast-enhanced magnetic resonance imaging (DCE- MRI) is a widely
used multi-phase technique routinely used in clinical practice. DCE and similar
datasets of dynamic medical data tend to contain redundant information on the
spatial and temporal components that may not be relevant for detection of the
object of interest and result in unnecessarily complex computer models with
long training times that may also under-perform at test time due to the
abundance of noisy heterogeneous data. This work attempts to increase the
training efficacy and performance of deep networks by determining redundant
information in the spatial and spectral components and show that the
performance of segmentation accuracy can be maintained and potentially
improved. Reported experiments include the evaluation of training/testing
efficacy on a heterogeneous dataset composed of abdominal images of pediatric
DCE patients, showing that drastic data reduction (higher than 80%) can
preserve the dynamic information and performance of the segmentation model,
while effectively suppressing noise and unwanted portion of the images.
</p>
<a href="http://arxiv.org/abs/2010.00003">arXiv:2010.00003</a> [<a href="http://arxiv.org/pdf/2010.00003">pdf</a>]

<h2>CrowdEst: A Method for Estimating (and not Simulating) Crowd Evacuation Parameters in Generic Environments. (arXiv:2010.00004v1 [cs.MA])</h2>
<h3>Estevso Testa, Rodrigo C. Barros, Soraia Raupp Musse</h3>
<p>Evacuation plans have been historically used as a safety measure for the
construction of buildings. The existing crowd simulators require fully-modeled
3D environments and enough time to prepare and simulate scenarios, where the
distribution and behavior of the crowd needs to be controlled. In addition, its
population, routes or even doors and passages may change, so the 3D model and
configurations have to be updated accordingly. This is a time-consuming task
that commonly has to be addressed within the crowd simulators. With that in
mind, we present a novel approach to estimate the resulting data of a given
evacuation scenario without actually simulating it. For such, we divide the
environment into smaller modular rooms with different configurations, in a
divide-and-conquer fashion. Next, we train an artificial neural network to
estimate all required data regarding the evacuation of a single room. After
collecting the estimated data from each room, we develop a heuristic capable of
aggregating per-room information so the full environment can be properly
evaluated. Our method presents an average error of 5% when compared to
evacuation time in a real-life environment. Our crowd estimator approach has
several advantages, such as not requiring to model the 3D environment, nor
learning how to use and configure a crowd simulator, which means any user can
easily use it. Furthermore, the computational time to estimate evacuation data
(inference time) is virtually zero, which is much better even when compared to
the best-case scenario in a real-time crowd simulator.
</p>
<a href="http://arxiv.org/abs/2010.00004">arXiv:2010.00004</a> [<a href="http://arxiv.org/pdf/2010.00004">pdf</a>]

<h2>RG-Flow: A hierarchical and explainable flow model based on renormalization group and sparse prior. (arXiv:2010.00029v1 [cs.LG])</h2>
<h3>Hong-Ye Hu, Dian Wu, Yi-Zhuang You, Bruno Olshausen, Yubei Chen</h3>
<p>Flow-based generative models have become an important class of unsupervised
learning approaches. In this work, we incorporate the key idea of
renormalization group (RG) and sparse prior distribution to design a
hierarchical flow-based generative model, called RG-Flow, which can separate
different scale information of images with disentangle representations at each
scale. We demonstrate our method mainly on the CelebA dataset and show that the
disentangled representation at different scales enables semantic manipulation
and style mixing of the images. To visualize the latent representation, we
introduce the receptive fields for flow-based models and find receptive fields
learned by RG-Flow are similar to convolutional neural networks. In addition,
we replace the widely adopted Gaussian prior distribution by sparse prior
distributions to further enhance the disentanglement of representations. From a
theoretical perspective, the proposed method has $O(\log L)$ complexity for
image inpainting compared to previous flow-based models with $O(L^2)$
complexity.
</p>
<a href="http://arxiv.org/abs/2010.00029">arXiv:2010.00029</a> [<a href="http://arxiv.org/pdf/2010.00029">pdf</a>]

<h2>A Supervised Machine Learning Approach for Accelerating the Design of Particulate Composites: Application to Thermal Conductivity. (arXiv:2010.00041v1 [physics.comp-ph])</h2>
<h3>Mohammad Saber Hashemi, Masoud Safdari, Azadeh Sheidaei</h3>
<p>In this paper, we present a supervised machine learning (ML) based
computational framework for designing particulate multifunctional composite
materials for desired thermal conductivity (TC). In this framework, the design
variables are physical descriptors of the material microstructure to link
microstructure to properties for material design. The design of experiment
(DoE) based on Sobol sequence was utilized to generate a sufficiently large
database for training ML models accurately. Microstructures were realized
through an efficient, fast packing algorithm, and the TC of microstructures
were obtained using our previous Fast Fourier Transform (FFT) homogenization
method. Thereafter, the ML methods constituting a reduced-order model (ROM) was
trained over the generated database to establish the complex relationship
between the structure and properties. Finally, the ROM is used for material
design through an optimization algorithm to solve the inverse problem of
finding the material with desired properties represented by its physical
descriptors. The results showed that the surrogate model is accurate in
predicting the behavior of microstructure with respect to high-fidelity FFT
simulations, and inverse design is robust in finding microstructure parameters
according to case studies.
</p>
<a href="http://arxiv.org/abs/2010.00041">arXiv:2010.00041</a> [<a href="http://arxiv.org/pdf/2010.00041">pdf</a>]

<h2>Sampling possible reconstructions of undersampled acquisitions in MR imaging. (arXiv:2010.00042v1 [eess.IV])</h2>
<h3>Kerem C. Tezcan, Christian F. Baumgartner, Ender Konukoglu</h3>
<p>Undersampling the k-space during MR acquisitions saves time, however results
in an ill-posed inversion problem, leading to an infinite set of images as
possible solutions. Traditionally, this is tackled as a reconstruction problem
by searching for a single "best" image out of this solution set according to
some chosen regularization or prior. This approach, however, misses the
possibility of other solutions and hence ignores the uncertainty in the
inversion process. In this paper, we propose a method that instead returns
multiple images which are possible under the acquisition model and the chosen
prior. To this end, we introduce a low dimensional latent space and model the
posterior distribution of the latent vectors given the acquisition data in
k-space, from which we can sample in the latent space and obtain the
corresponding images. We use a variational autoencoder for the latent model and
the Metropolis adjusted Langevin algorithm for the sampling. This approach
allows us to obtain multiple possible images and capture the uncertainty in the
inversion process under the used prior. We evaluate our method on images from
the Human Connectome Project dataset as well as in-house measured multi-coil
images and compare to two different methods. The results indicate that the
proposed method is capable of producing images that match the ground truth in
regions where acquired k-space data is informative and construct different
possible reconstructions, which show realistic structural variations, in
regions where acquired k-space data is not informative.

Keywords: Magnetic Resonance image reconstruction, uncertainty estimation,
inverse problems, sampling, MCMC, deep learning, unsupervised learning.
</p>
<a href="http://arxiv.org/abs/2010.00042">arXiv:2010.00042</a> [<a href="http://arxiv.org/pdf/2010.00042">pdf</a>]

<h2>Creative Captioning: An AI Grand Challenge Based on the Dixit Board Game. (arXiv:2010.00048v1 [cs.AI])</h2>
<h3>Maithilee Kunda, Irina Rabkina</h3>
<p>We propose a new class of "grand challenge" AI problems that we call creative
captioning---generating clever, interesting, or abstract captions for images,
as well as understanding such captions. Creative captioning draws on core AI
research areas of vision, natural language processing, narrative reasoning, and
social reasoning, and across all these areas, it requires sophisticated uses of
common sense and cultural knowledge. In this paper, we analyze several specific
research problems that fall under creative captioning, using the popular board
game Dixit as both inspiration and proposed testing ground. We expect that
Dixit could serve as an engaging and motivating benchmark for creative
captioning across numerous AI research communities for the coming 1-2 decades.
</p>
<a href="http://arxiv.org/abs/2010.00048">arXiv:2010.00048</a> [<a href="http://arxiv.org/pdf/2010.00048">pdf</a>]

<h2>Depth Estimation from Monocular Images and Sparse Radar Data. (arXiv:2010.00058v1 [cs.CV])</h2>
<h3>Juan-Ting Lin, Dengxin Dai, Luc Van Gool</h3>
<p>In this paper, we explore the possibility of achieving a more accurate depth
estimation by fusing monocular images and Radar points using a deep neural
network. We give a comprehensive study of the fusion between RGB images and
Radar measurements from different aspects and proposed a working solution based
on the observations. We find that the noise existing in Radar measurements is
one of the main key reasons that prevents one from applying the existing fusion
methods developed for LiDAR data and images to the new fusion problem between
Radar data and images. The experiments are conducted on the nuScenes dataset,
which is one of the first datasets which features Camera, Radar, and LiDAR
recordings in diverse scenes and weather conditions. Extensive experiments
demonstrate that our method outperforms existing fusion methods. We also
provide detailed ablation studies to show the effectiveness of each component
in our method.
</p>
<a href="http://arxiv.org/abs/2010.00058">arXiv:2010.00058</a> [<a href="http://arxiv.org/pdf/2010.00058">pdf</a>]

<h2>Light Field Compression by Residual CNN Assisted JPEG. (arXiv:2010.00062v1 [eess.IV])</h2>
<h3>Eisa Hedayati, Timothy C. Havens, Jeremy P. Bos</h3>
<p>Light field (LF) imaging has gained significant attention due to its recent
success in 3-dimensional (3D) displaying and rendering as well as augmented and
virtual reality usage. Nonetheless, because of the two extra dimensions, LFs
are much larger than conventional images. We develop a JPEG-assisted
learning-based technique to reconstruct an LF from a JPEG bitstream with a bit
per pixel ratio of 0.0047 on average. For compression, we keep the LF's center
view and use JPEG compression with 50\% quality. Our reconstruction pipeline
consists of a small JPEG enhancement network (JPEG-Hance), a depth estimation
network (Depth-Net), followed by view synthesizing by warping the enhanced
center view. Our pipeline is significantly faster than using video compression
on pseudo-sequences extracted from an LF, both in compression and
decompression, while maintaining effective performance. We show that with a 1\%
compression time cost and 18x speedup for decompression, our methods
reconstructed LFs have better structural similarity index metric (SSIM) and
comparable peak signal-to-noise ratio (PSNR) compared to the state-of-the-art
video compression techniques used to compress LFs.
</p>
<a href="http://arxiv.org/abs/2010.00062">arXiv:2010.00062</a> [<a href="http://arxiv.org/pdf/2010.00062">pdf</a>]

<h2>GCNNMatch: Graph Convolutional Neural Networks for Multi-Object Tracking via Sinkhorn Normalization. (arXiv:2010.00067v1 [cs.CV])</h2>
<h3>Ioannis Papakis, Abhijit Sarkar, Anuj Karpatne</h3>
<p>This paper proposes a novel method for online Multi-Object Tracking (MOT)
using Graph Convolutional Neural Network (GCNN) based feature extraction and
end-to-end feature matching for object association. The Graph based approach
incorporates both appearance and geometry of objects at past frames as well as
the current frame into the task of feature learning. This new paradigm enables
the network to leverage the "context" information of the geometry of objects
and allows us to model the interactions among the features of multiple objects.
Another central innovation of our proposed framework is the use of the Sinkhorn
algorithm for end-to-end learning of the associations among objects during
model training. The network is trained to predict object associations by taking
into account constraints specific to the MOT task. Experimental results
demonstrate the efficacy of the proposed approach in achieving top performance
on the MOT16 &amp; 17 Challenge problems among state-of-the-art online and
supervised approaches.
</p>
<a href="http://arxiv.org/abs/2010.00067">arXiv:2010.00067</a> [<a href="http://arxiv.org/pdf/2010.00067">pdf</a>]

<h2>Using Machine Learning to Augment Coarse-Grid Computational Fluid Dynamics Simulations. (arXiv:2010.00072v1 [physics.comp-ph])</h2>
<h3>Jaideep Pathak, Mustafa Mustafa, Karthik Kashinath, Emmanuel Motheau, Thorsten Kurth, Marcus Day</h3>
<p>Simulation of turbulent flows at high Reynolds number is a computationally
challenging task relevant to a large number of engineering and scientific
applications in diverse fields such as climate science, aerodynamics, and
combustion. Turbulent flows are typically modeled by the Navier-Stokes
equations. Direct Numerical Simulation (DNS) of the Navier-Stokes equations
with sufficient numerical resolution to capture all the relevant scales of the
turbulent motions can be prohibitively expensive. Simulation at
lower-resolution on a coarse-grid introduces significant errors. We introduce a
machine learning (ML) technique based on a deep neural network architecture
that corrects the numerical errors induced by a coarse-grid simulation of
turbulent flows at high-Reynolds numbers, while simultaneously recovering an
estimate of the high-resolution fields. Our proposed simulation strategy is a
hybrid ML-PDE solver that is capable of obtaining a meaningful high-resolution
solution trajectory while solving the system PDE at a lower resolution. The
approach has the potential to dramatically reduce the expense of turbulent flow
simulations. As a proof-of-concept, we demonstrate our ML-PDE strategy on a
two-dimensional turbulent (Rayleigh Number $Ra=10^9$) Rayleigh-B\'enard
Convection (RBC) problem.
</p>
<a href="http://arxiv.org/abs/2010.00072">arXiv:2010.00072</a> [<a href="http://arxiv.org/pdf/2010.00072">pdf</a>]

<h2>Stage-wise Conservative Linear Bandits. (arXiv:2010.00081v1 [cs.LG])</h2>
<h3>Ahmadreza Moradipari, Christos Thrampoulidis, Mahnoosh Alizadeh</h3>
<p>We study stage-wise conservative linear stochastic bandits: an instance of
bandit optimization, which accounts for (unknown) safety constraints that
appear in applications such as online advertising and medical trials. At each
stage, the learner must choose actions that not only maximize cumulative reward
across the entire time horizon but further satisfy a linear baseline constraint
that takes the form of a lower bound on the instantaneous reward. For this
problem, we present two novel algorithms, stage-wise conservative linear
Thompson Sampling (SCLTS) and stage-wise conservative linear UCB (SCLUCB), that
respect the baseline constraints and enjoy probabilistic regret bounds of order
O(\sqrt{T} \log^{3/2}T) and O(\sqrt{T} \log T), respectively. Notably, the
proposed algorithms can be adjusted with only minor modifications to tackle
different problem variations, such as constraints with bandit-feedback, or an
unknown sequence of baseline actions. We discuss these and other improvements
over the state-of-the-art. For instance, compared to existing solutions, we
show that SCLTS plays the (non-optimal) baseline action at most O(\log{T})
times (compared to O(\sqrt{T})). Finally, we make connections to another
studied form of safety constraints that takes the form of an upper bound on the
instantaneous reward. While this incurs additional complexity to the learning
process as the optimal action is not guaranteed to belong to the safe set at
each round, we show that SCLUCB can properly adjust in this setting via a
simple modification.
</p>
<a href="http://arxiv.org/abs/2010.00081">arXiv:2010.00081</a> [<a href="http://arxiv.org/pdf/2010.00081">pdf</a>]

<h2>Metrics for Benchmarking and Uncertainty Quantification: Quality, Applicability, and a Path to Best Practices for Machine Learning in Chemistry. (arXiv:2010.00110v1 [physics.chem-ph])</h2>
<h3>Gaurav Vishwakarma, Aditya Sonpal, Johannes Hachmann</h3>
<p>This review aims to draw attention to two issues of concern when we set out
to make machine learning work in the chemical and materials domain, i.e.,
statistical loss function metrics for the validation and benchmarking of
data-derived models, and the uncertainty quantification of predictions made by
them. They are often overlooked or underappreciated topics as chemists
typically only have limited training in statistics. Aside from helping to
assess the quality, reliability, and applicability of a given model, these
metrics are also key to comparing the performance of different models and thus
for developing guidelines and best practices for the successful application of
machine learning in chemistry.
</p>
<a href="http://arxiv.org/abs/2010.00110">arXiv:2010.00110</a> [<a href="http://arxiv.org/pdf/2010.00110">pdf</a>]

<h2>MaterialGAN: Reflectance Capture using a Generative SVBRDF Model. (arXiv:2010.00114v1 [cs.CV])</h2>
<h3>Yu Guo, Cameron Smith, Milo&#x161; Ha&#x161;an, Kalyan Sunkavalli, Shuang Zhao</h3>
<p>We address the problem of reconstructing spatially-varying BRDFs from a small
set of image measurements. This is a fundamentally under-constrained problem,
and previous work has relied on using various regularization priors or on
capturing many images to produce plausible results. In this work, we present
MaterialGAN, a deep generative convolutional network based on StyleGAN2,
trained to synthesize realistic SVBRDF parameter maps. We show that MaterialGAN
can be used as a powerful material prior in an inverse rendering framework: we
optimize in its latent representation to generate material maps that match the
appearance of the captured images when rendered. We demonstrate this framework
on the task of reconstructing SVBRDFs from images captured under flash
illumination using a hand-held mobile phone. Our method succeeds in producing
plausible material maps that accurately reproduce the target images, and
outperforms previous state-of-the-art material capture methods in evaluations
on both synthetic and real data. Furthermore, our GAN-based latent space allows
for high-level semantic material editing operations such as generating material
variations and material morphing.
</p>
<a href="http://arxiv.org/abs/2010.00114">arXiv:2010.00114</a> [<a href="http://arxiv.org/pdf/2010.00114">pdf</a>]

<h2>Multi-document Summarization with Maximal Marginal Relevance-guided Reinforcement Learning. (arXiv:2010.00117v1 [cs.CL])</h2>
<h3>Yuning Mao, Yanru Qu, Yiqing Xie, Xiang Ren, Jiawei Han</h3>
<p>While neural sequence learning methods have made significant progress in
single-document summarization (SDS), they produce unsatisfactory results on
multi-document summarization (MDS). We observe two major challenges when
adapting SDS advances to MDS: (1) MDS involves larger search space and yet more
limited training data, setting obstacles for neural methods to learn adequate
representations; (2) MDS needs to resolve higher information redundancy among
the source documents, which SDS methods are less effective to handle. To close
the gap, we present RL-MMR, Maximal Margin Relevance-guided Reinforcement
Learning for MDS, which unifies advanced neural SDS methods and statistical
measures used in classical MDS. RL-MMR casts MMR guidance on fewer promising
candidates, which restrains the search space and thus leads to better
representation learning. Additionally, the explicit redundancy measure in MMR
helps the neural representation of the summary to better capture redundancy.
Extensive experiments demonstrate that RL-MMR achieves state-of-the-art
performance on benchmark MDS datasets. In particular, we show the benefits of
incorporating MMR into end-to-end learning when adapting SDS to MDS in terms of
both learning effectiveness and efficiency.
</p>
<a href="http://arxiv.org/abs/2010.00117">arXiv:2010.00117</a> [<a href="http://arxiv.org/pdf/2010.00117">pdf</a>]

<h2>Self-Guided Multiple Instance Learning for Weakly Supervised Disease Classification and Localization in Chest Radiographs. (arXiv:2010.00127v1 [cs.CV])</h2>
<h3>Constantin Seibold, Jens Kleesiek, Heinz-Peter Schlemmer, Rainer Stiefelhagen</h3>
<p>The lack of fine-grained annotations hinders the deployment of automated
diagnosis systems, which require human-interpretable justification for their
decision process. In this paper, we address the problem of weakly supervised
identification and localization of abnormalities in chest radiographs. To that
end, we introduce a novel loss function for training convolutional neural
networks increasing the \emph{localization confidence} and assisting the
overall \emph{disease identification}. The loss leverages both image- and
patch-level predictions to generate auxiliary supervision. Rather than forming
strictly binary from the predictions as done in previous loss formulations, we
create targets in a more customized manner, which allows the loss to account
for possible misclassification. We show that the supervision provided within
the proposed learning scheme leads to better performance and more precise
predictions on prevalent datasets for multiple-instance learning as well as on
the NIH~ChestX-Ray14 benchmark for disease recognition than previously used
losses.
</p>
<a href="http://arxiv.org/abs/2010.00127">arXiv:2010.00127</a> [<a href="http://arxiv.org/pdf/2010.00127">pdf</a>]

<h2>Computing Graph Neural Networks: A Survey from Algorithms to Accelerators. (arXiv:2010.00130v1 [cs.LG])</h2>
<h3>Sergi Abadal, Akshay Jain, Robert Guirado, Jorge L&#xf3;pez-Alonso, Eduard Alarc&#xf3;n</h3>
<p>Graph Neural Networks (GNNs) have exploded onto the machine learning scene in
recent years owing to their capability to model and learn from graph-structured
data. Such an ability has strong implications in a wide variety of fields whose
data is inherently relational, for which conventional neural networks do not
perform well. Indeed, as recent reviews can attest, research in the area of
GNNs has grown rapidly and has lead to the development of a variety of GNN
algorithm variants as well as to the exploration of groundbreaking applications
in chemistry, neurology, electronics, or communication networks, among others.
At the current stage of research, however, the efficient processing of GNNs is
still an open challenge for several reasons. Besides of their novelty, GNNs are
hard to compute due to their dependence on the input graph, their combination
of dense and very sparse operations, or the need to scale to huge graphs in
some applications. In this context, this paper aims to make two main
contributions. On the one hand, a review of the field of GNNs is presented from
the perspective of computing. This includes a brief tutorial on the GNN
fundamentals, an overview of the evolution of the field in the last decade, and
a summary of operations carried out in the multiple phases of different GNN
algorithm variants. On the other hand, an in-depth analysis of current software
and hardware acceleration schemes is provided, from which a hardware-software,
graph-aware, and communication-centric vision for GNN accelerators is
distilled.
</p>
<a href="http://arxiv.org/abs/2010.00130">arXiv:2010.00130</a> [<a href="http://arxiv.org/pdf/2010.00130">pdf</a>]

<h2>DEEPMIR: A DEEP convolutional neural network for differential detection of cerebral Microbleeds and IRon deposits in MRI. (arXiv:2010.00148v1 [eess.IV])</h2>
<h3>Tanweer Rashid, Ahmed Abdulkadir, Ilya M. Nasrallah, Jeffrey B. Ware, Pascal Spincemaille, J. Rafael Romero, R. Nick Bryan, Susan R. Heckbert, Mohamad Habes</h3>
<p>Background: Cerebral microbleeds (CMBs) and non-hemorrhage iron deposits in
the basal ganglia have been associated with brain aging, vascular disease and
neurodegenerative disorders. Recent advances using quantitative susceptibility
mapping (QSM) make it possible to differentiate iron content from
mineralization in-vivo using magnetic resonance imaging (MRI). However,
automated detection of such lesions is still challenging, making quantification
in large cohort bases studies rather limited. Purpose: Development of a fully
automated method using deep learning for detecting CMBs and basal ganglia iron
deposits using multimodal MRI. Materials and Methods: We included a convenience
sample of 24 participants from the MESA cohort and used T2-weighted images,
susceptibility weighted imaging (SWI), and QSM to segment the lesions. We
developed a protocol for simultaneous manual annotation of CMBs and
non-hemorrhage iron deposits in the basal ganglia, which resulted in defining
the gold standard. This gold standard was then used to train a deep convolution
neural network (CNN) model. Specifically, we adapted the U-Net model with a
higher number of resolution layers to be able to detect small lesions such as
CMBs from standard resolution MRI which are used in cohort-based studies. The
detection performance was then evaluated using the cross-validation principle
in order to ensure generalization of the results. Results: With multi-class CNN
models, we achieved an average sensitivity and precision of about 0.8 and 0.6,
respectively for detecting CMBs. The same framework detected non-hemorrhage
iron deposits reaching an average sensitivity and precision of about 0.8.
Conclusions: Our results showed that deep learning could automate the detection
of small vessel disease lesions and including multimodal MR data such as QSM
can improve the detection of CMB and non-hemorrhage iron deposits.
</p>
<a href="http://arxiv.org/abs/2010.00148">arXiv:2010.00148</a> [<a href="http://arxiv.org/pdf/2010.00148">pdf</a>]

<h2>Learning from Mistakes: Combining Ontologies via Self-Training for Dialogue Generation. (arXiv:2010.00150v1 [cs.CL])</h2>
<h3>Lena Reed, Vrindavan Harrison, Shereen Oraby, Dilek Hakkani-Tur, Marilyn Walker</h3>
<p>Natural language generators (NLGs) for task-oriented dialogue typically take
a meaning representation (MR) as input. They are trained end-to-end with a
corpus of MR/utterance pairs, where the MRs cover a specific set of dialogue
acts and domain attributes. Creation of such datasets is labor-intensive and
time-consuming. Therefore, dialogue systems for new domain ontologies would
benefit from using data for pre-existing ontologies. Here we explore, for the
first time, whether it is possible to train an NLG for a new larger ontology
using existing training sets for the restaurant domain, where each set is based
on a different ontology. We create a new, larger combined ontology, and then
train an NLG to produce utterances covering it. For example, if one dataset has
attributes for family-friendly and rating information, and the other has
attributes for decor and service, our aim is an NLG for the combined ontology
that can produce utterances that realize values for family-friendly, rating,
decor and service. Initial experiments with a baseline neural
sequence-to-sequence model show that this task is surprisingly challenging. We
then develop a novel self-training method that identifies (errorful) model
outputs, automatically constructs a corrected MR input to form a new (MR,
utterance) training pair, and then repeatedly adds these new instances back
into the training data. We then test the resulting model on a new test set. The
result is a self-trained model whose performance is an absolute 75.4%
improvement over the baseline model. We also report a human qualitative
evaluation of the final model showing that it achieves high naturalness,
semantic coherence and grammaticality
</p>
<a href="http://arxiv.org/abs/2010.00150">arXiv:2010.00150</a> [<a href="http://arxiv.org/pdf/2010.00150">pdf</a>]

<h2>Deformable Kernel Convolutional Network for Video Extreme Super-Resolution. (arXiv:2010.00154v1 [cs.CV])</h2>
<h3>Xuan Xu, Xin Xiong, Jinge Wang, Xin Li</h3>
<p>Video super-resolution, which attempts to reconstruct high-resolution video
frames from their corresponding low-resolution versions, has received
increasingly more attention in recent years. Most existing approaches opt to
use deformable convolution to temporally align neighboring frames and apply
traditional spatial attention mechanism (convolution based) to enhance
reconstructed features. However, such spatial-only strategies cannot fully
utilize temporal dependency among video frames. In this paper, we propose a
novel deep learning based VSR algorithm, named Deformable Kernel Spatial
Attention Network (DKSAN). Thanks to newly designed Deformable Kernel
Convolution Alignment (DKC_Align) and Deformable Kernel Spatial Attention
(DKSA) modules, DKSAN can better exploit both spatial and temporal redundancies
to facilitate the information propagation across different layers. We have
tested DKSAN on AIM2020 Video Extreme Super-Resolution Challenge to
super-resolve videos with a scale factor as large as 16. Experimental results
demonstrate that our proposed DKSAN can achieve both better subjective and
objective performance compared with the existing state-of-the-art EDVR on
Vid3oC and IntVID datasets.
</p>
<a href="http://arxiv.org/abs/2010.00154">arXiv:2010.00154</a> [<a href="http://arxiv.org/pdf/2010.00154">pdf</a>]

<h2>GeoD: Consensus-based Geodesic Distributed Pose Graph Optimization. (arXiv:2010.00156v1 [cs.RO])</h2>
<h3>Eric Cristofalo, Eduardo Montijano, Mac Schwager</h3>
<p>We present a consensus-based distributed pose graph optimization algorithm
for obtaining an estimate of the 3D translation and rotation of each pose in a
pose graph, given noisy relative measurements between poses. The algorithm,
called GeoD, implements a continuous time distributed consensus protocol to
minimize the geodesic pose graph error. GeoD is distributed over the pose graph
itself, with a separate computation thread for each node in the graph, and
messages are passed only between neighboring nodes in the graph. We leverage
tools from Lyapunov theory and multi-agent consensus to prove the convergence
of the algorithm. We identify two new consistency conditions sufficient for
convergence: pairwise consistency of relative rotation measurements, and
minimal consistency of relative translation measurements. GeoD incorporates a
simple one step distributed initialization to satisfy both conditions. We
demonstrate GeoD on simulated and real world SLAM datasets. We compare to a
centralized pose graph optimizer with an optimality certificate (SE-Sync) and a
Distributed Gauss-Seidel (DGS) method. On average, GeoD converges 20 times more
quickly than DGS to a value with 3.4 times less error when compared to the
global minimum provided by SE-Sync. GeoD scales more favorably with graph size
than DGS, converging over 100 times faster on graphs larger than 1000 poses.
Lastly, we test GeoD on a multi-UAV vision-based SLAM scenario, where the UAVs
estimate their pose trajectories in a distributed manner using the relative
poses extracted from their on board camera images. We show qualitative
performance that is better than either the centralized SE-Sync or the
distributed DGS methods.
</p>
<a href="http://arxiv.org/abs/2010.00156">arXiv:2010.00156</a> [<a href="http://arxiv.org/pdf/2010.00156">pdf</a>]

<h2>Unknown Delay for Adversarial Bandit Setting with Multiple Play. (arXiv:2010.00161v1 [cs.LG])</h2>
<h3>Olusola T. Odeyomi</h3>
<p>This paper addresses the problem of unknown delays in adversarial multi-armed
bandit (MAB) with multiple play. Existing work on similar game setting focused
on only the case where the learner selects an arm in each round. However, there
are lots of applications in robotics where a learner needs to select more than
one arm per round. It is therefore worthwhile to investigate the effect of
delay when multiple arms are chosen. The multiple arms chosen per round in this
setting are such that they experience the same amount of delay. There can be an
aggregation of feedback losses from different combinations of arms selected at
different rounds, and the learner is faced with the challenge of associating
the feedback losses to the arms producing them. To address this problem, this
paper proposes a delayed exponential, exploitation and exploration for multiple
play (DEXP3.M) algorithm. The regret bound is only slightly worse than the
regret of DEXP3 already proposed for the single play setting with unknown
delay.
</p>
<a href="http://arxiv.org/abs/2010.00161">arXiv:2010.00161</a> [<a href="http://arxiv.org/pdf/2010.00161">pdf</a>]

<h2>Value-based Bayesian Meta-reinforcement Learning and Traffic Signal Control. (arXiv:2010.00163v1 [cs.LG])</h2>
<h3>Yayi Zou, Zhiwei Qin</h3>
<p>Reinforcement learning methods for traffic signal control has gained
increasing interests recently and achieved better performances compared with
traditional transportation methods. However, reinforcement learning based
methods usually requires heavy training data and computational resources which
largely limit its application in real-world traffic signal control. This makes
meta-learning, which enables data-efficient and fast-adaptation training by
leveraging the knowledge of previous learning experiences, catches attentions
in traffic signal control. In this paper, we propose a novel value-based
Bayesian meta-reinforcement learning framework BM-DQN to robustly speed up the
learning process in new scenarios by utilizing well-trained prior knowledge
learned from existing scenarios. This framework based on our proposed
fast-adaptation variation to Gradient-EM Bayesian Meta-learning and the fast
update advantage of DQN, which allows fast adaptation to new scenarios with
continual learning ability and robustness to uncertainty. The experiments on 2D
navigation and traffic signal control show that our proposed framework adapts
more quickly and robustly in new scenarios than previous methods, and
specifically, much better continual learning ability in heterogeneous
scenarios.
</p>
<a href="http://arxiv.org/abs/2010.00163">arXiv:2010.00163</a> [<a href="http://arxiv.org/pdf/2010.00163">pdf</a>]

<h2>Multi-label Classification of Common Bengali Handwritten Graphemes: Dataset and Challenge. (arXiv:2010.00170v1 [cs.CV])</h2>
<h3>Samiul Alam, Tahsin Reasat, Asif Shahriyar Sushmit, Sadi Mohammad Siddiquee, Fuad Rahman, Mahady Hasan, Ahmed Imtiaz Humayun</h3>
<p>Latin has historically led the state-of-the-art in handwritten optical
character recognition (OCR) research. Adapting existing systems from Latin to
alpha-syllabary languages is particularly challenging due to a sharp contrast
between their orthographies. The segmentation of graphical constituents
corresponding to characters becomes significantly hard due to a cursive writing
system and frequent use of diacritics in the alpha-syllabary family of
languages. We propose a labeling scheme based on graphemes (linguistic segments
of word formation) that makes segmentation inside alpha-syllabary words linear
and present the first dataset of Bengali handwritten graphemes that are
commonly used in an everyday context. The dataset is open-sourced as a part of
the Bengali.AI Handwritten Grapheme Classification Challenge on Kaggle to
benchmark vision algorithms for multi-label grapheme classification. From
competition proceedings, we see that deep learning methods can generalize to a
large span of uncommon graphemes even when they are absent during training.
</p>
<a href="http://arxiv.org/abs/2010.00170">arXiv:2010.00170</a> [<a href="http://arxiv.org/pdf/2010.00170">pdf</a>]

<h2>Towards Self-learning Edge Intelligence in 6G. (arXiv:2010.00176v1 [cs.NI])</h2>
<h3>Yong Xiao, Guangming Shi, Yingyu Li, Walid Saad, H. Vincent Poor</h3>
<p>Edge intelligence, also called edge-native artificial intelligence (AI), is
an emerging technological framework focusing on seamless integration of AI,
communication networks, and mobile edge computing. It has been considered to be
one of the key missing components in the existing 5G network and is widely
recognized to be one of the most sought-after functions for tomorrow's wireless
6G cellular systems. In this article, we identify the key requirements and
challenges of edge-native AI in 6G. A self-learning architecture based on
self-supervised Generative Adversarial Nets (GANs) is introduced to
\blu{demonstrate the potential performance improvement that can be achieved by
automatic data learning and synthesizing at the edge of the network}. We
evaluate the performance of our proposed self-learning architecture in a
university campus shuttle system connected via a 5G network. Our result shows
that the proposed architecture has the potential to identify and classify
unknown services that emerge in edge computing networks. Future trends and key
research problems for self-learning-enabled 6G edge intelligence are also
discussed.
</p>
<a href="http://arxiv.org/abs/2010.00176">arXiv:2010.00176</a> [<a href="http://arxiv.org/pdf/2010.00176">pdf</a>]

<h2>Training Data Augmentation for Deep Learning RF Systems. (arXiv:2010.00178v1 [cs.LG])</h2>
<h3>William H. Clark IV, Steven Hauser, William C. Headley, Alan J. Michaels</h3>
<p>Applications of machine learning are subject to three major components that
contribute to the final performance metrics. Within the specifics of neural
networks, and deep learning specifically, the first two are the architecture
for the model being trained and the training approach used. This work focuses
on the third component, the data being used during training. The questions that
arise are then "what is in the data" and "what within the data matters?"
Looking into the Radio Frequency Machine Learning (RFML) field of Modulation
Classification, the use of synthetic, captured, and augmented data are examined
and compared to provide insights about the quantity and quality of the
available data presented. In general, all three data types have useful
contributions to a final application, but captured data germane to the intended
use case will always provide more significant information and enable the
greatest performance. Despite the benefit of captured data, the difficulties
that arise from collection often make the quantity of data needed to achieve
peak performance impractical. This paper helps quantify the balance between
real and synthetic data, offering concrete examples where training data is
parametrically varied in size and source.
</p>
<a href="http://arxiv.org/abs/2010.00178">arXiv:2010.00178</a> [<a href="http://arxiv.org/pdf/2010.00178">pdf</a>]

<h2>Deep Group-wise Variational Diffeomorphic Image Registration. (arXiv:2010.00231v1 [eess.IV])</h2>
<h3>Tycho F.A. van der Ouderaa, Ivana I&#x161;gum, Wouter B. Veldhuis, Bob D. de Vos</h3>
<p>Deep neural networks are increasingly used for pair-wise image registration.
We propose to extend current learning-based image registration to allow
simultaneous registration of multiple images. To achieve this, we build upon
the pair-wise variational and diffeomorphic VoxelMorph approach and present a
general mathematical framework that enables both registration of multiple
images to their geodesic average and registration in which any of the available
images can be used as a fixed image. In addition, we provide a likelihood based
on normalized mutual information, a well-known image similarity metric in
registration, between multiple images, and a prior that allows for explicit
control over the viscous fluid energy to effectively regularize deformations.
We trained and evaluated our approach using intra-patient registration of
breast MRI and Thoracic 4DCT exams acquired over multiple time points.
Comparison with Elastix and VoxelMorph demonstrates competitive quantitative
performance of the proposed method in terms of image similarity and reference
landmark distances at significantly faster registration.
</p>
<a href="http://arxiv.org/abs/2010.00231">arXiv:2010.00231</a> [<a href="http://arxiv.org/pdf/2010.00231">pdf</a>]

<h2>Optimal Task Assignment to Heterogeneous Federated Learning Devices. (arXiv:2010.00239v1 [cs.AI])</h2>
<h3>La&#xe9;rcio Lima Pilla (ParSys - LRI)</h3>
<p>Federated Learning provides new opportunities for training machine learning
models while respecting data privacy. This technique is based on heterogeneous
devices that work together to iteratively train a model while never sharing
their own data. Given the synchronous nature of this training, the performance
of Federated Learning systems is dictated by the slowest devices, also known as
stragglers. In this paper, we investigate the problem of minimizing the
duration of Federated Learning rounds by controlling how much data each device
uses for training. We formulate this problem as a makespan minimization problem
with identical, independent, and atomic tasks that have to be assigned to
heterogeneous resources with non-decreasing cost functions while respecting
lower and upper limits of tasks per resource. Based on this formulation, we
propose a polynomial-time algorithm named OLAR and prove that it provides
optimal schedules. We evaluate OLAR in an extensive experimental evaluation
using simulation that includes comparisons to other algorithms from the state
of the art and new extensions to them. Our results indicate that OLAR provides
optimal solutions with a small execution time. They also show that the presence
of lower and upper limits of tasks per resource erase any benefits that
suboptimal heuristics could provide in terms of algorithm execution time.
</p>
<a href="http://arxiv.org/abs/2010.00239">arXiv:2010.00239</a> [<a href="http://arxiv.org/pdf/2010.00239">pdf</a>]

<h2>MLRSNet: A Multi-label High Spatial Resolution Remote Sensing Dataset for Semantic Scene Understanding. (arXiv:2010.00243v1 [cs.CV])</h2>
<h3>Xiaoman Qi, PanPan Zhu, Yuebin Wang, Liqiang Zhang, Junhuan Peng, Mengfan Wu, Jialong Chen, Xudong Zhao, Ning Zang, P.Takis Mathiopoulos</h3>
<p>To better understand scene images in the field of remote sensing, multi-label
annotation of scene images is necessary. Moreover, to enhance the performance
of deep learning models for dealing with semantic scene understanding tasks, it
is vital to train them on large-scale annotated data. However, most existing
datasets are annotated by a single label, which cannot describe the complex
remote sensing images well because scene images might have multiple land cover
classes. Few multi-label high spatial resolution remote sensing datasets have
been developed to train deep learning models for multi-label based tasks, such
as scene classification and image retrieval. To address this issue, in this
paper, we construct a multi-label high spatial resolution remote sensing
dataset named MLRSNet for semantic scene understanding with deep learning from
the overhead perspective. It is composed of high-resolution optical satellite
or aerial images. MLRSNet contains a total of 109,161 samples within 46 scene
categories, and each image has at least one of 60 predefined labels. We have
designed visual recognition tasks, including multi-label based image
classification and image retrieval, in which a wide variety of deep learning
approaches are evaluated with MLRSNet. The experimental results demonstrate
that MLRSNet is a significant benchmark for future research, and it complements
the current widely used datasets such as ImageNet, which fills gaps in
multi-label image research. Furthermore, we will continue to expand the
MLRSNet. MLRSNet and all related materials have been made publicly available at
https://data.mendeley.com/datasets/7j9bv9vwsx/2 and
https://github.com/cugbrs/MLRSNet.git.
</p>
<a href="http://arxiv.org/abs/2010.00243">arXiv:2010.00243</a> [<a href="http://arxiv.org/pdf/2010.00243">pdf</a>]

<h2>Predicting the flow field in a U-bend with deep neural networks. (arXiv:2010.00258v1 [cs.LG])</h2>
<h3>Gergely Hajgat&#xf3;, B&#xe1;lint Gyires-T&#xf3;th, Gy&#xf6;rgy Pa&#xe1;l</h3>
<p>This paper describes a study based on computational fluid dynamics (CFD) and
deep neural networks that focusing on predicting the flow field in differently
distorted U-shaped pipes. The main motivation of this work was to get an
insight about the justification of the deep learning paradigm in hydrodynamic
hull optimisation processes that heavily depend on computing turbulent flow
fields and that could be accelerated with models like the one presented. The
speed-up can be even several orders of magnitude by surrogating the CFD model
with a deep convolutional neural network. An automated geometry creation and
evaluation process was set up to generate differently shaped two-dimensional
U-bends and to carry out CFD simulation on them. This process resulted in a
database with different geometries and the corresponding flow fields
(2-dimensional velocity distribution), both represented on 128x128 equidistant
grids. This database was used to train an encoder-decoder style deep
convolutional neural network to predict the velocity distribution from the
geometry. The effect of two different representations of the geometry (binary
image and signed distance function) on the predictions was examined, both
models gave acceptable predictions with a speed-up of two orders of magnitude.
</p>
<a href="http://arxiv.org/abs/2010.00258">arXiv:2010.00258</a> [<a href="http://arxiv.org/pdf/2010.00258">pdf</a>]

<h2>${\rm N{\small ode}S{\small ig}}$: Random Walk Diffusion meets Hashing for Scalable Graph Embeddings. (arXiv:2010.00261v1 [cs.LG])</h2>
<h3>Abdulkadir &#xc7;elikkanat, Apostolos N. Papadopoulos, Fragkiskos D. Malliaros</h3>
<p>Learning node representations is a crucial task with a plethora of
interdisciplinary applications. Nevertheless, as the size of the networks
increases, most widely used models face computational challenges to scale to
large networks. While there is a recent effort towards designing algorithms
that solely deal with scalability issues, most of them behave poorly in terms
of accuracy on downstream tasks. In this paper, we aim at studying models that
balance the trade-off between efficiency and accuracy. In particular, we
propose ${\rm N{\small ode}S{\small ig}}$, a scalable embedding model that
computes binary node representations. ${\rm N{\small ode}S{\small ig}}$
exploits random walk diffusion probabilities via stable random projection
hashing, towards efficiently computing embeddings in the Hamming space. Our
extensive experimental evaluation on various graphs has demonstrated that the
proposed model achieves a good balance between accuracy and efficiency compared
to well-known baseline models on two downstream tasks.
</p>
<a href="http://arxiv.org/abs/2010.00261">arXiv:2010.00261</a> [<a href="http://arxiv.org/pdf/2010.00261">pdf</a>]

<h2>Action Units Recognition with Pairwise Deep Architecture. (arXiv:2010.00288v1 [cs.CV])</h2>
<h3>Junya Saito, Kentaro Murase</h3>
<p>In this paper, we propose a new automatic Action Units (AUs) recognition
method used in a competition, Affective Behavior Analysis in-the-wild (ABAW).
Our method uses pairwise deep architecture to tackle a problem of AUs label
criteria change in different videos. While the baseline score is 0.31, our
method achieved 0.65 in validation dataset of the competition.
</p>
<a href="http://arxiv.org/abs/2010.00288">arXiv:2010.00288</a> [<a href="http://arxiv.org/pdf/2010.00288">pdf</a>]

<h2>Phonemer at WNUT-2020 Task 2: Sequence Classification Using COVID Twitter BERT and Bagging Ensemble Technique based on Plurality Voting. (arXiv:2010.00294v1 [cs.CL])</h2>
<h3>Anshul Wadhawan</h3>
<p>This paper presents the approach that we employed to tackle the EMNLP
WNUT-2020 Shared Task 2 : Identification of informative COVID-19 English
Tweets. The task is to develop a system that automatically identifies whether
an English Tweet related to the novel coronavirus (COVID-19) is informative or
not. We solve the task in three stages. The first stage involves pre-processing
the dataset by filtering only relevant information. This is followed by
experimenting with multiple deep learning models like CNNs, RNNs and
Transformer based models. In the last stage, we propose an ensemble of the best
model trained on different subsets of the provided dataset. Our final approach
achieved an F1-score of 0.9037 and we were ranked sixth overall with F1-score
as the evaluation criteria.
</p>
<a href="http://arxiv.org/abs/2010.00294">arXiv:2010.00294</a> [<a href="http://arxiv.org/pdf/2010.00294">pdf</a>]

<h2>Reinforcement Learning Using Expectation Maximization Based Guided Policy Search for Stochastic Dynamics. (arXiv:2010.00304v1 [eess.SY])</h2>
<h3>Prakash Mallick, Zhiyong Chen, Mohsen Zamani</h3>
<p>Guided policy search algorithms have been proven to work with incredible
accuracy for not only controlling a complicated dynamical system, but also
learning optimal policies from various unseen instances. One assumes true
nature of the states in almost all of the well known policy search and learning
algorithms. This paper deals with a trajectory optimization procedure for an
unknown dynamical system subject to measurement noise using expectation
maximization and extends it to learning (optimal) policies which have less
noise because of lower variance in the optimal trajectories. Theoretical and
empirical evidence of learnt optimal policies of the new approach is depicted
in comparison to some well known baselines which are evaluated on an autonomous
system with widely used performance metrics.
</p>
<a href="http://arxiv.org/abs/2010.00304">arXiv:2010.00304</a> [<a href="http://arxiv.org/pdf/2010.00304">pdf</a>]

<h2>CoLAKE: Contextualized Language and Knowledge Embedding. (arXiv:2010.00309v1 [cs.CL])</h2>
<h3>Tianxiang Sun, Yunfan Shao, Xipeng Qiu, Qipeng Guo, Yaru Hu, Xuanjing Huang, Zheng Zhang</h3>
<p>With the emerging branch of incorporating factual knowledge into pre-trained
language models such as BERT, most existing models consider shallow, static,
and separately pre-trained entity embeddings, which limits the performance
gains of these models. Few works explore the potential of deep contextualized
knowledge representation when injecting knowledge. In this paper, we propose
the Contextualized Language and Knowledge Embedding (CoLAKE), which jointly
learns contextualized representation for both language and knowledge with the
extended MLM objective. Instead of injecting only entity embeddings, CoLAKE
extracts the knowledge context of an entity from large-scale knowledge bases.
To handle the heterogeneity of knowledge context and language context, we
integrate them in a unified data structure, word-knowledge graph (WK graph).
CoLAKE is pre-trained on large-scale WK graphs with the modified Transformer
encoder. We conduct experiments on knowledge-driven tasks, knowledge probing
tasks, and language understanding tasks. Experimental results show that CoLAKE
outperforms previous counterparts on most of the tasks. Besides, CoLAKE
achieves surprisingly high performance on our synthetic task called
word-knowledge graph completion, which shows the superiority of simultaneously
contextualizing language and knowledge representation.
</p>
<a href="http://arxiv.org/abs/2010.00309">arXiv:2010.00309</a> [<a href="http://arxiv.org/pdf/2010.00309">pdf</a>]

<h2>"Did you really mean what you said?" : Sarcasm Detection in Hindi-English Code-Mixed Data using Bilingual Word Embeddings. (arXiv:2010.00310v1 [cs.CL])</h2>
<h3>Akshita Aggarwal, Anshul Wadhawan, Anshima Chaudhary, Kavita Maurya</h3>
<p>With the increased use of social media platforms by people across the world,
many new interesting NLP problems have come into existence. One such being the
detection of sarcasm in the social media texts. We present a corpus of tweets
for training custom word embeddings and a Hinglish dataset labelled for sarcasm
detection. We propose a deep learning based approach to address the issue of
sarcasm detection in Hindi-English code mixed tweets using bilingual word
embeddings derived from FastText and Word2Vec approaches. We experimented with
various deep learning models, including CNNs, LSTMs, Bi-directional LSTMs (with
and without attention). We were able to outperform all state-of-the-art
performances with our deep learning models, with attention based Bi-directional
LSTMs giving the best performance exhibiting an accuracy of 78.49%.
</p>
<a href="http://arxiv.org/abs/2010.00310">arXiv:2010.00310</a> [<a href="http://arxiv.org/pdf/2010.00310">pdf</a>]

<h2>Deep-3DAligner: Unsupervised 3D Point Set Registration Network With Optimizable Latent Vector. (arXiv:2010.00321v1 [cs.CV])</h2>
<h3>Lingjing Wang, Xiang Li, Yi Fang</h3>
<p>Point cloud registration is the process of aligning a pair of point sets via
searching for a geometric transformation. Unlike classical optimization-based
methods, recent learning-based methods leverage the power of deep learning for
registering a pair of point sets. In this paper, we propose to develop a novel
model that organically integrates the optimization to learning, aiming to
address the technical challenges in 3D registration. More specifically, in
addition to the deep transformation decoding network, our framework introduce
an optimizable deep \underline{S}patial \underline{C}orrelation
\underline{R}epresentation (SCR) feature. The SCR feature and weights of the
transformation decoder network are jointly updated towards the minimization of
an unsupervised alignment loss. We further propose an adaptive Chamfer loss for
aligning partial shapes. To verify the performance of our proposed method, we
conducted extensive experiments on the ModelNet40 dataset. The results
demonstrate that our method achieves significantly better performance than the
previous state-of-the-art approaches in the full/partial point set registration
task.
</p>
<a href="http://arxiv.org/abs/2010.00321">arXiv:2010.00321</a> [<a href="http://arxiv.org/pdf/2010.00321">pdf</a>]

<h2>Workflow Provenance in the Lifecycle of Scientific Machine Learning. (arXiv:2010.00330v1 [cs.DB])</h2>
<h3>Renan Souza, Leonardo G. Azevedo, V&#xed;tor Louren&#xe7;o, Elton Soares, Raphael Thiago, Rafael Brand&#xe3;o, Daniel Civitarese, Emilio Vital Brazil, Marcio Moreno, Patrick Valduriez, Marta Mattoso, Renato Cerqueira, Marco A. S. Netto</h3>
<p>Machine Learning (ML) has already fundamentally changed several businesses.
More recently, it has also been profoundly impacting the computational science
and engineering domains, like geoscience, climate science, and health science.
In these domains, users need to perform comprehensive data analyses combining
scientific data and ML models to provide for critical requirements, such as
reproducibility, model explainability, and experiment data understanding.
However, scientific ML is multidisciplinary, heterogeneous, and affected by the
physical constraints of the domain, making such analyses even more challenging.
In this work, we leverage workflow provenance techniques to build a holistic
view to support the lifecycle of scientific ML. We contribute with (i)
characterization of the lifecycle and taxonomy for data analyses; (ii) design
principles to build this view, with a W3C PROV compliant data representation
and a reference system architecture; and (iii) lessons learned after an
evaluation in an Oil &amp; Gas case using an HPC cluster with 393 nodes and 946
GPUs. The experiments show that the principles enable queries that integrate
domain semantics with ML models while keeping low overhead (&lt;1%), high
scalability, and an order of magnitude of query acceleration under certain
workloads against without our representation.
</p>
<a href="http://arxiv.org/abs/2010.00330">arXiv:2010.00330</a> [<a href="http://arxiv.org/pdf/2010.00330">pdf</a>]

<h2>Fault Injection Analytics: A Novel Approach to Discover Failure Modes in Cloud-Computing Systems. (arXiv:2010.00331v1 [cs.SE])</h2>
<h3>Domenico Cotroneo, Luigi De Simone, Pietro Liguori, Roberto Natella</h3>
<p>Cloud computing systems fail in complex and unexpected ways due to unexpected
combinations of events and interactions between hardware and software
components. Fault injection is an effective means to bring out these failures
in a controlled environment. However, fault injection experiments produce
massive amounts of data, and manually analyzing these data is inefficient and
error-prone, as the analyst can miss severe failure modes that are yet unknown.
This paper introduces a new paradigm (fault injection analytics) that applies
unsupervised machine learning on execution traces of the injected system, to
ease the discovery and interpretation of failure modes. We evaluated the
proposed approach in the context of fault injection experiments on the
OpenStack cloud computing platform, where we show that the approach can
accurately identify failure modes with a low computational cost.
</p>
<a href="http://arxiv.org/abs/2010.00331">arXiv:2010.00331</a> [<a href="http://arxiv.org/pdf/2010.00331">pdf</a>]

<h2>Training general representations for remote sensing using in-domain knowledge. (arXiv:2010.00332v1 [cs.CV])</h2>
<h3>Maxim Neumann, Andr&#xe9; Susano Pinto, Xiaohua Zhai, Neil Houlsby</h3>
<p>Automatically finding good and general remote sensing representations allows
to perform transfer learning on a wide range of applications - improving the
accuracy and reducing the required number of training samples. This paper
investigates development of generic remote sensing representations, and
explores which characteristics are important for a dataset to be a good source
for representation learning. For this analysis, five diverse remote sensing
datasets are selected and used for both, disjoint upstream representation
learning and downstream model training and evaluation. A common evaluation
protocol is used to establish baselines for these datasets that achieve
state-of-the-art performance. As the results indicate, especially with a low
number of available training samples a significant performance enhancement can
be observed when including additionally in-domain data in comparison to
training models from scratch or fine-tuning only on ImageNet (up to 11% and
40%, respectively, at 100 training samples). All datasets and pretrained
representation models are published online.
</p>
<a href="http://arxiv.org/abs/2010.00332">arXiv:2010.00332</a> [<a href="http://arxiv.org/pdf/2010.00332">pdf</a>]

<h2>More Than Just Good Passwords? A Study on Usability and Security Perceptions of Risk-based Authentication. (arXiv:2010.00339v1 [cs.CR])</h2>
<h3>Stephan Wiefling, Markus D&#xfc;rmuth, Luigi Lo Iacono</h3>
<p>Risk-based Authentication (RBA) is an adaptive security measure to strengthen
password-based authentication. RBA monitors additional features during login,
and when observed feature values differ significantly from previously seen
ones, users have to provide additional authentication factors such as a
verification code. RBA has the potential to offer more usable authentication,
but the usability and the security perceptions of RBA are not studied well.

We present the results of a between-group lab study (n=65) to evaluate
usability and security perceptions of two RBA variants, one 2FA variant, and
password-only authentication. Our study shows with significant results that RBA
is considered to be more usable than the studied 2FA variants, while it is
perceived as more secure than password-only authentication in general and
comparably secure to 2FA in a variety of application types. We also observed
RBA usability problems and provide recommendations for mitigation. Our
contribution provides a first deeper understanding of the users' perception of
RBA and helps to improve RBA implementations for a broader user acceptance.
</p>
<a href="http://arxiv.org/abs/2010.00339">arXiv:2010.00339</a> [<a href="http://arxiv.org/pdf/2010.00339">pdf</a>]

<h2>Emergence of a finite-size-scaling function in the supervised learning of the Ising phase transition. (arXiv:2010.00351v1 [cond-mat.stat-mech])</h2>
<h3>Dongkyu Kim, Dong-Hee Kim</h3>
<p>We investigate the connection between the supervised learning of the binary
phase classification in the ferromagnetic Ising model and the standard
finite-size-scaling theory of the second-order phase transition. Proposing a
minimal one-free-parameter neural network model, we analytically formulate the
supervised learning problem for the canonical ensemble being used as a training
data set. We show that just one free parameter is capable enough to describe
the data-driven emergence of the universal finite-size-scaling function in the
network output that is observed in a large neural network, theoretically
validating its critical point prediction for unseen test data from different
underlying lattices yet in the same universality class of the Ising
criticality. We also numerically demonstrate the interpretation with the
proposed one-parameter model by providing an example of finding a critical
point with the learning of the Landau mean-field free energy being applied to
the real data set from the uncorrelated random scale-free graph with a large
degree exponent.
</p>
<a href="http://arxiv.org/abs/2010.00351">arXiv:2010.00351</a> [<a href="http://arxiv.org/pdf/2010.00351">pdf</a>]

<h2>Meta-Consolidation for Continual Learning. (arXiv:2010.00352v1 [cs.CV])</h2>
<h3>K J Joseph, Vineeth N Balasubramanian</h3>
<p>The ability to continuously learn and adapt itself to new tasks, without
losing grasp of already acquired knowledge is a hallmark of biological learning
systems, which current deep learning systems fall short of. In this work, we
present a novel methodology for continual learning called MERLIN:
Meta-Consolidation for Continual Learning.

We assume that weights of a neural network $\boldsymbol \psi$, for solving
task $\boldsymbol t$, come from a meta-distribution $p(\boldsymbol{\psi|t})$.
This meta-distribution is learned and consolidated incrementally. We operate in
the challenging online continual learning setting, where a data point is seen
by the model only once.

Our experiments with continual learning benchmarks of MNIST, CIFAR-10,
CIFAR-100 and Mini-ImageNet datasets show consistent improvement over five
baselines, including a recent state-of-the-art, corroborating the promise of
MERLIN.
</p>
<a href="http://arxiv.org/abs/2010.00352">arXiv:2010.00352</a> [<a href="http://arxiv.org/pdf/2010.00352">pdf</a>]

<h2>When will the mist clear? On the Interpretability of Machine Learning for Medical Applications: a survey. (arXiv:2010.00353v1 [cs.AI])</h2>
<h3>Antonio-Jes&#xfa;s Banegas-Luna, Jorge Pe&#xf1;a-Garc&#xed;a, Adrian Iftene, Fiorella Guadagni, Patrizia Ferroni, Noemi Scarpato, Fabio Massimo Zanzotto, Andr&#xe9;s Bueno-Crespo, Horacio P&#xe9;rez-S&#xe1;nchez</h3>
<p>Artificial Intelligence is providing astonishing results, with medicine being
one of its favourite playgrounds. In a few decades, computers may be capable of
formulating diagnoses and choosing the correct treatment, while robots may
perform surgical operations, and conversational agents could interact with
patients as virtual coaches. Machine Learning and, in particular, Deep Neural
Networks are behind this revolution. In this scenario, important decisions will
be controlled by standalone machines that have learned predictive models from
provided data. Among the most challenging targets of interest in medicine are
cancer diagnosis and therapies but, to start this revolution, software tools
need to be adapted to cover the new requirements. In this sense, learning tools
are becoming a commodity in Python and Matlab libraries, just to name two, but
to exploit all their possibilities, it is essential to fully understand how
models are interpreted and which models are more interpretable than others. In
this survey, we analyse current machine learning models, frameworks, databases
and other related tools as applied to medicine - specifically, to cancer
research - and we discuss their interpretability, performance and the necessary
input data. From the evidence available, ANN, LR and SVM have been observed to
be the preferred models. Besides, CNNs, supported by the rapid development of
GPUs and tensor-oriented programming libraries, are gaining in importance.
However, the interpretability of results by doctors is rarely considered which
is a factor that needs to be improved. We therefore consider this study to be a
timely contribution to the issue.
</p>
<a href="http://arxiv.org/abs/2010.00353">arXiv:2010.00353</a> [<a href="http://arxiv.org/pdf/2010.00353">pdf</a>]

<h2>Detecting White Supremacist Hate Speech using Domain Specific Word Embedding with Deep Learning and BERT. (arXiv:2010.00357v1 [cs.CL])</h2>
<h3>Hind Saleh Alatawi, Areej Maatog Alhothali, Kawthar Mustafa Moria</h3>
<p>White supremacists embrace a radical ideology that considers white people
superior to people of other races. The critical influence of these groups is no
longer limited to social media; they also have a significant effect on society
in many ways by promoting racial hatred and violence. White supremacist hate
speech is one of the most recently observed harmful content on social
media.Traditional channels of reporting hate speech have proved inadequate due
to the tremendous explosion of information, and therefore, it is necessary to
find an automatic way to detect such speech in a timely manner. This research
investigates the viability of automatically detecting white supremacist hate
speech on Twitter by using deep learning and natural language processing
techniques. Through our experiments, we used two approaches, the first approach
is by using domain-specific embeddings which are extracted from white
supremacist corpus in order to catch the meaning of this white supremacist
slang with bidirectional Long Short-Term Memory (LSTM) deep learning model,
this approach reached a 0.74890 F1-score. The second approach is by using the
one of the most recent language model which is BERT, BERT model provides the
state of the art of most NLP tasks. It reached to a 0.79605 F1-score. Both
approaches are tested on a balanced dataset given that our experiments were
based on textual data only. The dataset was combined from dataset created from
Twitter and a Stormfront dataset compiled from that white supremacist forum.
</p>
<a href="http://arxiv.org/abs/2010.00357">arXiv:2010.00357</a> [<a href="http://arxiv.org/pdf/2010.00357">pdf</a>]

<h2>How LSTM Encodes Syntax: Exploring Context Vectors and Semi-Quantization on Natural Text. (arXiv:2010.00363v1 [cs.CL])</h2>
<h3>Chihiro Shibata, Kei Uchiumi, Daichi Mochihashi</h3>
<p>Long Short-Term Memory recurrent neural network (LSTM) is widely used and
known to capture informative long-term syntactic dependencies. However, how
such information are reflected in its internal vectors for natural text has not
yet been sufficiently investigated. We analyze them by learning a language
model where syntactic structures are implicitly given. We empirically show that
the context update vectors, i.e. outputs of internal gates, are approximately
quantized to binary or ternary values to help the language model to count the
depth of nesting accurately, as Suzgun et al. (2019) recently show for
synthetic Dyck languages. For some dimensions in the context vector, we show
that their activations are highly correlated with the depth of phrase
structures, such as VP and NP. Moreover, with an $L_1$ regularization, we also
found that it can accurately predict whether a word is inside a phrase
structure or not from a small number of components of the context vector. Even
for the case of learning from raw text, context vectors are shown to still
correlate well with the phrase structures. Finally, we show that natural
clusters of the functional words and the part of speeches that trigger phrases
are represented in a small but principal subspace of the context-update vector
of LSTM.
</p>
<a href="http://arxiv.org/abs/2010.00363">arXiv:2010.00363</a> [<a href="http://arxiv.org/pdf/2010.00363">pdf</a>]

<h2>SESQA: semi-supervised learning for speech quality assessment. (arXiv:2010.00368v1 [eess.AS])</h2>
<h3>Joan Serr&#xe0;, Jordi Pons, Santiago Pascual</h3>
<p>Automatic speech quality assessment is an important, transversal task whose
progress is hampered by the scarcity of human annotations, poor generalization
to unseen recording conditions, and a lack of flexibility of existing
approaches. In this work, we tackle these problems with a semi-supervised
learning approach, combining available annotations with programmatically
generated data, and using 3 different optimization criteria together with 5
complementary auxiliary tasks. Our results show that such a semi-supervised
approach can cut the error of existing methods by more than 36%, while
providing additional benefits in terms of reusable features or auxiliary
outputs. Improvement is further corroborated with an out-of-sample test showing
promising generalization capabilities.
</p>
<a href="http://arxiv.org/abs/2010.00368">arXiv:2010.00368</a> [<a href="http://arxiv.org/pdf/2010.00368">pdf</a>]

<h2>Task Agnostic Continual Learning Using Online Variational Bayes with Fixed-Point Updates. (arXiv:2010.00373v1 [stat.ML])</h2>
<h3>Chen Zeno, Itay Golan, Elad Hoffer, Daniel Soudry</h3>
<p>Background: Catastrophic forgetting is the notorious vulnerability of neural
networks to the changes in the data distribution during learning. This
phenomenon has long been considered a major obstacle for using learning agents
in realistic continual learning settings. A large body of continual learning
research assumes that task boundaries are known during training. However, only
a few works consider scenarios in which task boundaries are unknown or not well
defined -- task agnostic scenarios. The optimal Bayesian solution for this
requires an intractable online Bayes update to the weights posterior.
Contributions: We aim to approximate the online Bayes update as accurately as
possible. To do so, we derive novel fixed-point equations for the online
variational Bayes optimization problem, for multivariate Gaussian parametric
distributions. By iterating the posterior through these fixed-point equations,
we obtain an algorithm (FOO-VB) for continual learning which can handle
non-stationary data distribution using a fixed architecture and without using
external memory (i.e. without access to previous data). We demonstrate that our
method (FOO-VB) outperforms existing methods in task agnostic scenarios. FOO-VB
Pytorch implementation will be available online.
</p>
<a href="http://arxiv.org/abs/2010.00373">arXiv:2010.00373</a> [<a href="http://arxiv.org/pdf/2010.00373">pdf</a>]

<h2>GraphXCOVID: Explainable Deep Graph Diffusion Pseudo-Labelling for Identifying COVID-19 on Chest X-rays. (arXiv:2010.00378v1 [cs.LG])</h2>
<h3>Angelica I Aviles-Rivero, Philip Sellars, Carola-Bibiane Sch&#xf6;nlieb, Nicolas Papadakis</h3>
<p>Can one learn to diagnose COVID-19 under extreme minimal supervision? Since
the outbreak of the novel COVID-19 there has been a rush for developing
Artificial Intelligence techniques for expert-level disease identification on
Chest X-ray data. In particular, the use of deep supervised learning has become
the go-to paradigm. However, the performance of such models is heavily
dependent on the availability of a large and representative labelled dataset.
The creation of which is a heavily expensive and time consuming task, and
especially imposes a great challenge for a novel disease. Semi-supervised
learning has shown the ability to match the incredible performance of
supervised models whilst requiring a small fraction of the labelled examples.
This makes the semi-supervised paradigm an attractive option for identifying
COVID-19. In this work, we introduce a graph based deep semi-supervised
framework for classifying COVID-19 from chest X-rays. Our framework introduces
an optimisation model for graph diffusion that reinforces the natural relation
among the tiny labelled set and the vast unlabelled data. We then connect the
diffusion prediction output as pseudo-labels that are used in an iterative
scheme in a deep net. We demonstrate, through our experiments, that our model
is able to outperform the current leading supervised model with a tiny fraction
of the labelled examples. Finally, we provide attention maps to accommodate the
radiologist's mental model, better fitting their perceptual and cognitive
abilities. These visualisation aims to assist the radiologist in judging
whether the diagnostic is correct or not, and in consequence to accelerate the
decision.
</p>
<a href="http://arxiv.org/abs/2010.00378">arXiv:2010.00378</a> [<a href="http://arxiv.org/pdf/2010.00378">pdf</a>]

<h2>Deep matrix factorizations. (arXiv:2010.00380v1 [cs.LG])</h2>
<h3>Pierre De Handschutter, Nicolas Gillis, Xavier Siebert</h3>
<p>Constrained low-rank matrix approximations have been known for decades as
powerful linear dimensionality reduction techniques to be able to extract the
information contained in large data sets in a relevant way. However, such
low-rank approaches are unable to mine complex, interleaved features that
underlie hierarchical semantics. Recently, deep matrix factorization (deep MF)
was introduced to deal with the extraction of several layers of features and
has been shown to reach outstanding performances on unsupervised tasks. Deep MF
was motivated by the success of deep learning, as it is conceptually close to
some neural networks paradigms. In this paper, we present the main models,
algorithms, and applications of deep MF through a comprehensive literature
review. We also discuss theoretical questions and perspectives of research.
</p>
<a href="http://arxiv.org/abs/2010.00380">arXiv:2010.00380</a> [<a href="http://arxiv.org/pdf/2010.00380">pdf</a>]

<h2>Student-Initiated Action Advising via Advice Novelty. (arXiv:2010.00381v1 [cs.LG])</h2>
<h3>Ercument Ilhan, Diego Perez-Liebana</h3>
<p>Action advising is a knowledge exchange mechanism between peers, namely
student and teacher, that can help tackle exploration and sample inefficiency
problems in deep reinforcement learning. Due to the practical limitations in
peer-to-peer communication and the negative implications of over-advising, the
peer responsible for initiating these interactions needs to do so only when
it's most adequate to exchange advice. Most recently, student-initiated
techniques that utilise state novelty and uncertainty estimations have obtained
promising results. However, these estimations have several weaknesses, such as
having no information regarding the characteristics of convergence and being
subject to delays that occur in the presence of experience replay dynamics. We
propose a student-initiated action advising algorithm that alleviates these
shortcomings. Specifically, we employ Random Network Distillation (RND) to
measure the novelty of an advice, for the student to determine whether to
proceed with the request; furthermore, we perform RND updates only for the
advised states to ensure that the student's convergence will not prevent it
from utilising the teacher's knowledge at any stage of learning. Experiments in
GridWorld and simplified versions of five Atari games show that our approach
can perform on par with the state-of-the-art and demonstrate significant
advantages in the scenarios where the existing methods are prone to fail.
</p>
<a href="http://arxiv.org/abs/2010.00381">arXiv:2010.00381</a> [<a href="http://arxiv.org/pdf/2010.00381">pdf</a>]

<h2>A Deep Learning Framework for COVID Outbreak Prediction. (arXiv:2010.00382v1 [cs.LG])</h2>
<h3>Neeraj, Jimson Mathew, Ranjan Kumar Behera</h3>
<p>The outbreak of COVID-19 i.e. a variation of coronavirus, also known as novel
corona virus causing respiratory disease is a big concern worldwide since the
end of December 2019. As of September 12, 2020, it has turned into an epidemic
outbreak with more than 29 million confirmed cases and around 1 million
reported deaths worldwide. It has created an urgent need to monitor and
forecast COVID-19 spread behavior to better control this spread. Among all the
popular models for COVID-19 forecasting, statistical models are receiving much
attention in media. However, statistical models are showing less accuracy for
long term forecasting, as there is high level of uncertainty and required data
is also not sufficiently available. In this paper, we propose a comparative
analysis of deep learning models to forecast the COVID-19 outbreak as an
alternative to statistical models. We propose a new Attention-based
encoder-decoder model, named Attention-Long Short Term Memory (AttentionLSTM).
LSTM based neural network layer architecture incorporates the idea of
fine-grained attention mechanism i.e., attention on hidden state dimensions
instead of hidden state vector itself, which is capable of highlighting the
importance and contribution of each hidden state dimension. It helps in
detection on crucial temporal information, resulting in a highly interpretable
network. Additionally, we implement a learnable vector embedding for time. As,
time in a vector representation can be easily added with many architectures.
This vector representation is called Time2Vec. We have used COVID-19 data
repository by the Center for Systems Science and Engineering (CSSE) at Johns
Hopkins University to assess the proposed model's performance. The proposed
model give superior forecasting accuracy compared to other existing methods.
</p>
<a href="http://arxiv.org/abs/2010.00382">arXiv:2010.00382</a> [<a href="http://arxiv.org/pdf/2010.00382">pdf</a>]

<h2>Topological feature study of slope failure process via persistent homology-based machine learning. (arXiv:2010.00391v1 [physics.geo-ph])</h2>
<h3>Shengdong Zhang, Shihui You, Longfei Chen, Xiaofei Liu</h3>
<p>Using software UDEC to simulate the instability failure process of slope
under seismic load, studing the dynamic response of slope failure, obtaining
the deformation characteristics and displacement cloud map of slope, then
analyzing the instability state of slope by using the theory of persistent
homology, generates bar code map and extracts the topological characteristics
of slope from bar code map. The topological characteristics corresponding to
the critical state of slope instability are found, and the relationship between
topological characteristics and instability evolution is established. Finally,
it provides a topological research tool for slope failure prediction. The
results show that the change of the longest Betti 1 bar code reflects the
evolution process of the slope and the law of instability failure. Using
discrete element method and persistent homology theory to study the failure
characteristics of slope under external load can better understand the failure
mechanism of slope, provide theoretical basis for engineering protection, and
also provide a new mathematical method for slope safety design and disaster
prediction research.
</p>
<a href="http://arxiv.org/abs/2010.00391">arXiv:2010.00391</a> [<a href="http://arxiv.org/pdf/2010.00391">pdf</a>]

<h2>Analyzing Koopman approaches to physics-informed machine learning for long-term sea-surface temperature forecasting. (arXiv:2010.00399v1 [physics.geo-ph])</h2>
<h3>Julian Rice, Wenwei Xu, Andrew August</h3>
<p>Accurately predicting sea-surface temperature weeks to months into the future
is an important step toward long term weather forecasting. Standard
atmosphere-ocean coupled numerical models provide accurate sea-surface
forecasts on the scale of a few days to a few weeks, but many important weather
systems require greater foresight. In this paper we propose machine-learning
approaches sea-surface temperature forecasting that are accurate on the scale
of dozens of weeks. Our approach is based in Koopman operator theory, a useful
tool for dynamical systems modelling. With this approach, we predict sea
surface temperature in the Gulf of Mexico up to 180 days into the future based
on a present image of thermal conditions and three years of historical training
data. We evaluate the combination of a basic Koopman method with a
convolutional autoencoder, and a newly proposed "consistent Koopman" method, in
various permutations. We show that the Koopman approach consistently
outperforms baselines, and we discuss the utility of our additional assumptions
and methods in this sea-surface temperature domain.
</p>
<a href="http://arxiv.org/abs/2010.00399">arXiv:2010.00399</a> [<a href="http://arxiv.org/pdf/2010.00399">pdf</a>]

<h2>DeepFakesON-Phys: DeepFakes Detection based on Heart Rate Estimation. (arXiv:2010.00400v1 [cs.CV])</h2>
<h3>Javier Hernandez-Ortega, Ruben Tolosana, Julian Fierrez, Aythami Morales</h3>
<p>This work introduces a novel DeepFake detection framework based on
physiological measurement. In particular, we consider information related to
the heart rate using remote photoplethysmography (rPPG). rPPG methods analyze
video sequences looking for subtle color changes in the human skin, revealing
the presence of human blood under the tissues. In this work we investigate to
what extent rPPG is useful for the detection of DeepFake videos.

The proposed fake detector named DeepFakesON-Phys uses a Convolutional
Attention Network (CAN), which extracts spatial and temporal information from
video frames, analyzing and combining both sources to better detect fake
videos. This detection approach has been experimentally evaluated using the
latest public databases in the field: Celeb-DF and DFDC. The results achieved,
above 98% AUC (Area Under the Curve) on both databases, outperform the state of
the art and prove the success of fake detectors based on physiological
measurement to detect the latest DeepFake videos.
</p>
<a href="http://arxiv.org/abs/2010.00400">arXiv:2010.00400</a> [<a href="http://arxiv.org/pdf/2010.00400">pdf</a>]

<h2>Quasar Detection using Linear Support Vector Machine with Learning From Mistakes Methodology. (arXiv:2010.00401v1 [cs.LG])</h2>
<h3>Aniruddh Herle, Janamejaya Channegowda, Dinakar Prabhu</h3>
<p>The field of Astronomy requires the collection and assimilation of vast
volumes of data. The data handling and processing problem has become severe as
the sheer volume of data produced by scientific instruments each night grows
exponentially. This problem becomes extensive for conventional methods of
processing the data, which was mostly manual, but is the perfect setting for
the use of Machine Learning approaches. While building classifiers for
Astronomy, the cost of losing a rare object like supernovae or quasars to
detection losses is far more severe than having many false positives, given the
rarity and scientific value of these objects. In this paper, a Linear Support
Vector Machine (LSVM) is explored to detect Quasars, which are extremely bright
objects in which a supermassive black hole is surrounded by a luminous
accretion disk. In Astronomy, it is vital to correctly identify quasars, as
they are very rare in nature. Their rarity creates a class-imbalance problem
that needs to be taken into consideration. The class-imbalance problem and high
cost of misclassification are taken into account while designing the
classifier. To achieve this detection, a novel classifier is explored, and its
performance is evaluated. It was observed that LSVM along with Ensemble Bagged
Trees (EBT) achieved a 10x reduction in the False Negative Rate, using the
Learning from Mistakes methodology.
</p>
<a href="http://arxiv.org/abs/2010.00401">arXiv:2010.00401</a> [<a href="http://arxiv.org/pdf/2010.00401">pdf</a>]

<h2>From Trees to Continuous Embeddings and Back: Hyperbolic Hierarchical Clustering. (arXiv:2010.00402v1 [cs.DS])</h2>
<h3>Ines Chami, Albert Gu, Vaggos Chatziafratis, Christopher R&#xe9;</h3>
<p>Similarity-based Hierarchical Clustering (HC) is a classical unsupervised
machine learning algorithm that has traditionally been solved with heuristic
algorithms like Average-Linkage. Recently, Dasgupta reframed HC as a discrete
optimization problem by introducing a global cost function measuring the
quality of a given tree. In this work, we provide the first continuous
relaxation of Dasgupta's discrete optimization problem with provable quality
guarantees. The key idea of our method, HypHC, is showing a direct
correspondence from discrete trees to continuous representations (via the
hyperbolic embeddings of their leaf nodes) and back (via a decoding algorithm
that maps leaf embeddings to a dendrogram), allowing us to search the space of
discrete binary trees with continuous optimization. Building on analogies
between trees and hyperbolic space, we derive a continuous analogue for the
notion of lowest common ancestor, which leads to a continuous relaxation of
Dasgupta's discrete objective. We can show that after decoding, the global
minimizer of our continuous relaxation yields a discrete tree with a (1 +
epsilon)-factor approximation for Dasgupta's optimal tree, where epsilon can be
made arbitrarily small and controls optimization challenges. We experimentally
evaluate HypHC on a variety of HC benchmarks and find that even approximate
solutions found with gradient descent have superior clustering quality than
agglomerative heuristics or other gradient based algorithms. Finally, we
highlight the flexibility of HypHC using end-to-end training in a downstream
classification task.
</p>
<a href="http://arxiv.org/abs/2010.00402">arXiv:2010.00402</a> [<a href="http://arxiv.org/pdf/2010.00402">pdf</a>]

<h2>A Direct-Indirect Hybridization Approach to Control-Limited DDP. (arXiv:2010.00411v1 [cs.RO])</h2>
<h3>Carlos Mastalli, Wolfgang Merkt, Josep Marti-Saumell, Joan Sola, Nicolas Mansard, Sethu Vijayakumar</h3>
<p>Optimal control is a widely used tool for synthesizing motions and controls
for user-defined tasks under physical constraints. A common approach is to
formulate it using direct multiple-shooting and then to use off-the-shelf
nonlinear programming solvers that can easily handle arbitrary constraints on
the controls and states. However, these methods are not fast enough for many
robotics applications such as real-time humanoid motor control. Exploiting the
sparse structure of optimal control problem, such as in Differential
DynamicProgramming (DDP), has proven to significantly boost the computational
efficiency, and recent works have been focused on handling arbitrary
constraints. Despite that, DDP has been associated with poor numerical
convergence, particularly when considering long time horizons. One of the main
reasons is due to system instabilities and poor warm-starting (only controls).
This paper presents control-limited Feasibility-driven DDP (Box-FDDP), a solver
that incorporates a direct-indirect hybridization of the control-limited DDP
algorithm. Concretely, the forward and backward passes handle feasibility and
control limits. We showcase the impact and importance of our method on a set of
challenging optimal control problems against the Box-DDP and squashing-function
approach.
</p>
<a href="http://arxiv.org/abs/2010.00411">arXiv:2010.00411</a> [<a href="http://arxiv.org/pdf/2010.00411">pdf</a>]

<h2>Learning to be safe, in finite time. (arXiv:2010.00417v1 [cs.LG])</h2>
<h3>Agustin Castellano, Juan Bazerque, Enrique Mallada</h3>
<p>This paper aims to put forward the concept that learning to take safe actions
in unknown environments, even with probability one guarantees, can be achieved
without the need for an unbounded number of exploratory trials, provided that
one is willing to relax its optimality requirements mildly. We focus on the
canonical multi-armed bandit problem and seek to study the
exploration-preservation trade-off intrinsic within safe learning. More
precisely, by defining a handicap metric that counts the number of unsafe
actions, we provide an algorithm for discarding unsafe machines (or actions),
with probability one, that achieves constant handicap. Our algorithm is rooted
in the classical sequential probability ratio test, redefined here for
continuing tasks. Under standard assumptions on sufficient exploration, our
rule provably detects all unsafe machines in an (expected) finite number of
rounds. The analysis also unveils a trade-off between the number of rounds
needed to secure the environment and the probability of discarding safe
machines. Our decision rule can wrap around any other algorithm to optimize a
specific auxiliary goal since it provides a safe environment to search for
(approximately) optimal policies. Simulations corroborate our theoretical
findings and further illustrate the aforementioned trade-offs.
</p>
<a href="http://arxiv.org/abs/2010.00417">arXiv:2010.00417</a> [<a href="http://arxiv.org/pdf/2010.00417">pdf</a>]

<h2>A computationally efficient reconstruction algorithm for circular cone-beam computed tomography using shallow neural networks. (arXiv:2010.00421v1 [eess.IV])</h2>
<h3>Marinus J. Lagerwerf, Daniel M Pelt, Willem Jan Palenstijn, K Joost Batenburg</h3>
<p>Circular cone-beam (CCB) Computed Tomography (CT) has become an integral part
of industrial quality control, materials science and medical imaging. The need
to acquire and process each scan in a short time naturally leads to trade-offs
between speed and reconstruction quality, creating a need for fast
reconstruction algorithms capable of creating accurate reconstructions from
limited data.

In this paper we introduce the Neural Network Feldkamp-Davis-Kress (NN-FDK)
algorithm. This algorithm adds a machine learning component to the FDK
algorithm to improve its reconstruction accuracy while maintaining its
computational efficiency. Moreover, the NN-FDK algorithm is designed such that
it has low training data requirements and is fast to train. This ensures that
the proposed algorithm can be used to improve image quality in high throughput
CT scanning settings, where FDK is currently used to keep pace with the
acquisition speed using readily available computational resources.

We compare the NN-FDK algorithm to two standard CT reconstruction algorithms
and to two popular deep neural networks trained to remove reconstruction
artifacts from the 2D slices of an FDK reconstruction. We show that the NN-FDK
reconstruction algorithm is substantially faster in computing a reconstruction
than all the tested alternative methods except for the standard FDK algorithm
and we show it can compute accurate CCB CT reconstructions in cases of high
noise, a low number of projection angles or large cone angles. Moreover, we
show that the training time of an NN-FDK network is orders of magnitude lower
than the considered deep neural networks, with only a slight reduction in
reconstruction accuracy.
</p>
<a href="http://arxiv.org/abs/2010.00421">arXiv:2010.00421</a> [<a href="http://arxiv.org/pdf/2010.00421">pdf</a>]

<h2>Community detection, pattern recognition, and hypergraph-based learning: approaches using metric geometry and persistent homology. (arXiv:2010.00435v1 [cs.SI])</h2>
<h3>Dong Quan Ngoc Nguyen, Lin Xing, Lizhen Lin</h3>
<p>Hypergraph data appear and are hidden in many places in the modern age. They
are data structure that can be used to model many real data examples since
their structures contain information about higher order relations among data
points. One of the main contributions of our paper is to introduce a new
topological structure to hypergraph data which bears a resemblance to a usual
metric space structure. Using this new topological space structure of
hypergraph data, we propose several approaches to study community detection
problem, detecting persistent features arising from homological structure of
hypergraph data. Also based on the topological space structure of hypergraph
data introduced in our paper, we introduce a modified nearest neighbors methods
which is a generalization of the classical nearest neighbors methods from
machine learning. Our modified nearest neighbors methods have an advantage of
being very flexible and applicable even for discrete structures as in
hypergraphs. We then apply our modified nearest neighbors methods to study sign
prediction problem in hypegraph data constructed using our method.
</p>
<a href="http://arxiv.org/abs/2010.00435">arXiv:2010.00435</a> [<a href="http://arxiv.org/pdf/2010.00435">pdf</a>]

<h2>Learning Set Functions that are Sparse in Non-Orthogonal Fourier Bases. (arXiv:2010.00439v1 [cs.LG])</h2>
<h3>Chris Wendler, Andisheh Amrollahi, Bastian Seifert, Andreas Krause, Markus P&#xfc;schel</h3>
<p>Many applications of machine learning on discrete domains, such as learning
preference functions in recommender systems or auctions, can be reduced to
estimating a set function that is sparse in the Fourier domain. In this work,
we present a new family of algorithms for learning Fourier-sparse set
functions. They require at most $nk - k \log_2 k + k$ queries (set function
evaluations), under mild conditions on the Fourier coefficients, where $n$ is
the size of the ground set and $k$ the number of non-zero Fourier coefficients.
In contrast to other work that focused on the orthogonal Walsh-Hadamard
transform, our novel algorithms operate with recently introduced non-orthogonal
Fourier transforms that offer different notions of Fourier-sparsity. These
naturally arise when modeling, e.g., sets of items forming substitutes and
complements. We demonstrate effectiveness on several real-world applications.
</p>
<a href="http://arxiv.org/abs/2010.00439">arXiv:2010.00439</a> [<a href="http://arxiv.org/pdf/2010.00439">pdf</a>]

<h2>X-Fields: Implicit Neural View-, Light- and Time-Image Interpolation. (arXiv:2010.00450v1 [cs.CV])</h2>
<h3>Mojtaba Bemana, Karol Myszkowski, Hans-Peter Seidel, Tobias Ritschel</h3>
<p>We suggest to represent an X-Field -a set of 2D images taken across different
view, time or illumination conditions, i.e., video, light field, reflectance
fields or combinations thereof-by learning a neural network (NN) to map their
view, time or light coordinates to 2D images. Executing this NN at new
coordinates results in joint view, time or light interpolation. The key idea to
make this workable is a NN that already knows the "basic tricks" of graphics
(lighting, 3D projection, occlusion) in a hard-coded and differentiable form.
The NN represents the input to that rendering as an implicit map, that for any
view, time, or light coordinate and for any pixel can quantify how it will move
if view, time or light coordinates change (Jacobian of pixel position with
respect to view, time, illumination, etc.). Our X-Field representation is
trained for one scene within minutes, leading to a compact set of trainable
parameters and hence real-time navigation in view, time and illumination.
</p>
<a href="http://arxiv.org/abs/2010.00450">arXiv:2010.00450</a> [<a href="http://arxiv.org/pdf/2010.00450">pdf</a>]

<h2>From Handcrafted to Deep Features for Pedestrian Detection: A Survey. (arXiv:2010.00456v1 [cs.CV])</h2>
<h3>Jiale Cao, Yanwei Pang, Jin Xie, Fahad Shahbaz Khan, Ling Shao</h3>
<p>Pedestrian detection is an important but challenging problem in computer
vision, especially in human-centric tasks. Over the past decade, significant
improvement has been witnessed with the help of handcrafted features and deep
features. Here we present a comprehensive survey on recent advances in
pedestrian detection. First, we provide a detailed review of single-spectral
pedestrian detection that includes handcrafted features based methods and deep
features based approaches. For handcrafted features based methods, we present
an extensive review of approaches and find that handcrafted features with large
freedom degrees in shape and space have better performance. In the case of deep
features based approaches, we split them into pure CNN based methods and those
employing both handcrafted and CNN based features. We give the statistical
analysis and tendency of these methods, where feature enhanced, part-aware, and
post-processing methods have attracted main attention. In addition to
single-spectral pedestrian detection, we also review multi-spectral pedestrian
detection, which provides more robust features for illumination variance.
Furthermore, we introduce some related datasets and evaluation metrics, and
compare some representative methods. We conclude this survey by emphasizing
open problems that need to be addressed and highlighting various future
directions. Researchers can track an up-to-date list at
https://github.com/JialeCao001/PedSurvey.
</p>
<a href="http://arxiv.org/abs/2010.00456">arXiv:2010.00456</a> [<a href="http://arxiv.org/pdf/2010.00456">pdf</a>]

<h2>A survey on natural language processing (nlp) and applications in insurance. (arXiv:2010.00462v1 [stat.ML])</h2>
<h3>Antoine Ly, Benno Uthayasooriyar, Tingting Wang</h3>
<p>Text is the most widely used means of communication today. This data is
abundant but nevertheless complex to exploit within algorithms. For years,
scientists have been trying to implement different techniques that enable
computers to replicate some mechanisms of human reading. During the past five
years, research disrupted the capacity of the algorithms to unleash the value
of text data. It brings today, many opportunities for the insurance
industry.Understanding those methods and, above all, knowing how to apply them
is a major challenge and key to unleash the value of text data that have been
stored for many years. Processing language with computer brings many new
opportunities especially in the insurance sector where reports are central in
the information used by insurers. SCOR's Data Analytics team has been working
on the implementation of innovative tools or products that enable the use of
the latest research on text analysis. Understanding text mining techniques in
insurance enhances the monitoring of the underwritten risks and many processes
that finally benefit policyholders.This article proposes to explain
opportunities that Natural Language Processing (NLP) are providing to
insurance. It details different methods used today in practice traces back the
story of them. We also illustrate the implementation of certain methods using
open source libraries and python codes that we have developed to facilitate the
use of these techniques.After giving a general overview on the evolution of
text mining during the past few years,we share about how to conduct a full
study with text mining and share some examples to serve those models into
insurance products or services. Finally, we explained in more details every
step that composes a Natural Language Processing study to ensure the reader can
have a deep understanding on the implementation.
</p>
<a href="http://arxiv.org/abs/2010.00462">arXiv:2010.00462</a> [<a href="http://arxiv.org/pdf/2010.00462">pdf</a>]

<h2>Bag of Tricks for Adversarial Training. (arXiv:2010.00467v1 [cs.LG])</h2>
<h3>Tianyu Pang, Xiao Yang, Yinpeng Dong, Hang Su, Jun Zhu</h3>
<p>Adversarial training (AT) is one of the most effective strategies for
promoting model robustness. However, recent benchmarks show that most of the
proposed improvements on AT are less effective than simply early stopping the
training procedure. This counter-intuitive fact motivates us to investigate the
implementation details of tens of AT methods. Surprisingly, we find that the
basic training settings (e.g., weight decay, learning rate schedule, etc.) used
in these methods are highly inconsistent, which could largely affect the model
performance as shown in our experiments. For example, a slightly different
value of weight decay can reduce the model robust accuracy by more than 7%,
which is probable to override the potential promotion induced by the proposed
methods. In this work, we provide comprehensive evaluations on the effects of
basic training tricks and hyperparameter settings for adversarially trained
models. We provide a reasonable baseline setting and re-implement previous
defenses to achieve new state-of-the-art results.
</p>
<a href="http://arxiv.org/abs/2010.00467">arXiv:2010.00467</a> [<a href="http://arxiv.org/pdf/2010.00467">pdf</a>]

<h2>High Quality Remote Sensing Image Super-Resolution Using Deep Memory Connected Network. (arXiv:2010.00472v1 [eess.IV])</h2>
<h3>Wenjia Xu, Guangluan Xu, Yang Wang, Xian Sun, Daoyu Lin, Yirong Wu</h3>
<p>Single image super-resolution is an effective way to enhance the spatial
resolution of remote sensing image, which is crucial for many applications such
as target detection and image classification. However, existing methods based
on the neural network usually have small receptive fields and ignore the image
detail. We propose a novel method named deep memory connected network (DMCN)
based on a convolutional neural network to reconstruct high-quality
super-resolution images. We build local and global memory connections to
combine image detail with environmental information. To further reduce
parameters and ease time-consuming, we propose downsampling units, shrinking
the spatial size of feature maps. We test DMCN on three remote sensing datasets
with different spatial resolution. Experimental results indicate that our
method yields promising improvements in both accuracy and visual performance
over the current state-of-the-art.
</p>
<a href="http://arxiv.org/abs/2010.00472">arXiv:2010.00472</a> [<a href="http://arxiv.org/pdf/2010.00472">pdf</a>]

<h2>Mini-DDSM: Mammography-based Automatic Age Estimation. (arXiv:2010.00494v1 [cs.CV])</h2>
<h3>Charitha Dissanayake Lekamlage, Fabia Afzal, Erik Westerberg, Abbas Cheddad</h3>
<p>Age estimation has attracted attention for its various medical applications.
There are many studies on human age estimation from biomedical images. However,
there is no research done on mammograms for age estimation, as far as we know.
The purpose of this study is to devise an AI-based model for estimating age
from mammogram images. Due to lack of public mammography data sets that have
the age attribute, we resort to using a web crawler to download thumbnail
mammographic images and their age fields from the public data set; the Digital
Database for Screening Mammography. The original images in this data set
unfortunately can only be retrieved by a software which is broken.
Subsequently, we extracted deep learning features from the collected data set,
by which we built a model using Random Forests regressor to estimate the age
automatically. The performance assessment was measured using the mean absolute
error values. The average error value out of 10 tests on random selection of
samples was around 8 years. In this paper, we show the merits of this approach
to fill up missing age values. We ran logistic and linear regression models on
another independent data set to further validate the advantage of our proposed
work. This paper also introduces the free-access Mini-DDSM data set.
</p>
<a href="http://arxiv.org/abs/2010.00494">arXiv:2010.00494</a> [<a href="http://arxiv.org/pdf/2010.00494">pdf</a>]

<h2>Ray-based classification framework for high-dimensional data. (arXiv:2010.00500v1 [cs.LG])</h2>
<h3>Justyna P. Zwolak, Sandesh S. Kalantre, Thomas McJunkin, Brian J. Weber, Jacob M. Taylor</h3>
<p>While classification of arbitrary structures in high dimensions may require
complete quantitative information, for simple geometrical structures,
low-dimensional qualitative information about the boundaries defining the
structures can suffice. Rather than using dense, multi-dimensional data, we
propose a deep neural network (DNN) classification framework that utilizes a
minimal collection of one-dimensional representations, called \emph{rays}, to
construct the "fingerprint" of the structure(s) based on substantially reduced
information. We empirically study this framework using a synthetic dataset of
double and triple quantum dot devices and apply it to the classification
problem of identifying the device state. We show that the performance of the
ray-based classifier is already on par with traditional 2D images for low
dimensional systems, while significantly cutting down the data acquisition
cost.
</p>
<a href="http://arxiv.org/abs/2010.00500">arXiv:2010.00500</a> [<a href="http://arxiv.org/pdf/2010.00500">pdf</a>]

<h2>PipeTune: Pipeline Parallelism of Hyper and System Parameters Tuning for Deep Learning Clusters. (arXiv:2010.00501v1 [cs.DC])</h2>
<h3>Isabelly Rocha, Nathaniel Morrison, Lydia Y. Chen, Pascal Felber, Robert Birke, Valerio Schiavoni</h3>
<p>DNN learning jobs are common in today's clusters due to the advances in AI
driven services such as machine translation and image recognition. The most
critical phase of these jobs for model performance and learning cost is the
tuning of hyperparameters. Existing approaches make use of techniques such as
early stopping criteria to reduce the tuning impact on learning cost. However,
these strategies do not consider the impact that certain hyperparameters and
systems parameters have on training time. This paper presents PipeTune, a
framework for DNN learning jobs that addresses the trade-offs between these two
types of parameters. PipeTune takes advantage of the high parallelism and
recurring characteristics of such jobs to minimize the learning cost via a
pipelined simultaneous tuning of both hyper and system parameters. Our
experimental evaluation using three different types of workloads indicates that
PipeTune achieves up to 22.6% reduction and 1.7x speed up on tuning and
training time, respectively. PipeTune not only improves performance but also
lowers energy consumption up to 29%.
</p>
<a href="http://arxiv.org/abs/2010.00501">arXiv:2010.00501</a> [<a href="http://arxiv.org/pdf/2010.00501">pdf</a>]

<h2>Cardea: An Open Automated Machine Learning Framework for Electronic Health Records. (arXiv:2010.00509v1 [cs.LG])</h2>
<h3>Sarah Alnegheimish, Najat Alrashed, Faisal Aleissa, Shahad Althobaiti, Dongyu Liu, Mansour Alsaleh, Kalyan Veeramachaneni</h3>
<p>An estimated 180 papers focusing on deep learning and EHR were published
between 2010 and 2018. Despite the common workflow structure appearing in these
publications, no trusted and verified software framework exists, forcing
researchers to arduously repeat previous work. In this paper, we propose
Cardea, an extensible open-source automated machine learning framework
encapsulating common prediction problems in the health domain and allows users
to build predictive models with their own data. This system relies on two
components: Fast Healthcare Interoperability Resources (FHIR) -- a standardized
data structure for electronic health systems -- and several AUTOML frameworks
for automated feature engineering, model selection, and tuning. We augment
these components with an adaptive data assembler and comprehensive data- and
model- auditing capabilities. We demonstrate our framework via 5 prediction
tasks on MIMIC-III and Kaggle datasets, which highlight Cardea's human
competitiveness, flexibility in problem definition, extensive feature
generation capability, adaptable automatic data assembler, and its usability.
</p>
<a href="http://arxiv.org/abs/2010.00509">arXiv:2010.00509</a> [<a href="http://arxiv.org/pdf/2010.00509">pdf</a>]

<h2>Few-Shot Classification By Few-Iteration Meta-Learning. (arXiv:2010.00511v1 [cs.CV])</h2>
<h3>Ardhendu Shekhar Tripathi, Martin Danelljan, Luc Van Gool, Radu Timofte</h3>
<p>Learning in a low-data regime from only a few labeled examples is an
important, but challenging problem. Recent advancements within meta-learning
have demonstrated encouraging performance, in particular, for the task of
few-shot classification. We propose a novel optimization-based meta-learning
approach for few-shot classification. It consists of an embedding network,
providing a general representation of the image, and a base learner module. The
latter learns a linear classifier during the inference through an unrolled
optimization procedure. We design an inner learning objective composed of (i) a
robust classification loss on the support set and (ii) an entropy loss,
allowing transductive learning from unlabeled query samples. By employing an
efficient initialization module and a Steepest Descent based optimization
algorithm, our base learner predicts a powerful classifier within only a few
iterations. Further, our strategy enables important aspects of the base learner
objective to be learned during meta-training. To the best of our knowledge,
this work is the first to integrate both induction and transduction into the
base learner in an optimization-based meta-learning framework. We perform a
comprehensive experimental analysis, demonstrating the effectiveness of our
approach on four few-shot classification datasets.
</p>
<a href="http://arxiv.org/abs/2010.00511">arXiv:2010.00511</a> [<a href="http://arxiv.org/pdf/2010.00511">pdf</a>]

<h2>Neural encoding with visual attention. (arXiv:2010.00516v1 [cs.CV])</h2>
<h3>Meenakshi Khosla, Gia H. Ngo, Keith Jamison, Amy Kuceyeski, Mert R. Sabuncu</h3>
<p>Visual perception is critically influenced by the focus of attention. Due to
limited resources, it is well known that neural representations are biased in
favor of attended locations. Using concurrent eye-tracking and functional
Magnetic Resonance Imaging (fMRI) recordings from a large cohort of human
subjects watching movies, we first demonstrate that leveraging gaze
information, in the form of attentional masking, can significantly improve
brain response prediction accuracy in a neural encoding model. Next, we propose
a novel approach to neural encoding by including a trainable soft-attention
module. Using our new approach, we demonstrate that it is possible to learn
visual attention policies by end-to-end learning merely on fMRI response data,
and without relying on any eye-tracking. Interestingly, we find that attention
locations estimated by the model on independent data agree well with the
corresponding eye fixation patterns, despite no explicit supervision to do so.
Together, these findings suggest that attention modules can be instrumental in
neural encoding models of visual stimuli.
</p>
<a href="http://arxiv.org/abs/2010.00516">arXiv:2010.00516</a> [<a href="http://arxiv.org/pdf/2010.00516">pdf</a>]

<h2>Why Adversarial Interaction Creates Non-Homogeneous Patterns: A Pseudo-Reaction-Diffusion Model for Turing Instability. (arXiv:2010.00521v1 [cs.LG])</h2>
<h3>Litu Rout</h3>
<p>Long after Turing's seminal Reaction-Diffusion (RD) model, the elegance of
his fundamental equations alleviated much of the skepticism surrounding pattern
formation. Though Turing model is a simplification and an idealization, it is
one of the best-known theoretical models to explain patterns as a reminiscent
of those observed in nature. Over the years, concerted efforts have been made
to align theoretical models to explain patterns in real systems. The apparent
difficulty in identifying the specific dynamics of the RD system makes the
problem particularly challenging. Interestingly, we observe Turing-like
patterns in a system of neurons with adversarial interaction. In this study, we
establish the involvement of Turing instability to create such patterns. By
theoretical and empirical studies, we present a pseudo-reaction-diffusion model
to explain the mechanism that may underlie these phenomena. While supervised
learning attains homogeneous equilibrium, this paper suggests that the
introduction of an adversary helps break this homogeneity to create
non-homogeneous patterns at equilibrium. Further, we prove that randomly
initialized gradient descent with over-parameterization can converge
exponentially fast to an $\epsilon$-stationary point even under adversarial
interaction. In addition, different from sole supervision, we show that the
solutions obtained under adversarial interaction are not limited to a tiny
subspace around initialization.
</p>
<a href="http://arxiv.org/abs/2010.00521">arXiv:2010.00521</a> [<a href="http://arxiv.org/pdf/2010.00521">pdf</a>]

<h2>A biologically plausible neural network for multi-channel Canonical Correlation Analysis. (arXiv:2010.00525v1 [q-bio.NC])</h2>
<h3>David Lipshutz, Yanis Bahroun, Siavash Golkar, Anirvan M. Sengupta, Dmitri B. Chkovskii</h3>
<p>Cortical pyramidal neurons receive inputs from multiple distinct neural
populations and integrate these inputs in separate dendritic compartments. We
explore the possibility that cortical microcircuits implement Canonical
Correlation Analysis (CCA), an unsupervised learning method that projects the
inputs onto a common subspace so as to maximize the correlations between the
projections. To this end, we seek a multi-channel CCA algorithm that can be
implemented in a biologically plausible neural network. For biological
plausibility, we require that the network operates in the online setting and
its synaptic update rules are local. Starting from a novel CCA objective
function, we derive an online optimization algorithm whose optimization steps
can be implemented in a single-layer neural network with multi-compartmental
neurons and local non-Hebbian learning rules. We also derive an extension of
our online CCA algorithm with adaptive output rank and output whitening.
Interestingly, the extension maps onto a neural network whose neural
architecture and synaptic updates resemble neural circuitry and synaptic
plasticity observed experimentally in cortical pyramidal neurons.
</p>
<a href="http://arxiv.org/abs/2010.00525">arXiv:2010.00525</a> [<a href="http://arxiv.org/pdf/2010.00525">pdf</a>]

<h2>A Multi-modal Machine Learning Approach and Toolkit to Automate Recognition of Early Stages of Dementia among British Sign Language Users. (arXiv:2010.00536v1 [cs.CV])</h2>
<h3>Xing Liang, Anastassia Angelopoulou, Epaminondas Kapetanios, Bencie Woll, Reda Al-batat, Tyron Woolfe</h3>
<p>The ageing population trend is correlated with an increased prevalence of
acquired cognitive impairments such as dementia. Although there is no cure for
dementia, a timely diagnosis helps in obtaining necessary support and
appropriate medication. Researchers are working urgently to develop effective
technological tools that can help doctors undertake early identification of
cognitive disorder. In particular, screening for dementia in ageing Deaf
signers of British Sign Language (BSL) poses additional challenges as the
diagnostic process is bound up with conditions such as quality and availability
of interpreters, as well as appropriate questionnaires and cognitive tests. On
the other hand, deep learning based approaches for image and video analysis and
understanding are promising, particularly the adoption of Convolutional Neural
Network (CNN), which require large amounts of training data. In this paper,
however, we demonstrate novelty in the following way: a) a multi-modal machine
learning based automatic recognition toolkit for early stages of dementia among
BSL users in that features from several parts of the body contributing to the
sign envelope, e.g., hand-arm movements and facial expressions, are combined,
b) universality in that it is possible to apply our technique to users of any
sign language, since it is language independent, c) given the trade-off between
complexity and accuracy of machine learning (ML) prediction models as well as
the limited amount of training and testing data being available, we show that
our approach is not over-fitted and has the potential to scale up.
</p>
<a href="http://arxiv.org/abs/2010.00536">arXiv:2010.00536</a> [<a href="http://arxiv.org/pdf/2010.00536">pdf</a>]

<h2>Robustness Analysis of Neural Networks via Efficient Partitioning: Theory and Applications in Control Systems. (arXiv:2010.00540v1 [cs.LG])</h2>
<h3>Michael Everett, Golnaz Habibi, Jonathan P. How</h3>
<p>Neural networks (NNs) are now routinely implemented on systems that must
operate in uncertain environments, but the tools for formally analyzing how
this uncertainty propagates to NN outputs are not yet commonplace. Computing
tight bounds on NN output sets (given an input set) provides a measure of
confidence associated with the NN decisions and is essential to deploy NNs on
safety-critical systems. Recent works approximate the propagation of sets
through nonlinear activations or partition the uncertainty set to provide a
guaranteed outer bound on the set of possible NN outputs. However, the bound
looseness causes excessive conservatism and/or the computation is too slow for
online analysis. This paper unifies propagation and partition approaches to
provide a family of robustness analysis algorithms that give tighter bounds
than existing works for the same amount of computation time (or reduced
computational effort for a desired accuracy level). Moreover, we provide new
partitioning techniques that are aware of their current bound estimates and
desired boundary shape (e.g., lower bounds, weighted $\ell_\infty$-ball, convex
hull), leading to further improvements in the computation-tightness tradeoff.
The paper demonstrates the tighter bounds and reduced conservatism of the
proposed robustness analysis framework with examples from model-free RL and
forward kinematics learning.
</p>
<a href="http://arxiv.org/abs/2010.00540">arXiv:2010.00540</a> [<a href="http://arxiv.org/pdf/2010.00540">pdf</a>]

<h2>Deep learning for time series classification. (arXiv:2010.00567v1 [cs.LG])</h2>
<h3>Hassan Ismail Fawaz</h3>
<p>Time series analysis is a field of data science which is interested in
analyzing sequences of numerical values ordered in time. Time series are
particularly interesting because they allow us to visualize and understand the
evolution of a process over time. Their analysis can reveal trends,
relationships and similarities across the data. There exists numerous fields
containing data in the form of time series: health care (electrocardiogram,
blood sugar, etc.), activity recognition, remote sensing, finance (stock market
price), industry (sensors), etc. Time series classification consists of
constructing algorithms dedicated to automatically label time series data. The
sequential aspect of time series data requires the development of algorithms
that are able to harness this temporal property, thus making the existing
off-the-shelf machine learning models for traditional tabular data suboptimal
for solving the underlying task. In this context, deep learning has emerged in
recent years as one of the most effective methods for tackling the supervised
classification task, particularly in the field of computer vision. The main
objective of this thesis was to study and develop deep neural networks
specifically constructed for the classification of time series data. We thus
carried out the first large scale experimental study allowing us to compare the
existing deep methods and to position them compared other non-deep learning
based state-of-the-art methods. Subsequently, we made numerous contributions in
this area, notably in the context of transfer learning, data augmentation,
ensembling and adversarial attacks. Finally, we have also proposed a novel
architecture, based on the famous Inception network (Google), which ranks among
the most efficient to date.
</p>
<a href="http://arxiv.org/abs/2010.00567">arXiv:2010.00567</a> [<a href="http://arxiv.org/pdf/2010.00567">pdf</a>]

<h2>DASGIL: Domain Adaptation for Semantic and Geometric-aware Image-based Localization. (arXiv:2010.00573v1 [cs.CV])</h2>
<h3>Hanjiang Hu, Ming Cheng, Zhe Liu, Hesheng Wang</h3>
<p>Long-Term visual localization under changing environments is a challenging
problem in autonomous driving and mobile robotics due to season, illumination
variance, etc. Image retrieval for localization is an efficient and effective
solution to the problem. In this paper, we propose a novel multi-task
architecture to fuse the geometric and semantic information into the
multi-scale latent embedding representation for visual place recognition. To
use the high-quality ground truths without any human effort, depth and
segmentation generator model is trained on virtual synthetic dataset and domain
adaptation is adopted from synthetic to real-world dataset. The multi-scale
model presents the strong generalization ability on real-world KITTI dataset
though trained on the virtual KITTI 2 dataset. The proposed approach is
validated on the Extended CMU-Seasons dataset through a series of crucial
comparison experiments, where our performance outperforms state-of-the-art
baselines for retrieval-based localization under the challenging environment.
</p>
<a href="http://arxiv.org/abs/2010.00573">arXiv:2010.00573</a> [<a href="http://arxiv.org/pdf/2010.00573">pdf</a>]

<h2>D3C: Reducing the Price of Anarchy in Multi-Agent Learning. (arXiv:2010.00575v1 [cs.MA])</h2>
<h3>Ian Gemp, Kevin R. McKee, Richard Everett, Edgar A. Du&#xe9;&#xf1;ez-Guzm&#xe1;n, Yoram Bachrach, David Balduzzi, Andrea Tacchetti</h3>
<p>Even in simple multi-agent systems, fixed incentives can lead to outcomes
that are poor for the group and each individual agent. We propose a method,
D3C, for online adjustment of agent incentives that reduces the loss incurred
at a Nash equilibrium. Agents adjust their incentives by learning to mix their
incentive with that of other agents, until a compromise is reached in a
distributed fashion. We show that D3C improves outcomes for each agent and the
group as a whole on several social dilemmas including a traffic network with
Braess's paradox, a prisoner's dilemma, and several reinforcement learning
domains.
</p>
<a href="http://arxiv.org/abs/2010.00575">arXiv:2010.00575</a> [<a href="http://arxiv.org/pdf/2010.00575">pdf</a>]

<h2>Understanding Self-supervised Learning with Dual Deep Networks. (arXiv:2010.00578v1 [cs.LG])</h2>
<h3>Yuandong Tian, Lantao Yu, Xinlei Chen, Surya Ganguli</h3>
<p>We propose a novel theoretical framework to understand self-supervised
learning methods that employ dual pairs of deep ReLU networks (e.g., SimCLR,
BYOL). First, we prove that in each SGD update of SimCLR, the weights at each
layer are updated by a \emph{covariance operator} that specifically amplifies
initial random selectivities that vary across data samples but survive averages
over data augmentations, which we show leads to the emergence of hierarchical
features, if the input data are generated from a hierarchical latent tree
model. With the same framework, we also show analytically that BYOL works due
to an implicit contrastive term, acting as an approximate covariance operator.
The term is formed by the inter-play between the zero-mean operation of
BatchNorm and the extra predictor in the online network. Extensive ablation
studies justify our theoretical findings.
</p>
<a href="http://arxiv.org/abs/2010.00578">arXiv:2010.00578</a> [<a href="http://arxiv.org/pdf/2010.00578">pdf</a>]

<h2>Multi-agent Social Reinforcement Learning Improves Generalization. (arXiv:2010.00581v1 [cs.LG])</h2>
<h3>Kamal Ndousse, Douglas Eck, Sergey Levine, Natasha Jaques</h3>
<p>Social learning is a key component of human and animal intelligence. By
taking cues from the behavior of experts in their environment, social learners
can acquire sophisticated behavior and rapidly adapt to new circumstances. This
paper investigates whether independent reinforcement learning (RL) agents in a
multi-agent environment can use social learning to improve their performance
using cues from other agents. We find that in most circumstances, vanilla
model-free RL agents do not use social learning, even in environments in which
individual exploration is expensive. We analyze the reasons for this
deficiency, and show that by introducing a model-based auxiliary loss we are
able to train agents to lever-age cues from experts to solve hard exploration
tasks. The generalized social learning policy learned by these agents allows
them to not only outperform the experts with which they trained, but also
achieve better zero-shot transfer performance than solo learners when deployed
to novel environments with experts. In contrast, agents that have not learned
to rely on social learning generalize poorly and do not succeed in the transfer
task. Further,we find that by mixing multi-agent and solo training, we can
obtain agents that use social learning to out-perform agents trained alone,
even when experts are not avail-able. This demonstrates that social learning
has helped improve agents' representation of the task itself. Our results
indicate that social learning can enable RL agents to not only improve
performance on the task at hand, but improve generalization to novel
environments.
</p>
<a href="http://arxiv.org/abs/2010.00581">arXiv:2010.00581</a> [<a href="http://arxiv.org/pdf/2010.00581">pdf</a>]

<h2>Utilizing Transfer Learning and a Customized Loss Function for Optic Disc Segmentation from Retinal Images. (arXiv:2010.00583v1 [eess.IV])</h2>
<h3>Abdullah Sarhan, Ali Al-Khaz&#xc1;ly, Adam Gorner, Andrew Swift, Jon Rokne, Reda Alhajj, Andrew Crichton</h3>
<p>Accurate segmentation of the optic disc from a retinal image is vital to
extracting retinal features that may be highly correlated with retinal
conditions such as glaucoma. In this paper, we propose a deep-learning based
approach capable of segmenting the optic disc given a high-precision retinal
fundus image. Our approach utilizes a UNET-based model with a VGG16 encoder
trained on the ImageNet dataset. This study can be distinguished from other
studies in the customization made for the VGG16 model, the diversity of the
datasets adopted, the duration of disc segmentation, the loss function
utilized, and the number of parameters required to train our model. Our
approach was tested on seven publicly available datasets augmented by a dataset
from a private clinic that was annotated by two Doctors of Optometry through a
web portal built for this purpose. We achieved an accuracy of 99.78\% and a
Dice coefficient of 94.73\% for a disc segmentation from a retinal image in
0.03 seconds. The results obtained from comprehensive experiments demonstrate
the robustness of our approach to disc segmentation of retinal images obtained
from different sources.
</p>
<a href="http://arxiv.org/abs/2010.00583">arXiv:2010.00583</a> [<a href="http://arxiv.org/pdf/2010.00583">pdf</a>]

<h2>Universal Adversarial Audio Perturbations. (arXiv:1908.03173v4 [cs.LG] UPDATED)</h2>
<h3>Sajjad Abdoli, Luiz G. Hafemann, Jerome Rony, Ismail Ben Ayed, Patrick Cardinal, Alessandro L. Koerich</h3>
<p>We demonstrate the existence of universal adversarial perturbations, which
can fool a family of audio classification architectures, for both targeted and
untargeted attack scenarios. We propose two methods for finding such
perturbations. The first method is based on an iterative, greedy approach that
is well-known in computer vision: it aggregates small perturbations to the
input so as to push it to the decision boundary. The second method, which is
the main contribution of this work, is a novel penalty formulation, which finds
targeted and untargeted universal adversarial perturbations. Differently from
the greedy approach, the penalty method minimizes an appropriate objective
function on a batch of samples. Therefore, it produces more successful attacks
when the number of training samples is limited. Moreover, we provide a proof
that the proposed penalty method theoretically converges to a solution that
corresponds to universal adversarial perturbations. We also demonstrate that it
is possible to provide successful attacks using the penalty method when only
one sample from the target dataset is available for the attacker. Experimental
results on attacking various 1D CNN architectures have shown attack success
rates higher than 85.0% and 83.1% for targeted and untargeted attacks,
respectively using the proposed penalty method.
</p>
<a href="http://arxiv.org/abs/1908.03173">arXiv:1908.03173</a> [<a href="http://arxiv.org/pdf/1908.03173">pdf</a>]

<h2>Aspect and Opinion Term Extraction for Hotel Reviews using Transfer Learning and Auxiliary Labels. (arXiv:1909.11879v5 [cs.CL] UPDATED)</h2>
<h3>Yosef Ardhito Winatmoko, Ali Akbar Septiandri, Arie Pratama Sutiono</h3>
<p>Aspect and opinion term extraction is a critical step in Aspect-Based
Sentiment Analysis (ABSA). Our study focuses on evaluating transfer learning
using pre-trained BERT (Devlin et al., 2018) to classify tokens from hotel
reviews in bahasa Indonesia. The primary challenge is the language informality
of the review texts. By utilizing transfer learning from a multilingual model,
we achieved up to 2% difference on token level F1-score compared to the
state-of-the-art Bi-LSTM model with fewer training epochs (3 vs. 200 epochs).
The fine-tuned model clearly outperforms the Bi-LSTM model on the entity level.
Furthermore, we propose a method to include CRF with auxiliary labels as an
output layer for the BERT-based models. The CRF addition further improves the
F1-score for both token and entity level.
</p>
<a href="http://arxiv.org/abs/1909.11879">arXiv:1909.11879</a> [<a href="http://arxiv.org/pdf/1909.11879">pdf</a>]

<h2>Detecting AI Trojans Using Meta Neural Analysis. (arXiv:1910.03137v4 [cs.AI] UPDATED)</h2>
<h3>Xiaojun Xu, Qi Wang, Huichen Li, Nikita Borisov, Carl A. Gunter, Bo Li</h3>
<p>In machine learning Trojan attacks, an adversary trains a corrupted model
that obtains good performance on normal data but behaves maliciously on data
samples with certain trigger patterns. Several approaches have been proposed to
detect such attacks, but they make undesirable assumptions about the attack
strategies or require direct access to the trained models, which restricts
their utility in practice.

This paper addresses these challenges by introducing a Meta Neural Trojan
Detection (MNTD) pipeline that does not make assumptions on the attack
strategies and only needs black-box access to models. The strategy is to train
a meta-classifier that predicts whether a given target model is Trojaned. To
train the meta-model without knowledge of the attack strategy, we introduce a
technique called jumbo learning that samples a set of Trojaned models following
a general distribution. We then dynamically optimize a query set together with
the meta-classifier to distinguish between Trojaned and benign models.

We evaluate MNTD with experiments on vision, speech, tabular data and natural
language text datasets, and against different Trojan attacks such as data
poisoning attack, model manipulation attack, and latent attack. We show that
MNTD achieves 97% detection AUC score and significantly outperforms existing
detection approaches. In addition, MNTD generalizes well and achieves high
detection performance against unforeseen attacks. We also propose a robust MNTD
pipeline which achieves 90% detection AUC even when the attacker aims to evade
the detection with full knowledge of the system.
</p>
<a href="http://arxiv.org/abs/1910.03137">arXiv:1910.03137</a> [<a href="http://arxiv.org/pdf/1910.03137">pdf</a>]

<h2>Learning to Generate 6-DoF Grasp Poses with Reachability Awareness. (arXiv:1910.06404v3 [cs.RO] UPDATED)</h2>
<h3>Xibai Lou, Yang Yang, Changhyun Choi</h3>
<p>Motivated by the stringent requirements of unstructured real-world where a
plethora of unknown objects reside in arbitrary locations of the surface, we
propose a voxel-based deep 3D Convolutional Neural Network (3D CNN) that
generates feasible 6-DoF grasp poses in unrestricted workspace with
reachability awareness. Unlike the majority of works that predict if a proposed
grasp pose within the restricted workspace will be successful solely based on
grasp pose stability, our approach further learns a reachability predictor that
evaluates if the grasp pose is reachable or not from robot's own experience. To
avoid the laborious real training data collection, we exploit the power of
simulation to train our networks on a large-scale synthetic dataset. This work
is an early attempt that simultaneously evaluates grasping reachability from
learned knowledge while proposing feasible grasp poses with 3D CNN.
Experimental results in both simulation and real-world demonstrate that our
approach outperforms several other methods and achieves 82.5% grasping success
rate on unknown objects.
</p>
<a href="http://arxiv.org/abs/1910.06404">arXiv:1910.06404</a> [<a href="http://arxiv.org/pdf/1910.06404">pdf</a>]

<h2>Deep Neural Network Fingerprinting by Conferrable Adversarial Examples. (arXiv:1912.00888v3 [cs.LG] UPDATED)</h2>
<h3>Nils Lukas, Yuxuan Zhang, Florian Kerschbaum</h3>
<p>In Machine Learning as a Service, a provider trains a deep neural network and
provides many users access. The hosted (source) model is susceptible to model
stealing attacks, where an adversary derives a \emph{surrogate model} from API
access to the source model. For post hoc detection of such attacks, the
provider needs a robust method to determine whether a suspect model is a
surrogate of their model. We propose a fingerprinting method for deep neural
network classifiers that extracts a set of inputs from the source model so that
only surrogates agree with the source model on the classification of such
inputs. These inputs are a subclass of transferable adversarial examples which
we call \emph{conferrable} adversarial examples that exclusively transfer with
a target label from a source model to its surrogates. We propose a new method
to generate these conferrable adversarial examples. We present an extensive
study on the unremovability of our fingerprint against fine-tuning, weight
pruning, retraining, retraining with different architectures, three model
extraction attacks from related work, transfer learning, adversarial training,
and two new adaptive attacks. Our fingerprint is robust against distillation,
related model extraction attacks, and even transfer learning when the attacker
has no access to the model provider's dataset. Our fingerprint is the first
method that reaches an AUC of 1.0 in verifying surrogates, compared to an AUC
of 0.63 by previous fingerprints.
</p>
<a href="http://arxiv.org/abs/1912.00888">arXiv:1912.00888</a> [<a href="http://arxiv.org/pdf/1912.00888">pdf</a>]

<h2>Extrinsic Kernel Ridge Regression Classifier for Planar Kendall Shape Space. (arXiv:1912.08202v2 [stat.ML] UPDATED)</h2>
<h3>Hwiyoung Lee, Vic Patrangenaru</h3>
<p>Kernel methods have had great success in Statistics and Machine Learning.
Despite their growing popularity, however, less effort has been drawn towards
developing kernel based classification methods on Riemannian manifolds due to
difficulty in dealing with non-Euclidean geometry. In this paper, motivated by
the extrinsic framework of manifold-valued data analysis, we propose a new
positive definite kernel on planar Kendall shape space $\Sigma_2^k$, called
extrinsic Veronese Whitney Gaussian kernel. We show that our approach can be
extended to develop Gaussian kernels on any embedded manifold. Furthermore,
kernel ridge regression classifier (KRRC) is implemented to address the shape
classification problem on $\Sigma_2^k$, and their promising performances are
illustrated through the real data analysis.
</p>
<a href="http://arxiv.org/abs/1912.08202">arXiv:1912.08202</a> [<a href="http://arxiv.org/pdf/1912.08202">pdf</a>]

<h2>Exploring Adversarial Attack in Spiking Neural Networks with Spike-Compatible Gradient. (arXiv:2001.01587v2 [cs.NE] UPDATED)</h2>
<h3>Ling Liang, Xing Hu, Lei Deng, Yujie Wu, Guoqi Li, Yufei Ding, Peng Li, Yuan Xie</h3>
<p>Recently, backpropagation through time inspired learning algorithms are
widely introduced into SNNs to improve the performance, which brings the
possibility to attack the models accurately given Spatio-temporal gradient
maps. We propose two approaches to address the challenges of gradient input
incompatibility and gradient vanishing. Specifically, we design a gradient to
spike converter to convert continuous gradients to ternary ones compatible with
spike inputs. Then, we design a gradient trigger to construct ternary gradients
that can randomly flip the spike inputs with a controllable turnover rate, when
meeting all zero gradients. Putting these methods together, we build an
adversarial attack methodology for SNNs trained by supervised algorithms.
Moreover, we analyze the influence of the training loss function and the firing
threshold of the penultimate layer, which indicates a "trap" region under the
cross-entropy loss that can be escaped by threshold tuning. Extensive
experiments are conducted to validate the effectiveness of our solution.
Besides the quantitative analysis of the influence factors, we evidence that
SNNs are more robust against adversarial attack than ANNs. This work can help
reveal what happens in SNN attack and might stimulate more research on the
security of SNN models and neuromorphic devices.
</p>
<a href="http://arxiv.org/abs/2001.01587">arXiv:2001.01587</a> [<a href="http://arxiv.org/pdf/2001.01587">pdf</a>]

<h2>A Deep Conditioning Treatment of Neural Networks. (arXiv:2002.01523v2 [cs.LG] UPDATED)</h2>
<h3>Naman Agarwal, Pranjal Awasthi, Satyen Kale</h3>
<p>We study the role of depth in training randomly initialized overparameterized
neural networks. We give a general result showing that depth improves
trainability of neural networks by improving the conditioning of certain kernel
matrices of the input data. This result holds for arbitrary non-linear
activation functions under a certain normalization. We provide versions of the
result that hold for training just the top layer of the neural network, as well
as for training all layers, via the neural tangent kernel. As applications of
these general results, we provide a generalization of the results of Das et al.
(2019) showing that learnability of deep random neural networks with a large
class of non-linear activations degrades exponentially with depth. We also show
how benign overfitting can occur in deep neural networks via the results of
Bartlett et al. (2019b). We also give experimental evidence that normalized
versions of ReLU are a viable alternative to more complex operations like Batch
Normalization in training deep neural networks.
</p>
<a href="http://arxiv.org/abs/2002.01523">arXiv:2002.01523</a> [<a href="http://arxiv.org/pdf/2002.01523">pdf</a>]

<h2>The Costs and Benefits of Goal-Directed Attention in Deep Convolutional Neural Networks. (arXiv:2002.02342v3 [cs.LG] UPDATED)</h2>
<h3>Xiaoliang Luo, Brett D. Roads, Bradley C. Love</h3>
<p>People deploy top-down, goal-directed attention to accomplish tasks, such as
finding lost keys. By tuning the visual system to relevant information sources,
object recognition can become more efficient (a benefit) and more biased toward
the target (a potential cost). Motivated by selective attention in
categorisation models, we developed a goal-directed attention mechanism that
can process naturalistic (photographic) stimuli. Our attention mechanism can be
incorporated into any existing deep convolutional neural network (DCNNs). The
processing stages in DCNNs have been related to ventral visual stream. In that
light, our attentional mechanism incorporates top-down influences from
prefrontal cortex (PFC) to support goal-directed behaviour. Akin to how
attention weights in categorisation models warp representational spaces, we
introduce a layer of attention weights to the mid-level of a DCNN that amplify
or attenuate activity to further a goal. We evaluated the attentional mechanism
using photographic stimuli, varying the attentional target. We found that
increasing goal-directed attention has benefits (increasing hit rates) and
costs (increasing false alarm rates). At a moderate level, attention improves
sensitivity (i.e., increases $d^\prime$) at only a moderate increase in bias
for tasks involving standard images, blended images, and natural adversarial
images chosen to fool DCNNs. These results suggest that goal-directed attention
can reconfigure general-purpose DCNNs to better suit the current task goal,
much like PFC modulates activity along the ventral stream. In addition to being
more parsimonious and brain consistent, the mid-level attention approach
performed better than a standard machine learning approach for transfer
learning, namely retraining the final network layer to accommodate the new
task.
</p>
<a href="http://arxiv.org/abs/2002.02342">arXiv:2002.02342</a> [<a href="http://arxiv.org/pdf/2002.02342">pdf</a>]

<h2>Decision-Making with Auto-Encoding Variational Bayes. (arXiv:2002.07217v2 [stat.ML] UPDATED)</h2>
<h3>Romain Lopez, Pierre Boyeau, Nir Yosef, Michael I. Jordan, Jeffrey Regier</h3>
<p>To make decisions based on a model fit by auto-encoding variational Bayes
(AEVB), practitioners often let the variational distribution serve as a
surrogate for the posterior distribution. This approach yields biased estimates
of the expected risk, and therefore poor decisions for two reasons. First, the
model fit by AEVB may yield biased statistics relative to the underlying data
distribution. Second, there may be strong discrepancies between the variational
distribution and the posterior. We explore how fitting the variational
distribution based on several objective functions other than the ELBO, while
continuing to fit the generative model based on the ELBO, affects the quality
of downstream decisions. For the probabilistic principal component analysis
model, we investigate how importance sampling error, as well as the biases in
model parameter estimates, vary across several approximate posteriors when used
as proposal distributions. Our theoretical results suggest that a posterior
approximation distinct from the variational distribution should be used for
making decisions. Motivated by these theoretical results, we propose learning
several approximate proposals for the best model and combining them using
multiple importance sampling for decision-making. In addition to toy examples,
we present a full-fledged case study of single-cell RNA sequencing. In this
challenging instance of multiple hypothesis testing, our proposed approach
surpasses the current state of the art. The code and datasets are available at
this http URL
</p>
<a href="http://arxiv.org/abs/2002.07217">arXiv:2002.07217</a> [<a href="http://arxiv.org/pdf/2002.07217">pdf</a>]

<h2>Autonomous robotic nanofabrication with reinforcement learning. (arXiv:2002.11952v2 [cond-mat.mes-hall] UPDATED)</h2>
<h3>Philipp Leinen, Malte Esders, Kristof T. Sch&#xfc;tt, Christian Wagner, Klaus-Robert M&#xfc;ller, F. Stefan Tautz</h3>
<p>The ability to handle single molecules as effectively as macroscopic
building-blocks would enable the construction of complex supramolecular
structures inaccessible to self-assembly. The fundamental challenges
obstructing this goal are the uncontrolled variability and poor observability
of atomic-scale conformations. Here, we present a strategy to work around both
obstacles, and demonstrate autonomous robotic nanofabrication by manipulating
single molecules. Our approach employs reinforcement learning (RL), which finds
solution strategies even in the face of large uncertainty and sparse feedback.
We demonstrate the potential of our RL approach by removing molecules
autonomously with a scanning probe microscope from a supramolecular structure
-- an exemplary task of subtractive manufacturing at the nanoscale. Our RL
agent reaches an excellent performance, enabling us to automate a task which
previously had to be performed by a human. We anticipate that our work opens
the way towards autonomous agents for the robotic construction of functional
supramolecular structures with speed, precision and perseverance beyond our
current capabilities.
</p>
<a href="http://arxiv.org/abs/2002.11952">arXiv:2002.11952</a> [<a href="http://arxiv.org/pdf/2002.11952">pdf</a>]

<h2>Behavior Planning For Connected Autonomous Vehicles Using Feedback Deep Reinforcement Learning. (arXiv:2003.04371v2 [cs.AI] UPDATED)</h2>
<h3>Songyang Han, Fei Miao</h3>
<p>With the development of communication technologies, connected autonomous
vehicles (CAVs) can share information with each other. We propose a novel
behavior planning method for CAVs to decide actions such as whether to change
lane or keep lane based on the observation and shared information from
neighbors, and to make sure that there exist corresponding control maneuvers
such as acceleration and steering angle to guarantee the safety of each
individual autonomous vehicle. We formulate this problem as a hybrid partially
observable Markov decision process (HPOMDP) to consider objectives such as
improving traffic flow efficiency and driving comfort and safety requirements.
The discrete state transition is determined by the proposed feedback deep
Q-learning algorithm using the feedback action from an underlying controller
based on control barrier functions. The feedback deep Q-learning algorithm we
design aims to solve the critical challenge of reinforcement learning (RL) in a
physical system: guaranteeing the safety of the system while the RL is
exploring the action space to increase the reward. We prove that our method
renders a forward invariant safe set for the continuous state physical dynamic
model of the system while the RL agent is learning. In experiments, our
behavior planning method can increase traffic flow and driving comfort compared
with the intelligent driving model (IDM). We also validate that our method
maintains safety during the learning process.
</p>
<a href="http://arxiv.org/abs/2003.04371">arXiv:2003.04371</a> [<a href="http://arxiv.org/pdf/2003.04371">pdf</a>]

<h2>Partial Quantifier Elimination By Certificate Clauses. (arXiv:2003.09667v4 [cs.LO] UPDATED)</h2>
<h3>Eugene Goldberg</h3>
<p>We study a modification of the Quantifier Elimination (QE) problem called
Partial QE (PQE) for propositional CNF formulas. In PQE, only a small subset of
target clauses is taken out of the scope of quantifiers. The appeal of PQE is
twofold. First, it provides a language for performing $\mathit{incremental}$
computations. Many verification problems (e.g. equivalence checking and model
checking) are inherently incremental and so can be solved in terms of PQE.
Second, PQE can be dramatically simpler than QE. We perform PQE by adding a set
of clauses depending only on free variables that make the target clauses
redundant. Proving redundancy of a target clause is done by derivation of a
"certificate" clause $\mathit{implying}$ the former. We implemented this idea
in a PQE algorithm called $\mathit{START}$. It bears some similarity to a
SAT-solver with conflict driven learning. A major difference here is that
$\mathit{START}$ backtracks as soon as a target clause is proved redundant
(even if no conflict occurred). We experimentally evaluate $\mathit{START}$ on
a practical problem. We use this problem to compare PQE with QE and QBF
solving.
</p>
<a href="http://arxiv.org/abs/2003.09667">arXiv:2003.09667</a> [<a href="http://arxiv.org/pdf/2003.09667">pdf</a>]

<h2>DeepSEE: Deep Disentangled Semantic Explorative Extreme Super-Resolution. (arXiv:2004.04433v2 [cs.CV] UPDATED)</h2>
<h3>Marcel C. B&#xfc;hler, Andr&#xe9;s Romero, Radu Timofte</h3>
<p>Super-resolution (SR) is by definition ill-posed. There are infinitely many
plausible high-resolution variants for a given low-resolution natural image.
Most of the current literature aims at a single deterministic solution of
either high reconstruction fidelity or photo-realistic perceptual quality. In
this work, we propose an explorative facial super-resolution framework,
DeepSEE, for Deep disentangled Semantic Explorative Extreme super-resolution.
To the best of our knowledge, DeepSEE is the first method to leverage semantic
maps for explorative super-resolution. In particular, it provides control of
the semantic regions, their disentangled appearance and it allows a broad range
of image manipulations. We validate DeepSEE on faces, for up to 32x
magnification and exploration of the space of super-resolution. Our code and
models are available at: https://mcbuehler.github.io/DeepSEE/
</p>
<a href="http://arxiv.org/abs/2004.04433">arXiv:2004.04433</a> [<a href="http://arxiv.org/pdf/2004.04433">pdf</a>]

<h2>On Box-Cox Transformation for Image Normality and Pattern Classification. (arXiv:2004.07210v3 [eess.IV] UPDATED)</h2>
<h3>Abbas Cheddad</h3>
<p>A unique member of the power transformation family is known as the Box-Cox
transformation. The latter can be seen as a mathematical operation that leads
to finding the optimum lambda ({\lambda}) value that maximizes the
log-likelihood function to transform a data to a normal distribution and to
reduce heteroscedasticity. In data analytics, a normality assumption underlies
a variety of statistical test models. This technique, however, is best known in
statistical analysis to handle one-dimensional data. Herein, this paper
revolves around the utility of such a tool as a pre-processing step to
transform two-dimensional data, namely, digital images and to study its effect.
Moreover, to reduce time complexity, it suffices to estimate the parameter
lambda in real-time for large two-dimensional matrices by merely considering
their probability density function as a statistical inference of the underlying
data distribution. We compare the effect of this light-weight Box-Cox
transformation with well-established state-of-the-art low light image
enhancement techniques. We also demonstrate the effectiveness of our approach
through several test-bed data sets for generic improvement of visual appearance
of images and for ameliorating the performance of a colour pattern
classification algorithm as an example application. Results with and without
the proposed approach, are compared using the AlexNet (transfer deep learning)
pretrained model. To the best of our knowledge, this is the first time that the
Box-Cox transformation is extended to digital images by exploiting histogram
transformation.
</p>
<a href="http://arxiv.org/abs/2004.07210">arXiv:2004.07210</a> [<a href="http://arxiv.org/pdf/2004.07210">pdf</a>]

<h2>Learning visual policies for building 3D shape categories. (arXiv:2004.07950v2 [cs.RO] UPDATED)</h2>
<h3>Alexander Pashevich, Igor Kalevatykh, Ivan Laptev, Cordelia Schmid</h3>
<p>Manipulation and assembly tasks require non-trivial planning of actions
depending on the environment and the final goal. Previous work in this domain
often assembles particular instances of objects from known sets of primitives.
In contrast, we aim to handle varying sets of primitives and to construct
different objects of a shape category. Given a single object instance of a
category, e.g. an arch, and a binary shape classifier, we learn a visual policy
to assemble other instances of the same category. In particular, we propose a
disassembly procedure and learn a state policy that discovers new object
instances and their assembly plans in state space. We then render simulated
states in the observation space and learn a heatmap representation to predict
alternative actions from a given input image. To validate our approach, we
first demonstrate its efficiency for building object categories in state space.
We then show the success of our visual policies for building arches from
different primitives. Moreover, we demonstrate (i) the reactive ability of our
method to re-assemble objects using additional primitives and (ii) the robust
performance of our policy for unseen primitives resembling building blocks used
during training. Our visual assembly policies are trained with no real images
and reach up to 95% success rate when evaluated on a real robot.
</p>
<a href="http://arxiv.org/abs/2004.07950">arXiv:2004.07950</a> [<a href="http://arxiv.org/pdf/2004.07950">pdf</a>]

<h2>Attention Based Real Image Restoration. (arXiv:2004.13524v2 [cs.CV] UPDATED)</h2>
<h3>Saeed Anwar, Nick Barnes, Lars Petersson</h3>
<p>Deep convolutional neural networks perform better on images containing
spatially invariant degradations, also known as synthetic degradations;
however, their performance is limited on real-degraded photographs and requires
multiple-stage network modeling. To advance the practicability of restoration
algorithms, this paper proposes a novel single-stage blind real image
restoration network (R$^2$Net) by employing a modular architecture. We use a
residual on the residual structure to ease the flow of low-frequency
information and apply feature attention to exploit the channel dependencies.
Furthermore, the evaluation in terms of quantitative metrics and visual quality
for four restoration tasks i.e. Denoising, Super-resolution, Raindrop Removal,
and JPEG Compression on 11 real degraded datasets against more than 30
state-of-the-art algorithms demonstrate the superiority of our R$^2$Net. We
also present the comparison on three synthetically generated degraded datasets
for denoising to showcase the capability of our method on synthetics denoising.
The codes, trained models, and results are available on
https://github.com/saeed-anwar/R2Net.
</p>
<a href="http://arxiv.org/abs/2004.13524">arXiv:2004.13524</a> [<a href="http://arxiv.org/pdf/2004.13524">pdf</a>]

<h2>A novel Region of Interest Extraction Layer for Instance Segmentation. (arXiv:2004.13665v2 [cs.CV] UPDATED)</h2>
<h3>Leonardo Rossi, Akbar Karimi, Andrea Prati</h3>
<p>Given the wide diffusion of deep neural network architectures for computer
vision tasks, several new applications are nowadays more and more feasible.
Among them, a particular attention has been recently given to instance
segmentation, by exploiting the results achievable by two-stage networks (such
as Mask R-CNN or Faster R-CNN), derived from R-CNN. In these complex
architectures, a crucial role is played by the Region of Interest (RoI)
extraction layer, devoted to extracting a coherent subset of features from a
single Feature Pyramid Network (FPN) layer attached on top of a backbone.

This paper is motivated by the need to overcome the limitations of existing
RoI extractors which select only one (the best) layer from FPN. Our intuition
is that all the layers of FPN retain useful information. Therefore, the
proposed layer (called Generic RoI Extractor - GRoIE) introduces non-local
building blocks and attention mechanisms to boost the performance.

A comprehensive ablation study at component level is conducted to find the
best set of algorithms and parameters for the GRoIE layer. Moreover, GRoIE can
be integrated seamlessly with every two-stage architecture for both object
detection and instance segmentation tasks. Therefore, the improvements brought
about by the use of GRoIE in different state-of-the-art architectures are also
evaluated. The proposed layer leads up to gain a 1.1% AP improvement on
bounding box detection and 1.7% AP improvement on instance segmentation.

The code is publicly available on GitHub repository at
https://github.com/IMPLabUniPr/mmdetection/tree/groie_dev
</p>
<a href="http://arxiv.org/abs/2004.13665">arXiv:2004.13665</a> [<a href="http://arxiv.org/pdf/2004.13665">pdf</a>]

<h2>A convolutional neural-network model of human cochlear mechanics and filter tuning for real-time applications. (arXiv:2004.14832v3 [eess.AS] UPDATED)</h2>
<h3>Deepak Baby, Arthur Van Den Broucke, Sarah Verhulst</h3>
<p>Auditory models are commonly used as feature extractors for automatic
speech-recognition systems or as front-ends for robotics, machine-hearing and
hearing-aid applications. Although auditory models can capture the biophysical
and nonlinear properties of human hearing in great detail, these biophysical
models are computationally expensive and cannot be used in real-time
applications. We present a hybrid approach where convolutional neural networks
are combined with computational neuroscience to yield a real-time end-to-end
model for human cochlear mechanics, including level-dependent filter tuning
(CoNNear). The CoNNear model was trained on acoustic speech material and its
performance and applicability were evaluated using (unseen) sound stimuli
commonly employed in cochlear mechanics research. The CoNNear model accurately
simulates human cochlear frequency selectivity and its dependence on sound
intensity, an essential quality for robust speech intelligibility at negative
speech-to-background-noise ratios. The CoNNear architecture is based on
parallel and differentiable computations and has the power to achieve real-time
human performance. These unique CoNNear features will enable the next
generation of human-like machine-hearing applications.
</p>
<a href="http://arxiv.org/abs/2004.14832">arXiv:2004.14832</a> [<a href="http://arxiv.org/pdf/2004.14832">pdf</a>]

<h2>Learning Joint Articulatory-Acoustic Representations with Normalizing Flows. (arXiv:2005.09463v2 [eess.AS] UPDATED)</h2>
<h3>Pramit Saha, Sidney Fels</h3>
<p>The articulatory geometric configurations of the vocal tract and the acoustic
properties of the resultant speech sound are considered to have a strong causal
relationship. This paper aims at finding a joint latent representation between
the articulatory and acoustic domain for vowel sounds via invertible neural
network models, while simultaneously preserving the respective domain-specific
features. Our model utilizes a convolutional autoencoder architecture and
normalizing flow-based models to allow both forward and inverse mappings in a
semi-supervised manner, between the mid-sagittal vocal tract geometry of a two
degrees-of-freedom articulatory synthesizer with 1D acoustic wave model and the
Mel-spectrogram representation of the synthesized speech sounds. Our approach
achieves satisfactory performance in achieving both articulatory-to-acoustic as
well as acoustic-to-articulatory mapping, thereby demonstrating our success in
achieving a joint encoding of both the domains.
</p>
<a href="http://arxiv.org/abs/2005.09463">arXiv:2005.09463</a> [<a href="http://arxiv.org/pdf/2005.09463">pdf</a>]

<h2>Modelling Hierarchical Structure between Dialogue Policy and Natural Language Generator with Option Framework for Task-oriented Dialogue System. (arXiv:2006.06814v3 [cs.CL] UPDATED)</h2>
<h3>Jianhong Wang, Yuan Zhang, Tae-Kyun Kim, Yunjie Gu</h3>
<p>Designing task-oriented dialogue systems is a challenging research topic,
since it needs not only to generate utterances fulfilling user requests but
also to guarantee the comprehensibility. Many previous works trained end-to-end
(E2E) models with supervised learning (SL), however, the bias in annotated
system utterances remains as a bottleneck. Reinforcement learning (RL) deals
with the problem through using non-differentiable evaluation metrics (e.g., the
success rate) as rewards. Nonetheless, existing works with RL showed that the
comprehensibility of generated system utterances could be corrupted when
improving the performance on fulfilling user requests. In o gur work, we (1)
propose modelling the hierarchical structure between dialogue policy and
natural language generator (NLG) with the option framework, called HDNO, where
the latent dialogue act is applied to avoid designing specific dialogue act
representations; (2) train HDNO via hierarchical reinforcement learning (HRL),
as well as suggest the asynchronous updates between dialogue policy and NLG
during training to theoretically guarantee their convergence to a local
maximizer; and (3) propose using a discriminator modelled with language models
as an additional reward to further improve the comprehensibility. We test HDNO
on MultiWoz 2.0 and MultiWoz 2.1, the datasets on multi-domain dialogues, in
comparison with word-level E2E model trained with RL, LaRL and HDSA, showing
improvements on the performance evaluated by automatic evaluation metrics and
human evaluation. Finally, we demonstrate the semantic meanings of latent
dialogue acts to show the ability of explanation.
</p>
<a href="http://arxiv.org/abs/2006.06814">arXiv:2006.06814</a> [<a href="http://arxiv.org/pdf/2006.06814">pdf</a>]

<h2>Cloud detection in Landsat-8 imagery in Google Earth Engine based on a deep neural network. (arXiv:2006.10358v2 [eess.IV] UPDATED)</h2>
<h3>Zhixiang Yin, Feng Ling, Giles M. Foody, Xinyan Li, Yun Du</h3>
<p>Google Earth Engine (GEE) provides a convenient platform for applications
based on optical satellite imagery of large areas. With such data sets, the
detection of cloud is often a necessary prerequisite step. Recently, deep
learning-based cloud detection methods have shown their potential for cloud
detection but they can only be applied locally, leading to inefficient data
downloading time and storage problems. This letter proposes a method to
directly perform cloud detection in Landsat-8 imagery in GEE based on deep
learning (DeepGEE-CD). A deep neural network (DNN) was first trained locally,
and then the trained DNN was deployed in the JavaScript client of GEE. An
experiment was undertaken to validate the proposed method with a set of
Landsat-8 images and the results show that DeepGEE-CD outperformed the widely
used function of mask (Fmask) algorithm. The proposed DeepGEE-CD approach can
accurately detect cloud in Landsat-8 imagery without downloading it, making it
a promising method for routine cloud detection of Landsat-8 imagery in GEE.
</p>
<a href="http://arxiv.org/abs/2006.10358">arXiv:2006.10358</a> [<a href="http://arxiv.org/pdf/2006.10358">pdf</a>]

<h2>EndoSLAM Dataset and An Unsupervised Monocular Visual Odometry and Depth Estimation Approach for Endoscopic Videos: Endo-SfMLearner. (arXiv:2006.16670v3 [cs.CV] UPDATED)</h2>
<h3>Kutsev Bengisu Ozyoruk, Guliz Irem Gokceler, Gulfize Coskun, Kagan Incetan, Yasin Almalioglu, Faisal Mahmood, Eva Curto, Luis Perdigoto, Marina Oliveira, Hasan Sahin, Helder Araujo, Henrique Alexandrino, Nicholas J. Durr, Hunter B. Gilbert, Mehmet Turan</h3>
<p>Deep learning techniques hold promise to develop dense topography
reconstruction and pose estimation methods for endoscopic videos. However,
currently available datasets do not support effective quantitative
benchmarking. In this paper, we introduce a comprehensive endoscopic SLAM
dataset consisting of 3D point cloud data for six porcine organs, capsule and
standard endoscopy recordings as well as synthetically generated data. A Panda
robotic arm, two commercially available capsule endoscopes, two conventional
endoscopes with different camera properties, and two high precision 3D scanners
were employed to collect data from 8 ex-vivo porcine gastrointestinal
(GI)-tract organs. In total, 35 sub-datasets are provided with 6D pose ground
truth for the ex-vivo part: 18 sub-dataset for colon, 12 sub-datasets for
stomach and 5 sub-datasets for small intestine, while four of these contain
polyp-mimicking elevations carried out by an expert gastroenterologist.
Synthetic capsule endoscopy frames from GI-tract with both depth and pose
annotations are included to facilitate the study of simulation-to-real transfer
learning algorithms. Additionally, we propound Endo-SfMLearner, an unsupervised
monocular depth and pose estimation method that combines residual networks with
spatial attention module in order to dictate the network to focus on
distinguishable and highly textured tissue regions. The proposed approach makes
use of a brightness-aware photometric loss to improve the robustness under fast
frame-to-frame illumination changes. To exemplify the use-case of the EndoSLAM
dataset, the performance of Endo-SfMLearner is extensively compared with the
state-of-the-art. The codes and the link for the dataset are publicly available
at https://github.com/CapsuleEndoscope/EndoSLAM. A video demonstrating the
experimental setup and procedure is accessible through
https://www.youtube.com/watch?v=G_LCe0aWWdQ.
</p>
<a href="http://arxiv.org/abs/2006.16670">arXiv:2006.16670</a> [<a href="http://arxiv.org/pdf/2006.16670">pdf</a>]

<h2>Descending through a Crowded Valley -- Benchmarking Deep Learning Optimizers. (arXiv:2007.01547v3 [cs.LG] UPDATED)</h2>
<h3>Robin M. Schmidt, Frank Schneider, Philipp Hennig</h3>
<p>Choosing the optimizer is considered to be among the most crucial decisions
of deep learning engineers, and it is not an easy one. The growing literature
now lists hundreds of optimization methods. In the absence of clear theoretical
guidance and conclusive empirical evidence, the decision is often made based on
personal anecdotes. In this work, we aim to replace these anecdotes, if not
with a conclusive ranking, then at least with evidence-backed heuristics. To do
so, we perform an extensive, standardized benchmark of more than a dozen
particularly popular deep learning optimizers while giving a concise overview
of the wide range of possible choices. Analyzing almost 35,000 individual runs,
we contribute the following three points: (i) Optimizer performance varies
greatly across tasks. (ii) We observe that evaluating multiple optimizers with
default parameters works approximately as well as tuning the hyperparameters of
a single, fixed optimizer. (iii) While we can not discern an optimization
method clearly dominating across all tested tasks, we identify a significantly
reduced subset of specific algorithms and parameter choices that generally lead
to competitive results in our experiments. This subset includes popular
favorites and some lesser-known contenders. We have open-sourced all our
experimental results, making them directly available as challenging and
well-tuned baselines. This allows for more meaningful comparisons when
evaluating novel optimization methods without requiring any further
computational efforts.
</p>
<a href="http://arxiv.org/abs/2007.01547">arXiv:2007.01547</a> [<a href="http://arxiv.org/pdf/2007.01547">pdf</a>]

<h2>Deep Learning based Dimple Segmentation for Quantitative Fractography. (arXiv:2007.02267v3 [eess.IV] UPDATED)</h2>
<h3>Ashish Sinha, K S Suresh</h3>
<p>In this work, we try to address the challenging problem of dimple detection
and segmentation in Titanium alloys using machine learning methods, especially
neural networks. The images i.e. fractographs are obtained using a Scanning
Election Microscope (SEM). To determine the cause of fracture in metals we
address the problem of segmentation of dimples in fractographs i.e. the
fracture surface of metals using supervised machine learning methods.
Determining the cause of fracture would help us in material property,
mechanical property prediction and development of new fracture-resistant
materials. This method would also help in correlating the topography of the
fracture surface with the mechanical properties of the material. Our proposed
novel model achieves the best performance as compared to other previous
approaches. To the best of our knowledge, this is one the first work in
fractography using fully convolutional neural networks with self-attention for
supervised learning of dimple fractography, though it can be easily extended to
account for brittle characteristics as well.
</p>
<a href="http://arxiv.org/abs/2007.02267">arXiv:2007.02267</a> [<a href="http://arxiv.org/pdf/2007.02267">pdf</a>]

<h2>Domain aware medical image classifier interpretation by counterfactual impact analysis. (arXiv:2007.06312v2 [cs.CV] UPDATED)</h2>
<h3>Dimitrios Lenis, David Major, Maria Wimmer, Astrid Berg, Gert Sluiter, Katja B&#xfc;hler</h3>
<p>The success of machine learning methods for computer vision tasks has driven
a surge in computer assisted prediction for medicine and biology. Based on a
data-driven relationship between input image and pathological classification,
these predictors deliver unprecedented accuracy. Yet, the numerous approaches
trying to explain the causality of this learned relationship have fallen short:
time constraints, coarse, diffuse and at times misleading results, caused by
the employment of heuristic techniques like Gaussian noise and blurring, have
hindered their clinical adoption.

In this work, we discuss and overcome these obstacles by introducing a
neural-network based attribution method, applicable to any trained predictor.
Our solution identifies salient regions of an input image in a single
forward-pass by measuring the effect of local image-perturbations on a
predictor's score. We replace heuristic techniques with a strong neighborhood
conditioned inpainting approach, avoiding anatomically implausible, hence
adversarial artifacts. We evaluate on public mammography data and compare
against existing state-of-the-art methods. Furthermore, we exemplify the
approach's generalizability by demonstrating results on chest X-rays. Our
solution shows, both quantitatively and qualitatively, a significant reduction
of localization ambiguity and clearer conveying results, without sacrificing
time efficiency.
</p>
<a href="http://arxiv.org/abs/2007.06312">arXiv:2007.06312</a> [<a href="http://arxiv.org/pdf/2007.06312">pdf</a>]

<h2>Speech2Video Synthesis with 3D Skeleton Regularization and Expressive Body Poses. (arXiv:2007.09198v3 [cs.CV] UPDATED)</h2>
<h3>Sibo Zhang, Miao Liao, Peng Wang, Hao Zhu, Xinxin Zuo, Ruigang Yang</h3>
<p>In this paper, we propose a novel approach to convert given speech audio to a
photo-realistic speaking video of a specific person, where the output video has
synchronized, realistic, and expressive rich body dynamics. We achieve this by
first generating 3D skeleton movements from the audio sequence using a
recurrent neural network (RNN), and then synthesizing the output video via a
conditional generative adversarial network (GAN). To make the skeleton movement
realistic and expressive, we embed the knowledge of an articulated 3D human
skeleton and a learned dictionary of personal speech iconic gestures into the
generation process in both learning and testing pipelines. The former prevents
the generation of unreasonable body distortion, while the later helps our model
quickly learn meaningful body movement through a few recorded videos. To
produce photo-realistic and high-resolution video with motion details, we
propose to insert part attention mechanisms in the conditional GAN, where each
detailed part, e.g. head and hand, is automatically zoomed in to have their own
discriminators. To validate our approach, we collect a dataset with 20
high-quality videos from 1 male and 1 female model reading various documents
under different topics. Compared with previous SoTA pipelines handling similar
tasks, our approach achieves better results by a user study.
</p>
<a href="http://arxiv.org/abs/2007.09198">arXiv:2007.09198</a> [<a href="http://arxiv.org/pdf/2007.09198">pdf</a>]

<h2>Compiling ONNX Neural Network Models Using MLIR. (arXiv:2008.08272v2 [cs.PL] UPDATED)</h2>
<h3>Tian Jin, Gheorghe-Teodor Bercea, Tung D. Le, Tong Chen, Gong Su, Haruki Imai, Yasushi Negishi, Anh Leu, Kevin O&#x27;Brien, Kiyokuni Kawachiya, Alexandre E. Eichenberger</h3>
<p>Deep neural network models are becoming increasingly popular and have been
used in various tasks such as computer vision, speech recognition, and natural
language processing. Machine learning models are commonly trained in a
resource-rich environment and then deployed in a distinct environment such as
high availability machines or edge devices. To assist the portability of
models, the open-source community has proposed the Open Neural Network Exchange
(ONNX) standard. In this paper, we present a high-level, preliminary report on
our onnx-mlir compiler, which generates code for the inference of deep neural
network models described in the ONNX format. Onnx-mlir is an open-source
compiler implemented using the Multi-Level Intermediate Representation (MLIR)
infrastructure recently integrated in the LLVM project. Onnx-mlir relies on the
MLIR concept of dialects to implement its functionality. We propose here two
new dialects: (1) an ONNX specific dialect that encodes the ONNX standard
semantics, and (2) a loop-based dialect to provide for a common lowering point
for all ONNX dialect operations. Each intermediate representation facilitates
its own characteristic set of graph-level and loop-based optimizations
respectively. We illustrate our approach by following several models through
the proposed representations and we include some early optimization work and
performance results.
</p>
<a href="http://arxiv.org/abs/2008.08272">arXiv:2008.08272</a> [<a href="http://arxiv.org/pdf/2008.08272">pdf</a>]

<h2>Deep Inverse Reinforcement Learning for Structural Evolution of Small Molecules. (arXiv:2008.11804v2 [q-bio.BM] UPDATED)</h2>
<h3>Brighter Agyemang, Wei-Ping Wu, Daniel Addo, Michael Y. Kpiebaareh, Ebenezer Nanor, Charles Roland Haruna</h3>
<p>The size and quality of chemical libraries to the drug discovery pipeline are
crucial for developing new drugs or repurposing existing drugs. Existing
techniques such as combinatorial organic synthesis and High-Throughput
Screening usually make the process extraordinarily tough and complicated since
the search space of synthetically feasible drugs is exorbitantly huge. While
reinforcement learning has been mostly exploited in the literature for
generating novel compounds, the requirement of designing a reward function that
succinctly represents the learning objective could prove daunting in certain
complex domains. Generative Adversarial Network-based methods also mostly
discard the discriminator after training and could be hard to train. In this
study, we propose a framework for training a compound generator and learning a
transferable reward function based on the entropy maximization inverse
reinforcement learning paradigm. We show from our experiments that the inverse
reinforcement learning route offers a rational alternative for generating
chemical compounds in domains where reward function engineering may be less
appealing or impossible while data exhibiting the desired objective is readily
available.
</p>
<a href="http://arxiv.org/abs/2008.11804">arXiv:2008.11804</a> [<a href="http://arxiv.org/pdf/2008.11804">pdf</a>]

<h2>Patch-based Brain Age Estimation from MR Images. (arXiv:2008.12965v2 [cs.CV] UPDATED)</h2>
<h3>Kyriaki-Margarita Bintsi, Vasileios Baltatzis, Arinbj&#xf6;rn Kolbeinsson, Alexander Hammers, Daniel Rueckert</h3>
<p>Brain age estimation from Magnetic Resonance Images (MRI) derives the
difference between a subject's biological brain age and their chronological
age. This is a potential biomarker for neurodegeneration, e.g. as part of
Alzheimer's disease. Early detection of neurodegeneration manifesting as a
higher brain age can potentially facilitate better medical care and planning
for affected individuals. Many studies have been proposed for the prediction of
chronological age from brain MRI using machine learning and specifically deep
learning techniques. Contrary to most studies, which use the whole brain
volume, in this study, we develop a new deep learning approach that uses 3D
patches of the brain as well as convolutional neural networks (CNNs) to develop
a localised brain age estimator. In this way, we can obtain a visualization of
the regions that play the most important role for estimating brain age, leading
to more anatomically driven and interpretable results, and thus confirming
relevant literature which suggests that the ventricles and the hippocampus are
the areas that are most informative. In addition, we leverage this knowledge in
order to improve the overall performance on the task of age estimation by
combining the results of different patches using an ensemble method, such as
averaging or linear regression. The network is trained on the UK Biobank
dataset and the method achieves state-of-the-art results with a Mean Absolute
Error of 2.46 years for purely regional estimates, and 2.13 years for an
ensemble of patches before bias correction, while 1.96 years after bias
correction.
</p>
<a href="http://arxiv.org/abs/2008.12965">arXiv:2008.12965</a> [<a href="http://arxiv.org/pdf/2008.12965">pdf</a>]

<h2>Quasi-symplectic Langevin Variational Autoencoder. (arXiv:2009.01675v2 [stat.ML] UPDATED)</h2>
<h3>Zihao Wang, Herv&#xe9; Delingette</h3>
<p>Variational autoencoder (VAE) as one of the well investigated generative
model is very popular in nowadays neural learning research works. To leverage
VAE in practical tasks which have high dimensions and huge dataset often face
the problem of low variance evidence lower bounds construction. Markov chain
Monte Carlo (MCMC) is an effective approach to tight the evidence lower bound
(ELBO) for approximating the posterior distribution. Hamiltonian Variational
Autoencoder (HVAE) is one of the effective MCMC inspired approaches for
constructing the unbiased low-variance ELBO which is also amenable for
reparameterization trick. The solution significantly improves the performance
of the posterior estimation effectiveness, yet, a main drawback of HVAE is the
leapfrog method need to access the posterior gradient twice which leads to bad
inference efficiency performance and the GPU memory requirement is fair large.
This flaw limited the application of Hamiltonian based inference framework for
large scale networks inference. To tackle this problem, we propose a
Quasi-symplectic Langevin Variational autoencoder (Langevin-VAE), which can be
a significant improvement over resource usage efficiency. We qualitatively and
quantitatively demonstrate the effectiveness of the Langevin-VAE compared to
the state-of-art gradients informed inference framework.
</p>
<a href="http://arxiv.org/abs/2009.01675">arXiv:2009.01675</a> [<a href="http://arxiv.org/pdf/2009.01675">pdf</a>]

<h2>Reinforcement Learning in Non-Stationary Discrete-Time Linear-Quadratic Mean-Field Games. (arXiv:2009.04350v3 [eess.SY] UPDATED)</h2>
<h3>Muhammad Aneeq uz Zaman, Kaiqing Zhang, Erik Miehling, Tamer Ba&#x15f;ar</h3>
<p>In this paper, we study large population multi-agent reinforcement learning
(RL) in the context of discrete-time linear-quadratic mean-field games
(LQ-MFGs). Our setting differs from most existing work on RL for MFGs, in that
we consider a non-stationary MFG over an infinite horizon. We propose an
actor-critic algorithm to iteratively compute the mean-field equilibrium (MFE)
of the LQ-MFG. There are two primary challenges: i) the non-stationarity of the
MFG induces a linear-quadratic tracking problem, which requires solving a
backwards-in-time (non-causal) equation that cannot be solved by standard
(causal) RL algorithms; ii) Many RL algorithms assume that the states are
sampled from the stationary distribution of a Markov chain (MC), that is, the
chain is already mixed, an assumption that is not satisfied for real data
sources. We first identify that the mean-field trajectory follows linear
dynamics, allowing the problem to be reformulated as a linear quadratic
Gaussian problem. Under this reformulation, we propose an actor-critic
algorithm that allows samples to be drawn from an unmixed MC. Finite-sample
convergence guarantees for the algorithm are then provided. To characterize the
performance of our algorithm in multi-agent RL, we have developed an error
bound with respect to the Nash equilibrium of the finite-population game.
</p>
<a href="http://arxiv.org/abs/2009.04350">arXiv:2009.04350</a> [<a href="http://arxiv.org/pdf/2009.04350">pdf</a>]

<h2>Simple Simultaneous Ensemble Learning in Genetic Programming. (arXiv:2009.06037v3 [cs.NE] UPDATED)</h2>
<h3>Marco Virgolin</h3>
<p>Learning ensembles by bagging can substantially improve the generalization
performance of low-bias high-variance estimators, including those evolved by
Genetic Programming (GP). Yet, the best way to learn ensembles in GP remains to
be determined. This work attempts to fill the gap between existing GP ensemble
learning algorithms, which are often either simple but expensive, or efficient
but complex. We propose a new algorithm that is both simple and efficient,
named Simple Simultaneous Ensemble Genetic Programming (2SEGP). 2SEGP is
obtained by relatively minor modifications to fitness evaluation and selection
of a classic GP algorithm, and its only drawback is an (arguably small)
increase of the fitness evaluation cost from the classic $\mathcal{O}(n \ell)$
to $\mathcal{O}(n(\ell + \beta))$, with $n$ the number of observations and
$\ell$/$\beta$ the estimator/ensemble size. Experimental comparisons on
real-world datasets between supervised classification and regression show that,
despite its simplicity, 2SEGP fares very well against state-of-the-art
(ensemble and not) GP algorithms. We further provide insights into what matters
in 2SEGP by (i) scaling $\beta$, (ii) ablating the proposed selection method,
(iii) observing the evolvability induced by traditional subtree variation.
</p>
<a href="http://arxiv.org/abs/2009.06037">arXiv:2009.06037</a> [<a href="http://arxiv.org/pdf/2009.06037">pdf</a>]

<h2>DynamicVAE: Decoupling Reconstruction Error and Disentangled Representation Learning. (arXiv:2009.06795v2 [cs.LG] UPDATED)</h2>
<h3>Huajie Shao, Haohong Lin, Qinmin Yang, Shuochao Yao, Han Zhao, Tarek Abdelzaher</h3>
<p>This paper challenges the common assumption that the weight $\beta$, in
$\beta$-VAE, should be larger than $1$ in order to effectively disentangle
latent factors. We demonstrate that $\beta$-VAE, with $\beta &lt; 1$, can not only
attain good disentanglement but also significantly improve reconstruction
accuracy via dynamic control. The paper removes the inherent trade-off between
reconstruction accuracy and disentanglement for $\beta$-VAE. Existing methods,
such as $\beta$-VAE and FactorVAE, assign a large weight to the KL-divergence
term in the objective function, leading to high reconstruction errors for the
sake of better disentanglement. To mitigate this problem, a ControlVAE has
recently been developed that dynamically tunes the KL-divergence weight in an
attempt to control the trade-off to more a favorable point. However, ControlVAE
fails to eliminate the conflict between the need for a large $\beta$ (for
disentanglement) and the need for a small $\beta$. Instead, we propose
DynamicVAE that maintains a different $\beta$ at different stages of training,
thereby decoupling disentanglement and reconstruction accuracy. In order to
evolve the weight, $\beta$, along a trajectory that enables such decoupling,
DynamicVAE leverages a modified incremental PI (proportional-integral)
controller, and employs a moving average as well as a hybrid annealing method
to evolve the value of KL-divergence smoothly in a tightly controlled fashion.
We theoretically prove the stability of the proposed approach. Evaluation
results on three benchmark datasets demonstrate that DynamicVAE significantly
improves the reconstruction accuracy while achieving disentanglement comparable
to the best of existing methods. The results verify that our method can
separate disentangled representation learning and reconstruction, removing the
inherent tension between the two.
</p>
<a href="http://arxiv.org/abs/2009.06795">arXiv:2009.06795</a> [<a href="http://arxiv.org/pdf/2009.06795">pdf</a>]

<h2>The importance of fillers for text representations of speech transcripts. (arXiv:2009.11340v2 [cs.CL] UPDATED)</h2>
<h3>Tanvi Dinkar, Pierre Colombo, Matthieu Labeau, Chlo&#xe9; Clavel</h3>
<p>While being an essential component of spoken language, fillers (e.g."um" or
"uh") often remain overlooked in Spoken Language Understanding (SLU) tasks. We
explore the possibility of representing them with deep contextualised
embeddings, showing improvements on modelling spoken language and two
downstream tasks - predicting a speaker's stance and expressed confidence.
</p>
<a href="http://arxiv.org/abs/2009.11340">arXiv:2009.11340</a> [<a href="http://arxiv.org/pdf/2009.11340">pdf</a>]

<h2>Adapting BERT for Word Sense Disambiguation with Gloss Selection Objective and Example Sentences. (arXiv:2009.11795v2 [cs.CL] UPDATED)</h2>
<h3>Boon Peng Yap, Andrew Koh, Eng Siong Chng</h3>
<p>Domain adaptation or transfer learning using pre-trained language models such
as BERT has proven to be an effective approach for many natural language
processing tasks. In this work, we propose to formulate word sense
disambiguation as a relevance ranking task, and fine-tune BERT on sequence-pair
ranking task to select the most probable sense definition given a context
sentence and a list of candidate sense definitions. We also introduce a data
augmentation technique for WSD using existing example sentences from WordNet.
Using the proposed training objective and data augmentation technique, our
models are able to achieve state-of-the-art results on the English all-words
benchmark datasets.
</p>
<a href="http://arxiv.org/abs/2009.11795">arXiv:2009.11795</a> [<a href="http://arxiv.org/pdf/2009.11795">pdf</a>]

<h2>DialoGLUE: A Natural Language Understanding Benchmark for Task-Oriented Dialogue. (arXiv:2009.13570v2 [cs.CL] UPDATED)</h2>
<h3>Shikib Mehri, Mihail Eric, Dilek Hakkani-Tur</h3>
<p>A long-standing goal of task-oriented dialogue research is the ability to
flexibly adapt dialogue models to new domains. To progress research in this
direction, we introduce DialoGLUE (Dialogue Language Understanding Evaluation),
a public benchmark consisting of 7 task-oriented dialogue datasets covering 4
distinct natural language understanding tasks, designed to encourage dialogue
research in representation-based transfer, domain adaptation, and
sample-efficient task learning. We release several strong baseline models,
demonstrating performance improvements over a vanilla BERT architecture and
state-of-the-art results on 5 out of 7 tasks, by pre-training on a large
open-domain dialogue corpus and task-adaptive self-supervised training. Through
the DialoGLUE benchmark, the baseline methods, and our evaluation scripts, we
hope to facilitate progress towards the goal of developing more general
task-oriented dialogue models.
</p>
<a href="http://arxiv.org/abs/2009.13570">arXiv:2009.13570</a> [<a href="http://arxiv.org/pdf/2009.13570">pdf</a>]

<h2>Apollo: An Adaptive Parameter-wise Diagonal Quasi-Newton Method for Nonconvex Stochastic Optimization. (arXiv:2009.13586v2 [cs.LG] UPDATED)</h2>
<h3>Xuezhe Ma</h3>
<p>In this paper, we introduce Apollo, a quasi-Newton method for nonconvex
stochastic optimization, which dynamically incorporates the curvature of the
loss function by approximating the Hessian via a diagonal matrix. Importantly,
the update and storage of the diagonal approximation of Hessian is as efficient
as adaptive first-order optimization methods with linear complexity for both
time and memory. To handle nonconvexity, we replace the Hessian with its
rectified absolute value, which is guaranteed to be positive-definite.
Experiments on three tasks of vision and language show that Apollo achieves
significant improvements over other stochastic optimization methods, including
SGD and variants of Adam, in term of both convergence speed and generalization
performance. The implementation of the algorithm is available at
https://github.com/XuezheMax/apollo.
</p>
<a href="http://arxiv.org/abs/2009.13586">arXiv:2009.13586</a> [<a href="http://arxiv.org/pdf/2009.13586">pdf</a>]

<h2>One Person, One Model, One World: Learning Continual User Representation without Forgetting. (arXiv:2009.13724v2 [cs.IR] UPDATED)</h2>
<h3>Fajie Yuan, Guoxiao Zhang, Alexandros Karatzoglou, Xiangnan He, Joemon Jose, Beibei Kong, Yudong Li</h3>
<p>Learning generic user representations which can then be applied to other
user-related tasks (e.g., profile prediction and recommendation) has recently
attracted much attention. Existing approaches often derive an individual set of
model parameters for each task by training their own data. However, the
representation of a user usually has some potential commonalities. As such,
these separately trained representations could be suboptimal in performance as
well as inefficient in terms of parameter sharing. In this paper, we delve on
the research to continually learn user representations task by task, whereby
new tasks are learned while using parameters from old ones. A new problem
arises since when new tasks are trained, previously learned parameters are very
likely to be modified, and thus, an artificial neural network (ANN)-based model
may lose its capacity to serve for well-trained previous tasks forever, termed
as catastrophic forgetting. To address this issue, we present Conure which is
the first continual, or lifelong, user representation learner -- i.e., learning
new tasks over time without forgetting old ones. Specifically, we propose
iteratively removing unimportant weights by pruning on a well-optimized
backbone representation model, enlightened by fact that neural network models
are highly over-parameterized. Then, we are able to learn a coming task by
sharing previous parameters and training new ones only on the empty space after
pruning. We conduct extensive experiments on two real-world datasets across
nine tasks and demonstrate that Conure performs largely better than common
models without purposely preserving such old "knowledge", and is competitive or
sometimes better than models which are trained either individually for each
task or simultaneously by preparing all task data together.
</p>
<a href="http://arxiv.org/abs/2009.13724">arXiv:2009.13724</a> [<a href="http://arxiv.org/pdf/2009.13724">pdf</a>]

<h2>Current Time Series Anomaly Detection Benchmarks are Flawed and are Creating the Illusion of Progress. (arXiv:2009.13807v2 [cs.LG] UPDATED)</h2>
<h3>Renjie Wu, Eamonn J. Keogh</h3>
<p>Time series anomaly detection has been a perennially important topic in data
science, with papers dating back to the 1950s. However, in recent years there
has been an explosion of interest in this topic, much of it driven by the
success of deep learning in other domains and for other time series tasks. Most
of these papers test on one or more of a handful of popular benchmark datasets,
created by Yahoo, Numenta, NASA, etc. In this work we make a surprising claim.
The majority of the individual exemplars in these datasets suffer from one or
more of four flaws. Because of these four flaws, we believe that many published
comparisons of anomaly detection algorithms may be unreliable, and more
importantly, much of the apparent progress in recent years may be illusionary.
In addition to demonstrating these claims, with this paper we introduce the UCR
Time Series Anomaly Datasets. We believe that this resource will perform a
similar role as the UCR Time Series Classification Archive, by providing the
community with a benchmark that allows meaningful comparisons between
approaches and a meaningful gauge of overall progress.
</p>
<a href="http://arxiv.org/abs/2009.13807">arXiv:2009.13807</a> [<a href="http://arxiv.org/pdf/2009.13807">pdf</a>]

<h2>Realistic Image Normalization for Multi-Domain Segmentation. (arXiv:2009.14024v2 [cs.LG] UPDATED)</h2>
<h3>Pierre-Luc Delisle, Benoit Anctil-Robitaille, Christian Desrosiers, Herve Lombaert</h3>
<p>Image normalization is a building block in medical image analysis.
Conventional approaches are customarily utilized on a per-dataset basis. This
strategy, however, prevents the current normalization algorithms from fully
exploiting the complex joint information available across multiple datasets.
Consequently, ignoring such joint information has a direct impact on the
performance of segmentation algorithms. This paper proposes to revisit the
conventional image normalization approach by instead learning a common
normalizing function across multiple datasets. Jointly normalizing multiple
datasets is shown to yield consistent normalized images as well as an improved
image segmentation. To do so, a fully automated adversarial and task-driven
normalization approach is employed as it facilitates the training of realistic
and interpretable images while keeping performance on-par with the
state-of-the-art. The adversarial training of our network aims at finding the
optimal transfer function to improve both the segmentation accuracy and the
generation of realistic images. We evaluated the performance of our normalizer
on both infant and adult brains images from the iSEG, MRBrainS and ABIDE
datasets. Results reveal the potential of our normalization approach for
segmentation, with Dice improvements of up to 57.5% over our baseline. Our
method can also enhance data availability by increasing the number of samples
available when learning from multiple imaging domains.
</p>
<a href="http://arxiv.org/abs/2009.14024">arXiv:2009.14024</a> [<a href="http://arxiv.org/pdf/2009.14024">pdf</a>]

<h2>Direct Multi-hop Attention based Graph Neural Network. (arXiv:2009.14332v2 [cs.LG] UPDATED)</h2>
<h3>Guangtao Wang, Rex Ying, Jing Huang, Jure Leskovec</h3>
<p>Introducing self-attention mechanism in graph neural networks (GNNs) achieved
state-of-the-art performance for graph representation learning. However, at
every layer, attention is only computed between two connected nodes and depends
solely on the representation of both nodes. This attention computation cannot
account for the multi-hop neighbors which supply graph structure context
information and have influence on the node representation learning as well. In
this paper, we propose Direct Multi-hop Attention based Graph neural Network
(DAGN) for graph representation learning, a principled way to incorporate
multi-hop neighboring context into attention computation, enabling long-range
interactions at every layer. To compute attention between nodes that are
multiple hops away, DAGN diffuses the attention scores from neighboring nodes
to non-neighboring nodes, thus increasing the receptive field for every message
passing layer. Unlike previous methods, DAGN uses a diffusion prior on
attention values, to efficiently account for all paths between the pair of
nodes when computing multi-hop attention weights. This helps DAGN capture
large-scale structural information in a single layer, and learn more
informative attention distribution. Experimental results on standard
semi-supervised node classification as well as the knowledge graph completion
show that DAGN achieves state-of-the-art results: DAGN achieves up to 5.7%
relative error reduction over the previous state-of-the-art on Cora, Citeseer,
and Pubmed. DAGN also obtains the best performance on a large-scale Open Graph
Benchmark dataset. On knowledge graph completion DAGN advances state-of-the-art
on WN18RR and FB15k-237 across four different performance metrics.
</p>
<a href="http://arxiv.org/abs/2009.14332">arXiv:2009.14332</a> [<a href="http://arxiv.org/pdf/2009.14332">pdf</a>]

<h2>Multi-Pen Robust Robotic 3D Drawing Using Closed-Loop Planning. (arXiv:2009.14501v2 [cs.RO] UPDATED)</h2>
<h3>Ruishuang Liu, Weiwei Wan, Keisuke Koyama, Kensuke Harada</h3>
<p>This paper develops a flexible and robust robotic system for autonomous
drawing on 3D surfaces. The system takes 2D drawing strokes and a 3D target
surface (mesh or point clouds) as input. It maps the 2D strokes onto the 3D
surface and generates a robot motion to draw the mapped strokes using visual
recognition, grasp pose reasoning, and motion planning. The system is flexible
compared to conventional robotic drawing systems as we do not fix drawing tools
to the end of a robot arm. Instead, a robot selects drawing tools using a
vision system and holds drawing tools for painting using its hand. Meanwhile,
with the flexibility, the system has high robustness thanks to the following
crafts: First, a high-quality mapping method is developed to minimize
deformation in the strokes. Second, visual detection is used to re-estimate the
drawing tool's pose before executing each drawing motion. Third, force control
is employed to avoid noisy visual detection and calibration, and ensure a firm
touch between the pen tip and a target surface. Fourth, error detection and
recovery are implemented to deal with unexpected problems. The planning and
executions are performed in a closed-loop manner until the strokes are
successfully drawn. We evaluate the system and analyze the necessity of the
various crafts using different real-word tasks. The results show that the
proposed system is flexible and robust to generate a robot motion from picking
and placing the pens to successfully drawing 3D strokes on given surfaces.
</p>
<a href="http://arxiv.org/abs/2009.14501">arXiv:2009.14501</a> [<a href="http://arxiv.org/pdf/2009.14501">pdf</a>]

<h2>Distance Correlation Based Brain Functional Connectivity Estimation and Non-Convex Multi-Task Learning for Developmental fMRI Studies. (arXiv:2010.00116v1 [q-bio.QM])</h2>
<h3>Li Xiao, Biao Cai, Gang Qu, Julia M. Stephen, Tony W. Wilson, Vince D. Calhoun, Yu-Ping Wang</h3>
<p>Resting-state functional magnetic resonance imaging (rs-fMRI)-derived
functional connectivity patterns have been extensively utilized to delineate
global functional organization of the human brain in health, development, and
neuropsychiatric disorders. In this paper, we investigate how functional
connectivity in males and females differs in an age prediction framework. We
first estimate functional connectivity between regions-of-interest (ROIs) using
distance correlation instead of Pearson's correlation. Distance correlation, as
a multivariate statistical method, explores spatial relations of voxel-wise
time courses within individual ROIs and measures both linear and nonlinear
dependence, capturing more complex information of between-ROI interactions.
Then, a novel non-convex multi-task learning (NC-MTL) model is proposed to
study age-related gender differences in functional connectivity, where age
prediction for each gender group is viewed as one task. Specifically, in the
proposed NC-MTL model, we introduce a composite regularizer with a combination
of non-convex $\ell_{2,1-2}$ and $\ell_{1-2}$ regularization terms for
selecting both common and task-specific features. Finally, we validate the
proposed NC-MTL model along with distance correlation based functional
connectivity on rs-fMRI of the Philadelphia Neurodevelopmental Cohort for
predicting ages of both genders. The experimental results demonstrate that the
proposed NC-MTL model outperforms other competing MTL models in age prediction,
as well as characterizing developmental gender differences in functional
connectivity patterns.
</p>
<a href="http://arxiv.org/abs/2010.00116">arXiv:2010.00116</a> [<a href="http://arxiv.org/pdf/2010.00116">pdf</a>]

<h2>Sparse Regression with Multi-type Regularized Feature Modeling. (arXiv:1810.03136v2 [stat.CO] UPDATED)</h2>
<h3>Sander Devriendt, Katrien Antonio, Tom Reynkens, Roel Verbelen</h3>
<p>Within the statistical and machine learning literature, regularization
techniques are often used to construct sparse (predictive) models. Most
regularization strategies only work for data where all predictors are treated
identically, such as Lasso regression for (continuous) predictors treated as
linear effects. However, many predictive problems involve different types of
predictors and require a tailored regularization term. We propose a multi-type
Lasso penalty that acts on the objective function as a sum of subpenalties, one
for each type of predictor. As such, we allow for predictor selection and level
fusion within a predictor in a data-driven way, simultaneous with the parameter
estimation process. We develop a new estimation strategy for convex predictive
models with this multi-type penalty. Using the theory of proximal operators,
our estimation procedure is computationally efficient, partitioning the overall
optimization problem into easier to solve subproblems, specific for each
predictor type and its associated penalty. Earlier research applies
approximations to non-differentiable penalties to solve the optimization
problem. The proposed SMuRF algorithm removes the need for approximations and
achieves a higher accuracy and computational efficiency. This is demonstrated
with an extensive simulation study and the analysis of a case-study on
insurance pricing analytics.
</p>
<a href="http://arxiv.org/abs/1810.03136">arXiv:1810.03136</a> [<a href="http://arxiv.org/pdf/1810.03136">pdf</a>]

