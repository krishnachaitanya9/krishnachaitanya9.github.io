---
title: Latest Deep Learning Papers
date: 2021-02-28 21:00:01 +0800
featured-img: DL-logo
categories: [Deep_Learning]
tags: [Deep_Learning]
mathjax: Yes
---

<h1>Your arXiv Feed (53 Articles)</h1>
<h2>Robust Pollen Imagery Classification with Generative Modeling and Mixup Training. (arXiv:2102.13143v1 [cs.CV])</h2>
<h3>Jaideep Murkute</h3>
<p>Deep learning approaches have shown great success in image classification
tasks and can aid greatly towards the fast and reliable classification of
pollen grain aerial imagery. However, often-times deep learning methods in the
setting of natural images can suffer generalization problems and yield poor
performance on unseen test distribution. In this work, we present and a robust
deep learning framework that can generalize well for pollen grain
aerobiological imagery classification. We develop a convolutional neural
network-based pollen grain classification approach and combine some of the best
practices in deep learning for better generalization. In addition to
commonplace approaches like data-augmentation and weight regularization, we
utilize implicit regularization methods like manifold mixup to allow learning
of smoother decision boundaries. We also make use of proven state-of-the-art
architectural choices like EfficientNet convolutional neural networks. Inspired
by the success of generative modeling with variational autoencoders, we train
models with a richer learning objective which can allow the model to focus on
the relevant parts of the image. Finally, we create an ensemble of neural
networks, for the robustness of the test set predictions. Based on our
experiments, we show improved generalization performance as measured with a
weighted F1-score with the aforementioned approaches. The proposed approach
earned a fourth-place in the final rankings in the ICPR-2020 Pollen Grain
Classification Challenge; with a 0.972578 weighted F1 score,0.950828 macro
average F1 scores, and 0.972877 recognition accuracy.
</p>
<a href="http://arxiv.org/abs/2102.13143" target="_blank">arXiv:2102.13143</a> [<a href="http://arxiv.org/pdf/2102.13143" target="_blank">pdf</a>]

<h2>Multi-Domain Learning by Meta-Learning: Taking Optimal Steps in Multi-Domain Loss Landscapes by Inner-Loop Learning. (arXiv:2102.13147v1 [cs.CV])</h2>
<h3>Anthony Sicilia, Xingchen Zhao, Davneet Minhas, Erin O&#x27;Connor, Howard Aizenstein, William Klunk, Dana Tudorascu, Seong Jae Hwang</h3>
<p>We consider a model-agnostic solution to the problem of Multi-Domain Learning
(MDL) for multi-modal applications. Many existing MDL techniques are
model-dependent solutions which explicitly require nontrivial architectural
changes to construct domain-specific modules. Thus, properly applying these MDL
techniques for new problems with well-established models, e.g. U-Net for
semantic segmentation, may demand various low-level implementation efforts. In
this paper, given emerging multi-modal data (e.g., various structural
neuroimaging modalities), we aim to enable MDL purely algorithmically so that
widely used neural networks can trivially achieve MDL in a model-independent
manner. To this end, we consider a weighted loss function and extend it to an
effective procedure by employing techniques from the recently active area of
learning-to-learn (meta-learning). Specifically, we take inner-loop gradient
steps to dynamically estimate posterior distributions over the hyperparameters
of our loss function. Thus, our method is model-agnostic, requiring no
additional model parameters and no network architecture changes; instead, only
a few efficient algorithmic modifications are needed to improve performance in
MDL. We demonstrate our solution to a fitting problem in medical imaging,
specifically, in the automatic segmentation of white matter hyperintensity
(WMH). We look at two neuroimaging modalities (T1-MR and FLAIR) with
complementary information fitting for our problem.
</p>
<a href="http://arxiv.org/abs/2102.13147" target="_blank">arXiv:2102.13147</a> [<a href="http://arxiv.org/pdf/2102.13147" target="_blank">pdf</a>]

<h2>Efficient and Interpretable Robot Manipulation with Graph Neural Networks. (arXiv:2102.13177v1 [cs.RO])</h2>
<h3>Yixin Lin, Austin S. Wang, Akshara Rai</h3>
<p>Many manipulation tasks can be naturally cast as a sequence of spatial
relationships and constraints between objects. We aim to discover and scale
these task-specific spatial relationships by representing manipulation tasks as
operations over graphs. To do this, we pose manipulating a large, variable
number of objects as a probabilistic classification problem over actions,
objects and goals, learned using graph neural networks (GNNs). Our formulation
first transforms the environment into a graph representation, then applies a
trained GNN policy to predict which object to manipulate towards which goal
state. Our GNN policies are trained using very few expert demonstrations on
simple tasks, and exhibits generalization over number and configurations of
objects in the environment and even to new, more complex tasks, and provide
interpretable explanations for their decision-making. We present experiments
which show that a single learned GNN policy can solve a variety of
blockstacking tasks in both simulation and real hardware.
</p>
<a href="http://arxiv.org/abs/2102.13177" target="_blank">arXiv:2102.13177</a> [<a href="http://arxiv.org/pdf/2102.13177" target="_blank">pdf</a>]

<h2>CollisionIK: A Per-Instant Pose Optimization Method for Generating Robot Motions with Environment Collision Avoidance. (arXiv:2102.13187v1 [cs.RO])</h2>
<h3>Daniel Rakita, Haochen Shi, Bilge Mutlu, Michael Gleicher</h3>
<p>In this work, we present a per-instant pose optimization method that can
generate configurations that achieve specified pose or motion objectives as
best as possible over a sequence of solutions, while also simultaneously
avoiding collisions with static or dynamic obstacles in the environment. We
cast our method as a multi-objective, non-linear constrained optimization-based
IK problem where each term in the objective function encodes a particular pose
objective. We demonstrate how to effectively incorporate environment collision
avoidance as a single term in this multi-objective, optimization-based IK
structure, and provide solutions for how to spatially represent and organize
external environments such that data can be efficiently passed to a real-time,
performance-critical optimization loop. We demonstrate the effectiveness of our
method by comparing it to various state-of-the-art methods in a testbed of
simulation experiments and discuss the implications of our work based on our
results.
</p>
<a href="http://arxiv.org/abs/2102.13187" target="_blank">arXiv:2102.13187</a> [<a href="http://arxiv.org/pdf/2102.13187" target="_blank">pdf</a>]

<h2>Learning Controller Gains on Bipedal Walking Robots via User Preferences. (arXiv:2102.13201v1 [cs.RO])</h2>
<h3>Noel Csomay-Shanklin, Maegan Tucker, Min Dai, Jenna Reher, Aaron D. Ames</h3>
<p>Experimental demonstration of complex robotic behaviors relies heavily on
finding the correct controller gains. This painstaking process is often
completed by a domain expert, requiring deep knowledge of the relationship
between parameter values and the resulting behavior of the system. Even when
such knowledge is possessed, it can take significant effort to navigate the
nonintuitive landscape of possible parameter combinations. In this work, we
explore the extent to which preference-based learning can be used to optimize
controller gains online by repeatedly querying the user for their preferences.
This general methodology is applied to two variants of control Lyapunov
function based nonlinear controllers framed as quadratic programs, which have
nice theoretic properties but are challenging to realize in practice. These
controllers are successfully demonstrated both on the planar underactuated
biped, AMBER, and on the 3D underactuated biped, Cassie. We experimentally
evaluate the performance of the learned controllers and show that the proposed
method is repeatably able to learn gains that yield stable and robust
locomotion.
</p>
<a href="http://arxiv.org/abs/2102.13201" target="_blank">arXiv:2102.13201</a> [<a href="http://arxiv.org/pdf/2102.13201" target="_blank">pdf</a>]

<h2>Motion Planning for a Pair of Tethered Robots. (arXiv:2102.13212v1 [cs.RO])</h2>
<h3>Reza H. Teshnizi, Dylan A. Shell</h3>
<p>Considering an environment containing polygonal obstacles, we address the
problem of planning motions for a pair of planar robots connected to one
another via a cable of limited length. Much like prior problems with a single
robot connected via a cable to a fixed base, straight line-of-sight visibility
plays an important role. The present paper shows how the reduced visibility
graph provides a natural discretization and captures the essential topological
considerations very effectively for the two robot case as well. Unlike the
single robot case, however, the bounded cable length introduces considerations
around coordination (or equivalently, when viewed from the point of view of a
centralized planner, relative timing) that complicates the matter. Indeed, the
paper has to introduce a rather more involved formalization than prior
single-robot work in order to establish the core theoretical result -- a
theorem permitting the problem to be cast as one of finding paths rather than
trajectories. Once affirmed, the planning problem reduces to a straightforward
graph search with an elegant representation of the connecting cable, demanding
only a few extra ancillary checks that ensure sufficiency of cable to guarantee
feasibility of the solution. We describe our implementation of A${}^\star$
search, and report experimental results. Lastly, we prescribe an optimal
execution for the solutions provided by the algorithm.
</p>
<a href="http://arxiv.org/abs/2102.13212" target="_blank">arXiv:2102.13212</a> [<a href="http://arxiv.org/pdf/2102.13212" target="_blank">pdf</a>]

<h2>On the Visual-based Safe Landing of UAVs in Populated Areas: a Crucial Aspect for Urban Deployment. (arXiv:2102.13253v1 [cs.RO])</h2>
<h3>Javier Gonz&#xe1;lez-Trejo, Diego Mercado-Ravell, Israel Becerra, Rafael Murrieta-Cid</h3>
<p>Autonomous landing of Unmanned Aerial Vehicles (UAVs) in crowded scenarios is
crucial for successful deployment of UAVs in populated areas, particularly in
emergency landing situations where the highest priority is to avoid hurting
people. In this work, a new visual-based algorithm for identifying Safe Landing
Zones (SLZ) in crowded scenarios is proposed, considering a camera mounted on
an UAV, where the people in the scene move with unknown dynamics. To do so, a
density map is generated for each image frame using a Deep Neural Network, from
where a binary occupancy map is obtained aiming to overestimate the people's
location for security reasons. Then, the occupancy map is projected to the
head's plane, and the SLZ candidates are obtained as circular regions in the
head's plane with a minimum security radius. Finally, to keep track of the SLZ
candidates, a multiple instance tracking algorithm is implemented using Kalman
Filters along with the Hungarian algorithm for data association. Several
scenarios were studied to prove the validity of the proposed strategy,
including public datasets and real uncontrolled scenarios with people moving in
public squares, taken from an UAV in flight. The study showed promising results
in the search of preventing the UAV from hurting people during emergency
landing.
</p>
<a href="http://arxiv.org/abs/2102.13253" target="_blank">arXiv:2102.13253</a> [<a href="http://arxiv.org/pdf/2102.13253" target="_blank">pdf</a>]

<h2>Boundary-induced and scene-aggregated network for monocular depth prediction. (arXiv:2102.13258v1 [cs.CV])</h2>
<h3>Feng Xue, Junfeng Cao, Yu Zhou, Fei Sheng, Yankai Wang, Anlong Ming</h3>
<p>Monocular depth prediction is an important task in scene understanding. It
aims to predict the dense depth of a single RGB image. With the development of
deep learning, the performance of this task has made great improvements.
However, two issues remain unresolved: (1) The deep feature encodes the wrong
farthest region in a scene, which leads to a distorted 3D structure of the
predicted depth; (2) The low-level features are insufficient utilized, which
makes it even harder to estimate the depth near the edge with sudden depth
change. To tackle these two issues, we propose the Boundary-induced and
Scene-aggregated network (BS-Net). In this network, the Depth Correlation
Encoder (DCE) is first designed to obtain the contextual correlations between
the regions in an image, and perceive the farthest region by considering the
correlations. Meanwhile, the Bottom-Up Boundary Fusion (BUBF) module is
designed to extract accurate boundary that indicates depth change. Finally, the
Stripe Refinement module (SRM) is designed to refine the dense depth induced by
the boundary cue, which improves the boundary accuracy of the predicted depth.
Several experimental results on the NYUD v2 dataset and \xff{the iBims-1
dataset} illustrate the state-of-the-art performance of the proposed approach.
And the SUN-RGBD dataset is employed to evaluate the generalization of our
method. Code is available at https://github.com/XuefengBUPT/BS-Net.
</p>
<a href="http://arxiv.org/abs/2102.13258" target="_blank">arXiv:2102.13258</a> [<a href="http://arxiv.org/pdf/2102.13258" target="_blank">pdf</a>]

<h2>Improving Robustness of Learning-based Autonomous Steering Using Adversarial Images. (arXiv:2102.13262v1 [cs.CV])</h2>
<h3>Yu Shen, Laura Zheng, Manli Shu, Weizi Li, Tom Goldstein, Ming C. Lin</h3>
<p>For safety of autonomous driving, vehicles need to be able to drive under
various lighting, weather, and visibility conditions in different environments.
These external and environmental factors, along with internal factors
associated with sensors, can pose significant challenges to perceptual data
processing, hence affecting the decision-making and control of the vehicle. In
this work, we address this critical issue by introducing a framework for
analyzing robustness of the learning algorithm w.r.t varying quality in the
image input for autonomous driving. Using the results of sensitivity analysis,
we further propose an algorithm to improve the overall performance of the task
of "learning to steer". The results show that our approach is able to enhance
the learning outcomes up to 48%. A comparative study drawn between our approach
and other related techniques, such as data augmentation and adversarial
training, confirms the effectiveness of our algorithm as a way to improve the
robustness and generalization of neural network training for autonomous
driving.
</p>
<a href="http://arxiv.org/abs/2102.13262" target="_blank">arXiv:2102.13262</a> [<a href="http://arxiv.org/pdf/2102.13262" target="_blank">pdf</a>]

<h2>Robot Navigation in a Crowd by Integrating Deep Reinforcement Learning and Online Planning. (arXiv:2102.13265v1 [cs.RO])</h2>
<h3>Zhiqian Zhou, Pengming Zhu, Zhiwen Zeng, Junhao Xiao, Huimin Lu, Zongtan Zhou</h3>
<p>It is still an open and challenging problem for mobile robots navigating
along time-efficient and collision-free paths in a crowd. The main challenge
comes from the complex and sophisticated interaction mechanism, which requires
the robot to understand the crowd and perform proactive and foresighted
behaviors. Deep reinforcement learning is a promising solution to this problem.
However, most previous learning methods incur a tremendous computational
burden. To address these problems, we propose a graph-based deep reinforcement
learning method, SG-DQN, that (i) introduces a social attention mechanism to
extract an efficient graph representation for the crowd-robot state; (ii)
directly evaluates the coarse q-values of the raw state with a learned dueling
deep Q network(DQN); and then (iii) refines the coarse q-values via online
planning on possible future trajectories. The experimental results indicate
that our model can help the robot better understand the crowd and achieve a
high success rate of more than 0.99 in the crowd navigation task. Compared
against previous state-of-the-art algorithms, our algorithm achieves an
equivalent, if not better, performance while requiring less than half of the
computational cost.
</p>
<a href="http://arxiv.org/abs/2102.13265" target="_blank">arXiv:2102.13265</a> [<a href="http://arxiv.org/pdf/2102.13265" target="_blank">pdf</a>]

<h2>Many-to-One Distribution Learning and K-Nearest Neighbor Smoothing for Thoracic Disease Identification. (arXiv:2102.13269v1 [cs.CV])</h2>
<h3>Yi Zhou, Lei Huang, Tianfei Zhou, Ling Shao</h3>
<p>Chest X-rays are an important and accessible clinical imaging tool for the
detection of many thoracic diseases. Over the past decade, deep learning, with
a focus on the convolutional neural network (CNN), has become the most powerful
computer-aided diagnosis technology for improving disease identification
performance. However, training an effective and robust deep CNN usually
requires a large amount of data with high annotation quality. For chest X-ray
imaging, annotating large-scale data requires professional domain knowledge and
is time-consuming. Thus, existing public chest X-ray datasets usually adopt
language pattern based methods to automatically mine labels from reports.
However, this results in label uncertainty and inconsistency. In this paper, we
propose many-to-one distribution learning (MODL) and K-nearest neighbor
smoothing (KNNS) methods from two perspectives to improve a single model's
disease identification performance, rather than focusing on an ensemble of
models. MODL integrates multiple models to obtain a soft label distribution for
optimizing the single target model, which can reduce the effects of original
label uncertainty. Moreover, KNNS aims to enhance the robustness of the target
model to provide consistent predictions on images with similar medical
findings. Extensive experiments on the public NIH Chest X-ray and CheXpert
datasets show that our model achieves consistent improvements over the
state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2102.13269" target="_blank">arXiv:2102.13269</a> [<a href="http://arxiv.org/pdf/2102.13269" target="_blank">pdf</a>]

<h2>A Reconfigurable Winograd CNN Accelerator with Nesting Decomposition Algorithm for Computing Convolution with Large Filters. (arXiv:2102.13272v1 [cs.CV])</h2>
<h3>Jingbo Jiang, Xizi Chen, Chi-Ying Tsui</h3>
<p>Recent literature found that convolutional neural networks (CNN) with large
filters perform well in some applications such as image semantic segmentation.
Winograd transformation helps to reduce the number of multiplications in a
convolution but suffers from numerical instability when the convolution filter
size gets large. This work proposes a nested Winograd algorithm to iteratively
decompose a large filter into a sequence of 3x3 tiles which can then be
accelerated with a 3x3 Winograd algorithm. Compared with the state-of-art
OLA-Winograd algorithm, the proposed algorithm reduces the multiplications by
1.41 to 3.29 times for computing 5x5 to 9x9 convolutions.
</p>
<a href="http://arxiv.org/abs/2102.13272" target="_blank">arXiv:2102.13272</a> [<a href="http://arxiv.org/pdf/2102.13272" target="_blank">pdf</a>]

<h2>MixSearch: Searching for Domain Generalized Medical Image Segmentation Architectures. (arXiv:2102.13280v1 [cs.CV])</h2>
<h3>Luyan Liu, Zhiwei Wen, Songwei Liu, Hong-Yu Zhou, Hongwei Zhu, Weicheng Xie, Linlin Shen, Kai Ma, Yefeng Zheng</h3>
<p>Considering the scarcity of medical data, most datasets in medical image
analysis are an order of magnitude smaller than those of natural images.
However, most Network Architecture Search (NAS) approaches in medical images
focused on specific datasets and did not take into account the generalization
ability of the learned architectures on unseen datasets as well as different
domains. In this paper, we address this point by proposing to search for
generalizable U-shape architectures on a composited dataset that mixes medical
images from multiple segmentation tasks and domains creatively, which is named
MixSearch. Specifically, we propose a novel approach to mix multiple
small-scale datasets from multiple domains and segmentation tasks to produce a
large-scale dataset. Then, a novel weaved encoder-decoder structure is designed
to search for a generalized segmentation network in both cell-level and
network-level. The network produced by the proposed MixSearch framework
achieves state-of-the-art results compared with advanced encoder-decoder
networks across various datasets.
</p>
<a href="http://arxiv.org/abs/2102.13280" target="_blank">arXiv:2102.13280</a> [<a href="http://arxiv.org/pdf/2102.13280" target="_blank">pdf</a>]

<h2>V-RVO: Decentralized Multi-Agent Collision Avoidance using Voronoi Diagrams and Reciprocal Velocity Obstacles. (arXiv:2102.13281v1 [cs.RO])</h2>
<h3>Senthil Hariharan Arul, Dinesh Manocha</h3>
<p>We present a decentralized collision avoidance method for dense environments
that is based on buffered Voronoi cells (BVC) and reciprocal velocity obstacles
(RVO). Our approach is designed for scenarios with large number of close
proximity agents and provides passive-friendly collision avoidance guarantees.
The Voronoi cells are superimposed with RVO cones to compute a suitable
direction for each agent and we use that direction for computing a local
collision-free path. Our approach can satisfy double-integrator dynamics
constraints and we use the properties of the BVC to formulate a simple,
decentralized deadlock resolution strategy. We demonstrate the benefits of
V-RVO in complex scenarios with tens of agents in close proximity. In practice,
V-RVO's performance is comparable to prior velocity-obstacle methods and the
collision avoidance behavior is significantly less conservative than ORCA.
</p>
<a href="http://arxiv.org/abs/2102.13281" target="_blank">arXiv:2102.13281</a> [<a href="http://arxiv.org/pdf/2102.13281" target="_blank">pdf</a>]

<h2>Continuous Face Aging Generative Adversarial Networks. (arXiv:2102.13318v1 [cs.CV])</h2>
<h3>Seogkyu Jeon, Pilhyeon Lee, Kibeom Hong, Hyeran Byun</h3>
<p>Face aging is the task aiming to translate the faces in input images to
designated ages. To simplify the problem, previous methods have limited
themselves only able to produce discrete age groups, each of which consists of
ten years. Consequently, the exact ages of the translated results are unknown
and it is unable to obtain the faces of different ages within groups. To this
end, we propose the continuous face aging generative adversarial networks
(CFA-GAN). Specifically, to make the continuous aging feasible, we propose to
decompose image features into two orthogonal features: the identity and the age
basis features. Moreover, we introduce the novel loss function for identity
preservation which maximizes the cosine similarity between the original and the
generated identity basis features. With the qualitative and quantitative
evaluations on MORPH, we demonstrate the realistic and continuous aging ability
of our model, validating its superiority against existing models. To the best
of our knowledge, this work is the first attempt to handle continuous target
ages.
</p>
<a href="http://arxiv.org/abs/2102.13318" target="_blank">arXiv:2102.13318</a> [<a href="http://arxiv.org/pdf/2102.13318" target="_blank">pdf</a>]

<h2>Domain Adapting Ability of Self-Supervised Learning for Face Recognition. (arXiv:2102.13319v1 [cs.CV])</h2>
<h3>Chun-Hsien Lin, Bing-Fei Wu</h3>
<p>Although deep convolutional networks have achieved great performance in face
recognition tasks, the challenge of domain discrepancy still exists in real
world applications. Lack of domain coverage of training data (source domain)
makes the learned models degenerate in a testing scenario (target domain). In
face recognition tasks, classes in two domains are usually different, so
classical domain adaptation approaches, assuming there are shared classes in
domains, may not be reasonable solutions for this problem. In this paper,
self-supervised learning is adopted to learn a better embedding space where the
subjects in target domain are more distinguishable. The learning goal is
maximizing the similarity between the embeddings of each image and its mirror
in both domains. The experiments show its competitive results compared with
prior works. To know the reason why it can achieve such performance, we further
discuss how this approach affects the learning of embeddings.
</p>
<a href="http://arxiv.org/abs/2102.13319" target="_blank">arXiv:2102.13319</a> [<a href="http://arxiv.org/pdf/2102.13319" target="_blank">pdf</a>]

<h2>Class Knowledge Overlay to Visual Feature Learning for Zero-Shot Image Classification. (arXiv:2102.13322v1 [cs.CV])</h2>
<h3>Cheng Xie, Ting Zeng, Hongxin Xiang, Keqin Li, Yun Yang, Qing Liu</h3>
<p>New categories can be discovered by transforming semantic features into
synthesized visual features without corresponding training samples in zero-shot
image classification. Although significant progress has been made in generating
high-quality synthesized visual features using generative adversarial networks,
guaranteeing semantic consistency between the semantic features and visual
features remains very challenging. In this paper, we propose a novel zero-shot
learning approach, GAN-CST, based on class knowledge to visual feature learning
to tackle the problem. The approach consists of three parts, class knowledge
overlay, semi-supervised learning and triplet loss. It applies class knowledge
overlay (CKO) to obtain knowledge not only from the corresponding class but
also from other classes that have the knowledge overlay. It ensures that the
knowledge-to-visual learning process has adequate information to generate
synthesized visual features. The approach also applies a semi-supervised
learning process to re-train knowledge-to-visual model. It contributes to
reinforcing synthesized visual features generation as well as new category
prediction. We tabulate results on a number of benchmark datasets demonstrating
that the proposed model delivers superior performance over state-of-the-art
approaches.
</p>
<a href="http://arxiv.org/abs/2102.13322" target="_blank">arXiv:2102.13322</a> [<a href="http://arxiv.org/pdf/2102.13322" target="_blank">pdf</a>]

<h2>Knowledge Distillation Circumvents Nonlinearity for Optical Convolutional Neural Networks. (arXiv:2102.13323v1 [cs.CV])</h2>
<h3>Jinlin Xiang, Shane Colburn, Arka Majumdar, Eli Shlizerman</h3>
<p>In recent years, Convolutional Neural Networks (CNNs) have enabled ubiquitous
image processing applications. As such, CNNs require fast runtime (forward
propagation) to process high-resolution visual streams in real time. This is
still a challenging task even with state-of-the-art graphics and tensor
processing units. The bottleneck in computational efficiency primarily occurs
in the convolutional layers. Performing operations in the Fourier domain is a
promising way to accelerate forward propagation since it transforms
convolutions into elementwise multiplications, which are considerably faster to
compute for large kernels. Furthermore, such computation could be implemented
using an optical 4f system with orders of magnitude faster operation. However,
a major challenge in using this spectral approach, as well as in an optical
implementation of CNNs, is the inclusion of a nonlinearity between each
convolutional layer, without which CNN performance drops dramatically. Here, we
propose a Spectral CNN Linear Counterpart (SCLC) network architecture and
develop a Knowledge Distillation (KD) approach to circumvent the need for a
nonlinearity and successfully train such networks. While the KD approach is
known in machine learning as an effective process for network pruning, we adapt
the approach to transfer the knowledge from a nonlinear network (teacher) to a
linear counterpart (student). We show that the KD approach can achieve
performance that easily surpasses the standard linear version of a CNN and
could approach the performance of the nonlinear network. Our simulations show
that the possibility of increasing the resolution of the input image allows our
proposed 4f optical linear network to perform more efficiently than a nonlinear
network with the same accuracy on two fundamental image processing tasks: (i)
object classification and (ii) semantic segmentation.
</p>
<a href="http://arxiv.org/abs/2102.13323" target="_blank">arXiv:2102.13323</a> [<a href="http://arxiv.org/pdf/2102.13323" target="_blank">pdf</a>]

<h2>Zero-Shot Learning Based on Knowledge Sharing. (arXiv:2102.13326v1 [cs.CV])</h2>
<h3>Zeng Ting, Xiang Hongxin, Xie Cheng, Yang Yun, Liu Qing</h3>
<p>Zero-Shot Learning (ZSL) is an emerging research that aims to solve the
classification problems with very few training data. The present works on ZSL
mainly focus on the mapping of learning semantic space to visual space. It
encounters many challenges that obstruct the progress of ZSL research. First,
the representation of the semantic feature is inadequate to represent all
features of the categories. Second, the domain drift problem still exists
during the transfer from semantic space to visual space. In this paper, we
introduce knowledge sharing (KS) to enrich the representation of semantic
features. Based on KS, we apply a generative adversarial network to generate
pseudo visual features from semantic features that are very close to the real
visual features. Abundant experimental results from two benchmark datasets of
ZSL show that the proposed approach has a consistent improvement.
</p>
<a href="http://arxiv.org/abs/2102.13326" target="_blank">arXiv:2102.13326</a> [<a href="http://arxiv.org/pdf/2102.13326" target="_blank">pdf</a>]

<h2>Mitigating Domain Mismatch in Face Recognition Using Style Matching. (arXiv:2102.13327v1 [cs.CV])</h2>
<h3>Chun-Hsien Lin, Bing-Fei Wu</h3>
<p>Despite outstanding performance on public benchmarks, face recognition still
suffers due to domain mismatch between training (source) and testing (target)
data. Furthermore, these domains are not shared classes, which complicates
domain adaptation. Since this is also a fine-grained classification problem
which does not strictly follow the low-density separation principle,
conventional domain adaptation approaches do not resolve these problems. In
this paper, we formulate domain mismatch in face recognition as a style
mismatch problem for which we propose two methods. First, we design a domain
discriminator with human-level judgment to mine target-like images in the
training data to mitigate the domain gap. Second, we extract style
representations in low-level feature maps of the backbone model, and match the
style distributions of the two domains to find a common style representation.
Evaluations on verification and open-set and closed-set identification
protocols show that both methods yield good improvements, and that performance
is more robust if they are combined. Our approach is competitive with related
work, and its effectiveness is verified in a practical application.
</p>
<a href="http://arxiv.org/abs/2102.13327" target="_blank">arXiv:2102.13327</a> [<a href="http://arxiv.org/pdf/2102.13327" target="_blank">pdf</a>]

<h2>Dual-MTGAN: Stochastic and Deterministic Motion Transfer for Image-to-Video Synthesis. (arXiv:2102.13329v1 [cs.CV])</h2>
<h3>Fu-En Yang, Jing-Cheng Chang, Yuan-Hao Lee, Yu-Chiang Frank Wang</h3>
<p>Generating videos with content and motion variations is a challenging task in
computer vision. While the recent development of GAN allows video generation
from latent representations, it is not easy to produce videos with particular
content of motion patterns of interest. In this paper, we propose Dual Motion
Transfer GAN (Dual-MTGAN), which takes image and video data as inputs while
learning disentangled content and motion representations. Our Dual-MTGAN is
able to perform deterministic motion transfer and stochastic motion generation.
Based on a given image, the former preserves the input content and transfers
motion patterns observed from another video sequence, and the latter directly
produces videos with plausible yet diverse motion patterns based on the input
image. The proposed model is trained in an end-to-end manner, without the need
to utilize pre-defined motion features like pose or facial landmarks. Our
quantitative and qualitative results would confirm the effectiveness and
robustness of our model in addressing such conditioned image-to-video tasks.
</p>
<a href="http://arxiv.org/abs/2102.13329" target="_blank">arXiv:2102.13329</a> [<a href="http://arxiv.org/pdf/2102.13329" target="_blank">pdf</a>]

<h2>A Universal Model for Cross Modality Mapping by Relational Reasoning. (arXiv:2102.13360v1 [cs.CV])</h2>
<h3>Zun Li, Congyan Lang, Liqian Liang, Tao Wang, Songhe Feng, Jun Wu, Yidong Li</h3>
<p>With the aim of matching a pair of instances from two different modalities,
cross modality mapping has attracted growing attention in the computer vision
community. Existing methods usually formulate the mapping function as the
similarity measure between the pair of instance features, which are embedded to
a common space. However, we observe that the relationships among the instances
within a single modality (intra relations) and those between the pair of
heterogeneous instances (inter relations) are insufficiently explored in
previous approaches. Motivated by this, we redefine the mapping function with
relational reasoning via graph modeling, and further propose a GCN-based
Relational Reasoning Network (RR-Net) in which inter and intra relations are
efficiently computed to universally resolve the cross modality mapping problem.
Concretely, we first construct two kinds of graph, i.e., Intra Graph and Inter
Graph, to respectively model intra relations and inter relations. Then RR-Net
updates all the node features and edge features in an iterative manner for
learning intra and inter relations simultaneously. Last, RR-Net outputs the
probabilities over the edges which link a pair of heterogeneous instances to
estimate the mapping results. Extensive experiments on three example tasks,
i.e., image classification, social recommendation and sound recognition,
clearly demonstrate the superiority and universality of our proposed model.
</p>
<a href="http://arxiv.org/abs/2102.13360" target="_blank">arXiv:2102.13360</a> [<a href="http://arxiv.org/pdf/2102.13360" target="_blank">pdf</a>]

<h2>Where to look at the movies : Analyzing visual attention to understand movie editing. (arXiv:2102.13378v1 [cs.CV])</h2>
<h3>Alexandre Bruckert, Marc Christie, Olivier Le Meur</h3>
<p>In the process of making a movie, directors constantly care about where the
spectator will look on the screen. Shot composition, framing, camera movements
or editing are tools commonly used to direct attention. In order to provide a
quantitative analysis of the relationship between those tools and gaze
patterns, we propose a new eye-tracking database, containing gaze pattern
information on movie sequences, as well as editing annotations, and we show how
state-of-the-art computational saliency techniques behave on this dataset. In
this work, we expose strong links between movie editing and spectators
scanpaths, and open several leads on how the knowledge of editing information
could improve human visual attention modeling for cinematic content. The
dataset generated and analysed during the current study is available at
https://github.com/abruckert/eye_tracking_filmmaking
</p>
<a href="http://arxiv.org/abs/2102.13378" target="_blank">arXiv:2102.13378</a> [<a href="http://arxiv.org/pdf/2102.13378" target="_blank">pdf</a>]

<h2>Point Cloud Upsampling and Normal Estimation using Deep Learning for Robust Surface Reconstruction. (arXiv:2102.13391v1 [cs.CV])</h2>
<h3>Rajat Sharma, Tobias Schwandt, Christian Kunert, Steffen Urban, Wolfgang Broll</h3>
<p>The reconstruction of real-world surfaces is on high demand in various
applications. Most existing reconstruction approaches apply 3D scanners for
creating point clouds which are generally sparse and of low density. These
points clouds will be triangulated and used for visualization in combination
with surface normals estimated by geometrical approaches. However, the quality
of the reconstruction depends on the density of the point cloud and the
estimation of the surface normals. In this paper, we present a novel deep
learning architecture for point cloud upsampling that enables subsequent stable
and smooth surface reconstruction. A noisy point cloud of low density with
corresponding point normals is used to estimate a point cloud with higher
density and appendant point normals. To this end, we propose a compound loss
function that encourages the network to estimate points that lie on a surface
including normals accurately predicting the orientation of the surface. Our
results show the benefit of estimating normals together with point positions.
The resulting point cloud is smoother, more complete, and the final surface
reconstruction is much closer to ground truth.
</p>
<a href="http://arxiv.org/abs/2102.13391" target="_blank">arXiv:2102.13391</a> [<a href="http://arxiv.org/pdf/2102.13391" target="_blank">pdf</a>]

<h2>Unifying Remote Sensing Image Retrieval and Classification with Robust Fine-tuning. (arXiv:2102.13392v1 [cs.CV])</h2>
<h3>Dimitri Gominski, Val&#xe9;rie Gouet-Brunet, Liming Chen</h3>
<p>Advances in high resolution remote sensing image analysis are currently
hampered by the difficulty of gathering enough annotated data for training deep
learning methods, giving rise to a variety of small datasets and associated
dataset-specific methods. Moreover, typical tasks such as classification and
retrieval lack a systematic evaluation on standard benchmarks and training
datasets, which make it hard to identify durable and generalizable scientific
contributions. We aim at unifying remote sensing image retrieval and
classification with a new large-scale training and testing dataset, SF300,
including both vertical and oblique aerial images and made available to the
research community, and an associated fine-tuning method. We additionally
propose a new adversarial fine-tuning method for global descriptors. We show
that our framework systematically achieves a boost of retrieval and
classification performance on nine different datasets compared to an ImageNet
pretrained baseline, with currently no other method to compare to.
</p>
<a href="http://arxiv.org/abs/2102.13392" target="_blank">arXiv:2102.13392</a> [<a href="http://arxiv.org/pdf/2102.13392" target="_blank">pdf</a>]

<h2>Panoramic annular SLAM with loop closure and global optimization. (arXiv:2102.13400v1 [cs.RO])</h2>
<h3>Hao Chen, Weijian Hu, Kailun Yang, Jian Bai, Kaiwei Wang</h3>
<p>In this paper, we propose PA-SLAM, a monocular panoramic annular visual SLAM
system with loop closure and global optimization. A hybrid point selection
strategy is put forward in the tracking front-end, which ensures repeatability
of keypoints and enables loop closure detection based on the bag-of-words
approach. Every detected loop candidate is verified geometrically and the
$Sim(3)$ relative pose constraint is estimated to perform pose graph
optimization and global bundle adjustment in the back-end. A comprehensive set
of experiments on real-world datasets demonstrates that the hybrid point
selection strategy allows reliable loop closure detection, and the accumulated
error and scale drift have been significantly reduced via global optimization,
enabling PA-SLAM to reach state-of-the-art accuracy while maintaining high
robustness and efficiency.
</p>
<a href="http://arxiv.org/abs/2102.13400" target="_blank">arXiv:2102.13400</a> [<a href="http://arxiv.org/pdf/2102.13400" target="_blank">pdf</a>]

<h2>Autonomous Quadrotor Flight despite Rotor Failure with Onboard Vision Sensors: Frames vs. Events. (arXiv:2102.13406v1 [cs.RO])</h2>
<h3>Sihao Sun, Giovanni Cioffi, Coen de Visser, Davide Scaramuzza</h3>
<p>Fault-tolerant control is crucial for safety-critical systems, such as
quadrotors. State-of-art flight controllers can stabilize and control a
quadrotor even when subjected to the complete loss of a rotor. However, these
methods rely on external sensors, such as GPS or motion capture systems, for
state estimation. To the best of our knowledge, this has not yet been achieved
with only onboard sensors. In this paper, we propose the first algorithm that
combines fault-tolerant control and onboard vision-based state estimation to
achieve position control of a quadrotor subjected to complete failure of one
rotor. Experimental validations show that our approach is able to accurately
control the position of a quadrotor during a motor failure scenario, without
the aid of any external sensors. The primary challenge to vision-based state
estimation stems from the inevitable high-speed yaw rotation (over 20 rad/s) of
the damaged quadrotor, causing motion blur to cameras, which is detrimental to
visual inertial odometry (VIO). We compare two types of visual inputs to the
vision-based state estimation algorithm: standard frames and events.
Experimental results show the advantage of using an event camera especially in
low light environments due to its inherent high dynamic range and high temporal
resolution. We believe that our approach will render autonomous quadrotors
safer in both GPS denied or degraded environments. We release both our
controller and VIO algorithm open source.
</p>
<a href="http://arxiv.org/abs/2102.13406" target="_blank">arXiv:2102.13406</a> [<a href="http://arxiv.org/pdf/2102.13406" target="_blank">pdf</a>]

<h2>Accurate Visual-Inertial SLAM by Feature Re-identification. (arXiv:2102.13438v1 [cs.CV])</h2>
<h3>Xiongfeng Peng, Zhihua Liu, Qiang Wang, Yun-Tae Kim, Myungjae Jeon</h3>
<p>We propose a novel feature re-identification method for real-time
visual-inertial SLAM. The front-end module of the state-of-the-art
visual-inertial SLAM methods (e.g. visual feature extraction and matching
schemes) relies on feature tracks across image frames, which are easily broken
in challenging scenarios, resulting in insufficient visual measurement and
accumulated error in pose estimation. In this paper, we propose an efficient
drift-less SLAM method by re-identifying existing features from a
spatial-temporal sensitive sub-global map. The re-identified features over a
long time span serve as augmented visual measurements and are incorporated into
the optimization module which can gradually decrease the accumulative error in
the long run, and further build a drift-less global map in the system.
Extensive experiments show that our feature re-identification method is both
effective and efficient. Specifically, when combining the feature
re-identification with the state-of-the-art SLAM method [11], our method
achieves 67.3% and 87.5% absolute translation error reduction with only a small
additional computational cost on two public SLAM benchmark DBs: EuRoC and
TUM-VI respectively.
</p>
<a href="http://arxiv.org/abs/2102.13438" target="_blank">arXiv:2102.13438</a> [<a href="http://arxiv.org/pdf/2102.13438" target="_blank">pdf</a>]

<h2>ACDnet: An action detection network for real-time edge computing based on flow-guided feature approximation and memory aggregation. (arXiv:2102.13493v1 [cs.CV])</h2>
<h3>Yu Liu, Fan Yang, Dominique Ginhac</h3>
<p>Interpreting human actions requires understanding the spatial and temporal
context of the scenes. State-of-the-art action detectors based on Convolutional
Neural Network (CNN) have demonstrated remarkable results by adopting
two-stream or 3D CNN architectures. However, these methods typically operate in
a non-real-time, ofline fashion due to system complexity to reason
spatio-temporal information. Consequently, their high computational cost is not
compliant with emerging real-world scenarios such as service robots or public
surveillance where detection needs to take place at resource-limited edge
devices. In this paper, we propose ACDnet, a compact action detection network
targeting real-time edge computing which addresses both efficiency and
accuracy. It intelligently exploits the temporal coherence between successive
video frames to approximate their CNN features rather than naively extracting
them. It also integrates memory feature aggregation from past video frames to
enhance current detection stability, implicitly modeling long temporal cues
over time. Experiments conducted on the public benchmark datasets UCF-24 and
JHMDB-21 demonstrate that ACDnet, when integrated with the SSD detector, can
robustly achieve detection well above real-time (75 FPS). At the same time, it
retains reasonable accuracy (70.92 and 49.53 frame mAP) compared to other
top-performing methods using far heavier configurations. Codes will be
available at https://github.com/dginhac/ACDnet.
</p>
<a href="http://arxiv.org/abs/2102.13493" target="_blank">arXiv:2102.13493</a> [<a href="http://arxiv.org/pdf/2102.13493" target="_blank">pdf</a>]

<h2>Detection of Alzheimer's Disease Using Graph-Regularized Convolutional Neural Network Based on Structural Similarity Learning of Brain Magnetic Resonance Images. (arXiv:2102.13517v1 [cs.CV])</h2>
<h3>Kuo Yang, Emad A. Mohammed, Behrouz H. Far</h3>
<p>Objective: This paper presents an Alzheimer's disease (AD) detection method
based on learning structural similarity between Magnetic Resonance Images
(MRIs) and representing this similarity as a graph. Methods: We construct the
similarity graph using embedded features of the input image (i.e., Non-Demented
(ND), Very Mild Demented (VMD), Mild Demented (MD), and Moderated Demented
(MDTD)). We experiment and compare different dimension-reduction and clustering
algorithms to construct the best similarity graph to capture the similarity
between the same class images using the cosine distance as a similarity
measure. We utilize the similarity graph to present (sample) the training data
to a convolutional neural network (CNN). We use the similarity graph as a
regularizer in the loss function of a CNN model to minimize the distance
between the input images and their k-nearest neighbours in the similarity graph
while minimizing the categorical cross-entropy loss between the training image
predictions and the actual image class labels. Results: We conduct extensive
experiments with several pre-trained CNN models and compare the results to
other recent methods. Conclusion: Our method achieves superior performance on
the testing dataset (accuracy = 0.986, area under receiver operating
characteristics curve = 0.998, F1 measure = 0.987). Significance: The
classification results show an improvement in the prediction accuracy compared
to the other methods. We release all the code used in our experiments to
encourage reproducible research in this area
</p>
<a href="http://arxiv.org/abs/2102.13517" target="_blank">arXiv:2102.13517</a> [<a href="http://arxiv.org/pdf/2102.13517" target="_blank">pdf</a>]

<h2>Nested-block self-attention for robust radiotherapy planning segmentation. (arXiv:2102.13541v1 [cs.CV])</h2>
<h3>Harini Veeraraghavan, Jue Jiang, Sharif Elguindi, Sean L. Berry, Ifeanyirochukwu Onochie, Aditya Apte, Laura Cervino, Joseph O. Deasy</h3>
<p>Although deep convolutional networks have been widely studied for head and
neck (HN) organs at risk (OAR) segmentation, their use for routine clinical
treatment planning is limited by a lack of robustness to imaging artifacts, low
soft tissue contrast on CT, and the presence of abnormal anatomy. In order to
address these challenges, we developed a computationally efficient nested block
self-attention (NBSA) method that can be combined with any convolutional
network. Our method achieves computational efficiency by performing non-local
calculations within memory blocks of fixed spatial extent. Contextual
dependencies are captured by passing information in a raster scan order between
blocks, as well as through a second attention layer that causes bi-directional
attention flow. We implemented our approach on three different networks to
demonstrate feasibility. Following training using 200 cases, we performed
comprehensive evaluations using conventional and clinical metrics on a separate
set of 172 test scans sourced from external and internal institution datasets
without any exclusion criteria. NBSA required a similar number of computations
(15.7 gflops) as the most efficient criss-cross attention (CCA) method and
generated significantly more accurate segmentations for brain stem (Dice of
0.89 vs. 0.86) and parotid glands (0.86 vs. 0.84) than CCA. NBSA's
segmentations were less variable than multiple 3D methods, including for small
organs with low soft-tissue contrast such as the submandibular glands (surface
Dice of 0.90).
</p>
<a href="http://arxiv.org/abs/2102.13541" target="_blank">arXiv:2102.13541</a> [<a href="http://arxiv.org/pdf/2102.13541" target="_blank">pdf</a>]

<h2>Using Deep Learning to Automate the Detection of Flaws in Nuclear Fuel Channel UT Scans. (arXiv:2102.13635v1 [cs.CV])</h2>
<h3>Issam Hammad, Ryan Simpson, Hippolyte Djonon Tsague, Sarah Hall</h3>
<p>Nuclear reactor inspections are critical to ensure the safety and reliability
of plants operation. Inspections occur during planned outages and include the
inspection of the reactor's fuel channels. In Canada, Ultrasonic Testing (UT)
is used to inspect the health of fuel channels in Canada's Deuterium Uranium
(CANDU) reactors. Currently, analysis of the UT scans is performed by manual
visualization and measurement to locate, characterize, and disposition flaws.
Therefore, there is a motivation to develop an automated method that is fast
and accurate. In this paper, a proof of concept (PoC) that automates the
detection of flaws in nuclear fuel channel UT scans using a convolutional
neural network (CNN) is presented. This industry research was conducted at
Alithya Digital Technology Corporation in Pickering, Ontario, Canada. The CNN
model was trained after constructing a dataset using historical UT scans and
the corresponding inspection results. This data was obtained from a large
nuclear power generation company in Ontario. The requirement for this prototype
was to identify the location of at least a portion of each flaw in fuel channel
scans while minimizing false positives (FPs). This allows for automatic
detection of the location of each flaw where further manual analysis is
performed to identify the extent and the type of the flaw. Based on the defined
requirement, the proposed model was able to achieve 100% accuracy for UT scans
with minor chatter and a 100% sensitivity with minimal FPs for complicated UT
scans with severe chatter using 18 UT full test scans.
</p>
<a href="http://arxiv.org/abs/2102.13635" target="_blank">arXiv:2102.13635</a> [<a href="http://arxiv.org/pdf/2102.13635" target="_blank">pdf</a>]

<h2>Surgical Visual Domain Adaptation: Results from the MICCAI 2020 SurgVisDom Challenge. (arXiv:2102.13644v1 [cs.CV])</h2>
<h3>Aneeq Zia, Kiran Bhattacharyya, Xi Liu, Ziheng Wang, Satoshi Kondo, Emanuele Colleoni, Beatrice van Amsterdam, Razeen Hussain, Raabid Hussain, Lena Maier-Hein, Danail Stoyanov, Stefanie Speidel, Anthony Jarc</h3>
<p>Surgical data science is revolutionizing minimally invasive surgery by
enabling context-aware applications. However, many challenges exist around
surgical data (and health data, more generally) needed to develop context-aware
models. This work - presented as part of the Endoscopic Vision (EndoVis)
challenge at the Medical Image Computing and Computer Assisted Intervention
(MICCAI) 2020 conference - seeks to explore the potential for visual domain
adaptation in surgery to overcome data privacy concerns. In particular, we
propose to use video from virtual reality (VR) simulations of surgical
exercises in robotic-assisted surgery to develop algorithms to recognize tasks
in a clinical-like setting. We present the performance of the different
approaches to solve visual domain adaptation developed by challenge
participants. Our analysis shows that the presented models were unable to learn
meaningful motion based features form VR data alone, but did significantly
better when small amount of clinical-like data was also made available. Based
on these results, we discuss promising methods and further work to address the
problem of visual domain adaptation in surgical data science. We also release
the challenge dataset publicly at https://www.synapse.org/surgvisdom2020.
</p>
<a href="http://arxiv.org/abs/2102.13644" target="_blank">arXiv:2102.13644</a> [<a href="http://arxiv.org/pdf/2102.13644" target="_blank">pdf</a>]

<h2>Labelling Vertebrae with 2D Reformations of Multidetector CT Images: An Adversarial Approach for Incorporating Prior Knowledge of Spine Anatomy. (arXiv:1902.02205v4 [cs.CV] UPDATED)</h2>
<h3>Anjany Sekuboyina, Markus Rempfler, Alexander Valentinitsch, Bjoern H. Menze, Jan S. Kirschke</h3>
<p>Purpose: To use and test a labelling algorithm that operates on
two-dimensional (2D) reformations, rather than three-dimensional (3D) data to
locate and identify vertebrae.

Methods: We improved the Btrfly Net (described by Sekuboyina et al) that
works on sagittal and coronal maximum intensity projections (MIP) and augmented
it with two additional components: spine-localization and adversarial a
priori-learning. Furthermore, we explored two variants of adversarial training
schemes that incorporated the anatomical a priori knowledge into the Btrfly
Net. We investigated the superiority of the proposed approach for labelling
vertebrae on three datasets: a public benchmarking dataset of 302 CT scans and
two in-house datasets with a total of 238 CT scans. We employed Wilcoxon
signed-rank test to compute the statistical significance of the improvement in
performance observed due to various architectural components in our approach.

Results: On the public dataset, our approach using the described
Btrfly(pe-eb) network performed on par with current state-of-the-art methods
achieving a statistically significant (p &lt; .001) vertebrae identification rate
of 88.5+/-0.2 % and localization distances of less than 7-mm. On the in-house
datasets that had a higher inter-scan data variability, we obtained an
identification rate of 85.1+/-1.2%.

Conclusion: An identification performance comparable to existing 3D
approaches was achieved when labelling vertebrae on 2D MIPs. The performance
was further improved using the proposed adversarial training regime that
effectively enforced local spine a priori knowledge during training. Lastly,
spine-localization increased the generalizability of our approach by
homogenizing the content in the MIPs.
</p>
<a href="http://arxiv.org/abs/1902.02205" target="_blank">arXiv:1902.02205</a> [<a href="http://arxiv.org/pdf/1902.02205" target="_blank">pdf</a>]

<h2>Unsupervised Domain Adaptation for Object Detection via Cross-Domain Semi-Supervised Learning. (arXiv:1911.07158v4 [cs.CV] UPDATED)</h2>
<h3>Fuxun Yu, Di Wang, Yinpeng Chen, Nikolaos Karianakis, Tong Shen, Pei Yu, Dimitrios Lymberopoulos, Sidi Lu, Weisong Shi, Xiang Chen</h3>
<p>Current state-of-the-art object detectors can have significant performance
drop when deployed in the wild due to domain gaps with training data.
Unsupervised Domain Adaptation (UDA) is a promising approach to adapt models
for new domains/environments without any expensive label cost. However, without
ground truth labels, most prior works on UDA for object detection tasks can
only perform coarse image-level and/or feature-level adaptation by using
adversarial learning methods. In this work, we show that such adversarial-based
methods can only reduce the domain style gap, but cannot address the domain
content distribution gap that is shown to be important for object detectors. To
overcome this limitation, we propose the Cross-Domain Semi-Supervised Learning
(CDSSL) framework by leveraging high-quality pseudo labels to learn better
representations from the target domain directly. To enable SSL for cross-domain
object detection, we propose fine-grained domain transfer,
progressive-confidence-based label sharpening and imbalanced sampling strategy
to address two challenges: (i) non-identical distribution between source and
target domain data, (ii) error amplification/accumulation due to noisy pseudo
labeling on the target domain. Experiment results show that our proposed
approach consistently achieves new state-of-the-art performance (2.2% - 9.5%
better than prior best work on mAP) under various domain gap scenarios. The
code will be released.
</p>
<a href="http://arxiv.org/abs/1911.07158" target="_blank">arXiv:1911.07158</a> [<a href="http://arxiv.org/pdf/1911.07158" target="_blank">pdf</a>]

<h2>Bottom-Up Temporal Action Localization with Mutual Regularization. (arXiv:2002.07358v3 [cs.CV] UPDATED)</h2>
<h3>Peisen Zhao, Lingxi Xie, Chen Ju, Ya Zhang, Yanfeng Wang, Qi Tian</h3>
<p>Recently, temporal action localization (TAL), i.e., finding specific action
segments in untrimmed videos, has attracted increasing attentions of the
computer vision community. State-of-the-art solutions for TAL involves
evaluating the frame-level probabilities of three action-indicating phases,
i.e. starting, continuing, and ending; and then post-processing these
predictions for the final localization. This paper delves deep into this
mechanism, and argues that existing methods, by modeling these phases as
individual classification tasks, ignored the potential temporal constraints
between them. This can lead to incorrect and/or inconsistent predictions when
some frames of the video input lack sufficient discriminative information. To
alleviate this problem, we introduce two regularization terms to mutually
regularize the learning procedure: the Intra-phase Consistency (IntraC)
regularization is proposed to make the predictions verified inside each phase;
and the Inter-phase Consistency (InterC) regularization is proposed to keep
consistency between these phases. Jointly optimizing these two terms, the
entire framework is aware of these potential constraints during an end-to-end
optimization process. Experiments are performed on two popular TAL datasets,
THUMOS14 and ActivityNet1.3. Our approach clearly outperforms the baseline both
quantitatively and qualitatively. The proposed regularization also generalizes
to other TAL methods (e.g., TSA-Net and PGCN). code:
https://github.com/PeisenZhao/Bottom-Up-TAL-with-MR
</p>
<a href="http://arxiv.org/abs/2002.07358" target="_blank">arXiv:2002.07358</a> [<a href="http://arxiv.org/pdf/2002.07358" target="_blank">pdf</a>]

<h2>A Generalized Asymmetric Dual-front Model for Active Contours and Image Segmentation. (arXiv:2006.07839v2 [cs.CV] UPDATED)</h2>
<h3>Da Chen, Jack Spencer, Jean-Marie Mirebeau, Ke Chen, Minglei Shu, Laurent D. Cohen</h3>
<p>The Voronoi diagram-based dual-front active contour models are known as a
powerful and efficient way for addressing the image segmentation and domain
partitioning problems. In the basic formulation of the dual-front models, the
evolving contours can be considered as the interfaces of adjacent Voronoi
regions. Among these dual-front models, a crucial ingredient is regarded as the
geodesic metrics by which the geodesic distances and the corresponding Voronoi
diagram can be estimated. In this paper, we introduce a type of asymmetric
quadratic metrics dual-front model. The metrics considered are built by the
integration of the image features and a vector field derived from the evolving
contours. The use of the asymmetry enhancement can reduce the risk of contour
shortcut or leakage problems especially when the initial contours are far away
from the target boundaries or the images have complicated intensity
distributions. Moreover, the proposed dual-front model can be applied for image
segmentation in conjunction with various region-based homogeneity terms. The
numerical experiments on both synthetic and real images show that the proposed
dual-front model indeed achieves encouraging results.
</p>
<a href="http://arxiv.org/abs/2006.07839" target="_blank">arXiv:2006.07839</a> [<a href="http://arxiv.org/pdf/2006.07839" target="_blank">pdf</a>]

<h2>Hard negative examples are hard, but useful. (arXiv:2007.12749v2 [cs.CV] UPDATED)</h2>
<h3>Hong Xuan, Abby Stylianou, Xiaotong Liu, Robert Pless</h3>
<p>Triplet loss is an extremely common approach to distance metric learning.
Representations of images from the same class are optimized to be mapped closer
together in an embedding space than representations of images from different
classes. Much work on triplet losses focuses on selecting the most useful
triplets of images to consider, with strategies that select dissimilar examples
from the same class or similar examples from different classes. The consensus
of previous research is that optimizing with the \textit{hardest} negative
examples leads to bad training behavior. That's a problem -- these hardest
negatives are literally the cases where the distance metric fails to capture
semantic similarity. In this paper, we characterize the space of triplets and
derive why hard negatives make triplet loss training fail. We offer a simple
fix to the loss function and show that, with this fix, optimizing with hard
negative examples becomes feasible. This leads to more generalizable features,
and image retrieval results that outperform state of the art for datasets with
high intra-class variance.
</p>
<a href="http://arxiv.org/abs/2007.12749" target="_blank">arXiv:2007.12749</a> [<a href="http://arxiv.org/pdf/2007.12749" target="_blank">pdf</a>]

<h2>Semantics-aware Adaptive Knowledge Distillation for Sensor-to-Vision Action Recognition. (arXiv:2009.00210v3 [cs.CV] UPDATED)</h2>
<h3>Yang Liu, Guanbin Li, Liang Lin</h3>
<p>Existing vision-based action recognition is susceptible to occlusion and
appearance variations, while wearable sensors can alleviate these challenges by
capturing human motion with one-dimensional time-series signal. For the same
action, the knowledge learned from vision sensors and wearable sensors, may be
related and complementary. However, there exists significantly large modality
difference between action data captured by wearable-sensor and vision-sensor in
data dimension, data distribution and inherent information content. In this
paper, we propose a novel framework, named Semantics-aware Adaptive Knowledge
Distillation Networks (SAKDN), to enhance action recognition in vision-sensor
modality (videos) by adaptively transferring and distilling the knowledge from
multiple wearable sensors. The SAKDN uses multiple wearable-sensors as teacher
modalities and uses RGB videos as student modality. To preserve local temporal
relationship and facilitate employing visual deep learning model, we transform
one-dimensional time-series signals of wearable sensors to two-dimensional
images by designing a gramian angular field based virtual image generation
model. Then, we build a novel Similarity-Preserving Adaptive Multi-modal Fusion
Module to adaptively fuse intermediate representation knowledge from different
teacher networks. Finally, to fully exploit and transfer the knowledge of
multiple well-trained teacher networks to the student network, we propose a
novel Graph-guided Semantically Discriminative Mapping loss, which utilizes
graph-guided ablation analysis to produce a good visual explanation
highlighting the important regions across modalities and concurrently
preserving the interrelations of original data. Experimental results on
Berkeley-MHAD, UTD-MHAD and MMAct datasets well demonstrate the effectiveness
of our proposed SAKDN.
</p>
<a href="http://arxiv.org/abs/2009.00210" target="_blank">arXiv:2009.00210</a> [<a href="http://arxiv.org/pdf/2009.00210" target="_blank">pdf</a>]

<h2>Memory Clustering using Persistent Homology for Multimodality- and Discontinuity-Sensitive Learning of Optimal Control Warm-starts. (arXiv:2010.01024v2 [cs.RO] UPDATED)</h2>
<h3>Wolfgang Merkt, Vladimir Ivan, Traiko Dinev, Ioannis Havoutis, Sethu Vijayakumar</h3>
<p>Shooting methods are an efficient approach to solving nonlinear optimal
control problems. As they use local optimization, they exhibit favorable
convergence when initialized with a good warm-start but may not converge at all
if provided with a poor initial guess. Recent work has focused on providing an
initial guess from a learned model trained on samples generated during an
offline exploration of the problem space. However, in practice the solutions
contain discontinuities introduced by system dynamics or the environment.
Additionally, in many cases multiple equally suitable, i.e., multi-modal,
solutions exist to solve a problem. Classic learning approaches smooth across
the boundary of these discontinuities and thus generalize poorly. In this work,
we apply tools from algebraic topology to extract information on the underlying
structure of the solution space. In particular, we introduce a method based on
persistent homology to automatically cluster the dataset of precomputed
solutions to obtain different candidate initial guesses. We then train a
Mixture-of-Experts within each cluster to predict state and control
trajectories to warm-start the optimal control solver and provide a comparison
with modality-agnostic learning. We demonstrate our method on a cart-pole toy
problem and a quadrotor avoiding obstacles, and show that clustering samples
based on inherent structure improves the warm-start quality.
</p>
<a href="http://arxiv.org/abs/2010.01024" target="_blank">arXiv:2010.01024</a> [<a href="http://arxiv.org/pdf/2010.01024" target="_blank">pdf</a>]

<h2>Survey on 3D face reconstruction from uncalibrated images. (arXiv:2011.05740v2 [cs.CV] UPDATED)</h2>
<h3>Araceli Morales, Gemma Piella, Federico M. Sukno</h3>
<p>Recently, a lot of attention has been focused on the incorporation of 3D data
into face analysis and its applications. Despite providing a more accurate
representation of the face, 3D facial images are more complex to acquire than
2D pictures. As a consequence, great effort has been invested in developing
systems that reconstruct 3D faces from an uncalibrated 2D image. However, the
3D-from-2D face reconstruction problem is ill-posed, thus prior knowledge is
needed to restrict the solutions space. In this work, we review 3D face
reconstruction methods proposed in the last decade, focusing on those that only
use 2D pictures captured under uncontrolled conditions. We present a
classification of the proposed methods based on the technique used to add prior
knowledge, considering three main strategies, namely, statistical model
fitting, photometry, and deep learning, and reviewing each of them separately.
In addition, given the relevance of statistical 3D facial models as prior
knowledge, we explain the construction procedure and provide a list of the most
popular publicly available 3D facial models. After the exhaustive study of
3D-from-2D face reconstruction approaches, we observe that the deep learning
strategy is rapidly growing since the last few years, becoming the standard
choice in replacement of the widespread statistical model fitting. Unlike the
other two strategies, photometry-based methods have decreased in number due to
the need for strong underlying assumptions that limit the quality of their
reconstructions compared to statistical model fitting and deep learning
methods. The review also identifies current challenges and suggests avenues for
future research.
</p>
<a href="http://arxiv.org/abs/2011.05740" target="_blank">arXiv:2011.05740</a> [<a href="http://arxiv.org/pdf/2011.05740" target="_blank">pdf</a>]

<h2>Bi-directional Domain Adaptation for Sim2Real Transfer of Embodied Navigation Agents. (arXiv:2011.12421v2 [cs.RO] UPDATED)</h2>
<h3>Joanne Truong, Sonia Chernova, Dhruv Batra</h3>
<p>Deep reinforcement learning models are notoriously data hungry, yet
real-world data is expensive and time consuming to obtain. The solution that
many have turned to is to use simulation for training before deploying the
robot in a real environment. Simulation offers the ability to train large
numbers of robots in parallel, and offers an abundance of data. However, no
simulation is perfect, and robots trained solely in simulation fail to
generalize to the real-world, resulting in a "sim-vs-real gap". How can we
overcome the trade-off between the abundance of less accurate, artificial data
from simulators and the scarcity of reliable, real-world data? In this paper,
we propose Bi-directional Domain Adaptation (BDA), a novel approach to bridge
the sim-vs-real gap in both directions -- real2sim to bridge the visual domain
gap, and sim2real to bridge the dynamics domain gap. We demonstrate the
benefits of BDA on the task of PointGoal Navigation. BDA with only 5k
real-world (state, action, next-state) samples matches the performance of a
policy fine-tuned with ~600k samples, resulting in a speed-up of ~120x.
</p>
<a href="http://arxiv.org/abs/2011.12421" target="_blank">arXiv:2011.12421</a> [<a href="http://arxiv.org/pdf/2011.12421" target="_blank">pdf</a>]

<h2>Volumetric Occupancy Mapping With Probabilistic Depth Completion for Robotic Navigation. (arXiv:2012.03023v2 [cs.RO] UPDATED)</h2>
<h3>Marija Popovic, Florian Thomas, Sotiris Papatheodorou, Nils Funk, Teresa Vidal-Calleja, Stefan Leutenegger</h3>
<p>In robotic applications, a key requirement for safe and efficient motion
planning is the ability to map obstacle-free space in unknown, cluttered 3D
environments. However, commodity-grade RGB-D cameras commonly used for sensing
fail to register valid depth values on shiny, glossy, bright, or distant
surfaces, leading to missing data in the map. To address this issue, we propose
a framework leveraging probabilistic depth completion as an additional input
for spatial mapping. We introduce a deep learning architecture providing
uncertainty estimates for the depth completion of RGB-D images. Our pipeline
exploits the inferred missing depth values and depth uncertainty to complement
raw depth images and improve the speed and quality of free space mapping.
Evaluations on synthetic data show that our approach maps significantly more
correct free space with relatively low error when compared against using raw
data alone in different indoor environments; thereby producing more complete
maps that can be directly used for robotic navigation tasks. The performance of
our framework is validated using real-world data.
</p>
<a href="http://arxiv.org/abs/2012.03023" target="_blank">arXiv:2012.03023</a> [<a href="http://arxiv.org/pdf/2012.03023" target="_blank">pdf</a>]

<h2>You Only Need Adversarial Supervision for Semantic Image Synthesis. (arXiv:2012.04781v2 [cs.CV] UPDATED)</h2>
<h3>Vadim Sushko, Edgar Sch&#xf6;nfeld, Dan Zhang, Juergen Gall, Bernt Schiele, Anna Khoreva</h3>
<p>Despite their recent successes, GAN models for semantic image synthesis still
suffer from poor image quality when trained with only adversarial supervision.
Historically, additionally employing the VGG-based perceptual loss has helped
to overcome this issue, significantly improving the synthesis quality, but at
the same time limiting the progress of GAN models for semantic image synthesis.
In this work, we propose a novel, simplified GAN model, which needs only
adversarial supervision to achieve high quality results. We re-design the
discriminator as a semantic segmentation network, directly using the given
semantic label maps as the ground truth for training. By providing stronger
supervision to the discriminator as well as to the generator through spatially-
and semantically-aware discriminator feedback, we are able to synthesize images
of higher fidelity with better alignment to their input label maps, making the
use of the perceptual loss superfluous. Moreover, we enable high-quality
multi-modal image synthesis through global and local sampling of a 3D noise
tensor injected into the generator, which allows complete or partial image
change. We show that images synthesized by our model are more diverse and
follow the color and texture distributions of real images more closely. We
achieve an average improvement of $6$ FID and $5$ mIoU points over the state of
the art across different datasets using only adversarial supervision.
</p>
<a href="http://arxiv.org/abs/2012.04781" target="_blank">arXiv:2012.04781</a> [<a href="http://arxiv.org/pdf/2012.04781" target="_blank">pdf</a>]

<h2>Non-Rigid Neural Radiance Fields: Reconstruction and Novel View Synthesis of a Dynamic Scene From Monocular Video. (arXiv:2012.12247v3 [cs.CV] UPDATED)</h2>
<h3>Edgar Tretschk, Ayush Tewari, Vladislav Golyanik, Michael Zollh&#xf6;fer, Christoph Lassner, Christian Theobalt</h3>
<p>We present Non-Rigid Neural Radiance Fields (NR-NeRF), a reconstruction and
novel view synthesis approach for general non-rigid dynamic scenes. Our
approach takes RGB images of a dynamic scene as input, e.g., from a monocular
video recording, and creates a high-quality space-time geometry and appearance
representation. In particular, we show that even a single handheld
consumer-grade camera is sufficient to synthesize sophisticated renderings of a
dynamic scene from novel virtual camera views, for example a `bullet-time'
video effect. Our method disentangles the dynamic scene into a canonical volume
and its deformation. Scene deformation is implemented as ray bending, where
straight rays are deformed non-rigidly to represent scene motion. We also
propose a novel rigidity regression network that enables us to better constrain
rigid regions of the scene, which leads to more stable results. The ray bending
and rigidity network are trained without any explicit supervision. In addition
to novel view synthesis, our formulation enables dense correspondence
estimation across views and time, as well as compelling video editing
applications such as motion exaggeration. We demonstrate the effectiveness of
our method using extensive evaluations, including ablation studies and
comparisons to the state of the art. We urge the reader to watch the
supplemental video for qualitative results. Our code will be open sourced.
</p>
<a href="http://arxiv.org/abs/2012.12247" target="_blank">arXiv:2012.12247</a> [<a href="http://arxiv.org/pdf/2012.12247" target="_blank">pdf</a>]

<h2>Object Detection Made Simpler by Eliminating Heuristic NMS. (arXiv:2101.11782v2 [cs.CV] UPDATED)</h2>
<h3>Qiang Zhou, Chaohui Yu, Chunhua Shen, Zhibin Wang, Hao Li</h3>
<p>We show a simple NMS-free, end-to-end object detection framework, of which
the network is a minimal modification to a one-stage object detector such as
the FCOS detection model [Tian et al. 2019]. We attain on par or even improved
detection accuracy compared with the original one-stage detector. It performs
detection at almost the same inference speed, while being even simpler in that
now the post-processing NMS (non-maximum suppression) is eliminated during
inference. If the network is capable of identifying only one positive sample
for prediction for each ground-truth object instance in an image, then NMS
would become unnecessary. This is made possible by attaching a compact PSS head
for automatic selection of the single positive sample for each instance (see
Fig. 1). As the learning objective involves both one-to-many and one-to-one
label assignments, there is a conflict in the labels of some training examples,
making the learning challenging. We show that by employing a stop-gradient
operation, we can successfully tackle this issue and train the detector. On the
COCO dataset, our simple design achieves superior performance compared to both
the FCOS baseline detector with NMS post-processing and the recent end-to-end
NMS-free detectors. Our extensive ablation studies justify the rationale of the
design choices.
</p>
<a href="http://arxiv.org/abs/2101.11782" target="_blank">arXiv:2101.11782</a> [<a href="http://arxiv.org/pdf/2101.11782" target="_blank">pdf</a>]

<h2>Fast Online Planning for Bipedal Locomotion via Centroidal Model Predictive Gait Synthesis. (arXiv:2102.04122v2 [cs.RO] UPDATED)</h2>
<h3>Yijie Guo, Mingguo Zhao</h3>
<p>The planning of whole-body motion and step time for bipedal locomotion is
constructed as a model predictive control (MPC) problem, in which a sequence of
optimization problems need to be solved online. While directly solving these
problems is extremely time-consuming, we propose a predictive gait synthesizer
to solve them quickly online. Based on the full dimensional model, a library of
gaits with different speeds and periods is first constructed offline. Then the
proposed gait synthesizer generates real-time gaits by synthesizing the gait
library based on the online prediction of centroidal dynamics. We prove that
the generated gaits are feasible solutions of the MPC optimization problems.
Thus our proposed gait synthesizer works as a fast MPC-style planner to
guarantee the feasibility and stability of the full dimensional robot.
Simulation and experimental results on an 8 degrees of freedom (DoF) bipedal
robot are provided to show the performance and robustness of this approach for
walking and standing.
</p>
<a href="http://arxiv.org/abs/2102.04122" target="_blank">arXiv:2102.04122</a> [<a href="http://arxiv.org/pdf/2102.04122" target="_blank">pdf</a>]

<h2>The Hessigheim 3D (H3D) Benchmark on Semantic Segmentation of High-Resolution 3D Point Clouds and Textured Meshes from UAV LiDAR and Multi-View-Stereo. (arXiv:2102.05346v2 [cs.CV] UPDATED)</h2>
<h3>Michael K&#xf6;lle, Dominik Laupheimer, Stefan Schmohl, Norbert Haala, Franz Rottensteiner, Jan Dirk Wegner, Hugo Ledoux</h3>
<p>Automated semantic segmentation and object detection are of great importance
in geospatial data analysis. However, supervised machine learning systems such
as convolutional neural networks require large corpora of annotated training
data. Especially in the geospatial domain, such datasets are quite scarce.
Within this paper, we aim to alleviate this issue by introducing a new
annotated 3D dataset that is unique in three ways: i) The dataset consists of
both an Unmanned Aerial Vehicle (UAV) laser scanning point cloud and a 3D
textured mesh. ii) The point cloud features a mean point density of about 800
pts/sqm and the oblique imagery used for 3D mesh texturing realizes a ground
sampling distance of about 2-3 cm. This enables the identification of
fine-grained structures and represents the state of the art in UAV-based
mapping. iii) Both data modalities will be published for a total of three
epochs allowing applications such as change detection. The dataset depicts the
village of Hessigheim (Germany), henceforth referred to as H3D. It is designed
to promote research in the field of 3D data analysis on one hand and to
evaluate and rank existing and emerging approaches for semantic segmentation of
both data modalities on the other hand. Ultimately, we hope that H3D will
become a widely used benchmark dataset in company with the well-established
ISPRS Vaihingen 3D Semantic Labeling Challenge benchmark (V3D). The dataset can
be downloaded from
https://ifpwww.ifp.uni-stuttgart.de/benchmark/hessigheim/default.aspx.
</p>
<a href="http://arxiv.org/abs/2102.05346" target="_blank">arXiv:2102.05346</a> [<a href="http://arxiv.org/pdf/2102.05346" target="_blank">pdf</a>]

<h2>Generation For Adaption: A GAN-Based Approach for 3D Domain Adaption with Point Cloud Data. (arXiv:2102.07373v2 [cs.CV] UPDATED)</h2>
<h3>Junxuan Huang, Chunming Qiao</h3>
<p>Recent deep networks have achieved good performance on a variety of 3d points
classification tasks. However, these models often face challenges in "wild
tasks".There are considerable differences between the labeled training/source
data collected by one Lidar and unseen test/target data collected by a
different Lidar. Unsupervised domain adaptation (UDA) seeks to overcome such a
problem without target domain labels.Instead of aligning features between
source data and target data,we propose a method that use a Generative
adversarial network to generate synthetic data from the source domain so that
the output is close to the target domain.Experiments show that our approach
performs better than other state-of-the-art UDA methods in three popular 3D
object/scene datasets (i.e., ModelNet, ShapeNet and ScanNet) for cross-domain
3D objects classification.
</p>
<a href="http://arxiv.org/abs/2102.07373" target="_blank">arXiv:2102.07373</a> [<a href="http://arxiv.org/pdf/2102.07373" target="_blank">pdf</a>]

<h2>A Straightforward Framework For Video Retrieval Using CLIP. (arXiv:2102.12443v2 [cs.CV] UPDATED)</h2>
<h3>Jes&#xfa;s Andr&#xe9;s Portillo-Quintero, Jos&#xe9; Carlos Ortiz-Bayliss, Hugo Terashima-Mar&#xed;n</h3>
<p>Video Retrieval is a challenging task where a text query is matched to a
video or vice versa. Most of the existing approaches for addressing such a
problem rely on annotations made by the users. Although simple, this approach
is not always feasible in practice. In this work, we explore the application of
the language-image model, CLIP, to obtain video representations without the
need for said annotations. This model was explicitly trained to learn a common
space where images and text can be compared. Using various techniques described
in this document, we extended its application to videos, obtaining
state-of-the-art results on the MSR-VTT and MSVD benchmarks.
</p>
<a href="http://arxiv.org/abs/2102.12443" target="_blank">arXiv:2102.12443</a> [<a href="http://arxiv.org/pdf/2102.12443" target="_blank">pdf</a>]

<h2>CelebA-Spoof Challenge 2020 on Face Anti-Spoofing: Methods and Results. (arXiv:2102.12642v2 [cs.CV] UPDATED)</h2>
<h3>Yuanhan Zhang, Zhenfei Yin, Jing Shao, Ziwei Liu, Shuo Yang, Yuanjun Xiong, Wei Xia, Yan Xu, Man Luo, Jian Liu, Jianshu Li, Zhijun Chen, Mingyu Guo, Hui Li, Junfu Liu, Pengfei Gao, Tianqi Hong, Hao Han, Shijie Liu, Xinhua Chen, Di Qiu, Cheng Zhen, Dashuang Liang, Yufeng Jin, Zhanlong Hao</h3>
<p>As facial interaction systems are prevalently deployed, security and
reliability of these systems become a critical issue, with substantial research
efforts devoted. Among them, face anti-spoofing emerges as an important area,
whose objective is to identify whether a presented face is live or spoof.
Recently, a large-scale face anti-spoofing dataset, CelebA-Spoof which
comprised of 625,537 pictures of 10,177 subjects has been released. It is the
largest face anti-spoofing dataset in terms of the numbers of the data and the
subjects. This paper reports methods and results in the CelebA-Spoof Challenge
2020 on Face AntiSpoofing which employs the CelebA-Spoof dataset. The model
evaluation is conducted online on the hidden test set. A total of 134
participants registered for the competition, and 19 teams made valid
submissions. We will analyze the top ranked solutions and present some
discussion on future work directions.
</p>
<a href="http://arxiv.org/abs/2102.12642" target="_blank">arXiv:2102.12642</a> [<a href="http://arxiv.org/pdf/2102.12642" target="_blank">pdf</a>]

<h2>Maximizing Cosine Similarity Between Spatial Features for Unsupervised Domain Adaptation in Semantic Segmentation. (arXiv:2102.13002v2 [cs.CV] UPDATED)</h2>
<h3>Inseop Chung, Daesik Kim, Nojun Kwak</h3>
<p>We propose a novel method that tackles the problem of unsupervised domain
adaptation for semantic segmentation by maximizing the cosine similarity
between the source and the target domain at the feature level. A segmentation
network mainly consists of two parts, a feature extractor and a classification
head. We expect that if we can make the two domains have small domain gap at
the feature level, they would also have small domain discrepancy at the
classification head. Our method computes a cosine similarity matrix between the
source feature map and the target feature map, then we maximize the elements
exceeding a threshold to guide the target features to have high similarity with
the most similar source feature. Moreover, we use a class-wise source feature
dictionary which stores the latest features of the source domain to prevent the
unmatching problem when computing the cosine similarity matrix and be able to
compare a target feature with various source features from various images.
Through extensive experiments, we verify that our method gains performance on
two unsupervised domain adaptation tasks (GTA5$\to$ Cityscaspes and
SYNTHIA$\to$ Cityscapes).
</p>
<a href="http://arxiv.org/abs/2102.13002" target="_blank">arXiv:2102.13002</a> [<a href="http://arxiv.org/pdf/2102.13002" target="_blank">pdf</a>]

<h2>Where to go next: Learning a Subgoal Recommendation Policy for Navigation Among Pedestrians. (arXiv:2102.13073v2 [cs.RO] UPDATED)</h2>
<h3>Bruno Brito, Michael Everett, Jonathan P. How, Javier Alonso-Mora</h3>
<p>Robotic navigation in environments shared with other robots or humans remains
challenging because the intentions of the surrounding agents are not directly
observable and the environment conditions are continuously changing. Local
trajectory optimization methods, such as model predictive control (MPC), can
deal with those changes but require global guidance, which is not trivial to
obtain in crowded scenarios. This paper proposes to learn, via deep
Reinforcement Learning (RL), an interaction-aware policy that provides
long-term guidance to the local planner. In particular, in simulations with
cooperative and non-cooperative agents, we train a deep network to recommend a
subgoal for the MPC planner. The recommended subgoal is expected to help the
robot in making progress towards its goal and accounts for the expected
interaction with other agents. Based on the recommended subgoal, the MPC
planner then optimizes the inputs for the robot satisfying its kinodynamic and
collision avoidance constraints. Our approach is shown to substantially improve
the navigation performance in terms of number of collisions as compared to
prior MPC frameworks, and in terms of both travel time and number of collisions
compared to deep RL methods in cooperative, competitive and mixed multiagent
scenarios.
</p>
<a href="http://arxiv.org/abs/2102.13073" target="_blank">arXiv:2102.13073</a> [<a href="http://arxiv.org/pdf/2102.13073" target="_blank">pdf</a>]

