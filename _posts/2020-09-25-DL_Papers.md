---
title: Latest Deep Learning Papers
date: 2020-12-13 21:00:01 +0800
featured-img: DL-logo
categories: [Deep_Learning]
tags: [Deep_Learning]
mathjax: Yes
---

<h1>Your arXiv Feed (209 Articles)</h1>
<h2>Data-driven Method for Estimating Aircraft Mass from Quick Access Recorder using Aircraft Dynamics and Multilayer Perceptron Neural Network. (arXiv:2012.05907v1 [cs.LG])</h2>
<h3>Xinyu He, Fang He, Xinting Zhu, Lishuai Li</h3>
<p>Accurate aircraft-mass estimation is critical to airlines from the
safety-management and performance-optimization viewpoints. Overloading an
aircraft with passengers and baggage might result in a safety hazard. In
contrast, not fully utilizing an aircraft's payload-carrying capacity
undermines its operational efficiency and airline profitability. However,
accurate determination of the aircraft mass for each operating flight is not
feasible because it is impractical to weigh each aircraft component, including
the payload. The existing methods for aircraft-mass estimation are dependent on
the aircraft- and engine-performance parameters, which are usually considered
proprietary information. Moreover, the values of these parameters vary under
different operating conditions while those of others might be subject to large
estimation errors. This paper presents a data-driven method involving use of
the quick access recorder (QAR)-a digital flight-data recorder-installed on all
aircrafts to record the initial aircraft climb mass during each flight. The
method requires users to select appropriate parameters among several thousand
others recorded by the QAR using physical models. The selected data are
subsequently processed and provided as input to a multilayer perceptron neural
network for building the model for initial-climb aircraft-mass prediction.
Thus, the proposed method offers the advantages of both the model-based and
data-driven approaches for aircraft-mass estimation. Because this method does
not explicitly rely on any aircraft or engine parameter, it is universally
applicable to all aircraft types. In this study, the proposed method was
applied to a set of Boeing 777-300ER aircrafts, the results of which
demonstrated reasonable accuracy. Airlines can use this tool to better utilize
aircraft's payload.
</p>
<a href="http://arxiv.org/abs/2012.05907" target="_blank">arXiv:2012.05907</a> [<a href="http://arxiv.org/pdf/2012.05907" target="_blank">pdf</a>]

<h2>Blending MPC & Value Function Approximation for Efficient Reinforcement Learning. (arXiv:2012.05909v1 [cs.LG])</h2>
<h3>Mohak Bhardwaj, Sanjiban Choudhury, Byron Boots</h3>
<p>Model-Predictive Control (MPC) is a powerful tool for controlling complex,
real-world systems that uses a model to make predictions about future behavior.
For each state encountered, MPC solves an online optimization problem to choose
a control action that will minimize future cost. This is a surprisingly
effective strategy, but real-time performance requirements warrant the use of
simple models. If the model is not sufficiently accurate, then the resulting
controller can be biased, limiting performance. We present a framework for
improving on MPC with model-free reinforcement learning (RL). The key insight
is to view MPC as constructing a series of local Q-function approximations. We
show that by using a parameter $\lambda$, similar to the trace decay parameter
in TD($\lambda$), we can systematically trade-off learned value estimates
against the local Q-function approximations. We present a theoretical analysis
that shows how error from inaccurate models in MPC and value function
estimation in RL can be balanced. We further propose an algorithm that changes
$\lambda$ over time to reduce the dependence on MPC as our estimates of the
value function improve, and test the efficacy our approach on challenging
high-dimensional manipulation tasks with biased models in simulation. We
demonstrate that our approach can obtain performance comparable with MPC with
access to true dynamics even under severe model bias and is more sample
efficient as compared to model-free RL.
</p>
<a href="http://arxiv.org/abs/2012.05909" target="_blank">arXiv:2012.05909</a> [<a href="http://arxiv.org/pdf/2012.05909" target="_blank">pdf</a>]

<h2>A Simplistic Machine Learning Approach to Contact Tracing. (arXiv:2012.05940v1 [cs.LG])</h2>
<h3>Carlos G&#xf3;mez, Niamh Belton, Boi Quach, Jack Nicholls, Devanshu Anand</h3>
<p>This report is based on the modified NIST challenge, Too Close For Too Long,
provided by the SFI Centre for Machine Learning (ML-Labs). The modified
challenge excludes the time calculation (too long) aspect. By handcrafting
features from phone instrumental data we develop two machine learning models, a
GBM and an MLP, to estimate distance between two phones. Our method is able to
outperform the leading NIST challenge result by the Hong Kong University of
Science and Technology (HKUST) by a significant margin.
</p>
<a href="http://arxiv.org/abs/2012.05940" target="_blank">arXiv:2012.05940</a> [<a href="http://arxiv.org/pdf/2012.05940" target="_blank">pdf</a>]

<h2>Convex Potential Flows: Universal Probability Distributions with Optimal Transport and Convex Optimization. (arXiv:2012.05942v1 [cs.LG])</h2>
<h3>Chin-Wei Huang, Ricky T. Q. Chen, Christos Tsirigotis, Aaron Courville</h3>
<p>Flow-based models are powerful tools for designing probabilistic models with
tractable density. This paper introduces Convex Potential Flows (CP-Flow), a
natural and efficient parameterization of invertible models inspired by the
optimal transport (OT) theory. CP-Flows are the gradient map of a strongly
convex neural potential function. The convexity implies invertibility and
allows us to resort to convex optimization to solve the convex conjugate for
efficient inversion. To enable maximum likelihood training, we derive a new
gradient estimator of the log-determinant of the Jacobian, which involves
solving an inverse-Hessian vector product using the conjugate gradient method.
The gradient estimator has constant-memory cost, and can be made effectively
unbiased by reducing the error tolerance level of the convex optimization
routine. Theoretically, we prove that CP-Flows are universal density
approximators and are optimal in the OT sense. Our empirical results show that
CP-Flow performs competitively on standard benchmarks of density estimation and
variational inference.
</p>
<a href="http://arxiv.org/abs/2012.05942" target="_blank">arXiv:2012.05942</a> [<a href="http://arxiv.org/pdf/2012.05942" target="_blank">pdf</a>]

<h2>Autonomous Cooperative Wall Building by a Team of Unmanned Aerial Vehicles in the MBZIRC 2020 Competition. (arXiv:2012.05946v1 [cs.RO])</h2>
<h3>Tomas Baca, Robert Penicka, Petr Stepan, Matej Petrlik, Vojtech Spurny, Daniel Hert, Martin Saska</h3>
<p>This paper presents a system for autonomous cooperative wall building with a
team of Unmanned Aerial Vehicles (UAVs). The system was developed for Challenge
2 of the Mohamed Bin Zayed International Robotics Challenge (MBZIRC) 2020. The
wall-building scenario of Challenge 2 featured an initial stack of bricks and
wall structure where the individual bricks had to be placed by a team of three
UAVs. The objective of the task was to maximize collected points for placing
the bricks within the restricted construction time while following the
prescribed wall pattern. The proposed approach uses initial scanning to find a
priori unknown locations of the bricks and the wall structure. Each UAV is then
assigned to individual bricks and wall placing locations and further perform
grasping and placement using onboard resources only. The developed system
consists of methods for scanning a given area, RGB-D detection of bricks and
wall placement locations, precise grasping and placing of bricks, and
coordination of multiple UAVs. The paper describes the overall system,
individual components, experimental verification in demanding outdoor
conditions, the achieved results in the competition, and lessons learned. The
presented CTU-UPenn-NYU approach achieved the overall best performance among
all participants to won the MBZIRC competition by collecting the highest number
of points by correct placement of a high number of bricks.
</p>
<a href="http://arxiv.org/abs/2012.05946" target="_blank">arXiv:2012.05946</a> [<a href="http://arxiv.org/pdf/2012.05946" target="_blank">pdf</a>]

<h2>Super-resolution Guided Pore Detection for Fingerprint Recognition. (arXiv:2012.05959v1 [cs.CV])</h2>
<h3>Syeda Nyma Ferdous, Ali Dabouei, Jeremy Dawson, Nasser M Nasrabadi</h3>
<p>Performance of fingerprint recognition algorithms substantially rely on fine
features extracted from fingerprints. Apart from minutiae and ridge patterns,
pore features have proven to be usable for fingerprint recognition. Although
features from minutiae and ridge patterns are quite attainable from
low-resolution images, using pore features is practical only if the fingerprint
image is of high resolution which necessitates a model that enhances the image
quality of the conventional 500 ppi legacy fingerprints preserving the fine
details. To find a solution for recovering pore information from low-resolution
fingerprints, we adopt a joint learning-based approach that combines both
super-resolution and pore detection networks. Our modified single image
Super-Resolution Generative Adversarial Network (SRGAN) framework helps to
reliably reconstruct high-resolution fingerprint samples from low-resolution
ones assisting the pore detection network to identify pores with a high
accuracy. The network jointly learns a distinctive feature representation from
a real low-resolution fingerprint sample and successfully synthesizes a
high-resolution sample from it. To add discriminative information and
uniqueness for all the subjects, we have integrated features extracted from a
deep fingerprint verifier with the SRGAN quality discriminator. We also add
ridge reconstruction loss, utilizing ridge patterns to make the best use of
extracted features. Our proposed method solves the recognition problem by
improving the quality of fingerprint images. High recognition accuracy of the
synthesized samples that is close to the accuracy achieved using the original
high-resolution images validate the effectiveness of our proposed model.
</p>
<a href="http://arxiv.org/abs/2012.05959" target="_blank">arXiv:2012.05959</a> [<a href="http://arxiv.org/pdf/2012.05959" target="_blank">pdf</a>]

<h2>Clustering multivariate functional data using unsupervised binary trees. (arXiv:2012.05973v1 [stat.ML])</h2>
<h3>Golovkine Steven, Klutchnikoff Nicolas, Patilea Valentin</h3>
<p>We propose a model-based clustering algorithm for a general class of
functional data for which the components could be curves or images. The random
functional data realizations could be measured with error at discrete, and
possibly random, points in the definition domain. The idea is to build a set of
binary trees by recursive splitting of the observations. The number of groups
are determined in a data-driven way. The new algorithm provides easily
interpretable results and fast predictions for online data sets. Results on
simulated datasets reveal good performance in various complex settings. The
methodology is applied to the analysis of vehicle trajectories on a German
roundabout.
</p>
<a href="http://arxiv.org/abs/2012.05973" target="_blank">arXiv:2012.05973</a> [<a href="http://arxiv.org/pdf/2012.05973" target="_blank">pdf</a>]

<h2>Image-Graph-Image Translation via Auto-Encoding. (arXiv:2012.05975v1 [cs.CV])</h2>
<h3>Chenyang Lu, Gijs Dubbelman</h3>
<p>This work presents the first convolutional neural network that learns an
image-to-graph translation task without needing external supervision. Obtaining
graph representations of image content, where objects are represented as nodes
and their relationships as edges, is an important task in scene understanding.
Current approaches follow a fully-supervised approach thereby requiring
meticulous annotations. To overcome this, we are the first to present a
self-supervised approach based on a fully-differentiable auto-encoder in which
the bottleneck encodes the graph's nodes and edges. This self-supervised
approach can currently encode simple line drawings into graphs and obtains
comparable results to a fully-supervised baseline in terms of F1 score on
triplet matching. Besides these promising results, we provide several
directions for future research on how our approach can be extended to cover
more complex imagery.
</p>
<a href="http://arxiv.org/abs/2012.05975" target="_blank">arXiv:2012.05975</a> [<a href="http://arxiv.org/pdf/2012.05975" target="_blank">pdf</a>]

<h2>CommPOOL: An Interpretable Graph Pooling Framework for Hierarchical Graph Representation Learning. (arXiv:2012.05980v1 [cs.LG])</h2>
<h3>Haoteng Tang, Guixiang Ma, Lifang He, Heng Huang, Liang Zhan</h3>
<p>Recent years have witnessed the emergence and flourishing of hierarchical
graph pooling neural networks (HGPNNs) which are effective graph representation
learning approaches for graph level tasks such as graph classification.
However, current HGPNNs do not take full advantage of the graph's intrinsic
structures (e.g., community structure). Moreover, the pooling operations in
existing HGPNNs are difficult to be interpreted. In this paper, we propose a
new interpretable graph pooling framework - CommPOOL, that can capture and
preserve the hierarchical community structure of graphs in the graph
representation learning process. Specifically, the proposed community pooling
mechanism in CommPOOL utilizes an unsupervised approach for capturing the
inherent community structure of graphs in an interpretable manner. CommPOOL is
a general and flexible framework for hierarchical graph representation learning
that can further facilitate various graph-level tasks. Evaluations on five
public benchmark datasets and one synthetic dataset demonstrate the superior
performance of CommPOOL in graph representation learning for graph
classification compared to the state-of-the-art baseline methods, and its
effectiveness in capturing and preserving the community structure of graphs.
</p>
<a href="http://arxiv.org/abs/2012.05980" target="_blank">arXiv:2012.05980</a> [<a href="http://arxiv.org/pdf/2012.05980" target="_blank">pdf</a>]

<h2>Certifying Incremental Quadratic Constraints for Neural Networks. (arXiv:2012.05981v1 [cs.LG])</h2>
<h3>Navid Hashemi, Justin Ruths, Mahyar Fazlyab</h3>
<p>Abstracting neural networks with constraints they impose on their inputs and
outputs can be very useful in the analysis of neural network classifiers and to
derive optimization-based algorithms for certification of stability and
robustness of feedback systems involving neural networks. In this paper, we
propose a convex program, in the form of a Linear Matrix Inequality (LMI), to
certify incremental quadratic constraints on the map of neural networks over a
region of interest. These certificates can capture several useful properties
such as (local) Lipschitz continuity, one-sided Lipschitz continuity,
invertibility, and contraction. We illustrate the utility of our approach in
two different settings. First, we develop a semidefinite program to compute
guaranteed and sharp upper bounds on the local Lipschitz constant of neural
networks and illustrate the results on random networks as well as networks
trained on MNIST. Second, we consider a linear time-invariant system in
feedback with an approximate model predictive controller parameterized by a
neural network. We then turn the stability analysis into a semidefinite
feasibility program and estimate an ellipsoidal invariant set for the
closed-loop system.
</p>
<a href="http://arxiv.org/abs/2012.05981" target="_blank">arXiv:2012.05981</a> [<a href="http://arxiv.org/pdf/2012.05981" target="_blank">pdf</a>]

<h2>A Generative Approach for Detection-driven Underwater Image Enhancement. (arXiv:2012.05990v1 [cs.CV])</h2>
<h3>Chelsey Edge, Md Jahidul Islam, Christopher Morse, Junaed Sattar</h3>
<p>In this paper, we introduce a generative model for image enhancement
specifically for improving diver detection in the underwater domain. In
particular, we present a model that integrates generative adversarial network
(GAN)-based image enhancement with the diver detection task. Our proposed
approach restructures the GAN objective function to include information from a
pre-trained diver detector with the goal to generate images which would enhance
the accuracy of the detector in adverse visual conditions. By incorporating the
detector output into both the generator and discriminator networks, our model
is able to focus on enhancing images beyond aesthetic qualities and
specifically to improve robotic detection of scuba divers. We train our network
on a large dataset of scuba divers, using a state-of-the-art diver detector,
and demonstrate its utility on images collected from oceanic explorations of
human-robot teams. Experimental evaluations demonstrate that our approach
significantly improves diver detection performance over raw, unenhanced images,
and even outperforms detection performance on the output of state-of-the-art
underwater image enhancement algorithms. Finally, we demonstrate the inference
performance of our network on embedded devices to highlight the feasibility of
operating on board mobile robotic platforms.
</p>
<a href="http://arxiv.org/abs/2012.05990" target="_blank">arXiv:2012.05990</a> [<a href="http://arxiv.org/pdf/2012.05990" target="_blank">pdf</a>]

<h2>Strong Admissibility for Abstract Dialectical Frameworks. (arXiv:2012.05997v1 [cs.AI])</h2>
<h3>Atefeh Keshavarzi Zafarghandi, Rineke Verbrugge, Bart Verheij</h3>
<p>Abstract dialectical frameworks (ADFs) have been introduced as a formalism
for modeling and evaluating argumentation allowing general logical satisfaction
conditions. Different criteria used to settle the acceptance of arguments are
called semantics. Semantics of ADFs have so far mainly been defined based on
the concept of admissibility. However, the notion of strongly admissible
semantics studied for abstract argumentation frameworks has not yet been
introduced for ADFs. In the current work we present the concept of strong
admissibility of interpretations for ADFs. Further, we show that strongly
admissible interpretations of ADFs form a lattice with the grounded
interpretation as top element.
</p>
<a href="http://arxiv.org/abs/2012.05997" target="_blank">arXiv:2012.05997</a> [<a href="http://arxiv.org/pdf/2012.05997" target="_blank">pdf</a>]

<h2>An IoT Framework for Heart Disease Prediction based on MDCNN Classifier. (arXiv:2012.05999v1 [cs.LG])</h2>
<h3>Mohammad Ayoub Khan</h3>
<p>Nowadays, heart disease is the leading cause of death worldwide. Predicting
heart disease is a complex task since it requires experience along with
advanced knowledge. Internet of Things (IoT) technology has lately been adopted
in healthcare systems to collect sensor values for heart disease diagnosis and
prediction. Many researchers have focused on the diagnosis of heart disease,
yet the accuracy of the diagnosis results is low. To address this issue, an IoT
framework is proposed to evaluate heart disease more accurately using a
Modified Deep Convolutional Neural Network (MDCNN). The smartwatch and heart
monitor device that is attached to the patient monitors the blood pressure and
electrocardiogram (ECG). The MDCNN is utilized for classifying the received
sensor data into normal and abnormal. The performance of the system is analyzed
by comparing the proposed MDCNN with existing deep learning neural networks and
logistic regression. The results demonstrate that the proposed MDCNN based
heart disease prediction system performs better than other methods. The
proposed method shows that for the maximum number of records, the MDCNN
achieves an accuracy of 98.2 which is better than existing classifiers.
</p>
<a href="http://arxiv.org/abs/2012.05999" target="_blank">arXiv:2012.05999</a> [<a href="http://arxiv.org/pdf/2012.05999" target="_blank">pdf</a>]

<h2>The Three Ghosts of Medical AI: Can the Black-Box Present Deliver?. (arXiv:2012.06000v1 [cs.AI])</h2>
<h3>Thomas P. Quinn, Stephan Jacobs, Manisha Senadeera, Vuong Le, Simon Coghlan</h3>
<p>Our title alludes to the three Christmas ghosts encountered by Ebenezer
Scrooge in \textit{A Christmas Carol}, who guide Ebenezer through the past,
present, and future of Christmas holiday events. Similarly, our article will
take readers through a journey of the past, present, and future of medical AI.
In doing so, we focus on the crux of modern machine learning: the reliance on
powerful but intrinsically opaque models. When applied to the healthcare
domain, these models fail to meet the needs for transparency that their
clinician and patient end-users require. We review the implications of this
failure, and argue that opaque models (1) lack quality assurance, (2) fail to
elicit trust, and (3) restrict physician-patient dialogue. We then discuss how
upholding transparency in all aspects of model design and model validation can
help ensure the reliability of medical AI.
</p>
<a href="http://arxiv.org/abs/2012.06000" target="_blank">arXiv:2012.06000</a> [<a href="http://arxiv.org/pdf/2012.06000" target="_blank">pdf</a>]

<h2>Overcoming Catastrophic Forgetting in Graph Neural Networks. (arXiv:2012.06002v1 [cs.LG])</h2>
<h3>Huihui Liu, Yiding Yang, Xinchao Wang</h3>
<p>Catastrophic forgetting refers to the tendency that a neural network
"forgets" the previous learned knowledge upon learning new tasks. Prior methods
have been focused on overcoming this problem on convolutional neural networks
(CNNs), where the input samples like images lie in a grid domain, but have
largely overlooked graph neural networks (GNNs) that handle non-grid data. In
this paper, we propose a novel scheme dedicated to overcoming catastrophic
forgetting problem and hence strengthen continual learning in GNNs. At the
heart of our approach is a generic module, termed as topology-aware weight
preserving~(TWP), applicable to arbitrary form of GNNs in a plug-and-play
fashion. Unlike the main stream of CNN-based continual learning methods that
rely on solely slowing down the updates of parameters important to the
downstream task, TWP explicitly explores the local structures of the input
graph, and attempts to stabilize the parameters playing pivotal roles in the
topological aggregation. We evaluate TWP on different GNN backbones over
several datasets, and demonstrate that it yields performances superior to the
state of the art. Code is publicly available at
\url{https://github.com/hhliu79/TWP}.
</p>
<a href="http://arxiv.org/abs/2012.06002" target="_blank">arXiv:2012.06002</a> [<a href="http://arxiv.org/pdf/2012.06002" target="_blank">pdf</a>]

<h2>Learning to Resolve Conflicts for Multi-Agent Path Finding with Conflict-Based Search. (arXiv:2012.06005v1 [cs.AI])</h2>
<h3>Taoan Huang, Bistra Dilkina, Sven Koenig</h3>
<p>Conflict-Based Search (CBS) is a state-of-the-art algorithm for multi-agent
path finding. At the high level, CBS repeatedly detects conflicts and resolves
one of them by splitting the current problem into two subproblems. Previous
work chooses the conflict to resolve by categorizing the conflict into three
classes and always picking a conflict from the highest-priority class. In this
work, we propose an oracle for conflict selection that results in smaller
search tree sizes than the one used in previous work. However, the computation
of the oracle is slow. Thus, we propose a machine-learning framework for
conflict selection that observes the decisions made by the oracle and learns a
conflict-selection strategy represented by a linear ranking function that
imitates the oracle's decisions accurately and quickly. Experiments on
benchmark maps indicate that our method significantly improves the success
rates, the search tree sizes and runtimes over the current state-of-the-art CBS
solver.
</p>
<a href="http://arxiv.org/abs/2012.06005" target="_blank">arXiv:2012.06005</a> [<a href="http://arxiv.org/pdf/2012.06005" target="_blank">pdf</a>]

<h2>xRAI: Explainable Representations through AI. (arXiv:2012.06006v1 [cs.AI])</h2>
<h3>Christiann Bartelt, Sascha Marton, Heiner Stuckenschmidt</h3>
<p>We present xRAI an approach for extracting symbolic representations of the
mathematical function a neural network was supposed to learn from the trained
network. The approach is based on the idea of training a so-called
interpretation network that receives the weights and biases of the trained
network as input and outputs the numerical representation of the function the
network was supposed to learn that can be directly translated into a symbolic
representation. We show that interpretation nets for different classes of
functions can be trained on synthetic data offline using Boolean functions and
low-order polynomials as examples. We show that the training is rather
efficient and the quality of the results are promising. Our work aims to
provide a contribution to the problem of better understanding neural decision
making by making the target function explicit
</p>
<a href="http://arxiv.org/abs/2012.06006" target="_blank">arXiv:2012.06006</a> [<a href="http://arxiv.org/pdf/2012.06006" target="_blank">pdf</a>]

<h2>Price Suggestion for Online Second-hand Items with Texts and Images. (arXiv:2012.06008v1 [cs.AI])</h2>
<h3>Liang Han, Zhaozheng Yin, Zhurong Xia, Mingqian Tang, Rong Jin</h3>
<p>This paper presents an intelligent price suggestion system for online
second-hand listings based on their uploaded images and text descriptions. The
goal of price prediction is to help sellers set effective and reasonable prices
for their second-hand items with the images and text descriptions uploaded to
the online platforms. Specifically, we design a multi-modal price suggestion
system which takes as input the extracted visual and textual features along
with some statistical item features collected from the second-hand item
shopping platform to determine whether the image and text of an uploaded
second-hand item are qualified for reasonable price suggestion with a binary
classification model, and provide price suggestions for second-hand items with
qualified images and text descriptions with a regression model. To satisfy
different demands, two different constraints are added into the joint training
of the classification model and the regression model. Moreover, a customized
loss function is designed for optimizing the regression model to provide price
suggestions for second-hand items, which can not only maximize the gain of the
sellers but also facilitate the online transaction. We also derive a set of
metrics to better evaluate the proposed price suggestion system. Extensive
experiments on a large real-world dataset demonstrate the effectiveness of the
proposed multi-modal price suggestion system.
</p>
<a href="http://arxiv.org/abs/2012.06008" target="_blank">arXiv:2012.06008</a> [<a href="http://arxiv.org/pdf/2012.06008" target="_blank">pdf</a>]

<h2>Vision-based Price Suggestion for Online Second-hand Items. (arXiv:2012.06009v1 [cs.CV])</h2>
<h3>Liang Han, Zhaozheng Yin, Zhurong Xia, Li Guo, Mingqian Tang, Rong Jin</h3>
<p>Different from shopping in physical stores, where people have the opportunity
to closely check a product (e.g., touching the surface of a T-shirt or smelling
the scent of perfume) before making a purchase decision, online shoppers rely
greatly on the uploaded product images to make any purchase decision. The
decision-making is challenging when selling or purchasing second-hand items
online since estimating the items' prices is not trivial. In this work, we
present a vision-based price suggestion system for the online second-hand item
shopping platform. The goal of vision-based price suggestion is to help sellers
set effective prices for their second-hand listings with the images uploaded to
the online platforms.

First, we propose to better extract representative visual features from the
images with the aid of some other image-based item information (e.g., category,
brand). Then, we design a vision-based price suggestion module which takes the
extracted visual features along with some statistical item features from the
shopping platform as the inputs to determine whether an uploaded item image is
qualified for price suggestion by a binary classification model, and provide
price suggestions for items with qualified images by a regression model.
According to two demands from the platform, two different objective functions
are proposed to jointly optimize the classification model and the regression
model. For better model training, we also propose a warm-up training strategy
for the joint optimization. Extensive experiments on a large real-world dataset
demonstrate the effectiveness of our vision-based price prediction system.
</p>
<a href="http://arxiv.org/abs/2012.06009" target="_blank">arXiv:2012.06009</a> [<a href="http://arxiv.org/pdf/2012.06009" target="_blank">pdf</a>]

<h2>Performance-Weighed Policy Sampling for Meta-Reinforcement Learning. (arXiv:2012.06016v1 [cs.LG])</h2>
<h3>Ibrahim Ahmed, Marcos Quinones-Grueiro, Gautam Biswas</h3>
<p>This paper discusses an Enhanced Model-Agnostic Meta-Learning (E-MAML)
algorithm that generates fast convergence of the policy function from a small
number of training examples when applied to new learning tasks. Built on top of
Model-Agnostic Meta-Learning (MAML), E-MAML maintains a set of policy
parameters learned in the environment for previous tasks. We apply E-MAML to
developing reinforcement learning (RL)-based online fault tolerant control
schemes for dynamic systems. The enhancement is applied when a new fault
occurs, to re-initialize the parameters of a new RL policy that achieves faster
adaption with a small number of samples of system behavior with the new fault.
This replaces the random task sampling step in MAML. Instead, it exploits the
extant previously generated experiences of the controller. The enhancement is
sampled to maximally span the parameter space to facilitate adaption to the new
fault. We demonstrate the performance of our approach combining E-MAML with
proximal policy optimization (PPO) on the well-known cart pole example, and
then on the fuel transfer system of an aircraft.
</p>
<a href="http://arxiv.org/abs/2012.06016" target="_blank">arXiv:2012.06016</a> [<a href="http://arxiv.org/pdf/2012.06016" target="_blank">pdf</a>]

<h2>A MAC-less Neural Inference Processor Supporting Compressed, Variable Precision Weights. (arXiv:2012.06018v1 [cs.CV])</h2>
<h3>Vincenzo Liguori</h3>
<p>This paper introduces two architectures for the inference of convolutional
neural networks (CNNs). Both architectures exploit weight sparsity and
compression to reduce computational complexity and bandwidth. The first
architecture uses multiply-accumulators (MACs) but avoids unnecessary
multiplications by skipping zero weights. The second architecture exploits
weight sparsity at the level of their bit representation by substituting
resource-intensive MACs with much smaller Bit Layer Multiply Accumulators
(BLMACs). The use of BLMACs also allows variable precision weights as variable
size integers and even floating points. Some details of an implementation of
the second architecture are given. Weight compression with arithmetic coding is
also discussed as well as bandwidth implications. Finally, some implementation
results for a pathfinder design and various technologies are presented.
</p>
<a href="http://arxiv.org/abs/2012.06018" target="_blank">arXiv:2012.06018</a> [<a href="http://arxiv.org/pdf/2012.06018" target="_blank">pdf</a>]

<h2>Uncertainty-Aware Deep Calibrated Salient Object Detection. (arXiv:2012.06020v1 [cs.CV])</h2>
<h3>Jing Zhang, Yuchao Dai, Xin Yu, Mehrtash Harandi, Nick Barnes, Richard Hartley</h3>
<p>Existing deep neural network based salient object detection (SOD) methods
mainly focus on pursuing high network accuracy. However, those methods overlook
the gap between network accuracy and prediction confidence, known as the
confidence uncalibration problem. Thus, state-of-the-art SOD networks are prone
to be overconfident. In other words, the predicted confidence of the networks
does not reflect the real probability of correctness of salient object
detection, which significantly hinder their real-world applicability. In this
paper, we introduce an uncertaintyaware deep SOD network, and propose two
strategies from different perspectives to prevent deep SOD networks from being
overconfident. The first strategy, namely Boundary Distribution Smoothing
(BDS), generates continuous labels by smoothing the original binary
ground-truth with respect to pixel-wise uncertainty. The second strategy,
namely Uncertainty-Aware Temperature Scaling (UATS), exploits a relaxed Sigmoid
function during both training and testing with spatially-variant temperature
scaling to produce softened output. Both strategies can be incorporated into
existing deep SOD networks with minimal efforts. Moreover, we propose a new
saliency evaluation metric, namely dense calibration measure C, to measure how
the model is calibrated on a given dataset. Extensive experimental results on
seven benchmark datasets demonstrate that our solutions can not only better
calibrate SOD models, but also improve the network accuracy.
</p>
<a href="http://arxiv.org/abs/2012.06020" target="_blank">arXiv:2012.06020</a> [<a href="http://arxiv.org/pdf/2012.06020" target="_blank">pdf</a>]

<h2>Motion and Force Planning for Manipulating Heavy Objects by Pivoting. (arXiv:2012.06022v1 [cs.RO])</h2>
<h3>Amin Fakhari, Aditya Patankar, Nilanjan Chakraborty</h3>
<p>Manipulation of objects by exploiting their contact with the environment can
enhance both the dexterity and payload capability of robotic manipulators. A
common way to manipulate heavy objects beyond the payload capability of a robot
is to use a sequence of pivoting motions, wherein, an object is moved while
some contact points between the object and a support surface are kept fixed.
The goal of this paper is to develop an algorithmic approach for automated plan
generation for object manipulation with a sequence of pivoting motions. A plan
for manipulating a heavy object consists of a sequence of joint angles of the
manipulator, the corresponding object poses, as well as the joint torques
required to move the object. The constraint of maintaining object contact with
the ground during manipulation results in nonlinear constraints in the
configuration space of the robot, which is challenging for motion planning
algorithms. Exploiting the fact that pivoting motion corresponds to movements
in a subgroup of the group of rigid body motions, SE(3), we present a novel
task-space based planning approach for computing a motion plan for both the
manipulator and the object while satisfying contact constraints. We also
combine our motion planning algorithm with a grasping force synthesis algorithm
to ensure that friction constraints at the contacts and actuator torque
constraints are satisfied. We present simulation results with a dual-armed
Baxter robot to demonstrate our approach.
</p>
<a href="http://arxiv.org/abs/2012.06022" target="_blank">arXiv:2012.06022</a> [<a href="http://arxiv.org/pdf/2012.06022" target="_blank">pdf</a>]

<h2>Cost-to-Go Function Generating Networks for High Dimensional Motion Planning. (arXiv:2012.06023v1 [cs.RO])</h2>
<h3>Jinwook Huh, Volkan Isler, Daniel D. Lee</h3>
<p>This paper presents c2g-HOF networks which learn to generate cost-to-go
functions for manipulator motion planning. The c2g-HOF architecture consists of
a cost-to-go function over the configuration space represented as a neural
network (c2g-network) as well as a Higher Order Function (HOF) network which
outputs the weights of the c2g-network for a given input workspace. Both
networks are trained end-to-end in a supervised fashion using costs computed
from traditional motion planners. Once trained, c2g-HOF can generate a smooth
and continuous cost-to-go function directly from workspace sensor inputs
(represented as a point cloud in 3D or an image in 2D). At inference time, the
weights of the c2g-network are computed very efficiently and near-optimal
trajectories are generated by simply following the gradient of the cost-to-go
function. We compare c2g-HOF with traditional planning algorithms for various
robots and planning scenarios. The experimental results indicate that planning
with c2g-HOF is significantly faster than other motion planning algorithms,
resulting in orders of magnitude improvement when including collision checking.
Furthermore, despite being trained from sparsely sampled trajectories in
configuration space, c2g-HOF generalizes to generate smoother, and often lower
cost, trajectories. We demonstrate cost-to-go based planning on a 7 DoF
manipulator arm where motion planning in a complex workspace requires only 0.13
seconds for the entire trajectory.
</p>
<a href="http://arxiv.org/abs/2012.06023" target="_blank">arXiv:2012.06023</a> [<a href="http://arxiv.org/pdf/2012.06023" target="_blank">pdf</a>]

<h2>Robustness and Transferability of Universal Attacks on Compressed Models. (arXiv:2012.06024v1 [cs.LG])</h2>
<h3>Alberto G. Matachana, Kenneth T. Co, Luis Mu&#xf1;oz-Gonz&#xe1;lez, David Martinez, Emil C. Lupu</h3>
<p>Neural network compression methods like pruning and quantization are very
effective at efficiently deploying Deep Neural Networks (DNNs) on edge devices.
However, DNNs remain vulnerable to adversarial examples-inconspicuous inputs
that are specifically designed to fool these models. In particular, Universal
Adversarial Perturbations (UAPs), are a powerful class of adversarial attacks
which create adversarial perturbations that can generalize across a large set
of inputs. In this work, we analyze the effect of various compression
techniques to UAP attacks, including different forms of pruning and
quantization. We test the robustness of compressed models to white-box and
transfer attacks, comparing them with their uncompressed counterparts on
CIFAR-10 and SVHN datasets. Our evaluations reveal clear differences between
pruning methods, including Soft Filter and Post-training Pruning. We observe
that UAP transfer attacks between pruned and full models are limited,
suggesting that the systemic vulnerabilities across these models are different.
This finding has practical implications as using different compression
techniques can blunt the effectiveness of black-box transfer attacks. We show
that, in some scenarios, quantization can produce gradient-masking, giving a
false sense of security. Finally, our results suggest that conclusions about
the robustness of compressed models to UAP attacks is application dependent,
observing different phenomena in the two datasets used in our experiments.
</p>
<a href="http://arxiv.org/abs/2012.06024" target="_blank">arXiv:2012.06024</a> [<a href="http://arxiv.org/pdf/2012.06024" target="_blank">pdf</a>]

<h2>Reinforcement Learning Agents for Ubisoft's Roller Champions. (arXiv:2012.06031v1 [cs.LG])</h2>
<h3>Nancy Iskander, Aurelien Simoni, Eloi Alonso, Maxim Peter</h3>
<p>In recent years, Reinforcement Learning (RL) has seen increasing popularity
in research and popular culture. However, skepticism still surrounds the
practicality of RL in modern video game development. In this paper, we
demonstrate by example that RL can be a great tool for Artificial Intelligence
(AI) design in modern, non-trivial video games. We present our RL system for
Ubisoft's Roller Champions, a 3v3 Competitive Multiplayer Sports Game played on
an oval-shaped skating arena. Our system is designed to keep up with agile,
fast-paced development, taking 1--4 days to train a new model following
gameplay changes. The AIs are adapted for various game modes, including a 2v2
mode, a Training with Bots mode, in addition to the Classic game mode where
they replace players who have disconnected. We observe that the AIs develop
sophisticated co-ordinated strategies, and can aid in balancing the game as an
added bonus. Please see the accompanying video at https://vimeo.com/466780171
(password: rollerRWRL2020) for examples.
</p>
<a href="http://arxiv.org/abs/2012.06031" target="_blank">arXiv:2012.06031</a> [<a href="http://arxiv.org/pdf/2012.06031" target="_blank">pdf</a>]

<h2>Data-based Discovery of Governing Equations. (arXiv:2012.06036v1 [cs.LG])</h2>
<h3>Waad Subber, Piyush Pandita, Sayan Ghosh, Steven Atkinson, Genghis Khan, Liping Wang, Roger Ghanem</h3>
<p>Most common mechanistic models are traditionally presented in mathematical
forms to explain a given physical phenomenon. Machine learning algorithms, on
the other hand, provide a mechanism to map the input data to output without
explicitly describing the underlying physical process that generated the data.
We propose a Data-based Physics Discovery (DPD) framework for automatic
discovery of governing equations from observed data. Without a prior definition
of the model structure, first a free-form of the equation is discovered, and
then calibrated and validated against the available data. In addition to the
observed data, the DPD framework can utilize available prior physical models,
and domain expert feedback. When prior models are available, the DPD framework
can discover an additive or multiplicative correction term represented
symbolically. The correction term can be a function of the existing input
variable to the prior model, or a newly introduced variable. In case a prior
model is not available, the DPD framework discovers a new data-based standalone
model governing the observations. We demonstrate the performance of the
proposed framework on a real-world application in the aerospace industry.
</p>
<a href="http://arxiv.org/abs/2012.06036" target="_blank">arXiv:2012.06036</a> [<a href="http://arxiv.org/pdf/2012.06036" target="_blank">pdf</a>]

<h2>Provable Defense against Privacy Leakage in Federated Learning from Representation Perspective. (arXiv:2012.06043v1 [cs.LG])</h2>
<h3>Jingwei Sun, Ang Li, Binghui Wang, Huanrui Yang, Hai Li, Yiran Chen</h3>
<p>Federated learning (FL) is a popular distributed learning framework that can
reduce privacy risks by not explicitly sharing private data. However, recent
works demonstrated that sharing model updates makes FL vulnerable to inference
attacks. In this work, we show our key observation that the data representation
leakage from gradients is the essential cause of privacy leakage in FL. We also
provide an analysis of this observation to explain how the data presentation is
leaked. Based on this observation, we propose a defense against model inversion
attack in FL. The key idea of our defense is learning to perturb data
representation such that the quality of the reconstructed data is severely
degraded, while FL performance is maintained. In addition, we derive certified
robustness guarantee to FL and convergence guarantee to FedAvg, after applying
our defense. To evaluate our defense, we conduct experiments on MNIST and
CIFAR10 for defending against the DLG attack and GS attack. Without sacrificing
accuracy, the results demonstrate that our proposed defense can increase the
mean squared error between the reconstructed data and the raw data by as much
as more than 160X for both DLG attack and GS attack, compared with baseline
defense methods. The privacy of the FL system is significantly improved.
</p>
<a href="http://arxiv.org/abs/2012.06043" target="_blank">arXiv:2012.06043</a> [<a href="http://arxiv.org/pdf/2012.06043" target="_blank">pdf</a>]

<h2>Mesoscopic photogrammetry with an unstabilized phone camera. (arXiv:2012.06044v1 [cs.CV])</h2>
<h3>Kevin C. Zhou, Colin Cooke, Jaehee Park, Ruobing Qian, Roarke Horstmeyer, Joseph A. Izatt, Sina Farsiu</h3>
<p>We present a feature-free photogrammetric technique that enables quantitative
3D mesoscopic (mm-scale height variation) imaging with tens-of-micron accuracy
from sequences of images acquired by a smartphone at close range (several cm)
under freehand motion without additional hardware. Our end-to-end,
pixel-intensity-based approach jointly registers and stitches all the images by
estimating a coaligned height map, which acts as a pixel-wise radial
deformation field that orthorectifies each camera image to allow homographic
registration. The height maps themselves are reparameterized as the output of
an untrained encoder-decoder convolutional neural network (CNN) with the raw
camera images as the input, which effectively removes many reconstruction
artifacts. Our method also jointly estimates both the camera's dynamic 6D pose
and its distortion using a nonparametric model, the latter of which is
especially important in mesoscopic applications when using cameras not designed
for imaging at short working distances, such as smartphone cameras. We also
propose strategies for reducing computation time and memory, applicable to
other multi-frame registration problems. Finally, we demonstrate our method
using sequences of multi-megapixel images captured by an unstabilized
smartphone on a variety of samples (e.g., painting brushstrokes, circuit board,
seeds).
</p>
<a href="http://arxiv.org/abs/2012.06044" target="_blank">arXiv:2012.06044</a> [<a href="http://arxiv.org/pdf/2012.06044" target="_blank">pdf</a>]

<h2>Interactive Weak Supervision: Learning Useful Heuristics for Data Labeling. (arXiv:2012.06046v1 [cs.LG])</h2>
<h3>Benedikt Boecking, Willie Neiswanger, Eric Xing, Artur Dubrawski</h3>
<p>Obtaining large annotated datasets is critical for training successful
machine learning models and it is often a bottleneck in practice. Weak
supervision offers a promising alternative for producing labeled datasets
without ground truth annotations by generating probabilistic labels using
multiple noisy heuristics. This process can scale to large datasets and has
demonstrated state of the art performance in diverse domains such as healthcare
and e-commerce. One practical issue with learning from user-generated
heuristics is that their creation requires creativity, foresight, and domain
expertise from those who hand-craft them, a process which can be tedious and
subjective. We develop the first framework for interactive weak supervision in
which a method proposes heuristics and learns from user feedback given on each
proposed heuristic. Our experiments demonstrate that only a small number of
feedback iterations are needed to train models that achieve highly competitive
test set performance without access to ground truth training labels. We conduct
user studies, which show that users are able to effectively provide feedback on
heuristics and that test set results track the performance of simulated
oracles.
</p>
<a href="http://arxiv.org/abs/2012.06046" target="_blank">arXiv:2012.06046</a> [<a href="http://arxiv.org/pdf/2012.06046" target="_blank">pdf</a>]

<h2>KNN Classification with One-step Computation. (arXiv:2012.06047v1 [cs.LG])</h2>
<h3>Shichao Zhang, Jiaye Li</h3>
<p>KNN classification is a query triggered yet improvisational learning mode, in
which they are carried out only when a test data is predicted that set a
suitable K value and search the K nearest neighbors from the whole training
sample space, referred them to the lazy part of KNN classification. This lazy
part has been the bottleneck problem of applying KNN classification. In this
paper, a one-step computation is proposed to replace the lazy part of KNN
classification. The one-step computation actually transforms the lazy part to a
matrix computation as follows. Given a test data, training samples are first
applied to fit the test data with the least squares loss function. And then, a
relationship matrix is generated by weighting all training samples according to
their influence on the test data. Finally, a group lasso is employed to perform
sparse learning of the relationship matrix. In this way, setting K value and
searching K nearest neighbors are both integrated to a unified computation. In
addition, a new classification rule is proposed for improving the performance
of one-step KNN classification. The proposed approach is experimentally
evaluated, and demonstrated that the one-step KNN classification is efficient
and promising.
</p>
<a href="http://arxiv.org/abs/2012.06047" target="_blank">arXiv:2012.06047</a> [<a href="http://arxiv.org/pdf/2012.06047" target="_blank">pdf</a>]

<h2>Interactive Search Based on Deep Reinforcement Learning. (arXiv:2012.06052v1 [cs.LG])</h2>
<h3>Yang Yu, Zhenhao Gu, Rong Tao, Jingtian Ge, Kenglun Chang</h3>
<p>With the continuous development of machine learning technology, major
e-commerce platforms have launched recommendation systems based on it to serve
a large number of customers with different needs more efficiently. Compared
with traditional supervised learning, reinforcement learning can better capture
the user's state transition in the decision-making process, and consider a
series of user actions, not just the static characteristics of the user at a
certain moment. In theory, it will have a long-term perspective, producing a
more effective recommendation. The special requirements of reinforcement
learning for data make it need to rely on an offline virtual system for
training. Our project mainly establishes a virtual user environment for offline
training. At the same time, we tried to improve a reinforcement learning
algorithm based on bi-clustering to expand the action space and recommended
path space of the recommendation agent.
</p>
<a href="http://arxiv.org/abs/2012.06052" target="_blank">arXiv:2012.06052</a> [<a href="http://arxiv.org/pdf/2012.06052" target="_blank">pdf</a>]

<h2>Spatio-attentive Graphs for Human-Object Interaction Detection. (arXiv:2012.06060v1 [cs.CV])</h2>
<h3>Frederic Z. Zhang, Dylan Campbell, Stephen Gould</h3>
<p>We address the problem of detecting human--object interactions in images
using graphical neural networks. Our network constructs a bipartite graph of
nodes representing detected humans and objects, wherein messages passed between
the nodes encode relative spatial and appearance information. Unlike existing
approaches that separate appearance and spatial features, our method fuses
these two cues within a single graphical model allowing information conditioned
on both modalities to influence the prediction of interactions with neighboring
nodes. Through extensive experimentation we demonstrate the advantages of
fusing relative spatial information with appearance features in the computation
of adjacency structure, message passing and the ultimate refined graph
features. On the popular HICO-DET benchmark dataset, our model outperforms
state-of-the-art with an mAP of 27.18, a 10% relative improvement.
</p>
<a href="http://arxiv.org/abs/2012.06060" target="_blank">arXiv:2012.06060</a> [<a href="http://arxiv.org/pdf/2012.06060" target="_blank">pdf</a>]

<h2>Deep Learning Approach for Matrix Completion Using Manifold Learning. (arXiv:2012.06063v1 [cs.LG])</h2>
<h3>Saeid Mehrdad, Mohammad Hossein Kahaei</h3>
<p>Matrix completion has received vast amount of attention and research due to
its wide applications in various study fields. Existing methods of matrix
completion consider only nonlinear (or linear) relations among entries in a
data matrix and ignore linear (or nonlinear) relationships latent. This paper
introduces a new latent variables model for data matrix which is a combination
of linear and nonlinear models and designs a novel deep-neural-network-based
matrix completion algorithm to address both linear and nonlinear relations
among entries of data matrix. The proposed method consists of two branches. The
first branch learns the latent representations of columns and reconstructs the
columns of the partially observed matrix through a series of hidden neural
network layers. The second branch does the same for the rows. In addition,
based on multi-task learning principles, we enforce these two branches work
together and introduce a new regularization technique to reduce over-fitting.
More specifically, the missing entries of data are recovered as a main task and
manifold learning is performed as an auxiliary task. The auxiliary task
constrains the weights of the network so it can be considered as a regularizer,
improving the main task and reducing over-fitting. Experimental results
obtained on the synthetic data and several real-world data verify the
effectiveness of the proposed method compared with state-of-the-art matrix
completion methods.
</p>
<a href="http://arxiv.org/abs/2012.06063" target="_blank">arXiv:2012.06063</a> [<a href="http://arxiv.org/pdf/2012.06063" target="_blank">pdf</a>]

<h2>Adaptive Submodular Meta-Learning. (arXiv:2012.06070v1 [cs.LG])</h2>
<h3>Shaojie Tang, Jing Yuan</h3>
<p>Meta-Learning has gained increasing attention in the machine learning and
artificial intelligence communities. In this paper, we introduce and study an
adaptive submodular meta-learning problem. The input of our problem is a set of
items, where each item has a random state which is initially unknown. The only
way to observe an item's state is to select that item. Our objective is to
adaptively select a group of items that achieve the best performance over a set
of tasks, where each task is represented as an adaptive monotone and submodular
function that maps sets of items and their states to a real number. To reduce
the computational cost while maintaining a personalized solution for each
future task, we first select a initial solution set based on previously
observed tasks, then adaptively add the remaining items to the initial set when
a new task arrives. As compared to the solution where a brand new solution is
computed for each new task, our meta-learning based approach leads to lower
computational overhead at test time since the initial solution set is
pre-computed in the training stage. To solve this problem, we propose a
two-phase greedy policy and show that it achieves a $\frac{e-1}{2e-1}$
approximation ratio.
</p>
<a href="http://arxiv.org/abs/2012.06070" target="_blank">arXiv:2012.06070</a> [<a href="http://arxiv.org/pdf/2012.06070" target="_blank">pdf</a>]

<h2>Smooth Bandit Optimization: Generalization to H\"older Space. (arXiv:2012.06076v1 [cs.LG])</h2>
<h3>Yusha Liu, Yining Wang, Aarti Singh</h3>
<p>We consider bandit optimization of a smooth reward function, where the goal
is cumulative regret minimization. This problem has been studied for
$\alpha$-H\"older continuous (including Lipschitz) functions with $0&lt;\alpha\leq
1$. Our main result is in generalization of the reward function to H\"older
space with exponent $\alpha&gt;1$ to bridge the gap between Lipschitz bandits and
infinitely-differentiable models such as linear bandits. For H\"older
continuous functions, approaches based on random sampling in bins of a
discretized domain suffices as optimal. In contrast, we propose a class of
two-layer algorithms that deploy misspecified linear/polynomial bandit
algorithms in bins. We demonstrate that the proposed algorithm can exploit
higher-order smoothness of the function by deriving a regret upper bound of
$\tilde{O}(T^\frac{d+\alpha}{d+2\alpha})$ for when $\alpha&gt;1$, which matches
existing lower bound. We also study adaptation to unknown function smoothness
over a continuous scale of H\"older spaces indexed by $\alpha$, with a bandit
model selection approach applied with our proposed two-layer algorithms. We
show that it achieves regret rate that matches the existing lower bound for
adaptation within the $\alpha\leq 1$ subset.
</p>
<a href="http://arxiv.org/abs/2012.06076" target="_blank">arXiv:2012.06076</a> [<a href="http://arxiv.org/pdf/2012.06076" target="_blank">pdf</a>]

<h2>Deep Neural Networks Are Effective At Learning High-Dimensional Hilbert-Valued Functions From Limited Data. (arXiv:2012.06081v1 [cs.LG])</h2>
<h3>Ben Adcock, Simone Brugiapaglia, Nick Dexter, Sebastian Moraga</h3>
<p>The accurate approximation of scalar-valued functions from sample points is a
key task in mathematical modeling and computational science. Recently, machine
learning techniques based on Deep Neural Networks (DNNs) have begun to emerge
as promising tools for function approximation in scientific computing problems,
with impressive results achieved on problems where the dimension of the
underlying data or problem domain is large. In this work, we broaden this
perspective by focusing on approximation of functions that are Hilbert-valued,
i.e. they take values in a separable, but typically infinite-dimensional,
Hilbert space. This problem arises in many science and engineering problems, in
particular those involving the solution of parametric Partial Differential
Equations (PDEs). Such problems are challenging for three reasons. First,
pointwise samples are expensive to acquire. Second, the domain of the function
is usually high dimensional, and third, the range lies in a Hilbert space. Our
contributions are twofold. First, we present a novel result on DNN training for
holomorphic functions with so-called hidden anisotropy. This result introduces
a DNN training procedure and a full theoretical analysis with explicit
guarantees on the error and sample complexity. This error bound is explicit in
the three key errors occurred in the approximation procedure: best
approximation error, measurement error and physical discretization error. Our
result shows that there is a procedure for learning Hilbert-valued functions
via DNNs that performs as well as current best-in-class schemes. Second, we
provide preliminary numerical results illustrating the practical performance of
DNNs on Hilbert-valued functions arising as solutions to parametric PDEs. We
consider different parameters, modify the DNN architecture to achieve better
and competitive results and compare these to current best-in-class schemes.
</p>
<a href="http://arxiv.org/abs/2012.06081" target="_blank">arXiv:2012.06081</a> [<a href="http://arxiv.org/pdf/2012.06081" target="_blank">pdf</a>]

<h2>Monocular Real-time Full Body Capture with Inter-part Correlations. (arXiv:2012.06087v1 [cs.CV])</h2>
<h3>Yuxiao Zhou, Marc Habermann, Ikhsanul Habibie, Ayush Tewari, Christian Theobalt, Feng Xu</h3>
<p>We present the first method for real-time full body capture that estimates
shape and motion of body and hands together with a dynamic 3D face model from a
single color image. Our approach uses a new neural network architecture that
exploits correlations between body and hands at high computational efficiency.
Unlike previous works, our approach is jointly trained on multiple datasets
focusing on hand, body or face separately, without requiring data where all the
parts are annotated at the same time, which is much more difficult to create at
sufficient variety. The possibility of such multi-dataset training enables
superior generalization ability. In contrast to earlier monocular full body
methods, our approach captures more expressive 3D face geometry and color by
estimating the shape, expression, albedo and illumination parameters of a
statistical face model. Our method achieves competitive accuracy on public
benchmarks, while being significantly faster and providing more complete face
reconstructions.
</p>
<a href="http://arxiv.org/abs/2012.06087" target="_blank">arXiv:2012.06087</a> [<a href="http://arxiv.org/pdf/2012.06087" target="_blank">pdf</a>]

<h2>Generative Learning With Euler Particle Transport. (arXiv:2012.06094v1 [cs.LG])</h2>
<h3>Yuan Gao, Jian Huang, Yuling Jiao, Jin Liu, Xiliang Lu, Zhijian Yang</h3>
<p>We propose an Euler particle transport (EPT) approach for generative
learning. The proposed approach is motivated by the problem of finding an
optimal transport map from a reference distribution to a target distribution
characterized by the Monge-Ampere equation. Interpreting the infinitesimal
linearization of the Monge-Ampere equation from the perspective of gradient
flows in measure spaces leads to a stochastic McKean-Vlasov equation. We use
the forward Euler method to solve this equation. The resulting forward Euler
map pushes forward a reference distribution to the target. This map is the
composition of a sequence of simple residual maps, which are computationally
stable and easy to train. The key task in training is the estimation of the
density ratios or differences that determine the residual maps. We estimate the
density ratios (differences) based on the Bregman divergence with a gradient
penalty using deep density-ratio (difference) fitting. We show that the
proposed density-ratio (difference) estimators do not suffer from the "curse of
dimensionality" if data is supported on a lower-dimensional manifold. Numerical
experiments with multi-mode synthetic datasets and comparisons with the
existing methods on real benchmark datasets support our theoretical results and
demonstrate the effectiveness of the proposed method.
</p>
<a href="http://arxiv.org/abs/2012.06094" target="_blank">arXiv:2012.06094</a> [<a href="http://arxiv.org/pdf/2012.06094" target="_blank">pdf</a>]

<h2>A Review of Hidden Markov Models and Recurrent Neural Networks for Event Detection and Localization in Biomedical Signals. (arXiv:2012.06104v1 [cs.LG])</h2>
<h3>Yassin Khalifa, Danilo Mandic, Ervin Sejdi&#x107;</h3>
<p>Biomedical signals carry signature rhythms of complex physiological processes
that control our daily bodily activity. The properties of these rhythms
indicate the nature of interaction dynamics among physiological processes that
maintain a homeostasis. Abnormalities associated with diseases or disorders
usually appear as disruptions in the structure of the rhythms which makes
isolating these rhythms and the ability to differentiate between them,
indispensable. Computer aided diagnosis systems are ubiquitous nowadays in
almost every medical facility and more closely in wearable technology, and
rhythm or event detection is the first of many intelligent steps that they
perform. How these rhythms are isolated? How to develop a model that can
describe the transition between processes in time? Many methods exist in the
literature that address these questions and perform the decoding of biomedical
signals into separate rhythms. In here, we demystify the most effective methods
that are used for detection and isolation of rhythms or events in time series
and highlight the way in which they were applied to different biomedical
signals and how they contribute to information fusion. The key strengths and
limitations of these methods are also discussed as well as the challenges
encountered with application in biomedical signals.
</p>
<a href="http://arxiv.org/abs/2012.06104" target="_blank">arXiv:2012.06104</a> [<a href="http://arxiv.org/pdf/2012.06104" target="_blank">pdf</a>]

<h2>A novel joint points and silhouette-based method to estimate 3D human pose and shape. (arXiv:2012.06109v1 [cs.CV])</h2>
<h3>Zhongguo Li, Anders Heyden, Magnus Oskarsson</h3>
<p>This paper presents a novel method for 3D human pose and shape estimation
from images with sparse views, using joint points and silhouettes, based on a
parametric model. Firstly, the parametric model is fitted to the joint points
estimated by deep learning-based human pose estimation. Then, we extract the
correspondence between the parametric model of pose fitting and silhouettes on
2D and 3D space. A novel energy function based on the correspondence is built
and minimized to fit parametric model to the silhouettes. Our approach uses
sufficient shape information because the energy function of silhouettes is
built from both 2D and 3D space. This also means that our method only needs
images from sparse views, which balances data used and the required prior
information. Results on synthetic data and real data demonstrate the
competitive performance of our approach on pose and shape estimation of the
human body.
</p>
<a href="http://arxiv.org/abs/2012.06109" target="_blank">arXiv:2012.06109</a> [<a href="http://arxiv.org/pdf/2012.06109" target="_blank">pdf</a>]

<h2>I-GCN: Robust Graph Convolutional Network via Influence Mechanism. (arXiv:2012.06110v1 [cs.LG])</h2>
<h3>Haoxi Zhan, Xiaobing Pei</h3>
<p>Deep learning models for graphs, especially Graph Convolutional Networks
(GCNs), have achieved remarkable performance in the task of semi-supervised
node classification. However, recent studies show that GCNs suffer from
adversarial perturbations. Such vulnerability to adversarial attacks
significantly decreases the stability of GCNs when being applied to
security-critical applications. Defense methods such as preprocessing,
attention mechanism and adversarial training have been discussed by various
studies. While being able to achieve desirable performance when the
perturbation rates are low, such methods are still vulnerable to high
perturbation rates. Meanwhile, some defending algorithms perform poorly when
the node features are not visible. Therefore, in this paper, we propose a novel
mechanism called influence mechanism, which is able to enhance the robustness
of the GCNs significantly. The influence mechanism divides the effect of each
node into two parts: introverted influence which tries to maintain its own
features and extroverted influence which exerts influences on other nodes.
Utilizing the influence mechanism, we propose the Influence GCN (I-GCN) model.
Extensive experiments show that our proposed model is able to achieve higher
accuracy rates than state-of-the-art methods when defending against
non-targeted attacks.
</p>
<a href="http://arxiv.org/abs/2012.06110" target="_blank">arXiv:2012.06110</a> [<a href="http://arxiv.org/pdf/2012.06110" target="_blank">pdf</a>]

<h2>Pair-view Unsupervised Graph Representation Learning. (arXiv:2012.06113v1 [cs.LG])</h2>
<h3>You Li, Binli Luo, Ning Gui</h3>
<p>Low-dimension graph embeddings have proved extremely useful in various
downstream tasks in large graphs, e.g., link-related content recommendation and
node classification tasks, etc. Most existing embedding approaches take nodes
as the basic unit for information aggregation, e.g., node perception fields in
GNN or con-textual nodes in random walks. The main drawback raised by such
node-view is its lack of support for expressing the compound relationships
between nodes, which results in the loss of a certain degree of graph
information during embedding. To this end, this paper pro-poses PairE(Pair
Embedding), a solution to use "pair", a higher level unit than a "node" as the
core for graph embeddings. Accordingly, a multi-self-supervised auto-encoder is
designed to fulfill two pretext tasks, to reconstruct the feature distribution
for respective pairs and their surrounding context. PairE has three major
advantages: 1) Informative, embedding beyond node-view are capable to preserve
richer information of the graph; 2) Simple, the solutions provided by PairE are
time-saving, storage-efficient, and require the fewer hyper-parameters; 3) High
adaptability, with the introduced translator operator to map pair embeddings to
the node embeddings, PairE can be effectively used in both the link-based and
the node-based graph analysis. Experiment results show that PairE consistently
outperforms the state of baselines in all four downstream tasks, especially
with significant edges in the link-prediction and multi-label node
classification tasks.
</p>
<a href="http://arxiv.org/abs/2012.06113" target="_blank">arXiv:2012.06113</a> [<a href="http://arxiv.org/pdf/2012.06113" target="_blank">pdf</a>]

<h2>How to Train PointGoal Navigation Agents on a (Sample and Compute) Budget. (arXiv:2012.06117v1 [cs.CV])</h2>
<h3>Erik Wijmans, Irfan Essa, Dhruv Batra</h3>
<p>PointGoal navigation has seen significant recent interest and progress,
spurred on by the Habitat platform and associated challenge. In this paper, we
study PointGoal navigation under both a sample budget (75 million frames) and a
compute budget (1 GPU for 1 day). We conduct an extensive set of experiments,
cumulatively totaling over 50,000 GPU-hours, that let us identify and discuss a
number of ostensibly minor but significant design choices -- the advantage
estimation procedure (a key component in training), visual encoder
architecture, and a seemingly minor hyper-parameter change. Overall, these
design choices to lead considerable and consistent improvements over the
baselines present in Savva et al. Under a sample budget, performance for RGB-D
agents improves 8 SPL on Gibson (14% relative improvement) and 20 SPL on
Matterport3D (38% relative improvement). Under a compute budget, performance
for RGB-D agents improves by 19 SPL on Gibson (32% relative improvement) and 35
SPL on Matterport3D (220% relative improvement). We hope our findings and
recommendations will make serve to make the community's experiments more
efficient.
</p>
<a href="http://arxiv.org/abs/2012.06117" target="_blank">arXiv:2012.06117</a> [<a href="http://arxiv.org/pdf/2012.06117" target="_blank">pdf</a>]

<h2>DSRNA: Differentiable Search of Robust Neural Architectures. (arXiv:2012.06122v1 [cs.LG])</h2>
<h3>Ramtin Hosseini, Xingyi Yang, Pengtao Xie</h3>
<p>In deep learning applications, the architectures of deep neural networks are
crucial in achieving high accuracy. Many methods have been proposed to search
for high-performance neural architectures automatically. However, these
searched architectures are prone to adversarial attacks. A small perturbation
of the input data can render the architecture to change prediction outcomes
significantly. To address this problem, we propose methods to perform
differentiable search of robust neural architectures. In our methods, two
differentiable metrics are defined to measure architectures' robustness, based
on certified lower bound and Jacobian norm bound. Then we search for robust
architectures by maximizing the robustness metrics. Different from previous
approaches which aim to improve architectures' robustness in an implicit way:
performing adversarial training and injecting random noise, our methods
explicitly and directly maximize robustness metrics to harvest robust
architectures. On CIFAR-10, ImageNet, and MNIST, we perform game-based
evaluation and verification-based evaluation on the robustness of our methods.
The experimental results show that our methods 1) are more robust to various
norm-bound attacks than several robust NAS baselines; 2) are more accurate than
baselines when there are no attacks; 3) have significantly higher certified
lower bounds than baselines.
</p>
<a href="http://arxiv.org/abs/2012.06122" target="_blank">arXiv:2012.06122</a> [<a href="http://arxiv.org/pdf/2012.06122" target="_blank">pdf</a>]

<h2>A Log-likelihood Regularized KL Divergence for Video Prediction with A 3D Convolutional Variational Recurrent Network. (arXiv:2012.06123v1 [cs.CV])</h2>
<h3>Haziq Razali, Basura Fernando</h3>
<p>The use of latent variable models has shown to be a powerful tool for
modeling probability distributions over sequences. In this paper, we introduce
a new variational model that extends the recurrent network in two ways for the
task of video frame prediction. First, we introduce 3D convolutions inside all
modules including the recurrent model for future frame prediction, inputting
and outputting a sequence of video frames at each timestep. This enables us to
better exploit spatiotemporal information inside the variational recurrent
model, allowing us to generate high-quality predictions. Second, we enhance the
latent loss of the variational model by introducing a maximum likelihood
estimate in addition to the KL divergence that is commonly used in variational
models. This simple extension acts as a stronger regularizer in the variational
autoencoder loss function and lets us obtain better results and
generalizability. Experiments show that our model outperforms existing video
prediction methods on several benchmarks while requiring fewer parameters.
</p>
<a href="http://arxiv.org/abs/2012.06123" target="_blank">arXiv:2012.06123</a> [<a href="http://arxiv.org/pdf/2012.06123" target="_blank">pdf</a>]

<h2>A Dark Flash Normal Camera. (arXiv:2012.06125v1 [cs.CV])</h2>
<h3>Zhihao Xia, Jason Lawrence, Supreeth Achar</h3>
<p>Casual photography is often performed in uncontrolled lighting that can
result in low quality images and degrade the performance of downstream
processing. We consider the problem of estimating surface normal and
reflectance maps of scenes depicting people despite these conditions by
supplementing the available visible illumination with a single near infrared
(NIR) light source and camera, a so-called "dark flash image". Our method takes
as input a single color image captured under arbitrary visible lighting and a
single dark flash image captured under controlled front-lit NIR lighting at the
same viewpoint, and computes a normal map, a diffuse albedo map, and a specular
intensity map of the scene. Since ground truth normal and reflectance maps of
faces are difficult to capture, we propose a novel training technique that
combines information from two readily available and complementary sources: a
stereo depth signal and photometric shading cues. We evaluate our method over a
range of subjects and lighting conditions and describe two applications:
optimizing stereo geometry and filling the shadows in an image.
</p>
<a href="http://arxiv.org/abs/2012.06125" target="_blank">arXiv:2012.06125</a> [<a href="http://arxiv.org/pdf/2012.06125" target="_blank">pdf</a>]

<h2>Learning Omni-frequency Region-adaptive Representations for Real Image Super-Resolution. (arXiv:2012.06131v1 [cs.CV])</h2>
<h3>Xin Li, Xin Jin, Tao Yu, Yingxue Pang, Simeng Sun, Zhizheng Zhang, Zhibo Chen</h3>
<p>Traditional single image super-resolution (SISR) methods that focus on
solving single and uniform degradation (i.e., bicubic down-sampling), typically
suffer from poor performance when applied into real-world low-resolution (LR)
images due to the complicated realistic degradations. The key to solving this
more challenging real image super-resolution (RealSR) problem lies in learning
feature representations that are both informative and content-aware. In this
paper, we propose an Omni-frequency Region-adaptive Network (ORNet) to address
both challenges, here we call features of all low, middle and high frequencies
omni-frequency features. Specifically, we start from the frequency perspective
and design a Frequency Decomposition (FD) module to separate different
frequency components to comprehensively compensate the information lost for
real LR image. Then, considering the different regions of real LR image have
different frequency information lost, we further design a Region-adaptive
Frequency Aggregation (RFA) module by leveraging dynamic convolution and
spatial attention to adaptively restore frequency components for different
regions. The extensive experiments endorse the effective, and scenario-agnostic
nature of our OR-Net for RealSR.
</p>
<a href="http://arxiv.org/abs/2012.06131" target="_blank">arXiv:2012.06131</a> [<a href="http://arxiv.org/pdf/2012.06131" target="_blank">pdf</a>]

<h2>Color-related Local Binary Pattern: A Learned Local Descriptor for Color Image Recognition. (arXiv:2012.06132v1 [cs.CV])</h2>
<h3>Bin Xiao, Tao Geng, Xiuli Bi, Weisheng Li</h3>
<p>Local binary pattern (LBP) as a kind of local feature has shown its
simplicity, easy implementation and strong discriminating power in image
recognition. Although some LBP variants are specifically investigated for color
image recognition, the color information of images is not adequately considered
and the curse of dimensionality in classification is easily caused in these
methods. In this paper, a color-related local binary pattern (cLBP) which
learns the dominant patterns from the decoded LBP is proposed for color images
recognition. This paper first proposes a relative similarity space (RSS) that
represents the color similarity between image channels for describing a color
image. Then, the decoded LBP which can mine the correlation information between
the LBP feature maps correspond to each color channel of RSS traditional RGB
spaces, is employed for feature extraction. Finally, a feature learning
strategy is employed to learn the dominant color-related patterns for reducing
the dimension of feature vector and further improving the discriminatively of
features. The theoretic analysis show that the proposed RSS can provide more
discriminative information, and has higher noise robustness as well as higher
illumination variation robustness than traditional RGB space. Experimental
results on four groups, totally twelve public color image datasets show that
the proposed method outperforms most of the LBP variants for color image
recognition in terms of dimension of features, recognition accuracy under
noise-free, noisy and illumination variation conditions.
</p>
<a href="http://arxiv.org/abs/2012.06132" target="_blank">arXiv:2012.06132</a> [<a href="http://arxiv.org/pdf/2012.06132" target="_blank">pdf</a>]

<h2>Intrinsic Temporal Regularization for High-resolution Human Video Synthesis. (arXiv:2012.06134v1 [cs.CV])</h2>
<h3>Lingbo Yang, Zhanning Gao, Peiran Ren, Siwei Ma, Wen Gao</h3>
<p>Temporal consistency is crucial for extending image processing pipelines to
the video domain, which is often enforced with flow-based warping error over
adjacent frames. Yet for human video synthesis, such scheme is less reliable
due to the misalignment between source and target video as well as the
difficulty in accurate flow estimation. In this paper, we propose an effective
intrinsic temporal regularization scheme to mitigate these issues, where an
intrinsic confidence map is estimated via the frame generator to regulate
motion estimation via temporal loss modulation. This creates a shortcut for
back-propagating temporal loss gradients directly to the front-end motion
estimator, thus improving training stability and temporal coherence in output
videos. We apply our intrinsic temporal regulation to single-image generator,
leading to a powerful "INTERnet" capable of generating $512\times512$
resolution human action videos with temporal-coherent, realistic visual
details. Extensive experiments demonstrate the superiority of proposed INTERnet
over several competitive baselines.
</p>
<a href="http://arxiv.org/abs/2012.06134" target="_blank">arXiv:2012.06134</a> [<a href="http://arxiv.org/pdf/2012.06134" target="_blank">pdf</a>]

<h2>Classifying Breast Histopathology Images with a Ductal Instance-Oriented Pipeline. (arXiv:2012.06136v1 [cs.CV])</h2>
<h3>Beibin Li, Ezgi Mercan, Sachin Mehta, Stevan Knezevich, Corey W. Arnold, Donald L. Weaver, Joann G. Elmore, Linda G. Shapiro</h3>
<p>In this study, we propose the Ductal Instance-Oriented Pipeline (DIOP) that
contains a duct-level instance segmentation model, a tissue-level semantic
segmentation model, and three-levels of features for diagnostic classification.
Based on recent advancements in instance segmentation and the Mask R-CNN model,
our duct-level segmenter tries to identify each ductal individual inside a
microscopic image; then, it extracts tissue-level information from the
identified ductal instances. Leveraging three levels of information obtained
from these ductal instances and also the histopathology image, the proposed
DIOP outperforms previous approaches (both feature-based and CNN-based) in all
diagnostic tasks; for the four-way classification task, the DIOP achieves
comparable performance to general pathologists in this unique dataset. The
proposed DIOP only takes a few seconds to run in the inference time, which
could be used interactively on most modern computers. More clinical
explorations are needed to study the robustness and generalizability of this
system in the future.
</p>
<a href="http://arxiv.org/abs/2012.06136" target="_blank">arXiv:2012.06136</a> [<a href="http://arxiv.org/pdf/2012.06136" target="_blank">pdf</a>]

<h2>AdvantageNAS: Efficient Neural Architecture Search with Credit Assignment. (arXiv:2012.06138v1 [cs.LG])</h2>
<h3>Rei Sato, Jun Sakuma, Youhei Akimoto</h3>
<p>Neural architecture search (NAS) is an approach for automatically designing a
neural network architecture without human effort or expert knowledge. However,
the high computational cost of NAS limits its use in commercial applications.
Two recent NAS paradigms, namely one-shot and sparse propagation, which reduce
the time and space complexities, respectively, provide clues for solving this
problem. In this paper, we propose a novel search strategy for one-shot and
sparse propagation NAS, namely AdvantageNAS, which further reduces the time
complexity of NAS by reducing the number of search iterations. AdvantageNAS is
a gradient-based approach that improves the search efficiency by introducing
credit assignment in gradient estimation for architecture updates. Experiments
on the NAS-Bench-201 and PTB dataset show that AdvantageNAS discovers an
architecture with higher performance under a limited time budget compared to
existing sparse propagation NAS. To further reveal the reliabilities of
AdvantageNAS, we investigate it theoretically and find that it monotonically
improves the expected loss and thus converges.
</p>
<a href="http://arxiv.org/abs/2012.06138" target="_blank">arXiv:2012.06138</a> [<a href="http://arxiv.org/pdf/2012.06138" target="_blank">pdf</a>]

<h2>Exploiting Behavioral Consistence for Universal User Representation. (arXiv:2012.06146v1 [cs.LG])</h2>
<h3>Jie Gu, Feng Wang, Qinghui Sun, Zhiquan Ye, Xiaoxiao Xu, Jingmin Chen, Jun Zhang</h3>
<p>User modeling is critical for developing personalized services in industry. A
common way for user modeling is to learn user representations that can be
distinguished by their interests or preferences. In this work, we focus on
developing universal user representation model. The obtained universal
representations are expected to contain rich information, and be applicable to
various downstream applications without further modifications (e.g., user
preference prediction and user profiling). Accordingly, we can be free from the
heavy work of training task-specific models for every downstream task as in
previous works. In specific, we propose Self-supervised User Modeling Network
(SUMN) to encode behavior data into the universal representation. It includes
two key components. The first one is a new learning objective, which guides the
model to fully identify and preserve valuable user information under a
self-supervised learning framework. The other one is a multi-hop aggregation
layer, which benefits the model capacity in aggregating diverse behaviors.
Extensive experiments on benchmark datasets show that our approach can
outperform state-of-the-art unsupervised representation methods, and even
compete with supervised ones.
</p>
<a href="http://arxiv.org/abs/2012.06146" target="_blank">arXiv:2012.06146</a> [<a href="http://arxiv.org/pdf/2012.06146" target="_blank">pdf</a>]

<h2>Theory-guided hard constraint projection (HCP): a knowledge-based data-driven scientific machine learning method. (arXiv:2012.06148v1 [cs.LG])</h2>
<h3>Yuntian Chen, Dou Huang, Dongxiao Zhang, Junsheng Zeng, Nanzhe Wang, Haoran Zhang, Jinyue Yan</h3>
<p>Machine learning models have been successfully used in many scientific and
engineering fields. However, it remains difficult for a model to simultaneously
utilize domain knowledge and experimental observation data. The application of
knowledge-based symbolic AI represented by an expert system is limited by the
expressive ability of the model, and data-driven connectionism AI represented
by neural networks is prone to produce predictions that violate physical
mechanisms. In order to fully integrate domain knowledge with observations, and
make full use of the prior information and the strong fitting ability of neural
networks, this study proposes theory-guided hard constraint projection (HCP).
This model converts physical constraints, such as governing equations, into a
form that is easy to handle through discretization, and then implements hard
constraint optimization through projection. Based on rigorous mathematical
proofs, theory-guided HCP can ensure that model predictions strictly conform to
physical mechanisms in the constraint patch. The performance of the
theory-guided HCP is verified by experiments based on the heterogeneous
subsurface flow problem. Due to the application of hard constraints, compared
with fully connected neural networks and soft constraint models, such as
theory-guided neural networks and physics-informed neural networks,
theory-guided HCP requires fewer data, and achieves higher prediction accuracy
and stronger robustness to noisy observations.
</p>
<a href="http://arxiv.org/abs/2012.06148" target="_blank">arXiv:2012.06148</a> [<a href="http://arxiv.org/pdf/2012.06148" target="_blank">pdf</a>]

<h2>Superpixel Segmentation Based on Spatially Constrained Subspace Clustering. (arXiv:2012.06149v1 [cs.CV])</h2>
<h3>Hua Li, Yuheng Jia, Runmin Cong, Wenhui Wu, Sam Kwong, Chuanbo Chen</h3>
<p>Superpixel segmentation aims at dividing the input image into some
representative regions containing pixels with similar and consistent intrinsic
properties, without any prior knowledge about the shape and size of each
superpixel. In this paper, to alleviate the limitation of superpixel
segmentation applied in practical industrial tasks that detailed boundaries are
difficult to be kept, we regard each representative region with independent
semantic information as a subspace, and correspondingly formulate superpixel
segmentation as a subspace clustering problem to preserve more detailed content
boundaries. We show that a simple integration of superpixel segmentation with
the conventional subspace clustering does not effectively work due to the
spatial correlation of the pixels within a superpixel, which may lead to
boundary confusion and segmentation error when the correlation is ignored.
Consequently, we devise a spatial regularization and propose a novel convex
locality-constrained subspace clustering model that is able to constrain the
spatial adjacent pixels with similar attributes to be clustered into a
superpixel and generate the content-aware superpixels with more detailed
boundaries. Finally, the proposed model is solved by an efficient alternating
direction method of multipliers (ADMM) solver. Experiments on different
standard datasets demonstrate that the proposed method achieves superior
performance both quantitatively and qualitatively compared with some
state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2012.06149" target="_blank">arXiv:2012.06149</a> [<a href="http://arxiv.org/pdf/2012.06149" target="_blank">pdf</a>]

<h2>A Robust Aerial Gripper for Passive Grasping and Impulsive Release using Scotch Yoke Mechanism. (arXiv:2012.06152v1 [cs.RO])</h2>
<h3>V. SRajashekhar, M. R. Vibha, Kaushik Das, Debasish Ghose</h3>
<p>Aerial transportation requires a simple yet reliable gripper for picking and
placing objects of interest. In this work, we design an aerial gripper for
passive grasping and impulsive release of ferrous coated objects. Permanent
magnets are used for passive grasping and the Scotch Yoke mechanism is used for
providing impulsive force to drop the object. The load carrying capacity of the
gripper is calculated theoretically and experimentally. The parameters such as
the radius of the rotating disk and length of the slider in the Scotch Yoke
mechanism were optimized using weighted geometric programming. The dimensions
of the gripper mount were derived considering the various components of the
gripper. The gripper was mounted on an Unmanned Aerial Vehicle (UAV) and the
tests were done by carrying ferrous coated cuboid shaped objects of different
sizes and masses. These tests were done in manual and autonomous mode in the
outdoor environment.
</p>
<a href="http://arxiv.org/abs/2012.06152" target="_blank">arXiv:2012.06152</a> [<a href="http://arxiv.org/pdf/2012.06152" target="_blank">pdf</a>]

<h2>Underactuated Motion Planning and Control for Jumping with Wheeled-Bipedal Robots. (arXiv:2012.06156v1 [cs.RO])</h2>
<h3>Hua Chen, Bingheng Wang, Zejun Hong, Cong Shen, Patrick M. Wensing, Wei Zhang</h3>
<p>This paper studies jumping for wheeled-bipedal robots, a motion that takes
full advantage of the benefits from the hybrid wheeled and legged design
features. A comprehensive hierarchical scheme for motion planning and control
of jumping with wheeled-bipedal robots is developed. Underactuation of the
wheeled-bipedal dynamics is the main difficulty to be addressed, especially in
the planning problem. To tackle this issue, a novel wheeled-spring-loaded
inverted pendulum (W-SLIP) model is proposed to characterize the essential
dynamics of wheeled-bipedal robots during jumping. Relying on a
differential-flatness-like property of the W-SLIP model, a tractable quadratic
programming based solution is devised for planning jumping motions for
wheeled-bipedal robots. Combined with a kinematic planning scheme accounting
for the flight phase motion, a complete planning scheme for the W-SLIP model is
developed. To enable accurate tracking of the planned trajectories, a linear
quadratic regulator based wheel controller and a task-space whole-body
controller for the other joints are blended through disturbance observers. The
overall planning and control scheme is validated using V-REP simulations of a
prototype wheeled-bipedal robot.
</p>
<a href="http://arxiv.org/abs/2012.06156" target="_blank">arXiv:2012.06156</a> [<a href="http://arxiv.org/pdf/2012.06156" target="_blank">pdf</a>]

<h2>VaryFairyTED : A Fair in Rating Predictor for Public Speeches by Awareness of Verbal and Gesture Quality. (arXiv:2012.06157v1 [cs.AI])</h2>
<h3>Rupam Acharyya, Ankani Chattoraj, Shouman Das, Md. Iftekhar Tanveer, Ehsan Hoque</h3>
<p>The role of verbal and non-verbal cues towards great public speaking has been
a topic of exploration for many decades. We identify a commonality across
present theories, the element of "variety or heterogeneity" in channels or
modes of communication (e.g. resorting to stories, scientific facts, emotional
connections, facial expressions etc.) which is essential for effectively
communicating information. We use this observation to formalize a novel
HEterogeneity Metric, HEM, that quantifies the quality of a talk both in the
verbal and non-verbal domain (transcript and facial gestures). We use TED talks
as an input repository of public speeches because it consists of speakers from
a diverse community besides having a wide outreach. We show that there is an
interesting relationship between HEM and the ratings of TED talks given to
speakers by viewers. It emphasizes that HEM inherently and successfully
represents the quality of a talk based on "variety or heterogeneity". Further,
we also discover that HEM successfully captures the prevalent bias in ratings
with respect to race and gender, that we call sensitive attributes (because
prediction based on these might result in unfair outcome). We incorporate the
HEM metric into the loss function of a neural network with the goal to reduce
unfairness in rating predictions with respect to race and gender. Our results
show that the modified loss function improves fairness in prediction without
considerably affecting prediction accuracy of the neural network. Our work ties
together a novel metric for public speeches in both verbal and non-verbal
domain with the computational power of a neural network to design a fair
prediction system for speakers.
</p>
<a href="http://arxiv.org/abs/2012.06157" target="_blank">arXiv:2012.06157</a> [<a href="http://arxiv.org/pdf/2012.06157" target="_blank">pdf</a>]

<h2>Conceptualization and Framework of Hybrid Intelligence Systems. (arXiv:2012.06161v1 [cs.AI])</h2>
<h3>Nikhil Prakash, Kory W. Mathewson</h3>
<p>As artificial intelligence (AI) systems are getting ubiquitous within our
society, issues related to its fairness, accountability, and transparency are
increasing rapidly. As a result, researchers are integrating humans with AI
systems to build robust and reliable hybrid intelligence systems. However, a
proper conceptualization of these systems does not underpin this rapid growth.
This article provides a precise definition of hybrid intelligence systems as
well as explains its relation with other similar concepts through our proposed
framework and examples from contemporary literature. The framework breakdowns
the relationship between a human and a machine in terms of the degree of
coupling and the directive authority of each party. Finally, we argue that all
AI systems are hybrid intelligence systems, so human factors need to be
examined at every stage of such systems' lifecycle.
</p>
<a href="http://arxiv.org/abs/2012.06161" target="_blank">arXiv:2012.06161</a> [<a href="http://arxiv.org/pdf/2012.06161" target="_blank">pdf</a>]

<h2>Few-Shot Segmentation Without Meta-Learning: A Good Transductive Inference Is All You Need?. (arXiv:2012.06166v1 [cs.CV])</h2>
<h3>Malik Boudiaf, Hoel Kervadec, Ziko Imtiaz Masud, Pablo Piantanida, Ismail Ben Ayed, Jose Dolz</h3>
<p>Few-shot segmentation has recently attracted substantial interest, with the
popular meta-learning paradigm widely dominating the literature. We show that
the way inference is performed for a given few-shot segmentation task has a
substantial effect on performances, an aspect that has been overlooked in the
literature. We introduce a transductive inference, which leverages the
statistics of the unlabeled pixels of a task by optimizing a new loss
containing three complementary terms: (i) a standard cross-entropy on the
labeled pixels; (ii) the entropy of posteriors on the unlabeled query pixels;
and (iii) a global KL-divergence regularizer based on the proportion of the
predicted foreground region. Our inference uses a simple linear classifier of
the extracted features, has a computational load comparable to inductive
inference and can be used on top of any base training. Using standard
cross-entropy training on the base classes, our inference yields highly
competitive performances on well-known few-shot segmentation benchmarks. On
PASCAL-5i, it brings about 5% improvement over the best performing
state-of-the-art method in the 5-shot scenario, while being on par in the
1-shot setting. Even more surprisingly, this gap widens as the number of
support samples increases, reaching up to 6% in the 10-shot scenario.
Furthermore, we introduce a more realistic setting with domain shift, where the
base and novel classes are drawn from different datasets. In this setting, we
found that our method achieves the best performances.
</p>
<a href="http://arxiv.org/abs/2012.06166" target="_blank">arXiv:2012.06166</a> [<a href="http://arxiv.org/pdf/2012.06166" target="_blank">pdf</a>]

<h2>OpenHoldem: An Open Toolkit for Large-Scale Imperfect-Information Game Research. (arXiv:2012.06168v1 [cs.LG])</h2>
<h3>Kai Li, Hang Xu, Meng Zhang, Enmin Zhao, Zhe Wu, Junliang Xing, Kaiqi Huang</h3>
<p>Owning to the unremitting efforts by a few institutes, significant progress
has recently been made in designing superhuman AIs in No-limit Texas Hold'em
(NLTH), the primary testbed for large-scale imperfect-information game
research. However, it remains challenging for new researchers to study this
problem since there are no standard benchmarks for comparing with existing
methods, which seriously hinders further developments in this research area. In
this work, we present OpenHoldem, an integrated toolkit for large-scale
imperfect-information game research using NLTH. OpenHoldem makes three main
contributions to this research direction: 1) a standardized evaluation protocol
for thoroughly evaluating different NLTH AIs, 2) three publicly available
strong baselines for NLTH AI, and 3) an online testing platform with
easy-to-use APIs for public NLTH AI evaluation. We have released OpenHoldem at
this http URL, hoping it facilitates further studies on the unsolved
theoretical and computational issues in this area and cultivate crucial
research problems like opponent modeling, large-scale equilibrium-finding, and
human-computer interactive learning.
</p>
<a href="http://arxiv.org/abs/2012.06168" target="_blank">arXiv:2012.06168</a> [<a href="http://arxiv.org/pdf/2012.06168" target="_blank">pdf</a>]

<h2>AViNet: Diving Deep into Audio-Visual Saliency Prediction. (arXiv:2012.06170v1 [cs.CV])</h2>
<h3>Samyak Jain, Pradeep Yarlagadda, Ramanathan Subramanian, Vineet Gandhi</h3>
<p>We propose the \textbf{AViNet} architecture for audiovisual saliency
prediction. AViNet is a fully convolutional encoder-decoder architecture. The
encoder combines visual features learned for action recognition, with audio
embeddings learned via an aural network designed to classify objects and
scenes. The decoder infers a saliency map via trilinear interpolation and 3D
convolutions, combining hierarchical features. The overall architecture is
conceptually simple, causal, and runs in real-time (60 fps). AViNet outperforms
the state-of-the-art on ten (seven audiovisual and three visual-only) datasets
while surpassing human performance on the CC, SIM, and AUC metrics for the AVE
dataset. Visual features maximally account for saliency on existing datasets
with audio-only contributing to minor gains, except in specific contexts like
social events. Our work, therefore, motivates the need to curate saliency
datasets reflective of real-life, where both the visual and aural modalities
complimentarily drive saliency. Our code and pre-trained models are available
at https://github.com/samyak0210/VideoSaliency
</p>
<a href="http://arxiv.org/abs/2012.06170" target="_blank">arXiv:2012.06170</a> [<a href="http://arxiv.org/pdf/2012.06170" target="_blank">pdf</a>]

<h2>Detailed 3D Human Body Reconstruction from Multi-view Images Combining Voxel Super-Resolution and Learned Implicit Representation. (arXiv:2012.06178v1 [cs.CV])</h2>
<h3>Zhongguo Li, Magnus Oskarsson, Anders Heyden</h3>
<p>The task of reconstructing detailed 3D human body models from images is
interesting but challenging in computer vision due to the high freedom of human
bodies. In order to tackle the problem, we propose a coarse-to-fine method to
reconstruct a detailed 3D human body from multi-view images combining voxel
super-resolution based on learning the implicit representation. Firstly, the
coarse 3D models are estimated by learning an implicit representation based on
multi-scale features which are extracted by multi-stage hourglass networks from
the multi-view images. Then, taking the low resolution voxel grids which are
generated by the coarse 3D models as input, the voxel super-resolution based on
an implicit representation is learned through a multi-stage 3D convolutional
neural network. Finally, the refined detailed 3D human body models can be
produced by the voxel super-resolution which can preserve the details and
reduce the false reconstruction of the coarse 3D models. Benefiting from the
implicit representation, the training process in our method is memory efficient
and the detailed 3D human body produced by our method from multi-view images is
the continuous decision boundary with high-resolution geometry. In addition,
the coarse-to-fine method based on voxel super-resolution can remove false
reconstructions and preserve the appearance details in the final
reconstruction, simultaneously. In the experiments, our method quantitatively
and qualitatively achieves the competitive 3D human body reconstructions from
images with various poses and shapes on both the real and synthetic datasets.
</p>
<a href="http://arxiv.org/abs/2012.06178" target="_blank">arXiv:2012.06178</a> [<a href="http://arxiv.org/pdf/2012.06178" target="_blank">pdf</a>]

<h2>Writer Identification and Writer Retrieval Based on NetVLAD with Re-ranking. (arXiv:2012.06186v1 [cs.CV])</h2>
<h3>Shervin Rasoulzadeh, Bagher Babaali</h3>
<p>This paper addresses writer identification and retrieval which is a
challenging problem in the document analysis field. In this work, a novel
pipeline is proposed for the problem by employing a unified neural network
architecture consisting of the ResNet-20 as a feature extractor and an
integrated NetVLAD layer, inspired by the vectors of locally aggregated
descriptors (VLAD), in the head of the latter part. Having defined this
architecture, triplet semi-hard loss function is used to directly learn an
embedding for individual input image patches. Generalised max-pooling is used
for the aggregation of embedded descriptors of each handwritten image. In the
evaluation part, for identification and retrieval, re-ranking has been done
based on query expansion and $k$-reciprocal nearest neighbours, and it is shown
that the pipeline can benefit tremendously from this step. Experimental
evaluation shows that our writer identification and writer retrieval pipeline
is superior compared to the state-of-the-art pipelines, as our results on the
publicly available ICDAR13 and CVL datasets set new standards by achieving
96.5\% and 98.4\% mAP, respectively.
</p>
<a href="http://arxiv.org/abs/2012.06186" target="_blank">arXiv:2012.06186</a> [<a href="http://arxiv.org/pdf/2012.06186" target="_blank">pdf</a>]

<h2>Neural Dynamic Mode Decomposition for End-to-End Modeling of Nonlinear Dynamics. (arXiv:2012.06191v1 [stat.ML])</h2>
<h3>Tomoharu Iwata, Yoshinobu Kawahara</h3>
<p>Koopman spectral analysis has attracted attention for understanding nonlinear
dynamical systems by which we can analyze nonlinear dynamics with a linear
regime by lifting observations using a nonlinear function. For analysis, we
need to find an appropriate lift function. Although several methods have been
proposed for estimating a lift function based on neural networks, the existing
methods train neural networks without spectral analysis. In this paper, we
propose neural dynamic mode decomposition, in which neural networks are trained
such that the forecast error is minimized when the dynamics is modeled based on
spectral decomposition in the lifted space. With our proposed method, the
forecast error is backpropagated through the neural networks and the spectral
decomposition, enabling end-to-end learning of Koopman spectral analysis. When
information is available on the frequencies or the growth rates of the
dynamics, the proposed method can exploit it as regularizers for training. We
also propose an extension of our approach when observations are influenced by
exogenous control time-series. Our experiments demonstrate the effectiveness of
our proposed method in terms of eigenvalue estimation and forecast performance.
</p>
<a href="http://arxiv.org/abs/2012.06191" target="_blank">arXiv:2012.06191</a> [<a href="http://arxiv.org/pdf/2012.06191" target="_blank">pdf</a>]

<h2>Learning Edge-Preserved Image Stitching from Large-Baseline Deep Homography. (arXiv:2012.06194v1 [cs.CV])</h2>
<h3>Lang Nie, Chunyu Lin, Kang Liao, Yao Zhao</h3>
<p>Image stitching is a classical and crucial technique in computer vision,
which aims to generate the image with a wide field of view. The traditional
methods heavily depend on the feature detection and require that scene features
be dense and evenly distributed in the image, leading to varying ghosting
effects and poor robustness. Learning methods usually suffer from fixed view
and input size limitations, showing a lack of generalization ability on other
real datasets. In this paper, we propose an image stitching learning framework,
which consists of a large-baseline deep homography module and an edge-preserved
deformation module. First, we propose a large-baseline deep homography module
to estimate the accurate projective transformation between the reference image
and the target image in different scales of features. After that, an
edge-preserved deformation module is designed to learn the deformation rules of
image stitching from edge to content, eliminating the ghosting effects as much
as possible. In particular, the proposed learning framework can stitch images
of arbitrary views and input sizes, thus contribute to a supervised deep image
stitching method with excellent generalization capability in other real images.
Experimental results demonstrate that our homography module significantly
outperforms the existing deep homography methods in the large baseline scenes.
In image stitching, our method is superior to the existing learning method and
shows competitive performance with state-of-the-art traditional methods.
</p>
<a href="http://arxiv.org/abs/2012.06194" target="_blank">arXiv:2012.06194</a> [<a href="http://arxiv.org/pdf/2012.06194" target="_blank">pdf</a>]

<h2>Garment Recommendation with Memory Augmented Neural Networks. (arXiv:2012.06200v1 [cs.CV])</h2>
<h3>Lavinia De Divitiis, Federico Becattini, Claudio Baecchi, Alberto Del Bimbo</h3>
<p>Fashion plays a pivotal role in society. Combining garments appropriately is
essential for people to communicate their personality and style. Also different
events require outfits to be thoroughly chosen to comply with underlying social
clothing rules. Therefore, combining garments appropriately might not be
trivial. The fashion industry has turned this into a massive source of income,
relying on complex recommendation systems to retrieve and suggest appropriate
clothing items for customers. To perform better recommendations, personalized
suggestions can be performed, taking into account user preferences or purchase
histories. In this paper, we propose a garment recommendation system to pair
different clothing items, namely tops and bottoms, exploiting a Memory
Augmented Neural Network (MANN). By training a memory writing controller, we
are able to store a non-redundant subset of samples, which is then used to
retrieve a ranked list of suitable bottoms to complement a given top. In
particular, we aim at retrieving a variety of modalities in which a certain
garment can be combined. To refine our recommendations, we then include user
preferences via Matrix Factorization. We experiment on IQON3000, a dataset
collected from an online fashion community, reporting state of the art results.
</p>
<a href="http://arxiv.org/abs/2012.06200" target="_blank">arXiv:2012.06200</a> [<a href="http://arxiv.org/pdf/2012.06200" target="_blank">pdf</a>]

<h2>$\pi$-ROAD: a Learn-as-You-Go Framework for On-Demand Emergency Slices in V2X Scenarios. (arXiv:2012.06208v1 [cs.LG])</h2>
<h3>Armin Okic, Lanfranco Zanzi, Vincenzo Sciancalepore, Alessandro Redondi, Xavier Costa-Perez</h3>
<p>Vehicle-to-everything (V2X) is expected to become one of the main drivers of
5G business in the near future. Dedicated \emph{network slices} are envisioned
to satisfy the stringent requirements of advanced V2X services, such as
autonomous driving, aimed at drastically reducing road casualties. However, as
V2X services become more mission-critical, new solutions need to be devised to
guarantee their successful service delivery even in exceptional situations,
e.g. road accidents, congestion, etc. In this context, we propose $\pi$-ROAD, a
\emph{deep learning} framework to automatically learn regular mobile traffic
patterns along roads, detect non-recurring events and classify them by severity
level. $\pi$-ROAD enables operators to \emph{proactively} instantiate dedicated
\emph{Emergency Network Slices (ENS)} as needed while re-dimensioning the
existing slices according to their service criticality level. Our framework is
validated by means of real mobile network traces collected within $400~km$ of a
highway in Europe and augmented with publicly available information on related
road events. Our results show that $\pi$-ROAD successfully detects and
classifies non-recurring road events and reduces up to $30\%$ the impact of ENS
on already running services.
</p>
<a href="http://arxiv.org/abs/2012.06208" target="_blank">arXiv:2012.06208</a> [<a href="http://arxiv.org/pdf/2012.06208" target="_blank">pdf</a>]

<h2>Structured Policy Representation: Imposing Stability in arbitrarily conditioned dynamic systems. (arXiv:2012.06224v1 [cs.RO])</h2>
<h3>Julen Urain, Davide Tateo, Tianyu Ren, Jan Peters</h3>
<p>We present a new family of deep neural network-based dynamic systems. The
presented dynamics are globally stable and can be conditioned with an arbitrary
context state. We show how these dynamics can be used as structured robot
policies. Global stability is one of the most important and straightforward
inductive biases as it allows us to impose reasonable behaviors outside the
region of the demonstrations.
</p>
<a href="http://arxiv.org/abs/2012.06224" target="_blank">arXiv:2012.06224</a> [<a href="http://arxiv.org/pdf/2012.06224" target="_blank">pdf</a>]

<h2>Query Understanding for Natural Language Enterprise Search. (arXiv:2012.06238v1 [cs.LG])</h2>
<h3>Francisco Borges, Georgios Balikas, Marc Brette, Guillaume Kempf, Arvind Srikantan, Matthieu Landos, Darya Brazouskaya, Qianqian Shi</h3>
<p>Natural Language Search (NLS) extends the capabilities of search engines that
perform keyword search allowing users to issue queries in a more "natural"
language. The engine tries to understand the meaning of the queries and to map
the query words to the symbols it supports like Persons, Organizations, Time
Expressions etc.. It, then, retrieves the information that satisfies the user's
need in different forms like an answer, a record or a list of records. We
present an NLS system we implemented as part of the Search service of a major
CRM platform. The system is currently in production serving thousands of
customers. Our user studies showed that creating dynamic reports with NLS saved
more than 50% of our user's time compared to achieving the same result with
navigational search. We describe the architecture of the system, the
particularities of the CRM domain as well as how they have influenced our
design decisions. Among several submodules of the system we detail the role of
a Deep Learning Named Entity Recognizer. The paper concludes with discussion
over the lessons learned while developing this product.
</p>
<a href="http://arxiv.org/abs/2012.06238" target="_blank">arXiv:2012.06238</a> [<a href="http://arxiv.org/pdf/2012.06238" target="_blank">pdf</a>]

<h2>The Implicit Bias for Adaptive Optimization Algorithms on Homogeneous Neural Networks. (arXiv:2012.06244v1 [cs.LG])</h2>
<h3>Bohan Wang, Qi Meng, Wei Chen</h3>
<p>Despite their overwhelming capacity to overfit, deep neural networks trained
by specific optimization algorithms tend to generalize relatively well to
unseen data. Recently, researchers explained it by investigating the implicit
bias of optimization algorithms. A remarkable progress is the work [18], which
proves gradient descent (GD) maximizes the margin of homogeneous deep neural
networks. Except the first-order optimization algorithms like GD, adaptive
algorithms such as AdaGrad, RMSProp and Adam are popular owing to its rapid
training process. Meanwhile, numerous works have provided empirical evidence
that adaptive methods may suffer from poor generalization performance. However,
theoretical explanation for the generalization of adaptive optimization
algorithms is still lacking. In this paper, we study the implicit bias of
adaptive optimization algorithms on homogeneous neural networks. In particular,
we study the convergent direction of parameters when they are optimizing the
logistic loss. We prove that the convergent direction of RMSProp is the same
with GD, while for AdaGrad, the convergent direction depends on the adaptive
conditioner. Technically, we provide a unified framework to analyze convergent
direction of adaptive optimization algorithms by constructing novel and
nontrivial adaptive gradient flow and surrogate margin. The theoretical
findings explain the superiority on generalization of exponential moving
average strategy that is adopted by RMSProp and Adam. To the best of knowledge,
it is the first work to study the convergent direction of adaptive
optimizations on non-linear deep neural networks
</p>
<a href="http://arxiv.org/abs/2012.06244" target="_blank">arXiv:2012.06244</a> [<a href="http://arxiv.org/pdf/2012.06244" target="_blank">pdf</a>]

<h2>EarthNet2021: A novel large-scale dataset and challenge for forecasting localized climate impacts. (arXiv:2012.06246v1 [cs.LG])</h2>
<h3>Christian Requena-Mesa, Vitus Benson, Joachim Denzler, Jakob Runge, Markus Reichstein</h3>
<p>Climate change is global, yet its concrete impacts can strongly vary between
different locations in the same region. Seasonal weather forecasts currently
operate at the mesoscale (&gt; 1 km). For more targeted mitigation and adaptation,
modelling impacts to &lt; 100 m is needed. Yet, the relationship between driving
variables and Earth's surface at such local scales remains unresolved by
current physical models. Large Earth observation datasets now enable us to
create machine learning models capable of translating coarse weather
information into high-resolution Earth surface forecasts. Here, we define
high-resolution Earth surface forecasting as video prediction of satellite
imagery conditional on mesoscale weather forecasts. Video prediction has been
tackled with deep learning models. Developing such models requires
analysis-ready datasets. We introduce EarthNet2021, a new, curated dataset
containing target spatio-temporal Sentinel 2 satellite imagery at 20 m
resolution, matched with high-resolution topography and mesoscale (1.28 km)
weather variables. With over 32000 samples it is suitable for training deep
neural networks. Comparing multiple Earth surface forecasts is not trivial.
Hence, we define the EarthNetScore, a novel ranking criterion for models
forecasting Earth surface reflectance. For model intercomparison we frame
EarthNet2021 as a challenge with four tracks based on different test sets.
These allow evaluation of model validity and robustness as well as model
applicability to extreme events and the complete annual vegetation cycle. In
addition to forecasting directly observable weather impacts through
satellite-derived vegetation indices, capable Earth surface models will enable
downstream applications such as crop yield prediction, forest health
assessments, coastline management, or biodiversity monitoring. Find data, code,
and how to participate at www.earthnet.tech .
</p>
<a href="http://arxiv.org/abs/2012.06246" target="_blank">arXiv:2012.06246</a> [<a href="http://arxiv.org/pdf/2012.06246" target="_blank">pdf</a>]

<h2>Structured learning of rigid-body dynamics: A survey and unified view. (arXiv:2012.06250v1 [cs.LG])</h2>
<h3>A. Ren&#xe9; Geist, Sebastian Trimpe</h3>
<p>Accurate models of mechanical system dynamics are often critical for
model-based control and reinforcement learning. Fully data-driven dynamics
models promise to ease the process of modeling and analysis, but require
considerable amounts of data for training and often do not generalize well to
unseen parts of the state space. Combining data-driven modelling with prior
analytical knowledge is an attractive alternative as the inclusion of
structural knowledge into a regression model improves the model's data
efficiency and physical integrity. In this article, we survey supervised
regression models that combine rigid-body mechanics with data-driven modelling
techniques. We analyze the different latent functions (such as kinetic energy
or dissipative forces) and operators (such as differential operators and
projection matrices) underlying common descriptions of rigid-body mechanics.
Based on this analysis, we provide a unified view on the combination of
data-driven regression models, such as neural networks and Gaussian processes,
with analytical model priors. Further, we review and discuss key techniques for
designing structured models such as automatic differentiation.
</p>
<a href="http://arxiv.org/abs/2012.06250" target="_blank">arXiv:2012.06250</a> [<a href="http://arxiv.org/pdf/2012.06250" target="_blank">pdf</a>]

<h2>One Point is All You Need: Directional Attention Point for Feature Learning. (arXiv:2012.06257v1 [cs.CV])</h2>
<h3>Liqiang Lin, Pengdi Huang, Chi-Wing Fu, Kai Xu, Hao Zhang, Hui Huang</h3>
<p>We present a novel attention-based mechanism for learning enhanced point
features for tasks such as point cloud classification and segmentation. Our key
message is that if the right attention point is selected, then "one point is
all you need" -- not a sequence as in a recurrent model and not a pre-selected
set as in all prior works. Also, where the attention point is should be
learned, from data and specific to the task at hand. Our mechanism is
characterized by a new and simple convolution, which combines the feature at an
input point with the feature at its associated attention point. We call such a
point adirectional attention point (DAP), since it is found by adding to the
original point an offset vector that is learned by maximizing the task
performance in training. We show that our attention mechanism can be easily
incorporated into state-of-the-art point cloud classification and segmentation
networks. Extensive experiments on common benchmarks such as ModelNet40,
ShapeNetPart, and S3DIS demonstrate that our DAP-enabled networks consistently
outperform the respective original networks, as well as all other competitive
alternatives, including those employing pre-selected sets of attention points.
</p>
<a href="http://arxiv.org/abs/2012.06257" target="_blank">arXiv:2012.06257</a> [<a href="http://arxiv.org/pdf/2012.06257" target="_blank">pdf</a>]

<h2>Comparison of Anomaly Detectors: Context Matters. (arXiv:2012.06260v1 [cs.LG])</h2>
<h3>V&#xed;t &#x160;kv&#xe1;ra, Jan Franc&#x16f;, Mat&#x11b;j Zorek, Tom&#xe1;&#x161; Pevn&#xfd;, V&#xe1;clav &#x160;m&#xed;dl</h3>
<p>Deep generative models are challenging the classical methods in the field of
anomaly detection nowadays. Every new method provides evidence of outperforming
its predecessors, often with contradictory results. The objective of this
comparison is twofold: comparison of anomaly detection methods of various
paradigms, and identification of sources of variability that can yield
different results. The methods were compared on popular tabular and image
datasets. While the one class support-vector machine (OC-SVM) had no rival on
the tabular datasets, the best results on the image data were obtained either
by a feature-matching GAN or a combination of variational autoencoder (VAE) and
OC-SVM, depending on the experimental conditions. The main sources of
variability that can influence the performance of the methods were identified
to be: the range of searched hyper-parameters, the methodology of model
selection, and the choice of the anomalous samples. All our code and results
are available for download.
</p>
<a href="http://arxiv.org/abs/2012.06260" target="_blank">arXiv:2012.06260</a> [<a href="http://arxiv.org/pdf/2012.06260" target="_blank">pdf</a>]

<h2>Motion Mappings for Continuous Bilateral Teleoperation. (arXiv:2012.06268v1 [cs.RO])</h2>
<h3>Xiao Gao, Jo&#xe3;o Silv&#xe9;rio, Emmanuel Pignat, Sylvain Calinon, Miao Li, Xiaohui Xiao</h3>
<p>Mapping operator motions to a robot is a key problem in teleoperation. Due to
differences between workspaces, such as object locations, it is particularly
challenging to derive smooth motion mappings that fulfill different goals (e.g.
picking objects with different poses on the two sides or passing through key
points). Indeed, most state-of-the-art methods rely on mode switches, leading
to a discontinuous, low-transparency experience. In this paper, we propose a
unified formulation for position, orientation and velocity mappings based on
the poses of objects of interest in the operator and robot workspaces. We apply
it in the context of bilateral teleoperation. Two possible implementations to
achieve the proposed mappings are studied: an iterative approach based on
locally-weighted translations and rotations, and a neural network approach.
Evaluations are conducted both in simulation and using two torque-controlled
Franka Emika Panda robots. Our results show that, despite longer training
times, the neural network approach provides faster mapping evaluations and
lower interaction forces for the operator, which are crucial for continuous,
real-time teleoperation.
</p>
<a href="http://arxiv.org/abs/2012.06268" target="_blank">arXiv:2012.06268</a> [<a href="http://arxiv.org/pdf/2012.06268" target="_blank">pdf</a>]

<h2>Hard-ODT: Hardware-Friendly Online Decision Tree Learning Algorithm and System. (arXiv:2012.06272v1 [cs.LG])</h2>
<h3>Zhe Lin, Sharad Sinha, Wei Zhang</h3>
<p>Decision trees are machine learning models commonly used in various
application scenarios. In the era of big data, traditional decision tree
induction algorithms are not suitable for learning large-scale datasets due to
their stringent data storage requirement. Online decision tree learning
algorithms have been devised to tackle this problem by concurrently training
with incoming samples and providing inference results. However, even the most
up-to-date online tree learning algorithms still suffer from either high memory
usage or high computational intensity with dependency and long latency, making
them challenging to implement in hardware. To overcome these difficulties, we
introduce a new quantile-based algorithm to improve the induction of the
Hoeffding tree, one of the state-of-the-art online learning models. The
proposed algorithm is light-weight in terms of both memory and computational
demand, while still maintaining high generalization ability. A series of
optimization techniques dedicated to the proposed algorithm have been
investigated from the hardware perspective, including coarse-grained and
fine-grained parallelism, dynamic and memory-based resource sharing, pipelining
with data forwarding. Following this, we present Hard-ODT, a high-performance,
hardware-efficient and scalable online decision tree learning system on a
field-programmable gate array (FPGA) with system-level optimization techniques.
Performance and resource utilization are modeled for the complete learning
system for early and fast analysis of the trade-off between various design
metrics. Finally, we propose a design flow in which the proposed learning
system is applied to FPGA run-time power monitoring as a case study.
</p>
<a href="http://arxiv.org/abs/2012.06272" target="_blank">arXiv:2012.06272</a> [<a href="http://arxiv.org/pdf/2012.06272" target="_blank">pdf</a>]

<h2>Dual Control for Exploitation and Exploration (DCEE) in Autonomous Search. (arXiv:2012.06276v1 [cs.RO])</h2>
<h3>Wen-Hua Chen, Callum Rhodes, Cunjia Liu</h3>
<p>This paper proposes an optimal autonomous search framework, namely Dual
Control for Exploration and Exploitation (DCEE), for a target at unknown
location in an unknown environment. Source localisation is to find sources of
atmospheric hazardous material release in a partially unknown environment. This
paper proposes a control theoretic approach to this autonomous search problem.
To cope with an unknown target location, at each step, the target location is
estimated by Bayesian inference. Then a control action is taken to minimise the
error between future robot position and the hypothesised future estimation of
the target location. The latter is generated by hypothesised measurements at
the corresponding future robot positions (due to the control action) with the
current estimation of the target location as a prior. It shows that this
approach can take into account both the error between the next robot position
and the estimate of the target location, and the uncertainty of the estimate.
This approach is further extended to the case with not only an unknown source
location, but also an unknown local environment (e.g. wind speed and
direction). Different from current information theoretic approaches, this new
control theoretic approach achieves the optimal trade-off between exploitation
and exploration in a unknown environment with an unknown target by driving the
robot moving towards estimated target location while reducing its estimation
uncertainty. This scheme is implemented using particle filtering on a mobile
robot. Simulation and experimental studies demonstrate promising performance of
the proposed approach. The relationships between the proposed approach,
informative path planning, dual control, and classic model predictive control
are discussed and compared.
</p>
<a href="http://arxiv.org/abs/2012.06276" target="_blank">arXiv:2012.06276</a> [<a href="http://arxiv.org/pdf/2012.06276" target="_blank">pdf</a>]

<h2>Video Camera Identification from Sensor Pattern Noise with a Constrained ConvNet. (arXiv:2012.06277v1 [cs.CV])</h2>
<h3>Derrick Timmerman, Swaroop Bennabhaktula, Enrique Alegre, George Azzopardi</h3>
<p>The identification of source cameras from videos, though it is a highly
relevant forensic analysis topic, has been studied much less than its
counterpart that uses images. In this work we propose a method to identify the
source camera of a video based on camera specific noise patterns that we
extract from video frames. For the extraction of noise pattern features, we
propose an extended version of a constrained convolutional layer capable of
processing color inputs. Our system is designed to classify individual video
frames which are in turn combined by a majority vote to identify the source
camera. We evaluated this approach on the benchmark VISION data set consisting
of 1539 videos from 28 different cameras. To the best of our knowledge, this is
the first work that addresses the challenge of video camera identification on a
device level. The experiments show that our approach is very promising,
achieving up to 93.1% accuracy while being robust to the WhatsApp and YouTube
compression techniques. This work is part of the EU-funded project 4NSEEK
focused on forensics against child sexual abuse.
</p>
<a href="http://arxiv.org/abs/2012.06277" target="_blank">arXiv:2012.06277</a> [<a href="http://arxiv.org/pdf/2012.06277" target="_blank">pdf</a>]

<h2>Unsupervised Learning of slow features for Data Efficient Regression. (arXiv:2012.06279v1 [cs.LG])</h2>
<h3>Oliver Struckmeier, Kshitij Tiwari, Ville Kyrki</h3>
<p>Research in computational neuroscience suggests that the human brain's
unparalleled data efficiency is a result of highly efficient mechanisms to
extract and organize slowly changing high level features from continuous
sensory inputs. In this paper, we apply this slowness principle to a state of
the art representation learning method with the goal of performing data
efficient learning of down-stream regression tasks. To this end, we propose the
slow variational autoencoder (S-VAE), an extension to the $\beta$-VAE which
applies a temporal similarity constraint to the latent representations. We
empirically compare our method to the $\beta$-VAE and the Temporal Difference
VAE (TD-VAE), a state-of-the-art method for next frame prediction in latent
space with temporal abstraction. We evaluate the three methods against their
data-efficiency on down-stream tasks using a synthetic 2D ball tracking
dataset, a dataset from a reinforcent learning environment and a dataset
generated using the DeepMind Lab environment. In all tasks, the proposed method
outperformed the baselines both with dense and especially sparse labeled data.
The S-VAE achieved similar or better performance compared to the baselines with
$20\%$ to $93\%$ less data.
</p>
<a href="http://arxiv.org/abs/2012.06279" target="_blank">arXiv:2012.06279</a> [<a href="http://arxiv.org/pdf/2012.06279" target="_blank">pdf</a>]

<h2>Acoustic Leak Detection in Water Networks. (arXiv:2012.06280v1 [cs.LG])</h2>
<h3>Robert M&#xfc;ller, Steffen Illium, Fabian Ritz, Tobias Schr&#xf6;der, Christian Platschek, J&#xf6;rg Ochs, Claudia Linnhoff-Popoien</h3>
<p>In this work, we present a general procedure for acoustic leak detection in
water networks that satisfies multiple real-world constraints such as energy
efficiency and ease of deployment. Based on recordings from seven contact
microphones attached to the water supply network of a municipal suburb, we
trained several shallow and deep anomaly detection models. Inspired by how
human experts detect leaks using electronic sounding-sticks, we use these
models to repeatedly listen for leaks over a predefined decision horizon. This
way we avoid constant monitoring of the system. While we found the detection of
leaks in close proximity to be a trivial task for almost all models, neural
network based approaches achieve better results at the detection of distant
leaks.
</p>
<a href="http://arxiv.org/abs/2012.06280" target="_blank">arXiv:2012.06280</a> [<a href="http://arxiv.org/pdf/2012.06280" target="_blank">pdf</a>]

<h2>ADD: Augmented Disentanglement Distillation Framework for Improving Stock Trend Forecasting. (arXiv:2012.06289v1 [cs.LG])</h2>
<h3>Hongshun Tang, Lijun Wu, Weiqing Liu, Jiang Bian</h3>
<p>Stock trend forecasting has become a popular research direction that attracts
widespread attention in the financial field. Though deep learning methods have
achieved promising results, there are still many limitations, for example, how
to extract clean features from the raw stock data. In this paper, we introduce
an \emph{Augmented Disentanglement Distillation (ADD)} approach to remove
interferential features from the noised raw data. Specifically, we present 1) a
disentanglement structure to separate excess and market information from the
stock data to avoid the two factors disturbing each other's own prediction.
Besides, by applying 2) a dynamic self-distillation method over the
disentanglement framework, other implicit interference factors can also be
removed. Further, thanks to the decoder module in our framework, 3) a novel
strategy is proposed to augment the training samples based on the different
excess and market features to improve performance. We conduct experiments on
the Chinese stock market data. Results show that our method significantly
improves the stock trend forecasting performances, as well as the actual
investment income through backtesting, which strongly demonstrates the
effectiveness of our approach.
</p>
<a href="http://arxiv.org/abs/2012.06289" target="_blank">arXiv:2012.06289</a> [<a href="http://arxiv.org/pdf/2012.06289" target="_blank">pdf</a>]

<h2>Crowd Vetting: Rejecting Adversaries via Collaboration--with Application to Multi-Robot Flocking. (arXiv:2012.06291v1 [cs.RO])</h2>
<h3>Frederik Mallmann-Trenn, Matthew Cavorsi, Stephanie Gil</h3>
<p>We characterize the advantage of using a robot's neighborhood to find and
eliminate adversarial robots in the presence of a Sybil attack. We show that by
leveraging the opinions of its neighbors on the trustworthiness of transmitted
data, robots can detect adversaries with high probability. We characterize a
number of communication rounds required to achieve this result to be a function
of the communication quality and the proportion of legitimate to malicious
robots. This result enables increased resiliency of many multi-robot
algorithms. Because our results are finite time and not asymptotic, they are
particularly well-suited for problems with a time critical nature. We develop
two algorithms, \emph{FindSpoofedRobots} that determines trusted neighbors with
high probability, and \emph{FindResilientAdjacencyMatrix} that enables
distributed computation of graph properties in an adversarial setting. We apply
our methods to a flocking problem where a team of robots must track a moving
target in the presence of adversarial robots. We show that by using our
algorithms, the team of robots are able to maintain tracking ability of the
dynamic target.
</p>
<a href="http://arxiv.org/abs/2012.06291" target="_blank">arXiv:2012.06291</a> [<a href="http://arxiv.org/pdf/2012.06291" target="_blank">pdf</a>]

<h2>Spotlight-based 3D Instrument Guidance for Retinal Surgery. (arXiv:2012.06292v1 [cs.RO])</h2>
<h3>Mingchuan Zhou, Jiahao Wu, Ali Ebrahimi, Niravkumar Patel, Changyan He, Peter Gehlbach, Russell H Taylor, Alois Knoll, M Ali Nasseri, Iulian I Iordachita</h3>
<p>Retinal surgery is a complex activity that can be challenging for a surgeon
to perform effectively and safely. Image guided robot-assisted surgery is one
of the promising solutions that bring significant surgical enhancement in
treatment outcome and reduce the physical limitations of human surgeons. In
this paper, we demonstrate a novel method for 3D guidance of the instrument
based on the projection of spotlight in the single microscope images. The
spotlight projection mechanism is firstly analyzed and modeled with a
projection on both a plane and a sphere surface. To test the feasibility of the
proposed method, a light fiber is integrated into the instrument which is
driven by the Steady-Hand Eye Robot (SHER). The spot of light is segmented and
tracked on a phantom retina using the proposed algorithm. The static
calibration and dynamic test results both show that the proposed method can
easily archive 0.5 mm of tip-to-surface distance which is within the clinically
acceptable accuracy for intraocular visual guidance.
</p>
<a href="http://arxiv.org/abs/2012.06292" target="_blank">arXiv:2012.06292</a> [<a href="http://arxiv.org/pdf/2012.06292" target="_blank">pdf</a>]

<h2>EventKG+BT: Generation of Interactive Biography Timelines from a Knowledge Graph. (arXiv:2012.06306v1 [cs.AI])</h2>
<h3>Simon Gottschalk, Elena Demidova</h3>
<p>Research on notable accomplishments and important events in the life of
people of public interest usually requires close reading of long encyclopedic
or biographical sources, which is a tedious and time-consuming task. Whereas
semantic reference sources, such as the EventKG knowledge graph, provide
structured representations of relevant facts, they often include hundreds of
events and temporal relations for particular entities. In this paper, we
present EventKG+BT - a timeline generation system that creates concise and
interactive spatio-temporal representations of biographies from a knowledge
graph using distant supervision.
</p>
<a href="http://arxiv.org/abs/2012.06306" target="_blank">arXiv:2012.06306</a> [<a href="http://arxiv.org/pdf/2012.06306" target="_blank">pdf</a>]

<h2>Learning Order Parameters from Videos of Dynamical Phases for Skyrmions with Neural Networks. (arXiv:2012.06308v1 [cs.LG])</h2>
<h3>Weidi Wang, Zeyuan Wang, Yinghui Zhang, Bo Sun, Ke Xia</h3>
<p>The ability to recognize dynamical phenomena (e.g., dynamical phases) and
dynamical processes in physical events from videos, then to abstract physical
concepts and reveal physical laws, lies at the core of human intelligence. The
main purposes of this paper are to use neural networks for classifying the
dynamical phases of some videos and to demonstrate that neural networks can
learn physical concepts from them. To this end, we employ multiple neural
networks to recognize the static phases (image format) and dynamical phases
(video format) of a particle-based skyrmion model. Our results show that neural
networks, without any prior knowledge, can not only correctly classify these
phases, but also predict the phase boundaries which agree with those obtained
by simulation. We further propose a parameter visualization scheme to interpret
what neural networks have learned. We show that neural networks can learn two
order parameters from videos of dynamical phases and predict the critical
values of two order parameters. Finally, we demonstrate that only two order
parameters are needed to identify videos of skyrmion dynamical phases. It shows
that this parameter visualization scheme can be used to determine how many
order parameters are needed to fully recognize the input phases. Our work sheds
light on the future use of neural networks in discovering new physical concepts
and revealing unknown yet physical laws from videos.
</p>
<a href="http://arxiv.org/abs/2012.06308" target="_blank">arXiv:2012.06308</a> [<a href="http://arxiv.org/pdf/2012.06308" target="_blank">pdf</a>]

<h2>Artificial Intelligence for COVID-19 Detection -- A state-of-the-art review. (arXiv:2012.06310v1 [cs.LG])</h2>
<h3>Parsa Sarosh, Shabir A. Parah, Romany F Mansur, G. M. Bhat</h3>
<p>The emergence of COVID-19 has necessitated many efforts by the scientific
community for its proper management. An urgent clinical reaction is required in
the face of the unending devastation being caused by the pandemic. These
efforts include technological innovations for improvement in screening,
treatment, vaccine development, contact tracing and, survival prediction. The
use of Deep Learning (DL) and Artificial Intelligence (AI) can be sought in all
of the above-mentioned spheres. This paper aims to review the role of Deep
Learning and Artificial intelligence in various aspects of the overall COVID-19
management and particularly for COVID-19 detection and classification. The DL
models are developed to analyze clinical modalities like CT scans and X-Ray
images of patients and predict their pathological condition. A DL model aims to
detect the COVID-19 pneumonia, classify and distinguish between COVID-19,
Community-Acquired Pneumonia (CAP), Viral and Bacterial pneumonia, and normal
conditions. Furthermore, sophisticated models can be built to segment the
affected area in the lungs and quantify the infection volume for a better
understanding of the extent of damage. Many models have been developed either
independently or with the help of pre-trained models like VGG19, ResNet50, and
AlexNet leveraging the concept of transfer learning. Apart from model
development, data preprocessing and augmentation are also performed to cope
with the challenge of insufficient data samples often encountered in medical
applications. It can be evaluated that DL and AI can be effectively implemented
to withstand the challenges posed by the global emergency
</p>
<a href="http://arxiv.org/abs/2012.06310" target="_blank">arXiv:2012.06310</a> [<a href="http://arxiv.org/pdf/2012.06310" target="_blank">pdf</a>]

<h2>Differentiable Histogram with Hard-Binning. (arXiv:2012.06311v1 [cs.LG])</h2>
<h3>Ibrahim Yusuf, George Igwegbe, Oluwafemi Azeez</h3>
<p>The simplicity and expressiveness of a histogram render it a useful feature
in different contexts including deep learning. Although the process of
computing a histogram is non-differentiable, researchers have proposed
differentiable approximations, which have some limitations. A differentiable
histogram that directly approximates the hard-binning operation in conventional
histograms is proposed. It combines the strength of existing differentiable
histograms and overcomes their individual challenges. In comparison to a
histogram computed using Numpy, the proposed histogram has an absolute
approximation error of 0.000158.
</p>
<a href="http://arxiv.org/abs/2012.06311" target="_blank">arXiv:2012.06311</a> [<a href="http://arxiv.org/pdf/2012.06311" target="_blank">pdf</a>]

<h2>Deep Reinforcement Learning for Long Term Hydropower Production Scheduling. (arXiv:2012.06312v1 [cs.LG])</h2>
<h3>Signe Riemer-Sorensen, Gjert H. Rosenlund</h3>
<p>We explore the use of deep reinforcement learning to provide strategies for
long term scheduling of hydropower production. We consider a use-case where the
aim is to optimise the yearly revenue given week-by-week inflows to the
reservoir and electricity prices. The challenge is to decide between immediate
water release at the spot price of electricity and storing the water for later
power production at an unknown price, given constraints on the system. We
successfully train a soft actor-critic algorithm on a simplified scenario with
historical data from the Nordic power market. The presented model is not ready
to substitute traditional optimisation tools but demonstrates the complementary
potential of reinforcement learning in the data-rich field of hydropower
scheduling.
</p>
<a href="http://arxiv.org/abs/2012.06312" target="_blank">arXiv:2012.06312</a> [<a href="http://arxiv.org/pdf/2012.06312" target="_blank">pdf</a>]

<h2>Self-Growing Spatial Graph Network for Context-Aware Pedestrian Trajectory Prediction. (arXiv:2012.06320v1 [cs.CV])</h2>
<h3>Sirin Haddad, Siew-Kei Lam</h3>
<p>Pedestrian trajectory prediction is an active research area with recent works
undertaken to embed accurate models of pedestrians social interactions and
their contextual compliance into dynamic spatial graphs. However, existing
works rely on spatial assumptions about the scene and dynamics, which entails a
significant challenge to adapt the graph structure in unknown environments for
an online system. %Additionally, tackling the same problem for streamed data
entails the inherent challenge of adapting the graph structure to represent
pedestrians interactions without reliance on spatial assumptions. In addition,
there is a lack of assessment approach for the relational modeling impact on
prediction performance. To fill this gap, we propose Social Trajectory
Recommender-Gated Graph Recurrent Neighborhood Network, (STR-GGRNN), which uses
data-driven adaptive online neighborhood recommendation based on the contextual
scene features and pedestrian visual cues. The neighborhood recommendation is
achieved by online Nonnegative Matrix Factorization (NMF) to construct the
graph adjacency matrices for predicting the pedestrians' trajectories. %and
evaluates the adjacency matrix against prediction errors. s Experiments based
on widely-used datasets show that our method outperforms the state-of-the-art.
Our best performing model achieves 12 cm ADE and $\sim$15 cm FDE on ETH-UCY
dataset. The proposed method takes only 0.49 seconds when sampling a total of
20K future trajectories per frame.
</p>
<a href="http://arxiv.org/abs/2012.06320" target="_blank">arXiv:2012.06320</a> [<a href="http://arxiv.org/pdf/2012.06320" target="_blank">pdf</a>]

<h2>Deep Reinforcement Learning for Stock Portfolio Optimization. (arXiv:2012.06325v1 [cs.LG])</h2>
<h3>Le Trung Hieu</h3>
<p>Stock portfolio optimization is the process of constant re-distribution of
money to a pool of various stocks. In this paper, we will formulate the problem
such that we can apply Reinforcement Learning for the task properly. To
maintain a realistic assumption about the market, we will incorporate
transaction cost and risk factor into the state as well. On top of that, we
will apply various state-of-the-art Deep Reinforcement Learning algorithms for
comparison. Since the action space is continuous, the realistic formulation
were tested under a family of state-of-the-art continuous policy gradients
algorithms: Deep Deterministic Policy Gradient (DDPG), Generalized
Deterministic Policy Gradient (GDPG) and Proximal Policy Optimization (PPO),
where the former two perform much better than the last one. Next, we will
present the end-to-end solution for the task with Minimum Variance Portfolio
Theory for stock subset selection, and Wavelet Transform for extracting
multi-frequency data pattern. Observations and hypothesis were discussed about
the results, as well as possible future research directions.1
</p>
<a href="http://arxiv.org/abs/2012.06325" target="_blank">arXiv:2012.06325</a> [<a href="http://arxiv.org/pdf/2012.06325" target="_blank">pdf</a>]

<h2>exploRNN: Understanding Recurrent Neural Networks through Visual Exploration. (arXiv:2012.06326v1 [cs.LG])</h2>
<h3>Alex B&#xe4;uerle, Raphael St&#xf6;rk, Timo Ropinski</h3>
<p>Due to the success of deep learning and its growing job market, students and
researchers from many areas are getting interested in learning about deep
learning technologies. Visualization has proven to be of great help during this
learning process, while most current educational visualizations are targeted
towards one specific architecture or use case. Unfortunately, recurrent neural
networks (RNNs), which are capable of processing sequential data, are not
covered yet, despite the fact that tasks on sequential data, such as text and
function analysis, are at the forefront of deep learning research. Therefore,
we propose exploRNN, the first interactively explorable, educational
visualization for RNNs. exploRNN allows for interactive experimentation with
RNNs, and provides in-depth information on their functionality and behavior
during training. By defining educational objectives targeted towards
understanding RNNs, and using these as guidelines throughout the visual design
process, we have designed exploRNN to communicate the most important concepts
of RNNs directly within a web browser. By means of exploRNN, we provide an
overview of the training process of RNNs at a coarse level, while also allowing
detailed inspection of the data-flow within LSTM cells. Within this paper, we
motivate our design of exploRNN, detail its realization, and discuss the
results of a user study investigating the benefits of exploRNN.
</p>
<a href="http://arxiv.org/abs/2012.06326" target="_blank">arXiv:2012.06326</a> [<a href="http://arxiv.org/pdf/2012.06326" target="_blank">pdf</a>]

<h2>Sheaf Neural Networks. (arXiv:2012.06333v1 [cs.LG])</h2>
<h3>Jakob Hansen, Thomas Gebhart</h3>
<p>We present a generalization of graph convolutional networks by generalizing
the diffusion operation underlying this class of graph neural networks. These
sheaf neural networks are based on the sheaf Laplacian, a generalization of the
graph Laplacian that encodes additional relational structure parameterized by
the underlying graph. The sheaf Laplacian and associated matrices provide an
extended version of the diffusion operation in graph convolutional networks,
providing a proper generalization for domains where relations between nodes are
non-constant, asymmetric, and varying in dimension. We show that the resulting
sheaf neural networks can outperform graph convolutional networks in domains
where relations between nodes are asymmetric and signed.
</p>
<a href="http://arxiv.org/abs/2012.06333" target="_blank">arXiv:2012.06333</a> [<a href="http://arxiv.org/pdf/2012.06333" target="_blank">pdf</a>]

<h2>The Why, What and How of Artificial General Intelligence Chip Development. (arXiv:2012.06338v1 [cs.LG])</h2>
<h3>Alex James</h3>
<p>The AI chips increasingly focus on implementing neural computing at low power
and cost. The intelligent sensing, automation, and edge computing applications
have been the market drivers for AI chips. Increasingly, the generalisation,
performance, robustness, and scalability of the AI chip solutions are compared
with human-like intelligence abilities. Such a requirement to transit from
application-specific to general intelligence AI chip must consider several
factors. This paper provides an overview of this cross-disciplinary field of
study, elaborating on the generalisation of intelligence as understood in
building artificial general intelligence (AGI) systems. This work presents a
listing of emerging AI chip technologies, classification of edge AI
implementations, and the funnel design flow for AGI chip development. Finally,
the design consideration required for building an AGI chip is listed along with
the methods for testing and validating it.
</p>
<a href="http://arxiv.org/abs/2012.06338" target="_blank">arXiv:2012.06338</a> [<a href="http://arxiv.org/pdf/2012.06338" target="_blank">pdf</a>]

<h2>Beyond Occam's Razor in System Identification: Double-Descent when Modeling Dynamics. (arXiv:2012.06341v1 [cs.LG])</h2>
<h3>Ant&#xf4;nio H. Ribeiro, Johannes N. Hendriks, Adrian G. Wills, Thomas B. Sch&#xf6;n</h3>
<p>System identification aims to build models of dynamical systems from data.
Traditionally, choosing the model requires the designer to balance between two
goals of conflicting nature; the model must be rich enough to capture the
system dynamics, but not so flexible that it learns spurious random effects
from the dataset. It is typically observed that model validation performance
follows a U-shaped curve as the model complexity increases. Recent developments
in machine learning and statistics, however, have observed situations where a
"double-descent" curve subsumes this U-shaped model-performance curve. With a
second decrease in performance occurring beyond the point where the model has
reached the capacity of interpolating - i.e., (near) perfectly fitting - the
training data. To the best of our knowledge, however, such phenomena have not
been studied within the context of the identification of dynamic systems. The
present paper aims to answer the question: "Can such a phenomenon also be
observed when estimating parameters of dynamic systems?" We show the answer is
yes, verifying such behavior experimentally both for artificially generated and
real-world datasets.
</p>
<a href="http://arxiv.org/abs/2012.06341" target="_blank">arXiv:2012.06341</a> [<a href="http://arxiv.org/pdf/2012.06341" target="_blank">pdf</a>]

<h2>Learning from Survey Propagation: a Neural Network for MAX-E-$3$-SAT. (arXiv:2012.06344v1 [cs.AI])</h2>
<h3>Raffaele Marino</h3>
<p>Many natural optimization problems are NP-hard, which implies that they are
probably hard to solve exactly in the worst-case. However, in practice, it
suffices to get reasonably good solutions for all (or even most) instances.
This paper presents a new algorithm for computing approximate solution in
${\Theta(N})$ for the MAX-E-$3$-SAT problem by using deep learning methodology.
This methodology allows us to create a learning algorithm able to fix Boolean
variables by using local information obtained by the Survey Propagation
algorithm. By performing an accurate analysis, on random CNF instances of the
MAX-E-$3$-SAT with several Boolean variables, we show that this new algorithm,
avoiding any decimation strategy, can build assignments better than a random
one, even if the convergence of the messages is not found. Although this
algorithm is not competitive with state-of-the-art MAX-SAT solvers, it can
solve substantially larger and more difficult problems than it ever saw during
training.
</p>
<a href="http://arxiv.org/abs/2012.06344" target="_blank">arXiv:2012.06344</a> [<a href="http://arxiv.org/pdf/2012.06344" target="_blank">pdf</a>]

<h2>Probabilistic Iterative LQR for Short Time Horizon MPC. (arXiv:2012.06349v1 [cs.RO])</h2>
<h3>Teguh Santoso Lembono, Sylvain Calinon</h3>
<p>Optimal control is often used in robotics for planning a trajectory to
achieve some desired behavior, as expressed by the cost function. Most works in
optimal control focus on finding a single optimal trajectory, which is then
typically tracked by another controller. In this work, we instead consider
trajectory distribution as the solution of an optimal control problem,
resulting in better tracking performance and a more stable controller. A
Gaussian distribution is first obtained from an iterative Linear Quadratic
Regulator (iLQR) solver. A short horizon Model Predictive Control (MPC) is then
used to track this distribution. We show that tracking a distribution is more
cost-efficient and robust as compared to tracking the mean or using iLQR
feedback control. The proposed method is validated with kinematic control of
7-DoF Panda manipulator and dynamic control of 6-DoF quadcopter in simulation.
</p>
<a href="http://arxiv.org/abs/2012.06349" target="_blank">arXiv:2012.06349</a> [<a href="http://arxiv.org/pdf/2012.06349" target="_blank">pdf</a>]

<h2>Cyclopean Geometry of Binocular Vision. (arXiv:2012.06363v1 [cs.CV])</h2>
<h3>Miles Hansard, Radu Horaud</h3>
<p>The geometry of binocular projection is analyzed, with reference to the
primate visual system. In particular, the effects of coordinated eye movements
on the retinal images are investigated. An appropriate oculomotor
parameterization is defined, and is shown to complement the classical version
and vergence angles. The midline horopter is identified, and subsequently used
to construct the epipolar geometry of the system. It is shown that the
Essential matrix can be obtained by combining the epipoles with the projection
of the midline horopter. A local model of the scene is adopted, in which depth
is measured relative to a plane containing the fixation point. The binocular
disparity field is given a symmetric parameterization, in which the unknown
scene-depths determine the location of corresponding image-features. The
resulting Cyclopean depth-map can be combined with the estimated oculomotor
parameters, to produce a local representation of the scene. The recovery of
visual direction and depth from retinal images is discussed, with reference to
the relevant psychophysical and neurophysiological literature.
</p>
<a href="http://arxiv.org/abs/2012.06363" target="_blank">arXiv:2012.06363</a> [<a href="http://arxiv.org/pdf/2012.06363" target="_blank">pdf</a>]

<h2>Feature Selection Based on Sparse Neural Network Layer with Normalizing Constraints. (arXiv:2012.06365v1 [cs.LG])</h2>
<h3>Peter Bugata, Peter Drotar</h3>
<p>Feature selection is important step in machine learning since it has shown to
improve prediction accuracy while depressing the curse of dimensionality of
high dimensional data. The neural networks have experienced tremendous success
in solving many nonlinear learning problems. Here, we propose new
neural-network based feature selection approach that introduces two constrains,
the satisfying of which leads to sparse FS layer. We have performed extensive
experiments on synthetic and real world data to evaluate performance of the
proposed FS. In experiments we focus on the high dimension, low sample size
data since those represent the main challenge for feature selection. The
results confirm that proposed Feature Selection Based on Sparse Neural Network
Layer with Normalizing Constraints (SNEL-FS) is able to select the important
features and yields superior performance compared to other conventional FS
methods.
</p>
<a href="http://arxiv.org/abs/2012.06365" target="_blank">arXiv:2012.06365</a> [<a href="http://arxiv.org/pdf/2012.06365" target="_blank">pdf</a>]

<h2>Hardware Beyond Backpropagation: a Photonic Co-Processor for Direct Feedback Alignment. (arXiv:2012.06373v1 [cs.LG])</h2>
<h3>Julien Launay, Iacopo Poli, Kilian M&#xfc;ller, Gustave Pariente, Igor Carron, Laurent Daudet, Florent Krzakala, Sylvain Gigan</h3>
<p>The scaling hypothesis motivates the expansion of models past trillions of
parameters as a path towards better performance. Recent significant
developments, such as GPT-3, have been driven by this conjecture. However, as
models scale-up, training them efficiently with backpropagation becomes
difficult. Because model, pipeline, and data parallelism distribute parameters
and gradients over compute nodes, communication is challenging to orchestrate:
this is a bottleneck to further scaling. In this work, we argue that
alternative training methods can mitigate these issues, and can inform the
design of extreme-scale training hardware. Indeed, using a synaptically
asymmetric method with a parallelizable backward pass, such as Direct Feedback
Alignement, communication needs are drastically reduced. We present a photonic
accelerator for Direct Feedback Alignment, able to compute random projections
with trillions of parameters. We demonstrate our system on benchmark tasks,
using both fully-connected and graph convolutional networks. Our hardware is
the first architecture-agnostic photonic co-processor for training neural
networks. This is a significant step towards building scalable hardware, able
to go beyond backpropagation, and opening new avenues for deep learning.
</p>
<a href="http://arxiv.org/abs/2012.06373" target="_blank">arXiv:2012.06373</a> [<a href="http://arxiv.org/pdf/2012.06373" target="_blank">pdf</a>]

<h2>Nonlinear Distribution Regression for Remote Sensing Applications. (arXiv:2012.06377v1 [cs.LG])</h2>
<h3>Jose E. Adsuara, Adri&#xe1;n P&#xe9;rez-Suay, Jordi Mu&#xf1;oz-Mar&#xed;, Anna Mateo-Sanchis, Maria Piles, Gustau Camps-Valls</h3>
<p>In many remote sensing applications one wants to estimate variables or
parameters of interest from observations. When the target variable is available
at a resolution that matches the remote sensing observations, standard
algorithms such as neural networks, random forests or Gaussian processes are
readily available to relate the two. However, we often encounter situations
where the target variable is only available at the group level, i.e.
collectively associated to a number of remotely sensed observations. This
problem setting is known in statistics and machine learning as {\em multiple
instance learning} or {\em distribution regression}. This paper introduces a
nonlinear (kernel-based) method for distribution regression that solves the
previous problems without making any assumption on the statistics of the
grouped data. The presented formulation considers distribution embeddings in
reproducing kernel Hilbert spaces, and performs standard least squares
regression with the empirical means therein. A flexible version to deal with
multisource data of different dimensionality and sample sizes is also presented
and evaluated. It allows working with the native spatial resolution of each
sensor, avoiding the need of match-up procedures. Noting the large
computational cost of the approach, we introduce an efficient version via
random Fourier features to cope with millions of points and groups.
</p>
<a href="http://arxiv.org/abs/2012.06377" target="_blank">arXiv:2012.06377</a> [<a href="http://arxiv.org/pdf/2012.06377" target="_blank">pdf</a>]

<h2>Parallelized Rate-Distortion Optimized Quantization Using Deep Learning. (arXiv:2012.06380v1 [cs.LG])</h2>
<h3>Dana Kianfar, Auke Wiggers, Amir Said, Reza Pourreza, Taco Cohen</h3>
<p>Rate-Distortion Optimized Quantization (RDOQ) has played an important role in
the coding performance of recent video compression standards such as H.264/AVC,
H.265/HEVC, VP9 and AV1. This scheme yields significant reductions in bit-rate
at the expense of relatively small increases in distortion. Typically, RDOQ
algorithms are prohibitively expensive to implement on real-time hardware
encoders due to their sequential nature and their need to frequently obtain
entropy coding costs. This work addresses this limitation using a neural
network-based approach, which learns to trade-off rate and distortion during
offline supervised training. As these networks are based solely on standard
arithmetic operations that can be executed on existing neural network hardware,
no additional area-on-chip needs to be reserved for dedicated RDOQ circuitry.
We train two classes of neural networks, a fully-convolutional network and an
auto-regressive network, and evaluate each as a post-quantization step designed
to refine cheap quantization schemes such as scalar quantization (SQ). Both
network architectures are designed to have a low computational overhead. After
training they are integrated into the HM 16.20 implementation of HEVC, and
their video coding performance is evaluated on a subset of the H.266/VVC SDR
common test sequences. Comparisons are made to RDOQ and SQ implementations in
HM 16.20. Our method achieves 1.64% BD-rate savings on luminosity compared to
the HM SQ anchor, and on average reaches 45% of the performance of the
iterative HM RDOQ algorithm.
</p>
<a href="http://arxiv.org/abs/2012.06380" target="_blank">arXiv:2012.06380</a> [<a href="http://arxiv.org/pdf/2012.06380" target="_blank">pdf</a>]

<h2>An AI-Assisted Design Method for Topology Optimization Without Pre-Optimized Training Data. (arXiv:2012.06384v1 [cs.LG])</h2>
<h3>Alex Halle, L. Flavio Campanile, Alexander Hasse</h3>
<p>In this publication, an AI-assisted design method based on topology
optimization is presented, which is able to obtain optimized designs in a
direct way, without iterative optimum search. The optimized designs are
provided by an artificial neural network, the predictor, on the basis of
boundary conditions and degree of filling (the volume percentage filled by
material) as input data. In the training phase, geometries generated on the
basis of random input data are evaluated with respect to given criteria and the
results of those evaluations flow into an objective function which is minimized
by adapting the predictor's parameters. Other than in state-of-the-art
procedures, no pre-optimized geometries are used during training.

After the training is completed, the presented AI-assisted design procedure
supplies geometries which are similar to the ones generated by conventional
topology optimizers, but requires a small fraction of the computational effort
required by those algorithms.
</p>
<a href="http://arxiv.org/abs/2012.06384" target="_blank">arXiv:2012.06384</a> [<a href="http://arxiv.org/pdf/2012.06384" target="_blank">pdf</a>]

<h2>RENATA: REpreseNtation And Training Alteration for Bias Mitigation. (arXiv:2012.06387v1 [cs.LG])</h2>
<h3>William Paul, Armin Hadzic, Neil Joshi, Phil Burlina</h3>
<p>We propose a novel method for enforcing AI fairness with respect to protected
or sensitive factors. This method uses a dual strategy performing Training And
Representation Alteration (RENATA) for mitigation of two of the most prominent
causes of AI bias, including: a) the use of representation learning alteration
via adversarial independence, to suppress the bias-inducing dependence of the
data representation from protected factors; and b) training set alteration via
intelligent augmentation, to address bias-causing data imbalance, by using
generative models that allow fine control of sensitive factors related to
underrepresented populations. When testing our methods on image analytics,
experiments demonstrate that RENATA significantly or fully debiases baseline
models while outperforming competing debiasing methods, e.g., with (% overall
accuracy, % accuracy gap) of (78.75, 0.5) vs. baseline method's (71.75, 10.5)
for EyePACS, and (73.71, 11.82) vs. the (69.08, 21.65) baseline for CelebA. As
an additional contribution, recognizing certain limitations in current metrics
used for assessing debiasing performance, this study proposes novel conjunctive
debiasing metrics. Our experiments also demonstrate the ability of these novel
metrics in assessing the Pareto efficiency of the proposed methods.
</p>
<a href="http://arxiv.org/abs/2012.06387" target="_blank">arXiv:2012.06387</a> [<a href="http://arxiv.org/pdf/2012.06387" target="_blank">pdf</a>]

<h2>Closeness and Uncertainty Aware Adversarial Examples Detection in Adversarial Machine Learning. (arXiv:2012.06390v1 [cs.LG])</h2>
<h3>Omer Faruk Tuna, Ferhat Ozgur Catak, M. Taner Eskil</h3>
<p>Deep neural network (DNN) architectures are considered to be robust to random
perturbations. Nevertheless, it was shown that they could be severely
vulnerable to slight but carefully crafted perturbations of the input, which
are termed as adversarial samples. In recent years, numerous studies have been
conducted to increase the reliability of DNN models by distinguishing
adversarial samples from regular inputs. In this work, we explore and assess
the usage of 2 different groups of metrics in detecting adversarial samples:
the ones which are based on the uncertainty estimation using Monte-Carlo
Dropout Sampling and the ones which are based on closeness measures in the
subspace of deep features extracted by the model. We also introduce a new
feature for adversarial detection, and we show that the performances of all
these metrics heavily depend on the strength of the attack being used.
</p>
<a href="http://arxiv.org/abs/2012.06390" target="_blank">arXiv:2012.06390</a> [<a href="http://arxiv.org/pdf/2012.06390" target="_blank">pdf</a>]

<h2>Learning physically consistent mathematical models from data using group sparsity. (arXiv:2012.06391v1 [cs.LG])</h2>
<h3>Suryanarayana Maddu, Bevan L. Cheeseman, Christian L. M&#xfc;ller, Ivo F. Sbalzarini</h3>
<p>We propose a statistical learning framework based on group-sparse regression
that can be used to 1) enforce conservation laws, 2) ensure model equivalence,
and 3) guarantee symmetries when learning or inferring differential-equation
models from measurement data. Directly learning $\textit{interpretable}$
mathematical models from data has emerged as a valuable modeling approach.
However, in areas like biology, high noise levels, sensor-induced correlations,
and strong inter-system variability can render data-driven models nonsensical
or physically inconsistent without additional constraints on the model
structure. Hence, it is important to leverage $\textit{prior}$ knowledge from
physical principles to learn "biologically plausible and physically consistent"
models rather than models that simply fit the data best. We present a novel
group Iterative Hard Thresholding (gIHT) algorithm and use stability selection
to infer physically consistent models with minimal parameter tuning. We show
several applications from systems biology that demonstrate the benefits of
enforcing $\textit{priors}$ in data-driven modeling.
</p>
<a href="http://arxiv.org/abs/2012.06391" target="_blank">arXiv:2012.06391</a> [<a href="http://arxiv.org/pdf/2012.06391" target="_blank">pdf</a>]

<h2>Spatial Temporal Transformer Network for Skeleton-based Action Recognition. (arXiv:2012.06399v1 [cs.CV])</h2>
<h3>Chiara Plizzari, Marco Cannici, Matteo Matteucci</h3>
<p>Skeleton-based human action recognition has achieved a great interest in
recent years, as skeleton data has been demonstrated to be robust to
illumination changes, body scales, dynamic camera views, and complex
background. Nevertheless, an effective encoding of the latent information
underlying the 3D skeleton is still an open problem. In this work, we propose a
novel Spatial-Temporal Transformer network (ST-TR) which models dependencies
between joints using the Transformer self-attention operator. In our ST-TR
model, a Spatial Self-Attention module (SSA) is used to understand intra-frame
interactions between different body parts, and a Temporal Self-Attention module
(TSA) to model inter-frame correlations. The two are combined in a two-stream
network which outperforms state-of-the-art models using the same input data on
both NTU-RGB+D 60 and NTU-RGB+D 120.
</p>
<a href="http://arxiv.org/abs/2012.06399" target="_blank">arXiv:2012.06399</a> [<a href="http://arxiv.org/pdf/2012.06399" target="_blank">pdf</a>]

<h2>Random Projections for Adversarial Attack Detection. (arXiv:2012.06405v1 [cs.CV])</h2>
<h3>Nathan Drenkow, Neil Fendley, Philippe Burlina</h3>
<p>Whilst adversarial attack detection has received considerable attention, it
remains a fundamentally challenging problem from two perspectives. First, while
threat models can be well-defined, attacker strategies may still vary widely
within those constraints. Therefore, detection should be considered as an
open-set problem, standing in contrast to most current detection strategies.
These methods take a closed-set view and train binary detectors, thus biasing
detection toward attacks seen during detector training. Second, information is
limited at test time and confounded by nuisance factors including the label and
underlying content of the image. Many of the current high-performing techniques
use training sets for dealing with some of these issues, but can be limited by
the overall size and diversity of those sets during the detection step. We
address these challenges via a novel strategy based on random subspace
analysis. We present a technique that makes use of special properties of random
projections, whereby we can characterize the behavior of clean and adversarial
examples across a diverse set of subspaces. We then leverage the
self-consistency (or inconsistency) of model activations to discern clean from
adversarial examples. Performance evaluation demonstrates that our technique
outperforms ($&gt;0.92$ AUC) competing state of the art (SOTA) attack strategies,
while remaining truly agnostic to the attack method itself. It also requires
significantly less training data, composed only of clean examples, when
compared to competing SOTA methods, which achieve only chance performance, when
evaluated in a more rigorous testing scenario.
</p>
<a href="http://arxiv.org/abs/2012.06405" target="_blank">arXiv:2012.06405</a> [<a href="http://arxiv.org/pdf/2012.06405" target="_blank">pdf</a>]

<h2>Learning How to Trade-Off Safety with Agility Using Deep Covariance Estimation for Perception Driven UAV Motion Planning. (arXiv:2012.06410v1 [cs.RO])</h2>
<h3>Onur Akgun, Kamil Canberk Atik, Mustafa Erdem, Mehmetcan Kaymaz, Bugrahan Yamak, N. Kemal Ure</h3>
<p>We investigate how to utilize predictive models for selecting appropriate
motion planning strategies based on perception uncertainty estimation for agile
unmanned aerial vehicle (UAV) navigation tasks. Although there are variety of
motion planning and perception algorithms for such tasks, the impact of
perception uncertainty is not explicitly handled in many of the current motion
algorithms, which leads to performance loss in real-life scenarios where the
measurement are often noisy due to external disturbances. We develop a novel
framework for embedding perception uncertainty to high level motion planning
management, in order to select the best available motion planning approach for
the currently estimated perception uncertainty. We estimate the uncertainty in
visual inputs using a deep neural network (CovNet) that explicitly predicts the
covariance of the current measurements. Next, we train a high level machine
learning model for predicting the lowest cost motion planning algorithm given
the current estimate of covariance as well as the UAV states. We demonstrate on
both real-life data and drone racing simulations that our approach, named
uncertainty driven motion planning switcher (UDS) yields the safest and fastest
trajectories among compared alternatives. Furthermore, we show that the
developed approach learns how to trade-off safety with agility by switching to
motion planners that leads to more agile trajectories when the estimated
covariance is high and vice versa.
</p>
<a href="http://arxiv.org/abs/2012.06410" target="_blank">arXiv:2012.06410</a> [<a href="http://arxiv.org/pdf/2012.06410" target="_blank">pdf</a>]

<h2>A Vision-based Sensing Approach for a Spherical Soft Robotic Arm. (arXiv:2012.06413v1 [cs.RO])</h2>
<h3>Matthias Hofer, Carmelo Sferrazza, Raffaello D&#x27;Andrea</h3>
<p>Sensory feedback is essential for the control of soft robotic systems and to
enable deployment in a variety of different tasks. Proprioception refers to
sensing the robot's own state and is of crucial importance in order to deploy
soft robotic systems outside of laboratory environments, i.e. where no external
sensing, such as motion capture systems, is available.

A vision-based sensing approach for a soft robotic arm made from fabric is
presented, leveraging the high-resolution sensory feedback provided by cameras.
No mechanical interaction between the sensor and the soft structure is required
and consequently, the compliance of the soft system is preserved. The
integration of a camera into an inflatable, fabric-based bellow actuator is
discussed. Three actuators, each featuring an integrated camera, are used to
control the spherical robotic arm and simultaneously provide sensory feedback
of the two rotational degrees of freedom. A convolutional neural network
architecture predicts the two angles describing the robot's orientation from
the camera images. Ground truth data is provided by a motion capture system
during the training phase of the supervised learning approach and its
evaluation thereafter.

The camera-based sensing approach is able to provide estimates of the
orientation in real-time with an accuracy of about one degree. The reliability
of the sensing approach is demonstrated by using the sensory feedback to
control the orientation of the robotic arm in closed-loop.
</p>
<a href="http://arxiv.org/abs/2012.06413" target="_blank">arXiv:2012.06413</a> [<a href="http://arxiv.org/pdf/2012.06413" target="_blank">pdf</a>]

<h2>A new automatic approach to seed image analysis: From acquisition to segmentation. (arXiv:2012.06414v1 [cs.CV])</h2>
<h3>A.M.P.G. Vale, M. Ucchesu, C. Di Ruberto, A. Loddo, J.M. Soares, G.Bacchetta</h3>
<p>Image Analysis offers a new tool for classifying vascular plant species based
on the morphological and colorimetric features of the seeds, and has made
significant contributions in systematic studies. However, in order to extract
the morphological and colorimetric features, it is necessary to segment the
image containing the samples to be analysed. This stage represents one of the
most challenging steps in image processing, as it is difficult to separate
uniform and homogeneous objects from the background. In this paper, we present
a new, open source plugin for the automatic segmentation of an image of a seed
sample. This plugin was written in Java to allow it to work with ImageJ open
source software. The new plugin was tested on a total of 3,386 seed samples
from 120 species belonging to the Fabaceae family. Digital images were acquired
using a flatbed scanner. In order to test the efficacy of this approach in
terms of identifying the edges of objects and separating them from the
background, each sample was scanned using four different hues of blue for the
background, and a total of 480 digital images were elaborated. The performance
of the new plugin was compared with a method based on double image acquisition
(with a black and white background) using the same seed samples, in which
images were manually segmented using the Core ImageJ plugin. The results showed
that the new plugin was able to segment all of the digital images without
generating any object detection errors. In addition, the new plugin was able to
segment images within an average of 0.02 s, while the average time for
execution with the manual method was 63 s. This new open source plugin is
proven to be able to work on a single image, and to be highly efficient in
terms of time and segmentation when working with large numbers of images and a
wide diversity of shapes.
</p>
<a href="http://arxiv.org/abs/2012.06414" target="_blank">arXiv:2012.06414</a> [<a href="http://arxiv.org/pdf/2012.06414" target="_blank">pdf</a>]

<h2>A Multi-task Joint Framework for Real-time Person Search. (arXiv:2012.06418v1 [cs.CV])</h2>
<h3>Ye Li, Kangning Yin, Jie Liang, Chunyu Wang, Guangqiang Yin</h3>
<p>Person search generally involves three important parts: person detection,
feature extraction and identity comparison. However, person search integrating
detection, extraction and comparison has the following drawbacks. Firstly, the
accuracy of detection will affect the accuracy of comparison. Secondly, it is
difficult to achieve real-time in real-world applications. To solve these
problems, we propose a Multi-task Joint Framework for real-time person search
(MJF), which optimizes the person detection, feature extraction and identity
comparison respectively. For the person detection module, we proposed the
YOLOv5-GS model, which is trained with person dataset. It combines the
advantages of the Ghostnet and the Squeeze-and-Excitation (SE) block, and
improves the speed and accuracy. For the feature extraction module, we design
the Model Adaptation Architecture (MAA), which could select different network
according to the number of people. It could balance the relationship between
accuracy and speed. For identity comparison, we propose a Three Dimension (3D)
Pooled Table and a matching strategy to improve identification accuracy. On the
condition of 1920*1080 resolution video and 500 IDs table, the identification
rate (IR) and frames per second (FPS) achieved by our method could reach 93.6%
and 25.7,
</p>
<a href="http://arxiv.org/abs/2012.06418" target="_blank">arXiv:2012.06418</a> [<a href="http://arxiv.org/pdf/2012.06418" target="_blank">pdf</a>]

<h2>When is Memorization of Irrelevant Training Data Necessary for High-Accuracy Learning?. (arXiv:2012.06421v1 [cs.LG])</h2>
<h3>Gavin Brown, Mark Bun, Vitaly Feldman, Adam Smith, Kunal Talwar</h3>
<p>Modern machine learning models are complex and frequently encode surprising
amounts of information about individual inputs. In extreme cases, complex
models appear to memorize entire input examples, including seemingly irrelevant
information (social security numbers from text, for example). In this paper, we
aim to understand whether this sort of memorization is necessary for accurate
learning. We describe natural prediction problems in which every sufficiently
accurate training algorithm must encode, in the prediction model, essentially
all the information about a large subset of its training examples. This remains
true even when the examples are high-dimensional and have entropy much higher
than the sample size, and even when most of that information is ultimately
irrelevant to the task at hand. Further, our results do not depend on the
training algorithm or the class of models used for learning.

Our problems are simple and fairly natural variants of the next-symbol
prediction and the cluster labeling tasks. These tasks can be seen as
abstractions of image- and text-related prediction problems. To establish our
results, we reduce from a family of one-way communication problems for which we
prove new information complexity lower bounds.
</p>
<a href="http://arxiv.org/abs/2012.06421" target="_blank">arXiv:2012.06421</a> [<a href="http://arxiv.org/pdf/2012.06421" target="_blank">pdf</a>]

<h2>Imitation-Based Active Camera Control with Deep Convolutional Neural Network. (arXiv:2012.06428v1 [cs.CV])</h2>
<h3>Christos Kyrkou</h3>
<p>The increasing need for automated visual monitoring and control for
applications such as smart camera surveillance, traffic monitoring, and
intelligent environments, necessitates the improvement of methods for visual
active monitoring. Traditionally, the active monitoring task has been handled
through a pipeline of modules such as detection, filtering, and control. In
this paper we frame active visual monitoring as an imitation learning problem
to be solved in a supervised manner using deep learning, to go directly from
visual information to camera movement in order to provide a satisfactory
solution by combining computer vision and control. A deep convolutional neural
network is trained end-to-end as the camera controller that learns the entire
processing pipeline needed to control a camera to follow multiple targets and
also estimate their density from a single image. Experimental results indicate
that the proposed solution is robust to varying conditions and is able to
achieve better monitoring performance both in terms of number of targets
monitored as well as in monitoring time than traditional approaches, while
reaching up to 25 FPS. Thus making it a practical and affordable solution for
multi-target active monitoring in surveillance and smart-environment
applications.
</p>
<a href="http://arxiv.org/abs/2012.06428" target="_blank">arXiv:2012.06428</a> [<a href="http://arxiv.org/pdf/2012.06428" target="_blank">pdf</a>]

<h2>Data Appraisal Without Data Sharing. (arXiv:2012.06430v1 [cs.LG])</h2>
<h3>Mimee Xu, Laurens van der Maaten, Awni Hannun</h3>
<p>One of the most effective approaches to improving the performance of a
machine-learning model is to acquire additional training data. To do so, a
model owner may seek to acquire relevant training data from a data owner.
Before procuring the data, the model owner needs to appraise the data. However,
the data owner generally does not want to share the data until after an
agreement is reached. The resulting Catch-22 prevents efficient data markets
from forming. To address this problem, we develop data appraisal methods that
do not require data sharing by using secure multi-party computation.
Specifically, we study methods that: (1) compute parameter gradient norms, (2)
perform model fine-tuning, and (3) compute influence functions. Our experiments
show that influence functions provide an appealing trade-off between
high-quality appraisal and required computation.
</p>
<a href="http://arxiv.org/abs/2012.06430" target="_blank">arXiv:2012.06430</a> [<a href="http://arxiv.org/pdf/2012.06430" target="_blank">pdf</a>]

<h2>Iso-Points: Optimizing Neural Implicit Surfaces with Hybrid Representations. (arXiv:2012.06434v1 [cs.CV])</h2>
<h3>Wang Yifan, Shihao Wu, Cengiz Oztireli, Olga Sorkine-Hornung</h3>
<p>Neural implicit functions have emerged as a powerful representation for
surfaces in 3D. Such a function can encode a high quality surface with
intricate details into the parameters of a deep neural network. However,
optimizing for the parameters for accurate and robust reconstructions remains a
challenge, especially when the input data is noisy or incomplete. In this work,
we develop a hybrid neural surface representation that allows us to impose
geometry-aware sampling and regularization, which significantly improves the
fidelity of reconstructions. We propose to use \emph{iso-points} as an explicit
representation for a neural implicit function. These points are computed and
updated on-the-fly during training to capture important geometric features and
impose geometric constraints on the optimization. We demonstrate that our
method can be adopted to improve state-of-the-art techniques for reconstructing
neural implicit surfaces from multi-view images or point clouds. Quantitative
and qualitative evaluations show that, compared with existing sampling and
optimization methods, our approach allows faster convergence, better
generalization, and accurate recovery of details and topology.
</p>
<a href="http://arxiv.org/abs/2012.06434" target="_blank">arXiv:2012.06434</a> [<a href="http://arxiv.org/pdf/2012.06434" target="_blank">pdf</a>]

<h2>D2-Net: Weakly-Supervised Action Localization via Discriminative Embeddings and Denoised Activations. (arXiv:2012.06440v1 [cs.CV])</h2>
<h3>Sanath Narayan, Hisham Cholakkal, Munawar Hayat, Fahad Shahbaz Khan, Ming-Hsuan Yang, Ling Shao</h3>
<p>This work proposes a weakly-supervised temporal action localization
framework, called D2-Net, which strives to temporally localize actions using
video-level supervision. Our main contribution is the introduction of a novel
loss formulation, which jointly enhances the discriminability of latent
embeddings and robustness of the output temporal class activations with respect
to foreground-background noise caused by weak supervision. The proposed
formulation comprises a discriminative and a denoising loss term for enhancing
temporal action localization. The discriminative term incorporates a
classification loss and utilizes a top-down attention mechanism to enhance the
separability of latent foreground-background embeddings. The denoising loss
term explicitly addresses the foreground-background noise in class activations
by simultaneously maximizing intra-video and inter-video mutual information
using a bottom-up attention mechanism. As a result, activations in the
foreground regions are emphasized whereas those in the background regions are
suppressed, thereby leading to more robust predictions. Comprehensive
experiments are performed on two benchmarks: THUMOS14 and ActivityNet1.2. Our
D2-Net performs favorably in comparison to the existing methods on both
datasets, achieving gains as high as 3.6% in terms of mean average precision on
THUMOS14.
</p>
<a href="http://arxiv.org/abs/2012.06440" target="_blank">arXiv:2012.06440</a> [<a href="http://arxiv.org/pdf/2012.06440" target="_blank">pdf</a>]

<h2>Relighting Images in the Wild with a Self-Supervised Siamese Auto-Encoder. (arXiv:2012.06444v1 [cs.CV])</h2>
<h3>Yang Liu, Alexandros Neophytou, Sunando Sengupta, Eric Sommerlade</h3>
<p>We propose a self-supervised method for image relighting of single view
images in the wild. The method is based on an auto-encoder which deconstructs
an image into two separate encodings, relating to the scene illumination and
content, respectively. In order to disentangle this embedding information
without supervision, we exploit the assumption that some augmentation
operations do not affect the image content and only affect the direction of the
light. A novel loss function, called spherical harmonic loss, is introduced
that forces the illumination embedding to convert to a spherical harmonic
vector. We train our model on large-scale datasets such as Youtube 8M and
CelebA. Our experiments show that our method can correctly estimate scene
illumination and realistically re-light input images, without any supervision
or a prior shape model. Compared to supervised methods, our approach has
similar performance and avoids common lighting artifacts.
</p>
<a href="http://arxiv.org/abs/2012.06444" target="_blank">arXiv:2012.06444</a> [<a href="http://arxiv.org/pdf/2012.06444" target="_blank">pdf</a>]

<h2>A New Neural Network Architecture Invariant to the Action of Symmetry Subgroups. (arXiv:2012.06452v1 [cs.LG])</h2>
<h3>Piotr Kicki, Mete Ozay, Piotr Skrzypczy&#x144;ski</h3>
<p>We propose a computationally efficient $G$-invariant neural network that
approximates functions invariant to the action of a given permutation subgroup
$G \leq S_n$ of the symmetric group on input data. The key element of the
proposed network architecture is a new $G$-invariant transformation module,
which produces a $G$-invariant latent representation of the input data.
Theoretical considerations are supported by numerical experiments, which
demonstrate the effectiveness and strong generalization properties of the
proposed method in comparison to other $G$-invariant neural networks.
</p>
<a href="http://arxiv.org/abs/2012.06452" target="_blank">arXiv:2012.06452</a> [<a href="http://arxiv.org/pdf/2012.06452" target="_blank">pdf</a>]

<h2>Cyclic orthogonal convolutions for long-range integration of features. (arXiv:2012.06462v1 [cs.CV])</h2>
<h3>Federica Freddi, Jezabel R Garcia, Michael Bromberg, Sepehr Jalali, Da-Shan Shiu, Alvin Chua, Alberto Bernacchia</h3>
<p>In Convolutional Neural Networks (CNNs) information flows across a small
neighbourhood of each pixel of an image, preventing long-range integration of
features before reaching deep layers in the network. We propose a novel
architecture that allows flexible information flow between features $z$ and
locations $(x,y)$ across the entire image with a small number of layers. This
architecture uses a cycle of three orthogonal convolutions, not only in $(x,y)$
coordinates, but also in $(x,z)$ and $(y,z)$ coordinates. We stack a sequence
of such cycles to obtain our deep network, named CycleNet. As this only
requires a permutation of the axes of a standard convolution, its performance
can be directly compared to a CNN. Our model obtains competitive results at
image classification on CIFAR-10 and ImageNet datasets, when compared to CNNs
of similar size. We hypothesise that long-range integration favours recognition
of objects by shape rather than texture, and we show that CycleNet transfers
better than CNNs to stylised images. On the Pathfinder challenge, where
integration of distant features is crucial, CycleNet outperforms CNNs by a
large margin. We also show that even when employing a small convolutional
kernel, the size of receptive fields of CycleNet reaches its maximum after one
cycle, while conventional CNNs require a large number of layers.
</p>
<a href="http://arxiv.org/abs/2012.06462" target="_blank">arXiv:2012.06462</a> [<a href="http://arxiv.org/pdf/2012.06462" target="_blank">pdf</a>]

<h2>DILIE: Deep Internal Learning for Image Enhancement. (arXiv:2012.06469v1 [cs.CV])</h2>
<h3>Indra Deep Mastan, Shanmuganathan Raman</h3>
<p>We consider the generic deep image enhancement problem where an input image
is transformed into a perceptually better-looking image. Recent methods for
image enhancement consider the problem by performing style transfer and image
restoration. The methods mostly fall into two categories: training data-based
and training data-independent (deep internal learning methods). We perform
image enhancement in the deep internal learning framework. Our Deep Internal
Learning for Image Enhancement framework enhances content features and style
features and uses contextual content loss for preserving image context in the
enhanced image. We show results on both hazy and noisy image enhancement. To
validate the results, we use structure similarity and perceptual error, which
is efficient in measuring the unrealistic deformation present in the images. We
show that the proposed framework outperforms the relevant state-of-the-art
works for image enhancement.
</p>
<a href="http://arxiv.org/abs/2012.06469" target="_blank">arXiv:2012.06469</a> [<a href="http://arxiv.org/pdf/2012.06469" target="_blank">pdf</a>]

<h2>Prediction of Hemolysis Tendency of Peptides using a Reliable Evaluation Method. (arXiv:2012.06470v1 [cs.LG])</h2>
<h3>Ali Raza, Hafiz Saud Arshad</h3>
<p>There are numerous peptides discovered through past decades, which exhibit
antimicrobial and anti-cancerous tendencies. Due to these reasons, peptides are
supposed to be sound therapeutic candidates. Some peptides can pose low
metabolic stability, high toxicity and high hemolity of peptides. This
highlights the importance for evaluating hemolytic tendencies and toxicity of
peptides, before using them for therapeutics. Traditional methods for
evaluation of toxicity of peptides can be time-consuming and costly. In this
study, we have extracted peptides data (Hemo-DB) from Database of Antimicrobial
Activity and Structure of Peptides (DBAASP) based on certain hemolity criteria
and we present a machine learning based method for prediction of hemolytic
tendencies of peptides (i.e. Hemolytic or Non-Hemolytic). Our model offers
significant improvement on hemolity prediction benchmarks. we also propose a
reliable clustering-based train-tests splitting method which ensures that no
peptide in train set is more than 40% similar to any peptide in test set. Using
this train-test split, we can get reliable estimated of expected model
performance on unseen data distribution or newly discovered peptides. Our model
tests 0.9986 AUC-ROC (Area Under Receiver Operating Curve) and 97.79% Accuracy
on test set of Hemo-DB using traditional random train-test splitting method.
Moreover, our model tests AUC-ROC of 0.997 and Accuracy of 97.58% while using
clustering-based train-test data split. Furthermore, we check our model on an
unseen data distribution (at Hemo-PI 3) and we recorded 0.8726 AUC-ROC and
79.5% accuracy. Using the proposed method, potential therapeutic peptides can
be screened, which may further in therapeutics and get reliable predictions for
unseen amino acids distribution of peptides and newly discovered peptides.
</p>
<a href="http://arxiv.org/abs/2012.06470" target="_blank">arXiv:2012.06470</a> [<a href="http://arxiv.org/pdf/2012.06470" target="_blank">pdf</a>]

<h2>Generating Human-Like Movement: A Comparison Between Two Approaches Based on Environmental Features. (arXiv:2012.06474v1 [cs.AI])</h2>
<h3>A. Zonta, S.K. Smit, A.E. Eiben</h3>
<p>Modelling realistic human behaviours in simulation is an ongoing challenge
that resides between several fields like social sciences, philosophy, and
artificial intelligence. Human movement is a special type of behaviour driven
by intent (e.g. to get groceries) and the surrounding environment (e.g.
curiosity to see new interesting places). Services available online and offline
do not normally consider the environment when planning a path, which is
decisive especially on a leisure trip. Two novel algorithms have been presented
to generate human-like trajectories based on environmental features. The
Attraction-Based A* algorithm includes in its computation information from the
environmental features meanwhile, the Feature-Based A* algorithm also injects
information from the real trajectories in its computation. The human-likeness
aspect has been tested by a human expert judging the final generated
trajectories as realistic. This paper presents a comparison between the two
approaches in some key metrics like efficiency, efficacy, and hyper-parameters
sensitivity. We show how, despite generating trajectories that are closer to
the real one according to our predefined metrics, the Feature-Based A*
algorithm fall short in time efficiency compared to the Attraction-Based A*
algorithm, hindering the usability of the model in the real world.
</p>
<a href="http://arxiv.org/abs/2012.06474" target="_blank">arXiv:2012.06474</a> [<a href="http://arxiv.org/pdf/2012.06474" target="_blank">pdf</a>]

<h2>EventHands: Real-Time Neural 3D Hand Reconstruction from an Event Stream. (arXiv:2012.06475v1 [cs.CV])</h2>
<h3>Viktor Rudnev, Vladislav Golyanik, Jiayi Wang, Hans-Peter Seidel, Franziska Mueller, Mohamed Elgharib, Christian Theobalt</h3>
<p>3D hand pose estimation from monocular videos is a long-standing and
challenging problem, which is now seeing a strong upturn. In this work, we
address it for the first time using a single event camera, i.e., an
asynchronous vision sensor reacting on brightness changes. Our EventHands
approach has characteristics previously not demonstrated with a single RGB or
depth camera such as high temporal resolution at low data throughputs and
real-time performance at 1000 Hz. Due to the different data modality of event
cameras compared to classical cameras, existing methods cannot be directly
applied to and re-trained for event streams. We thus design a new neural
approach which accepts a new event stream representation suitable for learning,
which is trained on newly-generated synthetic event streams and can generalise
to real data. Experiments show that EventHands outperforms recent monocular
methods using a colour (or depth) camera in terms of accuracy and its ability
to capture hand motions of unprecedented speed. Our method, the event stream
simulator and the dataset will be made publicly available.
</p>
<a href="http://arxiv.org/abs/2012.06475" target="_blank">arXiv:2012.06475</a> [<a href="http://arxiv.org/pdf/2012.06475" target="_blank">pdf</a>]

<h2>Technical Opinion: From Animal Behaviour to Autonomous Robots. (arXiv:2012.06492v1 [cs.RO])</h2>
<h3>Chinedu Pascal Ezenkwu, Andrew Starkey</h3>
<p>With the rising applications of robots in unstructured real-world
environments, roboticists are increasingly concerned with the problems posed by
the complexity of such environments. One solution to these problems is robot
autonomy. Since nature has already solved the problem of autonomy it can be a
suitable model for developing autonomous robots. This paper presents a concise
review on robot autonomy from the perspective of animal behaviour. It examines
some state-of-the-art techniques as well as suggesting possible research
directions.
</p>
<a href="http://arxiv.org/abs/2012.06492" target="_blank">arXiv:2012.06492</a> [<a href="http://arxiv.org/pdf/2012.06492" target="_blank">pdf</a>]

<h2>Unsupervised deep learning for individualized brain functional network identification. (arXiv:2012.06494v1 [cs.CV])</h2>
<h3>Hongming Li, Yong Fan</h3>
<p>A novel unsupervised deep learning method is developed to identify
individual-specific large scale brain functional networks (FNs) from
resting-state fMRI (rsfMRI) in an end-to-end learning fashion. Our method
leverages deep Encoder-Decoder networks and conventional brain decomposition
models to identify individual-specific FNs in an unsupervised learning
framework and facilitate fast inference for new individuals with one forward
pass of the deep network. Particularly, convolutional neural networks (CNNs)
with an Encoder-Decoder architecture are adopted to identify
individual-specific FNs from rsfMRI data by optimizing their data fitting and
sparsity regularization terms that are commonly used in brain decomposition
models. Moreover, a time-invariant representation learning module is designed
to learn features invariant to temporal orders of time points of rsfMRI data.
The proposed method has been validated based on a large rsfMRI dataset and
experimental results have demonstrated that our method could obtain
individual-specific FNs which are consistent with well-established FNs and are
informative for predicting brain age, indicating that the individual-specific
FNs identified truly captured the underlying variability of individualized
functional neuroanatomy.
</p>
<a href="http://arxiv.org/abs/2012.06494" target="_blank">arXiv:2012.06494</a> [<a href="http://arxiv.org/pdf/2012.06494" target="_blank">pdf</a>]

<h2>DeepObjStyle: Deep Object-based Photo Style Transfer. (arXiv:2012.06498v1 [cs.CV])</h2>
<h3>Indra Deep Mastan, Shanmuganathan Raman</h3>
<p>One of the major challenges of style transfer is the appropriate image
features supervision between the output image and the input (style and content)
images. An efficient strategy would be to define an object map between the
objects of the style and the content images. However, such a mapping is not
well established when there are semantic objects of different types and numbers
in the style and the content images. It also leads to content mismatch in the
style transfer output, which could reduce the visual quality of the results. We
propose an object-based style transfer approach, called DeepObjStyle, for the
style supervision in the training data-independent framework. DeepObjStyle
preserves the semantics of the objects and achieves better style transfer in
the challenging scenario when the style and the content images have a mismatch
of image features. We also perform style transfer of images containing a word
cloud to demonstrate that DeepObjStyle enables an appropriate image features
supervision. We validate the results using quantitative comparisons and user
studies.
</p>
<a href="http://arxiv.org/abs/2012.06498" target="_blank">arXiv:2012.06498</a> [<a href="http://arxiv.org/pdf/2012.06498" target="_blank">pdf</a>]

<h2>Confidence Estimation via Auxiliary Models. (arXiv:2012.06508v1 [cs.CV])</h2>
<h3>Charles Corbi&#xe8;re, Nicolas Thome, Antoine Saporta, Tuan-Hung Vu, Matthieu Cord, Patrick P&#xe9;rez</h3>
<p>Reliably quantifying the confidence of deep neural classifiers is a
challenging yet fundamental requirement for deploying such models in
safety-critical applications. In this paper, we introduce a novel target
criterion for model confidence, namely the true class probability (TCP). We
show that TCP offers better properties for confidence estimation than standard
maximum class probability (MCP). Since the true class is by essence unknown at
test time, we propose to learn TCP criterion from data with an auxiliary model,
introducing a specific learning scheme adapted to this context. We evaluate our
approach on the task of failure prediction and of self-training with
pseudo-labels for domain adaptation, which both necessitate effective
confidence estimates. Extensive experiments are conducted for validating the
relevance of the proposed approach in each task. We study various network
architectures and experiment with small and large datasets for image
classification and semantic segmentation. In every tested benchmark, our
approach outperforms strong baselines.
</p>
<a href="http://arxiv.org/abs/2012.06508" target="_blank">arXiv:2012.06508</a> [<a href="http://arxiv.org/pdf/2012.06508" target="_blank">pdf</a>]

<h2>Objectness-Guided Open Set Visual Search and Closed Set Detection. (arXiv:2012.06509v1 [cs.CV])</h2>
<h3>Nathan Drenkow, Philippe Burlina, Neil Fendley, Kachi Odoemene, Jared Markowitz</h3>
<p>Searching for small objects in large images is currently challenging for deep
learning systems, but is a task with numerous applications including remote
sensing and medical imaging. Thorough scanning of very large images is
computationally expensive, particularly at resolutions sufficient to capture
small objects. The smaller an object of interest, the more likely it is to be
obscured by clutter or otherwise deemed insignificant. We examine these issues
in the context of two complementary problems: closed-set object detection and
open-set target search. First, we present a method for predicting pixel-level
objectness from a low resolution gist image, which we then use to select
regions for subsequent evaluation at high resolution. This approach has the
benefit of not being fixed to a predetermined grid, allowing fewer costly
high-resolution glimpses than existing methods. Second, we propose a novel
strategy for open-set visual search that seeks to find all objects in an image
of the same class as a given target reference image. We interpret both
detection problems through a probabilistic, Bayesian lens, whereby the
objectness maps produced by our method serve as priors in a
maximum-a-posteriori approach to the detection step. We evaluate the end-to-end
performance of both the combination of our patch selection strategy with this
target search approach and the combination of our patch selection strategy with
standard object detection methods. Both our patch selection and target search
approaches are seen to significantly outperform baseline strategies.
</p>
<a href="http://arxiv.org/abs/2012.06509" target="_blank">arXiv:2012.06509</a> [<a href="http://arxiv.org/pdf/2012.06509" target="_blank">pdf</a>]

<h2>String Tightening as a Self-Organizing Phenomenon: Computation of Shortest Homotopic Path, Smooth Path, and Convex Hull. (arXiv:2012.06513v1 [cs.AI])</h2>
<h3>Bonny Banerjee</h3>
<p>The phenomenon of self-organization has been of special interest to the
neural network community for decades. In this paper, we study a variant of the
Self-Organizing Map (SOM) that models the phenomenon of self-organization of
the particles forming a string when the string is tightened from one or both
ends. The proposed variant, called the String Tightening Self-Organizing Neural
Network (STON), can be used to solve certain practical problems, such as
computation of shortest homotopic paths, smoothing paths to avoid sharp turns,
and computation of convex hull. These problems are of considerable interest in
computational geometry, robotics path planning, AI (diagrammatic reasoning),
VLSI routing, and geographical information systems. Given a set of obstacles
and a string with two fixed terminal points in a two dimensional space, the
STON model continuously tightens the given string until the unique shortest
configuration in terms of the Euclidean metric is reached. The STON minimizes
the total length of a string on convergence by dynamically creating and
selecting feature vectors in a competitive manner. Proof of correctness of this
anytime algorithm and experimental results obtained by its deployment are
presented in the paper.
</p>
<a href="http://arxiv.org/abs/2012.06513" target="_blank">arXiv:2012.06513</a> [<a href="http://arxiv.org/pdf/2012.06513" target="_blank">pdf</a>]

<h2>Detection of Binary Square Fiducial Markers Using an Event Camera. (arXiv:2012.06516v1 [cs.CV])</h2>
<h3>Hamid Sarmadi, Rafael Mu&#xf1;oz-Salinas, Miguel A. Olivares-Mendez, Rafael Medina-Carnicer</h3>
<p>Event cameras are a new type of image sensors that output changes in light
intensity (events) instead of absolute intensity values. They have a very high
temporal resolution and a high dynamic range. In this paper, we propose a
method to detect and decode binary square markers using an event camera. We
detect the edges of the markers by detecting line segments in an image created
from events in the current packet. The line segments are combined to form
marker candidates. The bit value of marker cells is decoded using the events on
their borders. To the best of our knowledge, no other approach exists for
detecting square binary markers directly from an event camera. Experimental
results show that the performance of our proposal is much superior to the one
from the RGB ArUco marker detector. Additionally, the proposed method can run
on a single CPU core in real-time.
</p>
<a href="http://arxiv.org/abs/2012.06516" target="_blank">arXiv:2012.06516</a> [<a href="http://arxiv.org/pdf/2012.06516" target="_blank">pdf</a>]

<h2>Dependency Decomposition and a Reject Option for Explainable Models. (arXiv:2012.06523v1 [cs.CV])</h2>
<h3>Jan Kronenberger, Anselm Haselhoff</h3>
<p>Deploying machine learning models in safety-related do-mains (e.g. autonomous
driving, medical diagnosis) demands for approaches that are explainable, robust
against adversarial attacks and aware of the model uncertainty. Recent deep
learning models perform extremely well in various inference tasks, but the
black-box nature of these approaches leads to a weakness regarding the three
requirements mentioned above. Recent advances offer methods to visualize
features, describe attribution of the input (e.g.heatmaps), provide textual
explanations or reduce dimensionality. However,are explanations for
classification tasks dependent or are they independent of each other? For
in-stance, is the shape of an object dependent on the color? What is the effect
of using the predicted class for generating explanations and vice versa? In the
context of explainable deep learning models, we present the first analysis of
dependencies regarding the probability distribution over the desired image
classification outputs and the explaining variables (e.g. attributes, texts,
heatmaps). Therefore, we perform an Explanation Dependency Decomposition (EDD).
We analyze the implications of the different dependencies and propose two ways
of generating the explanation. Finally, we use the explanation to verify
(accept or reject) the prediction
</p>
<a href="http://arxiv.org/abs/2012.06523" target="_blank">arXiv:2012.06523</a> [<a href="http://arxiv.org/pdf/2012.06523" target="_blank">pdf</a>]

<h2>A Hybrid Pricing and Cutting Approach for the Multi-Shift Full Truckload Vehicle Routing Problem. (arXiv:2012.06538v1 [cs.AI])</h2>
<h3>Ning Xue, Ruibin Bai, Rong Qu, Uwe Aickelin</h3>
<p>Full truckload transportation (FTL) in the form of freight containers
represents one of the most important transportation modes in international
trade. Due to large volume and scale, in FTL, delivery time is often less
critical but cost and service quality are crucial. Therefore, efficiently
solving large scale multiple shift FTL problems is becoming more and more
important and requires further research. In one of our earlier studies, a set
covering model and a three-stage solution method were developed for a
multi-shift FTL problem. This paper extends the previous work and presents a
significantly more efficient approach by hybridising pricing and cutting
strategies with metaheuristics (a variable neighbourhood search and a genetic
algorithm). The metaheuristics were adopted to find promising columns (vehicle
routes) guided by pricing and cuts are dynamically generated to eliminate
infeasible flow assignments caused by incompatible commodities. Computational
experiments on real-life and artificial benchmark FTL problems showed superior
performance both in terms of computational time and solution quality, when
compared with previous MIP based three-stage methods and two existing
metaheuristics. The proposed cutting and heuristic pricing approach can
efficiently solve large scale real-life FTL problems.
</p>
<a href="http://arxiv.org/abs/2012.06538" target="_blank">arXiv:2012.06538</a> [<a href="http://arxiv.org/pdf/2012.06538" target="_blank">pdf</a>]

<h2>LayoutGMN: Neural Graph Matching for Structural Layout Similarity. (arXiv:2012.06547v1 [cs.CV])</h2>
<h3>Akshay Gadi Patil, Manyi Li, Matthew Fisher, Manolis Savva, Hao Zhang</h3>
<p>We present a deep neural network to predict structural similarity between 2D
layouts by leveraging Graph Matching Networks (GMN). Our network, coined
LayoutGMN, learns the layout metric via neural graph matching, using an
attention-based GMN designed under a triplet network setting. To train our
network, we utilize weak labels obtained by pixel-wise Intersection-over-Union
(IoUs) to define the triplet loss. Importantly, LayoutGMN is built with a
structural bias which can effectively compensate for the lack of structure
awareness in IoUs. We demonstrate this on two prominent forms of layouts, viz.,
floorplans and UI designs, via retrieval experiments on large-scale datasets.
In particular, retrieval results by our network better match human judgement of
structural layout similarity compared to both IoUs and other baselines
including a state-of-the-art method based on graph neural networks and image
convolution. In addition, LayoutGMN is the first deep model to offer both
metric learning of structural layout similarity and structural matching between
layout elements.
</p>
<a href="http://arxiv.org/abs/2012.06547" target="_blank">arXiv:2012.06547</a> [<a href="http://arxiv.org/pdf/2012.06547" target="_blank">pdf</a>]

<h2>OPAC: Opportunistic Actor-Critic. (arXiv:2012.06555v1 [cs.LG])</h2>
<h3>Srinjoy Roy, Saptam Bakshi, Tamal Maharaj</h3>
<p>Actor-critic methods, a type of model-free reinforcement learning (RL), have
achieved state-of-the-art performances in many real-world domains in continuous
control. Despite their success, the wide-scale deployment of these models is
still a far cry. The main problems in these actor-critic methods are
inefficient exploration and sub-optimal policies. Soft Actor-Critic (SAC) and
Twin Delayed Deep Deterministic Policy Gradient (TD3), two cutting edge such
algorithms, suffer from these issues. SAC effectively addressed the problems of
sample complexity and convergence brittleness to hyper-parameters and thus
outperformed all state-of-the-art algorithms including TD3 in harder tasks,
whereas TD3 produced moderate results in all environments. SAC suffers from
inefficient exploration owing to the Gaussian nature of its policy which causes
borderline performance in simpler tasks. In this paper, we introduce
Opportunistic Actor-Critic (OPAC), a novel model-free deep RL algorithm that
employs better exploration policy and lesser variance. OPAC combines some of
the most powerful features of TD3 and SAC and aims to optimize a stochastic
policy in an off-policy way. For calculating the target Q-values, instead of
two critics, OPAC uses three critics and based on the environment complexity,
opportunistically chooses how the target Q-value is computed from the critics'
evaluation. We have systematically evaluated the algorithm on MuJoCo
environments where it achieves state-of-the-art performance and outperforms or
at least equals the performance of TD3 and SAC.
</p>
<a href="http://arxiv.org/abs/2012.06555" target="_blank">arXiv:2012.06555</a> [<a href="http://arxiv.org/pdf/2012.06555" target="_blank">pdf</a>]

<h2>Comprehension and Knowledge. (arXiv:2012.06561v1 [cs.AI])</h2>
<h3>Pavel Naumov, Kevin Ros</h3>
<p>The ability of an agent to comprehend a sentence is tightly connected to the
agent's prior experiences and background knowledge. The paper suggests to
interpret comprehension as a modality and proposes a complete bimodal logical
system that describes an interplay between comprehension and knowledge
modalities.
</p>
<a href="http://arxiv.org/abs/2012.06561" target="_blank">arXiv:2012.06561</a> [<a href="http://arxiv.org/pdf/2012.06561" target="_blank">pdf</a>]

<h2>Exploring Facial Expressions and Affective Domains for Parkinson Detection. (arXiv:2012.06563v1 [cs.CV])</h2>
<h3>Luis Felipe Gomez-Gomez, Aythami Morales, Julian Fierrez, Juan Rafael Orozco-Arroyave</h3>
<p>Parkinson's Disease (PD) is a neurological disorder that affects facial
movements and non-verbal communication. Patients with PD present a reduction in
facial movements called hypomimia which is evaluated in item 3.2 of the
MDS-UPDRS-III scale. In this work, we propose to use facial expression analysis
from face images based on affective domains to improve PD detection. We propose
different domain adaptation techniques to exploit the latest advances in face
recognition and Face Action Unit (FAU) detection. The principal contributions
of this work are: (1) a novel framework to exploit deep face architectures to
model hypomimia in PD patients; (2) we experimentally compare PD detection
based on single images vs. image sequences while the patients are evoked
various face expressions; (3) we explore different domain adaptation techniques
to exploit existing models initially trained either for Face Recognition or to
detect FAUs for the automatic discrimination between PD patients and healthy
subjects; and (4) a new approach to use triplet-loss learning to improve
hypomimia modeling and PD detection. The results on real face images from PD
patients show that we are able to properly model evoked emotions using image
sequences (neutral, onset-transition, apex, offset-transition, and neutral)
with accuracy improvements up to 5.5% (from 72.9% to 78.4%) with respect to
single-image PD detection. We also show that our proposed affective-domain
adaptation provides improvements in PD detection up to 8.9% (from 78.4% to
87.3% detection accuracy).
</p>
<a href="http://arxiv.org/abs/2012.06563" target="_blank">arXiv:2012.06563</a> [<a href="http://arxiv.org/pdf/2012.06563" target="_blank">pdf</a>]

<h2>Glucose values prediction five years ahead with a new framework of missing responses in reproducing kernel Hilbert spaces, and the use of continuous glucose monitoring technology. (arXiv:2012.06564v1 [stat.ML])</h2>
<h3>Marcos Matabuena, Paulo F&#xe9;lix, Carlos Meijide-Garcia, Francisco Gude</h3>
<p>AEGIS study possesses unique information on longitudinal changes in
circulating glucose from a random sample. However, some five-year outcomes,
such as glycosylated hemoglobin (A1C), are over 40\% missing data. To alleviate
this problem, this article proposes a new data analysis framework based on
learning in reproducing kernel Hilbert spaces (RKHS) with missing responses. In
particular, we extend the Hilbert-Schmidt dependence measure to this context
introducing a new bootstrap procedure in which we prove to be consistent. In
addition, we adapt or use existing algorithms/models of variable selection,
regression, or conformal inference to acquire new clinical findings with the
AEGIS study data. The fitted models allow: i) to identify new factors
associated with long-term glucose changes; ii) to highly the usefulness of CGM
technology predictive capacity; and iii) to improve and optimize clinical
interventions based on expected glucose changes according to patients' baseline
characteristics.
</p>
<a href="http://arxiv.org/abs/2012.06564" target="_blank">arXiv:2012.06564</a> [<a href="http://arxiv.org/pdf/2012.06564" target="_blank">pdf</a>]

<h2>A Comprehensive Study of Deep Video Action Recognition. (arXiv:2012.06567v1 [cs.CV])</h2>
<h3>Yi Zhu, Xinyu Li, Chunhui Liu, Mohammadreza Zolfaghari, Yuanjun Xiong, Chongruo Wu, Zhi Zhang, Joseph Tighe, R. Manmatha, Mu Li</h3>
<p>Video action recognition is one of the representative tasks for video
understanding. Over the last decade, we have witnessed great advancements in
video action recognition thanks to the emergence of deep learning. But we also
encountered new challenges, including modeling long-range temporal information
in videos, high computation costs, and incomparable results due to datasets and
evaluation protocol variances. In this paper, we provide a comprehensive survey
of over 200 existing papers on deep learning for video action recognition. We
first introduce the 17 video action recognition datasets that influenced the
design of models. Then we present video action recognition models in
chronological order: starting with early attempts at adapting deep learning,
then to the two-stream networks, followed by the adoption of 3D convolutional
kernels, and finally to the recent compute-efficient models. In addition, we
benchmark popular methods on several representative datasets and release code
for reproducibility. In the end, we discuss open problems and shed light on
opportunities for video action recognition to facilitate new research ideas.
</p>
<a href="http://arxiv.org/abs/2012.06567" target="_blank">arXiv:2012.06567</a> [<a href="http://arxiv.org/pdf/2012.06567" target="_blank">pdf</a>]

<h2>Analyzing and Improving Generative Adversarial Training for Generative Modeling and Out-of-Distribution Detection. (arXiv:2012.06568v1 [cs.LG])</h2>
<h3>Xuwang Yin, Shiying Li, Gustavo K. Rohde</h3>
<p>Generative adversarial training (GAT) is a recently introduced adversarial
defense method. Previous works have focused on empirical evaluations of its
application to training robust predictive models. In this paper we focus on
theoretical understanding of the GAT method and extending its application to
generative modeling and out-of-distribution detection. We analyze the optimal
solutions of the maximin formulation employed by the GAT objective, and make a
comparative analysis of the minimax formulation employed by GANs. We use
theoretical analysis and 2D simulations to understand the convergence property
of the training algorithm. Based on these results, we develop an incremental
generative training algorithm, and conduct comprehensive evaluations of the
algorithm's application to image generation and adversarial out-of-distribution
detection. Our results suggest that generative adversarial training is a
promising new direction for the above applications.
</p>
<a href="http://arxiv.org/abs/2012.06568" target="_blank">arXiv:2012.06568</a> [<a href="http://arxiv.org/pdf/2012.06568" target="_blank">pdf</a>]

<h2>Risk & returns around FOMC press conferences: a novel perspective from computer vision. (arXiv:2012.06573v1 [stat.ML])</h2>
<h3>Alexis Marchal</h3>
<p>I propose a new tool to characterize the resolution of uncertainty around
FOMC press conferences. It relies on the construction of a measure capturing
the level of discussion complexity between the Fed Chair and reporters during
the Q&amp;A sessions. I show that complex discussions are associated with higher
equity returns and a drop in realized volatility. The method creates an
attention score by quantifying how much the Chair needs to rely on reading
internal documents to be able to answer a question. This is accomplished by
building a novel dataset of video images of the press conferences and
leveraging recent deep learning algorithms from computer vision. This
alternative data provides new information on nonverbal communication that
cannot be extracted from the widely analyzed FOMC transcripts. This paper can
be seen as a proof of concept that certain videos contain valuable information
for the study of financial markets.
</p>
<a href="http://arxiv.org/abs/2012.06573" target="_blank">arXiv:2012.06573</a> [<a href="http://arxiv.org/pdf/2012.06573" target="_blank">pdf</a>]

<h2>Entropy Maximization and Meta Classification for Out-Of-Distribution Detection in Semantic Segmentation. (arXiv:2012.06575v1 [cs.CV])</h2>
<h3>Robin Chan, Matthias Rottmann, Hanno Gottschalk</h3>
<p>Deep neural networks (DNNs) for the semantic segmentation of images are
usually trained to operate on a predefined closed set of object classes. This
is in contrast to the "open world" setting where DNNs are envisioned to be
deployed to. From a functional safety point of view, the ability to detect
so-called "out-of-distribution" (OoD) samples, i.e., objects outside of a DNN's
semantic space, is crucial for many applications such as automated driving. A
natural baseline approach to OoD detection is to threshold on the pixel-wise
softmax entropy. We present a two-step procedure that significantly improves
that approach. Firstly, we utilize samples from the COCO dataset as OoD proxy
and introduce a second training objective to maximize the softmax entropy on
these samples. Starting from pretrained semantic segmentation networks we
re-train a number of DNNs on different in-distribution datasets and
consistently observe improved OoD detection performance when evaluating on
completely disjoint OoD datasets. Secondly, we perform a transparent
post-processing step to discard false positive OoD samples by so-called "meta
classification". To this end, we apply linear models to a set of hand-crafted
metrics derived from the DNN's softmax probabilities. In our experiments we
consistently observe a clear additional gain in OoD detection performance,
cutting down the number of detection errors by up to 52% when comparing the
best baseline with our results. We achieve this improvement sacrificing only
marginally in original segmentation performance. Therefore, our method
contributes to safer DNNs with more reliable overall system performance.
</p>
<a href="http://arxiv.org/abs/2012.06575" target="_blank">arXiv:2012.06575</a> [<a href="http://arxiv.org/pdf/2012.06575" target="_blank">pdf</a>]

<h2>Learning with Bounded Instance- and Label-dependent Label Noise. (arXiv:1709.03768v3 [stat.ML] UPDATED)</h2>
<h3>Jiacheng Cheng, Tongliang Liu, Kotagiri Ramamohanarao, Dacheng Tao</h3>
<p>Instance- and Label-dependent label Noise (ILN) widely exists in real-world
datasets but has been rarely studied. In this paper, we focus on Bounded
Instance- and Label-dependent label Noise (BILN), a particular case of ILN
where the label noise rates -- the probabilities that the true labels of
examples flip into the corrupted ones -- have upper bound less than $1$.
Specifically, we introduce the concept of distilled examples, i.e. examples
whose labels are identical with the labels assigned for them by the Bayes
optimal classifier, and prove that under certain conditions classifiers learnt
on distilled examples will converge to the Bayes optimal classifier. Inspired
by the idea of learning with distilled examples, we then propose a learning
algorithm with theoretical guarantees for its robustness to BILN. At last,
empirical evaluations on both synthetic and real-world datasets show
effectiveness of our algorithm in learning with BILN.
</p>
<a href="http://arxiv.org/abs/1709.03768" target="_blank">arXiv:1709.03768</a> [<a href="http://arxiv.org/pdf/1709.03768" target="_blank">pdf</a>]

<h2>Embedding Deep Networks into Visual Explanations. (arXiv:1709.05360v3 [cs.CV] UPDATED)</h2>
<h3>Zhongang Qi, Saeed Khorram, Fuxin Li</h3>
<p>In this paper, we propose a novel Explanation Neural Network (XNN) to explain
the predictions made by a deep network. The XNN works by learning a nonlinear
embedding of a high-dimensional activation vector of a deep network layer into
a low-dimensional explanation space while retaining faithfulness i.e., the
original deep learning predictions can be constructed from the few concepts
extracted by our explanation network. We then visualize such concepts for human
to learn about the high-level concepts that the deep network is using to make
decisions. We propose an algorithm called Sparse Reconstruction Autoencoder
(SRAE) for learning the embedding to the explanation space. SRAE aims to
reconstruct part of the original feature space while retaining faithfulness. A
pull-away term is applied to SRAE to make the bases of the explanation space
more orthogonal to each other. A visualization system is then introduced for
human understanding of the features in the explanation space. The proposed
method is applied to explain CNN models in image classification tasks. We
conducted a human study, which shows that the proposed approach outperforms
single saliency map baselines, and improves human performance on a difficult
classification tasks. Also, several novel metrics are introduced to evaluate
the performance of explanations quantitatively without human involvement.
</p>
<a href="http://arxiv.org/abs/1709.05360" target="_blank">arXiv:1709.05360</a> [<a href="http://arxiv.org/pdf/1709.05360" target="_blank">pdf</a>]

<h2>Knowledge-Based Regularization in Generative Modeling. (arXiv:1902.02068v2 [cs.LG] UPDATED)</h2>
<h3>Naoya Takeishi, Yoshinobu Kawahara</h3>
<p>Prior domain knowledge can greatly help to learn generative models. However,
it is often too costly to hard-code prior knowledge as a specific model
architecture, so we often have to use general-purpose models. In this paper, we
propose a method to incorporate prior knowledge of feature relations into the
learning of general-purpose generative models. To this end, we formulate a
regularizer that makes the marginals of a generative model to follow prescribed
relative dependence of features. It can be incorporated into off-the-shelf
learning methods of many generative models, including variational autoencoders
and generative adversarial networks, as its gradients can be computed using
standard backpropagation techniques. We show the effectiveness of the proposed
method with experiments on multiple types of datasets and generative models.
</p>
<a href="http://arxiv.org/abs/1902.02068" target="_blank">arXiv:1902.02068</a> [<a href="http://arxiv.org/pdf/1902.02068" target="_blank">pdf</a>]

<h2>Tensor Dropout for Robust Learning. (arXiv:1902.10758v4 [cs.LG] UPDATED)</h2>
<h3>Arinbj&#xf6;rn Kolbeinsson, Jean Kossaifi, Yannis Panagakis, Adrian Bulat, Anima Anandkumar, Ioanna Tzoulaki, Paul Matthews</h3>
<p>CNNs achieve remarkable performance by leveraging deep, over-parametrized
architectures, trained on large datasets. However, they have limited
generalization ability to data outside the training domain, and a lack of
robustness to noise and adversarial attacks. By building better inductive
biases, we can improve robustness and also obtain smaller networks that are
more memory and computationally efficient. While standard CNNs use matrix
computations, we study tensor layers that involve higher-order computations and
provide better inductive bias. Specifically, we impose low-rank tensor
structures on the weights of tensor regression layers to obtain compact
networks, and propose tensor dropout, a randomization in the tensor rank for
robustness. We show that our approach outperforms other methods for large-scale
image classification on ImageNet and CIFAR-100. We establish a new
state-of-the-art accuracy for phenotypic trait prediction on the largest
dataset of brain MRI, the UK Biobank brain MRI dataset, where multi-linear
structure is paramount. In all cases, we demonstrate superior performance and
significantly improved robustness, both to noisy inputs and to adversarial
attacks. We rigorously validate the theoretical validity of our approach by
establishing the link between our randomized decomposition and non-linear
dropout.
</p>
<a href="http://arxiv.org/abs/1902.10758" target="_blank">arXiv:1902.10758</a> [<a href="http://arxiv.org/pdf/1902.10758" target="_blank">pdf</a>]

<h2>Visualizing Deep Networks by Optimizing with Integrated Gradients. (arXiv:1905.00954v2 [cs.CV] UPDATED)</h2>
<h3>Zhongang Qi, Saeed Khorram, Li Fuxin</h3>
<p>Understanding and interpreting the decisions made by deep learning models is
valuable in many domains. In computer vision, computing heatmaps from a deep
network is a popular approach for visualizing and understanding deep networks.
However, heatmaps that do not correlate with the network may mislead human,
hence the performance of heatmaps in providing a faithful explanation to the
underlying deep network is crucial. In this paper, we propose I-GOS, which
optimizes for a heatmap so that the classification scores on the masked image
would maximally decrease. The main novelty of the approach is to compute
descent directions based on the integrated gradients instead of the normal
gradient, which avoids local optima and speeds up convergence. Compared with
previous approaches, our method can flexibly compute heatmaps at any resolution
for different user needs. Extensive experiments on several benchmark datasets
show that the heatmaps produced by our approach are more correlated with the
decision of the underlying deep network, in comparison with other
state-of-the-art approaches.
</p>
<a href="http://arxiv.org/abs/1905.00954" target="_blank">arXiv:1905.00954</a> [<a href="http://arxiv.org/pdf/1905.00954" target="_blank">pdf</a>]

<h2>Sliced Gromov-Wasserstein. (arXiv:1905.10124v3 [stat.ML] UPDATED)</h2>
<h3>Titouan Vayer, R&#xe9;mi Flamary, Romain Tavenard, Laetitia Chapel, Nicolas Courty</h3>
<p>Recently used in various machine learning contexts, the Gromov-Wasserstein
distance (GW) allows for comparing distributions whose supports do not
necessarily lie in the same metric space. However, this Optimal Transport (OT)
distance requires solving a complex non convex quadratic program which is most
of the time very costly both in time and memory. Contrary to GW, the
Wasserstein distance (W) enjoys several properties ({\em e.g.} duality) that
permit large scale optimization. Among those, the solution of W on the real
line, that only requires sorting discrete samples in 1D, allows defining the
Sliced Wasserstein (SW) distance. This paper proposes a new divergence based on
GW akin to SW. We first derive a closed form for GW when dealing with 1D
distributions, based on a new result for the related quadratic assignment
problem. We then define a novel OT discrepancy that can deal with large scale
distributions via a slicing approach and we show how it relates to the GW
distance while being $O(n\log(n))$ to compute. We illustrate the behavior of
this so called Sliced Gromov-Wasserstein (SGW) discrepancy in experiments where
we demonstrate its ability to tackle similar problems as GW while being several
order of magnitudes faster to compute.
</p>
<a href="http://arxiv.org/abs/1905.10124" target="_blank">arXiv:1905.10124</a> [<a href="http://arxiv.org/pdf/1905.10124" target="_blank">pdf</a>]

<h2>Adaptive Deep Kernel Learning. (arXiv:1905.12131v2 [cs.LG] UPDATED)</h2>
<h3>Prudencio Tossou, Basile Dura, Francois Laviolette, Mario Marchand, Alexandre Lacoste</h3>
<p>Deep kernel learning provides an elegant and principled framework for
combining the structural properties of deep learning algorithms with the
flexibility of kernel methods. By means of a deep neural network, we learn a
parametrized kernel operator that can be combined with a differentiable kernel
algorithm during inference. While previous work within this framework has
focused on learning a single kernel for large datasets, we learn a kernel
family for a variety of few-shot regression tasks. Compared to single deep
kernel learning, our algorithm enables the identification of the appropriate
kernel for each task during inference. As such, it is well adapted for complex
task distributions in a few-shot learning setting, which we demonstrate by
comparing against existing state-of-the-art algorithms using real-world,
few-shot regression tasks related to the field of drug discovery.
</p>
<a href="http://arxiv.org/abs/1905.12131" target="_blank">arXiv:1905.12131</a> [<a href="http://arxiv.org/pdf/1905.12131" target="_blank">pdf</a>]

<h2>De-biased Machine Learning in Instrumental Variable Models for Treatment Effects. (arXiv:1909.05244v3 [stat.ML] UPDATED)</h2>
<h3>Rahul Singh, Liyang Sun</h3>
<p>We introduce a de-biased machine learning (DML) approach to estimating
complier parameters with high-dimensional data. Complier parameters include
local average treatment effect, average complier characteristics, and complier
counterfactual outcome distributions. In our approach, the de-biasing is itself
performed by machine learning, a variant called automatic de-biased machine
learning (Auto-DML). By regularizing the balancing weights, it does not require
ad hoc trimming or censoring. We prove our estimator is consistent,
asymptotically normal, and semi-parametrically efficient. We use the new
approach to estimate the effect of 401(k) participation on the distribution of
net financial assets.
</p>
<a href="http://arxiv.org/abs/1909.05244" target="_blank">arXiv:1909.05244</a> [<a href="http://arxiv.org/pdf/1909.05244" target="_blank">pdf</a>]

<h2>Learning to Seek: Tiny Robot Learning (tinyRL) for Source Seeking on a Nano Quadcopter. (arXiv:1909.11236v5 [cs.RO] UPDATED)</h2>
<h3>Bardienus P. Duisterhof, Srivatsan Krishnan, Jonathan J. Cruz, Colby R. Banbury, William Fu, Aleksandra Faust, Guido C. H. E. de Croon, Vijay Janapa Reddi</h3>
<p>We present fully autonomous source seeking onboard a highly constrained nano
quadcopter, by contributing application-specific system and observation feature
design to enable inference of a deep-RL policy onboard a nano quadcopter. Our
deep-RL algorithm finds a high-performance solution to a challenging problem,
even in presence of high noise levels and generalizes across real and
simulation environments with different obstacle configurations. We verify our
approach with simulation and in-field testing on a Bitcraze CrazyFlie using
only the cheap and ubiquitous Cortex-M4 microcontroller unit. The results show
that by end-to-end application-specific system design, our contribution
consumes almost three times less additional power, as compared to competing
learning-based navigation approach onboard a nano quadcopter. Thanks to our
observation space, which we carefully design within the resource constraints,
our solution achieves a 94% success rate in cluttered and randomized test
environments, as compared to the previously achieved 80%. We also compare our
strategy to a simple finite state machine (FSM), geared towards efficient
exploration, and demonstrate that our policy is more robust and resilient at
obstacle avoidance as well as up to 70% more efficient in source seeking. To
this end, we contribute a cheap and lightweight end-to-end tiny robot learning
(tinyRL) solution, running onboard a nano quadcopter, that proves to be robust
and efficient in a challenging task using limited sensory input.
</p>
<a href="http://arxiv.org/abs/1909.11236" target="_blank">arXiv:1909.11236</a> [<a href="http://arxiv.org/pdf/1909.11236" target="_blank">pdf</a>]

<h2>Is There a Trade-Off Between Fairness and Accuracy? A Perspective Using Mismatched Hypothesis Testing. (arXiv:1910.07870v2 [stat.ML] UPDATED)</h2>
<h3>Sanghamitra Dutta, Dennis Wei, Hazar Yueksel, Pin-Yu Chen, Sijia Liu, Kush R. Varshney</h3>
<p>A trade-off between accuracy and fairness is almost taken as a given in the
existing literature on fairness in machine learning. Yet, it is not preordained
that accuracy should decrease with increased fairness. Novel to this work, we
examine fair classification through the lens of mismatched hypothesis testing:
trying to find a classifier that distinguishes between two ideal distributions
when given two mismatched distributions that are biased. Using Chernoff
information, a tool in information theory, we theoretically demonstrate that,
contrary to popular belief, there always exist ideal distributions such that
optimal fairness and accuracy (with respect to the ideal distributions) are
achieved simultaneously: there is no trade-off. Moreover, the same classifier
yields the lack of a trade-off with respect to ideal distributions while
yielding a trade-off when accuracy is measured with respect to the given
(possibly biased) dataset. To complement our main result, we formulate an
optimization to find ideal distributions and derive fundamental limits to
explain why a trade-off exists on the given biased dataset. We also derive
conditions under which active data collection can alleviate the
fairness-accuracy trade-off in the real world. Our results lead us to contend
that it is problematic to measure accuracy with respect to data that reflects
bias, and instead, we should be considering accuracy with respect to ideal,
unbiased data.
</p>
<a href="http://arxiv.org/abs/1910.07870" target="_blank">arXiv:1910.07870</a> [<a href="http://arxiv.org/pdf/1910.07870" target="_blank">pdf</a>]

<h2>The Wasserstein-Fourier Distance for Stationary Time Series. (arXiv:1912.05509v2 [stat.ML] UPDATED)</h2>
<h3>Elsa Cazelles, Arnaud Robert, Felipe Tobar</h3>
<p>We propose the Wasserstein-Fourier (WF) distance to measure the
(dis)similarity between time series by quantifying the displacement of their
energy across frequencies. The WF distance operates by calculating the
Wasserstein distance between the (normalised) power spectral densities (NPSD)
of time series. Yet this rationale has been considered in the past, we fill a
gap in the open literature providing a formal introduction of this distance,
together with its main properties from the joint perspective of Fourier
analysis and optimal transport. As the main aim of this work is to validate WF
as a general-purpose metric for time series, we illustrate its applicability on
three broad contexts. First, we rely on WF to implement a PCA-like
dimensionality reduction for NPSDs which allows for meaningful visualisation
and pattern recognition applications. Second, we show that the geometry induced
by WF on the space of NPSDs admits a geodesic interpolant between time series,
thus enabling data augmentation on the spectral domain, by averaging the
dynamic content of two signals. Third, we implement WF for time series
classification using parametric/non-parametric classifiers and compare it to
other classical metrics. Supported on theoretical results, as well as synthetic
illustrations and experiments on real-world data, this work establishes WF as a
meaningful and capable resource pertinent to general distance-based
applications of time series.
</p>
<a href="http://arxiv.org/abs/1912.05509" target="_blank">arXiv:1912.05509</a> [<a href="http://arxiv.org/pdf/1912.05509" target="_blank">pdf</a>]

<h2>DeeperForensics-1.0: A Large-Scale Dataset for Real-World Face Forgery Detection. (arXiv:2001.03024v2 [cs.CV] UPDATED)</h2>
<h3>Liming Jiang, Ren Li, Wayne Wu, Chen Qian, Chen Change Loy</h3>
<p>We present our on-going effort of constructing a large-scale benchmark for
face forgery detection. The first version of this benchmark,
DeeperForensics-1.0, represents the largest face forgery detection dataset by
far, with 60,000 videos constituted by a total of 17.6 million frames, 10 times
larger than existing datasets of the same kind. Extensive real-world
perturbations are applied to obtain a more challenging benchmark of larger
scale and higher diversity. All source videos in DeeperForensics-1.0 are
carefully collected, and fake videos are generated by a newly proposed
end-to-end face swapping framework. The quality of generated videos outperforms
those in existing datasets, validated by user studies. The benchmark features a
hidden test set, which contains manipulated videos achieving high deceptive
scores in human evaluations. We further contribute a comprehensive study that
evaluates five representative detection baselines and make a thorough analysis
of different settings.
</p>
<a href="http://arxiv.org/abs/2001.03024" target="_blank">arXiv:2001.03024</a> [<a href="http://arxiv.org/pdf/2001.03024" target="_blank">pdf</a>]

<h2>Reward-rational (implicit) choice: A unifying formalism for reward learning. (arXiv:2002.04833v4 [cs.LG] UPDATED)</h2>
<h3>Hong Jun Jeon, Smitha Milli, Anca D. Dragan</h3>
<p>It is often difficult to hand-specify what the correct reward function is for
a task, so researchers have instead aimed to learn reward functions from human
behavior or feedback. The types of behavior interpreted as evidence of the
reward function have expanded greatly in recent years. We've gone from
demonstrations, to comparisons, to reading into the information leaked when the
human is pushing the robot away or turning it off. And surely, there is more to
come. How will a robot make sense of all these diverse types of behavior? Our
key insight is that different types of behavior can be interpreted in a single
unifying formalism - as a reward-rational choice that the human is making,
often implicitly. The formalism offers both a unifying lens with which to view
past work, as well as a recipe for interpreting new sources of information that
are yet to be uncovered. We provide two examples to showcase this: interpreting
a new feedback type, and reading into how the choice of feedback itself leaks
information about the reward.
</p>
<a href="http://arxiv.org/abs/2002.04833" target="_blank">arXiv:2002.04833</a> [<a href="http://arxiv.org/pdf/2002.04833" target="_blank">pdf</a>]

<h2>Towards Precise Intra-camera Supervised Person Re-identification. (arXiv:2002.04932v2 [cs.CV] UPDATED)</h2>
<h3>Menglin Wang, Baisheng Lai, Haokun Chen, Jianqiang Huang, Xiaojin Gong, Xian-Sheng Hua</h3>
<p>Intra-camera supervision (ICS) for person re-identification (Re-ID) assumes
that identity labels are independently annotated within each camera view and no
inter-camera identity association is labeled. It is a new setting proposed
recently to reduce the burden of annotation while expect to maintain desirable
Re-ID performance. However, the lack of inter-camera labels makes the ICS Re-ID
problem much more challenging than the fully supervised counterpart. By
investigating the characteristics of ICS, this paper proposes camera-specific
non-parametric classifiers, together with a hybrid mining quintuplet loss, to
perform intra-camera learning. Then, an inter-camera learning module consisting
of a graph-based ID association step and a Re-ID model updating step is
conducted. Extensive experiments on three large-scale Re-ID datasets show that
our approach outperforms all existing ICS works by a great margin. Our approach
performs even comparable to state-of-the-art fully supervised methods in two of
the datasets.
</p>
<a href="http://arxiv.org/abs/2002.04932" target="_blank">arXiv:2002.04932</a> [<a href="http://arxiv.org/pdf/2002.04932" target="_blank">pdf</a>]

<h2>Parameterizing Branch-and-Bound Search Trees to Learn Branching Policies. (arXiv:2002.05120v3 [cs.LG] UPDATED)</h2>
<h3>Giulia Zarpellon, Jason Jo, Andrea Lodi, Yoshua Bengio</h3>
<p>Branch and Bound (B&amp;B) is the exact tree search method typically used to
solve Mixed-Integer Linear Programming problems (MILPs). Learning branching
policies for MILP has become an active research area, with most works proposing
to imitate the strong branching rule and specialize it to distinct classes of
problems. We aim instead at learning a policy that generalizes across
heterogeneous MILPs: our main hypothesis is that parameterizing the state of
the B&amp;B search tree can aid this type of generalization. We propose a novel
imitation learning framework, and introduce new input features and
architectures to represent branching. Experiments on MILP benchmark instances
clearly show the advantages of incorporating an explicit parameterization of
the state of the search tree to modulate the branching decisions, in terms of
both higher accuracy and smaller B&amp;B trees. The resulting policies
significantly outperform the current state-of-the-art method for "learning to
branch" by effectively allowing generalization to generic unseen instances.
</p>
<a href="http://arxiv.org/abs/2002.05120" target="_blank">arXiv:2002.05120</a> [<a href="http://arxiv.org/pdf/2002.05120" target="_blank">pdf</a>]

<h2>Out-of-Distribution Generalization via Risk Extrapolation (REx). (arXiv:2003.00688v4 [cs.LG] UPDATED)</h2>
<h3>David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai Zhang, Remi Le Priol, Aaron Courville</h3>
<p>Generalizing outside of the training distribution is an open challenge for
current machine learning systems. A weak form of out-of-distribution (OoD)
generalization is the ability to successfully interpolate between multiple
observed distributions. One way to achieve this is through robust optimization,
which seeks to minimize the worst-case risk over convex combinations of the
training distributions. However, a much stronger form of OoD generalization is
the ability of models to extrapolate beyond the distributions observed during
training. In pursuit of strong OoD generalization, we introduce the principle
of Risk Extrapolation (REx). REx can be viewed as encouraging robustness over
affine combinations of training risks, by encouraging strict equality between
training risks. We show conceptually how this principle enables extrapolation,
and demonstrate the effectiveness and scalability of instantiations of REx on
various OoD generalization tasks. Our code can be found at
https://github.com/capybaralet/REx_code_release.
</p>
<a href="http://arxiv.org/abs/2003.00688" target="_blank">arXiv:2003.00688</a> [<a href="http://arxiv.org/pdf/2003.00688" target="_blank">pdf</a>]

<h2>Knowledge Graphs. (arXiv:2003.02320v4 [cs.AI] UPDATED)</h2>
<h3>Aidan Hogan, Eva Blomqvist, Michael Cochez, Claudia d&#x27;Amato, Gerard de Melo, Claudio Gutierrez, Jos&#xe9; Emilio Labra Gayo, Sabrina Kirrane, Sebastian Neumaier, Axel Polleres, Roberto Navigli, Axel-Cyrille Ngonga Ngomo, Sabbir M. Rashid, Anisa Rula, Lukas Schmelzeisen, Juan Sequeda, Steffen Staab, Antoine Zimmermann</h3>
<p>In this paper we provide a comprehensive introduction to knowledge graphs,
which have recently garnered significant attention from both industry and
academia in scenarios that require exploiting diverse, dynamic, large-scale
collections of data. After some opening remarks, we motivate and contrast
various graph-based data models and query languages that are used for knowledge
graphs. We discuss the roles of schema, identity, and context in knowledge
graphs. We explain how knowledge can be represented and extracted using a
combination of deductive and inductive techniques. We summarise methods for the
creation, enrichment, quality assessment, refinement, and publication of
knowledge graphs. We provide an overview of prominent open knowledge graphs and
enterprise knowledge graphs, their applications, and how they use the
aforementioned techniques. We conclude with high-level future research
directions for knowledge graphs.
</p>
<a href="http://arxiv.org/abs/2003.02320" target="_blank">arXiv:2003.02320</a> [<a href="http://arxiv.org/pdf/2003.02320" target="_blank">pdf</a>]

<h2>Software Language Comprehension using a Program-Derived Semantics Graph. (arXiv:2004.00768v3 [cs.AI] UPDATED)</h2>
<h3>Roshni G. Iyer, Yizhou Sun, Wei Wang, Justin Gottschlich</h3>
<p>Traditional code transformation structures, such as abstract syntax trees
(ASTs), conteXtual flow graphs (XFGs), and more generally, compiler
intermediate representations (IRs), may have limitations in extracting
higher-order semantics from code. While work has already begun on higher-order
semantics lifting (e.g., Aroma's simplified parse tree (SPT), verified
lifting's lambda calculi, and Halide's intentional domain specific language
(DSL)), research in this area is still immature. To continue to advance this
research, we present the program-derived semantics graph, a new graphical
structure to capture semantics of code. The PSG is designed to provide a single
structure for capturing program semantics at multiple levels of abstraction.
The PSG may be in a class of emerging structural representations that cannot be
built from a traditional set of predefined rules and instead must be learned.
In this paper, we describe the PSG and its fundamental structural differences
compared to state-of-the-art structures. Although our exploration into the PSG
is in its infancy, our early results and architectural analysis indicate it is
a promising new research direction to automatically extract program semantics.
</p>
<a href="http://arxiv.org/abs/2004.00768" target="_blank">arXiv:2004.00768</a> [<a href="http://arxiv.org/pdf/2004.00768" target="_blank">pdf</a>]

<h2>Efficient Scale Estimation Methods using Lightweight Deep Convolutional Neural Networks for Visual Tracking. (arXiv:2004.02933v2 [cs.CV] UPDATED)</h2>
<h3>Seyed Mojtaba Marvasti-Zadeh, Hossein Ghanei-Yakhdan, Shohreh Kasaei</h3>
<p>In recent years, visual tracking methods that are based on discriminative
correlation filters (DCF) have been very promising. However, most of these
methods suffer from a lack of robust scale estimation skills. Although a wide
range of recent DCF-based methods exploit the features that are extracted from
deep convolutional neural networks (CNNs) in their translation model, the scale
of the visual target is still estimated by hand-crafted features. Whereas the
exploitation of CNNs imposes a high computational burden, this paper exploits
pre-trained lightweight CNNs models to propose two efficient scale estimation
methods, which not only improve the visual tracking performance but also
provide acceptable tracking speeds. The proposed methods are formulated based
on either holistic or region representation of convolutional feature maps to
efficiently integrate into DCF formulations to learn a robust scale model in
the frequency domain. Moreover, against the conventional scale estimation
methods with iterative feature extraction of different target regions, the
proposed methods exploit proposed one-pass feature extraction processes that
significantly improve the computational efficiency. Comprehensive experimental
results on the OTB-50, OTB-100, TC-128 and VOT-2018 visual tracking datasets
demonstrate that the proposed visual tracking methods outperform the
state-of-the-art methods, effectively.
</p>
<a href="http://arxiv.org/abs/2004.02933" target="_blank">arXiv:2004.02933</a> [<a href="http://arxiv.org/pdf/2004.02933" target="_blank">pdf</a>]

<h2>Supervised Contrastive Learning. (arXiv:2004.11362v4 [cs.LG] UPDATED)</h2>
<h3>Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, Dilip Krishnan</h3>
<p>Contrastive learning applied to self-supervised representation learning has
seen a resurgence in recent years, leading to state of the art performance in
the unsupervised training of deep image models. Modern batch contrastive
approaches subsume or significantly outperform traditional contrastive losses
such as triplet, max-margin and the N-pairs loss. In this work, we extend the
self-supervised batch contrastive approach to the fully-supervised setting,
allowing us to effectively leverage label information. Clusters of points
belonging to the same class are pulled together in embedding space, while
simultaneously pushing apart clusters of samples from different classes. We
analyze two possible versions of the supervised contrastive (SupCon) loss,
identifying the best-performing formulation of the loss. On ResNet-200, we
achieve top-1 accuracy of 81.4% on the ImageNet dataset, which is 0.8% above
the best number reported for this architecture. We show consistent
outperformance over cross-entropy on other datasets and two ResNet variants.
The loss shows benefits for robustness to natural corruptions and is more
stable to hyperparameter settings such as optimizers and data augmentations.
Our loss function is simple to implement, and reference TensorFlow code is
released at https://t.ly/supcon.
</p>
<a href="http://arxiv.org/abs/2004.11362" target="_blank">arXiv:2004.11362</a> [<a href="http://arxiv.org/pdf/2004.11362" target="_blank">pdf</a>]

<h2>PreCNet: Next Frame Video Prediction Based on Predictive Coding. (arXiv:2004.14878v2 [cs.CV] UPDATED)</h2>
<h3>Zdenek Straka, Tomas Svoboda, Matej Hoffmann</h3>
<p>Predictive coding, currently a highly influential theory in neuroscience, has
not been widely adopted in machine learning yet. In this work, we transform the
seminal model of Rao and Ballard (1999) into a modern deep learning framework
while remaining maximally faithful to the original schema. The resulting
network we propose (PreCNet) is tested on a widely used next frame video
prediction benchmark, which consists of images from an urban environment
recorded from a car-mounted camera. On this benchmark (training: 41k images
from KITTI dataset; testing: Caltech Pedestrian dataset), we achieve to our
knowledge the best performance to date when measured with the Structural
Similarity Index (SSIM). Performance on all measures was further improved when
a larger training set (2M images from BDD100k), pointing to the limitations of
the KITTI training set. This work demonstrates that an architecture carefully
based in a neuroscience model, without being explicitly tailored to the task at
hand, can exhibit unprecedented performance.
</p>
<a href="http://arxiv.org/abs/2004.14878" target="_blank">arXiv:2004.14878</a> [<a href="http://arxiv.org/pdf/2004.14878" target="_blank">pdf</a>]

<h2>A Data-Driven Approach for Discovering Stochastic Dynamical Systems with Non-Gaussian Levy Noise. (arXiv:2005.03769v2 [stat.ML] UPDATED)</h2>
<h3>Yang Li, Jinqiao Duan</h3>
<p>With the rapid increase of valuable observational, experimental and
simulating data for complex systems, great efforts are being devoted to
discovering governing laws underlying the evolution of these systems. However,
the existing techniques are limited to extract governing laws from data as
either deterministic differential equations or stochastic differential
equations with Gaussian noise. In the present work, we develop a new
data-driven approach to extract stochastic dynamical systems with non-Gaussian
symmetric L\'evy noise, as well as Gaussian noise. First, we establish a
feasible theoretical framework, by expressing the drift coefficient, diffusion
coefficient and jump measure (i.e., anomalous diffusion) for the underlying
stochastic dynamical system in terms of sample paths data. We then design a
numerical algorithm to compute the drift, diffusion coefficient and jump
measure, and thus extract a governing stochastic differential equation with
Gaussian and non-Gaussian noise. Finally, we demonstrate the efficacy and
accuracy of our approach by applying to several prototypical one-, two- and
three-dimensional systems. This new approach will become a tool in discovering
governing dynamical laws from noisy data sets, from observing or simulating
complex phenomena, such as rare events triggered by random fluctuations with
heavy as well as light tail statistical features.
</p>
<a href="http://arxiv.org/abs/2005.03769" target="_blank">arXiv:2005.03769</a> [<a href="http://arxiv.org/pdf/2005.03769" target="_blank">pdf</a>]

<h2>The Inverse G-Wishart Distribution and Variational Message Passing. (arXiv:2005.09876v3 [stat.ML] UPDATED)</h2>
<h3>L. Maestrini, M.P. Wand</h3>
<p>Message passing on a factor graph is a powerful paradigm for the coding of
approximate inference algorithms for arbitrarily graphical large models. The
notion of a factor graph fragment allows for compartmentalization of algebra
and computer code. We show that the Inverse G-Wishart family of distributions
enables fundamental variational message passing factor graph fragments to be
expressed elegantly and succinctly. Such fragments arise in models for which
approximate inference concerning covariance matrix or variance parameters is
made, and are ubiquitous in contemporary statistics and machine learning.
</p>
<a href="http://arxiv.org/abs/2005.09876" target="_blank">arXiv:2005.09876</a> [<a href="http://arxiv.org/pdf/2005.09876" target="_blank">pdf</a>]

<h2>Unsupervised Geometric Disentanglement for Surfaces via CFAN-VAE. (arXiv:2005.11622v2 [cs.CV] UPDATED)</h2>
<h3>N. Joseph Tatro, Stefan C. Schonsheck, Rongjie Lai</h3>
<p>Geometric disentanglement, the separation of latent codes for intrinsic (i.e.
identity) and extrinsic(i.e. pose) geometry, is a prominent task for generative
models of non-Euclidean data such as 3D deformable models. It provides greater
interpretability of the latent space, and leads to more control in generation.
This work introduces a mesh feature, the conformal factor and normal feature
(CFAN),for use in mesh convolutional autoencoders. We further propose CFAN-VAE,
a novel architecture that disentangles identity and pose using the CFAN
feature. Requiring no label information on the identity or pose during
training, CFAN-VAE achieves geometric disentanglement in an unsupervisedway.
Our comprehensive experiments, including reconstruction, interpolation,
generation, and identity/pose transfer, demonstrate CFAN-VAE achieves
state-of-the-art performance on unsupervised geometric disentanglement. We also
successfully detect a level of geometric disentanglement in mesh convolutional
autoencoders that encode xyz-coordinates directly by registering its latent
space to that of CFAN-VAE.
</p>
<a href="http://arxiv.org/abs/2005.11622" target="_blank">arXiv:2005.11622</a> [<a href="http://arxiv.org/pdf/2005.11622" target="_blank">pdf</a>]

<h2>When Does MAML Objective Have Benign Landscape?. (arXiv:2006.00453v2 [cs.LG] UPDATED)</h2>
<h3>Igor Molybog, Javad Lavaei</h3>
<p>The paper studies the complexity of the optimization problem behind the
Model-Agnostic Meta-Learning (MAML) algorithm. The goal of the study is to
determine the global convergence of MAML on sequential decision-making tasks
possessing a common structure. We are curious to know when, if at all, the
benign landscape of the underlying tasks results in a benign landscape of the
corresponding MAML objective. For illustration, we analyze the landscape of the
MAML objective on LQR tasks to determine what types of similarities in their
structures enable the algorithm to converge to the globally optimal solution.
</p>
<a href="http://arxiv.org/abs/2006.00453" target="_blank">arXiv:2006.00453</a> [<a href="http://arxiv.org/pdf/2006.00453" target="_blank">pdf</a>]

<h2>AlphaGAN: Fully Differentiable Architecture Search for Generative Adversarial Networks. (arXiv:2006.09134v2 [cs.CV] UPDATED)</h2>
<h3>Yuesong Tian, Li Shen, Li Shen, Guinan Su, Zhifeng Li, Wei Liu</h3>
<p>Generative Adversarial Networks (GANs) are formulated as minimax game
problems, whereby generators attempt to approach real data distributions by
virtue of adversarial learning against discriminators. The intrinsic problem
complexity poses the challenge to enhance the performance of generative
networks. In this work, we aim to boost model learning from the perspective of
network architectures, by incorporating recent progress on automated
architecture search into GANs. To this end, we propose a fully differentiable
search framework for generative adversarial networks, dubbed alphaGAN. The
searching process is formalized as solving a bi-level minimax optimization
problem, in which the outer-level objective aims for seeking a suitable network
architecture towards pure Nash Equilibrium conditioned on the generator and the
discriminator network parameters optimized with a traditional GAN loss in the
inner level. The entire optimization performs a first-order method by
alternately minimizing the two-level objective in a fully differentiable
manner, enabling architecture search to be completed in an enormous search
space. Extensive experiments on CIFAR-10 and STL-10 datasets show that our
algorithm can obtain high-performing architectures only with 3-GPU hours on a
single GPU in the search space comprised of approximate 2 ? 1011 possible
configurations. We also provide a comprehensive analysis on the behavior of the
searching process and the properties of searched architectures, which would
benefit further research on architectures for generative models. Pretrained
models and codes are available at https://github.com/yuesongtian/AlphaGAN.
</p>
<a href="http://arxiv.org/abs/2006.09134" target="_blank">arXiv:2006.09134</a> [<a href="http://arxiv.org/pdf/2006.09134" target="_blank">pdf</a>]

<h2>Sparse-RS: a versatile framework for query-efficient sparse black-box adversarial attacks. (arXiv:2006.12834v2 [cs.LG] UPDATED)</h2>
<h3>Francesco Croce, Maksym Andriushchenko, Naman D. Singh, Nicolas Flammarion, Matthias Hein</h3>
<p>Sparse adversarial perturbations received much less attention in the
literature compared to $l_2$- and $l_\infty$-attacks. However, it is equally
important to accurately assess the robustness of a model against sparse
perturbations. Motivated by this goal, we propose a versatile framework based
on random search, Sparse-RS, for score-based sparse targeted and untargeted
attacks in the black-box setting. Sparse-RS does not rely on substitute models
and achieves state-of-the-art success rate and query efficiency for multiple
sparse attack models: $l_0$-bounded perturbations, adversarial patches, and
adversarial frames. Unlike existing methods, the $l_0$-version of untargeted
Sparse-RS achieves almost 100% success rate on ImageNet by perturbing only 0.1%
of the total number of pixels, outperforming all existing white-box attacks
including $l_0$-PGD. Moreover, our untargeted Sparse-RS achieves very high
success rates even for the challenging settings of $20\times20$ adversarial
patches and $2$-pixel wide adversarial frames for $224\times224$ images.
Finally, we show that Sparse-RS can be applied for universal adversarial
patches where it significantly outperforms transfer-based approaches. The code
of our framework is available at https://github.com/fra31/sparse-rs.
</p>
<a href="http://arxiv.org/abs/2006.12834" target="_blank">arXiv:2006.12834</a> [<a href="http://arxiv.org/pdf/2006.12834" target="_blank">pdf</a>]

<h2>Direct Feedback Alignment Scales to Modern Deep Learning Tasks and Architectures. (arXiv:2006.12878v2 [stat.ML] UPDATED)</h2>
<h3>Julien Launay, Iacopo Poli, Fran&#xe7;ois Boniface, Florent Krzakala</h3>
<p>Despite being the workhorse of deep learning, the backpropagation algorithm
is no panacea. It enforces sequential layer updates, thus preventing efficient
parallelization of the training process. Furthermore, its biological
plausibility is being challenged. Alternative schemes have been devised; yet,
under the constraint of synaptic asymmetry, none have scaled to modern deep
learning tasks and architectures. Here, we challenge this perspective, and
study the applicability of Direct Feedback Alignment to neural view synthesis,
recommender systems, geometric learning, and natural language processing. In
contrast with previous studies limited to computer vision tasks, our findings
show that it successfully trains a large range of state-of-the-art deep
learning architectures, with performance close to fine-tuned backpropagation.
At variance with common beliefs, our work supports that challenging tasks can
be tackled in the absence of weight transport.
</p>
<a href="http://arxiv.org/abs/2006.12878" target="_blank">arXiv:2006.12878</a> [<a href="http://arxiv.org/pdf/2006.12878" target="_blank">pdf</a>]

<h2>A Nearest Neighbor Characterization of Lebesgue Points in Metric Measure Spaces. (arXiv:2007.03937v3 [cs.LG] UPDATED)</h2>
<h3>Tommaso Cesari (TSE), Roberto Colomboni (IIT)</h3>
<p>The property of almost every point being a Lebesgue point has proven to be
crucial for the consistency of several classification algorithms based on
nearest neighbors. We characterize Lebesgue points in terms of a 1-Nearest
Neighbor regression algorithm for pointwise estimation, fleshing out the role
played by tie-breaking rules in the corresponding convergence problem. We then
give an application of our results, proving the convergence of the risk of a
large class of 1-Nearest Neighbor classification algorithms in general metric
spaces where almost every point is a Lebesgue point.
</p>
<a href="http://arxiv.org/abs/2007.03937" target="_blank">arXiv:2007.03937</a> [<a href="http://arxiv.org/pdf/2007.03937" target="_blank">pdf</a>]

<h2>Self-Supervised Policy Adaptation during Deployment. (arXiv:2007.04309v2 [cs.LG] UPDATED)</h2>
<h3>Nicklas Hansen, Rishabh Jangir, Yu Sun, Guillem Aleny&#xe0;, Pieter Abbeel, Alexei A. Efros, Lerrel Pinto, Xiaolong Wang</h3>
<p>In most real world scenarios, a policy trained by reinforcement learning in
one environment needs to be deployed in another, potentially quite different
environment. However, generalization across different environments is known to
be hard. A natural solution would be to keep training after deployment in the
new environment, but this cannot be done if the new environment offers no
reward signal. Our work explores the use of self-supervision to allow the
policy to continue training after deployment without using any rewards. While
previous methods explicitly anticipate changes in the new environment, we
assume no prior knowledge of those changes yet still obtain significant
improvements. Empirical evaluations are performed on diverse simulation
environments from DeepMind Control suite and ViZDoom, as well as real robotic
manipulation tasks in continuously changing environments, taking observations
from an uncalibrated camera. Our method improves generalization in 31 out of 36
environments across various tasks and outperforms domain randomization on a
majority of environments.
</p>
<a href="http://arxiv.org/abs/2007.04309" target="_blank">arXiv:2007.04309</a> [<a href="http://arxiv.org/pdf/2007.04309" target="_blank">pdf</a>]

<h2>MASKS: A Multi-Classifier's verification approach. (arXiv:2007.10090v2 [cs.AI] UPDATED)</h2>
<h3>Amirhoshang Hoseinpour Dehkordi, Majid Alizadeh, Ebrahim Ardeshir-Larijani, Ali Movaghar</h3>
<p>Classifiers are one of the most widely applied approaches in Artificial
Intelligence (AI). However, the employment of classifiers in critical
applications would render any errors in these systems more consequential;
particularly due to the lack of formal verification methods in these systems.
This study aims to develop a verification method that eliminates errors through
the integration of multiple classifiers. In order to do this, primarily, we
have defined a special property for the classifiers which extracts the
knowledge of these classifiers. Secondly, we have designed a multi-agent
system, comprised of multiple classifiers, in order to check the satisfaction
of the aforementioned special property. Also, in order to help examine the
reasoning concerning the aggregation of the distributed knowledge, itself
gained through the combined effort of separate classifiers and acquired
external information sources, a dynamic epistemic logic-based method has been
proposed. Our proposed model is capable of verifying itself given specific
inputs if the cumulative knowledge of the entire system proves their
correctness, which results in self-awareness of this system. Finally, we
applied this model to the MNIST dataset, and it successfully reduced the error
rate to approximately one-tenth of the individual classifiers. In conclusion,
we have formulated and developed a Multi-Agent Systems' Knowledge-Sharing
algorithm (MASKS) and verified its utility compared to individual classifiers
using the MNIST dataset.
</p>
<a href="http://arxiv.org/abs/2007.10090" target="_blank">arXiv:2007.10090</a> [<a href="http://arxiv.org/pdf/2007.10090" target="_blank">pdf</a>]

<h2>Multi-Output Gaussian Processes with Functional Data: A Study on Coastal Flood Hazard Assessment. (arXiv:2007.14052v2 [stat.ML] UPDATED)</h2>
<h3>A. F. L&#xf3;pez-Lopera, D. Idier, J. Rohmer, F. Bachoc</h3>
<p>Most of the existing coastal flood Forecast and Early-Warning Systems do not
model the flood, but instead, rely on the prediction of hydrodynamic conditions
at the coast and on expert judgment. Recent scientific contributions are now
capable to precisely model flood events, even in situations where wave
overtopping plays a significant role. Such models are nevertheless
costly-to-evaluate and surrogate ones need to be exploited for substantial
computational savings. For the latter models, the hydro-meteorological forcing
conditions (inputs) or flood events (outputs) are conveniently parametrised
into scalar representations. However, they neglect the fact that inputs are
actually functions (more precisely, time series), and that floods spatially
propagate inland. Here, we introduce a multi-output Gaussian process model
accounting for both criteria. On various examples, we test its versatility for
both learning spatial maps and inferring unobserved ones. We demonstrate that
efficient implementations are obtained by considering tensor-structured data
and/or sparse-variational approximations. Finally, the proposed framework is
applied on a coastal application aiming at predicting flood events. We conclude
that accurate predictions are obtained in the order of minutes rather than the
couples of days required by dedicated hydrodynamic simulators.
</p>
<a href="http://arxiv.org/abs/2007.14052" target="_blank">arXiv:2007.14052</a> [<a href="http://arxiv.org/pdf/2007.14052" target="_blank">pdf</a>]

<h2>Geometric Interpretations of the Normalized Epipolar Error. (arXiv:2008.01254v6 [cs.CV] UPDATED)</h2>
<h3>Seong Hun Lee, Javier Civera</h3>
<p>In this work, we provide geometric interpretations of the normalized epipolar
error. Most notably, we show that it is directly related to the following
quantities: (1) the shortest distance between the two backprojected rays, (2)
the dihedral angle between the two bounding epipolar planes, and (3) the
$L_1$-optimal angular reprojection error.
</p>
<a href="http://arxiv.org/abs/2008.01254" target="_blank">arXiv:2008.01254</a> [<a href="http://arxiv.org/pdf/2008.01254" target="_blank">pdf</a>]

<h2>Future Trends for Human-AI Collaboration: A Comprehensive Taxonomy of AI/AGI Using Multiple Intelligences and Learning Styles. (arXiv:2008.04793v4 [cs.AI] UPDATED)</h2>
<h3>Andrzej Cichocki, Alexander P. Kuleshov</h3>
<p>This article discusses some trends and concepts in developing new generation
of future Artificial General Intelligence (AGI) systems which relate to complex
facets and different types of human intelligence, especially social, emotional,
attentional and ethical intelligence. We describe various aspects of multiple
human intelligences and learning styles, which may impact on a variety of AI
problem domains. Using the concept of 'multiple intelligences' rather than a
single type of intelligence, we categorize and provide working definitions of
various AGI depending on their cognitive skills or capacities. Future AI
systems will be able not only to communicate with human users and each other,
but also to efficiently exchange knowledge and wisdom with abilities of
cooperation, collaboration and even co-creating something new and valuable and
have meta-learning capacities. Multi-agent systems such as these can be used to
solve problems that would be difficult to solve by any individual intelligent
agent.

Key words: Artificial General Intelligence (AGI), multiple intelligences,
learning styles, physical intelligence, emotional intelligence, social
intelligence, attentional intelligence, moral-ethical intelligence, responsible
decision making, creative-innovative intelligence, cognitive functions,
meta-learning of AI systems.
</p>
<a href="http://arxiv.org/abs/2008.04793" target="_blank">arXiv:2008.04793</a> [<a href="http://arxiv.org/pdf/2008.04793" target="_blank">pdf</a>]

<h2>Explainability in Deep Reinforcement Learning. (arXiv:2008.06693v3 [cs.AI] UPDATED)</h2>
<h3>Alexandre Heuillet, Fabien Couthouis, Natalia D&#xed;az-Rodr&#xed;guez</h3>
<p>A large set of the explainable Artificial Intelligence (XAI) literature is
emerging on feature relevance techniques to explain a deep neural network (DNN)
output or explaining models that ingest image source data. However, assessing
how XAI techniques can help understand models beyond classification tasks, e.g.
for reinforcement learning (RL), has not been extensively studied. We review
recent works in the direction to attain Explainable Reinforcement Learning
(XRL), a relatively new subfield of Explainable Artificial Intelligence,
intended to be used in general public applications, with diverse audiences,
requiring ethical, responsible and trustable algorithms. In critical situations
where it is essential to justify and explain the agent's behaviour, better
explainability and interpretability of RL models could help gain scientific
insight on the inner workings of what is still considered a black box. We
evaluate mainly studies directly linking explainability to RL, and split these
into two categories according to the way the explanations are generated:
transparent algorithms and post-hoc explainaility. We also review the most
prominent XAI works from the lenses of how they could potentially enlighten the
further deployment of the latest advances in RL, in the demanding present and
future of everyday problems.
</p>
<a href="http://arxiv.org/abs/2008.06693" target="_blank">arXiv:2008.06693</a> [<a href="http://arxiv.org/pdf/2008.06693" target="_blank">pdf</a>]

<h2>We Learn Better Road Pothole Detection: from Attention Aggregation to Adversarial Domain Adaptation. (arXiv:2008.06840v2 [cs.CV] UPDATED)</h2>
<h3>Rui Fan, Hengli Wang, Mohammud J. Bocus, Ming Liu</h3>
<p>Manual visual inspection performed by certified inspectors is still the main
form of road pothole detection. This process is, however, not only tedious,
time-consuming and costly, but also dangerous for the inspectors. Furthermore,
the road pothole detection results are always subjective, because they depend
entirely on the individual experience. Our recently introduced disparity (or
inverse depth) transformation algorithm allows better discrimination between
damaged and undamaged road areas, and it can be easily deployed to any semantic
segmentation network for better road pothole detection results. To boost the
performance, we propose a novel attention aggregation (AA) framework, which
takes the advantages of different types of attention modules. In addition, we
develop an effective training set augmentation technique based on adversarial
domain adaptation, where the synthetic road RGB images and transformed road
disparity (or inverse depth) images are generated to enhance the training of
semantic segmentation networks. The experimental results demonstrate that,
firstly, the transformed disparity (or inverse depth) images become more
informative; secondly, AA-UNet and AA-RTFNet, our best performing
implementations, respectively outperform all other state-of-the-art
single-modal and data-fusion networks for road pothole detection; and finally,
the training set augmentation technique based on adversarial domain adaptation
not only improves the accuracy of the state-of-the-art semantic segmentation
networks, but also accelerates their convergence.
</p>
<a href="http://arxiv.org/abs/2008.06840" target="_blank">arXiv:2008.06840</a> [<a href="http://arxiv.org/pdf/2008.06840" target="_blank">pdf</a>]

<h2>How to Put Users in Control of their Data in Federated Top-N Recommendation with Learning to Rank. (arXiv:2008.07192v3 [cs.LG] UPDATED)</h2>
<h3>Vito Walter Anelli, Yashar Deldjoo, Tommaso Di Noia, Antonio Ferrara, Fedelucio Narducci</h3>
<p>Recommendation services are extensively adopted in several user-centered
applications as a tool to alleviate the information overload problem and help
users in orienteering in a vast space of possible choices. In such scenarios,
data ownership is a crucial concern since users may not be willing to share
their sensitive preferences (e.g., visited locations) with a central server.
Unfortunately, data harvesting and collection is at the basis of modern,
state-of-the-art approaches to recommendation. To address this issue, we
present FPL, an architecture in which users collaborate in training a central
factorization model while controlling the amount of sensitive data leaving
their devices. The proposed approach implements pair-wise learning-to-rank
optimization by following the Federated Learning principles, originally
conceived to mitigate the privacy risks of traditional machine learning. The
public implementation is available at https://split.to/sisinflab-fpl.
</p>
<a href="http://arxiv.org/abs/2008.07192" target="_blank">arXiv:2008.07192</a> [<a href="http://arxiv.org/pdf/2008.07192" target="_blank">pdf</a>]

<h2>Skeleton-based Action Recognition via Spatial and Temporal Transformer Networks. (arXiv:2008.07404v3 [cs.CV] UPDATED)</h2>
<h3>Chiara Plizzari, Marco Cannici, Matteo Matteucci</h3>
<p>Skeleton-based Human Activity Recognition has achieved great interest in
recent years as skeleton data has demonstrated being robust to illumination
changes, body scales, dynamic camera views, and complex background. In
particular, Spatial-Temporal Graph Convolutional Networks (ST-GCN) demonstrated
to be effective in learning both spatial and temporal dependencies on
non-Euclidean data such as skeleton graphs. Nevertheless, an effective encoding
of the latent information underlying the 3D skeleton is still an open problem,
especially when it comes to extracting effective information from joint motion
patterns and their correlations. In this work, we propose a novel
Spatial-Temporal Transformer network (ST-TR) which models dependencies between
joints using the Transformer self-attention operator. In our ST-TR model, a
Spatial Self-Attention module (SSA) is used to understand intra-frame
interactions between different body parts, and a Temporal Self-Attention module
(TSA) to model inter-frame correlations. The two are combined in a two-stream
network, whose performance is evaluated on three large-scale datasets,
NTU-RGB+D 60, NTU-RGB+D 120, and Kinetics Skeleton 400, outperforming the
state-of-the-art on NTU-RGB+D w.r.t. models using the same input data, i.e.,
joint information.
</p>
<a href="http://arxiv.org/abs/2008.07404" target="_blank">arXiv:2008.07404</a> [<a href="http://arxiv.org/pdf/2008.07404" target="_blank">pdf</a>]

<h2>m2caiSeg: Semantic Segmentation of Laparoscopic Images using Convolutional Neural Networks. (arXiv:2008.10134v2 [cs.CV] UPDATED)</h2>
<h3>Salman Maqbool, Aqsa Riaz, Hasan Sajid, Osman Hasan</h3>
<p>Autonomous surgical procedures, in particular minimal invasive surgeries, are
the next frontier for Artificial Intelligence research. However, the existing
challenges include precise identification of the human anatomy and the surgical
settings, and modeling the environment for training of an autonomous agent. To
address the identification of human anatomy and the surgical settings, we
propose a deep learning based semantic segmentation algorithm to identify and
label the tissues and organs in the endoscopic video feed of the human torso
region. We present an annotated dataset, m2caiSeg, created from endoscopic
video feeds of real-world surgical procedures. Overall, the data consists of
307 images, each of which is annotated for the organs and different surgical
instruments present in the scene. We propose and train a deep convolutional
neural network for the semantic segmentation task. To cater for the low
quantity of annotated data, we use unsupervised pre-training and data
augmentation. The trained model is evaluated on an independent test set of the
proposed dataset. We obtained a F1 score of 0.33 while using all the labeled
categories for the semantic segmentation task. Secondly, we labeled all
instruments into an 'Instruments' superclass to evaluate the model's
performance on discerning the various organs and obtained a F1 score of 0.57.
We propose a new dataset and a deep learning method for pixel level
identification of various organs and instruments in a endoscopic surgical
scene. Surgical scene understanding is one of the first steps towards
automating surgical procedures.
</p>
<a href="http://arxiv.org/abs/2008.10134" target="_blank">arXiv:2008.10134</a> [<a href="http://arxiv.org/pdf/2008.10134" target="_blank">pdf</a>]

<h2>Attr2Style: A Transfer Learning Approach for Inferring Fashion Styles via Apparel Attributes. (arXiv:2008.11662v2 [cs.CV] UPDATED)</h2>
<h3>Rajdeep Hazra Banerjee, Abhinav Ravi, Ujjal Kr Dutta</h3>
<p>Popular fashion e-commerce platforms mostly provide details about low-level
attributes of an apparel (eg, neck type, dress length, collar type) on their
product detail pages. However, customers usually prefer to buy apparel based on
their style information, or simply put, occasion (eg, party/ sports/ casual
wear). Application of a supervised image-captioning model to generate
style-based image captions is limited because obtaining ground-truth
annotations in the form of style-based captions is difficult. This is because
annotating style-based captions requires a certain amount of fashion domain
expertise, and also adds to the costs and manual effort. On the contrary,
low-level attribute based annotations are much more easily available. To
address this issue, we propose a transfer-learning based image captioning model
that is trained on a source dataset with sufficient attribute-based
ground-truth captions, and used to predict style-based captions on a target
dataset. The target dataset has only a limited amount of images with
style-based ground-truth captions. The main motivation of our approach comes
from the fact that most often there are correlations among the low-level
attributes and the higher-level styles for an apparel. We leverage this fact
and train our model in an encoder-decoder based framework using attention
mechanism. In particular, the encoder of the model is first trained on the
source dataset to obtain latent representations capturing the low-level
attributes. The trained model is fine-tuned to generate style-based captions
for the target dataset. To highlight the effectiveness of our method, we
qualitatively and quantitatively demonstrate that the captions generated by our
approach are close to the actual style information for the evaluated apparel. A
Proof Of Concept for our model is under pilot at Myntra where it is exposed to
some internal users for feedback.
</p>
<a href="http://arxiv.org/abs/2008.11662" target="_blank">arXiv:2008.11662</a> [<a href="http://arxiv.org/pdf/2008.11662" target="_blank">pdf</a>]

<h2>AttnGrounder: Talking to Cars with Attention. (arXiv:2009.05684v2 [cs.CV] UPDATED)</h2>
<h3>Vivek Mittal</h3>
<p>We propose Attention Grounder (AttnGrounder), a single-stage end-to-end
trainable model for the task of visual grounding. Visual grounding aims to
localize a specific object in an image based on a given natural language text
query. Unlike previous methods that use the same text representation for every
image region, we use a visual-text attention module that relates each word in
the given query with every region in the corresponding image for constructing a
region dependent text representation. Furthermore, for improving the
localization ability of our model, we use our visual-text attention module to
generate an attention mask around the referred object. The attention mask is
trained as an auxiliary task using a rectangular mask generated with the
provided ground-truth coordinates. We evaluate AttnGrounder on the Talk2Car
dataset and show an improvement of 3.26% over the existing methods.
</p>
<a href="http://arxiv.org/abs/2009.05684" target="_blank">arXiv:2009.05684</a> [<a href="http://arxiv.org/pdf/2009.05684" target="_blank">pdf</a>]

<h2>Zero-shot Synthesis with Group-Supervised Learning. (arXiv:2009.06586v2 [cs.CV] UPDATED)</h2>
<h3>Yunhao Ge, Sami Abu-El-Haija, Gan Xin, Laurent Itti</h3>
<p>Visual cognition of primates is superior to that of artificial neural
networks in its ability to 'envision' a visual object, even a newly-introduced
one, in different attributes including pose, position, color, texture, etc. To
aid neural networks to envision objects with different attributes, we propose a
family of objective functions, expressed on groups of examples, as a novel
learning framework that we term Group-Supervised Learning (GSL). GSL decomposes
inputs into a disentangled representation with swappable components that can be
recombined to synthesize new samples, trained through similarity mining within
groups of exemplars. For instance, images of red boats &amp; blue cars can be
decomposed and recombined to synthesize novel images of red cars. We describe a
general class of datasets admissible by GSL. We propose an implementation based
on auto-encoder, termed group-supervised zero-shot synthesis network (GZS-Net)
trained with our learning framework, that can produce a high-quality red car
even if no such example is witnessed during training. We test our model and
learning framework on existing benchmarks, in addition to new dataset that we
open-source. We qualitatively and quantitatively demonstrate that GZS-Net
trained with GSL outperforms state-of-the-art methods
</p>
<a href="http://arxiv.org/abs/2009.06586" target="_blank">arXiv:2009.06586</a> [<a href="http://arxiv.org/pdf/2009.06586" target="_blank">pdf</a>]

<h2>Pea-KD: Parameter-efficient and Accurate Knowledge Distillation on BERT. (arXiv:2009.14822v2 [cs.LG] UPDATED)</h2>
<h3>Ikhyun Cho, U Kang</h3>
<p>How can we efficiently compress a model while maintaining its performance?
Knowledge Distillation (KD) is one of the widely known methods for model
compression. In essence, KD trains a smaller student model based on a larger
teacher model and tries to retain the teacher model's level of performance as
much as possible. However, existing KD methods suffer from the following
limitations. First, since the student model is smaller in absolute size, it
inherently lacks model capacity. Second, the absence of an initial guide for
the student model makes it difficult for the student to imitate the teacher
model to its fullest. Conventional KD methods yield low performance due to
these limitations. In this paper, we propose Pea-KD (Parameter-efficient and
accurate Knowledge Distillation), a novel approach to KD. Pea-KD consists of
two main parts: Shuffled Parameter Sharing (SPS) and Pretraining with Teacher's
Predictions (PTP). Using this combination, we are capable of alleviating the
KD's limitations. SPS is a new parameter sharing method that increases the
student model capacity. PTP is a KD-specialized initialization method, which
can act as a good initial guide for the student. When combined, this method
yields a significant increase in student model's performance. Experiments
conducted on BERT with different datasets and tasks show that the proposed
approach improves the student model's performance by 4.4\% on average in four
GLUE tasks, outperforming existing KD baselines by significant margins.
</p>
<a href="http://arxiv.org/abs/2009.14822" target="_blank">arXiv:2009.14822</a> [<a href="http://arxiv.org/pdf/2009.14822" target="_blank">pdf</a>]

<h2>Accelerating Simulation of Stiff Nonlinear Systems using Continuous-Time Echo State Networks. (arXiv:2010.04004v4 [cs.LG] UPDATED)</h2>
<h3>Ranjan Anantharaman, Yingbo Ma, Shashi Gowda, Chris Laughman, Viral Shah, Alan Edelman, Chris Rackauckas</h3>
<p>Modern design, control, and optimization often requires simulation of highly
nonlinear models, leading to prohibitive computational costs. These costs can
be amortized by evaluating a cheap surrogate of the full model. Here we present
a general data-driven method, the continuous-time echo state network (CTESN),
for generating surrogates of nonlinear ordinary differential equations with
dynamics at widely separated timescales. We empirically demonstrate
near-constant time performance using our CTESNs on a physically motivated
scalable model of a heating system whose full execution time increases
exponentially, while maintaining relative error of within 0.2 %. We also show
that our model captures fast transients as well as slow dynamics effectively,
while other techniques such as physics informed neural networks have
difficulties trying to train and predict the highly nonlinear behavior of these
models.
</p>
<a href="http://arxiv.org/abs/2010.04004" target="_blank">arXiv:2010.04004</a> [<a href="http://arxiv.org/pdf/2010.04004" target="_blank">pdf</a>]

<h2>Ordinal Neural Network Transformation Models: Deep and interpretable regression models for ordinal outcomes. (arXiv:2010.08376v3 [stat.ML] UPDATED)</h2>
<h3>Lucas Kook, Lisa Herzog, Torsten Hothorn, Oliver D&#xfc;rr, Beate Sick</h3>
<p>Outcomes with a natural order commonly occur in prediction tasks and
oftentimes the available input data are a mixture of complex data, like images,
and tabular predictors. Deep Learning (DL) methods are state-of-the-art for
image classification tasks but frequently treat ordinal outcomes as unordered
and lack interpretability. In contrast, classical ordinal regression models
consider the outcome's order and yield interpretable predictor effects but are
limited to tabular data. We present ordinal neural network transformation
models (ONTRAMs), which unite DL with classical ordinal regression methods.
ONTRAMs are a special case of transformation models and trade off flexibility
and interpretability by additively decomposing the transformation function into
terms for image and tabular data using jointly trained neural networks. We
discuss how to interpret model components for both tabular and image data. The
proposed ONTRAMs achieve on-par performance with common DL models while being
directly interpretable and more efficient in training.
</p>
<a href="http://arxiv.org/abs/2010.08376" target="_blank">arXiv:2010.08376</a> [<a href="http://arxiv.org/pdf/2010.08376" target="_blank">pdf</a>]

<h2>Goal directed molecule generation using Monte Carlo Tree Search. (arXiv:2010.16399v2 [cs.LG] UPDATED)</h2>
<h3>Anand A. Rajasekar, Karthik Raman, Balaraman Ravindran</h3>
<p>One challenging and essential task in biochemistry is the generation of novel
molecules with desired properties. Novel molecule generation remains a
challenge since the molecule space is difficult to navigate through, and the
generated molecules should obey the rules of chemical valency. Through this
work, we propose a novel method, which we call unitMCTS, to perform molecule
generation by making a unit change to the molecule at every step using Monte
Carlo Tree Search. We show that this method outperforms the recently published
techniques on benchmark molecular optimization tasks such as QED and penalized
logP. We also demonstrate the usefulness of this method in improving molecule
properties while being similar to the starting molecule. Given that there is no
learning involved, our method finds desired molecules within a shorter amount
of time.
</p>
<a href="http://arxiv.org/abs/2010.16399" target="_blank">arXiv:2010.16399</a> [<a href="http://arxiv.org/pdf/2010.16399" target="_blank">pdf</a>]

<h2>Nonparametric Variable Screening with Optimal Decision Stumps. (arXiv:2011.02683v2 [stat.ML] UPDATED)</h2>
<h3>Jason M. Klusowski, Peter M. Tian</h3>
<p>Decision trees and their ensembles are endowed with a rich set of diagnostic
tools for ranking and screening variables in a predictive model. Despite the
widespread use of tree based variable importance measures, pinning down their
theoretical properties has been challenging and therefore largely unexplored.
To address this gap between theory and practice, we derive finite sample
performance guarantees for variable selection in nonparametric models using a
single-level CART decision tree (a decision stump). Under standard operating
assumptions in variable screening literature, we find that the marginal signal
strength of each variable and ambient dimensionality can be considerably weaker
and higher, respectively, than state-of-the-art nonparametric variable
selection methods. Furthermore, unlike previous marginal screening methods that
attempt to directly estimate each marginal projection via a truncated basis
expansion, the fitted model used here is a simple, parsimonious decision stump,
thereby eliminating the need for tuning the number of basis terms. Thus,
surprisingly, even though decision stumps are highly inaccurate for estimation
purposes, they can still be used to perform consistent model selection.
</p>
<a href="http://arxiv.org/abs/2011.02683" target="_blank">arXiv:2011.02683</a> [<a href="http://arxiv.org/pdf/2011.02683" target="_blank">pdf</a>]

<h2>Architecture Agnostic Neural Networks. (arXiv:2011.02712v2 [cs.LG] UPDATED)</h2>
<h3>Sabera Talukder, Guruprasad Raghavan, Yisong Yue</h3>
<p>In this paper, we explore an alternate method for synthesizing neural network
architectures, inspired by the brain's stochastic synaptic pruning. During a
person's lifetime, numerous distinct neuronal architectures are responsible for
performing the same tasks. This indicates that biological neural networks are,
to some degree, architecture agnostic. However, artificial networks rely on
their fine-tuned weights and hand-crafted architectures for their remarkable
performance. This contrast begs the question: Can we build artificial
architecture agnostic neural networks? To ground this study we utilize sparse,
binary neural networks that parallel the brain's circuits. Within this sparse,
binary paradigm we sample many binary architectures to create families of
architecture agnostic neural networks not trained via backpropagation. These
high-performing network families share the same sparsity, distribution of
binary weights, and succeed in both static and dynamic tasks. In summation, we
create an architecture manifold search procedure to discover families or
architecture agnostic neural networks.
</p>
<a href="http://arxiv.org/abs/2011.02712" target="_blank">arXiv:2011.02712</a> [<a href="http://arxiv.org/pdf/2011.02712" target="_blank">pdf</a>]

<h2>Deep traffic light detection by overlaying synthetic context on arbitrary natural images. (arXiv:2011.03841v3 [cs.CV] UPDATED)</h2>
<h3>Jean Pablo Vieira de Mello, Lucas Tabelini, Rodrigo F. Berriel, Thiago M. Paix&#xe3;o, Alberto F. de Souza, Claudine Badue, Nicu Sebe, Thiago Oliveira-Santos</h3>
<p>Deep neural networks come as an effective solution to many problems
associated with autonomous driving. By providing real image samples with
traffic context to the network, the model learns to detect and classify
elements of interest, such as pedestrians, traffic signs, and traffic lights.
However, acquiring and annotating real data can be extremely costly in terms of
time and effort. In this context, we propose a method to generate artificial
traffic-related training data for deep traffic light detectors. This data is
generated using basic non-realistic computer graphics to blend fake traffic
scenes on top of arbitrary image backgrounds that are not related to the
traffic domain. Thus, a large amount of training data can be generated without
annotation efforts. Furthermore, it also tackles the intrinsic data imbalance
problem in traffic light datasets, caused mainly by the low amount of samples
of the yellow state. Experiments show that it is possible to achieve results
comparable to those obtained with real training data from the problem domain,
yielding an average mAP and an average F1-score which are each nearly 4 p.p.
higher than the respective metrics obtained with a real-world reference model.
</p>
<a href="http://arxiv.org/abs/2011.03841" target="_blank">arXiv:2011.03841</a> [<a href="http://arxiv.org/pdf/2011.03841" target="_blank">pdf</a>]

<h2>Synthetic Data -- A Privacy Mirage. (arXiv:2011.07018v2 [cs.LG] UPDATED)</h2>
<h3>Theresa Stadler, Bristena Oprisanu, Carmela Troncoso</h3>
<p>Synthetic datasets drawn from generative models have been advertised as a
silver-bullet solution to privacy-preserving data publishing. In this work, we
show through an extensive privacy evaluation that such claims do not match
reality. First, synthetic data does not prevent attribute inference. Any data
characteristics preserved by a generative model for the purpose of data
analysis, can simultaneously be used by an adversary to reconstruct sensitive
information about individuals. Second, synthetic data does not protect against
linkage attacks. We demonstrate that high-dimensional synthetic datasets
preserve much more information about the raw data than the features in the
model's lower-dimensional approximation. This rich information can be exploited
by an adversary even when models are trained under differential privacy.
Moreover, we observe that some target records receive substantially less
protection than others and that the more complex the generative model, the more
difficult it is to predict which targets will remain vulnerable to inference
attacks. Finally, we show why generative models are unlikely to ever become an
appropriate solution to the problem of privacy-preserving data publishing.
</p>
<a href="http://arxiv.org/abs/2011.07018" target="_blank">arXiv:2011.07018</a> [<a href="http://arxiv.org/pdf/2011.07018" target="_blank">pdf</a>]

<h2>Enhanced 3DMM Attribute Control via Synthetic Dataset Creation Pipeline. (arXiv:2011.12833v2 [cs.CV] UPDATED)</h2>
<h3>Wonwoong Cho, Inyeop Lee, David Inouye</h3>
<p>While facial attribute manipulation of 2D images via Generative Adversarial
Networks (GANs) has become common in computer vision and graphics due to its
many practical uses, research on 3D attribute manipulation is relatively
undeveloped. Existing 3D attribute manipulation methods are limited because the
same semantic changes are applied to every 3D face. The key challenge for
developing better 3D attribute control methods is the lack of paired training
data in which one attribute is changed while other attributes are held fixed --
e.g., a pair of 3D faces where one is male and the other is female but all
other attributes, such as race and expression, are the same. To overcome this
challenge, we design a novel pipeline for generating paired 3D faces by
harnessing the power of GANs. On top of this pipeline, we then propose an
enhanced non-linear 3D conditional attribute controller that increases the
precision and diversity of 3D attribute control compared to existing methods.
We demonstrate the validity of our dataset creation pipeline and the superior
performance of our conditional attribute controller via quantitative and
qualitative evaluations.
</p>
<a href="http://arxiv.org/abs/2011.12833" target="_blank">arXiv:2011.12833</a> [<a href="http://arxiv.org/pdf/2011.12833" target="_blank">pdf</a>]

<h2>Estimation of Trocar and Tool Interaction Forces on the da Vinci Research Kit with Two-Step Deep Learning. (arXiv:2012.01479v2 [cs.RO] UPDATED)</h2>
<h3>Jie Ying Wu, Nural Yilmaz, Peter Kazanzides, Ugur Tumerdem</h3>
<p>Measurement of environment interaction forces during robotic
minimally-invasive surgery would enable haptic feedback to the surgeon, thereby
solving one long-standing limitation. Estimating this force from existing
sensor data avoids the challenge of retrofitting systems with force sensors,
but is difficult due to mechanical effects such as friction and compliance in
the robot mechanism. We have previously shown that neural networks can be
trained to estimate the internal robot joint torques, thereby enabling
estimation of external forces. In this work, we extend the method to estimate
external Cartesian forces and torques, and also present a two-step approach to
adapt to the specific surgical setup by compensating for forces due to the
interactions between the instrument shaft and cannula seal and between the
trocar and patient body. Experiments show that this approach provides estimates
of external forces and torques within a mean root-mean-square error (RMSE) of 2
N and 0.08 Nm, respectively. Furthermore, the two-step approach can add as
little as 5 minutes to the surgery setup time, with about 4 minutes to collect
intraoperative training data and 1 minute to train the second-step network.
</p>
<a href="http://arxiv.org/abs/2012.01479" target="_blank">arXiv:2012.01479</a> [<a href="http://arxiv.org/pdf/2012.01479" target="_blank">pdf</a>]

<h2>Graph Neural Networks for Improved El Ni\~no Forecasting. (arXiv:2012.01598v2 [cs.LG] UPDATED)</h2>
<h3>Salva R&#xfc;hling Cachay, Emma Erickson, Arthur Fender C. Bucker, Ernest Pokropek, Willa Potosnak, Salomey Osei, Bj&#xf6;rn L&#xfc;tjens</h3>
<p>Deep learning-based models have recently outperformed state-of-the-art
seasonal forecasting models, such as for predicting El Ni\~no-Southern
Oscillation (ENSO). However, current deep learning models are based on
convolutional neural networks which are difficult to interpret and can fail to
model large-scale atmospheric patterns called teleconnections. Hence, we
propose the application of spatiotemporal Graph Neural Networks (GNN) to
forecast ENSO at long lead times, finer granularity and improved predictive
skill than current state-of-the-art methods. The explicit modeling of
information flow via edges may also allow for more interpretable forecasts.
Preliminary results are promising and outperform state-of-the art systems for
projections 1 and 3 months ahead.
</p>
<a href="http://arxiv.org/abs/2012.01598" target="_blank">arXiv:2012.01598</a> [<a href="http://arxiv.org/pdf/2012.01598" target="_blank">pdf</a>]

<h2>Folding and Unfolding on Metagraphs. (arXiv:2012.01759v3 [cs.AI] UPDATED)</h2>
<h3>Ben Goertzel</h3>
<p>Typed metagraphs are defined as hypergraphs with types assigned to hyperedges
and their targets, and the potential to have targets of hyperedges connect to
whole links as well as targets. Directed typed metagraphs (DTMGs) are
introduced via partitioning the targets of each edge in a typed metagraph into
input, output and lateral sets; one can then look at "metapaths" in which
edges' output-sets are linked to other edges' input-sets. An initial algebra
approach to DTMGs is presented, including introduction of constructors for
building up DTMGs and laws regarding relationships among multiple ways of using
these constructors. A menagerie of useful morphism types is then defined on
DTMGs (catamorphisms, anamorphisms, histomorphisms, futumorphisms,
hylomorphisms, chronomorphisms, metamorphisms and metachronomorphisms),
providing a general abstract framework for formulating a broad variety of
metagraph operations. Deterministic and stochastic processes on typed
metagraphs are represented in terms of forests of DTMGs defined over a common
TMG, where the various morphisms can be straightforwardly extended to these
forests. A variation of the approach to undirected typed metagraphs is
presented; and it is indicated how the framework outlined can applied to
realistic metagraphs involving complexities like dependent and probabilistic
types, multidimensional values and dynamic processing including insertion and
deletion of edges.
</p>
<a href="http://arxiv.org/abs/2012.01759" target="_blank">arXiv:2012.01759</a> [<a href="http://arxiv.org/pdf/2012.01759" target="_blank">pdf</a>]

<h2>Domain Adaptation on Semantic Segmentation for Aerial Images. (arXiv:2012.02264v2 [cs.CV] UPDATED)</h2>
<h3>Ying Chen, Xu Ouyang, Kaiyue Zhu, Gady Agam</h3>
<p>Semantic segmentation has achieved significant advances in recent years.
While deep neural networks perform semantic segmentation well, their success
rely on pixel level supervision which is expensive and time-consuming. Further,
training using data from one domain may not generalize well to data from a new
domain due to a domain gap between data distributions in the different domains.
This domain gap is particularly evident in aerial images where visual
appearance depends on the type of environment imaged, season, weather, and time
of day when the environment is imaged. Subsequently, this distribution gap
leads to severe accuracy loss when using a pretrained segmentation model to
analyze new data with different characteristics. In this paper, we propose a
novel unsupervised domain adaptation framework to address domain shift in the
context of aerial semantic image segmentation. To this end, we solve the
problem of domain shift by learn the soft label distribution difference between
the source and target domains. Further, we also apply entropy minimization on
the target domain to produce high-confident prediction rather than using
high-confident prediction by pseudo-labeling. We demonstrate the effectiveness
of our domain adaptation framework using the challenge image segmentation
dataset of ISPRS, and show improvement over state-of-the-art methods in terms
of various metrics.
</p>
<a href="http://arxiv.org/abs/2012.02264" target="_blank">arXiv:2012.02264</a> [<a href="http://arxiv.org/pdf/2012.02264" target="_blank">pdf</a>]

<h2>Food Classification with Convolutional Neural Networks and Multi-Class Linear Discernment Analysis. (arXiv:2012.03170v2 [cs.CV] UPDATED)</h2>
<h3>Joshua Ball</h3>
<p>Convolutional neural networks (CNNs) have been successful in representing the
fully-connected inferencing ability perceived to be seen in the human brain:
they take full advantage of the hierarchy-style patterns commonly seen in
complex data and develop more patterns using simple features. Countless
implementations of CNNs have shown how strong their ability is to learn these
complex patterns, particularly in the realm of image classification. However,
the cost of getting a high performance CNN to a so-called "state of the art"
level is computationally costly. Even when using transfer learning, which
utilize the very deep layers from models such as MobileNetV2, CNNs still take a
great amount of time and resources. Linear discriminant analysis (LDA), a
generalization of Fisher's linear discriminant, can be implemented in a
multi-class classification method to increase separability of class features
while not needing a high performance system to do so for image classification.
Similarly, we also believe LDA has great promise in performing well. In this
paper, we discuss our process of developing a robust CNN for food
classification as well as our effective implementation of multi-class LDA and
prove that (1) CNN is superior to LDA for image classification and (2) why LDA
should not be left out of the races for image classification, particularly for
binary cases.
</p>
<a href="http://arxiv.org/abs/2012.03170" target="_blank">arXiv:2012.03170</a> [<a href="http://arxiv.org/pdf/2012.03170" target="_blank">pdf</a>]

<h2>Traffic flow prediction using Deep Sedenion Networks. (arXiv:2012.03874v2 [cs.CV] UPDATED)</h2>
<h3>Alabi Bojesomo, Panos Liatsis, Hasan Al Marzouqi</h3>
<p>In this paper, we present our solution to the Traffic4cast2020 traffic
prediction challenge. In this competition, participants are to predict future
traffic parameters (speed and volume) in three different cities: Berlin,
Istanbul and Moscow. The information provided includes nine channels where the
first eight represent the speed and volume for four different direction of
traffic (NE, NW, SE and SW), while the last channel is used to indicate
presence of traffic incidents. The expected output should have the first 8
channels of the input at six future timing intervals (5, 10, 15, 30, 45, and
60min), while a one hour duration of past traffic data, in 5mins intervals, are
provided as input. We solve the problem using a novel sedenion U-Net neural
network. Sedenion networks provide the means for efficient encoding of
correlated multimodal datasets. We use 12 of the 15 sedenion imaginary parts
for the dynamic inputs and the real sedenion component is used for the static
input. The sedenion output of the network is used to represent the multimodal
traffic predictions. Proposed system achieved a validation MSE of 1.33e-3 and a
test MSE of 1.31e-3.
</p>
<a href="http://arxiv.org/abs/2012.03874" target="_blank">arXiv:2012.03874</a> [<a href="http://arxiv.org/pdf/2012.03874" target="_blank">pdf</a>]

<h2>Semi-Supervised Off Policy Reinforcement Learning. (arXiv:2012.04809v2 [cs.LG] UPDATED)</h2>
<h3>Aaron Sonabend-W, Nilanjana Laha, Rajarshi Mukherjee, Tianxi Cai</h3>
<p>Reinforcement learning (RL) has shown great success in estimating sequential
treatment strategies which account for patient heterogeneity. However,
health-outcome information is often not well coded but rather embedded in
clinical notes. Extracting precise outcome information is a resource intensive
task. This translates into only small well-annotated cohorts available. We
propose a semi-supervised learning (SSL) approach that can efficiently leverage
a small sized labeled data $\mathcal{L}$ with true outcome observed, and a
large sized unlabeled data $\mathcal{U}$ with outcome surrogates $\pmb W$. In
particular we propose a theoretically justified SSL approach to Q-learning and
develop a robust and efficient SSL approach to estimating the value function of
the derived optimal STR, defined as the expected counterfactual outcome under
the optimal STR. Generalizing SSL to learning STR brings interesting
challenges. First, the feature distribution for predicting $Y_t$ is unknown in
the $Q$-learning procedure, as it includes unknown $Y_{t-1}$ due to the
sequential nature. Our methods for estimating optimal STR and its associated
value function, carefully adapts to this sequentially missing data structure.
Second, we modify the SSL framework to handle the use of surrogate variables
$\pmb W$ which are predictive of the outcome through the joint law
$\mathbb{P}_{Y,\pmb O,\pmb W}$, but are not part of the conditional
distribution of interest $\mathbb{P}_{Y|\pmb O}$. We provide theoretical
results to understand when and to what degree efficiency can be gained from
$\pmb W$ and $\pmb O$. Our approach is robust to misspecification of the
imputation models. Further, we provide a doubly robust value function estimator
for the derived STR. If either the Q functions or the propensity score
functions are correctly specified, our value function estimators are consistent
for the true value function.
</p>
<a href="http://arxiv.org/abs/2012.04809" target="_blank">arXiv:2012.04809</a> [<a href="http://arxiv.org/pdf/2012.04809" target="_blank">pdf</a>]

<h2>Progressive Network Grafting for Few-Shot Knowledge Distillation. (arXiv:2012.04915v2 [cs.CV] UPDATED)</h2>
<h3>Chengchao Shen, Xinchao Wang, Youtan Yin, Jie Song, Sihui Luo, Mingli Song</h3>
<p>Knowledge distillation has demonstrated encouraging performances in deep
model compression. Most existing approaches, however, require massive labeled
data to accomplish the knowledge transfer, making the model compression a
cumbersome and costly process. In this paper, we investigate the practical
few-shot knowledge distillation scenario, where we assume only a few samples
without human annotations are available for each category. To this end, we
introduce a principled dual-stage distillation scheme tailored for few-shot
data. In the first step, we graft the student blocks one by one onto the
teacher, and learn the parameters of the grafted block intertwined with those
of the other teacher blocks. In the second step, the trained student blocks are
progressively connected and then together grafted onto the teacher network,
allowing the learned student blocks to adapt themselves to each other and
eventually replace the teacher network. Experiments demonstrate that our
approach, with only a few unlabeled samples, achieves gratifying results on
CIFAR10, CIFAR100, and ILSVRC-2012. On CIFAR10 and CIFAR100, our performances
are even on par with those of knowledge distillation schemes that utilize the
full datasets. The source code is available at
https://github.com/zju-vipa/NetGraft.
</p>
<a href="http://arxiv.org/abs/2012.04915" target="_blank">arXiv:2012.04915</a> [<a href="http://arxiv.org/pdf/2012.04915" target="_blank">pdf</a>]

<h2>Robust Facial Landmark Detection by Multi-order Multi-constraint Deep Networks. (arXiv:2012.04927v2 [cs.CV] UPDATED)</h2>
<h3>Jun Wan, Zhihui Lai, Jing Li, Jie Zhou, Can Gao</h3>
<p>Recently, heatmap regression has been widely explored in facial landmark
detection and obtained remarkable performance. However, most of the existing
heatmap regression-based facial landmark detection methods neglect to explore
the high-order feature correlations, which is very important to learn more
representative features and enhance shape constraints. Moreover, no explicit
global shape constraints have been added to the final predicted landmarks,
which leads to a reduction in accuracy. To address these issues, in this paper,
we propose a Multi-order Multi-constraint Deep Network (MMDN) for more powerful
feature correlations and shape constraints learning. Specifically, an Implicit
Multi-order Correlating Geometry-aware (IMCG) model is proposed to introduce
the multi-order spatial correlations and multi-order channel correlations for
more discriminative representations. Furthermore, an Explicit Probability-based
Boundary-adaptive Regression (EPBR) method is developed to enhance the global
shape constraints and further search the semantically consistent landmarks in
the predicted boundary for robust facial landmark detection. It's interesting
to show that the proposed MMDN can generate more accurate boundary-adaptive
landmark heatmaps and effectively enhance shape constraints to the predicted
landmarks for faces with large pose variations and heavy occlusions.
Experimental results on challenging benchmark datasets demonstrate the
superiority of our MMDN over state-of-the-art facial landmark detection
methods. The code has been publicly available at
https://github.com/junwan2014/MMDN-master.
</p>
<a href="http://arxiv.org/abs/2012.04927" target="_blank">arXiv:2012.04927</a> [<a href="http://arxiv.org/pdf/2012.04927" target="_blank">pdf</a>]

<h2>MLComp: A Methodology for Machine Learning-based Performance Estimation and Adaptive Selection of Pareto-Optimal Compiler Optimization Sequences. (arXiv:2012.05270v2 [cs.LG] UPDATED)</h2>
<h3>Alessio Colucci, D&#xe1;vid Juh&#xe1;sz, Martin Mosbeck, Alberto Marchisio, Semeen Rehman, Manfred Kreutzer, Guenther Nadbath, Axel Jantsch, Muhammad Shafique</h3>
<p>Embedded systems have proliferated in various consumer and industrial
applications with the evolution of Cyber-Physical Systems and the Internet of
Things. These systems are subjected to stringent constraints so that embedded
software must be optimized for multiple objectives simultaneously, namely
reduced energy consumption, execution time, and code size. Compilers offer
optimization phases to improve these metrics. However, proper selection and
ordering of them depends on multiple factors and typically requires expert
knowledge. State-of-the-art optimizers facilitate different platforms and
applications case by case, and they are limited by optimizing one metric at a
time, as well as requiring a time-consuming adaptation for different targets
through dynamic profiling.

To address these problems, we propose the novel MLComp methodology, in which
optimization phases are sequenced by a Reinforcement Learning-based policy.
Training of the policy is supported by Machine Learning-based analytical models
for quick performance estimation, thereby drastically reducing the time spent
for dynamic profiling. In our framework, different Machine Learning models are
automatically tested to choose the best-fitting one. The trained Performance
Estimator model is leveraged to efficiently devise Reinforcement Learning-based
multi-objective policies for creating quasi-optimal phase sequences.

Compared to state-of-the-art estimation models, our Performance Estimator
model achieves lower relative error (&lt;2%) with up to 50x faster training time
over multiple platforms and application domains. Our Phase Selection Policy
improves execution time and energy consumption of a given code by up to 12% and
6%, respectively. The Performance Estimator and the Phase Selection Policy can
be trained efficiently for any target platform and application domain.
</p>
<a href="http://arxiv.org/abs/2012.05270" target="_blank">arXiv:2012.05270</a> [<a href="http://arxiv.org/pdf/2012.05270" target="_blank">pdf</a>]

<h2>Know Your Limits: Monotonicity & Softmax Make Neural Classifiers Overconfident on OOD Data. (arXiv:2012.05329v2 [cs.LG] UPDATED)</h2>
<h3>Dennis Ulmer, Giovanni Cin&#xe0;</h3>
<p>A crucial requirement for reliable deployment of deep learning models for
safety-critical applications is the ability to identify out-of-distribution
(OOD) data points, samples which differ from the training data and on which a
model might underperform. Previous work has attempted to tackle this problem
using uncertainty estimation techniques. However, there is empirical evidence
that a large family of these techniques do not detect OOD reliably in
classification tasks.

This paper puts forward a theoretical explanation for said experimental
findings. We prove that such techniques are not able to reliably identify OOD
samples in a classification setting, provided the models satisfy weak
assumptions about the monotonicity of feature values and resulting class
probabilities. This result stems from the interplay between the saturating
nature of activation functions like sigmoid or softmax, coupled with the most
widely-used uncertainty metrics.
</p>
<a href="http://arxiv.org/abs/2012.05329" target="_blank">arXiv:2012.05329</a> [<a href="http://arxiv.org/pdf/2012.05329" target="_blank">pdf</a>]

<h2>Visual Perception Generalization for Vision-and-Language Navigation via Meta-Learning. (arXiv:2012.05446v2 [cs.RO] UPDATED)</h2>
<h3>Ting Wang, Zongkai Wu, Donglin Wang</h3>
<p>Vision-and-language navigation (VLN) is a challenging task that requires an
agent to navigate in real-world environments by understanding natural language
instructions and visual information received in real-time. Prior works have
implemented VLN tasks on continuous environments or physical robots, all of
which use a fixed camera configuration due to the limitations of datasets, such
as 1.5 meters height, 90 degrees horizontal field of view (HFOV), etc. However,
real-life robots with different purposes have multiple camera configurations,
and the huge gap in visual information makes it difficult to directly transfer
the learned navigation model between various robots. In this paper, we propose
a visual perception generalization strategy based on meta-learning, which
enables the agent to fast adapt to a new camera configuration with a few shots.
In the training phase, we first locate the generalization problem to the visual
perception module, and then compare two meta-learning algorithms for better
generalization in seen and unseen environments. One of them uses the
Model-Agnostic Meta-Learning (MAML) algorithm that requires a few shot
adaptation, and the other refers to a metric-based meta-learning method with a
feature-wise affine transformation layer. The experiment results show that our
strategy successfully adapts the learned navigation model to a new camera
configuration, and the two algorithms show their advantages in seen and unseen
environments respectively.
</p>
<a href="http://arxiv.org/abs/2012.05446" target="_blank">arXiv:2012.05446</a> [<a href="http://arxiv.org/pdf/2012.05446" target="_blank">pdf</a>]

<h2>The Representation Power of Neural Networks: Breaking the Curse of Dimensionality. (arXiv:2012.05451v2 [cs.LG] UPDATED)</h2>
<h3>Moise Blanchard, M. Amine Bennouna</h3>
<p>In this paper, we analyze the number of neurons and training parameters that
a neural networks needs to approximate multivariate functions of bounded second
mixed derivatives -- Korobov functions. We prove upper bounds on these
quantities for shallow and deep neural networks, breaking the curse of
dimensionality. Our bounds hold for general activation functions, including
ReLU. We further prove that these bounds nearly match the minimal number of
parameters any continuous function approximator needs to approximate Korobov
functions, showing that neural networks are near-optimal function
approximators.
</p>
<a href="http://arxiv.org/abs/2012.05451" target="_blank">arXiv:2012.05451</a> [<a href="http://arxiv.org/pdf/2012.05451" target="_blank">pdf</a>]

<h2>One for More: Selecting Generalizable Samples for Generalizable ReID Model. (arXiv:2012.05475v2 [cs.CV] UPDATED)</h2>
<h3>Enwei Zhang, Xinyang Jiang, Hao Cheng, Ancong Wu, Fufu Yu, Ke Li, Xiaowei Guo, Feng Zheng, Wei-Shi Zheng, Xing Sun</h3>
<p>Current training objectives of existing person Re-IDentification (ReID)
models only ensure that the loss of the model decreases on selected training
batch, with no regards to the performance on samples outside the batch. It will
inevitably cause the model to over-fit the data in the dominant position (e.g.,
head data in imbalanced class, easy samples or noisy samples). %We call the
sample that updates the model towards generalizing on more data a generalizable
sample. The latest resampling methods address the issue by designing specific
criterion to select specific samples that trains the model generalize more on
certain type of data (e.g., hard samples, tail data), which is not adaptive to
the inconsistent real world ReID data distributions. Therefore, instead of
simply presuming on what samples are generalizable, this paper proposes a
one-for-more training objective that directly takes the generalization ability
of selected samples as a loss function and learn a sampler to automatically
select generalizable samples. More importantly, our proposed one-for-more based
sampler can be seamlessly integrated into the ReID training framework which is
able to simultaneously train ReID models and the sampler in an end-to-end
fashion. The experimental results show that our method can effectively improve
the ReID model training and boost the performance of ReID models.
</p>
<a href="http://arxiv.org/abs/2012.05475" target="_blank">arXiv:2012.05475</a> [<a href="http://arxiv.org/pdf/2012.05475" target="_blank">pdf</a>]

<h2>TFPnP: Tuning-free Plug-and-Play Proximal Algorithm with Applications to Inverse Imaging Problems. (arXiv:2012.05703v2 [cs.CV] UPDATED)</h2>
<h3>Kaixuan Wei, Angelica Aviles-Rivero, Jingwei Liang, Ying Fu, Hua Huang, Carola-Bibiane Sch&#xf6;nlieb</h3>
<p>Plug-and-Play (PnP) is a non-convex framework that combines proximal
algorithms, for example alternating direction method of multipliers (ADMM),
with advanced denoiser priors. Over the past few years, great empirical success
has been obtained by PnP algorithms, especially for the ones integrated with
deep learning-based denoisers. However, a crucial issue of PnP approaches is
the need of manual parameter tweaking. As it is essential to obtain
high-quality results across the high discrepancy in terms of imaging conditions
and varying scene content. In this work, we present a tuning-free PnP proximal
algorithm, which can automatically determine the internal parameters including
the penalty parameter, the denoising strength and the termination time. A core
part of our approach is to develop a policy network for automatic search of
parameters, which can be effectively learned via mixed model-free and
model-based deep reinforcement learning. We demonstrate, through a set of
numerical and visual experiments, that the learned policy can customize
different parameters for different states, and often more efficient and
effective than existing handcrafted criteria. Moreover, we discuss the
practical considerations of the plugged denoisers, which together with our
learned policy yield to state-of-the-art results. This is prevalent on both
linear and nonlinear exemplary inverse imaging problems, and in particular, we
show promising results on compressed sensing MRI, sparse-view CT and phase
retrieval.
</p>
<a href="http://arxiv.org/abs/2012.05703" target="_blank">arXiv:2012.05703</a> [<a href="http://arxiv.org/pdf/2012.05703" target="_blank">pdf</a>]

<h2>Flatland-RL : Multi-Agent Reinforcement Learning on Trains. (arXiv:2012.05893v2 [cs.AI] UPDATED)</h2>
<h3>Sharada Mohanty, Erik Nygren, Florian Laurent, Manuel Schneider, Christian Scheller, Nilabha Bhattacharya, Jeremy Watson, Adrian Egli, Christian Eichenberger, Christian Baumberger, Gereon Vienken, Irene Sturm, Guillaume Sartoretti, Giacomo Spigler</h3>
<p>Efficient automated scheduling of trains remains a major challenge for modern
railway systems. The underlying vehicle rescheduling problem (VRSP) has been a
major focus of Operations Research (OR) since decades. Traditional approaches
use complex simulators to study VRSP, where experimenting with a broad range of
novel ideas is time consuming and has a huge computational overhead. In this
paper, we introduce a two-dimensional simplified grid environment called
"Flatland" that allows for faster experimentation. Flatland does not only
reduce the complexity of the full physical simulation, but also provides an
easy-to-use interface to test novel approaches for the VRSP, such as
Reinforcement Learning (RL) and Imitation Learning (IL). In order to probe the
potential of Machine Learning (ML) research on Flatland, we (1) ran a first
series of RL and IL experiments and (2) design and executed a public Benchmark
at NeurIPS 2020 to engage a large community of researchers to work on this
problem. Our own experimental results, on the one hand, demonstrate that ML has
potential in solving the VRSP on Flatland. On the other hand, we identify key
topics that need further research. Overall, the Flatland environment has proven
to be a robust and valuable framework to investigate the VRSP for railway
networks. Our experiments provide a good starting point for further research
and for the participants of the NeurIPS 2020 Flatland Benchmark. All of these
efforts together have the potential to have a substantial impact on shaping the
mobility of the future.
</p>
<a href="http://arxiv.org/abs/2012.05893" target="_blank">arXiv:2012.05893</a> [<a href="http://arxiv.org/pdf/2012.05893" target="_blank">pdf</a>]

