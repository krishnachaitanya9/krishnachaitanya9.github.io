---
title: Latest Deep Learning Papers
date: 2021-01-11 21:00:01 +0800
featured-img: DL-logo
categories: [Deep_Learning]
tags: [Deep_Learning]
mathjax: Yes
---

<h1>Your arXiv Feed (216 Articles)</h1>
<h2>An Unsupervised Learning Method with Convolutional Auto-Encoder for Vessel Trajectory Similarity Computation. (arXiv:2101.03169v1 [cs.LG])</h2>
<h3>Maohan Liang, Ryan Wen Liu, Shichen Li, Zhe Xiao, Xin Liu, Feng Lu</h3>
<p>To achieve reliable mining results for massive vessel trajectories, one of
the most important challenges is how to efficiently compute the similarities
between different vessel trajectories. The computation of vessel trajectory
similarity has recently attracted increasing attention in the maritime data
mining research community. However, traditional shape- and warping-based
methods often suffer from several drawbacks such as high computational cost and
sensitivity to unwanted artifacts and non-uniform sampling rates, etc. To
eliminate these drawbacks, we propose an unsupervised learning method which
automatically extracts low-dimensional features through a convolutional
auto-encoder (CAE). In particular, we first generate the informative trajectory
images by remapping the raw vessel trajectories into two-dimensional matrices
while maintaining the spatio-temporal properties. Based on the massive vessel
trajectories collected, the CAE can learn the low-dimensional representations
of informative trajectory images in an unsupervised manner. The trajectory
similarity is finally equivalent to efficiently computing the similarities
between the learned low-dimensional features, which strongly correlate with the
raw vessel trajectories. Comprehensive experiments on realistic data sets have
demonstrated that the proposed method largely outperforms traditional
trajectory similarity computation methods in terms of efficiency and
effectiveness. The high-quality trajectory clustering performance could also be
guaranteed according to the CAE-based trajectory similarity computation
results.
</p>
<a href="http://arxiv.org/abs/2101.03169" target="_blank">arXiv:2101.03169</a> [<a href="http://arxiv.org/pdf/2101.03169" target="_blank">pdf</a>]

<h2>A review paper of bio-inspired environmental adaptive and precisely maneuverable soft robots. (arXiv:2101.03171v1 [cs.RO])</h2>
<h3>Mengqi Shen</h3>
<p>This paper summarizes the most recent research in soft robotic field from the
factors of material, actuation, mechanicsproperty, dimension &amp; scale and
architecture, and then presents the relations among the functionalities,
manufacturing process and the factors mentioned above.
</p>
<a href="http://arxiv.org/abs/2101.03171" target="_blank">arXiv:2101.03171</a> [<a href="http://arxiv.org/pdf/2101.03171" target="_blank">pdf</a>]

<h2>Analysis of Evolutionary Program Synthesis for Card Games. (arXiv:2101.03172v1 [cs.AI])</h2>
<h3>Rohan Saha, Cassidy Pirlot</h3>
<p>In this report, we inspect the application of an evolutionary approach to the
game of Rack'O, which is a card game revolving around the notion of decision
making. We first apply the evolutionary technique for obtaining a set of rules
over many generations and then compare them with a script written by a human
player. A high-level domain-specific language is used that deter-mines which
the sets of rules are synthesized. We report the results by providing a
comprehensive analysis of the set of rules and their implications.
</p>
<a href="http://arxiv.org/abs/2101.03172" target="_blank">arXiv:2101.03172</a> [<a href="http://arxiv.org/pdf/2101.03172" target="_blank">pdf</a>]

<h2>Predicting Individual Substance Abuse Vulnerability using Machine Learning Techniques. (arXiv:2101.03184v1 [cs.LG])</h2>
<h3>Uwaise Ibna Islam, Iqbal H. Sarker, Enamul Haque, Mohammed Moshiul Hoque</h3>
<p>Substance abuse is the unrestrained and detrimental use of psychoactive
chemical substances, unauthorized drugs, and alcohol. Continuous use of these
substances can ultimately lead a human to disastrous consequences. As patients
display a high rate of relapse, prevention at an early stage can be an
effective restraint. We therefore propose a binary classifier to identify any
individual's present vulnerability towards substance abuse by analyzing
subjects' socio-economic environment. We have collected data by a questionnaire
which is created after carefully assessing the commonly involved factors behind
substance abuse. Pearson's chi-squared test of independence is used to identify
key feature variables influencing substance abuse. Later we build the
predictive classifiers using machine learning classification algorithms on
those variables. Logistic regression classifier trained with 18 features can
predict individual vulnerability with the best accuracy.
</p>
<a href="http://arxiv.org/abs/2101.03184" target="_blank">arXiv:2101.03184</a> [<a href="http://arxiv.org/pdf/2101.03184" target="_blank">pdf</a>]

<h2>Deep Diffusion Processes for Active Learning of Hyperspectral Images. (arXiv:2101.03197v1 [cs.CV])</h2>
<h3>Abiy Tasissa, Duc Nguyen, James Murphy</h3>
<p>A method for active learning of hyperspectral images (HSI) is proposed, which
combines deep learning with diffusion processes on graphs. A deep variational
autoencoder extracts smoothed, denoised features from a high-dimensional HSI,
which are then used to make labeling queries based on graph diffusion
processes. The proposed method combines the robust representations of deep
learning with the mathematical tractability of diffusion geometry, and leads to
strong performance on real HSI.
</p>
<a href="http://arxiv.org/abs/2101.03197" target="_blank">arXiv:2101.03197</a> [<a href="http://arxiv.org/pdf/2101.03197" target="_blank">pdf</a>]

<h2>Extracting Pasture Phenotype and Biomass Percentages using Weakly Supervised Multi-target Deep Learning on a Small Dataset. (arXiv:2101.03198v1 [cs.CV])</h2>
<h3>Badri Narayanan, Mohamed Saadeldin, Paul Albert, Kevin McGuinness, Brian Mac Namee</h3>
<p>The dairy industry uses clover and grass as fodder for cows. Accurate
estimation of grass and clover biomass yield enables smart decisions in
optimizing fertilization and seeding density, resulting in increased
productivity and positive environmental impact. Grass and clover are usually
planted together, since clover is a nitrogen-fixing plant that brings nutrients
to the soil. Adjusting the right percentages of clover and grass in a field
reduces the need for external fertilization. Existing approaches for estimating
the grass-clover composition of a field are expensive and time consuming -
random samples of the pasture are clipped and then the components are
physically separated to weigh and calculate percentages of dry grass, clover
and weeds in each sample. There is growing interest in developing novel deep
learning based approaches to non-destructively extract pasture phenotype
indicators and biomass yield predictions of different plant species from
agricultural imagery collected from the field. Providing these indicators and
predictions from images alone remains a significant challenge. Heavy occlusions
in the dense mixture of grass, clover and weeds make it difficult to estimate
each component accurately. Moreover, although supervised deep learning models
perform well with large datasets, it is tedious to acquire large and diverse
collections of field images with precise ground truth for different biomass
yields. In this paper, we demonstrate that applying data augmentation and
transfer learning is effective in predicting multi-target biomass percentages
of different plant species, even with a small training dataset. The scheme
proposed in this paper used a training set of only 261 images and provided
predictions of biomass percentages of grass, clover, white clover, red clover,
and weeds with mean absolute error of 6.77%, 6.92%, 6.21%, 6.89%, and 4.80%
respectively.
</p>
<a href="http://arxiv.org/abs/2101.03198" target="_blank">arXiv:2101.03198</a> [<a href="http://arxiv.org/pdf/2101.03198" target="_blank">pdf</a>]

<h2>The Diabetic Buddy: A Diet Regulator andTracking System for Diabetics. (arXiv:2101.03203v1 [cs.CV])</h2>
<h3>Muhammad Usman, Kashif Ahmad, Amir Sohail, Marwa Qaraqe</h3>
<p>The prevalence of Diabetes mellitus (DM) in the Middle East is exceptionally
high as compared to the rest of the world. In fact, the prevalence of diabetes
in the Middle East is 17-20%, which is well above the global average of 8-9%.
Research has shown that food intake has strong connections with the blood
glucose levels of a patient. In this regard, there is a need to build automatic
tools to monitor the blood glucose levels of diabetics and their daily food
intake. This paper presents an automatic way of tracking continuous glucose and
food intake of diabetics using off-the-shelf sensors and machine learning,
respectively. Our system not only helps diabetics to track their daily food
intake but also assists doctors to analyze the impact of the food in-take on
blood glucose in real-time. For food recognition, we collected a large-scale
Middle-Eastern food dataset and proposed a fusion-based framework incorporating
several existing pre-trained deep models for Middle-Eastern food recognition.
</p>
<a href="http://arxiv.org/abs/2101.03203" target="_blank">arXiv:2101.03203</a> [<a href="http://arxiv.org/pdf/2101.03203" target="_blank">pdf</a>]

<h2>Optimizing Hospital Room Layout to Reduce the Risk of Patient Falls. (arXiv:2101.03210v1 [cs.AI])</h2>
<h3>Sarvenaz Chaeibakhsh, Roya Sabbagh Novin, Tucker Hermans, Andrew Merryweather, Alan Kuntz</h3>
<p>Despite years of research into patient falls in hospital rooms, falls and
related injuries remain a serious concern to patient safety. In this work, we
formulate a gradient-free constrained optimization problem to generate and
reconfigure the hospital room interior layout to minimize the risk of falls. We
define a cost function built on a hospital room fall model that takes into
account the supportive or hazardous effect of the patient's surrounding
objects, as well as simulated patient trajectories inside the room. We define a
constraint set that ensures the functionality of the generated room layouts in
addition to conforming to architectural guidelines. We solve this problem
efficiently using a variant of simulated annealing. We present results for two
real-world hospital room types and demonstrate a significant improvement of 18%
on average in patient fall risk when compared with a traditional hospital room
layout and 41% when compared with randomly generated layouts.
</p>
<a href="http://arxiv.org/abs/2101.03210" target="_blank">arXiv:2101.03210</a> [<a href="http://arxiv.org/pdf/2101.03210" target="_blank">pdf</a>]

<h2>DiPSeN: Differentially Private Self-normalizing Neural Networks For Adversarial Robustness in Federated Learning. (arXiv:2101.03218v1 [cs.LG])</h2>
<h3>Olakunle Ibitoye, M. Omair Shafiq, Ashraf Matrawy</h3>
<p>The need for robust, secure and private machine learning is an important goal
for realizing the full potential of the Internet of Things (IoT). Federated
learning has proven to help protect against privacy violations and information
leakage. However, it introduces new risk vectors which make machine learning
models more difficult to defend against adversarial samples. In this study, we
examine the role of differential privacy and self-normalization in mitigating
the risk of adversarial samples specifically in a federated learning
environment. We introduce DiPSeN, a Differentially Private Self-normalizing
Neural Network which combines elements of differential privacy noise with
self-normalizing techniques. Our empirical results on three publicly available
datasets show that DiPSeN successfully improves the adversarial robustness of a
deep learning classifier in a federated learning environment based on several
evaluation metrics.
</p>
<a href="http://arxiv.org/abs/2101.03218" target="_blank">arXiv:2101.03218</a> [<a href="http://arxiv.org/pdf/2101.03218" target="_blank">pdf</a>]

<h2>Benchmarking Machine Learning: How Fast Can Your Algorithms Go?. (arXiv:2101.03219v1 [cs.LG])</h2>
<h3>Zeyu Ning, Hugues Nelson Iradukunda, Qingquan Zhang, Ting Zhu</h3>
<p>This paper is focused on evaluating the effect of some different techniques
in machine learning speed-up, including vector caches, parallel execution, and
so on. The following content will include some review of the previous
approaches and our own experimental results.
</p>
<a href="http://arxiv.org/abs/2101.03219" target="_blank">arXiv:2101.03219</a> [<a href="http://arxiv.org/pdf/2101.03219" target="_blank">pdf</a>]

<h2>Glacier Calving Front Segmentation Using Attention U-Net. (arXiv:2101.03247v1 [cs.LG])</h2>
<h3>Michael Holzmann, Amirabbas Davari, Thorsten Seehaus, Matthias Braun, Andreas Maier, Vincent Christlein</h3>
<p>An essential climate variable to determine the tidewater glacier status is
the location of the calving front position and the separation of seasonal
variability from long-term trends. Previous studies have proposed deep
learning-based methods to semi-automatically delineate the calving fronts of
tidewater glaciers. They used U-Net to segment the ice and non-ice regions and
extracted the calving fronts in a post-processing step. In this work, we show a
method to segment the glacier calving fronts from SAR images in an end-to-end
fashion using Attention U-Net. The main objective is to investigate the
attention mechanism in this application. Adding attention modules to the
state-of-the-art U-Net network lets us analyze the learning process by
extracting its attention maps. We use these maps as a tool to search for proper
hyperparameters and loss functions in order to generate higher qualitative
results. Our proposed attention U-Net performs comparably to the standard U-Net
while providing additional insight into those regions on which the network
learned to focus more. In the best case, the attention U-Net achieves a 1.5%
better Dice score compared to the canonical U-Net with a glacier front line
prediction certainty of up to 237.12 meters.
</p>
<a href="http://arxiv.org/abs/2101.03247" target="_blank">arXiv:2101.03247</a> [<a href="http://arxiv.org/pdf/2101.03247" target="_blank">pdf</a>]

<h2>Bayesian U-Net for Segmenting Glaciers in SAR Imagery. (arXiv:2101.03249v1 [cs.LG])</h2>
<h3>Andreas Hartmann, Amirabbas Davari, Thorsten Seehaus, Matthias Braun, Andreas Maier, Vincent Christlein</h3>
<p>Fluctuations of the glacier calving front have an important influence over
the ice flow of whole glacier systems. It is therefore important to precisely
monitor the position of the calving front. However, the manual delineation of
SAR images is a difficult, laborious and subjective task. Convolutional neural
networks have previously shown promising results in automating the glacier
segmentation in SAR images, making them desirable for further exploration of
their possibilities. In this work, we propose to compute uncertainty and use it
in an Uncertainty Optimization regime as a novel two-stage process. By using
dropout as a random sampling layer in a U-Net architecture, we create a
probabilistic Bayesian Neural Network. With several forward passes, we create a
sampling distribution, which can estimate the model uncertainty for each pixel
in the segmentation mask. The additional uncertainty map information can serve
as a guideline for the experts in the manual annotation of the data.
Furthermore, feeding the uncertainty map to the network leads to 95.24% Dice
similarity, which is an overall improvement in the segmentation performance
compared to the state-of-the-art deterministic U-Net-based glacier segmentation
pipelines.
</p>
<a href="http://arxiv.org/abs/2101.03249" target="_blank">arXiv:2101.03249</a> [<a href="http://arxiv.org/pdf/2101.03249" target="_blank">pdf</a>]

<h2>Unobtrusive Pain Monitoring in Older Adults with Dementia using Pairwise and Contrastive Training. (arXiv:2101.03251v1 [cs.CV])</h2>
<h3>Siavash Rezaei, Abhishek Moturu, Shun Zhao, Kenneth M. Prkachin, Thomas Hadjistavropoulos, Babak Taati</h3>
<p>Although pain is frequent in old age, older adults are often undertreated for
pain. This is especially the case for long-term care residents with moderate to
severe dementia who cannot report their pain because of cognitive impairments
that accompany dementia. Nursing staff acknowledge the challenges of
effectively recognizing and managing pain in long-term care facilities due to
lack of human resources and, sometimes, expertise to use validated pain
assessment approaches on a regular basis. Vision-based ambient monitoring will
allow for frequent automated assessments so care staff could be automatically
notified when signs of pain are displayed. However, existing computer vision
techniques for pain detection are not validated on faces of older adults or
people with dementia, and this population is not represented in existing facial
expression datasets of pain. We present the first fully automated vision-based
technique validated on a dementia cohort. Our contributions are threefold.
First, we develop a deep learning-based computer vision system for detecting
painful facial expressions on a video dataset that is collected unobtrusively
from older adult participants with and without dementia. Second, we introduce a
pairwise comparative inference method that calibrates to each person and is
sensitive to changes in facial expression while using training data more
efficiently than sequence models. Third, we introduce a fast contrastive
training method that improves cross-dataset performance. Our pain estimation
model outperforms baselines by a wide margin, especially when evaluated on
faces of people with dementia. Pre-trained model and demo code available at
https://github.com/TaatiTeam/pain_detection_demo
</p>
<a href="http://arxiv.org/abs/2101.03251" target="_blank">arXiv:2101.03251</a> [<a href="http://arxiv.org/pdf/2101.03251" target="_blank">pdf</a>]

<h2>Synthetic Glacier SAR Image Generation from Arbitrary Masks Using Pix2Pix Algorithm. (arXiv:2101.03252v1 [cs.LG])</h2>
<h3>Rosanna Dietrich-Sussner, Amirabbas Davari, Thorsten Seehaus, Matthias Braun, Vincent Christlein, Andreas Maier, Christian Riess</h3>
<p>Supervised machine learning requires a large amount of labeled data to
achieve proper test results. However, generating accurately labeled
segmentation maps on remote sensing imagery, including images from synthetic
aperture radar (SAR), is tedious and highly subjective. In this work, we
propose to alleviate the issue of limited training data by generating synthetic
SAR images with the pix2pix algorithm \cite{isola2017image}. This algorithm
uses conditional Generative Adversarial Networks (cGANs) to generate an
artificial image while preserving the structure of the input. In our case, the
input is a segmentation mask, from which a corresponding synthetic SAR image is
generated. We present different models, perform a comparative study and
demonstrate that this approach synthesizes convincing glaciers in SAR images
with promising qualitative and quantitative results.
</p>
<a href="http://arxiv.org/abs/2101.03252" target="_blank">arXiv:2101.03252</a> [<a href="http://arxiv.org/pdf/2101.03252" target="_blank">pdf</a>]

<h2>Good Students Play Big Lottery Better. (arXiv:2101.03255v1 [cs.LG])</h2>
<h3>Haoyu Ma, Tianlong Chen, Ting-Kuei Hu, Chenyu You, Xiaohui Xie, Zhangyang Wang</h3>
<p>Lottery ticket hypothesis suggests that a dense neural network contains a
sparse sub-network that can match the test accuracy of the original dense net
when trained in isolation from (the same) random initialization. However, the
hypothesis failed to generalize to larger dense networks such as ResNet-50. As
a remedy, recent studies demonstrate that a sparse sub-network can still be
obtained by using a rewinding technique, which is to re-train it from
early-phase training weights or learning rates of the dense model, rather than
from random initialization.

Is rewinding the only or the best way to scale up lottery tickets? This paper
proposes a new, simpler and yet powerful technique for re-training the
sub-network, called "Knowledge Distillation ticket" (KD ticket). Rewinding
exploits the value of inheriting knowledge from the early training phase to
improve lottery tickets in large networks. In comparison, KD ticket addresses a
complementary possibility - inheriting useful knowledge from the late training
phase of the dense model. It is achieved by leveraging the soft labels
generated by the trained dense model to re-train the sub-network, instead of
the hard labels. Extensive experiments are conducted using several large deep
networks (e.g ResNet-50 and ResNet-110) on CIFAR-10 and ImageNet datasets.
Without bells and whistles, when applied by itself, KD ticket performs on par
or better than rewinding, while being nearly free of hyperparameters or ad-hoc
selection. KD ticket can be further applied together with rewinding, yielding
state-of-the-art results for large-scale lottery tickets.
</p>
<a href="http://arxiv.org/abs/2101.03255" target="_blank">arXiv:2101.03255</a> [<a href="http://arxiv.org/pdf/2101.03255" target="_blank">pdf</a>]

<h2>SyReNN: A Tool for Analyzing Deep Neural Networks. (arXiv:2101.03263v1 [cs.LG])</h2>
<h3>Matthew Sotoudeh, Aditya V. Thakur</h3>
<p>Deep Neural Networks (DNNs) are rapidly gaining popularity in a variety of
important domains. Formally, DNNs are complicated vector-valued functions which
come in a variety of sizes and applications. Unfortunately, modern DNNs have
been shown to be vulnerable to a variety of attacks and buggy behavior. This
has motivated recent work in formally analyzing the properties of such DNNs.
This paper introduces SyReNN, a tool for understanding and analyzing a DNN by
computing its symbolic representation. The key insight is to decompose the DNN
into linear functions. Our tool is designed for analyses using low-dimensional
subsets of the input space, a unique design point in the space of DNN analysis
tools. We describe the tool and the underlying theory, then evaluate its use
and performance on three case studies: computing Integrated Gradients,
visualizing a DNN's decision boundaries, and patching a DNN.
</p>
<a href="http://arxiv.org/abs/2101.03263" target="_blank">arXiv:2101.03263</a> [<a href="http://arxiv.org/pdf/2101.03263" target="_blank">pdf</a>]

<h2>Investigation by Driving Simulation of Tractor Overturning Accidents Caused by Steering Instability. (arXiv:2101.03270v1 [cs.RO])</h2>
<h3>Masahisa Watanabe, Kenshi Sakai</h3>
<p>Overturning tractors are the leading cause of fatalities on farms. Steering
instability contributes significantly to the tractor overturning. This study
investigated tractor overturning accidents caused by the steering instability
using a driving simulator. The general commercial driving simulator CarSim
(Mechanical Simulation Cooperation, MI, USA) was used. Tractor operations on
steep passage slopes were simulated to mimic conditions present for a real
accident case reported in Japan. Simulations were performed on roads with and
without slopes. The tractor overturned only when on the road with the steep
slope. The decrease in the vertical force on the front wheel caused the
steering instability and the tractor to overturn. The steering instability
caused understeer which prevents the operator from being able to control the
tractor properly. Subsequently, the tractor overturned in the simulation. The
tractor driving simulator was capable of reproducing the steering instability
which can lead to the overturning accident.
</p>
<a href="http://arxiv.org/abs/2101.03270" target="_blank">arXiv:2101.03270</a> [<a href="http://arxiv.org/pdf/2101.03270" target="_blank">pdf</a>]

<h2>Exploring Adversarial Fake Images on Face Manifold. (arXiv:2101.03272v1 [cs.CV])</h2>
<h3>Dongze Li, Wei Wang, Hongxing Fan, Jing Dong</h3>
<p>Images synthesized by powerful generative adversarial network (GAN) based
methods have drawn moral and privacy concerns. Although image forensic models
have reached great performance in detecting fake images from real ones, these
models can be easily fooled with a simple adversarial attack. But, the noise
adding adversarial samples are also arousing suspicion. In this paper, instead
of adding adversarial noise, we optimally search adversarial points on face
manifold to generate anti-forensic fake face images. We iteratively do a
gradient-descent with each small step in the latent space of a generative
model, e.g. Style-GAN, to find an adversarial latent vector, which is similar
to norm-based adversarial attack but in latent space. Then, the generated fake
images driven by the adversarial latent vectors with the help of GANs can
defeat main-stream forensic models. For examples, they make the accuracy of
deepfake detection models based on Xception or EfficientNet drop from over 90%
to nearly 0%, meanwhile maintaining high visual quality. In addition, we find
manipulating style vector $z$ or noise vectors $n$ at different levels have
impacts on attack success rate. The generated adversarial images mainly have
facial texture or face attributes changing.
</p>
<a href="http://arxiv.org/abs/2101.03272" target="_blank">arXiv:2101.03272</a> [<a href="http://arxiv.org/pdf/2101.03272" target="_blank">pdf</a>]

<h2>Identifying Human Edited Images using a CNN. (arXiv:2101.03275v1 [cs.CV])</h2>
<h3>Jordan Lee, Willy Lin, Konstantinos Ntalis, Anirudh Shah, William Tung, Maxwell Wulff</h3>
<p>Most non-professional photo manipulations are not made using propriety
software like Adobe Photoshop, which is expensive and complicated to use for
the average consumer selfie-taker or meme-maker. Instead, these individuals opt
for user friendly mobile applications like FaceTune and Pixlr to make human
face edits and alterations. Unfortunately, there is no existing dataset to
train a model to classify these type of manipulations. In this paper, we
present a generative model that approximates the distribution of human face
edits and a method for detecting Facetune and Pixlr manipulations to human
faces.
</p>
<a href="http://arxiv.org/abs/2101.03275" target="_blank">arXiv:2101.03275</a> [<a href="http://arxiv.org/pdf/2101.03275" target="_blank">pdf</a>]

<h2>Investigating the Effect of Sensor Modalities in Multi-Sensor Detection-Prediction Models. (arXiv:2101.03279v1 [cs.RO])</h2>
<h3>Abhishek Mohta, Fang-Chieh Chou, Brian C. Becker, Carlos Vallespi-Gonzalez, Nemanja Djuric</h3>
<p>Detection of surrounding objects and their motion prediction are critical
components of a self-driving system. Recently proposed models that jointly
address these tasks rely on a number of sensors to achieve state-of-the-art
performance. However, this increases system complexity and may result in a
brittle model that overfits to any single sensor modality while ignoring
others, leading to reduced generalization. We focus on this important problem
and analyze the contribution of sensor modalities towards the model
performance. In addition, we investigate the use of sensor dropout to mitigate
the above-mentioned issues, leading to a more robust, better-performing model
on real-world driving data.
</p>
<a href="http://arxiv.org/abs/2101.03279" target="_blank">arXiv:2101.03279</a> [<a href="http://arxiv.org/pdf/2101.03279" target="_blank">pdf</a>]

<h2>Detecting, Localising and Classifying Polyps from Colonoscopy Videos using Deep Learning. (arXiv:2101.03285v1 [cs.CV])</h2>
<h3>Yu Tian, Leonardo Zorron Cheng Tao Pu, Yuyuan Liu, Gabriel Maicas, Johan W. Verjans, Alastair D. Burt, Seon Ho Shin, Rajvinder Singh, Gustavo Carneiro</h3>
<p>In this paper, we propose and analyse a system that can automatically detect,
localise and classify polyps from colonoscopy videos. The detection of frames
with polyps is formulated as a few-shot anomaly classification problem, where
the training set is highly imbalanced with the large majority of frames
consisting of normal images and a small minority comprising frames with polyps.
Colonoscopy videos may contain blurry images and frames displaying feces and
water jet sprays to clean the colon -- such frames can mistakenly be detected
as anomalies, so we have implemented a classifier to reject these two types of
frames before polyp detection takes place. Next, given a frame containing a
polyp, our method localises (with a bounding box around the polyp) and
classifies it into five different classes. Furthermore, we study a method to
improve the reliability and interpretability of the classification result using
uncertainty estimation and classification calibration. Classification
uncertainty and calibration not only help improve classification accuracy by
rejecting low-confidence and high-uncertain results, but can be used by doctors
to decide how to decide on the classification of a polyp. All the proposed
detection, localisation and classification methods are tested using large data
sets and compared with relevant baseline approaches.
</p>
<a href="http://arxiv.org/abs/2101.03285" target="_blank">arXiv:2101.03285</a> [<a href="http://arxiv.org/pdf/2101.03285" target="_blank">pdf</a>]

<h2>How to Train Your Energy-Based Models. (arXiv:2101.03288v1 [cs.LG])</h2>
<h3>Yang Song, Diederik P. Kingma</h3>
<p>Energy-Based Models (EBMs), also known as non-normalized probabilistic
models, specify probability density or mass functions up to an unknown
normalizing constant. Unlike most other probabilistic models, EBMs do not place
a restriction on the tractability of the normalizing constant, thus are more
flexible to parameterize and can model a more expressive family of probability
distributions. However, the unknown normalizing constant of EBMs makes training
particularly difficult. Our goal is to provide a friendly introduction to
modern approaches for EBM training. We start by explaining maximum likelihood
training with Markov chain Monte Carlo (MCMC), and proceed to elaborate on
MCMC-free approaches, including Score Matching (SM) and Noise Constrastive
Estimation (NCE). We highlight theoretical connections among these three
approaches, and end with a brief survey on alternative training methods, which
are still under active research. Our tutorial is targeted at an audience with
basic understanding of generative models who want to apply EBMs or start a
research project in this direction.
</p>
<a href="http://arxiv.org/abs/2101.03288" target="_blank">arXiv:2101.03288</a> [<a href="http://arxiv.org/pdf/2101.03288" target="_blank">pdf</a>]

<h2>Entropy-Based Uncertainty Calibration for Generalized Zero-Shot Learning. (arXiv:2101.03292v1 [cs.CV])</h2>
<h3>Zhi Chen, Zi Huang, Jingjing Li, Zheng Zhang</h3>
<p>Compared to conventional zero-shot learning (ZSL) where recognising unseen
classes is the primary or only aim, the goal of generalized zero-shot learning
(GZSL) is to recognise both seen and unseen classes. Most GZSL methods
typically learn to synthesise visual representations from semantic information
on the unseen classes. However, these types of models are prone to overfitting
the seen classes, resulting in distribution overlap between the generated
features of the seen and unseen classes. The overlapping region is filled with
uncertainty as the model struggles to determine whether a test case from within
the overlap is seen or unseen. Further, these generative methods suffer in
scenarios with sparse training samples. The models struggle to learn the
distribution of high dimensional visual features and, therefore, fail to
capture the most discriminative inter-class features. To address these issues,
in this paper, we propose a novel framework that leverages dual variational
autoencoders with a triplet loss to learn discriminative latent features and
applies the entropy-based calibration to minimize the uncertainty in the
overlapped area between the seen and unseen classes. Specifically, the dual
generative model with the triplet loss synthesises inter-class discriminative
latent features that can be mapped from either visual or semantic space. To
calibrate the uncertainty for seen classes, we calculate the entropy over the
softmax probability distribution from a general classifier. With this approach,
recognising the seen samples within the seen classes is relatively
straightforward, and there is less risk that a seen sample will be
misclassified into an unseen class in the overlapped region. Extensive
experiments on six benchmark datasets demonstrate that the proposed method
outperforms state-of-the-art approaches.
</p>
<a href="http://arxiv.org/abs/2101.03292" target="_blank">arXiv:2101.03292</a> [<a href="http://arxiv.org/pdf/2101.03292" target="_blank">pdf</a>]

<h2>Estimation of Missing Data in Intelligent Transportation System. (arXiv:2101.03295v1 [cs.LG])</h2>
<h3>Bahareh Najafi, Saeedeh Parsaeefard, Alberto Leon-Garcia</h3>
<p>Missing data is a challenge in many applications, including intelligent
transportation systems (ITS). In this paper, we study traffic speed and travel
time estimations in ITS, where portions of the collected data are missing due
to sensor instability and communication errors at collection points. These
practical issues can be remediated by missing data analysis, which are mainly
categorized as either statistical or machine learning(ML)-based approaches.
Statistical methods require the prior probability distribution of the data
which is unknown in our application. Therefore, we focus on an ML-based
approach, Multi-Directional Recurrent Neural Network (M-RNN). M-RNN utilizes
both temporal and spatial characteristics of the data. We evaluate the
effectiveness of this approach on a TomTom dataset containing spatio-temporal
measurements of average vehicle speed and travel time in the Greater Toronto
Area (GTA). We evaluate the method under various conditions, where the results
demonstrate that M-RNN outperforms existing solutions,e.g., spline
interpolation and matrix completion, by up to 58% decreases in Root Mean Square
Error (RMSE).
</p>
<a href="http://arxiv.org/abs/2101.03295" target="_blank">arXiv:2101.03295</a> [<a href="http://arxiv.org/pdf/2101.03295" target="_blank">pdf</a>]

<h2>Robust Blockchained Federated Learning with Model Validation and Proof-of-Stake Inspired Consensus. (arXiv:2101.03300v1 [cs.LG])</h2>
<h3>Hang Chen, Syed Ali Asif, Jihong Park, Chien-Chung Shen, Mehdi Bennis</h3>
<p>Federated learning (FL) is a promising distributed learning solution that
only exchanges model parameters without revealing raw data. However, the
centralized architecture of FL is vulnerable to the single point of failure. In
addition, FL does not examine the legitimacy of local models, so even a small
fraction of malicious devices can disrupt global training. To resolve these
robustness issues of FL, in this paper, we propose a blockchain-based
decentralized FL framework, termed VBFL, by exploiting two mechanisms in a
blockchained architecture. First, we introduced a novel decentralized
validation mechanism such that the legitimacy of local model updates is
examined by individual validators. Second, we designed a dedicated
proof-of-stake consensus mechanism where stake is more frequently rewarded to
honest devices, which protects the legitimate local model updates by increasing
their chances of dictating the blocks appended to the blockchain. Together,
these solutions promote more federation within legitimate devices, enabling
robust FL. Our emulation results of the MNIST classification corroborate that
with 15% of malicious devices, VBFL achieves 87% accuracy, which is 7.4x higher
than Vanilla FL.
</p>
<a href="http://arxiv.org/abs/2101.03300" target="_blank">arXiv:2101.03300</a> [<a href="http://arxiv.org/pdf/2101.03300" target="_blank">pdf</a>]

<h2>Identifying Decision Points for Safe and Interpretable Reinforcement Learning in Hypotension Treatment. (arXiv:2101.03309v1 [cs.LG])</h2>
<h3>Kristine Zhang, Yuanheng Wang, Jianzhun Du, Brian Chu, Leo Anthony Celi, Ryan Kindle, Finale Doshi-Velez</h3>
<p>Many batch RL health applications first discretize time into fixed intervals.
However, this discretization both loses resolution and forces a policy
computation at each (potentially fine) interval. In this work, we develop a
novel framework to compress continuous trajectories into a few, interpretable
decision points --places where the batch data support multiple alternatives. We
apply our approach to create recommendations from a cohort of hypotensive
patients dataset. Our reduced state space results in faster planning and allows
easy inspection by a clinical expert.
</p>
<a href="http://arxiv.org/abs/2101.03309" target="_blank">arXiv:2101.03309</a> [<a href="http://arxiv.org/pdf/2101.03309" target="_blank">pdf</a>]

<h2>FakeBuster: A DeepFakes Detection Tool for Video Conferencing Scenarios. (arXiv:2101.03321v1 [cs.CV])</h2>
<h3>Vineet Mehta, Parul Gupta, Ramanathan Subramanian, Abhinav Dhall</h3>
<p>This paper proposes a new DeepFake detector FakeBuster for detecting
impostors during video conferencing and manipulated faces on social media.
FakeBuster is a standalone deep learning based solution, which enables a user
to detect if another person's video is manipulated or spoofed during a video
conferencing based meeting. This tool is independent of video conferencing
solutions and has been tested with Zoom and Skype applications. It uses a 3D
convolutional neural network for predicting video segment-wise fakeness scores.
The network is trained on a combination of datasets such as Deeperforensics,
DFDC, VoxCeleb, and deepfake videos created using locally captured (for video
conferencing scenarios) images. This leads to different environments and
perturbations in the dataset, which improves the generalization of the deepfake
network.
</p>
<a href="http://arxiv.org/abs/2101.03321" target="_blank">arXiv:2101.03321</a> [<a href="http://arxiv.org/pdf/2101.03321" target="_blank">pdf</a>]

<h2>Pushing the Envelope of Thin Crack Detection. (arXiv:2101.03326v1 [cs.CV])</h2>
<h3>Liang Xu, Taro Hatsutani, Xing Liu, Engkarat Techapanurak, Han Zou, Takayuki Okatani</h3>
<p>In this study, we consider the problem of detecting cracks from the image of
a concrete surface for automated inspection of infrastructure, such as bridges.
Its overall accuracy is determined by how accurately thin cracks with sub-pixel
widths can be detected. Our interest is in making it possible to detect cracks
close to the limit of thinness if it can be defined. Toward this end, we first
propose a method for training a CNN to make it detect cracks more accurately
than humans while training them on human-annotated labels. To achieve this
seemingly impossible goal, we intentionally lower the spatial resolution of
input images while maintaining that of their labels when training a CNN. This
makes it possible to annotate cracks that are too thin for humans to detect,
which we call super-human labels. We experimentally show that this makes it
possible to detect cracks from an image of one-third the resolution of images
used for annotation with about the same accuracy. We additionally propose three
methods for further improving the detection accuracy of thin cracks: i)
P-pooling to maintain small image structures during downsampling operations;
ii) Removal of short-segment cracks in a post-processing step utilizing a prior
of crack shapes learned using the VAE-GAN framework; iii) Modeling uncertainty
of the prediction to better handle hard labels beyond the limit of CNNs'
detection ability, which technically work as noisy labels. We experimentally
examine the effectiveness of these methods.
</p>
<a href="http://arxiv.org/abs/2101.03326" target="_blank">arXiv:2101.03326</a> [<a href="http://arxiv.org/pdf/2101.03326" target="_blank">pdf</a>]

<h2>Interpretable Multiple Treatment Revenue Uplift Modeling. (arXiv:2101.03336v1 [cs.LG])</h2>
<h3>Robin M. Gubela, Stefan Lessmann</h3>
<p>Big data and business analytics are critical drivers of business and societal
transformations. Uplift models support a firm's decision-making by predicting
the change of a customer's behavior due to a treatment. Prior work examines
models for single treatments and binary customer responses. The paper extends
corresponding approaches by developing uplift models for multiple treatments
and continuous outcomes. This facilitates selecting an optimal treatment from a
set of alternatives and estimating treatment effects in the form of business
outcomes of continuous scale. Another contribution emerges from an evaluation
of an uplift model's interpretability, whereas prior studies focus almost
exclusively on predictive performance. To achieve these goals, the paper
develops revenue uplift models for multiple treatments based on a recently
introduced algorithm for causal machine learning, the causal forest. Empirical
experimentation using two real-world marketing data sets demonstrates the
advantages of the proposed modeling approach over benchmarks and standard
marketing practices.
</p>
<a href="http://arxiv.org/abs/2101.03336" target="_blank">arXiv:2101.03336</a> [<a href="http://arxiv.org/pdf/2101.03336" target="_blank">pdf</a>]

<h2>Opportunities of Federated Learning in Connected, Cooperative and Automated Industrial Systems. (arXiv:2101.03367v1 [cs.LG])</h2>
<h3>Stefano Savazzi, Monica Nicoli, Mehdi Bennis, Sanaz Kianoush, Luca Barbieri</h3>
<p>Next-generation autonomous and networked industrial systems (i.e., robots,
vehicles, drones) have driven advances in ultra-reliable, low latency
communications (URLLC) and computing. These networked multi-agent systems
require fast, communication-efficient and distributed machine learning (ML) to
provide mission critical control functionalities. Distributed ML techniques,
including federated learning (FL), represent a mushrooming multidisciplinary
research area weaving in sensing, communication and learning. FL enables
continual model training in distributed wireless systems: rather than fusing
raw data samples at a centralized server, FL leverages a cooperative fusion
approach where networked agents, connected via URLLC, act as distributed
learners that periodically exchange their locally trained model parameters.
This article explores emerging opportunities of FL for the next-generation
networked industrial systems. Open problems are discussed, focusing on
cooperative driving in connected automated vehicles and collaborative robotics
in smart manufacturing.
</p>
<a href="http://arxiv.org/abs/2101.03367" target="_blank">arXiv:2101.03367</a> [<a href="http://arxiv.org/pdf/2101.03367" target="_blank">pdf</a>]

<h2>Active Fire Detection in Landsat-8 Imagery: a Large-Scale Dataset and a Deep-Learning Study. (arXiv:2101.03409v1 [cs.CV])</h2>
<h3>Gabriel Henrique de Almeida Pereira, Andr&#xe9; Minoro Fusioka, Bogdan Tomoyuki Nassu, Rodrigo Minetto</h3>
<p>Active fire detection in satellite imagery is of critical importance to the
management of environmental conservation policies, supporting decision-making
and law enforcement. This is a well established field, with many techniques
being proposed over the years, usually based on pixel or region-level
comparisons involving sensor-specific thresholds and neighborhood statistics.
In this paper, we address the problem of active fire detection using deep
learning techniques. In recent years, deep learning techniques have been
enjoying an enormous success in many fields, but their use for active fire
detection is relatively new, with open questions and demand for datasets and
architectures for evaluation. This paper addresses these issues by introducing
a new large-scale dataset for active fire detection, with over 150,000 image
patches (more than 200 GB of data) extracted from Landsat-8 images captured
around the world in August and September 2020, containing wildfires in several
locations. The dataset was split in two parts, and contains 10-band spectral
images with associated outputs, produced by three well known handcrafted
algorithms for active fire detection in the first part, and manually annotated
masks in the second part. We also present a study on how different
convolutional neural network architectures can be used to approximate these
handcrafted algorithms, and how models trained on automatically segmented
patches can be combined to achieve better performance than the original
algorithms - with the best combination having 87.2% precision and 92.4% recall
on our manually annotated dataset. The proposed dataset, source codes and
trained models are available on Github
(https://github.com/pereira-gha/activefire), creating opportunities for further
advances in the field
</p>
<a href="http://arxiv.org/abs/2101.03409" target="_blank">arXiv:2101.03409</a> [<a href="http://arxiv.org/pdf/2101.03409" target="_blank">pdf</a>]

<h2>Training Deep Architectures Without End-to-End Backpropagation: A Brief Survey. (arXiv:2101.03419v1 [cs.LG])</h2>
<h3>Shiyu Duan, Jose C. Principe</h3>
<p>This tutorial paper surveys training alternatives to end-to-end
backpropagation (E2EBP) -- the de facto standard for training deep
architectures. Modular training refers to strictly local training without both
the forward and the backward pass, i.e., dividing a deep architecture into
several nonoverlapping modules and training them separately without any
end-to-end operation. Between the fully global E2EBP and the strictly local
modular training, there are "weakly modular" hybrids performing training
without the backward pass only. These alternatives can match or surpass the
performance of E2EBP on challenging datasets such as ImageNet, and are gaining
increased attention primarily because they offer practical advantages over
E2EBP, which will be enumerated herein. In particular, they allow for greater
modularity and transparency in deep learning workflows, aligning deep learning
with the mainstream computer science engineering that heavily exploits
modularization for scalability. Modular training has also revealed novel
insights about learning and may have further implications on other important
research domains. Specifically, it induces natural and effective solutions to
some important practical problems such as data efficiency and transferability
estimation.
</p>
<a href="http://arxiv.org/abs/2101.03419" target="_blank">arXiv:2101.03419</a> [<a href="http://arxiv.org/pdf/2101.03419" target="_blank">pdf</a>]

<h2>Are We There Yet? Learning to Localize in Embodied Instruction Following. (arXiv:2101.03431v1 [cs.AI])</h2>
<h3>Shane Storks, Qiaozi Gao, Govind Thattai, Gokhan Tur</h3>
<p>Embodied instruction following is a challenging problem requiring an agent to
infer a sequence of primitive actions to achieve a goal environment state from
complex language and visual inputs. Action Learning From Realistic Environments
and Directives (ALFRED) is a recently proposed benchmark for this problem
consisting of step-by-step natural language instructions to achieve subgoals
which compose to an ultimate high-level goal. Key challenges for this task
include localizing target locations and navigating to them through visual
inputs, and grounding language instructions to visual appearance of objects. To
address these challenges, in this study, we augment the agent's field of view
during navigation subgoals with multiple viewing angles, and train the agent to
predict its relative spatial relation to the target location at each timestep.
We also improve language grounding by introducing a pre-trained object
detection module to the model pipeline. Empirical studies show that our
approach exceeds the baseline model performance.
</p>
<a href="http://arxiv.org/abs/2101.03431" target="_blank">arXiv:2101.03431</a> [<a href="http://arxiv.org/pdf/2101.03431" target="_blank">pdf</a>]

<h2>SPAGAN: Shortest Path Graph Attention Network. (arXiv:2101.03464v1 [cs.LG])</h2>
<h3>Yiding Yang, Xinchao Wang, Mingli Song, Junsong Yuan, Dacheng Tao</h3>
<p>Graph convolutional networks (GCN) have recently demonstrated their potential
in analyzing non-grid structure data that can be represented as graphs. The
core idea is to encode the local topology of a graph, via convolutions, into
the feature of a center node. In this paper, we propose a novel GCN model,
which we term as Shortest Path Graph Attention Network (SPAGAN). Unlike
conventional GCN models that carry out node-based attentions within each layer,
the proposed SPAGAN conducts path-based attention that explicitly accounts for
the influence of a sequence of nodes yielding the minimum cost, or shortest
path, between the center node and its higher-order neighbors. SPAGAN therefore
allows for a more informative and intact exploration of the graph structure and
further {a} more effective aggregation of information from distant neighbors
into the center node, as compared to node-based GCN methods. We test SPAGAN on
the downstream classification task on several standard datasets, and achieve
performances superior to the state of the art. Code is publicly available at
https://github.com/ihollywhy/SPAGAN.
</p>
<a href="http://arxiv.org/abs/2101.03464" target="_blank">arXiv:2101.03464</a> [<a href="http://arxiv.org/pdf/2101.03464" target="_blank">pdf</a>]

<h2>Using Crowdsourcing to Train Facial Emotion Machine Learning Models with Ambiguous Labels. (arXiv:2101.03477v1 [cs.CV])</h2>
<h3>Peter Washington, Onur Cezmi Mutlu, Emilie Leblanc, Aaron Kline, Cathy Hou, Brianna Chrisman, Nate Stockham, Kelley Paskov, Catalin Voss, Nick Haber, Dennis Wall</h3>
<p>Current emotion detection classifiers predict discrete emotions. However,
literature in psychology has documented that compound and ambiguous facial
expressions are often evoked by humans. As a stride towards development of
machine learning models that more accurately reflect compound and ambiguous
emotions, we replace traditional one-hot encoded label representations with a
crowd's distribution of labels. We center our study on the Child Affective
Facial Expression (CAFE) dataset, a gold standard dataset of pediatric facial
expressions which includes 100 human labels per image. We first acquire
crowdsourced labels for 207 emotions from CAFE and demonstrate that the
consensus labels from the crowd tend to match the consensus from the original
CAFE raters, validating the utility of crowdsourcing. We then train two
versions of a ResNet-152 classifier on CAFE images with two types of labels (1)
traditional one-hot encoding and (2) vector labels representing the crowd
distribution of responses. We compare the resulting output distributions of the
two classifiers. While the traditional F1-score for the one-hot encoding
classifier is much higher (94.33% vs. 78.68%), the output probability vector of
the crowd-trained classifier much more closely resembles the distribution of
human labels (t=3.2827, p=0.0014). For many applications of affective
computing, reporting an emotion probability distribution that more closely
resembles human interpretation can be more important than traditional machine
learning metrics. This work is a first step for engineers of interactive
systems to account for machine learning cases with ambiguous classes and we
hope it will generate a discussion about machine learning with ambiguous labels
and leveraging crowdsourcing as a potential solution.
</p>
<a href="http://arxiv.org/abs/2101.03477" target="_blank">arXiv:2101.03477</a> [<a href="http://arxiv.org/pdf/2101.03477" target="_blank">pdf</a>]

<h2>Activity Recognition with Moving Cameras and Few Training Examples: Applications for Detection of Autism-Related Headbanging. (arXiv:2101.03478v1 [cs.CV])</h2>
<h3>Peter Washington, Aaron Kline, Onur Cezmi Mutlu, Emilie Leblanc, Cathy Hou, Nate Stockham, Kelley Paskov, Brianna Chrisman, Dennis P. Wall</h3>
<p>Activity recognition computer vision algorithms can be used to detect the
presence of autism-related behaviors, including what are termed "restricted and
repetitive behaviors", or stimming, by diagnostic instruments. The limited data
that exist in this domain are usually recorded with a handheld camera which can
be shaky or even moving, posing a challenge for traditional feature
representation approaches for activity detection which mistakenly capture the
camera's motion as a feature. To address these issues, we first document the
advantages and limitations of current feature representation techniques for
activity recognition when applied to head banging detection. We then propose a
feature representation consisting exclusively of head pose keypoints. We create
a computer vision classifier for detecting head banging in home videos using a
time-distributed convolutional neural network (CNN) in which a single CNN
extracts features from each frame in the input sequence, and these extracted
features are fed as input to a long short-term memory (LSTM) network. On the
binary task of predicting head banging and no head banging within videos from
the Self Stimulatory Behaviour Dataset (SSBD), we reach a mean F1-score of
90.77% using 3-fold cross validation (with individual fold F1-scores of 83.3%,
89.0%, and 100.0%) when ensuring that no child who appeared in the train set
was in the test set for all folds. This work documents a successful technique
for training a computer vision classifier which can detect human motion with
few training examples and even when the camera recording the source clips is
unstable. The general methods described here can be applied by designers and
developers of interactive systems towards other human motion and pose
classification problems used in mobile and ubiquitous interactive systems.
</p>
<a href="http://arxiv.org/abs/2101.03478" target="_blank">arXiv:2101.03478</a> [<a href="http://arxiv.org/pdf/2101.03478" target="_blank">pdf</a>]

<h2>Reinforcement Learning Enabled Automatic Impedance Control of a Robotic Knee Prosthesis to Mimic the Intact Knee Motion in a Co-Adapting Environment. (arXiv:2101.03487v1 [cs.RO])</h2>
<h3>Ruofan Wu, Minhan Li, Zhikai Yao, Jennie Si, He (Helen) Huang</h3>
<p>Automatically configuring a robotic prosthesis to fit its user's needs and
physical conditions is a great technical challenge and a roadblock to the
adoption of the technology. Previously, we have successfully developed
reinforcement learning (RL) solutions toward addressing this issue. Yet, our
designs were based on using a subjectively prescribed target motion profile for
the robotic knee during level ground walking. This is not realistic for
different users and for different locomotion tasks. In this study for the first
time, we investigated the feasibility of RL enabled automatic configuration of
impedance parameter settings for a robotic knee to mimic the intact knee motion
in a co-adapting environment. We successfully achieved such tracking control by
an online policy iteration. We demonstrated our results in both OpenSim
simulations and two able-bodied (AB) subjects.
</p>
<a href="http://arxiv.org/abs/2101.03487" target="_blank">arXiv:2101.03487</a> [<a href="http://arxiv.org/pdf/2101.03487" target="_blank">pdf</a>]

<h2>Semantic Segmentation of Remote Sensing Images with Sparse Annotations. (arXiv:2101.03492v1 [cs.CV])</h2>
<h3>Yuansheng Hua, Diego Marcos, Lichao Mou, Xiao Xiang Zhu, Devis Tuia</h3>
<p>Training Convolutional Neural Networks (CNNs) for very high resolution images
requires a large quantity of high-quality pixel-level annotations, which is
extremely labor- and time-consuming to produce. Moreover, professional photo
interpreters might have to be involved for guaranteeing the correctness of
annotations. To alleviate such a burden, we propose a framework for semantic
segmentation of aerial images based on incomplete annotations, where annotators
are asked to label a few pixels with easy-to-draw scribbles. To exploit these
sparse scribbled annotations, we propose the FEature and Spatial relaTional
regulArization (FESTA) method to complement the supervised task with an
unsupervised learning signal that accounts for neighbourhood structures both in
spatial and feature terms.
</p>
<a href="http://arxiv.org/abs/2101.03492" target="_blank">arXiv:2101.03492</a> [<a href="http://arxiv.org/pdf/2101.03492" target="_blank">pdf</a>]

<h2>Joint Prediction of Remaining Useful Life and Failure Type of Train Wheelsets: A Multi-task Learning Approach. (arXiv:2101.03497v1 [cs.LG])</h2>
<h3>Weixin Wang</h3>
<p>The failures of train wheels account for disruptions of train operations and
even a large portion of train derailments. Remaining useful life (RUL) of a
wheelset measures the how soon the next failure will arrive, and the failure
type reveals how severe the failure will be. RUL prediction is a regression
task, whereas failure type is a classification task. In this paper, we propose
a multi-task learning approach to jointly accomplish these two tasks by using a
common input space to achieve more desirable results. We develop a convex
optimization formulation to integrate both least square loss and the negative
maximum likelihood of logistic regression, and model the joint sparsity as the
L2/L1 norm of the model parameters to couple feature selection across tasks.
The experiment results show that our method outperforms the single task
learning method by 3% in prediction accuracy.
</p>
<a href="http://arxiv.org/abs/2101.03497" target="_blank">arXiv:2101.03497</a> [<a href="http://arxiv.org/pdf/2101.03497" target="_blank">pdf</a>]

<h2>Sum-Rate Maximization for UAV-assisted Visible Light Communications using NOMA: Swarm Intelligence meets Machine Learning. (arXiv:2101.03498v1 [cs.LG])</h2>
<h3>Quoc-Viet Pham, Thien Huynh-The, Mamoun Alazab, Jun Zhao, Won-Joo Hwang</h3>
<p>As the integration of unmanned aerial vehicles (UAVs) into visible light
communications (VLC) can offer many benefits for massive-connectivity
applications and services in 5G and beyond, this work considers a UAV-assisted
VLC using non-orthogonal multiple-access. More specifically, we formulate a
joint problem of power allocation and UAV's placement to maximize the sum rate
of all users, subject to constraints on power allocation, quality of service of
users, and UAV's position. Since the problem is non-convex and NP-hard in
general, it is difficult to be solved optimally. Moreover, the problem is not
easy to be solved by conventional approaches, e.g., coordinate descent
algorithms, due to channel modeling in VLC. Therefore, we propose using harris
hawks optimization (HHO) algorithm to solve the formulated problem and obtain
an efficient solution. We then use the HHO algorithm together with artificial
neural networks to propose a design which can be used in real-time applications
and avoid falling into the "local minima" trap in conventional trainers.
Numerical results are provided to verify the effectiveness of the proposed
algorithm and further demonstrate that the proposed algorithm/HHO trainer is
superior to several alternative schemes and existing metaheuristic algorithms.
</p>
<a href="http://arxiv.org/abs/2101.03498" target="_blank">arXiv:2101.03498</a> [<a href="http://arxiv.org/pdf/2101.03498" target="_blank">pdf</a>]

<h2>Improved active output selection strategy for noisy environments. (arXiv:2101.03499v1 [cs.LG])</h2>
<h3>Adrian Prochaska, Julien Pillas, Bernard B&#xe4;ker</h3>
<p>The test bench time needed for model-based calibration can be reduced with
active learning methods for test design. This paper presents an improved
strategy for active output selection. This is the task of learning multiple
models in the same input dimensions and suits the needs of calibration tasks.
Compared to an existing strategy, we take into account the noise estimate,
which is inherent to Gaussian processes. The method is validated on three
different toy examples. The performance compared to the existing best strategy
is the same or better in each example. In a best case scenario, the new
strategy needs at least 10% less measurements compared to all other active or
passive strategies. Further efforts will evaluate the strategy on a real-world
application. Moreover, the implementation of more sophisticated active-learning
strategies for the query placement will be realized.
</p>
<a href="http://arxiv.org/abs/2101.03499" target="_blank">arXiv:2101.03499</a> [<a href="http://arxiv.org/pdf/2101.03499" target="_blank">pdf</a>]

<h2>Entropic Causal Inference: Identifiability and Finite Sample Results. (arXiv:2101.03501v1 [stat.ML])</h2>
<h3>Spencer Compton, Murat Kocaoglu, Kristjan Greenewald, Dmitriy Katz</h3>
<p>Entropic causal inference is a framework for inferring the causal direction
between two categorical variables from observational data. The central
assumption is that the amount of unobserved randomness in the system is not too
large. This unobserved randomness is measured by the entropy of the exogenous
variable in the underlying structural causal model, which governs the causal
relation between the observed variables. Kocaoglu et al. conjectured that the
causal direction is identifiable when the entropy of the exogenous variable is
not too large. In this paper, we prove a variant of their conjecture. Namely,
we show that for almost all causal models where the exogenous variable has
entropy that does not scale with the number of states of the observed
variables, the causal direction is identifiable from observational data. We
also consider the minimum entropy coupling-based algorithmic approach presented
by Kocaoglu et al., and for the first time demonstrate algorithmic
identifiability guarantees using a finite number of samples. We conduct
extensive experiments to evaluate the robustness of the method to relaxing some
of the assumptions in our theory and demonstrate that both the constant-entropy
exogenous variable and the no latent confounder assumptions can be relaxed in
practice. We also empirically characterize the number of observational samples
needed for causal identification. Finally, we apply the algorithm on Tuebingen
cause-effect pairs dataset.
</p>
<a href="http://arxiv.org/abs/2101.03501" target="_blank">arXiv:2101.03501</a> [<a href="http://arxiv.org/pdf/2101.03501" target="_blank">pdf</a>]

<h2>CapsField: Light Field-based Face and Expression Recognition in the Wild using Capsule Routing. (arXiv:2101.03503v1 [cs.CV])</h2>
<h3>Alireza Sepas-Moghaddam, Ali Etemad, Fernando Pereira, Paulo Lobato Correia</h3>
<p>Light field (LF) cameras provide rich spatio-angular visual representations
by sensing the visual scene from multiple perspectives and have recently
emerged as a promising technology to boost the performance of human-machine
systems such as biometrics and affective computing. Despite the significant
success of LF representation for constrained facial image analysis, this
technology has never been used for face and expression recognition in the wild.
In this context, this paper proposes a new deep face and expression recognition
solution, called CapsField, based on a convolutional neural network and an
additional capsule network that utilizes dynamic routing to learn hierarchical
relations between capsules. CapsField extracts the spatial features from facial
images and learns the angular part-whole relations for a selected set of 2D
sub-aperture images rendered from each LF image. To analyze the performance of
the proposed solution in the wild, the first in the wild LF face dataset, along
with a new complementary constrained face dataset captured from the same
subjects recorded earlier have been captured and are made available. A subset
of the in the wild dataset contains facial images with different expressions,
annotated for usage in the context of face expression recognition tests. An
extensive performance assessment study using the new datasets has been
conducted for the proposed and relevant prior solutions, showing that the
CapsField proposed solution achieves superior performance for both face and
expression recognition tasks when compared to the state-of-the-art.
</p>
<a href="http://arxiv.org/abs/2101.03503" target="_blank">arXiv:2101.03503</a> [<a href="http://arxiv.org/pdf/2101.03503" target="_blank">pdf</a>]

<h2>Cross-Modal Contrastive Learning of Representations for Navigation using Lightweight, Low-Cost Millimeter Wave Radar for Adverse Environmental Conditions. (arXiv:2101.03525v1 [cs.RO])</h2>
<h3>Jui-Te Huang, Chen-Lung Lu, Po-Kai Chang, Ching-I Huang, Chao-Chun Hsu, Zu Lin Ewe, Po-Jui Huang, Hsueh-Cheng Wang</h3>
<p>Deep reinforcement learning (RL), where the agent learns from mistakes, has
been successfully applied to a variety of tasks. With the aim of learning
collision-free policies for unmanned vehicles, deep RL has been used for
training with various types of data, such as colored images, depth images, and
LiDAR point clouds, without the use of classic map--localize--plan approaches.
However, existing methods are limited by their reliance on cameras and LiDAR
devices, which have degraded sensing under adverse environmental conditions
(e.g., smoky environments). In response, we propose the use of single-chip
millimeter-wave (mmWave) radar, which is lightweight and inexpensive, for
learning-based autonomous navigation. However, because mmWave radar signals are
often noisy and sparse, we propose a cross-modal contrastive learning for
representation (CM-CLR) method that maximizes the agreement between mmWave
radar data and LiDAR data in the training stage. We evaluated our method in
real-world robot compared with 1) a method with two separate networks using
cross-modal generative reconstruction and an RL policy and 2) a baseline RL
policy without cross-modal representation. Our proposed end-to-end deep RL
policy with contrastive learning successfully navigated the robot through
smoke-filled maze environments and achieved better performance compared with
generative reconstruction methods, in which noisy artifact walls or obstacles
were produced. All pretrained models and hardware settings are open access for
reproducing this study and can be obtained at
https://arg-nctu.github.io/projects/deeprl-mmWave.html
</p>
<a href="http://arxiv.org/abs/2101.03525" target="_blank">arXiv:2101.03525</a> [<a href="http://arxiv.org/pdf/2101.03525" target="_blank">pdf</a>]

<h2>Channel Boosting Feature Ensemble for Radar-based Object Detection. (arXiv:2101.03531v1 [cs.CV])</h2>
<h3>Shoaib Azam, Farzeen Munir, Moongu Jeon</h3>
<p>Autonomous vehicles are conceived to provide safe and secure services by
validating the safety standards as indicated by SOTIF-ISO/PAS-21448 (Safety of
the intended functionality). Keeping in this context, the perception of the
environment plays an instrumental role in conjunction with localization,
planning and control modules. As a pivotal algorithm in the perception stack,
object detection provides extensive insights into the autonomous vehicle's
surroundings. Camera and Lidar are extensively utilized for object detection
among different sensor modalities, but these exteroceptive sensors have
limitations in resolution and adverse weather conditions. In this work,
radar-based object detection is explored provides a counterpart sensor modality
to be deployed and used in adverse weather conditions. The radar gives complex
data; for this purpose, a channel boosting feature ensemble method with
transformer encoder-decoder network is proposed. The object detection task
using radar is formulated as a set prediction problem and evaluated on the
publicly available dataset in both good and good-bad weather conditions. The
proposed method's efficacy is extensively evaluated using the COCO evaluation
metric, and the best-proposed model surpasses its state-of-the-art counterpart
method by $12.55\%$ and $12.48\%$ in both good and good-bad weather conditions.
</p>
<a href="http://arxiv.org/abs/2101.03531" target="_blank">arXiv:2101.03531</a> [<a href="http://arxiv.org/pdf/2101.03531" target="_blank">pdf</a>]

<h2>Heatmap-based Object Detection and Tracking with a Fully Convolutional Neural Network. (arXiv:2101.03541v1 [cs.CV])</h2>
<h3>Fabian Amherd, Elias Rodriguez</h3>
<p>The main topic of this paper is a brief overview of the field of Artificial
Intelligence. The core of this paper is a practical implementation of an
algorithm for object detection and tracking. The ability to detect and track
fast-moving objects is crucial for various applications of Artificial
Intelligence like autonomous driving, ball tracking in sports, robotics or
object counting. As part of this paper the Fully Convolutional Neural Network
"CueNet" was developed. It detects and tracks the cueball on a labyrinth game
robustly and reliably. While CueNet V1 has a single input image, the approach
with CueNet V2 was to take three consecutive 240 x 180-pixel images as an input
and transform them into a probability heatmap for the cueball's location. The
network was tested with a separate video that contained all sorts of
distractions to test its robustness. When confronted with our testing data,
CueNet V1 predicted the correct cueball location in 99.6% of all frames, while
CueNet V2 had 99.8% accuracy.
</p>
<a href="http://arxiv.org/abs/2101.03541" target="_blank">arXiv:2101.03541</a> [<a href="http://arxiv.org/pdf/2101.03541" target="_blank">pdf</a>]

<h2>PowerEvaluationBALD: Efficient Evaluation-Oriented Deep (Bayesian) Active Learning with Stochastic Acquisition Functions. (arXiv:2101.03552v1 [cs.LG])</h2>
<h3>Andreas Kirsch</h3>
<p>We develop BatchEvaluationBALD, a new acquisition function for deep Bayesian
active learning, as an expansion of BatchBALD that takes into account an
evaluation set of unlabeled data, for example, the pool set. We also develop a
variant for the non-Bayesian setting, which we call Evaluation Information
Gain. To reduce computational requirements and allow these methods to scale to
larger acquisition batch sizes, we introduce stochastic acquisition functions
that use importance-sampling of tempered acquisition scores. We call this
method PowerEvaluationBALD. We show in first experiments that
PowerEvaluationBALD works on par with BatchEvaluationBALD, which outperforms
BatchBALD on Repeated MNIST (MNISTx2), while massively reducing the
computational requirements compared to BatchBALD or BatchEvaluationBALD.
</p>
<a href="http://arxiv.org/abs/2101.03552" target="_blank">arXiv:2101.03552</a> [<a href="http://arxiv.org/pdf/2101.03552" target="_blank">pdf</a>]

<h2>Sub-Goal Social Force Model for Collective Pedestrian Motion Under Vehicle Influence. (arXiv:2101.03554v1 [cs.RO])</h2>
<h3>Dongfang Yang, Fatema T. Johora, Keith A. Redmill, &#xdc;mit &#xd6;zg&#xfc;ner, J&#xf6;rg P. M&#xfc;ller</h3>
<p>In mixed traffic scenarios, a certain number of pedestrians might coexist in
a small area while interacting with vehicles. In this situation, every
pedestrian must simultaneously react to the surrounding pedestrians and
vehicles. Analytical modeling of such collective pedestrian motion can benefit
intelligent transportation practices like shared space design and urban
autonomous driving. This work proposed the sub-goal social force model (SG-SFM)
to describe the collective pedestrian motion under vehicle influence. The
proposed model introduced a new design of vehicle influence on pedestrian
motion, which was smoothly combined with the influence of surrounding
pedestrians using the sub-goal concept. This model aims to describe generalized
pedestrian motion, i.e., it is applicable to various vehicle-pedestrian
interaction patterns. The generalization was verified by both quantitative and
qualitative evaluation. The quantitative evaluation was conducted to reproduce
pedestrian motion in three different datasets, HBS, CITR, and DUT. It also
compared two different ways of calibrating the model parameters. The
qualitative evaluation examined the simulation of collective pedestrian motion
in a series of fundamental vehicle-pedestrian interaction scenarios. The above
evaluation results demonstrated the effectiveness of the proposed model.
</p>
<a href="http://arxiv.org/abs/2101.03554" target="_blank">arXiv:2101.03554</a> [<a href="http://arxiv.org/pdf/2101.03554" target="_blank">pdf</a>]

<h2>Learning from Satisfying Assignments Using Risk Minimization. (arXiv:2101.03558v1 [cs.LG])</h2>
<h3>Manjish Pal. Subham Pokhriyal</h3>
<p>In this paper we consider the problem of Learning from Satisfying Assignments
introduced by \cite{1} of finding a distribution that is a close approximation
to the uniform distribution over the satisfying assignments of a low complexity
Boolean function $f$. In a later work \cite{2} consider the same problem but
with the knowledge of some continuous distribution $D$ and the objective being
to estimate $D_f$, which is $D$ restricted to the satisfying assignments of an
unknown Boolean function $f$. We consider these problems from the point of view
of parameter estimation techniques in statistical machine learning and prove
similar results that are based on standard optimization algorithms for Risk
Minimization.
</p>
<a href="http://arxiv.org/abs/2101.03558" target="_blank">arXiv:2101.03558</a> [<a href="http://arxiv.org/pdf/2101.03558" target="_blank">pdf</a>]

<h2>Stabilized Nested Rollout Policy Adaptation. (arXiv:2101.03563v1 [cs.AI])</h2>
<h3>Tristan Cazenave, Jean-Baptiste Sevestre, Matthieu Toulemont</h3>
<p>Nested Rollout Policy Adaptation (NRPA) is a Monte Carlo search algorithm for
single player games. In this paper we propose to modify NRPA in order to
improve the stability of the algorithm. Experiments show it improves the
algorithm for different application domains: SameGame, Traveling Salesman with
Time Windows and Expression Discovery.
</p>
<a href="http://arxiv.org/abs/2101.03563" target="_blank">arXiv:2101.03563</a> [<a href="http://arxiv.org/pdf/2101.03563" target="_blank">pdf</a>]

<h2>Curvature-based Feature Selection with Application in Classifying Electronic Health Records. (arXiv:2101.03581v1 [cs.LG])</h2>
<h3>Zheming Zuo, Jie Li, Noura Al Moubayed</h3>
<p>Electronic Health Records (EHRs) are widely applied in healthcare facilities
nowadays. Due to the inherent heterogeneity, unbalanced, incompleteness, and
high-dimensional nature of EHRs, it is a challenging task to employ machine
learning algorithms to analyse such EHRs for prediction and diagnostics within
the scope of precision medicine. Dimensionality reduction is an efficient data
preprocessing technique for the analysis of high dimensional data that reduces
the number of features while improving the performance of the data analysis,
e.g. classification. In this paper, we propose an efficient curvature-based
feature selection method for supporting more precise diagnosis. The proposed
method is a filter-based feature selection method, which directly utilises the
Menger Curvature for ranking all the attributes in the given data set. We
evaluate the performance of our method against conventional PCA and recent ones
including BPCM, GSAM, WCNN, BLS II, VIBES, 2L-MJFA, RFGA, and VAF. Our method
achieves state-of-the-art performance on four benchmark healthcare data sets
including CCRFDS, BCCDS, BTDS, and DRDDS with impressive 24.73% and 13.93%
improvements respectively on BTDS and CCRFDS, 7.97% improvement on BCCDS, and
3.63% improvement on DRDDS. Our CFS source code is publicly available at
https://github.com/zhemingzuo/CFS.
</p>
<a href="http://arxiv.org/abs/2101.03581" target="_blank">arXiv:2101.03581</a> [<a href="http://arxiv.org/pdf/2101.03581" target="_blank">pdf</a>]

<h2>Provably Approximated ICP. (arXiv:2101.03588v1 [cs.CV])</h2>
<h3>Ibrahim Jubran, Alaa Maalouf, Ron Kimmel, Dan Feldman</h3>
<p>The goal of the \emph{alignment problem} is to align a (given) point cloud $P
= \{p_1,\cdots,p_n\}$ to another (observed) point cloud $Q =
\{q_1,\cdots,q_n\}$. That is, to compute a rotation matrix $R \in \mathbb{R}^{3
\times 3}$ and a translation vector $t \in \mathbb{R}^{3}$ that minimize the
sum of paired distances $\sum_{i=1}^n D(Rp_i-t,q_i)$ for some distance function
$D$. A harder version is the \emph{registration problem}, where the
correspondence is unknown, and the minimum is also over all possible
correspondence functions from $P$ to $Q$. Heuristics such as the Iterative
Closest Point (ICP) algorithm and its variants were suggested for these
problems, but none yield a provable non-trivial approximation for the global
optimum.

We prove that there \emph{always} exists a "witness" set of $3$ pairs in $P
\times Q$ that, via novel alignment algorithm, defines a constant factor
approximation (in the worst case) to this global optimum. We then provide
algorithms that recover this witness set and yield the first provable constant
factor approximation for the: (i) alignment problem in $O(n)$ expected time,
and (ii) registration problem in polynomial time. Such small witness sets exist
for many variants including points in $d$-dimensional space, outlier-resistant
cost functions, and different correspondence types.

Extensive experimental results on real and synthetic datasets show that our
approximation constants are, in practice, close to $1$, and up to x$10$ times
smaller than state-of-the-art algorithms.
</p>
<a href="http://arxiv.org/abs/2101.03588" target="_blank">arXiv:2101.03588</a> [<a href="http://arxiv.org/pdf/2101.03588" target="_blank">pdf</a>]

<h2>Target Detection and Segmentation in Circular-Scan Synthetic-Aperture-Sonar Images using Semi-Supervised Convolutional Encoder-Decoders. (arXiv:2101.03603v1 [cs.CV])</h2>
<h3>Isaac J. Sledge, Matthew S. Emigh, Jonathan L. King, Denton L. Woods, J. Tory Cobb, Jose C. Principe</h3>
<p>We propose a saliency-based, multi-target detection and segmentation
framework for multi-aspect, semi-coherent imagery formed from circular-scan,
synthetic-aperture sonar (CSAS). Our framework relies on a multi-branch,
convolutional encoder-decoder network (MB-CEDN). The encoder portion extracts
features from one or more CSAS images of the targets. These features are then
split off and fed into multiple decoders that perform pixel-level
classification on the extracted features to roughly mask the target in an
unsupervised-trained manner and detect foreground and background pixels in a
supervised-trained manner. Each of these target-detection estimates provide
different perspectives as to what constitute a target. These opinions are
cascaded into a deep-parsing network to model contextual and spatial
constraints that help isolate targets better than either solution estimate
alone.

We evaluate our framework using real-world CSAS data with five broad target
classes. Since we are the first to consider both CSAS target detection and
segmentation, we adapt existing image and video-processing network topologies
from the literature for comparative purposes. We show that our framework
outperforms supervised deep networks. It greatly outperforms state-of-the-art
unsupervised approaches for diverse target and seafloor types.
</p>
<a href="http://arxiv.org/abs/2101.03603" target="_blank">arXiv:2101.03603</a> [<a href="http://arxiv.org/pdf/2101.03603" target="_blank">pdf</a>]

<h2>Combining Neural Network Models for Blood Cell Classification. (arXiv:2101.03604v1 [cs.CV])</h2>
<h3>Indraneel Ghosh, Siddhant Kundu</h3>
<p>The objective of the study is to evaluate the efficiency of a multi layer
neural network models built by combining Recurrent Neural Network(RNN) and
Convolutional Neural Network(CNN) for solving the problem of classifying
different types of White Blood Cells. This can have applications in the
pharmaceutical and healthcare industry for automating the analysis of blood
tests and other processes requiring identifying the nature of blood cells in a
given image sample. It can also be used in the diagnosis of various
blood-related diseases in patients.
</p>
<a href="http://arxiv.org/abs/2101.03604" target="_blank">arXiv:2101.03604</a> [<a href="http://arxiv.org/pdf/2101.03604" target="_blank">pdf</a>]

<h2>The Gaussian Neural Process. (arXiv:2101.03606v1 [stat.ML])</h2>
<h3>Wessel P. Bruinsma, James Requeima, Andrew Y. K. Foong, Jonathan Gordon, Richard E. Turner</h3>
<p>Neural Processes (NPs; Garnelo et al., 2018a,b) are a rich class of models
for meta-learning that map data sets directly to predictive stochastic
processes. We provide a rigorous analysis of the standard maximum-likelihood
objective used to train conditional NPs. Moreover, we propose a new member to
the Neural Process family called the Gaussian Neural Process (GNP), which
models predictive correlations, incorporates translation equivariance, provides
universal approximation guarantees, and demonstrates encouraging performance.
</p>
<a href="http://arxiv.org/abs/2101.03606" target="_blank">arXiv:2101.03606</a> [<a href="http://arxiv.org/pdf/2101.03606" target="_blank">pdf</a>]

<h2>Neurocognitive Informatics Manifesto. (arXiv:2101.03609v1 [cs.AI])</h2>
<h3>W&#x142;odzis&#x142;aw Duch</h3>
<p>Informatics studies all aspects of the structure of natural and artificial
information systems. Theoretical and abstract approaches to information have
made great advances, but human information processing is still unmatched in
many areas, including information management, representation and understanding.
Neurocognitive informatics is a new, emerging field that should help to improve
the matching of artificial and natural systems, and inspire better
computational algorithms to solve problems that are still beyond the reach of
machines. In this position paper examples of neurocognitive inspirations and
promising directions in this area are given.
</p>
<a href="http://arxiv.org/abs/2101.03609" target="_blank">arXiv:2101.03609</a> [<a href="http://arxiv.org/pdf/2101.03609" target="_blank">pdf</a>]

<h2>Explainable Artificial Intelligence (XAI): An Engineering Perspective. (arXiv:2101.03613v1 [cs.LG])</h2>
<h3>F. Hussain, R. Hussain, E. Hossain</h3>
<p>The remarkable advancements in Deep Learning (DL) algorithms have fueled
enthusiasm for using Artificial Intelligence (AI) technologies in almost every
domain; however, the opaqueness of these algorithms put a question mark on
their applications in safety-critical systems. In this regard, the
`explainability' dimension is not only essential to both explain the inner
workings of black-box algorithms, but it also adds accountability and
transparency dimensions that are of prime importance for regulators, consumers,
and service providers. eXplainable Artificial Intelligence (XAI) is the set of
techniques and methods to convert the so-called black-box AI algorithms to
white-box algorithms, where the results achieved by these algorithms and the
variables, parameters, and steps taken by the algorithm to reach the obtained
results, are transparent and explainable. To complement the existing literature
on XAI, in this paper, we take an `engineering' approach to illustrate the
concepts of XAI. We discuss the stakeholders in XAI and describe the
mathematical contours of XAI from engineering perspective. Then we take the
autonomous car as a use-case and discuss the applications of XAI for its
different components such as object detection, perception, control, action
decision, and so on. This work is an exploratory study to identify new avenues
of research in the field of XAI.
</p>
<a href="http://arxiv.org/abs/2101.03613" target="_blank">arXiv:2101.03613</a> [<a href="http://arxiv.org/pdf/2101.03613" target="_blank">pdf</a>]

<h2>Occupancy Detection in Room Using Sensor Data. (arXiv:2101.03616v1 [cs.LG])</h2>
<h3>Mohammadhossein Toutiaee</h3>
<p>With the advent of Internet of Thing (IoT), and ubiquitous data collected
every moment by either portable (smart phone) or fixed (sensor) devices, it is
important to gain insights and meaningful information from the sensor data in
context-aware computing environments. Many researches have been implemented by
scientists in different fields, to analyze such data for the purpose of
security, energy efficiency, building reliability and smart environments. One
study, that many researchers are interested in, is to utilize Machine Learning
techniques for occupancy detection where the aforementioned sensors gather
information about the environment. This paper provides a solution to detect
occupancy using sensor data by using and testing several variables.
Additionally we show the analysis performed over the gathered data using
Machine Learning and pattern recognition mechanisms is possible to determine
the occupancy of indoor environments. Seven famous algorithms in Machine
Learning, namely as Decision Tree, Random Forest, Gradient Boosting Machine,
Logistic Regression, Naive Bayes, Kernelized SVM and K-Nearest Neighbors are
tested and compared in this study.
</p>
<a href="http://arxiv.org/abs/2101.03616" target="_blank">arXiv:2101.03616</a> [<a href="http://arxiv.org/pdf/2101.03616" target="_blank">pdf</a>]

<h2>Compliant Fins for Locomotion in Granular Media. (arXiv:2101.03624v1 [cs.RO])</h2>
<h3>Dongting Li, Sichuan Huang, Yong Tang, Hamidreza Marvi, Daniel M. Aukes</h3>
<p>In this paper, we present an approach to study the behavior of compliant
plates in granular media and optimize the performance of a robot that utilizes
this technique for mobility. From previous work and fundamental tests on thin
plate force generation inside granular media, we introduce an origami-inspired
mechanism with non-linear compliance in the joints that can be used in granular
propulsion. This concept utilizes one-sided joint limits to create an
asymmetric gait cycle that avoids more complicated alternatives often found in
other swimming/digging robots. To analyze its locomotion as well as its shape
and propulsive force, we utilize granular Resistive Force Theory (RFT) as a
starting point. Adding compliance to this theory enables us to predict the
time-based evolution of compliant plates when they are dragged and rotated. It
also permits more rational design of swimming robots where fin design variables
may be optimized against the characteristics of the granular medium. This is
done using a Python-based dynamic simulation library to model the deformation
of the plates and optimize aspects of the robot's gait. Finally, we prototype
and test robot with a gait optimized using the modelling techniques mentioned
above.
</p>
<a href="http://arxiv.org/abs/2101.03624" target="_blank">arXiv:2101.03624</a> [<a href="http://arxiv.org/pdf/2101.03624" target="_blank">pdf</a>]

<h2>Machine Learning Towards Intelligent Systems: Applications, Challenges, and Opportunities. (arXiv:2101.03655v1 [cs.LG])</h2>
<h3>MohammadNoor Injadat, Abdallah Moubayed, Ali Bou Nassif, Abdallah Shami</h3>
<p>The emergence and continued reliance on the Internet and related technologies
has resulted in the generation of large amounts of data that can be made
available for analyses. However, humans do not possess the cognitive
capabilities to understand such large amounts of data. Machine learning (ML)
provides a mechanism for humans to process large amounts of data, gain insights
about the behavior of the data, and make more informed decision based on the
resulting analysis. ML has applications in various fields. This review focuses
on some of the fields and applications such as education, healthcare, network
security, banking and finance, and social media. Within these fields, there are
multiple unique challenges that exist. However, ML can provide solutions to
these challenges, as well as create further research opportunities.
Accordingly, this work surveys some of the challenges facing the aforementioned
fields and presents some of the previous literature works that tackled them.
Moreover, it suggests several research opportunities that benefit from the use
of ML to address these challenges.
</p>
<a href="http://arxiv.org/abs/2101.03655" target="_blank">arXiv:2101.03655</a> [<a href="http://arxiv.org/pdf/2101.03655" target="_blank">pdf</a>]

<h2>Aligning Robot's Behaviours and Users' Perceptions Through Participatory Prototyping. (arXiv:2101.03660v1 [cs.RO])</h2>
<h3>Pamela Carreno-Medrano, Leimin Tian, Aimee Allen, Shanti Sumartojo, Michael Mintrom, Enrique Coronado, Gentiane Venture, Elizabeth Croft, Dana Kulic</h3>
<p>Robots are increasingly being deployed in public spaces. However, the general
population rarely has the opportunity to nominate what they would prefer or
expect a robot to do in these contexts. Since most people have little or no
experience interacting with a robot, it is not surprising that robots deployed
in the real world may fail to gain acceptance or engage their intended users.
To address this issue, we examine users' understanding of robots in public
spaces and their expectations of appropriate uses of robots in these spaces.
Furthermore, we investigate how these perceptions and expectations change as
users engage and interact with a robot. To support this goal, we conducted a
participatory design workshop in which participants were actively involved in
the prototyping and testing of a robot's behaviours in simulation and on the
physical robot. Our work highlights how social and interaction contexts
influence users' perception of robots in public spaces and how users' design
and understanding of what are appropriate robot behaviors shifts as they
observe the enactment of their designs.
</p>
<a href="http://arxiv.org/abs/2101.03660" target="_blank">arXiv:2101.03660</a> [<a href="http://arxiv.org/pdf/2101.03660" target="_blank">pdf</a>]

<h2>Time-Series Regeneration with Convolutional Recurrent Generative Adversarial Network for Remaining Useful Life Estimation. (arXiv:2101.03678v1 [cs.LG])</h2>
<h3>Xuewen Zhang, Yan Qin, Chau Yuen (Fellow IEEE), Lahiru Jayasinghe, Xiang Liu</h3>
<p>For health prognostic task, ever-increasing efforts have been focused on
machine learning-based methods, which are capable of yielding accurate
remaining useful life (RUL) estimation for industrial equipment or components
without exploring the degradation mechanism. A prerequisite ensuring the
success of these methods depends on a wealth of run-to-failure data, however,
run-to-failure data may be insufficient in practice. That is, conducting a
substantial amount of destructive experiments not only is high costs, but also
may cause catastrophic consequences. Out of this consideration, an enhanced RUL
framework focusing on data self-generation is put forward for both non-cyclic
and cyclic degradation patterns for the first time. It is designed to enrich
data from a data-driven way, generating realistic-like time-series to enhance
current RUL methods. First, high-quality data generation is ensured through the
proposed convolutional recurrent generative adversarial network (CR-GAN), which
adopts a two-channel fusion convolutional recurrent neural network. Next, a
hierarchical framework is proposed to combine generated data into current RUL
estimation methods. Finally, the efficacy of the proposed method is verified
through both non-cyclic and cyclic degradation systems. With the enhanced RUL
framework, an aero-engine system following non-cyclic degradation has been
tested using three typical RUL models. State-of-art RUL estimation results are
achieved by enhancing capsule network with generated time-series. Specifically,
estimation errors evaluated by the index score function have been reduced by
21.77%, and 32.67% for the two employed operating conditions, respectively.
Besides, the estimation error is reduced to zero for the Lithium-ion battery
system, which presents cyclic degradation.
</p>
<a href="http://arxiv.org/abs/2101.03678" target="_blank">arXiv:2101.03678</a> [<a href="http://arxiv.org/pdf/2101.03678" target="_blank">pdf</a>]

<h2>MAAS: Multi-modal Assignation for Active Speaker Detection. (arXiv:2101.03682v1 [cs.CV])</h2>
<h3>Juan Le&#xf3;n-Alc&#xe1;zar, Fabian Caba Heilbron, Ali Thabet, Bernard Ghanem</h3>
<p>Active speaker detection requires a solid integration of multi-modal cues.
While individual modalities can approximate a solution, accurate predictions
can only be achieved by explicitly fusing the audio and visual features and
modeling their temporal progression. Despite its inherent muti-modal nature,
current methods still focus on modeling and fusing short-term audiovisual
features for individual speakers, often at frame level. In this paper we
present a novel approach to active speaker detection that directly addresses
the multi-modal nature of the problem, and provides a straightforward strategy
where independent visual features from potential speakers in the scene are
assigned to a previously detected speech event. Our experiments show that, an
small graph data structure built from a single frame, allows to approximate an
instantaneous audio-visual assignment problem. Moreover, the temporal extension
of this initial graph achieves a new state-of-the-art on the AVA-ActiveSpeaker
dataset with a mAP of 88.8\%.
</p>
<a href="http://arxiv.org/abs/2101.03682" target="_blank">arXiv:2101.03682</a> [<a href="http://arxiv.org/pdf/2101.03682" target="_blank">pdf</a>]

<h2>Modeling Household Online Shopping Demand in the U.S.: A Machine Learning Approach and Comparative Investigation between 2009 and 2017. (arXiv:2101.03690v1 [cs.LG])</h2>
<h3>Limon Barua, Bo Zou, Yan (Joann) Zhou, Yulin Liu</h3>
<p>Despite the rapid growth of online shopping and research interest in the
relationship between online and in-store shopping, national-level modeling and
investigation of the demand for online shopping with a prediction focus remain
limited in the literature. This paper differs from prior work and leverages two
recent releases of the U.S. National Household Travel Survey (NHTS) data for
2009 and 2017 to develop machine learning (ML) models, specifically gradient
boosting machine (GBM), for predicting household-level online shopping
purchases. The NHTS data allow for not only conducting nationwide investigation
but also at the level of households, which is more appropriate than at the
individual level given the connected consumption and shopping needs of members
in a household. We follow a systematic procedure for model development
including employing Recursive Feature Elimination algorithm to select input
variables (features) in order to reduce the risk of model overfitting and
increase model explainability. Extensive post-modeling investigation is
conducted in a comparative manner between 2009 and 2017, including quantifying
the importance of each input variable in predicting online shopping demand, and
characterizing value-dependent relationships between demand and the input
variables. In doing so, two latest advances in machine learning techniques,
namely Shapley value-based feature importance and Accumulated Local Effects
plots, are adopted to overcome inherent drawbacks of the popular techniques in
current ML modeling. The modeling and investigation are performed both at the
national level and for three of the largest cities (New York, Los Angeles, and
Houston). The models developed and insights gained can be used for online
shopping-related freight demand generation and may also be considered for
evaluating the potential impact of relevant policies on online shopping demand.
</p>
<a href="http://arxiv.org/abs/2101.03690" target="_blank">arXiv:2101.03690</a> [<a href="http://arxiv.org/pdf/2101.03690" target="_blank">pdf</a>]

<h2>Exploiting a Fleet of UAVs for Monitoring and Data Acquisition of a Distributed Sensor Network. (arXiv:2101.03693v1 [cs.RO])</h2>
<h3>S. MahmoudZadeh, A. Yazdani, A. Elmi, A. Abbasi, P. Ghanooni</h3>
<p>This study proposes an efficient data collection strategy exploiting a team
of Unmanned Aerial Vehicles (UAVs) to monitor and collect the data of a large
distributed sensor network usually used for environmental monitoring,
meteorology, agriculture, and renewable energy applications. The study develops
a collaborative mission planning system that enables a team of UAVs to conduct
and complete the mission of sensors' data collection collaboratively while
considering existing constraints of the UAV payload and battery capacity. The
proposed mission planner system employs the Differential Evolution (DE)
optimization algorithm enabling UAVs to maximize the number of visited sensor
nodes given the priority of the sensors and avoiding the redundant collection
of sensors' data. The proposed mission planner is evaluated through extensive
simulation and comparative analysis. The simulation results confirm the
effectiveness and fidelity of the proposed mission planner to be used for the
distributed sensor network monitoring and data collection.
</p>
<a href="http://arxiv.org/abs/2101.03693" target="_blank">arXiv:2101.03693</a> [<a href="http://arxiv.org/pdf/2101.03693" target="_blank">pdf</a>]

<h2>Learning to Segment Rigid Motions from Two Frames. (arXiv:2101.03694v1 [cs.CV])</h2>
<h3>Gengshan Yang, Deva Ramanan</h3>
<p>Appearance-based detectors achieve remarkable performance on common scenes,
but tend to fail for scenarios lack of training data. Geometric motion
segmentation algorithms, however, generalize to novel scenes, but have yet to
achieve comparable performance to appearance-based ones, due to noisy motion
estimations and degenerate motion configurations. To combine the best of both
worlds, we propose a modular network, whose architecture is motivated by a
geometric analysis of what independent object motions can be recovered from an
egomotion field. It takes two consecutive frames as input and predicts
segmentation masks for the background and multiple rigidly moving objects,
which are then parameterized by 3D rigid transformations. Our method achieves
state-of-the-art performance for rigid motion segmentation on KITTI and Sintel.
The inferred rigid motions lead to a significant improvement for depth and
scene flow estimation. At the time of submission, our method ranked 1st on
KITTI scene flow leaderboard, out-performing the best published method (scene
flow error: 4.89% vs 6.31%).
</p>
<a href="http://arxiv.org/abs/2101.03694" target="_blank">arXiv:2101.03694</a> [<a href="http://arxiv.org/pdf/2101.03694" target="_blank">pdf</a>]

<h2>A Cooperative Dynamic Task Assignment Framework for COTSBot AUVs. (arXiv:2101.03696v1 [cs.RO])</h2>
<h3>Amin Abbasi, Somaiyeh MahmoudZadeh, Amirmehdi Yazdani</h3>
<p>This paper presents a cooperative dynamic task assignment framework for a
certain class of Autonomous Underwater Vehicles (AUVs) employed to control
outbreak of Crown-Of-Thorns Starfish (COTS) in Australia's Great Barrier Reef.
The problem of monitoring and controlling the COTS is transcribed into a
constrained task assignment problem in which eradicating clusters of COTS, by
the injection system of COTSbot AUVs, is considered as a task. A probabilistic
map of the operating environment including seabed terrain, clusters of COTS,
and coastlines is constructed. Then, a novel heuristic algorithm called
Heuristic Fleet Cooperation (HFC) is developed to provide a cooperative
injection of the COTSbot AUVs to the maximum possible COTS in an assigned
mission time. Extensive simulation studies together with quantitative
performance analysis are conducted to demonstrate the effectiveness and
robustness of the proposed cooperative task assignment algorithm in eradicating
the COTS in the Great Barrier Reef.
</p>
<a href="http://arxiv.org/abs/2101.03696" target="_blank">arXiv:2101.03696</a> [<a href="http://arxiv.org/pdf/2101.03696" target="_blank">pdf</a>]

<h2>RepVGG: Making VGG-style ConvNets Great Again. (arXiv:2101.03697v1 [cs.CV])</h2>
<h3>Xiaohan Ding, Xiangyu Zhang, Ningning Ma, Jungong Han, Guiguang Ding, Jian Sun</h3>
<p>We present a simple but powerful architecture of convolutional neural
network, which has a VGG-like inference-time body composed of nothing but a
stack of 3x3 convolution and ReLU, while the training-time model has a
multi-branch topology. Such decoupling of the training-time and inference-time
architecture is realized by a structural re-parameterization technique so that
the model is named RepVGG. On ImageNet, RepVGG reaches over 80\% top-1
accuracy, which is the first time for a plain model, to the best of our
knowledge. On NVIDIA 1080Ti GPU, RepVGG models run 83% faster than ResNet-50 or
101% faster than ResNet-101 with higher accuracy and show favorable
accuracy-speed trade-off compared to the state-of-the-art models like
EfficientNet and RegNet. The code and trained models are available at
https://github.com/megvii-model/RepVGG.
</p>
<a href="http://arxiv.org/abs/2101.03697" target="_blank">arXiv:2101.03697</a> [<a href="http://arxiv.org/pdf/2101.03697" target="_blank">pdf</a>]

<h2>Condition Assessment of Stay Cables through Enhanced Time Series Classification Using a Deep Learning Approach. (arXiv:2101.03701v1 [cs.LG])</h2>
<h3>Zhiming Zhang, Jin Yan, Liangding Li, Hong Pan, Chuanzhi Dong</h3>
<p>This study proposes a data-driven method that detects cable damage from
measured cable forces by recognizing biased patterns from the intact
conditions. The proposed method solves the pattern recognition problem for
cable damage detection through time series classification (TSC) in deep
learning, considering that the cable's behavior can be implicitly represented
by the measured cable force series. A deep learning model, long short term
memory fully convolutional network (LSTM-FCN), is leveraged by assigning
appropriate inputs and representative class labels for the TSC problem, First,
a TSC classifier is trained and validated using the data collected under intact
conditions of stay cables, setting the segmented data series as input and the
cable (or cable pair) ID as class labels. Subsequently, the classifier is
tested using the data collected under possible damaged conditions. Finally, the
cable or cable pair corresponding to the least classification accuracy is
recommended as the most probable damaged cable or cable pair. The proposed
method was tested on an in-service cable-stayed bridge with damaged stay
cables. Two scenarios in the proposed TSC scheme were investigated: 1) raw time
series of cable forces were fed into the classifiers; and 2) cable force ratios
were inputted in the classifiers considering the possible variation of force
distribution between cable pairs due to cable damage. Combining the results of
TSC testing in these two scenarios, the cable with rupture was correctly
identified. This study proposes a data-driven methodology for cable damage
detection that requires the least data preprocessing and feature engineering,
which enables fast and convenient early detection in real applications.
</p>
<a href="http://arxiv.org/abs/2101.03701" target="_blank">arXiv:2101.03701</a> [<a href="http://arxiv.org/pdf/2101.03701" target="_blank">pdf</a>]

<h2>A Transfer Learning-based State of Charge Estimation for Lithium-Ion Battery at Varying Ambient Temperatures. (arXiv:2101.03704v1 [cs.LG])</h2>
<h3>Yan Qin, Stefan Adams, Chau Yuen</h3>
<p>Accurate and reliable state of charge (SoC) estimation becomes increasingly
important to provide a stable and efficient environment for Lithium-ion
batteries (LiBs) powered devices. Most data-driven SoC models are built for a
fixed ambient temperature, which neglect the high sensitivity of LiBs to
temperature and may cause severe prediction errors. Nevertheless, a systematic
evaluation of the impact of temperature on SoC estimation and ways for a prompt
adjustment of the estimation model to new temperatures using limited data have
been hardly discussed. To solve these challenges, a novel SoC estimation method
is proposed by exploiting temporal dynamics of measurements and transferring
consistent estimation ability among different temperatures. First, temporal
dynamics, which is presented by correlations between the past fluctuation and
the future motion, is extracted using canonical variate analysis. Next, two
models, including a reference SoC estimation model and an estimation ability
monitoring model, are developed with temporal dynamics. The monitoring model
provides a path to quantitatively evaluate the influences of temperature on SoC
estimation ability. After that, once the inability of the reference SoC
estimation model is detected, consistent temporal dynamics between temperatures
are selected for transfer learning. Finally, the efficacy of the proposed
method is verified through a benchmark. Our proposed method not only reduces
prediction errors at fixed temperatures (e.g., reduced by 24.35% at -20{\deg}C,
49.82% at 25{\deg}C) but also improves prediction accuracies at new
temperatures.
</p>
<a href="http://arxiv.org/abs/2101.03704" target="_blank">arXiv:2101.03704</a> [<a href="http://arxiv.org/pdf/2101.03704" target="_blank">pdf</a>]

<h2>FedAR: Activity and Resource-Aware Federated Learning Model for Distributed Mobile Robots. (arXiv:2101.03705v1 [cs.LG])</h2>
<h3>Ahmed Imteaj, M. Hadi Amini</h3>
<p>Smartphones, autonomous vehicles, and the Internet-of-things (IoT) devices
are considered the primary data source for a distributed network. Due to a
revolutionary breakthrough in internet availability and continuous improvement
of the IoT devices capabilities, it is desirable to store data locally and
perform computation at the edge, as opposed to share all local information with
a centralized computation agent. A recently proposed Machine Learning (ML)
algorithm called Federated Learning (FL) paves the path towards preserving data
privacy, performing distributed learning, and reducing communication overhead
in large-scale machine learning (ML) problems. This paper proposes an FL model
by monitoring client activities and leveraging available local computing
resources, particularly for resource-constrained IoT devices (e.g., mobile
robots), to accelerate the learning process. We assign a trust score to each FL
client, which is updated based on the client's activities. We consider a
distributed mobile robot as an FL client with resource limitations either in
memory, bandwidth, processor, or battery life. We consider such mobile robots
as FL clients to understand their resource-constrained behavior in a real-world
setting. We consider an FL client to be untrustworthy if the client infuses
incorrect models or repeatedly gives slow responses during the FL process.
After disregarding the ineffective and unreliable client, we perform local
training on the selected FL clients. To further reduce the straggler issue, we
enable an asynchronous FL mechanism by performing aggregation on the FL server
without waiting for a long period to receive a particular client's response.
</p>
<a href="http://arxiv.org/abs/2101.03705" target="_blank">arXiv:2101.03705</a> [<a href="http://arxiv.org/pdf/2101.03705" target="_blank">pdf</a>]

<h2>Preconditioned training of normalizing flows for variational inference in inverse problems. (arXiv:2101.03709v1 [stat.ML])</h2>
<h3>Ali Siahkoohi, Gabrio Rizzuti, Mathias Louboutin, Philipp A. Witte, Felix J. Herrmann</h3>
<p>Obtaining samples from the posterior distribution of inverse problems with
expensive forward operators is challenging especially when the unknowns involve
the strongly heterogeneous Earth. To meet these challenges, we propose a
preconditioning scheme involving a conditional normalizing flow (NF) capable of
sampling from a low-fidelity posterior distribution directly. This conditional
NF is used to speed up the training of the high-fidelity objective involving
minimization of the Kullback-Leibler divergence between the predicted and the
desired high-fidelity posterior density for indirect measurements at hand. To
minimize costs associated with the forward operator, we initialize the
high-fidelity NF with the weights of the pretrained low-fidelity NF, which is
trained beforehand on available model and data pairs. Our numerical
experiments, including a 2D toy and a seismic compressed sensing example,
demonstrate that thanks to the preconditioning considerable speed-ups are
achievable compared to training NFs from scratch.
</p>
<a href="http://arxiv.org/abs/2101.03709" target="_blank">arXiv:2101.03709</a> [<a href="http://arxiv.org/pdf/2101.03709" target="_blank">pdf</a>]

<h2>ArrowGAN : Learning to Generate Videos by Learning Arrow of Time. (arXiv:2101.03710v1 [cs.CV])</h2>
<h3>Kibeom Hong, Youngjung Uh, Hyeran Byun</h3>
<p>Training GANs on videos is even more sophisticated than on images because
videos have a distinguished dimension: time. While recent methods designed a
dedicated architecture considering time, generated videos are still far from
indistinguishable from real videos. In this paper, we introduce ArrowGAN
framework, where the discriminators learns to classify arrow of time as an
auxiliary task and the generators tries to synthesize forward-running videos.
We argue that the auxiliary task should be carefully chosen regarding the
target domain. In addition, we explore categorical ArrowGAN with recent
techniques in conditional image generation upon ArrowGAN framework, achieving
the state-of-the-art performance on categorical video generation. Our extensive
experiments validate the effectiveness of arrow of time as a self-supervisory
task, and demonstrate that all our components of categorical ArrowGAN lead to
the improvement regarding video inception score and Frechet video distance on
three datasets: Weizmann, UCFsports, and UCF-101.
</p>
<a href="http://arxiv.org/abs/2101.03710" target="_blank">arXiv:2101.03710</a> [<a href="http://arxiv.org/pdf/2101.03710" target="_blank">pdf</a>]

<h2>Learning from Weakly-labeled Web Videos via Exploring Sub-Concepts. (arXiv:2101.03713v1 [cs.CV])</h2>
<h3>Kunpeng Li, Zizhao Zhang, Guanhang Wu, Xuehan Xiong, Chen-Yu Lee, Zhichao Lu, Yun Fu, Tomas Pfister</h3>
<p>Learning visual knowledge from massive weakly-labeled web videos has
attracted growing research interests thanks to the large corpus of easily
accessible video data on the Internet. However, for video action recognition,
the action of interest might only exist in arbitrary clips of untrimmed web
videos, resulting in high label noises in the temporal space. To address this
issue, we introduce a new method for pre-training video action recognition
models using queried web videos. Instead of trying to filter out, we propose to
convert the potential noises in these queried videos to useful supervision
signals by defining the concept of Sub-Pseudo Label (SPL). Specifically, SPL
spans out a new set of meaningful "middle ground" label space constructed by
extrapolating the original weak labels during video querying and the prior
knowledge distilled from a teacher model. Consequently, SPL provides enriched
supervision for video models to learn better representations. SPL is fairly
simple and orthogonal to popular teacher-student self-training frameworks
without extra training cost. We validate the effectiveness of our method on
four video action recognition datasets and a weakly-labeled image dataset to
study the generalization ability. Experiments show that SPL outperforms several
existing pre-training strategies using pseudo-labels and the learned
representations lead to competitive results when fine-tuning on HMDB-51 and
UCF-101 compared with recent pre-training methods.
</p>
<a href="http://arxiv.org/abs/2101.03713" target="_blank">arXiv:2101.03713</a> [<a href="http://arxiv.org/pdf/2101.03713" target="_blank">pdf</a>]

<h2>Wheelchair Behavior Recognition for Visualizing Sidewalk Accessibility by Deep Neural Networks. (arXiv:2101.03724v1 [cs.LG])</h2>
<h3>Takumi Watanabe, Hiroki Takahashi, Goh Sato, Yusuke Iwasawa, Yutaka Matsuo, Ikuko Eguchi Yairi</h3>
<p>This paper introduces our methodology to estimate sidewalk accessibilities
from wheelchair behavior via a triaxial accelerometer in a smartphone installed
under a wheelchair seat. Our method recognizes sidewalk accessibilities from
environmental factors, e.g. gradient, curbs, and gaps, which influence
wheelchair bodies and become a burden for people with mobility difficulties.
This paper developed and evaluated a prototype system that visualizes sidewalk
accessibility information by extracting knowledge from wheelchair acceleration
using deep neural networks. Firstly, we created a supervised convolutional
neural network model to classify road surface conditions using wheelchair
acceleration data. Secondly, we applied a weakly supervised method to extract
representations of road surface conditions without manual annotations. Finally,
we developed a self-supervised variational autoencoder to assess sidewalk
barriers for wheelchair users. The results show that the proposed method
estimates sidewalk accessibilities from wheelchair accelerations and extracts
knowledge of accessibilities by weakly supervised and self-supervised
approaches.
</p>
<a href="http://arxiv.org/abs/2101.03724" target="_blank">arXiv:2101.03724</a> [<a href="http://arxiv.org/pdf/2101.03724" target="_blank">pdf</a>]

<h2>Reinforcement Learning under Model Risk for Biomanufacturing Fermentation Control. (arXiv:2101.03735v1 [stat.ML])</h2>
<h3>Bo Wang, Wei Xie, Tugce Martagan, Alp Akcay</h3>
<p>In the biopharmaceutical manufacturing, fermentation process plays a critical
role impacting on productivity and profit. Since biotherapeutics are
manufactured in living cells whose biological mechanisms are complex and have
highly variable outputs, in this paper, we introduce a model-based
reinforcement learning framework accounting for model risk to support
bioprocess online learning and guide the optimal and robust customized stopping
policy for fermentation process. Specifically, built on the dynamic mechanisms
of protein and impurity generation, we first construct a probabilistic model
characterizing the impact of underlying bioprocess stochastic uncertainty on
impurity and protein growth rates. Since biopharmaceutical manufacturing often
has very limited data during the development and early stage of production, we
derive the posterior distribution quantifying the process model risk, and
further develop the Bayesian rule based knowledge update to support the online
learning on underlying stochastic process. With the prediction risk accounting
for both bioprocess stochastic uncertainty and model risk, the proposed
reinforcement learning framework can proactively hedge all sources of
uncertainties and support the optimal and robust customized decision making. We
conduct the structural analysis of optimal policy and study the impact of model
risk on the policy selection. We can show that it asymptotically converges to
the optimal policy obtained under perfect information of underlying stochastic
process. Our case studies demonstrate that the proposed framework can greatly
improve the biomanufacturing industrial practice.
</p>
<a href="http://arxiv.org/abs/2101.03735" target="_blank">arXiv:2101.03735</a> [<a href="http://arxiv.org/pdf/2101.03735" target="_blank">pdf</a>]

<h2>Hierarchical Clustering using Auto-encoded Compact Representation for Time-series Analysis. (arXiv:2101.03742v1 [cs.LG])</h2>
<h3>Soma Bandyopadhyay, Anish Datta, Arpan Pal (TCS Research, TATA Consultancy Services, Kolkata, India)</h3>
<p>Getting a robust time-series clustering with best choice of distance measure
and appropriate representation is always a challenge. We propose a novel
mechanism to identify the clusters combining learned compact representation of
time-series, Auto Encoded Compact Sequence (AECS) and hierarchical clustering
approach. Proposed algorithm aims to address the large computing time issue of
hierarchical clustering as learned latent representation AECS has a length much
less than the original length of time-series and at the same time want to
enhance its performance.Our algorithm exploits Recurrent Neural Network (RNN)
based under complete Sequence to Sequence(seq2seq) autoencoder and
agglomerative hierarchical clustering with a choice of best distance measure to
recommend the best clustering. Our scheme selects the best distance measure and
corresponding clustering for both univariate and multivariate time-series. We
have experimented with real-world time-series from UCR and UCI archive taken
from diverse application domains like health, smart-city, manufacturing etc.
Experimental results show that proposed method not only produce close to
benchmark results but also in some cases outperform the benchmark.
</p>
<a href="http://arxiv.org/abs/2101.03742" target="_blank">arXiv:2101.03742</a> [<a href="http://arxiv.org/pdf/2101.03742" target="_blank">pdf</a>]

<h2>Cognitive Visual Inspection Service for LCD Manufacturing Industry. (arXiv:2101.03747v1 [cs.CV])</h2>
<h3>Yuanyuan Ding, Junchi Yan, Guoqiang Hu, Jun Zhu</h3>
<p>With the rapid growth of display devices, quality inspection via machine
vision technology has become increasingly important for flat-panel displays
(FPD) industry. This paper discloses a novel visual inspection system for
liquid crystal display (LCD), which is currently a dominant type in the FPD
industry. The system is based on two cornerstones: robust/high-performance
defect recognition model and cognitive visual inspection service architecture.
A hybrid application of conventional computer vision technique and the latest
deep convolutional neural network (DCNN) leads to an integrated defect
detection, classfication and impact evaluation model that can be economically
trained with only image-level class annotations to achieve a high inspection
accuracy. In addition, the properly trained model is robust to the variation of
the image qulity, significantly alleviating the dependency between the model
prediction performance and the image aquisition environment. This in turn
justifies the decoupling of the defect recognition functions from the front-end
device to the back-end serivce, motivating the design and realization of the
cognitive visual inspection service architecture. Empirical case study is
performed on a large-scale real-world LCD dataset from a manufacturing line
with different layers and products, which shows the promising utility of our
system, which has been deployed in a real-world LCD manufacturing line from a
major player in the world.
</p>
<a href="http://arxiv.org/abs/2101.03747" target="_blank">arXiv:2101.03747</a> [<a href="http://arxiv.org/pdf/2101.03747" target="_blank">pdf</a>]

<h2>Deep Learning-based Face Super-resolution: A Survey. (arXiv:2101.03749v1 [cs.CV])</h2>
<h3>Junjun Jiang, Chenyang Wang, Xianming Liu, Jiayi Ma</h3>
<p>Face super-resolution, also known as face hallucination, which is aimed at
enhancing the resolution of low-resolution (LR) one or a sequence of face
images to generate the corresponding high-resolution (HR) face images, is a
domain-specific image super-resolution problem. Recently, face super-resolution
has received considerable attention, and witnessed dazzling advances with deep
learning techniques. To date, few summaries of the studies on the deep
learning-based face super-resolution are available. In this survey, we present
a comprehensive review of deep learning techniques in face super-resolution in
a systematic manner. First, we summarize the problem formulation of face
super-resolution. Second, we compare the differences between generic image
super-resolution and face super-resolution. Third, datasets and performance
metrics commonly used in facial hallucination are presented. Fourth, we roughly
categorize existing methods according to the utilization of face-specific
information. In each category, we start with a general description of design
principles, present an overview of representative approaches, and compare the
similarities and differences among various methods. Finally, we envision
prospects for further technical advancement in this field.
</p>
<a href="http://arxiv.org/abs/2101.03749" target="_blank">arXiv:2101.03749</a> [<a href="http://arxiv.org/pdf/2101.03749" target="_blank">pdf</a>]

<h2>Investigating the Vision Transformer Model for Image Retrieval Tasks. (arXiv:2101.03771v1 [cs.CV])</h2>
<h3>Socratis Gkelios, Yiannis Boutalis, Savvas A. Chatzichristofis</h3>
<p>This paper introduces a plug-and-play descriptor that can be effectively
adopted for image retrieval tasks without prior initialization or preparation.
The description method utilizes the recently proposed Vision Transformer
network while it does not require any training data to adjust parameters. In
image retrieval tasks, the use of Handcrafted global and local descriptors has
been very successfully replaced, over the last years, by the Convolutional
Neural Networks (CNN)-based methods. However, the experimental evaluation
conducted in this paper on several benchmarking datasets against 36
state-of-the-art descriptors from the literature demonstrates that a neural
network that contains no convolutional layer, such as Vision Transformer, can
shape a global descriptor and achieve competitive results. As fine-tuning is
not required, the presented methodology's low complexity encourages adoption of
the architecture as an image retrieval baseline model, replacing the
traditional and well adopted CNN-based approaches and inaugurating a new era in
image retrieval approaches.
</p>
<a href="http://arxiv.org/abs/2101.03771" target="_blank">arXiv:2101.03771</a> [<a href="http://arxiv.org/pdf/2101.03771" target="_blank">pdf</a>]

<h2>Deep Adversarial Inconsistent Cognitive Sampling for Multi-view Progressive Subspace Clustering. (arXiv:2101.03783v1 [cs.CV])</h2>
<h3>Renhao Sun, Yang Wang, Zhao Zhang, Richang Hong, Meng Wang (Fellow, IEEE)</h3>
<p>Deep multi-view clustering methods have achieved remarkable performance.
However, all of them failed to consider the difficulty labels (uncertainty of
ground-truth for training samples) over multi-view samples, which may result
into a nonideal clustering network for getting stuck into poor local optima
during training process; worse still, the difficulty labels from multi-view
samples are always inconsistent, such fact makes it even more challenging to
handle. In this paper, we propose a novel Deep Adversarial Inconsistent
Cognitive Sampling (DAICS) method for multi-view progressive subspace
clustering. A multiview binary classification (easy or difficult) loss and a
feature similarity loss are proposed to jointly learn a binary classifier and a
deep consistent feature embedding network, throughout an adversarial minimax
game over difficulty labels of multiview consistent samples. We develop a
multi-view cognitive sampling strategy to select the input samples from easy to
difficult for multi-view clustering network training. However, the
distributions of easy and difficult samples are mixed together, hence not
trivial to achieve the goal. To resolve it, we define a sampling probability
with theoretical guarantee. Based on that, a golden section mechanism is
further designed to generate a sample set boundary to progressively select the
samples with varied difficulty labels via a gate unit, which is utilized to
jointly learn a multi-view common progressive subspace and clustering network
for more efficient clustering. Experimental results on four real-world datasets
demonstrate the superiority of DAICS over the state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2101.03783" target="_blank">arXiv:2101.03783</a> [<a href="http://arxiv.org/pdf/2101.03783" target="_blank">pdf</a>]

<h2>Predictive Analysis of Chikungunya. (arXiv:2101.03785v1 [cs.LG])</h2>
<h3>Sayed Erfan Arefin, Tasnia Ashrafi Heya, Dr Moinul Zaber</h3>
<p>Chikungunya is an emerging threat for health security all over the world
which is spreading very fast. Researches for proper forecasting of the
incidence rate of chikungunya has been going on in many places in which DARPA
has done a very extensive summarized result from 2014 to 2017 with the data of
suspected cases, confirmed cases, deaths, population and incidence rate in
different countries. In this project, we have analysed the dataset from DARPA
and extended it to predict the incidence rate using different features of
weather like temperature, humidity, dewiness, wind and pressure along with the
latitude and longitude of every country. We had to use different APIs to find
out these extra features from 2014-2016. After creating a pure dataset, we have
used Linear Regression to predict the incidence rate and calculated the
accuracy and error rate.
</p>
<a href="http://arxiv.org/abs/2101.03785" target="_blank">arXiv:2101.03785</a> [<a href="http://arxiv.org/pdf/2101.03785" target="_blank">pdf</a>]

<h2>WiCV 2020: The Seventh Women In Computer Vision Workshop. (arXiv:2101.03787v1 [cs.CV])</h2>
<h3>Hazel Doughty, Nour Karessli, Kathryn Leonard, Boyi Li, Carianne Martinez, Azadeh Mobasher, Arsha Nagrani, Srishti Yadav</h3>
<p>In this paper we present the details of Women in Computer Vision Workshop -
WiCV 2020, organized in alongside virtual CVPR 2020. This event aims at
encouraging the women researchers in the field of computer vision. It provides
a voice to a minority (female) group in computer vision community and focuses
on increasingly the visibility of these researchers, both in academia and
industry. WiCV believes that such an event can play an important role in
lowering the gender imbalance in the field of computer vision. WiCV is
organized each year where it provides a.) opportunity for collaboration with
between researchers b.) mentorship to female junior researchers c.) financial
support to presenters to overcome monetary burden and d.) large and diverse
choice of role models, who can serve as examples to younger researchers at the
beginning of their careers. In this paper, we present a report on the workshop
program, trends over the past years, a summary of statistics regarding
presenters, attendees, and sponsorship for the current workshop.
</p>
<a href="http://arxiv.org/abs/2101.03787" target="_blank">arXiv:2101.03787</a> [<a href="http://arxiv.org/pdf/2101.03787" target="_blank">pdf</a>]

<h2>Second Hand Price Prediction for Tesla Vehicles. (arXiv:2101.03788v1 [cs.LG])</h2>
<h3>Sayed Erfan Arefin</h3>
<p>The Tesla vehicles became very popular in the car industry as it was
affordable in the consumer market and it left no carbon footprint. Due to the
large decline in the stock prices of Tesla Inc. at the beginning of 2019, Tesla
owners started selling their vehicles in the used car market. These used car
prices depended on attributes such as the model of the vehicle, year of
production, miles driven, and the battery used for the vehicle. Prices were
different for a specific vehicle in different months. In this paper, it is
discussed how a machine learning technique is being implemented in order to
develop a second-hand Teslavehicle price prediction system. To reach this goal,
different machine learning techniques such as decision trees, support vector
machine (SVM), random forest, and deep learning were investigated and finally
was implemented with boosted decision tree regression. I the future, it is
intended to use a more sophisticated algorithm for better accuracy.
</p>
<a href="http://arxiv.org/abs/2101.03788" target="_blank">arXiv:2101.03788</a> [<a href="http://arxiv.org/pdf/2101.03788" target="_blank">pdf</a>]

<h2>The Gaze and Mouse Signal as additional Source for User Fingerprints in Browser Applications. (arXiv:2101.03793v1 [cs.CV])</h2>
<h3>Wolfgang Fuhl, Nikolai Sanamrad, Enkelejda Kasneci</h3>
<p>In this work we inspect different data sources for browser fingerprints. We
show which disadvantages and limitations browser statistics have and how this
can be avoided with other data sources. Since human visual behavior is a rich
source of information and also contains person specific information, it is a
valuable source for browser fingerprints. However, human gaze acquisition in
the browser also has disadvantages, such as inaccuracies via webcam and the
restriction that the user must first allow access to the camera. However, it is
also known that the mouse movements and the human gaze correlate and therefore,
the mouse movements can be used instead of the gaze signal. In our evaluation
we show the influence of all possible combinations of the three information
sources for user recognition and describe our simple approach in detail. The
data and the Matlab code can be downloaded here
https://atreus.informatik.uni-tuebingen.de/seafile/d/8e2ab8c3fdd444e1a135/?p=%2FThe%20Gaze%20and%20Mouse%20Signal%20as%20additional%20Source%20...&amp;mode=list
</p>
<a href="http://arxiv.org/abs/2101.03793" target="_blank">arXiv:2101.03793</a> [<a href="http://arxiv.org/pdf/2101.03793" target="_blank">pdf</a>]

<h2>Coronary Plaque Analysis for CT Angiography Clinical Research. (arXiv:2101.03799v1 [cs.CV])</h2>
<h3>Felix Denzinger, Michael Wels, Christian Hopfgartner, Jing Lu, Max Sch&#xf6;binger, Andreas Maier, Michael S&#xfc;hling</h3>
<p>The analysis of plaque deposits in the coronary vasculature is an important
topic in current clinical research. From a technical side mostly new algorithms
for different sub tasks - e.g. centerline extraction or vessel/plaque
segmentation - are proposed. However, to enable clinical research with the help
of these algorithms, a software solution, which enables manual correction,
comprehensive visual feedback and tissue analysis capabilities, is needed.
Therefore, we want to present such an integrated software solution. It is able
to perform robust automatic centerline extraction and inner and outer vessel
wall segmentation, while providing easy to use manual correction tools. Also,
it allows for annotation of lesions along the centerlines, which can be further
analyzed regarding their tissue composition. Furthermore, it enables research
in upcoming technologies and research directions: it does support dual energy
CT scans with dedicated plaque analysis and the quantification of the fatty
tissue surrounding the vasculature, also in automated set-ups.
</p>
<a href="http://arxiv.org/abs/2101.03799" target="_blank">arXiv:2101.03799</a> [<a href="http://arxiv.org/pdf/2101.03799" target="_blank">pdf</a>]

<h2>Multi-objective Conflict-based Search for Multi-agent Path Finding. (arXiv:2101.03805v1 [cs.AI])</h2>
<h3>Zhongqiang Ren, Sivakumar Rathinam, Howie Choset</h3>
<p>Conventional multi-agent path planners typically compute an ensemble of paths
while optimizing a single objective, such as path length. However, many
applications may require multiple objectives, say fuel consumption and
completion time, to be simultaneously optimized during planning and these
criteria may not be readily compared and sometimes lie in competition with each
other. Naively applying existing multi-objective search algorithms to
multi-agent path finding may prove to be inefficient as the size of the space
of possible solutions, i.e., the Pareto-optimal set, can grow exponentially
with the number of agents (the dimension of the search space). This article
presents an approach named Multi-objective Conflict-based Search (MO-CBS) that
bypasses this so-called curse of dimensionality by leveraging prior
Conflict-based Search (CBS), a well-known algorithm for single-objective
multi-agent path finding, and principles of dominance from multi-objective
optimization literature. We prove that MO-CBS is able to compute the entire
Pareto-optimal set. Our results show that MO-CBS can solve problem instances
with hundreds of Pareto-optimal solutions which the standard multi-objective A*
algorithms could not find within a bounded time.
</p>
<a href="http://arxiv.org/abs/2101.03805" target="_blank">arXiv:2101.03805</a> [<a href="http://arxiv.org/pdf/2101.03805" target="_blank">pdf</a>]

<h2>Multi-Domain Image-to-Image Translation with Adaptive Inference Graph. (arXiv:2101.03806v1 [cs.CV])</h2>
<h3>The-Phuc Nguyen, St&#xe9;phane Lathuili&#xe8;re, Elisa Ricci</h3>
<p>In this work, we address the problem of multi-domain image-to-image
translation with particular attention paid to computational cost. In
particular, current state of the art models require a large and deep model in
order to handle the visual diversity of multiple domains. In a context of
limited computational resources, increasing the network size may not be
possible. Therefore, we propose to increase the network capacity by using an
adaptive graph structure. At inference time, the network estimates its own
graph by selecting specific sub-networks. Sub-network selection is implemented
using Gumbel-Softmax in order to allow end-to-end training. This approach leads
to an adjustable increase in number of parameters while preserving an almost
constant computational cost. Our evaluation on two publicly available datasets
of facial and painting images shows that our adaptive strategy generates better
images with fewer artifacts than literature methods
</p>
<a href="http://arxiv.org/abs/2101.03806" target="_blank">arXiv:2101.03806</a> [<a href="http://arxiv.org/pdf/2101.03806" target="_blank">pdf</a>]

<h2>WDR FACE: The First Database for Studying Face Detection in Wide Dynamic Range. (arXiv:2101.03826v1 [cs.CV])</h2>
<h3>Ziyi Liu, Jie Yang, Mengchen Lin, Kenneth Kam Fai Lai, Svetlana Yanushkevich, Orly Yadid-Pecht</h3>
<p>Currently, face detection approaches focus on facial information by varying
specific parameters including pose, occlusion, lighting, background, race, and
gender. These studies only utilized the information obtained from low dynamic
range images, however, face detection in wide dynamic range (WDR) scenes has
received little attention. To our knowledge, there is no publicly available WDR
database for face detection research. To facilitate and support future face
detection research in the WDR field, we propose the first WDR database for face
detection, called WDR FACE, which contains a total of 398 16-bit megapixel
grayscale wide dynamic range images collected from 29 subjects. These WDR
images (WDRIs) were taken in eight specific WDR scenes. The dynamic range of
90% images surpasses 60,000:1, and that of 70% images exceeds 65,000:1.
Furthermore, we show the effect of different face detection procedures on the
WDRIs in our database. This is done with 25 different tone mapping operators
and five different face detectors. We provide preliminary experimental results
of face detection on this unique WDR database.
</p>
<a href="http://arxiv.org/abs/2101.03826" target="_blank">arXiv:2101.03826</a> [<a href="http://arxiv.org/pdf/2101.03826" target="_blank">pdf</a>]

<h2>Closing the Planning-Learning Loop with Application to Autonomous Driving in a Crowd. (arXiv:2101.03834v1 [cs.RO])</h2>
<h3>Panpan Cai, David Hsu</h3>
<p>Imagine an autonomous robot vehicle driving in dense, possibly unregulated
urban traffic. To contend with an uncertain, interactive environment with many
traffic participants, the robot vehicle has to perform long-term planning in
order to drive effectively and approach human-level performance. Planning
explicitly over a long time horizon, however, incurs prohibitive computational
cost and is impractical under real-time constraints. To achieve real-time
performance for large-scale planning, this paper introduces Learning from Tree
Search for Driving (LeTS-Drive), which integrates planning and learning in a
close loop. LeTS-Drive learns a driving policy from a planner based on
sparsely-sampled tree search. It then guides online planning using this learned
policy for real-time vehicle control. These two steps are repeated to form a
close loop so that the planner and the learner inform each other and both
improve in synchrony. The entire algorithm evolves on its own in a
self-supervised manner, without explicit human efforts on data labeling. We
applied LeTS-Drive to autonomous driving in crowded urban environments in
simulation. Experimental results clearly show that LeTS-Drive outperforms
either planning or learning alone, as well as open-loop integration of planning
and learning.
</p>
<a href="http://arxiv.org/abs/2101.03834" target="_blank">arXiv:2101.03834</a> [<a href="http://arxiv.org/pdf/2101.03834" target="_blank">pdf</a>]

<h2>The Semantic Adjacency Criterion in Time Intervals Mining. (arXiv:2101.03842v1 [cs.LG])</h2>
<h3>Alexander Shknevsky, Yuval Shahar, Robert Moskovitch</h3>
<p>Frequent temporal patterns discovered in time-interval-based multivariate
data, although syntactically correct, might be non-transparent: For some
pattern instances, there might exist intervals for the same entity that
contradict the pattern's usual meaning. We conjecture that non-transparent
patterns are also less useful as classification or prediction features. We
propose a new pruning constraint during a frequent temporal-pattern discovery
process, the Semantic Adjacency Criterion [SAC], which exploits domain
knowledge to filter out patterns that contain potentially semantically
contradictory components. We have defined three SAC versions, and tested their
effect in three medical domains. We embedded these criteria in a
frequent-temporal-pattern discovery framework. Previously, we had informally
presented the SAC principle and showed that using it to prune patterns enhances
the repeatability of their discovery in the same clinical domain. Here, we
define formally the semantics of three SAC variations, and compare the use of
the set of pruned patterns to the use of the complete set of discovered
patterns, as features for classification and prediction tasks in three
different medical domains. We induced four classifiers for each task, using
four machine-learning methods: Random Forests, Naive Bayes, SVM, and Logistic
Regression. The features were frequent temporal patterns discovered in each
data set. SAC-based temporal pattern-discovery reduced by up to 97% the number
of discovered patterns and by up to 98% the discovery runtime. But the
classification and prediction performance of the reduced SAC-based
pattern-based features set, was as good as when using the complete set. Using
SAC can significantly reduce the number of discovered frequent interval-based
temporal patterns, and the corresponding computational effort, without losing
classification or prediction performance.
</p>
<a href="http://arxiv.org/abs/2101.03842" target="_blank">arXiv:2101.03842</a> [<a href="http://arxiv.org/pdf/2101.03842" target="_blank">pdf</a>]

<h2>Spherical Transformer: Adapting Spherical Signal to ConvolutionalNetworks. (arXiv:2101.03848v1 [cs.CV])</h2>
<h3>Haikuan Du, Hui Cao, Shen Cai, Junchi Yan, Siyu Zhang</h3>
<p>Convolutional neural networks (CNNs) have been widely used in various vision
tasks, e.g. image classification, semantic segmentation, etc. Unfortunately,
standard 2D CNNs are not well suited for spherical signals such as panorama
images or spherical projections, as the sphere is an unstructured grid. In this
paper, we present Spherical Transformer which can transform spherical signals
into vectors that can be directly processed by standard CNNs such that many
well-designed CNNs architectures can be reused across tasks and datasets by
pretraining. To this end, the proposed method first uses locally structured
sampling methods such as HEALPix to construct a transformer grid by using the
information of spherical points and its adjacent points, and then transforms
the spherical signals to the vectors through the grid. By building the
Spherical Transformer module, we can use multiple CNN architectures directly.
We evaluate our approach on the tasks of spherical MNIST recognition, 3D object
classification and omnidirectional image semantic segmentation. For 3D object
classification, we further propose a rendering-based projection method to
improve the performance and a rotational-equivariant model to improve the
anti-rotation ability. Experimental results on three tasks show that our
approach achieves superior performance over state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2101.03848" target="_blank">arXiv:2101.03848</a> [<a href="http://arxiv.org/pdf/2101.03848" target="_blank">pdf</a>]

<h2>Deep Neural Networks to Recover Unknown Physical Parameters from Oscillating Time Series. (arXiv:2101.03850v1 [cs.LG])</h2>
<h3>Antoine Garcon, Julian Vexler, Dmitry Budker, Stefan Kramer</h3>
<p>Deep neural networks (DNNs) are widely used in pattern-recognition tasks for
which a human comprehensible, quantitative description of the data-generating
process, e.g., in the form of equations, cannot be achieved. While doing so,
DNNs often produce an abstract (entangled and non-interpretable) representation
of the data-generating process. This is one of the reasons why DNNs are not
extensively used in physics-signal processing: physicists generally require
their analyses to yield quantitative information about the studied systems. In
this article we use DNNs to disentangle components of oscillating time series,
and recover meaningful information. We show that, because DNNs can find useful
abstract feature representations, they can be used when prior knowledge about
the signal-generating process exists, but is not complete, as it is
particularly the case in "new-physics" searches. To this aim, we train our DNN
on synthetic oscillating time series to perform two tasks: a regression of the
signal latent parameters and signal denoising by an Autoencoder-like
architecture. We show that the regression and denoising performance is similar
to those of least-square curve fittings (LS-fit) with true latent parameters'
initial guesses, in spite of the DNN needing no initial guesses at all. We then
explore applications in which we believe our architecture could prove useful
for time-series processing in physics, when prior knowledge is incomplete. As
an example, we employ DNNs as a tool to inform LS-fits when initial guesses are
unknown. We show that the regression can be performed on some latent
parameters, while ignoring the existence of others. Because the Autoencoder
needs no prior information about the physical model, the remaining unknown
latent parameters can still be captured, thus making use of partial prior
knowledge, while leaving space for data exploration and discoveries.
</p>
<a href="http://arxiv.org/abs/2101.03850" target="_blank">arXiv:2101.03850</a> [<a href="http://arxiv.org/pdf/2101.03850" target="_blank">pdf</a>]

<h2>Deep Interactive Bayesian Reinforcement Learning via Meta-Learning. (arXiv:2101.03864v1 [cs.LG])</h2>
<h3>Luisa Zintgraf, Sam Devlin, Kamil Ciosek, Shimon Whiteson, Katja Hofmann</h3>
<p>Agents that interact with other agents often do not know a priori what the
other agents' strategies are, but have to maximise their own online return
while interacting with and learning about others. The optimal adaptive
behaviour under uncertainty over the other agents' strategies w.r.t. some prior
can in principle be computed using the Interactive Bayesian Reinforcement
Learning framework. Unfortunately, doing so is intractable in most settings,
and existing approximation methods are restricted to small tasks. To overcome
this, we propose to meta-learn approximate belief inference and Bayes-optimal
behaviour for a given prior. To model beliefs over other agents, we combine
sequential and hierarchical Variational Auto-Encoders, and meta-train this
inference model alongside the policy. We show empirically that our approach
outperforms existing methods that use a model-free approach, sample from the
approximate posterior, maintain memory-free models of others, or do not fully
utilise the known structure of the environment.
</p>
<a href="http://arxiv.org/abs/2101.03864" target="_blank">arXiv:2101.03864</a> [<a href="http://arxiv.org/pdf/2101.03864" target="_blank">pdf</a>]

<h2>Variational Embeddings for Community Detection and Node Representation. (arXiv:2101.03885v1 [cs.LG])</h2>
<h3>Rayyan Ahmad Khan, Muhammad Umer Anwaar, Omran Kaddah, Martin Kleinsteuber</h3>
<p>In this paper, we study how to simultaneously learn two highly correlated
tasks of graph analysis, i.e., community detection and node representation
learning. We propose an efficient generative model called VECoDeR for jointly
learning Variational Embeddings for Community Detection and node
Representation. VECoDeR assumes that every node can be a member of one or more
communities. The node embeddings are learned in such a way that connected nodes
are not only "closer" to each other but also share similar community
assignments. A joint learning framework leverages community-aware node
embeddings for better community detection. We demonstrate on several graph
datasets that VECoDeR effectively out-performs many competitive baselines on
all three tasks i.e. node classification, overlapping community detection and
non-overlapping community detection. We also show that VECoDeR is
computationally efficient and has quite robust performance with varying
hyperparameters.
</p>
<a href="http://arxiv.org/abs/2101.03885" target="_blank">arXiv:2101.03885</a> [<a href="http://arxiv.org/pdf/2101.03885" target="_blank">pdf</a>]

<h2>Trear: Transformer-based RGB-D Egocentric Action Recognition. (arXiv:2101.03904v1 [cs.CV])</h2>
<h3>Xiangyu Li, Yonghong Hou, Pichao Wang, Zhimin Gao, Mingliang Xu, Wanqing Li</h3>
<p>In this paper, we propose a \textbf{Tr}ansformer-based RGB-D
\textbf{e}gocentric \textbf{a}ction \textbf{r}ecognition framework, called
Trear. It consists of two modules, inter-frame attention encoder and
mutual-attentional fusion block. Instead of using optical flow or recurrent
units, we adopt self-attention mechanism to model the temporal structure of the
data from different modalities. Input frames are cropped randomly to mitigate
the effect of the data redundancy. Features from each modality are interacted
through the proposed fusion block and combined through a simple yet effective
fusion operation to produce a joint RGB-D representation. Empirical experiments
on two large egocentric RGB-D datasets, THU-READ and FPHA, and one small
dataset, WCVS, have shown that the proposed method outperforms the
state-of-the-art results by a large margin.
</p>
<a href="http://arxiv.org/abs/2101.03904" target="_blank">arXiv:2101.03904</a> [<a href="http://arxiv.org/pdf/2101.03904" target="_blank">pdf</a>]

<h2>Learning Semantically Meaningful Features for Interpretable Classifications. (arXiv:2101.03919v1 [cs.CV])</h2>
<h3>Sandareka Wickramanayake, Wynne Hsu, Mong Li Lee</h3>
<p>Learning semantically meaningful features is important for Deep Neural
Networks to win end-user trust. Attempts to generate post-hoc explanations fall
short in gaining user confidence as they do not improve the interpretability of
feature representations learned by the models. In this work, we propose
Semantic Convolutional Neural Network (SemCNN) that has an additional Concept
layer to learn the associations between visual features and word phrases.
SemCNN employs an objective function that optimizes for both the prediction
accuracy as well as the semantic meaningfulness of the learned feature
representations. Further, SemCNN makes its decisions as a weighted sum of the
contributions of these features leading to fully interpretable decisions.
Experiment results on multiple benchmark datasets demonstrate that SemCNN can
learn features with clear semantic meaning and their corresponding
contributions to the model decision without compromising prediction accuracy.
Furthermore, these learned concepts are transferrable and can be applied to new
classes of objects that have similar concepts.
</p>
<a href="http://arxiv.org/abs/2101.03919" target="_blank">arXiv:2101.03919</a> [<a href="http://arxiv.org/pdf/2101.03919" target="_blank">pdf</a>]

<h2>Cycle Generative Adversarial Networks Algorithm With Style Transfer For Image Generation. (arXiv:2101.03921v1 [cs.CV])</h2>
<h3>Anugrah Akbar Praramadhan, Guntur Eka Saputra</h3>
<p>The biggest challenge faced by a Machine Learning Engineer is the lack of
data they have, especially for 2-dimensional images. The image is processed to
be trained into a Machine Learning model so that it can recognize patterns in
the data and provide predictions. This research is intended to create a
solution using the Cycle Generative Adversarial Networks (GANs) algorithm in
overcoming the problem of lack of data. Then use Style Transfer to be able to
generate a new image based on the given style. Based on the results of testing
the resulting model has been carried out several improvements, previously the
loss value of the photo generator: 3.1267, monet style generator: 3.2026, photo
discriminator: 0.6325, and monet style discriminator: 0.6931 to photo
generator: 2.3792, monet style generator: 2.7291, photo discriminator: 0.5956,
and monet style discriminator: 0.4940. It is hoped that the research will make
the application of this solution useful in the fields of Education, Arts,
Information Technology, Medicine, Astronomy, Automotive and other important
fields.
</p>
<a href="http://arxiv.org/abs/2101.03921" target="_blank">arXiv:2101.03921</a> [<a href="http://arxiv.org/pdf/2101.03921" target="_blank">pdf</a>]

<h2>A novel shape matching descriptor for real-time hand gesture recognition. (arXiv:2101.03923v1 [cs.CV])</h2>
<h3>Michalis Lazarou, Bo Li, Tania Stathaki</h3>
<p>The current state-of-the-art hand gesture recognition methodologies heavily
rely in the use of machine learning. However there are scenarios that machine
learning cannot be applied successfully, for example in situations where data
is scarce. This is the case when one-to-one matching is required between a
query and a database of hand gestures where each gesture represents a unique
class. In situations where learning algorithms cannot be trained, classic
computer vision techniques such as feature extraction can be used to identify
similarities between objects. Shape is one of the most important features that
can be extracted from images, however the most accurate shape matching
algorithms tend to be computationally inefficient for real-time applications.
In this work we present a novel shape matching methodology for real-time hand
gesture recognition. Extensive experiments were carried out comparing our
method with other shape matching methods with respect to accuracy and
computational complexity using our own collected hand gesture database as well
as the MPEG-7 database that is widely used for comparing 2D shape matching
algorithms. Our method outperforms the other methods and provides the optimal
combination of accuracy and computational efficiency for real-time
applications.
</p>
<a href="http://arxiv.org/abs/2101.03923" target="_blank">arXiv:2101.03923</a> [<a href="http://arxiv.org/pdf/2101.03923" target="_blank">pdf</a>]

<h2>The Vulnerability of Semantic Segmentation Networks to Adversarial Attacks in Autonomous Driving: Enhancing Extensive Environment Sensing. (arXiv:2101.03924v1 [cs.CV])</h2>
<h3>Andreas B&#xe4;r, Jonas L&#xf6;hdefink, Nikhil Kapoor, Serin J. Varghese, Fabian H&#xfc;ger, Peter Schlicht, Tim Fingscheidt</h3>
<p>Enabling autonomous driving (AD) can be considered one of the biggest
challenges in today's technology. AD is a complex task accomplished by several
functionalities, with environment perception being one of its core functions.
Environment perception is usually performed by combining the semantic
information captured by several sensors, i.e., lidar or camera. The semantic
information from the respective sensor can be extracted by using convolutional
neural networks (CNNs) for dense prediction. In the past, CNNs constantly
showed state-of-the-art performance on several vision-related tasks, such as
semantic segmentation of traffic scenes using nothing but the red-green-blue
(RGB) images provided by a camera. Although CNNs obtain state-of-the-art
performance on clean images, almost imperceptible changes to the input,
referred to as adversarial perturbations, may lead to fatal deception. The goal
of this article is to illuminate the vulnerability aspects of CNNs used for
semantic segmentation with respect to adversarial attacks, and share insights
into some of the existing known adversarial defense strategies. We aim to
clarify the advantages and disadvantages associated with applying CNNs for
environment perception in AD to serve as a motivation for future research in
this field.
</p>
<a href="http://arxiv.org/abs/2101.03924" target="_blank">arXiv:2101.03924</a> [<a href="http://arxiv.org/pdf/2101.03924" target="_blank">pdf</a>]

<h2>ORDNet: Capturing Omni-Range Dependencies for Scene Parsing. (arXiv:2101.03929v1 [cs.CV])</h2>
<h3>Shaofei Huang, Si Liu, Tianrui Hui, Jizhong Han, Bo Li, Jiashi Feng, Shuicheng Yan</h3>
<p>Learning to capture dependencies between spatial positions is essential to
many visual tasks, especially the dense labeling problems like scene parsing.
Existing methods can effectively capture long-range dependencies with
self-attention mechanism while short ones by local convolution. However, there
is still much gap between long-range and short-range dependencies, which
largely reduces the models' flexibility in application to diverse spatial
scales and relationships in complicated natural scene images. To fill such a
gap, we develop a Middle-Range (MR) branch to capture middle-range dependencies
by restricting self-attention into local patches. Also, we observe that the
spatial regions which have large correlations with others can be emphasized to
exploit long-range dependencies more accurately, and thus propose a Reweighed
Long-Range (RLR) branch. Based on the proposed MR and RLR branches, we build an
Omni-Range Dependencies Network (ORDNet) which can effectively capture short-,
middle- and long-range dependencies. Our ORDNet is able to extract more
comprehensive context information and well adapt to complex spatial variance in
scene images. Extensive experiments show that our proposed ORDNet outperforms
previous state-of-the-art methods on three scene parsing benchmarks including
PASCAL Context, COCO Stuff and ADE20K, demonstrating the superiority of
capturing omni-range dependencies in deep models for scene parsing task.
</p>
<a href="http://arxiv.org/abs/2101.03929" target="_blank">arXiv:2101.03929</a> [<a href="http://arxiv.org/pdf/2101.03929" target="_blank">pdf</a>]

<h2>Learn-n-Route: Learning implicit preferences for vehicle routing. (arXiv:2101.03936v1 [cs.AI])</h2>
<h3>Rocsildes Canoy, V&#xed;ctor Bucarey, Jayanta Mandi, Tias Guns</h3>
<p>We investigate a learning decision support system for vehicle routing, where
the routing engine learns implicit preferences that human planners have when
manually creating route plans (or routings). The goal is to use these learned
subjective preferences on top of the distance-based objective criterion in
vehicle routing systems. This is an alternative to the practice of
distinctively formulating a custom VRP for every company with its own routing
requirements. Instead, we assume the presence of past vehicle routing solutions
over similar sets of customers, and learn to make similar choices. The learning
approach is based on the concept of learning a Markov model, which corresponds
to a probabilistic transition matrix, rather than a deterministic distance
matrix. This nevertheless allows us to use existing arc routing VRP software in
creating the actual routings, and to optimize over both distances and
preferences at the same time. For the learning, we explore different schemes to
construct the probabilistic transition matrix that can co-evolve with changing
preferences over time. Our results on a use-case with a small transportation
company show that our method is able to generate results that are close to the
manually created solutions, without needing to characterize all constraints and
sub-objectives explicitly. Even in the case of changes in the customer sets,
our method is able to find solutions that are closer to the actual routings
than when using only distances, and hence, solutions that require fewer manual
changes when transformed into practical routings.
</p>
<a href="http://arxiv.org/abs/2101.03936" target="_blank">arXiv:2101.03936</a> [<a href="http://arxiv.org/pdf/2101.03936" target="_blank">pdf</a>]

<h2>Predicting Patient Outcomes with Graph Representation Learning. (arXiv:2101.03940v1 [cs.LG])</h2>
<h3>Emma Rocheteau, Catherine Tong, Petar Veli&#x10d;kovi&#x107;, Nicholas Lane, Pietro Li&#xf2;</h3>
<p>Recent work on predicting patient outcomes in the Intensive Care Unit (ICU)
has focused heavily on the physiological time series data, largely ignoring
sparse data such as diagnoses and medications. When they are included, they are
usually concatenated in the late stages of a model, which may struggle to learn
from rarer disease patterns. Instead, we propose a strategy to exploit
diagnoses as relational information by connecting similar patients in a graph.
To this end, we propose LSTM-GNN for patient outcome prediction tasks: a hybrid
model combining Long Short-Term Memory networks (LSTMs) for extracting temporal
features and Graph Neural Networks (GNNs) for extracting the patient
neighbourhood information. We demonstrate that LSTM-GNNs outperform the
LSTM-only baseline on length of stay prediction tasks on the eICU database.
More generally, our results indicate that exploiting information from
neighbouring patient cases using graph neural networks is a promising research
direction, yielding tangible returns in supervised learning performance on
Electronic Health Records.
</p>
<a href="http://arxiv.org/abs/2101.03940" target="_blank">arXiv:2101.03940</a> [<a href="http://arxiv.org/pdf/2101.03940" target="_blank">pdf</a>]

<h2>Impact of Interventional Policies Including Vaccine on Covid-19 Propagation and Socio-Economic Factors. (arXiv:2101.03944v1 [cs.LG])</h2>
<h3>Haonan Wu, Rajarshi Banerjee, Indhumathi Venkatachalam, Daniel Percy-Hughes, Praveen Chougale</h3>
<p>A novel coronavirus disease has emerged (later named COVID-19) and caused the
world to enter a new reality, with many direct and indirect factors influencing
it. Some are human-controllable (e.g. interventional policies, mobility and the
vaccine); some are not (e.g. the weather). We have sought to test how a change
in these human-controllable factors might influence two measures: the number of
daily cases against economic impact. If applied at the right level and with
up-to-date data to measure, policymakers would be able to make targeted
interventions and measure their cost. This study aims to provide a predictive
analytics framework to model, predict and simulate COVID-19 propagation and the
socio-economic impact of interventions intended to reduce the spread of the
disease such as policy and/or vaccine. It allows policymakers, government
representatives and business leaders to make better-informed decisions about
the potential effect of various interventions with forward-looking views via
scenario planning. We have leveraged a recently launched open-source COVID-19
big data platform and used published research to find potentially relevant
variables (features) and leveraged in-depth data quality checks and analytics
for feature selection and predictions. An advanced machine learning pipeline
has been developed armed with a self-evolving model, deployed on a modern
machine learning architecture. It has high accuracy for trend prediction
(back-tested with r-squared) and is augmented with interpretability for deeper
insights.
</p>
<a href="http://arxiv.org/abs/2101.03944" target="_blank">arXiv:2101.03944</a> [<a href="http://arxiv.org/pdf/2101.03944" target="_blank">pdf</a>]

<h2>Evolving Reinforcement Learning Algorithms. (arXiv:2101.03958v1 [cs.LG])</h2>
<h3>John D. Co-Reyes, Yingjie Miao, Daiyi Peng, Esteban Real, Sergey Levine, Quoc V. Le, Honglak Lee, Aleksandra Faust</h3>
<p>We propose a method for meta-learning reinforcement learning algorithms by
searching over the space of computational graphs which compute the loss
function for a value-based model-free RL agent to optimize. The learned
algorithms are domain-agnostic and can generalize to new environments not seen
during training. Our method can both learn from scratch and bootstrap off known
existing algorithms, like DQN, enabling interpretable modifications which
improve performance. Learning from scratch on simple classical control and
gridworld tasks, our method rediscovers the temporal-difference (TD) algorithm.
Bootstrapped from DQN, we highlight two learned algorithms which obtain good
generalization performance over other classical control tasks, gridworld type
tasks, and Atari games. The analysis of the learned algorithm behavior shows
resemblance to recently proposed RL algorithms that address overestimation in
value-based methods.
</p>
<a href="http://arxiv.org/abs/2101.03958" target="_blank">arXiv:2101.03958</a> [<a href="http://arxiv.org/pdf/2101.03958" target="_blank">pdf</a>]

<h2>Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity. (arXiv:2101.03961v1 [cs.LG])</h2>
<h3>William Fedus, Barret Zoph, Noam Shazeer</h3>
<p>In deep learning, models typically reuse the same parameters for all inputs.
Mixture of Experts (MoE) defies this and instead selects different parameters
for each incoming example. The result is a sparsely-activated model -- with
outrageous numbers of parameters -- but a constant computational cost. However,
despite several notable successes of MoE, widespread adoption has been hindered
by complexity, communication costs and training instability -- we address these
with the Switch Transformer. We simplify the MoE routing algorithm and design
intuitive improved models with reduced communication and computational costs.
Our proposed training techniques help wrangle the instabilities and we show
large sparse models may be trained, for the first time, with lower precision
(bfloat16) formats. We design models based off T5-Base and T5-Large to obtain
up to 7x increases in pre-training speed with the same computational resources.
These improvements extend into multilingual settings where we measure gains
over the mT5-Base version across all 101 languages. Finally, we advance the
current scale of language models by pre-training up to trillion parameter
models on the "Colossal Clean Crawled Corpus" and achieve a 4x speedup over the
T5-XXL model.
</p>
<a href="http://arxiv.org/abs/2101.03961" target="_blank">arXiv:2101.03961</a> [<a href="http://arxiv.org/pdf/2101.03961" target="_blank">pdf</a>]

<h2>How Much Automation Does a Data Scientist Want?. (arXiv:2101.03970v1 [cs.LG])</h2>
<h3>Dakuo Wang, Q. Vera Liao, Yunfeng Zhang, Udayan Khurana, Horst Samulowitz, Soya Park, Michael Muller, Lisa Amini</h3>
<p>Data science and machine learning (DS/ML) are at the heart of the recent
advancements of many Artificial Intelligence (AI) applications. There is an
active research thread in AI, \autoai, that aims to develop systems for
automating end-to-end the DS/ML Lifecycle. However, do DS and ML workers really
want to automate their DS/ML workflow? To answer this question, we first
synthesize a human-centered AutoML framework with 6 User Role/Personas, 10
Stages and 43 Sub-Tasks, 5 Levels of Automation, and 5 Types of Explanation,
through reviewing research literature and marketing reports. Secondly, we use
the framework to guide the design of an online survey study with 217 DS/ML
workers who had varying degrees of experience, and different user roles
"matching" to our 6 roles/personas. We found that different user personas
participated in distinct stages of the lifecycle -- but not all stages. Their
desired levels of automation and types of explanation for AutoML also varied
significantly depending on the DS/ML stage and the user persona. Based on the
survey results, we argue there is no rationale from user needs for complete
automation of the end-to-end DS/ML lifecycle. We propose new next steps for
user-controlled DS/ML automation.
</p>
<a href="http://arxiv.org/abs/2101.03970" target="_blank">arXiv:2101.03970</a> [<a href="http://arxiv.org/pdf/2101.03970" target="_blank">pdf</a>]

<h2>Technology Readiness Levels for Machine Learning Systems. (arXiv:2101.03989v1 [cs.LG])</h2>
<h3>Alexander Lavin, Ciar&#xe1;n M. Gilligan-Lee, Alessya Visnjic, Siddha Ganju, Dava Newman, Sujoy Ganguly, Danny Lange, At&#x131;l&#x131;m G&#xfc;ne&#x15f; Baydin, Amit Sharma, Adam Gibson, Yarin Gal, Eric P. Xing, Chris Mattmann, James Parr</h3>
<p>The development and deployment of machine learning (ML) systems can be
executed easily with modern tools, but the process is typically rushed and
means-to-an-end. The lack of diligence can lead to technical debt, scope creep
and misaligned objectives, model misuse and failures, and expensive
consequences. Engineering systems, on the other hand, follow well-defined
processes and testing standards to streamline development for high-quality,
reliable results. The extreme is spacecraft systems, where mission critical
measures and robustness are ingrained in the development process. Drawing on
experience in both spacecraft engineering and ML (from research through product
across domain areas), we have developed a proven systems engineering approach
for machine learning development and deployment. Our "Machine Learning
Technology Readiness Levels" (MLTRL) framework defines a principled process to
ensure robust, reliable, and responsible systems while being streamlined for ML
workflows, including key distinctions from traditional software engineering.
Even more, MLTRL defines a lingua franca for people across teams and
organizations to work collaboratively on artificial intelligence and machine
learning technologies. Here we describe the framework and elucidate it with
several real world use-cases of developing ML methods from basic research
through productization and deployment, in areas such as medical diagnostics,
consumer computer vision, satellite imagery, and particle physics.
</p>
<a href="http://arxiv.org/abs/2101.03989" target="_blank">arXiv:2101.03989</a> [<a href="http://arxiv.org/pdf/2101.03989" target="_blank">pdf</a>]

<h2>Individual Mobility Prediction: An Interpretable Activity-based Hidden Markov Approach. (arXiv:2101.03996v1 [cs.LG])</h2>
<h3>Baichuan Mo, Zhan Zhao, Haris N. Koutsopoulos, Jinhua Zhao</h3>
<p>Individual mobility is driven by demand for activities with diverse
spatiotemporal patterns, but existing methods for mobility prediction often
overlook the underlying activity patterns. To address this issue, this study
develops an activity-based modeling framework for individual mobility
prediction. Specifically, an input-output hidden Markov model (IOHMM) framework
is proposed to simultaneously predict the (continuous) time and (discrete)
location of an individual's next trip using transit smart card data. The
prediction task can be transformed into predicting the hidden activity duration
and end location. Based on a case study of Hong Kong's metro system, we show
that the proposed model can achieve similar prediction performance as the
state-of-the-art long short-term memory (LSTM) model. Unlike LSTM, the proposed
IOHMM model can also be used to analyze hidden activity patterns, which
provides meaningful behavioral interpretation for why an individual makes a
certain trip. Therefore, the activity-based prediction framework offers a way
to preserve the predictive power of advanced machine learning methods while
enhancing our ability to generate insightful behavioral explanations, which is
useful for enhancing situational awareness in user-centric transportation
applications such as personalized traveler information.
</p>
<a href="http://arxiv.org/abs/2101.03996" target="_blank">arXiv:2101.03996</a> [<a href="http://arxiv.org/pdf/2101.03996" target="_blank">pdf</a>]

<h2>Evaluating Deep Learning Approaches for Covid19 Fake News Detection. (arXiv:2101.04012v1 [cs.LG])</h2>
<h3>Apurva Wani, Isha Joshi, Snehal Khandve, Vedangi Wagh, Raviraj Joshi</h3>
<p>Social media platforms like Facebook, Twitter, and Instagram have enabled
connection and communication on a large scale. It has revolutionized the rate
at which information is shared and enhanced its reach. However, another side of
the coin dictates an alarming story. These platforms have led to an increase in
the creation and spread of fake news. The fake news has not only influenced
people in the wrong direction but also claimed human lives. During these
critical times of the Covid19 pandemic, it is easy to mislead people and make
them believe in fatal information. Therefore it is important to curb fake news
at source and prevent it from spreading to a larger audience. We look at
automated techniques for fake news detection from a data mining perspective. We
evaluate different supervised text classification algorithms on Contraint@AAAI
2021 Covid-19 Fake news detection dataset. The classification algorithms are
based on Convolutional Neural Networks (CNN), Long Short Term Memory (LSTM),
and Bidirectional Encoder Representations from Transformers (BERT). We also
evaluate the importance of unsupervised learning in the form of language model
pre-training and distributed word representations using unlabelled covid tweets
corpus. We report the best accuracy of 98.41\% on the Covid-19 Fake news
detection dataset.
</p>
<a href="http://arxiv.org/abs/2101.04012" target="_blank">arXiv:2101.04012</a> [<a href="http://arxiv.org/pdf/2101.04012" target="_blank">pdf</a>]

<h2>Contrastive Learning Improves Critical Event Prediction in COVID-19 Patients. (arXiv:2101.04013v1 [cs.LG])</h2>
<h3>Tingyi Wanyan, Hossein Honarvar, Suraj K. Jaladanki, Chengxi Zang, Nidhi Naik, Sulaiman Somani, Jessica K. De Freitas, Ishan Paranjpe, Akhil Vaid, Riccardo Miotto, Girish N. Nadkarni, Marinka Zitnik, ArifulAzad, Fei Wang, Ying Ding, Benjamin S. Glicksberg</h3>
<p>Machine Learning (ML) models typically require large-scale, balanced training
data to be robust, generalizable, and effective in the context of healthcare.
This has been a major issue for developing ML models for the
coronavirus-disease 2019 (COVID-19) pandemic where data is highly imbalanced,
particularly within electronic health records (EHR) research. Conventional
approaches in ML use cross-entropy loss (CEL) that often suffers from poor
margin classification. For the first time, we show that contrastive loss (CL)
improves the performance of CEL especially for imbalanced EHR data and the
related COVID-19 analyses. This study has been approved by the Institutional
Review Board at the Icahn School of Medicine at Mount Sinai. We use EHR data
from five hospitals within the Mount Sinai Health System (MSHS) to predict
mortality, intubation, and intensive care unit (ICU) transfer in hospitalized
COVID-19 patients over 24 and 48 hour time windows. We train two sequential
architectures (RNN and RETAIN) using two loss functions (CEL and CL). Models
are tested on full sample data set which contain all available data and
restricted data set to emulate higher class imbalance.CL models consistently
outperform CEL models with the restricted data set on these tasks with
differences ranging from 0.04 to 0.15 for AUPRC and 0.05 to 0.1 for AUROC. For
the restricted sample, only the CL model maintains proper clustering and is
able to identify important features, such as pulse oximetry. CL outperforms CEL
in instances of severe class imbalance, on three EHR outcomes with respect to
three performance metrics: predictive power, clustering, and feature
importance. We believe that the developed CL framework can be expanded and used
for EHR ML work in general.
</p>
<a href="http://arxiv.org/abs/2101.04013" target="_blank">arXiv:2101.04013</a> [<a href="http://arxiv.org/pdf/2101.04013" target="_blank">pdf</a>]

<h2>A Commonsense Reasoning Framework for Explanatory Emotion Attribution, Generation and Re-classification. (arXiv:2101.04017v1 [cs.AI])</h2>
<h3>Antonio Lieto, Gian Luca Pozzato, Stefano Zoia, Viviana Patti, Rossana Damiano</h3>
<p>In this work we present an explainable system for emotion attribution and
recommendation (called DEGARI) relying on a recently introduced commonsense
reasoning framework (the TCL logic) which is based on a human-like procedure
for the automatic generation of novel concepts in a Description Logics
knowledge base. Starting from an ontological formalization of emotions (known
as ArsEmotica), the system exploits the logic TCL to automatically generate
novel commonsense semantic representations of compound emotions (e.g. Love as
derived from the combination of Joy and Trust according to the ArsEmotica
model). The generated emotions correspond to prototypes, i.e. commonsense
representations of given concepts, and have been used to reclassify
emotion-related contents in a variety of artistic domains, ranging from art
datasets to the editorial content available in RaiPlay, the online multimedia
platform of RAI Radiotelevisione Italiana (the Italian public broadcasting
company). We have tested our system (1) by reclassifying the available contents
in the tested dataset with respect to the new generated compound emotions (2)
with an evaluation, in the form of a controlled user study experiment, of the
feasibility of using the obtained reclassifications as recommended emotional
content. The obtained results are encouraging and pave the way to many possible
further improvements and research directions.
</p>
<a href="http://arxiv.org/abs/2101.04017" target="_blank">arXiv:2101.04017</a> [<a href="http://arxiv.org/pdf/2101.04017" target="_blank">pdf</a>]

<h2>Unchain the Search Space with Hierarchical Differentiable Architecture Search. (arXiv:2101.04028v1 [cs.CV])</h2>
<h3>Guanting Liu, Yujie Zhong, Sheng Guo, Matthew R. Scott, Weilin Huang</h3>
<p>Differentiable architecture search (DAS) has made great progress in searching
for high-performance architectures with reduced computational cost. However,
DAS-based methods mainly focus on searching for a repeatable cell structure,
which is then stacked sequentially in multiple stages to form the networks.
This configuration significantly reduces the search space, and ignores the
importance of connections between the cells. To overcome this limitation, in
this paper, we propose a Hierarchical Differentiable Architecture Search
(H-DAS) that performs architecture search both at the cell level and at the
stage level. Specifically, the cell-level search space is relaxed so that the
networks can learn stage-specific cell structures. For the stage-level search,
we systematically study the architectures of stages, including the number of
cells in each stage and the connections between the cells. Based on insightful
observations, we design several search rules and losses, and mange to search
for better stage-level architectures. Such hierarchical search space greatly
improves the performance of the networks without introducing expensive search
cost. Extensive experiments on CIFAR10 and ImageNet demonstrate the
effectiveness of the proposed H-DAS. Moreover, the searched stage-level
architectures can be combined with the cell structures searched by existing DAS
methods to further boost the performance. Code is available at:
https://github.com/MalongTech/research-HDAS
</p>
<a href="http://arxiv.org/abs/2101.04028" target="_blank">arXiv:2101.04028</a> [<a href="http://arxiv.org/pdf/2101.04028" target="_blank">pdf</a>]

<h2>Colorectal Polyp Detection in Real-world Scenario: Design and Experiment Study. (arXiv:2101.04034v1 [cs.CV])</h2>
<h3>Xinzi Sun, Dechun Wang, Chenxi Zhang, Pengfei Zhang, Zinan Xiong, Yu Cao, Benyuan Liu, Xiaowei Liu, Shuijiao Chen</h3>
<p>Colorectal polyps are abnormal tissues growing on the intima of the colon or
rectum with a high risk of developing into colorectal cancer, the third leading
cause of cancer death worldwide. Early detection and removal of colon polyps
via colonoscopy have proved to be an effective approach to prevent colorectal
cancer. Recently, various CNN-based computer-aided systems have been developed
to help physicians detect polyps. However, these systems do not perform well in
real-world colonoscopy operations due to the significant difference between
images in a real colonoscopy and those in the public datasets. Unlike the
well-chosen clear images with obvious polyps in the public datasets, images
from a colonoscopy are often blurry and contain various artifacts such as
fluid, debris, bubbles, reflection, specularity, contrast, saturation, and
medical instruments, with a wide variety of polyps of different sizes, shapes,
and textures. All these factors pose a significant challenge to effective polyp
detection in a colonoscopy. To this end, we collect a private dataset that
contains 7,313 images from 224 complete colonoscopy procedures. This dataset
represents realistic operation scenarios and thus can be used to better train
the models and evaluate a system's performance in practice. We propose an
integrated system architecture to address the unique challenges for polyp
detection. Extensive experiments results show that our system can effectively
detect polyps in a colonoscopy with excellent performance in real time.
</p>
<a href="http://arxiv.org/abs/2101.04034" target="_blank">arXiv:2101.04034</a> [<a href="http://arxiv.org/pdf/2101.04034" target="_blank">pdf</a>]

<h2>Evaluating Disentanglement of Structured Latent Representations. (arXiv:2101.04041v1 [cs.LG])</h2>
<h3>Rapha&#xeb;l Dang-Nhu, Angelika Steger</h3>
<p>We design the first multi-layer disentanglement metric operating at all
hierarchy levels of a structured latent representation, and derive its
theoretical properties. Applied to object-centric representations, our metric
unifies the evaluation of both object separation between latent slots and
internal slot disentanglement into a common mathematical framework. It also
addresses the problematic dependence on segmentation mask sharpness of previous
pixel-level segmentation metrics such as ARI. Perhaps surprisingly, our
experimental results show that good ARI values do not guarantee a disentangled
representation, and that the exclusive focus on this metric has led to
counterproductive choices in some previous evaluations. As an additional
technical contribution, we present a new algorithm for obtaining feature
importances that handles slot permutation invariance in the representation.
</p>
<a href="http://arxiv.org/abs/2101.04041" target="_blank">arXiv:2101.04041</a> [<a href="http://arxiv.org/pdf/2101.04041" target="_blank">pdf</a>]

<h2>Learning to Ignore: Fair and Task Independent Representations. (arXiv:2101.04047v1 [cs.LG])</h2>
<h3>Linda Helen Boedi, Dr. Helmut Grabner</h3>
<p>Training fair machine learning models, aiming for their interpretability and
solving the problem of domain shift has gained a lot of interest in the last
years. There is a vast amount of work addressing these topics, mostly in
separation. In this work we show that they can be seen as a common framework of
learning invariant representations. The representations should allow to predict
the target while at the same time being invariant to sensitive attributes which
split the dataset into subgroups. Our approach is based on the simple
observation that it is impossible for any learning algorithm to differentiate
samples if they have the same feature representation. This is formulated as an
additional loss (regularizer) enforcing a common feature representation across
subgroups. We apply it to learn fair models and interpret the influence of the
sensitive attribute. Furthermore it can be used for domain adaptation,
transferring knowledge and learning effectively from very few examples. In all
applications it is essential not only to learn to predict the target, but also
to learn what to ignore.
</p>
<a href="http://arxiv.org/abs/2101.04047" target="_blank">arXiv:2101.04047</a> [<a href="http://arxiv.org/pdf/2101.04047" target="_blank">pdf</a>]

<h2>Horizontal-to-Vertical Video Conversion. (arXiv:2101.04051v1 [cs.CV])</h2>
<h3>Tun Zhu, Daoxin Zhang, Tianran Wang, Xiaolong Jiang, Jiawei Li, Yao Hu, Jianke Zhu</h3>
<p>Alongside the prevalence of mobile videos, the general public leans towards
consuming vertical videos on hand-held devices. To revitalize the exposure of
horizontal contents, we hereby set forth the exploration of automated
horizontal-to-vertical (abbreviated as H2V) video conversion with our proposed
H2V framework, accompanied by an accurately annotated H2V-142K dataset.
Concretely, H2V framework integrates video shot boundary detection, subject
selection and multi-object tracking to facilitate the subject-preserving
conversion, wherein the key is subject selection. To achieve so, we propose a
Rank-SS module that detects human objects, then selects the subject-to-preserve
via exploiting location, appearance, and salient cues. Afterward, the framework
automatically crops the video around the subject to produce vertical contents
from horizontal sources. To build and evaluate our H2V framework, H2V-142K
dataset is densely annotated with subject bounding boxes for 125 videos with
132K frames and 9,500 video covers, upon which we demonstrate superior subject
selection performance comparing to traditional salient approaches, and exhibit
promising horizontal-to-vertical conversion performance overall. By publicizing
this dataset as well as our approach, we wish to pave the way for more valuable
endeavors on the horizontal-to-vertical video conversion task.
</p>
<a href="http://arxiv.org/abs/2101.04051" target="_blank">arXiv:2101.04051</a> [<a href="http://arxiv.org/pdf/2101.04051" target="_blank">pdf</a>]

<h2>Anomaly Detection for Aggregated Data Using Multi-Graph Autoencoder. (arXiv:2101.04053v1 [cs.LG])</h2>
<h3>Tomer Meirman, Roni Stern, Gilad Katz</h3>
<p>In data systems, activities or events are continuously collected in the field
to trace their proper executions. Logging, which means recording sequences of
events, can be used for analyzing system failures and malfunctions, and
identifying the causes and locations of such issues. In our research we focus
on creating an Anomaly detection models for system logs. The task of anomaly
detection is identifying unexpected events in dataset, which differ from the
normal behavior. Anomaly detection models also assist in data systems analysis
tasks.

Modern systems may produce such a large amount of events monitoring every
individual event is not feasible. In such cases, the events are often
aggregated over a fixed period of time, reporting the number of times every
event has occurred in that time period. This aggregation facilitates scaling,
but requires a different approach for anomaly detection. In this research, we
present a thorough analysis of the aggregated data and the relationships
between aggregated events. Based on the initial phase of our research we
present graphs representations of our aggregated dataset, which represent the
different relationships between aggregated instances in the same context.

Using the graph representation, we propose Multiple-graphs autoencoder MGAE,
a novel convolutional graphs-autoencoder model which exploits the relationships
of the aggregated instances in our unique dataset. MGAE outperforms standard
graph-autoencoder models and the different experiments. With our novel MGAE we
present 60% decrease in reconstruction error in comparison to standard graph
autoencoder, which is expressed in reconstructing high-degree relationships.
</p>
<a href="http://arxiv.org/abs/2101.04053" target="_blank">arXiv:2101.04053</a> [<a href="http://arxiv.org/pdf/2101.04053" target="_blank">pdf</a>]

<h2>Towards Real-World Blind Face Restoration with Generative Facial Prior. (arXiv:2101.04061v1 [cs.CV])</h2>
<h3>Xintao Wang, Yu Li, Honglun Zhang, Ying Shan</h3>
<p>Blind face restoration usually relies on facial priors, such as facial
geometry prior or reference prior, to restore realistic and faithful details.
However, very low-quality inputs cannot offer accurate geometric prior while
high-quality references are inaccessible, limiting the applicability in
real-world scenarios. In this work, we propose GFP-GAN that leverages rich and
diverse priors encapsulated in a pretrained face GAN for blind face
restoration. This Generative Facial Prior (GFP) is incorporated into the face
restoration process via novel channel-split spatial feature transform layers,
which allow our method to achieve a good balance of realness and fidelity.
Thanks to the powerful generative facial prior and delicate designs, our
GFP-GAN could jointly restore facial details and enhance colors with just a
single forward pass, while GAN inversion methods require expensive
image-specific optimization at inference. Extensive experiments show that our
method achieves superior performance to prior art on both synthetic and
real-world datasets.
</p>
<a href="http://arxiv.org/abs/2101.04061" target="_blank">arXiv:2101.04061</a> [<a href="http://arxiv.org/pdf/2101.04061" target="_blank">pdf</a>]

<h2>Deeplite Neutrino^{TM}: An End-to-End Framework for Constrained Deep Learning Model Optimization. (arXiv:2101.04073v1 [cs.LG])</h2>
<h3>Anush Sankaran, Olivier Mastropietro, Ehsan Saboori, Yasser Idris, Davis Sawyer, MohammadHossein AskariHemmat, Ghouthi Boukli Hacene</h3>
<p>Designing deep learning-based solutions is becoming a race for training
deeper models with a greater number of layers. While a large-size deeper model
could provide competitive accuracy, it creates a lot of logistical challenges
and unreasonable resource requirements during development and deployment. This
has been one of the key reasons for deep learning models not being excessively
used in various production environments, especially in edge devices. There is
an immediate requirement for optimizing and compressing these deep learning
models, to enable on-device intelligence. In this research, we introduce a
black-box framework, Deeplite Neutrino^{TM} for production-ready optimization
of deep learning models. The framework provides an easy mechanism for the
end-users to provide constraints such as a tolerable drop in accuracy or target
size of the optimized models, to guide the whole optimization process. The
framework is easy to include in an existing production pipeline and is
available as a Python Package, supporting PyTorch and Tensorflow libraries. The
optimization performance of the framework is shown across multiple benchmark
datasets and popular deep learning models. Further, the framework is currently
used in production and the results and testimonials from several clients are
summarized.
</p>
<a href="http://arxiv.org/abs/2101.04073" target="_blank">arXiv:2101.04073</a> [<a href="http://arxiv.org/pdf/2101.04073" target="_blank">pdf</a>]

<h2>System Design for a Data-driven and Explainable Customer Sentiment Monitor. (arXiv:2101.04086v1 [cs.LG])</h2>
<h3>An Nguyen, Stefan Foerstel, Thomas Kittler, Andrey Kurzyukov, Leo Schwinn, Dario Zanca, Tobias Hipp, Da Jun Sun, Michael Schrapp, Eva Rothgang, Bjoern Eskofier</h3>
<p>The most important goal of customer services is to keep the customer
satisfied. However, service resources are always limited and must be
prioritized. Therefore, it is important to identify customers who potentially
become unsatisfied and might lead to escalations. Today this prioritization of
customers is often done manually. Data science on IoT data (esp. log data) for
machine health monitoring, as well as analytics on enterprise data for customer
relationship management (CRM) have mainly been researched and applied
independently. In this paper, we present a framework for a data-driven decision
support system which combines IoT and enterprise data to model customer
sentiment. Such decision support systems can help to prioritize customers and
service resources to effectively troubleshoot problems or even avoid them. The
framework is applied in a real-world case study with a major medical device
manufacturer. This includes a fully automated and interpretable machine
learning pipeline designed to meet the requirements defined with domain experts
and end users. The overall framework is currently deployed, learns and
evaluates predictive models from terabytes of IoT and enterprise data to
actively monitor the customer sentiment for a fleet of thousands of high-end
medical devices. Furthermore, we provide an anonymized industrial benchmark
dataset for the research community.
</p>
<a href="http://arxiv.org/abs/2101.04086" target="_blank">arXiv:2101.04086</a> [<a href="http://arxiv.org/pdf/2101.04086" target="_blank">pdf</a>]

<h2>Remote Pulse Estimation in the Presence of Face Masks. (arXiv:2101.04096v1 [cs.CV])</h2>
<h3>Jeremy Speth, Nathan Vance, Patrick Flynn, Kevin Bowyer, Adam Czajka</h3>
<p>Remote photoplethysmography (rPPG) is a known family of techniques for
monitoring blood volume changes from a camera. It may be especially useful for
widespread contact-less health monitoring when used to analyze face video from
consumer-grade visible-light cameras. The COVID-19 pandemic has caused the
widespread use of protective face masks to prevent virus transmission. We found
that occlusions from face masks affect face video-based rPPG as the mean
absolute error of blood volume estimation is nearly doubled when the face is
partially occluded by protective masks. To our knowledge, this paper is the
first to analyse the impact of face masks on the accuracy of blood volume pulse
estimation and offers several novel elements: (a) two publicly available pulse
estimation datasets acquired from 86 unmasked and 61 masked subjects, (b)
evaluations of handcrafted algorithms and a 3D convolutional neural network
trained on videos of full (unmasked) faces and synthetically generated masks,
and (c) data augmentation method (a generator adding a synthetic mask to a face
video). Our findings help identify how face masks degrade accuracy of face
video analysis, and we discuss paths toward more robust pulse estimation in
their presence. The datasets and source codes of all proposed methods are
available along with this paper.
</p>
<a href="http://arxiv.org/abs/2101.04096" target="_blank">arXiv:2101.04096</a> [<a href="http://arxiv.org/pdf/2101.04096" target="_blank">pdf</a>]

<h2>Correlated Weights in Infinite Limits of Deep Convolutional Neural Networks. (arXiv:2101.04097v1 [stat.ML])</h2>
<h3>Adri&#xe0; Garriga-Alonso, Mark van der Wilk</h3>
<p>Infinite width limits of deep neural networks often have tractable forms.
They have been used to analyse the behaviour of finite networks, as well as
being useful methods in their own right. When investigating infinitely wide
CNNs it was observed that the correlations arising from spatial weight sharing
disappear in the infinite limit. This is undesirable, as spatial correlation is
the main motivation behind CNNs. We show that the loss of this property is not
a consequence of the infinite limit, but rather of choosing an independent
weight prior. Correlating the weights maintains the correlations in the
activations. Varying the amount of correlation interpolates between
independent-weight limits and mean-pooling. Empirical evaluation of the
infinitely wide network shows that optimal performance is achieved between the
extremes, indicating that correlations can be useful.
</p>
<a href="http://arxiv.org/abs/2101.04097" target="_blank">arXiv:2101.04097</a> [<a href="http://arxiv.org/pdf/2101.04097" target="_blank">pdf</a>]

<h2>Neural Re-Rendering of Humans from a Single Image. (arXiv:2101.04104v1 [cs.CV])</h2>
<h3>Kripasindhu Sarkar, Dushyant Mehta, Weipeng Xu, Vladislav Golyanik, Christian Theobalt</h3>
<p>Human re-rendering from a single image is a starkly under-constrained
problem, and state-of-the-art algorithms often exhibit undesired artefacts,
such as over-smoothing, unrealistic distortions of the body parts and garments,
or implausible changes of the texture. To address these challenges, we propose
a new method for neural re-rendering of a human under a novel user-defined pose
and viewpoint, given one input image. Our algorithm represents body pose and
shape as a parametric mesh which can be reconstructed from a single image and
easily reposed. Instead of a colour-based UV texture map, our approach further
employs a learned high-dimensional UV feature map to encode appearance. This
rich implicit representation captures detailed appearance variation across
poses, viewpoints, person identities and clothing styles better than learned
colour texture maps. The body model with the rendered feature maps is fed
through a neural image-translation network that creates the final rendered
colour image. The above components are combined in an end-to-end-trained neural
network architecture that takes as input a source person image, and images of
the parametric body model in the source pose and desired target pose.
Experimental evaluation demonstrates that our approach produces higher quality
single image re-rendering results than existing methods.
</p>
<a href="http://arxiv.org/abs/2101.04104" target="_blank">arXiv:2101.04104</a> [<a href="http://arxiv.org/pdf/2101.04104" target="_blank">pdf</a>]

<h2>Controllable Guarantees for Fair Outcomes via Contrastive Information Estimation. (arXiv:2101.04108v1 [cs.LG])</h2>
<h3>Umang Gupta, Aaron Ferber, Bistra Dilkina, Greg Ver Steeg</h3>
<p>Controlling bias in training datasets is vital for ensuring equal treatment,
or parity, between different groups in downstream applications. A naive
solution is to transform the data so that it is statistically independent of
group membership, but this may throw away too much information when a
reasonable compromise between fairness and accuracy is desired. Another common
approach is to limit the ability of a particular adversary who seeks to
maximize parity. Unfortunately, representations produced by adversarial
approaches may still retain biases as their efficacy is tied to the complexity
of the adversary used during training. To this end, we theoretically establish
that by limiting the mutual information between representations and protected
attributes, we can assuredly control the parity of any downstream classifier.
We demonstrate an effective method for controlling parity through mutual
information based on contrastive information estimators and show that they
outperform approaches that rely on variational bounds based on complex
generative models. We test our approach on UCI Adult and Heritage Health
datasets and demonstrate that our approach provides more informative
representations across a range of desired parity thresholds while providing
strong theoretical guarantees on the parity of any downstream algorithm.
</p>
<a href="http://arxiv.org/abs/2101.04108" target="_blank">arXiv:2101.04108</a> [<a href="http://arxiv.org/pdf/2101.04108" target="_blank">pdf</a>]

<h2>A Permutation-based Model for Crowd Labeling: Optimal Estimation and Robustness. (arXiv:1606.09632v3 [cs.LG] UPDATED)</h2>
<h3>Nihar B. Shah, Sivaraman Balakrishnan, Martin J. Wainwright</h3>
<p>The task of aggregating and denoising crowd-labeled data has gained increased
significance with the advent of crowdsourcing platforms and massive datasets.
We propose a permutation-based model for crowd labeled data that is a
significant generalization of the classical Dawid-Skene model, and introduce a
new error metric by which to compare different estimators. We derive global
minimax rates for the permutation-based model that are sharp up to logarithmic
factors, and match the minimax lower bounds derived under the simpler
Dawid-Skene model. We then design two computationally-efficient estimators: the
WAN estimator for the setting where the ordering of workers in terms of their
abilities is approximately known, and the OBI-WAN estimator where that is not
known. For each of these estimators, we provide non-asymptotic bounds on their
performance. We conduct synthetic simulations and experiments on real-world
crowdsourcing data, and the experimental results corroborate our theoretical
findings.
</p>
<a href="http://arxiv.org/abs/1606.09632" target="_blank">arXiv:1606.09632</a> [<a href="http://arxiv.org/pdf/1606.09632" target="_blank">pdf</a>]

<h2>Particle filter re-detection for visual tracking via correlation filters. (arXiv:1711.10069v3 [cs.CV] UPDATED)</h2>
<h3>Di Yuan, Xiaohuan Lu, Donghao Li, Yingyi Liang, Xinming Zhang</h3>
<p>Most of the correlation filter based tracking algorithms can achieve good
performance and maintain fast computational speed. However, in some complicated
tracking scenes, there is a fatal defect that causes the object to be located
inaccurately. In order to address this problem, we propose a particle filter
redetection based tracking approach for accurate object localization. During
the tracking process, the kernelized correlation filter (KCF) based tracker
locates the object by relying on the maximum response value of the response
map; when the response map becomes ambiguous, the KCF tracking result becomes
unreliable. Our method can provide more candidates by particle resampling to
detect the object accordingly. Additionally, we give a new object scale
evaluation mechanism, which merely considers the differences between the
maximum response values in consecutive frames. Extensive experiments on OTB2013
and OTB2015 datasets demonstrate that the proposed tracker performs favorably
in relation to the state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/1711.10069" target="_blank">arXiv:1711.10069</a> [<a href="http://arxiv.org/pdf/1711.10069" target="_blank">pdf</a>]

<h2>Thompson Sampling for Combinatorial Semi-Bandits. (arXiv:1803.04623v4 [cs.LG] UPDATED)</h2>
<h3>Siwei Wang, Wei Chen</h3>
<p>We study the application of the Thompson sampling (TS) methodology to the
stochastic combinatorial multi-armed bandit (CMAB) framework. We analyze the
standard TS algorithm for the general CMAB, and obtain the first
distribution-dependent regret bound of $O(mK_{\max}\log T / \Delta_{\min})$,
where $m$ is the number of arms, $K_{\max}$ is the size of the largest super
arm, $T$ is the time horizon, and $\Delta_{\min}$ is the minimum gap between
the expected reward of the optimal solution and any non-optimal solution. We
also show that one cannot directly replace the exact offline oracle with an
approximation oracle in TS algorithm for even the classical MAB problem. Then
we expand the analysis to two special cases: the linear reward case and the
matroid bandit case. When the reward function is linear, the regret of the TS
algorithm achieves a better bound $O(m\log K_{\max}\log T / \Delta_{\min})$.
For matroid bandit, we could remove the independence assumption across arms and
achieve a regret upper bound that matches the lower bound for the matroid case.
Finally, we use some experiments to show the comparison between regrets of TS
and other existing algorithms like CUCB and ESCB.
</p>
<a href="http://arxiv.org/abs/1803.04623" target="_blank">arXiv:1803.04623</a> [<a href="http://arxiv.org/pdf/1803.04623" target="_blank">pdf</a>]

<h2>On the Effect of Suboptimal Estimation of Mutual Information in Feature Selection and Classification. (arXiv:1804.11021v3 [stat.ML] UPDATED)</h2>
<h3>Kiran Karra, Lamine Mili</h3>
<p>This paper introduces a new property of estimators of the strength of
statistical association, which helps characterize how well an estimator will
perform in scenarios where dependencies between continuous and discrete random
variables need to be rank ordered. The new property, termed the estimator
response curve, is easily computable and provides a marginal distribution
agnostic way to assess an estimator's performance. It overcomes notable
drawbacks of current metrics of assessment, including statistical power, bias,
and consistency. We utilize the estimator response curve to test various
measures of the strength of association that satisfy the data processing
inequality (DPI), and show that the CIM estimator's performance compares
favorably to kNN, vME, AP, and H_{MI} estimators of mutual information. The
estimators which were identified to be suboptimal, according to the estimator
response curve, perform worse than the more optimal estimators when tested with
real-world data from four different areas of science, all with varying
dimensionalities and sizes.
</p>
<a href="http://arxiv.org/abs/1804.11021" target="_blank">arXiv:1804.11021</a> [<a href="http://arxiv.org/pdf/1804.11021" target="_blank">pdf</a>]

<h2>Fast and Robust Matching for Multimodal Remote Sensing Image Registration. (arXiv:1808.06194v6 [cs.CV] UPDATED)</h2>
<h3>Yuanxin Ye, Lorenzo Bruzzone, Jie Shan, Francesca Bovolo, Qing Zhu</h3>
<p>While image registration has been studied in remote sensing community for
decades, registering multimodal data [e.g., optical, light detection and
ranging (LiDAR), synthetic aperture radar (SAR), and map] remains a challenging
problem because of significant nonlinear intensity differences between such
data. To address this problem, this paper presents a fast and robust matching
framework integrating local descriptors for multimodal registration. In the
proposed framework, a local descriptor, such as Histogram of Oriented Gradient
(HOG), Local Self Similarity (LSS), or Speeded-Up Robust Feature (SURF), is
first extracted at each pixel to form a pixel-wise feature representation of an
image. Then we define a similarity measure based on the feature representation
in frequency domain using the 3 Dimensional Fast Fourier Transform (3DFFT)
technique, followed by a template matching scheme to detect control points
between images. In this procedure, we also propose a novel pixel-wise feature
representation using orientated gradients of images, which is named channel
features of orientated gradients (CFOG). This novel feature is an extension of
the pixel-wise HOG descriptors, and outperforms that both in matching
performance and computational efficiency. The major advantage of the proposed
framework includes: (1) structural similarity representation using the
pixel-wise feature description and (2) high computational efficiency due to the
use of 3DFFT. Experimental results on different types of multimodal images show
the superior matching performance of the proposed framework than the
state-of-the-art methods. Moreover, we design an automatic registration system
for very large-size multimodal images (more than 20000*20000 pixels) based on
the proposed framework. Experimental results show the effectiveness of the
designed registration system.The matlab code is available in this manuscript.
</p>
<a href="http://arxiv.org/abs/1808.06194" target="_blank">arXiv:1808.06194</a> [<a href="http://arxiv.org/pdf/1808.06194" target="_blank">pdf</a>]

<h2>Deep Likelihood Network for Image Restoration with Multiple Degradation Levels. (arXiv:1904.09105v4 [cs.CV] UPDATED)</h2>
<h3>Yiwen Guo, Ming Lu, Wangmeng Zuo, Changshui Zhang, Yurong Chen</h3>
<p>Convolutional neural networks have been proven effective in a variety of
image restoration tasks. Most state-of-the-art solutions, however, are trained
using images with a single particular degradation level, and their performance
deteriorates drastically when applied to other degradation settings. In this
paper, we propose deep likelihood network (DL-Net), aiming at generalizing
off-the-shelf image restoration networks to succeed over a spectrum of
degradation levels. We slightly modify an off-the-shelf network by appending a
simple recursive module, which is derived from a fidelity term, for
disentangling the computation for multiple degradation levels. Extensive
experimental results on image inpainting, interpolation, and super-resolution
show the effectiveness of our DL-Net.
</p>
<a href="http://arxiv.org/abs/1904.09105" target="_blank">arXiv:1904.09105</a> [<a href="http://arxiv.org/pdf/1904.09105" target="_blank">pdf</a>]

<h2>Salient Object Detection in the Deep Learning Era: An In-Depth Survey. (arXiv:1904.09146v5 [cs.CV] UPDATED)</h2>
<h3>Wenguan Wang, Qiuxia Lai, Huazhu Fu, Jianbing Shen, Haibin Ling, Ruigang Yang</h3>
<p>As an essential problem in computer vision, salient object detection (SOD)
has attracted an increasing amount of research attention over the years. Recent
advances in SOD are predominantly led by deep learning-based solutions (named
deep SOD). To enable in-depth understanding of deep SOD, in this paper, we
provide a comprehensive survey covering various aspects, ranging from algorithm
taxonomy to unsolved issues. In particular, we first review deep SOD algorithms
from different perspectives, including network architecture, level of
supervision, learning paradigm, and object-/instance-level detection. Following
that, we summarize and analyze existing SOD datasets and evaluation metrics.
Then, we benchmark a large group of representative SOD models, and provide
detailed analyses of the comparison results. Moreover, we study the performance
of SOD algorithms under different attribute settings, which has not been
thoroughly explored previously, by constructing a novel SOD dataset with rich
attribute annotations covering various salient object types, challenging
factors, and scene categories. We further analyze, for the first time in the
field, the robustness of SOD models to random input perturbations and
adversarial attacks. We also look into the generalization and difficulty of
existing SOD datasets. Finally, we discuss several open issues of SOD and
outline future research directions.
</p>
<a href="http://arxiv.org/abs/1904.09146" target="_blank">arXiv:1904.09146</a> [<a href="http://arxiv.org/pdf/1904.09146" target="_blank">pdf</a>]

<h2>Hierarchy Composition GAN for High-fidelity Image Synthesis. (arXiv:1905.04693v3 [cs.CV] UPDATED)</h2>
<h3>Fangneng Zhan, Jiaxing Huang, Shijian Lu</h3>
<p>Despite the rapid progress of generative adversarial networks (GANs) in image
synthesis in recent years, the existing image synthesis approaches work in
either geometry domain or appearance domain alone which often introduces
various synthesis artifacts. This paper presents an innovative Hierarchical
Composition GAN (HIC-GAN) that incorporates image synthesis in geometry and
appearance domains into an end-to-end trainable network and achieves superior
synthesis realism in both domains simultaneously. We design an innovative
hierarchical composition mechanism that is capable of learning realistic
composition geometry and handling occlusions while multiple foreground objects
are involved in image composition. In addition, we introduce a novel attention
mask mechanism that guides to adapt the appearance of foreground objects which
also helps to provide better training reference for learning in geometry
domain. Extensive experiments on scene text image synthesis, portrait editing
and indoor rendering tasks show that the proposed HIC-GAN achieves superior
synthesis performance qualitatively and quantitatively.
</p>
<a href="http://arxiv.org/abs/1905.04693" target="_blank">arXiv:1905.04693</a> [<a href="http://arxiv.org/pdf/1905.04693" target="_blank">pdf</a>]

<h2>Dataset2Vec: Learning Dataset Meta-Features. (arXiv:1905.11063v4 [cs.LG] UPDATED)</h2>
<h3>Hadi S. Jomaa, Lars Schmidt-Thieme, Josif Grabocka</h3>
<p>Meta-learning, or learning to learn, is a machine learning approach that
utilizes prior learning experiences to expedite the learning process on unseen
tasks. As a data-driven approach, meta-learning requires meta-features that
represent the primary learning tasks or datasets, and are estimated
traditonally as engineered dataset statistics that require expert domain
knowledge tailored for every meta-task. In this paper, first, we propose a
meta-feature extractor called Dataset2Vec that combines the versatility of
engineered dataset meta-features with the expressivity of meta-features learned
by deep neural networks. Primary learning tasks or datasets are represented as
hierarchical sets, i.e., as a set of sets, esp. as a set of predictor/target
pairs, and then a DeepSet architecture is employed to regress meta-features on
them. Second, we propose a novel auxiliary meta-learning task with abundant
data called dataset similarity learning that aims to predict if two batches
stem from the same dataset or different ones. In an experiment on a large-scale
hyperparameter optimization task for 120 UCI datasets with varying schemas as a
meta-learning task, we show that the meta-features of Dataset2Vec outperform
the expert engineered meta-features and thus demonstrate the usefulness of
learned meta-features for datasets with varying schemas for the first time.
</p>
<a href="http://arxiv.org/abs/1905.11063" target="_blank">arXiv:1905.11063</a> [<a href="http://arxiv.org/pdf/1905.11063" target="_blank">pdf</a>]

<h2>Fast mixing of Metropolized Hamiltonian Monte Carlo: Benefits of multi-step gradients. (arXiv:1905.12247v3 [stat.ML] UPDATED)</h2>
<h3>Yuansi Chen, Raaz Dwivedi, Martin J. Wainwright, Bin Yu</h3>
<p>Hamiltonian Monte Carlo (HMC) is a state-of-the-art Markov chain Monte Carlo
sampling algorithm for drawing samples from smooth probability densities over
continuous spaces. We study the variant most widely used in practice,
Metropolized HMC with the St\"{o}rmer-Verlet or leapfrog integrator, and make
two primary contributions. First, we provide a non-asymptotic upper bound on
the mixing time of the Metropolized HMC with explicit choices of step-size and
number of leapfrog steps. This bound gives a precise quantification of the
faster convergence of Metropolized HMC relative to simpler MCMC algorithms such
as the Metropolized random walk, or Metropolized Langevin algorithm. Second, we
provide a general framework for sharpening mixing time bounds of Markov chains
initialized at a substantial distance from the target distribution over
continuous spaces. We apply this sharpening device to the Metropolized random
walk and Langevin algorithms, thereby obtaining improved mixing time bounds
from a non-warm initial distribution.
</p>
<a href="http://arxiv.org/abs/1905.12247" target="_blank">arXiv:1905.12247</a> [<a href="http://arxiv.org/pdf/1905.12247" target="_blank">pdf</a>]

<h2>Mean-field Behaviour of Neural Tangent Kernel for Deep Neural Networks. (arXiv:1905.13654v9 [stat.ML] UPDATED)</h2>
<h3>Soufiane Hayou, Arnaud Doucet, Judith Rousseau</h3>
<p>Recent work by Jacot et al. (2018) has shown that training a neural network
of any kind with gradient descent in parameter space is strongly related to
kernel gradient descent in function space with respect to the Neural Tangent
Kernel (NTK). Lee et al. (2019) built on this result by establishing that the
output of a neural network trained using gradient descent can be approximated
by a linear model for wide networks. In parallel, a recent line of studies
(Schoenholz et al. 2017; Hayou et al. 2019) has suggested that a special
initialization, known as the Edge of Chaos, improves training. In this paper,
we bridge the gap between these two concepts by quantifying the impact of the
initialization and the activation function on the NTK when the network depth
becomes large. In particular, we show that the performance of wide deep neural
networks cannot be explained by the NTK regime and we provide experiments
illustrating our theoretical results.
</p>
<a href="http://arxiv.org/abs/1905.13654" target="_blank">arXiv:1905.13654</a> [<a href="http://arxiv.org/pdf/1905.13654" target="_blank">pdf</a>]

<h2>Does Learning Require Memorization? A Short Tale about a Long Tail. (arXiv:1906.05271v4 [cs.LG] UPDATED)</h2>
<h3>Vitaly Feldman</h3>
<p>State-of-the-art results on image recognition tasks are achieved using
over-parameterized learning algorithms that (nearly) perfectly fit the training
set and are known to fit well even random labels. This tendency to memorize the
labels of the training data is not explained by existing theoretical analyses.
Memorization of the training data also presents significant privacy risks when
the training data contains sensitive personal information and thus it is
important to understand whether such memorization is necessary for accurate
learning.

We provide the first conceptual explanation and a theoretical model for this
phenomenon. Specifically, we demonstrate that for natural data distributions
memorization of labels is necessary for achieving close-to-optimal
generalization error. Crucially, even labels of outliers and noisy labels need
to be memorized. The model is motivated and supported by the results of several
recent empirical works. In our model, data is sampled from a mixture of
subpopulations and our results show that memorization is necessary whenever the
distribution of subpopulation frequencies is long-tailed. Image and text data
is known to be long-tailed and therefore our results establish a formal link
between these empirical phenomena. Our results allow to quantify the cost of
limiting memorization in learning and explain the disparate effects that
privacy and model compression have on different subgroups.
</p>
<a href="http://arxiv.org/abs/1906.05271" target="_blank">arXiv:1906.05271</a> [<a href="http://arxiv.org/pdf/1906.05271" target="_blank">pdf</a>]

<h2>A Novel Approach for Robust Multi Human Action Recognition and Summarization based on 3D Convolutional Neural Networks. (arXiv:1907.11272v3 [cs.CV] UPDATED)</h2>
<h3>Noor Almaadeed, Omar Elharrouss, Somaya Al-Maadeed, Ahmed Bouridane, Azeddine Beghdadi</h3>
<p>Human actions in videos are 3D signals. However, there are a few methods
available for multiple human action recognition. For long videos, it's
difficult to search within a video for a specific action and/or person. For
that, this paper proposes a new technic for multiple human action recognition
and summarization for surveillance videos. The proposed approach proposes a new
representation of the data by extracting the sequence of each person from the
scene. This is followed by an analysis of each sequence to detect and recognize
the corresponding actions using 3D convolutional neural networks (3DCNNs).
Action-based video summarization is performed by saving each person's action at
each time of the video. Results of this work revealed that the proposed method
provides accurate multi human action recognition that easily used for
summarization of any action. Further, for other videos that can be collected
from the internet, which are complex and not built for surveillance
applications, the proposed model was evaluated on some datasets like UCF101 and
YouTube without any preprocessing. For this category of videos, the
summarization is performed on the video sequences by summarizing the actions in
each subsequence. The results obtained demonstrate its efficiency compared to
state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/1907.11272" target="_blank">arXiv:1907.11272</a> [<a href="http://arxiv.org/pdf/1907.11272" target="_blank">pdf</a>]

<h2>Comparative Analysis of CNN-based Spatiotemporal Reasoning in Videos. (arXiv:1909.05165v2 [cs.CV] UPDATED)</h2>
<h3>Okan K&#xf6;p&#xfc;kl&#xfc;, Fabian Herzog, Gerhard Rigoll</h3>
<p>Understanding actions and gestures in video streams requires temporal
reasoning of the spatial content from different time instants, i.e.,
spatiotemporal (ST) modeling. In this survey paper, we have made a comparative
analysis of different ST modeling techniques for action and gecture recognition
tasks. Since Convolutional Neural Networks (CNNs) are proved to be an effective
tool as a feature extractor for static images, we apply ST modeling techniques
on the features of static images from different time instants extracted by
CNNs. All techniques are trained end-to-end together with a CNN feature
extraction part and evaluated on two publicly available benchmarks: The Jester
and the Something-Something datasets. The Jester dataset contains various
dynamic and static hand gestures, whereas the Something-Something dataset
contains actions of human-object interactions. The common characteristic of
these two benchmarks is that the designed architectures need to capture the
full temporal content of videos in order to correctly classify
actions/gestures. Contrary to expectations, experimental results show that
Recurrent Neural Network (RNN) based ST modeling techniques yield inferior
results compared to other techniques such as fully convolutional architectures.
Codes and pretrained models of this work are publicly available.
</p>
<a href="http://arxiv.org/abs/1909.05165" target="_blank">arXiv:1909.05165</a> [<a href="http://arxiv.org/pdf/1909.05165" target="_blank">pdf</a>]

<h2>Geomorphological Analysis Using Unpiloted Aircraft Systems, Structure from Motion, and Deep Learning. (arXiv:1909.12874v4 [cs.RO] UPDATED)</h2>
<h3>Zhiang Chen, Tyler R. Scott, Sarah Bearman, Harish Anand, Devin Keating, Chelsea Scott, J Ramon Arrowsmith, Jnaneshwar Das</h3>
<p>We present a pipeline for geomorphological analysis that uses structure from
motion (SfM) and deep learning on close-range aerial imagery to estimate
spatial distributions of rock traits (size, roundness, and orientation) along a
tectonic fault scarp. The properties of the rocks on the fault scarp derive
from the combination of initial volcanic fracturing and subsequent tectonic and
geomorphic fracturing, and our pipeline allows scientists to leverage UAS-based
imagery to gain a better understanding of such surface processes. We start by
using SfM on aerial imagery to produce georeferenced orthomosaics and digital
elevation models (DEM). A human expert then annotates rocks on a set of image
tiles sampled from the orthomosaics, and these annotations are used to train a
deep neural network to detect and segment individual rocks in the entire site.
The extracted semantic information (rock masks) on large volumes of unlabeled,
high-resolution SfM products allows subsequent structural analysis and shape
descriptors to estimate rock size, roundness, and orientation. We present
results of two experiments conducted along a fault scarp in the Volcanic
Tablelands near Bishop, California. We conducted the first, proof-of-concept
experiment with a DJI Phantom 4 Pro equipped with an RGB camera and inspected
if elevation information assisted instance segmentation from RGB channels.
Rock-trait histograms along and across the fault scarp were obtained with the
neural network inference. In the second experiment, we deployed a hexrotor and
a multispectral camera to produce a DEM and five spectral orthomosaics in red,
green, blue, red edge, and near infrared. We focused on examining the
effectiveness of different combinations of input channels in instance
segmentation.
</p>
<a href="http://arxiv.org/abs/1909.12874" target="_blank">arXiv:1909.12874</a> [<a href="http://arxiv.org/pdf/1909.12874" target="_blank">pdf</a>]

<h2>Towards Efficient Local Causal Structure Learning. (arXiv:1910.01288v3 [cs.AI] UPDATED)</h2>
<h3>Shuai Yang, Hao Wang, Kui Yu, Fuyuan Cao, Xindong Wu</h3>
<p>Local causal structure learning aims to discover and distinguish direct
causes (parents) and direct effects (children) of a variable of interest from
data. While emerging successes have been made, existing methods need to search
a large space to distinguish direct causes from direct effects of a target
variable \emph{T}. To tackle this issue, we propose a novel Efficient Local
Causal Structure learning algorithm, named ELCS. Specifically, we first propose
the concept of N-structures, then design an efficient Markov Blanket (MB)
discovery subroutine to integrate MB learning with N-structures to learn the MB
of \emph{T} and simultaneously distinguish direct causes from direct effects of
\emph{T}. With the proposed MB subroutine, ELCS starts from the target
variable, sequentially finds MBs of variables connected to the target variable
and simultaneously constructs local causal structures over MBs until the direct
causes and direct effects of the target variable have been distinguished. Using
eight Bayesian networks the extensive experiments have validated that ELCS
achieves better accuracy and efficiency than the state-of-the-art algorithms.
</p>
<a href="http://arxiv.org/abs/1910.01288" target="_blank">arXiv:1910.01288</a> [<a href="http://arxiv.org/pdf/1910.01288" target="_blank">pdf</a>]

<h2>Quick survey of graph-based fraud detection methods. (arXiv:1910.11299v3 [cs.LG] UPDATED)</h2>
<h3>Paul Irofti, Andrei Patrascu, Andra Baltoiu</h3>
<p>In general, anomaly detection is the problem of distinguishing between normal
data samples with well defined patterns or signatures and those that do not
conform to the expected profiles. Financial transactions, customer reviews,
social media posts are all characterized by relational information. In these
networks, fraudulent behaviour may appear as a distinctive graph edge, such as
spam message, a node or a larger subgraph structure, such as when a group of
clients engage in money laundering schemes. Most commonly, these networks are
represented as attributed graphs, with numerical features complementing
relational information. We present a survey on anomaly detection techniques
used for fraud detection that exploit both the graph structure underlying the
data and the contextual information contained in the attributes.
</p>
<a href="http://arxiv.org/abs/1910.11299" target="_blank">arXiv:1910.11299</a> [<a href="http://arxiv.org/pdf/1910.11299" target="_blank">pdf</a>]

<h2>Kernelized Similarity Learning and Embedding for Dynamic Texture Synthesis. (arXiv:1911.04254v3 [cs.CV] UPDATED)</h2>
<h3>Shiming Chen, Peng Zhang, Qinmu Peng, Zehong Cao, Xinge You</h3>
<p>Dynamic texture (DT) exhibits statistical stationarity in the spatial domain
and stochastic repetitiveness in the temporal dimension, indicating that
different frames of DT possess a high similarity correlation that is critical
prior knowledge. However, existing methods cannot effectively learn a promising
synthesis model for high-dimensional DT from a small number of training data.
In this paper, we propose a novel DT synthesis method, which makes full use of
similarity prior knowledge to address this issue. Our method bases on the
proposed kernel similarity embedding, which not only can mitigate the
high-dimensionality and small sample issues, but also has the advantage of
modeling nonlinear feature relationship. Specifically, we first raise two
hypotheses that are essential for DT model to generate new frames using
similarity correlation. Then, we integrate kernel learning and extreme learning
machine into a unified synthesis model to learn kernel similarity embedding for
representing DT. Extensive experiments on DT videos collected from the internet
and two benchmark datasets, i.e., Gatech Graphcut Textures and Dyntex,
demonstrate that the learned kernel similarity embedding can effectively
exhibit the discriminative representation for DT. Accordingly, our method is
capable of preserving the long-term temporal continuity of the synthesized DT
sequences with excellent sustainability and generalization. Meanwhile, it
effectively generates realistic DT videos with fast speed and low computation,
compared with the state-of-the-art methods. The code and more synthesis videos
are available at our project page
https://shiming-chen.github.io/Similarity-page/Similarit.html.
</p>
<a href="http://arxiv.org/abs/1911.04254" target="_blank">arXiv:1911.04254</a> [<a href="http://arxiv.org/pdf/1911.04254" target="_blank">pdf</a>]

<h2>Rate-Regularization and Generalization in VAEs. (arXiv:1911.04594v3 [cs.LG] UPDATED)</h2>
<h3>Alican Bozkurt, Babak Esmaeili, Jean-Baptiste Tristan, Dana H. Brooks, Jennifer G. Dy, Jan-Willem van de Meent</h3>
<p>Variational autoencoders (VAEs) optimize an objective that comprises a
reconstruction loss (the distortion) and a KL term (the rate). The rate is an
upper bound on the mutual information, which is often interpreted as a
regularizer that controls the degree of compression. We here examine whether
inclusion of the rate term also improves generalization. We perform
rate-distortion analyses in which we control the strength of the rate term, the
network capacity, and the difficulty of the generalization problem. Lowering
the strength of the rate term paradoxically improves generalization in most
settings, and reducing the mutual information typically leads to underfitting.
Moreover, we show that generalization performance continues to improve even
after the mutual information saturates, indicating that the gap on the bound
(i.e. the KL divergence relative to the inference marginal) affects
generalization. This suggests that the standard spherical Gaussian prior is not
an inductive bias that typically improves generalization, prompting further
work to understand what choices of priors improve generalization in VAEs.
</p>
<a href="http://arxiv.org/abs/1911.04594" target="_blank">arXiv:1911.04594</a> [<a href="http://arxiv.org/pdf/1911.04594" target="_blank">pdf</a>]

<h2>Attention-Privileged Reinforcement Learning. (arXiv:1911.08363v3 [cs.AI] UPDATED)</h2>
<h3>Sasha Salter, Dushyant Rao, Markus Wulfmeier, Raia Hadsell, Ingmar Posner</h3>
<p>Image-based Reinforcement Learning is known to suffer from poor sample
efficiency and generalisation to unseen visuals such as distractors
(task-independent aspects of the observation space). Visual domain
randomisation encourages transfer by training over visual factors of variation
that may be encountered in the target domain. This increases learning
complexity, can negatively impact learning rate and performance, and requires
knowledge of potential variations during deployment. In this paper, we
introduce Attention-Privileged Reinforcement Learning (APRiL) which uses a
self-supervised attention mechanism to significantly alleviate these drawbacks:
by focusing on task-relevant aspects of the observations, attention provides
robustness to distractors as well as significantly increased learning
efficiency. APRiL trains two attention-augmented actor-critic agents: one
purely based on image observations, available across training and transfer
domains; and one with access to privileged information (such as environment
states) available only during training. Experience is shared between both
agents and their attention mechanisms are aligned. The image-based policy can
then be deployed without access to privileged information. We experimentally
demonstrate accelerated and more robust learning on a diverse set of domains,
leading to improved final performance for environments both within and outside
the training distribution.
</p>
<a href="http://arxiv.org/abs/1911.08363" target="_blank">arXiv:1911.08363</a> [<a href="http://arxiv.org/pdf/1911.08363" target="_blank">pdf</a>]

<h2>Spatial-Aware GAN for Unsupervised Person Re-identification. (arXiv:1911.11312v2 [cs.CV] UPDATED)</h2>
<h3>Fangneng Zhan, Changgong Zhang</h3>
<p>The recent person re-identification research has achieved great success by
learning from a large number of labeled person images. On the other hand, the
learned models often experience significant performance drops when applied to
images collected in a different environment. Unsupervised domain adaptation
(UDA) has been investigated to mitigate this constraint, but most existing
systems adapt images at pixel level only and ignore obvious discrepancies at
spatial level. This paper presents an innovative UDA-based person
re-identification network that is capable of adapting images at both spatial
and pixel levels simultaneously. A novel disentangled cycle-consistency loss is
designed which guides the learning of spatial-level and pixel-level adaptation
in a collaborative manner. In addition, a novel multi-modal mechanism is
incorporated which is capable of generating images of different geometry views
and augmenting training images effectively. Extensive experiments over a number
of public datasets show that the proposed UDA network achieves superior person
re-identification performance as compared with the state-of-the-art.
</p>
<a href="http://arxiv.org/abs/1911.11312" target="_blank">arXiv:1911.11312</a> [<a href="http://arxiv.org/pdf/1911.11312" target="_blank">pdf</a>]

<h2>Winning the Lottery with Continuous Sparsification. (arXiv:1912.04427v4 [cs.LG] UPDATED)</h2>
<h3>Pedro Savarese, Hugo Silva, Michael Maire</h3>
<p>The search for efficient, sparse deep neural network models is most
prominently performed by pruning: training a dense, overparameterized network
and removing parameters, usually via following a manually-crafted heuristic.
Additionally, the recent Lottery Ticket Hypothesis conjectures that, for a
typically-sized neural network, it is possible to find small sub-networks
which, when trained from scratch on a comparable budget, match the performance
of the original dense counterpart. We revisit fundamental aspects of pruning
algorithms, pointing out missing ingredients in previous approaches, and
develop a method, Continuous Sparsification, which searches for sparse networks
based on a novel approximation of an intractable $\ell_0$ regularization. We
compare against dominant heuristic-based methods on pruning as well as ticket
search -- finding sparse subnetworks that can be successfully re-trained from
an early iterate. Empirical results show that we surpass the state-of-the-art
for both objectives, across models and datasets, including VGG trained on
CIFAR-10 and ResNet-50 trained on ImageNet. In addition to setting a new
standard for pruning, Continuous Sparsification also offers fast parallel
ticket search, opening doors to new applications of the Lottery Ticket
Hypothesis.
</p>
<a href="http://arxiv.org/abs/1912.04427" target="_blank">arXiv:1912.04427</a> [<a href="http://arxiv.org/pdf/1912.04427" target="_blank">pdf</a>]

<h2>Deep Attention Based Semi-Supervised 2D-Pose Estimation for Surgical Instruments. (arXiv:1912.04618v2 [cs.CV] UPDATED)</h2>
<h3>Mert Kayhan, Okan K&#xf6;p&#xfc;kl&#xfc;, Mhd Hasan Sarhan, Mehmet Yigitsoy, Abouzar Eslami, Gerhard Rigoll</h3>
<p>For many practical problems and applications, it is not feasible to create a
vast and accurately labeled dataset, which restricts the application of deep
learning in many areas. Semi-supervised learning algorithms intend to improve
performance by also leveraging unlabeled data. This is very valuable for
2D-pose estimation task where data labeling requires substantial time and is
subject to noise. This work aims to investigate if semi-supervised learning
techniques can achieve acceptable performance level that makes using these
algorithms during training justifiable. To this end, a lightweight network
architecture is introduced and mean teacher, virtual adversarial training and
pseudo-labeling algorithms are evaluated on 2D-pose estimation for surgical
instruments. For the applicability of pseudo-labelling algorithm, we propose a
novel confidence measure, total variation. Experimental results show that
utilization of semi-supervised learning improves the performance on unseen
geometries drastically while maintaining high accuracy for seen geometries. For
RMIT benchmark, our lightweight architecture outperforms state-of-the-art with
supervised learning. For Endovis benchmark, pseudo-labelling algorithm improves
the supervised baseline achieving the new state-of-the-art performance.
</p>
<a href="http://arxiv.org/abs/1912.04618" target="_blank">arXiv:1912.04618</a> [<a href="http://arxiv.org/pdf/1912.04618" target="_blank">pdf</a>]

<h2>Image Classification with Deep Learning in the Presence of Noisy Labels: A Survey. (arXiv:1912.05170v3 [cs.LG] UPDATED)</h2>
<h3>G&#xf6;rkem Algan, Ilkay Ulusoy</h3>
<p>Image classification systems recently made a giant leap with the advancement
of deep neural networks. However, these systems require an excessive amount of
labeled data to be adequately trained. Gathering a correctly annotated dataset
is not always feasible due to several factors, such as the expensiveness of the
labeling process or difficulty of correctly classifying data, even for the
experts. Because of these practical challenges, label noise is a common problem
in real-world datasets, and numerous methods to train deep neural networks with
label noise are proposed in the literature. Although deep neural networks are
known to be relatively robust to label noise, their tendency to overfit data
makes them vulnerable to memorizing even random noise. Therefore, it is crucial
to consider the existence of label noise and develop counter algorithms to fade
away its adverse effects to train deep neural networks efficiently. Even though
an extensive survey of machine learning techniques under label noise exists,
the literature lacks a comprehensive survey of methodologies centered
explicitly around deep learning in the presence of noisy labels. This paper
aims to present these algorithms while categorizing them into one of the two
subgroups: noise model based and noise model free methods. Algorithms in the
first group aim to estimate the noise structure and use this information to
avoid the adverse effects of noisy labels. Differently, methods in the second
group try to come up with inherently noise robust algorithms by using
approaches like robust losses, regularizers or other learning paradigms.
</p>
<a href="http://arxiv.org/abs/1912.05170" target="_blank">arXiv:1912.05170</a> [<a href="http://arxiv.org/pdf/1912.05170" target="_blank">pdf</a>]

<h2>Attack-Resistant Federated Learning with Residual-based Reweighting. (arXiv:1912.11464v3 [cs.LG] UPDATED)</h2>
<h3>Shuhao Fu, Chulin Xie, Bo Li, Qifeng Chen</h3>
<p>Federated learning has a variety of applications in multiple domains by
utilizing private training data stored on different devices. However, the
aggregation process in federated learning is highly vulnerable to adversarial
attacks so that the global model may behave abnormally under attacks. To tackle
this challenge, we present a novel aggregation algorithm with residual-based
reweighting to defend federated learning. Our aggregation algorithm combines
repeated median regression with the reweighting scheme in iteratively
reweighted least squares. Our experiments show that our aggregation algorithm
outperforms other alternative algorithms in the presence of label-flipping and
backdoor attacks. We also provide theoretical analysis for our aggregation
algorithm.
</p>
<a href="http://arxiv.org/abs/1912.11464" target="_blank">arXiv:1912.11464</a> [<a href="http://arxiv.org/pdf/1912.11464" target="_blank">pdf</a>]

<h2>Inference for Batched Bandits. (arXiv:2002.03217v3 [cs.LG] UPDATED)</h2>
<h3>Kelly W. Zhang, Lucas Janson, Susan A. Murphy</h3>
<p>As bandit algorithms are increasingly utilized in scientific studies and
industrial applications, there is an associated increasing need for reliable
inference methods based on the resulting adaptively-collected data. In this
work, we develop methods for inference on data collected in batches using a
bandit algorithm. We first prove that the ordinary least squares estimator
(OLS), which is asymptotically normal on independently sampled data, is not
asymptotically normal on data collected using standard bandit algorithms when
there is no unique optimal arm. This asymptotic non-normality result implies
that the naive assumption that the OLS estimator is approximately normal can
lead to Type-1 error inflation and confidence intervals with below-nominal
coverage probabilities. Second, we introduce the Batched OLS estimator (BOLS)
that we prove is (1) asymptotically normal on data collected from both
multi-arm and contextual bandits and (2) robust to non-stationarity in the
baseline reward.
</p>
<a href="http://arxiv.org/abs/2002.03217" target="_blank">arXiv:2002.03217</a> [<a href="http://arxiv.org/pdf/2002.03217" target="_blank">pdf</a>]

<h2>Resource Management in Wireless Networks via Multi-Agent Deep Reinforcement Learning. (arXiv:2002.06215v2 [cs.LG] UPDATED)</h2>
<h3>Navid Naderializadeh, Jaroslaw Sydir, Meryem Simsek, Hosein Nikopour</h3>
<p>We propose a mechanism for distributed resource management and interference
mitigation in wireless networks using multi-agent deep reinforcement learning
(RL). We equip each transmitter in the network with a deep RL agent that
receives delayed observations from its associated users, while also exchanging
observations with its neighboring agents, and decides on which user to serve
and what transmit power to use at each scheduling interval. Our proposed
framework enables agents to make decisions simultaneously and in a distributed
manner, unaware of the concurrent decisions of other agents. Moreover, our
design of the agents' observation and action spaces is scalable, in the sense
that an agent trained on a scenario with a specific number of transmitters and
users can be applied to scenarios with different numbers of transmitters and/or
users. Simulation results demonstrate the superiority of our proposed approach
compared to decentralized baselines in terms of the tradeoff between average
and $5^{th}$ percentile user rates, while achieving performance close to, and
even in certain cases outperforming, that of a centralized
information-theoretic baseline. We also show that our trained agents are robust
and maintain their performance gains when experiencing mismatches between train
and test deployments.
</p>
<a href="http://arxiv.org/abs/2002.06215" target="_blank">arXiv:2002.06215</a> [<a href="http://arxiv.org/pdf/2002.06215" target="_blank">pdf</a>]

<h2>Dissecting Neural ODEs. (arXiv:2002.08071v4 [cs.LG] UPDATED)</h2>
<h3>Stefano Massaroli, Michael Poli, Jinkyoo Park, Atsushi Yamashita, Hajime Asama</h3>
<p>Continuous deep learning architectures have recently re-emerged as Neural
Ordinary Differential Equations (Neural ODEs). This infinite-depth approach
theoretically bridges the gap between deep learning and dynamical systems,
offering a novel perspective. However, deciphering the inner working of these
models is still an open challenge, as most applications apply them as generic
black-box modules. In this work we "open the box", further developing the
continuous-depth formulation with the aim of clarifying the influence of
several design choices on the underlying dynamics.
</p>
<a href="http://arxiv.org/abs/2002.08071" target="_blank">arXiv:2002.08071</a> [<a href="http://arxiv.org/pdf/2002.08071" target="_blank">pdf</a>]

<h2>Exactly Computing the Local Lipschitz Constant of ReLU Networks. (arXiv:2003.01219v2 [stat.ML] UPDATED)</h2>
<h3>Matt Jordan, Alexandros G. Dimakis</h3>
<p>The local Lipschitz constant of a neural network is a useful metric with
applications in robustness, generalization, and fairness evaluation. We provide
novel analytic results relating the local Lipschitz constant of nonsmooth
vector-valued functions to a maximization over the norm of the generalized
Jacobian. We present a sufficient condition for which backpropagation always
returns an element of the generalized Jacobian, and reframe the problem over
this broad class of functions. We show strong inapproximability results for
estimating Lipschitz constants of ReLU networks, and then formulate an
algorithm to compute these quantities exactly. We leverage this algorithm to
evaluate the tightness of competing Lipschitz estimators and the effects of
regularized training on the Lipschitz constant.
</p>
<a href="http://arxiv.org/abs/2003.01219" target="_blank">arXiv:2003.01219</a> [<a href="http://arxiv.org/pdf/2003.01219" target="_blank">pdf</a>]

<h2>Graphs, Convolutions, and Neural Networks. (arXiv:2003.03777v4 [cs.LG] UPDATED)</h2>
<h3>Fernando Gama, Elvin Isufi, Geert Leus, Alejandro Ribeiro</h3>
<p>Network data can be conveniently modeled as a graph signal, where data values
are assigned to nodes of a graph that describes the underlying network
topology. Successful learning from network data is built upon methods that
effectively exploit this graph structure. In this work, we leverage graph
signal processing to characterize the representation space of graph neural
networks (GNNs). We discuss the role of graph convolutional filters in GNNs and
show that any architecture built with such filters has the fundamental
properties of permutation equivariance and stability to changes in the
topology. These two properties offer insight about the workings of GNNs and
help explain their scalability and transferability properties which, coupled
with their local and distributed nature, make GNNs powerful tools for learning
in physical networks. We also introduce GNN extensions using edge-varying and
autoregressive moving average graph filters and discuss their properties.
Finally, we study the use of GNNs in recommender systems and learning
decentralized controllers for robot swarms.
</p>
<a href="http://arxiv.org/abs/2003.03777" target="_blank">arXiv:2003.03777</a> [<a href="http://arxiv.org/pdf/2003.03777" target="_blank">pdf</a>]

<h2>Learning Discrete State Abstractions With Deep Variational Inference. (arXiv:2003.04300v3 [cs.LG] UPDATED)</h2>
<h3>Ondrej Biza, Robert Platt, Jan-Willem van de Meent, Lawson L. S. Wong</h3>
<p>Abstraction is crucial for effective sequential decision making in domains
with large state spaces. In this work, we propose an information bottleneck
method for learning approximate bisimulations, a type of state abstraction. We
use a deep neural encoder to map states onto continuous embeddings. We map
these embeddings onto a discrete representation using an action-conditioned
hidden Markov model, which is trained end-to-end with the neural network. Our
method is suited for environments with high-dimensional states and learns from
a stream of experience collected by an agent acting in a Markov decision
process. Through this learned discrete abstract model, we can efficiently plan
for unseen goals in a multi-goal Reinforcement Learning setting. We test our
method in simplified robotic manipulation domains with image states. We also
compare it against previous model-based approaches to finding bisimulations in
discrete grid-world-like environments. Source code is available at
https://github.com/ondrejba/discrete_abstractions.
</p>
<a href="http://arxiv.org/abs/2003.04300" target="_blank">arXiv:2003.04300</a> [<a href="http://arxiv.org/pdf/2003.04300" target="_blank">pdf</a>]

<h2>HEMlets PoSh: Learning Part-Centric Heatmap Triplets for 3D Human Pose and Shape Estimation. (arXiv:2003.04894v2 [cs.CV] UPDATED)</h2>
<h3>Kun Zhou, Xiaoguang Han, Nianjuan Jiang, Kui Jia, Jiangbo Lu</h3>
<p>Estimating 3D human pose from a single image is a challenging task. This work
attempts to address the uncertainty of lifting the detected 2D joints to the 3D
space by introducing an intermediate state-Part-Centric Heatmap Triplets
(HEMlets), which shortens the gap between the 2D observation and the 3D
interpretation. The HEMlets utilize three joint-heatmaps to represent the
relative depth information of the end-joints for each skeletal body part. In
our approach, a Convolutional Network (ConvNet) is first trained to predict
HEMlets from the input image, followed by a volumetric joint-heatmap
regression. We leverage on the integral operation to extract the joint
locations from the volumetric heatmaps, guaranteeing end-to-end learning.
Despite the simplicity of the network design, the quantitative comparisons show
a significant performance improvement over the best-of-grade methods (e.g.
$20\%$ on Human3.6M). The proposed method naturally supports training with
"in-the-wild" images, where only weakly-annotated relative depth information of
skeletal joints is available. This further improves the generalization ability
of our model, as validated by qualitative comparisons on outdoor images.
Leveraging the strength of the HEMlets pose estimation, we further design and
append a shallow yet effective network module to regress the SMPL parameters of
the body pose and shape. We term the entire HEMlets-based human pose and shape
recovery pipeline HEMlets PoSh. Extensive quantitative and qualitative
experiments on the existing human body recovery benchmarks justify the
state-of-the-art results obtained with our HEMlets PoSh approach.
</p>
<a href="http://arxiv.org/abs/2003.04894" target="_blank">arXiv:2003.04894</a> [<a href="http://arxiv.org/pdf/2003.04894" target="_blank">pdf</a>]

<h2>Coronary Artery Segmentation from Intravascular Optical Coherence Tomography Using Deep Capsules. (arXiv:2003.06080v3 [cs.LG] UPDATED)</h2>
<h3>Arjun Balaji, Lachlan Kelsey, Kamran Majeed, Carl Schultz, Barry Doyle</h3>
<p>The segmentation and analysis of coronary arteries from intravascular optical
coherence tomography (IVOCT) is an important aspect of diagnosing and managing
coronary artery disease. Current image processing methods are hindered by the
time needed to generate expert-labelled datasets and the potential for bias
during the analysis. Therefore, automated, robust, unbiased and timely geometry
extraction from IVOCT, using image processing, would be beneficial to
clinicians. With clinical application in mind, we aim to develop a model with a
small memory footprint that is fast at inference time without sacrificing
segmentation quality. Using a large IVOCT dataset of 12,011 expert-labelled
images we construct a new deep learning method based on capsules which
automatically produces lumen segmentations. Our dataset contains images with
both blood and light artefacts (22.8%), as well as metallic (23.1%) and
bioresorbable stents (2.5%). We split the dataset into a training (70%),
validation (20%) and test (10%) set and rigorously investigate design
variations with respect to upsampling regimes and input selection. We show that
our developments lead to a model, DeepCap, that is on par with state-of-the-art
machine learning methods in terms of segmentation quality and robustness, while
using as little as 12% of the parameters. This enables DeepCap to have per
image inference times up to 70% faster on GPU and up to 95% faster on CPU
compared to other state-of-the-art models. DeepCap is a robust automated
segmentation tool that can aid clinicians to extract unbiased geometrical data
from IVOCT.
</p>
<a href="http://arxiv.org/abs/2003.06080" target="_blank">arXiv:2003.06080</a> [<a href="http://arxiv.org/pdf/2003.06080" target="_blank">pdf</a>]

<h2>BoostTree and BoostForest for Ensemble Learning. (arXiv:2003.09737v2 [cs.LG] UPDATED)</h2>
<h3>Changming Zhao, Dongrui Wu, Jian Huang, Ye Yuan, Hai-Tao Zhang, Ruimin Peng, Zhenhua Shi, Chenfeng Guo</h3>
<p>Bootstrap aggregating (Bagging) and boosting are two popular ensemble
learning approaches, which combine multiple base learners to generate a
composite model for more accurate and more reliable performance. They have been
widely used in biology, engineering, healthcare, etc. This article proposes
BoostForest, which is an ensemble learning approach using BoostTree as base
learners and can be used for both classification and regression. BoostTree
constructs a tree model by gradient boosting. It achieves high randomness
(diversity) by sampling its parameters randomly from a parameter pool, and
selecting a subset of features randomly at node splitting. BoostForest further
increases the randomness by bootstrapping the training data in constructing
different BoostTrees. BoostForest outperformed four classical ensemble learning
approaches (Random Forest, Extra-Trees, XGBoost and LightGBM) on 34
classification and regression datasets. Remarkably, BoostForest has only one
hyper-parameter (the number of BoostTrees), which can be easily specified. Our
code is publicly available, and the proposed ensemble learning framework can
also be used to combine many other base learners.
</p>
<a href="http://arxiv.org/abs/2003.09737" target="_blank">arXiv:2003.09737</a> [<a href="http://arxiv.org/pdf/2003.09737" target="_blank">pdf</a>]

<h2>Distilling Knowledge from Graph Convolutional Networks. (arXiv:2003.10477v4 [cs.CV] UPDATED)</h2>
<h3>Yiding Yang, Jiayan Qiu, Mingli Song, Dacheng Tao, Xinchao Wang</h3>
<p>Existing knowledge distillation methods focus on convolutional neural
networks (CNNs), where the input samples like images lie in a grid domain, and
have largely overlooked graph convolutional networks (GCN) that handle non-grid
data. In this paper, we propose to our best knowledge the first dedicated
approach to distilling knowledge from a pre-trained GCN model. To enable the
knowledge transfer from the teacher GCN to the student, we propose a local
structure preserving module that explicitly accounts for the topological
semantics of the teacher. In this module, the local structure information from
both the teacher and the student are extracted as distributions, and hence
minimizing the distance between these distributions enables topology-aware
knowledge transfer from the teacher, yielding a compact yet high-performance
student model. Moreover, the proposed approach is readily extendable to dynamic
graph models, where the input graphs for the teacher and the student may
differ. We evaluate the proposed method on two different datasets using GCN
models of different architectures, and demonstrate that our method achieves the
state-of-the-art knowledge distillation performance for GCN models. Code is
publicly available at https://github.com/ihollywhy/DistillGCN.PyTorch.
</p>
<a href="http://arxiv.org/abs/2003.10477" target="_blank">arXiv:2003.10477</a> [<a href="http://arxiv.org/pdf/2003.10477" target="_blank">pdf</a>]

<h2>Physics-informed learning of governing equations from scarce data. (arXiv:2005.03448v2 [cs.LG] UPDATED)</h2>
<h3>Zhao Chen, Yang Liu, Hao Sun</h3>
<p>Harnessing data to discover the underlying governing laws or equations that
describe the behavior of complex physical systems can significantly advance our
modeling, simulation and understanding of such systems in various science and
engineering disciplines. This work introduces a novel physics-informed deep
learning framework to discover governing partial differential equations (PDEs)
from scarce and noisy data for nonlinear spatiotemporal systems. In particular,
this approach seamlessly integrates the strengths of deep neural networks for
rich representation learning, physics embedding, automatic differentiation and
sparse regression to (1) approximate the solution of system variables, (2)
compute essential derivatives, as well as (3) identify the key derivative terms
and parameters that form the structure and explicit expression of the PDEs. The
efficacy and robustness of this method are demonstrated, both numerically and
experimentally, on discovering a variety of PDE systems with different levels
of data scarcity and noise accounting for different initial/boundary
conditions. The resulting computational framework shows the potential for
closed-form model discovery in practical applications where large and accurate
datasets are intractable to capture.
</p>
<a href="http://arxiv.org/abs/2005.03448" target="_blank">arXiv:2005.03448</a> [<a href="http://arxiv.org/pdf/2005.03448" target="_blank">pdf</a>]

<h2>C3VQG: Category Consistent Cyclic Visual Question Generation. (arXiv:2005.07771v5 [cs.CV] UPDATED)</h2>
<h3>Shagun Uppal, Anish Madan, Sarthak Bhagat, Yi Yu, Rajiv Ratn Shah</h3>
<p>Visual Question Generation (VQG) is the task of generating natural questions
based on an image. Popular methods in the past have explored image-to-sequence
architectures trained with maximum likelihood which have demonstrated
meaningful generated questions given an image and its associated ground-truth
answer. VQG becomes more challenging if the image contains rich contextual
information describing its different semantic categories. In this paper, we try
to exploit the different visual cues and concepts in an image to generate
questions using a variational autoencoder (VAE) without ground-truth answers.
Our approach solves two major shortcomings of existing VQG systems: (i)
minimize the level of supervision and (ii) replace generic questions with
category relevant generations. Most importantly, by eliminating expensive
answer annotations, the required supervision is weakened. Using different
categories enables us to exploit different concepts as the inference requires
only the image and the category. Mutual information is maximized between the
image, question, and answer category in the latent space of our VAE. A novel
category consistent cyclic loss is proposed to enable the model to generate
consistent predictions with respect to the answer category, reducing
redundancies and irregularities. Additionally, we also impose supplementary
constraints on the latent space of our generative model to provide structure
based on categories and enhance generalization by encapsulating decorrelated
features within each dimension. Through extensive experiments, the proposed
model, C3VQG outperforms state-of-the-art VQG methods with weak supervision.
</p>
<a href="http://arxiv.org/abs/2005.07771" target="_blank">arXiv:2005.07771</a> [<a href="http://arxiv.org/pdf/2005.07771" target="_blank">pdf</a>]

<h2>Understanding the Message Passing in Graph Neural Networks via Power Iteration Clustering. (arXiv:2006.00144v3 [cs.LG] UPDATED)</h2>
<h3>Xue Li, Yuanzhi Cheng</h3>
<p>The mechanism of message passing in graph neural networks (GNNs) is still
mysterious. Apart from convolutional neural networks, no theoretical origin for
GNNs has been proposed. To our surprise, message passing can be best understood
in terms of power iteration. By fully or partly removing activation functions
and layer weights of GNNs, we propose subspace power iteration clustering
(SPIC) models that iteratively learn with only one aggregator. Experiments show
that our models extend GNNs and enhance their capability to process random
featured networks. Moreover, we demonstrate the redundancy of some
state-of-the-art GNNs in design and define a lower limit for model evaluation
by a random aggregator of message passing. Our findings push the boundaries of
the theoretical understanding of neural networks.
</p>
<a href="http://arxiv.org/abs/2006.00144" target="_blank">arXiv:2006.00144</a> [<a href="http://arxiv.org/pdf/2006.00144" target="_blank">pdf</a>]

<h2>A Variational Approach to Privacy and Fairness. (arXiv:2006.06332v2 [stat.ML] UPDATED)</h2>
<h3>Borja Rodr&#xed;guez-G&#xe1;lvez, Ragnar Thobaben, Mikael Skoglund</h3>
<p>In this article, we propose a new variational approach to learn private
and/or fair representations. This approach is based on the Lagrangians of a new
formulation of the privacy and fairness optimization problems that we propose.
In this formulation, we aim at generating representations of the data that keep
a prescribed level of the relevant information that is not shared by the
private or sensitive data, while minimizing the remaining information they
keep. The proposed approach (i) exhibits the similarities of the privacy and
fairness problems, (ii) allows us to control the trade-off between utility and
privacy or fairness through the Lagrange multiplier parameter, and (iii) can be
comfortably incorporated to common representation learning algorithms such as
the VAE, the $\beta$-VAE, the VIB, or the nonlinear IB.
</p>
<a href="http://arxiv.org/abs/2006.06332" target="_blank">arXiv:2006.06332</a> [<a href="http://arxiv.org/pdf/2006.06332" target="_blank">pdf</a>]

<h2>An Unsupervised Information-Theoretic Perceptual Quality Metric. (arXiv:2006.06752v3 [cs.CV] UPDATED)</h2>
<h3>Sangnie Bhardwaj, Ian Fischer, Johannes Ball&#xe9;, Troy Chinen</h3>
<p>Tractable models of human perception have proved to be challenging to build.
Hand-designed models such as MS-SSIM remain popular predictors of human image
quality judgements due to their simplicity and speed. Recent modern deep
learning approaches can perform better, but they rely on supervised data which
can be costly to gather: large sets of class labels such as ImageNet, image
quality ratings, or both. We combine recent advances in information-theoretic
objective functions with a computational architecture informed by the
physiology of the human visual system and unsupervised training on pairs of
video frames, yielding our Perceptual Information Metric (PIM). We show that
PIM is competitive with supervised metrics on the recent and challenging BAPPS
image quality assessment dataset and outperforms them in predicting the ranking
of image compression methods in CLIC 2020. We also perform qualitative
experiments using the ImageNet-C dataset, and establish that PIM is robust with
respect to architectural details.
</p>
<a href="http://arxiv.org/abs/2006.06752" target="_blank">arXiv:2006.06752</a> [<a href="http://arxiv.org/pdf/2006.06752" target="_blank">pdf</a>]

<h2>Graph Meta Learning via Local Subgraphs. (arXiv:2006.07889v4 [cs.LG] UPDATED)</h2>
<h3>Kexin Huang, Marinka Zitnik</h3>
<p>Prevailing methods for graphs require abundant label and edge information for
learning. When data for a new task are scarce, meta-learning can learn from
prior experiences and form much-needed inductive biases for fast adaption to
new tasks. Here, we introduce G-Meta, a novel meta-learning algorithm for
graphs. G-Meta uses local subgraphs to transfer subgraph-specific information
and learn transferable knowledge faster via meta gradients. G-Meta learns how
to quickly adapt to a new task using only a handful of nodes or edges in the
new task and does so by learning from data points in other graphs or related,
albeit disjoint label sets. G-Meta is theoretically justified as we show that
the evidence for a prediction can be found in the local subgraph surrounding
the target node or edge. Experiments on seven datasets and nine baseline
methods show that G-Meta outperforms existing methods by up to 16.3%. Unlike
previous methods, G-Meta successfully learns in challenging, few-shot learning
settings that require generalization to completely new graphs and
never-before-seen labels. Finally, G-Meta scales to large graphs, which we
demonstrate on a new Tree-of-Life dataset comprising of 1,840 graphs, a
two-orders of magnitude increase in the number of graphs used in prior work.
</p>
<a href="http://arxiv.org/abs/2006.07889" target="_blank">arXiv:2006.07889</a> [<a href="http://arxiv.org/pdf/2006.07889" target="_blank">pdf</a>]

<h2>Parameterized MDPs and Reinforcement Learning Problems -- A Maximum Entropy Principle Based Framework. (arXiv:2006.09646v2 [cs.LG] UPDATED)</h2>
<h3>Amber Srivastava, Srinivasa M Salapaka</h3>
<p>We present a framework to address a class of sequential decision making
problems. Our framework features learning the optimal control policy with
robustness to noisy data, determining the unknown state and action parameters,
and performing sensitivity analysis with respect to problem parameters. We
consider two broad categories of sequential decision making problems modelled
as infinite horizon Markov Decision Processes (MDPs) with (and without) an
absorbing state. The central idea underlying our framework is to quantify
exploration in terms of the Shannon Entropy of the trajectories under the MDP
and determine the stochastic policy that maximizes it while guaranteeing a low
value of the expected cost along a trajectory. This resulting policy enhances
the quality of exploration early on in the learning process, and consequently
allows faster convergence rates and robust solutions even in the presence of
noisy data as demonstrated in our comparisons to popular algorithms such as
Q-learning, Double Q-learning and entropy regularized Soft Q-learning. The
framework extends to the class of parameterized MDP and RL problems, where
states and actions are parameter dependent, and the objective is to determine
the optimal parameters along with the corresponding optimal policy. Here, the
associated cost function can possibly be non-convex with multiple poor local
minima. Simulation results applied to a 5G small cell network problem
demonstrate successful determination of communication routes and the small cell
locations. We also obtain sensitivity measures to problem parameters and
robustness to noisy environment data.
</p>
<a href="http://arxiv.org/abs/2006.09646" target="_blank">arXiv:2006.09646</a> [<a href="http://arxiv.org/pdf/2006.09646" target="_blank">pdf</a>]

<h2>On the Role of Sparsity and DAG Constraints for Learning Linear DAGs. (arXiv:2006.10201v3 [cs.LG] UPDATED)</h2>
<h3>Ignavier Ng, AmirEmad Ghassami, Kun Zhang</h3>
<p>Learning graphical structures based on Directed Acyclic Graphs (DAGs) is a
challenging problem, partly owing to the large search space of possible graphs.
A recent line of work formulates the structure learning problem as a continuous
constrained optimization task using the least squares objective and an
algebraic characterization of DAGs. However, the formulation requires a hard
DAG constraint and may lead to optimization difficulties. In this paper, we
study the asymptotic role of the sparsity and DAG constraints for learning DAG
models in the linear Gaussian and non-Gaussian cases, and investigate their
usefulness in the finite sample regime. Based on the theoretical results, we
formulate a likelihood-based score function, and show that one only has to
apply soft sparsity and DAG constraints to learn a DAG equivalent to the ground
truth DAG. This leads to an unconstrained optimization problem that is much
easier to solve. Using gradient-based optimization and GPU acceleration, our
procedure can easily handle thousands of nodes while retaining a high accuracy.
Extensive experiments validate the effectiveness of our proposed method and
show that the DAG-penalized likelihood objective is indeed favorable over the
least squares one with the hard DAG constraint.
</p>
<a href="http://arxiv.org/abs/2006.10201" target="_blank">arXiv:2006.10201</a> [<a href="http://arxiv.org/pdf/2006.10201" target="_blank">pdf</a>]

<h2>Ultrahyperbolic Representation Learning. (arXiv:2007.00211v5 [cs.LG] UPDATED)</h2>
<h3>Marc T. Law, Jos Stam</h3>
<p>In machine learning, data is usually represented in a (flat) Euclidean space
where distances between points are along straight lines. Researchers have
recently considered more exotic (non-Euclidean) Riemannian manifolds such as
hyperbolic space which is well suited for tree-like data. In this paper, we
propose a representation living on a pseudo-Riemannian manifold of constant
nonzero curvature. It is a generalization of hyperbolic and spherical
geometries where the nondegenerate metric tensor need not be positive definite.
We provide the necessary learning tools in this geometry and extend
gradient-based optimization techniques. More specifically, we provide
closed-form expressions for distances via geodesics and define a descent
direction to minimize some objective function. Our novel framework is applied
to graph representations.
</p>
<a href="http://arxiv.org/abs/2007.00211" target="_blank">arXiv:2007.00211</a> [<a href="http://arxiv.org/pdf/2007.00211" target="_blank">pdf</a>]

<h2>Adversarial Example Games. (arXiv:2007.00720v6 [cs.LG] UPDATED)</h2>
<h3>Avishek Joey Bose, Gauthier Gidel, Hugo Berard, Andre Cianflone, Pascal Vincent, Simon Lacoste-Julien, William L. Hamilton</h3>
<p>The existence of adversarial examples capable of fooling trained neural
network classifiers calls for a much better understanding of possible attacks
to guide the development of safeguards against them. This includes attack
methods in the challenging non-interactive blackbox setting, where adversarial
attacks are generated without any access, including queries, to the target
model. Prior attacks in this setting have relied mainly on algorithmic
innovations derived from empirical observations (e.g., that momentum helps),
lacking principled transferability guarantees. In this work, we provide a
theoretical foundation for crafting transferable adversarial examples to entire
hypothesis classes. We introduce Adversarial Example Games (AEG), a framework
that models the crafting of adversarial examples as a min-max game between a
generator of attacks and a classifier. AEG provides a new way to design
adversarial examples by adversarially training a generator and a classifier
from a given hypothesis class (e.g., architecture). We prove that this game has
an equilibrium, and that the optimal generator is able to craft adversarial
examples that can attack any classifier from the corresponding hypothesis
class. We demonstrate the efficacy of AEG on the MNIST and CIFAR-10 datasets,
outperforming prior state-of-the-art approaches with an average relative
improvement of $29.9\%$ and $47.2\%$ against undefended and robust models
(Table 2 &amp; 3) respectively.
</p>
<a href="http://arxiv.org/abs/2007.00720" target="_blank">arXiv:2007.00720</a> [<a href="http://arxiv.org/pdf/2007.00720" target="_blank">pdf</a>]

<h2>Accurate Bounding-box Regression with Distance-IoU Loss for Visual Tracking. (arXiv:2007.01864v3 [cs.CV] UPDATED)</h2>
<h3>Di Yuan, Nana Fan, Xiaojun Chang, Qiao Liu, Zhenyu He</h3>
<p>Most existing trackers are based on using a classifier and multi-scale
estimation to estimate the target state. Consequently, and as expected,
trackers have become more stable while tracking accuracy has stagnated. While
trackers adopt a maximum overlap method based on an intersection-over-union
(IoU) loss to mitigate this problem, there are defects in the IoU loss itself,
that make it impossible to continue to optimize the objective function when a
given bounding box is completely contained within/without another bounding box;
this makes it very challenging to accurately estimate the target state.
Accordingly, in this paper, we address the above-mentioned problem by proposing
a novel tracking method based on a distance-IoU (DIoU) loss, such that the
proposed tracker consists of target estimation and target classification. The
target estimation part is trained to predict the DIoU score between the target
ground-truth bounding-box and the estimated bounding-box. The DIoU loss can
maintain the advantage provided by the IoU loss while minimizing the distance
between the center points of two bounding boxes, thereby making the target
estimation more accurate. Moreover, we introduce a classification part that is
trained online and optimized with a Conjugate-Gradient-based strategy to
guarantee real-time tracking speed. Comprehensive experimental results
demonstrate that the proposed method achieves competitive tracking accuracy
when compared to state-of-the-art trackers while with a real-time tracking
speed.
</p>
<a href="http://arxiv.org/abs/2007.01864" target="_blank">arXiv:2007.01864</a> [<a href="http://arxiv.org/pdf/2007.01864" target="_blank">pdf</a>]

<h2>Human Trajectory Forecasting in Crowds: A Deep Learning Perspective. (arXiv:2007.03639v3 [cs.CV] UPDATED)</h2>
<h3>Parth Kothari, Sven Kreiss, Alexandre Alahi</h3>
<p>Since the past few decades, human trajectory forecasting has been a field of
active research owing to its numerous real-world applications: evacuation
situation analysis, deployment of intelligent transport systems, traffic
operations, to name a few. Early works handcrafted this representation based on
domain knowledge. However, social interactions in crowded environments are not
only diverse but often subtle. Recently, deep learning methods have
outperformed their handcrafted counterparts, as they learned about human-human
interactions in a more generic data-driven fashion. In this work, we present an
in-depth analysis of existing deep learning-based methods for modelling social
interactions. We propose two knowledge-based data-driven methods to effectively
capture these social interactions. To objectively compare the performance of
these interaction-based forecasting models, we develop a large scale
interaction-centric benchmark TrajNet++, a significant yet missing component in
the field of human trajectory forecasting. We propose novel performance metrics
that evaluate the ability of a model to output socially acceptable
trajectories. Experiments on TrajNet++ validate the need for our proposed
metrics, and our method outperforms competitive baselines on both real-world
and synthetic datasets.
</p>
<a href="http://arxiv.org/abs/2007.03639" target="_blank">arXiv:2007.03639</a> [<a href="http://arxiv.org/pdf/2007.03639" target="_blank">pdf</a>]

<h2>Submodular Meta-Learning. (arXiv:2007.05852v2 [cs.LG] UPDATED)</h2>
<h3>Arman Adibi, Aryan Mokhtari, Hamed Hassani</h3>
<p>In this paper, we introduce a discrete variant of the meta-learning
framework. Meta-learning aims at exploiting prior experience and data to
improve performance on future tasks. By now, there exist numerous formulations
for meta-learning in the continuous domain. Notably, the Model-Agnostic
Meta-Learning (MAML) formulation views each task as a continuous optimization
problem and based on prior data learns a suitable initialization that can be
adapted to new, unseen tasks after a few simple gradient updates. Motivated by
this terminology, we propose a novel meta-learning framework in the discrete
domain where each task is equivalent to maximizing a set function under a
cardinality constraint. Our approach aims at using prior data, i.e., previously
visited tasks, to train a proper initial solution set that can be quickly
adapted to a new task at a relatively low computational cost. This approach
leads to (i) a personalized solution for each individual task, and (ii)
significantly reduced computational cost at test time compared to the case
where the solution is fully optimized once the new task is revealed. The
training procedure is performed by solving a challenging discrete optimization
problem for which we present deterministic and randomized algorithms. In the
case where the tasks are monotone and submodular, we show strong theoretical
guarantees for our proposed methods even though the training objective may not
be submodular. We also demonstrate the effectiveness of our framework on two
real-world problem instances where we observe that our methods lead to a
significant reduction in computational complexity in solving the new tasks
while incurring a small performance loss compared to when the tasks are fully
optimized.
</p>
<a href="http://arxiv.org/abs/2007.05852" target="_blank">arXiv:2007.05852</a> [<a href="http://arxiv.org/pdf/2007.05852" target="_blank">pdf</a>]

<h2>Relaxing the I.I.D. Assumption: Adaptively Minimax Optimal Regret via Root-Entropic Regularization. (arXiv:2007.06552v2 [stat.ML] UPDATED)</h2>
<h3>Blair Bilodeau, Jeffrey Negrea, Daniel M. Roy</h3>
<p>We consider sequential prediction with expert advice when data are generated
from distributions varying arbitrarily within an unknown constraint set. We
quantify relaxations of the classical i.i.d. assumption in terms of these
constraint sets, with i.i.d. sequences at one extreme and adversarial
mechanisms at the other. The Hedge algorithm, long known to be minimax optimal
in the adversarial regime, was recently shown to be minimax optimal for i.i.d.
data. We show that Hedge with deterministic learning rates is suboptimal
between these extremes, and present a new algorithm that adaptively achieves
the minimax optimal rate of regret with respect to our relaxations of the
i.i.d. assumption, and does so without knowledge of the underlying constraint
set. We analyze our algorithm using the follow-the-regularized-leader
framework, and prove it corresponds to Hedge with an adaptive learning rate
that implicitly scales as the square root of the entropy of the current
predictive distribution, rather than the entropy of the initial predictive
distribution.
</p>
<a href="http://arxiv.org/abs/2007.06552" target="_blank">arXiv:2007.06552</a> [<a href="http://arxiv.org/pdf/2007.06552" target="_blank">pdf</a>]

<h2>A Decentralized Approach to Bayesian Learning. (arXiv:2007.06799v4 [stat.ML] UPDATED)</h2>
<h3>Anjaly Parayil, He Bai, Jemin George, Prudhvi Gurram</h3>
<p>Motivated by decentralized approaches to machine learning, we propose a
collaborative Bayesian learning algorithm taking the form of decentralized
Langevin dynamics in a non-convex setting. Our analysis show that the initial
KL-divergence between the Markov Chain and the target posterior distribution is
exponentially decreasing while the error contributions to the overall
KL-divergence from the additive noise is decreasing in polynomial time. We
further show that the polynomial-term experiences speed-up with number of
agents and provide sufficient conditions on the time-varying step-sizes to
guarantee convergence to the desired distribution. The performance of the
proposed algorithm is evaluated on a wide variety of machine learning tasks.
The empirical results show that the performance of individual agents with
locally available data is on par with the centralized setting with considerable
improvement in the convergence rate.
</p>
<a href="http://arxiv.org/abs/2007.06799" target="_blank">arXiv:2007.06799</a> [<a href="http://arxiv.org/pdf/2007.06799" target="_blank">pdf</a>]

<h2>Understanding Implicit Regularization in Over-Parameterized Nonlinear Statistical Model. (arXiv:2007.08322v2 [stat.ML] UPDATED)</h2>
<h3>Jianqing Fan, Zhuoran Yang, Mengxin Yu</h3>
<p>We study the implicit regularization phenomenon induced by simple
optimization algorithms in over-parameterized nonlinear statistical models.
Specifically, we study both vector and matrix single index models where the
link function is nonlinear and unknown, the signal parameter is either a sparse
vector or a low-rank symmetric matrix, and the response variable can be
heavy-tailed. To gain a better understanding of the role played by implicit
regularization in the nonlinear models without excess technicality, we assume
that the distribution of the covariates is known a priori. For both the vector
and matrix settings, we construct an over-parameterized least-squares loss
function by employing the score function transform and a robust truncation step
designed specifically for heavy-tailed data. We propose to estimate the true
parameter by applying regularization-free gradient descent to the loss
function. When the initialization is close to the origin and the stepsize is
sufficiently small, we prove that the obtained solution achieves minimax
optimal statistical rates of convergence in both the vector and matrix cases.
In particular, for the vector single index model with Gaussian covariates, our
proposed estimator is shown to further enjoy the oracle statistical rate. Our
results capture the implicit regularization phenomenon in over-parameterized
nonlinear and noisy statistical models with possibly heavy-tailed data.
</p>
<a href="http://arxiv.org/abs/2007.08322" target="_blank">arXiv:2007.08322</a> [<a href="http://arxiv.org/pdf/2007.08322" target="_blank">pdf</a>]

<h2>The Effects of Approximate Multiplication on Convolutional Neural Networks. (arXiv:2007.10500v2 [cs.LG] UPDATED)</h2>
<h3>Min Soo Kim, Alberto A. Del Barrio, HyunJin Kim, Nader Bagherzadeh</h3>
<p>This paper analyzes the effects of approximate multiplication when performing
inferences on deep convolutional neural networks (CNNs). The approximate
multiplication can reduce the cost of the underlying circuits so that CNN
inferences can be performed more efficiently in hardware accelerators. The
study identifies the critical factors in the convolution, fully-connected, and
batch normalization layers that allow more accurate CNN predictions despite the
errors from approximate multiplication. The same factors also provide an
arithmetic explanation of why bfloat16 multiplication performs well on CNNs.
The experiments are performed with recognized network architectures to show
that the approximate multipliers can produce predictions that are nearly as
accurate as the FP32 references, without additional training. For example, the
ResNet and Inception-v4 models with Mitch-$w$6 multiplication produces Top-5
errors that are within 0.2% compared to the FP32 references. A brief cost
comparison of Mitch-$w$6 against bfloat16 is presented, where a MAC operation
saves up to 80% of energy compared to the bfloat16 arithmetic. The most
far-reaching contribution of this paper is the analytical justification that
multiplications can be approximated while additions need to be exact in CNN MAC
operations.
</p>
<a href="http://arxiv.org/abs/2007.10500" target="_blank">arXiv:2007.10500</a> [<a href="http://arxiv.org/pdf/2007.10500" target="_blank">pdf</a>]

<h2>Nonclosedness of Sets of Neural Networks in Sobolev Spaces. (arXiv:2007.11730v3 [stat.ML] UPDATED)</h2>
<h3>Scott Mahan, Emily King, Alex Cloninger</h3>
<p>We examine the closedness of sets of realized neural networks of a fixed
architecture in Sobolev spaces. For an exactly $m$-times differentiable
activation function $\rho$, we construct a sequence of neural networks
$(\Phi_n)_{n \in \mathbb{N}}$ whose realizations converge in order-$(m-1)$
Sobolev norm to a function that cannot be realized exactly by a neural network.
Thus, sets of realized neural networks are not closed in order-$(m-1)$ Sobolev
spaces $W^{m-1,p}$ for $p \in [1,\infty)$. We further show that these sets are
not closed in $W^{m,p}$ under slightly stronger conditions on the $m$-th
derivative of $\rho$. For a real analytic activation function, we show that
sets of realized neural networks are not closed in $W^{k,p}$ for \textit{any}
$k \in \mathbb{N}$. The nonclosedness allows for approximation of non-network
target functions with unbounded parameter growth. We partially characterize the
rate of parameter growth for most activation functions by showing that a
specific sequence of realized neural networks can approximate the activation
function's derivative with weights increasing inversely proportional to the
$L^p$ approximation error. Finally, we present experimental results showing
that networks are capable of closely approximating non-network target functions
with increasing parameters via training.
</p>
<a href="http://arxiv.org/abs/2007.11730" target="_blank">arXiv:2007.11730</a> [<a href="http://arxiv.org/pdf/2007.11730" target="_blank">pdf</a>]

<h2>All at Once: Temporally Adaptive Multi-Frame Interpolation with Advanced Motion Modeling. (arXiv:2007.11762v2 [cs.CV] UPDATED)</h2>
<h3>Zhixiang Chi, Rasoul Mohammadi Nasiri, Zheng Liu, Juwei Lu, Jin Tang, Konstantinos N Plataniotis</h3>
<p>Recent advances in high refresh rate displays as well as the increased
interest in high rate of slow motion and frame up-conversion fuel the demand
for efficient and cost-effective multi-frame video interpolation solutions. To
that regard, inserting multiple frames between consecutive video frames are of
paramount importance for the consumer electronics industry. State-of-the-art
methods are iterative solutions interpolating one frame at the time. They
introduce temporal inconsistencies and clearly noticeable visual artifacts.

Departing from the state-of-the-art, this work introduces a true multi-frame
interpolator. It utilizes a pyramidal style network in the temporal domain to
complete the multi-frame interpolation task in one-shot. A novel flow
estimation procedure using a relaxed loss function, and an advanced,
cubic-based, motion model is also used to further boost interpolation accuracy
when complex motion segments are encountered. Results on the Adobe240 dataset
show that the proposed method generates visually pleasing, temporally
consistent frames, outperforms the current best off-the-shelf method by 1.57db
in PSNR with 8 times smaller model and 7.7 times faster. The proposed method
can be easily extended to interpolate a large number of new frames while
remaining efficient because of the one-shot mechanism.
</p>
<a href="http://arxiv.org/abs/2007.11762" target="_blank">arXiv:2007.11762</a> [<a href="http://arxiv.org/pdf/2007.11762" target="_blank">pdf</a>]

<h2>Design and Implementation of a Maxi-Sized Mobile Robot (Karo) for Rescue Missions. (arXiv:2008.10396v2 [cs.RO] UPDATED)</h2>
<h3>Soheil Habibian, Mehdi Dadvar, Behzad Peykari, Alireza Hosseini, M. Hossein Salehzadeh, Alireza H.M. Hosseini, Farshid Najafi</h3>
<p>Rescue robots are expected to carry out reconnaissance and dexterity
operations in unknown environments comprising unstructured obstacles. Although
a wide variety of designs and implementations have been presented within the
field of rescue robotics, embedding all mobility, dexterity, and reconnaissance
capabilities in a single robot remains a challenging problem. This paper
explains the design and implementation of Karo, a mobile robot that exhibits a
high degree of mobility at the side of maintaining required dexterity and
exploration capabilities for urban search and rescue (USAR) missions. We first
elicit the system requirements of a standard rescue robot from the frameworks
of Rescue Robot League (RRL) of RoboCup and then, propose the conceptual design
of Karo by drafting a locomotion and manipulation system. Considering that,
this work presents comprehensive design processes along with detail mechanical
design of the robot's platform and its 7-DOF manipulator. Further, we present
the design and implementation of the command and control system by discussing
the robot's power system, sensors, and hardware systems. In conjunction with
this, we elucidate the way that Karo's software system and human-robot
interface are implemented and employed. Furthermore, we undertake extensive
evaluations of Karo's field performance to investigate whether the principal
objective of this work has been satisfied. We demonstrate that Karo has
effectively accomplished assigned standardized rescue operations by evaluating
all aspects of its capabilities in both RRL's test suites and training suites
of a fire department. Finally, the comprehensiveness of Karo's capabilities has
been verified by drawing quantitative comparisons between Karo's performance
and other leading robots participating in RRL.
</p>
<a href="http://arxiv.org/abs/2008.10396" target="_blank">arXiv:2008.10396</a> [<a href="http://arxiv.org/pdf/2008.10396" target="_blank">pdf</a>]

<h2>Adversarial Image Composition with Auxiliary Illumination. (arXiv:2009.08255v2 [cs.CV] UPDATED)</h2>
<h3>Fangneng Zhan, Shijian Lu, Changgong Zhang, Feiying Ma, Xuansong Xie</h3>
<p>Dealing with the inconsistency between a foreground object and a background
image is a challenging task in high-fidelity image composition.
State-of-the-art methods strive to harmonize the composed image by adapting the
style of foreground objects to be compatible with the background image, whereas
the potential shadow of foreground objects within the composed image which is
critical to the composition realism is largely neglected. In this paper, we
propose an Adversarial Image Composition Net (AIC-Net) that achieves realistic
image composition by considering potential shadows that the foreground object
projects in the composed image. A novel branched generation mechanism is
proposed, which disentangles the generation of shadows and the transfer of
foreground styles for optimal accomplishment of the two tasks simultaneously. A
differentiable spatial transformation module is designed which bridges the
local harmonization and the global harmonization to achieve their joint
optimization effectively. Extensive experiments on pedestrian and car
composition tasks show that the proposed AIC-Net achieves superior composition
performance qualitatively and quantitatively.
</p>
<a href="http://arxiv.org/abs/2009.08255" target="_blank">arXiv:2009.08255</a> [<a href="http://arxiv.org/pdf/2009.08255" target="_blank">pdf</a>]

<h2>Unsupervised Model Adaptation for Continual Semantic Segmentation. (arXiv:2009.12518v2 [cs.LG] UPDATED)</h2>
<h3>Serban Stan, Mohammad Rostami</h3>
<p>We develop an algorithm for adapting a semantic segmentation model that is
trained using a labeled source domain to generalize well in an unlabeled target
domain. A similar problem has been studied extensively in the unsupervised
domain adaptation (UDA) literature, but existing UDA algorithms require access
to both the source domain labeled data and the target domain unlabeled data for
training a domain agnostic semantic segmentation model. Relaxing this
constraint enables a user to adapt pretrained models to generalize in a target
domain, without requiring access to source data. To this end, we learn a
prototypical distribution for the source domain in an intermediate embedding
space. This distribution encodes the abstract knowledge that is learned from
the source domain. We then use this distribution for aligning the target domain
distribution with the source domain distribution in the embedding space. We
provide theoretical analysis and explain conditions under which our algorithm
is effective. Experiments on benchmark adaptation task demonstrate our method
achieves competitive performance even compared with joint UDA approaches.
</p>
<a href="http://arxiv.org/abs/2009.12518" target="_blank">arXiv:2009.12518</a> [<a href="http://arxiv.org/pdf/2009.12518" target="_blank">pdf</a>]

<h2>Joint Scene and Object Tracking for Cost-Effective Augmented Reality Assisted Patient Positioning in Radiation Therapy. (arXiv:2010.01895v2 [cs.CV] UPDATED)</h2>
<h3>Hamid Sarmadi, Rafael Mu&#xf1;oz-Salinas, M. &#xc1;lvaro Berb&#xed;s, Antonio Luna, R. Medina-Carnicer</h3>
<p>BACKGROUND AND OBJECTIVE: The research done in the field of Augmented Reality
(AR) for patient positioning in radiation therapy is scarce. We propose an
efficient and cost-effective algorithm for tracking the scene and the patient
to interactively assist the patient's positioning process by providing visual
feedback to the operator. Up to our knowledge, this is the first framework that
can be employed for mobile interactive AR to guide patient positioning.
METHODS: We propose a point cloud processing method that combined with a
fiducial marker-mapper algorithm and the generalized ICP algorithm tracks the
patient and the camera precisely and efficiently only using the CPU unit. The
alignment between the 3D reference model and body marker map is calculated
employing an efficient body reconstruction algorithm. RESULTS: Our quantitative
evaluation shows that the proposed method achieves a translational and
rotational error of 4.17 mm/0.82 deg at 9 fps. Furthermore, the qualitative
results demonstrate the usefulness of our algorithm in patient positioning on
different human subjects. CONCLUSION: Since our algorithm achieves a relatively
high frame rate and accuracy employing a regular laptop (without the usage of a
dedicated GPU), it is a very cost-effective AR-based patient positioning
method. It also opens the way for other researchers by introducing a framework
that could be improved upon for better mobile interactive AR patient
positioning solutions in the future.
</p>
<a href="http://arxiv.org/abs/2010.01895" target="_blank">arXiv:2010.01895</a> [<a href="http://arxiv.org/pdf/2010.01895" target="_blank">pdf</a>]

<h2>UniNet: Scalable Network Representation Learning with Metropolis-Hastings Sampling. (arXiv:2010.04895v2 [cs.LG] UPDATED)</h2>
<h3>Xingyu Yao, Yingxia Shao, Bin Cui, Lei Chen</h3>
<p>Network representation learning (NRL) technique has been successfully adopted
in various data mining and machine learning applications. Random walk based NRL
is one popular paradigm, which uses a set of random walks to capture the
network structural information, and then employs word2vec models to learn the
low-dimensional representations. However, until now there is lack of a
framework, which unifies existing random walk based NRL models and supports to
efficiently learn from large networks. The main obstacle comes from the diverse
random walk models and the inefficient sampling method for the random walk
generation. In this paper, we first introduce a new and efficient edge sampler
based on Metropolis-Hastings sampling technique, and theoretically show the
convergence property of the edge sampler to arbitrary discrete probability
distributions. Then we propose a random walk model abstraction, in which users
can easily define different transition probability by specifying dynamic edge
weights and random walk states. The abstraction is efficiently supported by our
edge sampler, since our sampler can draw samples from unnormalized probability
distribution in constant time complexity. Finally, with the new edge sampler
and random walk model abstraction, we carefully implement a scalable NRL
framework called UniNet. We conduct comprehensive experiments with five random
walk based NRL models over eleven real-world datasets, and the results clearly
demonstrate the efficiency of UniNet over billion-edge networks.
</p>
<a href="http://arxiv.org/abs/2010.04895" target="_blank">arXiv:2010.04895</a> [<a href="http://arxiv.org/pdf/2010.04895" target="_blank">pdf</a>]

<h2>General stochastic separation theorems with optimal bounds. (arXiv:2010.05241v2 [cs.AI] UPDATED)</h2>
<h3>Bogdan Grechuk, Alexander N. Gorban, Ivan Y. Tyukin</h3>
<p>Phenomenon of stochastic separability was revealed and used in machine
learning to correct errors of Artificial Intelligence (AI) systems and analyze
AI instabilities. In high-dimensional datasets under broad assumptions each
point can be separated from the rest of the set by simple and robust Fisher's
discriminant (is Fisher separable). Errors or clusters of errors can be
separated from the rest of the data. The ability to correct an AI system also
opens up the possibility of an attack on it, and the high dimensionality
induces vulnerabilities caused by the same stochastic separability that holds
the keys to understanding the fundamentals of robustness and adaptivity in
high-dimensional data-driven AI. To manage errors and analyze vulnerabilities,
the stochastic separation theorems should evaluate the probability that the
dataset will be Fisher separable in given dimensionality and for a given class
of distributions. Explicit and optimal estimates of these separation
probabilities are required, and this problem is solved in present work. The
general stochastic separation theorems with optimal probability estimates are
obtained for important classes of distributions: log-concave distribution,
their convex combinations and product distributions. The standard i.i.d.
assumption was significantly relaxed. These theorems and estimates can be used
both for correction of high-dimensional data driven AI systems and for analysis
of their vulnerabilities. The third area of application is the emergence of
memories in ensembles of neurons, the phenomena of grandmother's cells and
sparse coding in the brain, and explanation of unexpected effectiveness of
small neural ensembles in high-dimensional brain.
</p>
<a href="http://arxiv.org/abs/2010.05241" target="_blank">arXiv:2010.05241</a> [<a href="http://arxiv.org/pdf/2010.05241" target="_blank">pdf</a>]

<h2>Hyperparameter Auto-tuning in Self-Supervised Robotic Learning. (arXiv:2010.08252v3 [cs.RO] UPDATED)</h2>
<h3>Jiancong Huang, Juan Rojas, Matthieu Zimmer, Hongmin Wu, Yisheng Guan, Paul Weng</h3>
<p>Policy optimization in reinforcement learning requires the selection of
numerous hyperparameters across different environments. Fixing them incorrectly
may negatively impact optimization performance leading notably to insufficient
or redundant learning. Insufficient learning (due to convergence to local
optima) results in under-performing policies whilst redundant learning wastes
time and resources. The effects are further exacerbated when using single
policies to solve multi-task learning problems. Observing that the Evidence
Lower Bound (ELBO) used in Variational Auto-Encoders correlates with the
diversity of image samples, we propose an auto-tuning technique based on the
ELBO for self-supervised reinforcement learning. Our approach can auto-tune
three hyperparameters: the replay buffer size, the number of policy gradient
updates during each epoch, and the number of exploration steps during each
epoch. We use a state-of-the-art self-supervised robot learning framework
(Reinforcement Learning with Imagined Goals (RIG) using Soft Actor-Critic) as
baseline for experimental verification. Experiments show that our method can
auto-tune online and yields the best performance at a fraction of the time and
computational resources. Code, video, and appendix for simulated and real-robot
experiments can be found at the project page \url{www.JuanRojas.net/autotune}.
</p>
<a href="http://arxiv.org/abs/2010.08252" target="_blank">arXiv:2010.08252</a> [<a href="http://arxiv.org/pdf/2010.08252" target="_blank">pdf</a>]

<h2>On the Generalisation Capabilities of Fingerprint Presentation Attack Detection Methods in the Short Wave Infrared Domain. (arXiv:2010.09566v2 [cs.CV] UPDATED)</h2>
<h3>Jascha Kolberg, Marta Gomez-Barrero, Christoph Busch</h3>
<p>Nowadays, fingerprint-based biometric recognition systems are becoming
increasingly popular. However, in spite of their numerous advantages, biometric
capture devices are usually exposed to the public and thus vulnerable to
presentation attacks (PAs). Therefore, presentation attack detection (PAD)
methods are of utmost importance in order to distinguish between bona fide and
attack presentations. Due to the nearly unlimited possibilities to create new
presentation attack instruments (PAIs), unknown attacks are a threat to
existing PAD algorithms. This fact motivates research on generalisation
capabilities in order to find PAD methods that are resilient to new attacks. In
this context, we evaluate the generalisability of multiple PAD algorithms on a
dataset of 19,711 bona fide and 4,339 PA samples, including 45 different PAI
species. The PAD data is captured in the short wave infrared domain and the
results discuss the advantages and drawbacks of this PAD technique regarding
unknown attacks.
</p>
<a href="http://arxiv.org/abs/2010.09566" target="_blank">arXiv:2010.09566</a> [<a href="http://arxiv.org/pdf/2010.09566" target="_blank">pdf</a>]

<h2>Imitation Learning of Hierarchical Driving Model: from Continuous Intention to Continuous Trajectory. (arXiv:2010.10393v2 [cs.RO] UPDATED)</h2>
<h3>Yunkai Wang, Dongkun Zhang, Jingke Wang, Zexi Chen, Yue Wang, Rong Xiong</h3>
<p>One of the challenges to reduce the gap between the machine and the human
level driving is how to endow the system with the learning capacity to deal
with the coupled complexity of environments, intentions, and dynamics. In this
paper, we propose a hierarchical driving model with explicit model of
continuous intention and continuous dynamics, which decouples the complexity in
the observation-to-action reasoning in the human driving data. Specifically,
the continuous intention module takes the route planning map obtained by GPS
and IMU, perception from a RGB camera and LiDAR as input to generate a
potential map encoded with obstacles and intentions being expressed as grid
based potentials. Then, the potential map is regarded as a condition, together
with the current dynamics, to generate a continuous trajectory as output by a
continuous function approximator network, whose derivatives can be used for
supervision without additional parameters. Finally, we validate our method on
both datasets and simulator, demonstrating that our method has higher
prediction accuracy of displacement and velocity and generates smoother
trajectories. The method is also deployed on the real vehicle with loop
latency, validating its effectiveness. To the best of our knowledge, this is
the first work to produce the driving trajectory using a continuous function
approximator network.
</p>
<a href="http://arxiv.org/abs/2010.10393" target="_blank">arXiv:2010.10393</a> [<a href="http://arxiv.org/pdf/2010.10393" target="_blank">pdf</a>]

<h2>SCOP: Scientific Control for Reliable Neural Network Pruning. (arXiv:2010.10732v2 [cs.CV] UPDATED)</h2>
<h3>Yehui Tang, Yunhe Wang, Yixing Xu, Dacheng Tao, Chunjing Xu, Chao Xu, Chang Xu</h3>
<p>This paper proposes a reliable neural network pruning algorithm by setting up
a scientific control. Existing pruning methods have developed various
hypotheses to approximate the importance of filters to the network and then
execute filter pruning accordingly. To increase the reliability of the results,
we prefer to have a more rigorous research design by including a scientific
control group as an essential part to minimize the effect of all factors except
the association between the filter and expected network output. Acting as a
control group, knockoff feature is generated to mimic the feature map produced
by the network filter, but they are conditionally independent of the example
label given the real feature map. We theoretically suggest that the knockoff
condition can be approximately preserved given the information propagation of
network layers. Besides the real feature map on an intermediate layer, the
corresponding knockoff feature is brought in as another auxiliary input signal
for the subsequent layers. Redundant filters can be discovered in the
adversarial process of different features. Through experiments, we demonstrate
the superiority of the proposed algorithm over state-of-the-art methods. For
example, our method can reduce 57.8% parameters and 60.2% FLOPs of ResNet-101
with only 0.01% top-1 accuracy loss on ImageNet. The code is available at
https://github.com/huawei-noah/Pruning/tree/master/SCOP_NeurIPS2020.
</p>
<a href="http://arxiv.org/abs/2010.10732" target="_blank">arXiv:2010.10732</a> [<a href="http://arxiv.org/pdf/2010.10732" target="_blank">pdf</a>]

<h2>MELD: Meta-Reinforcement Learning from Images via Latent State Models. (arXiv:2010.13957v2 [cs.LG] UPDATED)</h2>
<h3>Tony Z. Zhao, Anusha Nagabandi, Kate Rakelly, Chelsea Finn, Sergey Levine</h3>
<p>Meta-reinforcement learning algorithms can enable autonomous agents, such as
robots, to quickly acquire new behaviors by leveraging prior experience in a
set of related training tasks. However, the onerous data requirements of
meta-training compounded with the challenge of learning from sensory inputs
such as images have made meta-RL challenging to apply to real robotic systems.
Latent state models, which learn compact state representations from a sequence
of observations, can accelerate representation learning from visual inputs. In
this paper, we leverage the perspective of meta-learning as task inference to
show that latent state models can \emph{also} perform meta-learning given an
appropriately defined observation space. Building on this insight, we develop
meta-RL with latent dynamics (MELD), an algorithm for meta-RL from images that
performs inference in a latent state model to quickly acquire new skills given
observations and rewards. MELD outperforms prior meta-RL methods on several
simulated image-based robotic control problems, and enables a real WidowX
robotic arm to insert an Ethernet cable into new locations given a sparse task
completion signal after only $8$ hours of real world meta-training. To our
knowledge, MELD is the first meta-RL algorithm trained in a real-world robotic
control setting from images.
</p>
<a href="http://arxiv.org/abs/2010.13957" target="_blank">arXiv:2010.13957</a> [<a href="http://arxiv.org/pdf/2010.13957" target="_blank">pdf</a>]

<h2>Learning to Plan Optimistically: Uncertainty-Guided Deep Exploration via Latent Model Ensembles. (arXiv:2010.14641v2 [cs.LG] UPDATED)</h2>
<h3>Tim Seyde, Wilko Schwarting, Sertac Karaman, Daniela Rus</h3>
<p>Learning complex behaviors through interaction requires coordinated long-term
planning. Random exploration and novelty search lack task-centric guidance and
waste effort on non-informative interactions. Instead, decision making should
target samples with the potential to optimize performance far into the future,
while only reducing uncertainty where conducive to this objective. This paper
presents latent optimistic value exploration (LOVE), a strategy that enables
deep exploration through optimism in the face of uncertain long-term rewards.
We combine finite horizon rollouts from a latent model with value function
estimates to predict infinite horizon returns and recover associated
uncertainty through ensembling. Policy training then proceeds on an upper
confidence bound (UCB) objective to identify and select the interactions most
promising to improve long-term performance. We apply LOVE to visual control
tasks in continuous state-action spaces and demonstrate improved sample
complexity on a selection of benchmarking tasks.
</p>
<a href="http://arxiv.org/abs/2010.14641" target="_blank">arXiv:2010.14641</a> [<a href="http://arxiv.org/pdf/2010.14641" target="_blank">pdf</a>]

<h2>Depth Self-Optimized Learning Toward Data Science. (arXiv:2011.02842v2 [cs.LG] UPDATED)</h2>
<h3>Ziqi Zhang</h3>
<p>We propose a two-stage model called Depth Self-Optimized Learning (DSOL),
which aims to realize ANN depth self-configuration, self-optimization as well
as ANN training without manual intervention. In the first stage of DSOL, it
will configure ANN of specific depth according to a specific dataset. In the
second stage, DSOL will continuously optimize ANN based on Reinforcement
Learning (RL). Finally, the optimal depth is returned to the first stage of
DSOL for training, so that DSOL can configure the appropriate ANN depth and
perform more reasonable optimization when processing similar datasets again. In
the experiment, we ran DSOL on the Iris and Boston housing datasets, and the
results showed that DSOL performed well. We have uploaded the experiment
records and code to our Github.
</p>
<a href="http://arxiv.org/abs/2011.02842" target="_blank">arXiv:2011.02842</a> [<a href="http://arxiv.org/pdf/2011.02842" target="_blank">pdf</a>]

<h2>Fast Hybrid Cascade for Voxel-based 3D Object Classification. (arXiv:2011.04522v2 [cs.CV] UPDATED)</h2>
<h3>Hui Cao, Jie Wang, Yuqi Liu, Siyu Zhang, Shen Cai</h3>
<p>Voxel-based 3D object classification has been frequently studied in recent
years. The previous methods often directly convert the classic 2D convolution
into a 3D form applied to an object with binary voxel representation. In this
paper, we investigate the reason why binary voxel representation is not very
suitable for 3D convolution and how to simultaneously improve the performance
both in accuracy and speed. We show that by giving each voxel a signed distance
value, the accuracy will gain about 30% promotion compared with binary voxel
representation using a two-layer fully connected network. We then propose a
fast fully connected and convolution hybrid cascade network for voxel-based 3D
object classification. This threestage cascade network can divide 3D models
into three categories: easy, moderate and hard. Consequently, the mean
inference time (0.3ms) can speedup about 5x and 2x compared with the
state-of-the-art point cloud and voxel based methods respectively, while
achieving the highest accuracy in the latter category of methods (92%).
Experiments with ModelNet andMNIST verify the performance of the proposed
hybrid cascade network.
</p>
<a href="http://arxiv.org/abs/2011.04522" target="_blank">arXiv:2011.04522</a> [<a href="http://arxiv.org/pdf/2011.04522" target="_blank">pdf</a>]

<h2>Benign Overfitting in Binary Classification of Gaussian Mixtures. (arXiv:2011.09148v2 [stat.ML] UPDATED)</h2>
<h3>Ke Wang, Christos Thrampoulidis</h3>
<p>Deep neural networks generalize well despite being exceedingly
overparametrized, but understanding the statistical principles behind this so
called benign-overfitting phenomenon is not yet well understood. Recently there
has been remarkable progress towards understanding benign-overfitting in
simpler models, such as linear regression and, even more recently, linear
classification. This paper studies benign-overfitting for data generated from a
popular binary Gaussian mixtures model (GMM) and classifiers trained by
support-vector machines (SVM). Our approach has two steps. First, we leverage
an idea introduced in (Muthukumar et al. 2020) to relate the SVM solution to
the least-squares (LS) solution. Second, we derive novel non-asymptotic bounds
on the classification error of LS solution. Combining the two gives sufficient
conditions on the overparameterization ratio and the signal-to-noise ratio that
lead to benign overfitting. We corroborate our theoretical findings with
numerical simulations.
</p>
<a href="http://arxiv.org/abs/2011.09148" target="_blank">arXiv:2011.09148</a> [<a href="http://arxiv.org/pdf/2011.09148" target="_blank">pdf</a>]

<h2>Watch and Learn: Mapping Language and Noisy Real-world Videos with Self-supervision. (arXiv:2011.09634v2 [cs.CV] UPDATED)</h2>
<h3>Yujie Zhong, Linhai Xie, Sen Wang, Lucia Specia, Yishu Miao</h3>
<p>In this paper, we teach machines to understand visuals and natural language
by learning the mapping between sentences and noisy video snippets without
explicit annotations. Firstly, we define a self-supervised learning framework
that captures the cross-modal information. A novel adversarial learning module
is then introduced to explicitly handle the noises in the natural videos, where
the subtitle sentences are not guaranteed to be strongly corresponded to the
video snippets. For training and evaluation, we contribute a new dataset
`ApartmenTour' that contains a large number of online videos and subtitles. We
carry out experiments on the bidirectional retrieval tasks between sentences
and videos, and the results demonstrate that our proposed model achieves the
state-of-the-art performance on both retrieval tasks and exceeds several strong
baselines. The dataset can be downloaded at https://github.com/zyj-13/WAL.
</p>
<a href="http://arxiv.org/abs/2011.09634" target="_blank">arXiv:2011.09634</a> [<a href="http://arxiv.org/pdf/2011.09634" target="_blank">pdf</a>]

<h2>Provable Multi-Objective Reinforcement Learning with Generative Models. (arXiv:2011.10134v2 [cs.LG] UPDATED)</h2>
<h3>Dongruo Zhou, Jiahao Chen, Quanquan Gu</h3>
<p>Multi-objective reinforcement learning (MORL) is an extension of ordinary,
single-objective reinforcement learning (RL) that is applicable to many
real-world tasks where multiple objectives exist without known relative costs.
We study the problem of single policy MORL, which learns an optimal policy
given the preference of objectives. Existing methods require strong assumptions
such as exact knowledge of the multi-objective Markov decision process, and are
analyzed in the limit of infinite data and time. We propose a new algorithm
called model-based envelop value iteration (EVI), which generalizes the
enveloped multi-objective $Q$-learning algorithm in Yang et al., 2019. Our
method can learn a near-optimal value function with polynomial sample
complexity and linear convergence speed. To the best of our knowledge, this is
the first finite-sample analysis of MORL algorithms.
</p>
<a href="http://arxiv.org/abs/2011.10134" target="_blank">arXiv:2011.10134</a> [<a href="http://arxiv.org/pdf/2011.10134" target="_blank">pdf</a>]

<h2>Forecasting Black Sigatoka Infection Risks with Latent Neural ODEs. (arXiv:2012.00752v2 [cs.LG] UPDATED)</h2>
<h3>Yuchen Wang, Matthieu Chan Chee, Ziyad Edher, Minh Duc Hoang, Shion Fujimori, Sornnujah Kathirgamanathan, Jesse Bettencourt</h3>
<p>Black Sigatoka disease severely decreases global banana production, and
climate change aggravates the problem by altering fungal species distributions.
Due to the heavy financial burden of managing this infectious disease, farmers
in developing countries face significant banana crop losses. Though scientists
have produced mathematical models of infectious diseases, adapting these models
to incorporate climate effects is difficult. We present MR. NODE (Multiple
predictoR Neural ODE), a neural network that models the dynamics of black
Sigatoka infection learnt directly from data via Neural Ordinary Differential
Equations. Our method encodes external predictor factors into the latent space
in addition to the variable that we infer, and it can also predict the
infection risk at an arbitrary point in time. Empirically, we demonstrate on
historical climate data that our method has superior generalization performance
on time points up to one month in the future and unseen irregularities. We
believe that our method can be a useful tool to control the spread of black
Sigatoka.
</p>
<a href="http://arxiv.org/abs/2012.00752" target="_blank">arXiv:2012.00752</a> [<a href="http://arxiv.org/pdf/2012.00752" target="_blank">pdf</a>]

<h2>Origin-Aware Next Destination Recommendation with Personalized Preference Attention. (arXiv:2012.01915v3 [cs.AI] UPDATED)</h2>
<h3>Nicholas Lim, Bryan Hooi, See-Kiong Ng, Xueou Wang, Yong Liang Goh, Renrong Weng, Rui Tan</h3>
<p>Next destination recommendation is an important task in the transportation
domain of taxi and ride-hailing services, where users are recommended with
personalized destinations given their current origin location. However, recent
recommendation works do not satisfy this origin-awareness property, and only
consider learning from historical destination locations, without origin
information. Thus, the resulting approaches are unable to learn and predict
origin-aware recommendations based on the user's current location, leading to
sub-optimal performance and poor real-world practicality. Hence, in this work,
we study the origin-aware next destination recommendation task. We propose the
Spatial-Temporal Origin-Destination Personalized Preference Attention
(STOD-PPA) encoder-decoder model to learn origin-origin (OO),
destination-destination (DD), and origin-destination (OD) relationships by
first encoding both origin and destination sequences with spatial and temporal
factors in local and global views, then decoding them through personalized
preference attention to predict the next destination. Experimental results on
seven real-world user trajectory taxi datasets show that our model
significantly outperforms baseline and state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2012.01915" target="_blank">arXiv:2012.01915</a> [<a href="http://arxiv.org/pdf/2012.01915" target="_blank">pdf</a>]

<h2>Convergence Rates for Multi-classs Logistic Regression Near Minimum. (arXiv:2012.04576v2 [cs.LG] UPDATED)</h2>
<h3>Dwight Nwaigwe, Marek Rychlik</h3>
<p>Training a neural network is typically done via variations of gradient
descent. If a minimum of the loss function exists and gradient descent is used
as the training method, we provide an expression that relates learning rate to
the rate of convergence to the minimum. We also discuss existence of a minimum.
</p>
<a href="http://arxiv.org/abs/2012.04576" target="_blank">arXiv:2012.04576</a> [<a href="http://arxiv.org/pdf/2012.04576" target="_blank">pdf</a>]

<h2>The Representation Power of Neural Networks: Breaking the Curse of Dimensionality. (arXiv:2012.05451v3 [cs.LG] UPDATED)</h2>
<h3>Moise Blanchard, M. Amine Bennouna</h3>
<p>In this paper, we analyze the number of neurons and training parameters that
a neural networks needs to approximate multivariate functions of bounded second
mixed derivatives -- Korobov functions. We prove upper bounds on these
quantities for shallow and deep neural networks, breaking the curse of
dimensionality. Our bounds hold for general activation functions, including
ReLU. We further prove that these bounds nearly match the minimal number of
parameters any continuous function approximator needs to approximate Korobov
functions, showing that neural networks are near-optimal function
approximators.
</p>
<a href="http://arxiv.org/abs/2012.05451" target="_blank">arXiv:2012.05451</a> [<a href="http://arxiv.org/pdf/2012.05451" target="_blank">pdf</a>]

<h2>Learning Omni-frequency Region-adaptive Representations for Real Image Super-Resolution. (arXiv:2012.06131v2 [cs.CV] UPDATED)</h2>
<h3>Xin Li, Xin Jin, Tao Yu, Yingxue Pang, Simeng Sun, Zhizheng Zhang, Zhibo Chen</h3>
<p>Traditional single image super-resolution (SISR) methods that focus on
solving single and uniform degradation (i.e., bicubic down-sampling), typically
suffer from poor performance when applied into real-world low-resolution (LR)
images due to the complicated realistic degradations. The key to solving this
more challenging real image super-resolution (RealSR) problem lies in learning
feature representations that are both informative and content-aware. In this
paper, we propose an Omni-frequency Region-adaptive Network (ORNet) to address
both challenges, here we call features of all low, middle and high frequencies
omni-frequency features. Specifically, we start from the frequency perspective
and design a Frequency Decomposition (FD) module to separate different
frequency components to comprehensively compensate the information lost for
real LR image. Then, considering the different regions of real LR image have
different frequency information lost, we further design a Region-adaptive
Frequency Aggregation (RFA) module by leveraging dynamic convolution and
spatial attention to adaptively restore frequency components for different
regions. The extensive experiments endorse the effective, and scenario-agnostic
nature of our OR-Net for RealSR.
</p>
<a href="http://arxiv.org/abs/2012.06131" target="_blank">arXiv:2012.06131</a> [<a href="http://arxiv.org/pdf/2012.06131" target="_blank">pdf</a>]

<h2>ProLab: perceptually uniform projective colour coordinate system. (arXiv:2012.07653v2 [cs.CV] UPDATED)</h2>
<h3>Ivan A. Konovalenko, Anna A. Smagina, Dmitry P. Nikolaev, Petr P. Nikolaev</h3>
<p>In this work, we propose proLab: a new colour coordinate system derived as a
3D projective transformation of CIE XYZ. We show that proLab is far ahead of
the widely used CIELAB coordinate system (though inferior to the modern
CAM16-UCS) according to perceptual uniformity evaluated by the STRESS metric in
reference to the CIEDE2000 colour difference formula. At the same time, angular
errors of chromaticity estimation that are standard for linear colour spaces
can also be used in proLab since projective transformations preserve the
linearity of manifolds. Unlike in linear spaces, angular errors for different
hues are normalized according to human colour discrimination thresholds within
proLab. We also demonstrate that shot noise in proLab is more homoscedastic
than in CAM16-UCS or other standard colour spaces. This makes proLab a
convenient coordinate system in which to perform linear colour analysis.
</p>
<a href="http://arxiv.org/abs/2012.07653" target="_blank">arXiv:2012.07653</a> [<a href="http://arxiv.org/pdf/2012.07653" target="_blank">pdf</a>]

<h2>StrokeGAN: Reducing Mode Collapse in Chinese Font Generation via Stroke Encoding. (arXiv:2012.08687v2 [cs.CV] UPDATED)</h2>
<h3>Jinshan Zeng, Qi Chen, Yunxin Liu, Mingwen Wang, Yuan Yao</h3>
<p>The generation of stylish Chinese fonts is an important problem involved in
many applications. Most of existing generation methods are based on the deep
generative models, particularly, the generative adversarial networks (GAN)
based models. However, these deep generative models may suffer from the mode
collapse issue, which significantly degrades the diversity and quality of
generated results. In this paper, we introduce a one-bit stroke encoding to
capture the key mode information of Chinese characters and then incorporate it
into CycleGAN, a popular deep generative model for Chinese font generation. As
a result we propose an efficient method called StrokeGAN, mainly motivated by
the observation that the stroke encoding contains amount of mode information of
Chinese characters. In order to reconstruct the one-bit stroke encoding of the
associated generated characters, we introduce a stroke-encoding reconstruction
loss imposed on the discriminator. Equipped with such one-bit stroke encoding
and stroke-encoding reconstruction loss, the mode collapse issue of CycleGAN
can be significantly alleviated, with an improved preservation of strokes and
diversity of generated characters. The effectiveness of StrokeGAN is
demonstrated by a series of generation tasks over nine datasets with different
fonts. The numerical results demonstrate that StrokeGAN generally outperforms
the state-of-the-art methods in terms of content and recognition accuracies, as
well as certain stroke error, and also generates more realistic characters.
</p>
<a href="http://arxiv.org/abs/2012.08687" target="_blank">arXiv:2012.08687</a> [<a href="http://arxiv.org/pdf/2012.08687" target="_blank">pdf</a>]

<h2>Exploring Instance-Level Uncertainty for Medical Detection. (arXiv:2012.12880v2 [cs.CV] UPDATED)</h2>
<h3>Jiawei Yang, Yuan Liang, Yao Zhang, Weinan Song, Kun Wang, Lei He</h3>
<p>The ability of deep learning to predict with uncertainty is recognized as key
for its adoption in clinical routines. Moreover, performance gain has been
enabled by modelling uncertainty according to empirical evidence. While
previous work has widely discussed the uncertainty estimation in segmentation
and classification tasks, its application on bounding-box-based detection has
been limited, mainly due to the challenge of bounding box aligning. In this
work, we explore to augment a 2.5D detection CNN with two different
bounding-box-level (or instance-level) uncertainty estimates, i.e., predictive
variance and Monte Carlo (MC) sample variance. Experiments are conducted for
lung nodule detection on LUNA16 dataset, a task where significant semantic
ambiguities can exist between nodules and non-nodules. Results show that our
method improves the evaluating score from 84.57% to 88.86% by utilizing a
combination of both types of variances. Moreover, we show the generated
uncertainty enables superior operating points compared to using the probability
threshold only, and can further boost the performance to 89.52%. Example nodule
detections are visualized to further illustrate the advantages of our method.
</p>
<a href="http://arxiv.org/abs/2012.12880" target="_blank">arXiv:2012.12880</a> [<a href="http://arxiv.org/pdf/2012.12880" target="_blank">pdf</a>]

<h2>Adversarial Momentum-Contrastive Pre-Training. (arXiv:2012.13154v2 [cs.CV] UPDATED)</h2>
<h3>Cong Xu, Min Yang</h3>
<p>Deep neural networks are vulnerable to semantic invariant corruptions and
imperceptible artificial perturbations. Although data augmentation can improve
the robustness against the former, it offers no guarantees against the latter.
Adversarial training, on the other hand, is quite the opposite. Recent studies
have shown that adversarial self-supervised pre-training is helpful to extract
the invariant representations under both data augmentations and adversarial
perturbations. Based on the MoCo's idea, this paper proposes a novel
adversarial momentum-contrastive (AMOC) pre-training approach, which designs
two dynamic memory banks to maintain the historical clean and adversarial
representations respectively, so as to exploit the discriminative
representations that are consistent in a long period. Compared with the
existing self-supervised pre-training approaches, AMOC can use a smaller batch
size and fewer training epochs but learn more robust features. Empirical
results show that the developed approach further improves the current
state-of-the-art adversarial robustness. Our code is available at
\url{https://github.com/MTandHJ/amoc}.
</p>
<a href="http://arxiv.org/abs/2012.13154" target="_blank">arXiv:2012.13154</a> [<a href="http://arxiv.org/pdf/2012.13154" target="_blank">pdf</a>]

<h2>A Tutorial on Sparse Gaussian Processes and Variational Inference. (arXiv:2012.13962v4 [cs.LG] UPDATED)</h2>
<h3>Felix Leibfried, Vincent Dutordoir, ST John, Nicolas Durrande</h3>
<p>Gaussian processes (GPs) provide a framework for Bayesian inference that can
offer principled uncertainty estimates for a large range of problems. For
example, if we consider regression problems with Gaussian likelihoods, a GP
model enjoys a posterior in closed form. However, identifying the posterior GP
scales cubically with the number of training examples and requires to store all
examples in memory. In order to overcome these obstacles, sparse GPs have been
proposed that approximate the true posterior GP with pseudo-training examples.
Importantly, the number of pseudo-training examples is user-defined and enables
control over computational and memory complexity. In the general case, sparse
GPs do not enjoy closed-form solutions and one has to resort to approximate
inference. In this context, a convenient choice for approximate inference is
variational inference (VI), where the problem of Bayesian inference is cast as
an optimization problem -- namely, to maximize a lower bound of the log
marginal likelihood. This paves the way for a powerful and versatile framework,
where pseudo-training examples are treated as optimization arguments of the
approximate posterior that are jointly identified together with hyperparameters
of the generative model (i.e. prior and likelihood). The framework can
naturally handle a wide scope of supervised learning problems, ranging from
regression with heteroscedastic and non-Gaussian likelihoods to classification
problems with discrete labels, but also multilabel problems. The purpose of
this tutorial is to provide access to the basic matter for readers without
prior knowledge in both GPs and VI. A proper exposition to the subject enables
also access to more recent advances (like importance-weighted VI as well as
inderdomain, multioutput and deep GPs) that can serve as an inspiration for new
research ideas.
</p>
<a href="http://arxiv.org/abs/2012.13962" target="_blank">arXiv:2012.13962</a> [<a href="http://arxiv.org/pdf/2012.13962" target="_blank">pdf</a>]

<h2>Analysis of Dominant Classes in Universal Adversarial Perturbations. (arXiv:2012.14352v2 [cs.LG] UPDATED)</h2>
<h3>Jon Vadillo, Roberto Santana, Jose A. Lozano</h3>
<p>The reasons why Deep Neural Networks are susceptible to being fooled by
adversarial examples remains an open discussion. Indeed, many different
strategies can be employed to efficiently generate adversarial attacks, some of
them relying on different theoretical justifications. Among these strategies,
universal (input-agnostic) perturbations are of particular interest, due to
their capability to fool a network independently of the input in which the
perturbation is applied. In this work, we investigate an intriguing phenomenon
of universal perturbations, which has been reported previously in the
literature, yet without a proven justification: universal perturbations change
the predicted classes for most inputs into one particular (dominant) class,
even if this behavior is not specified during the creation of the perturbation.
In order to justify the cause of this phenomenon, we propose a number of
hypotheses and experimentally test them using a speech command classification
problem in the audio domain as a testbed. Our analyses reveal interesting
properties of universal perturbations, suggest new methods to generate such
attacks and provide an explanation of dominant classes, under both a geometric
and a data-feature perspective.
</p>
<a href="http://arxiv.org/abs/2012.14352" target="_blank">arXiv:2012.14352</a> [<a href="http://arxiv.org/pdf/2012.14352" target="_blank">pdf</a>]

<h2>Subtype-aware Unsupervised Domain Adaptation for Medical Diagnosis. (arXiv:2101.00318v2 [cs.CV] UPDATED)</h2>
<h3>Xiaofeng Liu, Xiongchang Liu, Bo Hu, Wenxuan Ji, Fangxu Xing, Jun Lu, Jane You, C.-C. Jay Kuo, Georges El Fakhri, Jonghye Woo</h3>
<p>Recent advances in unsupervised domain adaptation (UDA) show that
transferable prototypical learning presents a powerful means for class
conditional alignment, which encourages the closeness of cross-domain class
centroids. However, the cross-domain inner-class compactness and the underlying
fine-grained subtype structure remained largely underexplored. In this work, we
propose to adaptively carry out the fine-grained subtype-aware alignment by
explicitly enforcing the class-wise separation and subtype-wise compactness
with intermediate pseudo labels. Our key insight is that the unlabeled subtypes
of a class can be divergent to one another with different conditional and label
shifts, while inheriting the local proximity within a subtype. The cases of
with or without the prior information on subtype numbers are investigated to
discover the underlying subtype structure in an online fashion. The proposed
subtype-aware dynamic UDA achieves promising results on medical diagnosis
tasks.
</p>
<a href="http://arxiv.org/abs/2101.00318" target="_blank">arXiv:2101.00318</a> [<a href="http://arxiv.org/pdf/2101.00318" target="_blank">pdf</a>]

<h2>A Hybrid Attention Mechanism for Weakly-Supervised Temporal Action Localization. (arXiv:2101.00545v2 [cs.CV] UPDATED)</h2>
<h3>Ashraful Islam, Chengjiang Long, Richard J. Radke</h3>
<p>Weakly supervised temporal action localization is a challenging vision task
due to the absence of ground-truth temporal locations of actions in the
training videos. With only video-level supervision during training, most
existing methods rely on a Multiple Instance Learning (MIL) framework to
predict the start and end frame of each action category in a video. However,
the existing MIL-based approach has a major limitation of only capturing the
most discriminative frames of an action, ignoring the full extent of an
activity. Moreover, these methods cannot model background activity effectively,
which plays an important role in localizing foreground activities. In this
paper, we present a novel framework named HAM-Net with a hybrid attention
mechanism which includes temporal soft, semi-soft and hard attentions to
address these issues. Our temporal soft attention module, guided by an
auxiliary background class in the classification module, models the background
activity by introducing an "action-ness" score for each video snippet.
Moreover, our temporal semi-soft and hard attention modules, calculating two
attention scores for each video snippet, help to focus on the less
discriminative frames of an action to capture the full action boundary. Our
proposed approach outperforms recent state-of-the-art methods by at least 2.2%
mAP at IoU threshold 0.5 on the THUMOS14 dataset, and by at least 1.3% mAP at
IoU threshold 0.75 on the ActivityNet1.2 dataset. Code can be found at:
https://github.com/asrafulashiq/hamnet.
</p>
<a href="http://arxiv.org/abs/2101.00545" target="_blank">arXiv:2101.00545</a> [<a href="http://arxiv.org/pdf/2101.00545" target="_blank">pdf</a>]

<h2>A Predictive Model for Geographic Distributions of Mangroves. (arXiv:2101.00967v2 [cs.LG] UPDATED)</h2>
<h3>Lynn Wahab, Ezzat Chebaro, Jad Ismail, Amir Nasrelddine, Ali El-Zein</h3>
<p>Climate change is an impending disaster which is of pressing concern more and
more every year. Countless efforts have been made to study the long-term
effects of climate change on agriculture, land resources, and biodiversity.
Studies involving marine life, however, are less prevalent in the literature.
Our research studies the available data on the population of mangroves (groups
of shrubs or small trees living in saline coastal intertidal zones) and their
correlations to climate change variables, specifically, temperature, heat
content, various sea levels, and sea salinity. Mangroves are especially
relevant to oceanic ecosystems because of their protective nature towards other
marine life, as well as their high absorption rate of carbon dioxide, and their
ability to withstand varying levels of salinity of our coasts. The change in
global distribution was studied based on global distributions of the previous
year, as well as ocean heat content, salinity, temperature, halosteric sea
level, thermosteric sea level, and total steric sea level. The best performing
predictive model was a support vector regressor, which yielded a correlation
coefficient of 0.9998.
</p>
<a href="http://arxiv.org/abs/2101.00967" target="_blank">arXiv:2101.00967</a> [<a href="http://arxiv.org/pdf/2101.00967" target="_blank">pdf</a>]

<h2>VIS30K: A Collection of Figures and Tables from IEEE Visualization Conference Publications. (arXiv:2101.01036v3 [cs.CV] UPDATED)</h2>
<h3>Jian Chen, Meng Ling, Rui Li, Petra Isenberg, Tobias Isenberg, Michael Sedlmair, Torsten M&#xf6;ller, Robert S. Laramee, Han-Wei Shen, Katharina W&#xfc;nsche, Qiru Wang</h3>
<p>We present the VIS30K dataset, a collection of 29,689 images that represents
30 years of figures and tables from each track of the IEEE Visualization
conference series (Vis, SciVis, InfoVis, VAST). VIS30K's comprehensive coverage
of the scientific literature in visualization not only reflects the progress of
the field but also enables researchers to study the evolution of the
state-of-the-art and to find relevant work based on graphical content. We
describe the dataset and our semi-automatic collection process, which couples
convolutional neural networks (CNN) with curation. Extracting figures and
tables semi-automatically allows us to verify that no images are overlooked or
extracted erroneously. To improve quality further, we engaged in a peer-search
process for high-quality figures from early IEEE Visualization papers. With the
resulting data, we also contribute VISImageNavigator (VIN,
visimagenavigator.github.io), a web-based tool that facilitates searching and
exploring VIS30K by author names, paper keywords, title and abstract, and
years.
</p>
<a href="http://arxiv.org/abs/2101.01036" target="_blank">arXiv:2101.01036</a> [<a href="http://arxiv.org/pdf/2101.01036" target="_blank">pdf</a>]

<h2>Multi-Model Least Squares-Based Recomputation Framework for Large Data Analysis. (arXiv:2101.01271v3 [cs.LG] UPDATED)</h2>
<h3>Wandong Zhang (1 and 2), QM Jonathan Wu (1), Yimin Yang (2 and 3), WG Will Zhao (2 and 4), Hui Zhang (5) ((1) University of Windsor, (2) Lakehead University, (3) Vector Institute for Artificial Intelligence, (4) CEGEP de Ste Foy, (5) Hunan University)</h3>
<p>Most multilayer least squares (LS)-based neural networks are structured with
two separate stages: unsupervised feature encoding and supervised pattern
classification. Once the unsupervised learning is finished, the latent encoding
would be fixed without supervised fine-tuning. However, in complex tasks such
as handling the ImageNet dataset, there are often many more clues that can be
directly encoded, while the unsupervised learning, by definition cannot know
exactly what is useful for a certain task. This serves as the motivation to
retrain the latent space representations to learn some clues that unsupervised
learning has not yet learned. In particular, the error matrix from the output
layer is pulled back to each hidden layer, and the parameters of the hidden
layer are recalculated with Moore-Penrose (MP) inverse for more generalized
representations. In this paper, a recomputation-based multilayer network using
MP inverse (RML-MP) is developed. A sparse RML-MP (SRML-MP) model to boost the
performance of RML-MP is then proposed. The experimental results with varying
training samples (from 3 K to 1.8 M) show that the proposed models provide
better generalization performance than most representation learning algorithms.
</p>
<a href="http://arxiv.org/abs/2101.01271" target="_blank">arXiv:2101.01271</a> [<a href="http://arxiv.org/pdf/2101.01271" target="_blank">pdf</a>]

<h2>Optimized Execution of PDDL Plans using Behavior Trees. (arXiv:2101.01964v2 [cs.RO] UPDATED)</h2>
<h3>Francisco Mart&#xed;n, Matteo Morelli, Huascar Espinoza, Francisco J. R. Lera, Vicente Matell&#xe1;n</h3>
<p>Robots need task planning to sequence and execute actions toward achieving
their goals. On the other hand, Behavior Trees provide a mathematical model for
specifying plan execution in an intrinsically composable, reactive, and robust
way. PDDL (Planning Domain Definition Language) has become the standard
description language for most planners. In this paper, we present a novel
algorithm to systematically create behavior trees from PDDL plans to execute
them. This approach uses the execution graph of the plan to generate a behavior
tree. The most remarkable contribution of this approach is the algorithm to
build a Behavior Tree that optimizes its execution by paralyzing actions,
applicable to any plan, taking into account the actions' causal relationships.
We demonstrate the improvement in the execution of plans in mobile robots using
the ROS2 Planning System framework.
</p>
<a href="http://arxiv.org/abs/2101.01964" target="_blank">arXiv:2101.01964</a> [<a href="http://arxiv.org/pdf/2101.01964" target="_blank">pdf</a>]

<h2>Who's a Good Boy? Reinforcing Canine Behavior in Real-Time using Machine Learning. (arXiv:2101.02380v2 [cs.CV] UPDATED)</h2>
<h3>Jason Stock, Tom Cavey</h3>
<p>In this paper we outline the development methodology for an automatic dog
treat dispenser which combines machine learning and embedded hardware to
identify and reward dog behaviors in real-time. Using machine learning
techniques for training an image classification model we identify three
behaviors of our canine companions: "sit", "stand", and "lie down" with up to
92% test accuracy and 39 frames per second. We evaluate a variety of neural
network architectures, interpretability methods, model quantization and
optimization techniques to develop a model specifically for an NVIDIA Jetson
Nano. We detect the aforementioned behaviors in real-time and reinforce
positive actions by making inference on the Jetson Nano and transmitting a
signal to a servo motor to release rewards from a treat delivery apparatus.
</p>
<a href="http://arxiv.org/abs/2101.02380" target="_blank">arXiv:2101.02380</a> [<a href="http://arxiv.org/pdf/2101.02380" target="_blank">pdf</a>]

<h2>HAVANA: Hierarchical and Variation-Normalized Autoencoder for Person Re-identification. (arXiv:2101.02568v2 [cs.CV] UPDATED)</h2>
<h3>Jiawei Ren, Xiao Ma, Chen Xu, Haiyu Zhao, Shuai Yi</h3>
<p>Person Re-Identification (Re-ID) is of great importance to the many video
surveillance systems. Learning discriminative features for Re-ID remains a
challenge due to the large variations in the image space, e.g., continuously
changing human poses, illuminations and point of views. In this paper, we
propose HAVANA, a novel extensible, light-weight HierArchical and
VAriation-Normalized Autoencoder that learns features robust to intra-class
variations. In contrast to existing generative approaches that prune the
variations with heavy extra supervised signals, HAVANA suppresses the
intra-class variations with a Variation-Normalized Autoencoder trained with no
additional supervision. We also introduce a novel Jensen-Shannon triplet loss
for contrastive distribution learning in Re-ID. In addition, we present
Hierarchical Variation Distiller, a hierarchical VAE to factorize the latent
representation and explicitly model the variations. To the best of our
knowledge, HAVANA is the first VAE-based framework for person ReID.
</p>
<a href="http://arxiv.org/abs/2101.02568" target="_blank">arXiv:2101.02568</a> [<a href="http://arxiv.org/pdf/2101.02568" target="_blank">pdf</a>]

<h2>Demand Forecasting for Platelet Usage: from Univariate Time Series to Multivariate Models. (arXiv:2101.02305v1 [cs.LG] CROSS LISTED)</h2>
<h3>Maryam Motamedi, Na Li, Douglas G. Down, Nancy M. Heddle</h3>
<p>Platelet products are both expensive and have very short shelf lives. As
usage rates for platelets are highly variable, the effective management of
platelet demand and supply is very important yet challenging. The primary goal
of this paper is to present an efficient forecasting model for platelet demand
at Canadian Blood Services (CBS). To accomplish this goal, four different
demand forecasting methods, ARIMA (Auto Regressive Moving Average), Prophet,
lasso regression (least absolute shrinkage and selection operator) and LSTM
(Long Short-Term Memory) networks are utilized and evaluated. We use a large
clinical dataset for a centralized blood distribution centre for four hospitals
in Hamilton, Ontario, spanning from 2010 to 2018 and consisting of daily
platelet transfusions along with information such as the product
specifications, the recipients' characteristics, and the recipients' laboratory
test results. This study is the first to utilize different methods from
statistical time series models to data-driven regression and a machine learning
technique for platelet transfusion using clinical predictors and with different
amounts of data. We find that the multivariate approaches have the highest
accuracy in general, however, if sufficient data are available, a simpler time
series approach such as ARIMA appears to be sufficient. We also comment on the
approach to choose clinical indicators (inputs) for the multivariate models.
</p>
<a href="http://arxiv.org/abs/2101.02305" target="_blank">arXiv:2101.02305</a> [<a href="http://arxiv.org/pdf/2101.02305" target="_blank">pdf</a>]

