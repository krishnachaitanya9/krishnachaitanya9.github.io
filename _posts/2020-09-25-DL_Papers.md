---
title: Latest Deep Learning Papers
date: 2020-11-08 21:00:01 +0800
featured-img: DL-logo
categories: [Deep_Learning]
tags: [Deep_Learning]
mathjax: Yes
---

<h1>Your arXiv Feed (149 Articles)</h1>
<h2>A Tree-structure Convolutional Neural Network for Temporal Features Exaction on Sensor-based Multi-resident Activity Recognition. (arXiv:2011.03042v1 [cs.CV])</h2>
<h3>Jingjing Cao, Fukang Guo, Xin Lai, Qiang Zhou, Jinshan Dai</h3>
<p>With the propagation of sensor devices applied in smart home, activity
recognition has ignited huge interest and most existing works assume that there
is only one habitant. While in reality, there are generally multiple residents
at home, which brings greater challenge to recognize activities. In addition,
many conventional approaches rely on manual time series data segmentation
ignoring the inherent characteristics of events and their heuristic
hand-crafted feature generation algorithms are difficult to exploit distinctive
features to accurately classify different activities. To address these issues,
we propose an end-to-end Tree-Structure Convolutional neural network based
framework for Multi-Resident Activity Recognition (TSC-MRAR). First, we treat
each sample as an event and obtain the current event embedding through the
previous sensor readings in the sliding window without splitting the time
series data. Then, in order to automatically generate the temporal features, a
tree-structure network is designed to derive the temporal dependence of nearby
readings. The extracted features are fed into the fully connected layer, which
can jointly learn the residents labels and the activity labels simultaneously.
Finally, experiments on CASAS datasets demonstrate the high performance in
multi-resident activity recognition of our model compared to state-of-the-art
techniques.
</p>
<a href="http://arxiv.org/abs/2011.03042" target="_blank">arXiv:2011.03042</a> [<a href="http://arxiv.org/pdf/2011.03042" target="_blank">pdf</a>]

<h2>Identifying and interpreting tuning dimensions in deep networks. (arXiv:2011.03043v1 [cs.LG])</h2>
<h3>Nolan S. Dey, J. Eric Taylor, Bryan P. Tripp, Alexander Wong, Graham W. Taylor</h3>
<p>In neuroscience, a tuning dimension is a stimulus attribute that accounts for
much of the activation variance of a group of neurons. These are commonly used
to decipher the responses of such groups. While researchers have attempted to
manually identify an analogue to these tuning dimensions in deep neural
networks, we are unaware of an automatic way to discover them. This work
contributes an unsupervised framework for identifying and interpreting "tuning
dimensions" in deep networks. Our method correctly identifies the tuning
dimensions of a synthetic Gabor filter bank and tuning dimensions of the first
two layers of InceptionV1 trained on ImageNet.
</p>
<a href="http://arxiv.org/abs/2011.03043" target="_blank">arXiv:2011.03043</a> [<a href="http://arxiv.org/pdf/2011.03043" target="_blank">pdf</a>]

<h2>MorphEyes: Variable Baseline Stereo For Quadrotor Navigation. (arXiv:2011.03077v1 [cs.RO])</h2>
<h3>Nitin J. Sanket, Chahat Deep Singh, Varun Asthana, Cornelia Ferm&#xfc;ller, Yiannis Aloimonos</h3>
<p>Morphable design and depth-based visual control are two upcoming trends
leading to advancements in the field of quadrotor autonomy. Stereo-cameras have
struck the perfect balance of weight and accuracy of depth estimation but
suffer from the problem of depth range being limited and dictated by the
baseline chosen at design time. In this paper, we present a framework for
quadrotor navigation based on a stereo camera system whose baseline can be
adapted on-the-fly. We present a method to calibrate the system at a small
number of discrete baselines and interpolate the parameters for the entire
baseline range. We present an extensive theoretical analysis of calibration and
synchronization errors. We showcase three different applications of such a
system for quadrotor navigation: (a) flying through a forest, (b) flying
through an unknown shaped/location static/dynamic gap, and (c) accurate 3D pose
detection of an independently moving object. We show that our variable baseline
system is more accurate and robust in all three scenarios. To our knowledge,
this is the first work that applies the concept of morphable design to achieve
a variable baseline stereo vision system on a quadrotor.
</p>
<a href="http://arxiv.org/abs/2011.03077" target="_blank">arXiv:2011.03077</a> [<a href="http://arxiv.org/pdf/2011.03077" target="_blank">pdf</a>]

<h2>A Tunable Robust Pruning Framework Through Dynamic Network Rewiring of DNNs. (arXiv:2011.03083v1 [cs.CV])</h2>
<h3>Souvik Kundu, Mahdi Nazemi, Peter A. Beerel, Massoud Pedram</h3>
<p>This paper presents a dynamic network rewiring (DNR) method to generate
pruned deep neural network (DNN) models that are robust against adversarial
attacks yet maintain high accuracy on clean images. In particular, the
disclosed DNR method is based on a unified constrained optimization formulation
using a hybrid loss function that merges ultra-high model compression with
robust adversarial training. This training strategy dynamically adjusts
inter-layer connectivity based on per-layer normalized momentum computed from
the hybrid loss function. In contrast to existing robust pruning frameworks
that require multiple training iterations, the proposed learning strategy
achieves an overall target pruning ratio with only a single training iteration
and can be tuned to support both irregular and structured channel pruning. To
evaluate the merits of DNR, experiments were performed with two widely accepted
models, namely VGG16 and ResNet-18, on CIFAR-10, CIFAR-100 as well as with
VGG16 on Tiny-ImageNet. Compared to the baseline uncompressed models, DNR
provides over20x compression on all the datasets with no significant drop in
either clean or adversarial classification accuracy. Moreover, our experiments
show that DNR consistently finds compressed models with better clean and
adversarial image classification performance than what is achievable through
state-of-the-art alternatives.
</p>
<a href="http://arxiv.org/abs/2011.03083" target="_blank">arXiv:2011.03083</a> [<a href="http://arxiv.org/pdf/2011.03083" target="_blank">pdf</a>]

<h2>RealAnt: An Open-Source Low-Cost Quadruped for Research in Real-World Reinforcement Learning. (arXiv:2011.03085v1 [cs.RO])</h2>
<h3>Rinu Boney, Jussi Sainio, Mikko Kaivola, Arno Solin, Juho Kannala</h3>
<p>Current robot platforms available for research are either very expensive or
unable to handle the abuse of exploratory controls in reinforcement learning.
We develop RealAnt, a minimal low-cost physical version of the popular 'Ant'
benchmark used in reinforcement learning. RealAnt costs only $410 in materials
and can be assembled in less than an hour. We validate the platform with
reinforcement learning experiments and provide baseline results on a set of
benchmark tasks. We demonstrate that the TD3 algorithm can learn to walk the
RealAnt from less than 45 minutes of experience. We also provide simulator
versions of the robot (with the same dimensions, state-action spaces, and
delayed noisy observations) in the MuJoCo and PyBullet simulators. We
open-source hardware designs, supporting software, and baseline results for
ease of reproducibility.
</p>
<a href="http://arxiv.org/abs/2011.03085" target="_blank">arXiv:2011.03085</a> [<a href="http://arxiv.org/pdf/2011.03085" target="_blank">pdf</a>]

<h2>Mining Functionally Related Genes with Semi-Supervised Learning. (arXiv:2011.03089v1 [cs.LG])</h2>
<h3>Kaiyu Shen, Razvan Bunescu, Sarah E. Wyatt</h3>
<p>The study of biological processes can greatly benefit from tools that
automatically predict gene functions or directly cluster genes based on shared
functionality. Existing data mining methods predict protein functionality by
exploiting data obtained from high-throughput experiments or meta-scale
information from public databases. Most existing prediction tools are targeted
at predicting protein functions that are described in the gene ontology (GO).
However, in many cases biologists wish to discover functionally related genes
for which GO terms are inadequate. In this paper, we introduce a rich set of
features and use them in conjunction with semisupervised learning approaches in
order to expand an initial set of seed genes to a larger cluster of
functionally related genes. Among all the semi-supervised methods that were
evaluated, the framework of learning with positive and unlabeled examples (LPU)
is shown to be especially appropriate for mining functionally related genes.
When evaluated on experimentally validated benchmark data, the LPU approaches1
significantly outperform a standard supervised learning algorithm as well as an
established state-of-the-art method. Given an initial set of seed genes, our
best performing approach could be used to mine functionally related genes in a
wide range of organisms.
</p>
<a href="http://arxiv.org/abs/2011.03089" target="_blank">arXiv:2011.03089</a> [<a href="http://arxiv.org/pdf/2011.03089" target="_blank">pdf</a>]

<h2>Towards Keypoint Guided Self-Supervised Depth Estimation. (arXiv:2011.03091v1 [cs.CV])</h2>
<h3>Kristijan Bartol, David Bojanic, Tomislav Petkovic, Tomislav Pribanic, Yago Diez Donoso</h3>
<p>This paper proposes to use keypoints as a self-supervision clue for learning
depth map estimation from a collection of input images. As ground truth depth
from real images is difficult to obtain, there are many unsupervised and
self-supervised approaches to depth estimation that have been proposed. Most of
these unsupervised approaches use depth map and ego-motion estimations to
reproject the pixels from the current image into the adjacent image from the
image collection. Depth and ego-motion estimations are evaluated based on pixel
intensity differences between the correspondent original and reprojected
pixels. Instead of reprojecting the individual pixels, we propose to first
select image keypoints in both images and then reproject and compare the
correspondent keypoints of the two images. The keypoints should describe the
distinctive image features well. By learning a deep model with and without the
keypoint extraction technique, we show that using the keypoints improve the
depth estimation learning. We also propose some future directions for
keypoint-guided learning of structure-from-motion problems.
</p>
<a href="http://arxiv.org/abs/2011.03091" target="_blank">arXiv:2011.03091</a> [<a href="http://arxiv.org/pdf/2011.03091" target="_blank">pdf</a>]

<h2>End-to-end Deep Learning Methods for Automated Damage Detection in Extreme Events at Various Scales. (arXiv:2011.03098v1 [cs.CV])</h2>
<h3>Yongsheng Bai, Halil Sezen, Alper Yilmaz</h3>
<p>Robust Mask R-CNN (Mask Regional Convolu-tional Neural Network) methods are
proposed and tested for automatic detection of cracks on structures or their
components that may be damaged during extreme events, such as earth-quakes. We
curated a new dataset with 2,021 labeled images for training and validation and
aimed to find end-to-end deep neural networks for crack detection in the field.
With data augmentation and parameters fine-tuning, Path Aggregation Network
(PANet) with spatial attention mechanisms and High-resolution Network (HRNet)
are introduced into Mask R-CNNs. The tests on three public datasets with low-
or high-resolution images demonstrate that the proposed methods can achieve a
big improvement over alternative networks, so the proposed method may be
sufficient for crack detection for a variety of scales in real applications.
</p>
<a href="http://arxiv.org/abs/2011.03098" target="_blank">arXiv:2011.03098</a> [<a href="http://arxiv.org/pdf/2011.03098" target="_blank">pdf</a>]

<h2>Smart Time-Multiplexing of Quads Solves the Multicamera Interference Problem. (arXiv:2011.03102v1 [cs.CV])</h2>
<h3>Tomislav Pribanic, Tomislav Petkovic, David Bojanic, Kristijan Bartol</h3>
<p>Time-of-flight (ToF) cameras are becoming increasingly popular for 3D
imaging. Their optimal usage has been studied from the several aspects. One of
the open research problems is the possibility of a multicamera interference
problem when two or more ToF cameras are operating simultaneously. In this work
we present an efficient method to synchronize multiple operating ToF cameras.
Our method is based on the time-division multiplexing, but unlike traditional
time multiplexing, it does not decrease the effective camera frame rate.
Additionally, for unsynchronized cameras, we provide a robust method to extract
from their corresponding video streams, frames which are not subject to
multicamera interference problem. We demonstrate our approach through a series
of experiments and with a different level of support available for triggering,
ranging from a hardware triggering to purely random software triggering.
</p>
<a href="http://arxiv.org/abs/2011.03102" target="_blank">arXiv:2011.03102</a> [<a href="http://arxiv.org/pdf/2011.03102" target="_blank">pdf</a>]

<h2>Can Human Sex Be Learned Using Only 2D Body Keypoint Estimations?. (arXiv:2011.03104v1 [cs.CV])</h2>
<h3>Kristijan Bartol, Tomislav Pribanic, David Bojanic, Tomislav Petkovic</h3>
<p>In this paper, we analyze human male and female sex recognition problem and
present a fully automated classification system using only 2D keypoints. The
keypoints represent human joints. A keypoint set consists of 15 joints and the
keypoint estimations are obtained using an OpenPose 2D keypoint detector. We
learn a deep learning model to distinguish males and females using the
keypoints as input and binary labels as output. We use two public datasets in
the experimental section - 3DPeople and PETA. On PETA dataset, we report a 77%
accuracy. We provide model performance details on both PETA and 3DPeople. To
measure the effect of noisy 2D keypoint detections on the performance, we run
separate experiments on 3DPeople ground truth and noisy keypoint data. Finally,
we extract a set of factors that affect the classification accuracy and propose
future work. The advantage of the approach is that the input is small and the
architecture is simple, which enables us to run many experiments and keep the
real-time performance in inference. The source code, with the experiments and
data preparation scripts, are available on GitHub
(https://github.com/kristijanbartol/human-sex-classifier).
</p>
<a href="http://arxiv.org/abs/2011.03104" target="_blank">arXiv:2011.03104</a> [<a href="http://arxiv.org/pdf/2011.03104" target="_blank">pdf</a>]

<h2>Learning Rolling Shutter Correction from Real Data without Camera Motion Assumption. (arXiv:2011.03106v1 [cs.CV])</h2>
<h3>Jiawei Mo, Md Jahidul Islam, Junaed Sattar</h3>
<p>The rolling shutter mechanism in modern cameras generates distortions as the
images are formed on the sensor through a row-by-row readout process; this is
highly undesirable for photography and vision-based algorithms (e.g.,
structure-from-motion and visual SLAM). In this paper, we propose a deep neural
network to predict depth and camera poses for single-frame rolling shutter
correction. Compared to the state-of-the-art, the proposed method has no
assumptions on camera motion. It is enabled by training on real images captured
by rolling shutter cameras instead of synthetic ones generated with certain
motion assumption. Consequently, the proposed method performs better for real
rolling shutter images. This makes it possible for numerous vision-based
algorithms to use imagery captured using rolling shutter cameras and produce
highly accurate results. Our evaluations on the TUM rolling shutter dataset
using DSO and COLMAP validate the accuracy and robustness of the proposed
method.
</p>
<a href="http://arxiv.org/abs/2011.03106" target="_blank">arXiv:2011.03106</a> [<a href="http://arxiv.org/pdf/2011.03106" target="_blank">pdf</a>]

<h2>Convergent Algorithms for (Relaxed) Minimax Fairness. (arXiv:2011.03108v1 [cs.LG])</h2>
<h3>Emily Diana, Wesley Gill, Michael Kearns, Krishnaram Kenthapadi, Aaron Roth</h3>
<p>We consider a recently introduced framework in which fairness is measured by
worst-case outcomes across groups, rather than by the more standard
$\textit{difference}$ between group outcomes. In this framework we provide
provably convergent $\textit{oracle-efficient}$ learning algorithms (or
equivalently, reductions to non-fair learning) for $\textit{minimax group
fairness}$. Here the goal is that of minimizing the maximum loss across all
groups, rather than equalizing group losses. Our algorithms apply to both
regression and classification settings and support both overall error and false
positive or false negative rates as the fairness measure of interest. They also
support relaxations of the fairness constraints, thus permitting study of the
tradeoff between overall accuracy and minimax fairness. We compare the
experimental behavior and performance of our algorithms across a variety of
fairness-sensitive data sets and show cases in which minimax fairness is
strictly and strongly preferable to equal outcome notions, in the sense that
equal outcomes can only be obtained by artificially inflating the harm
inflicted on some groups compared to what they suffer under the minimax
solution.
</p>
<a href="http://arxiv.org/abs/2011.03108" target="_blank">arXiv:2011.03108</a> [<a href="http://arxiv.org/pdf/2011.03108" target="_blank">pdf</a>]

<h2>Uncertainty-Aware Vehicle Orientation Estimation for Joint Detection-Prediction Models. (arXiv:2011.03114v1 [cs.CV])</h2>
<h3>Henggang Cui, Fang-Chieh Chou, Jake Charland, Carlos Vallespi-Gonzalez, Nemanja Djuric</h3>
<p>Object detection is a critical component of a self-driving system, tasked
with inferring the current states of the surrounding traffic actors. While
there exist a number of studies on the problem of inferring the position and
shape of vehicle actors, understanding actors' orientation remains a challenge
for existing state-of-the-art detectors. Orientation is an important property
for downstream modules of an autonomous system, particularly relevant for
motion prediction of stationary or reversing actors where current approaches
struggle. We focus on this task and present a method that extends the existing
models that perform joint object detection and motion prediction, allowing us
to more accurately infer vehicle orientations. In addition, the approach is
able to quantify prediction uncertainty, outputting the probability that the
inferred orientation is flipped, which allows for improved motion prediction
and safer autonomous operations. Empirical results show the benefits of the
approach, obtaining state-of-the-art performance on the open-sourced nuScenes
data set.
</p>
<a href="http://arxiv.org/abs/2011.03114" target="_blank">arXiv:2011.03114</a> [<a href="http://arxiv.org/pdf/2011.03114" target="_blank">pdf</a>]

<h2>LBGP: Learning Based Goal Planning for Autonomous Following in Front. (arXiv:2011.03125v1 [cs.RO])</h2>
<h3>Payam Nikdel, Richard Vaughan, Mo Chen</h3>
<p>This paper investigates a hybrid solution which combines deep reinforcement
learning (RL) and classical trajectory planning for the following in front
application. Here, an autonomous robot aims to stay ahead of a person as the
person freely walks around. Following in front is a challenging problem as the
user's intended trajectory is unknown and needs to be estimated, explicitly or
implicitly, by the robot. In addition, the robot needs to find a feasible way
to safely navigate ahead of human trajectory. Our deep RL module implicitly
estimates human trajectory and produces short-term navigational goals to guide
the robot. These goals are used by a trajectory planner to smoothly navigate
the robot to the short-term goals, and eventually in front of the user. We
employ curriculum learning in the deep RL module to efficiently achieve a high
return. Our system outperforms the state-of-the-art in following ahead and is
more reliable compared to end-to-end alternatives in both the simulation and
real world experiments. In contrast to a pure deep RL approach, we demonstrate
zero-shot transfer of the trained policy from simulation to the real world.
</p>
<a href="http://arxiv.org/abs/2011.03125" target="_blank">arXiv:2011.03125</a> [<a href="http://arxiv.org/pdf/2011.03125" target="_blank">pdf</a>]

<h2>STReSSD: Sim-To-Real from Sound for Stochastic Dynamics. (arXiv:2011.03136v1 [cs.RO])</h2>
<h3>Carolyn Matl, Yashraj Narang, Dieter Fox, Ruzena Bajcsy, Fabio Ramos</h3>
<p>Sound is an information-rich medium that captures dynamic physical events.
This work presents STReSSD, a framework that uses sound to bridge the
simulation-to-reality gap for stochastic dynamics, demonstrated for the
canonical case of a bouncing ball. A physically-motivated noise model is
presented to capture stochastic behavior of the balls upon collision with the
environment. A likelihood-free Bayesian inference framework is used to infer
the parameters of the noise model, as well as a material property called the
coefficient of restitution, from audio observations. The same inference
framework and the calibrated stochastic simulator are then used to learn a
probabilistic model of ball dynamics. The predictive capabilities of the
dynamics model are tested in two robotic experiments. First, open-loop
predictions anticipate probabilistic success of bouncing a ball into a cup. The
second experiment integrates audio perception with a robotic arm to track and
deflect a bouncing ball in real-time. We envision that this work is a step
towards integrating audio-based inference for dynamic robotic tasks.
Experimental results can be viewed at https://youtu.be/b7pOrgZrArk.
</p>
<a href="http://arxiv.org/abs/2011.03136" target="_blank">arXiv:2011.03136</a> [<a href="http://arxiv.org/pdf/2011.03136" target="_blank">pdf</a>]

<h2>Ellipse Loss for Scene-Compliant Motion Prediction. (arXiv:2011.03139v1 [cs.CV])</h2>
<h3>Henggang Cui, Hoda Shajari, Sai Yalamanchi, Nemanja Djuric</h3>
<p>Motion prediction is a critical part of self-driving technology, responsible
for inferring future behavior of traffic actors in autonomous vehicle's
surroundings. In order to ensure safe and efficient operations, prediction
models need to output accurate trajectories that obey the map constraints. In
this paper, we address this task and propose a novel ellipse loss that allows
the models to better reason about scene compliance and predict more realistic
trajectories. Ellipse loss penalizes off-road predictions directly in a
supervised manner, by projecting the output trajectories into the top-down map
frame using a differentiable trajectory rasterizer module. Moreover, it takes
into account the actor dimension and orientation, providing more direct
training signals to the model. We applied ellipse loss to a recently proposed
state-of-the-art joint detection-prediction model to showcase its benefits.
Evaluation results on a large-scale autonomous driving data set strongly
indicate that our method allows for more accurate and more realistic trajectory
predictions.
</p>
<a href="http://arxiv.org/abs/2011.03139" target="_blank">arXiv:2011.03139</a> [<a href="http://arxiv.org/pdf/2011.03139" target="_blank">pdf</a>]

<h2>Contact Localization for Robot Arms in Motion without Torque Sensing. (arXiv:2011.03142v1 [cs.RO])</h2>
<h3>Jacky Liang, Oliver Kroemer</h3>
<p>Detecting and localizing contacts is essential for robot manipulators to
perform contact-rich tasks in unstructured environments. While robot skins can
localize contacts on the surface of robot arms, these sensors are not yet
robust or easily accessible. As such, prior works have explored using
proprioceptive observations, such as joint velocities and torques, to perform
contact localization. Many past approaches assume the robot is static during
contact incident, a single contact is made at a time, or having access to
accurate dynamics models and joint torque sensing. In this work, we relax these
assumptions and propose using Domain Randomization to train a neural network to
localize contacts of robot arms in motion without joint torque observations.
Our method uses a novel cylindrical projection encoding of the robot arm
surface, which allows the network to use convolution layers to process input
features and transposed convolution layers to predict contacts. The trained
network achieves a contact detection accuracy of 91.5% and a mean contact
localization error of 3.0cm. We further demonstrate an application of the
contact localization model in an obstacle mapping task, evaluated in both
simulation and the real world.
</p>
<a href="http://arxiv.org/abs/2011.03142" target="_blank">arXiv:2011.03142</a> [<a href="http://arxiv.org/pdf/2011.03142" target="_blank">pdf</a>]

<h2>Predicting special care during the COVID-19 pandemic: A machine learning approach. (arXiv:2011.03143v1 [cs.LG])</h2>
<h3>Vitor Bezzan, Cleber D. Rocco</h3>
<p>More than ever COVID-19 is putting pressure on health systems all around the
world, especially in Brazil. In this study we propose an analytical approach
based on statistics and machine learning that uses lab exam data coming from
patients to predict whether patients are going to require special care
(hospitalisation in regular or special-care units). We also predict the number
of days the patients will stay under such care. The two-step procedure
developed uses Bayesian Optimisation to select the best model among several
candidates leads us to final models that achieve 0.94 area under ROC curve
performance for the first target and 1.87 root mean squared error for the
second target (which is a 77% improvement over the mean baseline), making our
model ready to be deployed as a decision system that could be available for
everyone interested. The analytical approach can be used in other diseases and
can help the planning hospital capacity.
</p>
<a href="http://arxiv.org/abs/2011.03143" target="_blank">arXiv:2011.03143</a> [<a href="http://arxiv.org/pdf/2011.03143" target="_blank">pdf</a>]

<h2>RetinaGAN: An Object-aware Approach to Sim-to-Real Transfer. (arXiv:2011.03148v1 [cs.RO])</h2>
<h3>Daniel Ho, Kanishka Rao, Zhuo Xu, Eric Jang, Mohi Khansari, Yunfei Bai</h3>
<p>The success of deep reinforcement learning (RL) and imitation learning (IL)
in vision-based robotic manipulation typically hinges on the expense of large
scale data collection. With simulation, data to train a policy can be collected
efficiently at scale, but the visual gap between sim and real makes deployment
in the real world difficult. We introduce RetinaGAN, a generative adversarial
network (GAN) approach to adapt simulated images to realistic ones with
object-detection consistency. RetinaGAN is trained in an unsupervised manner
without task loss dependencies, and preserves general object structure and
texture in adapted images. We evaluate our method on three real world tasks:
grasping, pushing, and door opening. RetinaGAN improves upon the performance of
prior sim-to-real methods for RL-based object instance grasping and continues
to be effective even in the limited data regime. When applied to a pushing task
in a similar visual domain, RetinaGAN demonstrates transfer with no additional
real data requirements. We also show our method bridges the visual gap for a
novel door opening task using imitation learning in a new visual domain. Visit
the project website at https://retinagan.github.io/
</p>
<a href="http://arxiv.org/abs/2011.03148" target="_blank">arXiv:2011.03148</a> [<a href="http://arxiv.org/pdf/2011.03148" target="_blank">pdf</a>]

<h2>Affinity LCFCN: Learning to Segment Fish with Weak Supervision. (arXiv:2011.03149v1 [cs.CV])</h2>
<h3>Issam Laradji, Alzayat Saleh, Pau Rodriguez, Derek Nowrouzezahrai, Mostafa Rahimi Azghadi, David Vazquez</h3>
<p>Aquaculture industries rely on the availability of accurate fish body
measurements, e.g., length, width and mass. Manual methods that rely on
physical tools like rulers are time and labour intensive. Leading automatic
approaches rely on fully-supervised segmentation models to acquire these
measurements but these require collecting per-pixel labels -- also time
consuming and laborious: i.e., it can take up to two minutes per fish to
generate accurate segmentation labels, almost always requiring at least some
manual intervention. We propose an automatic segmentation model efficiently
trained on images labeled with only point-level supervision, where each fish is
annotated with a single click. This labeling process requires significantly
less manual intervention, averaging roughly one second per fish. Our approach
uses a fully convolutional neural network with one branch that outputs
per-pixel scores and another that outputs an affinity matrix. We aggregate
these two outputs using a random walk to obtain the final, refined per-pixel
segmentation output. We train the entire model end-to-end with an LCFCN loss,
resulting in our A-LCFCN method. We validate our model on the DeepFish dataset,
which contains many fish habitats from the north-eastern Australian region. Our
experimental results confirm that A-LCFCN outperforms a fully-supervised
segmentation model at fixed annotation budget. Moreover, we show that A-LCFCN
achieves better segmentation results than LCFCN and a standard baseline. We
have released the code at \url{https://github.com/IssamLaradji/affinity_lcfcn}.
</p>
<a href="http://arxiv.org/abs/2011.03149" target="_blank">arXiv:2011.03149</a> [<a href="http://arxiv.org/pdf/2011.03149" target="_blank">pdf</a>]

<h2>Efficient Hyperparameter Tuning with Dynamic Accuracy Derivative-Free Optimization. (arXiv:2011.03151v1 [cs.LG])</h2>
<h3>Matthias J. Ehrhardt, Lindon Roberts</h3>
<p>Many machine learning solutions are framed as optimization problems which
rely on good hyperparameters. Algorithms for tuning these hyperparameters
usually assume access to exact solutions to the underlying learning problem,
which is typically not practical. Here, we apply a recent dynamic accuracy
derivative-free optimization method to hyperparameter tuning, which allows
inexact evaluations of the learning problem while retaining convergence
guarantees. We test the method on the problem of learning elastic net weights
for a logistic classifier, and demonstrate its robustness and efficiency
compared to a fixed accuracy approach. This demonstrates a promising approach
for hyperparameter tuning, with both convergence guarantees and practical
performance.
</p>
<a href="http://arxiv.org/abs/2011.03151" target="_blank">arXiv:2011.03151</a> [<a href="http://arxiv.org/pdf/2011.03151" target="_blank">pdf</a>]

<h2>Confusable Learning for Large-class Few-Shot Classification. (arXiv:2011.03154v1 [cs.CV])</h2>
<h3>Bingcong Li, Bo Han, Zhuowei Wang, Jing Jiang, Guodong Long</h3>
<p>Few-shot image classification is challenging due to the lack of ample samples
in each class. Such a challenge becomes even tougher when the number of classes
is very large, i.e., the large-class few-shot scenario. In this novel scenario,
existing approaches do not perform well because they ignore confusable classes,
namely similar classes that are difficult to distinguish from each other. These
classes carry more information. In this paper, we propose a biased learning
paradigm called Confusable Learning, which focuses more on confusable classes.
Our method can be applied to mainstream meta-learning algorithms. Specifically,
our method maintains a dynamically updating confusion matrix, which analyzes
confusable classes in the dataset. Such a confusion matrix helps meta learners
to emphasize on confusable classes. Comprehensive experiments on Omniglot,
Fungi, and ImageNet demonstrate the efficacy of our method over
state-of-the-art baselines.
</p>
<a href="http://arxiv.org/abs/2011.03154" target="_blank">arXiv:2011.03154</a> [<a href="http://arxiv.org/pdf/2011.03154" target="_blank">pdf</a>]

<h2>Parametric Flatten-T Swish: An Adaptive Non-linear Activation Function For Deep Learning. (arXiv:2011.03155v1 [cs.LG])</h2>
<h3>Hock Hung Chieng, Noorhaniza Wahid, Pauline Ong</h3>
<p>Activation function is a key component in deep learning that performs
non-linear mappings between the inputs and outputs. Rectified Linear Unit
(ReLU) has been the most popular activation function across the deep learning
community. However, ReLU contains several shortcomings that can result in
inefficient training of the deep neural networks, these are: 1) the negative
cancellation property of ReLU tends to treat negative inputs as unimportant
information for the learning, resulting in a performance degradation; 2) the
inherent predefined nature of ReLU is unlikely to promote additional
flexibility, expressivity, and robustness to the networks; 3) the mean
activation of ReLU is highly positive and leads to bias shift effect in network
layers; and 4) the multilinear structure of ReLU restricts the non-linear
approximation power of the networks. To tackle these shortcomings, this paper
introduced Parametric Flatten-T Swish (PFTS) as an alternative to ReLU. By
taking ReLU as a baseline method, the experiments showed that PFTS improved
classification accuracy on SVHN dataset by 0.31%, 0.98%, 2.16%, 17.72%, 1.35%,
0.97%, 39.99%, and 71.83% on DNN-3A, DNN-3B, DNN-4, DNN- 5A, DNN-5B, DNN-5C,
DNN-6, and DNN-7, respectively. Besides, PFTS also achieved the highest mean
rank among the comparison methods. The proposed PFTS manifested higher
non-linear approximation power during training and thereby improved the
predictive performance of the networks.
</p>
<a href="http://arxiv.org/abs/2011.03155" target="_blank">arXiv:2011.03155</a> [<a href="http://arxiv.org/pdf/2011.03155" target="_blank">pdf</a>]

<h2>Wasserstein-based fairness interpretability framework for machine learning models. (arXiv:2011.03156v1 [cs.LG])</h2>
<h3>Alexey Miroshnikov, Konstandinos Kotsiopoulos, Ryan Franks, Arjun Ravi Kannan</h3>
<p>In this article, we introduce a fairness interpretability framework for
measuring and explaining bias in classification and regression models at the
level of a distribution. In our work, motivated by the ideas of Dwork et al.
(2012), we measure the model bias across sub-population distributions using the
Wasserstein metric. The transport theory characterization of the Wasserstein
metric allows us to take into account the sign of the bias across the model
distribution which in turn yields the decomposition of the model bias into
positive and negative components. To understand how predictors contribute to
the model bias, we introduce and theoretically characterize bias predictor
attributions called bias explanations. We also provide the formulation for the
bias explanations that take into account the impact of missing values. In
addition, motivated by the works of Strumbelj and Kononenko (2014) and Lundberg
and Lee (2017) we construct additive bias explanations by employing cooperative
game theory.
</p>
<a href="http://arxiv.org/abs/2011.03156" target="_blank">arXiv:2011.03156</a> [<a href="http://arxiv.org/pdf/2011.03156" target="_blank">pdf</a>]

<h2>Leveraging an Efficient and Semantic Location Embedding to Seek New Ports of Bike Share Services. (arXiv:2011.03158v1 [cs.LG])</h2>
<h3>Yuan Wang, Chenwei Wang, Yinan Ling, Keita Yokoyama, Hsin-Tai Wu, Yi Fang</h3>
<p>For short distance traveling in crowded urban areas, bike share services are
becoming popular owing to the flexibility and convenience. To expand the
service coverage, one of the key tasks is to seek new service ports, which
requires to well understand the underlying features of the existing service
ports. In this paper, we propose a new model, named for Efficient and Semantic
Location Embedding (ESLE), which carries both geospatial and semantic
information of the geo-locations. To generate ESLE, we first train a
multi-label model with a deep Convolutional Neural Network (CNN) by feeding the
static map-tile images and then extract location embedding vectors from the
model. Compared to most recent relevant literature, ESLE is not only much
cheaper in computation, but also easier to interpret via a systematic semantic
analysis. Finally, we apply ESLE to seek new service ports for NTT DOCOMO's
bike share services operated in Japan. The initial results demonstrate the
effectiveness of ESLE, and provide a few insights that might be difficult to
discover by using the conventional approaches.
</p>
<a href="http://arxiv.org/abs/2011.03158" target="_blank">arXiv:2011.03158</a> [<a href="http://arxiv.org/pdf/2011.03158" target="_blank">pdf</a>]

<h2>Learning Power Control for Cellular Systems with Heterogeneous Graph Neural Network. (arXiv:2011.03164v1 [cs.LG])</h2>
<h3>Jia Guo, Chenyang Yang</h3>
<p>Optimizing power control in multi-cell cellular networks with deep learning
enables such a non-convex problem to be implemented in real-time. When channels
are time-varying, the deep neural networks (DNNs) need to be re-trained
frequently, which calls for low training complexity. To reduce the number of
training samples and the size of DNN required to achieve good performance, a
promising approach is to embed the DNNs with priori knowledge. Since cellular
networks can be modelled as a graph, it is natural to employ graph neural
networks (GNNs) for learning, which exhibit permutation invariance (PI) and
equivalence (PE) properties. Unlike the homogeneous GNNs that have been used
for wireless problems, whose outputs are invariant or equivalent to arbitrary
permutations of vertexes, heterogeneous GNNs (HetGNNs), which are more
appropriate to model cellular networks, are only invariant or equivalent to
some permutations. If the PI or PE properties of the HetGNN do not match the
property of the task to be learned, the performance degrades dramatically. In
this paper, we show that the power control policy has a combination of
different PI and PE properties, and existing HetGNN does not satisfy these
properties. We then design a parameter sharing scheme for HetGNN such that the
learned relationship satisfies the desired properties. Simulation results show
that the sample complexity and the size of designed GNN for learning the
optimal power control policy in multi-user multi-cell networks are much lower
than the existing DNNs, when achieving the same sum rate loss from the
numerically obtained solutions.
</p>
<a href="http://arxiv.org/abs/2011.03164" target="_blank">arXiv:2011.03164</a> [<a href="http://arxiv.org/pdf/2011.03164" target="_blank">pdf</a>]

<h2>Neural Stochastic Contraction Metrics for Robust Control and Estimation. (arXiv:2011.03168v1 [cs.LG])</h2>
<h3>Hiroyasu Tsukamoto, Soon-Jo Chung, Jean-Jacques E. Slotine</h3>
<p>We present neural stochastic contraction metrics, a new design framework for
provably-stable robust control and estimation for a class of stochastic
nonlinear systems. It exploits a spectrally-normalized deep neural network to
construct a contraction metric, sampled via simplified convex optimization in
the stochastic setting. Spectral normalization constrains the state-derivatives
of the metric to be Lipschitz continuous, and thereby ensures exponential
boundedness of the mean squared distance of system trajectories under
stochastic disturbances. This framework allows autonomous agents to approximate
optimal stable control and estimation policies in real-time, and outperforms
existing nonlinear control and estimation techniques including the
state-dependent Riccati equation, iterative LQR, EKF, and deterministic neural
contraction metric method, as illustrated in simulations.
</p>
<a href="http://arxiv.org/abs/2011.03168" target="_blank">arXiv:2011.03168</a> [<a href="http://arxiv.org/pdf/2011.03168" target="_blank">pdf</a>]

<h2>GHFP: Gradually Hard Filter Pruning. (arXiv:2011.03170v1 [cs.CV])</h2>
<h3>Linhang Cai, Zhulin An, Yongjun Xu</h3>
<p>Filter pruning is widely used to reduce the computation of deep learning,
enabling the deployment of Deep Neural Networks (DNNs) in resource-limited
devices. Conventional Hard Filter Pruning (HFP) method zeroizes pruned filters
and stops updating them, thus reducing the search space of the model. On the
contrary, Soft Filter Pruning (SFP) simply zeroizes pruned filters, keeping
updating them in the following training epochs, thus maintaining the capacity
of the network. However, SFP, together with its variants, converges much slower
than HFP due to its larger search space. Our question is whether SFP-based
methods and HFP can be combined to achieve better performance and speed up
convergence. Firstly, we generalize SFP-based methods and HFP to analyze their
characteristics. Then we propose a Gradually Hard Filter Pruning (GHFP) method
to smoothly switch from SFP-based methods to HFP during training and pruning,
thus maintaining a large search space at first, gradually reducing the capacity
of the model to ensure a moderate convergence speed. Experimental results on
CIFAR-10/100 show that our method achieves the state-of-the-art performance.
</p>
<a href="http://arxiv.org/abs/2011.03170" target="_blank">arXiv:2011.03170</a> [<a href="http://arxiv.org/pdf/2011.03170" target="_blank">pdf</a>]

<h2>Multi-output Gaussian Process Modulated Poisson Processes for Event Prediction. (arXiv:2011.03172v1 [stat.ML])</h2>
<h3>Salman Jahani, Shiyu Zhou, Dharmaraj Veeramani, Jeff Schmidt</h3>
<p>Prediction of events such as part replacement and failure events plays a
critical role in reliability engineering. Event stream data are commonly
observed in manufacturing and teleservice systems. Designing predictive models
for individual units based on such event streams is challenging and an
under-explored problem. In this work, we propose a non-parametric prognostic
framework for individualized event prediction based on the inhomogeneous
Poisson processes with a multivariate Gaussian convolution process (MGCP) prior
on the intensity functions. The MGCP prior on the intensity functions of the
inhomogeneous Poisson processes maps data from similar historical units to the
current unit under study which facilitates sharing of information and allows
for analysis of flexible event patterns. To facilitate inference, we derive a
variational inference scheme for learning and estimation of parameters in the
resulting MGCP modulated Poisson process model. Experimental results are shown
on both synthetic data as well as real-world data for fleet based event
prediction.
</p>
<a href="http://arxiv.org/abs/2011.03172" target="_blank">arXiv:2011.03172</a> [<a href="http://arxiv.org/pdf/2011.03172" target="_blank">pdf</a>]

<h2>There is no trade-off: enforcing fairness can improve accuracy. (arXiv:2011.03173v1 [stat.ML])</h2>
<h3>Subha Maity, Debarghya Mukherjee, Mikhail Yurochkin, Yuekai Sun</h3>
<p>One of the main barriers to the broader adoption of algorithmic fairness in
machine learning is the trade-off between fairness and performance of ML
models: many practitioners are unwilling to sacrifice the performance of their
ML model for fairness. In this paper, we show that this trade-off may not be
necessary. If the algorithmic biases in an ML model are due to sampling biases
in the training data, then enforcing algorithmic fairness may improve the
performance of the ML model on unbiased test data. We study conditions under
which enforcing algorithmic fairness helps practitioners learn the Bayes
decision rule for (unbiased) test data from biased training data. We also
demonstrate the practical implications of our theoretical results in real-world
ML tasks.
</p>
<a href="http://arxiv.org/abs/2011.03173" target="_blank">arXiv:2011.03173</a> [<a href="http://arxiv.org/pdf/2011.03173" target="_blank">pdf</a>]

<h2>ULSD: Unified Line Segment Detection across Pinhole, Fisheye, and Spherical Cameras. (arXiv:2011.03174v1 [cs.CV])</h2>
<h3>Hao Li, Huai Yu, Wen Yang, Lei Yu, Sebastian Scherer</h3>
<p>Line segment detection is essential for high-level tasks in computer vision
and robotics. Currently, most stateof-the-art (SOTA) methods are dedicated to
detecting straight line segments in undistorted pinhole images, thus
distortions on fisheye or spherical images may largely degenerate their
performance. Targeting at the unified line segment detection (ULSD) for both
distorted and undistorted images, we propose to represent line segments with
the Bezier curve model. Then the line segment detection is tackled by the
Bezier curve regression with an end-to-end network, which is model-free and
without any undistortion preprocessing. Experimental results on the pinhole,
fisheye, and spherical image datasets validate the superiority of the proposed
ULSD to the SOTA methods both in accuracy and efficiency (40.6fps for pinhole
images). The source code is available at
https://github.com/lh9171338/Unified-LineSegment-Detection.
</p>
<a href="http://arxiv.org/abs/2011.03174" target="_blank">arXiv:2011.03174</a> [<a href="http://arxiv.org/pdf/2011.03174" target="_blank">pdf</a>]

<h2>On the Ergodicity, Bias and Asymptotic Normality of Randomized Midpoint Sampling Method. (arXiv:2011.03176v1 [stat.ML])</h2>
<h3>Ye He, Krishnakumar Balasubramanian, Murat A. Erdogdu</h3>
<p>The randomized midpoint method, proposed by [SL19], has emerged as an optimal
discretization procedure for simulating the continuous time Langevin
diffusions. Focusing on the case of strong-convex and smooth potentials, in
this paper, we analyze several probabilistic properties of the randomized
midpoint discretization method for both overdamped and underdamped Langevin
diffusions. We first characterize the stationary distribution of the discrete
chain obtained with constant step-size discretization and show that it is
biased away from the target distribution. Notably, the step-size needs to go to
zero to obtain asymptotic unbiasedness. Next, we establish the asymptotic
normality for numerical integration using the randomized midpoint method and
highlight the relative advantages and disadvantages over other discretizations.
Our results collectively provide several insights into the behavior of the
randomized midpoint discretization method, including obtaining confidence
intervals for numerical integrations.
</p>
<a href="http://arxiv.org/abs/2011.03176" target="_blank">arXiv:2011.03176</a> [<a href="http://arxiv.org/pdf/2011.03176" target="_blank">pdf</a>]

<h2>Beyond Marginal Uncertainty: How Accurately can Bayesian Regression Models Estimate Posterior Predictive Correlations?. (arXiv:2011.03178v1 [cs.LG])</h2>
<h3>Chaoqi Wang, Shengyang Sun, Roger Grosse</h3>
<p>While uncertainty estimation is a well-studied topic in deep learning, most
such work focuses on marginal uncertainty estimates, i.e. the predictive mean
and variance at individual input locations. But it is often more useful to
estimate predictive correlations between the function values at different input
locations. In this paper, we consider the problem of benchmarking how
accurately Bayesian models can estimate predictive correlations. We first
consider a downstream task which depends on posterior predictive correlations:
transductive active learning (TAL). We find that TAL makes better use of
models' uncertainty estimates than ordinary active learning, and recommend this
as a benchmark for evaluating Bayesian models. Since TAL is too expensive and
indirect to guide development of algorithms, we introduce two metrics which
more directly evaluate the predictive correlations and which can be computed
efficiently: meta-correlations (i.e. the correlations between the models
correlation estimates and the true values), and cross-normalized likelihoods
(XLL). We validate these metrics by demonstrating their consistency with TAL
performance and obtain insights about the relative performance of current
Bayesian neural net and Gaussian process models.
</p>
<a href="http://arxiv.org/abs/2011.03178" target="_blank">arXiv:2011.03178</a> [<a href="http://arxiv.org/pdf/2011.03178" target="_blank">pdf</a>]

<h2>FedSL: Federated Split Learning on Distributed Sequential Data in Recurrent Neural Networks. (arXiv:2011.03180v1 [cs.LG])</h2>
<h3>Ali Abedi, Shehroz S. Khan</h3>
<p>Federated Learning (FL) and Split Learning (SL) are privacy-preserving
Machine-Learning (ML) techniques that enable training ML models over data
distributed among clients without requiring direct access to their raw data.
Existing FL and SL approaches work on horizontally or vertically partitioned
data and cannot handle sequentially partitioned data where segments of
multiple-segment sequential data are distributed across clients. In this paper,
we propose a novel federated split learning framework, FedSL, to train models
on distributed sequential data. The most common ML models to train on
sequential data are Recurrent Neural Networks (RNNs). Since the proposed
framework is privacy preserving, segments of multiple-segment sequential data
cannot be shared between clients or between clients and server. To circumvent
this limitation, we propose a novel SL approach tailored for RNNs. A RNN is
split into sub-networks, and each sub-network is trained on one client
containing single segments of multiple-segment training sequences. During local
training, the sub-networks on different clients communicate with each other to
capture latent dependencies between consecutive segments of multiple-segment
sequential data on different clients, but without sharing raw data or complete
model parameters. After training local sub-networks with local sequential data
segments, all clients send their sub-networks to a federated server where
sub-networks are aggregated to generate a global model. The experimental
results on simulated and real-world datasets demonstrate that the proposed
method successfully train models on distributed sequential data, while
preserving privacy, and outperforms previous FL and centralized learning
approaches in terms of achieving higher accuracy in fewer communication rounds.
</p>
<a href="http://arxiv.org/abs/2011.03180" target="_blank">arXiv:2011.03180</a> [<a href="http://arxiv.org/pdf/2011.03180" target="_blank">pdf</a>]

<h2>Learning Online Data Association. (arXiv:2011.03183v1 [cs.LG])</h2>
<h3>Yilun Du, Joshua Tenenbaum, Tomas Lozano-Perez, Leslie Kaelbling</h3>
<p>When an agent interacts with a complex environment, it receives a stream of
percepts in which it may detect entities, such as objects or people. To build
up a coherent, low-variance estimate of the underlying state, it is necessary
to fuse information from multiple detections over time. To do this fusion, the
agent must decide which detections to associate with one another. We address
this data-association problem in the setting of an online filter, in which each
observation is processed by aggregating into an existing object hypothesis.
Classic methods with strong probabilistic foundations exist, but they are
computationally expensive and require models that can be difficult to acquire.
In this work, we use the deep-learning tools of sparse attention and
representation learning to learn a machine that processes a stream of
detections and outputs a set of hypotheses about objects in the world. We
evaluate this approach on simple clustering problems, problems with dynamics,
and a complex image-based domain. We find that it generalizes well from short
to long observation sequences and from a few to many hypotheses, outperforming
other learning approaches and classical non-learning methods.
</p>
<a href="http://arxiv.org/abs/2011.03183" target="_blank">arXiv:2011.03183</a> [<a href="http://arxiv.org/pdf/2011.03183" target="_blank">pdf</a>]

<h2>Revisiting Model-Agnostic Private Learning: Faster Rates and Active Learning. (arXiv:2011.03186v1 [cs.LG])</h2>
<h3>Chong Liu, Yuqing Zhu, Kamalika Chaudhuri, Yu-Xiang Wang</h3>
<p>The Private Aggregation of Teacher Ensembles (PATE) framework is one of the
most promising recent approaches in differentially private learning. Existing
theoretical analysis shows that PATE consistently learns any VC-classes in the
realizable setting, but falls short in explaining its success in more general
cases where the error rate of the optimal classifier is bounded away from zero.
We fill in this gap by introducing the Tsybakov Noise Condition (TNC) and
establish stronger and more interpretable learning bounds. These bounds provide
new insights into when PATE works and improve over existing results even in the
narrower realizable setting. We also investigate the compelling idea of using
active learning for saving privacy budget. The novel components in the proofs
include a more refined analysis of the majority voting classifier -- which
could be of independent interest -- and an observation that the synthetic
"student" learning problem is nearly realizable by construction under the
Tsybakov noise condition.
</p>
<a href="http://arxiv.org/abs/2011.03186" target="_blank">arXiv:2011.03186</a> [<a href="http://arxiv.org/pdf/2011.03186" target="_blank">pdf</a>]

<h2>KompaRe: A Knowledge Graph Comparative Reasoning System. (arXiv:2011.03189v1 [cs.AI])</h2>
<h3>Lihui Liu, Boxin Du, Heng Ji, Hanghang Tong</h3>
<p>Reasoning is a fundamental capability for harnessing valuable insight,
knowledge and patterns from knowledge graphs. Existing work has primarily been
focusing on point-wise reasoning, including search, link predication, entity
prediction, subgraph matching and so on. This paper introduces comparative
reasoning over knowledge graphs, which aims to infer the commonality and
inconsistency with respect to multiple pieces of clues. We envision that the
comparative reasoning will complement and expand the existing point-wise
reasoning over knowledge graphs. In detail, we develop KompaRe, the first of
its kind prototype system that provides comparative reasoning capability over
large knowledge graphs. We present both the system architecture and its core
algorithms, including knowledge segment extraction, pairwise reasoning and
collective reasoning. Empirical evaluations demonstrate the efficacy of the
proposed KompaRe.
</p>
<a href="http://arxiv.org/abs/2011.03189" target="_blank">arXiv:2011.03189</a> [<a href="http://arxiv.org/pdf/2011.03189" target="_blank">pdf</a>]

<h2>Explainable AI meets Healthcare: A Study on Heart Disease Dataset. (arXiv:2011.03195v1 [cs.LG])</h2>
<h3>Devam Dave, Het Naik, Smiti Singhal, Pankesh Patel</h3>
<p>With the increasing availability of structured and unstructured data and the
swift progress of analytical techniques, Artificial Intelligence (AI) is
bringing a revolution to the healthcare industry. With the increasingly
indispensable role of AI in healthcare, there are growing concerns over the
lack of transparency and explainability in addition to potential bias
encountered by predictions of the model. This is where Explainable Artificial
Intelligence (XAI) comes into the picture. XAI increases the trust placed in an
AI system by medical practitioners as well as AI researchers, and thus,
eventually, leads to an increasingly widespread deployment of AI in healthcare.

In this paper, we present different interpretability techniques. The aim is
to enlighten practitioners on the understandability and interpretability of
explainable AI systems using a variety of techniques available which can be
very advantageous in the health-care domain. Medical diagnosis model is
responsible for human life and we need to be confident enough to treat a
patient as instructed by a black-box model. Our paper contains examples based
on the heart disease dataset and elucidates on how the explainability
techniques should be preferred to create trustworthiness while using AI systems
in healthcare.
</p>
<a href="http://arxiv.org/abs/2011.03195" target="_blank">arXiv:2011.03195</a> [<a href="http://arxiv.org/pdf/2011.03195" target="_blank">pdf</a>]

<h2>Resource-Constrained Federated Learning with Heterogeneous Labels and Models. (arXiv:2011.03206v1 [cs.LG])</h2>
<h3>Gautham Krishna Gudur, Bala Shyamala Balaji, Satheesh K. Perepu</h3>
<p>Various IoT applications demand resource-constrained machine learning
mechanisms for different applications such as pervasive healthcare, activity
monitoring, speech recognition, real-time computer vision, etc. This
necessitates us to leverage information from multiple devices with few
communication overheads. Federated Learning proves to be an extremely viable
option for distributed and collaborative machine learning. Particularly,
on-device federated learning is an active area of research, however, there are
a variety of challenges in addressing statistical (non-IID data) and model
heterogeneities. In addition, in this paper we explore a new challenge of
interest -- to handle label heterogeneities in federated learning. To this end,
we propose a framework with simple $\alpha$-weighted federated aggregation of
scores which leverages overlapping information gain across labels, while saving
bandwidth costs in the process. Empirical evaluation on Animals-10 dataset
(with 4 labels for effective elucidation of results) indicates an average
deterministic accuracy increase of at least ~16.7%. We also demonstrate the
on-device capabilities of our proposed framework by experimenting with
federated learning and inference across different iterations on a Raspberry Pi
2, a single-board computing platform.
</p>
<a href="http://arxiv.org/abs/2011.03206" target="_blank">arXiv:2011.03206</a> [<a href="http://arxiv.org/pdf/2011.03206" target="_blank">pdf</a>]

<h2>Learning a Geometric Representation for Data-Efficient Depth Estimation via Gradient Field and Contrastive Loss. (arXiv:2011.03207v1 [cs.CV])</h2>
<h3>Dongseok Shim, H. Jin Kim</h3>
<p>Estimating a depth map from a single RGB image has been investigated widely
for localization, mapping, and 3-dimensional object detection. Recent studies
on a single-view depth estimation are mostly based on deep Convolutional neural
Networks (ConvNets) which require a large amount of training data paired with
densely annotated labels. Depth annotation tasks are both expensive and
inefficient, so it is inevitable to leverage RGB images which can be collected
very easily to boost the performance of ConvNets without depth labels. However,
most self-supervised learning algorithms are focused on capturing the semantic
information of images to improve the performance in classification or object
detection, not in depth estimation. In this paper, we show that existing
self-supervised methods do not perform well on depth estimation and propose a
gradient-based self-supervised learning algorithm with momentum contrastive
loss to help ConvNets extract the geometric information with unlabeled images.
As a result, the network can estimate the depth map accurately with a
relatively small amount of annotated data. To show that our method is
independent of the model structure, we evaluate our method with two different
monocular depth estimation algorithms. Our method outperforms the previous
state-of-the-art self-supervised learning algorithms and shows the efficiency
of labeled data in triple compared to random initialization on the NYU Depth v2
dataset.
</p>
<a href="http://arxiv.org/abs/2011.03207" target="_blank">arXiv:2011.03207</a> [<a href="http://arxiv.org/pdf/2011.03207" target="_blank">pdf</a>]

<h2>Task-relevant Representation Learning for Networked Robotic Perception. (arXiv:2011.03216v1 [cs.RO])</h2>
<h3>Manabu Nakanoya, Sandeep Chinchali, Alexandros Anemogiannis, Akul Datta, Sachin Katti, Marco Pavone</h3>
<p>Today, even the most compute-and-power constrained robots can measure
complex, high data-rate video and LIDAR sensory streams. Often, such robots,
ranging from low-power drones to space and subterranean rovers, need to
transmit high-bitrate sensory data to a remote compute server if they are
uncertain or cannot scalably run complex perception or mapping tasks locally.
However, today's representations for sensory data are mostly designed for
human, not robotic, perception and thus often waste precious compute or
wireless network resources to transmit unimportant parts of a scene that are
unnecessary for a high-level robotic task. This paper presents an algorithm to
learn task-relevant representations of sensory data that are co-designed with a
pre-trained robotic perception model's ultimate objective. Our algorithm
aggressively compresses robotic sensory data by up to 11x more than competing
methods. Further, it achieves high accuracy and robust generalization on
diverse tasks including Mars terrain classification with low-power deep
learning accelerators, neural motion planning, and environmental timeseries
classification.
</p>
<a href="http://arxiv.org/abs/2011.03216" target="_blank">arXiv:2011.03216</a> [<a href="http://arxiv.org/pdf/2011.03216" target="_blank">pdf</a>]

<h2>User-Dependent Neural Sequence Models for Continuous-Time Event Data. (arXiv:2011.03231v1 [stat.ML])</h2>
<h3>Alex Boyd, Robert Bamler, Stephan Mandt, Padhraic Smyth</h3>
<p>Continuous-time event data are common in applications such as individual
behavior data, financial transactions, and medical health records. Modeling
such data can be very challenging, in particular for applications with many
different types of events, since it requires a model to predict the event types
as well as the time of occurrence. Recurrent neural networks that parameterize
time-varying intensity functions are the current state-of-the-art for
predictive modeling with such data. These models typically assume that all
event sequences come from the same data distribution. However, in many
applications event sequences are generated by different sources, or users, and
their characteristics can be very different. In this paper, we extend the broad
class of neural marked point process models to mixtures of latent embeddings,
where each mixture component models the characteristic traits of a given user.
Our approach relies on augmenting these models with a latent variable that
encodes user characteristics, represented by a mixture model over user behavior
that is trained via amortized variational inference. We evaluate our methods on
four large real-world datasets and demonstrate systematic improvements from our
approach over existing work for a variety of predictive metrics such as
log-likelihood, next event ranking, and source-of-sequence identification.
</p>
<a href="http://arxiv.org/abs/2011.03231" target="_blank">arXiv:2011.03231</a> [<a href="http://arxiv.org/pdf/2011.03231" target="_blank">pdf</a>]

<h2>Efficient image retrieval using multi neural hash codes and bloom filters. (arXiv:2011.03234v1 [cs.CV])</h2>
<h3>Sourin Chakrabarti</h3>
<p>This paper aims to deliver an efficient and modified approach for image
retrieval using multiple neural hash codes and limiting the number of queries
using bloom filters by identifying false positives beforehand. Traditional
approaches involving neural networks for image retrieval tasks tend to use
higher layers for feature extraction. But it has been seen that the activations
of lower layers have proven to be more effective in a number of scenarios. In
our approach, we have leveraged the use of local deep convolutional neural
networks which combines the powers of both the features of lower and higher
layers for creating feature maps which are then compressed using PCA and fed to
a bloom filter after binary sequencing using a modified multi k-means approach.
The feature maps obtained are further used in the image retrieval process in a
hierarchical coarse-to-fine manner by first comparing the images in the higher
layers for semantically similar images and then gradually moving towards the
lower layers searching for structural similarities. While searching, the neural
hashes for the query image are again calculated and queried in the bloom filter
which tells us whether the query image is absent in the set or maybe present.
If the bloom filter doesn't necessarily rule out the query, then it goes into
the image retrieval process. This approach can be particularly helpful in cases
where the image store is distributed since the approach supports parallel
querying.
</p>
<a href="http://arxiv.org/abs/2011.03234" target="_blank">arXiv:2011.03234</a> [<a href="http://arxiv.org/pdf/2011.03234" target="_blank">pdf</a>]

<h2>Channel Pruning via Multi-Criteria based on Weight Dependency. (arXiv:2011.03240v1 [cs.CV])</h2>
<h3>Yangchun Yan, Chao Li, Rongzuo Guo, Kang Yang, Yongjun Xu</h3>
<p>Channel pruning has demonstrated its effectiveness in compressing ConvNets.
In many prior arts, the importance of an output feature map is only determined
by its associated filter. However, these methods ignore a small part of weights
in the next layer which disappear as the feature map is removed. They ignore
the dependency of the weights, so that, a part of weights are pruned without
being evaluated. In addition, many pruning methods use only one criterion for
evaluation, and find a sweet-spot of pruning structure and accuracy in a
trial-and-error fashion, which can be time-consuming. To address the above
issues, we proposed a channel pruning algorithm via multi-criteria based on
weight dependency, CPMC, which can compress a variety of models efficiently. We
design the importance of the feature map in three aspects, including its
associated weight value, computational cost and parameter quantity. Use the
phenomenon of weight dependency, We get the importance by assessing its
associated filter and the corresponding partial weights of the next layer. Then
we use global normalization to achieve cross-layer comparison. Our method can
compress various CNN models, including VGGNet, ResNet and DenseNet, on various
image classification datasets. Extensive experiments have shown CPMC
outperforms the others significantly.
</p>
<a href="http://arxiv.org/abs/2011.03240" target="_blank">arXiv:2011.03240</a> [<a href="http://arxiv.org/pdf/2011.03240" target="_blank">pdf</a>]

<h2>Sequential Minimal Optimization for One-Class Slab Support Vector Machine. (arXiv:2011.03243v1 [cs.LG])</h2>
<h3>Bagesh Kumar, Ayush Sinha, Sourin Chakrabarti, Aashutosh Khandelwal, Harsh Jain, Prof. O.P.Vyas</h3>
<p>One Class Slab Support Vector Machines (OCSSVM) have turned out to be better
in terms of accuracy in certain classes of classification problems than the
traditional SVMs and One Class SVMs or even other One class classifiers. This
paper proposes fast training method for One Class Slab SVMs using an updated
Sequential Minimal Optimization (SMO) which divides the multi variable
optimization problem to smaller sub problems of size two that can then be
solved analytically. The results indicate that this training method scales
better to large sets of training data than other Quadratic Programming (QP)
solvers.
</p>
<a href="http://arxiv.org/abs/2011.03243" target="_blank">arXiv:2011.03243</a> [<a href="http://arxiv.org/pdf/2011.03243" target="_blank">pdf</a>]

<h2>Hi-UCD: A Large-scale Dataset for Urban Semantic Change Detection in Remote Sensing Imagery. (arXiv:2011.03247v1 [cs.CV])</h2>
<h3>Shiqi Tian, Yanfei Zhong, Ailong Ma, Zhuo Zheng</h3>
<p>With the acceleration of the urban expansion, urban change detection (UCD),
as a significant and effective approach, can provide the change information
with respect to geospatial objects for dynamical urban analysis. However,
existing datasets suffer from three bottlenecks: (1) lack of high spatial
resolution images; (2) lack of semantic annotation; (3) lack of long-range
multi-temporal images. In this paper, we propose a large scale benchmark
dataset, termed Hi-UCD. This dataset uses aerial images with a spatial
resolution of 0.1 m provided by the Estonia Land Board, including three-time
phases, and semantically annotated with nine classes of land cover to obtain
the direction of ground objects change. It can be used for detecting and
analyzing refined urban changes. We benchmark our dataset using some classic
methods in binary and multi-class change detection. Experimental results show
that Hi-UCD is challenging yet useful. We hope the Hi-UCD can become a strong
benchmark accelerating future research.
</p>
<a href="http://arxiv.org/abs/2011.03247" target="_blank">arXiv:2011.03247</a> [<a href="http://arxiv.org/pdf/2011.03247" target="_blank">pdf</a>]

<h2>ASFGNN: Automated Separated-Federated Graph Neural Network. (arXiv:2011.03248v1 [cs.LG])</h2>
<h3>Longfei Zheng, Jun Zhou, Chaochao Chen, Bingzhe Wu, Li Wang, Benyu Zhang</h3>
<p>Graph Neural Networks (GNNs) have achieved remarkable performance by taking
advantage of graph data. The success of GNN models always depends on rich
features and adjacent relationships. However, in practice, such data are
usually isolated by different data owners (clients) and thus are likely to be
Non-Independent and Identically Distributed (Non-IID). Meanwhile, considering
the limited network status of data owners, hyper-parameters optimization for
collaborative learning approaches is time-consuming in data isolation
scenarios. To address these problems, we propose an Automated
Separated-Federated Graph Neural Network (ASFGNN) learning paradigm. ASFGNN
consists of two main components, i.e., the training of GNN and the tuning of
hyper-parameters. Specifically, to solve the data Non-IID problem, we first
propose a separated-federated GNN learning model, which decouples the training
of GNN into two parts: the message passing part that is done by clients
separately, and the loss computing part that is learnt by clients federally. To
handle the time-consuming parameter tuning problem, we leverage Bayesian
optimization technique to automatically tune the hyper-parameters of all the
clients. We conduct experiments on benchmark datasets and the results
demonstrate that ASFGNN significantly outperforms the naive federated GNN, in
terms of both accuracy and parameter-tuning efficiency.
</p>
<a href="http://arxiv.org/abs/2011.03248" target="_blank">arXiv:2011.03248</a> [<a href="http://arxiv.org/pdf/2011.03248" target="_blank">pdf</a>]

<h2>Learning Behavior Trees with Genetic Programming in Unpredictable Environments. (arXiv:2011.03252v1 [cs.RO])</h2>
<h3>Matteo Iovino, Jonathan Styrud, Pietro Falco, Christian Smith</h3>
<p>Modern industrial applications require robots to be able to operate in
unpredictable environments, and programs to be created with a minimal effort,
as there may be frequent changes to the task. In this paper, we show that
genetic programming can be effectively used to learn the structure of a
behavior tree (BT) to solve a robotic task in an unpredictable environment.
Moreover, we propose to use a simple simulator for the learning and demonstrate
that the learned BTs can solve the same task in a realistic simulator, reaching
convergence without the need for task specific heuristics. The learned solution
is tolerant to faults, making our method appealing for real robotic
applications.
</p>
<a href="http://arxiv.org/abs/2011.03252" target="_blank">arXiv:2011.03252</a> [<a href="http://arxiv.org/pdf/2011.03252" target="_blank">pdf</a>]

<h2>Communication-efficient Decentralized Local SGD over Undirected Networks. (arXiv:2011.03255v1 [cs.LG])</h2>
<h3>Tiancheng Qin, S. Rasoul Etesami, C&#xe9;sar A. Uribe</h3>
<p>We consider the distributed learning problem where a network of $n$ agents
seeks to minimize a global function $F$. Agents have access to $F$ through
noisy gradients, and they can locally communicate with their neighbors a
network. We study the Decentralized Local SDG method, where agents perform a
number of local gradient steps and occasionally exchange information with their
neighbors. Previous algorithmic analysis efforts have focused on the specific
network topology (star topology) where a leader node aggregates all agents'
information. We generalize that setting to an arbitrary network by analyzing
the trade-off between the number of communication rounds and the computational
effort of each agent. We bound the expected optimality gap in terms of the
number of iterates $T$, the number of workers $n$, and the spectral gap of the
underlying network. Our main results show that by using only $R=\Omega(n)$
communication rounds, one can achieve an error that scales as $O({1}/{nT})$,
where the number of communication rounds is independent of $T$ and only depends
on the number of agents. Finally, we provide numerical evidence of our
theoretical results through experiments on real and synthetic data.
</p>
<a href="http://arxiv.org/abs/2011.03255" target="_blank">arXiv:2011.03255</a> [<a href="http://arxiv.org/pdf/2011.03255" target="_blank">pdf</a>]

<h2>Trust Issues: Uncertainty Estimation Does Not Enable Reliable OOD Detection On Medical Tabular Data. (arXiv:2011.03274v1 [cs.LG])</h2>
<h3>Dennis Ulmer, Lotta Meijerink, Giovanni Cin&#xe0;</h3>
<p>When deploying machine learning models in high-stakes real-world environments
such as health care, it is crucial to accurately assess the uncertainty
concerning a model's prediction on abnormal inputs. However, there is a
scarcity of literature analyzing this problem on medical data, especially on
mixed-type tabular data such as Electronic Health Records. We close this gap by
presenting a series of tests including a large variety of contemporary
uncertainty estimation techniques, in order to determine whether they are able
to identify out-of-distribution (OOD) patients. In contrast to previous work,
we design tests on realistic and clinically relevant OOD groups, and run
experiments on real-world medical data. We find that almost all techniques fail
to achieve convincing results, partly disagreeing with earlier findings.
</p>
<a href="http://arxiv.org/abs/2011.03274" target="_blank">arXiv:2011.03274</a> [<a href="http://arxiv.org/pdf/2011.03274" target="_blank">pdf</a>]

<h2>Sample-efficient Reinforcement Learning in Robotic Table Tennis. (arXiv:2011.03275v1 [cs.RO])</h2>
<h3>Jonas Tebbe, Lukas Krauch, Yapeng Gao, Andreas Zell</h3>
<p>Reinforcement learning (RL) has recently shown impressive success in various
computer games and simulations. Most of these successes are based on numerous
episodes to be learned from. For typical robotic applications, however, the
number of feasible attempts is very limited. In this paper we present a
sample-efficient RL algorithm applied to the example of a table tennis robot.
In table tennis every stroke is different, of varying placement, speed and
spin. Therefore, an accurate return has be found depending on a
high-dimensional continuous state space. To make learning in few trials
possible the method is embedded into our robot system. In this way we can use a
one-step environment. The state space depends on the ball at hitting time
(position, velocity, spin) and the action is the racket state (orientation,
velocity) at hitting. An actor-critic based deterministic policy gradient
algorithm was developed for accelerated learning. Our approach shows
competitive performance in both simulation and on the real robot in different
challenging scenarios. Accurate results are always obtained within under 200
episodes of training. A demonstration video is provided as supplementary
material.
</p>
<a href="http://arxiv.org/abs/2011.03275" target="_blank">arXiv:2011.03275</a> [<a href="http://arxiv.org/pdf/2011.03275" target="_blank">pdf</a>]

<h2>"What's This?" -- Learning to Segment Unknown Objects from Manipulation Sequences. (arXiv:2011.03279v1 [cs.CV])</h2>
<h3>Wout Boerdijk, Martin Sundermeyer, Maximilian Durner, Rudolph Triebel</h3>
<p>We present a novel framework for self-supervised grasped object segmentation
with a robotic manipulator. Our method successively learns an agnostic
foreground segmentation followed by a distinction between manipulator and
object solely by observing the motion between consecutive RGB frames. In
contrast to previous approaches, we propose a single, end-to-end trainable
architecture which jointly incorporates motion cues and semantic knowledge.
Furthermore, while the motion of the manipulator and the object are substantial
cues for our algorithm, we present means to robustly deal with distraction
objects moving in the background, as well as with completely static scenes. Our
method neither depends on any visual registration of a kinematic robot or 3D
object models, nor on precise hand-eye calibration or any additional sensor
data. By extensive experimental evaluation we demonstrate the superiority of
our framework and provide detailed insights on its capability of dealing with
the aforementioned extreme cases of motion. We also show that training a
semantic segmentation network with the automatically labeled data achieves
results on par with manually annotated training data. Code and pretrained
models will be made publicly available.
</p>
<a href="http://arxiv.org/abs/2011.03279" target="_blank">arXiv:2011.03279</a> [<a href="http://arxiv.org/pdf/2011.03279" target="_blank">pdf</a>]

<h2>Event-VPR: End-to-End Weakly Supervised Network Architecture for Event-based Visual Place Recognition. (arXiv:2011.03290v1 [cs.CV])</h2>
<h3>Delei Kong, Zheng Fang, Haojia Li, Kuanxu Hou, Sonya Coleman, Dermot Kerr</h3>
<p>Traditional visual place recognition (VPR) methods generally use frame-based
cameras, which is easy to fail due to dramatic illumination changes or fast
motions. In this paper, we propose an end-to-end visual place recognition
network for event cameras, which can achieve good place recognition performance
in challenging environments. The key idea of the proposed algorithm is firstly
to characterize the event streams with the EST voxel grid, then extract
features using a convolution network, and finally aggregate features using an
improved VLAD network to realize end-to-end visual place recognition using
event streams. To verify the effectiveness of the proposed algorithm, we
compare the proposed method with classical VPR methods on the event-based
driving datasets (MVSEC, DDD17) and the synthetic datasets (Oxford RobotCar).
Experimental results show that the proposed method can achieve much better
performance in challenging scenarios. To our knowledge, this is the first
end-to-end event-based VPR method. The accompanying source code is available at
https://github.com/kongdelei/Event-VPR.
</p>
<a href="http://arxiv.org/abs/2011.03290" target="_blank">arXiv:2011.03290</a> [<a href="http://arxiv.org/pdf/2011.03290" target="_blank">pdf</a>]

<h2>Learning to Orient Surfaces by Self-supervised Spherical CNNs. (arXiv:2011.03298v1 [cs.CV])</h2>
<h3>Riccardo Spezialetti, Federico Stella, Marlon Marcon, Luciano Silva, Samuele Salti, Luigi Di Stefano</h3>
<p>Defining and reliably finding a canonical orientation for 3D surfaces is key
to many Computer Vision and Robotics applications. This task is commonly
addressed by handcrafted algorithms exploiting geometric cues deemed as
distinctive and robust by the designer. Yet, one might conjecture that humans
learn the notion of the inherent orientation of 3D objects from experience and
that machines may do so alike. In this work, we show the feasibility of
learning a robust canonical orientation for surfaces represented as point
clouds. Based on the observation that the quintessential property of a
canonical orientation is equivariance to 3D rotations, we propose to employ
Spherical CNNs, a recently introduced machinery that can learn equivariant
representations defined on the Special Orthogonal group SO(3). Specifically,
spherical correlations compute feature maps whose elements define 3D rotations.
Our method learns such feature maps from raw data by a self-supervised training
procedure and robustly selects a rotation to transform the input point cloud
into a learned canonical orientation. Thereby, we realize the first end-to-end
learning approach to define and extract the canonical orientation of 3D shapes,
which we aptly dub Compass. Experiments on several public datasets prove its
effectiveness at orienting local surface patches as well as whole objects.
</p>
<a href="http://arxiv.org/abs/2011.03298" target="_blank">arXiv:2011.03298</a> [<a href="http://arxiv.org/pdf/2011.03298" target="_blank">pdf</a>]

<h2>Deep coastal sea elements forecasting using U-Net based models. (arXiv:2011.03303v1 [cs.LG])</h2>
<h3>Jes&#xfa;s Garc&#xed;a Fern&#xe1;ndez, Ismail Alaoui Abdellaoui, Siamak Mehrkanoon</h3>
<p>Due to the development of deep learning techniques applied to satellite
imagery, weather forecasting that uses remote sensing data has also been the
subject of major progress. The present paper investigates multiple steps ahead
frame prediction for coastal sea elements in the Netherlands using U-Net based
architectures. Hourly data from the Copernicus observation programme spanned
over a period of 2 years has been used to train the models and make the
forecasting, including seasonal predictions. We propose a variation of the
U-Net architecture and also extend this novel model using residual connections,
parallel convolutions and asymmetric convolutions in order to propose three
additional architectures. In particular, we show that the architecture equipped
with parallel and asymmetric convolutions as well as skip connections is
particularly suited for this task, outperforming the other three discussed
models.
</p>
<a href="http://arxiv.org/abs/2011.03303" target="_blank">arXiv:2011.03303</a> [<a href="http://arxiv.org/pdf/2011.03303" target="_blank">pdf</a>]

<h2>Towards Efficient Scene Understanding via Squeeze Reasoning. (arXiv:2011.03308v1 [cs.CV])</h2>
<h3>Xiangtai Li, Xia Li, Ansheng You, Li Zhang, Guangliang Cheng, Kuiyuan Yang, Yunhai Tong, Zhouchen Lin</h3>
<p>Graph-based convolutional model such as non-local block has shown to be
effective for strengthening the context modeling ability in convolutional
neural networks (CNNs). However, its pixel-wise computational overhead is
prohibitive which renders it unsuitable for high resolution imagery. In this
paper, we explore the efficiency of context graph reasoning and propose a novel
framework called Squeeze Reasoning. Instead of propagating information on the
spatial map, we first learn to squeeze the input feature into a channel-wise
global vector and perform reasoning within the single vector where the
computation cost can be significantly reduced. Specifically, we build the node
graph in the vector where each node represents an abstract semantic concept.
The refined feature within the same semantic category results to be consistent,
which is thus beneficial for downstream tasks. We show that our approach can be
modularized as an end-to-end trained block and can be easily plugged into
existing networks. Despite its simplicity and being lightweight, our strategy
allows us to establish a new state-of-the-art on semantic segmentation and show
significant improvements with respect to strong, state-of-the-art baselines on
various other scene understanding tasks including object detection, instance
segmentation and panoptic segmentation. Code will be made available to foster
any further research
</p>
<a href="http://arxiv.org/abs/2011.03308" target="_blank">arXiv:2011.03308</a> [<a href="http://arxiv.org/pdf/2011.03308" target="_blank">pdf</a>]

<h2>Generative adversarial training of product of policies for robust and adaptive movement primitives. (arXiv:2011.03316v1 [cs.RO])</h2>
<h3>Emmanuel Pignat, Hakan Girgin, Sylvain Calinon</h3>
<p>In learning from demonstrations, many generative models of trajectories make
simplifying assumptions of independence. Correctness is sacrificed in the name
of tractability and speed of the learning phase.

The ignored dependencies, which often are the kinematic and dynamic
constraints of the system, are then only restored when synthesizing the motion,
which introduces possibly heavy distortions.

In this work, we propose to use those approximate trajectory distributions as
close-to-optimal discriminators in the popular generative adversarial framework
to stabilize and accelerate the learning procedure.

The two problems of adaptability and robustness are addressed with our
method.

In order to adapt the motions to varying contexts, we propose to use a
product of Gaussian policies defined in several parametrized task spaces.
Robustness to perturbations and varying dynamics is ensured with the use of
stochastic gradient descent and ensemble methods to learn the stochastic
dynamics. Two experiments are performed on a 7-DoF manipulator to validate the
approach.
</p>
<a href="http://arxiv.org/abs/2011.03316" target="_blank">arXiv:2011.03316</a> [<a href="http://arxiv.org/pdf/2011.03316" target="_blank">pdf</a>]

<h2>Kernel Dependence Network. (arXiv:2011.03320v1 [cs.LG])</h2>
<h3>Chieh Wu, Aria Masoomi, Arthur Gretton, Jennifer Dy</h3>
<p>We propose a greedy strategy to spectrally train a deep network for
multi-class classification. Each layer is defined as a composition of linear
weights with the feature map of a Gaussian kernel acting as the activation
function. At each layer, the linear weights are learned by maximizing the
dependence between the layer output and the labels using the Hilbert Schmidt
Independence Criterion (HSIC). By constraining the solution space on the
Stiefel Manifold, we demonstrate how our network construct (Kernel Dependence
Network or KNet) can be solved spectrally while leveraging the eigenvalues to
automatically find the width and the depth of the network. We theoretically
guarantee the existence of a solution for the global optimum while providing
insight into our network's ability to generalize.
</p>
<a href="http://arxiv.org/abs/2011.03320" target="_blank">arXiv:2011.03320</a> [<a href="http://arxiv.org/pdf/2011.03320" target="_blank">pdf</a>]

<h2>Understanding Double Descent Requires a Fine-Grained Bias-Variance Decomposition. (arXiv:2011.03321v1 [stat.ML])</h2>
<h3>Ben Adlam, Jeffrey Pennington</h3>
<p>Classical learning theory suggests that the optimal generalization
performance of a machine learning model should occur at an intermediate model
complexity, with simpler models exhibiting high bias and more complex models
exhibiting high variance of the predictive function. However, such a simple
trade-off does not adequately describe deep learning models that simultaneously
attain low bias and variance in the heavily overparameterized regime. A primary
obstacle in explaining this behavior is that deep learning algorithms typically
involve multiple sources of randomness whose individual contributions are not
visible in the total variance. To enable fine-grained analysis, we describe an
interpretable, symmetric decomposition of the variance into terms associated
with the randomness from sampling, initialization, and the labels. Moreover, we
compute the high-dimensional asymptotic behavior of this decomposition for
random feature kernel regression, and analyze the strikingly rich phenomenology
that arises. We find that the bias decreases monotonically with the network
width, but the variance terms exhibit non-monotonic behavior and can diverge at
the interpolation boundary, even in the absence of label noise. The divergence
is caused by the \emph{interaction} between sampling and initialization and can
therefore be eliminated by marginalizing over samples (i.e. bagging) \emph{or}
over the initial parameters (i.e. ensemble learning).
</p>
<a href="http://arxiv.org/abs/2011.03321" target="_blank">arXiv:2011.03321</a> [<a href="http://arxiv.org/pdf/2011.03321" target="_blank">pdf</a>]

<h2>Safe trajectory of a piece moved by a robot. (arXiv:2011.03330v1 [cs.RO])</h2>
<h3>Ernest Benedito, Oliver Bond, Thomas Babb, Juan R. Pacha, Sandeep Kumar, Joan Sol&#xe0;-Morales</h3>
<p>In this work, we propose a mathematical model for a physical problem based on
the movement of a metal piece held by a robot. Using the principles of Kirchoff
plate theory, a set of equations determining stresses and deformations caused
during the motion, have been provided. We also discuss possible numerical
treatment of these equations and finally, a solution to the one-dimensional
analog of the problem has been presented.
</p>
<a href="http://arxiv.org/abs/2011.03330" target="_blank">arXiv:2011.03330</a> [<a href="http://arxiv.org/pdf/2011.03330" target="_blank">pdf</a>]

<h2>Scalable Unsupervised Multi-Criteria Trajectory Segmentation and Driving Preference Mining. (arXiv:2011.03331v1 [cs.RO])</h2>
<h3>Florian Barth, Stefan Funke, Tobias Skovgaard Jepsen, Claudius Proissl</h3>
<p>We present analysis techniques for large trajectory data sets that aim to
provide a semantic understanding of trajectories reaching beyond them being
point sequences in time and space. The presented techniques use a driving
preference model w.r.t. road segment traversal costs, e.g., travel time and
distance, to analyze and explain trajectories.

In particular, we present trajectory mining techniques that can (a) find
interesting points within a trajectory indicating, e.g., a via-point, and (b)
recover the driving preferences of a driver based on their chosen trajectory.
We evaluate our techniques on the tasks of via-point identification and
personalized routing using a data set of more than 1 million vehicle
trajectories collected throughout Denmark during a 3-year period. Our
techniques can be implemented efficiently and are highly parallelizable,
allowing them to scale to millions or billions of trajectories.
</p>
<a href="http://arxiv.org/abs/2011.03331" target="_blank">arXiv:2011.03331</a> [<a href="http://arxiv.org/pdf/2011.03331" target="_blank">pdf</a>]

<h2>Occlusion-Aware Search for Object Retrieval in Clutter. (arXiv:2011.03334v1 [cs.RO])</h2>
<h3>Wissam Bejjani, Wisdom C. Agboh, Mehmet R. Dogar, Matteo Leonetti</h3>
<p>We address the manipulation task of retrieving a target object from a
cluttered shelf. When the target object is hidden, the robot must search
through the clutter for retrieving it. Solving this task requires reasoning
over the likely locations of the target object. It also requires physics
reasoning over multi-object interactions and future occlusions. In this work,
we present a data-driven approach for generating occlusion-aware actions in
closed-loop. We present a hybrid planner that explores likely states generated
from a learned distribution over the location of the target object. The search
is guided by a heuristic trained with reinforcement learning to evaluate
occluded observations. We evaluate our approach in different environments with
varying clutter densities and physics parameters. The results validate that our
approach can search and retrieve a target object in different physics
environments, while only being trained in simulation. It achieves near
real-time behaviour with a success rate exceeding 88%.
</p>
<a href="http://arxiv.org/abs/2011.03334" target="_blank">arXiv:2011.03334</a> [<a href="http://arxiv.org/pdf/2011.03334" target="_blank">pdf</a>]

<h2>Drone Positioning for Visible Light Communication with Drone-Mounted LED and Camera. (arXiv:2011.03348v1 [cs.RO])</h2>
<h3>Yukito Onodera, Hiroki Takano, Daisuke Hisano, Yu Nakayama</h3>
<p>The world is often stricken by catastrophic disasters. On-demand
drone-mounted visible light communication (VLC) networks are suitable for
monitoring disaster-stricken areas for leveraging disaster-response operations.
The concept of an image sensor-based VLC has also attracted attention in the
recent past for establishing stable links using unstably moving drones.
However, existing works did not sufficiently consider the one-to-many image
sensor-based VLC system. Thus, this paper proposes the concept of a one-to-many
image sensor-based VLC between a camera and multiple drone-mounted LED lights
with a drone-positioning algorithm to avoid interference among VLC links.
Multiple drones are deployed on-demand in a disaster-stricken area to monitor
the ground and continuously send image data to a camera with image sensor-based
visible light communication (VLC) links. The proposed idea is demonstrated with
the proof-of-concept (PoC) implemented with drones that are equipped with LED
panels and a 4K camera. As a result, we confirmed the feasibility of the
proposed system.
</p>
<a href="http://arxiv.org/abs/2011.03348" target="_blank">arXiv:2011.03348</a> [<a href="http://arxiv.org/pdf/2011.03348" target="_blank">pdf</a>]

<h2>Deep learning architectures for inference of AC-OPF solutions. (arXiv:2011.03352v1 [cs.LG])</h2>
<h3>Thomas Falconer, Letif Mones</h3>
<p>We present a systematic comparison between neural network (NN) architectures
for inference of AC-OPF solutions. Using fully connected NNs as a baseline we
demonstrate the efficacy of leveraging network topology in models by
constructing abstract representations of electrical grids in the graph domain
for convolutional and graph NNs. The performance of the NN models is compared
for both the direct (as regressors predicting optimal generator set-points) and
indirect (as classifiers predicting the active set of constraints) approaches
and computational gains for obtaining optimal solutions are also presented.
</p>
<a href="http://arxiv.org/abs/2011.03352" target="_blank">arXiv:2011.03352</a> [<a href="http://arxiv.org/pdf/2011.03352" target="_blank">pdf</a>]

<h2>Self Supervised Learning for Object Localisation in 3D Tomographic Images. (arXiv:2011.03353v1 [cs.CV])</h2>
<h3>Yaroslav Zharov, Alexey Ershov, Tilo Baumbach</h3>
<p>While a lot of work is dedicated to self-supervised learning, most of it is
dealing with 2D images of natural scenes and objects. In this paper, we focus
on \textit{volumetric} images obtained by means of the X-Ray Computed
Tomography (CT). We describe two pretext training tasks which are designed
taking into account the specific properties of volumetric data. We propose two
ways to transfer a trained network to the downstream task of object
localization with a zero amount of manual markup. Despite its simplicity, the
proposed method shows its applicability to practical tasks of object
localization and data reduction.
</p>
<a href="http://arxiv.org/abs/2011.03353" target="_blank">arXiv:2011.03353</a> [<a href="http://arxiv.org/pdf/2011.03353" target="_blank">pdf</a>]

<h2>Gamma Radiation Source Localization for Micro Aerial Vehicles with a Miniature Single-Detector Compton Event Camera. (arXiv:2011.03356v1 [cs.RO])</h2>
<h3>Tomas Baca, Petr Stibinger, Daniela Doubravova, Daniel Turecek, Jaroslav Solc, Jan Rusnak, Martin Saska, Jan Jakubek</h3>
<p>A novel miniature system for localization and estimation of compact sources
of gamma radiation for Micro Aerial Vehicles is presented in this paper. The
system utilizes a single-sensor Compton camera. The sensor is extremely small
and weighs only 40 g, which opens the possibility for use on the widely
accepted sub-250 g class of drones. The Compton camera uses the MiniPIX TPX3
CdTe event camera to measure Compton scattering products of incoming
high-energy gamma photons. The 3D position and the sub-nanosecond time delay of
the measured scattering products are used to reconstruct sets of possible
directions to the source. An onboard filter fuses the measurements and
estimates the position of the radiation source during the flight. The
computations are executed in real-time onboard and allow integration of the
sensor info into a fully-autonomous system. Moreover, the real-time nature of
the estimator potentially allows estimating states of a moving radiation
source. The proposed method was validated in a real-world experiment with a
Cs137 radiation source. The approach is able to localize a gamma source without
the need to estimate the gradient or contours of radiation intensity, which
opens possibilities for localizing sources in a cluttered and urban
environment.
</p>
<a href="http://arxiv.org/abs/2011.03356" target="_blank">arXiv:2011.03356</a> [<a href="http://arxiv.org/pdf/2011.03356" target="_blank">pdf</a>]

<h2>A New Inference algorithm of Dynamic Uncertain Causality Graph based on Conditional Sampling Method for Complex Cases. (arXiv:2011.03359v1 [cs.AI])</h2>
<h3>Hao Nie, Qin Zhang</h3>
<p>Dynamic Uncertain Causality Graph(DUCG) is a recently proposed model for
diagnoses of complex systems. It performs well for industry system such as
nuclear power plants, chemical system and spacecrafts. However, the variable
state combination explosion in some cases is still a problem that may result in
inefficiency or even disability in DUCG inference. In the situation of clinical
diagnoses, when a lot of intermediate causes are unknown while the downstream
results are known in a DUCG graph, the combination explosion may appear during
the inference computation. Monte Carlo sampling is a typical algorithm to solve
this problem. However, we are facing the case that the occurrence rate of the
case is very small, e.g. $10^{-20}$, which means a huge number of samplings are
needed. This paper proposes a new scheme based on conditional stochastic
simulation which obtains the final result from the expectation of the
conditional probability in sampling loops instead of counting the sampling
frequency, and thus overcomes the problem. As a result, the proposed algorithm
requires much less time than the DUCG recursive inference algorithm presented
earlier. Moreover, a simple analysis of convergence rate based on a designed
example is given to show the advantage of the proposed method. % In addition,
supports for logic gate, logic cycles, and parallelization, which exist in
DUCG, are also addressed in this paper. The new algorithm reduces the time
consumption a lot and performs 3 times faster than old one with 2.7% error
ratio in a practical graph for Viral Hepatitis B.
</p>
<a href="http://arxiv.org/abs/2011.03359" target="_blank">arXiv:2011.03359</a> [<a href="http://arxiv.org/pdf/2011.03359" target="_blank">pdf</a>]

<h2>Domain Adaptive Person Re-Identification via Coupling Optimization. (arXiv:2011.03363v1 [cs.CV])</h2>
<h3>Xiaobin Liu, Shiliang Zhang</h3>
<p>Domain adaptive person Re-Identification (ReID) is challenging owing to the
domain gap and shortage of annotations on target scenarios. To handle those two
challenges, this paper proposes a coupling optimization method including the
Domain-Invariant Mapping (DIM) method and the Global-Local distance
Optimization (GLO), respectively. Different from previous methods that transfer
knowledge in two stages, the DIM achieves a more efficient one-stage knowledge
transfer by mapping images in labeled and unlabeled datasets to a shared
feature space. GLO is designed to train the ReID model with unsupervised
setting on the target domain. Instead of relying on existing optimization
strategies designed for supervised training, GLO involves more images in
distance optimization, and achieves better robustness to noisy label
prediction. GLO also integrates distance optimizations in both the global
dataset and local training batch, thus exhibits better training efficiency.
Extensive experiments on three large-scale datasets, i.e., Market-1501,
DukeMTMC-reID, and MSMT17, show that our coupling optimization outperforms
state-of-the-art methods by a large margin. Our method also works well in
unsupervised training, and even outperforms several recent domain adaptive
methods.
</p>
<a href="http://arxiv.org/abs/2011.03363" target="_blank">arXiv:2011.03363</a> [<a href="http://arxiv.org/pdf/2011.03363" target="_blank">pdf</a>]

<h2>Disentangling 3D Prototypical Networks For Few-Shot Concept Learning. (arXiv:2011.03367v1 [cs.CV])</h2>
<h3>Mihir Prabhudesai, Shamit Lal, Darshan Patil, Hsiao-Yu Tung, Adam W Harley, Katerina Fragkiadaki</h3>
<p>We present neural architectures that disentangle RGB-D images into objects'
shapes and styles and a map of the background scene, and explore their
applications for few-shot 3D object detection and few-shot concept
classification. Our networks incorporate architectural biases that reflect the
image formation process, 3D geometry of the world scene, and shape-style
interplay. They are trained end-to-end self-supervised by predicting views in
static scenes, alongside a small number of 3D object boxes. Objects and scenes
are represented in terms of 3D feature grids in the bottleneck of the network.
We show that the proposed 3D neural representations are compositional: they can
generate novel 3D scene feature maps by mixing object shapes and styles,
resizing and adding the resulting object 3D feature maps over background scene
feature maps. We show that classifiers for object categories, color, materials,
and spatial relationships trained over the disentangled 3D feature sub-spaces
generalize better with dramatically fewer examples than the current
state-of-the-art, and enable a visual question answering system that uses them
as its modules to generalize one-shot to novel objects in the scene.
</p>
<a href="http://arxiv.org/abs/2011.03367" target="_blank">arXiv:2011.03367</a> [<a href="http://arxiv.org/pdf/2011.03367" target="_blank">pdf</a>]

<h2>FDNAS: Improving Data Privacy and Model Diversity in AutoML. (arXiv:2011.03372v1 [cs.LG])</h2>
<h3>Chunhui Zhang, Yongyuan Liang, Xiaoming Yuan, Lei Cheng</h3>
<p>To prevent the leakage of private information while enabling automated
machine intelligence, there is an emerging trend to integrate federated
learning and Neural Architecture Search (NAS). Although promising as it may
seem, the coupling of difficulties from both two tenets makes the algorithm
development quite challenging. In particular, how to efficiently search the
optimal neural architecture directly from massive non-iid data of clients in a
federated manner remains to be a hard nut to crack. To tackle this challenge,
in this paper, by leveraging the advances in proxy-less NAS, we propose a
Federated Direct Neural Architecture Search (FDNAS) framework that allows
hardware-aware NAS from decentralized non-iid data of clients. To further adapt
for various data distributions of clients, inspired by meta-learning, a cluster
Federated Direct Neural Architecture Search (CFDNAS) framework is proposed to
achieve client-aware NAS, in the sense that each client can learn a tailored
deep learning model for its particular data distribution. Extensive experiments
on real-world non-iid datasets show state-of-the-art accuracy-efficiency
trade-offs for various hardware and data distributions of clients. Our codes
will be released publicly upon paper acceptance.
</p>
<a href="http://arxiv.org/abs/2011.03372" target="_blank">arXiv:2011.03372</a> [<a href="http://arxiv.org/pdf/2011.03372" target="_blank">pdf</a>]

<h2>A Scalable MIP-based Method for Learning Optimal Multivariate Decision Trees. (arXiv:2011.03375v1 [cs.LG])</h2>
<h3>Haoran Zhu, Pavankumar Murali, Dzung T. Phan, Lam M. Nguyen, Jayant R. Kalagnanam</h3>
<p>Several recent publications report advances in training optimal decision
trees (ODT) using mixed-integer programs (MIP), due to algorithmic advances in
integer programming and a growing interest in addressing the inherent
suboptimality of heuristic approaches such as CART. In this paper, we propose a
novel MIP formulation, based on a 1-norm support vector machine model, to train
a multivariate ODT for classification problems. We provide cutting plane
techniques that tighten the linear relaxation of the MIP formulation, in order
to improve run times to reach optimality. Using 36 data-sets from the
University of California Irvine Machine Learning Repository, we demonstrate
that our formulation outperforms its counterparts in the literature by an
average of about 10% in terms of mean out-of-sample testing accuracy across the
data-sets. We provide a scalable framework to train multivariate ODT on large
data-sets by introducing a novel linear programming (LP) based data selection
method to choose a subset of the data for training. Our method is able to
routinely handle large data-sets with more than 7,000 sample points and
outperform heuristics methods and other MIP based techniques. We present
results on data-sets containing up to 245,000 samples. Existing MIP-based
methods do not scale well on training data-sets beyond 5,500 samples.
</p>
<a href="http://arxiv.org/abs/2011.03375" target="_blank">arXiv:2011.03375</a> [<a href="http://arxiv.org/pdf/2011.03375" target="_blank">pdf</a>]

<h2>Deep Learning-based Cattle Activity Classification Using Joint Time-frequency Data Representation. (arXiv:2011.03381v1 [cs.LG])</h2>
<h3>Seyedeh Faezeh Hosseini Noorbin, Siamak Layeghy, Brano Kusy, Raja Jurdak, Greg Bishop-hurley, Marius Portmann</h3>
<p>Automated cattle activity classification allows herders to continuously
monitor the health and well-being of livestock, resulting in increased quality
and quantity of beef and dairy products. In this paper, a sequential deep
neural network is used to develop a behavioural model and to classify cattle
behaviour and activities. The key focus of this paper is the exploration of a
joint time-frequency domain representation of the sensor data, which is
provided as the input to the neural network classifier. Our exploration is
based on a real-world data set with over 3 million samples, collected from
sensors with a tri-axial accelerometer, magnetometer and gyroscope, attached to
collar tags of 10 dairy cows and collected over a one month period. The key
results of this paper is that the joint time-frequency data representation,
even when used in conjunction with a relatively basic neural network
classifier, can outperform the best cattle activity classifiers reported in the
literature. With a more systematic exploration of neural network classifier
architectures and hyper-parameters, there is potential for even further
improvements. Finally, we demonstrate that the time-frequency domain data
representation allows us to efficiently trade-off a large reduction of model
size and computational complexity for a very minor reduction in classification
accuracy. This shows the potential for our classification approach to run on
resource-constrained embedded and IoT devices.
</p>
<a href="http://arxiv.org/abs/2011.03381" target="_blank">arXiv:2011.03381</a> [<a href="http://arxiv.org/pdf/2011.03381" target="_blank">pdf</a>]

<h2>Adversarial Skill Learning for Robust Manipulation. (arXiv:2011.03383v1 [cs.RO])</h2>
<h3>Pingcheng Jian, Chao Yang, Di Guo, Huaping Liu, Fuchun Sun</h3>
<p>Deep reinforcement learning has made significant progress in robotic
manipulation tasks and it works well in the ideal disturbance-free environment.
However, in a real-world environment, both internal and external disturbances
are inevitable, thus the performance of the trained policy will dramatically
drop. To improve the robustness of the policy, we introduce the adversarial
training mechanism to the robotic manipulation tasks in this paper, and an
adversarial skill learning algorithm based on soft actor-critic (SAC) is
proposed for robust manipulation. Extensive experiments are conducted to
demonstrate that the learned policy is robust to internal and external
disturbances. Additionally, the proposed algorithm is evaluated in both the
simulation environment and on the real robotic platform.
</p>
<a href="http://arxiv.org/abs/2011.03383" target="_blank">arXiv:2011.03383</a> [<a href="http://arxiv.org/pdf/2011.03383" target="_blank">pdf</a>]

<h2>Noise2Sim -- Similarity-based Self-Learning for Image Denoising. (arXiv:2011.03384v1 [cs.LG])</h2>
<h3>Chuang Niu, Ge Wang</h3>
<p>The key idea behind denoising methods is to perform a mean/averaging
operation, either locally or non-locally. An observation on classic denoising
methods is that non-local mean (NLM) outcomes are typically superior to locally
denoised results. Despite achieving the best performance in image denoising,
the supervised deep denoising methods require paired noise-clean data which are
often unavailable. To address this challenge, Noise2Noise methods are based on
the fact that paired noise-clean images can be replaced by paired noise-noise
images which are easier to collect. However, in many scenarios the collection
of paired noise-noise images are still impractical. To bypass labeled images,
Noise2Void methods predict masked pixels from their surroundings in a single
noisy image only. It is pitiful that neither Noise2Noise nor Noise2Void methods
utilize self-similarities in an image as NLM methods do, while
self-similarities/symmetries play a critical role in modern sciences. Here we
propose Noise2Sim, an NLM-inspired self-learning method for image denoising.
Specifically, Noise2Sim leverages self-similarities of image patches and learns
to map between the center pixels of similar patches for self-consistent image
denoising. Our statistical analysis shows that Noise2Sim tends to be equivalent
to Noise2Noise under mild conditions. To accelerate the process of finding
similar image patches, we design an efficient two-step procedure to provide
data for Noise2Sim training, which can be iteratively conducted if needed.
Extensive experiments demonstrate the superiority of Noise2Sim over Noise2Noise
and Noise2Void on common benchmark datasets.
</p>
<a href="http://arxiv.org/abs/2011.03384" target="_blank">arXiv:2011.03384</a> [<a href="http://arxiv.org/pdf/2011.03384" target="_blank">pdf</a>]

<h2>Underspecification Presents Challenges for Credibility in Modern Machine Learning. (arXiv:2011.03395v1 [cs.LG])</h2>
<h3>Alexander D&#x27;Amour, Katherine Heller, Dan Moldovan, Ben Adlam, Babak Alipanahi, Alex Beutel, Christina Chen, Jonathan Deaton, Jacob Eisenstein, Matthew D. Hoffman, Farhad Hormozdiari, Neil Houlsby, Shaobo Hou, Ghassen Jerfel, Alan Karthikesalingam, Mario Lucic, Yian Ma, Cory McLean, Diana Mincu, Akinori Mitani, Andrea Montanari, Zachary Nado, Vivek Natarajan, Christopher Nielson, Thomas F. Osborne, Rajiv Raman, Kim Ramasamy, Rory Sayres, Jessica Schrouff, Martin Seneviratne, Shannon Sequeira, Harini Suresh, Victor Veitch, Max Vladymyrov, Xuezhi Wang, Kellie Webster, Steve Yadlowsky, Taedong Yun, Xiaohua Zhai, D. Sculley</h3>
<p>ML models often exhibit unexpectedly poor behavior when they are deployed in
real-world domains. We identify underspecification as a key reason for these
failures. An ML pipeline is underspecified when it can return many predictors
with equivalently strong held-out performance in the training domain.
Underspecification is common in modern ML pipelines, such as those based on
deep learning. Predictors returned by underspecified pipelines are often
treated as equivalent based on their training domain performance, but we show
here that such predictors can behave very differently in deployment domains.
This ambiguity can lead to instability and poor model behavior in practice, and
is a distinct failure mode from previously identified issues arising from
structural mismatch between training and deployment domains. We show that this
problem appears in a wide variety of practical ML pipelines, using examples
from computer vision, medical imaging, natural language processing, clinical
risk prediction based on electronic health records, and medical genomics. Our
results show the need to explicitly account for underspecification in modeling
pipelines that are intended for real-world deployment in any domain.
</p>
<a href="http://arxiv.org/abs/2011.03395" target="_blank">arXiv:2011.03395</a> [<a href="http://arxiv.org/pdf/2011.03395" target="_blank">pdf</a>]

<h2>Illumination Normalization by Partially Impossible Encoder-Decoder Cost Function. (arXiv:2011.03428v1 [cs.CV])</h2>
<h3>Steve Dias Da Cruz, Bertram Taetz, Thomas Stifter, Didier Stricker</h3>
<p>Images recorded during the lifetime of computer vision based systems undergo
a wide range of illumination and environmental conditions affecting the
reliability of previously trained machine learning models. Image normalization
is hence a valuable preprocessing component to enhance the models' robustness.
To this end, we introduce a new strategy for the cost function formulation of
encoder-decoder networks to average out all the unimportant information in the
input images (e.g. environmental features and illumination changes) to focus on
the reconstruction of the salient features (e.g. class instances). Our method
exploits the availability of identical sceneries under different illumination
and environmental conditions for which we formulate a partially impossible
reconstruction target: the input image will not convey enough information to
reconstruct the target in its entirety. Its applicability is assessed on three
publicly available datasets. We combine the triplet loss as a regularizer in
the latent space representation and a nearest neighbour search to improve the
generalization to unseen illuminations and class instances. The importance of
the aforementioned post-processing is highlighted on an automotive application.
To this end, we release a synthetic dataset of sceneries from three different
passenger compartments where each scenery is rendered under ten different
illumination and environmental conditions: see https://sviro.kl.dfki.de
</p>
<a href="http://arxiv.org/abs/2011.03428" target="_blank">arXiv:2011.03428</a> [<a href="http://arxiv.org/pdf/2011.03428" target="_blank">pdf</a>]

<h2>Deep Cross-modal Proxy Hashing. (arXiv:2011.03451v1 [cs.CV])</h2>
<h3>Rong-Cheng Tu, Xian-Ling Mao, Rongxin Tu, Wei Wei, Heyan Huang</h3>
<p>Due to their high retrieval efficiency and low storage cost for cross-modal
search task, cross-modal hashing methods have attracted considerable attention.
For supervised cross-modal hashing methods, how to make the learned hash codes
preserve semantic structure information sufficiently is a key point to further
enhance the retrieval performance. As far as we know, almost all supervised
cross-modal hashing methods preserve semantic structure information depending
on at-least-one similarity definition fully or partly, i.e., it defines two
datapoints as similar ones if they share at least one common category otherwise
they are dissimilar. Obviously, the at-least-one similarity misses abundant
semantic structure information. To tackle this problem, in this paper, we
propose a novel Deep Cross-modal Proxy Hashing, called DCPH. Specifically, DCPH
first learns a proxy hashing network to generate a discriminative proxy hash
code for each category. Then, by utilizing the learned proxy hash code as
supervised information, a novel $Margin$-$SoftMax$-$like\ loss$ is proposed
without defining the at-least-one similarity between datapoints. By minimizing
the novel $Margin$-$SoftMax$-$like\ loss$, the learned hash codes will
simultaneously preserve the cross-modal similarity and abundant semantic
structure information well. Extensive experiments on two benchmark datasets
show that the proposed method outperforms the state-of-the-art baselines in
cross-modal retrieval task.
</p>
<a href="http://arxiv.org/abs/2011.03451" target="_blank">arXiv:2011.03451</a> [<a href="http://arxiv.org/pdf/2011.03451" target="_blank">pdf</a>]

<h2>Improving Sales Forecasting Accuracy: A Tensor Factorization Approach with Demand Awareness. (arXiv:2011.03452v1 [cs.LG])</h2>
<h3>Xuan Bi, Gediminas Adomavicius, William Li, Annie Qu</h3>
<p>Due to accessible big data collections from consumers, products, and stores,
advanced sales forecasting capabilities have drawn great attention from many
companies especially in the retail business because of its importance in
decision making. Improvement of the forecasting accuracy, even by a small
percentage, may have a substantial impact on companies' production and
financial planning, marketing strategies, inventory controls, supply chain
management, and eventually stock prices. Specifically, our research goal is to
forecast the sales of each product in each store in the near future. Motivated
by tensor factorization methodologies for personalized context-aware
recommender systems, we propose a novel approach called the Advanced Temporal
Latent-factor Approach to Sales forecasting (ATLAS), which achieves accurate
and individualized prediction for sales by building a single
tensor-factorization model across multiple stores and products. Our
contribution is a combination of: tensor framework (to leverage information
across stores and products), a new regularization function (to incorporate
demand dynamics), and extrapolation of tensor into future time periods using
state-of-the-art statistical (seasonal auto-regressive integrated
moving-average models) and machine-learning (recurrent neural networks) models.
The advantages of ATLAS are demonstrated on eight product category datasets
collected by the Information Resource, Inc., where a total of 165 million
weekly sales transactions from more than 1,500 grocery stores over 15,560
products are analyzed.
</p>
<a href="http://arxiv.org/abs/2011.03452" target="_blank">arXiv:2011.03452</a> [<a href="http://arxiv.org/pdf/2011.03452" target="_blank">pdf</a>]

<h2>Complex Query Answering with Neural Link Predictors. (arXiv:2011.03459v1 [cs.LG])</h2>
<h3>Erik Arakelyan, Daniel Daza, Pasquale Minervini, Michael Cochez</h3>
<p>Neural link predictors are immensely useful for identifying missing edges in
large scale Knowledge Graphs. However, it is still not clear how to use these
models for answering more complex queries that arise in a number of domains,
such as queries using logical conjunctions, disjunctions, and existential
quantifiers, while accounting for missing edges. In this work, we propose a
framework for efficiently answering complex queries on incomplete Knowledge
Graphs. We translate each query into an end-to-end differentiable objective,
where the truth value of each atom is computed by a pre-trained neural link
predictor. We then analyse two solutions to the optimisation problem, including
gradient-based and combinatorial search. In our experiments, the proposed
approach produces more accurate results than state-of-the-art methods --
black-box neural models trained on millions of generated queries -- without the
need of training on a large and diverse set of complex queries. Using orders of
magnitude less training data, we obtain relative improvements ranging from 8%
up to 40% in Hits@3 across different knowledge graphs containing factual
information. Finally, we demonstrate that it is possible to explain the outcome
of our model in terms of the intermediate solutions identified for each of the
complex query atoms.
</p>
<a href="http://arxiv.org/abs/2011.03459" target="_blank">arXiv:2011.03459</a> [<a href="http://arxiv.org/pdf/2011.03459" target="_blank">pdf</a>]

<h2>HAVEN: A Unity-based Virtual Robot Environment to Showcase HRI-based Augmented Reality. (arXiv:2011.03464v1 [cs.RO])</h2>
<h3>Andre Cleaver, Darren Tang, Victoria Chen, Jivko Sinapov</h3>
<p>Due to the COVID-19 pandemic, conducting Human-Robot Interaction (HRI)
studies in person is not permissible due to social distancing practices to
limit the spread of the virus. Therefore, a virtual reality (VR) simulation
with a virtual robot may offer an alternative to real-life HRI studies. Like a
real intelligent robot, a virtual robot can utilize the same advanced
algorithms to behave autonomously. This paper introduces HAVEN (HRI-based
Augmentation in a Virtual robot Environment using uNity), a VR simulation that
enables users to interact with a virtual robot. The goal of this system design
is to enable researchers to conduct HRI Augmented Reality studies using a
virtual robot without being in a real environment. This framework also
introduces two common HRI experiment designs: a hallway passing scenario and
human-robot team object retrieval scenario. Both reflect HAVEN's potential as a
tool for future AR-based HRI studies.
</p>
<a href="http://arxiv.org/abs/2011.03464" target="_blank">arXiv:2011.03464</a> [<a href="http://arxiv.org/pdf/2011.03464" target="_blank">pdf</a>]

<h2>Accelerating combinatorial filter reduction through constraints. (arXiv:2011.03471v1 [cs.RO])</h2>
<h3>Yulin Zhang, Hazhar Rahmani, Dylan A. Shell, Jason M. O&#x27;Kane</h3>
<p>Reduction of combinatorial filters involves compressing state representations
that robots use. Such optimization arises in automating the construction of
minimalist robots. But exact combinatorial filter reduction is an NP-complete
problem and all current techniques are either inexact or formalized with
exponentially many constraints. This paper proposes a new formalization needing
only a polynomial number of constraints, and characterizes these constraints in
three different forms: nonlinear, linear, and conjunctive normal form.
Empirical results show that constraints in conjunctive normal form capture the
problem most effectively, leading to a method that outperforms the others.
Further examination indicates that a substantial proportion of constraints
remain inactive during iterative filter reduction. To leverage this
observation, we introduce just-in-time generation of such constraints, which
yields improvements in efficiency and has the potential to minimize large
filters.
</p>
<a href="http://arxiv.org/abs/2011.03471" target="_blank">arXiv:2011.03471</a> [<a href="http://arxiv.org/pdf/2011.03471" target="_blank">pdf</a>]

<h2>Massively Parallel Graph Drawing and Representation Learning. (arXiv:2011.03479v1 [cs.LG])</h2>
<h3>Christian B&#xf6;hm, Claudia Plant</h3>
<p>To fully exploit the performance potential of modern multi-core processors,
machine learning and data mining algorithms for big data must be parallelized
in multiple ways. Today's CPUs consist of multiple cores, each following an
independent thread of control, and each equipped with multiple arithmetic units
which can perform the same operation on a vector of multiple data objects.
Graph embedding, i.e. converting the vertices of a graph into numerical vectors
is a data mining task of high importance and is useful for graph drawing
(low-dimensional vectors) and graph representation learning (high-dimensional
vectors). In this paper, we propose MulticoreGEMPE (Graph Embedding by
Minimizing the Predictive Entropy), an information-theoretic method which can
generate low and high-dimensional vectors. MulticoreGEMPE applies MIMD
(Multiple Instructions Multiple Data, using OpenMP) and SIMD (Single
Instructions Multiple Data, using AVX-512) parallelism. We propose general
ideas applicable in other graph-based algorithms like \emph{vectorized hashing}
and \emph{vectorized reduction}. Our experimental evaluation demonstrates the
superiority of our approach.
</p>
<a href="http://arxiv.org/abs/2011.03479" target="_blank">arXiv:2011.03479</a> [<a href="http://arxiv.org/pdf/2011.03479" target="_blank">pdf</a>]

<h2>Learning with Molecules beyond Graph Neural Networks. (arXiv:2011.03488v1 [cs.LG])</h2>
<h3>Gustav Sourek, Filip Zelezny, Ondrej Kuzelka</h3>
<p>We demonstrate a deep learning framework which is inherently based in the
highly expressive language of relational logic, enabling to, among other
things, capture arbitrarily complex graph structures. We show how Graph Neural
Networks and similar models can be easily covered in the framework by
specifying the underlying propagation rules in the relational logic. The
declarative nature of the used language then allows to easily modify and extend
the propagation schemes into complex structures, such as the molecular rings
which we choose for a short demonstration in this paper.
</p>
<a href="http://arxiv.org/abs/2011.03488" target="_blank">arXiv:2011.03488</a> [<a href="http://arxiv.org/pdf/2011.03488" target="_blank">pdf</a>]

<h2>Optimization-based Trajectory Planning for Tethered Marsupial Robots. (arXiv:2011.03491v1 [cs.RO])</h2>
<h3>S. Martinez-Rozas, D. Alejo, F. Caballero, L. Merino</h3>
<p>This paper presents a non-linear optimization method for trajectory planning
in a marsupial robot configuration. Particularly, the paper addresses the
planning problem of an unmanned aerial vehicle (UAV) linked to an unmanned
ground vehicle (UGV) by means of a tether. The result is a collision-free
trajectory for UAV and tether, assuming the UGV position is static. The
optimizer takes into account constraints related to the UAV, UGV and tether
positions, obstacles and temporal aspects of the motion such as limited robot
velocities and accelerations, and finally the tether state, which is not
required to be tense. The problem is formulated in a weighted multi-objective
optimization framework. Results from simulated scenarios demonstrate that the
approach is able to generate obstacle free and smooth trajectories for the UAV
and tether from the marsupial system.
</p>
<a href="http://arxiv.org/abs/2011.03491" target="_blank">arXiv:2011.03491</a> [<a href="http://arxiv.org/pdf/2011.03491" target="_blank">pdf</a>]

<h2>The Value Equivalence Principle for Model-Based Reinforcement Learning. (arXiv:2011.03506v1 [cs.LG])</h2>
<h3>Christopher Grimm, Andr&#xe9; Barreto, Satinder Singh, David Silver</h3>
<p>Learning models of the environment from data is often viewed as an essential
component to building intelligent reinforcement learning (RL) agents. The
common practice is to separate the learning of the model from its use, by
constructing a model of the environment's dynamics that correctly predicts the
observed state transitions. In this paper we argue that the limited
representational resources of model-based RL agents are better used to build
models that are directly useful for value-based planning. As our main
contribution, we introduce the principle of value equivalence: two models are
value equivalent with respect to a set of functions and policies if they yield
the same Bellman updates. We propose a formulation of the model learning
problem based on the value equivalence principle and analyze how the set of
feasible solutions is impacted by the choice of policies and functions.
Specifically, we show that, as we augment the set of policies and functions
considered, the class of value equivalent models shrinks, until eventually
collapsing to a single point corresponding to a model that perfectly describes
the environment. In many problems, directly modelling state-to-state
transitions may be both difficult and unnecessary. By leveraging the
value-equivalence principle one may find simpler models without compromising
performance, saving computation and memory. We illustrate the benefits of
value-equivalent model learning with experiments comparing it against more
traditional counterparts like maximum likelihood estimation. More generally, we
argue that the principle of value equivalence underlies a number of recent
empirical successes in RL, such as Value Iteration Networks, the Predictron,
Value Prediction Networks, TreeQN, and MuZero, and provides a first theoretical
underpinning of those results.
</p>
<a href="http://arxiv.org/abs/2011.03506" target="_blank">arXiv:2011.03506</a> [<a href="http://arxiv.org/pdf/2011.03506" target="_blank">pdf</a>]

<h2>Do We Need to Compensate for Motion Distortion and Doppler Effects in Radar-Based Navigation?. (arXiv:2011.03512v1 [cs.RO])</h2>
<h3>Keenan Burnett, Angela P. Schoellig, Timothy D. Barfoot</h3>
<p>In order to tackle the challenge of unfavorable weather conditions such as
rain and snow, radar is being revisited as a parallel sensing modality to
vision and lidar. Recent works have made tremendous progress in applying radar
to odometry and place recognition. However, these works have so far ignored the
impact of motion distortion and Doppler effects on radar-based navigation,
which may be significant in the self-driving car domain where speeds can be
high. In this work, we demonstrate the effect of these distortions on
radar-only odometry using the Oxford Radar RobotCar Dataset and metric
localization using our own data-taking platform. We present a lightweight
estimator that can recover the motion between a pair of radar scans while
accounting for both effects. Our conclusion is that both motion distortion and
the Doppler effect are significant in different aspects of radar navigation,
with the former more prominent than the latter.
</p>
<a href="http://arxiv.org/abs/2011.03512" target="_blank">arXiv:2011.03512</a> [<a href="http://arxiv.org/pdf/2011.03512" target="_blank">pdf</a>]

<h2>Large-scale multilingual audio visual dubbing. (arXiv:2011.03530v1 [cs.CV])</h2>
<h3>Yi Yang, Brendan Shillingford, Yannis Assael, Miaosen Wang, Wendi Liu, Yutian Chen, Yu Zhang, Eren Sezener, Luis C. Cobo, Misha Denil, Yusuf Aytar, Nando de Freitas</h3>
<p>We describe a system for large-scale audiovisual translation and dubbing,
which translates videos from one language to another. The source language's
speech content is transcribed to text, translated, and automatically
synthesized into target language speech using the original speaker's voice. The
visual content is translated by synthesizing lip movements for the speaker to
match the translated audio, creating a seamless audiovisual experience in the
target language. The audio and visual translation subsystems each contain a
large-scale generic synthesis model trained on thousands of hours of data in
the corresponding domain. These generic models are fine-tuned to a specific
speaker before translation, either using an auxiliary corpus of data from the
target speaker, or using the video to be translated itself as the input to the
fine-tuning process. This report gives an architectural overview of the full
system, as well as an in-depth discussion of the video dubbing component. The
role of the audio and text components in relation to the full system is
outlined, but their design is not discussed in detail. Translated and dubbed
demo videos generated using our system can be viewed at
https://www.youtube.com/playlist?list=PLSi232j2ZA6_1Exhof5vndzyfbxAhhEs5
</p>
<a href="http://arxiv.org/abs/2011.03530" target="_blank">arXiv:2011.03530</a> [<a href="http://arxiv.org/pdf/2011.03530" target="_blank">pdf</a>]

<h2>Regularized Regression Problem in hyper-RKHS for Learning Kernels. (arXiv:1809.09910v2 [cs.LG] UPDATED)</h2>
<h3>Fanghui Liu, Lei Shi, Xiaolin Huang, Jie Yang, Johan A.K. Suykens</h3>
<p>This paper generalizes the two-stage kernel learning framework, illustrates
its utility for kernel learning and out-of-sample extensions, and proves
{asymptotic} convergence results for the introduced kernel learning model.
Algorithmically, we extend target alignment by hyper-kernels in the two-stage
kernel learning framework. The associated kernel learning task is formulated as
a regression problem in a hyper-reproducing kernel Hilbert space (hyper-RKHS),
i.e., learning on the space of kernels itself. To solve this problem, we
present two regression models with bivariate forms in this space, including
kernel ridge regression (KRR) and support vector regression (SVR) in the
hyper-RKHS. By doing so, it provides significant model flexibility for kernel
learning with outstanding performance in real-world applications. Specifically,
our kernel learning framework is general, that is, the learned underlying
kernel can be positive definite or indefinite, which adapts to various
requirements in kernel learning. Theoretically, we study the convergence
behavior of these learning algorithms in the hyper-RKHS and derive the learning
rates. Different from the traditional approximation analysis in RKHS, our
analyses need to consider the non-trivial independence of pairwise samples and
the characterisation of hyper-RKHS. To the best of our knowledge, this is the
first work in learning theory to study the approximation performance of
regularized regression problem in hyper-RKHS.
</p>
<a href="http://arxiv.org/abs/1809.09910" target="_blank">arXiv:1809.09910</a> [<a href="http://arxiv.org/pdf/1809.09910" target="_blank">pdf</a>]

<h2>Sanity Checks for Saliency Maps. (arXiv:1810.03292v3 [cs.CV] UPDATED)</h2>
<h3>Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, Been Kim</h3>
<p>Saliency methods have emerged as a popular tool to highlight features in an
input deemed relevant for the prediction of a learned model. Several saliency
methods have been proposed, often guided by visual appeal on image data. In
this work, we propose an actionable methodology to evaluate what kinds of
explanations a given method can and cannot provide. We find that reliance,
solely, on visual assessment can be misleading. Through extensive experiments
we show that some existing saliency methods are independent both of the model
and of the data generating process. Consequently, methods that fail the
proposed tests are inadequate for tasks that are sensitive to either data or
model, such as, finding outliers in the data, explaining the relationship
between inputs and outputs that the model learned, and debugging the model. We
interpret our findings through an analogy with edge detection in images, a
technique that requires neither training data nor model. Theory in the case of
a linear model and a single-layer convolutional neural network supports our
experimental findings.
</p>
<a href="http://arxiv.org/abs/1810.03292" target="_blank">arXiv:1810.03292</a> [<a href="http://arxiv.org/pdf/1810.03292" target="_blank">pdf</a>]

<h2>Fast and Simple Natural-Gradient Variational Inference with Mixture of Exponential-family Approximations. (arXiv:1906.02914v3 [stat.ML] UPDATED)</h2>
<h3>Wu Lin, Mohammad Emtiyaz Khan, Mark Schmidt</h3>
<p>Natural-gradient methods enable fast and simple algorithms for variational
inference, but due to computational difficulties, their use is mostly limited
to \emph{minimal} exponential-family (EF) approximations. In this paper, we
extend their application to estimate \emph{structured} approximations such as
mixtures of EF distributions. Such approximations can fit complex, multimodal
posterior distributions and are generally more accurate than unimodal EF
approximations. By using a \emph{minimal conditional-EF} representation of such
approximations, we derive simple natural-gradient updates. Our empirical
results demonstrate a faster convergence of our natural-gradient method
compared to black-box gradient-based methods with reparameterization gradients.
Our work expands the scope of natural gradients for Bayesian inference and
makes them more widely applicable than before.
</p>
<a href="http://arxiv.org/abs/1906.02914" target="_blank">arXiv:1906.02914</a> [<a href="http://arxiv.org/pdf/1906.02914" target="_blank">pdf</a>]

<h2>Unsupervised State Representation Learning in Atari. (arXiv:1906.08226v6 [cs.LG] UPDATED)</h2>
<h3>Ankesh Anand, Evan Racah, Sherjil Ozair, Yoshua Bengio, Marc-Alexandre C&#xf4;t&#xe9;, R Devon Hjelm</h3>
<p>State representation learning, or the ability to capture latent generative
factors of an environment, is crucial for building intelligent agents that can
perform a wide variety of tasks. Learning such representations without
supervision from rewards is a challenging open problem. We introduce a method
that learns state representations by maximizing mutual information across
spatially and temporally distinct features of a neural encoder of the
observations. We also introduce a new benchmark based on Atari 2600 games where
we evaluate representations based on how well they capture the ground truth
state variables. We believe this new framework for evaluating representation
learning models will be crucial for future representation learning research.
Finally, we compare our technique with other state-of-the-art generative and
contrastive representation learning methods. The code associated with this work
is available at https://github.com/mila-iqia/atari-representation-learning
</p>
<a href="http://arxiv.org/abs/1906.08226" target="_blank">arXiv:1906.08226</a> [<a href="http://arxiv.org/pdf/1906.08226" target="_blank">pdf</a>]

<h2>On the Realization and Analysis of Circular Harmonic Transforms for Feature Detection. (arXiv:1907.12165v5 [cs.CV] UPDATED)</h2>
<h3>Hugh L Kennedy</h3>
<p>Circular-harmonic spectra are a compact representation of local image
features in two dimensions. It is well known that the computational complexity
of such transforms is greatly reduced when polar separability is exploited in
steerable filter-banks. Further simplifications are possible when Cartesian
separability is incorporated using the radial apodization (i.e. weight, window,
or taper) described here, as a consequence of the Laguerre/Hermite
correspondence over polar/Cartesian coordinates. The chosen form also mitigates
undesirable discretization artefacts due to angular aliasing. The possible
utility of circular-harmonic spectra for the description of simple features is
illustrated using real data from an airborne electro-optic sensor. The spectrum
is deployed in a test-statistic to detect and characterize corners of arbitrary
angle and orientation (i.e. wedges). The test-statistic considers uncertainty
due to finite sampling and clutter/noise.
</p>
<a href="http://arxiv.org/abs/1907.12165" target="_blank">arXiv:1907.12165</a> [<a href="http://arxiv.org/pdf/1907.12165" target="_blank">pdf</a>]

<h2>DROGON: A Trajectory Prediction Model based on Intention-Conditioned Behavior Reasoning. (arXiv:1908.00024v3 [cs.CV] UPDATED)</h2>
<h3>Chiho Choi, Srikanth Malla, Abhishek Patil, Joon Hee Choi</h3>
<p>We propose a Deep RObust Goal-Oriented trajectory prediction Network (DROGON)
for accurate vehicle trajectory prediction by considering behavioral intentions
of vehicles in traffic scenes. Our main insight is that the behavior (i.e.,
motion) of drivers can be reasoned from their high level possible goals (i.e.,
intention) on the road. To succeed in such behavior reasoning, we build a
conditional prediction model to forecast goal-oriented trajectories with the
following stages: (i) relational inference where we encode relational
interactions of vehicles using the perceptual context; (ii) intention
estimation to compute the probability distributions of intentional goals based
on the inferred relations; and (iii) behavior reasoning where we reason about
the behaviors of vehicles as trajectories conditioned on the intentions. To
this end, we extend the proposed framework to the pedestrian trajectory
prediction task, showing the potential applicability toward general trajectory
prediction.
</p>
<a href="http://arxiv.org/abs/1908.00024" target="_blank">arXiv:1908.00024</a> [<a href="http://arxiv.org/pdf/1908.00024" target="_blank">pdf</a>]

<h2>Progressive Transfer Learning. (arXiv:1908.02492v3 [cs.CV] UPDATED)</h2>
<h3>Zhengxu Yu, Dong Shen, Zhongming Jin, Jianqiang Huang, Deng Cai, Xian-Sheng Hua</h3>
<p>Model fine-tuning is a widely used transfer learning approach in person
Re-identification (ReID) applications, which fine-tuning a pre-trained feature
extraction model into the target scenario instead of training a model from
scratch. It is challenging due to the significant variations inside the target
scenario, e.g., different camera viewpoint, illumination changes, and
occlusion. These variations result in a gap between the distribution of each
mini-batch and the whole dataset's distribution when using mini-batch training.
In this paper, we study model fine-tuning from the perspective of the
aggregation and utilization of the global information of the dataset when using
mini-batch training. Specifically, we introduce a novel network structure
called Batch-related Convolutional Cell (BConv-Cell), which progressively
collects the global information of the dataset into a latent state and uses it
to rectify the extracted feature. Based on BConv-Cells, we further proposed the
Progressive Transfer Learning (PTL) method to facilitate the model fine-tuning
process by jointly optimizing the BConv-Cells and the pre-trained ReID model.
Empirical experiments show that our proposal can improve the performance of the
ReID model greatly on MSMT17, Market-1501, CUHK03 and DukeMTMC-reID datasets.
Moreover, we extend our proposal to the general image classification task. The
experiments in several image classification benchmark datasets demonstrate that
our proposal can significantly improve the performance of baseline models. The
code has been released at \url{https://github.com/ZJULearning/PTL}
</p>
<a href="http://arxiv.org/abs/1908.02492" target="_blank">arXiv:1908.02492</a> [<a href="http://arxiv.org/pdf/1908.02492" target="_blank">pdf</a>]

<h2>Adaptive Ensemble of Classifiers with Regularization for Imbalanced Data Classification. (arXiv:1908.03595v3 [cs.LG] UPDATED)</h2>
<h3>Chen Wang, Chengyuan Deng, Zhoulu Yu, Dafeng Hui, Xiaofeng Gong, Ruisen Luo</h3>
<p>The dynamic ensemble selection of classifiers is an effective approach for
processing label-imbalanced data classifications. However, such a technique is
prone to overfitting, owing to the lack of regularization methods and the
dependence of the aforementioned technique on local geometry. In this study,
focusing on binary imbalanced data classification, a novel dynamic ensemble
method, namely adaptive ensemble of classifiers with regularization (AER), is
proposed, to overcome the stated limitations. The method solves the overfitting
problem through implicit regularization. Specifically, it leverages the
properties of stochastic gradient descent to obtain the solution with the
minimum norm, thereby achieving regularization; furthermore, it interpolates
the ensemble weights by exploiting the global geometry of data to further
prevent overfitting. According to our theoretical proofs, the seemingly
complicated AER paradigm, in addition to its regularization capabilities, can
actually reduce the asymptotic time and memory complexities of several other
algorithms. We evaluate the proposed AER method on seven benchmark imbalanced
datasets from the UCI machine learning repository and one artificially
generated GMM-based dataset with five variations. The results show that the
proposed algorithm outperforms the major existing algorithms based on multiple
metrics in most cases, and two hypothesis tests (McNemar's and Wilcoxon tests)
verify the statistical significance further. In addition, the proposed method
has other preferred properties such as special advantages in dealing with
highly imbalanced data, and it pioneers the research on the regularization for
dynamic ensemble methods.
</p>
<a href="http://arxiv.org/abs/1908.03595" target="_blank">arXiv:1908.03595</a> [<a href="http://arxiv.org/pdf/1908.03595" target="_blank">pdf</a>]

<h2>Adaptive Sampling for Stochastic Risk-Averse Learning. (arXiv:1910.12511v3 [cs.LG] UPDATED)</h2>
<h3>Sebastian Curi, Kfir. Y. Levy, Stefanie Jegelka, Andreas Krause</h3>
<p>In high-stakes machine learning applications, it is crucial to not only
perform well on average, but also when restricted to difficult examples. To
address this, we consider the problem of training models in a risk-averse
manner. We propose an adaptive sampling algorithm for stochastically optimizing
the Conditional Value-at-Risk (CVaR) of a loss distribution, which measures its
performance on the $\alpha$ fraction of most difficult examples. We use a
distributionally robust formulation of the CVaR to phrase the problem as a
zero-sum game between two players, and solve it efficiently using regret
minimization. Our approach relies on sampling from structured Determinantal
Point Processes (DPPs), which enables scaling it to large data sets. Finally,
we empirically demonstrate its effectiveness on large-scale convex and
non-convex learning tasks.
</p>
<a href="http://arxiv.org/abs/1910.12511" target="_blank">arXiv:1910.12511</a> [<a href="http://arxiv.org/pdf/1910.12511" target="_blank">pdf</a>]

<h2>Feature Noise Induces Loss Discrepancy Across Groups. (arXiv:1911.09876v2 [cs.LG] UPDATED)</h2>
<h3>Fereshte Khani, Percy Liang</h3>
<p>The performance of standard learning procedures has been observed to differ
widely across groups. Recent studies usually attribute this loss discrepancy to
an information deficiency for one group (e.g., one group has less data). In
this work, we point to a more subtle source of loss discrepancy---feature
noise. Our main result is that even when there is no information deficiency
specific to one group (e.g., both groups have infinite data), adding the same
amount of feature noise to all individuals leads to loss discrepancy. For
linear regression, we thoroughly characterize the effect of feature noise on
loss discrepancy in terms of the amount of noise, the difference between
moments of the two groups, and whether group information is used or not. We
then show this loss discrepancy does not vanish immediately if a shift in
distribution causes the groups to have similar moments. On three real-world
datasets, we show feature noise increases the loss discrepancy if groups have
different distributions, while it does not affect the loss discrepancy on
datasets where groups have similar distributions.
</p>
<a href="http://arxiv.org/abs/1911.09876" target="_blank">arXiv:1911.09876</a> [<a href="http://arxiv.org/pdf/1911.09876" target="_blank">pdf</a>]

<h2>Insights into Ordinal Embedding Algorithms: A Systematic Evaluation. (arXiv:1912.01666v4 [cs.LG] UPDATED)</h2>
<h3>Leena Chennuru Vankadara, Siavash Haghiri, Faiz Ul Wahab, Michael Lohaus, Ulrike von Luxburg</h3>
<p>The objective of ordinal embedding is to find a Euclidean representation of a
set of abstract items, using only answers to triplet comparisons of the form
"Is item $i$ closer to the item $j$ or item $k$?". In recent years, numerous
algorithms have been proposed to solve this problem. However, there does not
exist a fair and thorough assessment of these embedding methods and therefore
several key questions remain unanswered: Which algorithms scale better with
increasing sample size or dimension? Which ones perform better when the
embedding dimension is small or few triplet comparisons are available? In our
paper, we address these questions and provide the first comprehensive and
systematic empirical evaluation of existing algorithms as well as a new neural
network approach. In the large triplet regime, we find that simple, relatively
unknown, non-convex methods consistently outperform all other algorithms,
including elaborate approaches based on neural networks or landmark approaches.
This finding can be explained by our insight that many of the non-convex
optimization approaches do not suffer from local optima. In the low triplet
regime, our neural network approach is either competitive or significantly
outperforms all the other methods. Our comprehensive assessment is enabled by
our unified library of popular embedding algorithms that leverages GPU
resources and allows for fast and accurate embeddings of millions of data
points.
</p>
<a href="http://arxiv.org/abs/1912.01666" target="_blank">arXiv:1912.01666</a> [<a href="http://arxiv.org/pdf/1912.01666" target="_blank">pdf</a>]

<h2>EnsemFDet: An Ensemble Approach to Fraud Detection based on Bipartite Graph. (arXiv:1912.11113v4 [cs.LG] UPDATED)</h2>
<h3>Yuxiang Ren, Hao Zhu, Jiawei Zhang, Peng Dai, Liefeng Bo</h3>
<p>Fraud detection is extremely critical for e-commerce business. It is the
intent of the companies to detect and prevent fraud as early as possible.
Existing fraud detection methods try to identify unexpected dense subgraphs and
treat related nodes as suspicious. Spectral relaxation-based methods solve the
problem efficiently but hurt the performance due to the relaxed constraints.
Besides, many methods cannot be accelerated with parallel computation or
control the number of returned suspicious nodes because they provide a set of
subgraphs with diverse node sizes. These drawbacks affect the real-world
applications of existing methods. In this paper, we propose an Ensemble-based
Fraud Detection (EnsemFDet) method to scale up fraud detection in bipartite
graphs by decomposing the original problem into subproblems on small-sized
subgraphs. By oversampling the graph and solving the subproblems, the ensemble
approach further votes suspicious nodes without sacrificing the prediction
accuracy. Extensive experiments have been done on real transaction data from
JD.com, which is one of the world's largest e-commerce platforms. Experimental
results demonstrate the effectiveness, practicability, and scalability of
EnsemFDet. More specifically, EnsemFDet is up to 100x faster than the
state-of-the-art methods due to its parallelism with all aspects of data.
</p>
<a href="http://arxiv.org/abs/1912.11113" target="_blank">arXiv:1912.11113</a> [<a href="http://arxiv.org/pdf/1912.11113" target="_blank">pdf</a>]

<h2>GraphBGS: Background Subtraction via Recovery of Graph Signals. (arXiv:2001.06404v2 [cs.CV] UPDATED)</h2>
<h3>Jhony H. Giraldo, Thierry Bouwmans</h3>
<p>Background subtraction is a fundamental pre-processing task in computer
vision. This task becomes challenging in real scenarios due to variations in
the background for both static and moving camera sequences. Several deep
learning methods for background subtraction have been proposed in the
literature with competitive performances. However, these models show
performance degradation when tested on unseen videos; and they require huge
amount of data to avoid overfitting. Recently, graph-based algorithms have been
successful approaching unsupervised and semi-supervised learning problems.
Furthermore, the theory of graph signal processing and semi-supervised learning
have been combined leading to new insights in the field of machine learning. In
this paper, concepts of recovery of graph signals are introduced in the problem
of background subtraction. We propose a new algorithm called Graph BackGround
Subtraction (GraphBGS), which is composed of: instance segmentation, background
initialization, graph construction, graph sampling, and a semi-supervised
algorithm inspired from the theory of recovery of graph signals. Our algorithm
has the advantage of requiring less labeled data than deep learning methods
while having competitive results on both: static and moving camera videos.
GraphBGS outperforms unsupervised and supervised methods in several challenging
conditions on the publicly available Change Detection (CDNet2014), and UCSD
background subtraction databases.
</p>
<a href="http://arxiv.org/abs/2001.06404" target="_blank">arXiv:2001.06404</a> [<a href="http://arxiv.org/pdf/2001.06404" target="_blank">pdf</a>]

<h2>CO-Optimal Transport. (arXiv:2002.03731v3 [stat.ML] UPDATED)</h2>
<h3>Ievgen Redko, Titouan Vayer, R&#xe9;mi Flamary, Nicolas Courty</h3>
<p>Optimal transport (OT) is a powerful geometric and probabilistic tool for
finding correspondences and measuring similarity between two distributions.
Yet, its original formulation relies on the existence of a cost function
between the samples of the two distributions, which makes it impractical when
they are supported on different spaces. To circumvent this limitation, we
propose a novel OT problem, named COOT for CO-Optimal Transport, that
simultaneously optimizes two transport maps between both samples and features,
contrary to other approaches that either discard the individual features by
focusing on pairwise distances between samples or need to model explicitly the
relations between them. We provide a thorough theoretical analysis of our
problem, establish its rich connections with other OT-based distances and
demonstrate its versatility with two machine learning applications in
heterogeneous domain adaptation and co-clustering/data summarization, where
COOT leads to performance improvements over the state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2002.03731" target="_blank">arXiv:2002.03731</a> [<a href="http://arxiv.org/pdf/2002.03731" target="_blank">pdf</a>]

<h2>Self-Supervised Object-in-Gripper Segmentation from Robotic Motions. (arXiv:2002.04487v3 [cs.CV] UPDATED)</h2>
<h3>Wout Boerdijk, Martin Sundermeyer, Maximilian Durner, Rudolph Triebel</h3>
<p>Accurate object segmentation is a crucial task in the context of robotic
manipulation. However, creating sufficient annotated training data for neural
networks is particularly time consuming and often requires manual labeling. To
this end, we propose a simple, yet robust solution for learning to segment
unknown objects grasped by a robot. Specifically, we exploit motion and
temporal cues in RGB video sequences. Using optical flow estimation we first
learn to predict segmentation masks of our given manipulator. Then, these
annotations are used in combination with motion cues to automatically
distinguish between background, manipulator and unknown, grasped object. In
contrast to existing systems our approach is fully self-supervised and
independent of precise camera calibration, 3D models or potentially imperfect
depth data. We perform a thorough comparison with alternative baselines and
approaches from literature. The object masks and views are shown to be suitable
training data for segmentation networks that generalize to novel environments
and also allow for watertight 3D reconstruction.
</p>
<a href="http://arxiv.org/abs/2002.04487" target="_blank">arXiv:2002.04487</a> [<a href="http://arxiv.org/pdf/2002.04487" target="_blank">pdf</a>]

<h2>Robust Reinforcement Learning via Adversarial training with Langevin Dynamics. (arXiv:2002.06063v2 [cs.LG] UPDATED)</h2>
<h3>Parameswaran Kamalaruban, Yu-Ting Huang, Ya-Ping Hsieh, Paul Rolland, Cheng Shi, Volkan Cevher</h3>
<p>We introduce a sampling perspective to tackle the challenging task of
training robust Reinforcement Learning (RL) agents. Leveraging the powerful
Stochastic Gradient Langevin Dynamics, we present a novel, scalable two-player
RL algorithm, which is a sampling variant of the two-player policy gradient
method. Our algorithm consistently outperforms existing baselines, in terms of
generalization across different training and testing conditions, on several
MuJoCo environments. Our experiments also show that, even for objective
functions that entirely ignore potential environmental shifts, our sampling
approach remains highly robust in comparison to standard RL algorithms.
</p>
<a href="http://arxiv.org/abs/2002.06063" target="_blank">arXiv:2002.06063</a> [<a href="http://arxiv.org/pdf/2002.06063" target="_blank">pdf</a>]

<h2>In-Hand Object Pose Tracking via Contact Feedback and GPU-Accelerated Robotic Simulation. (arXiv:2002.12160v4 [cs.RO] UPDATED)</h2>
<h3>Jacky Liang, Ankur Handa, Karl Van Wyk, Viktor Makoviychuk, Oliver Kroemer, Dieter Fox</h3>
<p>Tracking the pose of an object while it is being held and manipulated by a
robot hand is difficult for vision-based methods due to significant occlusions.
Prior works have explored using contact feedback and particle filters to
localize in-hand objects. However, they have mostly focused on the static grasp
setting and not when the object is in motion, as doing so requires modeling of
complex contact dynamics. In this work, we propose using GPU-accelerated
parallel robot simulations and derivative-free, sample-based optimizers to
track in-hand object poses with contact feedback during manipulation. We use
physics simulation as the forward model for robot-object interactions, and the
algorithm jointly optimizes for the state and the parameters of the
simulations, so they better match with those of the real world. Our method runs
in real-time (30Hz) on a single GPU, and it achieves an average point cloud
distance error of 6mm in simulation experiments and 13mm in the real-world
ones. View experiment videos at
https://sites.google.com/view/in-hand-object-pose-tracking/
</p>
<a href="http://arxiv.org/abs/2002.12160" target="_blank">arXiv:2002.12160</a> [<a href="http://arxiv.org/pdf/2002.12160" target="_blank">pdf</a>]

<h2>Affinity guided Geometric Semi-Supervised Metric Learning. (arXiv:2002.12394v2 [cs.CV] UPDATED)</h2>
<h3>Ujjal Kr Dutta, Mehrtash Harandi, Chellu Chandra Sekhar</h3>
<p>In this paper, we revamp the forgotten classical Semi-Supervised Distance
Metric Learning (SSDML) problem from a Riemannian geometric lens, to leverage
stochastic optimization within a end-to-end deep framework. The motivation
comes from the fact that apart from a few classical SSDML approaches learning a
linear Mahalanobis metric, deep SSDML has not been studied. We first extend
existing SSDML methods to their deep counterparts and then propose a new method
to overcome their limitations. Due to the nature of constraints on our metric
parameters, we leverage Riemannian optimization. Our deep SSDML method with a
novel affinity propagation based triplet mining strategy outperforms its
competitors.
</p>
<a href="http://arxiv.org/abs/2002.12394" target="_blank">arXiv:2002.12394</a> [<a href="http://arxiv.org/pdf/2002.12394" target="_blank">pdf</a>]

<h2>Robotic Grasping through Combined Image-Based Grasp Proposal and 3D Reconstruction. (arXiv:2003.01649v3 [cs.RO] UPDATED)</h2>
<h3>Daniel Yang, Tarik Tosun, Ben Eisner, Volkan Isler, Daniel Lee</h3>
<p>We present a novel approach to robotic grasp planning using both a learned
grasp proposal network and a learned 3D shape reconstruction network. Our
system generates 6-DOF grasps from a single RGB-D image of the target object,
which is provided as input to both networks. By using the geometric
reconstruction to refine the the candidate grasp produced by the grasp proposal
network, our system is able to accurately grasp both known and unknown objects,
even when the grasp location on the object is not visible in the input image.

This paper presents the network architectures, training procedures, and grasp
refinement method that comprise our system. Experiments demonstrate the
efficacy of our system at grasping both known and unknown objects (91% success
rate in a physical robot environment, 84% success rate in a simulated
environment). We additionally perform ablation studies that show the benefits
of combining a learned grasp proposal with geometric reconstruction for
grasping, and also show that our system outperforms several baselines in a
grasping task.
</p>
<a href="http://arxiv.org/abs/2003.01649" target="_blank">arXiv:2003.01649</a> [<a href="http://arxiv.org/pdf/2003.01649" target="_blank">pdf</a>]

<h2>K-Core based Temporal Graph Convolutional Network for Dynamic Graphs. (arXiv:2003.09902v4 [cs.LG] UPDATED)</h2>
<h3>Jingxin Liu, Chang Xu, Chang Yin, Weiqiang Wu, You Song</h3>
<p>Graph representation learning is a fundamental task in various applications
that strives to learn low-dimensional embeddings for nodes that can preserve
graph topology information. However, many existing methods focus on static
graphs while ignoring evolving graph patterns. Inspired by the success of graph
convolutional networks(GCNs) in static graph embedding, we propose a novel
k-core based temporal graph convolutional network, the CTGCN, to learn node
representations for dynamic graphs. In contrast to previous dynamic graph
embedding methods, CTGCN can preserve both local connective proximity and
global structural similarity while simultaneously capturing graph dynamics. In
the proposed framework, the traditional graph convolution is generalized into
two phases, feature transformation and feature aggregation, which gives the
CTGCN more flexibility and enables the CTGCN to learn connective and structural
information under the same framework. Experimental results on 7 real-world
graphs demonstrate that the CTGCN outperforms existing state-of-the-art graph
embedding methods in several tasks, including link prediction and structural
role classification. The source code of this work can be obtained from
\url{https://github.com/jhljx/CTGCN}.
</p>
<a href="http://arxiv.org/abs/2003.09902" target="_blank">arXiv:2003.09902</a> [<a href="http://arxiv.org/pdf/2003.09902" target="_blank">pdf</a>]

<h2>Self-Supervised Learning for Domain Adaptation on Point-Clouds. (arXiv:2003.12641v3 [cs.CV] UPDATED)</h2>
<h3>Idan Achituve, Haggai Maron, Gal Chechik</h3>
<p>Self-supervised learning (SSL) is a technique for learning useful
representations from unlabeled data. It has been applied effectively to domain
adaptation (DA) on images and videos. It is still unknown if and how it can be
leveraged for domain adaptation in 3D perception problems. Here we describe the
first study of SSL for DA on point clouds. We introduce a new family of pretext
tasks, Deformation Reconstruction, inspired by the deformations encountered in
sim-to-real transformations. In addition, we propose a novel training procedure
for labeled point cloud data motivated by the MixUp method called Point cloud
Mixup (PCM). Evaluations on domain adaptations datasets for classification and
segmentation, demonstrate a large improvement over existing and baseline
methods.
</p>
<a href="http://arxiv.org/abs/2003.12641" target="_blank">arXiv:2003.12641</a> [<a href="http://arxiv.org/pdf/2003.12641" target="_blank">pdf</a>]

<h2>Adaptive Personalized Federated Learning. (arXiv:2003.13461v3 [cs.LG] UPDATED)</h2>
<h3>Yuyang Deng, Mohammad Mahdi Kamani, Mehrdad Mahdavi</h3>
<p>Investigation of the degree of personalization in federated learning
algorithms has shown that only maximizing the performance of the global model
will confine the capacity of the local models to personalize. In this paper, we
advocate an adaptive personalized federated learning (APFL) algorithm, where
each client will train their local models while contributing to the global
model. We derive the generalization bound of mixture of local and global
models, and find the optimal mixing parameter. We also propose a
communication-efficient optimization method to collaboratively learn the
personalized models and analyze its convergence in both smooth strongly convex
and nonconvex settings. The extensive experiments demonstrate the effectiveness
of our personalization schema, as well as the correctness of established
generalization theories.
</p>
<a href="http://arxiv.org/abs/2003.13461" target="_blank">arXiv:2003.13461</a> [<a href="http://arxiv.org/pdf/2003.13461" target="_blank">pdf</a>]

<h2>Spatio-temporal Tubelet Feature Aggregation and Object Linking in Videos. (arXiv:2004.00451v2 [cs.CV] UPDATED)</h2>
<h3>Daniel Cores, V&#xed;ctor M. Brea, Manuel Mucientes</h3>
<p>This paper addresses the problem of how to exploit spatio-temporal
information available in videos to improve the object detection precision. We
propose a two stage object detector called FANet based on short-term
spatio-temporal feature aggregation to give a first detection set, and
long-term object linking to refine these detections. Firstly, we generate a set
of short tubelet proposals containing the object in $N$ consecutive frames.
Then, we aggregate RoI pooled deep features through the tubelet using a
temporal pooling operator that summarizes the information with a fixed size
output independent of the number of input frames. On top of that, we define a
double head implementation that we feed with spatio-temporal aggregated
information for spatio-temporal object classification, and with spatial
information extracted from the current frame for object localization and
spatial classification. Furthermore, we also specialize each head branch
architecture to better perform in each task taking into account the input data.
Finally, a long-term linking method builds long tubes using the previously
calculated short tubelets to overcome detection errors. We have evaluated our
model in the widely used ImageNet VID dataset achieving a 80.9% mAP, which is
the new state-of-the-art result for single models. Also, in the challenging
small object detection dataset USC-GRAD-STDdb, our proposal outperforms the
single frame baseline by 5.4% mAP.
</p>
<a href="http://arxiv.org/abs/2004.00451" target="_blank">arXiv:2004.00451</a> [<a href="http://arxiv.org/pdf/2004.00451" target="_blank">pdf</a>]

<h2>L^2UWE: A Framework for the Efficient Enhancement of Low-Light Underwater Images Using Local Contrast and Multi-Scale Fusion. (arXiv:2005.13736v2 [cs.CV] UPDATED)</h2>
<h3>Tunai Porto Marques, Alexandra Branzan Albu</h3>
<p>Images captured underwater often suffer from suboptimal illumination settings
that can hide important visual features, reducing their quality. We present a
novel single-image low-light underwater image enhancer, L^2UWE, that builds on
our observation that an efficient model of atmospheric lighting can be derived
from local contrast information. We create two distinct models and generate two
enhanced images from them: one that highlights finer details, the other focused
on darkness removal. A multi-scale fusion process is employed to combine these
images while emphasizing regions of higher luminance, saliency and local
contrast. We demonstrate the performance of L^2UWE by using seven metrics to
test it against seven state-of-the-art enhancement methods specific to
underwater and low-light scenes. Code available at:
https://github.com/tunai/l2uwe.
</p>
<a href="http://arxiv.org/abs/2005.13736" target="_blank">arXiv:2005.13736</a> [<a href="http://arxiv.org/pdf/2005.13736" target="_blank">pdf</a>]

<h2>Bayesian Neural Networks. (arXiv:2006.01490v2 [stat.ML] UPDATED)</h2>
<h3>Tom Charnock, Laurence Perreault-Levasseur, Fran&#xe7;ois Lanusse</h3>
<p>In recent times, neural networks have become a powerful tool for the analysis
of complex and abstract data models. However, their introduction intrinsically
increases our uncertainty about which features of the analysis are
model-related and which are due to the neural network. This means that
predictions by neural networks have biases which cannot be trivially
distinguished from being due to the true nature of the creation and observation
of data or not. In order to attempt to address such issues we discuss Bayesian
neural networks: neural networks where the uncertainty due to the network can
be characterised. In particular, we present the Bayesian statistical framework
which allows us to categorise uncertainty in terms of the ingrained randomness
of observing certain data and the uncertainty from our lack of knowledge about
how data can be created and observed. In presenting such techniques we show how
errors in prediction by neural networks can be obtained in principle, and
provide the two favoured methods for characterising these errors. We will also
describe how both of these methods have substantial pitfalls when put into
practice, highlighting the need for other statistical techniques to truly be
able to do inference when using neural networks.
</p>
<a href="http://arxiv.org/abs/2006.01490" target="_blank">arXiv:2006.01490</a> [<a href="http://arxiv.org/pdf/2006.01490" target="_blank">pdf</a>]

<h2>Learning Active Task-Oriented Exploration Policies for Bridging the Sim-to-Real Gap. (arXiv:2006.01952v2 [cs.RO] UPDATED)</h2>
<h3>Jacky Liang, Saumya Saxena, Oliver Kroemer</h3>
<p>Training robotic policies in simulation suffers from the sim-to-real gap, as
simulated dynamics can be different from real-world dynamics. Past works
tackled this problem through domain randomization and online
system-identification. The former is sensitive to the manually-specified
training distribution of dynamics parameters and can result in behaviors that
are overly conservative. The latter requires learning policies that
concurrently perform the task and generate useful trajectories for system
identification. In this work, we propose and analyze a framework for learning
exploration policies that explicitly perform task-oriented exploration actions
to identify task-relevant system parameters. These parameters are then used by
model-based trajectory optimization algorithms to perform the task in the real
world. We instantiate the framework in simulation with the Linear Quadratic
Regulator as well as in the real world with pouring and object dragging tasks.
Experiments show that task-oriented exploration helps model-based policies
adapt to systems with initially unknown parameters, and it leads to better task
performance than task-agnostic exploration.
</p>
<a href="http://arxiv.org/abs/2006.01952" target="_blank">arXiv:2006.01952</a> [<a href="http://arxiv.org/pdf/2006.01952" target="_blank">pdf</a>]

<h2>Policy-focused Agent-based Modeling using RL Behavioral Models. (arXiv:2006.05048v3 [cs.LG] UPDATED)</h2>
<h3>Osonde A. Osoba, Raffaele Vardavas, Justin Grana, Rushil Zutshi, Amber Jaycocks</h3>
<p>Agent-based Models (ABMs) are valuable tools for policy analysis. ABMs help
analysts explore the emergent consequences of policy interventions in
multi-agent decision-making settings. But the validity of inferences drawn from
ABM explorations depends on the quality of the ABM agents' behavioral models.
Standard specifications of agent behavioral models rely either on heuristic
decision-making rules or on regressions trained on past data. Both prior
specification modes have limitations. This paper examines the value of
reinforcement learning (RL) models as adaptive, high-performing, and
behaviorally-valid models of agent decision-making in ABMs. We test the
hypothesis that RL agents are effective as utility-maximizing agents in policy
ABMs. We also address the problem of adapting RL algorithms to handle
multi-agency in games by adapting and extending methods from recent literature.
We evaluate the performance of such RL-based ABM agents via experiments on two
policy-relevant ABMs: a minority game ABM, and an ABM of Influenza
Transmission. We run some analytic experiments on our AI-equipped ABMs e.g.
explorations of the effects of behavioral heterogeneity in a population and the
emergence of synchronization in a population. The experiments show that RL
behavioral models are effective at producing reward-seeking or
reward-maximizing behaviors in ABM agents. Furthermore, RL behavioral models
can learn to outperform the default adaptive behavioral models in the two ABMs
examined.
</p>
<a href="http://arxiv.org/abs/2006.05048" target="_blank">arXiv:2006.05048</a> [<a href="http://arxiv.org/pdf/2006.05048" target="_blank">pdf</a>]

<h2>Customized Handling of Unintended Interface Operation in Assistive Robots. (arXiv:2007.02092v2 [cs.RO] UPDATED)</h2>
<h3>Deepak Gopinath, Mahdieh Nejati Javaremi, Brenna D. Argall</h3>
<p>We present an assistance system that reasons about a human's intended actions
during robot teleoperation in order to provide appropriate corrections for
unintended behavior. We model the human's physical interaction with a control
interface during robot teleoperation and distinguish between intended and
measured physical actions explicitly. By reasoning over the unobserved
intentions using model-based inference techniques, our assistive system
provides customized corrections on a user's issued commands. We validate our
algorithm with a 10-person human subject study in which we evaluate the
performance of the proposed assistance paradigms. Our results show that the
assistance paradigms helped to significantly reduce task completion time,
number of mode switches, cognitive workload, and user frustration and improve
overall user satisfaction.
</p>
<a href="http://arxiv.org/abs/2007.02092" target="_blank">arXiv:2007.02092</a> [<a href="http://arxiv.org/pdf/2007.02092" target="_blank">pdf</a>]

<h2>Learning Graph Structure With A Finite-State Automaton Layer. (arXiv:2007.04929v2 [cs.LG] UPDATED)</h2>
<h3>Daniel D. Johnson, Hugo Larochelle, Daniel Tarlow</h3>
<p>Graph-based neural network models are producing strong results in a number of
domains, in part because graphs provide flexibility to encode domain knowledge
in the form of relational structure (edges) between nodes in the graph. In
practice, edges are used both to represent intrinsic structure (e.g., abstract
syntax trees of programs) and more abstract relations that aid reasoning for a
downstream task (e.g., results of relevant program analyses). In this work, we
study the problem of learning to derive abstract relations from the intrinsic
graph structure. Motivated by their power in program analyses, we consider
relations defined by paths on the base graph accepted by a finite-state
automaton. We show how to learn these relations end-to-end by relaxing the
problem into learning finite-state automata policies on a graph-based POMDP and
then training these policies using implicit differentiation. The result is a
differentiable Graph Finite-State Automaton (GFSA) layer that adds a new edge
type (expressed as a weighted adjacency matrix) to a base graph. We demonstrate
that this layer can find shortcuts in grid-world graphs and reproduce simple
static analyses on Python programs. Additionally, we combine the GFSA layer
with a larger graph-based model trained end-to-end on the variable misuse
program understanding task, and find that using the GFSA layer leads to better
performance than using hand-engineered semantic edges or other baseline methods
for adding learned edge types.
</p>
<a href="http://arxiv.org/abs/2007.04929" target="_blank">arXiv:2007.04929</a> [<a href="http://arxiv.org/pdf/2007.04929" target="_blank">pdf</a>]

<h2>Scale Equivariance Improves Siamese Tracking. (arXiv:2007.09115v2 [cs.CV] UPDATED)</h2>
<h3>Ivan Sosnovik, Artem Moskalev, Arnold Smeulders</h3>
<p>Siamese trackers turn tracking into similarity estimation between a template
and the candidate regions in the frame. Mathematically, one of the key
ingredients of success of the similarity function is translation equivariance.
Non-translation-equivariant architectures induce a positional bias during
training, so the location of the target will be hard to recover from the
feature space. In real life scenarios, objects undergoe various transformations
other than translation, such as rotation or scaling. Unless the model has an
internal mechanism to handle them, the similarity may degrade. In this paper,
we focus on scaling and we aim to equip the Siamese network with additional
built-in scale equivariance to capture the natural variations of the target a
priori. We develop the theory for scale-equivariant Siamese trackers, and
provide a simple recipe for how to make a wide range of existing trackers
scale-equivariant. We present SE-SiamFC, a scale-equivariant variant of SiamFC
built according to the recipe. We conduct experiments on OTB and VOT benchmarks
and on the synthetically generated T-MNIST and S-MNIST datasets. We demonstrate
that a built-in additional scale equivariance is useful for visual object
tracking.
</p>
<a href="http://arxiv.org/abs/2007.09115" target="_blank">arXiv:2007.09115</a> [<a href="http://arxiv.org/pdf/2007.09115" target="_blank">pdf</a>]

<h2>How Does Data Augmentation Affect Privacy in Machine Learning?. (arXiv:2007.10567v2 [cs.LG] UPDATED)</h2>
<h3>Da Yu, Huishuai Zhang, Wei Chen, Jian Yin, Tie-Yan Liu</h3>
<p>It is observed in the literature that data augmentation can significantly
mitigate membership inference (MI) attack. However, in this work, we challenge
this observation by proposing new MI attacks to utilize the information of
augmented data. MI attack is widely used to measure the model's information
leakage of the training set. We establish the optimal membership inference when
the model is trained with augmented data, which inspires us to formulate the MI
attack as a set classification problem, i.e., classifying a set of augmented
instances instead of a single data point, and design input permutation
invariant features. Empirically, we demonstrate that the proposed approach
universally outperforms original methods when the model is trained with data
augmentation. Even further, we show that the proposed approach can achieve
higher MI attack success rates on models trained with some data augmentation
than the existing methods on models trained without data augmentation. Notably,
we achieve a 70.1% MI attack success rate on CIFAR10 against a wide residual
network while the previous best approach only attains 61.9%. This suggests the
privacy risk of models trained with data augmentation could be largely
underestimated.
</p>
<a href="http://arxiv.org/abs/2007.10567" target="_blank">arXiv:2007.10567</a> [<a href="http://arxiv.org/pdf/2007.10567" target="_blank">pdf</a>]

<h2>Geometric compression of invariant manifolds in neural nets. (arXiv:2007.11471v3 [cs.LG] UPDATED)</h2>
<h3>Jonas Paccolat, Leonardo Petrini, Mario Geiger, Kevin Tyloo, Matthieu Wyart</h3>
<p>We study how neural networks compress uninformative input space in models
where data lie in $d$ dimensions, but whose label only vary within a linear
manifold of dimension $d_\parallel &lt; d$. We show that for a one-hidden layer
network initialized with infinitesimal weights (i.e. in the \textit{feature
learning} regime) trained with gradient descent, the uninformative
$d_\perp=d-d_\parallel$ space is compressed by a factor $\lambda\sim \sqrt{p}$,
where $p$ is the size of the training set. We quantify the benefit of such a
compression on the test error $\epsilon$. For large initialization of the
weights (the \textit{lazy training} regime), no compression occurs and for
regular boundaries separating labels we find that $\epsilon \sim p^{-\beta}$,
with $\beta_\text{Lazy} = d / (3d-2)$. Compression improves the learning curves
so that $\beta_\text{Feature} = (2d-1)/(3d-2)$ if $d_\parallel = 1$ and
$\beta_\text{Feature} = (d + d_\perp/2)/(3d-2)$ if $d_\parallel &gt; 1$. We test
these predictions for a stripe model where boundaries are parallel interfaces
($d_\parallel=1$) as well as for a cylindrical boundary ($d_\parallel=2$). Next
we show that compression shapes the Neural Tangent Kernel (NTK) evolution in
time, so that its top eigenvectors become more informative and display a larger
projection on the labels. Consequently, kernel learning with the frozen NTK at
the end of training outperforms the initial NTK. We confirm these predictions
both for a one-hidden layer FC network trained on the stripe model and for a
16-layers CNN trained on MNIST, for which we also find
$\beta_\text{Feature}&gt;\beta_\text{Lazy}$. The great similarities found in these
two cases support that compression is central to the training of MNIST, and
puts forward kernel-PCA on the evolving NTK as a useful diagnostic of
compression in deep nets.
</p>
<a href="http://arxiv.org/abs/2007.11471" target="_blank">arXiv:2007.11471</a> [<a href="http://arxiv.org/pdf/2007.11471" target="_blank">pdf</a>]

<h2>Data-efficient visuomotor policy training using reinforcement learning and generative models. (arXiv:2007.13134v2 [cs.RO] UPDATED)</h2>
<h3>Ali Ghadirzadeh, Petra Poklukar, Ville Kyrki, Danica Kragic, M&#xe5;rten Bj&#xf6;rkman</h3>
<p>We present a data-efficient framework for solving visuomotor sequential
decision-making problems which exploits the combination of reinforcement
learning (RL) and latent variable generative models. Our framework trains deep
visuomotor policies by introducing an action latent variable such that the
feed-forward policy search can be divided into three parts: (i) training a
sub-policy that outputs a distribution over the action latent variable given a
state of the system, (ii) unsupervised training of a generative model that
outputs a sequence of motor actions conditioned on the latent action variable,
and (iii) supervised training of the deep visuomotor policy in an end-to-end
fashion. Our approach enables safe exploration and alleviates the
data-inefficiency problem as it exploits prior knowledge about valid sequences
of motor actions. Moreover, we provide a set of measures for evaluation of
generative models such that we are able to predict the performance of the RL
policy training prior to the actual training on a physical robot. We define two
novel measures of disentanglement and local linearity for assessing the quality
of latent representations, and complement them with existing measures for
assessment of the learned distribution. We experimentally determine the
characteristics of different generative models that have the most influence on
performance of the final policy training on a robotic picking task.
</p>
<a href="http://arxiv.org/abs/2007.13134" target="_blank">arXiv:2007.13134</a> [<a href="http://arxiv.org/pdf/2007.13134" target="_blank">pdf</a>]

<h2>Real-Time Uncertainty Estimation in Computer Vision via Uncertainty-Aware Distribution Distillation. (arXiv:2007.15857v2 [cs.CV] UPDATED)</h2>
<h3>Yichen Shen, Zhilu Zhang, Mert R. Sabuncu, Lin Sun</h3>
<p>Calibrated estimates of uncertainty are critical for many real-world computer
vision applications of deep learning. While there are several widely-used
uncertainty estimation methods, dropout inference stands out for its simplicity
and efficacy. This technique, however, requires multiple forward passes through
the network during inference and therefore can be too resource-intensive to be
deployed in real-time applications. We propose a simple, easy-to-optimize
distillation method for learning the conditional predictive distribution of a
pre-trained dropout model for fast, sample-free uncertainty estimation in
computer vision tasks. We empirically test the effectiveness of the proposed
method on both semantic segmentation and depth estimation tasks and demonstrate
our method can significantly reduce the inference time, enabling real-time
uncertainty quantification, while achieving improved quality of both the
uncertainty estimates and predictive performance over the regular dropout
model.
</p>
<a href="http://arxiv.org/abs/2007.15857" target="_blank">arXiv:2007.15857</a> [<a href="http://arxiv.org/pdf/2007.15857" target="_blank">pdf</a>]

<h2>Learning to Drive (L2D) as a Low-Cost Benchmark for Real-World Reinforcement Learning. (arXiv:2008.00715v2 [cs.RO] UPDATED)</h2>
<h3>Ari Viitala, Rinu Boney, Yi Zhao, Alexander Ilin, Juho Kannala</h3>
<p>We present Learning to Drive (L2D), a low-cost benchmark for real-world
reinforcement learning (RL). L2D involves a simple and reproducible
experimental setup where an RL agent has to learn to drive a Donkey car around
three miniature tracks, given only monocular image observations and speed of
the car. The agent has to learn to drive from disengagements, which occurs when
it drives off the track. We present and open-source our training pipeline,
which makes it straightforward to apply any existing RL algorithm to the task
of autonomous driving with a Donkey car. We test imitation learning,
state-of-the-art model-free, and model-based algorithms on the proposed L2D
benchmark. Our results show that existing RL algorithms can learn to drive the
car from scratch in less than five minutes of interaction. We demonstrate that
RL algorithms can learn from sparse and noisy disengagement to drive even
faster than imitation learning and a human operator.
</p>
<a href="http://arxiv.org/abs/2008.00715" target="_blank">arXiv:2008.00715</a> [<a href="http://arxiv.org/pdf/2008.00715" target="_blank">pdf</a>]

<h2>EasyRL: A Simple and Extensible Reinforcement Learning Framework. (arXiv:2008.01700v2 [cs.AI] UPDATED)</h2>
<h3>Neil Hulbert, Sam Spillers, Brandon Francis, James Haines-Temons, Ken Gil Romero, Benjamin De Jager, Sam Wong, Kevin Flora, Bowei Huang, Athirai A. Irissappane</h3>
<p>In recent years, Reinforcement Learning (RL), has become a popular field of
study as well as a tool for enterprises working on cutting-edge artificial
intelligence research. To this end, many researchers have built RL frameworks
such as openAI Gym and KerasRL for ease of use. While these works have made
great strides towards bringing down the barrier of entry for those new to RL,
we propose a much simpler framework called EasyRL, by providing an interactive
graphical user interface for users to train and evaluate RL agents. As it is
entirely graphical, EasyRL does not require programming knowledge for training
and testing simple built-in RL agents. EasyRL also supports custom RL agents
and environments, which can be highly beneficial for RL researchers in
evaluating and comparing their RL models.
</p>
<a href="http://arxiv.org/abs/2008.01700" target="_blank">arXiv:2008.01700</a> [<a href="http://arxiv.org/pdf/2008.01700" target="_blank">pdf</a>]

<h2>An Unsupervised Domain Adaptation Scheme for Single-Stage Artwork Recognition in Cultural Sites. (arXiv:2008.01882v2 [cs.CV] UPDATED)</h2>
<h3>Giovanni Pasqualino, Antonino Furnari, Giovanni Signorello, Giovanni Maria Farinella</h3>
<p>Recognizing artworks in a cultural site using images acquired from the user's
point of view (First Person Vision) allows to build interesting applications
for both the visitors and the site managers. However, current object detection
algorithms working in fully supervised settings need to be trained with large
quantities of labeled data, whose collection requires a lot of times and high
costs in order to achieve good performance. Using synthetic data generated from
the 3D model of the cultural site to train the algorithms can reduce these
costs. On the other hand, when these models are tested with real images, a
significant drop in performance is observed due to the differences between real
and synthetic images. In this study we consider the problem of Unsupervised
Domain Adaptation for object detection in cultural sites. To address this
problem, we created a new dataset containing both synthetic and real images of
16 different artworks. We hence investigated different domain adaptation
techniques based on one-stage and two-stage object detector, image-to-image
translation and feature alignment. Based on the observation that single-stage
detectors are more robust to the domain shift in the considered settings, we
proposed a new method which builds on RetinaNet and feature alignment that we
called DA-RetinaNet. The proposed approach achieves better results than
compared methods on the proposed dataset and on Cityscapes. To support research
in this field we release the dataset at the following link
https://iplab.dmi.unict.it/EGO-CH-OBJ-UDA/ and the code of the proposed
architecture at https://github.com/fpv-iplab/DA-RetinaNet.
</p>
<a href="http://arxiv.org/abs/2008.01882" target="_blank">arXiv:2008.01882</a> [<a href="http://arxiv.org/pdf/2008.01882" target="_blank">pdf</a>]

<h2>NAS-Bench-301 and the Case for Surrogate Benchmarks for Neural Architecture Search. (arXiv:2008.09777v3 [cs.LG] UPDATED)</h2>
<h3>Julien Siems, Lucas Zimmer, Arber Zela, Jovita Lukasik, Margret Keuper, Frank Hutter</h3>
<p>The most significant barrier to the advancement of Neural Architecture Search
(NAS) is its demand for large computational resources, which hinders
scientifically sound empirical evaluations. As a remedy, several tabular NAS
benchmarks were proposed to simulate runs of NAS methods in seconds. However,
all existing tabular NAS benchmarks are limited to extremely small
architectural spaces since they rely on exhaustive evaluations of the space.
This leads to unrealistic results that do not transfer to larger search spaces.
To overcome this fundamental limitation, we propose NAS-Bench-301, the first
surrogate NAS benchmark, using a search space containing $10^{18}$
architectures, many orders of magnitude larger than any previous tabular NAS
benchmark. After motivating the benefits of a surrogate benchmark over a
tabular one, we fit various regression models on our dataset, which consists of
$\sim$60k architecture evaluations, and build surrogates via deep ensembles to
also model uncertainty. We benchmark a wide range of NAS algorithms using
NAS-Bench-301 and obtain comparable results to the true benchmark at a fraction
of the real cost. Finally, we show how NAS-Bench-301 can be used to generate
new scientific insights.
</p>
<a href="http://arxiv.org/abs/2008.09777" target="_blank">arXiv:2008.09777</a> [<a href="http://arxiv.org/pdf/2008.09777" target="_blank">pdf</a>]

<h2>Adversarial Attacks on Deep Learning Systems for User Identification based on Motion Sensors. (arXiv:2009.01109v2 [cs.LG] UPDATED)</h2>
<h3>Cezara Benegui, Radu Tudor Ionescu</h3>
<p>For the time being, mobile devices employ implicit authentication mechanisms,
namely, unlock patterns, PINs or biometric-based systems such as fingerprint or
face recognition. While these systems are prone to well-known attacks, the
introduction of an explicit and unobtrusive authentication layer can greatly
enhance security. In this study, we focus on deep learning methods for explicit
authentication based on motion sensor signals. In this scenario, attackers
could craft adversarial examples with the aim of gaining unauthorized access
and even restraining a legitimate user to access his mobile device. To our
knowledge, this is the first study that aims at quantifying the impact of
adversarial attacks on machine learning models used for user identification
based on motion sensors. To accomplish our goal, we study multiple methods for
generating adversarial examples. We propose three research questions regarding
the impact and the universality of adversarial examples, conducting relevant
experiments in order to answer our research questions. Our empirical results
demonstrate that certain adversarial example generation methods are specific to
the attacked classification model, while others tend to be generic. We thus
conclude that deep neural networks trained for user identification tasks based
on motion sensors are subject to a high percentage of misclassification when
given adversarial input.
</p>
<a href="http://arxiv.org/abs/2009.01109" target="_blank">arXiv:2009.01109</a> [<a href="http://arxiv.org/pdf/2009.01109" target="_blank">pdf</a>]

<h2>Stochastic-YOLO: Efficient Probabilistic Object Detection under Dataset Shifts. (arXiv:2009.02967v2 [cs.CV] UPDATED)</h2>
<h3>Tiago Azevedo, Ren&#xe9; de Jong, Matthew Mattina, Partha Maji</h3>
<p>In image classification tasks, the evaluation of models' robustness to
increased dataset shifts with a probabilistic framework is very well studied.
However, object detection (OD) tasks pose other challenges for uncertainty
estimation and evaluation. For example, one needs to evaluate both the quality
of the label uncertainty (i.e., what?) and spatial uncertainty (i.e., where?)
for a given bounding box, but that evaluation cannot be performed with more
traditional average precision metrics (e.g., mAP). In this paper, we adapt the
well-established YOLOv3 architecture to generate uncertainty estimations by
introducing stochasticity in the form of Monte Carlo Dropout (MC-Drop), and
evaluate it across different levels of dataset shift. We call this novel
architecture Stochastic-YOLO, and provide an efficient implementation to
effectively reduce the burden of the MC-Drop sampling mechanism at inference
time. Finally, we provide some sensitivity analyses, while arguing that
Stochastic-YOLO is a sound approach that improves different components of
uncertainty estimations, in particular spatial uncertainties.
</p>
<a href="http://arxiv.org/abs/2009.02967" target="_blank">arXiv:2009.02967</a> [<a href="http://arxiv.org/pdf/2009.02967" target="_blank">pdf</a>]

<h2>Learning Interpretable Feature Context Effects in Discrete Choice. (arXiv:2009.03417v2 [cs.LG] UPDATED)</h2>
<h3>Kiran Tomlinson, Austin R. Benson</h3>
<p>The outcomes of elections, product sales, and the structure of social
connections are all determined by the choices individuals make when presented
with a set of options, so understanding the factors that contribute to choice
is crucial. Of particular interest are context effects, which occur when the
set of available options influences a chooser's relative preferences, as they
violate traditional rationality assumptions yet are widespread in practice.
However, identifying these effects from observed choices is challenging, often
requiring foreknowledge of the effect to be measured. In contrast, we provide a
method for the automatic discovery of a broad class of context effects from
observed choice data. Our models are easier to train and more flexible than
existing models and also yield intuitive, interpretable, and statistically
testable context effects. Using our models, we identify new context effects in
widely used choice datasets and provide the first analysis of choice set
context effects in social network growth.
</p>
<a href="http://arxiv.org/abs/2009.03417" target="_blank">arXiv:2009.03417</a> [<a href="http://arxiv.org/pdf/2009.03417" target="_blank">pdf</a>]

<h2>Rotate to Attend: Convolutional Triplet Attention Module. (arXiv:2010.03045v2 [cs.CV] UPDATED)</h2>
<h3>Diganta Misra, Trikay Nalamada, Ajay Uppili Arasanipalai, Qibin Hou</h3>
<p>Benefiting from the capability of building inter-dependencies among channels
or spatial locations, attention mechanisms have been extensively studied and
broadly used in a variety of computer vision tasks recently. In this paper, we
investigate light-weight but effective attention mechanisms and present triplet
attention, a novel method for computing attention weights by capturing
cross-dimension interaction using a three-branch structure. For an input
tensor, triplet attention builds inter-dimensional dependencies by the rotation
operation followed by residual transformations and encodes inter-channel and
spatial information with negligible computational overhead. Our method is
simple as well as efficient and can be easily plugged into classic backbone
networks as an add-on module. We demonstrate the effectiveness of our method on
various challenging tasks including image classification on ImageNet-1k and
object detection on MSCOCO and PASCAL VOC datasets. Furthermore, we provide
extensive in-sight into the performance of triplet attention by visually
inspecting the GradCAM and GradCAM++ results. The empirical evaluation of our
method supports our intuition on the importance of capturing dependencies
across dimensions when computing attention weights. Code for this paper can be
publicly accessed at https://github.com/LandskapeAI/triplet-attention
</p>
<a href="http://arxiv.org/abs/2010.03045" target="_blank">arXiv:2010.03045</a> [<a href="http://arxiv.org/pdf/2010.03045" target="_blank">pdf</a>]

<h2>Belief-Grounded Networks for Accelerated Robot Learning under Partial Observability. (arXiv:2010.09170v4 [cs.RO] UPDATED)</h2>
<h3>Hai Nguyen, Brett Daley, Xinchao Song, Christopher Amato, Robert Platt</h3>
<p>Many important robotics problems are partially observable in the sense that a
single visual or force-feedback measurement is insufficient to reconstruct the
state. Standard approaches involve learning a policy over beliefs or
observation-action histories. However, both of these have drawbacks; it is
expensive to track the belief online, and it is hard to learn policies directly
over histories. We propose a method for policy learning under partial
observability called the Belief-Grounded Network (BGN) in which an auxiliary
belief-reconstruction loss incentivizes a neural network to concisely summarize
its input history. Since the resulting policy is a function of the history
rather than the belief, it can be executed easily at runtime. We compare BGN
against several baselines on classic benchmark tasks as well as three novel
robotic touch-sensing tasks. BGN outperforms all other tested methods and its
learned policies work well when transferred onto a physical robot.
</p>
<a href="http://arxiv.org/abs/2010.09170" target="_blank">arXiv:2010.09170</a> [<a href="http://arxiv.org/pdf/2010.09170" target="_blank">pdf</a>]

<h2>Tensor-based Intrinsic Subspace Representation Learning for Multi-view Clustering. (arXiv:2010.09193v4 [cs.LG] UPDATED)</h2>
<h3>Qinghai Zheng, Jihua Zhu, Zhongyu Li, Haoyu Tang, Shuangxun Ma</h3>
<p>As a hot research topic, many multi-view clustering methods are proposed in
recent years. Nevertheless, most existing algorithms merely take the consensus
information among different views into consideration for clustering. Actually,
it may hinder the multi-view clustering performance in real-life applications,
since different views usually contain diverse statistic properties. To address
this problem, we propose a novel Tensor-based Intrinsic Subspace Representation
Learning (TISRL) for multi-view clustering in this paper. Concretely, the rank
preserving decomposition is proposed firstly to effectively deal with the
diverse statistic information contained in different views. Then, to achieve
the intrinsic subspace representation, the tensor-singular value decomposition
based low-rank tensor constraint is also utilized in our method. It can be seen
that the specific information of multi-view data is fully investigated by the
rank preserving decomposition, and the high-order correlations of multi-view
data are also mined by the low-rank tensor constraint. The objective function
can be optimized by an augmented Lagrangian multiplier based alternating
direction minimization algorithm. Experimental results on nine common used
real-world multi-view datasets illustrate the superiority of TISRL.
</p>
<a href="http://arxiv.org/abs/2010.09193" target="_blank">arXiv:2010.09193</a> [<a href="http://arxiv.org/pdf/2010.09193" target="_blank">pdf</a>]

<h2>Action-Conditional Recurrent Kalman Networks For Forward and Inverse Dynamics Learning. (arXiv:2010.10201v2 [cs.RO] UPDATED)</h2>
<h3>Vaisakh Shaj, Philipp Becker, Dieter Buchler, Harit Pandya, Niels van Duijkeren, C. James Taylor, Marc Hanheide, Gerhard Neumann</h3>
<p>Estimating accurate forward and inverse dynamics models is a crucial
component of model-based control for sophisticated robots such as robots driven
by hydraulics, artificial muscles, or robots dealing with different contact
situations. Analytic models to such processes are often unavailable or
inaccurate due to complex hysteresis effects, unmodelled friction and stiction
phenomena,and unknown effects during contact situations. A promising approach
is to obtain spatio-temporal models in a data-driven way using recurrent neural
networks, as they can overcome those issues. However, such models often do not
meet accuracy demands sufficiently, degenerate in performance for the required
high sampling frequencies and cannot provide uncertainty estimates. We adopt a
recent probabilistic recurrent neural network architecture, called Re-current
Kalman Networks (RKNs), to model learning by conditioning its transition
dynamics on the control actions. RKNs outperform standard recurrent networks
such as LSTMs on many state estimation tasks. Inspired by Kalman filters, the
RKN provides an elegant way to achieve action conditioning within its recurrent
cell by leveraging additive interactions between the current latent state and
the action variables. We present two architectures, one for forward model
learning and one for inverse model learning. Both architectures significantly
outperform exist-ing model learning frameworks as well as analytical models in
terms of prediction performance on a variety of real robot dynamics models.
</p>
<a href="http://arxiv.org/abs/2010.10201" target="_blank">arXiv:2010.10201</a> [<a href="http://arxiv.org/pdf/2010.10201" target="_blank">pdf</a>]

<h2>Learning from Suboptimal Demonstration via Self-Supervised Reward Regression. (arXiv:2010.11723v2 [cs.RO] UPDATED)</h2>
<h3>Letian Chen, Rohan Paleja, Matthew Gombolay</h3>
<p>Learning from Demonstration (LfD) seeks to democratize robotics by enabling
non-roboticist end-users to teach robots to perform a task by providing a human
demonstration. However, modern LfD techniques, e.g. inverse reinforcement
learning (IRL), assume users provide at least stochastically optimal
demonstrations. This assumption fails to hold in most real-world scenarios.
Recent attempts to learn from sub-optimal demonstration leverage pairwise
rankings and following the Luce-Shepard rule. However, we show these approaches
make incorrect assumptions and thus suffer from brittle, degraded performance.
We overcome these limitations in developing a novel approach that bootstraps
off suboptimal demonstrations to synthesize optimality-parameterized data to
train an idealized reward function. We empirically validate we learn an
idealized reward function with ~0.95 correlation with ground-truth reward
versus ~0.75 for prior work. We can then train policies achieving ~200%
improvement over the suboptimal demonstration and ~90% improvement over prior
work. We present a physical demonstration of teaching a robot a topspin strike
in table tennis that achieves 32% faster returns and 40% more topspin than user
demonstration.
</p>
<a href="http://arxiv.org/abs/2010.11723" target="_blank">arXiv:2010.11723</a> [<a href="http://arxiv.org/pdf/2010.11723" target="_blank">pdf</a>]

<h2>Real-Time Edge Classification: Optimal Offloading under Token Bucket Constraints. (arXiv:2010.13737v2 [cs.LG] UPDATED)</h2>
<h3>Ayan Chakrabarti, Roch Gu&#xe9;rin, Chenyang Lu, Jiangnan Liu</h3>
<p>To deploy machine learning-based algorithms for real-time applications with
strict latency constraints, we consider an edge-computing setting where a
subset of inputs are offloaded to the edge for processing by an accurate but
resource-intensive model, and the rest are processed only by a less-accurate
model on the device itself. Both models have computational costs that match
available compute resources, and process inputs with low-latency. But
offloading incurs network delays, and to manage these delays to meet
application deadlines, we use a token bucket to constrain the average rate and
burst length of transmissions from the device. We introduce a Markov Decision
Process-based framework to make offload decisions under these constraints,
based on the local model's confidence and the token bucket state, with the goal
of minimizing a specified error measure for the application. Beyond isolated
decisions for individual devices, we also propose approaches to allow multiple
devices connected to the same access switch to share their bursting allocation.
We evaluate and analyze the policies derived using our framework on the
standard ImageNet image classification benchmark.
</p>
<a href="http://arxiv.org/abs/2010.13737" target="_blank">arXiv:2010.13737</a> [<a href="http://arxiv.org/pdf/2010.13737" target="_blank">pdf</a>]

<h2>Scaling Laws for Autoregressive Generative Modeling. (arXiv:2010.14701v2 [cs.LG] UPDATED)</h2>
<h3>Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom B. Brown, Prafulla Dhariwal, Scott Gray, Chris Hallacy, Benjamin Mann, Alec Radford, Aditya Ramesh, Nick Ryder, Daniel M. Ziegler, John Schulman, Dario Amodei, Sam McCandlish</h3>
<p>We identify empirical scaling laws for the cross-entropy loss in four
domains: generative image modeling, video modeling, multimodal
image$\leftrightarrow$text models, and mathematical problem solving. In all
cases autoregressive Transformers smoothly improve in performance as model size
and compute budgets increase, following a power-law plus constant scaling law.
The optimal model size also depends on the compute budget through a power-law,
with exponents that are nearly universal across all data domains.

The cross-entropy loss has an information theoretic interpretation as
$S($True$) + D_{\mathrm{KL}}($True$||$Model$)$, and the empirical scaling laws
suggest a prediction for both the true data distribution's entropy and the KL
divergence between the true and model distributions. With this interpretation,
billion-parameter Transformers are nearly perfect models of the YFCC100M image
distribution downsampled to an $8\times 8$ resolution, and we can forecast the
model size needed to achieve any given reducible loss (ie $D_{\mathrm{KL}}$) in
nats/image for other resolutions.

We find a number of additional scaling laws in specific domains: (a) we
identify a scaling relation for the mutual information between captions and
images in multimodal models, and show how to answer the question "Is a picture
worth a thousand words?"; (b) in the case of mathematical problem solving, we
identify scaling laws for model performance when extrapolating beyond the
training distribution; (c) we finetune generative image models for ImageNet
classification and find smooth scaling of the classification loss and error
rate, even as the generative loss levels off. Taken together, these results
strengthen the case that scaling laws have important implications for neural
network performance, including on downstream tasks.
</p>
<a href="http://arxiv.org/abs/2010.14701" target="_blank">arXiv:2010.14701</a> [<a href="http://arxiv.org/pdf/2010.14701" target="_blank">pdf</a>]

<h2>Deep Reactive Planning in Dynamic Environments. (arXiv:2011.00155v2 [cs.RO] UPDATED)</h2>
<h3>Kei Ota, Devesh K. Jha, Tadashi Onishi, Asako Kanezaki, Yusuke Yoshiyasu, Yoko Sasaki, Toshisada Mariyama, Daniel Nikovski</h3>
<p>The main novelty of the proposed approach is that it allows a robot to learn
an end-to-end policy which can adapt to changes in the environment during
execution. While goal conditioning of policies has been studied in the RL
literature, such approaches are not easily extended to cases where the robot's
goal can change during execution. This is something that humans are naturally
able to do. However, it is difficult for robots to learn such reflexes (i.e.,
to naturally respond to dynamic environments), especially when the goal
location is not explicitly provided to the robot, and instead needs to be
perceived through a vision sensor. In the current work, we present a method
that can achieve such behavior by combining traditional kinematic planning,
deep learning, and deep reinforcement learning in a synergistic fashion to
generalize to arbitrary environments. We demonstrate the proposed approach for
several reaching and pick-and-place tasks in simulation, as well as on a real
system of a 6-DoF industrial manipulator. A video describing our work could be
found \url{https://youtu.be/hE-Ew59GRPQ}.
</p>
<a href="http://arxiv.org/abs/2011.00155" target="_blank">arXiv:2011.00155</a> [<a href="http://arxiv.org/pdf/2011.00155" target="_blank">pdf</a>]

<h2>A Framework of Combining Short-Term Spatial/Frequency Feature Extraction and Long-Term IndRNN for Activity Recognition. (arXiv:2011.00395v2 [cs.CV] UPDATED)</h2>
<h3>Beidi Zhao, Shuai Li, Yanbo Gao, Chuankun Li, Wanqing Li</h3>
<p>Smartphone sensors based human activity recognition is attracting increasing
interests nowadays with the popularization of smartphones. With the high
sampling rates of smartphone sensors, it is a highly long-range temporal
recognition problem, especially with the large intra-class distances such as
the smartphones carried at different locations such as in the bag or on the
body, and the small inter-class distances such as taking train or subway. To
address this problem, we propose a new framework of combining short-term
spatial/frequency feature extraction and a long-term Independently Recurrent
Neural Network (IndRNN) for activity recognition. Considering the periodic
characteristics of the sensor data, short-term temporal features are first
extracted in the spatial and frequency domains. Then the IndRNN, which is able
to capture long-term patterns, is used to further obtain the long-term features
for classification. In view of the large differences when the smartphone is
carried at different locations, a group based location recognition is first
developed to pinpoint the location of the smartphone. The Sussex-Huawei
Locomotion (SHL) dataset from the SHL Challenge is used for evaluation. An
earlier version of the proposed method has won the second place award in the
SHL Challenge 2020 (the first place if not considering multiple models fusion
approach). The proposed method is further improved in this paper and achieves
80.72$\%$ accuracy, better than the existing methods using a single model.
</p>
<a href="http://arxiv.org/abs/2011.00395" target="_blank">arXiv:2011.00395</a> [<a href="http://arxiv.org/pdf/2011.00395" target="_blank">pdf</a>]

<h2>Semi-supervised Federated Learning for Activity Recognition. (arXiv:2011.00851v2 [cs.LG] UPDATED)</h2>
<h3>Yuchen Zhao, Hanyang Liu, Honglin Li, Payam Barnaghi, Hamed Haddadi</h3>
<p>The proliferation of IoT sensors and edge devices makes it possible to use
deep learning models to recognise daily activities locally using in-home
monitoring technologies. Recently, federated learning systems that use edge
devices as clients to collect and utilise IoT sensory data for human activity
recognition have been commonly used as a new way to combine local
(individual-level) and global (group-level) models. This approach provides
better scalability and generalisability and also offers higher privacy compared
with the traditional centralised analysis and learning models. The assumption
behind federated learning, however, relies on supervised learning on clients.
This requires a large volume of labelled data, which is difficult to collect in
uncontrolled IoT environments such as remote in-home monitoring.

In this paper, we propose an activity recognition system that uses
semi-supervised federated learning, wherein clients conduct unsupervised
learning on autoencoders with unlabelled local data to learn general
representations, and a cloud server conducts supervised learning on an activity
classifier with labelled data. Our experimental results show that using
autoencoders and a long short-term memory (LSTM) classifier, the accuracy of
our proposed system is comparable to that of a supervised federated learning
system. Meanwhile, we demonstrate that our system is not affected by the
Non-IID distribution of local data, and can even achieve better accuracy than
supervised federated learning on some datasets. Additionally, we show that our
proposed system can reduce the number of needed labels in the system and the
size of local models without losing much accuracy, and has shorter local
activity recognition time than supervised federated learning.
</p>
<a href="http://arxiv.org/abs/2011.00851" target="_blank">arXiv:2011.00851</a> [<a href="http://arxiv.org/pdf/2011.00851" target="_blank">pdf</a>]

<h2>SLAM in the Field: An Evaluation of Monocular Mapping and Localization on Challenging Dynamic Agricultural Environment. (arXiv:2011.01122v2 [cs.CV] UPDATED)</h2>
<h3>Fangwen Shu, Paul Lesur, Yaxu Xie, Alain Pagani, Didier Stricker</h3>
<p>This paper demonstrates a system capable of combining a sparse, indirect,
monocular visual SLAM, with both offline and real-time Multi-View Stereo (MVS)
reconstruction algorithms. This combination overcomes many obstacles
encountered by autonomous vehicles or robots employed in agricultural
environments, such as overly repetitive patterns, need for very detailed
reconstructions, and abrupt movements caused by uneven roads. Furthermore, the
use of a monocular SLAM makes our system much easier to integrate with an
existing device, as we do not rely on a LiDAR (which is expensive and power
consuming), or stereo camera (whose calibration is sensitive to external
perturbation e.g. camera being displaced). To the best of our knowledge, this
paper presents the first evaluation results for monocular SLAM, and our work
further explores unsupervised depth estimation on this specific application
scenario by simulating RGB-D SLAM to tackle the scale ambiguity, and shows our
approach produces reconstructions that are helpful to various agricultural
tasks. Moreover, we highlight that our experiments provide meaningful insight
to improve monocular SLAM systems under agricultural settings.
</p>
<a href="http://arxiv.org/abs/2011.01122" target="_blank">arXiv:2011.01122</a> [<a href="http://arxiv.org/pdf/2011.01122" target="_blank">pdf</a>]

<h2>ProbRobScene: A Probabilistic Specification Language for 3D Robotic Manipulation Environments. (arXiv:2011.01126v2 [cs.RO] UPDATED)</h2>
<h3>Craig Innes, Subramanian Ramamoorthy</h3>
<p>Robotic control tasks are often first run in simulation for the purposes of
verification, debugging and data augmentation. Many methods exist to specify
what task a robot must complete, but few exist to specify what range of
environments a user expects such tasks to be achieved in. ProbRobScene is a
probabilistic specification language for describing robotic manipulation
environments. Using the language, a user need only specify the relational
constraints that must hold between objects in a scene. ProbRobScene will then
automatically generate scenes which conform to this specification. By combining
aspects of probabilistic programming languages and convex geometry, we provide
a method for sampling this space of possible environments efficiently. We
demonstrate the usefulness of our language by using it to debug a robotic
controller in a tabletop robot manipulation environment.
</p>
<a href="http://arxiv.org/abs/2011.01126" target="_blank">arXiv:2011.01126</a> [<a href="http://arxiv.org/pdf/2011.01126" target="_blank">pdf</a>]

<h2>Reducing Neural Network Parameter Initialization Into an SMT Problem. (arXiv:2011.01191v2 [cs.LG] UPDATED)</h2>
<h3>Mohamad H. Danesh</h3>
<p>Training a neural network (NN) depends on multiple factors, including but not
limited to the initial weights. In this paper, we focus on initializing deep NN
parameters such that it performs better, comparing to random or zero
initialization. We do this by reducing the process of initialization into an
SMT solver. Previous works consider certain activation functions on small NNs,
however the studied NN is a deep network with different activation functions.
Our experiments show that the proposed approach for parameter initialization
achieves better performance comparing to randomly initialized networks.
</p>
<a href="http://arxiv.org/abs/2011.01191" target="_blank">arXiv:2011.01191</a> [<a href="http://arxiv.org/pdf/2011.01191" target="_blank">pdf</a>]

<h2>Multi Projection Fusion for Real-time Semantic Segmentation of 3D LiDAR Point Clouds. (arXiv:2011.01974v2 [cs.CV] UPDATED)</h2>
<h3>Yara Ali Alnaggar, Mohamed Afifi, Karim Amer, Mohamed Elhelw</h3>
<p>Semantic segmentation of 3D point cloud data is essential for enhanced
high-level perception in autonomous platforms. Furthermore, given the
increasing deployment of LiDAR sensors onboard of cars and drones, a special
emphasis is also placed on non-computationally intensive algorithms that
operate on mobile GPUs. Previous efficient state-of-the-art methods relied on
2D spherical projection of point clouds as input for 2D fully convolutional
neural networks to balance the accuracy-speed trade-off. This paper introduces
a novel approach for 3D point cloud semantic segmentation that exploits
multiple projections of the point cloud to mitigate the loss of information
inherent in single projection methods. Our Multi-Projection Fusion (MPF)
framework analyzes spherical and bird's-eye view projections using two separate
highly-efficient 2D fully convolutional models then combines the segmentation
results of both views. The proposed framework is validated on the SemanticKITTI
dataset where it achieved a mIoU of 55.5 which is higher than state-of-the-art
projection-based methods RangeNet++ and PolarNet while being 1.6x faster than
the former and 3.1x faster than the latter.
</p>
<a href="http://arxiv.org/abs/2011.01974" target="_blank">arXiv:2011.01974</a> [<a href="http://arxiv.org/pdf/2011.01974" target="_blank">pdf</a>]

<h2>Filtering for Aggregate Hidden Markov Models with Continuous Observations. (arXiv:2011.02521v2 [stat.ML] UPDATED)</h2>
<h3>Qinsheng Zhang, Rahul Singh, Yongxin Chen</h3>
<p>We consider a class of filtering problems for large populations where each
individual is modeled by the same hidden Markov model (HMM). In this paper, we
focus on aggregate inference problems in HMMs with discrete state space and
continuous observation space. The continuous observations are aggregated in a
way such that the individuals are indistinguishable from measurements. We
propose an aggregate inference algorithm called continuous observation
collective forward-backward algorithm. It extends the recently proposed
collective forward-backward algorithm for aggregate inference in HMMs with
discrete observations to the case of continuous observations. The efficacy of
this algorithm is illustrated through several numerical experiments.
</p>
<a href="http://arxiv.org/abs/2011.02521" target="_blank">arXiv:2011.02521</a> [<a href="http://arxiv.org/pdf/2011.02521" target="_blank">pdf</a>]

<h2>Generating Large-Scale Trajectories Efficiently using Double Descriptions of Polynomials. (arXiv:2011.02662v2 [cs.RO] UPDATED)</h2>
<h3>Zhepei Wang, Hongkai Ye, Chao Xu, Fei Gao</h3>
<p>For quadrotor trajectory planning, describing a polynomial trajectory through
coefficients and end-derivatives both enjoy their own convenience in energy
minimization. We name them double descriptions of polynomial trajectories. The
transformation between them, causing most of the inefficiency and instability,
is formally analyzed in this paper. Leveraging its analytic structure, we
design a linear-complexity scheme for both jerk/snap minimization and parameter
gradient evaluation, which possesses efficiency, stability, flexibility, and
scalability. With the help of our scheme, generating an energy optimal (minimum
snap) trajectory only costs 1 $\mu s$ per piece at the scale up to 1,000,000
pieces. Moreover, generating large-scale energy-time optimal trajectories is
also accelerated by an order of magnitude against conventional methods.
</p>
<a href="http://arxiv.org/abs/2011.02662" target="_blank">arXiv:2011.02662</a> [<a href="http://arxiv.org/pdf/2011.02662" target="_blank">pdf</a>]

<h2>Restless-UCB, an Efficient and Low-complexity Algorithm for Online Restless Bandits. (arXiv:2011.02664v2 [cs.LG] UPDATED)</h2>
<h3>Siwei Wang, Longbo Huang, John C.S. Lui</h3>
<p>We study the online restless bandit problem, where the state of each arm
evolves according to a Markov chain, and the reward of pulling an arm depends
on both the pulled arm and the current state of the corresponding Markov chain.
In this paper, we propose Restless-UCB, a learning policy that follows the
explore-then-commit framework. In Restless-UCB, we present a novel method to
construct offline instances, which only requires $O(N)$ time-complexity ($N$ is
the number of arms) and is exponentially better than the complexity of existing
learning policy. We also prove that Restless-UCB achieves a regret upper bound
of $\tilde{O}((N+M^3)T^{2\over 3})$, where $M$ is the Markov chain state space
size and $T$ is the time horizon. Compared to existing algorithms, our result
eliminates the exponential factor (in $M,N$) in the regret upper bound, due to
a novel exploitation of the sparsity in transitions in general restless bandit
problems. As a result, our analysis technique can also be adopted to tighten
the regret bounds of existing algorithms. Finally, we conduct experiments based
on real-world dataset, to compare the Restless-UCB policy with state-of-the-art
benchmarks. Our results show that Restless-UCB outperforms existing algorithms
in regret, and significantly reduces the running time.
</p>
<a href="http://arxiv.org/abs/2011.02664" target="_blank">arXiv:2011.02664</a> [<a href="http://arxiv.org/pdf/2011.02664" target="_blank">pdf</a>]

<h2>Goal-driven Long-Term Trajectory Prediction. (arXiv:2011.02751v2 [cs.CV] UPDATED)</h2>
<h3>Hung Tran, Vuong Le, Truyen Tran</h3>
<p>The prediction of humans' short-term trajectories has advanced significantly
with the use of powerful sequential modeling and rich environment feature
extraction. However, long-term prediction is still a major challenge for the
current methods as the errors could accumulate along the way. Indeed,
consistent and stable prediction far to the end of a trajectory inherently
requires deeper analysis into the overall structure of that trajectory, which
is related to the pedestrian's intention on the destination of the journey. In
this work, we propose to model a hypothetical process that determines
pedestrians' goals and the impact of such process on long-term future
trajectories. We design Goal-driven Trajectory Prediction model - a
dual-channel neural network that realizes such intuition. The two channels of
the network take their dedicated roles and collaborate to generate future
trajectories. Different than conventional goal-conditioned, planning-based
methods, the model architecture is designed to generalize the patterns and work
across different scenes with arbitrary geometrical and semantic structures. The
model is shown to outperform the state-of-the-art in various settings,
especially in large prediction horizons. This result is another evidence for
the effectiveness of adaptive structured representation of visual and
geometrical features in human behavior analysis.
</p>
<a href="http://arxiv.org/abs/2011.02751" target="_blank">arXiv:2011.02751</a> [<a href="http://arxiv.org/pdf/2011.02751" target="_blank">pdf</a>]

<h2>Digital Twins: State of the Art Theory and Practice, Challenges, and Open Research Questions. (arXiv:2011.02833v2 [cs.LG] UPDATED)</h2>
<h3>Angira Sharma, Edward Kosasih, Jie Zhang, Alexandra Brintrup, Anisoara Calinescu</h3>
<p>Digital Twin was introduced over a decade ago, as an innovative
all-encompassing tool, with perceived benefits including real-time monitoring,
simulation and forecasting. However, the theoretical framework and practical
implementations of digital twins (DT) are still far from this vision. Although
successful implementations exist, sufficient implementation details are not
publicly available, therefore it is difficult to assess their effectiveness,
draw comparisons and jointly advance the DT methodology. This work explores the
various DT features and current approaches, the shortcomings and reasons behind
the delay in the implementation and adoption of digital twin. Advancements in
machine learning, internet of things and big data have contributed hugely to
the improvements in DT with regards to its real-time monitoring and forecasting
properties. Despite this progress and individual company-based efforts, certain
research gaps exist in the field, which have caused delay in the widespread
adoption of this concept. We reviewed relevant works and identified that the
major reasons for this delay are the lack of a universal reference framework,
domain dependence, security concerns of shared data, reliance of digital twin
on other technologies, and lack of quantitative metrics. We define the
necessary components of a digital twin required for a universal reference
framework, which also validate its uniqueness as a concept compared to similar
concepts like simulation, autonomous systems, etc. This work further assesses
the digital twin applications in different domains and the current state of
machine learning and big data in it. It thus answers and identifies novel
research questions, both of which will help to better understand and advance
the theory and practice of digital twins.
</p>
<a href="http://arxiv.org/abs/2011.02833" target="_blank">arXiv:2011.02833</a> [<a href="http://arxiv.org/pdf/2011.02833" target="_blank">pdf</a>]

<h2>Revisiting Stereo Depth Estimation From a Sequence-to-Sequence Perspective with Transformers. (arXiv:2011.02910v2 [cs.CV] UPDATED)</h2>
<h3>Zhaoshuo Li, Xingtong Liu, Francis X. Creighton, Russell H. Taylor, Mathias Unberath</h3>
<p>Stereo depth estimation relies on optimal correspondence matching between
pixels on epipolar lines in the left and right image to infer depth. Rather
than matching individual pixels, in this work, we revisit the problem from a
sequence-to-sequence correspondence perspective to replace cost volume
construction with dense pixel matching using position information and
attention. This approach, named STereo TRansformer (STTR), has several
advantages: It 1) relaxes the limitation of a fixed disparity range, 2)
identifies occluded regions and provides confidence of estimation, and 3)
imposes uniqueness constraints during the matching process. We report promising
results on both synthetic and real-world datasets and demonstrate that STTR
generalizes well across different domains, even without fine-tuning. Our code
is publicly available at https://github.com/mli0603/stereo-transformer.
</p>
<a href="http://arxiv.org/abs/2011.02910" target="_blank">arXiv:2011.02910</a> [<a href="http://arxiv.org/pdf/2011.02910" target="_blank">pdf</a>]

<h2>Measuring Data Collection Quality for Community Healthcare. (arXiv:2011.02962v2 [cs.LG] UPDATED)</h2>
<h3>Ramesha Karunasena, Mohammad Sarparajul Ambiya, Arunesh Sinha, Ruchit Nagar, Saachi Dalal, Divy Thakkar, Milind Tambe</h3>
<p>Machine learning has tremendous potential to provide targeted interventions
in low-resource communities, however the availability of high-quality public
health data is a significant challenge. In this work, we partner with field
experts at an non-governmental organization (NGO) in India to define and test a
data collection quality score for each health worker who collects data. This
challenging unlabeled data problem is handled by building upon domain-expert's
guidance to design a useful data representation that is then clustered to infer
a data quality score. We also provide a more interpretable version of the
score. These scores already provide for a measurement of data collection
quality; in addition, we also predict the quality for future time steps and
find our results to be very accurate. Our work was successfully field tested
and is in the final stages of deployment in Rajasthan, India.
</p>
<a href="http://arxiv.org/abs/2011.02962" target="_blank">arXiv:2011.02962</a> [<a href="http://arxiv.org/pdf/2011.02962" target="_blank">pdf</a>]

