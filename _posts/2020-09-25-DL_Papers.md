---
title: Latest Deep Learning Papers
date: 2020-12-09 21:00:01 +0800
featured-img: DL-logo
categories: [Deep_Learning]
tags: [Deep_Learning]
mathjax: Yes
---

<h1>Your arXiv Feed (180 Articles)</h1>
<h2>Spectral clustering via adaptive layer aggregation for multi-layer networks. (arXiv:2012.04646v1 [stat.ML])</h2>
<h3>Sihan Huang, Haolei Weng, Yang Feng</h3>
<p>One of the fundamental problems in network analysis is detecting community
structure in multi-layer networks, of which each layer represents one type of
edge information among the nodes. We propose integrative spectral clustering
approaches based on effective convex layer aggregations. Our aggregation
methods are strongly motivated by a delicate asymptotic analysis of the
spectral embedding of weighted adjacency matrices and the downstream $k$-means
clustering, in a challenging regime where community detection consistency is
impossible. In fact, the methods are shown to estimate the optimal convex
aggregation, which minimizes the mis-clustering error under some specialized
multi-layer network models. Our analysis further suggests that clustering using
Gaussian mixture models is generally superior to the commonly used $k$-means in
spectral clustering. Extensive numerical studies demonstrate that our adaptive
aggregation techniques, together with Gaussian mixture model clustering, make
the new spectral clustering remarkably competitive compared to several
popularly used methods.
</p>
<a href="http://arxiv.org/abs/2012.04646" target="_blank">arXiv:2012.04646</a> [<a href="http://arxiv.org/pdf/2012.04646" target="_blank">pdf</a>]

<h2>Application of a novel machine learning based optimization algorithm (ActivO) for accelerating simulation-driven engine design. (arXiv:2012.04649v1 [cs.LG])</h2>
<h3>Opeoluwa Owoyele, Pinaki Pal</h3>
<p>A novel design optimization approach (ActivO) that employs an ensemble of
machine learning algorithms is presented. The proposed approach is a
surrogate-based scheme, where the predictions of a weak leaner and a strong
learner are utilized within an active learning loop. The weak learner is used
to identify promising regions within the design space to explore, while the
strong learner is used to determine the exact location of the optimum within
promising regions. For each design iteration, exploration is done by randomly
selecting evaluation points within regions where the weak learner-predicted
fitness is high. The global optimum obtained by using the strong learner as a
surrogate is also evaluated to enable rapid convergence once the most promising
region has been identified. The proposed approach is tested on two optimization
problems. The first is a cosine mixture test problem with 25 local optima and
one global optimum. In the second problem, the objective is to minimize
indicated specific fuel consumption of a compression-ignition internal
combustion (IC) engine while adhering to desired constraints associated with
in-cylinder pressure and emissions, by finding the optimal combination of nine
design parameters relating to fuel injection, thermodynamic conditions, and
in-cylinder flow. The efficacy of the proposed approach is compared to that of
a genetic algorithm, which is widely used within the IC engine community for
engine optimization, showing that ActivO reduces the number of function
evaluations needed to reach the global optimum, and thereby time-to-design by
80%.
</p>
<a href="http://arxiv.org/abs/2012.04649" target="_blank">arXiv:2012.04649</a> [<a href="http://arxiv.org/pdf/2012.04649" target="_blank">pdf</a>]

<h2>A Dataset and Application for Facial Recognition of Individual Gorillas in Zoo Environments. (arXiv:2012.04689v1 [cs.CV])</h2>
<h3>Otto Brookes, Tilo Burghardt</h3>
<p>We put forward a video dataset with 5k+ facial bounding box annotations
across a troop of 7 western lowland gorillas at Bristol Zoo Gardens. Training
on this dataset, we implement and evaluate a standard deep learning pipeline on
the task of facially recognising individual gorillas in a zoo environment. We
show that a basic YOLOv3-powered application is able to perform identifications
at 92% mAP when utilising single frames only. Tracking-by-detection-association
and identity voting across short tracklets yields an improved robust
performance of 97% mAP. To facilitate easy utilisation for enriching the
research capabilities of zoo environments, we publish the code, video dataset,
weights, and ground-truth annotations at data.bris.ac.uk.
</p>
<a href="http://arxiv.org/abs/2012.04689" target="_blank">arXiv:2012.04689</a> [<a href="http://arxiv.org/pdf/2012.04689" target="_blank">pdf</a>]

<h2>Locally optimal detection of stochastic targeted universal adversarial perturbations. (arXiv:2012.04692v1 [cs.CV])</h2>
<h3>Amish Goel, Pierre Moulin</h3>
<p>Deep learning image classifiers are known to be vulnerable to small
adversarial perturbations of input images. In this paper, we derive the locally
optimal generalized likelihood ratio test (LO-GLRT) based detector for
detecting stochastic targeted universal adversarial perturbations (UAPs) of the
classifier inputs. We also describe a supervised training method to learn the
detector's parameters, and demonstrate better performance of the detector
compared to other detection methods on several popular image classification
datasets.
</p>
<a href="http://arxiv.org/abs/2012.04692" target="_blank">arXiv:2012.04692</a> [<a href="http://arxiv.org/pdf/2012.04692" target="_blank">pdf</a>]

<h2>Emergence of Different Modes of Tool Use in a Reaching and Dragging Task. (arXiv:2012.04700v1 [cs.RO])</h2>
<h3>Khuong Nguyen, Yoonsuck Choe</h3>
<p>Tool use is an important milestone in the evolution of intelligence. In this
paper, we investigate different modes of tool use that emerge in a reaching and
dragging task. In this task, a jointed arm with a gripper must grab a tool (T,
I, or L-shaped) and drag an object down to the target location (the bottom of
the arena). The simulated environment had real physics such as gravity and
friction. We trained a deep-reinforcement learning based controller (with raw
visual and proprioceptive input) with minimal reward shaping information to
tackle this task. We observed the emergence of a wide range of unexpected
behaviors, not directly encoded in the motor primitives or reward functions.
Examples include hitting the object to the target location, correcting error of
initial contact, throwing the tool toward the object, as well as normal
expected behavior such as wide sweep. Also, we further analyzed these behaviors
based on the type of tool and the initial position of the target object. Our
results show a rich repertoire of behaviors, beyond the basic built-in
mechanisms of the deep reinforcement learning method we used.
</p>
<a href="http://arxiv.org/abs/2012.04700" target="_blank">arXiv:2012.04700</a> [<a href="http://arxiv.org/pdf/2012.04700" target="_blank">pdf</a>]

<h2>ODFNet: Using orientation distribution functions to characterize 3D point clouds. (arXiv:2012.04708v1 [cs.CV])</h2>
<h3>Yusuf H. Sahin, Alican Mertan, Gozde Unal</h3>
<p>Learning new representations of 3D point clouds is an active research area in
3D vision, as the order-invariant point cloud structure still presents
challenges to the design of neural network architectures. Recent works explored
learning either global or local features or both for point clouds, however none
of the earlier methods focused on capturing contextual shape information by
analysing local orientation distribution of points. In this paper, we leverage
on point orientation distributions around a point in order to obtain an
expressive local neighborhood representation for point clouds. We achieve this
by dividing the spherical neighborhood of a given point into predefined cone
volumes, and statistics inside each volume are used as point features. In this
way, a local patch can be represented by not only the selected point's nearest
neighbors, but also considering a point density distribution defined along
multiple orientations around the point. We are then able to construct an
orientation distribution function (ODF) neural network that involves an
ODFBlock which relies on mlp (multi-layer perceptron) layers. The new ODFNet
model achieves state-of the-art accuracy for object classification on
ModelNet40 and ScanObjectNN datasets, and segmentation on ShapeNet S3DIS
datasets.
</p>
<a href="http://arxiv.org/abs/2012.04708" target="_blank">arXiv:2012.04708</a> [<a href="http://arxiv.org/pdf/2012.04708" target="_blank">pdf</a>]

<h2>Canonical Capsules: Unsupervised Capsules in Canonical Pose. (arXiv:2012.04718v1 [cs.CV])</h2>
<h3>Weiwei Sun, Andrea Tagliasacchi, Boyang Deng, Sara Sabour, Soroosh Yazdani, Geoffrey Hinton, Kwang Moo Yi</h3>
<p>We propose an unsupervised capsule architecture for 3D point clouds. We
compute capsule decompositions of objects through permutation-equivariant
attention, and self-supervise the process by training with pairs of randomly
rotated objects. Our key idea is to aggregate the attention masks into semantic
keypoints, and use these to supervise a decomposition that satisfies the
capsule invariance/equivariance properties. This not only enables the training
of a semantically consistent decomposition, but also allows us to learn a
canonicalization operation that enables object-centric reasoning. In doing so,
we require neither classification labels nor manually-aligned training datasets
to train. Yet, by learning an object-centric representation in an unsupervised
manner, our method outperforms the state-of-the-art on 3D point cloud
reconstruction, registration, and unsupervised classification. We will release
the code and dataset to reproduce our results as soon as the paper is
published.
</p>
<a href="http://arxiv.org/abs/2012.04718" target="_blank">arXiv:2012.04718</a> [<a href="http://arxiv.org/pdf/2012.04718" target="_blank">pdf</a>]

<h2>SDSS-V Algorithms: Fast, Collision-Free Trajectory Planning for Heavily Overlapping Robotic Fiber Positioners. (arXiv:2012.04721v1 [cs.RO])</h2>
<h3>Conor Sayres, Jos&#xe9; R. S&#xe1;nchez-Gallego, Michael R. Blanton, Ricardo Araujo, Mohamed Bouri, Lo&#xef;c Grossen, Jean-Paul Kneib, Juna A. Kollmeier, Luzius Kronig, Richard W. Pogge, Sarah Tuttle</h3>
<p>Robotic fiber positioner (RFP) arrays are becoming heavily adopted in wide
field massively multiplexed spectroscopic survey instruments. RFP arrays
decrease nightly operational overheads through rapid reconfiguration between
fields and exposures. In comparison to similar instruments, SDSS-V has selected
a very dense RFP packing scheme where any point in a field is typically
accessible to three or more robots. This design provides flexibility in target
assignment. However, the task of collision-less trajectory planning is
especially challenging. We present two multi-agent distributed control
strategies that are highly efficient and computationally inexpensive for
determining collision-free paths for RFPs in heavily overlapping workspaces. We
demonstrate that a reconfiguration path between two arbitrary robot
configurations can be efficiently found if "folded" state, in which all robot
arms are retracted and aligned in a lattice-like orientation, is inserted
between the initial and final states. Although developed for SDSS-V, the
approach we describe is generic and so applicable to a wide range of RFP
designs and layouts. Robotic fiber positioner technology continues to advance
rapidly, and in the near future ultra-densely packed RFP designs may be
feasible. Our algorithms are especially capable in routing paths in very
crowded environments, where we see efficient results even in regimes
significantly more crowded than the SDSS-V RFP design.
</p>
<a href="http://arxiv.org/abs/2012.04721" target="_blank">arXiv:2012.04721</a> [<a href="http://arxiv.org/pdf/2012.04721" target="_blank">pdf</a>]

<h2>Neural Mechanics: Symmetry and Broken Conservation Laws in Deep Learning Dynamics. (arXiv:2012.04728v1 [cs.LG])</h2>
<h3>Daniel Kunin, Javier Sagastuy-Brena, Surya Ganguli, Daniel L.K. Yamins, Hidenori Tanaka</h3>
<p>Predicting the dynamics of neural network parameters during training is one
of the key challenges in building a theoretical foundation for deep learning. A
central obstacle is that the motion of a network in high-dimensional parameter
space undergoes discrete finite steps along complex stochastic gradients
derived from real-world datasets. We circumvent this obstacle through a
unifying theoretical framework based on intrinsic symmetries embedded in a
network's architecture that are present for any dataset. We show that any such
symmetry imposes stringent geometric constraints on gradients and Hessians,
leading to an associated conservation law in the continuous-time limit of
stochastic gradient descent (SGD), akin to Noether's theorem in physics. We
further show that finite learning rates used in practice can actually break
these symmetry induced conservation laws. We apply tools from finite difference
methods to derive modified gradient flow, a differential equation that better
approximates the numerical trajectory taken by SGD at finite learning rates. We
combine modified gradient flow with our framework of symmetries to derive exact
integral expressions for the dynamics of certain parameter combinations. We
empirically validate our analytic predictions for learning dynamics on VGG-16
trained on Tiny ImageNet. Overall, by exploiting symmetry, our work
demonstrates that we can analytically describe the learning dynamics of various
parameter combinations at finite learning rates and batch sizes for state of
the art architectures trained on any dataset.
</p>
<a href="http://arxiv.org/abs/2012.04728" target="_blank">arXiv:2012.04728</a> [<a href="http://arxiv.org/pdf/2012.04728" target="_blank">pdf</a>]

<h2>On 1/n neural representation and robustness. (arXiv:2012.04729v1 [cs.LG])</h2>
<h3>Josue Nassar, Piotr Aleksander Sokol, SueYeon Chung, Kenneth D. Harris, Il Memming Park</h3>
<p>Understanding the nature of representation in neural networks is a goal
shared by neuroscience and machine learning. It is therefore exciting that both
fields converge not only on shared questions but also on similar approaches. A
pressing question in these areas is understanding how the structure of the
representation used by neural networks affects both their generalization, and
robustness to perturbations. In this work, we investigate the latter by
juxtaposing experimental results regarding the covariance spectrum of neural
representations in the mouse V1 (Stringer et al) with artificial neural
networks. We use adversarial robustness to probe Stringer et al's theory
regarding the causal role of a 1/n covariance spectrum. We empirically
investigate the benefits such a neural code confers in neural networks, and
illuminate its role in multi-layer architectures. Our results show that
imposing the experimentally observed structure on artificial neural networks
makes them more robust to adversarial attacks. Moreover, our findings
complement the existing theory relating wide neural networks to kernel methods,
by showing the role of intermediate representations.
</p>
<a href="http://arxiv.org/abs/2012.04729" target="_blank">arXiv:2012.04729</a> [<a href="http://arxiv.org/pdf/2012.04729" target="_blank">pdf</a>]

<h2>Long Term Motion Prediction Using Keyposes. (arXiv:2012.04731v1 [cs.CV])</h2>
<h3>Sena Kiciroglu, Wei Wang, Mathieu Salzmann, Pascal Fua</h3>
<p>Long term human motion prediction is an essential component in
safety-critical applications, such as human-robot interaction and autonomous
driving. We argue that, to achieve long term forecasting, predicting human pose
at every time instant is unnecessary because human motion follows patterns that
are well-represented by a few essential poses in the sequence. We call such
poses "keyposes", and approximate complex motions by linearly interpolating
between subsequent keyposes. We show that learning the sequence of such
keyposes allows us to predict very long term motion, up to 5 seconds in the
future. In particular, our predictions are much more realistic and better
preserve the motion dynamics than those obtained by the state-of-the-art
methods. Furthermore, our approach models the future keyposes
probabilistically, which, during inference, lets us generate diverse future
motions via sampling.
</p>
<a href="http://arxiv.org/abs/2012.04731" target="_blank">arXiv:2012.04731</a> [<a href="http://arxiv.org/pdf/2012.04731" target="_blank">pdf</a>]

<h2>CARAFE++: Unified Content-Aware ReAssembly of FEatures. (arXiv:2012.04733v1 [cs.CV])</h2>
<h3>Jiaqi Wang, Kai Chen, Rui Xu, Ziwei Liu, Chen Change Loy, Dahua Lin</h3>
<p>Feature reassembly, i.e. feature downsampling and upsampling, is a key
operation in a number of modern convolutional network architectures, e.g.,
residual networks and feature pyramids. Its design is critical for dense
prediction tasks such as object detection and semantic/instance segmentation.
In this work, we propose unified Content-Aware ReAssembly of FEatures
(CARAFE++), a universal, lightweight and highly effective operator to fulfill
this goal. CARAFE++ has several appealing properties: (1) Unlike conventional
methods such as pooling and interpolation that only exploit sub-pixel
neighborhood, CARAFE++ aggregates contextual information within a large
receptive field. (2) Instead of using a fixed kernel for all samples (e.g.
convolution and deconvolution), CARAFE++ generates adaptive kernels on-the-fly
to enable instance-specific content-aware handling. (3) CARAFE++ introduces
little computational overhead and can be readily integrated into modern network
architectures. We conduct comprehensive evaluations on standard benchmarks in
object detection, instance/semantic segmentation and image inpainting. CARAFE++
shows consistent and substantial gains across all the tasks (2.5% APbox, 2.1%
APmask, 1.94% mIoU, 1.35 dB respectively) with negligible computational
overhead. It shows great potential to serve as a strong building block for
modern deep networks.
</p>
<a href="http://arxiv.org/abs/2012.04733" target="_blank">arXiv:2012.04733</a> [<a href="http://arxiv.org/pdf/2012.04733" target="_blank">pdf</a>]

<h2>Graph-Based Generative Representation Learning of Semantically and Behaviorally Augmented Floorplans. (arXiv:2012.04735v1 [cs.LG])</h2>
<h3>Vahid Azizi, Muhammad Usman, Honglu Zhou, Petros Faloutsos, Mubbasir Kapadia</h3>
<p>Floorplans are commonly used to represent the layout of buildings. In
computer aided-design (CAD) floorplans are usually represented in the form of
hierarchical graph structures. Research works towards computational techniques
that facilitate the design process, such as automated analysis and
optimization, often use simple floorplan representations that ignore the
semantics of the space and do not take into account usage related analytics. We
present a floorplan embedding technique that uses an attributed graph to
represent the geometric information as well as design semantics and behavioral
features of the inhabitants as node and edge attributes. A Long Short-Term
Memory (LSTM) Variational Autoencoder (VAE) architecture is proposed and
trained to embed attributed graphs as vectors in a continuous space. A user
study is conducted to evaluate the coupling of similar floorplans retrieved
from the embedding space with respect to a given input (e.g., design layout).
The qualitative, quantitative and user-study evaluations show that our
embedding framework produces meaningful and accurate vector representations for
floorplans. In addition, our proposed model is a generative model. We studied
and showcased its effectiveness for generating new floorplans. We also release
the dataset that we have constructed and which, for each floorplan, includes
the design semantics attributes as well as simulation generated human
behavioral features for further study in the community.
</p>
<a href="http://arxiv.org/abs/2012.04735" target="_blank">arXiv:2012.04735</a> [<a href="http://arxiv.org/pdf/2012.04735" target="_blank">pdf</a>]

<h2>River: machine learning for streaming data in Python. (arXiv:2012.04740v1 [cs.LG])</h2>
<h3>Jacob Montiel, Max Halford, Saulo Martiello Mastelini, Geoffrey Bolmier, Raphael Sourty, Robin Vaysse, Adil Zouitine, Heitor Murilo Gomes, Jesse Read, Talel Abdessalem, Albert Bifet</h3>
<p>River is a machine learning library for dynamic data streams and continual
learning. It provides multiple state-of-the-art learning methods, data
generators/transformers, performance metrics and evaluators for different
stream learning problems. It is the result from the merger of the two most
popular packages for stream learning in Python: Creme and scikit-multiflow.
River introduces a revamped architecture based on the lessons learnt from the
seminal packages. River's ambition is to be the go-to library for doing machine
learning on streaming data. Additionally, this open source package brings under
the same umbrella a large community of practitioners and researchers. The
source code is available at https://github.com/online-ml/river.
</p>
<a href="http://arxiv.org/abs/2012.04740" target="_blank">arXiv:2012.04740</a> [<a href="http://arxiv.org/pdf/2012.04740" target="_blank">pdf</a>]

<h2>Robust Neural Routing Through Space Partitions for Camera Relocalization in Dynamic Indoor Environments. (arXiv:2012.04746v1 [cs.CV])</h2>
<h3>Siyan Dong, Qingnan Fan, He Wang, Ji Shi, Li Yi, Thomas Funkhouser, Baoquan Chen, Leonidas Guibas</h3>
<p>Localizing the camera in a known indoor environment is a key building block
for scene mapping, robot navigation, AR, etc. Recent advances estimate the
camera pose via optimization over the 2D/3D-3D correspondences established
between the coordinates in 2D/3D camera space and 3D world space. Such a
mapping is estimated with either a convolution neural network or a decision
tree using only the static input image sequence, which makes these approaches
vulnerable to dynamic indoor environments that are quite common yet challenging
in the real world. To address the aforementioned issues, in this paper, we
propose a novel outlier-aware neural tree which bridges the two worlds, deep
learning and decision tree approaches. It builds on three important blocks; (a)
a hierarchical space partition over the indoor scene to construct the decision
tree; (b) a neural routing function, implemented as a deep classification
network, employed for better 3D scene understanding; and (c) an outlier
rejection module used to filter out dynamic points during the hierarchical
routing process. Our proposed algorithm is evaluated on the RIO-10 benchmark
developed for camera relocalization in dynamic indoor environment. It achieves
robust neural routing through space partitions and outperforms the
state-of-the-art approaches by around 30\% on camera pose accuracy, while
running comparably fast for evaluation.
</p>
<a href="http://arxiv.org/abs/2012.04746" target="_blank">arXiv:2012.04746</a> [<a href="http://arxiv.org/pdf/2012.04746" target="_blank">pdf</a>]

<h2>STELAR: Spatio-temporal Tensor Factorization with Latent Epidemiological Regularization. (arXiv:2012.04747v1 [cs.LG])</h2>
<h3>Nikos Kargas, Cheng Qian, Nicholas D. Sidiropoulos, Cao Xiao, Lucas M. Glass, Jimeng Sun</h3>
<p>Accurate prediction of the transmission of epidemic diseases such as COVID-19
is crucial for implementing effective mitigation measures. In this work, we
develop a tensor method to predict the evolution of epidemic trends for many
regions simultaneously. We construct a 3-way spatio-temporal tensor (location,
attribute, time) of case counts and propose a nonnegative tensor factorization
with latent epidemiological model regularization named STELAR. Unlike standard
tensor factorization methods which cannot predict slabs ahead, STELAR enables
long-term prediction by incorporating latent temporal regularization through a
system of discrete-time difference equations of a widely adopted
epidemiological model. We use latent instead of location/attribute-level
epidemiological dynamics to capture common epidemic profile sub-types and
improve collaborative learning and prediction. We conduct experiments using
both county- and state-level COVID-19 data and show that our model can identify
interesting latent patterns of the epidemic. Finally, we evaluate the
predictive ability of our method and show superior performance compared to the
baselines, achieving up to 21% lower root mean square error and 25% lower mean
absolute error for county-level prediction.
</p>
<a href="http://arxiv.org/abs/2012.04747" target="_blank">arXiv:2012.04747</a> [<a href="http://arxiv.org/pdf/2012.04747" target="_blank">pdf</a>]

<h2>Mitigating the Impact of Adversarial Attacks in Very Deep Networks. (arXiv:2012.04750v1 [cs.CV])</h2>
<h3>Mohammed Hassanin, Ibrahim Radwan, Nour Moustafa, Murat Tahtali, Neeraj Kumar</h3>
<p>Deep Neural Network (DNN) models have vulnerabilities related to security
concerns, with attackers usually employing complex hacking techniques to expose
their structures. Data poisoning-enabled perturbation attacks are complex
adversarial ones that inject false data into models. They negatively impact the
learning process, with no benefit to deeper networks, as they degrade a model's
accuracy and convergence rates. In this paper, we propose an
attack-agnostic-based defense method for mitigating their influence. In it, a
Defensive Feature Layer (DFL) is integrated with a well-known DNN architecture
which assists in neutralizing the effects of illegitimate perturbation samples
in the feature space. To boost the robustness and trustworthiness of this
method for correctly classifying attacked input samples, we regularize the
hidden space of a trained model with a discriminative loss function called
Polarized Contrastive Loss (PCL). It improves discrimination among samples in
different classes and maintains the resemblance of those in the same class.
Also, we integrate a DFL and PCL in a compact model for defending against data
poisoning attacks. This method is trained and tested using the CIFAR-10 and
MNIST datasets with data poisoning-enabled perturbation attacks, with the
experimental results revealing its excellent performance compared with those of
recent peer techniques.
</p>
<a href="http://arxiv.org/abs/2012.04750" target="_blank">arXiv:2012.04750</a> [<a href="http://arxiv.org/pdf/2012.04750" target="_blank">pdf</a>]

<h2>EvoCraft: A New Challenge for Open-Endedness. (arXiv:2012.04751v1 [cs.AI])</h2>
<h3>Djordje Grbic, Rasmus Berg Palm, Elias Najarro, Claire Glanois, Sebastian Risi</h3>
<p>This paper introduces EvoCraft, a framework for Minecraft designed to study
open-ended algorithms. We introduce an API that provides an open-source Python
interface for communicating with Minecraft to place and track blocks. In
contrast to previous work in Minecraft that focused on learning to play the
game, the grand challenge we pose here is to automatically search for
increasingly complex artifacts in an open-ended fashion. Compared to other
environments used to study open-endedness, Minecraft allows the construction of
almost any kind of structure, including actuated machines with circuits and
mechanical components. We present initial baseline results in evolving simple
Minecraft creations through both interactive and automated evolution. While
evolution succeeds when tasked to grow a structure towards a specific target,
it is unable to find a solution when rewarded for creating a simple machine
that moves. Thus, EvoCraft offers a challenging new environment for automated
search methods (such as evolution) to find complex artifacts that we hope will
spur the development of more open-ended algorithms. A Python implementation of
the EvoCraft framework is available at:
https://github.com/real-itu/Evocraft-py.
</p>
<a href="http://arxiv.org/abs/2012.04751" target="_blank">arXiv:2012.04751</a> [<a href="http://arxiv.org/pdf/2012.04751" target="_blank">pdf</a>]

<h2>Automatic Registration and Convex Clustering of Time Series. (arXiv:2012.04756v1 [stat.ML])</h2>
<h3>Michael Weylandt, George Michailidis</h3>
<p>Clustering of time series data exhibits a number of challenges not present in
other settings, notably the problem of registration (alignment) of observed
signals. Typical approaches include pre-registration to a user-specified
template or time warping approaches which attempt to optimally align series
with a minimum of distortion. For many signals obtained from recording or
sensing devices, these methods may be unsuitable as a template signal is not
available for pre-registration, while the distortion of warping approaches may
obscure meaningful temporal information. We propose a new method for automatic
time series alignment within a convex clustering problem. Our approach,
Temporal Registration using Optimal Unitary Transformations (TROUT), is based
on a novel distance metric between time series that is easy to compute and
automatically identifies optimal alignment between pairs of time series. By
embedding our new metric in a convex formulation, we retain well-known
advantages of computational and statistical performance. We provide an
efficient algorithm for TROUT-based clustering and demonstrate its superior
performance over a range of competitors.
</p>
<a href="http://arxiv.org/abs/2012.04756" target="_blank">arXiv:2012.04756</a> [<a href="http://arxiv.org/pdf/2012.04756" target="_blank">pdf</a>]

<h2>Concept Drift and Covariate Shift Detection Ensemble with Lagged Labels. (arXiv:2012.04759v1 [cs.AI])</h2>
<h3>Yiming Xu, Diego Klabjan</h3>
<p>In model serving, having one fixed model during the entire often life-long
inference process is usually detrimental to model performance, as data
distribution evolves over time, resulting in lack of reliability of the model
trained on historical data. It is important to detect changes and retrain the
model in time. The existing methods generally have three weaknesses: 1) using
only classification error rate as signal, 2) assuming ground truth labels are
immediately available after features from samples are received and 3) unable to
decide what data to use to retrain the model when change occurs. We address the
first problem by utilizing six different signals to capture a wide range of
characteristics of data, and we address the second problem by allowing lag of
labels, where labels of corresponding features are received after a lag in
time. For the third problem, our proposed method automatically decides what
data to use to retrain based on the signals. Extensive experiments on
structured and unstructured data for different type of data changes establish
that our method consistently outperforms the state-of-the-art methods by a
large margin.
</p>
<a href="http://arxiv.org/abs/2012.04759" target="_blank">arXiv:2012.04759</a> [<a href="http://arxiv.org/pdf/2012.04759" target="_blank">pdf</a>]

<h2>Simultaneous Grouping and Denoising via Sparse Convex Wavelet Clustering. (arXiv:2012.04762v1 [stat.ML])</h2>
<h3>Michael Weylandt, T. Mitchell Roddenberry, Genevera I. Allen</h3>
<p>Clustering is a ubiquitous problem in data science and signal processing. In
many applications where we observe noisy signals, it is common practice to
first denoise the data, perhaps using wavelet denoising, and then to apply a
clustering algorithm. In this paper, we develop a sparse convex wavelet
clustering approach that simultaneously denoises and discovers groups. Our
approach utilizes convex fusion penalties to achieve agglomeration and
group-sparse penalties to denoise through sparsity in the wavelet domain. In
contrast to common practice which denoises then clusters, our method is a
unified, convex approach that performs both simultaneously. Our method yields
denoised (wavelet-sparse) cluster centroids that both improve interpretability
and data compression. We demonstrate our method on synthetic examples and in an
application to NMR spectroscopy.
</p>
<a href="http://arxiv.org/abs/2012.04762" target="_blank">arXiv:2012.04762</a> [<a href="http://arxiv.org/pdf/2012.04762" target="_blank">pdf</a>]

<h2>Methodology for Mining, Discovering and Analyzing Semantic Human Mobility Behaviors. (arXiv:2012.04767v1 [cs.LG])</h2>
<h3>Clement Moreau, Thomas Devogele, Veronika Peralta, Laurent Etienne, Cyril de Runz</h3>
<p>Several institutes produce large semantic data sets about daily activities
and human mobility. The analysis and the understanding of these data are
crucial for urban planning, socio-psychology and political sciences or
epidemiology. Currently, none of usual data mining process is customized for a
complete analysis of semantic mobility sequences from data to understandable
behaviors. Based on an extended review of the literature, we define in this
article a new methodological pipeline, SIMBA (Semantic Indicators for Mobility
and Behavior Analysis), for mine and analyze semantic mobility sequences in
order to discover coherent information and human behaviors. A framework for
semantic sequence mobility analysis and clustering explicability integrating
different complementary statistical indicators and visual tools is proposed. To
validate this methodology, we use a large set of real daily mobility sequences
obtained from one household-travel survey. Complementary knowledge are
automatically discovered help to this method.
</p>
<a href="http://arxiv.org/abs/2012.04767" target="_blank">arXiv:2012.04767</a> [<a href="http://arxiv.org/pdf/2012.04767" target="_blank">pdf</a>]

<h2>A Data-Driven Analytical Framework of Estimating Multimodal Travel Demand Patterns using Mobile Device Location Data. (arXiv:2012.04776v1 [cs.LG])</h2>
<h3>Chenfeng Xiong, Aref Darzi, Yixuan Pan, Sepehr Ghader, Lei Zhang</h3>
<p>While benefiting people's daily life in so many ways, smartphones and their
location-based services are generating massive mobile device location data that
has great potential to help us understand travel demand patterns and make
transportation planning for the future. While recent studies have analyzed
human travel behavior using such new data sources, limited research has been
done to extract multimodal travel demand patterns out of them. This paper
presents a data-driven analytical framework to bridge the gap. To be able to
successfully detect travel modes using the passively collected location
information, we conduct a smartphone-based GPS survey to collect ground truth
observations. Then a jointly trained single-layer model and deep neural network
for travel mode imputation is developed. Being "wide" and "deep" at the same
time, this model combines the advantages of both types of models. The framework
also incorporates the multimodal transportation network in order to evaluate
the closeness of trip routes to the nearby rail, metro, highway and bus lines
and therefore enhance the imputation accuracy. To showcase the applications of
the introduced framework in answering real-world planning needs, a separate
mobile device location data is processed through trip end identification and
attribute generation, in a way that the travel mode imputation can be directly
applied. The estimated multimodal travel demand patterns are then validated
against typical household travel surveys in the same Washington D.C. and
Baltimore Metropolitan Regions.
</p>
<a href="http://arxiv.org/abs/2012.04776" target="_blank">arXiv:2012.04776</a> [<a href="http://arxiv.org/pdf/2012.04776" target="_blank">pdf</a>]

<h2>You Only Need Adversarial Supervision for Semantic Image Synthesis. (arXiv:2012.04781v1 [cs.CV])</h2>
<h3>Vadim Sushko, Edgar Sch&#xf6;nfeld, Dan Zhang, Juergen Gall, Bernt Schiele, Anna Khoreva</h3>
<p>Despite their recent successes, GAN models for semantic image synthesis still
suffer from poor image quality when trained with only adversarial supervision.
Historically, additionally employing the VGG-based perceptual loss has helped
to overcome this issue, significantly improving the synthesis quality, but at
the same time limiting the progress of GAN models for semantic image synthesis.
In this work, we propose a novel, simplified GAN model, which needs only
adversarial supervision to achieve high quality results. We re-design the
discriminator as a semantic segmentation network, directly using the given
semantic label maps as the ground truth for training. By providing stronger
supervision to the discriminator as well as to the generator through spatially-
and semantically-aware discriminator feedback, we are able to synthesize images
of higher fidelity with better alignment to their input label maps, making the
use of the perceptual loss superfluous. Moreover, we enable high-quality
multi-modal image synthesis through global and local sampling of a 3D noise
tensor injected into the generator, which allows complete or partial image
change. We show that images synthesized by our model are more diverse and
follow the color and texture distributions of real images more closely. We
achieve an average improvement of $6$ FID and $5$ mIoU points over the state of
the art across different datasets using only adversarial supervision.
</p>
<a href="http://arxiv.org/abs/2012.04781" target="_blank">arXiv:2012.04781</a> [<a href="http://arxiv.org/pdf/2012.04781" target="_blank">pdf</a>]

<h2>Deep Learning based Multi-Modal Sensing for Tracking and State Extraction of Small Quadcopters. (arXiv:2012.04794v1 [cs.CV])</h2>
<h3>Zhibo Zhang, Chen Zeng, Maulikkumar Dhameliya, Souma Chowdhury, Rahul Rai</h3>
<p>This paper proposes a multi-sensor based approach to detect, track, and
localize a quadcopter unmanned aerial vehicle (UAV). Specifically, a pipeline
is developed to process monocular RGB and thermal video (captured from a fixed
platform) to detect and track the UAV in our FoV. Subsequently, a 2D planar
lidar is used to allow conversion of pixel data to actual distance
measurements, and thereby enable localization of the UAV in global coordinates.
The monocular data is processed through a deep learning-based object detection
method that computes an initial bounding box for the UAV. The thermal data is
processed through a thresholding and Kalman filter approach to detect and track
the bounding box. Training and testing data are prepared by combining a set of
original experiments conducted in a motion capture environment and publicly
available UAV image data. The new pipeline compares favorably to existing
methods and demonstrates promising tracking and localization capacity of sample
experiments.
</p>
<a href="http://arxiv.org/abs/2012.04794" target="_blank">arXiv:2012.04794</a> [<a href="http://arxiv.org/pdf/2012.04794" target="_blank">pdf</a>]

<h2>A Statistical Test for Probabilistic Fairness. (arXiv:2012.04800v1 [cs.LG])</h2>
<h3>Bahar Taskesen, Jose Blanchet, Daniel Kuhn, Viet Anh Nguyen</h3>
<p>Algorithms are now routinely used to make consequential decisions that affect
human lives. Examples include college admissions, medical interventions or law
enforcement. While algorithms empower us to harness all information hidden in
vast amounts of data, they may inadvertently amplify existing biases in the
available datasets. This concern has sparked increasing interest in fair
machine learning, which aims to quantify and mitigate algorithmic
discrimination. Indeed, machine learning models should undergo intensive tests
to detect algorithmic biases before being deployed at scale. In this paper, we
use ideas from the theory of optimal transport to propose a statistical
hypothesis test for detecting unfair classifiers. Leveraging the geometry of
the feature space, the test statistic quantifies the distance of the empirical
distribution supported on the test samples to the manifold of distributions
that render a pre-trained classifier fair. We develop a rigorous hypothesis
testing mechanism for assessing the probabilistic fairness of any pre-trained
logistic classifier, and we show both theoretically as well as empirically that
the proposed test is asymptotically correct. In addition, the proposed
framework offers interpretability by identifying the most favorable
perturbation of the data so that the given classifier becomes fair.
</p>
<a href="http://arxiv.org/abs/2012.04800" target="_blank">arXiv:2012.04800</a> [<a href="http://arxiv.org/pdf/2012.04800" target="_blank">pdf</a>]

<h2>GATSBI: An Online GTSP-Based Algorithm for Targeted Surface Bridge Inspection. (arXiv:2012.04803v1 [cs.RO])</h2>
<h3>Kevin Yu, Harnaik Dhami, Kartik Madhira, Pratap Tokekar</h3>
<p>We study the problem of visually inspecting the surface of a bridge using an
Unmanned Aerial Vehicle (UAV) for defects. We do not assume that the geometric
model of the bridge is known. The UAV is equipped with a LiDAR and RGB sensor
that is used to build a 3D semantic map of the environment. Our planner, termed
GATSBI, plans in an online fashion a path that is targeted towards inspecting
all points on the surface of the bridge. The input to GATSBI consists of a 3D
occupancy grid map of the part of the environment seen by the UAV so far. We
use semantic segmentation to segment the voxels into those that are part of the
bridge and the surroundings. Inspecting a bridge voxel requires the UAV to take
images from a desired viewing angle and distance. We then create a Generalized
Traveling Salesperson Problem (GTSP) instance to cluster candidate viewpoints
for inspecting the bridge voxels and use an off-the-shelf GTSP solver to find
the optimal path for the given instance. As more parts of the environment are
seen, we replan the path. We evaluate the performance of our algorithm through
high-fidelity simulations conducted in Gazebo. We compare the performance of
this algorithm with a frontier exploration algorithm. Our evaluation reveals
that targeting the inspection to only the segmented bridge voxels and planning
carefully using a GTSP solver leads to more efficient inspection than the
baseline algorithms.
</p>
<a href="http://arxiv.org/abs/2012.04803" target="_blank">arXiv:2012.04803</a> [<a href="http://arxiv.org/pdf/2012.04803" target="_blank">pdf</a>]

<h2>Semi-Supervised Off Policy Reinforcement Learning. (arXiv:2012.04809v1 [cs.LG])</h2>
<h3>Aaron Sonabend-W, Nilanjana Laha, Rajarshi Mukherjee, Tianxi Cai</h3>
<p>Reinforcement learning (RL) has shown great success in estimating sequential
treatment strategies which account for patient heterogeneity. However,
health-outcome information is often not well coded but rather embedded in
clinical notes. Extracting precise outcome information is a resource intensive
task. This translates into only small well-annotated cohorts available. We
propose a semi-supervised learning (SSL) approach that can efficiently leverage
a small sized labeled data $\mathcal{L}$ with true outcome observed, and a
large sized unlabeled data $\mathcal{U}$ with outcome surrogates $\pmb W$. In
particular we propose a theoretically justified SSL approach to Q-learning and
develop a robust and efficient SSL approach to estimating the value function of
the derived optimal STR, defined as the expected counterfactual outcome under
the optimal STR. Generalizing SSL to learning STR brings interesting
challenges. First, the feature distribution for predicting $Y_t$ is unknown in
the $Q$-learning procedure, as it includes unknown $Y_{t-1}$ due to the
sequential nature. Our methods for estimating optimal STR and its associated
value function, carefully adapts to this sequentially missing data structure.
Second, we modify the SSL framework to handle the use of surrogate variables
$\pmb W$ which are predictive of the outcome through the joint law
$\mathbb{P}_{Y,\pmb O,\pmb W}$, but are not part of the conditional
distribution of interest $\mathbb{P}_{Y|\pmb O}$. We provide theoretical
results to understand when and to what degree efficiency can be gained from
$\pmb W$ and $\pmb O$. Our approach is robust to misspecification of the
imputation models. Further, we provide a doubly robust value function estimator
for the derived STR. If either the Q functions or the propensity score
functions are correctly specified, our value function estimators are consistent
for the true value function.
</p>
<a href="http://arxiv.org/abs/2012.04809" target="_blank">arXiv:2012.04809</a> [<a href="http://arxiv.org/pdf/2012.04809" target="_blank">pdf</a>]

<h2>Two-phase Pseudo Label Densification for Self-training based Domain Adaptation. (arXiv:2012.04828v1 [cs.CV])</h2>
<h3>Inkyu Shin, Sanghyun Woo, Fei Pan, InSo Kweon</h3>
<p>Recently, deep self-training approaches emerged as a powerful solution to the
unsupervised domain adaptation. The self-training scheme involves iterative
processing of target data; it generates target pseudo labels and retrains the
network. However, since only the confident predictions are taken as pseudo
labels, existing self-training approaches inevitably produce sparse pseudo
labels in practice. We see this is critical because the resulting insufficient
training-signals lead to a suboptimal, error-prone model. In order to tackle
this problem, we propose a novel Two-phase Pseudo Label Densification
framework, referred to as TPLD. In the first phase, we use sliding window
voting to propagate the confident predictions, utilizing intrinsic
spatial-correlations in the images. In the second phase, we perform a
confidence-based easy-hard classification. For the easy samples, we now employ
their full pseudo labels. For the hard ones, we instead adopt adversarial
learning to enforce hard-to-easy feature alignment. To ease the training
process and avoid noisy predictions, we introduce the bootstrapping mechanism
to the original self-training loss. We show the proposed TPLD can be easily
integrated into existing self-training based approaches and improves the
performance significantly. Combined with the recently proposed CRST
self-training framework, we achieve new state-of-the-art results on two
standard UDA benchmarks.
</p>
<a href="http://arxiv.org/abs/2012.04828" target="_blank">arXiv:2012.04828</a> [<a href="http://arxiv.org/pdf/2012.04828" target="_blank">pdf</a>]

<h2>Semi-supervised Active Learning for Instance Segmentation via Scoring Predictions. (arXiv:2012.04829v1 [cs.CV])</h2>
<h3>Jun Wang, Shaoguo Wen, Kaixing Chen, Jianghua Yu, Xin Zhou, Peng Gao, Changsheng Li, Guotong Xie</h3>
<p>Active learning generally involves querying the most representative samples
for human labeling, which has been widely studied in many fields such as image
classification and object detection. However, its potential has not been
explored in the more complex instance segmentation task that usually has
relatively higher annotation cost. In this paper, we propose a novel and
principled semi-supervised active learning framework for instance segmentation.
Specifically, we present an uncertainty sampling strategy named Triplet Scoring
Predictions (TSP) to explicitly incorporate samples ranking clues from classes,
bounding boxes and masks. Moreover, we devise a progressive pseudo labeling
regime using the above TSP in semi-supervised manner, it can leverage both the
labeled and unlabeled data to minimize labeling effort while maximize
performance of instance segmentation. Results on medical images datasets
demonstrate that the proposed method results in the embodiment of knowledge
from available data in a meaningful way. The extensive quantitatively and
qualitatively experiments show that, our method can yield the best-performing
model with notable less annotation costs, compared with state-of-the-arts.
</p>
<a href="http://arxiv.org/abs/2012.04829" target="_blank">arXiv:2012.04829</a> [<a href="http://arxiv.org/pdf/2012.04829" target="_blank">pdf</a>]

<h2>Proactive Interaction Framework for Intelligent Social Receptionist Robots. (arXiv:2012.04832v1 [cs.RO])</h2>
<h3>Yang Xue, Fan Wang, Hao Tian, Min Zhao, Jiangyong Li, Haiqing Pan, Yueqiang Dong</h3>
<p>Proactive human-robot interaction (HRI) allows the receptionist robots to
actively greet people and offer services based on vision, which has been found
to improve acceptability and customer satisfaction. Existing approaches are
either based on multi-stage decision processes or based on end-to-end decision
models. However, the rule-based approaches require sedulous expert efforts and
only handle minimal pre-defined scenarios. On the other hand, existing works
with end-to-end models are limited to very general greetings or few behavior
patterns (typically less than 10). To address those challenges, we propose a
new end-to-end framework, the TransFormer with Visual Tokens for Human-Robot
Interaction (TFVT-HRI). The proposed framework extracts visual tokens of
relative objects from an RGB camera first. To ensure the correct interpretation
of the scenario, a transformer decision model is then employed to process the
visual tokens, which is augmented with the temporal and spatial information. It
predicts the appropriate action to take in each scenario and identifies the
right target. Our data is collected from an in-service receptionist robot in an
office building, which is then annotated by experts for appropriate proactive
behavior. The action set includes 1000+ diverse patterns by combining language,
emoji expression, and body motions. We compare our model with other SOTA
end-to-end models on both offline test sets and online user experiments in
realistic office building environments to validate this framework. It is
demonstrated that the decision model achieves SOTA performance in action
triggering and selection, resulting in more humanness and intelligence when
compared with the previous reactive reception policies.
</p>
<a href="http://arxiv.org/abs/2012.04832" target="_blank">arXiv:2012.04832</a> [<a href="http://arxiv.org/pdf/2012.04832" target="_blank">pdf</a>]

<h2>A Topological Filter for Learning with Label Noise. (arXiv:2012.04835v1 [cs.CV])</h2>
<h3>Pengxiang Wu, Songzhu Zheng, Mayank Goswami, Dimitris Metaxas, Chao Chen</h3>
<p>Noisy labels can impair the performance of deep neural networks. To tackle
this problem, in this paper, we propose a new method for filtering label noise.
Unlike most existing methods relying on the posterior probability of a noisy
classifier, we focus on the much richer spatial behavior of data in the latent
representational space. By leveraging the high-order topological information of
data, we are able to collect most of the clean data and train a high-quality
model. Theoretically we prove that this topological approach is guaranteed to
collect the clean data with high probability. Empirical results show that our
method outperforms the state-of-the-arts and is robust to a broad spectrum of
noise types and levels.
</p>
<a href="http://arxiv.org/abs/2012.04835" target="_blank">arXiv:2012.04835</a> [<a href="http://arxiv.org/pdf/2012.04835" target="_blank">pdf</a>]

<h2>Deep Unsupervised Image Anomaly Detection: An Information Theoretic Framework. (arXiv:2012.04837v1 [cs.CV])</h2>
<h3>Fei Ye, Huangjie Zheng, Chaoqin Huang, Ya Zhang</h3>
<p>Surrogate task based methods have recently shown great promise for
unsupervised image anomaly detection. However, there is no guarantee that the
surrogate tasks share the consistent optimization direction with anomaly
detection. In this paper, we return to a direct objective function for anomaly
detection with information theory, which maximizes the distance between normal
and anomalous data in terms of the joint distribution of images and their
representation. Unfortunately, this objective function is not directly
optimizable under the unsupervised setting where no anomalous data is provided
during training. Through mathematical analysis of the above objective function,
we manage to decompose it into four components. In order to optimize in an
unsupervised fashion, we show that, under the assumption that distribution of
the normal and anomalous data are separable in the latent space, its lower
bound can be considered as a function which weights the trade-off between
mutual information and entropy. This objective function is able to explain why
the surrogate task based methods are effective for anomaly detection and
further point out the potential direction of improvement. Based on this object
function we introduce a novel information theoretic framework for unsupervised
image anomaly detection. Extensive experiments have demonstrated that the
proposed framework significantly outperforms several state-of-the-arts on
multiple benchmark data sets.
</p>
<a href="http://arxiv.org/abs/2012.04837" target="_blank">arXiv:2012.04837</a> [<a href="http://arxiv.org/pdf/2012.04837" target="_blank">pdf</a>]

<h2>Robust Domain Randomised Reinforcement Learning through Peer-to-Peer Distillation. (arXiv:2012.04839v1 [cs.LG])</h2>
<h3>Chenyang Zhao, Timothy Hospedales</h3>
<p>In reinforcement learning, domain randomisation is an increasingly popular
technique for learning more general policies that are robust to domain-shifts
at deployment. However, naively aggregating information from randomised domains
may lead to high variance in gradient estimation and unstable learning process.
To address this issue, we present a peer-to-peer online distillation strategy
for RL termed P2PDRL, where multiple workers are each assigned to a different
environment, and exchange knowledge through mutual regularisation based on
Kullback-Leibler divergence. Our experiments on continuous control tasks show
that P2PDRL enables robust learning across a wider randomisation distribution
than baselines, and more robust generalisation to new environments at testing.
</p>
<a href="http://arxiv.org/abs/2012.04839" target="_blank">arXiv:2012.04839</a> [<a href="http://arxiv.org/pdf/2012.04839" target="_blank">pdf</a>]

<h2>One-Vote Veto: A Self-Training Strategy for Low-Shot Learning of a Task-Invariant Embedding to Diagnose Glaucoma. (arXiv:2012.04841v1 [cs.CV])</h2>
<h3>Rui Fan, Christopher Bowd, Nicole Brye, Mark Christopher, Robert N. Weinreb, David Kriegman, Linda Zangwill</h3>
<p>Convolutional neural networks (CNNs) are a promising technique for automated
glaucoma diagnosis from images of the fundus, and these images are routinely
acquired as part of an ophthalmic exam. Nevertheless, CNNs typically require a
large amount of well-labeled data for training, which may not be available in
many biomedical image classification applications, especially when diseases are
rare and where labeling by experts is costly.

This paper makes two contributions to address this issue: (1) It introduces a
new network architecture and training method for low-shot learning when labeled
data are limited and imbalanced, and (2) it introduces a new semi-supervised
learning strategy that uses additional unlabeled training data to achieve great
accuracy. Our multi-task twin neural network (MTTNN) can use any backbone CNN,
and we demonstrate with ResNet-50 and MobileNet-v2 that its accuracy with
limited training data approaches the accuracy of a finetuned backbone trained
with a dataset that is 50 times larger. We also introduce One-Vote Veto (OVV)
self-training, a semi-supervised learning strategy, that is designed
specifically for MTTNNs. By taking both self-predictions and
contrastive-predictions of the unlabeled training data into account, OVV
self-training provides additional pseudo labels for finetuning a pretrained
MTTNN. Using a large dataset with more than 50,000 fundus images acquired over
25 years, extensive experimental results demonstrate the effectiveness of
low-shot learning with MTTNN and semi-supervised learning with OVV. Three
additional, smaller clinical datasets of fundus images acquired under different
conditions (cameras, instruments, locations, populations), are used to
demonstrate generalizability of the methods. Source code and pretrained models
will be publicly available upon publication.
</p>
<a href="http://arxiv.org/abs/2012.04841" target="_blank">arXiv:2012.04841</a> [<a href="http://arxiv.org/pdf/2012.04841" target="_blank">pdf</a>]

<h2>Improving the Fairness of Deep Generative Models without Retraining. (arXiv:2012.04842v1 [cs.CV])</h2>
<h3>Shuhan Tan, Yujun Shen, Bolei Zhou</h3>
<p>Generative Adversarial Networks (GANs) have recently advanced face synthesis
by learning the underlying distribution of observed data. However, it will lead
to a biased image generation due to the imbalanced training data or the mode
collapse issue. Prior work typically addresses the fairness of data generation
by balancing the training data that correspond to the concerned attributes. In
this work, we propose a simple yet effective method to improve the fairness of
image generation for a pre-trained GAN model without retraining. We utilize the
recent work of GAN interpretation to identify the directions in the latent
space corresponding to the target attributes, and then manipulate a set of
latent codes with balanced attribute distributions over output images. We learn
a Gaussian Mixture Model (GMM) to fit a distribution of the latent code set,
which supports the sampling of latent codes for producing images with a more
fair attribute distribution. Experiments show that our method can substantially
improve the fairness of image generation, outperforming potential baselines
both quantitatively and qualitatively. The images generated from our method are
further applied to reveal and quantify the biases in commercial face
classifiers and face super-resolution model.
</p>
<a href="http://arxiv.org/abs/2012.04842" target="_blank">arXiv:2012.04842</a> [<a href="http://arxiv.org/pdf/2012.04842" target="_blank">pdf</a>]

<h2>Toward an Affective Touch Robot: Subjective and Physiological Evaluation of Gentle Stroke Motion Using a Human-Imitation Hand. (arXiv:2012.04844v1 [cs.RO])</h2>
<h3>Tomoki Ishikura, Akishige Yuguchi, Yuki Kitamura, Sung-Gwi Cho, Ming Ding, Jun Takamatsu, Wataru Sato, Sakiko Yoshikawa, Tsukasa Ogasawara</h3>
<p>Affective touch offers positive psychological and physiological benefits such
as the mitigation of stress and pain. If a robot could realize human-like
affective touch, it would open up new application areas, including supporting
care work. In this research, we focused on the gentle stroking motion of a
robot to evoke the same emotions that human touch would evoke: in other words,
an affective touch robot. We propose a robot that is able to gently stroke the
back of a human using our designed human-imitation hand. To evaluate the
emotional effects of this affective touch, we compared the results of a
combination of two agents (the human-imitation hand and the human hand), at two
stroke speeds (3 and 30 cm/s). The results of the subjective and physiological
evaluations highlighted the following three findings: 1) the subjects evaluated
strokes similarly with regard to the stroke speed of the human and
human-imitation hand, in both the subjective and physiological evaluations; 2)
the subjects felt greater pleasure and arousal at the faster stroke rate (30
cm/s rather than 3 cm/s); and 3) poorer fitting of the human-imitation hand due
to the bending of the back had a negative emotional effect on the subjects.
</p>
<a href="http://arxiv.org/abs/2012.04844" target="_blank">arXiv:2012.04844</a> [<a href="http://arxiv.org/pdf/2012.04844" target="_blank">pdf</a>]

<h2>SnapMix: Semantically Proportional Mixing for Augmenting Fine-grained Data. (arXiv:2012.04846v1 [cs.CV])</h2>
<h3>Shaoli Huang, Xinchao Wang, Dacheng Tao</h3>
<p>Data mixing augmentation has proved effective in training deep models. Recent
methods mix labels mainly based on the mixture proportion of image pixels. As
the main discriminative information of a fine-grained image usually resides in
subtle regions, methods along this line are prone to heavy label noise in
fine-grained recognition. We propose in this paper a novel scheme, termed as
Semantically Proportional Mixing (SnapMix), which exploits class activation map
(CAM) to lessen the label noise in augmenting fine-grained data. SnapMix
generates the target label for a mixed image by estimating its intrinsic
semantic composition, and allows for asymmetric mixing operations and ensures
semantic correspondence between synthetic images and target labels. Experiments
show that our method consistently outperforms existing mixed-based approaches
on various datasets and under different network depths. Furthermore, by
incorporating the mid-level features, the proposed SnapMix achieves top-level
performance, demonstrating its potential to serve as a solid baseline for
fine-grained recognition. Our code is available at
https://github.com/Shaoli-Huang/SnapMix.git.
</p>
<a href="http://arxiv.org/abs/2012.04846" target="_blank">arXiv:2012.04846</a> [<a href="http://arxiv.org/pdf/2012.04846" target="_blank">pdf</a>]

<h2>Reconstruction of Backbone Curves for Snake Robots. (arXiv:2012.04855v1 [cs.RO])</h2>
<h3>Tianyu Wang, Bo Lin, Baxi Chong, Julian Whitman, Matthew Travers, Daniel I. Goldman, Greg Blekherman, Howie Choset</h3>
<p>Snake robots composed of alternating single-axis pitch and yaw joints have
many internal degrees of freedom, which make them capable of versatile
three-dimensional locomotion. Often, snake robot motions are planned
kinematically by a chronological sequence of continuous backbone curves that
capture desired macroscopic shapes of the robot. However, as the geometric
arrangement of single-axis rotary joints creates constraints on the rotations
in the robot, it is challenging for the robot to reconstruct an arbitrary 3-D
curve. When the robot configuration does not accurately achieve the desired
shapes defined by these backbone curves, the robot can have undesired contact
with the environment, such that the robot does not achieve the desired motion.
In this work, we propose a method for snake robots to reconstruct desired
backbone curves by posing an optimization problem that exploits the robot's
geometric structure. We verified that our method enables accurate
curve-configuration conversions through its applications to commonly used 3-D
gaits. We also demonstrated via robot experiments that 1) our method results in
smooth locomotion on the robot; 2) our method allows the robot to approach the
numerically predicted locomotive performance of a sequence of continuous
backbone curve.
</p>
<a href="http://arxiv.org/abs/2012.04855" target="_blank">arXiv:2012.04855</a> [<a href="http://arxiv.org/pdf/2012.04855" target="_blank">pdf</a>]

<h2>Accurate and Fast Federated Learning via IID and Communication-Aware Grouping. (arXiv:2012.04857v1 [cs.LG])</h2>
<h3>Jin-woo Lee, Jaehoon Oh, Yooju Shin, Jae-Gil Lee, Se-Young Yoon</h3>
<p>Federated learning has emerged as a new paradigm of collaborative machine
learning; however, it has also faced several challenges such as non-independent
and identically distributed(IID) data and high communication cost. To this end,
we propose a novel framework of IID and communication-aware group federated
learning that simultaneously maximizes both accuracy and communication speed by
grouping nodes based on data distributions and physical locations of the nodes.
Furthermore, we provide a formal convergence analysis and an efficient
optimization algorithm called FedAvg-IC. Experimental results show that,
compared with the state-of-the-art algorithms, FedAvg-IC improved the test
accuracy by up to 22.2% and simultaneously reduced the communication time to as
small as 12%.
</p>
<a href="http://arxiv.org/abs/2012.04857" target="_blank">arXiv:2012.04857</a> [<a href="http://arxiv.org/pdf/2012.04857" target="_blank">pdf</a>]

<h2>Model-agnostic Fits for Understanding Information Seeking Patterns in Humans. (arXiv:2012.04858v1 [cs.AI])</h2>
<h3>Soumya Chatterjee, Pradeep Shenoy</h3>
<p>In decision making tasks under uncertainty, humans display characteristic
biases in seeking, integrating, and acting upon information relevant to the
task. Here, we reexamine data from previous carefully designed experiments,
collected at scale, that measured and catalogued these biases in aggregate
form. We design deep learning models that replicate these biases in aggregate,
while also capturing individual variation in behavior. A key finding of our
work is that paucity of data collected from each individual subject can be
overcome by sampling large numbers of subjects from the population, while still
capturing individual differences. In addition, we can predict human behavior
with high accuracy without making any assumptions about task goals, reward
structure, or individual biases, thus providing a model-agnostic fit to human
behavior in the task. Such an approach can sidestep potential limitations in
modeler-specified inductive biases, and has implications for computational
modeling of human cognitive function in general, and of human-AI interfaces in
particular.
</p>
<a href="http://arxiv.org/abs/2012.04858" target="_blank">arXiv:2012.04858</a> [<a href="http://arxiv.org/pdf/2012.04858" target="_blank">pdf</a>]

<h2>Scalable Neural Tangent Kernel of Recurrent Architectures. (arXiv:2012.04859v1 [cs.LG])</h2>
<h3>Sina Alemohammad, Randall Balestriero, Zichao Wang, Richard Baraniuk</h3>
<p>Kernels derived from deep neural networks (DNNs) in the infinite-width
provide not only high performance in a range of machine learning tasks but also
new theoretical insights into DNN training dynamics and generalization. In this
paper, we extend the family of kernels associated with recurrent neural
networks (RNNs), which were previously derived only for simple RNNs, to more
complex architectures that are bidirectional RNNs and RNNs with average
pooling. We also develop a fast GPU implementation to exploit its full
practical potential. While RNNs are typically only applied to time-series data,
we demonstrate that classifiers using RNN-based kernels outperform a range of
baseline methods on 90 non-time-series datasets from the UCI data repository.
</p>
<a href="http://arxiv.org/abs/2012.04859" target="_blank">arXiv:2012.04859</a> [<a href="http://arxiv.org/pdf/2012.04859" target="_blank">pdf</a>]

<h2>Skillearn: Machine Learning Inspired by Humans' Learning Skills. (arXiv:2012.04863v1 [cs.LG])</h2>
<h3>Pengtao Xie, Xuefeng Du, Hao Ban</h3>
<p>Humans, as the most powerful learners on the planet, have accumulated a lot
of learning skills, such as learning through tests, interleaving learning,
self-explanation, active recalling, to name a few. These learning skills and
methodologies enable humans to learn new topics more effectively and
efficiently. We are interested in investigating whether humans' learning skills
can be borrowed to help machines to learn better. Specifically, we aim to
formalize these skills and leverage them to train better machine learning (ML)
models. To achieve this goal, we develop a general framework -- Skillearn,
which provides a principled way to represent humans' learning skills
mathematically and use the formally-represented skills to improve the training
of ML models. In two case studies, we apply Skillearn to formalize two learning
skills of humans: learning by passing tests and interleaving learning, and use
the formalized skills to improve neural architecture search. Experiments on
various datasets show that trained using the skills formalized by Skillearn, ML
models achieve significantly better performance.
</p>
<a href="http://arxiv.org/abs/2012.04863" target="_blank">arXiv:2012.04863</a> [<a href="http://arxiv.org/pdf/2012.04863" target="_blank">pdf</a>]

<h2>EvaLDA: Efficient Evasion Attacks Towards Latent Dirichlet Allocation. (arXiv:2012.04864v1 [cs.LG])</h2>
<h3>Qi Zhou, Haipeng Chen, Yitao Zheng, Zhen Wang</h3>
<p>As one of the most powerful topic models, Latent Dirichlet Allocation (LDA)
has been used in a vast range of tasks, including document understanding,
information retrieval and peer-reviewer assignment. Despite its tremendous
popularity, the security of LDA has rarely been studied. This poses severe
risks to security-critical tasks such as sentiment analysis and peer-reviewer
assignment that are based on LDA. In this paper, we are interested in knowing
whether LDA models are vulnerable to adversarial perturbations of benign
document examples during inference time. We formalize the evasion attack to LDA
models as an optimization problem and prove it to be NP-hard. We then propose a
novel and efficient algorithm, EvaLDA to solve it. We show the effectiveness of
EvaLDA via extensive empirical evaluations. For instance, in the NIPS dataset,
EvaLDA can averagely promote the rank of a target topic from 10 to around 7 by
only replacing 1% of the words with similar words in a victim document. Our
work provides significant insights into the power and limitations of evasion
attacks to LDA models.
</p>
<a href="http://arxiv.org/abs/2012.04864" target="_blank">arXiv:2012.04864</a> [<a href="http://arxiv.org/pdf/2012.04864" target="_blank">pdf</a>]

<h2>Deep Lesion Tracker: Monitoring Lesions in 4D Longitudinal Imaging Studies. (arXiv:2012.04872v1 [cs.CV])</h2>
<h3>Jinzheng Cai, Youbao Tang, Ke Yan, Adam P. Harrison, Jing Xiao, Gigin Lin, Le Lu</h3>
<p>Monitoring treatment response in longitudinal studies plays an important role
in clinical practice. Accurately identifying lesions across serial imaging
follow-up is the core to the monitoring procedure. Typically this incorporates
both image and anatomical considerations. However, matching lesions manually is
labor-intensive and time-consuming. In this work, we present deep lesion
tracker (DLT), a deep learning approach that uses both appearance- and
anatomical-based signals. To incorporate anatomical constraints, we propose an
anatomical signal encoder, which prevents lesions being matched with visually
similar but spurious regions. In addition, we present a new formulation for
Siamese networks that avoids the heavy computational loads of 3D
cross-correlation. To present our network with greater varieties of images, we
also propose a self-supervised learning (SSL) strategy to train trackers with
unpaired images, overcoming barriers to data collection. To train and evaluate
our tracker, we introduce and release the first lesion tracking benchmark,
consisting of 3891 lesion pairs from the public DeepLesion database. The
proposed method, DLT, locates lesion centers with a mean error distance of 7
mm. This is 5% better than a leading registration algorithm while running 14
times faster on whole CT volumes. We demonstrate even greater improvements over
detector or similarity-learning alternatives. DLT also generalizes well on an
external clinical test set of 100 longitudinal studies, achieving 88% accuracy.
Finally, we plug DLT into an automatic tumor monitoring workflow where it leads
to an accuracy of 85% in assessing lesion treatment responses, which is only
0.46% lower than the accuracy of manual inputs.
</p>
<a href="http://arxiv.org/abs/2012.04872" target="_blank">arXiv:2012.04872</a> [<a href="http://arxiv.org/pdf/2012.04872" target="_blank">pdf</a>]

<h2>LSTM recurrent neural network assisted aircraft stall prediction for enhanced situational awareness. (arXiv:2012.04876v1 [cs.LG])</h2>
<h3>Tahsin Sejat Saniat, Tahiat Goni, Shaikat M. Galib</h3>
<p>Since the dawn of mankind's introduction to powered flights, there have been
multiple incidents which can be attributed to aircraft stalls. Most modern-day
aircraft are equipped with advanced warning systems to warn the pilots about a
potential stall, so that pilots may adopt the necessary recovery measures. But
these warnings often have a short window before the aircraft actually enters a
stall and require the pilots to act promptly to prevent it. In this paper, we
propose a deep learning based approach to predict an Impending stall, well in
advance, even before the stall-warning is triggered. We leverage the
capabilities of long short-term memory (LSTM) recurrent neural networks (RNN)
and propose a novel approach to predict potential stalls from the sequential
in-flight sensor data. Three different neural network architectures were
explored. The neural network models, trained on 26400 seconds of simulator
flight data are able to predict a potential stall with &gt; 95% accuracy,
approximately 10 seconds in advance of the stall-warning trigger. This can
significantly augment the Pilot's preparedness to handle an unexpected stall
and will add an additional layer of safety to the traditional stall warning
systems.
</p>
<a href="http://arxiv.org/abs/2012.04876" target="_blank">arXiv:2012.04876</a> [<a href="http://arxiv.org/pdf/2012.04876" target="_blank">pdf</a>]

<h2>JANUS: Benchmarking Commercial and Open-Source Cloud and Edge Platforms for Object and Anomaly Detection Workloads. (arXiv:2012.04880v1 [cs.CV])</h2>
<h3>Karthick Shankar, Pengcheng Wang, Ran Xu, Ashraf Mahgoub, Somali Chaterji</h3>
<p>With diverse IoT workloads, placing compute and analytics close to where data
is collected is becoming increasingly important. We seek to understand what is
the performance and the cost implication of running analytics on IoT data at
the various available platforms. These workloads can be compute-light, such as
outlier detection on sensor data, or compute-intensive, such as object
detection from video feeds obtained from drones. In our paper, JANUS, we
profile the performance/$ and the compute versus communication cost for a
compute-light IoT workload and a compute-intensive IoT workload. In addition,
we also look at the pros and cons of some of the proprietary deep-learning
object detection packages, such as Amazon Rekognition, Google Vision, and Azure
Cognitive Services, to contrast with open-source and tunable solutions, such as
Faster R-CNN (FRCNN). We find that AWS IoT Greengrass delivers at least 2X
lower latency and 1.25X lower cost compared to all other cloud platforms for
the compute-light outlier detection workload. For the compute-intensive
streaming video analytics task, an opensource solution to object detection
running on cloud VMs saves on dollar costs compared to proprietary solutions
provided by Amazon, Microsoft, and Google, but loses out on latency (up to 6X).
If it runs on a low-powered edge device, the latency is up to 49X lower.
</p>
<a href="http://arxiv.org/abs/2012.04880" target="_blank">arXiv:2012.04880</a> [<a href="http://arxiv.org/pdf/2012.04880" target="_blank">pdf</a>]

<h2>DS-Net: Dynamic Spatiotemporal Network for Video Salient Object Detection. (arXiv:2012.04886v1 [cs.CV])</h2>
<h3>Yuting Su, Weikang Wang, Jing Liu, Peiguang Jing, Xiaokang Yang</h3>
<p>As moving objects always draw more attention of human eyes, the temporal
motive information is always exploited complementarily with spatial information
to detect salient objects in videos. Although efficient tools such as optical
flow have been proposed to extract temporal motive information, it often
encounters difficulties when used for saliency detection due to the movement of
camera or the partial movement of salient objects. In this paper, we
investigate the complimentary roles of spatial and temporal information and
propose a novel dynamic spatiotemporal network (DS-Net) for more effective
fusion of spatiotemporal information. We construct a symmetric two-bypass
network to explicitly extract spatial and temporal features. A dynamic weight
generator (DWG) is designed to automatically learn the reliability of
corresponding saliency branch. And a top-down cross attentive aggregation (CAA)
procedure is designed so as to facilitate dynamic complementary aggregation of
spatiotemporal features. Finally, the features are modified by spatial
attention with the guidance of coarse saliency map and then go through decoder
part for final saliency map. Experimental results on five benchmarks VOS,
DAVIS, FBMS, SegTrack-v2, and ViSal demonstrate that the proposed method
achieves superior performance than state-of-the-art algorithms. The source code
is available at https://github.com/TJUMMG/DS-Net.
</p>
<a href="http://arxiv.org/abs/2012.04886" target="_blank">arXiv:2012.04886</a> [<a href="http://arxiv.org/pdf/2012.04886" target="_blank">pdf</a>]

<h2>Generative Data Augmentation for Vehicle Detection in Aerial Images. (arXiv:2012.04902v1 [cs.CV])</h2>
<h3>Hilmi Kumdakc&#x131;, Cihan &#xd6;ng&#xfc;n, Alptekin Temizel</h3>
<p>Scarcity of training data is one of the prominent problems for deep networks
which require large amounts data. Data augmentation is a widely used method to
increase the number of training samples and their variations. In this paper, we
focus on improving vehicle detection performance in aerial images and propose a
generative augmentation method which does not need any extra supervision than
the bounding box annotations of the vehicle objects in the training dataset.
The proposed method increases the performance of vehicle detection by allowing
detectors to be trained with higher number of instances, especially when there
are limited number of training instances. The proposed method is generic in the
sense that it can be integrated with different generators. The experiments show
that the method increases the Average Precision by up to 25.2% and 25.7% when
integrated with Pluralistic and DeepFill respectively.
</p>
<a href="http://arxiv.org/abs/2012.04902" target="_blank">arXiv:2012.04902</a> [<a href="http://arxiv.org/pdf/2012.04902" target="_blank">pdf</a>]

<h2>ESAD: End-to-end Deep Semi-supervised Anomaly Detection. (arXiv:2012.04905v1 [cs.LG])</h2>
<h3>Chaoqin Huang, Fei Ye, Ya Zhang, Yan-Feng Wang, Qi Tian</h3>
<p>This paper explores semi-supervised anomaly detection, a more practical
setting for anomaly detection where a small set of labeled outlier samples are
provided in addition to a large amount of unlabeled data for training.
Rethinking the optimization target of anomaly detection, we propose a new
objective function that measures the KL-divergence between normal and anomalous
data, and prove that two factors: the mutual information between the data and
latent representations, and the entropy of latent representations, constitute
an integral objective function for anomaly detection. To resolve the
contradiction in simultaneously optimizing the two factors, we propose a novel
encoder-decoder-encoder structure, with the first encoder focusing on
optimizing the mutual information and the second encoder focusing on optimizing
the entropy. The two encoders are enforced to share similar encoding with a
consistent constraint on their latent representations. Extensive experiments
have revealed that the proposed method significantly outperforms several
state-of-the-arts on multiple benchmark datasets, including medical diagnosis
and several classic anomaly detection benchmarks.
</p>
<a href="http://arxiv.org/abs/2012.04905" target="_blank">arXiv:2012.04905</a> [<a href="http://arxiv.org/pdf/2012.04905" target="_blank">pdf</a>]

<h2>Progressive Network Grafting for Few-Shot Knowledge Distillation. (arXiv:2012.04915v1 [cs.CV])</h2>
<h3>Chengchao Shen, Xinchao Wang, Youtan Yin, Jie Song, Sihui Luo, Mingli Song</h3>
<p>Knowledge distillation has demonstrated encouraging performances in deep
model compression. Most existing approaches, however, require massive labeled
data to accomplish the knowledge transfer, making the model compression a
cumbersome and costly process. In this paper, we investigate the practical
few-shot knowledge distillation scenario, where we assume only a few samples
without human annotations are available for each category. To this end, we
introduce a principled dual-stage distillation scheme tailored for few-shot
data. In the first step, we graft the student blocks one by one onto the
teacher, and learn the parameters of the grafted block intertwined with those
of the other teacher blocks. In the second step, the trained student blocks are
progressively connected and then together grafted onto the teacher network,
allowing the learned student blocks to adapt themselves to each other and
eventually replace the teacher network. Experiments demonstrate that our
approach, with only a few unlabeled samples, achieves gratifying results on
CIFAR10, CIFAR100, and ILSVRC-2012. On CIFAR10 and CIFAR100, our performances
are even on par with those of knowledge distillation schemes that utilize the
full datasets. The source code is available at
https://github.com/zju-vipa/NetGraft.
</p>
<a href="http://arxiv.org/abs/2012.04915" target="_blank">arXiv:2012.04915</a> [<a href="http://arxiv.org/pdf/2012.04915" target="_blank">pdf</a>]

<h2>Kernel Anomalous Change Detection for Remote Sensing Imagery. (arXiv:2012.04920v1 [cs.CV])</h2>
<h3>Jos&#xe9; A. Padr&#xf3;n-Hidalgo, Valero Laparra, Nathan Longbotham, Gustau Camps-Valls</h3>
<p>Anomalous change detection (ACD) is an important problem in remote sensing
image processing. Detecting not only pervasive but also anomalous or extreme
changes has many applications for which methodologies are available. This paper
introduces a nonlinear extension of a full family of anomalous change
detectors. In particular, we focus on algorithms that utilize Gaussian and
elliptically contoured (EC) distribution and extend them to their nonlinear
counterparts based on the theory of reproducing kernels' Hilbert space. We
illustrate the performance of the kernel methods introduced in both pervasive
and ACD problems with real and simulated changes in multispectral and
hyperspectral imagery with different resolutions (AVIRIS, Sentinel-2,
WorldView-2, and Quickbird). A wide range of situations is studied in real
examples, including droughts, wildfires, and urbanization. Excellent
performance in terms of detection accuracy compared to linear formulations is
achieved, resulting in improved detection accuracy and reduced false-alarm
rates. Results also reveal that the EC assumption may be still valid in Hilbert
spaces. We provide an implementation of the algorithms as well as a database of
natural anomalous changes in real scenarios this http URL
</p>
<a href="http://arxiv.org/abs/2012.04920" target="_blank">arXiv:2012.04920</a> [<a href="http://arxiv.org/pdf/2012.04920" target="_blank">pdf</a>]

<h2>Towards Annotation-Free Evaluation of Cross-Lingual Image Captioning. (arXiv:2012.04925v1 [cs.CV])</h2>
<h3>Aozhu Chen, Xinyi Huang, Hailan Lin, Xirong Li</h3>
<p>Cross-lingual image captioning, with its ability to caption an unlabeled
image in a target language other than English, is an emerging topic in the
multimedia field. In order to save the precious human resource from re-writing
reference sentences per target language, in this paper we make a brave attempt
towards annotation-free evaluation of cross-lingual image captioning. Depending
on whether we assume the availability of English references, two scenarios are
investigated. For the first scenario with the references available, we propose
two metrics, i.e., WMDRel and CLinRel. WMDRel measures the semantic relevance
between a model-generated caption and machine translation of an English
reference using their Word Mover's Distance. By projecting both captions into a
deep visual feature space, CLinRel is a visual-oriented cross-lingual relevance
measure. As for the second scenario, which has zero reference and is thus more
challenging, we propose CMedRel to compute a cross-media relevance between the
generated caption and the image content, in the same visual feature space as
used by CLinRel. The promising results show high potential of the new metrics
for evaluation with no need of references in the target language.
</p>
<a href="http://arxiv.org/abs/2012.04925" target="_blank">arXiv:2012.04925</a> [<a href="http://arxiv.org/pdf/2012.04925" target="_blank">pdf</a>]

<h2>Improving Gradient Flow with Unrolled Highway Expectation Maximization. (arXiv:2012.04926v1 [cs.LG])</h2>
<h3>Chonghyuk Song, Eunseok Kim, Inwook Shim</h3>
<p>Integrating model-based machine learning methods into deep neural
architectures allows one to leverage both the expressive power of deep neural
nets and the ability of model-based methods to incorporate domain-specific
knowledge. In particular, many works have employed the expectation maximization
(EM) algorithm in the form of an unrolled layer-wise structure that is jointly
trained with a backbone neural network. However, it is difficult to
discriminatively train the backbone network by backpropagating through the EM
iterations as they are prone to the vanishing gradient problem. To address this
issue, we propose Highway Expectation Maximization Networks (HEMNet), which is
comprised of unrolled iterations of the generalized EM (GEM) algorithm based on
the Newton-Rahpson method. HEMNet features scaled skip connections, or
highways, along the depths of the unrolled architecture, resulting in improved
gradient flow during backpropagation while incurring negligible additional
computation and memory costs compared to standard unrolled EM. Furthermore,
HEMNet preserves the underlying EM procedure, thereby fully retaining the
convergence properties of the original EM algorithm. We achieve significant
improvement in performance on several semantic segmentation benchmarks and
empirically show that HEMNet effectively alleviates gradient decay.
</p>
<a href="http://arxiv.org/abs/2012.04926" target="_blank">arXiv:2012.04926</a> [<a href="http://arxiv.org/pdf/2012.04926" target="_blank">pdf</a>]

<h2>Robust Facial Landmark Detection by Multi-order Multi-constraint Deep Networks. (arXiv:2012.04927v1 [cs.CV])</h2>
<h3>Jun Wan, Zhihui Lai, Jing Li, Jie Zhou, Can Gao</h3>
<p>Recently, heatmap regression has been widely explored in facial landmark
detection and obtained remarkable performance. However, most of the existing
heatmap regression-based facial landmark detection methods neglect to explore
the high-order feature correlations, which is very important to learn more
representative features and enhance shape constraints. Moreover, no explicit
global shape constraints have been added to the final predicted landmarks,
which leads to a reduction in accuracy. To address these issues, in this paper,
we propose a Multi-order Multi-constraint Deep Network (MMDN) for more powerful
feature correlations and shape constraints learning. Specifically, an Implicit
Multi-order Correlating Geometry-aware (IMCG) model is proposed to introduce
the multi-order spatial correlations and multi-order channel correlations for
more discriminative representations. Furthermore, an Explicit Probability-based
Boundary-adaptive Regression (EPBR) method is developed to enhance the global
shape constraints and further search the semantically consistent landmarks in
the predicted boundary for robust facial landmark detection. It's interesting
to show that the proposed MMDN can generate more accurate boundary-adaptive
landmark heatmaps and effectively enhance shape constraints to the predicted
landmarks for faces with large pose variations and heavy occlusions.
Experimental results on challenging benchmark datasets demonstrate the
superiority of our MMDN over state-of-the-art facial landmark detection
methods.
</p>
<a href="http://arxiv.org/abs/2012.04927" target="_blank">arXiv:2012.04927</a> [<a href="http://arxiv.org/pdf/2012.04927" target="_blank">pdf</a>]

<h2>Distributed Training of Graph Convolutional Networks using Subgraph Approximation. (arXiv:2012.04930v1 [cs.LG])</h2>
<h3>Alexandra Angerd, Keshav Balasubramanian, Murali Annavaram</h3>
<p>Modern machine learning techniques are successfully being adapted to data
modeled as graphs. However, many real-world graphs are typically very large and
do not fit in memory, often making the problem of training machine learning
models on them intractable. Distributed training has been successfully employed
to alleviate memory problems and speed up training in machine learning domains
in which the input data is assumed to be independently identical distributed
(i.i.d). However, distributing the training of non i.i.d data such as graphs
that are used as training inputs in Graph Convolutional Networks (GCNs) causes
accuracy problems since information is lost at the graph partitioning
boundaries.

In this paper, we propose a training strategy that mitigates the lost
information across multiple partitions of a graph through a subgraph
approximation scheme. Our proposed approach augments each sub-graph with a
small amount of edge and vertex information that is approximated from all other
sub-graphs. The subgraph approximation approach helps the distributed training
system converge at single-machine accuracy, while keeping the memory footprint
low and minimizing synchronization overhead between the machines.
</p>
<a href="http://arxiv.org/abs/2012.04930" target="_blank">arXiv:2012.04930</a> [<a href="http://arxiv.org/pdf/2012.04930" target="_blank">pdf</a>]

<h2>Lipschitz Regularized CycleGAN for Improving Semantic Robustness in Unpaired Image-to-image Translation. (arXiv:2012.04932v1 [cs.CV])</h2>
<h3>Zhiwei Jia, Bodi Yuan, Kangkang Wang, Hong Wu, David Clifford, Zhiqiang Yuan, Hao Su</h3>
<p>For unpaired image-to-image translation tasks, GAN-based approaches are
susceptible to semantic flipping, i.e., contents are not preserved
consistently. We argue that this is due to (1) the difference in semantic
statistics between source and target domains and (2) the learned generators
being non-robust. In this paper, we proposed a novel approach, Lipschitz
regularized CycleGAN, for improving semantic robustness and thus alleviating
the semantic flipping issue. During training, we add a gradient penalty loss to
the generators, which encourages semantically consistent transformations. We
evaluate our approach on multiple common datasets and compare with several
existing GAN-based methods. Both quantitative and visual results suggest the
effectiveness and advantage of our approach in producing robust transformations
with fewer semantic flipping.
</p>
<a href="http://arxiv.org/abs/2012.04932" target="_blank">arXiv:2012.04932</a> [<a href="http://arxiv.org/pdf/2012.04932" target="_blank">pdf</a>]

<h2>AMVNet: Assertion-based Multi-View Fusion Network for LiDAR Semantic Segmentation. (arXiv:2012.04934v1 [cs.CV])</h2>
<h3>Venice Erin Liong, Thi Ngoc Tho Nguyen, Sergi Widjaja, Dhananjai Sharma, Zhuang Jie Chong</h3>
<p>In this paper, we present an Assertion-based Multi-View Fusion network
(AMVNet) for LiDAR semantic segmentation which aggregates the semantic features
of individual projection-based networks using late fusion. Given class scores
from different projection-based networks, we perform assertion-guided point
sampling on score disagreements and pass a set of point-level features for each
sampled point to a simple point head which refines the predictions. This
modular-and-hierarchical late fusion approach provides the flexibility of
having two independent networks with a minor overhead from a light-weight
network. Such approaches are desirable for robotic systems, e.g. autonomous
vehicles, for which the computational and memory resources are often limited.
Extensive experiments show that AMVNet achieves state-of-the-art results in
both the SemanticKITTI and nuScenes benchmark datasets and that our approach
outperforms the baseline method of combining the class scores of the
projection-based networks.
</p>
<a href="http://arxiv.org/abs/2012.04934" target="_blank">arXiv:2012.04934</a> [<a href="http://arxiv.org/pdf/2012.04934" target="_blank">pdf</a>]

<h2>Anomaly Detection in Time Series with Triadic Motif Fields and Application in Atrial Fibrillation ECG Classification. (arXiv:2012.04936v1 [cs.LG])</h2>
<h3>Yadong Zhang, Xin Chen</h3>
<p>In the time-series analysis, the time series motifs and the order patterns in
time series can reveal general temporal patterns and dynamic features. Triadic
Motif Field (TMF) is a simple and effective time-series image encoding method
based on triadic time series motifs. Electrocardiography (ECG) signals are
time-series data widely used to diagnose various cardiac anomalies. The TMF
images contain the features characterizing the normal and Atrial Fibrillation
(AF) ECG signals. Considering the quasi-periodic characteristics of ECG
signals, the dynamic features can be extracted from the TMF images with the
transfer learning pre-trained convolutional neural network (CNN) models. With
the extracted features, the simple classifiers, such as the Multi-Layer
Perceptron (MLP), the logistic regression, and the random forest, can be
applied for accurate anomaly detection. With the test dataset of the PhysioNet
Challenge 2017 database, the TMF classification model with the VGG16 transfer
learning model and MLP classifier demonstrates the best performance with the
95.50% ROC-AUC and 88.43% F1 score in the AF classification. Besides, the TMF
classification model can identify AF patients in the test dataset with high
precision. The feature vectors extracted from the TMF images show clear
patient-wise clustering with the t-distributed Stochastic Neighbor Embedding
technique. Above all, the TMF classification model has very good clinical
interpretability. The patterns revealed by symmetrized Gradient-weighted Class
Activation Mapping have a clear clinical interpretation at the beat and rhythm
levels.
</p>
<a href="http://arxiv.org/abs/2012.04936" target="_blank">arXiv:2012.04936</a> [<a href="http://arxiv.org/pdf/2012.04936" target="_blank">pdf</a>]

<h2>Removing Class Imbalance using Polarity-GAN: An Uncertainty Sampling Approach. (arXiv:2012.04937v1 [cs.CV])</h2>
<h3>Kumari Deepshikha, Anugunj Naman</h3>
<p>Class imbalance is a challenging issue in practical classification problems
for deep learning models as well as for traditional models. Traditionally
successful countermeasures such as synthetic over-sampling have had limited
success with complex, structured data handled by deep learning models. In this
work, we propose to use a Generative Adversarial Network (GAN) equipped with a
generator network G, a discriminator network D and a classifier network C to
remove the class-imbalance in visual data sets. The generator network is
initialized with auto-encoder to make it stable. The discriminator D ensures
that G adheres to class distribution of imbalanced class. In conventional
methods, where Generator G competes with discriminator D in a min-max game, we
propose to further add an additional classifier network to the original
network. Now, the generator network tries to compete in a min-max game with
Discriminator as well as the new classifier that we have introduced. An
additional condition is enforced on generator network G to produce points in
the convex hull of desired imbalanced class. Further the contention of
adversarial game with classifier C, pushes conditional distribution learned by
G towards the periphery of the respective class, compensating the problem of
class imbalance. Experimental evidence shows that this initialization results
in stable training of the network. We achieve state of the art performance on
extreme visual classification task on the FashionMNIST, MNIST, SVHN, ExDark,
MVTec Anomaly Detection dataset, Chest X-Ray dataset and others.
</p>
<a href="http://arxiv.org/abs/2012.04937" target="_blank">arXiv:2012.04937</a> [<a href="http://arxiv.org/pdf/2012.04937" target="_blank">pdf</a>]

<h2>Disentangling Derivatives, Uncertainty and Error in Gaussian Process Models. (arXiv:2012.04947v1 [stat.ML])</h2>
<h3>Juan Emmanuel Johnson, Valero Laparra, Gustau Camps-Valls</h3>
<p>Gaussian Processes (GPs) are a class of kernel methods that have shown to be
very useful in geoscience applications. They are widely used because they are
simple, flexible and provide very accurate estimates for nonlinear problems,
especially in parameter retrieval. An addition to a predictive mean function,
GPs come equipped with a useful property: the predictive variance function
which provides confidence intervals for the predictions. The GP formulation
usually assumes that there is no input noise in the training and testing
points, only in the observations. However, this is often not the case in Earth
observation problems where an accurate assessment of the instrument error is
usually available. In this paper, we showcase how the derivative of a GP model
can be used to provide an analytical error propagation formulation and we
analyze the predictive variance and the propagated error terms in a temperature
prediction problem from infrared sounding data.
</p>
<a href="http://arxiv.org/abs/2012.04947" target="_blank">arXiv:2012.04947</a> [<a href="http://arxiv.org/pdf/2012.04947" target="_blank">pdf</a>]

<h2>Conjugate Mixture Models for Clustering Multimodal Data. (arXiv:2012.04951v1 [stat.ML])</h2>
<h3>Vasil Khalidov, Florence Forbes, Radu Horaud</h3>
<p>The problem of multimodal clustering arises whenever the data are gathered
with several physically different sensors. Observations from different
modalities are not necessarily aligned in the sense there there is no obvious
way to associate or to compare them in some common space. A solution may
consist in considering multiple clustering tasks independently for each
modality. The main difficulty with such an approach is to guarantee that the
unimodal clusterings are mutually consistent. In this paper we show that
multimodal clustering can be addressed within a novel framework, namely
conjugate mixture models. These models exploit the explicit transformations
that are often available between an unobserved parameter space (objects) and
each one of the observation spaces (sensors). We formulate the problem as a
likelihood maximization task and we derive the associated conjugate
expectation-maximization algorithm. The convergence properties of the proposed
algorithm are thoroughly investigated. Several local/global optimization
techniques are proposed in order to increase its convergence speed. Two
initialization strategies are proposed and compared. A consistent
model-selection criterion is proposed. The algorithm and its variants are
tested and evaluated within the task of 3D localization of several speakers
using both auditory and visual data.
</p>
<a href="http://arxiv.org/abs/2012.04951" target="_blank">arXiv:2012.04951</a> [<a href="http://arxiv.org/pdf/2012.04951" target="_blank">pdf</a>]

<h2>Have convolutions already made recurrence obsolete for unconstrained handwritten text recognition ?. (arXiv:2012.04954v1 [cs.CV])</h2>
<h3>Denis Coquenet, Yann Soullard, Cl&#xe9;ment Chatelain, Thierry Paquet</h3>
<p>Unconstrained handwritten text recognition remains an important challenge for
deep neural networks. These last years, recurrent networks and more
specifically Long Short-Term Memory networks have achieved state-of-the-art
performance in this field. Nevertheless, they are made of a large number of
trainable parameters and training recurrent neural networks does not support
parallelism. This has a direct influence on the training time of such
architectures, with also a direct consequence on the time required to explore
various architectures. Recently, recurrence-free architectures such as Fully
Convolutional Networks with gated mechanisms have been proposed as one possible
alternative achieving competitive results. In this paper, we explore
convolutional architectures and compare them to a CNN+BLSTM baseline. We
propose an experimental study regarding different architectures on an offline
handwriting recognition task using the RIMES dataset, and a modified version of
it that consists of augmenting the images with notebook backgrounds that are
printed grids.
</p>
<a href="http://arxiv.org/abs/2012.04954" target="_blank">arXiv:2012.04954</a> [<a href="http://arxiv.org/pdf/2012.04954" target="_blank">pdf</a>]

<h2>Recurrence-free unconstrained handwritten text recognition using gated fully convolutional network. (arXiv:2012.04961v1 [cs.CV])</h2>
<h3>Denis Coquenet, Cl&#xe9;ment Chatelain, Thierry Paquet</h3>
<p>Unconstrained handwritten text recognition is a major step in most document
analysis tasks. This is generally processed by deep recurrent neural networks
and more specifically with the use of Long Short-Term Memory cells. The main
drawbacks of these components are the large number of parameters involved and
their sequential execution during training and prediction. One alternative
solution to using LSTM cells is to compensate the long time memory loss with an
heavy use of convolutional layers whose operations can be executed in parallel
and which imply fewer parameters. In this paper we present a Gated Fully
Convolutional Network architecture that is a recurrence-free alternative to the
well-known CNN+LSTM architectures. Our model is trained with the CTC loss and
shows competitive results on both the RIMES and IAM datasets. We release all
code to enable reproduction of our experiments:
https://github.com/FactoDeepLearning/LinePytorchOCR.
</p>
<a href="http://arxiv.org/abs/2012.04961" target="_blank">arXiv:2012.04961</a> [<a href="http://arxiv.org/pdf/2012.04961" target="_blank">pdf</a>]

<h2>Hateful Memes Detection via Complementary Visual and Linguistic Networks. (arXiv:2012.04977v1 [cs.CV])</h2>
<h3>Weibo Zhang, Guihua Liu, Zhuohua Li, Fuqing Zhu</h3>
<p>Hateful memes are widespread in social media and convey negative information.
The main challenge of hateful memes detection is that the expressive meaning
can not be well recognized by a single modality. In order to further integrate
modal information, we investigate a candidate solution based on complementary
visual and linguistic network in Hateful Memes Challenge 2020. In this way,
more comprehensive information of the multi-modality could be explored in
detail. Both contextual-level and sensitive object-level information are
considered in visual and linguistic embedding to formulate the complex
multi-modal scenarios. Specifically, a pre-trained classifier and object
detector are utilized to obtain the contextual features and region-of-interests
(RoIs) from the input, followed by the position representation fusion for
visual embedding. While linguistic embedding is composed of three components,
i.e., the sentence words embedding, position embedding and the corresponding
Spacy embedding (Sembedding), which is a symbol represented by vocabulary
extracted by Spacy. Both visual and linguistic embedding are fed into the
designed Complementary Visual and Linguistic (CVL) networks to produce the
prediction for hateful memes. Experimental results on Hateful Memes Challenge
Dataset demonstrate that CVL provides a decent performance, and produces 78:48%
and 72:95% on the criteria of AUROC and Accuracy. Code is available at
https://github.com/webYFDT/hateful.
</p>
<a href="http://arxiv.org/abs/2012.04977" target="_blank">arXiv:2012.04977</a> [<a href="http://arxiv.org/pdf/2012.04977" target="_blank">pdf</a>]

<h2>Driving Behavior Explanation with Multi-level Fusion. (arXiv:2012.04983v1 [cs.LG])</h2>
<h3>H&#xe9;di Ben-Younes, &#xc9;loi Zablocki, Patrick P&#xe9;rez, Matthieu Cord</h3>
<p>In this era of active development of autonomous vehicles, it becomes crucial
to provide driving systems with the capacity to explain their decisions. In
this work, we focus on generating high-level driving explanations as the
vehicle drives. We present BEEF, for BEhavior Explanation with Fusion, a deep
architecture which explains the behavior of a trajectory prediction model.
Supervised by annotations of human driving decisions justifications, BEEF
learns to fuse features from multiple levels. Leveraging recent advances in the
multi-modal fusion literature, BEEF is carefully designed to model the
correlations between high-level decisions features and mid-level perceptual
features. The flexibility and efficiency of our approach are validated with
extensive experiments on the HDD and BDD-X datasets.
</p>
<a href="http://arxiv.org/abs/2012.04983" target="_blank">arXiv:2012.04983</a> [<a href="http://arxiv.org/pdf/2012.04983" target="_blank">pdf</a>]

<h2>Hybrid Quantum Computing -- Tabu Search Algorithm for Partitioning Problems: preliminary study on the Traveling Salesman Problem. (arXiv:2012.04984v1 [cs.AI])</h2>
<h3>Eneko Osaba, Esther Villar-Rodriguez, Izaskun Oregi, Aitor Moreno-Fernandez-de-Leceta</h3>
<p>Quantum Computing is considered as the next frontier in computing, and it is
attracting a lot of attention from the current scientific community. This kind
of computation provides to researchers with a revolutionary paradigm for
addressing complex optimization problems, offering a significant speed
advantage and an efficient search ability. Anyway, despite hopes placed in this
field are high, Quantum Computing is still in an incipient stage of
development. For this reason, present architectures show certain limitations in
terms of computational capabilities and performance. These limitations have
motivated the carrying out of this paper. With this paper, we contribute to the
field introducing a novel solving scheme coined as hybrid Quantum Computing -
Tabu Search Algorithm. Main pillars of operation of the proposed method are a
greater control over the access to quantum resources, and a considerable
reduction of non-profitable accesses. For assessing the quality of our method,
we have used the well-known TSP as benchmarking problem. Furthermore, the
performance of QTA has been compared with QBSolv -- a state-of-the-art
decomposing solver -- on a set of 7 different TSP instances. The obtained
experimental outcomes support the preliminary conclusion that QTA is an
approach which offers promising results for solving partitioning problems,
while it drastically reduces the access to QC resources. Furthermore, we also
contribute in this paper to the field of Transfer Optimization by developing
and using a evolutionary multiform multitasking algorithm as initialization
method for the introduced hybrid Quantum Computing - Tabu Search Algorithm.
Concretely, the evolutionary multitasking algorithm implemented is a multiform
variant of the recently published Coevolutionary Variable Neighborhood Search
Algorithm for Discrete Multitasking.
</p>
<a href="http://arxiv.org/abs/2012.04984" target="_blank">arXiv:2012.04984</a> [<a href="http://arxiv.org/pdf/2012.04984" target="_blank">pdf</a>]

<h2>Group-Wise Semantic Mining for Weakly Supervised Semantic Segmentation. (arXiv:2012.05007v1 [cs.CV])</h2>
<h3>Xueyi Li, Tianfei Zhou, Jianwu Li, Yi Zhou, Zhaoxiang Zhang</h3>
<p>Acquiring sufficient ground-truth supervision to train deep visual models has
been a bottleneck over the years due to the data-hungry nature of deep
learning. This is exacerbated in some structured prediction tasks, such as
semantic segmentation, which requires pixel-level annotations. This work
addresses weakly supervised semantic segmentation (WSSS), with the goal of
bridging the gap between image-level annotations and pixel-level segmentation.
We formulate WSSS as a novel group-wise learning task that explicitly models
semantic dependencies in a group of images to estimate more reliable pseudo
ground-truths, which can be used for training more accurate segmentation
models. In particular, we devise a graph neural network (GNN) for group-wise
semantic mining, wherein input images are represented as graph nodes, and the
underlying relations between a pair of images are characterized by an efficient
co-attention mechanism. Moreover, in order to prevent the model from paying
excessive attention to common semantics only, we further propose a graph
dropout layer, encouraging the model to learn more accurate and complete object
responses. The whole network is end-to-end trainable by iterative message
passing, which propagates interaction cues over the images to progressively
improve the performance. We conduct experiments on the popular PASCAL VOC 2012
and COCO benchmarks, and our model yields state-of-the-art performance. Our
code is available at: https://github.com/Lixy1997/Group-WSSS.
</p>
<a href="http://arxiv.org/abs/2012.05007" target="_blank">arXiv:2012.05007</a> [<a href="http://arxiv.org/pdf/2012.05007" target="_blank">pdf</a>]

<h2>Strong but Simple Baseline with Dual-Granularity Triplet Loss for Visible-Thermal Person Re-Identification. (arXiv:2012.05010v1 [cs.CV])</h2>
<h3>Haijun Liu, Yanxia Chai, Xiaoheng Tan, Dong Li, Xichuan Zhou</h3>
<p>In this letter, we propose a conceptually simple and effective
dual-granularity triplet loss for visible-thermal person re-identification
(VT-ReID). In general, ReID models are always trained with the sample-based
triplet loss and identification loss from the fine granularity level. It is
possible when a center-based loss is introduced to encourage the intra-class
compactness and inter-class discrimination from the coarse granularity level.
Our proposed dual-granularity triplet loss well organizes the sample-based
triplet loss and center-based triplet loss in a hierarchical fine to coarse
granularity manner, just with some simple configurations of typical operations,
such as pooling and batch normalization. Experiments on RegDB and SYSU-MM01
datasets show that with only the global features our dual-granularity triplet
loss can improve the VT-ReID performance by a significant margin. It can be a
strong VT-ReID baseline to boost future research with high quality.
</p>
<a href="http://arxiv.org/abs/2012.05010" target="_blank">arXiv:2012.05010</a> [<a href="http://arxiv.org/pdf/2012.05010" target="_blank">pdf</a>]

<h2>Machine Learning for Glacier Monitoring in the Hindu Kush Himalaya. (arXiv:2012.05013v1 [cs.CV])</h2>
<h3>Shimaa Baraka, Benjamin Akera, Bibek Aryal, Tenzing Sherpa, Finu Shresta, Anthony Ortiz, Kris Sankaran, Juan Lavista Ferres, Mir Matin, Yoshua Bengio</h3>
<p>Glacier mapping is key to ecological monitoring in the hkh region. Climate
change poses a risk to individuals whose livelihoods depend on the health of
glacier ecosystems. In this work, we present a machine learning based approach
to support ecological monitoring, with a focus on glaciers. Our approach is
based on semi-automated mapping from satellite images. We utilize readily
available remote sensing data to create a model to identify and outline both
clean ice and debris-covered glaciers from satellite imagery. We also release
data and develop a web tool that allows experts to visualize and correct model
predictions, with the ultimate aim of accelerating the glacier mapping process.
</p>
<a href="http://arxiv.org/abs/2012.05013" target="_blank">arXiv:2012.05013</a> [<a href="http://arxiv.org/pdf/2012.05013" target="_blank">pdf</a>]

<h2>vLPD-Net: A Registration-aided Domain Adaptation Network for 3D Point Cloud Based Place Recognition. (arXiv:2012.05018v1 [cs.CV])</h2>
<h3>Zhijian Qiao, Hanjiang Hu, Siyuan Chen, Zhe Liu, Zhuowen Shen, Hesheng Wang</h3>
<p>In the field of large-scale SLAM for autonomous driving and mobile robotics,
3D point cloud based place recognition has aroused significant research
interest due to its robustness to changing environments with drastic daytime
and weather variance. However, it is time-consuming and effort-costly to obtain
high-quality point cloud data and groundtruth for registration and place
recognition model training in the real world. To this end, a novel
registration-aided 3D domain adaptation network for point cloud based place
recognition is proposed. A structure-aware registration network is introduced
to help learn feature from geometric properties and a matching rate based
triplet loss is involved for metric learning. The model is trained through a
new virtual LiDAR dataset through GTA-V with diverse weather and daytime
conditions and domain adaptation is implemented to the real-world domain by
aligning the local and global features. Extensive experiments have been
conducted to validate the effectiveness of the structure-aware registration
network and domain adaptation. Our results outperform state-of-the-art 3D place
recognition baselines on the real-world Oxford RobotCar dataset with the
visualization of large-scale registration on the virtual dataset.
</p>
<a href="http://arxiv.org/abs/2012.05018" target="_blank">arXiv:2012.05018</a> [<a href="http://arxiv.org/pdf/2012.05018" target="_blank">pdf</a>]

<h2>NSL: Hybrid Interpretable Learning From Noisy Raw Data. (arXiv:2012.05023v1 [cs.LG])</h2>
<h3>Daniel Cunnington, Alessandra Russo, Mark Law, Jorge Lobo, Lance Kaplan</h3>
<p>Inductive Logic Programming (ILP) systems learn generalised, interpretable
rules in a data-efficient manner utilising existing background knowledge.
However, current ILP systems require training examples to be specified in a
structured logical format. Neural networks learn from unstructured data,
although their learned models may be difficult to interpret and are vulnerable
to data perturbations at run-time. This paper introduces a hybrid
neural-symbolic learning framework, called NSL, that learns interpretable rules
from labelled unstructured data. NSL combines pre-trained neural networks for
feature extraction with FastLAS, a state-of-the-art ILP system for rule
learning under the answer set semantics. Features extracted by the neural
components define the structured context of labelled examples and the
confidence of the neural predictions determines the level of noise of the
examples. Using the scoring function of FastLAS, NSL searches for short,
interpretable rules that generalise over such noisy examples. We evaluate our
framework on propositional and first-order classification tasks using the MNIST
dataset as raw data. Specifically, we demonstrate that NSL is able to learn
robust rules from perturbed MNIST data and achieve comparable or superior
accuracy when compared to neural network and random forest baselines whilst
being more general and interpretable.
</p>
<a href="http://arxiv.org/abs/2012.05023" target="_blank">arXiv:2012.05023</a> [<a href="http://arxiv.org/pdf/2012.05023" target="_blank">pdf</a>]

<h2>Generating Out of Distribution Adversarial Attack using Latent Space Poisoning. (arXiv:2012.05027v1 [cs.CV])</h2>
<h3>Ujjwal Upadhyay, Prerana Mukherjee</h3>
<p>Traditional adversarial attacks rely upon the perturbations generated by
gradients from the network which are generally safeguarded by gradient guided
search to provide an adversarial counterpart to the network. In this paper, we
propose a novel mechanism of generating adversarial examples where the actual
image is not corrupted rather its latent space representation is utilized to
tamper with the inherent structure of the image while maintaining the
perceptual quality intact and to act as legitimate data samples. As opposed to
gradient-based attacks, the latent space poisoning exploits the inclination of
classifiers to model the independent and identical distribution of the training
dataset and tricks it by producing out of distribution samples. We train a
disentangled variational autoencoder (beta-VAE) to model the data in latent
space and then we add noise perturbations using a class-conditioned
distribution function to the latent space under the constraint that it is
misclassified to the target label. Our empirical results on MNIST, SVHN, and
CelebA dataset validate that the generated adversarial examples can easily fool
robust l_0, l_2, l_inf norm classifiers designed using provably robust defense
mechanisms.
</p>
<a href="http://arxiv.org/abs/2012.05027" target="_blank">arXiv:2012.05027</a> [<a href="http://arxiv.org/pdf/2012.05027" target="_blank">pdf</a>]

<h2>Scene Text Detection with Scribble Lines. (arXiv:2012.05030v1 [cs.CV])</h2>
<h3>Wenqing Zhang, Yang Qiu, Minghui Liao, Rui Zhang, Xiaolin Wei, Xiang Bai</h3>
<p>Scene text detection, which is one of the most popular topics in both
academia and industry, can achieve remarkable performance with sufficient
training data. However, the annotation costs of scene text detection are huge
with traditional labeling methods due to the various shapes of texts. Thus, it
is practical and insightful to study simpler labeling methods without harming
the detection performance. In this paper, we propose to annotate the texts by
scribble lines instead of polygons for text detection. It is a general labeling
method for texts with various shapes and requires low labeling costs.
Furthermore, a weakly-supervised scene text detection framework is proposed to
use the scribble lines for text detection. The experiments on several
benchmarks show that the proposed method bridges the performance gap between
the weakly labeling method and the original polygon-based labeling methods,
with even better performance. We will release the weak annotations of the
benchmarks in our experiments and hope it will benefit the field of scene text
detection to achieve better performance with simpler annotations.
</p>
<a href="http://arxiv.org/abs/2012.05030" target="_blank">arXiv:2012.05030</a> [<a href="http://arxiv.org/pdf/2012.05030" target="_blank">pdf</a>]

<h2>ReCoG: A Deep Learning Framework with Heterogeneous Graph for Interaction-Aware Trajectory Prediction. (arXiv:2012.05032v1 [cs.RO])</h2>
<h3>Xiaoyu Mo, Yang Xing, Chen Lv</h3>
<p>Predicting the future trajectory of surrounding vehicles is essential for the
navigation of autonomous vehicles in complex real-world driving scenarios. It
is challenging as a vehicle's motion is affected by many factors, including its
surrounding infrastructures and vehicles. In this work, we develop the ReCoG
(Recurrent Convolutional and Graph Neural Networks), which is a general scheme
that represents vehicle interactions with infrastructure information as a
heterogeneous graph and applies graph neural networks (GNNs) to model the
high-level interactions for trajectory prediction. Nodes in the graph contain
corresponding features, where a vehicle node contains its sequential feature
encoded using Recurrent Neural Network (RNN), and an infrastructure node
contains spatial feature encoded using Convolutional Neural Network (CNN). Then
the ReCoG predicts the future trajectory of the target vehicle by jointly
considering all of the features. Experiments are conducted by using the
INTERACTION dataset. Experimental results show that the proposed ReCoG
outperforms other state-of-the-art methods in terms of different types of
displacement error, validating the feasibility and effectiveness of the
developed approach.
</p>
<a href="http://arxiv.org/abs/2012.05032" target="_blank">arXiv:2012.05032</a> [<a href="http://arxiv.org/pdf/2012.05032" target="_blank">pdf</a>]

<h2>Development of Autonomous Quadcopter. (arXiv:2012.05042v1 [cs.RO])</h2>
<h3>Mohammad Al-Fetyani, Mones Azazma</h3>
<p>The main objective of this work is demonstrated through two main aspects. The
first is the design of an adaptive neuro-fuzzy inference system (ANFIS)
controller to develop the attitude and altitude of a quadcopter. The second is
to establish the linearized mathematical model of the quadcopter in a simple
and clear way. To show the effectiveness of the ANFIS approach, the performance
of a well-trained ANFIS controller is compared to a classical
proportional-derivative (PD) controller and a properly tuned fuzzy logic
controller.
</p>
<a href="http://arxiv.org/abs/2012.05042" target="_blank">arXiv:2012.05042</a> [<a href="http://arxiv.org/pdf/2012.05042" target="_blank">pdf</a>]

<h2>Could robots be regarded as humans in future?. (arXiv:2012.05054v1 [cs.RO])</h2>
<h3>Huansheng Ning, Feifei Shi</h3>
<p>With the overwhelming advances in Artificial Intelligence (AI), brain science
and neuroscience, robots are developing towards a direction of much more
human-like and human-friendly. We can't help but wonder whether robots could be
regarded as humans in future? In this article, we propose a novel perspective
to analyze the essential difference between humans and robots, that is based on
their respective living spaces, particularly the independent and intrinsic
thinking space. We finally come to the conclusion that, only when robots own
the independent and intrinsic thinking space as humans, could they have the
prerequisites to be regarded as humans.
</p>
<a href="http://arxiv.org/abs/2012.05054" target="_blank">arXiv:2012.05054</a> [<a href="http://arxiv.org/pdf/2012.05054" target="_blank">pdf</a>]

<h2>Inference of Stochastic Dynamical Systems from Cross-Sectional Population Data. (arXiv:2012.05055v1 [cs.LG])</h2>
<h3>Anastasios Tsourtis, Yannis Pantazis, Ioannis Tsamardinos</h3>
<p>Inferring the driving equations of a dynamical system from population or
time-course data is important in several scientific fields such as
biochemistry, epidemiology, financial mathematics and many others. Despite the
existence of algorithms that learn the dynamics from trajectorial measurements
there are few attempts to infer the dynamical system straight from population
data. In this work, we deduce and then computationally estimate the
Fokker-Planck equation which describes the evolution of the population's
probability density, based on stochastic differential equations. Then,
following the USDL approach, we project the Fokker-Planck equation to a proper
set of test functions, transforming it into a linear system of equations.
Finally, we apply sparse inference methods to solve the latter system and thus
induce the driving forces of the dynamical system. Our approach is illustrated
in both synthetic and real data including non-linear, multimodal stochastic
differential equations, biochemical reaction networks as well as mass cytometry
biological measurements.
</p>
<a href="http://arxiv.org/abs/2012.05055" target="_blank">arXiv:2012.05055</a> [<a href="http://arxiv.org/pdf/2012.05055" target="_blank">pdf</a>]

<h2>Contrastive Transformation for Self-supervised Correspondence Learning. (arXiv:2012.05057v1 [cs.CV])</h2>
<h3>Ning Wang, Wengang Zhou, Houqiang Li</h3>
<p>In this paper, we focus on the self-supervised learning of visual
correspondence using unlabeled videos in the wild. Our method simultaneously
considers intra- and inter-video representation associations for reliable
correspondence estimation. The intra-video learning transforms the image
contents across frames within a single video via the frame pair-wise affinity.
To obtain the discriminative representation for instance-level separation, we
go beyond the intra-video analysis and construct the inter-video affinity to
facilitate the contrastive transformation across different videos. By forcing
the transformation consistency between intra- and inter-video levels, the
fine-grained correspondence associations are well preserved and the
instance-level feature discrimination is effectively reinforced. Our simple
framework outperforms the recent self-supervised correspondence methods on a
range of visual tasks including video object tracking (VOT), video object
segmentation (VOS), pose keypoint tracking, etc. It is worth mentioning that
our method also surpasses the fully-supervised affinity representation (e.g.,
ResNet) and performs competitively against the recent fully-supervised
algorithms designed for the specific tasks (e.g., VOT and VOS).
</p>
<a href="http://arxiv.org/abs/2012.05057" target="_blank">arXiv:2012.05057</a> [<a href="http://arxiv.org/pdf/2012.05057" target="_blank">pdf</a>]

<h2>Variational Nonlinear System Identification. (arXiv:2012.05072v1 [stat.ML])</h2>
<h3>Jarrad Courts, Adrian Wills, Thomas Sch&#xf6;n, Brett Ninness</h3>
<p>This paper considers parameter estimation for nonlinear state-space models,
which is an important but challenging problem. We address this challenge by
employing a variational inference (VI) approach, which is a principled method
that has deep connections to maximum likelihood estimation. This VI approach
ultimately provides estimates of the model as solutions to an optimisation
problem, which is deterministic, tractable and can be solved using standard
optimisation tools. A specialisation of this approach for systems with additive
Gaussian noise is also detailed. The proposed method is examined numerically on
a range of simulation and real examples with a focus on robustness to parameter
initialisations; we additionally perform favourable comparisons against
state-of-the-art alternatives.
</p>
<a href="http://arxiv.org/abs/2012.05072" target="_blank">arXiv:2012.05072</a> [<a href="http://arxiv.org/pdf/2012.05072" target="_blank">pdf</a>]

<h2>Robotic Communications for 5G and Beyond: Challenges and Research Opportunities. (arXiv:2012.05093v1 [cs.RO])</h2>
<h3>Yuanwei Liu, Xiao Liu, Xinyu Gao, Xidong Mu, Xiangwei Zhou, Octavia A. Dobre, H. Vincent Poor</h3>
<p>The ongoing surge in applications of robotics brings both opportunities and
challenges for the fifth-generation (5G) and beyond (B5G) of communication
networks. This article focuses on 5G/B5G-enabled terrestrial robotic
communications with an emphasis on distinct characteristics of such
communications. Firstly, signal and spatial modeling for robotic communications
are presented. To elaborate further, both the benefits and challenges derived
from robots' mobility are discussed. As a further advance, a novel simultaneous
localization and radio mapping (SLARM) framework is proposed for integrating
localization and communications into robotic networks. Furthermore, dynamic
trajectory design and resource allocation for both indoor and outdoor robots
are provided to verify the performance of robotic communications in the context
of typical robotic application scenarios.
</p>
<a href="http://arxiv.org/abs/2012.05093" target="_blank">arXiv:2012.05093</a> [<a href="http://arxiv.org/pdf/2012.05093" target="_blank">pdf</a>]

<h2>Deep Denoising of Flash and No-Flash Pairs for Photography in Low-Light Environments. (arXiv:2012.05116v1 [cs.CV])</h2>
<h3>Zhihao Xia, Micha&#xeb;l Gharbi, Federico Perazzi, Kalyan Sunkavalli, Ayan Chakrabarti</h3>
<p>We introduce a neural network-based method to denoise pairs of images taken
in quick succession in low-light environments, with and without a flash. Our
goal is to produce a high-quality rendering of the scene that preserves the
color and mood from the ambient illumination of the noisy no-flash image, while
recovering surface texture and detail revealed by the flash. Our network
outputs a gain map and a field of kernels, the latter obtained by linearly
mixing elements of a per-image low-rank kernel basis. We first apply the kernel
field to the no-flash image, and then multiply the result with the gain map to
create the final output. We show our network effectively learns to produce
high-quality images by combining a smoothed out estimate of the scene's ambient
appearance from the no-flash image, with high-frequency albedo details
extracted from the flash input. Our experiments show significant improvements
over alternative captures without a flash, and baseline denoisers that use
flash no-flash pairs. In particular, our method produces images that are both
noise-free and contain accurate ambient colors without the sharp shadows or
strong specular highlights visible in the flash image.
</p>
<a href="http://arxiv.org/abs/2012.05116" target="_blank">arXiv:2012.05116</a> [<a href="http://arxiv.org/pdf/2012.05116" target="_blank">pdf</a>]

<h2>Self-supervised Human Detection and Segmentation via Multi-view Consensus. (arXiv:2012.05119v1 [cs.CV])</h2>
<h3>Isinsu Katircioglu, Helge Rhodin, J&#xf6;rg Sp&#xf6;rri, Mathieu Salzmann, Pascal Fua</h3>
<p>Self-supervised detection and segmentation of foreground objects in complex
scenes is gaining attention as their fully-supervised counterparts require
overly large amounts of annotated data to deliver sufficient accuracy in
domain-specific applications. However, existing self-supervised approaches
predominantly rely on restrictive assumptions on appearance and motion, which
precludes their use in scenes depicting highly dynamic activities or involve
camera motion.

To mitigate this problem, we propose using a multi-camera framework in which
geometric constraints are embedded in the form of multi-view consistency during
training via coarse 3D localization in a voxel grid and fine-grained offset
regression. In this manner, we learn a joint distribution of proposals over
multiple views. At inference time, our method operates on single RGB images.

We show that our approach outperforms state-of-the-art self-supervised person
detection and segmentation techniques on images that visually depart from those
of standard benchmarks, as well as on those of the classical Human3.6M dataset.
</p>
<a href="http://arxiv.org/abs/2012.05119" target="_blank">arXiv:2012.05119</a> [<a href="http://arxiv.org/pdf/2012.05119" target="_blank">pdf</a>]

<h2>The Counterfactual NESS Definition of Causation. (arXiv:2012.05123v1 [cs.AI])</h2>
<h3>Sander Beckers</h3>
<p>In previous work with Joost Vennekens I proposed a definition of actual
causation that is based on certain plausible principles, thereby allowing the
debate on causation to shift away from its heavy focus on examples towards a
more systematic analysis. This paper contributes to that analysis in two ways.
First, I show that our definition is in fact a formalization of Wright's famous
NESS definition of causation combined with a counterfactual difference-making
condition. This means that our definition integrates two highly influential
approaches to causation that are claimed to stand in opposition to each other.
Second, I modify our definition to offer a substantial improvement: I weaken
the difference-making condition in such a way that it avoids the problematic
analysis of cases of preemption. The resulting Counterfactual NESS definition
of causation forms a natural compromise between counterfactual approaches and
the NESS approach.
</p>
<a href="http://arxiv.org/abs/2012.05123" target="_blank">arXiv:2012.05123</a> [<a href="http://arxiv.org/pdf/2012.05123" target="_blank">pdf</a>]

<h2>Federated Learning in Unreliable and Resource-Constrained Cellular Wireless Networks. (arXiv:2012.05137v1 [cs.LG])</h2>
<h3>Mohammad Salehi, Ekram Hossain</h3>
<p>With growth in the number of smart devices and advancements in their
hardware, in recent years, data-driven machine learning techniques have drawn
significant attention. However, due to privacy and communication issues, it is
not possible to collect this data at a centralized location. Federated learning
is a machine learning setting where the centralized location trains a learning
model over remote devices. Federated learning algorithms cannot be employed in
the real world scenarios unless they consider unreliable and
resource-constrained nature of the wireless medium. In this paper, we propose a
federated learning algorithm that is suitable for cellular wireless networks.
We prove its convergence, and provide the optimal scheduling policy that
maximizes the convergence rate. We also study the effect of local computation
steps and communication steps on the convergence of the proposed algorithm. We
prove, in practice, federated learning algorithms may solve a different problem
than the one that they have been employed for if the unreliability of wireless
channels is neglected. Finally, through numerous experiments on real and
synthetic datasets, we demonstrate the convergence of our proposed algorithm.
</p>
<a href="http://arxiv.org/abs/2012.05137" target="_blank">arXiv:2012.05137</a> [<a href="http://arxiv.org/pdf/2012.05137" target="_blank">pdf</a>]

<h2>Streaming Algorithms for Stochastic Multi-armed Bandits. (arXiv:2012.05142v1 [cs.LG])</h2>
<h3>Arnab Maiti, Vishakha Patil, Arindam Khan</h3>
<p>We study the Stochastic Multi-armed Bandit problem under bounded arm-memory.
In this setting, the arms arrive in a stream, and the number of arms that can
be stored in the memory at any time, is bounded. The decision-maker can only
pull arms that are present in the memory. We address the problem from the
perspective of two standard objectives: 1) regret minimization, and 2) best-arm
identification. For regret minimization, we settle an important open question
by showing an almost tight hardness. We show {\Omega}(T^{2/3}) cumulative
regret in expectation for arm-memory size of (n-1), where n is the number of
arms. For best-arm identification, we study two algorithms. First, we present
an O(r) arm-memory r-round adaptive streaming algorithm to find an
{\epsilon}-best arm. In r-round adaptive streaming algorithm for best-arm
identification, the arm pulls in each round are decided based on the observed
outcomes in the earlier rounds. The best-arm is the output at the end of r
rounds. The upper bound on the sample complexity of our algorithm matches with
the lower bound for any r-round adaptive streaming algorithm. Secondly, we
present a heuristic to find the {\epsilon}-best arm with optimal sample
complexity, by storing only one extra arm in the memory.
</p>
<a href="http://arxiv.org/abs/2012.05142" target="_blank">arXiv:2012.05142</a> [<a href="http://arxiv.org/pdf/2012.05142" target="_blank">pdf</a>]

<h2>Binding and Perspective Taking as Inference in a Generative Neural Network Model. (arXiv:2012.05152v1 [cs.LG])</h2>
<h3>Mahdi Sadeghi, Fabian Schrodt, Sebastian Otte, Martin V. Butz</h3>
<p>The ability to flexibly bind features into coherent wholes from different
perspectives is a hallmark of cognition and intelligence. Importantly, the
binding problem is not only relevant for vision but also for general
intelligence, sensorimotor integration, event processing, and language. Various
artificial neural network models have tackled this problem with dynamic neural
fields and related approaches. Here we focus on a generative encoder-decoder
architecture that adapts its perspective and binds features by means of
retrospective inference. We first train a model to learn sufficiently accurate
generative models of dynamic biological motion or other harmonic motion
patterns, such as a pendulum. We then scramble the input to a certain extent,
possibly vary the perspective onto it, and propagate the prediction error back
onto a binding matrix, that is, hidden neural states that determine feature
binding. Moreover, we propagate the error further back onto perspective taking
neurons, which rotate and translate the input features onto a known frame of
reference. Evaluations show that the resulting gradient-based inference process
solves the perspective taking and binding problem for known biological motion
patterns, essentially yielding a Gestalt perception mechanism. In addition,
redundant feature properties and population encodings are shown to be highly
useful. While we evaluate the algorithm on biological motion patterns, the
principled approach should be applicable to binding and Gestalt perception
problems in other domains.
</p>
<a href="http://arxiv.org/abs/2012.05152" target="_blank">arXiv:2012.05152</a> [<a href="http://arxiv.org/pdf/2012.05152" target="_blank">pdf</a>]

<h2>Simple is not Easy: A Simple Strong Baseline for TextVQA and TextCaps. (arXiv:2012.05153v1 [cs.CV])</h2>
<h3>Qi Zhu, Chenyu Gao, Peng Wang, Qi Wu</h3>
<p>Texts appearing in daily scenes that can be recognized by OCR (Optical
Character Recognition) tools contain significant information, such as street
name, product brand and prices. Two tasks -- text-based visual question
answering and text-based image captioning, with a text extension from existing
vision-language applications, are catching on rapidly. To address these
problems, many sophisticated multi-modality encoding frameworks (such as
heterogeneous graph structure) are being used. In this paper, we argue that a
simple attention mechanism can do the same or even better job without any bells
and whistles. Under this mechanism, we simply split OCR token features into
separate visual- and linguistic-attention branches, and send them to a popular
Transformer decoder to generate answers or captions. Surprisingly, we find this
simple baseline model is rather strong -- it consistently outperforms
state-of-the-art (SOTA) models on two popular benchmarks, TextVQA and all three
tasks of ST-VQA, although these SOTA models use far more complex encoding
mechanisms. Transferring it to text-based image captioning, we also surpass the
TextCaps Challenge 2020 winner. We wish this work to set the new baseline for
this two OCR text related applications and to inspire new thinking of
multi-modality encoder design. Code is available at
https://github.com/ZephyrZhuQi/ssbaseline
</p>
<a href="http://arxiv.org/abs/2012.05153" target="_blank">arXiv:2012.05153</a> [<a href="http://arxiv.org/pdf/2012.05153" target="_blank">pdf</a>]

<h2>Implicit Regularization in ReLU Networks with the Square Loss. (arXiv:2012.05156v1 [cs.LG])</h2>
<h3>Ohad Shamir, Gal Vardi</h3>
<p>Understanding the implicit regularization (or implicit bias) of gradient
descent has recently been a very active research area. However, the implicit
regularization in nonlinear neural networks is still poorly understood,
especially for regression losses such as the square loss. Perhaps surprisingly,
we prove that even for a single ReLU neuron, it is \emph{impossible} to
characterize the implicit regularization with the square loss by any explicit
function of the model parameters (although on the positive side, we show it can
be characterized approximately). For one hidden-layer networks, we prove a
similar result, where in general it is impossible to characterize implicit
regularization properties in this manner, \emph{except} for the "balancedness"
property identified in Du et al. [2018]. Our results suggest that a more
general framework than the one considered so far may be needed to understand
implicit regularization for nonlinear predictors, and provides some clues on
what this framework should be.
</p>
<a href="http://arxiv.org/abs/2012.05156" target="_blank">arXiv:2012.05156</a> [<a href="http://arxiv.org/pdf/2012.05156" target="_blank">pdf</a>]

<h2>Convex Regularization Behind Neural Reconstruction. (arXiv:2012.05169v1 [cs.LG])</h2>
<h3>Arda Sahiner, Morteza Mardani, Batu Ozturkler, Mert Pilanci, John Pauly</h3>
<p>Neural networks have shown tremendous potential for reconstructing
high-resolution images in inverse problems. The non-convex and opaque nature of
neural networks, however, hinders their utility in sensitive applications such
as medical imaging. To cope with this challenge, this paper advocates a convex
duality framework that makes a two-layer fully-convolutional ReLU denoising
network amenable to convex optimization. The convex dual network not only
offers the optimum training with convex solvers, but also facilitates
interpreting training and prediction. In particular, it implies training neural
networks with weight decay regularization induces path sparsity while the
prediction is piecewise linear filtering. A range of experiments with MNIST and
fastMRI datasets confirm the efficacy of the dual network optimization problem.
</p>
<a href="http://arxiv.org/abs/2012.05169" target="_blank">arXiv:2012.05169</a> [<a href="http://arxiv.org/pdf/2012.05169" target="_blank">pdf</a>]

<h2>Dynamical System Segmentation for Information Measures in Motion. (arXiv:2012.05183v1 [cs.RO])</h2>
<h3>Thomas A. Berrueta, Ana Pervan, Kathleen Fitzsimons, Todd D. Murphey</h3>
<p>Motions carry information about the underlying task being executed. Previous
work in human motion analysis suggests that complex motions may result from the
composition of fundamental submovements called movemes. The existence of finite
structure in motion motivates information-theoretic approaches to motion
analysis and robotic assistance. We define task embodiment as the amount of
task information encoded in an agent's motions. By decoding task-specific
information embedded in motion, we can use task embodiment to create detailed
performance assessments. We extract an alphabet of behaviors comprising a
motion without \textit{a priori} knowledge using a novel algorithm, which we
call dynamical system segmentation. For a given task, we specify an optimal
agent, and compute an alphabet of behaviors representative of the task. We
identify these behaviors in data from agent executions, and compare their
relative frequencies against that of the optimal agent using the
Kullback-Leibler divergence. We validate this approach using a dataset of human
subjects (n=53) performing a dynamic task, and under this measure find that
individuals receiving assistance better embody the task. Moreover, we find that
task embodiment is a better predictor of assistance than integrated
mean-squared-error.
</p>
<a href="http://arxiv.org/abs/2012.05183" target="_blank">arXiv:2012.05183</a> [<a href="http://arxiv.org/pdf/2012.05183" target="_blank">pdf</a>]

<h2>Physics-Guided Spoof Trace Disentanglement for Generic Face Anti-Spoofing. (arXiv:2012.05185v1 [cs.CV])</h2>
<h3>Yaojie Liu, Xiaoming Liu</h3>
<p>Prior studies show that the key to face anti-spoofing lies in the subtle
image pattern, termed "spoof trace", e.g., color distortion, 3D mask edge,
Moire pattern, and many others. Designing a generic face anti-spoofing model to
estimate those spoof traces can improve not only the generalization of the
spoof detection, but also the interpretability of the model's decision. Yet,
this is a challenging task due to the diversity of spoof types and the lack of
ground truth in spoof traces. In this work, we design a novel adversarial
learning framework to disentangle spoof faces into the spoof traces and the
live counterparts. Guided by physical properties, the spoof generation is
represented as a combination of additive process and inpainting process.
Additive process describes spoofing as spoof material introducing extra
patterns (e.g., moire pattern), where the live counterpart can be recovered by
removing those patterns. Inpainting process describes spoofing as spoof
material fully covering certain regions, where the live counterpart of those
regions has to be "guessed". We use 3 additive components and 1 inpainting
component to represent traces at different frequency bands. The disentangled
spoof traces can be utilized to synthesize realistic new spoof faces after
proper geometric correction, and the synthesized spoof can be used for training
and improve the generalization of spoof detection. Our approach demonstrates
superior spoof detection performance on 3 testing scenarios: known attacks,
unknown attacks, and open-set attacks. Meanwhile, it provides a
visually-convincing estimation of the spoof traces. Source code and pre-trained
models will be publicly available upon publication.
</p>
<a href="http://arxiv.org/abs/2012.05185" target="_blank">arXiv:2012.05185</a> [<a href="http://arxiv.org/pdf/2012.05185" target="_blank">pdf</a>]

<h2>Rigid and Articulated Point Registration with Expectation Conditional Maximization. (arXiv:2012.05191v1 [cs.CV])</h2>
<h3>Radu Horaud, Florence Forbes, Manuel Yguel, Guillaume Dewaele, Jian Zhang</h3>
<p>This paper addresses the issue of matching rigid and articulated shapes
through probabilistic point registration. The problem is recast into a missing
data framework where unknown correspondences are handled via mixture models.
Adopting a maximum likelihood principle, we introduce an innovative EM-like
algorithm, namely the Expectation Conditional Maximization for Point
Registration (ECMPR) algorithm. The algorithm allows the use of general
covariance matrices for the mixture model components and improves over the
isotropic covariance case. We analyse in detail the associated consequences in
terms of estimation of the registration parameters, and we propose an optimal
method for estimating the rotational and translational parameters based on
semi-definite positive relaxation. We extend rigid registration to articulated
registration. Robustness is ensured by detecting and rejecting outliers through
the addition of a uniform component to the Gaussian mixture model at hand. We
provide an in-depth analysis of our method and we compare it both theoretically
and experimentally with other robust methods for point registration.
</p>
<a href="http://arxiv.org/abs/2012.05191" target="_blank">arXiv:2012.05191</a> [<a href="http://arxiv.org/pdf/2012.05191" target="_blank">pdf</a>]

<h2>Cost-Based Budget Active Learning for Deep Learning. (arXiv:2012.05196v1 [cs.LG])</h2>
<h3>Patrick K. Gikunda, Nicolas Jouandeau</h3>
<p>Majorly classical Active Learning (AL) approach usually uses statistical
theory such as entropy and margin to measure instance utility, however it fails
to capture the data distribution information contained in the unlabeled data.
This can eventually cause the classifier to select outlier instances to label.
Meanwhile, the loss associated with mislabeling an instance in a typical
classification task is much higher than the loss associated with the opposite
error. To address these challenges, we propose a Cost-Based Bugdet Active
Learning (CBAL) which considers the classification uncertainty as well as
instance diversity in a population constrained by a budget. A principled
approach based on the min-max is considered to minimize both the labeling and
decision cost of the selected instances, this ensures a near-optimal results
with significantly less computational effort. Extensive experimental results
show that the proposed approach outperforms several state-of -the-art active
learning approaches.
</p>
<a href="http://arxiv.org/abs/2012.05196" target="_blank">arXiv:2012.05196</a> [<a href="http://arxiv.org/pdf/2012.05196" target="_blank">pdf</a>]

<h2>Predicting Prostate Cancer-Specific Mortality with A.I.-based Gleason Grading. (arXiv:2012.05197v1 [cs.CV])</h2>
<h3>Ellery Wulczyn, Kunal Nagpal, Matthew Symonds, Melissa Moran, Markus Plass, Robert Reihs, Farah Nader, Fraser Tan, Yuannan Cai, Trissia Brown, Isabelle Flament-Auvigne, Mahul B. Amin, Martin C. Stumpe, Heimo Muller, Peter Regitnig, Andreas Holzinger, Greg S. Corrado, Lily H. Peng, Po-Hsuan Cameron Chen, David F. Steiner, Kurt Zatloukal, Yun Liu, Craig H. Mermel</h3>
<p>Gleason grading of prostate cancer is an important prognostic factor but
suffers from poor reproducibility, particularly among non-subspecialist
pathologists. Although artificial intelligence (A.I.) tools have demonstrated
Gleason grading on-par with expert pathologists, it remains an open question
whether A.I. grading translates to better prognostication. In this study, we
developed a system to predict prostate-cancer specific mortality via A.I.-based
Gleason grading and subsequently evaluated its ability to risk-stratify
patients on an independent retrospective cohort of 2,807 prostatectomy cases
from a single European center with 5-25 years of follow-up (median: 13,
interquartile range 9-17). The A.I.'s risk scores produced a C-index of 0.84
(95%CI 0.80-0.87) for prostate cancer-specific mortality. Upon discretizing
these risk scores into risk groups analogous to pathologist Grade Groups (GG),
the A.I. had a C-index of 0.82 (95%CI 0.78-0.85). On the subset of cases with a
GG in the original pathology report (n=1,517), the A.I.'s C-indices were 0.87
and 0.85 for continuous and discrete grading, respectively, compared to 0.79
(95%CI 0.71-0.86) for GG obtained from the reports. These represent
improvements of 0.08 (95%CI 0.01-0.15) and 0.07 (95%CI 0.00-0.14) respectively.
Our results suggest that A.I.-based Gleason grading can lead to effective
risk-stratification and warrants further evaluation for improving disease
management.
</p>
<a href="http://arxiv.org/abs/2012.05197" target="_blank">arXiv:2012.05197</a> [<a href="http://arxiv.org/pdf/2012.05197" target="_blank">pdf</a>]

<h2>A Riemannian Block Coordinate Descent Method for Computing the Projection Robust Wasserstein Distance. (arXiv:2012.05199v1 [cs.LG])</h2>
<h3>Minhui Huang, Shiqian Ma, Lifeng Lai</h3>
<p>The Wasserstein distance has become increasingly important in machine
learning and deep learning. Despite its popularity, the Wasserstein distance is
hard to approximate because of the curse of dimensionality. A recently proposed
approach to alleviate the curse of dimensionality is to project the sampled
data from the high dimensional probability distribution onto a
lower-dimensional subspace, and then compute the Wasserstein distance between
the projected data. However, this approach requires to solve a max-min problem
over the Stiefel manifold, which is very challenging in practice. The only
existing work that solves this problem directly is the RGAS (Riemannian
Gradient Ascent with Sinkhorn Iteration) algorithm, which requires to solve an
entropy-regularized optimal transport problem in each iteration, and thus can
be costly for large-scale problems. In this paper, we propose a Riemannian
block coordinate descent (RBCD) method to solve this problem, which is based on
a novel reformulation of the regularized max-min problem over the Stiefel
manifold. We show that the complexity of arithmetic operations for RBCD to
obtain an $\epsilon$-stationary point is $O(\epsilon^{-3})$. This significantly
improves the corresponding complexity of RGAS, which is $O(\epsilon^{-12})$.
Moreover, our RBCD has very low per-iteration complexity, and hence is suitable
for large-scale problems. Numerical results on both synthetic and real datasets
demonstrate that our method is more efficient than existing methods, especially
when the number of sampled data is very large.
</p>
<a href="http://arxiv.org/abs/2012.05199" target="_blank">arXiv:2012.05199</a> [<a href="http://arxiv.org/pdf/2012.05199" target="_blank">pdf</a>]

<h2>Tactile Object Pose Estimation from the First Touch with Geometric Contact Rendering. (arXiv:2012.05205v1 [cs.RO])</h2>
<h3>Maria Bauza, Eric Valls, Bryan Lim, Theo Sechopoulos, Alberto Rodriguez</h3>
<p>In this paper, we present an approach to tactile pose estimation from the
first touch for known objects. First, we create an object-agnostic map from
real tactile observations to contact shapes. Next, for a new object with known
geometry, we learn a tailored perception model completely in simulation. To do
so, we simulate the contact shapes that a dense set of object poses would
produce on the sensor. Then, given a new contact shape obtained from the sensor
output, we match it against the pre-computed set using the object-specific
embedding learned purely in simulation using contrastive learning.

This results in a perception model that can localize objects from a single
tactile observation. It also allows reasoning over pose distributions and
including additional pose constraints coming from other perception systems or
multiple contacts. We provide quantitative results for four objects. Our
approach provides high accuracy pose estimations from distinctive tactile
observations while regressing pose distributions to account for those contact
shapes that could result from different object poses. We further extend and
test our approach in multi-contact scenarios where several tactile sensors are
simultaneously in contact with the object. Website:
this http URL
</p>
<a href="http://arxiv.org/abs/2012.05205" target="_blank">arXiv:2012.05205</a> [<a href="http://arxiv.org/pdf/2012.05205" target="_blank">pdf</a>]

<h2>Uncertainty Intervals for Graph-based Spatio-Temporal Traffic Prediction. (arXiv:2012.05207v1 [cs.LG])</h2>
<h3>Tijs Maas, Peter Bloem</h3>
<p>Many traffic prediction applications rely on uncertainty estimates instead of
the mean prediction. Statistical traffic prediction literature has a complete
subfield devoted to uncertainty modelling, but recent deep learning traffic
prediction models either lack this feature or make specific assumptions that
restrict its practicality. We propose Quantile Graph Wavenet, a Spatio-Temporal
neural network that is trained to estimate a density given the measurements of
previous timesteps, conditioned on a quantile. Our method of density estimation
is fully parameterised by our neural network and does not use a likelihood
approximation internally. The quantile loss function is asymmetric and this
makes it possible to model skewed densities. This approach produces uncertainty
estimates without the need to sample during inference, such as in Monte Carlo
Dropout, which makes our method also efficient.
</p>
<a href="http://arxiv.org/abs/2012.05207" target="_blank">arXiv:2012.05207</a> [<a href="http://arxiv.org/pdf/2012.05207" target="_blank">pdf</a>]

<h2>Data embedding and prediction by sparse tropical matrix factorization. (arXiv:2012.05210v1 [cs.LG])</h2>
<h3>Amra Omanovi&#x107;, Hilal Kazan, Polona Oblak, Toma&#x17e; Curk</h3>
<p>Matrix factorization methods are linear models, with limited capability to
model complex relations. In our work, we use tropical semiring to introduce
non-linearity into matrix factorization models. We propose a method called
Sparse Tropical Matrix Factorization (STMF) for the estimation of missing
(unknown) values. We evaluate the efficiency of the STMF method on both
synthetic data and biological data in the form of gene expression measurements
downloaded from The Cancer Genome Atlas (TCGA) database. Tests on unique
synthetic data showed that STMF approximation achieves a higher correlation
than non-negative matrix factorization (NMF), which is unable to recover
patterns effectively. On real data, STMF outperforms NMF on six out of nine
gene expression datasets. While NMF assumes normal distribution and tends
toward the mean value, STMF can better fit to extreme values and distributions.
STMF is the first work that uses tropical semiring on sparse data. We show that
in certain cases semirings are useful because they consider the structure,
which is different and simpler to understand than it is with standard linear
algebra.
</p>
<a href="http://arxiv.org/abs/2012.05210" target="_blank">arXiv:2012.05210</a> [<a href="http://arxiv.org/pdf/2012.05210" target="_blank">pdf</a>]

<h2>E3D: Event-Based 3D Shape Reconstruction. (arXiv:2012.05214v1 [cs.CV])</h2>
<h3>Alexis Baudron, Winston Wang, Oliver Cossairt, Aggelos Katsaggelos</h3>
<p>3D shape reconstruction is a primary component of augmented/virtual reality.
Despite being highly advanced, existing solutions based on RGB, RGB-D and Lidar
sensors are power and data intensive, which introduces challenges for
deployment in edge devices. We approach 3D reconstruction with an event camera,
a sensor with significantly lower power, latency and data expense while
enabling high dynamic range. While previous event-based 3D reconstruction
methods are primarily based on stereo vision, we cast the problem as multi-view
shape from silhouette using a monocular event camera. The output from a moving
event camera is a sparse point set of space-time gradients, largely sketching
scene/object edges and contours. We first introduce an event-to-silhouette
(E2S) neural network module to transform a stack of event frames to the
corresponding silhouettes, with additional neural branches for camera pose
regression. Second, we introduce E3D, which employs a 3D differentiable
renderer (PyTorch3D) to enforce cross-view 3D mesh consistency and fine-tune
the E2S and pose network. Lastly, we introduce a 3D-to-events simulation
pipeline and apply it to publicly available object datasets and generate
synthetic event/silhouette training pairs for supervised learning.
</p>
<a href="http://arxiv.org/abs/2012.05214" target="_blank">arXiv:2012.05214</a> [<a href="http://arxiv.org/pdf/2012.05214" target="_blank">pdf</a>]

<h2>Positional Encoding as Spatial Inductive Bias in GANs. (arXiv:2012.05217v1 [cs.CV])</h2>
<h3>Rui Xu, Xintao Wang, Kai Chen, Bolei Zhou, Chen Change Loy</h3>
<p>SinGAN shows impressive capability in learning internal patch distribution
despite its limited effective receptive field. We are interested in knowing how
such a translation-invariant convolutional generator could capture the global
structure with just a spatially i.i.d. input. In this work, taking SinGAN and
StyleGAN2 as examples, we show that such capability, to a large extent, is
brought by the implicit positional encoding when using zero padding in the
generators. Such positional encoding is indispensable for generating images
with high fidelity. The same phenomenon is observed in other generative
architectures such as DCGAN and PGGAN. We further show that zero padding leads
to an unbalanced spatial bias with a vague relation between locations. To offer
a better spatial inductive bias, we investigate alternative positional
encodings and analyze their effects. Based on a more flexible positional
encoding explicitly, we propose a new multi-scale training strategy and
demonstrate its effectiveness in the state-of-the-art unconditional generator
StyleGAN2. Besides, the explicit spatial inductive bias substantially improve
SinGAN for more versatile image manipulation.
</p>
<a href="http://arxiv.org/abs/2012.05217" target="_blank">arXiv:2012.05217</a> [<a href="http://arxiv.org/pdf/2012.05217" target="_blank">pdf</a>]

<h2>MorphNet: One-Shot Face Synthesis GAN for Detecting Recognition Bias. (arXiv:2012.05225v1 [cs.CV])</h2>
<h3>Nataniel Ruiz, Barry-John Theobald, Anurag Ranjan, Ahmed Hussein Abdelaziz, Nicholas Apostoloff</h3>
<p>To detect bias in face recognition networks, it can be useful to probe a
network under test using samples in which only specific attributes vary in some
controlled way. However, capturing a sufficiently large dataset with specific
control over the attributes of interest is difficult. In this work, we describe
a simulator that applies specific head pose and facial expression adjustments
to images of previously unseen people. The simulator first fits a 3D morphable
model to a provided image, applies the desired head pose and facial expression
controls, then renders the model into an image. Next, a conditional Generative
Adversarial Network (GAN) conditioned on the original image and the rendered
morphable model is used to produce the image of the original person with the
new facial expression and head pose. We call this conditional GAN -- MorphNet.
Images generated using MorphNet conserve the identity of the person in the
original image, and the provided control over head pose and facial expression
allows test sets to be created to identify robustness issues of a facial
recognition deep network with respect to pose and expression. Images generated
by MorphNet can also serve as data augmentation when training data are scarce.
We show that by augmenting small datasets of faces with new poses and
expressions improves the recognition performance by up to 9% depending on the
augmentation and data scarcity.
</p>
<a href="http://arxiv.org/abs/2012.05225" target="_blank">arXiv:2012.05225</a> [<a href="http://arxiv.org/pdf/2012.05225" target="_blank">pdf</a>]

<h2>Video Deblurring by Fitting to Test Data. (arXiv:2012.05228v1 [cs.CV])</h2>
<h3>Xuanchi Ren, Zian Qian, Qifeng Chen</h3>
<p>We present a novel approach to video deblurring by fitting a deep network to
the test video. One key observation is that some frames in a video with motion
blur are much sharper than others, and thus we can transfer the texture
information in those sharp frames to blurry frames. Our approach heuristically
selects sharp frames from a video and then trains a convolutional neural
network on these sharp frames. The trained network often absorbs enough details
in the scene to perform deblurring on all the video frames. As an internal
learning method, our approach has no domain gap between training and test data,
which is a problematic issue for existing video deblurring approaches. The
conducted experiments on real-world video data show that our model can
reconstruct clearer and sharper videos than state-of-the-art video deblurring
approaches. Code and data are available at
https://github.com/xrenaa/Deblur-by-Fitting.
</p>
<a href="http://arxiv.org/abs/2012.05228" target="_blank">arXiv:2012.05228</a> [<a href="http://arxiv.org/pdf/2012.05228" target="_blank">pdf</a>]

<h2>Rectifier Neural Network with a Dual-Pathway Architecture for Image Denoising. (arXiv:1609.03024v3 [cs.CV] UPDATED)</h2>
<h3>Keting Zhang, Liqing Zhang</h3>
<p>Recently deep neural networks based on tanh activation function have shown
their impressive power in image denoising. In this letter, we try to use
rectifier function instead of tanh and propose a dual-pathway rectifier neural
network by combining two rectifier neurons with reversed input and output
weights in the same hidden layer. We drive the equivalent activation function
and compare it to some typical activation functions for image denoising under
the same network architecture. The experimental results show that our model
achieves superior performances faster especially when the noise is small.
</p>
<a href="http://arxiv.org/abs/1609.03024" target="_blank">arXiv:1609.03024</a> [<a href="http://arxiv.org/pdf/1609.03024" target="_blank">pdf</a>]

<h2>Decoupled Classification Refinement: Hard False Positive Suppression for Object Detection. (arXiv:1810.04002v2 [cs.CV] UPDATED)</h2>
<h3>Bowen Cheng, Yunchao Wei, Rogerio Feris, Jinjun Xiong, Wen-mei Hwu, Thomas Huang, Humphrey Shi</h3>
<p>In this paper, we analyze failure cases of state-of-the-art detectors and
observe that most hard false positives result from classification instead of
localization and they have a large negative impact on the performance of object
detectors. We conjecture there are three factors: (1) Shared feature
representation is not optimal due to the mismatched goals of feature learning
for classification and localization; (2) multi-task learning helps, yet
optimization of the multi-task loss may result in sub-optimal for individual
tasks; (3) large receptive field for different scales leads to redundant
context information for small objects. We demonstrate the potential of detector
classification power by a simple, effective, and widely-applicable Decoupled
Classification Refinement (DCR) network. In particular, DCR places a separate
classification network in parallel with the localization network (base
detector). With ROI Pooling placed on the early stage of the classification
network, we enforce an adaptive receptive field in DCR. During training, DCR
samples hard false positives from the base detector and trains a strong
classifier to refine classification results. During testing, DCR refines all
boxes from the base detector. Experiments show competitive results on PASCAL
VOC and COCO without any bells and whistles. Our codes are available at:
https://github.com/bowenc0221/Decoupled-Classification-Refinement.
</p>
<a href="http://arxiv.org/abs/1810.04002" target="_blank">arXiv:1810.04002</a> [<a href="http://arxiv.org/pdf/1810.04002" target="_blank">pdf</a>]

<h2>Differentially Private ADMM for Distributed Medical Machine Learning. (arXiv:1901.02094v3 [cs.LG] UPDATED)</h2>
<h3>Jiahao Ding, Xiaoqi Qin, Wenjun Xu, Yanmin Gong, Chi Zhang, Miao Pan</h3>
<p>Due to massive amounts of data distributed across multiple locations,
distributed machine learning has attracted a lot of research interests.
Alternating Direction Method of Multipliers (ADMM) is a powerful method of
designing distributed machine learning algorithm, whereby each agent computes
over local datasets and exchanges computation results with its neighbor agents
in an iterative procedure. There exists significant privacy leakage during this
iterative process if the local data is sensitive. In this paper, we propose a
differentially private ADMM algorithm (P-ADMM) to provide dynamic
zero-concentrated differential privacy (dynamic zCDP), by inserting Gaussian
noise with linearly decaying variance. We prove that P-ADMM has the same
convergence rate compared to the non-private counterpart, i.e.,
$\mathcal{O}(1/K)$ with $K$ being the number of iterations and linear
convergence for general convex and strongly convex problems while providing
differentially private guarantee. Moreover, through our experiments performed
on real-world datasets, we empirically show that P-ADMM has the best-known
performance among the existing differentially private ADMM based algorithms.
</p>
<a href="http://arxiv.org/abs/1901.02094" target="_blank">arXiv:1901.02094</a> [<a href="http://arxiv.org/pdf/1901.02094" target="_blank">pdf</a>]

<h2>Deep Mixtures of Unigrams for uncovering Topics in Textual Data. (arXiv:1902.06615v2 [stat.ML] UPDATED)</h2>
<h3>Cinzia Viroli, Laura Anderlucci</h3>
<p>Mixtures of Unigrams are one of the simplest and most efficient tools for
clustering textual data, as they assume that documents related to the same
topic have similar distributions of terms, naturally described by Multinomials.
When the classification task is particularly challenging, such as when the
document-term matrix is high-dimensional and extremely sparse, a more composite
representation can provide better insight on the grouping structure. In this
work, we developed a deep version of mixtures of Unigrams for the unsupervised
classification of very short documents with a large number of terms, by
allowing for models with further deeper latent layers; the proposal is derived
in a Bayesian framework. The behaviour of the Deep Mixtures of Unigrams is
empirically compared with that of other traditional and state-of-the-art
methods, namely $k$-means with cosine distance, $k$-means with Euclidean
distance on data transformed according to Semantic Analysis, Partition Around
Medoids, Mixture of Gaussians on semantic-based transformed data, hierarchical
clustering according to Ward's method with cosine dissimilarity, Latent
Dirichlet Allocation, Mixtures of Unigrams estimated via the EM algorithm,
Spectral Clustering and Affinity Propagation clustering. The performance is
evaluated in terms of both correct classification rate and Adjusted Rand Index.
Simulation studies and real data analysis prove that going deep in clustering
such data highly improves the classification accuracy.
</p>
<a href="http://arxiv.org/abs/1902.06615" target="_blank">arXiv:1902.06615</a> [<a href="http://arxiv.org/pdf/1902.06615" target="_blank">pdf</a>]

<h2>Quantum-Inspired Support Vector Machine. (arXiv:1906.08902v3 [cs.LG] UPDATED)</h2>
<h3>Chen Ding, Tian-Yi Bao, He-Liang Huang</h3>
<p>Support vector machine (SVM) is a particularly powerful and flexible
supervised learning model that analyzes data for both classification and
regression, whose usual algorithm complexity scales polynomially with the
dimension of data space and the number of data points. To tackle the big data
challenge, a quantum SVM algorithm was proposed, which is claimed to achieve
exponential speedup for least squares SVM (LS-SVM). Here, inspired by the
quantum SVM algorithm, we present a quantum-inspired classical algorithm for
LS-SVM. In our approach, a improved fast sampling technique, namely indirect
sampling, is proposed for sampling the kernel matrix and classifying. We first
consider the LS-SVM with a linear kernel, and then discuss the generalization
of our method to non-linear kernels. Theoretical analysis shows our algorithm
can make classification with arbitrary success probability in logarithmic
runtime of both the dimension of data space and the number of data points for
low rank, low condition number and high dimensional data matrix, matching the
runtime of the quantum SVM.
</p>
<a href="http://arxiv.org/abs/1906.08902" target="_blank">arXiv:1906.08902</a> [<a href="http://arxiv.org/pdf/1906.08902" target="_blank">pdf</a>]

<h2>Learning Activation Functions: A new paradigm for understanding Neural Networks. (arXiv:1906.09529v3 [cs.LG] UPDATED)</h2>
<h3>Mohit Goyal, Rajan Goyal, Brejesh Lall</h3>
<p>The scope of research in the domain of activation functions remains limited
and centered around improving the ease of optimization or generalization
quality of neural networks (NNs). However, to develop a deeper understanding of
deep learning, it becomes important to look at the non linear component of NNs
more carefully. In this paper, we aim to provide a generic form of activation
function along with appropriate mathematical grounding so as to allow for
insights into the working of NNs in future. We propose "Self-Learnable
Activation Functions" (SLAF), which are learned during training and are capable
of approximating most of the existing activation functions. SLAF is given as a
weighted sum of pre-defined basis elements which can serve for a good
approximation of the optimal activation function. The coefficients for these
basis elements allow a search in the entire space of continuous functions
(consisting of all the conventional activations). We propose various training
routines which can be used to achieve performance with SLAF equipped neural
networks (SLNNs). We prove that SLNNs can approximate any neural network with
lipschitz continuous activations, to any arbitrary error highlighting their
capacity and possible equivalence with standard NNs. Also, SLNNs can be
completely represented as a collections of finite degree polynomial upto the
very last layer obviating several hyper parameters like width and depth. Since
the optimization of SLNNs is still a challenge, we show that using SLAF along
with standard activations (like ReLU) can provide performance improvements with
only a small increase in number of parameters.
</p>
<a href="http://arxiv.org/abs/1906.09529" target="_blank">arXiv:1906.09529</a> [<a href="http://arxiv.org/pdf/1906.09529" target="_blank">pdf</a>]

<h2>TabNet: Attentive Interpretable Tabular Learning. (arXiv:1908.07442v5 [cs.LG] UPDATED)</h2>
<h3>Sercan O. Arik, Tomas Pfister</h3>
<p>We propose a novel high-performance and interpretable canonical deep tabular
data learning architecture, TabNet. TabNet uses sequential attention to choose
which features to reason from at each decision step, enabling interpretability
and more efficient learning as the learning capacity is used for the most
salient features. We demonstrate that TabNet outperforms other neural network
and decision tree variants on a wide range of non-performance-saturated tabular
datasets and yields interpretable feature attributions plus insights into the
global model behavior. Finally, for the first time to our knowledge, we
demonstrate self-supervised learning for tabular data, significantly improving
performance with unsupervised representation learning when unlabeled data is
abundant.
</p>
<a href="http://arxiv.org/abs/1908.07442" target="_blank">arXiv:1908.07442</a> [<a href="http://arxiv.org/pdf/1908.07442" target="_blank">pdf</a>]

<h2>Confederated Machine Learning on Horizontally and Vertically Separated Medical Data for Large-Scale Health System Intelligence. (arXiv:1910.02109v3 [cs.LG] UPDATED)</h2>
<h3>Dianbo Liu, Kathe Fox, Griffin Weber, Tim Miller</h3>
<p>Health information is generally fragmented across silos. Though it is
technically feasible to unite data for analysis in a manner that underpins a
rapid learning healthcare system, privacy concerns and regulatory barriers
limit data centralization. Machine learning can be conducted in a federated
manner on patient datasets with the same set of variables, but separated across
sites of care. But federated learning cannot handle the situation where
different data types for a given patient are separated vertically across
different organizations and when patient ID matching across different
institutions is difficult. We call methods that enable machine learning model
training on data separated by two or more degrees confederated machine
learning. We proposed and evaluated a confederated learning to training machine
learning model to stratify the risk of several diseases among when data are
horizontally separated by individual, vertically separated by data type, and
separated by identity without patient ID matching.
</p>
<a href="http://arxiv.org/abs/1910.02109" target="_blank">arXiv:1910.02109</a> [<a href="http://arxiv.org/pdf/1910.02109" target="_blank">pdf</a>]

<h2>A Joint Model for Anomaly Detection and Trend Prediction on IT Operation Series. (arXiv:1910.03818v4 [cs.LG] UPDATED)</h2>
<h3>Run-Qing Chen, Guang-Hui Shi, Wan-Lei Zhao, Chang-Hui Liang</h3>
<p>Anomaly detection and trend prediction are two fundamental tasks in automatic
IT systems monitoring. In this paper, a joint model Anomaly Detector &amp; Trend
Predictor (ADTP) is proposed. In our design, the variational auto-encoder (VAE)
and long short-term memory (LSTM) are joined together to address both anomaly
detection and trend prediction. The prediction block (LSTM) takes clean input
from the reconstructed time series by VAE, which makes it robust to the
anomalies and noise. In the meantime, the LSTM block maintains the long-term
sequential patterns, which are out of the sight of a VAE encoding window. This
leads to the better performance of VAE in anomaly detection than it is trained
alone. In the whole processing pipeline, the spectral residual analysis is
integrated with VAE and LSTM to boost the performance of both. The superior
performance on two tasks is confirmed with the experiments on two challenging
evaluation benchmarks.
</p>
<a href="http://arxiv.org/abs/1910.03818" target="_blank">arXiv:1910.03818</a> [<a href="http://arxiv.org/pdf/1910.03818" target="_blank">pdf</a>]

<h2>Deep Independently Recurrent Neural Network (IndRNN). (arXiv:1910.06251v3 [cs.CV] UPDATED)</h2>
<h3>Shuai Li, Wanqing Li, Chris Cook, Yanbo Gao</h3>
<p>Recurrent neural networks (RNNs) are known to be difficult to train due to
the gradient vanishing and exploding problems and thus difficult to learn
long-term patterns and construct deep networks. To address these problems, this
paper proposes a new type of RNNs with the recurrent connection formulated as
Hadamard product, referred to as independently recurrent neural network
(IndRNN), where neurons in the same layer are independent of each other and
connected across layers. Due to the better behaved gradient backpropagation,
IndRNN with regulated recurrent weights effectively addresses the gradient
vanishing and exploding problems and thus long-term dependencies can be
learned. Moreover, an IndRNN can work with non-saturated activation functions
such as ReLU (rectified linear unit) and be still trained robustly. Different
deeper IndRNN architectures, including the basic stacked IndRNN, residual
IndRNN and densely connected IndRNN, have been investigated, all of which can
be much deeper than the existing RNNs. Furthermore, IndRNN reduces the
computation at each time step and can be over 10 times faster than the commonly
used Long short-term memory (LSTM). Experimental results have shown that the
proposed IndRNN is able to process very long sequences and construct very deep
networks. Better performance has been achieved on various tasks with IndRNNs
compared with the traditional RNN, LSTM and the popular Transformer.
</p>
<a href="http://arxiv.org/abs/1910.06251" target="_blank">arXiv:1910.06251</a> [<a href="http://arxiv.org/pdf/1910.06251" target="_blank">pdf</a>]

<h2>BoTorch: A Framework for Efficient Monte-Carlo Bayesian Optimization. (arXiv:1910.06403v3 [cs.LG] UPDATED)</h2>
<h3>Maximilian Balandat, Brian Karrer, Daniel R. Jiang, Samuel Daulton, Benjamin Letham, Andrew Gordon Wilson, Eytan Bakshy</h3>
<p>Bayesian optimization provides sample-efficient global optimization for a
broad range of applications, including automatic machine learning, engineering,
physics, and experimental design. We introduce BoTorch, a modern programming
framework for Bayesian optimization that combines Monte-Carlo (MC) acquisition
functions, a novel sample average approximation optimization approach,
auto-differentiation, and variance reduction techniques. BoTorch's modular
design facilitates flexible specification and optimization of probabilistic
models written in PyTorch, simplifying implementation of new acquisition
functions. Our approach is backed by novel theoretical convergence results and
made practical by a distinctive algorithmic foundation that leverages fast
predictive distributions, hardware acceleration, and deterministic
optimization. We also propose a novel "one-shot" formulation of the Knowledge
Gradient, enabled by a combination of our theoretical and software
contributions. In experiments, we demonstrate the improved sample efficiency of
BoTorch relative to other popular libraries.
</p>
<a href="http://arxiv.org/abs/1910.06403" target="_blank">arXiv:1910.06403</a> [<a href="http://arxiv.org/pdf/1910.06403" target="_blank">pdf</a>]

<h2>Embedding and learning with signatures. (arXiv:1911.13211v3 [stat.ML] UPDATED)</h2>
<h3>Adeline Fermanian</h3>
<p>Sequential and temporal data arise in many fields of research, such as
quantitative finance, medicine, or computer vision. A novel approach for
sequential learning, called the signature method and rooted in rough path
theory, is considered. Its basic principle is to represent multidimensional
paths by a graded feature set of their iterated integrals, called the
signature. This approach relies critically on an embedding principle, which
consists in representing discretely sampled data as paths, i.e., functions from
$[0,1]$ to $\mathbb{R}^d$. After a survey of machine learning methodologies for
signatures, the influence of embeddings on prediction accuracy is investigated
with an in-depth study of three recent and challenging datasets. It is shown
that a specific embedding, called lead-lag, is systematically the strongest
performer across all datasets and algorithms considered. Moreover, an empirical
study reveals that computing signatures over the whole path domain does not
lead to a loss of local information. It is concluded that, with a good
embedding, combining signatures with other simple algorithms achieves results
competitive with state-of-the-art, domain-specific approaches.
</p>
<a href="http://arxiv.org/abs/1911.13211" target="_blank">arXiv:1911.13211</a> [<a href="http://arxiv.org/pdf/1911.13211" target="_blank">pdf</a>]

<h2>GLIB: Efficient Exploration for Relational Model-Based Reinforcement Learning via Goal-Literal Babbling. (arXiv:2001.08299v3 [cs.AI] UPDATED)</h2>
<h3>Rohan Chitnis, Tom Silver, Joshua Tenenbaum, Leslie Pack Kaelbling, Tomas Lozano-Perez</h3>
<p>We address the problem of efficient exploration for transition model learning
in the relational model-based reinforcement learning setting without extrinsic
goals or rewards. Inspired by human curiosity, we propose goal-literal babbling
(GLIB), a simple and general method for exploration in such problems. GLIB
samples relational conjunctive goals that can be understood as specific,
targeted effects that the agent would like to achieve in the world, and plans
to achieve these goals using the transition model being learned. We provide
theoretical guarantees showing that exploration with GLIB will converge almost
surely to the ground truth model. Experimentally, we find GLIB to strongly
outperform existing methods in both prediction and planning on a range of
tasks, encompassing standard PDDL and PPDDL planning benchmarks and a robotic
manipulation task implemented in the PyBullet physics simulator. Video:
https://youtu.be/F6lmrPT6TOY Code: https://git.io/JIsTB
</p>
<a href="http://arxiv.org/abs/2001.08299" target="_blank">arXiv:2001.08299</a> [<a href="http://arxiv.org/pdf/2001.08299" target="_blank">pdf</a>]

<h2>NLocalSAT: Boosting Local Search with Solution Prediction. (arXiv:2001.09398v4 [cs.AI] UPDATED)</h2>
<h3>Wenjie Zhang, Zeyu Sun, Qihao Zhu, Ge Li, Shaowei Cai, Yingfei Xiong, Lu Zhang</h3>
<p>The Boolean satisfiability problem (SAT) is a famous NP-complete problem in
computer science. An effective way for solving a satisfiable SAT problem is the
stochastic local search (SLS). However, in this method, the initialization is
assigned in a random manner, which impacts the effectiveness of SLS solvers. To
address this problem, we propose NLocalSAT. NLocalSAT combines SLS with a
solution prediction model, which boosts SLS by changing initialization
assignments with a neural network. We evaluated NLocalSAT on five SLS solvers
(CCAnr, Sparrow, CPSparrow, YalSAT, and probSAT) with instances in the random
track of SAT Competition 2018. The experimental results show that solvers with
NLocalSAT achieve 27% ~ 62% improvement over the original SLS solvers.
</p>
<a href="http://arxiv.org/abs/2001.09398" target="_blank">arXiv:2001.09398</a> [<a href="http://arxiv.org/pdf/2001.09398" target="_blank">pdf</a>]

<h2>Stochastic tree ensembles for regularized nonlinear regression. (arXiv:2002.03375v2 [stat.ML] UPDATED)</h2>
<h3>Jingyu He, P. Richard Hahn</h3>
<p>This paper develops a novel stochastic tree ensemble method for nonlinear
regression, referred to as Accelerated Bayesian Additive Regression Trees, or
XBART. By combining regularization and stochastic search strategies from
Bayesian modeling with computationally efficient techniques from recursive
partitioning algorithms, XBART attains state-of-the-art performance at
prediction and function estimation. Simulation studies demonstrate that XBART
provides accurate point-wise estimates of the mean function and does so faster
than popular alternatives, such as BART, XGBoost, and neural networks (using
Keras) on a variety of test functions.

Additionally, it is demonstrated that using XBART to initialize the standard
BART MCMC algorithm considerably improves credible interval coverage and
reduces total run-time.

Finally, three basic theoretical results are established: 1) the single tree
version of the model is asymptotically consistent, 2) samples obtained from the
single-tree version of the algorithm correspond to posterior samples under a
particular likelihood and prior specification, and 3) the Markov chain produced
by the ensemble version of the algorithm has a unique stationary distribution.
</p>
<a href="http://arxiv.org/abs/2002.03375" target="_blank">arXiv:2002.03375</a> [<a href="http://arxiv.org/pdf/2002.03375" target="_blank">pdf</a>]

<h2>Minimum adjusted Rand index for two clusterings of a given size. (arXiv:2002.03677v3 [stat.ML] UPDATED)</h2>
<h3>Jos&#xe9; E. Chac&#xf3;n, Ana I. Rastrojo</h3>
<p>The adjusted Rand index (ARI) is commonly used in cluster analysis to measure
the degree of agreement between two data partitions. Since its introduction,
exploring the situations of extreme agreement and disagreement under different
circumstances has been a subject of interest, in order to achieve a better
understanding of this index. Here, an explicit formula for the lowest possible
value of the ARI for two clusterings of given sizes is shown, and moreover a
specific pair of clusterings achieving such a bound is provided.
</p>
<a href="http://arxiv.org/abs/2002.03677" target="_blank">arXiv:2002.03677</a> [<a href="http://arxiv.org/pdf/2002.03677" target="_blank">pdf</a>]

<h2>Deep Fourier Kernel for Self-Attentive Point Processes. (arXiv:2002.07281v4 [stat.ML] UPDATED)</h2>
<h3>Shixiang Zhu, Minghe Zhang, Ruyi Ding, Yao Xie</h3>
<p>We present a novel attention-based model for discrete event data to capture
complex non-linear temporal dependence structure. We borrow the idea from the
attention mechanism and incorporate it into the conditional intensity function
of the point processes. We further introduce a novel score function using
Fourier kernel embedding, whose spectrum is represented using neural networks,
which drastically differs from the traditional dot-product kernel and can
capture a more complex similarity structure. We establish the theoretical
properties of our approach and demonstrate our approach's competitive
performance compared to the state-of-the-art for synthetic and real data.
</p>
<a href="http://arxiv.org/abs/2002.07281" target="_blank">arXiv:2002.07281</a> [<a href="http://arxiv.org/pdf/2002.07281" target="_blank">pdf</a>]

<h2>Uncertainty in Structured Prediction. (arXiv:2002.07650v4 [stat.ML] UPDATED)</h2>
<h3>Andrey Malinin, Mark Gales</h3>
<p>Uncertainty estimation is important for ensuring safety and robustness of AI
systems. While most research in the area has focused on un-structured
prediction tasks, limited work has investigated general uncertainty estimation
approaches for structured prediction. Thus, this work aims to investigate
uncertainty estimation for structured prediction tasks within a single unified
and interpretable probabilistic ensemble-based framework. We consider:
uncertainty estimation for sequence data at the token-level and complete
sequence-level; interpretations for, and applications of, various measures of
uncertainty; and discuss both the theoretical and practical challenges
associated with obtaining them. This work also provides baselines for
token-level and sequence-level error detection, and sequence-level
out-of-domain input detection on the WMT14 English-French and WMT17
English-German translation and LibriSpeech speech recognition datasets.
</p>
<a href="http://arxiv.org/abs/2002.07650" target="_blank">arXiv:2002.07650</a> [<a href="http://arxiv.org/pdf/2002.07650" target="_blank">pdf</a>]

<h2>Batch Normalization Biases Residual Blocks Towards the Identity Function in Deep Networks. (arXiv:2002.10444v3 [cs.LG] UPDATED)</h2>
<h3>Soham De, Samuel L. Smith</h3>
<p>Batch normalization dramatically increases the largest trainable depth of
residual networks, and this benefit has been crucial to the empirical success
of deep residual networks on a wide range of benchmarks. We show that this key
benefit arises because, at initialization, batch normalization downscales the
residual branch relative to the skip connection, by a normalizing factor on the
order of the square root of the network depth. This ensures that, early in
training, the function computed by normalized residual blocks in deep networks
is close to the identity function (on average). We use this insight to develop
a simple initialization scheme that can train deep residual networks without
normalization. We also provide a detailed empirical study of residual networks,
which clarifies that, although batch normalized networks can be trained with
larger learning rates, this effect is only beneficial in specific compute
regimes, and has minimal benefits when the batch size is small.
</p>
<a href="http://arxiv.org/abs/2002.10444" target="_blank">arXiv:2002.10444</a> [<a href="http://arxiv.org/pdf/2002.10444" target="_blank">pdf</a>]

<h2>(De)Randomized Smoothing for Certifiable Defense against Patch Attacks. (arXiv:2002.10733v2 [cs.LG] UPDATED)</h2>
<h3>Alexander Levine, Soheil Feizi</h3>
<p>Patch adversarial attacks on images, in which the attacker can distort pixels
within a region of bounded size, are an important threat model since they
provide a quantitative model for physical adversarial attacks. In this paper,
we introduce a certifiable defense against patch attacks that guarantees for a
given image and patch attack size, no patch adversarial examples exist. Our
method is related to the broad class of randomized smoothing robustness schemes
which provide high-confidence probabilistic robustness certificates. By
exploiting the fact that patch attacks are more constrained than general sparse
attacks, we derive meaningfully large robustness certificates against them.
Additionally, in contrast to smoothing-based defenses against L_p and sparse
attacks, our defense method against patch attacks is de-randomized, yielding
improved, deterministic certificates. Compared to the existing patch
certification method proposed by Chiang et al. (2020), which relies on interval
bound propagation, our method can be trained significantly faster, achieves
high clean and certified robust accuracy on CIFAR-10, and provides certificates
at ImageNet scale. For example, for a 5-by-5 patch attack on CIFAR-10, our
method achieves up to around 57.6% certified accuracy (with a classifier with
around 83.8% clean accuracy), compared to at most 30.3% certified accuracy for
the existing method (with a classifier with around 47.8% clean accuracy). Our
results effectively establish a new state-of-the-art of certifiable defense
against patch attacks on CIFAR-10 and ImageNet. Code is available at
https://github.com/alevine0/patchSmoothing.
</p>
<a href="http://arxiv.org/abs/2002.10733" target="_blank">arXiv:2002.10733</a> [<a href="http://arxiv.org/pdf/2002.10733" target="_blank">pdf</a>]

<h2>Diversity inducing Information Bottleneck in Model Ensembles. (arXiv:2003.04514v3 [cs.LG] UPDATED)</h2>
<h3>Samarth Sinha, Homanga Bharadhwaj, Anirudh Goyal, Hugo Larochelle, Animesh Garg, Florian Shkurti</h3>
<p>Although deep learning models have achieved state-of-the-art performance on a
number of vision tasks, generalization over high dimensional multi-modal data,
and reliable predictive uncertainty estimation are still active areas of
research. Bayesian approaches including Bayesian Neural Nets (BNNs) do not
scale well to modern computer vision tasks, as they are difficult to train, and
have poor generalization under dataset-shift. This motivates the need for
effective ensembles which can generalize and give reliable uncertainty
estimates. In this paper, we target the problem of generating effective
ensembles of neural networks by encouraging diversity in prediction. We
explicitly optimize a diversity inducing adversarial loss for learning the
stochastic latent variables and thereby obtain diversity in the output
predictions necessary for modeling multi-modal data. We evaluate our method on
benchmark datasets: MNIST, CIFAR100, TinyImageNet and MIT Places 2, and
compared to the most competitive baselines show significant improvements in
classification accuracy, under a shift in the data distribution and in
out-of-distribution detection. Code will be released in this url
https://github.com/rvl-lab-utoronto/dibs
</p>
<a href="http://arxiv.org/abs/2003.04514" target="_blank">arXiv:2003.04514</a> [<a href="http://arxiv.org/pdf/2003.04514" target="_blank">pdf</a>]

<h2>Generalizable Pedestrian Detection: The Elephant In The Room. (arXiv:2003.08799v7 [cs.CV] UPDATED)</h2>
<h3>Irtiza Hasan, Shengcai Liao, Jinpeng Li, Saad Ullah Akram, Ling Shao</h3>
<p>Pedestrian detection is used in many vision based applications ranging from
video surveillance to autonomous driving. Despite achieving high performance,
it is still largely unknown how well existing detectors generalize to unseen
data. This is important because a practical detector should be ready to use in
various scenarios in applications. To this end, we conduct a comprehensive
study in this paper, using a general principle of direct cross-dataset
evaluation. Through this study, we find that existing state-of-the-art
pedestrian detectors, though perform quite well when trained and tested on the
same dataset, generalize poorly in cross dataset evaluation. We demonstrate
that there are two reasons for this trend. Firstly, their designs (e.g. anchor
settings) may be biased towards popular benchmarks in the traditional
single-dataset training and test pipeline, but as a result largely limit their
generalization capability. Secondly, the training source is generally not dense
in pedestrians and diverse in scenarios. Under direct cross-dataset evaluation,
surprisingly, we find that a general purpose object detector, without
pedestrian-tailored adaptation in design, generalizes much better compared to
existing state-of-the-art pedestrian detectors. Furthermore, we illustrate that
diverse and dense datasets, collected by crawling the web, serve to be an
efficient source of pre-training for pedestrian detection. Accordingly, we
propose a progressive training pipeline and find that it works well for
autonomous-driving oriented pedestrian detection. Consequently, the study
conducted in this paper suggests that more emphasis should be put on
cross-dataset evaluation for the future design of generalizable pedestrian
detectors. Code and models can be accessed at
https://github.com/hasanirtiza/Pedestron.
</p>
<a href="http://arxiv.org/abs/2003.08799" target="_blank">arXiv:2003.08799</a> [<a href="http://arxiv.org/pdf/2003.08799" target="_blank">pdf</a>]

<h2>Sublinear Regret and Belief Complexity in Gaussian Process Bandits via Information Thresholding. (arXiv:2003.10550v2 [cs.LG] UPDATED)</h2>
<h3>Amrit Singh Bedi, Dheeraj Peddireddy, Vaneet Aggarwal, Alec Koppel</h3>
<p>Bayesian optimization is a framework for global search via maximum a
posteriori updates rather than simulated annealing, and has gained prominence
for decision-making under uncertainty. In this work, we cast Bayesian
optimization as a multi-armed bandit problem, where the payoff function is
sampled from a Gaussian process (GP). Further, we focus on action selections
via upper confidence bound (UCB) or expected improvement (EI) due to their
prevalent use in practice. Prior works using GPs for bandits cannot allow the
iteration horizon $T$ to be large, as the complexity of computing the posterior
parameters scales cubically with the number of past observations. To circumvent
this computational burden, we propose a simple statistical test: only
incorporate an action into the GP posterior when its conditional entropy
exceeds an $\epsilon$ threshold. Doing so permits us to derive sublinear regret
bounds of GP bandit algorithms up to factors depending on the compression
parameter $\epsilon$ for both discrete and continuous action sets. Moreover,
the complexity of the GP posterior remains provably finite and depends on the
Shannon capacity of the observation space. Experimentally, we observe state of
the art accuracy and complexity tradeoffs for GP bandit algorithms applied to
global optimization, suggesting the merits of compressed GPs in bandit
settings.
</p>
<a href="http://arxiv.org/abs/2003.10550" target="_blank">arXiv:2003.10550</a> [<a href="http://arxiv.org/pdf/2003.10550" target="_blank">pdf</a>]

<h2>MiniSeg: An Extremely Minimum Network for Efficient COVID-19 Segmentation. (arXiv:2004.09750v2 [cs.CV] UPDATED)</h2>
<h3>Yu Qiu, Yun Liu, Shijie Li, Jing Xu</h3>
<p>The rapid spread of the new pandemic, i.e., COVID-19, has severely threatened
global health. Deep-learning-based computer-aided screening, e.g., COVID-19
infected CT area segmentation, has attracted much attention. However, the
publicly available COVID-19 training data are limited, easily causing
overfitting for traditional deep learning methods that are usually data-hungry
with millions of parameters. On the other hand, fast training/testing and low
computational cost are also necessary for quick deployment and development of
COVID-19 screening systems, but traditional deep learning methods are usually
computationally intensive. To address the above problems, we propose MiniSeg, a
lightweight deep learning model for efficient COVID-19 segmentation. Compared
with traditional segmentation methods, MiniSeg has several significant
strengths: i) it only has 83K parameters and is thus not easy to overfit; ii)
it has high computational efficiency and is thus convenient for practical
deployment; iii) it can be fast retrained by other users using their private
COVID-19 data for further improving performance. In addition, we build a
comprehensive COVID-19 segmentation benchmark for comparing MiniSeg to
traditional methods.
</p>
<a href="http://arxiv.org/abs/2004.09750" target="_blank">arXiv:2004.09750</a> [<a href="http://arxiv.org/pdf/2004.09750" target="_blank">pdf</a>]

<h2>JointsGait:A model-based Gait Recognition Method based on Gait Graph Convolutional Networks and Joints Relationship Pyramid Mapping. (arXiv:2005.08625v2 [cs.CV] UPDATED)</h2>
<h3>Na Li, Xinbo Zhao, Chong Ma</h3>
<p>Gait, as one of unique biometric features, has the advantage of being
recognized from a long distance away, can be widely used in public security.
Considering 3D pose estimation is more challenging than 2D pose estimation in
practice , we research on using 2D joints to recognize gait in this paper, and
a new model-based gait recognition method JointsGait is put forward to extract
gait information from 2D human body joints. Appearance-based gait recognition
algorithms are prevalent before. However, appearance features suffer from
external factors which can cause drastic appearance variations, e.g. clothing.
Unlike previous approaches, JointsGait firstly extracted spatio-temporal
features from 2D joints using gait graph convolutional networks, which are less
interfered by external factors. Secondly, Joints Relationship Pyramid Mapping
(JRPM) are proposed to map spatio-temporal gait features into a discriminative
feature space with biological advantages according to the relationship of human
joints when people are walking at various scales. Finally, we design a fusion
loss strategy to help the joints features to be insensitive to cross-view. Our
method is evaluated on two large datasets, Kinect Gait Biometry Dataset and
CASIA-B. On Kinect Gait Biometry Dataset database, JointsGait only uses
corresponding 2D coordinates of joints, but achieves satisfactory recognition
accuracy compared with those model-based algorithms using 3D joints. On CASIA-B
database, the proposed method greatly outperforms advanced model-based methods
in all walking conditions, even performs superior to state-of-art
appearance-based methods when clothing seriously affect people's appearance.
The experimental results demonstrate that JointsGait achieves the state-of-art
performance despite the low dimensional feature (2D body joints) and is less
affected by the view variations and clothing variation.
</p>
<a href="http://arxiv.org/abs/2005.08625" target="_blank">arXiv:2005.08625</a> [<a href="http://arxiv.org/pdf/2005.08625" target="_blank">pdf</a>]

<h2>Discovering Parametric Activation Functions. (arXiv:2006.03179v3 [cs.LG] UPDATED)</h2>
<h3>Garrett Bingham, Risto Miikkulainen</h3>
<p>Recent studies have shown that the choice of activation function can
significantly affect the performance of deep learning networks. However, the
benefits of novel activation functions have been inconsistent and task
dependent, and therefore the rectified linear unit (ReLU) is still the most
commonly used. This paper proposes a technique for customizing activation
functions automatically, resulting in reliable improvements in performance.
Evolutionary search is used to discover the general form of the function, and
gradient descent to optimize its parameters for different parts of the network
and over the learning process. Experiments with four different neural network
architectures on the CIFAR-10 and CIFAR-100 image classification datasets show
that this approach is effective. It discovers both general activation functions
and specialized functions for different architectures, consistently improving
accuracy over ReLU and other recently proposed activation functions by
significant margins. The approach can therefore be used as an automated
optimization step in applying deep learning to new tasks.
</p>
<a href="http://arxiv.org/abs/2006.03179" target="_blank">arXiv:2006.03179</a> [<a href="http://arxiv.org/pdf/2006.03179" target="_blank">pdf</a>]

<h2>A Model-free Learning Algorithm for Infinite-horizon Average-reward MDPs with Near-optimal Regret. (arXiv:2006.04354v2 [cs.LG] UPDATED)</h2>
<h3>Mehdi Jafarnia-Jahromi, Chen-Yu Wei, Rahul Jain, Haipeng Luo</h3>
<p>Recently, model-free reinforcement learning has attracted research attention
due to its simplicity, memory and computation efficiency, and the flexibility
to combine with function approximation. In this paper, we propose Exploration
Enhanced Q-learning (EE-QL), a model-free algorithm for infinite-horizon
average-reward Markov Decision Processes (MDPs) that achieves regret bound of
$O(\sqrt{T})$ for the general class of weakly communicating MDPs, where $T$ is
the number of interactions. EE-QL assumes that an online concentrating
approximation of the optimal average reward is available. This is the first
model-free learning algorithm that achieves $O(\sqrt T)$ regret without the
ergodic assumption, and matches the lower bound in terms of $T$ except for
logarithmic factors. Experiments show that the proposed algorithm performs as
well as the best known model-based algorithms.
</p>
<a href="http://arxiv.org/abs/2006.04354" target="_blank">arXiv:2006.04354</a> [<a href="http://arxiv.org/pdf/2006.04354" target="_blank">pdf</a>]

<h2>Neural Jump Ordinary Differential Equations. (arXiv:2006.04727v3 [stat.ML] UPDATED)</h2>
<h3>Calypso Herrera, Florian Krach, Josef Teichmann</h3>
<p>Combinations of neural ODEs with recurrent neural networks (RNN), like
GRU-ODE-Bayes or ODE-RNN are well suited to model irregularly observed time
series. While those models outperform existing discrete-time approaches, no
theoretical guarantees for their predictive capabilities are available.
Assuming that the irregularly-sampled time series data originates from a
continuous stochastic process, the $L^2$-optimal online prediction is the
conditional expectation given the currently available information. We introduce
the neural jump ODE (NJ-ODE) that provides a data-driven approach to learn,
continuously in time, the conditional expectation of a stochastic process. Our
approach models the conditional expectation between two observations with a
neural ODE and jumps whenever a new observation is made. We define a novel
training framework, which allows us to prove theoretical guarantees for the
first time. In particular, we show that the output of our model converges to
the $L^2$-optimal prediction. We provide experiments showing that the
theoretical results also hold empirically. Moreover, we experimentally show
that our model outperforms the baselines in more complex learning tasks and
give comparisons on real-world datasets.
</p>
<a href="http://arxiv.org/abs/2006.04727" target="_blank">arXiv:2006.04727</a> [<a href="http://arxiv.org/pdf/2006.04727" target="_blank">pdf</a>]

<h2>Spatial-Temporal Dynamic Graph Attention Networks for Ride-hailing Demand Prediction. (arXiv:2006.05905v2 [cs.LG] UPDATED)</h2>
<h3>Weiguo Pian, Yingbo Wu, Xiangmou Qu, Junpeng Cai, Ziyi Kou</h3>
<p>Ride-hailing demand prediction is an essential task in spatial-temporal data
mining. Accurate Ride-hailing demand prediction can help to pre-allocate
resources, improve vehicle utilization and user experiences. Graph
Convolutional Networks (GCN) is commonly used to model the complicated
irregular non-Euclidean spatial correlations. However, existing GCN-based
ride-hailing demand prediction methods only assign the same importance to
different neighbor regions, and maintain a fixed graph structure with static
spatial relationships throughout the timeline when extracting the irregular
non-Euclidean spatial correlations. In this paper, we propose the
Spatial-Temporal Dynamic Graph Attention Network (STDGAT), a novel ride-hailing
demand prediction method. Based on the attention mechanism of GAT, STDGAT
extracts different pair-wise correlations to achieve the adaptive importance
allocation for different neighbor regions. Moreover, in STDGAT, we design a
novel time-specific commuting-based graph attention mode to construct a dynamic
graph structure for capturing the dynamic time-specific spatial relationships
throughout the timeline. Extensive experiments are conducted on a real-world
ride-hailing demand dataset, and the experimental results demonstrate the
significant improvement of our method on three evaluation metrics RMSE, MAPE
and MAE over state-of-the-art baselines.
</p>
<a href="http://arxiv.org/abs/2006.05905" target="_blank">arXiv:2006.05905</a> [<a href="http://arxiv.org/pdf/2006.05905" target="_blank">pdf</a>]

<h2>On Mixup Regularization. (arXiv:2006.06049v2 [cs.LG] UPDATED)</h2>
<h3>Luigi Carratino, Moustapha Ciss&#xe9;, Rodolphe Jenatton, Jean-Philippe Vert</h3>
<p>Mixup is a data augmentation technique that creates new examples as convex
combinationsof training points and labels. This simple technique has
empirically shown to improvethe accuracy of many state-of-the-art models in
different settings and applications, butthe reasons behind this empirical
success remain poorly understood. In this paper wetake a substantial step in
explaining the theoretical foundations of Mixup, by clarifyingits
regularization effects. We show that Mixup can be interpreted as standard
empiricalrisk minimization estimator subject to a combination of data
transformation and randomperturbation of the transformed data. We gain two core
insights from this new interpretation.First, the data transformation suggests
that, at test time, a model trained with Mixup shouldalso be applied to
transformed data, a one-line change in code that we show empirically toimprove
both accuracy and calibration of the prediction. Second, we show how the
randomperturbation of the new interpretation of Mixup induces multiple known
regularizationschemes, including label smoothing and reduction of the Lipschitz
constant of the estimator.These schemes interact synergistically with each
other, resulting in a self calibrated andeffective regularization effect that
prevents overfitting and overconfident predictions. Wecorroborate our
theoretical analysis with experiments that support our conclusions.
</p>
<a href="http://arxiv.org/abs/2006.06049" target="_blank">arXiv:2006.06049</a> [<a href="http://arxiv.org/pdf/2006.06049" target="_blank">pdf</a>]

<h2>UniT: Unified Knowledge Transfer for Any-shot Object Detection and Segmentation. (arXiv:2006.07502v2 [cs.CV] UPDATED)</h2>
<h3>Siddhesh Khandelwal, Raghav Goyal, Leonid Sigal</h3>
<p>Methods for object detection and segmentation rely on large scale
instance-level annotations for training, which are difficult and time-consuming
to collect. Efforts to alleviate this look at varying degrees and quality of
supervision. Weakly-supervised approaches draw on image-level labels to build
detectors/segmentors, while zero/few-shot methods assume abundant
instance-level data for a set of base classes, and none to a few examples for
novel classes. This taxonomy has largely siloed algorithmic designs. In this
work, we aim to bridge this divide by proposing an intuitive and unified
semi-supervised model that is applicable to a range of supervision: from zero
to a few instance-level samples per novel class. For base classes, our model
learns a mapping from weakly-supervised to fully-supervised
detectors/segmentors. By learning and leveraging visual and lingual
similarities between the novel and base classes, we transfer those mappings to
obtain detectors/segmentors for novel classes; refining them with a few novel
class instance-level annotated samples, if available. The overall model is
end-to-end trainable and highly flexible. Through extensive experiments on
MS-COCO and Pascal VOC benchmark datasets we show improved performance in a
variety of settings.
</p>
<a href="http://arxiv.org/abs/2006.07502" target="_blank">arXiv:2006.07502</a> [<a href="http://arxiv.org/pdf/2006.07502" target="_blank">pdf</a>]

<h2>Learning Heuristic Selection with Dynamic Algorithm Configuration. (arXiv:2006.08246v2 [cs.AI] UPDATED)</h2>
<h3>David Speck, Andr&#xe9; Biedenkapp, Frank Hutter, Robert Mattm&#xfc;ller, Marius Lindauer</h3>
<p>A key challenge in satisficing planning is to use multiple heuristics within
one heuristic search. An aggregation of multiple heuristic estimates, for
example by taking the maximum, has the disadvantage that bad estimates of a
single heuristic can negatively affect the whole search. Since the performance
of a heuristic varies from instance to instance, approaches such as algorithm
selection can be successfully applied. In addition, alternating between
multiple heuristics during the search makes it possible to use all heuristics
equally and improve performance. However, all these approaches ignore the
internal search dynamics of a planning system, which can help to select the
most helpful heuristics for the current expansion step. We show that dynamic
algorithm configuration can be used for dynamic heuristic selection which takes
into account the internal search dynamics of a planning system. Furthermore, we
prove that this approach generalizes over existing approaches and that it can
exponentially improve the performance of the heuristic search. To learn dynamic
heuristic selection, we propose an approach based on reinforcement learning and
show empirically that domain-wise learned policies, which take the internal
search dynamics of a planning system into account, can exceed existing
approaches in terms of coverage.
</p>
<a href="http://arxiv.org/abs/2006.08246" target="_blank">arXiv:2006.08246</a> [<a href="http://arxiv.org/pdf/2006.08246" target="_blank">pdf</a>]

<h2>Targeted Adversarial Perturbations for Monocular Depth Prediction. (arXiv:2006.08602v2 [cs.CV] UPDATED)</h2>
<h3>Alex Wong, Safa Cicek, Stefano Soatto</h3>
<p>We study the effect of adversarial perturbations on the task of monocular
depth prediction. Specifically, we explore the ability of small, imperceptible
additive perturbations to selectively alter the perceived geometry of the
scene. We show that such perturbations can not only globally re-scale the
predicted distances from the camera, but also alter the prediction to match a
different target scene. We also show that, when given semantic or instance
information, perturbations can fool the network to alter the depth of specific
categories or instances in the scene, and even remove them while preserving the
rest of the scene. To understand the effect of targeted perturbations, we
conduct experiments on state-of-the-art monocular depth prediction methods. Our
experiments reveal vulnerabilities in monocular depth prediction networks, and
shed light on the biases and context learned by them.
</p>
<a href="http://arxiv.org/abs/2006.08602" target="_blank">arXiv:2006.08602</a> [<a href="http://arxiv.org/pdf/2006.08602" target="_blank">pdf</a>]

<h2>Riemannian Continuous Normalizing Flows. (arXiv:2006.10605v2 [stat.ML] UPDATED)</h2>
<h3>Emile Mathieu, Maximilian Nickel</h3>
<p>Normalizing flows have shown great promise for modelling flexible probability
distributions in a computationally tractable way. However, whilst data is often
naturally described on Riemannian manifolds such as spheres, torii, and
hyperbolic spaces, most normalizing flows implicitly assume a flat geometry,
making them either misspecified or ill-suited in these situations. To overcome
this problem, we introduce Riemannian continuous normalizing flows, a model
which admits the parametrization of flexible probability measures on smooth
manifolds by defining flows as the solution to ordinary differential equations.
We show that this approach can lead to substantial improvements on both
synthetic and real-world data when compared to standard flows or previously
introduced projected flows.
</p>
<a href="http://arxiv.org/abs/2006.10605" target="_blank">arXiv:2006.10605</a> [<a href="http://arxiv.org/pdf/2006.10605" target="_blank">pdf</a>]

<h2>When Does Preconditioning Help or Hurt Generalization?. (arXiv:2006.10732v4 [stat.ML] UPDATED)</h2>
<h3>Shun-ichi Amari, Jimmy Ba, Roger Grosse, Xuechen Li, Atsushi Nitanda, Taiji Suzuki, Denny Wu, Ji Xu</h3>
<p>While second order optimizers such as natural gradient descent (NGD) often
speed up optimization, their effect on generalization has been called into
question. This work presents a more nuanced view on how the \textit{implicit
bias} of first- and second-order methods affects the comparison of
generalization properties. We provide an exact asymptotic bias-variance
decomposition of the generalization error of overparameterized ridgeless
regression under a general class of preconditioner $\boldsymbol{P}$, and
consider the inverse population Fisher information matrix (used in NGD) as a
particular example. We determine the optimal $\boldsymbol{P}$ for both the bias
and variance, and find that the relative generalization performance of
different optimizers depends on the label noise and the "shape" of the signal
(true parameters): when the labels are noisy, the model is misspecified, or the
signal is misaligned with the features, NGD can achieve lower risk; conversely,
GD generalizes better than NGD under clean labels, a well-specified model, or
aligned signal. Based on this analysis, we discuss several approaches to manage
the bias-variance tradeoff, and the potential benefit of interpolating between
GD and NGD. We then extend our analysis to regression in the reproducing kernel
Hilbert space and demonstrate that preconditioned GD can decrease the
population risk faster than GD. Lastly, we empirically compare the
generalization error of first- and second-order optimizers in neural network
experiments, and observe robust trends matching our theoretical analysis.
</p>
<a href="http://arxiv.org/abs/2006.10732" target="_blank">arXiv:2006.10732</a> [<a href="http://arxiv.org/pdf/2006.10732" target="_blank">pdf</a>]

<h2>Regression Prior Networks. (arXiv:2006.11590v2 [cs.LG] UPDATED)</h2>
<h3>Andrey Malinin, Sergey Chervontsev, Ivan Provilkov, Mark Gales</h3>
<p>Prior Networks are a recently developed class of models which yield
interpretable measures of uncertainty and have been shown to outperform
state-of-the-art ensemble approaches on a range of tasks. They can also be used
to distill an ensemble of models via Ensemble Distribution Distillation
(EnD$^2$), such that its accuracy, calibration and uncertainty estimates are
retained within a single model. However, Prior Networks have so far been
developed only for classification tasks. This work extends Prior Networks and
EnD$^2$ to regression tasks by considering the Normal-Wishart distribution. The
properties of Regression Prior Networks are demonstrated on synthetic data,
selected UCI datasets and a monocular depth estimation task, where they yield
performance competitive with ensemble approaches.
</p>
<a href="http://arxiv.org/abs/2006.11590" target="_blank">arXiv:2006.11590</a> [<a href="http://arxiv.org/pdf/2006.11590" target="_blank">pdf</a>]

<h2>Training (Overparametrized) Neural Networks in Near-Linear Time. (arXiv:2006.11648v2 [cs.LG] UPDATED)</h2>
<h3>Jan van den Brand, Binghui Peng, Zhao Song, Omri Weinstein</h3>
<p>The slow convergence rate and pathological curvature issues of first-order
gradient methods for training deep neural networks, initiated an ongoing effort
for developing faster $\mathit{second}$-$\mathit{order}$ optimization
algorithms beyond SGD, without compromising the generalization error. Despite
their remarkable convergence rate ($\mathit{independent}$ of the training batch
size $n$), second-order algorithms incur a daunting slowdown in the
$\mathit{cost}$ $\mathit{per}$ $\mathit{iteration}$ (inverting the Hessian
matrix of the loss function), which renders them impractical. Very recently,
this computational overhead was mitigated by the works of [ZMG19,CGH+19},
yielding an $O(mn^2)$-time second-order algorithm for training two-layer
overparametrized neural networks of polynomial width $m$.

We show how to speed up the algorithm of [CGH+19], achieving an
$\tilde{O}(mn)$-time backpropagation algorithm for training (mildly
overparametrized) ReLU networks, which is near-linear in the dimension ($mn$)
of the full gradient (Jacobian) matrix. The centerpiece of our algorithm is to
reformulate the Gauss-Newton iteration as an $\ell_2$-regression problem, and
then use a Fast-JL type dimension reduction to $\mathit{precondition}$ the
underlying Gram matrix in time independent of $M$, allowing to find a
sufficiently good approximate solution via $\mathit{first}$-$\mathit{order}$
conjugate gradient. Our result provides a proof-of-concept that advanced
machinery from randomized linear algebra -- which led to recent breakthroughs
in $\mathit{convex}$ $\mathit{optimization}$ (ERM, LPs, Regression) -- can be
carried over to the realm of deep learning as well.
</p>
<a href="http://arxiv.org/abs/2006.11648" target="_blank">arXiv:2006.11648</a> [<a href="http://arxiv.org/pdf/2006.11648" target="_blank">pdf</a>]

<h2>The Depth-to-Width Interplay in Self-Attention. (arXiv:2006.12467v2 [cs.LG] UPDATED)</h2>
<h3>Yoav Levine, Noam Wies, Or Sharir, Hofit Bata, Amnon Shashua</h3>
<p>Self-attention architectures, which are rapidly pushing the frontier in
natural language processing, demonstrate a surprising depth-inefficient
behavior: previous works indicate that increasing the internal representation
(network width) is just as useful as increasing the number of self-attention
layers (network depth). We theoretically predict a width-dependent transition
between depth-efficiency and depth-inefficiency in self-attention. We conduct
systematic empirical ablations on networks of depths 6 to 48 that clearly
reveal the theoretically predicted behaviors, and provide explicit quantitative
suggestions regarding the optimal depth-to-width allocation for a given
self-attention network size. The race towards beyond 1-Trillion parameter
language models renders informed guidelines for increasing self-attention depth
and width in tandem an essential ingredient. Our guidelines elucidate the
depth-to-width trade-off in self-attention networks of sizes up to the scale of
GPT3 (which is too deep for its size), and beyond, marking an unprecedented
width of 30K as optimal for a 1-Trillion parameter self-attention network.
</p>
<a href="http://arxiv.org/abs/2006.12467" target="_blank">arXiv:2006.12467</a> [<a href="http://arxiv.org/pdf/2006.12467" target="_blank">pdf</a>]

<h2>Video Object Segmentation with Episodic Graph Memory Networks. (arXiv:2007.07020v4 [cs.CV] UPDATED)</h2>
<h3>Xiankai Lu, Wenguan Wang, Martin Danelljan, Tianfei Zhou, Jianbing Shen, Luc Van Gool</h3>
<p>How to make a segmentation model efficiently adapt to a specific video and to
online target appearance variations are fundamentally crucial issues in the
field of video object segmentation. In this work, a graph memory network is
developed to address the novel idea of "learning to update the segmentation
model". Specifically, we exploit an episodic memory network, organized as a
fully connected graph, to store frames as nodes and capture cross-frame
correlations by edges. Further, learnable controllers are embedded to ease
memory reading and writing, as well as maintain a fixed memory scale. The
structured, external memory design enables our model to comprehensively mine
and quickly store new knowledge, even with limited visual information, and the
differentiable memory controllers slowly learn an abstract method for storing
useful representations in the memory and how to later use these representations
for prediction, via gradient descent. In addition, the proposed graph memory
network yields a neat yet principled framework, which can generalize well both
one-shot and zero-shot video object segmentation tasks. Extensive experiments
on four challenging benchmark datasets verify that our graph memory network is
able to facilitate the adaptation of the segmentation network for case-by-case
video object segmentation.
</p>
<a href="http://arxiv.org/abs/2007.07020" target="_blank">arXiv:2007.07020</a> [<a href="http://arxiv.org/pdf/2007.07020" target="_blank">pdf</a>]

<h2>Explanation-Guided Training for Cross-Domain Few-Shot Classification. (arXiv:2007.08790v2 [cs.CV] UPDATED)</h2>
<h3>Jiamei Sun, Sebastian Lapuschkin, Wojciech Samek, Yunqing Zhao, Ngai-Man Cheung, Alexander Binder</h3>
<p>Cross-domain few-shot classification task (CD-FSC) combines few-shot
classification with the requirement to generalize across domains represented by
datasets. This setup faces challenges originating from the limited labeled data
in each class and, additionally, from the domain shift between training and
test sets. In this paper, we introduce a novel training approach for existing
FSC models. It leverages on the explanation scores, obtained from existing
explanation methods when applied to the predictions of FSC models, computed for
intermediate feature maps of the models. Firstly, we tailor the layer-wise
relevance propagation (LRP) method to explain the predictions of FSC models.
Secondly, we develop a model-agnostic explanation-guided training strategy that
dynamically finds and emphasizes the features which are important for the
predictions. Our contribution does not target a novel explanation method but
lies in a novel application of explanations for the training phase. We show
that explanation-guided training effectively improves the model generalization.
We observe improved accuracy for three different FSC models: RelationNet, cross
attention network, and a graph neural network-based formulation, on five
few-shot learning datasets: miniImagenet, CUB, Cars, Places, and Plantae. The
source code is available at https://github.com/SunJiamei/few-shot-lrp-guided
</p>
<a href="http://arxiv.org/abs/2007.08790" target="_blank">arXiv:2007.08790</a> [<a href="http://arxiv.org/pdf/2007.08790" target="_blank">pdf</a>]

<h2>Distributional Reinforcement Learning via Moment Matching. (arXiv:2007.12354v3 [cs.LG] UPDATED)</h2>
<h3>Thanh Tang Nguyen, Sunil Gupta, Svetha Venkatesh</h3>
<p>We consider the problem of learning a set of probability distributions from
the empirical Bellman dynamics in distributional reinforcement learning (RL), a
class of state-of-the-art methods that estimate the distribution, as opposed to
only the expectation, of the total return. We formulate a method that learns a
finite set of statistics from each return distribution via neural networks, as
in (Bellemare, Dabney, and Munos 2017; Dabney et al. 2018b). Existing
distributional RL methods however constrain the learned statistics to
\emph{predefined} functional forms of the return distribution which is both
restrictive in representation and difficult in maintaining the predefined
statistics. Instead, we learn \emph{unrestricted} statistics, i.e.,
deterministic (pseudo-)samples, of the return distribution by leveraging a
technique from hypothesis testing known as maximum mean discrepancy (MMD),
which leads to a simpler objective amenable to backpropagation. Our method can
be interpreted as implicitly matching all orders of moments between a return
distribution and its Bellman target. We establish sufficient conditions for the
contraction of the distributional Bellman operator and provide finite-sample
analysis for the deterministic samples in distribution approximation.
Experiments on the suite of Atari games show that our method outperforms the
standard distributional RL baselines and sets a new record in the Atari games
for non-distributed agents.
</p>
<a href="http://arxiv.org/abs/2007.12354" target="_blank">arXiv:2007.12354</a> [<a href="http://arxiv.org/pdf/2007.12354" target="_blank">pdf</a>]

<h2>Human-Robot Interaction in a Shared Augmented Reality Workspace. (arXiv:2007.12656v2 [cs.RO] UPDATED)</h2>
<h3>Shuwen Qiu, Hangxin Liu, Zeyu Zhang, Yixin Zhu, Song-Chun Zhu</h3>
<p>We design and develop a new shared Augmented Reality (AR) workspace for
Human-Robot Interaction (HRI), which establishes a bi-directional communication
between human agents and robots. In a prototype system, the shared AR workspace
enables a shared perception, so that a physical robot not only perceives the
virtual elements in its own view but also infers the utility of the human
agent--the cost needed to perceive and interact in AR--by sensing the human
agent's gaze and pose. Such a new HRI design also affords a shared
manipulation, wherein the physical robot can control and alter virtual objects
in AR as an active agent; crucially, a robot can proactively interact with
human agents, instead of purely passively executing received commands. In
experiments, we design a resource collection game that qualitatively
demonstrates how a robot perceives, processes, and manipulates in AR and
quantitatively evaluates the efficacy of HRI using the shared AR workspace. We
further discuss how the system can potentially benefit future HRI studies that
are otherwise challenging.
</p>
<a href="http://arxiv.org/abs/2007.12656" target="_blank">arXiv:2007.12656</a> [<a href="http://arxiv.org/pdf/2007.12656" target="_blank">pdf</a>]

<h2>Partial Feature Decorrelation for Non-I.I.D Image classification. (arXiv:2007.15241v3 [cs.LG] UPDATED)</h2>
<h3>Zhengxu Yu, Pengfei Wang, Chao Xiang, Liang Xie, Zhongming Jin, Jianqiang Huang, Xiaofei He, Deng Cai, Xian-Sheng Hua</h3>
<p>Most deep-learning-based image classification methods follow a consistency
hypothesis that all samples are generated under an independent and identically
distributed (I.I.D) setting. Such property can hardly be guaranteed in many
real-world applications, resulting in an agnostic context distribution shift
between training and testing environments. It can misguide the model overfit
some statistical correlations between features in the training set. To address
this problem, we present a novel Partial Feature Decorrelation Learning (PFDL)
Algorithm, which jointly optimizes a feature decomposition network and the
target image classification model. The feature decomposition network decomposes
feature embedding into independent and correlated parts such that the
correlations between features will be highlighted. Then, the correlated
features are used to help learn a stable feature representation by
decorrelating the highlighted correlations while optimizing the image
classification model. Extensive experiments demonstrate that our proposal can
improve the backbone model's accuracy in non-i.i.d image classification
datasets.
</p>
<a href="http://arxiv.org/abs/2007.15241" target="_blank">arXiv:2007.15241</a> [<a href="http://arxiv.org/pdf/2007.15241" target="_blank">pdf</a>]

<h2>Optimal Variance Control of the Score Function Gradient Estimator for Importance Weighted Bounds. (arXiv:2008.01998v2 [stat.ML] UPDATED)</h2>
<h3>Valentin Li&#xe9;vin, Andrea Dittadi, Anders Christensen, Ole Winther</h3>
<p>This paper introduces novel results for the score function gradient estimator
of the importance weighted variational bound (IWAE). We prove that in the limit
of large $K$ (number of importance samples) one can choose the control variate
such that the Signal-to-Noise ratio (SNR) of the estimator grows as $\sqrt{K}$.
This is in contrast to the standard pathwise gradient estimator where the SNR
decreases as $1/\sqrt{K}$. Based on our theoretical findings we develop a novel
control variate that extends on VIMCO. Empirically, for the training of both
continuous and discrete generative models, the proposed method yields superior
variance reduction, resulting in an SNR for IWAE that increases with $K$
without relying on the reparameterization trick. The novel estimator is
competitive with state-of-the-art reparameterization-free gradient estimators
such as Reweighted Wake-Sleep (RWS) and the thermodynamic variational objective
(TVO) when training generative models.
</p>
<a href="http://arxiv.org/abs/2008.01998" target="_blank">arXiv:2008.01998</a> [<a href="http://arxiv.org/pdf/2008.01998" target="_blank">pdf</a>]

<h2>PhysCap: Physically Plausible Monocular 3D Motion Capture in Real Time. (arXiv:2008.08880v2 [cs.CV] UPDATED)</h2>
<h3>Soshi Shimada, Vladislav Golyanik, Weipeng Xu, Christian Theobalt</h3>
<p>Marker-less 3D human motion capture from a single colour camera has seen
significant progress. However, it is a very challenging and severely ill-posed
problem. In consequence, even the most accurate state-of-the-art approaches
have significant limitations. Purely kinematic formulations on the basis of
individual joints or skeletons, and the frequent frame-wise reconstruction in
state-of-the-art methods greatly limit 3D accuracy and temporal stability
compared to multi-view or marker-based motion capture. Further, captured 3D
poses are often physically incorrect and biomechanically implausible, or
exhibit implausible environment interactions (floor penetration, foot skating,
unnatural body leaning and strong shifting in depth), which is problematic for
any use case in computer graphics. We, therefore, present PhysCap, the first
algorithm for physically plausible, real-time and marker-less human 3D motion
capture with a single colour camera at 25 fps. Our algorithm first captures 3D
human poses purely kinematically. To this end, a CNN infers 2D and 3D joint
positions, and subsequently, an inverse kinematics step finds space-time
coherent joint angles and global 3D pose. Next, these kinematic reconstructions
are used as constraints in a real-time physics-based pose optimiser that
accounts for environment constraints (e.g., collision handling and floor
placement), gravity, and biophysical plausibility of human postures. Our
approach employs a combination of ground reaction force and residual force for
plausible root control, and uses a trained neural network to detect foot
contact events in images. Our method captures physically plausible and
temporally stable global 3D human motion, without physically implausible
postures, floor penetrations or foot skating, from video in real time and in
general scenes. The video is available at
this http URL
</p>
<a href="http://arxiv.org/abs/2008.08880" target="_blank">arXiv:2008.08880</a> [<a href="http://arxiv.org/pdf/2008.08880" target="_blank">pdf</a>]

<h2>Learning Condition Invariant Features for Retrieval-Based Localization from 1M Images. (arXiv:2008.12165v2 [cs.CV] UPDATED)</h2>
<h3>Janine Thoma, Danda Pani Paudel, Ajad Chhatkuli, Luc Van Gool</h3>
<p>Image features for retrieval-based localization must be invariant to dynamic
objects (e.g. cars) as well as seasonal and daytime changes. Such invariances
are, up to some extent, learnable with existing methods using triplet-like
losses, given a large number of diverse training images. However, due to the
high algorithmic training complexity, there exists insufficient comparison
between different loss functions on large datasets. In this paper, we train and
evaluate several localization methods on three different benchmark datasets,
including Oxford RobotCar with over one million images. This large scale
evaluation yields valuable insights into the generalizability and performance
of retrieval-based localization. Based on our findings, we develop a novel
method for learning more accurate and better generalizing localization
features. It consists of two main contributions: (i) a feature volume-based
loss function, and (ii) hard positive and pairwise negative mining. On the
challenging Oxford RobotCar night condition, our method outperforms the
well-known triplet loss by 24.4% in localization accuracy within 5m.
</p>
<a href="http://arxiv.org/abs/2008.12165" target="_blank">arXiv:2008.12165</a> [<a href="http://arxiv.org/pdf/2008.12165" target="_blank">pdf</a>]

<h2>Planning with Learned Object Importance in Large Problem Instances using Graph Neural Networks. (arXiv:2009.05613v2 [cs.LG] UPDATED)</h2>
<h3>Tom Silver, Rohan Chitnis, Aidan Curtis, Joshua Tenenbaum, Tomas Lozano-Perez, Leslie Pack Kaelbling</h3>
<p>Real-world planning problems often involve hundreds or even thousands of
objects, straining the limits of modern planners. In this work, we address this
challenge by learning to predict a small set of objects that, taken together,
would be sufficient for finding a plan. We propose a graph neural network
architecture for predicting object importance in a single inference pass, thus
incurring little overhead while greatly reducing the number of objects that
must be considered by the planner. Our approach treats the planner and
transition model as black boxes, and can be used with any off-the-shelf
planner. Empirically, across classical planning, probabilistic planning, and
robotic task and motion planning, we find that our method results in planning
that is significantly faster than several baselines, including other partial
grounding strategies and lifted planners. We conclude that learning to predict
a sufficient set of objects for a planning problem is a simple, powerful, and
general mechanism for planning in large instances. Video:
https://youtu.be/FWsVJc2fvCE Code: https://git.io/JIsqX
</p>
<a href="http://arxiv.org/abs/2009.05613" target="_blank">arXiv:2009.05613</a> [<a href="http://arxiv.org/pdf/2009.05613" target="_blank">pdf</a>]

<h2>Extended Radial Basis Function Controller for Reinforcement Learning. (arXiv:2009.05866v2 [cs.LG] UPDATED)</h2>
<h3>Nicholas Capel, Naifu Zhang</h3>
<p>There have been attempts in reinforcement learning to exploit a priori
knowledge about the structure of the system. This paper proposes a hybrid
reinforcement learning controller which dynamically interpolates a model-based
linear controller and an arbitrary differentiable policy. The linear controller
is designed based on local linearised model knowledge, and stabilises the
system in a neighbourhood about an operating point. The coefficients of
interpolation between the two controllers are determined by a scaled distance
function measuring the distance between the current state and the operating
point. The overall hybrid controller is proven to maintain the stability
guarantee around the neighborhood of the operating point and still possess the
universal function approximation property of the arbitrary non-linear policy.
Learning has been done on both model-based (PILCO) and model-free (DDPG)
frameworks. Simulation experiments performed in OpenAI gym demonstrate
stability and robustness of the proposed hybrid controller. This paper thus
introduces a principled method allowing for the direct importing of control
methodology into reinforcement learning.
</p>
<a href="http://arxiv.org/abs/2009.05866" target="_blank">arXiv:2009.05866</a> [<a href="http://arxiv.org/pdf/2009.05866" target="_blank">pdf</a>]

<h2>BSN++: Complementary Boundary Regressor with Scale-Balanced Relation Modeling for Temporal Action Proposal Generation. (arXiv:2009.07641v2 [cs.CV] UPDATED)</h2>
<h3>Haisheng Su, Weihao Gan, Wei Wu, Junjie Yan, Yu Qiao</h3>
<p>Generating human action proposals in untrimmed videos is an important yet
challenging task with wide applications. Current methods often suffer from the
noisy boundary locations and the inferior quality of confidence scores used for
proposal retrieving. In this paper, we present BSN++, a new framework which
exploits complementary boundary regressor and relation modeling for temporal
proposal generation. First, we propose a novel boundary regressor based on the
complementary characteristics of both starting and ending boundary classifiers.
Specifically, we utilize the U-shaped architecture with nested skip connections
to capture rich contexts and introduce bi-directional boundary matching
mechanism to improve boundary precision. Second, to account for the
proposal-proposal relations ignored in previous methods, we devise a proposal
relation block to which includes two self-attention modules from the aspects of
position and channel. Furthermore, we find that there inevitably exists data
imbalanced problems in the positive/negative proposals and temporal durations,
which harm the model performance on tail distributions. To relieve this issue,
we introduce the scale-balanced re-sampling strategy. Extensive experiments are
conducted on two popular benchmarks: ActivityNet-1.3 and THUMOS14, which
demonstrate that BSN++ achieves the state-of-the-art performance. Not
surprisingly, the proposed BSN++ ranked 1st place in the CVPR19 - ActivityNet
challenge leaderboard on temporal action localization task.
</p>
<a href="http://arxiv.org/abs/2009.07641" target="_blank">arXiv:2009.07641</a> [<a href="http://arxiv.org/pdf/2009.07641" target="_blank">pdf</a>]

<h2>Learning to Detect Objects with a 1 Megapixel Event Camera. (arXiv:2009.13436v2 [cs.CV] UPDATED)</h2>
<h3>Etienne Perot, Pierre de Tournemire, Davide Nitti, Jonathan Masci, Amos Sironi</h3>
<p>Event cameras encode visual information with high temporal precision, low
data-rate, and high-dynamic range. Thanks to these characteristics, event
cameras are particularly suited for scenarios with high motion, challenging
lighting conditions and requiring low latency. However, due to the novelty of
the field, the performance of event-based systems on many vision tasks is still
lower compared to conventional frame-based solutions. The main reasons for this
performance gap are: the lower spatial resolution of event sensors, compared to
frame cameras; the lack of large-scale training datasets; the absence of well
established deep learning architectures for event-based processing. In this
paper, we address all these problems in the context of an event-based object
detection task. First, we publicly release the first high-resolution
large-scale dataset for object detection. The dataset contains more than 14
hours recordings of a 1 megapixel event camera, in automotive scenarios,
together with 25M bounding boxes of cars, pedestrians, and two-wheelers,
labeled at high frequency. Second, we introduce a novel recurrent architecture
for event-based detection and a temporal consistency loss for better-behaved
training. The ability to compactly represent the sequence of events into the
internal memory of the model is essential to achieve high accuracy. Our model
outperforms by a large margin feed-forward event-based architectures. Moreover,
our method does not require any reconstruction of intensity images from events,
showing that training directly from raw events is possible, more efficient, and
more accurate than passing through an intermediate intensity image. Experiments
on the dataset introduced in this work, for which events and gray level images
are available, show performance on par with that of highly tuned and studied
frame-based detectors.
</p>
<a href="http://arxiv.org/abs/2009.13436" target="_blank">arXiv:2009.13436</a> [<a href="http://arxiv.org/pdf/2009.13436" target="_blank">pdf</a>]

<h2>Pruning Filter in Filter. (arXiv:2009.14410v3 [cs.CV] UPDATED)</h2>
<h3>Fanxu Meng, Hao Cheng, Ke Li, Huixiang Luo, Xiaowei Guo, Guangming Lu, Xing Sun</h3>
<p>Pruning has become a very powerful and effective technique to compress and
accelerate modern neural networks. Existing pruning methods can be grouped into
two categories: filter pruning (FP) and weight pruning (WP). FP wins at
hardware compatibility but loses at the compression ratio compared with WP. To
converge the strength of both methods, we propose to prune the filter in the
filter. Specifically, we treat a filter $F \in \mathbb{R}^{C\times K\times K}$
as $K \times K$ stripes, i.e., $1\times 1$ filters $\in \mathbb{R}^{C}$, then
by pruning the stripes instead of the whole filter, we can achieve finer
granularity than traditional FP while being hardware friendly. We term our
method as SWP (\emph{Stripe-Wise Pruning}). SWP is implemented by introducing a
novel learnable matrix called Filter Skeleton, whose values reflect the shape
of each filter. As some recent work has shown that the pruned architecture is
more crucial than the inherited important weights, we argue that the
architecture of a single filter, i.e., the shape, also matters. Through
extensive experiments, we demonstrate that SWP is more effective compared to
the previous FP-based methods and achieves the state-of-art pruning ratio on
CIFAR-10 and ImageNet datasets without obvious accuracy drop. Code is available
at https://github.com/fxmeng/Pruning-Filter-in-Filter
</p>
<a href="http://arxiv.org/abs/2009.14410" target="_blank">arXiv:2009.14410</a> [<a href="http://arxiv.org/pdf/2009.14410" target="_blank">pdf</a>]

<h2>Assisting the Adversary to Improve GAN Training. (arXiv:2010.01274v2 [cs.LG] UPDATED)</h2>
<h3>Andreas Munk, William Harvey, Frank Wood</h3>
<p>Some of the most popular methods for improving the stability and performance
of GANs involve constraining or regularizing the discriminator. In this paper
we consider a largely overlooked regularization technique which we refer to as
the Adversary's Assistant (AdvAs). We motivate this using a different
perspective to that of prior work. Specifically, we consider a common mismatch
between theoretical analysis and practice: analysis often assumes that the
discriminator reaches its optimum on each iteration. In practice, this is
essentially never true, often leading to poor gradient estimates for the
generator. To address this, AdvAs is a theoretically motivated penalty imposed
on the generator based on the norm of the gradients used to train the
discriminator. This encourages the generator to move towards points where the
discriminator is optimal. We demonstrate the effect of applying AdvAs to
several GAN objectives, datasets and network architectures. The results
indicate a reduction in the mismatch between theory and practice and that AdvAs
can lead to improvement of GAN training, as measured by FID scores.
</p>
<a href="http://arxiv.org/abs/2010.01274" target="_blank">arXiv:2010.01274</a> [<a href="http://arxiv.org/pdf/2010.01274" target="_blank">pdf</a>]

<h2>A Transformer-based Framework for Multivariate Time Series Representation Learning. (arXiv:2010.02803v3 [cs.LG] UPDATED)</h2>
<h3>George Zerveas, Srideepika Jayaraman, Dhaval Patel, Anuradha Bhamidipaty, Carsten Eickhoff</h3>
<p>In this work we propose for the first time a transformer-based framework for
unsupervised representation learning of multivariate time series. Pre-trained
models can be potentially used for downstream tasks such as regression and
classification, forecasting and missing value imputation. By evaluating our
models on several benchmark datasets for multivariate time series regression
and classification, we show that not only does our modeling approach represent
the most successful method employing unsupervised learning of multivariate time
series presented to date, but also that it exceeds the current state-of-the-art
performance of supervised methods; it does so even when the number of training
samples is very limited, while offering computational efficiency. Finally, we
demonstrate that unsupervised pre-training of our transformer models offers a
substantial performance benefit over fully supervised learning, even without
leveraging additional unlabeled data, i.e., by reusing the same data samples
through the unsupervised objective.
</p>
<a href="http://arxiv.org/abs/2010.02803" target="_blank">arXiv:2010.02803</a> [<a href="http://arxiv.org/pdf/2010.02803" target="_blank">pdf</a>]

<h2>Can Federated Learning Save The Planet?. (arXiv:2010.06537v5 [cs.LG] UPDATED)</h2>
<h3>Xinchi Qiu, Titouan Parcollet, Daniel J. Beutel, Taner Topal, Akhil Mathur, Nicholas D. Lane</h3>
<p>Despite impressive results, deep learning-based technologies also raise
severe privacy and environmental concerns induced by the training procedure
often conducted in data centers. In response, alternatives to centralized
training such as Federated Learning (FL) have emerged. Perhaps unexpectedly, FL
in particular is starting to be deployed at a global scale by companies that
must adhere to new legal demands and policies originating from governments and
the civil society for privacy protection. However, the potential environmental
impact related to FL remains unclear and unexplored. This paper offers the
first-ever systematic study of the carbon footprint of FL. First, we propose a
rigorous model to quantify the carbon footprint, hence facilitating the
investigation of the relationship between FL design and carbon emissions. Then,
we compare the carbon footprint of FL to traditional centralized learning.
Finally, we highlight and connect the reported results to the future challenges
and trends in FL to reduce its environmental impact, including algorithms
efficiency, hardware capabilities, and stronger industry transparency.
</p>
<a href="http://arxiv.org/abs/2010.06537" target="_blank">arXiv:2010.06537</a> [<a href="http://arxiv.org/pdf/2010.06537" target="_blank">pdf</a>]

<h2>A Vector-based Representation to Enhance Head Pose Estimation. (arXiv:2010.07184v2 [cs.CV] UPDATED)</h2>
<h3>Zhiwen Cao, Zongcheng Chu, Dongfang Liu, Yingjie Chen</h3>
<p>This paper proposes to use the three vectors in a rotation matrix as the
representation in head pose estimation and develops a new neural network based
on the characteristic of such representation. We address two potential issues
existed in current head pose estimation works: 1. Public datasets for head pose
estimation use either Euler angles or quaternions to annotate data samples.
However, both of these annotations have the issue of discontinuity and thus
could result in some performance issues in neural network training. 2. Most
research works report Mean Absolute Error (MAE) of Euler angles as the
measurement of performance. We show that MAE may not reflect the actual
behavior especially for the cases of profile views. To solve these two
problems, we propose a new annotation method which uses three vectors to
describe head poses and a new measurement Mean Absolute Error of Vectors (MAEV)
to assess the performance. We also train a new neural network to predict the
three vectors with the constraints of orthogonality. Our proposed method
achieves state-of-the-art results on both AFLW2000 and BIWI datasets.
Experiments show our vector-based annotation method can effectively reduce
prediction errors for large pose angles.
</p>
<a href="http://arxiv.org/abs/2010.07184" target="_blank">arXiv:2010.07184</a> [<a href="http://arxiv.org/pdf/2010.07184" target="_blank">pdf</a>]

<h2>Very Fast Streaming Submodular Function Maximization. (arXiv:2010.10059v3 [cs.LG] UPDATED)</h2>
<h3>Sebastian Buschj&#xe4;ger, Philipp-Jan Honysz, Lukas Pfahler, Katharina Morik</h3>
<p>Data summarization has become a valuable tool in understanding even terabytes
of data. Due to their compelling theoretical properties, submodular functions
have been in the focus of summarization algorithms. These algorithms offer
worst-case approximations guarantees to the expense of higher computation and
memory requirements. However, many practical applications do not fall under
this worst-case, but are usually much more well-behaved. In this paper, we
propose a new submodular function maximization algorithm called ThreeSieves,
which ignores the worst-case, but delivers a good solution in high probability.
It selects the most informative items from a data-stream on the fly and
maintains a provable performance on a fixed memory budget. In an extensive
evaluation, we compare our method against $6$ other methods on $8$ different
datasets with and without concept drift. We show that our algorithm outperforms
current state-of-the-art algorithms and, at the same time, uses fewer
resources. Last, we highlight a real-world use-case of our algorithm for data
summarization in gamma-ray astronomy. We make our code publicly available at
https://github.com/sbuschjaeger/SubmodularStreamingMaximization.
</p>
<a href="http://arxiv.org/abs/2010.10059" target="_blank">arXiv:2010.10059</a> [<a href="http://arxiv.org/pdf/2010.10059" target="_blank">pdf</a>]

<h2>Tensor Train Random Projection. (arXiv:2010.10797v2 [stat.ML] UPDATED)</h2>
<h3>Yani Feng, Kejun Tang, Lianxing He, Pingqiang Zhou, Qifeng Liao</h3>
<p>This work proposes a novel tensor train random projection (TTRP) method for
dimension reduction, where the pairwise distances can be approximately
preserved. Based on the tensor train format, this new random projection method
can speed up the computation for high dimensional problems and requires less
storage with little loss in accuracy, compared with existing methods (e.g.,
very sparse random projection). Our TTRP is systematically constructed through
a rank-one TT-format with Rademacher random variables, which results in
efficient projection with small variances. The isometry property of TTRP is
proven in this work, and detailed numerical experiments with data sets
(synthetic, MNIST and CIFAR-10) are conducted to demonstrate the efficiency of
TTRP.
</p>
<a href="http://arxiv.org/abs/2010.10797" target="_blank">arXiv:2010.10797</a> [<a href="http://arxiv.org/pdf/2010.10797" target="_blank">pdf</a>]

<h2>A Comprehensive Study of Class Incremental Learning Algorithms for Visual Tasks. (arXiv:2011.01844v3 [cs.LG] UPDATED)</h2>
<h3>Eden Belouadah, Adrian Popescu, Ioannis Kanellos</h3>
<p>The ability of artificial agents to increment their capabilities when
confronted with new data is an open challenge in artificial intelligence. The
main challenge faced in such cases is catastrophic forgetting, i.e., the
tendency of neural networks to underfit past data when new ones are ingested. A
first group of approaches tackles forgetting by increasing deep model capacity
to accommodate new knowledge. A second type of approaches fix the deep model
size and introduce a mechanism whose objective is to ensure a good compromise
between stability and plasticity of the model. While the first type of
algorithms were compared thoroughly, this is not the case for methods which
exploit a fixed size model. Here, we focus on the latter, place them in a
common conceptual and experimental framework and propose the following
contributions: (1) define six desirable properties of incremental learning
algorithms and analyze them according to these properties, (2) introduce a
unified formalization of the class-incremental learning problem, (3) propose a
common evaluation framework which is more thorough than existing ones in terms
of number of datasets, size of datasets, size of bounded memory and number of
incremental states, (4) investigate the usefulness of herding for past
exemplars selection, (5) provide experimental evidence that it is possible to
obtain competitive performance without the use of knowledge distillation to
tackle catastrophic forgetting and (6) facilitate reproducibility by
integrating all tested methods in a common open-source repository. The main
experimental finding is that none of the existing algorithms achieves the best
results in all evaluated settings. Important differences arise notably if a
bounded memory of past classes is allowed or not.
</p>
<a href="http://arxiv.org/abs/2011.01844" target="_blank">arXiv:2011.01844</a> [<a href="http://arxiv.org/pdf/2011.01844" target="_blank">pdf</a>]

<h2>Generalized Negative Correlation Learning for Deep Ensembling. (arXiv:2011.02952v2 [cs.LG] UPDATED)</h2>
<h3>Sebastian Buschj&#xe4;ger, Lukas Pfahler, Katharina Morik</h3>
<p>Ensemble algorithms offer state of the art performance in many machine
learning applications. A common explanation for their excellent performance is
due to the bias-variance decomposition of the mean squared error which shows
that the algorithm's error can be decomposed into its bias and variance. Both
quantities are often opposed to each other and ensembles offer an effective way
to manage them as they reduce the variance through a diverse set of base
learners while keeping the bias low at the same time. Even though there have
been numerous works on decomposing other loss functions, the exact mathematical
connection is rarely exploited explicitly for ensembling, but merely used as a
guiding principle. In this paper, we formulate a generalized bias-variance
decomposition for arbitrary twice differentiable loss functions and study it in
the context of Deep Learning. We use this decomposition to derive a Generalized
Negative Correlation Learning (GNCL) algorithm which offers explicit control
over the ensemble's diversity and smoothly interpolates between the two
extremes of independent training and the joint training of the ensemble. We
show how GNCL encapsulates many previous works and discuss under which
circumstances training of an ensemble of Neural Networks might fail and what
ensembling method should be favored depending on the choice of the individual
networks. We make our code publicly available under
https://github.com/sbuschjaeger/gncl
</p>
<a href="http://arxiv.org/abs/2011.02952" target="_blank">arXiv:2011.02952</a> [<a href="http://arxiv.org/pdf/2011.02952" target="_blank">pdf</a>]

<h2>Kinematics-Guided Reinforcement Learning for Object-Aware 3D Ego-Pose Estimation. (arXiv:2011.04837v3 [cs.CV] UPDATED)</h2>
<h3>Zhengyi Luo, Ryo Hachiuma, Ye Yuan, Shun Iwase, Kris M. Kitani</h3>
<p>We propose a method for incorporating object interaction and human body
dynamics into the task of 3D ego-pose estimation using a head-mounted camera.
We use a kinematics model of the human body to represent the entire range of
human motion, and a dynamics model of the body to interact with objects inside
a physics simulator. By bringing together object modeling, kinematics modeling,
and dynamics modeling in a reinforcement learning (RL) framework, we enable
object-aware 3D ego-pose estimation. We devise several representational
innovations through the design of the state and action space to incorporate 3D
scene context and improve pose estimation quality. We also construct a
fine-tuning step to correct the drift and refine the estimated human-object
interaction. This is the first work to estimate a physically valid 3D full-body
interaction sequence with objects (e.g., chairs, boxes, obstacles) from
egocentric videos. Experiments with both controlled and in-the-wild settings
show that our method can successfully extract an object-conditioned 3D ego-pose
sequence that is consistent with the laws of physics.
</p>
<a href="http://arxiv.org/abs/2011.04837" target="_blank">arXiv:2011.04837</a> [<a href="http://arxiv.org/pdf/2011.04837" target="_blank">pdf</a>]

<h2>Stress Testing Method for Scenario Based Testing of Automated Driving Systems. (arXiv:2011.06553v2 [cs.RO] UPDATED)</h2>
<h3>Demin Nalic, Hexuan Li, Arno Eichberger, Christoph Wellershaus, Aleksa Pandurevic, Branko Rogic</h3>
<p>Classical approaches and procedures for testing of automated vehicles of SAE
levels 1 and 2 were based on defined scenarios with specific maneuvers,
depending on the function under test. For automated driving systems (ADS) of
SAE level 3+, the scenario space is infinite and calling for virtual testing
and verification. However, even in simulation, the generation of
safety-relevant scenarios for ADS is expensive and time-consuming. This leads
to a demand for stochastic and realistic traffic simulation. Therefore,
microscopic traffic flow simulation models (TFSM) are becoming a crucial part
of scenario-based testing of ADS. In this paper, a co-simulation between the
multi-body simulation software IPG CarMaker and the microscopic traffic flow
simulation software (TFSS) PTV Vissim is used. Although the TFSS could provide
realistic and stochastic behavior of the traffic participants, safety-critical
scenarios (SCS) occur rarely. In order to avoid this, a novel Stress Testing
Method (STM) is introduced. With this method, traffic participants are
manipulated via external driver DLL interface from PTV Vissim in the vicinity
of the vehicle under test in order to provoke defined critical maneuvers
derived from statistical accident data on highways in Austria. These external
driver models imitate human driving errors, resulting in an increase of
safety-critical scenarios. As a result, the presented STM method contributes to
an increase of safety-relevant scenarios for verification, testing and
assessment of ADS.
</p>
<a href="http://arxiv.org/abs/2011.06553" target="_blank">arXiv:2011.06553</a> [<a href="http://arxiv.org/pdf/2011.06553" target="_blank">pdf</a>]

<h2>Stein Variational Model Predictive Control. (arXiv:2011.07641v2 [cs.RO] UPDATED)</h2>
<h3>Alexander Lambert, Adam Fishman, Dieter Fox, Byron Boots, Fabio Ramos</h3>
<p>Decision making under uncertainty is critical to real-world, autonomous
systems. Model Predictive Control (MPC) methods have demonstrated favorable
performance in practice, but remain limited when dealing with complex
probability distributions. In this paper, we propose a generalization of MPC
that represents a multitude of solutions as posterior distributions. By casting
MPC as a Bayesian inference problem, we employ variational methods for
posterior computation, naturally encoding the complexity and multi-modality of
the decision making problem. We propose a Stein variational gradient descent
method to estimate the posterior directly over control parameters, given a cost
function and observed state trajectories. We show that this framework leads to
successful planning in challenging, non-convex optimal control problems.
</p>
<a href="http://arxiv.org/abs/2011.07641" target="_blank">arXiv:2011.07641</a> [<a href="http://arxiv.org/pdf/2011.07641" target="_blank">pdf</a>]

<h2>The Role of Edge Robotics As-a-Service in Monitoring COVID-19 Infection. (arXiv:2011.08482v3 [cs.RO] UPDATED)</h2>
<h3>Haimiao Mo, Shuai Ding (Member, IEEE), Shanlin Yang, Xi Zheng (Member, IEEE), Athanasios V. Vasilakos</h3>
<p>Deep learning technology has been widely used in edge computing. However,
pandemics like covid-19 require deep learning capabilities at mobile devices
(detect respiratory rate using mobile robotics or conduct CT scan using a
mobile scanner), which are severely constrained by the limited storage and
computation resources at the device level. To solve this problem, we propose a
three-tier architecture, including robot layers, edge layers, and cloud layers.
We adopt this architecture to design a non-contact respiratory monitoring
system to break down respiratory rate calculation tasks. Experimental results
of respiratory rate monitoring show that the proposed approach in this paper
significantly outperforms other approaches. It is supported by computation time
costs with 2.26 ms per frame, 27.48 ms per frame, 0.78 seconds for convolution
operation, similarity calculation, processing one-minute length respiratory
signals, respectively. And the computation time costs of our three-tier
architecture are less than that of edge+cloud architecture and cloud
architecture. Moreover, we use our three-tire architecture for CT image
diagnosis task decomposition. The evaluation of a CT image dataset of COVID-19
proves that our three-tire architecture is useful for resolving tasks on deep
learning networks by edge equipment. There are broad application scenarios in
smart hospitals in the future.
</p>
<a href="http://arxiv.org/abs/2011.08482" target="_blank">arXiv:2011.08482</a> [<a href="http://arxiv.org/pdf/2011.08482" target="_blank">pdf</a>]

<h2>Counterfactual Fairness with Disentangled Causal Effect Variational Autoencoder. (arXiv:2011.11878v2 [cs.LG] UPDATED)</h2>
<h3>Hyemi Kim, Seungjae Shin, JoonHo Jang, Kyungwoo Song, Weonyoung Joo, Wanmo Kang, Il-Chul Moon</h3>
<p>The problem of fair classification can be mollified if we develop a method to
remove the embedded sensitive information from the classification features.
This line of separating the sensitive information is developed through the
causal inference, and the causal inference enables the counterfactual
generations to contrast the what-if case of the opposite sensitive attribute.
Along with this separation with the causality, a frequent assumption in the
deep latent causal model defines a single latent variable to absorb the entire
exogenous uncertainty of the causal graph. However, we claim that such
structure cannot distinguish the 1) information caused by the intervention
(i.e., sensitive variable) and 2) information correlated with the intervention
from the data. Therefore, this paper proposes Disentangled Causal Effect
Variational Autoencoder (DCEVAE) to resolve this limitation by disentangling
the exogenous uncertainty into two latent variables: either 1) independent to
interventions or 2) correlated to interventions without causality.
Particularly, our disentangling approach preserves the latent variable
correlated to interventions in generating counterfactual examples. We show that
our method estimates the total effect and the counterfactual effect without a
complete causal graph. By adding a fairness regularization, DCEVAE generates a
counterfactual fair dataset while losing less original information. Also,
DCEVAE generates natural counterfactual images by only flipping sensitive
information. Additionally, we theoretically show the differences in the
covariance structures of DCEVAE and prior works from the perspective of the
latent disentanglement.
</p>
<a href="http://arxiv.org/abs/2011.11878" target="_blank">arXiv:2011.11878</a> [<a href="http://arxiv.org/pdf/2011.11878" target="_blank">pdf</a>]

<h2>A Deep Learning Bidirectional Temporal Tracking Algorithm for Automated Blood Cell Counting from Non-invasive Capillaroscopy Videos. (arXiv:2011.13371v2 [cs.CV] UPDATED)</h2>
<h3>Luojie Huang, Gregory N. McKay, Nicholas J. Durr</h3>
<p>Oblique back-illumination capillaroscopy has recently been introduced as a
method for high-quality, non-invasive blood cell imaging in human capillaries.
To make this technique practical for clinical blood cell counting, solutions
for automatic processing of acquired videos are needed. Here, we take the first
step towards this goal, by introducing a deep learning multi-cell tracking
model, named CycleTrack, which achieves accurate blood cell counting from
capillaroscopic videos. CycleTrack combines two simple online tracking models,
SORT and CenterTrack, and is tailored to features of capillary blood cell flow.
Blood cells are tracked by displacement vectors in two opposing temporal
directions (forward- and backward-tracking) between consecutive frames. This
approach yields accurate tracking despite rapidly moving and deforming blood
cells. The proposed model outperforms other baseline trackers, achieving 65.57%
Multiple Object Tracking Accuracy and 73.95% ID F1 score on test videos.
Compared to manual blood cell counting, CycleTrack achieves 96.58 $\pm$ 2.43%
cell counting accuracy among 8 test videos with 1000 frames each compared to
93.45% and 77.02% accuracy for independent CenterTrack and SORT almost without
additional time expense. It takes 800s to track and count approximately 8000
blood cells from 9,600 frames captured in a typical one-minute video. Moreover,
the blood cell velocity measured by CycleTrack demonstrates a consistent,
pulsatile pattern within the physiological range of heart rate. Lastly, we
discuss future improvements for the CycleTrack framework, which would enable
clinical translation of the oblique back-illumination microscope towards a
real-time and non-invasive point-of-care blood cell counting and analyzing
technology.
</p>
<a href="http://arxiv.org/abs/2011.13371" target="_blank">arXiv:2011.13371</a> [<a href="http://arxiv.org/pdf/2011.13371" target="_blank">pdf</a>]

<h2>Partial Gromov-Wasserstein Learning for Partial Graph Matching. (arXiv:2012.01252v2 [cs.LG] UPDATED)</h2>
<h3>Weijie Liu, Chao Zhang, Jiahao Xie, Zebang Shen, Hui Qian, Nenggan Zheng</h3>
<p>Graph matching finds the correspondence of nodes across two graphs and is a
basic task in graph-based machine learning. Numerous existing methods match
every node in one graph to one node in the other graph whereas two graphs
usually overlap partially in many \realworld{} applications. In this paper, a
partial Gromov-Wasserstein learning framework is proposed for partially
matching two graphs, which fuses the partial Gromov-Wasserstein distance and
the partial Wasserstein distance as the objective and updates the partial
transport map and the node embedding in an alternating fashion. The proposed
framework transports a fraction of the probability mass and matches node pairs
with high relative similarities across the two graphs. Incorporating an
embedding learning method, heterogeneous graphs can also be matched. Numerical
experiments on both synthetic and \realworld{} graphs demonstrate that our
framework can improve the F1 score by at least $20\%$ and often much more.
</p>
<a href="http://arxiv.org/abs/2012.01252" target="_blank">arXiv:2012.01252</a> [<a href="http://arxiv.org/pdf/2012.01252" target="_blank">pdf</a>]

<h2>Siamese Basis Function Networks for Defect Classification. (arXiv:2012.01338v4 [cs.CV] UPDATED)</h2>
<h3>Tobias Schlagenhauf, Faruk Yildirim, Benedikt Br&#xfc;ckner, J&#xfc;rgen Fleischer</h3>
<p>Defect classification on metallic surfaces is considered a critical issue
since substantial quantities of steel and other metals are processed by the
manufacturing industry on a daily basis. The authors propose a new approach
where they introduce the usage of so called Siamese Kernels in a Basis Function
Network to create the Siamese Basis Function Network (SBF-Network). The
underlying idea is to classify by comparison using similarity scores. This
classification is reinforced through efficient deep learning based feature
extraction methods. First, a center image is assigned to each Siamese Kernel.
The Kernels are then trained to generate encodings in a way that enables them
to distinguish their center from other images in the dataset. Using this
approach the authors created some kind of class-awareness inside the Siamese
Kernels. To classify a given image, each Siamese Kernel generates a feature
vector for its center as well as the given image. These vectors represent
encodings of the respective images in a lower-dimensional space. The distance
between each pair of encodings is then computed using the cosine distance
together with radial basis functions. The distances are fed into a multilayer
neural network to perform the classification. With this approach the authors
achieved outstanding results on the state of the art NEU surface defect
dataset.
</p>
<a href="http://arxiv.org/abs/2012.01338" target="_blank">arXiv:2012.01338</a> [<a href="http://arxiv.org/pdf/2012.01338" target="_blank">pdf</a>]

<h2>Batch Group Normalization. (arXiv:2012.02782v2 [cs.LG] UPDATED)</h2>
<h3>Xiao-Yun Zhou, Jiacheng Sun, Nanyang Ye, Xu Lan, Qijun Luo, Bo-Lin Lai, Pedro Esperanca, Guang-Zhong Yang, Zhenguo Li</h3>
<p>Deep Convolutional Neural Networks (DCNNs) are hard and time-consuming to
train. Normalization is one of the effective solutions. Among previous
normalization methods, Batch Normalization (BN) performs well at medium and
large batch sizes and is with good generalizability to multiple vision tasks,
while its performance degrades significantly at small batch sizes. In this
paper, we find that BN saturates at extreme large batch sizes, i.e., 128 images
per worker, i.e., GPU, as well and propose that the degradation/saturation of
BN at small/extreme large batch sizes is caused by noisy/confused statistic
calculation. Hence without adding new trainable parameters, using
multiple-layer or multi-iteration information, or introducing extra
computation, Batch Group Normalization (BGN) is proposed to solve the
noisy/confused statistic calculation of BN at small/extreme large batch sizes
with introducing the channel, height and width dimension to compensate. The
group technique in Group Normalization (GN) is used and a hyper-parameter G is
used to control the number of feature instances used for statistic calculation,
hence to offer neither noisy nor confused statistic for different batch sizes.
We empirically demonstrate that BGN consistently outperforms BN, Instance
Normalization (IN), Layer Normalization (LN), GN, and Positional Normalization
(PN), across a wide spectrum of vision tasks, including image classification,
Neural Architecture Search (NAS), adversarial learning, Few Shot Learning (FSL)
and Unsupervised Domain Adaptation (UDA), indicating its good performance,
robust stability to batch size and wide generalizability. For example, for
training ResNet-50 on ImageNet with a batch size of 2, BN achieves Top1
accuracy of 66.512% while BGN achieves 76.096% with notable improvement.
</p>
<a href="http://arxiv.org/abs/2012.02782" target="_blank">arXiv:2012.02782</a> [<a href="http://arxiv.org/pdf/2012.02782" target="_blank">pdf</a>]

<h2>Towards Better Object Detection in Scale Variation with Adaptive Feature Selection. (arXiv:2012.03265v2 [cs.CV] UPDATED)</h2>
<h3>Zehui Gong, Dong Li</h3>
<p>It is a common practice to exploit pyramidal feature representation to tackle
the problem of scale variation in object instances. However, most of them still
predict the objects in a certain range of scales based solely or mainly on a
single-level representation, yielding inferior detection performance. To this
end, we propose a novel adaptive feature selection module (AFSM), to
automatically learn the way to fuse multi-level representations in the channel
dimension, in a data-driven manner. It significantly improves the performance
of the detectors that have a feature pyramid structure, while introducing
nearly free inference overhead. Moreover, a class-aware sampling mechanism
(CASM) is proposed to tackle the class imbalance problem, by re-weighting the
sampling ratio to each of the training images, based on the statistical
characteristics of each class. This is crucial to improve the performance of
the minor classes. Experimental results demonstrate the effectiveness of the
proposed method, with 83.04% mAP at 15.96 FPS on the VOC dataset, and 39.48% AP
on the VisDrone-DET validation subset, respectively, outperforming other
state-of-the-art detectors considerably. The code is available at
https://github.com/ZeHuiGong/AFSM.git.
</p>
<a href="http://arxiv.org/abs/2012.03265" target="_blank">arXiv:2012.03265</a> [<a href="http://arxiv.org/pdf/2012.03265" target="_blank">pdf</a>]

<h2>Contrastive Divergence Learning is a Time Reversal Adversarial Game. (arXiv:2012.03295v2 [cs.LG] UPDATED)</h2>
<h3>Omer Yair, Tomer Michaeli</h3>
<p>Contrastive divergence (CD) learning is a classical method for fitting
unnormalized statistical models to data samples. Despite its wide-spread use,
the convergence properties of this algorithm are still not well understood. The
main source of difficulty is an unjustified approximation which has been used
to derive the gradient of the loss. In this paper, we present an alternative
derivation of CD that does not require any approximation and sheds new light on
the objective that is actually being optimized by the algorithm. Specifically,
we show that CD is an adversarial learning procedure, where a discriminator
attempts to classify whether a Markov chain generated from the model has been
time-reversed. Thus, although predating generative adversarial networks (GANs)
by more than a decade, CD is, in fact, closely related to these techniques. Our
derivation settles well with previous observations, which have concluded that
CD's update steps cannot be expressed as the gradients of any fixed objective
function. In addition, as a byproduct, our derivation reveals a simple
correction that can be used as an alternative to Metropolis-Hastings rejection,
which is required when the underlying Markov chain is inexact (e.g. when using
Langevin dynamics with a large step).
</p>
<a href="http://arxiv.org/abs/2012.03295" target="_blank">arXiv:2012.03295</a> [<a href="http://arxiv.org/pdf/2012.03295" target="_blank">pdf</a>]

<h2>Generalization bounds for deep learning. (arXiv:2012.04115v2 [stat.ML] UPDATED)</h2>
<h3>Guillermo Valle-P&#xe9;rez, Ard A. Louis</h3>
<p>Generalization in deep learning has been the topic of much recent theoretical
and empirical research. Here we introduce desiderata for techniques that
predict generalization errors for deep learning models in supervised learning.
Such predictions should 1) scale correctly with data complexity; 2) scale
correctly with training set size; 3) capture differences between architectures;
4) capture differences between optimization algorithms; 5) be quantitatively
not too far from the true error (in particular, be non-vacuous); 6) be
efficiently computable; and 7) be rigorous. We focus on generalization error
upper bounds, and introduce a categorisation of bounds depending on assumptions
on the algorithm and data. We review a wide range of existing approaches, from
classical VC dimension to recent PAC-Bayesian bounds, commenting on how well
they perform against the desiderata.

We next use a function-based picture to derive a marginal-likelihood
PAC-Bayesian bound. This bound is, by one definition, optimal up to a
multiplicative constant in the asymptotic limit of large training sets, as long
as the learning curve follows a power law, which is typically found in practice
for deep learning problems. Extensive empirical analysis demonstrates that our
marginal-likelihood PAC-Bayes bound fulfills desiderata 1-3 and 5. The results
for 6 and 7 are promising, but not yet fully conclusive, while only desideratum
4 is currently beyond the scope of our bound. Finally, we comment on why this
function-based bound performs significantly better than current parameter-based
PAC-Bayes bounds.
</p>
<a href="http://arxiv.org/abs/2012.04115" target="_blank">arXiv:2012.04115</a> [<a href="http://arxiv.org/pdf/2012.04115" target="_blank">pdf</a>]

<h2>Neural fidelity warping for efficient robot morphology design. (arXiv:2012.04195v2 [cs.RO] UPDATED)</h2>
<h3>Sha Hu, Zeshi Yang, Greg Mori</h3>
<p>We consider the problem of optimizing a robot morphology to achieve the best
performance for a target task, under computational resource limitations. The
evaluation process for each morphological design involves learning a controller
for the design, which can consume substantial time and computational resources.
To address the challenge of expensive robot morphology evaluation, we present a
continuous multi-fidelity Bayesian Optimization framework that efficiently
utilizes computational resources via low-fidelity evaluations. We identify the
problem of non-stationarity over fidelity space. Our proposed fidelity warping
mechanism can learn representations of learning epochs and tasks to model
non-stationary covariances between continuous fidelity evaluations which prove
challenging for off-the-shelf stationary kernels. Various experiments
demonstrate that our method can utilize the low-fidelity evaluations to
efficiently search for the optimal robot morphology, outperforming
state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2012.04195" target="_blank">arXiv:2012.04195</a> [<a href="http://arxiv.org/pdf/2012.04195" target="_blank">pdf</a>]

<h2>UnrealPerson: An Adaptive Pipeline towards Costless Person Re-identification. (arXiv:2012.04268v2 [cs.CV] UPDATED)</h2>
<h3>Tianyu Zhang, Lingxi Xie, Longhui Wei, Zijie Zhuang, Yongfei Zhang, Bo Li, Qi Tian</h3>
<p>The main difficulty of person re-identification (ReID) lies in collecting
annotated data and transferring the model across different domains. This paper
presents UnrealPerson, a novel pipeline that makes full use of unreal image
data to decrease the costs in both the training and deployment stages. Its
fundamental part is a system that can generate synthesized images of
high-quality and from controllable distributions. Instance-level annotation
goes with the synthesized data and is almost free. We point out some details in
image synthesis that largely impact the data quality. With 3,000 IDs and
120,000 instances, our method achieves a 38.5% rank-1 accuracy when being
directly transferred to MSMT17. It almost doubles the former record using
synthesized data and even surpasses previous direct transfer records using real
data. This offers a good basis for unsupervised domain adaption, where our
pre-trained model is easily plugged into the state-of-the-art algorithms
towards higher accuracy. In addition, the data distribution can be flexibly
adjusted to fit some corner ReID scenarios, which widens the application of our
pipeline. We will publish our data synthesis toolkit and synthesized data in
https://github.com/FlyHighest/UnrealPerson.
</p>
<a href="http://arxiv.org/abs/2012.04268" target="_blank">arXiv:2012.04268</a> [<a href="http://arxiv.org/pdf/2012.04268" target="_blank">pdf</a>]

<h2>Context-Aware Graph Convolution Network for Target Re-identification. (arXiv:2012.04298v2 [cs.CV] UPDATED)</h2>
<h3>Deyi Ji, Haoran Wang, Hanzhe Hu, Weihao Gan, Wei Wu, Junjie Yan</h3>
<p>Most existing re-identification methods focus on learning robust and
discriminative features with deep convolution networks. However, many of them
consider content similarity separately and fail to utilize the context
information of the query and gallery sets, e.g. probe-gallery and
gallery-gallery relations, thus hard samples may not be well solved due tothe
limited or even misleading information. In this paper,we present a novel
Context-Aware Graph Convolution Net-work (CAGCN), where the probe-gallery
relations are encoded into the graph nodes and the graph edge connections are
well controlled by the gallery-gallery relations. In this way, hard samples can
be addressed with the context information flows among other easy samples during
the graph reasoning. Specifically, we adopt an effective hard gallery sampler
to obtain high recall for positive samples while keeping a reasonable graph
size, which can also weaken the imbalanced problem in training process with low
computation complexity. Experiments show that the proposed method achieves
state-of-the-art performance on both person and vehicle re-identification
datasets in a plug and play fashion with limited overhead.
</p>
<a href="http://arxiv.org/abs/2012.04298" target="_blank">arXiv:2012.04298</a> [<a href="http://arxiv.org/pdf/2012.04298" target="_blank">pdf</a>]

<h2>Structure-Consistent Weakly Supervised Salient Object Detection with Local Saliency Coherence. (arXiv:2012.04404v2 [cs.CV] UPDATED)</h2>
<h3>Siyue Yu, Bingfeng Zhang, Jimin Xiao, Eng Gee Lim</h3>
<p>Sparse labels have been attracting much attention in recent years. However,
the performance gap between weakly supervised and fully supervised salient
object detection methods is huge, and most previous weakly supervised works
adopt complex training methods with many bells and whistles. In this work, we
propose a one-round end-to-end training approach for weakly supervised salient
object detection via scribble annotations without pre/post-processing
operations or extra supervision data. Since scribble labels fail to offer
detailed salient regions, we propose a local coherence loss to propagate the
labels to unlabeled regions based on image features and pixel distance, so as
to predict integral salient regions with complete object structures. We design
a saliency structure consistency loss as self-consistent mechanism to ensure
consistent saliency maps are predicted with different scales of the same image
as input, which could be viewed as a regularization technique to enhance the
model generalization ability. Additionally, we design an aggregation module
(AGGM) to better integrate high-level features, low-level features and global
context information for the decoder to aggregate various information. Extensive
experiments show that our method achieves a new state-of-the-art performance on
six benchmarks (e.g. for the ECSSD dataset: F_\beta = 0.8995, E_\xi = 0.9079
and MAE = 0.0489$), with an average gain of 4.60\% for F-measure, 2.05\% for
E-measure and 1.88\% for MAE over the previous best method on this task. Source
code is available at this http URL
</p>
<a href="http://arxiv.org/abs/2012.04404" target="_blank">arXiv:2012.04404</a> [<a href="http://arxiv.org/pdf/2012.04404" target="_blank">pdf</a>]

<h2>Combining Reinforcement Learning with Lin-Kernighan-Helsgaun Algorithm for the Traveling Salesman Problem. (arXiv:2012.04461v2 [cs.AI] UPDATED)</h2>
<h3>Jiongzhi Zheng, Kun He, Jianrong Zhou, Yan Jin, Chu-min Li</h3>
<p>We address the Traveling Salesman Problem (TSP), a famous NP-hard
combinatorial optimization problem. And we propose a variable strategy
reinforced approach, denoted as VSR-LKH, which combines three reinforcement
learning methods (Q-learning, Sarsa and Monte Carlo) with the well-known TSP
algorithm Lin-Kernighan-Helsgaun (LKH). VSR-LKH replaces the inflexible
traversal operation in LKH, and lets the program learn to make choice at each
search step by reinforcement learning. Experimental results on 111 TSP
benchmarks from the TSPLIB with up to 85,900 cities demonstrate the excellent
performance of the proposed method.
</p>
<a href="http://arxiv.org/abs/2012.04461" target="_blank">arXiv:2012.04461</a> [<a href="http://arxiv.org/pdf/2012.04461" target="_blank">pdf</a>]

<h2>MOCA: A Modular Object-Centric Approach for Interactive Instruction Following. (arXiv:2012.03208v1 [cs.AI] CROSS LISTED)</h2>
<h3>Kunal Pratap Singh, Suvaansh Bhambri, Byeonghwi Kim, Roozbeh Mottaghi, Jonghyun Choi</h3>
<p>Performing simple household tasks based on language directives is very
natural to humans, yet it remains an open challenge for an AI agent. Recently,
an `interactive instruction following' task has been proposed to foster
research in reasoning over long instruction sequences that requires object
interactions in a simulated environment. It involves solving open problems in
vision, language and navigation literature at each step. To address this
multifaceted problem, we propose a modular architecture that decouples the task
into visual perception and action policy, and name it as MOCA, a Modular
Object-Centric Approach. We evaluate our method on the ALFRED benchmark and
empirically validate that it outperforms prior arts by significant margins in
all metrics with good generalization performance (high success rate in unseen
environments). Our code is available at https://github.com/gistvision/moca.
</p>
<a href="http://arxiv.org/abs/2012.03208" target="_blank">arXiv:2012.03208</a> [<a href="http://arxiv.org/pdf/2012.03208" target="_blank">pdf</a>]

