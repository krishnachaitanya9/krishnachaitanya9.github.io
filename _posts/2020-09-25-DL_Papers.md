---
title: Latest Deep Learning Papers
date: 2020-11-18 21:00:01 +0800
featured-img: DL-logo
categories: [Deep_Learning]
tags: [Deep_Learning]
mathjax: Yes
---

<h1>Your arXiv Feed (165 Articles)</h1>
<h2>Towards Improved and Interpretable Deep Metric Learning via Attentive Grouping. (arXiv:2011.08877v1 [cs.CV])</h2>
<h3>Xinyi Xu, Zhangyang Wang, Cheng Deng, Hao Yuan, Shuiwang Ji</h3>
<p>Grouping has been commonly used in deep metric learning for computing diverse
features. However, current methods are prone to overfitting and lack
interpretability. In this work, we propose an improved and interpretable
grouping method to be integrated flexibly with any metric learning framework.
Our method is based on the attention mechanism with a learnable query for each
group. The query is fully trainable and can capture group-specific information
when combined with the diversity loss. An appealing property of our method is
that it naturally lends itself interpretability. The attention scores between
the learnable query and each spatial position can be interpreted as the
importance of that position. We formally show that our proposed grouping method
is invariant to spatial permutations of features. When used as a module in
convolutional neural networks, our method leads to translational invariance. We
conduct comprehensive experiments to evaluate our method. Our quantitative
results indicate that the proposed method outperforms prior methods
consistently and significantly across different datasets, evaluation metrics,
base models, and loss functions. For the first time to the best of our
knowledge, our interpretation results clearly demonstrate that the proposed
method enables the learning of distinct and diverse features across groups.
</p>
<a href="http://arxiv.org/abs/2011.08877" target="_blank">arXiv:2011.08877</a> [<a href="http://arxiv.org/pdf/2011.08877" target="_blank">pdf</a>]

<h2>Contrastive Registration for Unsupervised Medical Image Segmentation. (arXiv:2011.08894v1 [cs.CV])</h2>
<h3>Lihao Liu, Angelica I Aviles-Rivero, Carola-Bibiane Sch&#xf6;nlieb</h3>
<p>Medical image segmentation is a relevant task as it serves as the first step
for several diagnosis processes, thus it is indispensable in clinical usage.
Whilst major success has been reported using supervised techniques, they assume
a large and well-representative labelled set. This is a strong assumption in
the medical domain where annotations are expensive, time-consuming, and
inherent to human bias. To address this problem, unsupervised techniques have
been proposed in the literature yet it is still an open problem due to the
difficulty of learning any transformation pattern. In this work, we present a
novel optimisation model framed into a new CNN-based contrastive registration
architecture for unsupervised medical image segmentation. The core of our
approach is to exploit image-level registration and feature-level from a
contrastive learning mechanism, to perform registration-based segmentation.
Firstly, we propose an architecture to capture the image-to-image
transformation pattern via registration for unsupervised medical image
segmentation. Secondly, we embed a contrastive learning mechanism into the
registration architecture to enhance the discriminating capacity of the network
in the feature-level. We show that our proposed technique mitigates the major
drawbacks of existing unsupervised techniques. We demonstrate, through
numerical and visual experiments, that our technique substantially outperforms
the current state-of-the-art unsupervised segmentation methods on two major
medical image datasets.
</p>
<a href="http://arxiv.org/abs/2011.08894" target="_blank">arXiv:2011.08894</a> [<a href="http://arxiv.org/pdf/2011.08894" target="_blank">pdf</a>]

<h2>ZORB: A Derivative-Free Backpropagation Algorithm for Neural Networks. (arXiv:2011.08895v1 [cs.LG])</h2>
<h3>Varun Ranganathan, Alex Lewandowski</h3>
<p>Gradient descent and backpropagation have enabled neural networks to achieve
remarkable results in many real-world applications. Despite ongoing success,
training a neural network with gradient descent can be a slow and strenuous
affair. We present a simple yet faster training algorithm called Zeroth-Order
Relaxed Backpropagation (ZORB). Instead of calculating gradients, ZORB uses the
pseudoinverse of targets to backpropagate information. ZORB is designed to
reduce the time required to train deep neural networks without penalizing
performance. To illustrate the speed up, we trained a feed-forward neural
network with 11 layers on MNIST and observed that ZORB converged 300 times
faster than Adam while achieving a comparable error rate, without any
hyperparameter tuning. We also broaden the scope of ZORB to convolutional
neural networks, and apply it to subsamples of the CIFAR-10 dataset.
Experiments on standard classification and regression benchmarks demonstrate
ZORB's advantage over traditional backpropagation with Gradient Descent.
</p>
<a href="http://arxiv.org/abs/2011.08895" target="_blank">arXiv:2011.08895</a> [<a href="http://arxiv.org/pdf/2011.08895" target="_blank">pdf</a>]

<h2>Probing Fairness of Mobile Ocular Biometrics Methods Across Gender on VISOB 2.0 Dataset. (arXiv:2011.08898v1 [cs.CV])</h2>
<h3>Anoop Krishnan, Ali Almadan, Ajita Rattani</h3>
<p>Recent research has questioned the fairness of face-based recognition and
attribute classification methods (such as gender and race) for dark-skinned
people and women. Ocular biometrics in the visible spectrum is an alternate
solution over face biometrics, thanks to its accuracy, security, robustness
against facial expression, and ease of use in mobile devices. With the recent
COVID-19 crisis, ocular biometrics has a further advantage over face biometrics
in the presence of a mask. However, fairness of ocular biometrics has not been
studied till now. This first study aims to explore the fairness of ocular-based
authentication and gender classification methods across males and females. To
this aim, VISOB $2.0$ dataset, along with its gender annotations, is used for
the fairness analysis of ocular biometrics methods based on ResNet-50,
MobileNet-V2 and lightCNN-29 models. Experimental results suggest the
equivalent performance of males and females for ocular-based mobile
user-authentication in terms of genuine match rate (GMR) at lower false match
rates (FMRs) and an overall Area Under Curve (AUC). For instance, an AUC of
0.96 for females and 0.95 for males was obtained for lightCNN-29 on an average.
However, males significantly outperformed females in deep learning based gender
classification models based on ocular-region.
</p>
<a href="http://arxiv.org/abs/2011.08898" target="_blank">arXiv:2011.08898</a> [<a href="http://arxiv.org/pdf/2011.08898" target="_blank">pdf</a>]

<h2>Multimodal Prototypical Networks for Few-shot Learning. (arXiv:2011.08899v1 [cs.CV])</h2>
<h3>Frederik Pahde, Mihai Puscas, Tassilo Klein, Moin Nabi</h3>
<p>Although providing exceptional results for many computer vision tasks,
state-of-the-art deep learning algorithms catastrophically struggle in low data
scenarios. However, if data in additional modalities exist (e.g. text) this can
compensate for the lack of data and improve the classification results. To
overcome this data scarcity, we design a cross-modal feature generation
framework capable of enriching the low populated embedding space in few-shot
scenarios, leveraging data from the auxiliary modality. Specifically, we train
a generative model that maps text data into the visual feature space to obtain
more reliable prototypes. This allows to exploit data from additional
modalities (e.g. text) during training while the ultimate task at test time
remains classification with exclusively visual data. We show that in such cases
nearest neighbor classification is a viable approach and outperform
state-of-the-art single-modal and multimodal few-shot learning methods on the
CUB-200 and Oxford-102 datasets.
</p>
<a href="http://arxiv.org/abs/2011.08899" target="_blank">arXiv:2011.08899</a> [<a href="http://arxiv.org/pdf/2011.08899" target="_blank">pdf</a>]

<h2>Whose hand is this? Person Identification from Egocentric Hand Gestures. (arXiv:2011.08900v1 [cs.CV])</h2>
<h3>Satoshi Tsutsui, Yanwei Fu, David Crandall</h3>
<p>Recognizing people by faces and other biometrics has been extensively studied
in computer vision. But these techniques do not work for identifying the wearer
of an egocentric (first-person) camera because that person rarely (if ever)
appears in their own first-person view. But while one's own face is not
frequently visible, their hands are: in fact, hands are among the most common
objects in one's own field of view. It is thus natural to ask whether the
appearance and motion patterns of people's hands are distinctive enough to
recognize them. In this paper, we systematically study the possibility of
Egocentric Hand Identification (EHI) with unconstrained egocentric hand
gestures. We explore several different visual cues, including color, shape,
skin texture, and depth maps to identify users' hands. Extensive ablation
experiments are conducted to analyze the properties of hands that are most
distinctive. Finally, we show that EHI can improve generalization of other
tasks, such as gesture recognition, by training adversarially to encourage
these models to ignore differences between users.
</p>
<a href="http://arxiv.org/abs/2011.08900" target="_blank">arXiv:2011.08900</a> [<a href="http://arxiv.org/pdf/2011.08900" target="_blank">pdf</a>]

<h2>SIENA: Stochastic Multi-Expert Neural Patcher. (arXiv:2011.08908v1 [cs.LG])</h2>
<h3>Thai Le, Noseong Park, Dongwon Lee</h3>
<p>Neural network (NN) models that are solely trained to maximize the likelihood
of an observed dataset are often vulnerable to adversarial attacks. Even though
several methods have been proposed to enhance NN models' adversarial
robustness, they often require re-training from scratch. This leads to
redundant computation, especially in the NLP domain where current
state-of-the-art models, such as BERT and ROBERTA, require great time and space
resources. By borrowing ideas from Software Engineering, we, therefore, first
introduce the Neural Patching mechanism to improve adversarial robustness by
"patching" only parts of a NN model. Then, we propose a novel neural patching
algorithm, SIENA, that transforms a textual NN model into a stochastic ensemble
of multi-expert predictors by upgrading and re-training its last layer only.
SIENA forces adversaries to attack not only one but multiple models that are
specialized in diverse sub-sets of features, labels, and instances so that the
ensemble model becomes more robust to adversarial attacks. By conducting
comprehensive experiments, we demonstrate that all of CNN, RNN, BERT, and
ROBERTA-based textual models, once patched by SIENA, witness an absolute
increase of as much as 20% in accuracy on average under 5 different white and
black-box attacks, outperforming 6 defensive baselines across 4 public NLP
datasets.
</p>
<a href="http://arxiv.org/abs/2011.08908" target="_blank">arXiv:2011.08908</a> [<a href="http://arxiv.org/pdf/2011.08908" target="_blank">pdf</a>]

<h2>C-Learning: Learning to Achieve Goals via Recursive Classification. (arXiv:2011.08909v1 [cs.LG])</h2>
<h3>Benjamin Eysenbach, Ruslan Salakhutdinov, Sergey Levine</h3>
<p>We study the problem of predicting and controlling the future state
distribution of an autonomous agent. This problem, which can be viewed as a
reframing of goal-conditioned reinforcement learning (RL), is centered around
learning a conditional probability density function over future states. Instead
of directly estimating this density function, we indirectly estimate this
density function by training a classifier to predict whether an observation
comes from the future. Via Bayes' rule, predictions from our classifier can be
transformed into predictions over future states. Importantly, an off-policy
variant of our algorithm allows us to predict the future state distribution of
a new policy, without collecting new experience. This variant allows us to
optimize functionals of a policy's future state distribution, such as the
density of reaching a particular goal state. While conceptually similar to
Q-learning, our work lays a principled foundation for goal-conditioned RL as
density estimation, providing justification for goal-conditioned methods used
in prior work. This foundation makes hypotheses about Q-learning, including the
optimal goal-sampling ratio, which we confirm experimentally. Moreover, our
proposed method is competitive with prior goal-conditioned RL methods.
</p>
<a href="http://arxiv.org/abs/2011.08909" target="_blank">arXiv:2011.08909</a> [<a href="http://arxiv.org/pdf/2011.08909" target="_blank">pdf</a>]

<h2>Deep Learning Benchmarks and Datasets for Social Media Image Classification for Disaster Response. (arXiv:2011.08916v1 [cs.CV])</h2>
<h3>Firoj Alam, Ferda Ofli, Muhammad Imran, Tanvirul Alam, Umair Qazi</h3>
<p>During a disaster event, images shared on social media helps crisis managers
gain situational awareness and assess incurred damages, among other response
tasks. Recent advances in computer vision and deep neural networks have enabled
the development of models for real-time image classification for a number of
tasks, including detecting crisis incidents, filtering irrelevant images,
classifying images into specific humanitarian categories, and assessing the
severity of damage. Despite several efforts, past works mainly suffer from
limited resources (i.e., labeled images) available to train more robust deep
learning models. In this study, we propose new datasets for disaster type
detection, and informativeness classification, and damage severity assessment.
Moreover, we relabel existing publicly available datasets for new tasks. We
identify exact- and near-duplicates to form non-overlapping data splits, and
finally consolidate them to create larger datasets. In our extensive
experiments, we benchmark several state-of-the-art deep learning models and
achieve promising results. We release our datasets and models publicly, aiming
to provide proper baselines as well as to spur further research in the crisis
informatics community.
</p>
<a href="http://arxiv.org/abs/2011.08916" target="_blank">arXiv:2011.08916</a> [<a href="http://arxiv.org/pdf/2011.08916" target="_blank">pdf</a>]

<h2>TreeGen -- a Monte Carlo generator for data frames. (arXiv:2011.08922v1 [cs.LG])</h2>
<h3>Agnieszka Niemczynowicz, Gabriela Bia&#x142;osk&#xf3;rska, Joanna Nie&#x17c;urawska-Zaj&#x105;c, Rados&#x142;aw A. Kycia</h3>
<p>The typical problem in Data Science is creating a structure that encodes the
occurrence frequency of unique elements in rows and relations between different
rows of a data frame. We present the probability tree abstract data structure,
an extension of the decision tree, that facilitates more than two choices with
assigned probabilities. Such a tree represents statistical relations between
different rows of the data frame. The Probability Tree algorithmic structure is
supplied with the Generator module that is a Monte Carlo generator that
traverses through the tree. These two components are implemented in TreeGen
Python package. The package can be used in increasing data multiplicity,
compressing data preserving its statistical information, constructing
hierarchical models, exploring data, and in feature extraction.
</p>
<a href="http://arxiv.org/abs/2011.08922" target="_blank">arXiv:2011.08922</a> [<a href="http://arxiv.org/pdf/2011.08922" target="_blank">pdf</a>]

<h2>A New Dataset and Proposed Convolutional Neural Network Architecture for Classification of American Sign Language Digits. (arXiv:2011.08927v1 [cs.CV])</h2>
<h3>Arda Mavi</h3>
<p>In our interviews with people who work with speech impaired persons, we
learned that speech impaired people have difficulties in communicating with
other people around them who do not know the sign language, and this situation
may cause them to isolate themselves from society and lose their sense of
independence. With this paper, to increase the quality of life of individuals
with facilitating communication between individuals who use sign language and
who do not know this language, we created a new American Sign Language (ASL)
digits dataset that can help to create machine learning algorithms which need
to large and varied data to be successful, we published this dataset as Sign
Language Digits Dataset on Kaggle Datasets web page, we present a proposal
Convolutional Neural Network (CNN) architecture that can get 98% test accuracy
on our dataset, and we compared it with popular CNN models.
</p>
<a href="http://arxiv.org/abs/2011.08927" target="_blank">arXiv:2011.08927</a> [<a href="http://arxiv.org/pdf/2011.08927" target="_blank">pdf</a>]

<h2>Distributed Online Learning with Multiple Kernels. (arXiv:2011.08930v1 [cs.LG])</h2>
<h3>Jeongmin Chae, Songnam Hong</h3>
<p>In the Internet-of-Things (IoT) systems, there are plenty of informative data
provided by a massive number of IoT devices (e.g., sensors). Learning a
function from such data is of great interest in machine learning tasks for IoT
systems. Focusing on streaming (or sequential) data, we present a
privacy-preserving distributed online learning framework with multiplekernels
(named DOMKL). The proposed DOMKL is devised by leveraging the principles of an
online alternating direction of multipliers (OADMM) and a distributed Hedge
algorithm. We theoretically prove that DOMKL over T time slots can achieve an
optimal sublinear regret, implying that every learned function achieves the
performance of the best function in hindsight as in the state-of-the-art
centralized online learning method. Moreover, it is ensured that the learned
functions of any two neighboring learners have a negligible difference as T
grows, i.e., the so-called consensus constraints hold. Via experimental tests
with various real datasets, we verify the effectiveness of the proposed DOMKL
on regression and time-series prediction tasks.
</p>
<a href="http://arxiv.org/abs/2011.08930" target="_blank">arXiv:2011.08930</a> [<a href="http://arxiv.org/pdf/2011.08930" target="_blank">pdf</a>]

<h2>Analyzing and Mitigating Compression Defects in Deep Learning. (arXiv:2011.08932v1 [cs.CV])</h2>
<h3>Max Ehrlich, Larry Davis, Ser-Nam Lim, Abhinav Shrivastava</h3>
<p>With the proliferation of deep learning methods, many computer vision
problems which were considered academic are now viable in the consumer setting.
One drawback of consumer applications is lossy compression, which is necessary
from an engineering standpoint to efficiently and cheaply store and transmit
user images. Despite this, there has been little study of the effect of
compression on deep neural networks and benchmark datasets are often losslessly
compressed or compressed at high quality. Here we present a unified study of
the effects of JPEG compression on a range of common tasks and datasets. We
show that there is a significant penalty on common performance metrics for high
compression. We test several methods for mitigating this penalty, including a
novel method based on artifact correction which requires no labels to train.
</p>
<a href="http://arxiv.org/abs/2011.08932" target="_blank">arXiv:2011.08932</a> [<a href="http://arxiv.org/pdf/2011.08932" target="_blank">pdf</a>]

<h2>Dual-stream Multiple Instance Learning Network for Whole Slide Image Classification with Self-supervised Contrastive Learning. (arXiv:2011.08939v1 [cs.CV])</h2>
<h3>Bin Li, Yin Li, Kevin W. Eliceiri</h3>
<p>Whole slide images (WSIs) have large resolutions and usually lack localized
annotations. WSI classification can be cast as a multiple instance learning
(MIL) problem when only slide-level labels are available. We propose a
MIL-based method for WSI classification and tumor detection in WSI that does
not require localized annotations. First, we propose a novel MIL aggregator
that models the relations of the instances in a dual-stream architecture with
trainable distance measurement. Second, since WSIs can produce large or
unbalanced bags that hinder the training of MIL models, we propose to use
self-supervised contrastive learning to extract good representations for MIL
and alleviate the issue of prohibitive memory requirement for large bags.
Third, we propose a pyramidal fusion mechanism for multiscale WSI features that
further improves the classification and localization accuracy. The
classification accuracy of our model compares favorably to fully-supervised
methods, with less than 2\% accuracy gap on two representative WSI datasets,
and outperforms all previous MIL-based methods. Benchmark results on standard
MIL datasets further show the superior performance of our MIL aggregator over
other MIL models on general MIL problems.
</p>
<a href="http://arxiv.org/abs/2011.08939" target="_blank">arXiv:2011.08939</a> [<a href="http://arxiv.org/pdf/2011.08939" target="_blank">pdf</a>]

<h2>Near-Optimal Multi-Robot Motion Planning with Finite Sampling. (arXiv:2011.08944v1 [cs.RO])</h2>
<h3>Dror Dayan, Kiril Solovey, Marco Pavone, Dan Halperin</h3>
<p>An underlying structure in several sampling-based methods for continuous
multi-robot motion planning (MRMP) is the tensor roadmap (TR), which emerges
from combining multiple PRM graphs constructed for the individual robots via a
tensor product. We study the conditions under which the TR encodes a
near-optimal solution for MRMP---satisfying these conditions implies near
optimality for a variety of popular planners, including dRRT*, and the discrete
methods M* and CBS when applied to the continuous domain. We develop the first
finite-sample analysis of this kind, which specifies the number of samples,
their deterministic distribution, and magnitude of the connection radii that
should be used by each individual PRM graph, to guarantee near-optimality using
the TR. This significantly improves upon a previous asymptotic analysis,
wherein the number of samples tends to infinity, and supports guaranteed
high-quality solutions in practice, within bounded running time. To achieve our
new result, we first develop a sampling scheme, which we call the staggered
grid, for finite-sample motion planning for individual robots, which requires
significantly less samples than previous work. We then extend it to the much
more involved MRMP setting which requires to account for interactions among
multiple robots. Finally, we report on a few experiments that serve as a
verification of our theoretical findings and raise interesting questions for
further investigation.
</p>
<a href="http://arxiv.org/abs/2011.08944" target="_blank">arXiv:2011.08944</a> [<a href="http://arxiv.org/pdf/2011.08944" target="_blank">pdf</a>]

<h2>Multi-agent Reinforcement Learning Accelerated MCMC on Multiscale Inversion Problem. (arXiv:2011.08954v1 [cs.LG])</h2>
<h3>Eric Chung, Yalchin Efendiev, Wing Tat Leung, Sai-Mang Pun, Zecheng Zhang</h3>
<p>In this work, we propose a multi-agent actor-critic reinforcement learning
(RL) algorithm to accelerate the multi-level Monte Carlo Markov Chain (MCMC)
sampling algorithms. The policies (actors) of the agents are used to generate
the proposal in the MCMC steps; and the critic, which is centralized, is in
charge of estimating the long term reward. We verify our proposed algorithm by
solving an inverse problem with multiple scales. There are several difficulties
in the implementation of this problem by using traditional MCMC sampling.
Firstly, the computation of the posterior distribution involves evaluating the
forward solver, which is very time consuming for a problem with heterogeneous.
We hence propose to use the multi-level algorithm. More precisely, we use the
generalized multiscale finite element method (GMsFEM) as the forward solver in
evaluating a posterior distribution in the multi-level rejection procedure.
Secondly, it is hard to find a function which can generate samplings which are
meaningful. To solve this issue, we learn an RL policy as the proposal
generator. Our experiments show that the proposed method significantly improves
the sampling process
</p>
<a href="http://arxiv.org/abs/2011.08954" target="_blank">arXiv:2011.08954</a> [<a href="http://arxiv.org/pdf/2011.08954" target="_blank">pdf</a>]

<h2>Reactive Human-to-Robot Handovers of Arbitrary Objects. (arXiv:2011.08961v1 [cs.RO])</h2>
<h3>Wei Yang, Chris Paxton, Arsalan Mousavian, Yu-Wei Chao, Maya Cakmak, Dieter Fox</h3>
<p>Human-robot object handovers have been an actively studied area of robotics
over the past decade; however, very few techniques and systems have addressed
the challenge of handing over diverse objects with arbitrary appearance, size,
shape, and rigidity. In this paper, we present a vision-based system that
enables reactive human-to-robot handovers of unknown objects. Our approach
combines closed-loop motion planning with real-time, temporally-consistent
grasp generation to ensure reactivity and motion smoothness. Our system is
robust to different object positions and orientations, and can grasp both rigid
and non-rigid objects. We demonstrate the generalizability, usability, and
robustness of our approach on a novel benchmark set of 26 diverse household
objects, a user study with naive users (N=6) handing over a subset of 15
objects, and a systematic evaluation examining different ways of handing
objects. More results and videos can be found at
https://sites.google.com/nvidia.com/handovers-of-arbitrary-objects.
</p>
<a href="http://arxiv.org/abs/2011.08961" target="_blank">arXiv:2011.08961</a> [<a href="http://arxiv.org/pdf/2011.08961" target="_blank">pdf</a>]

<h2>Contrastive Weight Regularization for Large Minibatch SGD. (arXiv:2011.08968v1 [cs.LG])</h2>
<h3>Qiwei Yuan, Weizhe Hua, Yi Zhou, Cunxi Yu</h3>
<p>The minibatch stochastic gradient descent method (SGD) is widely applied in
deep learning due to its efficiency and scalability that enable training deep
networks with a large volume of data. Particularly in the distributed setting,
SGD is usually applied with large batch size. However, as opposed to
small-batch SGD, neural network models trained with large-batch SGD can hardly
generalize well, i.e., the validation accuracy is low. In this work, we
introduce a novel regularization technique, namely distinctive regularization
(DReg), which replicates a certain layer of the deep network and encourages the
parameters of both layers to be diverse. The DReg technique introduces very
little computation overhead. Moreover, we empirically show that optimizing the
neural network with DReg using large-batch SGD achieves a significant boost in
the convergence and improved generalization performance. We also demonstrate
that DReg can boost the convergence of large-batch SGD with momentum. We
believe that DReg can be used as a simple regularization trick to accelerate
large-batch training in deep learning.
</p>
<a href="http://arxiv.org/abs/2011.08968" target="_blank">arXiv:2011.08968</a> [<a href="http://arxiv.org/pdf/2011.08968" target="_blank">pdf</a>]

<h2>A User's Guide to Calibrating Robotics Simulators. (arXiv:2011.08985v1 [cs.LG])</h2>
<h3>Bhairav Mehta, Ankur Handa, Dieter Fox, Fabio Ramos</h3>
<p>Simulators are a critical component of modern robotics research. Strategies
for both perception and decision making can be studied in simulation first
before deployed to real world systems, saving on time and costs. Despite
significant progress on the development of sim-to-real algorithms, the analysis
of different methods is still conducted in an ad-hoc manner, without a
consistent set of tests and metrics for comparison. This paper fills this gap
and proposes a set of benchmarks and a framework for the study of various
algorithms aimed to transfer models and policies learnt in simulation to the
real world. We conduct experiments on a wide range of well known simulated
environments to characterize and offer insights into the performance of
different algorithms. Our analysis can be useful for practitioners working in
this area and can help make informed choices about the behavior and main
properties of sim-to-real algorithms. We open-source the benchmark, training
data, and trained models, which can be found at
https://github.com/NVlabs/sim-parameter-estimation.
</p>
<a href="http://arxiv.org/abs/2011.08985" target="_blank">arXiv:2011.08985</a> [<a href="http://arxiv.org/pdf/2011.08985" target="_blank">pdf</a>]

<h2>Minimal Solvers for Single-View Lens-Distorted Camera Auto-Calibration. (arXiv:2011.08988v1 [cs.CV])</h2>
<h3>Yaroslava Lochman, Oles Dobosevych, Rostyslav Hryniv, James Pritts</h3>
<p>This paper proposes minimal solvers that use combinations of imaged
translational symmetries and parallel scene lines to jointly estimate lens
undistortion with either affine rectification or focal length and absolute
orientation. We use constraints provided by orthogonal scene planes to recover
the focal length. We show that solvers using feature combinations can recover
more accurate calibrations than solvers using only one feature type on scenes
that have a balance of lines and texture. We also show that the proposed
solvers are complementary and can be used together in a RANSAC-based estimator
to improve auto-calibration accuracy. State-of-the-art performance is
demonstrated on a standard dataset of lens-distorted urban images. The code is
available at https://github.com/ylochman/single-view-autocalib.
</p>
<a href="http://arxiv.org/abs/2011.08988" target="_blank">arXiv:2011.08988</a> [<a href="http://arxiv.org/pdf/2011.08988" target="_blank">pdf</a>]

<h2>Towards Spatial Variability Aware Deep Neural Networks (SVANN): A Summary of Results. (arXiv:2011.08992v1 [cs.CV])</h2>
<h3>Jayant Gupta (1), Yiqun Xie (1), Shashi Shekhar (1) ((1) University of Minnesota)</h3>
<p>Spatial variability has been observed in many geo-phenomena including
climatic zones, USDA plant hardiness zones, and terrestrial habitat types
(e.g., forest, grasslands, wetlands, and deserts). However, current deep
learning methods follow a spatial-one-size-fits-all(OSFA) approach to train
single deep neural network models that do not account for spatial variability.
In this work, we propose and investigate a spatial-variability aware deep
neural network(SVANN) approach, where distinct deep neural network models are
built for each geographic area. We evaluate this approach using aerial imagery
from two geographic areas for the task of mapping urban gardens. The
experimental results show that SVANN provides better performance than OSFA in
terms of precision, recall,and F1-score to identify urban gardens.
</p>
<a href="http://arxiv.org/abs/2011.08992" target="_blank">arXiv:2011.08992</a> [<a href="http://arxiv.org/pdf/2011.08992" target="_blank">pdf</a>]

<h2>PassGoodPool: Joint Passengers and Goods Fleet Management with Reinforcement Learning aided Pricing, Matching, and Route Planning. (arXiv:2011.08999v1 [cs.AI])</h2>
<h3>Kaushik Manchella, Marina Haliem, Vaneet Aggarwal, Bharat Bhargava</h3>
<p>In this paper, we present a dynamic, demand aware, and pricing-based matching
and route planning framework that allows efficient pooling of multiple
passengers and goods in each vehicle. This approach includes the flexibility
for transferring goods through multiple hops from source to destination as well
as pooling of passengers. The key components of the proposed approach include
(i) Pricing by the vehicles to passengers which is based on the insertion cost,
which determines the matching based on passenger's acceptance/rejection, (ii)
Matching of goods to vehicles, and the multi-hop routing of goods, (iii) Route
planning of the vehicles to pick. up and drop passengers and goods, (i)
Dispatching idle vehicles to areas of anticipated high passenger and goods
demand using Deep Reinforcement Learning, and (v) Allowing for distributed
inference at each vehicle while collectively optimizing fleet objectives. Our
proposed framework can be deployed independently within each vehicle as this
minimizes computational costs associated with the gorwth of distributed systems
and democratizes decision-making to each individual. The proposed framework is
validated in a simulated environment, where we leverage realistic delivery
datasets such as the New York City Taxi public dataset and Google Maps traffic
data from delivery offering businesses.Simulations on a variety of vehicle
types, goods, and passenger utility functions show the effectiveness of our
approach as compared to baselines that do not consider combined load
transportation or dynamic multi-hop route planning. Our proposed method showed
improvements over the next best baseline in various aspects including a 15%
increase in fleet utilization and 20% increase in average vehicle profits.
</p>
<a href="http://arxiv.org/abs/2011.08999" target="_blank">arXiv:2011.08999</a> [<a href="http://arxiv.org/pdf/2011.08999" target="_blank">pdf</a>]

<h2>Explaining Conditions for Reinforcement Learning Behaviors from Real and Imagined Data. (arXiv:2011.09004v1 [cs.LG])</h2>
<h3>Aastha Acharya, Rebecca Russell, Nisar R. Ahmed</h3>
<p>The deployment of reinforcement learning (RL) in the real world comes with
challenges in calibrating user trust and expectations. As a step toward
developing RL systems that are able to communicate their competencies, we
present a method of generating human-interpretable abstract behavior models
that identify the experiential conditions leading to different task execution
strategies and outcomes. Our approach consists of extracting experiential
features from state representations, abstracting strategy descriptors from
trajectories, and training an interpretable decision tree that identifies the
conditions most predictive of different RL behaviors. We demonstrate our method
on trajectory data generated from interactions with the environment and on
imagined trajectory data that comes from a trained probabilistic world model in
a model-based RL setting.
</p>
<a href="http://arxiv.org/abs/2011.09004" target="_blank">arXiv:2011.09004</a> [<a href="http://arxiv.org/pdf/2011.09004" target="_blank">pdf</a>]

<h2>On the Relationship Between KR Approaches for Explainable Planning. (arXiv:2011.09006v1 [cs.AI])</h2>
<h3>Stylianos Loukas Vasileiou, William Yeoh, Tran Cao Son</h3>
<p>In this paper, we build upon notions from \emph{knowledge representation and
reasoning} (KR) to expand a preliminary logic-based framework that
characterizes the model reconciliation problem for explainable planning. We
also provide a detailed exposition on the relationship between similar KR
techniques, such as abductive explanations and belief change, and their
applicability to explainable planning.
</p>
<a href="http://arxiv.org/abs/2011.09006" target="_blank">arXiv:2011.09006</a> [<a href="http://arxiv.org/pdf/2011.09006" target="_blank">pdf</a>]

<h2>AttentiveNAS: Improving Neural Architecture Search via Attentive Sampling. (arXiv:2011.09011v1 [cs.CV])</h2>
<h3>Dilin Wang, Meng Li, Chengyue Gong, Vikas Chandra</h3>
<p>Neural architecture search (NAS) has shown great promise designing
state-of-the-art (SOTA) models that are both accurate and fast. Recently,
two-stage NAS, e.g. BigNAS, decouples the model training and searching process
and achieves good search efficiency. Two-stage NAS requires sampling from the
search space during training, which directly impacts the accuracy of the final
searched models. While uniform sampling has been widely used for simplicity, it
is agnostic of the model performance Pareto front, which are the main focus in
the search process, and thus, misses opportunities to further improve the model
accuracy. In this work, we propose AttentiveNAS that focuses on sampling the
networks to improve the performance Pareto. We also propose algorithms to
efficiently and effectively identify the networks on the Pareto during
training. Without extra re-training or post-processing, we can simultaneously
obtain a large number of networks across a wide range of FLOPs. Our discovered
model family, AttentiveNAS models, achieves top-1 accuracy from 77.3% to 80.7%
on ImageNet, and outperforms SOTA models, including BigNAS, Once-for-All
networks and FBNetV3. We also achieve ImageNet accuracy of 80.1% with only 491
MFLOPs.
</p>
<a href="http://arxiv.org/abs/2011.09011" target="_blank">arXiv:2011.09011</a> [<a href="http://arxiv.org/pdf/2011.09011" target="_blank">pdf</a>]

<h2>Statistical model-based evaluation of neural networks. (arXiv:2011.09015v1 [cs.LG])</h2>
<h3>Sandipan Das, Prakash B. Gohain, Alireza M. Javid, Yonina C. Eldar, Saikat Chatterjee</h3>
<p>Using a statistical model-based data generation, we develop an experimental
setup for the evaluation of neural networks (NNs). The setup helps to benchmark
a set of NNs vis-a-vis minimum-mean-square-error (MMSE) performance bounds.
This allows us to test the effects of training data size, data dimension, data
geometry, noise, and mismatch between training and testing conditions. In the
proposed setup, we use a Gaussian mixture distribution to generate data for
training and testing a set of competing NNs. Our experiments show the
importance of understanding the type and statistical conditions of data for
appropriate application and design of NNs
</p>
<a href="http://arxiv.org/abs/2011.09015" target="_blank">arXiv:2011.09015</a> [<a href="http://arxiv.org/pdf/2011.09015" target="_blank">pdf</a>]

<h2>FSPN: A New Class of Probabilistic Graphical Model. (arXiv:2011.09020v1 [cs.AI])</h2>
<h3>Ziniu Wu, Rong Zhu, Andreas Pfadler, Yuxing Han, Jiangneng Li, Zhengping Qian, Kai Zeng, Jingren Zhou</h3>
<p>We introduce FSPN, a new class of probabilistic graphical model (PGM). FSPN
is designed to overcome the drawbacks in expressiveness and tractability of
other PGMs. Specifically, Bayesian network (BN) has low inference efficiency
and sum product network performance significantly degrades in presence of
highly correlated variables. FSPN absorbs their advantages by adaptively
modeling the joint distribution of variables according to their dependence
degree. It can simultaneously attain the two desirable goals: high estimation
accuracy and fast inference speed. We present efficient probability inference
and structure learning algorithms for FSPN, along with theoretical analysis and
extensive evaluation evidence. Our experimental results on the benchmark
datasets indicate that FSPN is a new SOTA PGM.
</p>
<a href="http://arxiv.org/abs/2011.09020" target="_blank">arXiv:2011.09020</a> [<a href="http://arxiv.org/pdf/2011.09020" target="_blank">pdf</a>]

<h2>ADCPNet: Adaptive Disparity Candidates Prediction Network for Efficient Real-Time Stereo Matching. (arXiv:2011.09023v1 [cs.CV])</h2>
<h3>He Dai, Xuchong Zhang, Yongli Zhao, Hongbin Sun</h3>
<p>Efficient real-time disparity estimation is critical for the application of
stereo vision systems in various areas. Recently, stereo network based on
coarse-to-fine method has largely relieved the memory constraints and speed
limitations of large-scale network models. Nevertheless, all of the previous
coarse-to-fine designs employ constant offsets and three or more stages to
progressively refine the coarse disparity map, still resulting in
unsatisfactory computation accuracy and inference time when deployed on mobile
devices. This paper claims that the coarse matching errors can be corrected
efficiently with fewer stages as long as more accurate disparity candidates can
be provided. Therefore, we propose a dynamic offset prediction module to meet
different correction requirements of diverse objects and design an efficient
two-stage framework. Besides, we propose a disparity-independent convolution to
further improve the performance since it is more consistent with the local
statistical characteristics of the compact cost volume. The evaluation results
on multiple datasets and platforms clearly demonstrate that, the proposed
network outperforms the state-of-the-art lightweight models especially for
mobile devices in terms of accuracy and speed. Code will be made available.
</p>
<a href="http://arxiv.org/abs/2011.09023" target="_blank">arXiv:2011.09023</a> [<a href="http://arxiv.org/pdf/2011.09023" target="_blank">pdf</a>]

<h2>Domain Concretization from Examples: Addressing Missing Domain Knowledge via Robust Planning. (arXiv:2011.09034v1 [cs.AI])</h2>
<h3>Akshay Sharma, Piyush Rajesh Medikeri, Yu Zhang</h3>
<p>The assumption of complete domain knowledge is not warranted for robot
planning and decision-making in the real world. It could be due to design flaws
or arise from domain ramifications or qualifications. In such cases, existing
planning and learning algorithms could produce highly undesirable behaviors.
This problem is more challenging than partial observability in the sense that
the agent is unaware of certain knowledge, in contrast to it being partially
observable: the difference between known unknowns and unknown unknowns. In this
work, we formulate it as the problem of Domain Concretization, an inverse
problem to domain abstraction. Based on an incomplete domain model provided by
the designer and teacher traces from human users, our algorithm searches for a
candidate model set under a minimalistic model assumption. It then generates a
robust plan with the maximum probability of success under the set of candidate
models. In addition to a standard search formulation in the model-space, we
propose a sample-based search method and also an online version of it to
improve search time. We tested our approach on IPC domains and a simulated
robotics domain where incompleteness was introduced by removing domain features
from the complete model. Results show that our planning algorithm increases the
plan success rate without impacting the cost much.
</p>
<a href="http://arxiv.org/abs/2011.09034" target="_blank">arXiv:2011.09034</a> [<a href="http://arxiv.org/pdf/2011.09034" target="_blank">pdf</a>]

<h2>Your "Labrador" is My "Dog": Fine-Grained, or Not. (arXiv:2011.09040v1 [cs.CV])</h2>
<h3>Dongliang Chang, Kaiyue Pang, Yixiao Zheng, Zhanyu Ma, Yi-Zhe Song, Jun Guo</h3>
<p>Whether what you see in Figure 1 is a "labrador" or a "dog", is the question
we ask in this paper. While fine-grained visual classification (FGVC) strives
to arrive at the former, for the majority of us non-experts just "dog" would
probably suffice. The real question is therefore -- how can we tailor for
different fine-grained definitions under divergent levels of expertise. For
that, we re-envisage the traditional setting of FGVC, from single-label
classification, to that of top-down traversal of a pre-defined coarse-to-fine
label hierarchy -- so that our answer becomes "dog"--&gt;"gun
dog"--&gt;"retriever"--&gt;"labrador". To approach this new problem, we first conduct
a comprehensive human study where we confirm that most participants prefer
multi-granularity labels, regardless whether they consider themselves experts.
We then discover the key intuition that: coarse-level label prediction
exacerbates fine-grained feature learning, yet fine-level feature betters the
learning of coarse-level classifier. This discovery enables us to design a very
simple albeit surprisingly effective solution to our new problem, where we (i)
leverage level-specific classification heads to disentangle coarse-level
features with fine-grained ones, and (ii) allow finer-grained features to
participate in coarser-grained label predictions, which in turn helps with
better disentanglement. Experiments show that our method achieves superior
performance in the new FGVC setting, and performs better than state-of-the-art
on traditional single-label FGVC problem as well. Thanks to its simplicity, our
method can be easily implemented on top of any existing FGVC frameworks and is
parameter-free.
</p>
<a href="http://arxiv.org/abs/2011.09040" target="_blank">arXiv:2011.09040</a> [<a href="http://arxiv.org/pdf/2011.09040" target="_blank">pdf</a>]

<h2>Double-Prong ConvLSTM for Spatiotemporal Occupancy Prediction in Dynamic Environments. (arXiv:2011.09045v1 [cs.RO])</h2>
<h3>Maneekwan Toyungyernsub, Masha Itkina, Ransalu Senanayake, Mykel J. Kochenderfer</h3>
<p>Predicting the future occupancy state of an environment is important to
enable informed decisions for autonomous vehicles. Common challenges in
occupancy prediction include vanishing dynamic objects and blurred predictions,
especially for long prediction horizons. In this work, we propose a
double-prong neural network architecture to predict the spatiotemporal
evolution of the environment occupancy state. One prong is dedicated to
predicting how the static environment will be observed by the moving ego
vehicle. The other prong predicts how the dynamic objects in the environment
will move. Experiments conducted on the real-world Waymo Open Dataset indicate
that the fused output of the two prongs is capable of retaining dynamic objects
and reducing blurriness in the predictions for longer time horizons than
baseline models.
</p>
<a href="http://arxiv.org/abs/2011.09045" target="_blank">arXiv:2011.09045</a> [<a href="http://arxiv.org/pdf/2011.09045" target="_blank">pdf</a>]

<h2>A Hierarchical Multi-Modal Encoder for Moment Localization in Video Corpus. (arXiv:2011.09046v1 [cs.CV])</h2>
<h3>Bowen Zhang, Hexiang Hu, Joonseok Lee, Ming Zhao, Sheide Chammas, Vihan Jain, Eugene Ie, Fei Sha</h3>
<p>Identifying a short segment in a long video that semantically matches a text
query is a challenging task that has important application potentials in
language-based video search, browsing, and navigation. Typical retrieval
systems respond to a query with either a whole video or a pre-defined video
segment, but it is challenging to localize undefined segments in untrimmed and
unsegmented videos where exhaustively searching over all possible segments is
intractable. The outstanding challenge is that the representation of a video
must account for different levels of granularity in the temporal domain. To
tackle this problem, we propose the HierArchical Multi-Modal EncodeR (HAMMER)
that encodes a video at both the coarse-grained clip level and the fine-grained
frame level to extract information at different scales based on multiple
subtasks, namely, video retrieval, segment temporal localization, and masked
language modeling. We conduct extensive experiments to evaluate our model on
moment localization in video corpus on ActivityNet Captions and TVR datasets.
Our approach outperforms the previous methods as well as strong baselines,
establishing new state-of-the-art for this task.
</p>
<a href="http://arxiv.org/abs/2011.09046" target="_blank">arXiv:2011.09046</a> [<a href="http://arxiv.org/pdf/2011.09046" target="_blank">pdf</a>]

<h2>Visual Forecasting of Time Series with Image-to-Image Regression. (arXiv:2011.09052v1 [cs.CV])</h2>
<h3>Naftali Cohen, Srijan Sood, Zhen Zeng, Tucker Balch, Manuela Veloso</h3>
<p>Time series forecasting is essential for agents to make decisions in many
domains. Existing models rely on classical statistical methods to predict
future values based on previously observed numerical information. Yet,
practitioners often rely on visualizations such as charts and plots to reason
about their predictions. Inspired by the end-users, we re-imagine the topic by
creating a framework to produce visual forecasts, similar to the way humans
intuitively do. In this work, we take a novel approach by leveraging advances
in deep learning to extend the field of time series forecasting to a visual
setting. We do this by transforming the numerical analysis problem into the
computer vision domain. Using visualizations of time series data as input, we
train a convolutional autoencoder to produce corresponding visual forecasts. We
examine various synthetic and real datasets with diverse degrees of complexity.
Our experiments show that visual forecasting is effective for cyclic data but
somewhat less for irregular data such as stock price. Importantly, we find the
proposed visual forecasting method to outperform numerical baselines. We
attribute the success of the visual forecasting approach to the fact that we
convert the continuous numerical regression problem into a discrete domain with
quantization of the continuous target signal into pixel space.
</p>
<a href="http://arxiv.org/abs/2011.09052" target="_blank">arXiv:2011.09052</a> [<a href="http://arxiv.org/pdf/2011.09052" target="_blank">pdf</a>]

<h2>Liquid Warping GAN with Attention: A Unified Framework for Human Image Synthesis. (arXiv:2011.09055v1 [cs.CV])</h2>
<h3>Wen Liu, Zhixin Piao, Zhi Tu, Wenhan Luo, Lin Ma, Shenghua Gao</h3>
<p>We tackle human image synthesis, including human motion imitation, appearance
transfer, and novel view synthesis, within a unified framework. It means that
the model, once being trained, can be used to handle all these tasks. The
existing task-specific methods mainly use 2D keypoints to estimate the human
body structure. However, they only express the position information with no
abilities to characterize the personalized shape of the person and model the
limb rotations. In this paper, we propose to use a 3D body mesh recovery module
to disentangle the pose and shape. It can not only model the joint location and
rotation but also characterize the personalized body shape. To preserve the
source information, such as texture, style, color, and face identity, we
propose an Attentional Liquid Warping GAN with Attentional Liquid Warping Block
(AttLWB) that propagates the source information in both image and feature
spaces to the synthesized reference. Specifically, the source features are
extracted by a denoising convolutional auto-encoder for characterizing the
source identity well. Furthermore, our proposed method can support a more
flexible warping from multiple sources. To further improve the generalization
ability of the unseen source images, a one/few-shot adversarial learning is
applied. In detail, it firstly trains a model in an extensive training set.
Then, it finetunes the model by one/few-shot unseen image(s) in a
self-supervised way to generate high-resolution (512 x 512 and 1024 x 1024)
results. Also, we build a new dataset, namely iPER dataset, for the evaluation
of human motion imitation, appearance transfer, and novel view synthesis.
Extensive experiments demonstrate the effectiveness of our methods in terms of
preserving face identity, shape consistency, and clothes details. All codes and
dataset are available on
https://impersonator.org/work/impersonator-plus-plus.html.
</p>
<a href="http://arxiv.org/abs/2011.09055" target="_blank">arXiv:2011.09055</a> [<a href="http://arxiv.org/pdf/2011.09055" target="_blank">pdf</a>]

<h2>Layer-Wise Data-Free CNN Compression. (arXiv:2011.09058v1 [cs.CV])</h2>
<h3>Maxwell Horton, Yanzi Jin, Ali Farhadi, Mohammad Rastegari</h3>
<p>We present an efficient method for compressing a trained neural network
without using any data. Our data-free method requires 14x-450x fewer FLOPs than
comparable state-of-the-art methods. We break the problem of data-free network
compression into a number of independent layer-wise compressions. We show how
to efficiently generate layer-wise training data, and how to precondition the
network to maintain accuracy during layer-wise compression. We show
state-of-the-art performance on MobileNetV1 for data-free low-bit-width
quantization. We also show state-of-the-art performance on data-free pruning of
EfficientNet B0 when combining our method with end-to-end generative methods.
</p>
<a href="http://arxiv.org/abs/2011.09058" target="_blank">arXiv:2011.09058</a> [<a href="http://arxiv.org/pdf/2011.09058" target="_blank">pdf</a>]

<h2>Shaping Deep Feature Space towards Gaussian Mixture for Visual Classification. (arXiv:2011.09066v1 [cs.CV])</h2>
<h3>Weitao Wan, Jiansheng Chen, Cheng Yu, Tong Wu, Yuanyi Zhong, Ming-Hsuan Yang</h3>
<p>The softmax cross-entropy loss function has been widely used to train deep
models for various tasks. In this work, we propose a Gaussian mixture (GM) loss
function for deep neural networks for visual classification. Unlike the softmax
cross-entropy loss, our method explicitly shapes the deep feature space towards
a Gaussian Mixture distribution. With a classification margin and a likelihood
regularization, the GM loss facilitates both high classification performance
and accurate modeling of the feature distribution. The GM loss can be readily
used to distinguish abnormal inputs, such as the adversarial examples, based on
the discrepancy between feature distributions of the inputs and the training
set. Furthermore, theoretical analysis shows that a symmetric feature space can
be achieved by using the GM loss, which enables the models to perform robustly
against adversarial attacks. The proposed model can be implemented easily and
efficiently without using extra trainable parameters. Extensive evaluations
demonstrate that the proposed method performs favorably not only on image
classification but also on robust detection of adversarial examples generated
by strong attacks under different threat models.
</p>
<a href="http://arxiv.org/abs/2011.09066" target="_blank">arXiv:2011.09066</a> [<a href="http://arxiv.org/pdf/2011.09066" target="_blank">pdf</a>]

<h2>An analytical diabolo model for robotic learning and control. (arXiv:2011.09068v1 [cs.RO])</h2>
<h3>Felix von Drigalski, Devwrat Joshi, Takayuki Murooka, Kazutoshi Tanaka, Masashi Hamaya, Yoshihisa Ijiri</h3>
<p>In this paper, we present a diabolo model that can be used for training
agents in simulation to play diabolo, as well as running it on a real dual
robot arm system. We first derive an analytical model of the diabolo-string
system and compare its accuracy using data recorded via motion capture, which
we release as a public dataset of skilled play with diabolos of different
dynamics. We show that our model outperforms a deep-learning-based predictor,
both in terms of precision and physically consistent behavior. Next, we
describe a method based on optimal control to generate robot trajectories that
produce the desired diabolo trajectory, as well as a system to transform
higher-level actions into robot motions. Finally, we test our method on a real
robot system by playing the diabolo, and throwing it to and catching it from a
human player.
</p>
<a href="http://arxiv.org/abs/2011.09068" target="_blank">arXiv:2011.09068</a> [<a href="http://arxiv.org/pdf/2011.09068" target="_blank">pdf</a>]

<h2>Deep Positional and Relational Feature Learning for Rotation-Invariant Point Cloud Analysis. (arXiv:2011.09080v1 [cs.CV])</h2>
<h3>Ruixuan Yu, Xin Wei, Federico Tombari, Jian Sun</h3>
<p>In this paper we propose a rotation-invariant deep network for point clouds
analysis. Point-based deep networks are commonly designed to recognize roughly
aligned 3D shapes based on point coordinates, but suffer from performance drops
with shape rotations. Some geometric features, e.g., distances and angles of
points as inputs of network, are rotation-invariant but lose positional
information of points. In this work, we propose a novel deep network for point
clouds by incorporating positional information of points as inputs while
yielding rotation-invariance. The network is hierarchical and relies on two
modules: a positional feature embedding block and a relational feature
embedding block. Both modules and the whole network are proven to be
rotation-invariant when processing point clouds as input. Experiments show
state-of-the-art classification and segmentation performances on benchmark
datasets, and ablation studies demonstrate effectiveness of the network design.
</p>
<a href="http://arxiv.org/abs/2011.09080" target="_blank">arXiv:2011.09080</a> [<a href="http://arxiv.org/pdf/2011.09080" target="_blank">pdf</a>]

<h2>Weighted Entropy Modification for Soft Actor-Critic. (arXiv:2011.09083v1 [cs.LG])</h2>
<h3>Yizhou Zhao, Song-Chun Zhu</h3>
<p>We generalize the existing principle of the maximum Shannon entropy in
reinforcement learning (RL) to weighted entropy by characterizing the
state-action pairs with some qualitative weights, which can be connected with
prior knowledge, experience replay, and evolution process of the policy. We
propose an algorithm motivated for self-balancing exploration with the
introduced weight function, which leads to state-of-the-art performance on
Mujoco tasks despite its simplicity in implementation.
</p>
<a href="http://arxiv.org/abs/2011.09083" target="_blank">arXiv:2011.09083</a> [<a href="http://arxiv.org/pdf/2011.09083" target="_blank">pdf</a>]

<h2>MUST-GAN: Multi-level Statistics Transfer for Self-driven Person Image Generation. (arXiv:2011.09084v1 [cs.CV])</h2>
<h3>Tianxiang Ma, Bo Peng, Wei Wang, Jing Dong</h3>
<p>Pose-guided person image generation usually involves using paired
source-target images to supervise the training, which significantly increases
the data preparation effort and limits the application of the models. To deal
with this problem, we propose a novel multi-level statistics transfer model,
which disentangles and transfers multi-level appearance features from person
images and merges them with pose features to reconstruct the source person
images themselves. So that the source images can be used as supervision for
self-driven person image generation. Specifically, our model extracts
multi-level features from the appearance encoder and learns the optimal
appearance representation through attention mechanism and attributes
statistics. Then we transfer them to a pose-guided generator for re-fusion of
appearance and pose. Our approach allows for flexible manipulation of person
appearance and pose properties to perform pose transfer and clothes style
transfer tasks. Experimental results on the DeepFashion dataset demonstrate our
method's superiority compared with state-of-the-art supervised and unsupervised
methods. In addition, our approach also performs well in the wild.
</p>
<a href="http://arxiv.org/abs/2011.09084" target="_blank">arXiv:2011.09084</a> [<a href="http://arxiv.org/pdf/2011.09084" target="_blank">pdf</a>]

<h2>Tracking and Visualizing Signs of Degradation for an Early Failure Prediction of a Rolling Bearing. (arXiv:2011.09086v1 [cs.RO])</h2>
<h3>Sana Talmoudi (1), Tetsuya Kanada (2), Yasuhisa Hirata (3) ((1) Department of Robotics, Graduate Faculty of Engineering, Tohoku University, (2) D&#x27;isum Inc.)</h3>
<p>Predictive maintenance, i.e. predicting failure to be few steps ahead of the
fault, is one of the pillars of Industry 4.0. An effective method for that is
to track early signs of degradation before a failure happens. This paper
presents an innovative failure predictive scheme for machines. The proposed
scheme combines the use of full spectrum of the vibration data caused by the
machines and data visualization technologies. This scheme is featured by no
training data required and by quick start after installation. First, we propose
to use full spectrum (as high-dimensional data vector) with no cropping and no
complex feature extraction and to visualize data behavior by mapping the high
dimensional vectors into a 2D map. We then can ensure the simplicity of process
and less possibility of overlooking of important information as well as
providing a human-friendly and human-understandable output. Second, we propose
Real-Time Data Tracker (RTDT) which predicts the failure at an appropriate time
with sufficient time for maintenance by plotting real-time frequency spectrum
data of the target machine on the 2D map composed from normal data. Third, we
show the test results of our proposal using vibration data of bearings from
real-world test-to-failure measurements provided by the public dataset, the IMS
dataset.
</p>
<a href="http://arxiv.org/abs/2011.09086" target="_blank">arXiv:2011.09086</a> [<a href="http://arxiv.org/pdf/2011.09086" target="_blank">pdf</a>]

<h2>UP-DETR: Unsupervised Pre-training for Object Detection with Transformers. (arXiv:2011.09094v1 [cs.CV])</h2>
<h3>Zhigang Dai, Bolun Cai, Yugeng Lin, Junying Chen</h3>
<p>Object detection with transformers (DETR) reaches competitive performance
with Faster R-CNN via a transformer encoder-decoder architecture. Inspired by
the great success of pre-training transformers in natural language processing,
we propose a pretext task named random query patch detection to unsupervisedly
pre-train DETR (UP-DETR) for object detection. Specifically, we randomly crop
patches from the given image and then feed them as queries to the decoder. The
model is pre-trained to detect these query patches from the original image.
During the pre-training, we address two critical issues: multi-task learning
and multi-query localization. (1) To trade-off multi-task learning of
classification and localization in the pretext task, we freeze the CNN backbone
and propose a patch feature reconstruction branch which is jointly optimized
with patch detection. (2) To perform multi-query localization, we introduce
UP-DETR from single-query patch and extend it to multi-query patches with
object query shuffle and attention mask. In our experiments, UP-DETR
significantly boosts the performance of DETR with faster convergence and higher
precision on PASCAL VOC and COCO datasets. The code will be available soon.
</p>
<a href="http://arxiv.org/abs/2011.09094" target="_blank">arXiv:2011.09094</a> [<a href="http://arxiv.org/pdf/2011.09094" target="_blank">pdf</a>]

<h2>Viewpoint-aware Progressive Clustering for Unsupervised Vehicle Re-identification. (arXiv:2011.09099v1 [cs.CV])</h2>
<h3>Aihua Zheng, Xia Sun, Chenglong Li, Jin Tang</h3>
<p>Vehicle re-identification (Re-ID) is an active task due to its importance in
large-scale intelligent monitoring in smart cities. Despite the rapid progress
in recent years, most existing methods handle vehicle Re-ID task in a
supervised manner, which is both time and labor-consuming and limits their
application to real-life scenarios. Recently, unsupervised person Re-ID methods
achieve impressive performance by exploring domain adaption or clustering-based
techniques. However, one cannot directly generalize these methods to vehicle
Re-ID since vehicle images present huge appearance variations in different
viewpoints. To handle this problem, we propose a novel viewpoint-aware
clustering algorithm for unsupervised vehicle Re-ID. In particular, we first
divide the entire feature space into different subspaces according to the
predicted viewpoints and then perform a progressive clustering to mine the
accurate relationship among samples. Comprehensive experiments against the
state-of-the-art methods on two multi-viewpoint benchmark datasets VeRi and
VeRi-Wild validate the promising performance of the proposed method in both
with and without domain adaption scenarios while handling unsupervised vehicle
Re-ID.
</p>
<a href="http://arxiv.org/abs/2011.09099" target="_blank">arXiv:2011.09099</a> [<a href="http://arxiv.org/pdf/2011.09099" target="_blank">pdf</a>]

<h2>Masked Linear Regression for Learning Local Receptive Fields for Facial Expression Synthesis. (arXiv:2011.09104v1 [cs.CV])</h2>
<h3>Nazar Khan, Arbish Akram, Arif Mahmood, Sania Ashraf, Kashif Murtaza</h3>
<p>Compared to facial expression recognition, expression synthesis requires a
very high-dimensional mapping. This problem exacerbates with increasing image
sizes and limits existing expression synthesis approaches to relatively small
images. We observe that facial expressions often constitute sparsely
distributed and locally correlated changes from one expression to another. By
exploiting this observation, the number of parameters in an expression
synthesis model can be significantly reduced. Therefore, we propose a
constrained version of ridge regression that exploits the local and sparse
structure of facial expressions. We consider this model as masked regression
for learning local receptive fields. In contrast to the existing approaches,
our proposed model can be efficiently trained on larger image sizes.
Experiments using three publicly available datasets demonstrate that our model
is significantly better than $\ell_0, \ell_1$ and $\ell_2$-regression, SVD
based approaches, and kernelized regression in terms of mean-squared-error,
visual quality as well as computational and spatial complexities. The reduction
in the number of parameters allows our method to generalize better even after
training on smaller datasets. The proposed algorithm is also compared with
state-of-the-art GANs including Pix2Pix, CycleGAN, StarGAN and GANimation.
These GANs produce photo-realistic results as long as the testing and the
training distributions are similar. In contrast, our results demonstrate
significant generalization of the proposed algorithm over out-of-dataset human
photographs, pencil sketches and even animal faces.
</p>
<a href="http://arxiv.org/abs/2011.09104" target="_blank">arXiv:2011.09104</a> [<a href="http://arxiv.org/pdf/2011.09104" target="_blank">pdf</a>]

<h2>Robot Task Planning for Low Entropy Belief States. (arXiv:2011.09105v1 [cs.RO])</h2>
<h3>Alphonsus Adu-Bredu, Zhen Zeng, Neha Pusalkar, Odest Chadwicke Jenkins</h3>
<p>Recent advances in computational perception have significantly improved the
ability of autonomous robots to perform state estimation with low entropy. Such
advances motivate a reconsideration of robot decision-making under uncertainty.
Current approaches to solving sequential decision-making problems model states
as inhabiting the extremes of the perceptual entropy spectrum. As such, these
methods are either incapable of overcoming perceptual errors or asymptotically
inefficient in solving problems with low perceptual entropy. With low entropy
perception in mind, we aim to explore a happier medium that balances
computational efficiency with the forms of uncertainty we now observe from
modern robot perception. We propose FastDownward Replanner (FD-Replan) as an
efficient task planning method for goal-directed robot reasoning. FD-Replan
combines belief space representation with the fast, goal-directed features of
classical planning to efficiently plan for low entropy goal-directed reasoning
tasks. We compare FD-Replan with current classical planning and belief space
planning approaches by solving low entropy goal-directed grocery packing tasks
in simulation. FD-Replan shows positive results and promise with respect to
planning time, execution time, and task success rate in our simulation
experiments.
</p>
<a href="http://arxiv.org/abs/2011.09105" target="_blank">arXiv:2011.09105</a> [<a href="http://arxiv.org/pdf/2011.09105" target="_blank">pdf</a>]

<h2>Vision-Based Shape Reconstruction of Soft Continuum Arms Using a Geometric Strain Parametrization. (arXiv:2011.09106v1 [cs.RO])</h2>
<h3>Ali AlBeladi, Girish Krishnan, Mohamed-Ali Belabbas, Seth Hutchinson</h3>
<p>Interest in soft continuum arms has increased as their inherent material
elasticity enables safe and adaptive interactions with the environment. However
to achieve full autonomy in these arms, accurate three-dimensional shape
sensing is needed. Vision-based solutions have been found to be effective in
estimating the shape of soft continuum arms. In this paper, a vision-based
shape estimator that utilizes a geometric strain based representation for the
soft continuum arm's shape, is proposed. This representation reduces the
dimension of the curved shape to a finite set of strain basis functions,
thereby allowing for efficient optimization for the shape that best fits the
observed image. Experimental results demonstrate the effectiveness of the
proposed approach in estimating the end effector with accuracy less than the
soft arm's radius. Multiple basis functions are also analyzed and compared for
the specific soft continuum arm in use.
</p>
<a href="http://arxiv.org/abs/2011.09106" target="_blank">arXiv:2011.09106</a> [<a href="http://arxiv.org/pdf/2011.09106" target="_blank">pdf</a>]

<h2>Effectiveness of Arbitrary Transfer Sets for Data-free Knowledge Distillation. (arXiv:2011.09113v1 [cs.LG])</h2>
<h3>Gaurav Kumar Nayak, Konda Reddy Mopuri, Anirban Chakraborty</h3>
<p>Knowledge Distillation is an effective method to transfer the learning across
deep neural networks. Typically, the dataset originally used for training the
Teacher model is chosen as the "Transfer Set" to conduct the knowledge transfer
to the Student. However, this original training data may not always be freely
available due to privacy or sensitivity concerns. In such scenarios, existing
approaches either iteratively compose a synthetic set representative of the
original training dataset, one sample at a time or learn a generative model to
compose such a transfer set. However, both these approaches involve complex
optimization (GAN training or several backpropagation steps to synthesize one
sample) and are often computationally expensive. In this paper, as a simple
alternative, we investigate the effectiveness of "arbitrary transfer sets" such
as random noise, publicly available synthetic, and natural datasets, all of
which are completely unrelated to the original training dataset in terms of
their visual or semantic contents. Through extensive experiments on multiple
benchmark datasets such as MNIST, FMNIST, CIFAR-10 and CIFAR-100, we discover
and validate surprising effectiveness of using arbitrary data to conduct
knowledge distillation when this dataset is "target-class balanced". We believe
that this important observation can potentially lead to designing baselines for
the data-free knowledge distillation task.
</p>
<a href="http://arxiv.org/abs/2011.09113" target="_blank">arXiv:2011.09113</a> [<a href="http://arxiv.org/pdf/2011.09113" target="_blank">pdf</a>]

<h2>Dehazing Cost Volume for Deep Multi-view Stereo in Scattering Media with Airlight and Scattering Coefficient Estimation. (arXiv:2011.09114v1 [cs.CV])</h2>
<h3>Yuki Fujimura, Motoharu Sonogashira, Masaaki Iiyama</h3>
<p>We propose a learning-based multi-view stereo (MVS) method in scattering
media, such as fog or smoke, with a novel cost volume, called the dehazing cost
volume. Images captured in scattering media are degraded due to light
scattering and attenuation caused by suspended particles. This degradation
depends on scene depth; thus, it is difficult for traditional MVS methods to
evaluate photometric consistency because the depth is unknown before
three-dimensional (3D) reconstruction. The dehazing cost volume can solve this
chicken-and-egg problem of depth estimation and image restoration by computing
the scattering effect using swept planes in the cost volume. We also propose a
method of estimating scattering parameters, such as airlight, and a scattering
coefficient, which are required for our dehazing cost volume. The output depth
of a network with our dehazing cost volume can be regarded as a function of
these parameters; thus, they are geometrically optimized with a sparse 3D point
cloud obtained at a structure-from-motion step. Experimental results on
synthesized hazy images indicate the effectiveness of our dehazing cost volume
against the ordinary cost volume regarding scattering media. We also
demonstrated the applicability of our dehazing cost volume to real foggy
scenes.
</p>
<a href="http://arxiv.org/abs/2011.09114" target="_blank">arXiv:2011.09114</a> [<a href="http://arxiv.org/pdf/2011.09114" target="_blank">pdf</a>]

<h2>More Informed Random Sample Consensus. (arXiv:2011.09116v1 [cs.RO])</h2>
<h3>Guoxiang Zhang, YangQuan Chen</h3>
<p>Random sample consensus (RANSAC) is a robust model-fitting algorithm. It is
widely used in many fields including image-stitching and point cloud
registration. In RANSAC, data is uniformly sampled for hypothesis generation.
However, this uniform sampling strategy does not fully utilize all the
information on many problems. In this paper, we propose a method that samples
data with a L\'{e}vy distribution together with a data sorting algorithm. In
the hypothesis sampling step of the proposed method, data is sorted with a
sorting algorithm we proposed, which sorts data based on the likelihood of a
data point being in the inlier set. Then, hypotheses are sampled from the
sorted data with L\'{e}vy distribution. The proposed method is evaluated on
both simulation and real-world public datasets. Our method shows better results
compared with the uniform baseline method.
</p>
<a href="http://arxiv.org/abs/2011.09116" target="_blank">arXiv:2011.09116</a> [<a href="http://arxiv.org/pdf/2011.09116" target="_blank">pdf</a>]

<h2>Barycode-based GJK Algorithm. (arXiv:2011.09117v1 [cs.RO])</h2>
<h3>Yu Zhang, Yangming Wu, Xigui Wang, Xiaocheng Zhou</h3>
<p>In this paper, we present a more efficient GJK algorithm to solve the
collision detection and distance query problems in 2D. We contribute in two
aspects: First, we propose a new barycode-based sub-distance algorithm that
does not only provide a simple and unified condition to determine the minimum
simplex but also improve the efficiency in distant, touching, and overlap cases
in distance query. Second, we provide a highly efficient implementation
subroutine for collision detection by optimizing the exit conditions of our GJK
distance algorithm, which shows dramatic improvements in run-time for
applications that only need binary results. We benchmark our methods along with
that of the well-known open-source collision detection libraries, such as
Bullet, FCL, OpenGJK, Box2D, and Apollo over a range of random datasets. The
results indicate that our methods and implementations outperform the
state-of-the-art in both collision detection and distance query.
</p>
<a href="http://arxiv.org/abs/2011.09117" target="_blank">arXiv:2011.09117</a> [<a href="http://arxiv.org/pdf/2011.09117" target="_blank">pdf</a>]

<h2>Adversarial Profiles: Detecting Out-Distribution & Adversarial Samples in Pre-trained CNNs. (arXiv:2011.09123v1 [cs.CV])</h2>
<h3>Arezoo Rajabi, Rakesh B. Bobba</h3>
<p>Despite high accuracy of Convolutional Neural Networks (CNNs), they are
vulnerable to adversarial and out-distribution examples. There are many
proposed methods that tend to detect or make CNNs robust against these fooling
examples. However, most such methods need access to a wide range of fooling
examples to retrain the network or to tune detection parameters. Here, we
propose a method to detect adversarial and out-distribution examples against a
pre-trained CNN without needing to retrain the CNN or needing access to a wide
variety of fooling examples. To this end, we create adversarial profiles for
each class using only one adversarial attack generation technique. We then wrap
a detector around the pre-trained CNN that applies the created adversarial
profile to each input and uses the output to decide whether or not the input is
legitimate. Our initial evaluation of this approach using MNIST dataset show
that adversarial profile based detection is effective in detecting at least 92
of out-distribution examples and 59% of adversarial examples.
</p>
<a href="http://arxiv.org/abs/2011.09123" target="_blank">arXiv:2011.09123</a> [<a href="http://arxiv.org/pdf/2011.09123" target="_blank">pdf</a>]

<h2>3D-FRONT: 3D Furnished Rooms with layOuts and semaNTics. (arXiv:2011.09127v1 [cs.CV])</h2>
<h3>Huan Fu, Bowen Cai, Lin Gao, Lingxiao Zhang, Cao Li, Zengqi Xun, Chengyue Sun, Yiyun Fei, Yu Zheng, Ying Li, Yi Liu, Peng Liu, Lin Ma, Le Weng, Xiaohang Hu, Xin Ma, Qian Qian, Rongfei Jia, Binqiang Zhao, Hao Zhang</h3>
<p>We introduce 3D-FRONT (3D Furnished Rooms with layOuts and semaNTics), a new,
large-scale, and comprehensive repository of synthetic indoor scenes
highlighted by professionally designed layouts and a large number of rooms
populated by high-quality textured 3D models with style compatibility. From
layout semantics down to texture details of individual objects, our dataset is
freely available to the academic community and beyond. Currently, 3D-FRONT
contains 18,797 rooms diversely furnished by 3D objects, far surpassing all
publicly available scene datasets. In addition, the 7,302 furniture objects all
come with high-quality textures. While the floorplans and layout designs are
directly sourced from professional creations, the interior designs in terms of
furniture styles, color, and textures have been carefully curated based on a
recommender system we develop to attain consistent styles as expert designs.
Furthermore, we release Trescope, a light-weight rendering tool, to support
benchmark rendering of 2D images and annotations from 3D-FRONT. We demonstrate
two applications, interior scene synthesis and texture synthesis, that are
especially tailored to the strengths of our new dataset. The project page is
at: https://tianchi.aliyun.com/specials/promotion/alibaba-3d-scene-dataset.
</p>
<a href="http://arxiv.org/abs/2011.09127" target="_blank">arXiv:2011.09127</a> [<a href="http://arxiv.org/pdf/2011.09127" target="_blank">pdf</a>]

<h2>Multigrid-in-Channels Neural Network Architectures. (arXiv:2011.09128v1 [cs.CV])</h2>
<h3>Moshe Eliasof, Jonathan Ephrath, Lars Ruthotto, Eran Treister</h3>
<p>We present a multigrid-in-channels (MGIC) approach that tackles the quadratic
growth of the number of parameters with respect to the number of channels in
standard convolutional neural networks (CNNs). It has been shown that there is
a redundancy in standard CNNs, as networks with light or sparse convolution
operators yield similar performance to full networks. However, the number of
parameters in the former networks also scales quadratically in width, while in
the latter case, the parameters typically have random sparsity patterns,
hampering hardware efficiency.Our approach for building CNN architectures
scales linearly with respect to the network's width while retaining full
coupling of the channels as in standard CNNs.To this end, we replace each
convolution block with its MGIC block utilizing a hierarchy of lightweight
convolutions. Our extensive experiments on image classification, segmentation,
and point cloud classification show that applying this strategy to different
architectures like ResNet and MobileNetV3 considerably reduces the number of
parameters while obtaining similar or better accuracy. For example, we obtain
76.1% top-1 accuracy on ImageNet with a lightweight network with similar
parameters and FLOPs to MobileNetV3.
</p>
<a href="http://arxiv.org/abs/2011.09128" target="_blank">arXiv:2011.09128</a> [<a href="http://arxiv.org/pdf/2011.09128" target="_blank">pdf</a>]

<h2>Semantic Scene Completion using Local Deep Implicit Functions on LiDAR Data. (arXiv:2011.09141v1 [cs.CV])</h2>
<h3>Christoph B. Rist, David Emmerichs, Markus Enzweiler, Dariu M. Gavrila</h3>
<p>Semantic scene completion is the task of jointly estimating 3D geometry and
semantics of objects and surfaces within a given extent. This is a particularly
challenging task on real-world data that is sparse and occluded. We propose a
scene segmentation network based on local Deep Implicit Functions as a novel
learning-based method for scene completion. Unlike previous work on scene
completion, our method produces a continuous scene representation that is not
based on voxelization. We encode raw point clouds into a latent space locally
and at multiple spatial resolutions. A global scene completion function is
subsequently assembled from the localized function patches. We show that this
continuous representation is suitable to encode geometric and semantic
properties of extensive outdoor scenes without the need for spatial
discretization (thus avoiding the trade-off between level of scene detail and
the scene extent that can be covered).

We train and evaluate our method on semantically annotated LiDAR scans from
the Semantic KITTI dataset. Our experiments verify that our method generates a
powerful representation that can be decoded into a dense 3D description of a
given scene. The performance of our method surpasses the state of the art on
the Semantic KITTI Scene Completion Benchmark in terms of both geometric and
semantic completion Intersection-over-Union (IoU).
</p>
<a href="http://arxiv.org/abs/2011.09141" target="_blank">arXiv:2011.09141</a> [<a href="http://arxiv.org/pdf/2011.09141" target="_blank">pdf</a>]

<h2>Benign Overfitting in Binary Classification of Gaussian Mixtures. (arXiv:2011.09148v1 [stat.ML])</h2>
<h3>Ke Wang, Christos Thrampoulidis</h3>
<p>Deep neural networks generalize well despite being exceedingly
overparametrized, but understanding the statistical principles behind this so
called benign-overfitting phenomenon is not yet well understood. Recently there
has been remarkable progress towards understanding benign-overfitting in
simpler models, such as linear regression and, even more recently, linear
classification. This paper studies benign-overfitting for data generated from a
popular binary Gaussian mixtures model (GMM) and classifiers trained by
support-vector machines (SVM). Our approach has two steps. First, we leverage
an idea introduced in (Muthukumar et al. 2020) to relate the SVM solution to
the least-squares (LS) solution. Second, we derive novel non-asymptotic bounds
on the classification error of LS solution. Combining the two gives sufficient
conditions on the overparameterization ratio and the signal-to-noise ratio that
lead to benign overfitting. We corroborate our theoretical findings with
numerical simulations.
</p>
<a href="http://arxiv.org/abs/2011.09148" target="_blank">arXiv:2011.09148</a> [<a href="http://arxiv.org/pdf/2011.09148" target="_blank">pdf</a>]

<h2>DeepNAG: Deep Non-Adversarial Gesture Generation. (arXiv:2011.09149v1 [cs.CV])</h2>
<h3>Mehran Maghoumi, Eugene M. Taranta II, Joseph J. LaViola Jr</h3>
<p>Synthetic data generation to improve classification performance (data
augmentation) is a well-studied problem. Recently, generative adversarial
networks (GAN) have shown superior image data augmentation performance, but
their suitability in gesture synthesis has received inadequate attention.
Further, GANs prohibitively require simultaneous generator and discriminator
network training. We tackle both issues in this work. We first discuss a novel,
device-agnostic GAN model for gesture synthesis called DeepGAN. Thereafter, we
formulate DeepNAG by introducing a new differentiable loss function based on
dynamic time warping and the average Hausdorff distance, which allows us to
train DeepGAN's generator without requiring a discriminator. Through
evaluations, we compare the utility of DeepGAN and DeepNAG against two
alternative techniques for training five recognizers using data augmentation
over six datasets. We further investigate the perceived quality of synthesized
samples via an Amazon Mechanical Turk user study based on the HYPE benchmark.
We find that DeepNAG outperforms DeepGAN in accuracy, training time (up to 17x
faster), and realism, thereby opening the door to a new line of research in
generator network design and training for gesture synthesis. Our source code is
available at https://www.deepnag.com.
</p>
<a href="http://arxiv.org/abs/2011.09149" target="_blank">arXiv:2011.09149</a> [<a href="http://arxiv.org/pdf/2011.09149" target="_blank">pdf</a>]

<h2>RSINet: Rotation-Scale Invariant Network for Online Visual Tracking. (arXiv:2011.09153v1 [cs.CV])</h2>
<h3>Yang Fang, Geun-Sik Jo, Chang-Hee Lee</h3>
<p>Most Siamese network-based trackers perform the tracking process without
model update, and cannot learn targetspecific variation adaptively. Moreover,
Siamese-based trackers infer the new state of tracked objects by generating
axis-aligned bounding boxes, which contain extra background noise, and are
unable to accurately estimate the rotation and scale transformation of moving
objects, thus potentially reducing tracking performance. In this paper, we
propose a novel Rotation-Scale Invariant Network (RSINet) to address the above
problem. Our RSINet tracker consists of a target-distractor discrimination
branch and a rotation-scale estimation branch, the rotation and scale knowledge
can be explicitly learned by a multi-task learning method in an end-to-end
manner. In addtion, the tracking model is adaptively optimized and updated
under spatio-temporal energy control, which ensures model stability and
reliability, as well as high tracking efficiency. Comprehensive experiments on
OTB-100, VOT2018, and LaSOT benchmarks demonstrate that our proposed RSINet
tracker yields new state-of-the-art performance compared with recent trackers,
while running at real-time speed about 45 FPS.
</p>
<a href="http://arxiv.org/abs/2011.09153" target="_blank">arXiv:2011.09153</a> [<a href="http://arxiv.org/pdf/2011.09153" target="_blank">pdf</a>]

<h2>Dense Contrastive Learning for Self-Supervised Visual Pre-Training. (arXiv:2011.09157v1 [cs.CV])</h2>
<h3>Xinlong Wang, Rufeng Zhang, Chunhua Shen, Tao Kong, Lei Li</h3>
<p>To date, most existing self-supervised learning methods are designed and
optimized for image classification. These pre-trained models can be sub-optimal
for dense prediction tasks due to the discrepancy between image-level
prediction and pixel-level prediction. To fill this gap, we aim to design an
effective, dense self-supervised learning method that directly works at the
level of pixels (or local features) by taking into account the correspondence
between local features. We present dense contrastive learning, which implements
self-supervised learning by optimizing a pairwise contrastive (dis)similarity
loss at the pixel level between two views of input images. Compared to the
baseline method MoCo-v2, our method introduces negligible computation overhead
(only &lt;1% slower), but demonstrates consistently superior performance when
transferring to downstream dense prediction tasks including object detection,
semantic segmentation and instance segmentation; and outperforms the
state-of-the-art methods by a large margin. Specifically, over the strong
MoCo-v2 baseline, our method achieves significant improvements of 2.0% AP on
PASCAL VOC object detection, 1.1% AP on COCO object detection, 0.9% AP on COCO
instance segmentation, 3.0% mIoU on PASCAL VOC semantic segmentation and 1.8%
mIoU on Cityscapes semantic segmentation. Code is available at:
https://git.io/AdelaiDet
</p>
<a href="http://arxiv.org/abs/2011.09157" target="_blank">arXiv:2011.09157</a> [<a href="http://arxiv.org/pdf/2011.09157" target="_blank">pdf</a>]

<h2>Privileged Knowledge Distillation for Online Action Detection. (arXiv:2011.09158v1 [cs.CV])</h2>
<h3>Peisen Zhao, Jiajie Wang, Lingxi Xie, Ya Zhang, Yanfeng Wang, Qi Tian</h3>
<p>Online Action Detection (OAD) in videos is proposed as a per-frame labeling
task to address the real-time prediction tasks that can only obtain the
previous and current video frames. This paper presents a novel
learning-with-privileged based framework for online action detection where the
future frames only observable at the training stages are considered as a form
of privileged information. Knowledge distillation is employed to transfer the
privileged information from the offline teacher to the online student. We note
that this setting is different from conventional KD because the difference
between the teacher and student models mostly lies in input data rather than
the network architecture. We propose Privileged Knowledge Distillation (PKD)
which (i) schedules a curriculum learning procedure and (ii) inserts auxiliary
nodes to the student model, both for shrinking the information gap and
improving learning performance. Compared to other OAD methods that explicitly
predict future frames, our approach avoids learning unpredictable unnecessary
yet inconsistent visual contents and achieves state-of-the-art accuracy on two
popular OAD benchmarks, TVSeries and THUMOS14.
</p>
<a href="http://arxiv.org/abs/2011.09158" target="_blank">arXiv:2011.09158</a> [<a href="http://arxiv.org/pdf/2011.09158" target="_blank">pdf</a>]

<h2>Positive-Congruent Training: Towards Regression-Free Model Updates. (arXiv:2011.09161v1 [cs.CV])</h2>
<h3>Sijie Yan, Yuanjun Xiong, Kaustav Kundu, Shuo Yang, Siqi Deng, Meng Wang, Wei Xia, Stefano Soatto</h3>
<p>Reducing inconsistencies in the behavior of different versions of an AI
system can be as important in practice as reducing its overall error. In image
classification, sample-wise inconsistencies appear as "negative flips:" A new
model incorrectly predicts the output for a test sample that was correctly
classified by the old (reference) model. Positive-congruent (PC) training aims
at reducing error rate while at the same time reducing negative flips, thus
maximizing congruency with the reference model only on positive predictions,
unlike model distillation. We propose a simple approach for PC training, Focal
Distillation, which enforces congruence with the reference model by giving more
weights to samples that were correctly classified. We also found that, if the
reference model itself can be chosen as an ensemble of multiple deep neural
networks, negative flips can be further reduced without affecting the new
model's accuracy.
</p>
<a href="http://arxiv.org/abs/2011.09161" target="_blank">arXiv:2011.09161</a> [<a href="http://arxiv.org/pdf/2011.09161" target="_blank">pdf</a>]

<h2>TJU-DHD: A Diverse High-Resolution Dataset for Object Detection. (arXiv:2011.09170v1 [cs.CV])</h2>
<h3>Yanwei Pang, Jiale Cao, Yazhao Li, Jin Xie, Hanqing Sun, Jinfeng Gong</h3>
<p>Vehicles, pedestrians, and riders are the most important and interesting
objects for the perception modules of self-driving vehicles and video
surveillance. However, the state-of-the-art performance of detecting such
important objects (esp. small objects) is far from satisfying the demand of
practical systems. Large-scale, rich-diversity, and high-resolution datasets
play an important role in developing better object detection methods to satisfy
the demand. Existing public large-scale datasets such as MS COCO collected from
websites do not focus on the specific scenarios. Moreover, the popular datasets
(e.g., KITTI and Citypersons) collected from the specific scenarios are limited
in the number of images and instances, the resolution, and the diversity. To
attempt to solve the problem, we build a diverse high-resolution dataset
(called TJU-DHD). The dataset contains 115,354 high-resolution images (52%
images have a resolution of 1624$\times$1200 pixels and 48% images have a
resolution of at least 2,560$\times$1,440 pixels) and 709,330 labeled objects
in total with a large variance in scale and appearance. Meanwhile, the dataset
has a rich diversity in season variance, illumination variance, and weather
variance. In addition, a new diverse pedestrian dataset is further built. With
the four different detectors (i.e., the one-stage RetinaNet, anchor-free FCOS,
two-stage FPN, and Cascade R-CNN), experiments about object detection and
pedestrian detection are conducted. We hope that the newly built dataset can
help promote the research on object detection and pedestrian detection in these
two scenes. The dataset is available at https://github.com/tjubiit/TJU-DHD.
</p>
<a href="http://arxiv.org/abs/2011.09170" target="_blank">arXiv:2011.09170</a> [<a href="http://arxiv.org/pdf/2011.09170" target="_blank">pdf</a>]

<h2>On Focal Loss for Class-Posterior Probability Estimation: A Theoretical Perspective. (arXiv:2011.09172v1 [stat.ML])</h2>
<h3>Nontawat Charoenphakdee, Jayakorn Vongkulbhisal, Nuttapong Chairatanakul, Masashi Sugiyama</h3>
<p>The focal loss has demonstrated its effectiveness in many real-world
applications such as object detection and image classification, but its
theoretical understanding has been limited so far. In this paper, we first
prove that the focal loss is classification-calibrated, i.e., its minimizer
surely yields the Bayes-optimal classifier and thus the use of the focal loss
in classification can be theoretically justified. However, we also prove a
negative fact that the focal loss is not strictly proper, i.e., the confidence
score of the classifier obtained by focal loss minimization does not match the
true class-posterior probability and thus it is not reliable as a
class-posterior probability estimator. To mitigate this problem, we next prove
that a particular closed-form transformation of the confidence score allows us
to recover the true class-posterior probability. Through experiments on
benchmark datasets, we demonstrate that our proposed transformation
significantly improves the accuracy of class-posterior probability estimation.
</p>
<a href="http://arxiv.org/abs/2011.09172" target="_blank">arXiv:2011.09172</a> [<a href="http://arxiv.org/pdf/2011.09172" target="_blank">pdf</a>]

<h2>Game Plan: What AI can do for Football, and What Football can do for AI. (arXiv:2011.09192v1 [cs.AI])</h2>
<h3>Karl Tuyls, Shayegan Omidshafiei, Paul Muller, Zhe Wang, Jerome Connor, Daniel Hennes, Ian Graham, William Spearman, Tim Waskett, Dafydd Steele, Pauline Luc, Adria Recasens, Alexandre Galashov, Gregory Thornton, Romuald Elie, Pablo Sprechmann, Pol Moreno, Kris Cao, Marta Garnelo, Praneet Dutta, Michal Valko, Nicolas Heess, Alex Bridgland, Julien Perolat, Bart De Vylder, Ali Eslami, Mark Rowland, Andrew Jaegle, Remi Munos, Trevor Back, Razia Ahamed, Simon Bouton, Nathalie Beauguerlange, Jackson Broshear, Thore Graepel, Demis Hassabis</h3>
<p>The rapid progress in artificial intelligence (AI) and machine learning has
opened unprecedented analytics possibilities in various team and individual
sports, including baseball, basketball, and tennis. More recently, AI
techniques have been applied to football, due to a huge increase in data
collection by professional teams, increased computational power, and advances
in machine learning, with the goal of better addressing new scientific
challenges involved in the analysis of both individual players' and coordinated
teams' behaviors. The research challenges associated with predictive and
prescriptive football analytics require new developments and progress at the
intersection of statistical learning, game theory, and computer vision. In this
paper, we provide an overarching perspective highlighting how the combination
of these fields, in particular, forms a unique microcosm for AI research, while
offering mutual benefits for professional teams, spectators, and broadcasters
in the years to come. We illustrate that this duality makes football analytics
a game changer of tremendous value, in terms of not only changing the game of
football itself, but also in terms of what this domain can mean for the field
of AI. We review the state-of-the-art and exemplify the types of analysis
enabled by combining the aforementioned fields, including illustrative examples
of counterfactual analysis using predictive models, and the combination of
game-theoretic analysis of penalty kicks with statistical learning of player
attributes. We conclude by highlighting envisioned downstream impacts,
including possibilities for extensions to other sports (real and virtual).
</p>
<a href="http://arxiv.org/abs/2011.09192" target="_blank">arXiv:2011.09192</a> [<a href="http://arxiv.org/pdf/2011.09192" target="_blank">pdf</a>]

<h2>Learning control for transmission and navigation with a mobile robot under unknown communication rates. (arXiv:2011.09193v1 [cs.RO])</h2>
<h3>L. Busoniu, V. S. Varma, J. Loheac, A. Codrean, O. Stefan, I.-C. Morarescu, S. Lasaulce</h3>
<p>In tasks such as surveying or monitoring remote regions, an autonomous robot
must move while transmitting data over a wireless network with unknown,
position-dependent transmission rates. For such a robot, this paper considers
the problem of transmitting a data buffer in minimum time, while possibly also
navigating towards a goal position. Two approaches are proposed, each
consisting of a machine-learning component that estimates the rate function
from samples; and of an optimal-control component that moves the robot given
the current rate function estimate. Simple obstacle avoidance is performed for
the case without a goal position. In extensive simulations, these methods
achieve competitive performance compared to known-rate and unknown-rate
baselines. A real indoor experiment is provided in which a Parrot AR.Drone 2
successfully learns to transmit the buffer.
</p>
<a href="http://arxiv.org/abs/2011.09193" target="_blank">arXiv:2011.09193</a> [<a href="http://arxiv.org/pdf/2011.09193" target="_blank">pdf</a>]

<h2>Communication-Aware Energy Efficient Trajectory Planning with Limited Channel Knowledge. (arXiv:2011.09206v1 [cs.RO])</h2>
<h3>D. Bonilla Licea, M. Bonilla, M. Ghogho, S. Lasaulce, V. S. Varma</h3>
<p>Wireless communications is nowadays an important aspect of robotics. There
are many applications in which a robot must move to a certain goal point while
transmitting information through a wireless channel which depends on the
particular trajectory chosen by the robot to reach the goal point. In this
context, we develop a method to generate optimum trajectories which allow the
robot to reach the goal point using little mechanical energy while transmitting
as much data as possible. This is done by optimizing the trajectory (path and
velocity profile) so that the robot consumes less energy while also offering
good wireless channel conditions. We consider a realistic wireless channel
model as well as a realistic dynamic model for the mobile robot (considered
here to be a drone). Simulations results illustrate the merits of the proposed
method.
</p>
<a href="http://arxiv.org/abs/2011.09206" target="_blank">arXiv:2011.09206</a> [<a href="http://arxiv.org/pdf/2011.09206" target="_blank">pdf</a>]

<h2>Res-GCNN: A Lightweight Residual Graph Convolutional Neural Networks for Human Trajectory Forecasting. (arXiv:2011.09214v1 [cs.CV])</h2>
<h3>Yanwu Ge, Mingliang Song</h3>
<p>Autonomous driving vehicles (ADVs) hold great hopes to solve traffic
congestion problems and reduce the number of traffic accidents. Accurate
trajectories prediction of other traffic agents around ADVs is of key
importance to achieve safe and efficient driving. Pedestrians, particularly,
are more challenging to forecast due to their complex social in-teractions and
randomly moving patterns. We propose a Residual Graph Convolutional Neural
Network (Res-GCNN), which models the interactive behaviors of pedes-trians by
using the adjacent matrix of the constructed graph for the current scene.
Though the proposed Res-GCNN is quite lightweight with only about 6.4 kilo
parameters which outperforms all other methods in terms of parameters size, our
experimental results show an improvement over the state of art by 13.3% on the
Final Displacement Error (FDE) which reaches 0.65 meter. As for the Average
Dis-placement Error (ADE), we achieve a suboptimal result (the value is 0.37
meter), which is also very competitive. The Res-GCNN is evaluated in the
platform with an NVIDIA GeForce RTX1080Ti GPU, and its mean inference time of
the whole dataset is only about 2.2 microseconds. Compared with other methods,
the proposed method shows strong potential for onboard application accounting
for forecasting accuracy and time efficiency. The code will be made publicly
available on GitHub.
</p>
<a href="http://arxiv.org/abs/2011.09214" target="_blank">arXiv:2011.09214</a> [<a href="http://arxiv.org/pdf/2011.09214" target="_blank">pdf</a>]

<h2>CGAP2: Context and gap aware predictive pose framework for early detection of gestures. (arXiv:2011.09216v1 [cs.CV])</h2>
<h3>Nishant Bhattacharya, Suresh Sundaram</h3>
<p>With a growing interest in autonomous vehicles' operation, there is an
equally increasing need for efficient anticipatory gesture recognition systems
for human-vehicle interaction. Existing gesture-recognition algorithms have
been primarily restricted to historical data. In this paper, we propose a novel
context and gap aware pose prediction framework(CGAP2), which predicts future
pose data for anticipatory recognition of gestures in an online fashion. CGAP2
implements an encoder-decoder architecture paired with a pose prediction module
to anticipate future frames followed by a shallow classifier. CGAP2 pose
prediction module uses 3D convolutional layers and depends on the number of
pose frames supplied, the time difference between each pose frame, and the
number of predicted pose frames. The performance of CGAP2 is evaluated on the
Human3.6M dataset with the MPJPE metric. For pose prediction of 15 frames in
advance, an error of 79.0mm is achieved. The pose prediction module consists of
only 26M parameters and can run at 50 FPS on the NVidia RTX Titan. Furthermore,
the ablation study indicates supplying higher context information to the pose
prediction module can be detrimental for anticipatory recognition. CGAP2 has a
1-second time advantage compared to other gesture recognition systems, which
can be crucial for autonomous vehicles.
</p>
<a href="http://arxiv.org/abs/2011.09216" target="_blank">arXiv:2011.09216</a> [<a href="http://arxiv.org/pdf/2011.09216" target="_blank">pdf</a>]

<h2>Prognostic and Health Management (PHM) tool for Robot Operating System (ROS). (arXiv:2011.09222v1 [cs.RO])</h2>
<h3>Hakan Gencturk, Elcin Erdogan, Mustafa Karaca, Ugur Yayan</h3>
<p>Nowadays, prognostics-aware systems are increasingly used in many systems and
it is critical for sustaining autonomy. All engineering systems, especially
robots, are not perfect. Absence of failures in a certain time is the perfect
system and it is impossible practically. In all engineering works, we must try
to predict or minimize/prevent failures in the system. Failures in the systems
are generally unknown, so prediction of these failures and reliability of the
system is made by prediction process. Reliability analysis is important for the
improving the system performance, extending system lifetime, etc. Prognostic
and Health Management (PHM) includes reliability, safety, predictive fault
detection / isolation, advanced diagnostics / prognostics, component lifecycle
tracking, health reporting and information management, etc. This study proposes
an open source robot prognostic and health management tool using model-based
methodology namely "Prognostics and Health Management tool for ROS". This tool
is a generic tool for using with any kind of robot (mobile robot, robot arm,
drone etc.) with compatible with ROS. Some features of this tool are managing /
monitoring robots' health, RUL, probability of task completion (PoTC) etc. User
is able to enter the necessary equations and components information (hazard
rates, robot configuration etc.) to the PHM tool and the other sensory data
like temperature, humidity, pressure, load etc. In addition to these, a case
study is conducted for the mobile robots (OTA) using this tool.
</p>
<a href="http://arxiv.org/abs/2011.09222" target="_blank">arXiv:2011.09222</a> [<a href="http://arxiv.org/pdf/2011.09222" target="_blank">pdf</a>]

<h2>FixBi: Bridging Domain Spaces for Unsupervised Domain Adaptation. (arXiv:2011.09230v1 [cs.CV])</h2>
<h3>Jaemin Na, Heechul Jung, HyungJin Chang, Wonjun Hwang</h3>
<p>Unsupervised domain adaptation (UDA) methods for learning domain invariant
representations have achieved remarkable progress. However, few studies have
been conducted on the case of large domain discrepancies between a source and a
target domain. In this paper, we propose a UDA method that effectively handles
such large domain discrepancies. We introduce a fixed ratio-based mixup to
augment multiple intermediate domains between the source and target domain.
From the augmented-domains, we train the source-dominant model and the
target-dominant model that have complementary characteristics. Using our
confidence-based learning methodologies, e.g., bidirectional matching with
high-confidence predictions and self-penalization using low-confidence
predictions, the models can learn from each other or from its own results.
Through our proposed methods, the models gradually transfer domain knowledge
from the source to the target domain. Extensive experiments demonstrate the
superiority of our proposed method on three public benchmarks: Office-31,
Office-Home, and VisDA-2017.
</p>
<a href="http://arxiv.org/abs/2011.09230" target="_blank">arXiv:2011.09230</a> [<a href="http://arxiv.org/pdf/2011.09230" target="_blank">pdf</a>]

<h2>A Multi-class Approach -- Building a Visual Classifier based on Textual Descriptions using Zero-Shot Learning. (arXiv:2011.09236v1 [cs.CV])</h2>
<h3>Preeti Jagdish Sajjan, Frank G. Glavin</h3>
<p>Machine Learning (ML) techniques for image classification routinely require
many labelled images for training the model and while testing, we ought to use
images belonging to the same domain as those used for training. In this paper,
we overcome the two main hurdles of ML, i.e. scarcity of data and constrained
prediction of the classification model. We do this by introducing a visual
classifier which uses a concept of transfer learning, namely Zero-Shot Learning
(ZSL), and standard Natural Language Processing techniques. We train a
classifier by mapping labelled images to their textual description instead of
training it for specific classes. Transfer learning involves transferring
knowledge across domains that are similar. ZSL intelligently applies the
knowledge learned while training for future recognition tasks. ZSL
differentiates classes as two types: seen and unseen classes. Seen classes are
the classes upon which we have trained our model and unseen classes are the
classes upon which we test our model. The examples from unseen classes have not
been encountered in the training phase. Earlier research in this domain focused
on developing a binary classifier but, in this paper, we present a multi-class
classifier with a Zero-Shot Learning approach.
</p>
<a href="http://arxiv.org/abs/2011.09236" target="_blank">arXiv:2011.09236</a> [<a href="http://arxiv.org/pdf/2011.09236" target="_blank">pdf</a>]

<h2>Indoor Point-to-Point Navigation with Deep Reinforcement Learning and Ultra-wideband. (arXiv:2011.09241v1 [cs.RO])</h2>
<h3>Enrico Sutera, Vittorio Mazzia, Francesco Salvetti, Giovanni Fantin, Marcello Chiaberge</h3>
<p>Indoor autonomous navigation requires a precise and accurate localization
system able to guide robots through cluttered, unstructured and dynamic
environments. Ultra-wideband (UWB) technology, as an indoor positioning system,
offers precise localization and tracking, but moving obstacles and
non-line-of-sight occurrences can generate noisy and unreliable signals. That,
combined with sensors noise, unmodeled dynamics and environment changes can
result in a failure of the guidance algorithm of the robot. We demonstrate how
a power-efficient and low computational cost point-to-point local planner,
learnt with deep reinforcement learning (RL), combined with UWB localization
technology can constitute a robust and resilient to noise short-range guidance
system complete solution. We trained the RL agent on a simulated environment
that encapsulates the robot dynamics and task constraints and then, we tested
the learnt point-to-point navigation policies in a real setting with more than
two-hundred experimental evaluations using UWB localization. Our results show
that the computational efficient end-to-end policy learnt in plain simulation,
that directly maps low-range sensors signals to robot controls, deployed in
combination with ultra-wideband noisy localization in a real environment, can
provide a robust, scalable and at-the-edge low-cost navigation system solution.
</p>
<a href="http://arxiv.org/abs/2011.09241" target="_blank">arXiv:2011.09241</a> [<a href="http://arxiv.org/pdf/2011.09241" target="_blank">pdf</a>]

<h2>Experimental Study on Reinforcement Learning-based Control of an Acrobot. (arXiv:2011.09246v1 [cs.RO])</h2>
<h3>Leo Dostal, Alexej Bespalko, Daniel A. Duecker</h3>
<p>We present computational and experimental results on how artificial
intelligence (AI) learns to control an Acrobot using reinforcement learning
(RL). Thereby the experimental setup is designed as an embedded system, which
is of interest for robotics and energy harvesting applications. Specifically,
we study the control of angular velocity of the Acrobot, as well as control of
its total energy, which is the sum of the kinetic and the potential energy. By
this means the RL algorithm is designed to drive the angular velocity or the
energy of the first pendulum of the Acrobot towards a desired value. With this,
libration or full rotation of the unactuated pendulum of the Acrobot is
achieved. Moreover, investigations of the Acrobot control are carried out,
which lead to insights about the influence of the state space discretization,
the episode length, the action space or the mass of the driven pendulum on the
RL control. By further numerous simulations and experiments the effects of
parameter variations are evaluated.
</p>
<a href="http://arxiv.org/abs/2011.09246" target="_blank">arXiv:2011.09246</a> [<a href="http://arxiv.org/pdf/2011.09246" target="_blank">pdf</a>]

<h2>Inverse Reinforcement Learning via Matching of Optimality Profiles. (arXiv:2011.09264v1 [cs.LG])</h2>
<h3>Luis Haug, Ivan Ovinnikon, Eugene Bykovets</h3>
<p>The goal of inverse reinforcement learning (IRL) is to infer a reward
function that explains the behavior of an agent performing a task. The
assumption that most approaches make is that the demonstrated behavior is
near-optimal. In many real-world scenarios, however, examples of truly optimal
behavior are scarce, and it is desirable to effectively leverage sets of
demonstrations of suboptimal or heterogeneous performance, which are easier to
obtain. We propose an algorithm that learns a reward function from such
demonstrations together with a weak supervision signal in the form of a
distribution over rewards collected during the demonstrations (or, more
generally, a distribution over cumulative discounted future rewards). We view
such distributions, which we also refer to as optimality profiles, as summaries
of the degree of optimality of the demonstrations that may, for example,
reflect the opinion of a human expert. Given an optimality profile and a small
amount of additional supervision, our algorithm fits a reward function, modeled
as a neural network, by essentially minimizing the Wasserstein distance between
the corresponding induced distribution and the optimality profile. We show that
our method is capable of learning reward functions such that policies trained
to optimize them outperform the demonstrations used for fitting the reward
functions.
</p>
<a href="http://arxiv.org/abs/2011.09264" target="_blank">arXiv:2011.09264</a> [<a href="http://arxiv.org/pdf/2011.09264" target="_blank">pdf</a>]

<h2>Continuous Emotion Recognition with Spatiotemporal Convolutional Neural Networks. (arXiv:2011.09280v1 [cs.CV])</h2>
<h3>Thomas Teixeira, Eric Granger, Alessandro Lameiras Koerich</h3>
<p>The attention in affect computing and emotion recognition has increased in
the last decade. Facial expressions are one of the most powerful ways for
depicting specific patterns in human behavior and describing human emotional
state. Nevertheless, even for humans, identifying facial expressions is
difficult, and automatic video-based systems for facial expression recognition
(FER) have often suffered from variations in expressions among individuals, and
from a lack of diverse and cross-culture training datasets. However, with video
sequences captured in-the-wild and more complex emotion representation such as
dimensional models, deep FER systems have the ability to learn more
discriminative feature representations. In this paper, we present a survey of
the state-of-the-art approaches based on convolutional neural networks (CNNs)
for long video sequences recorded with in-the-wild settings, by considering the
continuous emotion space of valence and arousal. Since few studies have used
3D-CNN for FER systems and dimensional representation of emotions, we propose
an inflated 3D-CNN architecture, allowing for weight inflation of pre-trained
2D-CNN model, in order to operate the essential transfer learning for our
video-based application. As a baseline, we also considered a 2D-CNN
architecture cascaded network with a long short term memory network, therefore
we could finally conclude with a model comparison over two approaches for
spatiotemporal representation of facial features and performing the regression
of valence/arousal values for emotion prediction. The experimental results on
RAF-DB and SEWA-DB datasets have shown that these fine-tuned architectures
allow to effectively encode the spatiotemporal information from raw pixel
images, and achieved far better results than the current state-of-the-art.
</p>
<a href="http://arxiv.org/abs/2011.09280" target="_blank">arXiv:2011.09280</a> [<a href="http://arxiv.org/pdf/2011.09280" target="_blank">pdf</a>]

<h2>Using Unity to Help Solve Intelligence. (arXiv:2011.09294v1 [cs.AI])</h2>
<h3>Tom Ward, Andrew Bolt, Nik Hemmings, Simon Carter, Manuel Sanchez, Ricardo Barreira, Seb Noury, Keith Anderson, Jay Lemmon, Jonathan Coe, Piotr Trochim, Tom Handley, Adrian Bolton</h3>
<p>In the pursuit of artificial general intelligence, our most significant
measurement of progress is an agent's ability to achieve goals in a wide range
of environments. Existing platforms for constructing such environments are
typically constrained by the technologies they are founded on, and are
therefore only able to provide a subset of scenarios necessary to evaluate
progress. To overcome these shortcomings, we present our use of Unity, a widely
recognized and comprehensive game engine, to create more diverse, complex,
virtual simulations. We describe the concepts and components developed to
simplify the authoring of these environments, intended for use predominantly in
the field of reinforcement learning. We also introduce a practical approach to
packaging and re-distributing environments in a way that attempts to improve
the robustness and reproducibility of experiment results. To illustrate the
versatility of our use of Unity compared to other solutions, we highlight
environments already created using our approach from published papers. We hope
that others can draw inspiration from how we adapted Unity to our needs, and
anticipate increasingly varied and complex environments to emerge from our
approach as familiarity grows.
</p>
<a href="http://arxiv.org/abs/2011.09294" target="_blank">arXiv:2011.09294</a> [<a href="http://arxiv.org/pdf/2011.09294" target="_blank">pdf</a>]

<h2>Explicitly Learning Topology for Differentiable Neural Architecture Search. (arXiv:2011.09300v1 [cs.CV])</h2>
<h3>Tao Huang, Shan You, Yibo Yang, Zhuozhuo Tu, Fei Wang, Chen Qian, Changshui Zhang</h3>
<p>Differentiable neural architecture search (DARTS) has gained much success in
discovering more flexible and diverse cell types. Current methods couple the
operations and topology during search, and simply derive optimal topology by a
hand-craft rule. However, topology also matters for neural architectures since
it controls the interactions between features of operations. In this paper, we
highlight the topology learning in differentiable NAS, and propose an explicit
topology modeling method, named TopoNAS, to directly decouple the operation
selection and topology during search. Concretely, we introduce a set of
topological variables and a combinatorial probabilistic distribution to
explicitly indicate the target topology. Besides, we also leverage a
passive-aggressive regularization to suppress invalid topology within supernet.
Our introduced topological variables can be jointly learned with operation
variables and supernet weights, and apply to various DARTS variants. Extensive
experiments on CIFAR-10 and ImageNet validate the effectiveness of our proposed
TopoNAS. The results show that TopoNAS does enable to search cells with more
diverse and complex topology, and boost the performance significantly. For
example, TopoNAS can improve DARTS by 0.16\% accuracy on CIFAR-10 dataset with
40\% parameters reduced or 0.35\% with similar parameters.
</p>
<a href="http://arxiv.org/abs/2011.09300" target="_blank">arXiv:2011.09300</a> [<a href="http://arxiv.org/pdf/2011.09300" target="_blank">pdf</a>]

<h2>End-to-End Object Detection with Adaptive Clustering Transformer. (arXiv:2011.09315v1 [cs.CV])</h2>
<h3>Minghang Zheng, Peng Gao, Xiaogang Wang, Hongsheng Li, Hao Dong</h3>
<p>End-to-end Object Detection with Transformer (DETR)proposes to perform object
detection with Transformer and achieve comparable performance with two-stage
object detection like Faster-RCNN. However, DETR needs huge computational
resources for training and inference due to the high-resolution spatial input.
In this paper, a novel variant of transformer named Adaptive Clustering
Transformer(ACT) has been proposed to reduce the computation cost for
high-resolution input. ACT cluster the query features adaptively using Locality
Sensitive Hashing (LSH) and ap-proximate the query-key interaction using the
prototype-key interaction. ACT can reduce the quadratic O(N2) complexity inside
self-attention into O(NK) where K is the number of prototypes in each layer.
ACT can be a drop-in module replacing the original self-attention module
without any training. ACT achieves a good balance between accuracy and
computation cost (FLOPs). The code is available as supplementary for the ease
of experiment replication and verification.
</p>
<a href="http://arxiv.org/abs/2011.09315" target="_blank">arXiv:2011.09315</a> [<a href="http://arxiv.org/pdf/2011.09315" target="_blank">pdf</a>]

<h2>Online Exemplar Fine-Tuning for Image-to-Image Translation. (arXiv:2011.09330v1 [cs.CV])</h2>
<h3>Taewon Kang, Soohyun Kim, Sunwoo Kim, Seungryong Kim</h3>
<p>Existing techniques to solve exemplar-based image-to-image translation within
deep convolutional neural networks (CNNs) generally require a training phase to
optimize the network parameters on domain-specific and task-specific
benchmarks, thus having limited applicability and generalization ability. In
this paper, we propose a novel framework, for the first time, to solve
exemplar-based translation through an online optimization given an input image
pair, called online exemplar fine-tuning (OEFT), in which we fine-tune the
off-the-shelf and general-purpose networks to the input image pair themselves.
We design two sub-networks, namely correspondence fine-tuning and multiple GAN
inversion, and optimize these network parameters and latent codes, starting
from the pre-trained ones, with well-defined loss functions. Our framework does
not require the off-line training phase, which has been the main challenge of
existing methods, but the pre-trained networks to enable optimization in
online. Experimental results prove that our framework is effective in having a
generalization power to unseen image pairs and clearly even outperforms the
state-of-the-arts needing the intensive training phase.
</p>
<a href="http://arxiv.org/abs/2011.09330" target="_blank">arXiv:2011.09330</a> [<a href="http://arxiv.org/pdf/2011.09330" target="_blank">pdf</a>]

<h2>Learning Interpretable Flight's 4D Landing Parameters Using Tunnel Gaussian Process. (arXiv:2011.09335v1 [cs.LG])</h2>
<h3>Sim Kuan Goh, Narendra Pratap Singh, Zhi Jun Lim, Sameer Alam</h3>
<p>Approach and landing accidents (ALAs) have resulted in a significant number
of hull losses worldwide, aside from runway excursion, hard landing, landing
short. Technologies (e.g., instrument landing system) and procedures (e.g.,
stabilized approach criteria) have been developed to reduce ALA risks. In this
paper, we propose a data-driven method to learn and interpret flight's 4D
approach and landing parameters to facilitate comprehensible and actionable
insights of landing dynamics for aircrew and air traffic controller (ATCO) in
real-time. Specifically, we develop a tunnel Gaussian process (TGP) model to
gain an insight into the landing dynamics of aircraft using advanced surface
movement guidance and control system (A-SMGCS) data, which then indicates the
stability of flight. TGP hybridizes the strengths of sparse variational
Gaussian process and polar Gaussian process to learn from a large amount of
data in cylindrical coordinates. We examine TGP qualitatively and
quantitatively by synthesizing two complex trajectory datasets. Empirically,
TGP reconstructed the structure of the synthesized trajectories. When applied
to operational A-SMGCS data, TGP provides the probabilistic description of
landing dynamics and interpretable 4D tunnel views of approach and landing
parameters. The 4D tunnel views can facilitate the analysis of procedure
adherence and augment existing aircrew and ATCO's display during the approach
and landing procedures, enabling necessary corrective actions. The proposed TGP
model can also provide insights and aid the design of landing procedures in
complex runway configurations such as parallel approach. Moreover, the
extension of TGP model to the next generation of landing systems (e.g., GNSS
landing system) is straight-forward. The interactive visualization of our
findings are available at https://simkuangoh.github.io/TunnelGP/.
</p>
<a href="http://arxiv.org/abs/2011.09335" target="_blank">arXiv:2011.09335</a> [<a href="http://arxiv.org/pdf/2011.09335" target="_blank">pdf</a>]

<h2>Bias-Variance Trade-off and Overlearning in Dynamic Decision Problems. (arXiv:2011.09349v1 [stat.ML])</h2>
<h3>A. Max Reppen, H. Mete Soner</h3>
<p>Modern Monte Carlo-type approaches to dynamic decision problems face the
classical bias-variance trade-off. Deep neural networks can overlearn the data
and construct feedback actions which are non-adapted to the information flow
and hence, become susceptible to generalization error. We prove asymptotic
overlearning for fixed training sets, but also provide a non-asymptotic upper
bound on overperformance based on the Rademacher complexity demonstrating the
convergence of these algorithms for sufficiently large training sets.
Numerically studied stylized examples illustrate these possibilities, the
dependence on the dimension and the effectiveness of this approach.
</p>
<a href="http://arxiv.org/abs/2011.09349" target="_blank">arXiv:2011.09349</a> [<a href="http://arxiv.org/pdf/2011.09349" target="_blank">pdf</a>]

<h2>FLaaS: Federated Learning as a Service. (arXiv:2011.09359v1 [cs.LG])</h2>
<h3>Nicolas Kourtellis, Kleomenis Katevas, Diego Perino</h3>
<p>Federated Learning (FL) is emerging as a promising technology to build
machine learning models in a decentralized, privacy-preserving fashion. Indeed,
FL enables local training on user devices, avoiding user data to be transferred
to centralized servers, and can be enhanced with differential privacy
mechanisms. Although FL has been recently deployed in real systems, the
possibility of collaborative modeling across different 3rd-party applications
has not yet been explored. In this paper, we tackle this problem and present
Federated Learning as a Service (FLaaS), a system enabling different scenarios
of 3rd-party application collaborative model building and addressing the
consequent challenges of permission and privacy management, usability, and
hierarchical model training. FLaaS can be deployed in different operational
environments. As a proof of concept, we implement it on a mobile phone setting
and discuss practical implications of results on simulated and real devices
with respect to on-device training CPU cost, memory footprint and power
consumed per FL model round. Therefore, we demonstrate FLaaS's feasibility in
building unique or joint FL models across applications for image object
detection in a few hours, across 100 devices.
</p>
<a href="http://arxiv.org/abs/2011.09359" target="_blank">arXiv:2011.09359</a> [<a href="http://arxiv.org/pdf/2011.09359" target="_blank">pdf</a>]

<h2>A Knowledge Distillation Ensemble Framework for Predicting Short and Long-term Hospitalisation Outcomes from Electronic Health Records Data. (arXiv:2011.09361v1 [cs.LG])</h2>
<h3>Zina M Ibrahim, Daniel Bean, Thomas Searle, Honghan Wu, Anthony Shek, Zeljko Kraljevic, James Galloway, Sam Norton, James T Teo, Richard JB Dobson</h3>
<p>The ability to perform accurate prognosis of patients is crucial for
proactive clinical decision making, informed resource management and
personalised care. Existing outcome prediction models suffer from a low recall
of infrequent positive outcomes. We present a highly-scalable and robust
machine learning framework to automatically predict adversity represented by
mortality and ICU admission from time-series vital signs and laboratory results
obtained within the first 24 hours of hospital admission. The stacked platform
comprises two components: a) an unsupervised LSTM Autoencoder that learns an
optimal representation of the time-series, using it to differentiate the less
frequent patterns which conclude with an adverse event from the majority
patterns that do not, and b) a gradient boosting model, which relies on the
constructed representation to refine prediction, incorporating static features
of demographics, admission details and clinical summaries. The model is used to
assess a patient's risk of adversity over time and provides visual
justifications of its prediction based on the patient's static features and
dynamic signals. Results of three case studies for predicting mortality and ICU
admission show that the model outperforms all existing outcome prediction
models, achieving PR-AUC of 0.93 (95$%$ CI: 0.878 - 0.969) in predicting
mortality in ICU and general ward settings and 0.987 (95$%$ CI: 0.985-0.995) in
predicting ICU admission.
</p>
<a href="http://arxiv.org/abs/2011.09361" target="_blank">arXiv:2011.09361</a> [<a href="http://arxiv.org/pdf/2011.09361" target="_blank">pdf</a>]

<h2>A Discussion on Practical Considerations with Sparse Regression Methodologies. (arXiv:2011.09362v1 [cs.LG])</h2>
<h3>Owais Sarwar, Benjamin Sauk, Nikolaos V. Sahinidis</h3>
<p>Sparse linear regression is a vast field and there are many different
algorithms available to build models. Two new papers published in Statistical
Science study the comparative performance of several sparse regression
methodologies, including the lasso and subset selection. Comprehensive
empirical analyses allow the researchers to demonstrate the relative merits of
each estimator and provide guidance to practitioners. In this discussion, we
summarize and compare the two studies and we examine points of agreement and
divergence, aiming to provide clarity and value to users. The authors have
started a highly constructive dialogue, our goal is to continue it.
</p>
<a href="http://arxiv.org/abs/2011.09362" target="_blank">arXiv:2011.09362</a> [<a href="http://arxiv.org/pdf/2011.09362" target="_blank">pdf</a>]

<h2>Self-Gradient Networks. (arXiv:2011.09364v1 [cs.LG])</h2>
<h3>Hossein Aboutalebi, Mohammad Javad Shafiee Alexander Wong</h3>
<p>The incredible effectiveness of adversarial attacks on fooling deep neural
networks poses a tremendous hurdle in the widespread adoption of deep learning
in safety and securitycritical domains. While adversarial defense mechanisms
have been proposed since the discovery of the adversarial vulnerability issue
of deep neural networks, there is a long path to fully understand and address
this issue. In this study, we hypothesize that part of the reason for the
incredible effectiveness of adversarial attacks is their ability to implicitly
tap into and exploit the gradient flow of a deep neural network. This innate
ability to exploit gradient flow makes defending against such attacks quite
challenging. Motivated by this hypothesis we argue that if a deep neural
network architecture can explicitly tap into its own gradient flow during the
training, it can boost its defense capability significantly. Inspired by this
fact, we introduce the concept of self-gradient networks, a novel deep neural
network architecture designed to be more robust against adversarial
perturbations. Gradient flow information is leveraged within self-gradient
networks to achieve greater perturbation stability beyond what can be achieved
in the standard training process. We conduct a theoretical analysis to gain
better insights into the behaviour of the proposed self-gradient networks to
illustrate the efficacy of leverage this additional gradient flow information.
The proposed self-gradient network architecture enables much more efficient and
effective adversarial training, leading to faster convergence towards an
adversarially robust solution by at least 10?. Experimental results demonstrate
the effectiveness of self-gradient networks when compared with state-of-the-art
adversarial learning strategies, with 10% improvement on the CIFAR10 dataset
under PGD and CW adversarial perturbations.
</p>
<a href="http://arxiv.org/abs/2011.09364" target="_blank">arXiv:2011.09364</a> [<a href="http://arxiv.org/pdf/2011.09364" target="_blank">pdf</a>]

<h2>Attentional Separation-and-Aggregation Network for Self-supervised Depth-Pose Learning in Dynamic Scenes. (arXiv:2011.09369v1 [cs.CV])</h2>
<h3>Feng Gao, Jincheng Yu, Hao Shen, Yu Wang, Huazhong Yang</h3>
<p>Learning depth and ego-motion from unlabeled videos via self-supervision from
epipolar projection can improve the robustness and accuracy of the 3D
perception and localization of vision-based robots. However, the rigid
projection computed by ego-motion cannot represent all scene points, such as
points on moving objects, leading to false guidance in these regions. To
address this problem, we propose an Attentional Separation-and-Aggregation
Network (ASANet), which can learn to distinguish and extract the scene's static
and dynamic characteristics via the attention mechanism. We further propose a
novel MotionNet with an ASANet as the encoder, followed by two separate
decoders, to estimate the camera's ego-motion and the scene's dynamic motion
field. Then, we introduce an auto-selecting approach to detect the moving
objects for dynamic-aware learning automatically. Empirical experiments
demonstrate that our method can achieve the state-of-the-art performance on the
KITTI benchmark.
</p>
<a href="http://arxiv.org/abs/2011.09369" target="_blank">arXiv:2011.09369</a> [<a href="http://arxiv.org/pdf/2011.09369" target="_blank">pdf</a>]

<h2>Introduction to Core-sets: an Updated Survey. (arXiv:2011.09384v1 [cs.LG])</h2>
<h3>Dan Feldman</h3>
<p>In optimization or machine learning problems we are given a set of items,
usually points in some metric space, and the goal is to minimize or maximize an
objective function over some space of candidate solutions. For example, in
clustering problems, the input is a set of points in some metric space, and a
common goal is to compute a set of centers in some other space (points, lines)
that will minimize the sum of distances to these points. In database queries,
we may need to compute such a some for a specific query set of $k$ centers.

However, traditional algorithms cannot handle modern systems that require
parallel real-time computations of infinite distributed streams from sensors
such as GPS, audio or video that arrive to a cloud, or networks of weaker
devices such as smartphones or robots.

Core-set is a "small data" summarization of the input "big data", where every
possible query has approximately the same answer on both data sets. Generic
techniques enable efficient coreset \changed{maintenance} of streaming,
distributed and dynamic data. Traditional algorithms can then be applied on
these coresets to maintain the approximated optimal solutions.

The challenge is to design coresets with provable tradeoff between their size
and approximation error. This survey summarizes such constructions in a
retrospective way, that aims to unified and simplify the state-of-the-art.
</p>
<a href="http://arxiv.org/abs/2011.09384" target="_blank">arXiv:2011.09384</a> [<a href="http://arxiv.org/pdf/2011.09384" target="_blank">pdf</a>]

<h2>Plug-And-Play Learned Gaussian-mixture Approximate Message Passing. (arXiv:2011.09388v1 [stat.ML])</h2>
<h3>Osman Musa, Peter Jung, Giuseppe Caire</h3>
<p>Deep unfolding showed to be a very successful approach for accelerating and
tuning classical signal processing algorithms. In this paper, we propose
learned Gaussian-mixture AMP (L-GM-AMP) - a plug-and-play compressed sensing
(CS) recovery algorithm suitable for any i.i.d. source prior. Our algorithm
builds upon Borgerding's learned AMP (LAMP), yet significantly improves it by
adopting a universal denoising function within the algorithm. The robust and
flexible denoiser is a byproduct of modelling source prior with a
Gaussian-mixture (GM), which can well approximate continuous, discrete, as well
as mixture distributions. Its parameters are learned using standard
backpropagation algorithm. To demonstrate robustness of the proposed algorithm,
we conduct Monte-Carlo (MC) simulations for both mixture and discrete
distributions. Numerical evaluation shows that the L-GM-AMP algorithm achieves
state-of-the-art performance without any knowledge of the source prior.
</p>
<a href="http://arxiv.org/abs/2011.09388" target="_blank">arXiv:2011.09388</a> [<a href="http://arxiv.org/pdf/2011.09388" target="_blank">pdf</a>]

<h2>Diverse Plausible Shape Completions from Ambiguous Depth Images. (arXiv:2011.09390v1 [cs.RO])</h2>
<h3>Brad Saund, Dmitry Berenson</h3>
<p>We propose PSSNet, a network architecture for generating diverse plausible 3D
reconstructions from a single 2.5D depth image. Existing methods tend to
produce only small variations on a single shape, even when multiple shapes are
consistent with an observation. To obtain diversity we alter a Variational Auto
Encoder by providing a learned shape bounding box feature as side information
during training. Since these features are known during training, we are able to
add a supervised loss to the encoder and noiseless values to the decoder. To
evaluate, we sample a set of completions from a network, construct a set of
plausible shape matches for each test observation, and compare using our
plausible diversity metric defined over sets of shapes. We perform experiments
using Shapenet mugs and partially-occluded YCB objects and find that our method
performs comparably in datasets with little ambiguity, and outperforms existing
methods when many shapes plausibly fit an observed depth image. We demonstrate
one use for PSSNet on a physical robot when grasping objects in occlusion and
clutter.
</p>
<a href="http://arxiv.org/abs/2011.09390" target="_blank">arXiv:2011.09390</a> [<a href="http://arxiv.org/pdf/2011.09390" target="_blank">pdf</a>]

<h2>Larq Compute Engine: Design, Benchmark, and Deploy State-of-the-Art Binarized Neural Networks. (arXiv:2011.09398v1 [cs.LG])</h2>
<h3>Tom Bannink, Adam Hillier, Lukas Geiger, Tim de Bruin, Leon Overweel, Jelmer Neeven, Koen Helwegen</h3>
<p>We introduce Larq Compute Engine, the world's fastest Binarized Neural
Network (BNN) inference engine, and use this framework to investigate several
important questions about the efficiency of BNNs and to design a new
state-of-the-art BNN architecture. LCE provides highly optimized
implementations of binary operations and accelerates binary convolutions by 8.5
- 18.5x compared to their full-precision counterparts on Pixel 1 phones. LCE's
integration with Larq and a sophisticated MLIR-based converter allow users to
move smoothly from training to deployment. By extending TensorFlow and
TensorFlow Lite, LCE supports models which combine binary and full-precision
layers, and can be easily integrated into existing applications. Using LCE, we
analyze the performance of existing BNN computer vision architectures and
develop QuickNet, a simple, easy-to-reproduce BNN that outperforms existing
binary networks in terms of latency and accuracy on ImageNet. Furthermore, we
investigate the impact of full-precision shortcuts and the relationship between
number of MACs and model latency. We are convinced that empirical performance
should drive BNN architecture design and hope this work will facilitate others
to design, benchmark and deploy binary models.
</p>
<a href="http://arxiv.org/abs/2011.09398" target="_blank">arXiv:2011.09398</a> [<a href="http://arxiv.org/pdf/2011.09398" target="_blank">pdf</a>]

<h2>Explainable AI for System Failures: Generating Explanations that ImproveHuman Assistance in Fault Recovery. (arXiv:2011.09407v1 [cs.AI])</h2>
<h3>Devleena Das, Siddhartha Banerjee, Sonia Chernova</h3>
<p>With the growing capabilities of intelligent systems, the integration of
artificial intelligence (AI) and robots in everyday life is increasing.
However, when interacting in such complex human environments, the failure of
intelligent systems, such as robots, can be inevitable, requiring recovery
assistance from users. In this work, we develop automated, natural language
explanations for failures encountered during an AI agents' plan execution.
These explanations are developed with a focus of helping non-expert users
understand different point of failures to better provide recovery assistance.
Specifically, we introduce a context-based information type for explanations
that can both help non-expert users understand the underlying cause of a system
failure, and select proper failure recoveries. Additionally, we extend an
existing sequence-to-sequence methodology to automatically generate our
context-based explanations. By doing so, we are able develop a model that can
generalize context-based explanations over both different failure types and
failure scenarios.
</p>
<a href="http://arxiv.org/abs/2011.09407" target="_blank">arXiv:2011.09407</a> [<a href="http://arxiv.org/pdf/2011.09407" target="_blank">pdf</a>]

<h2>Language Acquisition Environment for Human-Level Artificial Intelligence. (arXiv:2011.09410v1 [cs.AI])</h2>
<h3>Deokgun Park</h3>
<p>Despite recent advances in many application-specific domains, we do not know
how to build a human-level artificial intelligence (HLAI). We conjecture that
learning from others' experience with the language is the essential
characteristic that differentiates human intelligence from the rest. Humans can
update the action-value function only with the verbal description as if they
experience states, actions, and corresponding rewards sequences first hand. In
this paper, we present our ongoing effort to build an environment to facilitate
the research for models of this capability. In this environment, there are no
explicit definitions of tasks or rewards given when accomplishing those tasks.
Rather the models experience the experience of the human infants from fetus to
12 months. The agent should learn to speak the first words as a human child
does. We expect the environment will contribute to the research for HLAI.
</p>
<a href="http://arxiv.org/abs/2011.09410" target="_blank">arXiv:2011.09410</a> [<a href="http://arxiv.org/pdf/2011.09410" target="_blank">pdf</a>]

<h2>Convolutional Autoencoder for Blind Hyperspectral Image Unmixing. (arXiv:2011.09420v1 [cs.CV])</h2>
<h3>Yasiru Ranasinghe, Sanjaya Herath, Kavinga Weerasooriya, Mevan Ekanayake, Roshan Godaliyadda, Parakrama Ekanayake, Vijitha Herath</h3>
<p>In the remote sensing context spectral unmixing is a technique to decompose a
mixed pixel into two fundamental representatives: endmembers and abundances. In
this paper, a novel architecture is proposed to perform blind unmixing on
hyperspectral images. The proposed architecture consists of convolutional
layers followed by an autoencoder. The encoder transforms the feature space
produced through convolutional layers to a latent space representation. Then,
from these latent characteristics the decoder reconstructs the roll-out image
of the monochrome image which is at the input of the architecture; and each
single-band image is fed sequentially. Experimental results on real
hyperspectral data concludes that the proposed algorithm outperforms existing
unmixing methods at abundance estimation and generates competitive results for
endmember extraction with RMSE and SAD as the metrics, respectively.
</p>
<a href="http://arxiv.org/abs/2011.09420" target="_blank">arXiv:2011.09420</a> [<a href="http://arxiv.org/pdf/2011.09420" target="_blank">pdf</a>]

<h2>Understanding Variational Inference in Function-Space. (arXiv:2011.09421v1 [stat.ML])</h2>
<h3>David R. Burt, Sebastian W. Ober, Adri&#xe0; Garriga-Alonso, Mark van der Wilk</h3>
<p>Recent work has attempted to directly approximate the `function-space' or
predictive posterior distribution of Bayesian models, without approximating the
posterior distribution over the parameters. This is appealing in e.g. Bayesian
neural networks, where we only need the former, and the latter is hard to
represent. In this work, we highlight some advantages and limitations of
employing the Kullback-Leibler divergence in this setting. For example, we show
that minimizing the KL divergence between a wide class of parametric
distributions and the posterior induced by a (non-degenerate) Gaussian process
prior leads to an ill-defined objective function. Then, we propose (featurized)
Bayesian linear regression as a benchmark for `function-space' inference
methods that directly measures approximation quality. We apply this methodology
to assess aspects of the objective function and inference scheme considered in
Sun, Zhang, Shi, and Grosse (2018), emphasizing the quality of approximation to
Bayesian inference as opposed to predictive performance.
</p>
<a href="http://arxiv.org/abs/2011.09421" target="_blank">arXiv:2011.09421</a> [<a href="http://arxiv.org/pdf/2011.09421" target="_blank">pdf</a>]

<h2>A framework for the fine-grained evaluation of the instantaneous expected value of soccer possessions. (arXiv:2011.09426v1 [cs.LG])</h2>
<h3>Javier Fernandez (1 and 2), Luke Bornn (3), Daniel Cervone (4) ((1) Polytechnic University of Catalonia, (2) FC Barcelona, (3) Simon Fraser University, (4) Zelus Analytics)</h3>
<p>The expected possession value (EPV) of a soccer possession represents the
likelihood of a team scoring or receiving the next goal at any time instance.
By decomposing the EPV into a series of subcomponents that are estimated
separately, we develop a comprehensive analysis framework providing soccer
practitioners with the ability to evaluate the impact of both observed and
potential actions. We show we can obtain calibrated models for all the
components of EPV, including a set of yet-unexplored problems in soccer. We
produce visually-interpretable probability surfaces for potential passes from a
series of deep neural network architectures that learn from low-level
spatiotemporal data. Additionally, we present a series of novel practical
applications providing coaches with an enriched interpretation of specific game
situations.
</p>
<a href="http://arxiv.org/abs/2011.09426" target="_blank">arXiv:2011.09426</a> [<a href="http://arxiv.org/pdf/2011.09426" target="_blank">pdf</a>]

<h2>Fast Motion Understanding with Spatiotemporal Neural Networks and Dynamic Vision Sensors. (arXiv:2011.09427v1 [cs.CV])</h2>
<h3>Anthony Bisulco, Fernando Cladera Ojeda, Volkan Isler, Daniel D. Lee</h3>
<p>This paper presents a Dynamic Vision Sensor (DVS) based system for reasoning
about high speed motion. As a representative scenario, we consider the case of
a robot at rest reacting to a small, fast approaching object at speeds higher
than 15m/s. Since conventional image sensors at typical frame rates observe
such an object for only a few frames, estimating the underlying motion presents
a considerable challenge for standard computer vision systems and algorithms.
In this paper we present a method motivated by how animals such as insects
solve this problem with their relatively simple vision systems.

Our solution takes the event stream from a DVS and first encodes the temporal
events with a set of causal exponential filters across multiple time scales. We
couple these filters with a Convolutional Neural Network (CNN) to efficiently
extract relevant spatiotemporal features. The combined network learns to output
both the expected time to collision of the object, as well as the predicted
collision point on a discretized polar grid. These critical estimates are
computed with minimal delay by the network in order to react appropriately to
the incoming object. We highlight the results of our system to a toy dart
moving at 23.4m/s with a 24.73{\deg} error in ${\theta}$, 18.4mm average
discretized radius prediction error, and 25.03% median time to collision
prediction error.
</p>
<a href="http://arxiv.org/abs/2011.09427" target="_blank">arXiv:2011.09427</a> [<a href="http://arxiv.org/pdf/2011.09427" target="_blank">pdf</a>]

<h2>EWareNet: Emotion Aware Human Intent Prediction and Adaptive Spatial Profile Fusion for Social Robot Navigation. (arXiv:2011.09438v1 [cs.RO])</h2>
<h3>Venkatraman Narayanan, Bala Murali Manoghar, Rama Prashanth RV, Aniket Bera</h3>
<p>We present EWareNet, a novel intent-aware social robot navigation algorithm
among pedestrians. Our approach predicts the trajectory-based pedestrian intent
from historical gaits, which is then used for intent-guided navigation taking
into account social and proxemic constraints. To predict pedestrian intent, we
propose a transformer-based model that works on a commodity RGB-D camera
mounted onto a moving robot. Our intent prediction routine is integrated into a
mapless navigation scheme and makes no assumptions about the environment of
pedestrian motion. Our navigation scheme consists of a novel obstacle profile
representation methodology that is dynamically adjusted based on the pedestrian
pose, intent, and emotion. The navigation scheme is based on a reinforcement
learning algorithm that takes into consideration human intent and robot's
impact on human intent, in addition to the environmental configuration. We
outperform current state-of-art algorithms for intent prediction from 3D gaits.
</p>
<a href="http://arxiv.org/abs/2011.09438" target="_blank">arXiv:2011.09438</a> [<a href="http://arxiv.org/pdf/2011.09438" target="_blank">pdf</a>]

<h2>Cautious Bayesian Optimization for Efficient and Scalable Policy Search. (arXiv:2011.09445v1 [cs.RO])</h2>
<h3>Lukas P. Fr&#xf6;hlich, Melanie N. Zeilinger, Edgar D. Klenske</h3>
<p>Sample efficiency is one of the key factors when applying policy search to
real-world problems. In recent years, Bayesian Optimization (BO) has become
prominent in the field of robotics due to its sample efficiency and little
prior knowledge needed. However, one drawback of BO is its poor performance on
high-dimensional search spaces as it focuses on global search. In the policy
search setting, local optimization is typically sufficient as initial policies
are often available, e.g., via meta-learning, kinesthetic demonstrations or
sim-to-real approaches. In this paper, we propose to constrain the policy
search space to a sublevel-set of the Bayesian surrogate model's predictive
uncertainty. This simple yet effective way of constraining the policy update
enables BO to scale to high-dimensional spaces (&gt;100) as well as reduces the
risk of damaging the system. We demonstrate the effectiveness of our approach
on a wide range of problems, including a motor skills task, adapting deep RL
agents to new reward signals and a sim-to-real task for an inverted pendulum
system.
</p>
<a href="http://arxiv.org/abs/2011.09445" target="_blank">arXiv:2011.09445</a> [<a href="http://arxiv.org/pdf/2011.09445" target="_blank">pdf</a>]

<h2>Investigation of Warrior Robots Behavior by Using Evolutionary Algorithms. (arXiv:2011.09455v1 [cs.RO])</h2>
<h3>Shahriar Sharifi Borojerdi, Mehdi Karimi, Ehsan Amiri</h3>
<p>In this study, we review robots behavior especially warrior robots by using
evolutionary algorithms. This kind of algorithms is inspired by nature that
causes robots behaviors get resemble to collective behavior. Collective
behavior of creatures such as bees was shown that do some functions which
depended on interaction and cooperation would need to a well-organized system
so that all creatures within it carry out their duty, very well. For robots
which do not have any intelligence, we can define an algorithm and show the
results by a simple simulation.
</p>
<a href="http://arxiv.org/abs/2011.09455" target="_blank">arXiv:2011.09455</a> [<a href="http://arxiv.org/pdf/2011.09455" target="_blank">pdf</a>]

<h2>Counterfactual Credit Assignment in Model-Free Reinforcement Learning. (arXiv:2011.09464v1 [cs.LG])</h2>
<h3>Thomas Mesnard, Th&#xe9;ophane Weber, Fabio Viola, Shantanu Thakoor, Alaa Saade, Anna Harutyunyan, Will Dabney, Tom Stepleton, Nicolas Heess, Arthur Guez, Marcus Hutter, Lars Buesing, R&#xe9;mi Munos</h3>
<p>Credit assignment in reinforcement learning is the problem of measuring an
action influence on future rewards. In particular, this requires separating
skill from luck, ie. disentangling the effect of an action on rewards from that
of external factors and subsequent actions. To achieve this, we adapt the
notion of counterfactuals from causality theory to a model-free RL setup. The
key idea is to condition value functions on future events, by learning to
extract relevant information from a trajectory. We then propose to use these as
future-conditional baselines and critics in policy gradient algorithms and we
develop a valid, practical variant with provably lower variance, while
achieving unbiasedness by constraining the hindsight information not to contain
information about the agent actions. We demonstrate the efficacy and validity
of our algorithm on a number of illustrative problems.
</p>
<a href="http://arxiv.org/abs/2011.09464" target="_blank">arXiv:2011.09464</a> [<a href="http://arxiv.org/pdf/2011.09464" target="_blank">pdf</a>]

<h2>Detecting Hierarchical Changes in Latent Variable Models. (arXiv:2011.09465v1 [stat.ML])</h2>
<h3>Shintaro Fukushima, Kenji Yamanishi</h3>
<p>This paper addresses the issue of detecting hierarchical changes in latent
variable models (HCDL) from data streams. There are three different levels of
changes for latent variable models: 1) the first level is the change in data
distribution for fixed latent variables, 2) the second one is that in the
distribution over latent variables, and 3) the third one is that in the number
of latent variables. It is important to detect these changes because we can
analyze the causes of changes by identifying which level a change comes from
(change interpretability). This paper proposes an information-theoretic
framework for detecting changes of the three levels in a hierarchical way. The
key idea to realize it is to employ the MDL (minimum description length) change
statistics for measuring the degree of change, in combination with DNML
(decomposed normalized maximum likelihood) code-length calculation. We give a
theoretical basis for making reliable alarms for changes. Focusing on
stochastic block models, we employ synthetic and benchmark datasets to
empirically demonstrate the effectiveness of our framework in terms of change
interpretability as well as change detection.
</p>
<a href="http://arxiv.org/abs/2011.09465" target="_blank">arXiv:2011.09465</a> [<a href="http://arxiv.org/pdf/2011.09465" target="_blank">pdf</a>]

<h2>Gradient Starvation: A Learning Proclivity in Neural Networks. (arXiv:2011.09468v1 [cs.LG])</h2>
<h3>Mohammad Pezeshki, S&#xe9;kou-Oumar Kaba, Yoshua Bengio, Aaron Courville, Doina Precup, Guillaume Lajoie</h3>
<p>We identify and formalize a fundamental gradient descent phenomenon resulting
in a learning proclivity in over-parameterized neural networks. Gradient
Starvation arises when cross-entropy loss is minimized by capturing only a
subset of features relevant for the task, despite the presence of other
predictive features that fail to be discovered. This work provides a
theoretical explanation for the emergence of such feature imbalance in neural
networks. Using tools from Dynamical Systems theory, we identify simple
properties of learning dynamics during gradient descent that lead to this
imbalance, and prove that such a situation can be expected given certain
statistical structure in training data. Based on our proposed formalism, we
develop guarantees for a novel regularization method aimed at decoupling
feature learning dynamics, improving accuracy and robustness in cases hindered
by gradient starvation. We illustrate our findings with simple and real-world
out-of-distribution (OOD) generalization experiments.
</p>
<a href="http://arxiv.org/abs/2011.09468" target="_blank">arXiv:2011.09468</a> [<a href="http://arxiv.org/pdf/2011.09468" target="_blank">pdf</a>]

<h2>FROST: Faster and more Robust One-shot Semi-supervised Training. (arXiv:2011.09471v1 [cs.LG])</h2>
<h3>Helena E. Liu, Leslie N. Smith</h3>
<p>Recent advances in one-shot semi-supervised learning have lowered the barrier
for deep learning of new applications. However, the state-of-the-art for
semi-supervised learning is slow to train and the performance is sensitive to
the choices of the labeled data and hyper-parameter values. In this paper, we
present a one-shot semi-supervised learning method that trains up to an order
of magnitude faster and is more robust than state-of-the-art methods.
Specifically, we show that by combining semi-supervised learning with a
one-stage, single network version of self-training, our FROST methodology
trains faster and is more robust to choices for the labeled samples and changes
in hyper-parameters. Our experiments demonstrate FROST's capability to perform
well when the composition of the unlabeled data is unknown; that is when the
unlabeled data contain unequal numbers of each class and can contain
out-of-distribution examples that don't belong to any of the training classes.
High performance, speed of training, and insensitivity to hyper-parameters make
FROST the most practical method for one-shot semi-supervised training.
</p>
<a href="http://arxiv.org/abs/2011.09471" target="_blank">arXiv:2011.09471</a> [<a href="http://arxiv.org/pdf/2011.09471" target="_blank">pdf</a>]

<h2>Adversarial collision attacks on image hashing functions. (arXiv:2011.09473v1 [cs.CV])</h2>
<h3>Brian Dolhansky, Cristian Canton Ferrer</h3>
<p>Hashing images with a perceptual algorithm is a common approach to solving
duplicate image detection problems. However, perceptual image hashing
algorithms are differentiable, and are thus vulnerable to gradient-based
adversarial attacks. We demonstrate that not only is it possible to modify an
image to produce an unrelated hash, but an exact image hash collision between a
source and target image can be produced via minuscule adversarial
perturbations. In a white box setting, these collisions can be replicated
across nearly every image pair and hash type (including both deep and
non-learned hashes). Furthermore, by attacking points other than the output of
a hashing function, an attacker can avoid having to know the details of a
particular algorithm, resulting in collisions that transfer across different
hash sizes or model architectures. Using these techniques, an adversary can
poison the image lookup table of a duplicate image detection service, resulting
in undefined or unwanted behavior. Finally, we offer several potential
mitigations to gradient-based image hash attacks.
</p>
<a href="http://arxiv.org/abs/2011.09473" target="_blank">arXiv:2011.09473</a> [<a href="http://arxiv.org/pdf/2011.09473" target="_blank">pdf</a>]

<h2>Efficient Full-Matrix Adaptive Regularization. (arXiv:1806.02958v2 [cs.LG] UPDATED)</h2>
<h3>Naman Agarwal, Brian Bullins, Xinyi Chen, Elad Hazan, Karan Singh, Cyril Zhang, Yi Zhang</h3>
<p>Adaptive regularization methods pre-multiply a descent direction by a
preconditioning matrix. Due to the large number of parameters of machine
learning problems, full-matrix preconditioning methods are prohibitively
expensive. We show how to modify full-matrix adaptive regularization in order
to make it practical and effective. We also provide a novel theoretical
analysis for adaptive regularization in non-convex optimization settings. The
core of our algorithm, termed GGT, consists of the efficient computation of the
inverse square root of a low-rank matrix. Our preliminary experiments show
improved iteration-wise convergence rates across synthetic tasks and standard
deep learning benchmarks, and that the more carefully-preconditioned steps
sometimes lead to a better solution.
</p>
<a href="http://arxiv.org/abs/1806.02958" target="_blank">arXiv:1806.02958</a> [<a href="http://arxiv.org/pdf/1806.02958" target="_blank">pdf</a>]

<h2>A Survey of Inverse Reinforcement Learning: Challenges, Methods and Progress. (arXiv:1806.06877v3 [cs.LG] UPDATED)</h2>
<h3>Saurabh Arora, Prashant Doshi</h3>
<p>Inverse reinforcement learning (IRL) is the problem of inferring the reward
function of an agent, given its policy or observed behavior. Analogous to RL,
IRL is perceived both as a problem and as a class of methods. By categorically
surveying the current literature in IRL, this article serves as a reference for
researchers and practitioners of machine learning and beyond to understand the
challenges of IRL and select the approaches best suited for the problem on
hand. The survey formally introduces the IRL problem along with its central
challenges such as the difficulty in performing accurate inference and its
generalizability, its sensitivity to prior knowledge, and the disproportionate
growth in solution complexity with problem size. The article elaborates how the
current methods mitigate these challenges. We further discuss the extensions to
traditional IRL methods for handling: inaccurate and incomplete perception, an
incomplete model, multiple reward functions, and nonlinear reward functions.
This survey concludes the discussion with some broad advances in the research
area and currently open research questions.
</p>
<a href="http://arxiv.org/abs/1806.06877" target="_blank">arXiv:1806.06877</a> [<a href="http://arxiv.org/pdf/1806.06877" target="_blank">pdf</a>]

<h2>Go-Explore: a New Approach for Hard-Exploration Problems. (arXiv:1901.10995v3 [cs.LG] UPDATED)</h2>
<h3>Adrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O. Stanley, Jeff Clune</h3>
<p>A grand challenge in reinforcement learning is intelligent exploration,
especially when rewards are sparse or deceptive. Two Atari games serve as
benchmarks for such hard-exploration domains: Montezuma's Revenge and Pitfall.
On both games, current RL algorithms perform poorly, even those with intrinsic
motivation, which is the dominant method to improve performance on
hard-exploration domains. To address this shortfall, we introduce a new
algorithm called Go-Explore. It exploits the following principles: (1) remember
previously visited states, (2) first return to a promising state (without
exploration), then explore from it, and (3) solve simulated environments
through any available means (including by introducing determinism), then
robustify via imitation learning. The combined effect of these principles is a
dramatic performance improvement on hard-exploration problems. On Montezuma's
Revenge, Go-Explore scores a mean of over 43k points, almost 4 times the
previous state of the art. Go-Explore can also harness human-provided domain
knowledge and, when augmented with it, scores a mean of over 650k points on
Montezuma's Revenge. Its max performance of nearly 18 million surpasses the
human world record, meeting even the strictest definition of "superhuman"
performance. On Pitfall, Go-Explore with domain knowledge is the first
algorithm to score above zero. Its mean score of almost 60k points exceeds
expert human performance. Because Go-Explore produces high-performing
demonstrations automatically and cheaply, it also outperforms imitation
learning work where humans provide solution demonstrations. Go-Explore opens up
many new research directions into improving it and weaving its insights into
current RL algorithms. It may also enable progress on previously unsolvable
hard-exploration problems in many domains, especially those that harness a
simulator during training (e.g. robotics).
</p>
<a href="http://arxiv.org/abs/1901.10995" target="_blank">arXiv:1901.10995</a> [<a href="http://arxiv.org/pdf/1901.10995" target="_blank">pdf</a>]

<h2>KTBoost: Combined Kernel and Tree Boosting. (arXiv:1902.03999v4 [cs.LG] UPDATED)</h2>
<h3>Fabio Sigrist</h3>
<p>We introduce a novel boosting algorithm called `KTBoost' which combines
kernel boosting and tree boosting. In each boosting iteration, the algorithm
adds either a regression tree or reproducing kernel Hilbert space (RKHS)
regression function to the ensemble of base learners. Intuitively, the idea is
that discontinuous trees and continuous RKHS regression functions complement
each other, and that this combination allows for better learning of functions
that have parts with varying degrees of regularity such as discontinuities and
smooth parts. We empirically show that KTBoost significantly outperforms both
tree and kernel boosting in terms of predictive accuracy in a comparison on a
wide array of data sets.
</p>
<a href="http://arxiv.org/abs/1902.03999" target="_blank">arXiv:1902.03999</a> [<a href="http://arxiv.org/pdf/1902.03999" target="_blank">pdf</a>]

<h2>Robust learning with implicit residual networks. (arXiv:1905.10479v3 [cs.LG] UPDATED)</h2>
<h3>Viktor Reshniak, Clayton Webster</h3>
<p>In this effort, we propose a new deep architecture utilizing residual blocks
inspired by implicit discretization schemes. As opposed to the standard
feed-forward networks, the outputs of the proposed implicit residual blocks are
defined as the fixed points of the appropriately chosen nonlinear
transformations. We show that this choice leads to the improved stability of
both forward and backward propagations, has a favorable impact on the
generalization power and allows to control the robustness of the network with
only a few hyperparameters. In addition, the proposed reformulation of ResNet
does not introduce new parameters and can potentially lead to a reduction in
the number of required layers due to improved forward stability. Finally, we
derive the memory-efficient training algorithm, propose a stochastic
regularization technique and provide numerical results in support of our
findings.
</p>
<a href="http://arxiv.org/abs/1905.10479" target="_blank">arXiv:1905.10479</a> [<a href="http://arxiv.org/pdf/1905.10479" target="_blank">pdf</a>]

<h2>Learning Directed Graphical Models from Gaussian Data. (arXiv:1906.08050v3 [cs.LG] UPDATED)</h2>
<h3>Katherine Fitch</h3>
<p>In this paper, we introduce a new directed graphical model from Gaussian
data: the Gaussian graphical interaction model (GGIM). The development of this
model comes from considering stationary Gaussian processes on graphs, and
leveraging the equations between the resulting steady-state covariance matrix
and the Laplacian matrix representing the interaction graph. Through the
presentation of conceptually straightforward theory, we develop the new model
and provide interpretations of the edges in the graphical model in terms of
statistical measures. We show that when restricted to undirected graphs, the
Laplacian matrix representing a GGIM is equivalent to the standard inverse
covariance matrix that encodes conditional dependence relationships.
Furthermore, our approach leads to a natural definition of directed conditional
independence of two elements in a stationary Gaussian process. We demonstrate
that the problem of learning sparse GGIMs for a given observation set can be
framed as a LASSO problem. By comparison with the problem of inverse covariance
estimation, we prove a bound on the difference between the covariance matrix
corresponding to a sparse GGIM and the covariance matrix corresponding to the
$l_1$-norm penalized maximum log-likelihood estimate. Finally, we consider the
problem of learning GGIMs associated with sparse directed conditional
dependence relationships. In all, the new model presents a novel perspective on
directed relationships between variables and significantly expands on the state
of the art in Gaussian graphical modeling.
</p>
<a href="http://arxiv.org/abs/1906.08050" target="_blank">arXiv:1906.08050</a> [<a href="http://arxiv.org/pdf/1906.08050" target="_blank">pdf</a>]

<h2>Stochastic Gradient and Langevin Processes. (arXiv:1907.03215v6 [cs.LG] UPDATED)</h2>
<h3>Xiang Cheng, Dong Yin, Peter L. Bartlett, Michael I. Jordan</h3>
<p>We prove quantitative convergence rates at which discrete Langevin-like
processes converge to the invariant distribution of a related stochastic
differential equation. We study the setup where the additive noise can be
non-Gaussian and state-dependent and the potential function can be non-convex.
We show that the key properties of these processes depend on the potential
function and the second moment of the additive noise. We apply our theoretical
findings to studying the convergence of Stochastic Gradient Descent (SGD) for
non-convex problems and corroborate them with experiments using SGD to train
deep neural networks on the CIFAR-10 dataset.
</p>
<a href="http://arxiv.org/abs/1907.03215" target="_blank">arXiv:1907.03215</a> [<a href="http://arxiv.org/pdf/1907.03215" target="_blank">pdf</a>]

<h2>Deep K-SVD Denoising. (arXiv:1909.13164v2 [cs.LG] UPDATED)</h2>
<h3>Meyer Scetbon, Michael Elad, Peyman Milanfar</h3>
<p>This work considers noise removal from images, focusing on the well known
K-SVD denoising algorithm. This sparsity-based method was proposed in 2006, and
for a short while it was considered as state-of-the-art. However, over the
years it has been surpassed by other methods, including the recent
deep-learning-based newcomers. The question we address in this paper is whether
K-SVD was brought to its peak in its original conception, or whether it can be
made competitive again. The approach we take in answering this question is to
redesign the algorithm to operate in a supervised manner. More specifically, we
propose an end-to-end deep architecture with the exact K-SVD computational
path, and train it for optimized denoising. Our work shows how to overcome
difficulties arising in turning the K-SVD scheme into a differentiable, and
thus learnable, machine. With a small number of parameters to learn and while
preserving the original K-SVD essence, the proposed architecture is shown to
outperform the classical K-SVD algorithm substantially, and getting closer to
recent state-of-the-art learning-based denoising methods. Adopting a broader
context, this work touches on themes around the design of deep-learning
solutions for image processing tasks, while paving a bridge between classic
methods and novel deep-learning-based ones.
</p>
<a href="http://arxiv.org/abs/1909.13164" target="_blank">arXiv:1909.13164</a> [<a href="http://arxiv.org/pdf/1909.13164" target="_blank">pdf</a>]

<h2>The impact of catastrophic collisions and collision avoidance on a swarming behavior. (arXiv:1910.06412v4 [cs.RO] UPDATED)</h2>
<h3>Chris Taylor, Cameron Nowzari</h3>
<p>Swarms of autonomous agents are useful in many applications due to their
ability to accomplish tasks in a decentralized manner, making them more robust
to failures. Due to the difficulty in running experiments with large numbers of
hardware agents, researchers often make simplifying assumptions and remove
constraints that might be present in a real swarm deployment. While simplifying
away some constraints is tolerable, we feel that two in particular have been
overlooked: one, that agents in a swarm take up physical space, and two, that
agents might be damaged in collisions. Many existing works assume agents have
negligible size or pass through each other with no added penalty. It seems
possible to ignore these constraints using collision avoidance, but we show
using an illustrative example that this is easier said than done. In
particular, we show that collision avoidance can interfere with the intended
swarming behavior and significant parameter tuning is necessary to ensure the
behavior emerges as best as possible while collisions are avoided. We compare
four different collision avoidance algorithms, two of which we consider to be
the best decentralized collision avoidance algorithms available. Despite
putting significant effort into tuning each algorithm to perform at its best,
we believe our results show that further research is necessary to develop
swarming behaviors that can achieve their goal while avoiding collisions with
agents of non-negligible volume.
</p>
<a href="http://arxiv.org/abs/1910.06412" target="_blank">arXiv:1910.06412</a> [<a href="http://arxiv.org/pdf/1910.06412" target="_blank">pdf</a>]

<h2>gradSLAM: Dense SLAM meets Automatic Differentiation. (arXiv:1910.10672v2 [cs.RO] UPDATED)</h2>
<h3>Krishna Murthy Jatavallabhula, Soroush Saryazdi, Ganesh Iyer, Liam Paull</h3>
<p>Blending representation learning approaches with simultaneous localization
and mapping (SLAM) systems is an open question, because of their highly modular
and complex nature. Functionally, SLAM is an operation that transforms raw
sensor inputs into a distribution over the state(s) of the robot and the
environment. If this transformation (SLAM) were expressible as a differentiable
function, we could leverage task-based error signals to learn representations
that optimize task performance. However, several components of a typical dense
SLAM system are non-differentiable. In this work, we propose gradSLAM, a
methodology for posing SLAM systems as differentiable computational graphs,
which unifies gradient-based learning and SLAM. We propose differentiable
trust-region optimizers, surface measurement and fusion schemes, and
raycasting, without sacrificing accuracy. This amalgamation of dense SLAM with
computational graphs enables us to backprop all the way from 3D maps to 2D
pixels, opening up new possibilities in gradient-based learning for SLAM.

TL;DR: We leverage the power of automatic differentiation frameworks to make
dense SLAM differentiable.
</p>
<a href="http://arxiv.org/abs/1910.10672" target="_blank">arXiv:1910.10672</a> [<a href="http://arxiv.org/pdf/1910.10672" target="_blank">pdf</a>]

<h2>Crop Height and Plot Estimation for Phenotyping from Unmanned Aerial Vehicles using 3D LiDAR. (arXiv:1910.14031v3 [cs.RO] UPDATED)</h2>
<h3>Harnaik Dhami, Kevin Yu, Tianshu Xu, Qian Zhu, Kshitiz Dhakal, James Friel, Song Li, Pratap Tokekar</h3>
<p>We present techniques to measure crop heights using a 3D Light Detection and
Ranging (LiDAR) sensor mounted on an Unmanned Aerial Vehicle (UAV). Knowing the
height of plants is crucial to monitor their overall health and growth cycles,
especially for high-throughput plant phenotyping. We present a methodology for
extracting plant heights from 3D LiDAR point clouds, specifically focusing on
plot-based phenotyping environments. We also present a toolchain that can be
used to create phenotyping farms for use in Gazebo simulations. The tool
creates a randomized farm with realistic 3D plant and terrain models. We
conducted a series of simulations and hardware experiments in controlled and
natural settings. Our algorithm was able to estimate the plant heights in a
field with 112 plots with a root mean square error (RMSE) of 6.1 cm. This is
the first such dataset for 3D LiDAR from an airborne robot over a wheat field.
The developed simulation toolchain, algorithmic implementation, and datasets
can be found on the GitHub repository located at
https://github.com/hsd1121/PointCloudProcessing.
</p>
<a href="http://arxiv.org/abs/1910.14031" target="_blank">arXiv:1910.14031</a> [<a href="http://arxiv.org/pdf/1910.14031" target="_blank">pdf</a>]

<h2>Self-Supervised 3D Keypoint Learning for Ego-motion Estimation. (arXiv:1912.03426v3 [cs.CV] UPDATED)</h2>
<h3>Jiexiong Tang, Rares Ambrus, Vitor Guizilini, Sudeep Pillai, Hanme Kim, Patric Jensfelt, Adrien Gaidon</h3>
<p>Detecting and matching robust viewpoint-invariant keypoints is critical for
visual SLAM and Structure-from-Motion. State-of-the-art learning-based methods
generate training samples via homography adaptation to create 2D synthetic
views with known keypoint matches from a single image. This approach, however,
does not generalize to non-planar 3D scenes with illumination variations
commonly seen in real-world videos. In this work, we propose self-supervised
learning of depth-aware keypoints directly from unlabeled videos. We jointly
learn keypoint and depth estimation networks by combining appearance and
geometric matching via a differentiable structure-from-motion module based on
Procrustean residual pose correction. We describe how our self-supervised
keypoints can be integrated into state-of-the-art visual odometry frameworks
for robust and accurate ego-motion estimation of autonomous vehicles in
real-world conditions.
</p>
<a href="http://arxiv.org/abs/1912.03426" target="_blank">arXiv:1912.03426</a> [<a href="http://arxiv.org/pdf/1912.03426" target="_blank">pdf</a>]

<h2>DASZL: Dynamic Action Signatures for Zero-shot Learning. (arXiv:1912.03613v3 [cs.CV] UPDATED)</h2>
<h3>Tae Soo Kim, Jonathan D. Jones, Michael Peven, Zihao Xiao, Jin Bai, Yi Zhang, Weichao Qiu, Alan Yuille, Gregory D. Hager</h3>
<p>There are many realistic applications of activity recognition where the set
of potential activity descriptions is combinatorially large. This makes
end-to-end supervised training of a recognition system impractical as no
training set is practically able to encompass the entire label set. In this
paper, we present an approach to fine-grained recognition that models
activities as compositions of dynamic action signatures. This compositional
approach allows us to reframe fine-grained recognition as zero-shot activity
recognition, where a detector is composed "on the fly" from simple
first-principles state machines supported by deep-learned components. We
evaluate our method on the Olympic Sports and UCF101 datasets, where our model
establishes a new state of the art under multiple experimental paradigms. We
also extend this method to form a unique framework for zero-shot joint
segmentation and classification of activities in video and demonstrate the
first results in zero-shot decoding of complex action sequences on a
widely-used surgical dataset. Lastly, we show that we can use off-the-shelf
object detectors to recognize activities in completely de-novo settings with no
additional training.
</p>
<a href="http://arxiv.org/abs/1912.03613" target="_blank">arXiv:1912.03613</a> [<a href="http://arxiv.org/pdf/1912.03613" target="_blank">pdf</a>]

<h2>Weakly Supervised Instance Segmentation by Deep Community Learning. (arXiv:2001.11207v3 [cs.CV] UPDATED)</h2>
<h3>Jaedong Hwang, Seohyun Kim, Jeany Son, Bohyung Han</h3>
<p>We present a weakly supervised instance segmentation algorithm based on deep
community learning with multiple tasks. This task is formulated as a
combination of weakly supervised object detection and semantic segmentation,
where individual objects of the same class are identified and segmented
separately. We address this problem by designing a unified deep neural network
architecture, which has a positive feedback loop of object detection with
bounding box regression, instance mask generation, instance segmentation, and
feature extraction. Each component of the network makes active interactions
with others to improve accuracy, and the end-to-end trainability of our model
makes our results more robust and reproducible. The proposed algorithm achieves
state-of-the-art performance in the weakly supervised setting without any
additional training such as Fast R-CNN and Mask R-CNN on the standard benchmark
dataset. The implementation of our algorithm is available on the project
webpage: https://cv.snu.ac.kr/research/WSIS_CL.
</p>
<a href="http://arxiv.org/abs/2001.11207" target="_blank">arXiv:2001.11207</a> [<a href="http://arxiv.org/pdf/2001.11207" target="_blank">pdf</a>]

<h2>Overfitting Can Be Harmless for Basis Pursuit, But Only to a Degree. (arXiv:2002.00492v2 [cs.LG] UPDATED)</h2>
<h3>Peizhong Ju, Xiaojun Lin, Jia Liu</h3>
<p>Recently, there have been significant interests in studying the so-called
"double-descent" of the generalization error of linear regression models under
the overparameterized and overfitting regime, with the hope that such analysis
may provide the first step towards understanding why overparameterized deep
neural networks (DNN) still generalize well. However, to date most of these
studies focused on the min $\ell_2$-norm solution that overfits the data. In
contrast, in this paper we study the overfitting solution that minimizes the
$\ell_1$-norm, which is known as Basis Pursuit (BP) in the compressed sensing
literature. Under a sparse true linear regression model with $p$ i.i.d.
Gaussian features, we show that for a large range of $p$ up to a limit that
grows exponentially with the number of samples $n$, with high probability the
model error of BP is upper bounded by a value that decreases with $p$. To the
best of our knowledge, this is the first analytical result in the literature
establishing the double-descent of overfitting BP for finite $n$ and $p$.
Further, our results reveal significant differences between the double-descent
of BP and min $\ell_2$-norm solutions. Specifically, the double-descent
upper-bound of BP is independent of the signal strength, and for high SNR and
sparse models the descent-floor of BP can be much lower and wider than that of
min $\ell_2$-norm solutions.
</p>
<a href="http://arxiv.org/abs/2002.00492" target="_blank">arXiv:2002.00492</a> [<a href="http://arxiv.org/pdf/2002.00492" target="_blank">pdf</a>]

<h2>Weight mechanism: adding a constant in concatenation of series connect. (arXiv:2003.03500v2 [cs.CV] UPDATED)</h2>
<h3>Xiaojie Qi</h3>
<p>It is a consensus that feature maps in the shallow layer are more related to
image attributes such as texture and shape, whereas abstract semantic
representation exists in the deep layer. Meanwhile, some image information will
be lost in the process of the convolution operation. Naturally, the direct
method is combining them together to gain lost detailed information through
concatenation or adding. In fact, the image representation flowed in feature
fusion can not match with the semantic representation completely, and the
semantic deviation in different layers also destroy the information
purification, that leads to useless information being mixed into the fusion
layers. Therefore, it is crucial to narrow the gap among the fused layers and
reduce the impact of noises during fusion. In this paper, we propose a method
named weight mechanism to reduce the gap between feature maps in concatenation
of series connection, and we get a better result of 0.80% mIoU improvement on
Massachusetts building dataset by changing the weight of the concatenation of
series connection in residual U-Net. Specifically, we design a new architecture
named fused U-Net to test weight mechanism, and it also gains 0.12% mIoU
improvement.
</p>
<a href="http://arxiv.org/abs/2003.03500" target="_blank">arXiv:2003.03500</a> [<a href="http://arxiv.org/pdf/2003.03500" target="_blank">pdf</a>]

<h2>Restore from Restored: Video Restoration with Pseudo Clean Video. (arXiv:2003.04279v2 [cs.CV] UPDATED)</h2>
<h3>Seunghwan Lee, Donghyeon Cho, Jiwon Kim, Tae Hyun Kim</h3>
<p>In this study, we propose a self-supervised video denoising method called
"restore-from-restored." This method fine-tunes a pre-trained network by using
a pseudo clean video during the test phase. The pseudo clean video is obtained
by applying a noisy video to the baseline network. By adopting a fully
convolutional neural network (FCN) as the baseline, we can improve video
denoising performance without accurate optical flow estimation and registration
steps, in contrast to many conventional video restoration methods, due to the
translation equivariant property of the FCN. Specifically, the proposed method
can take advantage of plentiful similar patches existing across multiple
consecutive frames (i.e., patch-recurrence); these patches can boost the
performance of the baseline network by a large margin. We analyze the
restoration performance of the fine-tuned video denoising networks with the
proposed self-supervision-based learning algorithm, and demonstrate that the
FCN can utilize recurring patches without requiring accurate registration among
adjacent frames. In our experiments, we apply the proposed method to
state-of-the-art denoisers and show that our fine-tuned networks achieve a
considerable improvement in denoising performance.
</p>
<a href="http://arxiv.org/abs/2003.04279" target="_blank">arXiv:2003.04279</a> [<a href="http://arxiv.org/pdf/2003.04279" target="_blank">pdf</a>]

<h2>Restore from Restored: Single Image Denoising with Pseudo Clean Image. (arXiv:2003.04721v3 [cs.CV] UPDATED)</h2>
<h3>Seunghwan Lee, Dongkyu Lee, Donghyeon Cho, Jiwon Kim, Tae Hyun Kim</h3>
<p>In this study, we propose a simple and effective fine-tuning algorithm called
"restore-from-restored", which can greatly enhance the performance of fully
pre-trained image denoising networks. Many supervised denoising approaches can
produce satisfactory results using large external training datasets. However,
these methods have limitations in using internal information available in a
given test image. By contrast, recent self-supervised approaches can remove
noise in the input image by utilizing information from the specific test input.
However, such methods show relatively lower performance on known noise types
such as Gaussian noise compared to supervised methods. Thus, to combine
external and internal information, we fine-tune the fully pre-trained denoiser
using pseudo training set at test time. By exploiting internal self-similar
patches (i.e., patch-recurrence), the baseline network can be adapted to the
given specific input image. We demonstrate that our method can be easily
employed on top of the state-of-the-art denoising networks and further improve
the performance on numerous denoising benchmark datasets including real noisy
images.
</p>
<a href="http://arxiv.org/abs/2003.04721" target="_blank">arXiv:2003.04721</a> [<a href="http://arxiv.org/pdf/2003.04721" target="_blank">pdf</a>]

<h2>Fixing the train-test resolution discrepancy: FixEfficientNet. (arXiv:2003.08237v5 [cs.CV] UPDATED)</h2>
<h3>Hugo Touvron, Andrea Vedaldi, Matthijs Douze, Herv&#xe9; J&#xe9;gou</h3>
<p>This paper provides an extensive analysis of the performance of the
EfficientNet image classifiers with several recent training procedures, in
particular one that corrects the discrepancy between train and test images. The
resulting network, called FixEfficientNet, significantly outperforms the
initial architecture with the same number of parameters.

For instance, our FixEfficientNet-B0 trained without additional training data
achieves 79.3% top-1 accuracy on ImageNet with 5.3M parameters. This is a +0.5%
absolute improvement over the Noisy student EfficientNet-B0 trained with 300M
unlabeled images. An EfficientNet-L2 pre-trained with weak supervision on 300M
unlabeled images and further optimized with FixRes achieves 88.5% top-1
accuracy (top-5: 98.7%), which establishes the new state of the art for
ImageNet with a single crop.

These improvements are thoroughly evaluated with cleaner protocols than the
one usually employed for Imagenet, and particular we show that our improvement
remains in the experimental setting of ImageNet-v2, that is less prone to
overfitting, and with ImageNet Real Labels. In both cases we also establish the
new state of the art.
</p>
<a href="http://arxiv.org/abs/2003.08237" target="_blank">arXiv:2003.08237</a> [<a href="http://arxiv.org/pdf/2003.08237" target="_blank">pdf</a>]

<h2>Weakly-Supervised Reinforcement Learning for Controllable Behavior. (arXiv:2004.02860v2 [cs.LG] UPDATED)</h2>
<h3>Lisa Lee, Benjamin Eysenbach, Ruslan Salakhutdinov, Shixiang Shane Gu, Chelsea Finn</h3>
<p>Reinforcement learning (RL) is a powerful framework for learning to take
actions to solve tasks. However, in many settings, an agent must winnow down
the inconceivably large space of all possible tasks to the single task that it
is currently being asked to solve. Can we instead constrain the space of tasks
to those that are semantically meaningful? In this work, we introduce a
framework for using weak supervision to automatically disentangle this
semantically meaningful subspace of tasks from the enormous space of
nonsensical "chaff" tasks. We show that this learned subspace enables efficient
exploration and provides a representation that captures distance between
states. On a variety of challenging, vision-based continuous control problems,
our approach leads to substantial performance gains, particularly as the
complexity of the environment grows.
</p>
<a href="http://arxiv.org/abs/2004.02860" target="_blank">arXiv:2004.02860</a> [<a href="http://arxiv.org/pdf/2004.02860" target="_blank">pdf</a>]

<h2>Capsule Networks -- A Probabilistic Perspective. (arXiv:2004.03553v2 [cs.LG] UPDATED)</h2>
<h3>Lewis Smith, Lisa Schut, Yarin Gal, Mark van der Wilk</h3>
<p>'Capsule' models try to explicitly represent the poses of objects, enforcing
a linear relationship between an object's pose and that of its constituent
parts. This modelling assumption should lead to robustness to viewpoint changes
since the sub-object/super-object relationships are invariant to the poses of
the object. We describe a probabilistic generative model which encodes such
capsule assumptions, clearly separating the generative parts of the model from
the inference mechanisms. With a variational bound we explore the properties of
the generative model independently of the approximate inference scheme, and
gain insights into failures of the capsule assumptions and inference
amortisation. We experimentally demonstrate the applicability of our unified
objective, and demonstrate the use of test time optimisation to solve problems
inherent to amortised inference in our model.
</p>
<a href="http://arxiv.org/abs/2004.03553" target="_blank">arXiv:2004.03553</a> [<a href="http://arxiv.org/pdf/2004.03553" target="_blank">pdf</a>]

<h2>Design and Control of Roller Grasper V2 for In-Hand Manipulation. (arXiv:2004.08499v2 [cs.RO] UPDATED)</h2>
<h3>Shenli Yuan, Lin Shao, Connor L. Yako, Alex Gruebele, J. Kenneth Salisbury</h3>
<p>The ability to perform in-hand manipulation still remains an unsolved
problem; having this capability would allow robots to perform sophisticated
tasks requiring repositioning and reorienting of grasped objects. In this work,
we present a novel non-anthropomorphic robot grasper with the ability to
manipulate objects by means of active surfaces at the fingertips. Active
surfaces are achieved by spherical rolling fingertips with two degrees of
freedom (DoF) -- a pivoting motion for surface reorientation -- and a
continuous rolling motion for moving the object. A further DoF is in the base
of each finger, allowing the fingers to grasp objects over a range of size and
shapes. Instantaneous kinematics was derived and objects were successfully
manipulated both with a custom handcrafted control scheme as well as one
learned through imitation learning, in simulation and experimentally on the
hardware.
</p>
<a href="http://arxiv.org/abs/2004.08499" target="_blank">arXiv:2004.08499</a> [<a href="http://arxiv.org/pdf/2004.08499" target="_blank">pdf</a>]

<h2>Visuo-Linguistic Question Answering (VLQA) Challenge. (arXiv:2005.00330v3 [cs.CV] UPDATED)</h2>
<h3>Shailaja Keyur Sampat, Yezhou Yang, Chitta Baral</h3>
<p>Understanding images and text together is an important aspect of cognition
and building advanced Artificial Intelligence (AI) systems. As a community, we
have achieved good benchmarks over language and vision domains separately,
however joint reasoning is still a challenge for state-of-the-art computer
vision and natural language processing (NLP) systems. We propose a novel task
to derive joint inference about a given image-text modality and compile the
Visuo-Linguistic Question Answering (VLQA) challenge corpus in a question
answering setting. Each dataset item consists of an image and a reading
passage, where questions are designed to combine both visual and textual
information i.e., ignoring either modality would make the question
unanswerable. We first explore the best existing vision-language architectures
to solve VLQA subsets and show that they are unable to reason well. We then
develop a modular method with slightly better baseline performance, but it is
still far behind human performance. We believe that VLQA will be a good
benchmark for reasoning over a visuo-linguistic context. The dataset, code and
leaderboard is available at https://shailaja183.github.io/vlqa/.
</p>
<a href="http://arxiv.org/abs/2005.00330" target="_blank">arXiv:2005.00330</a> [<a href="http://arxiv.org/pdf/2005.00330" target="_blank">pdf</a>]

<h2>A Causal View on Robustness of Neural Networks. (arXiv:2005.01095v2 [cs.LG] UPDATED)</h2>
<h3>Cheng Zhang, Kun Zhang, Yingzhen Li</h3>
<p>We present a causal view on the robustness of neural networks against input
manipulations, which applies not only to traditional classification tasks but
also to general measurement data. Based on this view, we design a deep causal
manipulation augmented model (deep CAMA) which explicitly models possible
manipulations on certain causes leading to changes in the observed effect. We
further develop data augmentation and test-time fine-tuning methods to improve
deep CAMA's robustness. When compared with discriminative deep neural networks,
our proposed model shows superior robustness against unseen manipulations. As a
by-product, our model achieves disentangled representation which separates the
representation of manipulations from those of other latent causes.
</p>
<a href="http://arxiv.org/abs/2005.01095" target="_blank">arXiv:2005.01095</a> [<a href="http://arxiv.org/pdf/2005.01095" target="_blank">pdf</a>]

<h2>sEMG Gesture Recognition with a Simple Model of Attention. (arXiv:2006.03645v2 [cs.LG] UPDATED)</h2>
<h3>David Josephs, Carson Drake, Andrew Heroy, John Santerre</h3>
<p>Myoelectric control is one of the leading areas of research in the field of
robotic prosthetics. We present our research in surface electromyography (sEMG)
signal classification, where our simple and novel attention-based approach now
leads the industry, universally beating more complex, state-of-the-art models.
Our novel attention-based model achieves benchmark leading results on multiple
industry-standard datasets including 53 finger, wrist, and grasping motions,
improving over both sophisticated signal processing and CNN-based approaches.
Our strong results with a straightforward model also indicate that sEMG
represents a promising avenue for future machine learning research, with
applications not only in prosthetics, but also in other important areas, such
as diagnosis and prognostication of neurodegenerative diseases, computationally
mediated surgeries, and advanced robotic control. We reinforce this suggestion
with extensive ablative studies, demonstrating that a neural network can easily
extract higher order spatiotemporal features from noisy sEMG data collected by
affordable, consumer-grade sensors.
</p>
<a href="http://arxiv.org/abs/2006.03645" target="_blank">arXiv:2006.03645</a> [<a href="http://arxiv.org/pdf/2006.03645" target="_blank">pdf</a>]

<h2>The Criminality From Face Illusion. (arXiv:2006.03895v2 [cs.CV] UPDATED)</h2>
<h3>Kevin W. Bowyer, Michael King, Walter Scheirer, Kushal Vangara</h3>
<p>The automatic analysis of face images can generate predictions about a
person's gender, age, race, facial expression, body mass index, and various
other indices and conditions. A few recent publications have claimed success in
analyzing an image of a person's face in order to predict the person's status
as Criminal / Non-Criminal. Predicting criminality from face may initially seem
similar to other facial analytics, but we argue that attempts to create a
criminality-from-face algorithm are necessarily doomed to fail, that apparently
promising experimental results in recent publications are an illusion resulting
from inadequate experimental design, and that there is potentially a large
social cost to belief in the criminality from face illusion.
</p>
<a href="http://arxiv.org/abs/2006.03895" target="_blank">arXiv:2006.03895</a> [<a href="http://arxiv.org/pdf/2006.03895" target="_blank">pdf</a>]

<h2>Optimal Transport Graph Neural Networks. (arXiv:2006.04804v4 [stat.ML] UPDATED)</h2>
<h3>Gary B&#xe9;cigneul, Octavian-Eugen Ganea, Benson Chen, Regina Barzilay, Tommi Jaakkola</h3>
<p>Current graph neural network (GNN) architectures naively average or sum node
embeddings into an aggregated graph representation -- potentially losing
structural or semantic information. We here introduce OT-GNN, a model that
computes graph embeddings using parametric prototypes that highlight key facets
of different graph aspects. Towards this goal, we are (to our knowledge) the
first to successfully combine optimal transport (OT) with parametric graph
models. Graph representations are obtained from Wasserstein distances between
the set of GNN node embeddings and "prototype" point clouds as free parameters.
We theoretically prove that, unlike traditional sum aggregation, our function
class on point clouds satisfies a fundamental universal approximation theorem.
Empirically, we address an inherent collapse optimization issue by proposing a
noise contrastive regularizer to steer the model towards truly exploiting the
optimal transport geometry. Finally, we consistently report better
generalization performance on several molecular property prediction tasks,
while exhibiting smoother graph representations.
</p>
<a href="http://arxiv.org/abs/2006.04804" target="_blank">arXiv:2006.04804</a> [<a href="http://arxiv.org/pdf/2006.04804" target="_blank">pdf</a>]

<h2>Bayesian Additive Regression Trees with Model Trees. (arXiv:2006.07493v3 [stat.ML] UPDATED)</h2>
<h3>Estev&#xe3;o B. Prado, Rafael A. Moral, Andrew C. Parnell</h3>
<p>Bayesian Additive Regression Trees (BART) is a tree-based machine learning
method that has been successfully applied to regression and classification
problems. BART assumes regularisation priors on a set of trees that work as
weak learners and is very flexible for predicting in the presence of
non-linearity and high-order interactions. In this paper, we introduce an
extension of BART, called Model Trees BART (MOTR-BART), that considers
piecewise linear functions at node levels instead of piecewise constants. In
MOTR-BART, rather than having a unique value at node level for the prediction,
a linear predictor is estimated considering the covariates that have been used
as the split variables in the corresponding tree. In our approach, local
linearities are captured more efficiently and fewer trees are required to
achieve equal or better performance than BART. Via simulation studies and real
data applications, we compare MOTR-BART to its main competitors. R code for
MOTR-BART implementation is available at https://github.com/ebprado/MOTR-BART.
</p>
<a href="http://arxiv.org/abs/2006.07493" target="_blank">arXiv:2006.07493</a> [<a href="http://arxiv.org/pdf/2006.07493" target="_blank">pdf</a>]

<h2>Fast Robust Subspace Tracking via PCA in Sparse Data-Dependent Noise. (arXiv:2006.08030v2 [cs.LG] UPDATED)</h2>
<h3>Namrata Vaswani, Praneeth Narayanamurthy</h3>
<p>This work studies the robust subspace tracking (ST) problem. Robust ST can be
simply understood as a (slow) time-varying subspace extension of robust PCA. It
assumes that the true data lies in a low-dimensional subspace that is either
fixed or changes slowly with time. The goal is to track the changing subspaces
over time in the presence of additive sparse outliers and to do this quickly
(with a short delay). We introduce a "fast" mini-batch robust ST solution that
is provably correct under mild assumptions. Here "fast" means two things: (i)
the subspace changes can be detected and the subspaces can be tracked with
near-optimal delay, and (ii) the time complexity of doing this is the same as
that of simple (non-robust) PCA. Our main result assumes piecewise constant
subspaces (needed for identifiability), but we also provide a corollary for the
case when there is a little change at each time.

A second contribution is a novel non-asymptotic guarantee for PCA in linearly
data-dependent noise. An important setting where this is useful is for linearly
data dependent noise that is sparse with support that changes enough over time.
The analysis of the subspace update step of our proposed robust ST solution
uses this result.
</p>
<a href="http://arxiv.org/abs/2006.08030" target="_blank">arXiv:2006.08030</a> [<a href="http://arxiv.org/pdf/2006.08030" target="_blank">pdf</a>]

<h2>Discovering Symbolic Models from Deep Learning with Inductive Biases. (arXiv:2006.11287v2 [cs.LG] UPDATED)</h2>
<h3>Miles Cranmer, Alvaro Sanchez-Gonzalez, Peter Battaglia, Rui Xu, Kyle Cranmer, David Spergel, Shirley Ho</h3>
<p>We develop a general approach to distill symbolic representations of a
learned deep model by introducing strong inductive biases. We focus on Graph
Neural Networks (GNNs). The technique works as follows: we first encourage
sparse latent representations when we train a GNN in a supervised setting, then
we apply symbolic regression to components of the learned model to extract
explicit physical relations. We find the correct known equations, including
force laws and Hamiltonians, can be extracted from the neural network. We then
apply our method to a non-trivial cosmology example-a detailed dark matter
simulation-and discover a new analytic formula which can predict the
concentration of dark matter from the mass distribution of nearby cosmic
structures. The symbolic expressions extracted from the GNN using our technique
also generalized to out-of-distribution data better than the GNN itself. Our
approach offers alternative directions for interpreting neural networks and
discovering novel physical principles from the representations they learn.
</p>
<a href="http://arxiv.org/abs/2006.11287" target="_blank">arXiv:2006.11287</a> [<a href="http://arxiv.org/pdf/2006.11287" target="_blank">pdf</a>]

<h2>Gradient-EM Bayesian Meta-learning. (arXiv:2006.11764v2 [cs.LG] UPDATED)</h2>
<h3>Yayi Zou, Xiaoqi Lu</h3>
<p>Bayesian meta-learning enables robust and fast adaptation to new tasks with
uncertainty assessment. The key idea behind Bayesian meta-learning is empirical
Bayes inference of hierarchical model. In this work, we extend this framework
to include a variety of existing methods, before proposing our variant based on
gradient-EM algorithm. Our method improves computational efficiency by avoiding
back-propagation computation in the meta-update step, which is exhausting for
deep neural networks. Furthermore, it provides flexibility to the inner-update
optimization procedure by decoupling it from meta-update. Experiments on
sinusoidal regression, few-shot image classification, and policy-based
reinforcement learning show that our method not only achieves better accuracy
with less computation cost, but is also more robust to uncertainty.
</p>
<a href="http://arxiv.org/abs/2006.11764" target="_blank">arXiv:2006.11764</a> [<a href="http://arxiv.org/pdf/2006.11764" target="_blank">pdf</a>]

<h2>A sparse code increases the speed and efficiency of neuro-dynamic programming for optimal control tasks with correlated inputs. (arXiv:2006.11968v3 [cs.LG] UPDATED)</h2>
<h3>Peter N. Loxley</h3>
<p>Sparse codes in neuroscience have been suggested to offer certain
computational advantages over other neural representations of sensory data. To
explore this viewpoint, a sparse code is used to represent natural images in an
optimal control task solved with neuro-dynamic programming, and its
computational properties are investigated. The central finding is that when
feature inputs to a linear network are correlated, an over-complete sparse code
increases the memory capacity of the network in an efficient manner beyond that
possible for any complete code with the same-sized input, and also increases
the speed of learning the network weights. A complete sparse code is found to
maximise the memory capacity of a linear network by decorrelating its feature
inputs to transform the design matrix of the least-squares problem to one of
full rank. It also conditions the Hessian matrix of the least-squares problem,
thereby increasing the rate of convergence to the optimal network weights.
Other types of decorrelating codes would also achieve this. However, an
over-complete sparse code is found to be approximately decorrelated, extracting
a larger number of approximately decorrelated features from the same-sized
input, allowing it to efficiently increase memory capacity beyond that possible
for any complete code: a 2.25 times over-complete sparse code is shown to at
least double memory capacity compared with a complete sparse code using the
same input. This is used in sequential learning to store a potentially large
number of optimal control tasks in the network, while catastrophic forgetting
is avoided using a partitioned representation, yielding a cost-to-go function
approximator that generalizes over the states in each partition. Sparse code
advantages over dense codes and local codes are also discussed.
</p>
<a href="http://arxiv.org/abs/2006.11968" target="_blank">arXiv:2006.11968</a> [<a href="http://arxiv.org/pdf/2006.11968" target="_blank">pdf</a>]

<h2>Disentangling by Subspace Diffusion. (arXiv:2006.12982v2 [stat.ML] UPDATED)</h2>
<h3>David Pfau, Irina Higgins, Aleksandar Botev, S&#xe9;bastien Racani&#xe8;re</h3>
<p>We present a novel nonparametric algorithm for symmetry-based disentangling
of data manifolds, the Geometric Manifold Component Estimator (GEOMANCER).
GEOMANCER provides a partial answer to the question posed by Higgins et al.
(2018): is it possible to learn how to factorize a Lie group solely from
observations of the orbit of an object it acts on? We show that fully
unsupervised factorization of a data manifold is possible if the true metric of
the manifold is known and each factor manifold has nontrivial holonomy -- for
example, rotation in 3D. Our algorithm works by estimating the subspaces that
are invariant under random walk diffusion, giving an approximation to the de
Rham decomposition from differential geometry. We demonstrate the efficacy of
GEOMANCER on several complex synthetic manifolds. Our work reduces the question
of whether unsupervised disentangling is possible to the question of whether
unsupervised metric learning is possible, providing a unifying insight into the
geometric nature of representation learning.
</p>
<a href="http://arxiv.org/abs/2006.12982" target="_blank">arXiv:2006.12982</a> [<a href="http://arxiv.org/pdf/2006.12982" target="_blank">pdf</a>]

<h2>RGBT Salient Object Detection: A Large-scale Dataset and Benchmark. (arXiv:2007.03262v5 [cs.CV] UPDATED)</h2>
<h3>Zhengzheng Tu, Yan Ma, Zhun Li, Chenglong Li, Jieming Xu, Yongtao Liu</h3>
<p>Salient object detection in complex scenes and environments is a challenging
research topic. Most works focus on RGB-based salient object detection, which
limits its performance of real-life applications when confronted with adverse
conditions such as dark environments and complex backgrounds. Taking advantage
of RGB and thermal infrared images becomes a new research direction for
detecting salient object in complex scenes recently, as thermal infrared
spectrum imaging provides the complementary information and has been applied to
many computer vision tasks. However, current research for RGBT salient object
detection is limited by the lack of a large-scale dataset and comprehensive
benchmark. This work contributes such a RGBT image dataset named VT5000,
including 5000 spatially aligned RGBT image pairs with ground truth
annotations. VT5000 has 11 challenges collected in different scenes and
environments for exploring the robustness of algorithms. With this dataset, we
propose a powerful baseline approach, which extracts multi-level features
within each modality and aggregates these features of all modalities with the
attention mechanism, for accurate RGBT salient object detection. Extensive
experiments show that the proposed baseline approach outperforms the
state-of-the-art methods on VT5000 dataset and other two public datasets. In
addition, we carry out a comprehensive analysis of different algorithms of RGBT
salient object detection on VT5000 dataset, and then make several valuable
conclusions and provide some potential research directions for RGBT salient
object detection.
</p>
<a href="http://arxiv.org/abs/2007.03262" target="_blank">arXiv:2007.03262</a> [<a href="http://arxiv.org/pdf/2007.03262" target="_blank">pdf</a>]

<h2>Minimax Policy for Heavy-tailed Bandits. (arXiv:2007.10493v2 [stat.ML] UPDATED)</h2>
<h3>Lai Wei, Vaibhav Srivastava</h3>
<p>We study the stochastic Multi-Armed Bandit (MAB) problem under worst-case
regret and heavy-tailed reward distribution. We modify the minimax policy MOSS
for the sub-Gaussian reward distribution by using saturated empirical mean to
design a new algorithm called Robust MOSS. We show that if the moment of order
$1+\epsilon$ for the reward distribution exists, then the refined strategy has
a worst-case regret matching the lower bound while maintaining a
distribution-dependent logarithm regret.
</p>
<a href="http://arxiv.org/abs/2007.10493" target="_blank">arXiv:2007.10493</a> [<a href="http://arxiv.org/pdf/2007.10493" target="_blank">pdf</a>]

<h2>Maximum Mutation Reinforcement Learning for Scalable Control. (arXiv:2007.13690v5 [cs.LG] UPDATED)</h2>
<h3>Karush Suri, Xiao Qi Shi, Konstantinos N. Plataniotis, Yuri A. Lawryshyn</h3>
<p>Advances in Reinforcement Learning (RL) have successfully tackled sample
efficiency and overestimation bias. However, these methods often fall short of
scalable performance. On the other hand, genetic methods provide scalability
but depict hyperparameter sensitivity to evolutionary operations. We present
the Evolution-based Soft Actor-Critic (ESAC), a scalable RL algorithm. Our
contributions are threefold; ESAC (1) abstracts exploration from exploitation
by combining Evolution Strategies (ES) with Soft Actor-Critic (SAC), (2)
provides dominant skill transfer between offsprings by making use of soft
winner selections and genetic crossovers in hindsight and (3) improves
hyperparameter sensitivity in evolutions using Automatic Mutation Tuning (AMT).
AMT gradually replaces the entropy framework of SAC allowing the population to
succeed at the task while acting as randomly as possible, without making use of
backpropagation updates. On a range of challenging robot control tasks
consisting of high-dimensional action spaces and sparse rewards, ESAC
demonstrates improved performance and sample efficiency in comparison to the
Maximum Entropy framework. ESAC demonstrates scalability comparable to ES on
the basis of hardware resources and algorithm overhead. A complete
implementation of ESAC with notes on reproducibility and videos can be found at
the project website https://karush17.github.io/esac-web/.
</p>
<a href="http://arxiv.org/abs/2007.13690" target="_blank">arXiv:2007.13690</a> [<a href="http://arxiv.org/pdf/2007.13690" target="_blank">pdf</a>]

<h2>CenterHMR: Multi-Person Center-based Human Mesh Recovery. (arXiv:2008.12272v2 [cs.CV] UPDATED)</h2>
<h3>Yu Sun, Qian Bao, Wu Liu, Yili Fu, Michael J. Black, Tao Mei</h3>
<p>This paper focuses on multi-person 3D mesh recovery from a single RGB image.
Existing approaches predominantly follow a multi-stage pipeline, that detects
bounding boxes and then regresses the body from bounding-box-level features.
However, multi-person occlusion and truncation can make these features
ambiguous, which results in the failure of recovery. To deal with this problem,
we present a novel bottom-up single-shot method, named "Center-based Human Mesh
Recovery network (CenterHMR)". The key idea is to develop an explicit
center-based representation for bottom-up pixel-level estimation. Guided by the
body centers, our model effectively locates every person and learns robust and
discriminative features under occlusion. In an end-to-end manner, the model is
trained to estimate multiple differentiable maps that contain the information
of multi-person 3D body meshes and their locations.Furthermore, when
encountering severe multi-person occlusion, the body centers may be very close
or even overlapping. A collision-aware center representation is developed to
ensure a distinguishable distance between body centers. Our proposed CenterHMR
achieves state-of-the-art performance on four challenging
multi-person/occlusion benchmarks (3DPW, CMU Panoptic, MuPoTs-3D, and 3DOH50K).
Experiments on crowded/occluded datasets demonstrate the stability under
various types of occlusion. Due to the concise bottom-up single-shot design,
our released demo code (https://github.com/Arthur151/CenterHMR) is the first
open-source real-time (over 30 FPS) implementation of monocular multi-person 3D
mesh recovery.
</p>
<a href="http://arxiv.org/abs/2008.12272" target="_blank">arXiv:2008.12272</a> [<a href="http://arxiv.org/pdf/2008.12272" target="_blank">pdf</a>]

<h2>Puzzle-AE: Novelty Detection in Images through Solving Puzzles. (arXiv:2008.12959v2 [cs.CV] UPDATED)</h2>
<h3>Mohammadreza Salehi, Ainaz Eftekhar, Niousha Sadjadi, Mohammad Hossein Rohban, Hamid R. Rabiee</h3>
<p>Autoencoder, as an essential part of many anomaly detection methods, is
lacking flexibility on normal data in complex datasets. U-Net is proved to be
effective for this purpose but overfits on the training data if trained by just
using reconstruction error similar to other AE-based frameworks.
Puzzle-solving, as a pretext task of self-supervised learning (SSL) methods,
has earlier proved its ability in learning semantically meaningful features. We
show that training U-Nets based on this task is an effective remedy that
prevents overfitting and facilitates learning beyond pixel-level features.
Shortcut solutions, however, are a big challenge in SSL tasks, including jigsaw
puzzles. We propose adversarial robust training as an effective automatic
shortcut removal. We achieve competitive or superior results compared to the
State of the Art (SOTA) anomaly detection methods on various toy and real-world
datasets. Unlike many competitors, the proposed framework is stable, fast,
data-efficient, and does not require unprincipled early stopping.
</p>
<a href="http://arxiv.org/abs/2008.12959" target="_blank">arXiv:2008.12959</a> [<a href="http://arxiv.org/pdf/2008.12959" target="_blank">pdf</a>]

<h2>SNoRe: Scalable Unsupervised Learning of Symbolic Node Representations. (arXiv:2009.04535v2 [cs.LG] UPDATED)</h2>
<h3>Sebastian Me&#x17e;nar, Nada Lavra&#x10d;, Bla&#x17e; &#x160;krlj</h3>
<p>Learning from complex real-life networks is a lively research area, with
recent advances in learning information-rich, low-dimensional network node
representations. However, state-of-the-art methods are not necessarily
interpretable and are therefore not fully applicable to sensitive settings in
biomedical or user profiling tasks, where explicit bias detection is highly
relevant. The proposed SNoRe (Symbolic Node Representations) algorithm is
capable of learning symbolic, human-understandable representations of
individual network nodes, based on the similarity of neighborhood hashes which
serve as features. SNoRe's interpretable features are suitable for direct
explanation of individual predictions, which we demonstrate by coupling it with
the widely used instance explanation tool SHAP to obtain nomograms representing
the relevance of individual features for a given classification. To our
knowledge, this is one of the first such attempts in a structural node
embedding setting. In the experimental evaluation on eleven real-life datasets,
SNoRe proved to be competitive to strong baselines, such as variational graph
autoencoders, node2vec and LINE. The vectorized implementation of SNoRe scales
to large networks, making it suitable for contemporary network learning and
analysis tasks.
</p>
<a href="http://arxiv.org/abs/2009.04535" target="_blank">arXiv:2009.04535</a> [<a href="http://arxiv.org/pdf/2009.04535" target="_blank">pdf</a>]

<h2>Removing the Background by Adding the Background: Towards Background Robust Self-supervised Video Representation Learning. (arXiv:2009.05769v2 [cs.CV] UPDATED)</h2>
<h3>Jinpeng Wang, Yuting Gao, Ke Li, Yiqi Lin, Andy J. Ma, Hao Cheng, Pai Peng, Rongrong Ji, Xing Sun</h3>
<p>Self-supervised learning has shown great potentials in improving the video
representation ability of deep neural networks by getting supervision from the
data itself. However, some of the current methods tend to cheat from the
background, i.e., the prediction is highly dependent on the video background
instead of the motion, making the model vulnerable to background changes. To
mitigate the model reliance towards the background, we propose to remove the
background impact by adding the background. That is, given a video, we randomly
select a static frame and add it to every other frames to construct a
distracting video sample. Then we force the model to pull the feature of the
distracting video and the feature of the original video closer, so that the
model is explicitly restricted to resist the background influence, focusing
more on the motion changes. We term our method as \emph{Background Erasing}
(BE). It is worth noting that the implementation of our method is so simple and
neat and can be added to most of the SOTA methods without much efforts.
Specifically, BE brings 16.4% and 19.1% improvements with MoCo on the severely
biased datasets UCF101 and HMDB51, and 14.5% improvement on the less biased
dataset Diving48.
</p>
<a href="http://arxiv.org/abs/2009.05769" target="_blank">arXiv:2009.05769</a> [<a href="http://arxiv.org/pdf/2009.05769" target="_blank">pdf</a>]

<h2>Towards Scalable Bayesian Learning of Causal DAGs. (arXiv:2010.00684v2 [cs.LG] UPDATED)</h2>
<h3>Jussi Viinikka, Antti Hyttinen, Johan Pensar, Mikko Koivisto</h3>
<p>We give methods for Bayesian inference of directed acyclic graphs, DAGs, and
the induced causal effects from passively observed complete data. Our methods
build on a recent Markov chain Monte Carlo scheme for learning Bayesian
networks, which enables efficient approximate sampling from the graph
posterior, provided that each node is assigned a small number $K$ of candidate
parents. We present algorithmic techniques to significantly reduce the space
and time requirements, which make the use of substantially larger values of $K$
feasible. Furthermore, we investigate the problem of selecting the candidate
parents per node so as to maximize the covered posterior mass. Finally, we
combine our sampling method with a novel Bayesian approach for estimating
causal effects in linear Gaussian DAG models. Numerical experiments demonstrate
the performance of our methods in detecting ancestor-descendant relations, and
in causal effect estimation our Bayesian method is shown to outperform previous
approaches.
</p>
<a href="http://arxiv.org/abs/2010.00684" target="_blank">arXiv:2010.00684</a> [<a href="http://arxiv.org/pdf/2010.00684" target="_blank">pdf</a>]

<h2>Dirichlet Graph Variational Autoencoder. (arXiv:2010.04408v2 [cs.LG] UPDATED)</h2>
<h3>Jia Li, Tomasyu Yu, Jiajin Li, Honglei Zhang, Kangfei Zhao, YU Rong, Hong Cheng, Junzhou Huang</h3>
<p>Graph Neural Networks (GNNs) and Variational Autoencoders (VAEs) have been
widely used in modeling and generating graphs with latent factors. However,
there is no clear explanation of what these latent factors are and why they
perform well. In this work, we present Dirichlet Graph Variational Autoencoder
(DGVAE) with graph cluster memberships as latent factors. Our study connects
VAEs based graph generation and balanced graph cut, and provides a new way to
understand and improve the internal mechanism of VAEs based graph generation.
Specifically, we first interpret the reconstruction term of DGVAE as balanced
graph cut in a principled way. Furthermore, motivated by the low pass
characteristics in balanced graph cut, we propose a new variant of GNN named
Heatts to encode the input graph into cluster memberships. Heatts utilizes the
Taylor series for fast computation of heat kernels and has better low pass
characteristics than Graph Convolutional Networks (GCN). Through experiments on
graph generation and graph clustering, we demonstrate the effectiveness of our
proposed framework.
</p>
<a href="http://arxiv.org/abs/2010.04408" target="_blank">arXiv:2010.04408</a> [<a href="http://arxiv.org/pdf/2010.04408" target="_blank">pdf</a>]

<h2>Generalized Independent Noise Condition for Estimating Latent Variable Causal Graphs. (arXiv:2010.04917v2 [cs.LG] UPDATED)</h2>
<h3>Feng Xie, Ruichu Cai, Biwei Huang, Clark Glymour, Zhifeng Hao, Kun Zhang</h3>
<p>Causal discovery aims to recover causal structures or models underlying the
observed data. Despite its success in certain domains, most existing methods
focus on causal relations between observed variables, while in many scenarios
the observed ones may not be the underlying causal variables (e.g., image
pixels), but are generated by latent causal variables or confounders that are
causally related. To this end, in this paper, we consider Linear, Non-Gaussian
Latent variable Models (LiNGLaMs), in which latent confounders are also
causally related, and propose a Generalized Independent Noise (GIN) condition
to estimate such latent variable graphs. Specifically, for two observed random
vectors $\mathbf{Y}$ and $\mathbf{Z}$, GIN holds if and only if
$\omega^{\intercal}\mathbf{Y}$ and $\mathbf{Z}$ are statistically independent,
where $\omega$ is a parameter vector characterized from the cross-covariance
between $\mathbf{Y}$ and $\mathbf{Z}$. From the graphical view, roughly
speaking, GIN implies that causally earlier latent common causes of variables
in $\mathbf{Y}$ d-separate $\mathbf{Y}$ from $\mathbf{Z}$. Interestingly, we
find that the independent noise condition, i.e., if there is no confounder,
causes are independent from the error of regressing the effect on the causes,
can be seen as a special case of GIN. Moreover, we show that GIN helps locate
latent variables and identify their causal structure, including causal
directions. We further develop a recursive learning algorithm to achieve these
goals. Experimental results on synthetic and real-world data demonstrate the
effectiveness of our method.
</p>
<a href="http://arxiv.org/abs/2010.04917" target="_blank">arXiv:2010.04917</a> [<a href="http://arxiv.org/pdf/2010.04917" target="_blank">pdf</a>]

<h2>Regularised Least-Squares Regression with Infinite-Dimensional Output Space. (arXiv:2010.10973v2 [stat.ML] UPDATED)</h2>
<h3>Junhyunng Park, Krikamol Muandet</h3>
<p>We present some learning theory results on reproducing kernel Hilbert space
(RKHS) regression, where the output space is an infinite-dimensional Hilbert
space.
</p>
<a href="http://arxiv.org/abs/2010.10973" target="_blank">arXiv:2010.10973</a> [<a href="http://arxiv.org/pdf/2010.10973" target="_blank">pdf</a>]

<h2>NightOwl: Robotic Platform for Wheeled Service Robot. (arXiv:2010.11505v3 [cs.RO] UPDATED)</h2>
<h3>Resha Dwika Hefni Al-Fahsi, Kevin Aldian Winanta, Fauzan Pradana, Igi Ardiyanto, Adha Imam Cahyadi</h3>
<p>NightOwl is a robotic platform designed exclusively for a wheeled service
robot. The robot navigates autonomously in omnidirectional fashion movement and
equipped with LIDAR to sense the surrounding area. The platform itself was
built using the Robot Operating System (ROS) and written in two different
programming languages (C++ and Python). NightOwl is composed of several modular
programs, namely hardware controller, light detection and ranging (LIDAR),
simultaneous localization and mapping (SLAM), world model, path planning, robot
control, communication, and behaviour. The programs run in parallel and
communicate reciprocally to share various information. This paper explains the
role of modular programs in the term of input, process, and output. In
addition, NightOwl provides simulation visualized in both Gazebo and RViz. The
robot in its environment is visualized by Gazebo. Sensor data from LIDAR and
results from SLAM will be visualized by RViz.
</p>
<a href="http://arxiv.org/abs/2010.11505" target="_blank">arXiv:2010.11505</a> [<a href="http://arxiv.org/pdf/2010.11505" target="_blank">pdf</a>]

<h2>Keep your Eyes on the Lane: Real-time Attention-guided Lane Detection. (arXiv:2010.12035v2 [cs.CV] UPDATED)</h2>
<h3>Lucas Tabelini, Rodrigo Berriel, Thiago M. Paix&#xe3;o, Claudine Badue, Alberto F. De Souza, Thiago Oliveira-Santos</h3>
<p>Modern lane detection methods have achieved remarkable performances in
complex real-world scenarios, but many have issues maintaining real-time
efficiency, which is important for autonomous vehicles. In this work, we
propose LaneATT: an anchor-based deep lane detection model, which, akin to
other generic deep object detectors, uses the anchors for the feature pooling
step. Since lanes follow a regular pattern and are highly correlated, we
hypothesize that in some cases global information may be crucial to infer their
positions, especially in conditions such as occlusion, missing lane markers,
and others. Thus, this work proposes a novel anchor-based attention mechanism
that aggregates global information. The model was evaluated extensively on
three of the most widely used datasets in the literature. The results show that
our method outperforms the current state-of-the-art methods showing both higher
efficacy and efficiency. Moreover, an ablation study is performed along with a
discussion on efficiency trade-off options that are useful in practice.
</p>
<a href="http://arxiv.org/abs/2010.12035" target="_blank">arXiv:2010.12035</a> [<a href="http://arxiv.org/pdf/2010.12035" target="_blank">pdf</a>]

<h2>Throughput-Optimal Topology Design for Cross-Silo Federated Learning. (arXiv:2010.12229v2 [cs.LG] UPDATED)</h2>
<h3>Othmane Marfoq, Chuan Xu, Giovanni Neglia, Richard Vidal</h3>
<p>Federated learning usually employs a client-server architecture where an
orchestrator iteratively aggregates model updates from remote clients and
pushes them back a refined model. This approach may be inefficient in
cross-silo settings, as close-by data silos with high-speed access links may
exchange information faster than with the orchestrator, and the orchestrator
may become a communication bottleneck. In this paper we define the problem of
topology design for cross-silo federated learning using the theory of max-plus
linear systems to compute the system throughput---number of communication
rounds per time unit. We also propose practical algorithms that, under the
knowledge of measurable network characteristics, find a topology with the
largest throughput or with provable throughput guarantees. In realistic
Internet networks with 10 Gbps access links for silos, our algorithms speed up
training by a factor 9 and 1.5 in comparison to the master-slave architecture
and to state-of-the-art MATCHA, respectively. Speedups are even larger with
slower access links.
</p>
<a href="http://arxiv.org/abs/2010.12229" target="_blank">arXiv:2010.12229</a> [<a href="http://arxiv.org/pdf/2010.12229" target="_blank">pdf</a>]

<h2>Wide flat minima and optimal generalization in classifying high-dimensional Gaussian mixtures. (arXiv:2010.14761v2 [cs.LG] UPDATED)</h2>
<h3>Carlo Baldassi, Enrico M. Malatesta, Matteo Negri, Riccardo Zecchina</h3>
<p>We analyze the connection between minimizers with good generalizing
properties and high local entropy regions of a threshold-linear classifier in
Gaussian mixtures with the mean squared error loss function. We show that there
exist configurations that achieve the Bayes-optimal generalization error, even
in the case of unbalanced clusters. We explore analytically the error-counting
loss landscape in the vicinity of a Bayes-optimal solution, and show that the
closer we get to such configurations, the higher the local entropy, implying
that the Bayes-optimal solution lays inside a wide flat region. We also
consider the algorithmically relevant case of targeting wide flat minima of the
(differentiable) mean squared error loss. Our analytical and numerical results
show not only that in the balanced case the dependence on the norm of the
weights is mild, but also, in the unbalanced case, that the performances can be
improved.
</p>
<a href="http://arxiv.org/abs/2010.14761" target="_blank">arXiv:2010.14761</a> [<a href="http://arxiv.org/pdf/2010.14761" target="_blank">pdf</a>]

<h2>Representation learning for improved interpretability and classification accuracy of clinical factors from EEG. (arXiv:2010.15274v3 [cs.LG] UPDATED)</h2>
<h3>Garrett Honke, Irina Higgins, Nina Thigpen, Vladimir Miskovic, Katie Link, Sunny Duan, Pramod Gupta, Julia Klawohn, Greg Hajcak</h3>
<p>Despite extensive standardization, diagnostic interviews for mental health
disorders encompass substantial subjective judgment. Previous studies have
demonstrated that EEG-based neural measures can function as reliable objective
correlates of depression, or even predictors of depression and its course.
However, their clinical utility has not been fully realized because of 1) the
lack of automated ways to deal with the inherent noise associated with EEG data
at scale, and 2) the lack of knowledge of which aspects of the EEG signal may
be markers of a clinical disorder. Here we adapt an unsupervised pipeline from
the recent deep representation learning literature to address these problems by
1) learning a disentangled representation using $\beta$-VAE to denoise the
signal, and 2) extracting interpretable features associated with a sparse set
of clinical labels using a Symbol-Concept Association Network (SCAN). We
demonstrate that our method is able to outperform the canonical hand-engineered
baseline classification method on a number of factors, including participant
age and depression diagnosis. Furthermore, our method recovers a representation
that can be used to automatically extract denoised Event Related Potentials
(ERPs) from novel, single EEG trajectories, and supports fast supervised
re-mapping to various clinical labels, allowing clinicians to re-use a single
EEG representation regardless of updates to the standardized diagnostic system.
Finally, single factors of the learned disentangled representations often
correspond to meaningful markers of clinical factors, as automatically detected
by SCAN, allowing for human interpretability and post-hoc expert analysis of
the recommendations made by the model.
</p>
<a href="http://arxiv.org/abs/2010.15274" target="_blank">arXiv:2010.15274</a> [<a href="http://arxiv.org/pdf/2010.15274" target="_blank">pdf</a>]

<h2>Efficient image retrieval using multi neural hash codes and bloom filters. (arXiv:2011.03234v2 [cs.CV] UPDATED)</h2>
<h3>Sourin Chakrabarti</h3>
<p>This paper aims to deliver an efficient and modified approach for image
retrieval using multiple neural hash codes and limiting the number of queries
using bloom filters by identifying false positives beforehand. Traditional
approaches involving neural networks for image retrieval tasks tend to use
higher layers for feature extraction. But it has been seen that the activations
of lower layers have proven to be more effective in a number of scenarios. In
our approach, we have leveraged the use of local deep convolutional neural
networks which combines the powers of both the features of lower and higher
layers for creating feature maps which are then compressed using PCA and fed to
a bloom filter after binary sequencing using a modified multi k-means approach.
The feature maps obtained are further used in the image retrieval process in a
hierarchical coarse-to-fine manner by first comparing the images in the higher
layers for semantically similar images and then gradually moving towards the
lower layers searching for structural similarities. While searching, the neural
hashes for the query image are again calculated and queried in the bloom filter
which tells us whether the query image is absent in the set or maybe present.
If the bloom filter doesn't necessarily rule out the query, then it goes into
the image retrieval process. This approach can be particularly helpful in cases
where the image store is distributed since the approach supports parallel
querying.
</p>
<a href="http://arxiv.org/abs/2011.03234" target="_blank">arXiv:2011.03234</a> [<a href="http://arxiv.org/pdf/2011.03234" target="_blank">pdf</a>]

<h2>EfficientPose: An efficient, accurate and scalable end-to-end 6D multi object pose estimation approach. (arXiv:2011.04307v2 [cs.CV] UPDATED)</h2>
<h3>Yannick Bukschat, Marcus Vetter</h3>
<p>In this paper we introduce EfficientPose, a new approach for 6D object pose
estimation. Our method is highly accurate, efficient and scalable over a wide
range of computational resources. Moreover, it can detect the 2D bounding box
of multiple objects and instances as well as estimate their full 6D poses in a
single shot. This eliminates the significant increase in runtime when dealing
with multiple objects other approaches suffer from. These approaches aim to
first detect 2D targets, e.g. keypoints, and solve a Perspective-n-Point
problem for their 6D pose for each object afterwards. We also propose a novel
augmentation method for direct 6D pose estimation approaches to improve
performance and generalization, called 6D augmentation. Our approach achieves a
new state-of-the-art accuracy of 97.35% in terms of the ADD(-S) metric on the
widely-used 6D pose estimation benchmark dataset Linemod using RGB input, while
still running end-to-end at over 27 FPS. Through the inherent handling of
multiple objects and instances and the fused single shot 2D object detection as
well as 6D pose estimation, our approach runs even with multiple objects
(eight) end-to-end at over 26 FPS, making it highly attractive to many real
world scenarios. Code will be made publicly available at
https://github.com/ybkscht/EfficientPose.
</p>
<a href="http://arxiv.org/abs/2011.04307" target="_blank">arXiv:2011.04307</a> [<a href="http://arxiv.org/pdf/2011.04307" target="_blank">pdf</a>]

<h2>Deep Transfer Learning for Automated Diagnosis of Skin Lesions from Photographs. (arXiv:2011.04475v3 [cs.CV] UPDATED)</h2>
<h3>Emma Rocheteau, Doyoon Kim</h3>
<p>Melanoma is not the most common form of skin cancer, but it is the most
deadly. Currently, the disease is diagnosed by expert dermatologists, which is
costly and requires timely access to medical treatment. Recent advances in deep
learning have the potential to improve diagnostic performance, expedite urgent
referrals and reduce burden on clinicians. Through smart phones, the technology
could reach people who would not normally have access to such healthcare
services, e.g. in remote parts of the world, due to financial constraints or in
2020, COVID-19 cancellations. To this end, we have investigated various
transfer learning approaches by leveraging model parameters pre-trained on
ImageNet with finetuning on melanoma detection. We compare EfficientNet,
MnasNet, MobileNet, DenseNet, SqueezeNet, ShuffleNet, GoogleNet, ResNet,
ResNeXt, VGG and a simple CNN with and without transfer learning. We find the
mobile network, EfficientNet (with transfer learning) achieves the best mean
performance with an area under the receiver operating characteristic curve
(AUROC) of 0.931$\pm$0.005 and an area under the precision recall curve (AUPRC)
of 0.840$\pm$0.010. This is significantly better than general practitioners
(0.83$\pm$0.03 AUROC) and dermatologists (0.91$\pm$0.02 AUROC).
</p>
<a href="http://arxiv.org/abs/2011.04475" target="_blank">arXiv:2011.04475</a> [<a href="http://arxiv.org/pdf/2011.04475" target="_blank">pdf</a>]

<h2>Deep Reinforcement Learning for Navigation in AAA Video Games. (arXiv:2011.04764v2 [cs.LG] UPDATED)</h2>
<h3>Eloi Alonso, Maxim Peter, David Goumard, Joshua Romoff</h3>
<p>In video games, non-player characters (NPCs) are used to enhance the players'
experience in a variety of ways, e.g., as enemies, allies, or innocent
bystanders. A crucial component of NPCs is navigation, which allows them to
move from one point to another on the map. The most popular approach for NPC
navigation in the video game industry is to use a navigation mesh (NavMesh),
which is a graph representation of the map, with nodes and edges indicating
traversable areas. Unfortunately, complex navigation abilities that extend the
character's capacity for movement, e.g., grappling hooks, jetpacks,
teleportation, or double-jumps, increases the complexity of the NavMesh, making
it intractable in many practical scenarios. Game designers are thus constrained
to only add abilities that can be handled by a NavMesh if they want to have NPC
navigation. As an alternative, we propose to use Deep Reinforcement Learning
(Deep RL) to learn how to navigate 3D maps using any navigation ability. We
test our approach on complex 3D environments in the Unity game engine that are
notably an order of magnitude larger than maps typically used in the Deep RL
literature. One of these maps is directly modeled after a Ubisoft AAA game. We
find that our approach performs surprisingly well, achieving at least $90\%$
success rate on all tested scenarios. A video of our results is available at
https://youtu.be/WFIf9Wwlq8M.
</p>
<a href="http://arxiv.org/abs/2011.04764" target="_blank">arXiv:2011.04764</a> [<a href="http://arxiv.org/pdf/2011.04764" target="_blank">pdf</a>]

<h2>Optimized Loss Functions for Object detection: A Case Study on Nighttime Vehicle Detection. (arXiv:2011.05523v2 [cs.CV] UPDATED)</h2>
<h3>Shang Jiang, Haoran Qin, Bingli Zhang, Jieyu Zheng</h3>
<p>Loss functions is a crucial factor that affecting the detection precision in
object detection task. In this paper, we optimize both two loss functions for
classification and localization simultaneously. Firstly, by multiplying an
IoU-based coefficient by the standard cross entropy loss in classification loss
function, the correlation between localization and classification is
established. Compared to the existing studies, in which the correlation is only
applied to improve the localization accuracy for positive samples, this paper
utilizes the correlation to obtain the really hard negative samples and aims to
decrease the misclassified rate for negative samples. Besides, a novel
localization loss named MIoU is proposed by incorporating a Mahalanobis
distance between predicted box and target box, which eliminate the gradients
inconsistency problem in the DIoU loss, further improving the localization
accuracy. Finally, sufficient experiments for nighttime vehicle detection have
been done on two datasets. Our results show than when train with the proposed
loss functions, the detection performance can be outstandingly improved. The
source code and trained models are available at
https://github.com/therebellll/NegIoU-PosIoU-Miou.
</p>
<a href="http://arxiv.org/abs/2011.05523" target="_blank">arXiv:2011.05523</a> [<a href="http://arxiv.org/pdf/2011.05523" target="_blank">pdf</a>]

<h2>A Primal Approach to Constrained Policy Optimization: Global Optimality and Finite-Time Analysis. (arXiv:2011.05869v2 [cs.LG] UPDATED)</h2>
<h3>Tengyu Xu, Yingbin Liang, Guanghui Lan</h3>
<p>Safe reinforcement learning (SRL) problems are typically modeled as
constrained Markov Decision Process (CMDP), in which an agent explores the
environment to maximize the expected total reward and meanwhile avoids
violating certain constraints on a number of expected total costs. In general,
such SRL problems have nonconvex objective functions subject to multiple
nonconvex constraints, and hence are very challenging to solve, particularly to
provide a globally optimal policy. Many popular SRL algorithms adopt a
primal-dual structure which utilizes the updating of dual variables for
satisfying the constraints. In contrast, we propose a primal approach, called
constraint-rectified policy optimization (CRPO), which updates the policy
alternatingly between objective improvement and constraint satisfaction. CRPO
provides a primal-type algorithmic framework to solve SRL problems, where each
policy update can take any variant of policy optimization step. To demonstrate
the theoretical performance of CRPO, we adopt natural policy gradient (NPG) for
each policy update step and show that CRPO achieves an
$\mathcal{O}(1/\sqrt{T})$ convergence rate to the global optimal policy in the
constrained policy set and an $\mathcal{O}(1/\sqrt{T})$ error bound on
constraint satisfaction. This is the first finite-time analysis of SRL
algorithms with global optimality guarantee. Our empirical results demonstrate
that CRPO can outperform the existing primal-dual baseline algorithms
significantly.
</p>
<a href="http://arxiv.org/abs/2011.05869" target="_blank">arXiv:2011.05869</a> [<a href="http://arxiv.org/pdf/2011.05869" target="_blank">pdf</a>]

<h2>Implicit bias of any algorithm: bounding bias via margin. (arXiv:2011.06550v2 [stat.ML] UPDATED)</h2>
<h3>Elvis Dohmatob</h3>
<p>Consider $n$ points $x_1$,\ldots,$x_n$ in finite-dimensional euclidean space,
each having one of two colors. Suppose there exists a separating hyperplane
(identified with its unit normal vector $w)$ for the points, i.e a hyperplane
such that points of same color lie on the same side of the hyperplane. We
measure the quality of such a hyperplane by its margin $\gamma(w)$, defined as
minimum distance between any of the points $x_i$ and the hyperplane. In this
paper, we prove that the margin function $\gamma$ satisfies a nonsmooth
Kurdyka-Lojasiewicz inequality with exponent $1/2$. This result has
far-reaching consequences. For example, let $\gamma^{opt}$ be the maximum
possible margin for the problem and let $w^{opt}$ be the parameter for the
hyperplane which attains this value. Given any other separating hyperplane with
parameter $w$, let $d(w):=\|w-w^{opt}\|$ be the euclidean distance between $w$
and $w^{opt}$, also called the bias of $w$. From the previous KL-inequality, we
deduce that $(\gamma^{opt}-\gamma(w)) / R \le d(w) \le
2\sqrt{(\gamma^{opt}-\gamma(w))/\gamma^{opt}}$, where $R:=\max_i \|x_i\|$ is
the maximum distance of the points $x_i$ from the origin. Consequently, for any
optimization algorithm (gradient-descent or not), the bias of the iterates
converges at least as fast as the square-root of the rate of their convergence
of the margin. Thus, our work provides a generic tool for analyzing the
implicit bias of any algorithm in terms of its margin, in situations where a
specialized analysis might not be available: it is sufficient to establish a
good rate for converge of the margin, a task which is usually much easier.
</p>
<a href="http://arxiv.org/abs/2011.06550" target="_blank">arXiv:2011.06550</a> [<a href="http://arxiv.org/pdf/2011.06550" target="_blank">pdf</a>]

<h2>Deep Reinforcement Learning for Stochastic Computation Offloading in Digital Twin Networks. (arXiv:2011.08430v2 [cs.LG] UPDATED)</h2>
<h3>Yueyue Dai (Member, IEEE), Ke Zhang, Sabita Maharjan (Senior Member, IEEE), Yan Zhang (Fellow, IEEE)</h3>
<p>The rapid development of Industrial Internet of Things (IIoT) requires
industrial production towards digitalization to improve network efficiency.
Digital Twin is a promising technology to empower the digital transformation of
IIoT by creating virtual models of physical objects. However, the provision of
network efficiency in IIoT is very challenging due to resource-constrained
devices, stochastic tasks, and resources heterogeneity. Distributed resources
in IIoT networks can be efficiently exploited through computation offloading to
reduce energy consumption while enhancing data processing efficiency. In this
paper, we first propose a new paradigm Digital Twin Networks (DTN) to build
network topology and the stochastic task arrival model in IIoT systems. Then,
we formulate the stochastic computation offloading and resource allocation
problem to minimize the long-term energy efficiency. As the formulated problem
is a stochastic programming problem, we leverage Lyapunov optimization
technique to transform the original problem into a deterministic per-time slot
problem. Finally, we present Asynchronous Actor-Critic (AAC) algorithm to find
the optimal stochastic computation offloading policy. Illustrative results
demonstrate that our proposed scheme is able to significantly outperforms the
benchmarks.
</p>
<a href="http://arxiv.org/abs/2011.08430" target="_blank">arXiv:2011.08430</a> [<a href="http://arxiv.org/pdf/2011.08430" target="_blank">pdf</a>]

<h2>Edge Intelligence for Energy-efficient Computation Offloading and Resource Allocation in 5G Beyond. (arXiv:2011.08442v2 [cs.LG] UPDATED)</h2>
<h3>Yueyue Dai, Ke Zhang, Sabita Maharjan, Yan Zhang</h3>
<p>5G beyond is an end-edge-cloud orchestrated network that can exploit
heterogeneous capabilities of the end devices, edge servers, and the cloud and
thus has the potential to enable computation-intensive and delay-sensitive
applications via computation offloading. However, in multi user wireless
networks, diverse application requirements and the possibility of various radio
access modes for communication among devices make it challenging to design an
optimal computation offloading scheme. In addition, having access to complete
network information that includes variables such as wireless channel state, and
available bandwidth and computation resources, is a major issue. Deep
Reinforcement Learning (DRL) is an emerging technique to address such an issue
with limited and less accurate network information. In this paper, we utilize
DRL to design an optimal computation offloading and resource allocation
strategy for minimizing system energy consumption. We first present a
multi-user end-edge-cloud orchestrated network where all devices and base
stations have computation capabilities. Then, we formulate the joint
computation offloading and resource allocation problem as a Markov Decision
Process (MDP) and propose a new DRL algorithm to minimize system energy
consumption. Numerical results based on a real-world dataset demonstrate that
the proposed DRL-based algorithm significantly outperforms the benchmark
policies in terms of system energy consumption. Extensive simulations show that
learning rate, discount factor, and number of devices have considerable
influence on the performance of the proposed algorithm.
</p>
<a href="http://arxiv.org/abs/2011.08442" target="_blank">arXiv:2011.08442</a> [<a href="http://arxiv.org/pdf/2011.08442" target="_blank">pdf</a>]

<h2>A Divide et Impera Approach for 3D Shape Reconstruction from Multiple Views. (arXiv:2011.08534v2 [cs.CV] UPDATED)</h2>
<h3>Riccardo Spezialetti, David Joseph Tan, Alessio Tonioni, Keisuke Tateno, Federico Tombari</h3>
<p>Estimating the 3D shape of an object from a single or multiple images has
gained popularity thanks to the recent breakthroughs powered by deep learning.
Most approaches regress the full object shape in a canonical pose, possibly
extrapolating the occluded parts based on the learned priors. However, their
viewpoint invariant technique often discards the unique structures visible from
the input images. In contrast, this paper proposes to rely on viewpoint variant
reconstructions by merging the visible information from the given views. Our
approach is divided into three steps. Starting from the sparse views of the
object, we first align them into a common coordinate system by estimating the
relative pose between all the pairs. Then, inspired by the traditional voxel
carving, we generate an occupancy grid of the object taken from the silhouette
on the images and their relative poses. Finally, we refine the initial
reconstruction to build a clean 3D model which preserves the details from each
viewpoint. To validate the proposed method, we perform a comprehensive
evaluation on the ShapeNet reference benchmark in terms of relative pose
estimation and 3D shape reconstruction.
</p>
<a href="http://arxiv.org/abs/2011.08534" target="_blank">arXiv:2011.08534</a> [<a href="http://arxiv.org/pdf/2011.08534" target="_blank">pdf</a>]

<h2>Generating universal language adversarial examples by understanding and enhancing the transferability across neural models. (arXiv:2011.08558v2 [cs.LG] UPDATED)</h2>
<h3>Liping Yuan, Xiaoqing Zheng, Yi Zhou, Cho-Jui Hsieh, Kai-wei Chang, Xuanjing Huang</h3>
<p>Deep neural network models are vulnerable to adversarial attacks. In many
cases, malicious inputs intentionally crafted for one model can fool another
model in the black-box attack setting. However, there is a lack of systematic
studies on the transferability of adversarial examples and how to generate
universal adversarial examples. In this paper, we systematically study the
transferability of adversarial attacks for text classification models. In
particular, we conduct extensive experiments to investigate how various
factors, such as network architecture, input format, word embedding, and model
capacity, affect the transferability of adversarial attacks. Based on these
studies, we then propose universal black-box attack algorithms that can induce
adversarial examples to attack almost all existing models. These universal
adversarial examples reflect the defects of the learning process and the bias
in the training dataset. Finally, we generalize these adversarial examples into
universal word replacement rules that can be used for model diagnostics.
</p>
<a href="http://arxiv.org/abs/2011.08558" target="_blank">arXiv:2011.08558</a> [<a href="http://arxiv.org/pdf/2011.08558" target="_blank">pdf</a>]

<h2>Deep Active Surface Models. (arXiv:2011.08826v2 [cs.CV] UPDATED)</h2>
<h3>Udaranga Wickramasinghe, Graham Knott, Pascal Fua</h3>
<p>Active Surface Models have a long history of being useful to model complex 3D
surfaces but only Active Contours have been used in conjunction with deep
networks, and then only to produce the data term as well as meta-parameter maps
controlling them. In this paper, we advocate a much tighter integration. We
introduce layers that implement them that can be integrated seamlessly into
Graph Convolutional Networks to enforce sophisticated smoothness priors at an
acceptable computational cost. We will show that the resulting Deep Active
Surface Models outperform equivalent architectures that use traditional
regularization loss terms to impose smoothness priors for 3D surface
reconstruction from 2D images and for 3D volume segmentation.
</p>
<a href="http://arxiv.org/abs/2011.08826" target="_blank">arXiv:2011.08826</a> [<a href="http://arxiv.org/pdf/2011.08826" target="_blank">pdf</a>]

<h2>Memory-Efficient Learning of Stable Linear Dynamical Systems for Prediction and Control. (arXiv:2006.03937v3 [cs.LG] CROSS LISTED)</h2>
<h3>Giorgos Mamakoukas, Orest Xherija, T. D. Murphey</h3>
<p>Learning a stable Linear Dynamical System (LDS) from data involves creating
models that both minimize reconstruction error and enforce stability of the
learned representation. We propose a novel algorithm for learning stable LDSs.
Using a recent characterization of stable matrices, we present an optimization
method that ensures stability at every step and iteratively improves the
reconstruction error using gradient directions derived in this paper. When
applied to LDSs with inputs, our approach---in contrast to current methods for
learning stable LDSs---updates both the state and control matrices, expanding
the solution space and allowing for models with lower reconstruction error. We
apply our algorithm in simulations and experiments to a variety of problems,
including learning dynamic textures from image sequences and controlling a
robotic manipulator. Compared to existing approaches, our proposed method
achieves an orders-of-magnitude improvement in reconstruction error and
superior results in terms of control performance. In addition, it is provably
more memory-efficient, with an O(n^2) space complexity compared to O(n^4) of
competing alternatives, thus scaling to higher-dimensional systems when the
other methods fail.
</p>
<a href="http://arxiv.org/abs/2006.03937" target="_blank">arXiv:2006.03937</a> [<a href="http://arxiv.org/pdf/2006.03937" target="_blank">pdf</a>]

