---
title: Latest Deep Learning Papers
date: 2020-12-20 21:00:01 +0800
featured-img: DL-logo
categories: [Deep_Learning]
tags: [Deep_Learning]
mathjax: Yes
---

<h1>Your arXiv Feed (134 Articles)</h1>
<h2>Object Detection based on OcSaFPN in Aerial Images with Noise. (arXiv:2012.09859v1 [cs.CV])</h2>
<h3>Chengyuan Li, Jun Liu, Hailong Hong, Wenju Mao, Chenjie Wang, Chudi Hu, Xin Su, Bin Luo</h3>
<p>Taking the deep learning-based algorithms into account has become a crucial
way to boost object detection performance in aerial images. While various
neural network representations have been developed, previous works are still
inefficient to investigate the noise-resilient performance, especially on
aerial images with noise taken by the cameras with telephoto lenses, and most
of the research is concentrated in the field of denoising. Of course, denoising
usually requires an additional computational burden to obtain higher quality
images, while noise-resilient is more of a description of the robustness of the
network itself to different noises, which is an attribute of the algorithm
itself. For this reason, the work will be started by analyzing the
noise-resilient performance of the neural network, and then propose two
hypotheses to build a noise-resilient structure. Based on these hypotheses, we
compare the noise-resilient ability of the Oct-ResNet with frequency division
processing and the commonly used ResNet. In addition, previous feature pyramid
networks used for aerial object detection tasks are not specifically designed
for the frequency division feature maps of the Oct-ResNet, and they usually
lack attention to bridging the semantic gap between diverse feature maps from
different depths. On the basis of this, a novel octave convolution-based
semantic attention feature pyramid network (OcSaFPN) is proposed to get higher
accuracy in object detection with noise. The proposed algorithm tested on three
datasets demonstrates that the proposed OcSaFPN achieves a state-of-the-art
detection performance with Gaussian noise or multiplicative noise. In addition,
more experiments have proved that the OcSaFPN structure can be easily added to
existing algorithms, and the noise-resilient ability can be effectively
improved.
</p>
<a href="http://arxiv.org/abs/2012.09859" target="_blank">arXiv:2012.09859</a> [<a href="http://arxiv.org/pdf/2012.09859" target="_blank">pdf</a>]

<h2>Exploring Motion Boundaries in an End-to-End Network for Vision-based Parkinson's Severity Assessment. (arXiv:2012.09890v1 [cs.CV])</h2>
<h3>Amirhossein Dadashzadeh, Alan Whone, Michal Rolinski, Majid Mirmehdi</h3>
<p>Evaluating neurological disorders such as Parkinson's disease (PD) is a
challenging task that requires the assessment of several motor and non-motor
functions. In this paper, we present an end-to-end deep learning framework to
measure PD severity in two important components, hand movement and gait, of the
Unified Parkinson's Disease Rating Scale (UPDRS). Our method leverages on an
Inflated 3D CNN trained by a temporal segment framework to learn spatial and
long temporal structure in video data. We also deploy a temporal attention
mechanism to boost the performance of our model. Further, motion boundaries are
explored as an extra input modality to assist in obfuscating the effects of
camera motion for better movement assessment. We ablate the effects of
different data modalities on the accuracy of the proposed network and compare
with other popular architectures. We evaluate our proposed method on a dataset
of 25 PD patients, obtaining 72.3% and 77.1% top-1 accuracy on hand movement
and gait tasks respectively.
</p>
<a href="http://arxiv.org/abs/2012.09890" target="_blank">arXiv:2012.09890</a> [<a href="http://arxiv.org/pdf/2012.09890" target="_blank">pdf</a>]

<h2>Treatment Targeting by AUUC Maximization with Generalization Guarantees. (arXiv:2012.09897v1 [cs.LG])</h2>
<h3>Artem Betlei, Eustache Diemert, Massih-Reza Amini</h3>
<p>We consider the task of optimizing treatment assignment based on individual
treatment effect prediction. This task is found in many applications such as
personalized medicine or targeted advertising and has gained a surge of
interest in recent years under the name of Uplift Modeling. It consists in
targeting treatment to the individuals for whom it would be the most
beneficial. In real life scenarios, when we do not have access to ground-truth
individual treatment effect, the capacity of models to do so is generally
measured by the Area Under the Uplift Curve (AUUC), a metric that differs from
the learning objectives of most of the Individual Treatment Effect (ITE)
models. We argue that the learning of these models could inadvertently degrade
AUUC and lead to suboptimal treatment assignment. To tackle this issue, we
propose a generalization bound on the AUUC and present a novel learning
algorithm that optimizes a derivable surrogate of this bound, called AUUC-max.
Finally, we empirically demonstrate the tightness of this generalization bound,
its effectiveness for hyper-parameter tuning and show the efficiency of the
proposed algorithm compared to a wide range of competitive baselines on two
classical benchmarks.
</p>
<a href="http://arxiv.org/abs/2012.09897" target="_blank">arXiv:2012.09897</a> [<a href="http://arxiv.org/pdf/2012.09897" target="_blank">pdf</a>]

<h2>Attention-based Image Upsampling. (arXiv:2012.09904v1 [cs.CV])</h2>
<h3>Souvik Kundu, Hesham Mostafa, Sharath Nittur Sridhar, Sairam Sundaresan</h3>
<p>Convolutional layers are an integral part of many deep neural network
solutions in computer vision. Recent work shows that replacing the standard
convolution operation with mechanisms based on self-attention leads to improved
performance on image classification and object detection tasks. In this work,
we show how attention mechanisms can be used to replace another canonical
operation: strided transposed convolution. We term our novel attention-based
operation attention-based upsampling since it increases/upsamples the spatial
dimensions of the feature maps. Through experiments on single image
super-resolution and joint-image upsampling tasks, we show that attention-based
upsampling consistently outperforms traditional upsampling methods based on
strided transposed convolution or based on adaptive filters while using fewer
parameters. We show that the inherent flexibility of the attention mechanism,
which allows it to use separate sources for calculating the attention
coefficients and the attention targets, makes attention-based upsampling a
natural choice when fusing information from multiple image modalities.
</p>
<a href="http://arxiv.org/abs/2012.09904" target="_blank">arXiv:2012.09904</a> [<a href="http://arxiv.org/pdf/2012.09904" target="_blank">pdf</a>]

<h2>Research Reproducibility as a Survival Analysis. (arXiv:2012.09932v1 [stat.ML])</h2>
<h3>Edward Raff</h3>
<p>There has been increasing concern within the machine learning community that
we are in a reproducibility crisis. As many have begun to work on this problem,
all work we are aware of treat the issue of reproducibility as an intrinsic
binary property: a paper is or is not reproducible. Instead, we consider
modeling the reproducibility of a paper as a survival analysis problem. We
argue that this perspective represents a more accurate model of the underlying
meta-science question of reproducible research, and we show how a survival
analysis allows us to draw new insights that better explain prior longitudinal
data. The data and code can be found at
https://github.com/EdwardRaff/Research-Reproducibility-Survival-Analysis
</p>
<a href="http://arxiv.org/abs/2012.09932" target="_blank">arXiv:2012.09932</a> [<a href="http://arxiv.org/pdf/2012.09932" target="_blank">pdf</a>]

<h2>Increasing the efficiency of randomized trial estimates via linear adjustment for a prognostic score. (arXiv:2012.09935v1 [stat.ML])</h2>
<h3>Alejandro Schuler, David Walsh, Diana Hall, Jon Walsh, Charles Fisher</h3>
<p>Estimating causal effects from randomized experiments is central to clinical
research. Reducing the statistical uncertainty in these analyses is an
important objective for statisticians. Registries, prior trials, and health
records constitute a growing compendium of historical data on patients under
standard-of-care conditions that may be exploitable to this end. However, most
methods for historical borrowing achieve reductions in variance by sacrificing
strict type-I error rate control. Here, we propose a use of historical data
that exploits linear covariate adjustment to improve the efficiency of trial
analyses without incurring bias. Specifically, we train a prognostic model on
the historical data, then estimate the treatment effect using a linear
regression while adjusting for the trial subjects' predicted outcomes (their
prognostic scores). We prove that, under certain conditions, this prognostic
covariate adjustment procedure attains the minimum variance possible among a
large class of estimators. When those conditions are not met, prognostic
covariate adjustment is still more efficient than raw covariate adjustment and
the gain in efficiency is proportional to a measure of the predictive accuracy
of the prognostic model. We demonstrate the approach using simulations and a
reanalysis of an Alzheimer's Disease clinical trial and observe meaningful
reductions in mean-squared error and the estimated variance. Lastly, we provide
a simplified formula for asymptotic variance that enables power and sample size
calculations that account for the gains from the prognostic model for clinical
trial design.
</p>
<a href="http://arxiv.org/abs/2012.09935" target="_blank">arXiv:2012.09935</a> [<a href="http://arxiv.org/pdf/2012.09935" target="_blank">pdf</a>]

<h2>Reduced Order Modeling using Shallow ReLU Networks with Grassmann Layers. (arXiv:2012.09940v1 [cs.LG])</h2>
<h3>Kayla Bollinger, Hayden Schaeffer</h3>
<p>This paper presents a nonlinear model reduction method for systems of
equations using a structured neural network. The neural network takes the form
of a "three-layer" network with the first layer constrained to lie on the
Grassmann manifold and the first activation function set to identity, while the
remaining network is a standard two-layer ReLU neural network. The Grassmann
layer determines the reduced basis for the input space, while the remaining
layers approximate the nonlinear input-output system. The training alternates
between learning the reduced basis and the nonlinear approximation, and is
shown to be more effective than fixing the reduced basis and training the
network only. An additional benefit of this approach is, for data that lie on
low-dimensional subspaces, that the number of parameters in the network does
not need to be large. We show that our method can be applied to scientific
problems in the data-scarce regime, which is typically not well-suited for
neural network approximations. Examples include reduced order modeling for
nonlinear dynamical systems and several aerospace engineering problems.
</p>
<a href="http://arxiv.org/abs/2012.09940" target="_blank">arXiv:2012.09940</a> [<a href="http://arxiv.org/pdf/2012.09940" target="_blank">pdf</a>]

<h2>Guiding Neural Network Initialization via Marginal Likelihood Maximization. (arXiv:2012.09943v1 [stat.ML])</h2>
<h3>Anthony S. Tai, Chunfeng Huang</h3>
<p>We propose a simple, data-driven approach to help guide hyperparameter
selection for neural network initialization. We leverage the relationship
between neural network and Gaussian process models having corresponding
activation and covariance functions to infer the hyperparameter values
desirable for model initialization. Our experiment shows that marginal
likelihood maximization provides recommendations that yield near-optimal
prediction performance on MNIST classification task under experiment
constraints. Furthermore, our empirical results indicate consistency in the
proposed technique, suggesting that computation cost for the procedure could be
significantly reduced with smaller training sets.
</p>
<a href="http://arxiv.org/abs/2012.09943" target="_blank">arXiv:2012.09943</a> [<a href="http://arxiv.org/pdf/2012.09943" target="_blank">pdf</a>]

<h2>Fairkit, Fairkit, on the Wall, Who's the Fairest of Them All? Supporting Data Scientists in Training Fair Models. (arXiv:2012.09951v1 [cs.LG])</h2>
<h3>Brittany Johnson, Jesse Bartola, Rico Angell, Katherine Keith, Sam Witty, Stephen J. Giguere, Yuriy Brun</h3>
<p>Modern software relies heavily on data and machine learning, and affects
decisions that shape our world. Unfortunately, recent studies have shown that
because of biases in data, software systems frequently inject bias into their
decisions, from producing better closed caption transcriptions of men's voices
than of women's voices to overcharging people of color for financial loans. To
address bias in machine learning, data scientists need tools that help them
understand the trade-offs between model quality and fairness in their specific
data domains. Toward that end, we present fairkit-learn, a toolkit for helping
data scientists reason about and understand fairness. Fairkit-learn works with
state-of-the-art machine learning tools and uses the same interfaces to ease
adoption. It can evaluate thousands of models produced by multiple machine
learning algorithms, hyperparameters, and data permutations, and compute and
visualize a small Pareto-optimal set of models that describe the optimal
trade-offs between fairness and quality. We evaluate fairkit-learn via a user
study with 54 students, showing that students using fairkit-learn produce
models that provide a better balance between fairness and quality than students
using scikit-learn and IBM AI Fairness 360 toolkits. With fairkit-learn, users
can select models that are up to 67% more fair and 10% more accurate than the
models they are likely to train with scikit-learn.
</p>
<a href="http://arxiv.org/abs/2012.09951" target="_blank">arXiv:2012.09951</a> [<a href="http://arxiv.org/pdf/2012.09951" target="_blank">pdf</a>]

<h2>Learning Compositional Radiance Fields of Dynamic Human Heads. (arXiv:2012.09955v1 [cs.CV])</h2>
<h3>Ziyan Wang, Timur Bagautdinov, Stephen Lombardi, Tomas Simon, Jason Saragih, Jessica Hodgins, Michael Zollh&#xf6;fer</h3>
<p>Photorealistic rendering of dynamic humans is an important ability for
telepresence systems, virtual shopping, synthetic data generation, and more.
Recently, neural rendering methods, which combine techniques from computer
graphics and machine learning, have created high-fidelity models of humans and
objects. Some of these methods do not produce results with high-enough fidelity
for driveable human models (Neural Volumes) whereas others have extremely long
rendering times (NeRF). We propose a novel compositional 3D representation that
combines the best of previous methods to produce both higher-resolution and
faster results. Our representation bridges the gap between discrete and
continuous volumetric representations by combining a coarse 3D-structure-aware
grid of animation codes with a continuous learned scene function that maps
every position and its corresponding local animation code to its view-dependent
emitted radiance and local volume density. Differentiable volume rendering is
employed to compute photo-realistic novel views of the human head and upper
body as well as to train our novel representation end-to-end using only 2D
supervision. In addition, we show that the learned dynamic radiance field can
be used to synthesize novel unseen expressions based on a global animation
code. Our approach achieves state-of-the-art results for synthesizing novel
views of dynamic human heads and the upper body.
</p>
<a href="http://arxiv.org/abs/2012.09955" target="_blank">arXiv:2012.09955</a> [<a href="http://arxiv.org/pdf/2012.09955" target="_blank">pdf</a>]

<h2>Toward Transformer-Based Object Detection. (arXiv:2012.09958v1 [cs.CV])</h2>
<h3>Josh Beal, Eric Kim, Eric Tzeng, Dong Huk Park, Andrew Zhai, Dmitry Kislyuk</h3>
<p>Transformers have become the dominant model in natural language processing,
owing to their ability to pretrain on massive amounts of data, then transfer to
smaller, more specific tasks via fine-tuning. The Vision Transformer was the
first major attempt to apply a pure transformer model directly to images as
input, demonstrating that as compared to convolutional networks,
transformer-based architectures can achieve competitive results on benchmark
classification tasks. However, the computational complexity of the attention
operator means that we are limited to low-resolution inputs. For more complex
tasks such as detection or segmentation, maintaining a high input resolution is
crucial to ensure that models can properly identify and reflect fine details in
their output. This naturally raises the question of whether or not
transformer-based architectures such as the Vision Transformer are capable of
performing tasks other than classification. In this paper, we determine that
Vision Transformers can be used as a backbone by a common detection task head
to produce competitive COCO results. The model that we propose, ViT-FRCNN,
demonstrates several known properties associated with transformers, including
large pretraining capacity and fast fine-tuning performance. We also
investigate improvements over a standard detection backbone, including superior
performance on out-of-domain images, better performance on large objects, and a
lessened reliance on non-maximum suppression. We view ViT-FRCNN as an important
stepping stone toward a pure-transformer solution of complex vision tasks such
as object detection.
</p>
<a href="http://arxiv.org/abs/2012.09958" target="_blank">arXiv:2012.09958</a> [<a href="http://arxiv.org/pdf/2012.09958" target="_blank">pdf</a>]

<h2>Information-Preserving Contrastive Learning for Self-Supervised Representations. (arXiv:2012.09962v1 [cs.LG])</h2>
<h3>Tianhong Li, Lijie Fan, Yuan Yuan, Hao He, Yonglong Tian, Dina Katabi</h3>
<p>Contrastive learning is very effective at learning useful representations
without supervision. Yet contrastive learning has its limitations. It can learn
a shortcut that is irrelevant to the downstream task, and discard relevant
information. Past work has addressed this limitation via custom data
augmentations that eliminate the shortcut. This solution however does not work
for data modalities that are not interpretable by humans, e.g., radio signals.
For such modalities, it is hard for a human to guess which shortcuts may exist
in the signal, or how to alter the radio signals to eliminate the shortcuts.
Even for visual data, sometimes eliminating the shortcut may be undesirable.
The shortcut may be irrelevant to one downstream task but important to another.
In this case, it is desirable to learn a representation that captures both the
shortcut information and the information relevant to the other downstream task.
This paper presents information-preserving contrastive learning (IPCL), a new
framework for unsupervised representation learning that preserves relevant
information even in the presence of shortcuts. We empirically show that IPCL
addresses the above problems and outperforms contrastive learning on radio
signals and learning RGB data representation with different features that
support different downstream tasks.
</p>
<a href="http://arxiv.org/abs/2012.09962" target="_blank">arXiv:2012.09962</a> [<a href="http://arxiv.org/pdf/2012.09962" target="_blank">pdf</a>]

<h2>Relightable 3D Head Portraits from a Smartphone Video. (arXiv:2012.09963v1 [cs.CV])</h2>
<h3>Artem Sevastopolsky, Savva Ignatiev, Gonzalo Ferrer, Evgeny Burnaev, Victor Lempitsky</h3>
<p>In this work, a system for creating a relightable 3D portrait of a human head
is presented. Our neural pipeline operates on a sequence of frames captured by
a smartphone camera with the flash blinking (flash-no flash sequence). A coarse
point cloud reconstructed via structure-from-motion software and multi-view
denoising is then used as a geometric proxy. Afterwards, a deep rendering
network is trained to regress dense albedo, normals, and environmental lighting
maps for arbitrary new viewpoints. Effectively, the proxy geometry and the
rendering network constitute a relightable 3D portrait model, that can be
synthesized from an arbitrary viewpoint and under arbitrary lighting, e.g.
directional light, point light, or an environment map. The model is fitted to
the sequence of frames with human face-specific priors that enforce the
plausibility of albedo-lighting decomposition and operates at the interactive
frame rate. We evaluate the performance of the method under varying lighting
conditions and at the extrapolated viewpoints and compare with existing
relighting methods.
</p>
<a href="http://arxiv.org/abs/2012.09963" target="_blank">arXiv:2012.09963</a> [<a href="http://arxiv.org/pdf/2012.09963" target="_blank">pdf</a>]

<h2>Predicting Decisions in Language Based Persuasion Games. (arXiv:2012.09966v1 [cs.AI])</h2>
<h3>Reut Apel, Ido Erev, Roi Reichart, Moshe Tennenholtz</h3>
<p>Sender-receiver interactions, and specifically persuasion games, are widely
researched in economic modeling and artificial intelligence, and serve as a
solid foundation for powerful applications. However, in the classic persuasion
games setting, the messages sent from the expert to the decision-maker are
abstract or well-structured application-specific signals rather than natural
(human) language messages, although natural language is a very common
communication signal in real-world persuasion setups. This paper addresses the
use of natural language in persuasion games, exploring its impact on the
decisions made by the players and aiming to construct effective models for the
prediction of these decisions. For this purpose, we conduct an online repeated
interaction experiment. At each trial of the interaction, an informed expert
aims to sell an uninformed decision-maker a vacation in a hotel, by sending her
a review that describes the hotel. While the expert is exposed to several
scored reviews, the decision-maker observes only the single review sent by the
expert, and her payoff in case she chooses to take the hotel is a random draw
from the review score distribution available to the expert only. The expert's
payoff, in turn, depends on the number of times the decision-maker chooses the
hotel. We consider a number of modeling approaches for this setup, differing
from each other in the model type (deep neural network (DNN) vs. linear
classifier), the type of features used by the model (textual, behavioral or
both) and the source of the textual features (DNN-based vs. hand-crafted). Our
results demonstrate that given a prefix of the interaction sequence, our models
can predict the future decisions of the decision-maker, particularly when a
sequential modeling approach and hand-crafted textual features are applied.
</p>
<a href="http://arxiv.org/abs/2012.09966" target="_blank">arXiv:2012.09966</a> [<a href="http://arxiv.org/pdf/2012.09966" target="_blank">pdf</a>]

<h2>High Dimensional Level Set Estimation with Bayesian Neural Network. (arXiv:2012.09973v1 [stat.ML])</h2>
<h3>Huong Ha, Sunil Gupta, Santu Rana, Svetha Venkatesh</h3>
<p>Level Set Estimation (LSE) is an important problem with applications in
various fields such as material design, biotechnology, machine operational
testing, etc. Existing techniques suffer from the scalability issue, that is,
these methods do not work well with high dimensional inputs. This paper
proposes novel methods to solve the high dimensional LSE problems using
Bayesian Neural Networks. In particular, we consider two types of LSE problems:
(1) \textit{explicit} LSE problem where the threshold level is a fixed
user-specified value, and, (2) \textit{implicit} LSE problem where the
threshold level is defined as a percentage of the (unknown) maximum of the
objective function. For each problem, we derive the corresponding theoretic
information based acquisition function to sample the data points so as to
maximally increase the level set accuracy. Furthermore, we also analyse the
theoretical time complexity of our proposed acquisition functions, and suggest
a practical methodology to efficiently tune the network hyper-parameters to
achieve high model accuracy. Numerical experiments on both synthetic and
real-world datasets show that our proposed method can achieve better results
compared to existing state-of-the-art approaches.
</p>
<a href="http://arxiv.org/abs/2012.09973" target="_blank">arXiv:2012.09973</a> [<a href="http://arxiv.org/pdf/2012.09973" target="_blank">pdf</a>]

<h2>Handling uncertainty using features from pathology: opportunities in primary care data for developing high risk cancer survival methods. (arXiv:2012.09976v1 [cs.LG])</h2>
<h3>Goce Ristanoski, Jon Emery, Javiera Martinez-Gutierrez, Damien Mccarthy, Uwe Aickelin</h3>
<p>More than 144 000 Australians were diagnosed with cancer in 2019. The
majority will first present to their GP symptomatically, even for cancer for
which screening programs exist. Diagnosing cancer in primary care is
challenging due to the non-specific nature of cancer symptoms and its low
prevalence. Understanding the epidemiology of cancer symptoms and patterns of
presentation in patient's medical history from primary care data could be
important to improve earlier detection and cancer outcomes. As past medical
data about a patient can be incomplete, irregular or missing, this creates
additional challenges when attempting to use the patient's history for any new
diagnosis. Our research aims to investigate the opportunities in a patient's
pathology history available to a GP, initially focused on the results within
the frequently ordered full blood count to determine relevance to a future
high-risk cancer prognosis, and treatment outcome. We investigated how past
pathology test results can lead to deriving features that can be used to
predict cancer outcomes, with emphasis on patients at risk of not surviving the
cancer within 2-year period. This initial work focuses on patients with lung
cancer, although the methodology can be applied to other types of cancer and
other data within the medical record. Our findings indicate that even in cases
of incomplete or obscure patient history, hematological measures can be useful
in generating features relevant for predicting cancer risk and survival. The
results strongly indicate to add the use of pathology test data for potential
high-risk cancer diagnosis, and the utilize additional pathology metrics or
other primary care datasets even more for similar purposes.
</p>
<a href="http://arxiv.org/abs/2012.09976" target="_blank">arXiv:2012.09976</a> [<a href="http://arxiv.org/pdf/2012.09976" target="_blank">pdf</a>]

<h2>Unsupervised clustering of coral reef bioacoustics. (arXiv:2012.09982v1 [cs.LG])</h2>
<h3>Emma Ozanich, Aaron Thode, Peter Gerstoft, Lauren A. Freeman, Simon Freeman</h3>
<p>An unsupervised process is described for clustering automatic detections in
an acoustically active coral reef soundscape. First, acoustic metrics were
extracted from spectrograms and timeseries of each detection based on observed
properties of signal types and classified using unsupervised clustering
methods. Then, deep embedded clustering (DEC) was applied to fixed-length power
spectrograms of each detection to learn features and clusters. The clustering
methods were compared on simulated bioacoustic signals for fish calls and whale
song units with randomly varied signal parameters and additive white noise.
Overlap and density of the handpicked features led to reduced accuracy for
unsupervised clustering methods. DEC clustering identified clusters with fish
calls, whale song, and events with simultaneous fish calls and whale song, but
accuracy was reduced when the class sizes were imbalanced. Both clustering
approaches were applied to acoustic events detected on directional autonomous
seafloor acoustic recorder (DASAR) sensors on a Hawaiian coral reef in
February-March 2020. Unsupervised clustering of handpicked features did not
distinguish fish calls from whale song. DEC had high recall and correctly
classified a majority of whale song. Manual labels indicated a class imbalance
between fish calls and whale song at a 3-to-1 ratio, likely leading to reduced
DEC clustering accuracy.
</p>
<a href="http://arxiv.org/abs/2012.09982" target="_blank">arXiv:2012.09982</a> [<a href="http://arxiv.org/pdf/2012.09982" target="_blank">pdf</a>]

<h2>Objectron: A Large Scale Dataset of Object-Centric Videos in the Wild with Pose Annotations. (arXiv:2012.09988v1 [cs.CV])</h2>
<h3>Adel Ahmadyan, Liangkai Zhang, Jianing Wei, Artsiom Ablavatski, Matthias Grundmann</h3>
<p>3D object detection has recently become popular due to many applications in
robotics, augmented reality, autonomy, and image retrieval. We introduce the
Objectron dataset to advance the state of the art in 3D object detection and
foster new research and applications, such as 3D object tracking, view
synthesis, and improved 3D shape representation. The dataset contains
object-centric short videos with pose annotations for nine categories and
includes 4 million annotated images in 14,819 annotated videos. We also propose
a new evaluation metric, 3D Intersection over Union, for 3D object detection.
We demonstrate the usefulness of our dataset in 3D object detection tasks by
providing baseline models trained on this dataset. Our dataset and evaluation
source code are available online at this http URL
</p>
<a href="http://arxiv.org/abs/2012.09988" target="_blank">arXiv:2012.09988</a> [<a href="http://arxiv.org/pdf/2012.09988" target="_blank">pdf</a>]

<h2>Representation-Free Model Predictive Control for Dynamic Motions in Quadrupeds. (arXiv:2012.10002v1 [cs.RO])</h2>
<h3>Yanran Ding, Abhishek Pandala, Chuanzheng Li, Young-Ha Shin, Hae-Won Park</h3>
<p>This paper presents a novel Representation-Free Model Predictive Control
(RF-MPC) framework for controlling various dynamic motions of a quadrupedal
robot in three dimensional (3D) space. Our formulation directly represents the
rotational dynamics using the rotation matrix, which liberates us from the
issues associated with the use of Euler angles and quaternion as the
orientation representations. With a variation-based linearization scheme and a
carefully constructed cost function, the MPC control law is transcribed to the
standard Quadratic Program (QP) form. The MPC controller can operate at
real-time rates of 250 Hz on a quadruped robot. Experimental results including
periodic quadrupedal gaits and a controlled backflip validate that our control
strategy could stabilize dynamic motions that involve singularity in 3D
maneuvers.
</p>
<a href="http://arxiv.org/abs/2012.10002" target="_blank">arXiv:2012.10002</a> [<a href="http://arxiv.org/pdf/2012.10002" target="_blank">pdf</a>]

<h2>ErGAN: Generative Adversarial Networks for Entity Resolution. (arXiv:2012.10004v1 [cs.LG])</h2>
<h3>Jingyu Shao, Qing Wang, Asiri Wijesinghe, Erhard Rahm</h3>
<p>Entity resolution targets at identifying records that represent the same
real-world entity from one or more datasets. A major challenge in
learning-based entity resolution is how to reduce the label cost for training.
Due to the quadratic nature of record pair comparison, labeling is a costly
task that often requires a significant effort from human experts. Inspired by
recent advances of generative adversarial network (GAN), we propose a novel
deep learning method, called ErGAN, to address the challenge. ErGAN consists of
two key components: a label generator and a discriminator which are optimized
alternatively through adversarial learning. To alleviate the issues of
overfitting and highly imbalanced distribution, we design two novel modules for
diversity and propagation, which can greatly improve the model generalization
power. We have conducted extensive experiments to empirically verify the
labeling and learning efficiency of ErGAN. The experimental results show that
ErGAN beats the state-of-the-art baselines, including unsupervised,
semi-supervised, and unsupervised learning methods.
</p>
<a href="http://arxiv.org/abs/2012.10004" target="_blank">arXiv:2012.10004</a> [<a href="http://arxiv.org/pdf/2012.10004" target="_blank">pdf</a>]

<h2>Online Connectivity-aware Dynamic Deployment for Heterogeneous Multi-Robot Systems. (arXiv:2012.10008v1 [cs.RO])</h2>
<h3>Chendi Lin, Wenhao Luo, Katia Sycara</h3>
<p>In this paper, we consider the dynamic multi-robot distribution problem where
a heterogeneous group of networked robots is tasked to spread out and
simultaneously move towards multiple moving task areas while maintaining
connectivity. The heterogeneity of the system is characterized by various
categories of units and each robot carries different numbers of units per
category representing heterogeneous capabilities. Every task area with
different importance demands a total number of units contributed by all of the
robots within its area. Moreover, we assume the importance and the total number
of units requested from each task area is initially unknown. The robots need
first to explore, i.e., reach those areas, and then be allocated to the tasks
so to fulfill the requirements. The multi-robot distribution problem is
formulated as designing controllers to distribute the robots that maximize the
overall task fulfillment while minimizing the traveling costs in presence of
connectivity constraints. We propose a novel connectivity-aware multi-robot
redistribution approach that accounts for dynamic task allocation and
connectivity maintenance for a heterogeneous robot team. Such an approach could
generate sub-optimal robot controllers so that the amount of total unfulfilled
requirements of the tasks weighted by their importance is minimized and robots
stay connected at all times. Simulation and numerical results are provided to
demonstrate the effectiveness of the proposed approaches.
</p>
<a href="http://arxiv.org/abs/2012.10008" target="_blank">arXiv:2012.10008</a> [<a href="http://arxiv.org/pdf/2012.10008" target="_blank">pdf</a>]

<h2>Flow-based Generative Models for Learning Manifold to Manifold Mappings. (arXiv:2012.10013v1 [cs.CV])</h2>
<h3>Xingjian Zhen, Rudrasis Chakraborty, Liu Yang, Vikas Singh</h3>
<p>Many measurements or observations in computer vision and machine learning
manifest as non-Euclidean data. While recent proposals (like spherical CNN)
have extended a number of deep neural network architectures to manifold-valued
data, and this has often provided strong improvements in performance, the
literature on generative models for manifold data is quite sparse. Partly due
to this gap, there are also no modality transfer/translation models for
manifold-valued data whereas numerous such methods based on generative models
are available for natural images. This paper addresses this gap, motivated by a
need in brain imaging -- in doing so, we expand the operating range of certain
generative models (as well as generative models for modality transfer) from
natural images to images with manifold-valued measurements. Our main result is
the design of a two-stream version of GLOW (flow-based invertible generative
models) that can synthesize information of a field of one type of
manifold-valued measurements given another. On the theoretical side, we
introduce three kinds of invertible layers for manifold-valued data, which are
not only analogous to their functionality in flow-based generative models
(e.g., GLOW) but also preserve the key benefits (determinants of the Jacobian
are easy to calculate). For experiments, on a large dataset from the Human
Connectome Project (HCP), we show promising results where we can reliably and
accurately reconstruct brain images of a field of orientation distribution
functions (ODF) from diffusion tensor images (DTI), where the latter has a
$5\times$ faster acquisition time but at the expense of worse angular
resolution.
</p>
<a href="http://arxiv.org/abs/2012.10013" target="_blank">arXiv:2012.10013</a> [<a href="http://arxiv.org/pdf/2012.10013" target="_blank">pdf</a>]

<h2>Self-supervised Learning with Fully Convolutional Networks. (arXiv:2012.10017v1 [cs.CV])</h2>
<h3>Zhengeng Yang, Hongshan Yu, Yong He, Zhi-Hong Mao, Ajmal Mian</h3>
<p>Although deep learning based methods have achieved great success in many
computer vision tasks, their performance relies on a large number of densely
annotated samples that are typically difficult to obtain. In this paper, we
focus on the problem of learning representation from unlabeled data for
semantic segmentation. Inspired by two patch-based methods, we develop a novel
self-supervised learning framework by formulating the Jigsaw Puzzle problem as
a patch-wise classification process and solving it with a fully convolutional
network. By learning to solve a Jigsaw Puzzle problem with 25 patches and
transferring the learned features to semantic segmentation task on Cityscapes
dataset, we achieve a 5.8 percentage point improvement over the baseline model
that initialized from random values. Moreover, experiments show that our
self-supervised learning method can be applied to different datasets and
models. In particular, we achieved competitive performance with the
state-of-the-art methods on the PASCAL VOC2012 dataset using significant fewer
training images.
</p>
<a href="http://arxiv.org/abs/2012.10017" target="_blank">arXiv:2012.10017</a> [<a href="http://arxiv.org/pdf/2012.10017" target="_blank">pdf</a>]

<h2>EVA: Generating Longitudinal Electronic Health Records Using Conditional Variational Autoencoders. (arXiv:2012.10020v1 [cs.LG])</h2>
<h3>Siddharth Biswal, Soumya Ghosh, Jon Duke, Bradley Malin, Walter Stewart, Jimeng Sun</h3>
<p>Researchers require timely access to real-world longitudinal electronic
health records (EHR) to develop, test, validate, and implement machine learning
solutions that improve the quality and efficiency of healthcare. In contrast,
health systems value deeply patient privacy and data security. De-identified
EHRs do not adequately address the needs of health systems, as de-identified
data are susceptible to re-identification and its volume is also limited.
Synthetic EHRs offer a potential solution. In this paper, we propose EHR
Variational Autoencoder (EVA) for synthesizing sequences of discrete EHR
encounters (e.g., clinical visits) and encounter features (e.g., diagnoses,
medications, procedures). We illustrate that EVA can produce realistic EHR
sequences, account for individual differences among patients, and can be
conditioned on specific disease conditions, thus enabling disease-specific
studies. We design efficient, accurate inference algorithms by combining
stochastic gradient Markov Chain Monte Carlo with amortized variational
inference. We assess the utility of the methods on large real-world EHR
repositories containing over 250, 000 patients. Our experiments, which include
user studies with knowledgeable clinicians, indicate the generated EHR
sequences are realistic. We confirmed the performance of predictive models
trained on the synthetic data are similar with those trained on real EHRs.
Additionally, our findings indicate that augmenting real data with synthetic
EHRs results in the best predictive performance - improving the best baseline
by as much as 8% in top-20 recall.
</p>
<a href="http://arxiv.org/abs/2012.10020" target="_blank">arXiv:2012.10020</a> [<a href="http://arxiv.org/pdf/2012.10020" target="_blank">pdf</a>]

<h2>Leveraging Meta-path Contexts for Classification in Heterogeneous Information Networks. (arXiv:2012.10024v1 [cs.LG])</h2>
<h3>Xiang Li, Danhao Ding, Ben Kao, Yizhou Sun, Nikos Mamoulis</h3>
<p>A heterogeneous information network (HIN) has as vertices objects of
different types and as edges the relations between objects, which are also of
various types. We study the problem of classifying objects in HINs. Most
existing methods perform poorly when given scarce labeled objects as training
sets, and methods that improve classification accuracy under such scenarios are
often computationally expensive. To address these problems, we propose ConCH, a
graph neural network model. ConCH formulates the classification problem as a
multi-task learning problem that combines semi-supervised learning with
self-supervised learning to learn from both labeled and unlabeled data. ConCH
employs meta-paths, which are sequences of object types that capture semantic
relationships between objects. Based on meta-paths, it considers two sources of
information for an object x: (1) Meta-path-based neighbors of x are retrieved
and ranked, and the top-k neighbors are retained. (2) The meta-path instances
of x to its selected neighbors are used to derive meta-path-based contexts.
ConCH utilizes the above information to co-derive object embeddings and context
embeddings via graph convolution. It also uses the attention mechanism to fuse
the embeddings of x generated from various meta-paths to obtain x's final
embedding. We conduct extensive experiments to evaluate the performance of
ConCH against other 14 classification methods. Our results show that ConCH is
an effective and efficient method for HIN classification.
</p>
<a href="http://arxiv.org/abs/2012.10024" target="_blank">arXiv:2012.10024</a> [<a href="http://arxiv.org/pdf/2012.10024" target="_blank">pdf</a>]

<h2>Robustness to Spurious Correlations in Text Classification via Automatically Generated Counterfactuals. (arXiv:2012.10040v1 [cs.LG])</h2>
<h3>Zhao Wang, Aron Culotta</h3>
<p>Spurious correlations threaten the validity of statistical classifiers. While
model accuracy may appear high when the test data is from the same distribution
as the training data, it can quickly degrade when the test distribution
changes. For example, it has been shown that classifiers perform poorly when
humans make minor modifications to change the label of an example. One solution
to increase model reliability and generalizability is to identify causal
associations between features and classes. In this paper, we propose to train a
robust text classifier by augmenting the training data with automatically
generated counterfactual data. We first identify likely causal features using a
statistical matching approach. Next, we generate counterfactual samples for the
original training data by substituting causal features with their antonyms and
then assigning opposite labels to the counterfactual samples. Finally, we
combine the original data and counterfactual data to train a robust classifier.
Experiments on two classification tasks show that a traditional classifier
trained on the original data does very poorly on human-generated counterfactual
samples (e.g., 10%-37% drop in accuracy). However, the classifier trained on
the combined data is more robust and performs well on both the original test
data and the counterfactual test data (e.g., 12%-25% increase in accuracy
compared with the traditional classifier). Detailed analysis shows that the
robust classifier makes meaningful and trustworthy predictions by emphasizing
causal features and de-emphasizing non-causal features.
</p>
<a href="http://arxiv.org/abs/2012.10040" target="_blank">arXiv:2012.10040</a> [<a href="http://arxiv.org/pdf/2012.10040" target="_blank">pdf</a>]

<h2>3D Object Classification on Partial Point Clouds: A Practical Perspective. (arXiv:2012.10042v1 [cs.CV])</h2>
<h3>Zelin Xu, Ke Chen, Tong Zhang, C. L. Philip Chen, Kui Jia</h3>
<p>A point cloud is a popular shape representation adopted in 3D object
classification, which covers the whole surface of an object and is usually well
aligned. However, such an assumption can be invalid in practice, as point
clouds collected in real-world scenarios are typically scanned from visible
object parts observed under arbitrary SO(3) viewpoint, which are thus
incomplete due to self and inter-object occlusion. In light of this, this paper
introduces a practical setting to classify partial point clouds of object
instances under any poses. Compared to the classification of complete object
point clouds, such a problem is made more challenging in view of geometric
similarities of local shape across object classes and intra-class
dissimilarities of geometries restricted by their observation view. We consider
that specifying the location of partial point clouds on their object surface is
essential to alleviate suffering from the aforementioned challenges, which can
be solved via an auxiliary task of 6D object pose estimation. To this end, a
novel algorithm in an alignment-classification manner is proposed in this
paper, which consists of an alignment module predicting object pose for the
rigid transformation of visible point clouds to their canonical pose and a
typical point classifier such as PointNet++ and DGCNN. Experiment results on
the popular ModelNet40 and ScanNet datasets, which are adapted to a single-view
partial setting, demonstrate the proposed method can outperform three
alternative schemes extended from representative point cloud classifiers for
complete point clouds.
</p>
<a href="http://arxiv.org/abs/2012.10042" target="_blank">arXiv:2012.10042</a> [<a href="http://arxiv.org/pdf/2012.10042" target="_blank">pdf</a>]

<h2>Content Masked Loss: Human-Like Brush Stroke Planning in a Reinforcement Learning Painting Agent. (arXiv:2012.10043v1 [cs.CV])</h2>
<h3>Peter Schaldenbrand, Jean Oh</h3>
<p>The objective of most Reinforcement Learning painting agents is to minimize
the loss between a target image and the paint canvas. Human painter artistry
emphasizes important features of the target image rather than simply
reproducing it (DiPaola 2007). Using adversarial or L2 losses in the RL
painting models, although its final output is generally a work of finesse,
produces a stroke sequence that is vastly different from that which a human
would produce since the model does not have knowledge about the abstract
features in the target image. In order to increase the human-like planning of
the model without the use of expensive human data, we introduce a new loss
function for use with the model's reward function: Content Masked Loss. In the
context of robot painting, Content Masked Loss employs an object detection
model to extract features which are used to assign higher weight to regions of
the canvas that a human would find important for recognizing content. The
results, based on 332 human evaluators, show that the digital paintings
produced by our Content Masked model show detectable subject matter earlier in
the stroke sequence than existing methods without compromising on the quality
of the final painting.
</p>
<a href="http://arxiv.org/abs/2012.10043" target="_blank">arXiv:2012.10043</a> [<a href="http://arxiv.org/pdf/2012.10043" target="_blank">pdf</a>]

<h2>On the eigenvector bias of Fourier feature networks: From regression to solving multi-scale PDEs with physics-informed neural networks. (arXiv:2012.10047v1 [cs.LG])</h2>
<h3>Sifan Wang, Hanwen Wang, Paris Perdikaris</h3>
<p>Physics-informed neural networks (PINNs) are demonstrating remarkable promise
in integrating physical models with gappy and noisy observational data, but
they still struggle in cases where the target functions to be approximated
exhibit high-frequency or multi-scale features. In this work we investigate
this limitation through the lens of Neural Tangent Kernel (NTK) theory and
elucidate how PINNs are biased towards learning functions along the dominant
eigen-directions of their limiting NTK. Using this observation, we construct
novel architectures that employ spatio-temporal and multi-scale random Fourier
features, and justify how such coordinate embedding layers can lead to robust
and accurate PINN models. Numerical examples are presented for several
challenging cases where conventional PINN models fail, including wave
propagation and reaction-diffusion dynamics, illustrating how the proposed
methods can be used to effectively tackle both forward and inverse problems
involving partial differential equations with multi-scale behavior. All code an
data accompanying this manuscript will be made publicly available at
\url{https://github.com/PredictiveIntelligenceLab/MultiscalePINNs}.
</p>
<a href="http://arxiv.org/abs/2012.10047" target="_blank">arXiv:2012.10047</a> [<a href="http://arxiv.org/pdf/2012.10047" target="_blank">pdf</a>]

<h2>Transfer Learning Based Automatic Model Creation Tool For Resource Constraint Devices. (arXiv:2012.10056v1 [cs.LG])</h2>
<h3>Karthik Bhat, Manan Bhandari, ChangSeok Oh, Sujin Kim, Jeeho Yoo</h3>
<p>With the enhancement of Machine Learning, many tools are being designed to
assist developers to easily create their Machine Learning models. In this
paper, we propose a novel method for auto creation of such custom models for
constraint devices using transfer learning without the need to write any
machine learning code. We share the architecture of our automatic model
creation tool and the CNN Model created by it using pretrained models such as
YAMNet and MobileNetV2 as feature extractors. Finally, we demonstrate accuracy
and memory footprint of the model created from the tool by creating an
Automatic Image and Audio classifier and report the results of our experiments
using Stanford Cars and ESC-50 dataset.
</p>
<a href="http://arxiv.org/abs/2012.10056" target="_blank">arXiv:2012.10056</a> [<a href="http://arxiv.org/pdf/2012.10056" target="_blank">pdf</a>]

<h2>PointINet: Point Cloud Frame Interpolation Network. (arXiv:2012.10066v1 [cs.CV])</h2>
<h3>Fan Lu, Guang Chen, Sanqing Qu, Zhijun Li, Yinlong Liu, Alois Knoll</h3>
<p>LiDAR point cloud streams are usually sparse in time dimension, which is
limited by hardware performance. Generally, the frame rates of mechanical LiDAR
sensors are 10 to 20 Hz, which is much lower than other commonly used sensors
like cameras. To overcome the temporal limitations of LiDAR sensors, a novel
task named Point Cloud Frame Interpolation is studied in this paper. Given two
consecutive point cloud frames, Point Cloud Frame Interpolation aims to
generate intermediate frame(s) between them. To achieve that, we propose a
novel framework, namely Point Cloud Frame Interpolation Network (PointINet).
Based on the proposed method, the low frame rate point cloud streams can be
upsampled to higher frame rates. We start by estimating bi-directional 3D scene
flow between the two point clouds and then warp them to the given time step
based on the 3D scene flow. To fuse the two warped frames and generate
intermediate point cloud(s), we propose a novel learning-based points fusion
module, which simultaneously takes two warped point clouds into consideration.
We design both quantitative and qualitative experiments to evaluate the
performance of the point cloud frame interpolation method and extensive
experiments on two large scale outdoor LiDAR datasets demonstrate the
effectiveness of the proposed PointINet. Our code is available at
https://github.com/ispc-lab/PointINet.git.
</p>
<a href="http://arxiv.org/abs/2012.10066" target="_blank">arXiv:2012.10066</a> [<a href="http://arxiv.org/pdf/2012.10066" target="_blank">pdf</a>]

<h2>Fairness and Accuracy in Federated Learning. (arXiv:2012.10069v1 [cs.LG])</h2>
<h3>Wei Huang, Tianrui Li, Dexian Wang, Shengdong Du, Junbo Zhang</h3>
<p>In the federated learning setting, multiple clients jointly train a model
under the coordination of the central server, while the training data is kept
on the client to ensure privacy. Normally, inconsistent distribution of data
across different devices in a federated network and limited communication
bandwidth between end devices impose both statistical heterogeneity and
expensive communication as major challenges for federated learning. This paper
proposes an algorithm to achieve more fairness and accuracy in federated
learning (FedFa). It introduces an optimization scheme that employs a double
momentum gradient, thereby accelerating the convergence rate of the model. An
appropriate weight selection algorithm that combines the information quantity
of training accuracy and training frequency to measure the weights is proposed.
This procedure assists in addressing the issue of unfairness in federated
learning due to preferences for certain clients. Our results show that the
proposed FedFa algorithm outperforms the baseline algorithm in terms of
accuracy and fairness.
</p>
<a href="http://arxiv.org/abs/2012.10069" target="_blank">arXiv:2012.10069</a> [<a href="http://arxiv.org/pdf/2012.10069" target="_blank">pdf</a>]

<h2>TDN: Temporal Difference Networks for Efficient Action Recognition. (arXiv:2012.10071v1 [cs.CV])</h2>
<h3>Limin Wang, Zhan Tong, Bin Ji, Gangshan Wu</h3>
<p>Temporal modeling still remains challenging for action recognition in videos.
To mitigate this issue, this paper presents a new video architecture, termed as
Temporal Difference Network (TDN), with a focus on capturing multi-scale
temporal information for efficient action recognition. The core of our TDN is
to devise an efficient temporal module (TDM) by explicitly leveraging a
temporal difference operator, and systematically assess its effect on
short-term and long-term motion modeling. To fully capture temporal information
over the entire video, our TDN is established with a two-level difference
modeling paradigm. Specifically, for local motion modeling, temporal difference
over consecutive frames is used to supply 2D CNNs with finer motion pattern,
while for global motion modeling, temporal difference across segments is
incorporated to capture long-range structure for motion feature excitation. TDN
provides a simple and principled temporal modeling framework and could be
instantiated with the existing CNNs at a small extra computational cost. Our
TDN presents a new state of the art on the Something-Something V1 and V2
datasets and is on par with the best performance on the Kinetics-400 dataset.
In addition, we conduct in-depth ablation studies and plot the visualization
results of our TDN, hopefully providing insightful analysis on temporal
difference operation. We release the code at https://github.com/MCG-NJU/TDN.
</p>
<a href="http://arxiv.org/abs/2012.10071" target="_blank">arXiv:2012.10071</a> [<a href="http://arxiv.org/pdf/2012.10071" target="_blank">pdf</a>]

<h2>Semantics and explanation: why counterfactual explanations produce adversarial examples in deep neural networks. (arXiv:2012.10076v1 [cs.AI])</h2>
<h3>Kieran Browne, Ben Swift</h3>
<p>Recent papers in explainable AI have made a compelling case for
counterfactual modes of explanation. While counterfactual explanations appear
to be extremely effective in some instances, they are formally equivalent to
adversarial examples. This presents an apparent paradox for explainability
researchers: if these two procedures are formally equivalent, what accounts for
the explanatory divide apparent between counterfactual explanations and
adversarial examples? We resolve this paradox by placing emphasis back on the
semantics of counterfactual expressions. Producing satisfactory explanations
for deep learning systems will require that we find ways to interpret the
semantics of hidden layer representations in deep neural networks.
</p>
<a href="http://arxiv.org/abs/2012.10076" target="_blank">arXiv:2012.10076</a> [<a href="http://arxiv.org/pdf/2012.10076" target="_blank">pdf</a>]

<h2>AU-Guided Unsupervised Domain Adaptive Facial Expression Recognition. (arXiv:2012.10078v1 [cs.CV])</h2>
<h3>Kai Wang, Yuxin Gu, Xiaojiang Peng, Baigui Sun, Hao Li</h3>
<p>The domain diversities including inconsistent annotation and varied image
collection conditions inevitably exist among different facial expression
recognition (FER) datasets, which pose an evident challenge for adapting the
FER model trained on one dataset to another one. Recent works mainly focus on
domain-invariant deep feature learning with adversarial learning mechanism,
ignoring the sibling facial action unit (AU) detection task which has obtained
great progress. Considering AUs objectively determine facial expressions, this
paper proposes an AU-guided unsupervised Domain Adaptive FER (AdaFER)
framework. In AdaFER, we first leverage an advanced model for AU detection on
both source and target domain. Then, we compare the AU results to perform
AU-guided annotating, i.e., target faces that own the same AUs with source
faces would inherit the labels from source domain. Meanwhile, to achieve
domain-invariant compact features, we utilize an AU-guided triplet training
which randomly collects anchor-positive-negative triplets on both domains with
AUs. We conduct extensive experiments on several popular benchmarks and show
that AdaFER achieves state-of-the-art results on all the benchmarks.
</p>
<a href="http://arxiv.org/abs/2012.10078" target="_blank">arXiv:2012.10078</a> [<a href="http://arxiv.org/pdf/2012.10078" target="_blank">pdf</a>]

<h2>A Surrogate Lagrangian Relaxation-based Model Compression for Deep Neural Networks. (arXiv:2012.10079v1 [cs.LG])</h2>
<h3>Deniz Gurevin, Shanglin Zhou, Lynn Pepin, Bingbing Li, Mikhail Bragin, Caiwen Ding, Fei Miao</h3>
<p>Network pruning is a widely used technique to reduce computation cost and
model size for deep neural networks. However, the typical three-stage pipeline,
i.e., training, pruning and retraining (fine-tuning) significantly increases
the overall training trails. For instance, the retraining process could take up
to 80 epochs for ResNet-18 on ImageNet, that is 70% of the original model
training trails. In this paper, we develop a systematic weight-pruning
optimization approach based on Surrogate Lagrangian relaxation (SLR), which is
tailored to overcome difficulties caused by the discrete nature of the
weight-pruning problem while ensuring fast convergence. We decompose the
weight-pruning problem into subproblems, which are coordinated by updating
Lagrangian multipliers. Convergence is then accelerated by using quadratic
penalty terms. We evaluate the proposed method on image classification tasks,
i.e., ResNet-18, ResNet-50 and VGG-16 using ImageNet and CIFAR-10, as well as
object detection tasks, i.e., YOLOv3 and YOLOv3-tiny using COCO 2014,
PointPillars using KITTI 2017, and Ultra-Fast-Lane-Detection using TuSimple
lane detection dataset. Numerical testing results demonstrate that with the
adoption of the Surrogate Lagrangian Relaxation method, our SLR-based
weight-pruning optimization approach achieves a high model accuracy even at the
hard-pruning stage without retraining for many epochs, such as on PointPillars
object detection model on KITTI dataset where we achieve 9.44x compression rate
by only retraining for 3 epochs with less than 1% accuracy loss. As the
compression rate increases, SLR starts to perform better than ADMM and the
accuracy gap between them increases. SLR achieves 15.2% better accuracy than
ADMM on PointPillars after pruning under 9.49x compression. Given a limited
budget of retraining epochs, our approach quickly recovers the model accuracy.
</p>
<a href="http://arxiv.org/abs/2012.10079" target="_blank">arXiv:2012.10079</a> [<a href="http://arxiv.org/pdf/2012.10079" target="_blank">pdf</a>]

<h2>Crowd-Driven Mapping, Localization and Planning. (arXiv:2012.10099v1 [cs.RO])</h2>
<h3>Tingxiang Fan, Dawei Wang, Wenxi Liu, Jia Pan</h3>
<p>Navigation in dense crowds is a well-known open problem in robotics with many
challenges in mapping, localization, and planning. Traditional solutions
consider dense pedestrians as passive/active moving obstacles that are the
cause of all troubles: they negatively affect the sensing of static scene
landmarks and must be actively avoided for safety. In this paper, we provide a
new perspective: the crowd flow locally observed can be treated as a sensory
measurement about the surrounding scenario, encoding not only the scene's
traversability but also its social navigation preference. We demonstrate that
even using the crowd-flow measurement alone without any sensing about static
obstacles, our method still accomplishes good results for mapping,
localization, and social-aware planning in dense crowds. Videos of the
experiments are available at https://sites.google.com/view/crowdmapping.
</p>
<a href="http://arxiv.org/abs/2012.10099" target="_blank">arXiv:2012.10099</a> [<a href="http://arxiv.org/pdf/2012.10099" target="_blank">pdf</a>]

<h2>Frequency Consistent Adaptation for Real World Super Resolution. (arXiv:2012.10102v1 [cs.CV])</h2>
<h3>Xiaozhong Ji, Guangpin Tao, Yun Cao, Ying Tai, Tong Lu, Chengjie Wang, Jilin Li, Feiyue Huang</h3>
<p>Recent deep-learning based Super-Resolution (SR) methods have achieved
remarkable performance on images with known degradation. However, these methods
always fail in real-world scene, since the Low-Resolution (LR) images after the
ideal degradation (e.g., bicubic down-sampling) deviate from real source
domain. The domain gap between the LR images and the real-world images can be
observed clearly on frequency density, which inspires us to explictly narrow
the undesired gap caused by incorrect degradation. From this point of view, we
design a novel Frequency Consistent Adaptation (FCA) that ensures the frequency
domain consistency when applying existing SR methods to the real scene. We
estimate degradation kernels from unsupervised images and generate the
corresponding LR images. To provide useful gradient information for kernel
estimation, we propose Frequency Density Comparator (FDC) by distinguishing the
frequency density of images on different scales. Based on the domain-consistent
LR-HR pairs, we train easy-implemented Convolutional Neural Network (CNN) SR
models. Extensive experiments show that the proposed FCA improves the
performance of the SR model under real-world setting achieving state-of-the-art
results with high fidelity and plausible perception, thus providing a novel
effective framework for real-world SR application.
</p>
<a href="http://arxiv.org/abs/2012.10102" target="_blank">arXiv:2012.10102</a> [<a href="http://arxiv.org/pdf/2012.10102" target="_blank">pdf</a>]

<h2>Stable Implementation of Probabilistic ODE Solvers. (arXiv:2012.10106v1 [stat.ML])</h2>
<h3>Nicholas Kr&#xe4;mer, Philipp Hennig</h3>
<p>Probabilistic solvers for ordinary differential equations (ODEs) provide
efficient quantification of numerical uncertainty associated with simulation of
dynamical systems. Their convergence rates have been established by a growing
body of theoretical analysis. However, these algorithms suffer from numerical
instability when run at high order or with small step-sizes -- that is, exactly
in the regime in which they achieve the highest accuracy. The present work
proposes and examines a solution to this problem. It involves three components:
accurate initialisation, a coordinate change preconditioner that makes
numerical stability concerns step-size-independent, and square-root
implementation. Using all three techniques enables numerical computation of
probabilistic solutions of ODEs with algorithms of order up to 11, as
demonstrated on a set of challenging test problems. The resulting rapid
convergence is shown to be competitive to high-order, state-of-the-art,
classical methods. As a consequence, a barrier between analysing probabilistic
ODE solvers and applying them to interesting machine learning problems is
effectively removed.
</p>
<a href="http://arxiv.org/abs/2012.10106" target="_blank">arXiv:2012.10106</a> [<a href="http://arxiv.org/pdf/2012.10106" target="_blank">pdf</a>]

<h2>Hyperspectral Image Semantic Segmentation in Cityscapes. (arXiv:2012.10122v1 [cs.CV])</h2>
<h3>Yuxing Huang, Erqi Huang, Linsen Chen, Shaodi You, Ying Fu, Qiu Shen</h3>
<p>High-resolution hyperspectral images (HSIs) contain the response of each
pixel in different spectral bands, which can be used to effectively distinguish
various objects in complex scenes. While HSI cameras have become low cost,
algorithms based on it has not been well exploited. In this paper, we focus on
a novel topic, semi-supervised semantic segmentation in cityscapes using
HSIs.It is based on the idea that high-resolution HSIs in city scenes contain
rich spectral information, which can be easily associated to semantics without
manual labeling. Therefore, it enables low cost, highly reliable semantic
segmentation in complex scenes.Specifically, in this paper, we introduce a
semi-supervised HSI semantic segmentation network, which utilizes spectral
information to improve the coarse labels to a finer degree.The experimental
results show that our method can obtain highly competitive labels and even have
higher edge fineness than artificial fine labels in some classes. At the same
time, the results also show that the optimized labels can effectively improve
the effect of semantic segmentation. The combination of HSIs and semantic
segmentation proves that HSIs have great potential in high-level visual tasks.
</p>
<a href="http://arxiv.org/abs/2012.10122" target="_blank">arXiv:2012.10122</a> [<a href="http://arxiv.org/pdf/2012.10122" target="_blank">pdf</a>]

<h2>CodeVIO: Visual-Inertial Odometry with Learned Optimizable Dense Depth. (arXiv:2012.10133v1 [cs.CV])</h2>
<h3>Xingxing Zuo, Nathaniel Merrill, Wei Li, Yong Liu, Marc Pollefeys, Guoquan Huang</h3>
<p>In this work, we present a lightweight, tightly-coupled deep depth network
and visual-inertial odometry (VIO) system, which can provide accurate state
estimates and dense depth maps of the immediate surroundings. Leveraging the
proposed lightweight Conditional Variational Autoencoder (CVAE) for depth
inference and encoding, we provide the network with previously marginalized
sparse features from VIO to increase the accuracy of initial depth prediction
and generalization capability. The compact encoded depth maps are then updated
jointly with navigation states in a sliding window estimator in order to
provide the dense local scene geometry. We additionally propose a novel method
to obtain the CVAE's Jacobian which is shown to be more than an order of
magnitude faster than previous works, and we additionally leverage
First-Estimate Jacobian (FEJ) to avoid recalculation. As opposed to previous
works relying on completely dense residuals, we propose to only provide sparse
measurements to update the depth code and show through careful experimentation
that our choice of sparse measurements and FEJs can still significantly improve
the estimated depth maps. Our full system also exhibits state-of-the-art pose
estimation accuracy, and we show that it can run in real-time with
single-thread execution while utilizing GPU acceleration only for the network
and code Jacobian.
</p>
<a href="http://arxiv.org/abs/2012.10133" target="_blank">arXiv:2012.10133</a> [<a href="http://arxiv.org/pdf/2012.10133" target="_blank">pdf</a>]

<h2>Voronoi Progressive Widening: Efficient Online Solvers for Continuous Space MDPs and POMDPs with Provably Optimal Components. (arXiv:2012.10140v1 [cs.LG])</h2>
<h3>Michael H. Lim, Claire J. Tomlin, Zachary N. Sunberg</h3>
<p>Markov decision processes (MDPs) and partially observable MDPs (POMDPs) can
effectively represent complex real-world decision and control problems.
However, continuous space MDPs and POMDPs, i.e. those having continuous state,
action and observation spaces, are extremely difficult to solve, and there are
few online algorithms with convergence guarantees. This paper introduces
Voronoi Progressive Widening (VPW), a general technique to modify tree search
algorithms to effectively handle continuous or hybrid action spaces, and
proposes and evaluates three continuous space solvers: VOSS, VOWSS, and
VOMCPOW. VOSS and VOWSS are theoretical tools based on sparse sampling and
Voronoi optimistic optimization designed to justify VPW-based online solvers.
While previous algorithms have enjoyed convergence guarantees for problems with
continuous state and observation spaces, VOWSS is the first with global
convergence guarantees for problems that additionally have continuous action
spaces. VOMCPOW is a versatile and efficient VPW-based algorithm that
consistently outperforms POMCPOW and BOMCP in several simulation experiments.
</p>
<a href="http://arxiv.org/abs/2012.10140" target="_blank">arXiv:2012.10140</a> [<a href="http://arxiv.org/pdf/2012.10140" target="_blank">pdf</a>]

<h2>MASSIVE: Tractable and Robust Bayesian Learning of Many-Dimensional Instrumental Variable Models. (arXiv:2012.10141v1 [stat.ML])</h2>
<h3>Ioan Gabriel Bucur, Tom Claassen, Tom Heskes</h3>
<p>The recent availability of huge, many-dimensional data sets, like those
arising from genome-wide association studies (GWAS), provides many
opportunities for strengthening causal inference. One popular approach is to
utilize these many-dimensional measurements as instrumental variables
(instruments) for improving the causal effect estimate between other pairs of
variables. Unfortunately, searching for proper instruments in a
many-dimensional set of candidates is a daunting task due to the intractable
model space and the fact that we cannot directly test which of these candidates
are valid, so most existing search methods either rely on overly stringent
modeling assumptions or fail to capture the inherent model uncertainty in the
selection process. We show that, as long as at least some of the candidates are
(close to) valid, without knowing a priori which ones, they collectively still
pose enough restrictions on the target interaction to obtain a reliable causal
effect estimate. We propose a general and efficient causal inference algorithm
that accounts for model uncertainty by performing Bayesian model averaging over
the most promising many-dimensional instrumental variable models, while at the
same time employing weaker assumptions regarding the data generating process.
We showcase the efficiency, robustness and predictive performance of our
algorithm through experimental results on both simulated and real-world data.
</p>
<a href="http://arxiv.org/abs/2012.10141" target="_blank">arXiv:2012.10141</a> [<a href="http://arxiv.org/pdf/2012.10141" target="_blank">pdf</a>]

<h2>Hierarchical principles of embodied reinforcement learning: A review. (arXiv:2012.10147v1 [cs.AI])</h2>
<h3>Manfred Eppe, Christian Gumbsch, Matthias Kerzel, Phuong D.H. Nguyen, Martin V. Butz, Stefan Wermter</h3>
<p>Cognitive Psychology and related disciplines have identified several critical
mechanisms that enable intelligent biological agents to learn to solve complex
problems. There exists pressing evidence that the cognitive mechanisms that
enable problem-solving skills in these species build on hierarchical mental
representations. Among the most promising computational approaches to provide
comparable learning-based problem-solving abilities for artificial agents and
robots is hierarchical reinforcement learning. However, so far the existing
computational approaches have not been able to equip artificial agents with
problem-solving abilities that are comparable to intelligent animals, including
human and non-human primates, crows, or octopuses. Here, we first survey the
literature in Cognitive Psychology, and related disciplines, and find that many
important mental mechanisms involve compositional abstraction, curiosity, and
forward models. We then relate these insights with contemporary hierarchical
reinforcement learning methods, and identify the key machine intelligence
approaches that realise these mechanisms. As our main result, we show that all
important cognitive mechanisms have been implemented independently in isolated
computational architectures, and there is simply a lack of approaches that
integrate them appropriately. We expect our results to guide the development of
more sophisticated cognitively inspired hierarchical methods, so that future
artificial agents achieve a problem-solving performance on the level of
intelligent animals.
</p>
<a href="http://arxiv.org/abs/2012.10147" target="_blank">arXiv:2012.10147</a> [<a href="http://arxiv.org/pdf/2012.10147" target="_blank">pdf</a>]

<h2>SCNet: Training Inference Sample Consistency for Instance Segmentation. (arXiv:2012.10150v1 [cs.CV])</h2>
<h3>Thang Vu, Haeyong Kang, Chang D. Yoo</h3>
<p>Cascaded architectures have brought significant performance improvement in
object detection and instance segmentation. However, there are lingering issues
regarding the disparity in the Intersection-over-Union (IoU) distribution of
the samples between training and inference. This disparity can potentially
exacerbate detection accuracy. This paper proposes an architecture referred to
as Sample Consistency Network (SCNet) to ensure that the IoU distribution of
the samples at training time is close to that at inference time. Furthermore,
SCNet incorporates feature relay and utilizes global contextual information to
further reinforce the reciprocal relationships among classifying, detecting,
and segmenting sub-tasks. Extensive experiments on the standard COCO dataset
reveal the effectiveness of the proposed method over multiple evaluation
metrics, including box AP, mask AP, and inference speed. In particular, while
running 38\% faster, the proposed SCNet improves the AP of the box and mask
predictions by respectively 1.3 and 2.3 points compared to the strong Cascade
Mask R-CNN baseline. Code is available at
\url{https://github.com/thangvubk/SCNet}.
</p>
<a href="http://arxiv.org/abs/2012.10150" target="_blank">arXiv:2012.10150</a> [<a href="http://arxiv.org/pdf/2012.10150" target="_blank">pdf</a>]

<h2>A Holistically-Guided Decoder for Deep Representation Learning with Applications to Semantic Segmentation and Object Detection. (arXiv:2012.10162v1 [cs.CV])</h2>
<h3>Jianbo Liu, Sijie Ren, Yuanjie Zheng, Xiaogang Wang, Hongsheng Li</h3>
<p>Both high-level and high-resolution feature representations are of great
importance in various visual understanding tasks. To acquire high-resolution
feature maps with high-level semantic information, one common strategy is to
adopt dilated convolutions in the backbone networks to extract high-resolution
feature maps, such as the dilatedFCN-based methods for semantic segmentation.
However, due to many convolution operations are conducted on the
high-resolution feature maps, such methods have large computational complexity
and memory consumption. In this paper, we propose one novel holistically-guided
decoder which is introduced to obtain the high-resolution semantic-rich feature
maps via the multi-scale features from the encoder. The decoding is achieved
via novel holistic codeword generation and codeword assembly operations, which
take advantages of both the high-level and low-level features from the encoder
features. With the proposed holistically-guided decoder, we implement the
EfficientFCN architecture for semantic segmentation and HGD-FPN for object
detection and instance segmentation. The EfficientFCN achieves comparable or
even better performance than state-of-the-art methods with only 1/3 of their
computational costs for semantic segmentation on PASCAL Context, PASCAL VOC,
ADE20K datasets. Meanwhile, the proposed HGD-FPN achieves $&gt;2\%$ higher mean
Average Precision (mAP) when integrated into several object detection
frameworks with ResNet-50 encoding backbones.
</p>
<a href="http://arxiv.org/abs/2012.10162" target="_blank">arXiv:2012.10162</a> [<a href="http://arxiv.org/pdf/2012.10162" target="_blank">pdf</a>]

<h2>Which Heroes to Pick? Learning to Draft in MOBA Games with Neural Networks and Tree Search. (arXiv:2012.10171v1 [cs.AI])</h2>
<h3>Sheng Chen, Menghui Zhu, Deheng Ye, Weinan Zhang, Qiang Fu, Wei Yang</h3>
<p>Hero drafting is essential in MOBA game playing as it builds the team of each
side and directly affects the match outcome. State-of-the-art drafting methods
fail to consider: 1) drafting efficiency when the hero pool is expanded; 2) the
multi-round nature of a MOBA 5v5 match series, i.e., two teams play best-of-N
and the same hero is only allowed to be drafted once throughout the series. In
this paper, we formulate the drafting process as a multi-round combinatorial
game and propose a novel drafting algorithm based on neural networks and
Monte-Carlo tree search, named JueWuDraft. Specifically, we design a long-term
value estimation mechanism to handle the best-of-N drafting case. Taking Honor
of Kings, one of the most popular MOBA games at present, as a running case, we
demonstrate the practicality and effectiveness of JueWuDraft when compared to
state-of-the-art drafting methods.
</p>
<a href="http://arxiv.org/abs/2012.10171" target="_blank">arXiv:2012.10171</a> [<a href="http://arxiv.org/pdf/2012.10171" target="_blank">pdf</a>]

<h2>STNet: Scale Tree Network with Multi-level Auxiliator for Crowd Counting. (arXiv:2012.10189v1 [cs.CV])</h2>
<h3>Mingjie Wang, Hao Cai, Xianfeng Han, Jun Zhou, Minglun Gong</h3>
<p>Crowd counting remains a challenging task because the presence of drastic
scale variation, density inconsistency, and complex background can seriously
degrade the counting accuracy. To battle the ingrained issue of accuracy
degradation, we propose a novel and powerful network called Scale Tree Network
(STNet) for accurate crowd counting. STNet consists of two key components: a
Scale-Tree Diversity Enhancer and a Semi-supervised Multi-level Auxiliator.
Specifically, the Diversity Enhancer is designed to enrich scale diversity,
which alleviates limitations of existing methods caused by insufficient level
of scales. A novel tree structure is adopted to hierarchically parse
coarse-to-fine crowd regions. Furthermore, a simple yet effective Multi-level
Auxiliator is presented to aid in exploiting generalisable shared
characteristics at multiple levels, allowing more accurate pixel-wise
background cognition. The overall STNet is trained in an end-to-end manner,
without the needs for manually tuning loss weights between the main and the
auxiliary tasks. Extensive experiments on four challenging crowd datasets
demonstrate the superiority of the proposed method.
</p>
<a href="http://arxiv.org/abs/2012.10189" target="_blank">arXiv:2012.10189</a> [<a href="http://arxiv.org/pdf/2012.10189" target="_blank">pdf</a>]

<h2>Reconstructing a single-head formula to facilitate logical forgetting. (arXiv:2012.10191v1 [cs.AI])</h2>
<h3>Paolo Liberatore</h3>
<p>Logical forgetting may take exponential time in general, but it does not when
its input is a single-head propositional definite Horn formula. Single-head
means that no variable is the head of multiple clauses. An algorithm to make a
formula single-head if possible is shown. It improves over a previous one by
being complete: it always finds a single-head formula equivalent to the given
one if any.
</p>
<a href="http://arxiv.org/abs/2012.10191" target="_blank">arXiv:2012.10191</a> [<a href="http://arxiv.org/pdf/2012.10191" target="_blank">pdf</a>]

<h2>LGENet: Local and Global Encoder Network for Semantic Segmentation of Airborne Laser Scanning Point Clouds. (arXiv:2012.10192v1 [cs.CV])</h2>
<h3>Yaping Lin, George Vosselman, Yanpeng Cao, Michael Ying Yang</h3>
<p>Interpretation of Airborne Laser Scanning (ALS) point clouds is a critical
procedure for producing various geo-information products like 3D city models,
digital terrain models and land use maps. In this paper, we present a local and
global encoder network (LGENet) for semantic segmentation of ALS point clouds.
Adapting the KPConv network, we first extract features by both 2D and 3D point
convolutions to allow the network to learn more representative local geometry.
Then global encoders are used in the network to exploit contextual information
at the object and point level. We design a segment-based Edge Conditioned
Convolution to encode the global context between segments. We apply a
spatial-channel attention module at the end of the network, which not only
captures the global interdependencies between points but also models
interactions between channels. We evaluate our method on two ALS datasets
namely, the ISPRS benchmark dataset and DCF2019 dataset. For the ISPRS
benchmark dataset, our model achieves state-of-the-art results with an overall
accuracy of 0.845 and an average F1 score of 0.737. With regards to the DFC2019
dataset, our proposed network achieves an overall accuracy of 0.984 and an
average F1 score of 0.834.
</p>
<a href="http://arxiv.org/abs/2012.10192" target="_blank">arXiv:2012.10192</a> [<a href="http://arxiv.org/pdf/2012.10192" target="_blank">pdf</a>]

<h2>Exact Reduction of Huge Action Spaces in General Reinforcement Learning. (arXiv:2012.10200v1 [cs.LG])</h2>
<h3>Sultan Javed Majeed, Marcus Hutter</h3>
<p>The reinforcement learning (RL) framework formalizes the notion of learning
with interactions. Many real-world problems have large state-spaces and/or
action-spaces such as in Go, StarCraft, protein folding, and robotics or are
non-Markovian, which cause significant challenges to RL algorithms. In this
work we address the large action-space problem by sequentializing actions,
which can reduce the action-space size significantly, even down to two actions
at the expense of an increased planning horizon. We provide explicit and exact
constructions and equivalence proofs for all quantities of interest for
arbitrary history-based processes. In the case of MDPs, this could help RL
algorithms that bootstrap. In this work we show how action-binarization in the
non-MDP case can significantly improve Extreme State Aggregation (ESA) bounds.
ESA allows casting any (non-MDP, non-ergodic, history-based) RL problem into a
fixed-sized non-Markovian state-space with the help of a surrogate Markovian
process. On the upside, ESA enjoys similar optimality guarantees as Markovian
models do. But a downside is that the size of the aggregated state-space
becomes exponential in the size of the action-space. In this work, we patch
this issue by binarizing the action-space. We provide an upper bound on the
number of states of this binarized ESA that is logarithmic in the original
action-space size, a double-exponential improvement.
</p>
<a href="http://arxiv.org/abs/2012.10200" target="_blank">arXiv:2012.10200</a> [<a href="http://arxiv.org/pdf/2012.10200" target="_blank">pdf</a>]

<h2>Classification with Strategically Withheld Data. (arXiv:2012.10203v1 [cs.LG])</h2>
<h3>Anilesh K. Krishnaswamy, Haoming Li, David Rein, Hanrui Zhang, Vincent Conitzer</h3>
<p>Machine learning techniques can be useful in applications such as credit
approval and college admission. However, to be classified more favorably in
such contexts, an agent may decide to strategically withhold some of her
features, such as bad test scores. This is a missing data problem with a twist:
which data is missing {\em depends on the chosen classifier}, because the
specific classifier is what may create the incentive to withhold certain
feature values. We address the problem of training classifiers that are robust
to this behavior.

We design three classification methods: {\sc Mincut}, {\sc Hill-Climbing}
({\sc HC}) and Incentive-Compatible Logistic Regression ({\sc IC-LR}). We show
that {\sc Mincut} is optimal when the true distribution of data is fully known.
However, it can produce complex decision boundaries, and hence be prone to
overfitting in some cases. Based on a characterization of truthful classifiers
(i.e., those that give no incentive to strategically hide features), we devise
a simpler alternative called {\sc HC} which consists of a hierarchical ensemble
of out-of-the-box classifiers, trained using a specialized hill-climbing
procedure which we show to be convergent. For several reasons, {\sc Mincut} and
{\sc HC} are not effective in utilizing a large number of complementarily
informative features. To this end, we present {\sc IC-LR}, a modification of
Logistic Regression that removes the incentive to strategically drop features.
We also show that our algorithms perform well in experiments on real-world data
sets, and present insights into their relative performance in different
settings.
</p>
<a href="http://arxiv.org/abs/2012.10203" target="_blank">arXiv:2012.10203</a> [<a href="http://arxiv.org/pdf/2012.10203" target="_blank">pdf</a>]

<h2>On Modality Bias in the TVQA Dataset. (arXiv:2012.10210v1 [cs.CV])</h2>
<h3>Thomas Winterbottom, Sarah Xiao, Alistair McLean, Noura Al Moubayed</h3>
<p>TVQA is a large scale video question answering (video-QA) dataset based on
popular TV shows. The questions were specifically designed to require "both
vision and language understanding to answer". In this work, we demonstrate an
inherent bias in the dataset towards the textual subtitle modality. We infer
said bias both directly and indirectly, notably finding that models trained
with subtitles learn, on-average, to suppress video feature contribution. Our
results demonstrate that models trained on only the visual information can
answer ~45% of the questions, while using only the subtitles achieves ~68%. We
find that a bilinear pooling based joint representation of modalities damages
model performance by 9% implying a reliance on modality specific information.
We also show that TVQA fails to benefit from the RUBi modality bias reduction
technique popularised in VQA. By simply improving text processing using BERT
embeddings with the simple model first proposed for TVQA, we achieve
state-of-the-art results (72.13%) compared to the highly complex STAGE model
(70.50%). We recommend a multimodal evaluation framework that can highlight
biases in models and isolate visual and textual reliant subsets of data. Using
this framework we propose subsets of TVQA that respond exclusively to either or
both modalities in order to facilitate multimodal modelling as TVQA originally
intended.
</p>
<a href="http://arxiv.org/abs/2012.10210" target="_blank">arXiv:2012.10210</a> [<a href="http://arxiv.org/pdf/2012.10210" target="_blank">pdf</a>]

<h2>Fair for All: Best-effort Fairness Guarantees for Classification. (arXiv:2012.10216v1 [cs.LG])</h2>
<h3>Anilesh K. Krishnaswamy, Zhihao Jiang, Kangning Wang, Yu Cheng, Kamesh Munagala</h3>
<p>Standard approaches to group-based notions of fairness, such as \emph{parity}
and \emph{equalized odds}, try to equalize absolute measures of performance
across known groups (based on race, gender, etc.). Consequently, a group that
is inherently harder to classify may hold back the performance on other groups;
and no guarantees can be provided for unforeseen groups. Instead, we propose a
fairness notion whose guarantee, on each group $g$ in a class $\mathcal{G}$, is
relative to the performance of the best classifier on $g$. We apply this notion
to broad classes of groups, in particular, where (a) $\mathcal{G}$ consists of
all possible groups (subsets) in the data, and (b) $\mathcal{G}$ is more
streamlined.

For the first setting, which is akin to groups being completely unknown, we
devise the {\sc PF} (Proportional Fairness) classifier, which guarantees, on
any possible group $g$, an accuracy that is proportional to that of the optimal
classifier for $g$, scaled by the relative size of $g$ in the data set. Due to
including all possible groups, some of which could be too complex to be
relevant, the worst-case theoretical guarantees here have to be proportionally
weaker for smaller subsets.

For the second setting, we devise the {\sc BeFair} (Best-effort Fair)
framework which seeks an accuracy, on every $g \in \mathcal{G}$, which
approximates that of the optimal classifier on $g$, independent of the size of
$g$. Aiming for such a guarantee results in a non-convex problem, and we design
novel techniques to get around this difficulty when $\mathcal{G}$ is the set of
linear hypotheses. We test our algorithms on real-world data sets, and present
interesting comparative insights on their performance.
</p>
<a href="http://arxiv.org/abs/2012.10216" target="_blank">arXiv:2012.10216</a> [<a href="http://arxiv.org/pdf/2012.10216" target="_blank">pdf</a>]

<h2>SegGroup: Seg-Level Supervision for 3D Instance and Semantic Segmentation. (arXiv:2012.10217v1 [cs.CV])</h2>
<h3>An Tao, Yueqi Duan, Yi Wei, Jiwen Lu, Jie Zhou</h3>
<p>Most existing point cloud instance and semantic segmentation methods heavily
rely on strong supervision signals, which require point-level labels for every
point in the scene. However, strong supervision suffers from large annotation
cost, arousing the need to study efficient annotating. In this paper, we
propose a new form of weak supervision signal, namely seg-level labels, for
point cloud instance and semantic segmentation. Based on the widely-used
over-segmentation as pre-processor, we only annotate one point for each
instance to obtain seg-level labels. We further design a segment grouping
network (SegGroup) to generate pseudo point-level labels by hierarchically
grouping the unlabeled segments into the relevant nearby labeled segments, so
that existing methods can directly consume the pseudo labels for training.
Experimental results show that our SegGroup achieves comparable results with
the fully annotated point-level supervised methods on both point cloud instance
and semantic segmentation tasks and outperforms the recent scene-level and
subcloud-level supervised methods significantly.
</p>
<a href="http://arxiv.org/abs/2012.10217" target="_blank">arXiv:2012.10217</a> [<a href="http://arxiv.org/pdf/2012.10217" target="_blank">pdf</a>]

<h2>Artificial Intelligence ordered 3D vertex importance. (arXiv:2012.10232v1 [cs.AI])</h2>
<h3>Iva Vasic, Bata Vasic, Zorica Nikolic</h3>
<p>Ranking vertices of multidimensional networks is crucial in many areas of
research, including selecting and determining the importance of decisions. Some
decisions are significantly more important than others, and their weight
categorization is also imortant. This paper defines a completely new method for
determining the weight decisions using artificial intelligence for importance
ranking of three-dimensional network vertices, improving the existing Ordered
Statistics Vertex Extraction and Tracking Algorithm (OSVETA) based on
modulation of quantized indices (QIM) and error correction codes. The technique
we propose in this paper offers significant improvements the efficiency of
determination the importance of network vertices in relation to statistical
OSVETA criteria, replacing heuristic methods with methods of precise prediction
of modern neural networks. The new artificial intelligence technique enables a
significantly better definition of the 3D meshes and a better assessment of
their topological features. The new method contributions result in a greater
precision in defining stable vertices, significantly reducing the probability
of deleting mesh vertices.
</p>
<a href="http://arxiv.org/abs/2012.10232" target="_blank">arXiv:2012.10232</a> [<a href="http://arxiv.org/pdf/2012.10232" target="_blank">pdf</a>]

<h2>An Experimental Study of the Transferability of Spectral Graph Networks. (arXiv:2012.10258v1 [cs.LG])</h2>
<h3>Axel Nilsson, Xavier Bresson</h3>
<p>Spectral graph convolutional networks are generalizations of standard
convolutional networks for graph-structured data using the Laplacian operator.
A common misconception is the instability of spectral filters, i.e. the
impossibility to transfer spectral filters between graphs of variable size and
topology. This misbelief has limited the development of spectral networks for
multi-graph tasks in favor of spatial graph networks. However, recent works
have proved the stability of spectral filters under graph perturbation. Our
work complements and emphasizes further the high quality of spectral
transferability by benchmarking spectral graph networks on tasks involving
graphs of different size and connectivity. Numerical experiments exhibit
favorable performance on graph regression, graph classification, and node
classification problems on two graph benchmarks. The implementation of our
experiments is available on GitHub for reproducibility.
</p>
<a href="http://arxiv.org/abs/2012.10258" target="_blank">arXiv:2012.10258</a> [<a href="http://arxiv.org/pdf/2012.10258" target="_blank">pdf</a>]

<h2>Spacecraft Collision Risk Assessment with Probabilistic Programming. (arXiv:2012.10260v1 [cs.LG])</h2>
<h3>Giacomo Acciarini, Francesco Pinto, Sascha Metz, Sarah Boufelja, Sylvester Kaczmarek, Klaus Merz, Jos&#xe9; A. Martinez-Heras, Francesca Letizia, Christopher Bridges, At&#x131;l&#x131;m G&#xfc;ne&#x15f; Baydin</h3>
<p>Over 34,000 objects bigger than 10 cm in length are known to orbit Earth.
Among them, only a small percentage are active satellites, while the rest of
the population is made of dead satellites, rocket bodies, and debris that pose
a collision threat to operational spacecraft. Furthermore, the predicted growth
of the space sector and the planned launch of megaconstellations will add even
more complexity, therefore causing the collision risk and the burden on space
operators to increase. Managing this complex framework with internationally
agreed methods is pivotal and urgent. In this context, we build a novel
physics-based probabilistic generative model for synthetically generating
conjunction data messages, calibrated using real data. By conditioning on
observations, we use the model to obtain posterior distributions via Bayesian
inference. We show that the probabilistic programming approach to conjunction
assessment can help in making predictions and in finding the parameters that
explain the observed data in conjunction data messages, thus shedding more
light on key variables and orbital characteristics that more likely lead to
conjunction events. Moreover, our technique enables the generation of
physically accurate synthetic datasets of collisions, answering a fundamental
need of the space and machine learning communities working in this area.
</p>
<a href="http://arxiv.org/abs/2012.10260" target="_blank">arXiv:2012.10260</a> [<a href="http://arxiv.org/pdf/2012.10260" target="_blank">pdf</a>]

<h2>Adversarially Robust Estimate and Risk Analysis in Linear Regression. (arXiv:2012.10278v1 [stat.ML])</h2>
<h3>Yue Xing, Ruizhi Zhang, Guang Cheng</h3>
<p>Adversarially robust learning aims to design algorithms that are robust to
small adversarial perturbations on input variables. Beyond the existing studies
on the predictive performance to adversarial samples, our goal is to understand
statistical properties of adversarially robust estimates and analyze
adversarial risk in the setup of linear regression models. By discovering the
statistical minimax rate of convergence of adversarially robust estimators, we
emphasize the importance of incorporating model information, e.g., sparsity, in
adversarially robust learning. Further, we reveal an explicit connection of
adversarial and standard estimates, and propose a straightforward two-stage
adversarial learning framework, which facilitates to utilize model structure
information to improve adversarial robustness. In theory, the consistency of
the adversarially robust estimator is proven and its Bahadur representation is
also developed for the statistical inference purpose. The proposed estimator
converges in a sharp rate under either low-dimensional or sparse scenario.
Moreover, our theory confirms two phenomena in adversarially robust learning:
adversarial robustness hurts generalization, and unlabeled data help improve
the generalization. In the end, we conduct numerical simulations to verify our
theory.
</p>
<a href="http://arxiv.org/abs/2012.10278" target="_blank">arXiv:2012.10278</a> [<a href="http://arxiv.org/pdf/2012.10278" target="_blank">pdf</a>]

<h2>ROBY: Evaluating the Robustness of a Deep Model by its Decision Boundaries. (arXiv:2012.10282v1 [cs.LG])</h2>
<h3>Jinyin Chen, Zhen Wang, Haibin Zheng, Jun Xiao, Zhaoyan Ming</h3>
<p>With the successful application of deep learning models in many real-world
tasks, the model robustness becomes more and more critical. Often, we evaluate
the robustness of the deep models by attacking them with purposely generated
adversarial samples, which is computationally costly and dependent on the
specific attackers and the model types. This work proposes a generic evaluation
metric ROBY, a novel attack-independent robustness measure based on the model's
decision boundaries. Independent of adversarial samples, ROBY uses the
inter-class and intra-class statistic features to capture the features of the
model's decision boundaries. We experimented on ten state-of-the-art deep
models and showed that ROBY matches the robustness gold standard of attack
success rate (ASR) by a strong first-order generic attacker. with only 1% of
time cost. To the best of our knowledge, ROBY is the first lightweight
attack-independent robustness evaluation metric that can be applied to a wide
range of deep models. The code of ROBY is open sourced at
https://github.com/baaaad/ROBY-Evaluating-the-Robustness-of-a-Deep-Model-by-its-Decision-Boundaries.
</p>
<a href="http://arxiv.org/abs/2012.10282" target="_blank">arXiv:2012.10282</a> [<a href="http://arxiv.org/pdf/2012.10282" target="_blank">pdf</a>]

<h2>Temporal Bilinear Encoding Network of Audio-Visual Features at Low Sampling Rates. (arXiv:2012.10283v1 [cs.CV])</h2>
<h3>Feiyan Hu, Eva Mohedano, Noel O&#x27;Connor, Kevin McGuinness</h3>
<p>Current deep learning based video classification architectures are typically
trained end-to-end on large volumes of data and require extensive computational
resources. This paper aims to exploit audio-visual information in video
classification with a 1 frame per second sampling rate. We propose Temporal
Bilinear Encoding Networks (TBEN) for encoding both audio and visual long range
temporal information using bilinear pooling and demonstrate bilinear pooling is
better than average pooling on the temporal dimension for videos with low
sampling rate. We also embed the label hierarchy in TBEN to further improve the
robustness of the classifier. Experiments on the FGA240 fine-grained
classification dataset using TBEN achieve a new state-of-the-art
(hit@1=47.95%). We also exploit the possibility of incorporating TBEN with
multiple decoupled modalities like visual semantic and motion features:
experiments on UCF101 sampled at 1 FPS achieve close to state-of-the-art
accuracy (hit@1=91.03%) while requiring significantly less computational
resources than competing approaches for both training and prediction.
</p>
<a href="http://arxiv.org/abs/2012.10283" target="_blank">arXiv:2012.10283</a> [<a href="http://arxiv.org/pdf/2012.10283" target="_blank">pdf</a>]

<h2>Trying Bilinear Pooling in Video-QA. (arXiv:2012.10285v1 [cs.CV])</h2>
<h3>Thomas Winterbottom, Sarah Xiao, Alistair McLean, Noura Al Moubayed</h3>
<p>Bilinear pooling (BLP) refers to a family of operations recently developed
for fusing features from different modalities predominantly developed for VQA
models. A bilinear (outer-product) expansion is thought to encourage models to
learn interactions between two feature spaces and has experimentally
outperformed `simpler' vector operations (concatenation and
element-wise-addition/multiplication) on VQA benchmarks. Successive BLP
techniques have yielded higher performance with lower computational expense and
are often implemented alongside attention mechanisms. However, despite
significant progress in VQA, BLP methods have not been widely applied to more
recently explored video question answering (video-QA) tasks. In this paper, we
begin to bridge this research gap by applying BLP techniques to various
video-QA benchmarks, namely: TVQA, TGIF-QA, Ego-VQA and MSVD-QA. We share our
results on the TVQA baseline model, and the recently proposed
heterogeneous-memory-enchanced multimodal attention (HME) model. Our
experiments include both simply replacing feature concatenation in the existing
models with BLP, and a modified version of the TVQA baseline to accommodate BLP
we name the `dual-stream' model. We find that our relatively simple integration
of BLP does not increase, and mostly harms, performance on these video-QA
benchmarks. Using recently proposed theoretical multimodal fusion taxonomies,
we offer insight into why BLP-driven performance gain for video-QA benchmarks
may be more difficult to achieve than in earlier VQA models. We suggest a few
additional `best-practices' to consider when applying BLP to video-QA. We
stress that video-QA models should carefully consider where the complex
representational potential from BLP is actually needed to avoid computational
expense on `redundant' fusion.
</p>
<a href="http://arxiv.org/abs/2012.10285" target="_blank">arXiv:2012.10285</a> [<a href="http://arxiv.org/pdf/2012.10285" target="_blank">pdf</a>]

<h2>Boosting Monocular Depth Estimation with Lightweight 3D Point Fusion. (arXiv:2012.10296v1 [cs.CV])</h2>
<h3>Lam Huynh, Phong Nguyen, Jiri Matas, Esa Rahtu, Janne Heikkila</h3>
<p>In this paper, we address the problem of fusing monocular depth estimation
with a conventional multi-view stereo or SLAM to exploit the best of both
worlds, that is, the accurate dense depth of the first one and lightweightness
of the second one. More specifically, we use a conventional pipeline to produce
a sparse 3D point cloud that is fed to a monocular depth estimation network to
enhance its performance. In this way, we can achieve accuracy similar to
multi-view stereo with a considerably smaller number of weights. We also show
that even as few as 32 points is sufficient to outperform the best monocular
depth estimation methods, and around 200 points to gain full advantage of the
additional information. Moreover, we demonstrate the efficacy of our approach
by integrating it with a SLAM system built-in on mobile devices.
</p>
<a href="http://arxiv.org/abs/2012.10296" target="_blank">arXiv:2012.10296</a> [<a href="http://arxiv.org/pdf/2012.10296" target="_blank">pdf</a>]

<h2>Artificial Neural Networks to Impute Rounded Zeros in Compositional Data. (arXiv:2012.10300v1 [stat.ML])</h2>
<h3>Matthias Templ</h3>
<p>Methods of deep learning have become increasingly popular in recent years,
but they have not arrived in compositional data analysis. Imputation methods
for compositional data are typically applied on additive, centered or isometric
log-ratio representations of the data. Generally, methods for compositional
data analysis can only be applied to observed positive entries in a data
matrix. Therefore one tries to impute missing values or measurements that were
below a detection limit. In this paper, a new method for imputing rounded zeros
based on artificial neural networks is shown and compared with conventional
methods. We are also interested in the question whether for ANNs, a
representation of the data in log-ratios for imputation purposes, is relevant.
It can be shown, that ANNs are competitive or even performing better when
imputing rounded zeros of data sets with moderate size. They deliver better
results when data sets are big. Also, we can see that log-ratio transformations
within the artificial neural network imputation procedure nevertheless help to
improve the results. This proves that the theory of compositional data analysis
and the fulfillment of all properties of compositional data analysis is still
very important in the age of deep learning.
</p>
<a href="http://arxiv.org/abs/2012.10300" target="_blank">arXiv:2012.10300</a> [<a href="http://arxiv.org/pdf/2012.10300" target="_blank">pdf</a>]

<h2>The Danger of Reverse-Engineering of Automated Judicial Decision-Making Systems. (arXiv:2012.10301v1 [cs.LG])</h2>
<h3>Masha Medvedeva, Martijn Wieling, Michel Vols</h3>
<p>In this paper we discuss the implications of using machine learning for
judicial decision-making in situations where human rights may be infringed. We
argue that the use of such tools in these situations should be limited due to
inherent status quo bias and dangers of reverse-engineering. We discuss that
these issues already exist in the judicial systems without using machine
learning tools, but how introducing them might exacerbate them.
</p>
<a href="http://arxiv.org/abs/2012.10301" target="_blank">arXiv:2012.10301</a> [<a href="http://arxiv.org/pdf/2012.10301" target="_blank">pdf</a>]

<h2>Multi-characteristic Subject Selection from Biased Datasets. (arXiv:2012.10311v1 [cs.LG])</h2>
<h3>Tahereh Arabghalizi, Alexandros Labrinidis</h3>
<p>Subject selection plays a critical role in experimental studies, especially
ones with human subjects. Anecdotal evidence suggests that many such studies,
done at or near university campus settings suffer from selection bias, i.e.,
the too-many-college-kids-as-subjects problem. Unfortunately, traditional
sampling techniques, when applied over biased data, will typically return
biased results. In this paper, we tackle the problem of multi-characteristic
subject selection from biased datasets. We present a constrained
optimization-based method that finds the best possible sampling fractions for
the different population subgroups, based on the desired sampling fractions
provided by the researcher running the subject selection.We perform an
extensive experimental study, using a variety of real datasets. Our results
show that our proposed method outperforms the baselines for all problem
variations by up to 90%.
</p>
<a href="http://arxiv.org/abs/2012.10311" target="_blank">arXiv:2012.10311</a> [<a href="http://arxiv.org/pdf/2012.10311" target="_blank">pdf</a>]

<h2>Kernel Methods for Unobserved Confounding: Negative Controls, Proxies, and Instruments. (arXiv:2012.10315v1 [stat.ML])</h2>
<h3>Rahul Singh</h3>
<p>Negative control is a strategy for learning the causal relationship between
treatment and outcome in the presence of unmeasured confounding. The treatment
effect can nonetheless be identified if two auxiliary variables are available:
a negative control treatment (which has no effect on the actual outcome), and a
negative control outcome (which is not affected by the actual treatment). These
auxiliary variables can also be viewed as proxies for a traditional set of
control variables, and they bear resemblance to instrumental variables. I
propose a new family of non-parametric algorithms for learning treatment
effects with negative controls. I consider treatment effects of the population,
of sub-populations, and of alternative populations. I allow for data that may
be discrete or continuous, and low-, high-, or infinite-dimensional. I impose
the additional structure of the reproducing kernel Hilbert space (RKHS), a
popular non-parametric setting in machine learning. I prove uniform consistency
and provide finite sample rates of convergence. I evaluate the estimators in
simulations.
</p>
<a href="http://arxiv.org/abs/2012.10315" target="_blank">arXiv:2012.10315</a> [<a href="http://arxiv.org/pdf/2012.10315" target="_blank">pdf</a>]

<h2>Learning from History for Byzantine Robust Optimization. (arXiv:2012.10333v1 [cs.LG])</h2>
<h3>Sai Praneeth Karimireddy, Lie He, Martin Jaggi</h3>
<p>Byzantine robustness has received significant attention recently given its
importance for distributed and federated learning. In spite of this, we
identify severe flaws in existing algorithms even when the data across the
participants is assumed to be identical. First, we show that most existing
robust aggregation rules may not converge even in the absence of any Byzantine
attackers, because they are overly sensitive to the distribution of the noise
in the stochastic gradients. Secondly, we show that even if the aggregation
rules may succeed in limiting the influence of the attackers in a single round,
the attackers can couple their attacks across time eventually leading to
divergence. To address these issues, we present two surprisingly simple
strategies: a new iterative clipping procedure, and incorporating worker
momentum to overcome time-coupled attacks. This is the first provably robust
method for the standard stochastic non-convex optimization setting.
</p>
<a href="http://arxiv.org/abs/2012.10333" target="_blank">arXiv:2012.10333</a> [<a href="http://arxiv.org/pdf/2012.10333" target="_blank">pdf</a>]

<h2>Solving Black-Box Optimization Challenge via Learning Search Space Partition for Local Bayesian Optimization. (arXiv:2012.10335v1 [cs.LG])</h2>
<h3>Mikita Sazanovich, Anastasiya Nikolskaya, Yury Belousov, Aleksei Shpilman</h3>
<p>This paper describes our approach to solving the black-box optimization
challenge through learning search space partition for local Bayesian
optimization. We develop an algorithm for low budget optimization. We further
optimize the hyper-parameters of our algorithm using Bayesian optimization. Our
approach has been ranked 3rd in the competition.
</p>
<a href="http://arxiv.org/abs/2012.10335" target="_blank">arXiv:2012.10335</a> [<a href="http://arxiv.org/pdf/2012.10335" target="_blank">pdf</a>]

<h2>Application of computer simulation results and machine learning in analysis of microwave radiothermometry data. (arXiv:2012.10343v1 [cs.LG])</h2>
<h3>Maxim Polyakov, Illarion Popov, Alexander Losev, Alexander Khoperskov</h3>
<p>This work was done with the aim of developing the fundamental breast cancer
early differential diagnosis foundations based on modeling the space-time
temperature distribution using the microwave radiothermometry method and
obtained data intelligent analysis. The article deals with the machine learning
application in the microwave radiothermometry data analysis. The problems
associated with the construction mammary glands temperature fields computer
models for patients with various diagnostics classes, are also discussed. With
the help of a computer experiment, based on the machine learning algorithms set
(logistic regression, naive Bayesian classifier, support vector machine,
decision tree, gradient boosting, K-nearest neighbors, etc.) usage, the mammary
glands temperature fields computer models set adequacy.
</p>
<a href="http://arxiv.org/abs/2012.10343" target="_blank">arXiv:2012.10343</a> [<a href="http://arxiv.org/pdf/2012.10343" target="_blank">pdf</a>]

<h2>Small Business Classification By Name: Addressing Gender and Geographic Origin Biases. (arXiv:2012.10348v1 [cs.LG])</h2>
<h3>Daniel Shapiro</h3>
<p>Small business classification is a difficult and important task within many
applications, including customer segmentation. Training on small business names
introduces gender and geographic origin biases. A model for predicting one of
66 business types based only upon the business name was developed in this work
(top-1 f1-score = 60.2%). Two approaches to removing the bias from this model
are explored: replacing given names with a placeholder token, and augmenting
the training data with gender-swapped examples. The results for these
approaches is reported, and the bias in the model was reduced by hiding given
names from the model. However, bias reduction was accomplished at the expense
of classification performance (top-1 f1-score = 56.6%). Augmentation of the
training data with gender-swapping samples proved less effective at bias
reduction than the name hiding approach on the evaluated dataset.
</p>
<a href="http://arxiv.org/abs/2012.10348" target="_blank">arXiv:2012.10348</a> [<a href="http://arxiv.org/pdf/2012.10348" target="_blank">pdf</a>]

<h2>Universal Approximation in Dropout Neural Networks. (arXiv:2012.10351v1 [cs.LG])</h2>
<h3>Oxana A. Manita, Mark A. Peletier, Jacobus W. Portegies, Jaron Sanders, Albert Senen-Cerda</h3>
<p>We prove two universal approximation theorems for a range of dropout neural
networks. These are feed-forward neural networks in which each edge is given a
random $\{0,1\}$-valued filter, that have two modes of operation: in the first
each edge output is multiplied by its random filter, resulting in a random
output, while in the second each edge output is multiplied by the expectation
of its filter, leading to a deterministic output. It is common to use the
random mode during training and the deterministic mode during testing and
prediction.

Both theorems are of the following form: Given a function to approximate and
a threshold $\varepsilon&gt;0$, there exists a dropout network that is
$\varepsilon$-close in probability and in $L^q$. The first theorem applies to
dropout networks in the random mode. It assumes little on the activation
function, applies to a wide class of networks, and can even be applied to
approximation schemes other than neural networks. The core is an algebraic
property that shows that deterministic networks can be exactly matched in
expectation by random networks. The second theorem makes stronger assumptions
and gives a stronger result. Given a function to approximate, it provides
existence of a network that approximates in both modes simultaneously. Proof
components are a recursive replacement of edges by independent copies, and a
special first-layer replacement that couples the resulting larger network to
the input.

The functions to be approximated are assumed to be elements of general normed
spaces, and the approximations are measured in the corresponding norms. The
networks are constructed explicitly. Because of the different methods of proof,
the two results give independent insight into the approximation properties of
random dropout networks. With this, we establish that dropout neural networks
broadly satisfy a universal-approximation property.
</p>
<a href="http://arxiv.org/abs/2012.10351" target="_blank">arXiv:2012.10351</a> [<a href="http://arxiv.org/pdf/2012.10351" target="_blank">pdf</a>]

<h2>Assessing Pattern Recognition Performance of Neuronal Cultures through Accurate Simulation. (arXiv:2012.10355v1 [cs.CV])</h2>
<h3>Gabriele Lagani, Raffaele Mazziotti, Fabrizio Falchi, Claudio Gennaro, Guido Marco Cicchini, Tommaso Pizzorusso, Federico Cremisi, Giuseppe Amato</h3>
<p>Previous work has shown that it is possible to train neuronal cultures on
Multi-Electrode Arrays (MEAs), to recognize very simple patterns. However, this
work was mainly focused to demonstrate that it is possible to induce plasticity
in cultures, rather than performing a rigorous assessment of their pattern
recognition performance. In this paper, we address this gap by developing a
methodology that allows us to assess the performance of neuronal cultures on a
learning task. Specifically, we propose a digital model of the real cultured
neuronal networks; we identify biologically plausible simulation parameters
that allow us to reliably reproduce the behavior of real cultures; we use the
simulated culture to perform handwritten digit recognition and rigorously
evaluate its performance; we also show that it is possible to find improved
simulation parameters for the specific task, which can guide the creation of
real cultures.
</p>
<a href="http://arxiv.org/abs/2012.10355" target="_blank">arXiv:2012.10355</a> [<a href="http://arxiv.org/pdf/2012.10355" target="_blank">pdf</a>]

<h2>Learning Complex 3D Human Self-Contact. (arXiv:2012.10366v1 [cs.CV])</h2>
<h3>Mihai Fieraru, Mihai Zanfir, Elisabeta Oneata, Alin-Ionut Popa, Vlad Olaru, Cristian Sminchisescu</h3>
<p>Monocular estimation of three dimensional human self-contact is fundamental
for detailed scene analysis including body language understanding and behaviour
modeling. Existing 3d reconstruction methods do not focus on body regions in
self-contact and consequently recover configurations that are either far from
each other or self-intersecting, when they should just touch. This leads to
perceptually incorrect estimates and limits impact in those very fine-grained
analysis domains where detailed 3d models are expected to play an important
role. To address such challenges we detect self-contact and design 3d losses to
explicitly enforce it. Specifically, we develop a model for Self-Contact
Prediction (SCP), that estimates the body surface signature of self-contact,
leveraging the localization of self-contact in the image, during both training
and inference. We collect two large datasets to support learning and
evaluation: (1) HumanSC3D, an accurate 3d motion capture repository containing
$1,032$ sequences with $5,058$ contact events and $1,246,487$ ground truth 3d
poses synchronized with images collected from multiple views, and (2)
FlickrSC3D, a repository of $3,969$ images, containing $25,297$
surface-to-surface correspondences with annotated image spatial support. We
also illustrate how more expressive 3d reconstructions can be recovered under
self-contact signature constraints and present monocular detection of
face-touch as one of the multiple applications made possible by more accurate
self-contact models.
</p>
<a href="http://arxiv.org/abs/2012.10366" target="_blank">arXiv:2012.10366</a> [<a href="http://arxiv.org/pdf/2012.10366" target="_blank">pdf</a>]

<h2>Upper and Lower Bounds on the Performance of Kernel PCA. (arXiv:2012.10369v1 [cs.LG])</h2>
<h3>Maxime Haddouche, Benjamin Guedj, Omar Rivasplata, John Shawe-Taylor</h3>
<p>Principal Component Analysis (PCA) is a popular method for dimension
reduction and has attracted an unfailing interest for decades. Recently, kernel
PCA has emerged as an extension of PCA but, despite its use in practice, a
sound theoretical understanding of kernel PCA is missing. In this paper, we
contribute lower and upper bounds on the efficiency of kernel PCA, involving
the empirical eigenvalues of the kernel Gram matrix. Two bounds are for fixed
estimators, and two are for randomized estimators through the PAC-Bayes theory.
We control how much information is captured by kernel PCA on average, and we
dissect the bounds to highlight strengths and limitations of the kernel PCA
algorithm. Therefore, we contribute to the better understanding of kernel PCA.
Our bounds are briefly illustrated on a toy numerical example.
</p>
<a href="http://arxiv.org/abs/2012.10369" target="_blank">arXiv:2012.10369</a> [<a href="http://arxiv.org/pdf/2012.10369" target="_blank">pdf</a>]

<h2>Deep Learning and the Global Workspace Theory. (arXiv:2012.10390v1 [cs.AI])</h2>
<h3>Rufin VanRullen, Ryota Kanai</h3>
<p>Recent advances in deep learning have allowed Artificial Intelligence (AI) to
reach near human-level performance in many sensory, perceptual, linguistic or
cognitive tasks. There is a growing need, however, for novel, brain-inspired
cognitive architectures. The Global Workspace theory refers to a large-scale
system integrating and distributing information among networks of specialized
modules to create higher-level forms of cognition and awareness. We argue that
the time is ripe to consider explicit implementations of this theory using deep
learning techniques. We propose a roadmap based on unsupervised neural
translation between multiple latent spaces (neural networks trained for
distinct tasks, on distinct sensory inputs and/or modalities) to create a
unique, amodal global latent workspace (GLW). Potential functional advantages
of GLW are reviewed.
</p>
<a href="http://arxiv.org/abs/2012.10390" target="_blank">arXiv:2012.10390</a> [<a href="http://arxiv.org/pdf/2012.10390" target="_blank">pdf</a>]

<h2>Convergence dynamics of Generative Adversarial Networks: the dual metric flows. (arXiv:2012.10410v1 [stat.ML])</h2>
<h3>Gabriel Turinici</h3>
<p>Fitting neural networks often resorts to stochastic (or similar) gradient
descent which is a noise-tolerant (and efficient) resolution of a gradient
descent dynamics. It outputs a sequence of networks parameters, which sequence
evolves during the training steps. The gradient descent is the limit, when the
learning rate is small and the batch size is infinite, of this set of
increasingly optimal network parameters obtained during training. In this
contribution, we investigate instead the convergence in the Generative
Adversarial Networks used in machine learning. We study the limit of small
learning rate, and show that, similar to single network training, the GAN
learning dynamics tend, for vanishing learning rate to some limit dynamics.
This leads us to consider evolution equations in metric spaces (which is the
natural framework for evolving probability laws)that we call dual flows. We
give formal definitions of solutions and prove the convergence. The theory is
then applied to specific instances of GANs and we discuss how this insight
helps understand and mitigate the mode collapse.
</p>
<a href="http://arxiv.org/abs/2012.10410" target="_blank">arXiv:2012.10410</a> [<a href="http://arxiv.org/pdf/2012.10410" target="_blank">pdf</a>]

<h2>PC-RGNN: Point Cloud Completion and Graph Neural Network for 3D Object Detection. (arXiv:2012.10412v1 [cs.CV])</h2>
<h3>Yanan Zhang, Di Huang, Yunhong Wang</h3>
<p>LiDAR-based 3D object detection is an important task for autonomous driving
and current approaches suffer from sparse and partial point clouds of distant
and occluded objects. In this paper, we propose a novel two-stage approach,
namely PC-RGNN, dealing with such challenges by two specific solutions. On the
one hand, we introduce a point cloud completion module to recover high-quality
proposals of dense points and entire views with original structures preserved.
On the other hand, a graph neural network module is designed, which
comprehensively captures relations among points through a local-global
attention mechanism as well as multi-scale graph based context aggregation,
substantially strengthening encoded features. Extensive experiments on the
KITTI benchmark show that the proposed approach outperforms the previous
state-of-the-art baselines by remarkable margins, highlighting its
effectiveness.
</p>
<a href="http://arxiv.org/abs/2012.10412" target="_blank">arXiv:2012.10412</a> [<a href="http://arxiv.org/pdf/2012.10412" target="_blank">pdf</a>]

<h2>Separation and Concentration in Deep Networks. (arXiv:2012.10424v1 [cs.LG])</h2>
<h3>John Zarka, Florentin Guth, St&#xe9;phane Mallat</h3>
<p>Numerical experiments demonstrate that deep neural network classifiers
progressively separate class distributions around their mean, achieving linear
separability on the training set, and increasing the Fisher discriminant ratio.
We explain this mechanism with two types of operators. We prove that a
rectifier without biases applied to sign-invariant tight frames can separate
class means and increase Fisher ratios. On the opposite, a soft-thresholding on
tight frames can reduce within-class variabilities while preserving class
means. Variance reduction bounds are proved for Gaussian mixture models. For
image classification, we show that separation of class means can be achieved
with rectified wavelet tight frames that are not learned. It defines a
scattering transform. Learning $1 \times 1$ convolutional tight frames along
scattering channels and applying a soft-thresholding reduces within-class
variabilities. The resulting scattering network reaches the classification
accuracy of ResNet-18 on CIFAR-10 and ImageNet, with fewer layers and no
learned biases.
</p>
<a href="http://arxiv.org/abs/2012.10424" target="_blank">arXiv:2012.10424</a> [<a href="http://arxiv.org/pdf/2012.10424" target="_blank">pdf</a>]

<h2>Towards Robust Explanations for Deep Neural Networks. (arXiv:2012.10425v1 [cs.LG])</h2>
<h3>Ann-Kathrin Dombrowski, Christopher J. Anders, Klaus-Robert M&#xfc;ller, Pan Kessel</h3>
<p>Explanation methods shed light on the decision process of black-box
classifiers such as deep neural networks. But their usefulness can be
compromised because they are susceptible to manipulations. With this work, we
aim to enhance the resilience of explanations. We develop a unified theoretical
framework for deriving bounds on the maximal manipulability of a model. Based
on these theoretical insights, we present three different techniques to boost
robustness against manipulation: training with weight decay, smoothing
activation functions, and minimizing the Hessian of the network. Our
experimental results confirm the effectiveness of these approaches.
</p>
<a href="http://arxiv.org/abs/2012.10425" target="_blank">arXiv:2012.10425</a> [<a href="http://arxiv.org/pdf/2012.10425" target="_blank">pdf</a>]

<h2>Efficient Training of Robust Decision Trees Against Adversarial Examples. (arXiv:2012.10438v1 [cs.LG])</h2>
<h3>Dani&#xeb;l Vos, Sicco Verwer</h3>
<p>In the present day we use machine learning for sensitive tasks that require
models to be both understandable and robust. Although traditional models such
as decision trees are understandable, they suffer from adversarial attacks.
When a decision tree is used to differentiate between a user's benign and
malicious behavior, an adversarial attack allows the user to effectively evade
the model by perturbing the inputs the model receives. We can use algorithms
that take adversarial attacks into account to fit trees that are more robust.
In this work we propose an algorithm, GROOT, that is two orders of magnitude
faster than the state-of-the-art-work while scoring competitively on accuracy
against adversaries. GROOT accepts an intuitive and permissible threat model.
Where previous threat models were limited to distance norms, we allow each
feature to be perturbed with a user-specified parameter: either a maximum
distance or constraints on the direction of perturbation. Previous works
assumed that both benign and malicious users attempt model evasion but we allow
the user to select which classes perform adversarial attacks. Additionally, we
introduce a hyperparameter rho that allows GROOT to trade off performance in
the regular and adversarial settings.
</p>
<a href="http://arxiv.org/abs/2012.10438" target="_blank">arXiv:2012.10438</a> [<a href="http://arxiv.org/pdf/2012.10438" target="_blank">pdf</a>]

<h2>Graph HyperNetworks for Neural Architecture Search. (arXiv:1810.05749v3 [cs.LG] UPDATED)</h2>
<h3>Chris Zhang, Mengye Ren, Raquel Urtasun</h3>
<p>Neural architecture search (NAS) automatically finds the best task-specific
neural network topology, outperforming many manual architecture designs.
However, it can be prohibitively expensive as the search requires training
thousands of different networks, while each can last for hours. In this work,
we propose the Graph HyperNetwork (GHN) to amortize the search cost: given an
architecture, it directly generates the weights by running inference on a graph
neural network. GHNs model the topology of an architecture and therefore can
predict network performance more accurately than regular hypernetworks and
premature early stopping. To perform NAS, we randomly sample architectures and
use the validation accuracy of networks with GHN generated weights as the
surrogate search signal. GHNs are fast -- they can search nearly 10 times
faster than other random search methods on CIFAR-10 and ImageNet. GHNs can be
further extended to the anytime prediction setting, where they have found
networks with better speed-accuracy tradeoff than the state-of-the-art manual
designs.
</p>
<a href="http://arxiv.org/abs/1810.05749" target="_blank">arXiv:1810.05749</a> [<a href="http://arxiv.org/pdf/1810.05749" target="_blank">pdf</a>]

<h2>Centerline Depth World Reinforcement Learning-based Left Atrial Appendage Orifice Localization. (arXiv:1904.01241v2 [cs.CV] UPDATED)</h2>
<h3>Walid Abdullah Al, Il Dong Yun, Eun Ju Chun</h3>
<p>Left atrial appendage (LAA) closure (LAAC) is a minimally invasive
implant-based method to prevent cardiovascular stroke in patients with
non-valvular atrial fibrillation. Assessing the LAA orifice in preoperative CT
angiography plays a crucial role in choosing an appropriate LAAC implant size
and a proper C-arm angulation. However, accurate orifice localization is hard
because of the high anatomic variation of LAA, and unclear position and
orientation of the orifice in available CT views. Deep localization models also
yield high error in localizing the orifice in CT image because of the tiny
structure of orifice compared to the vastness of CT image. In this paper, we
propose a centerline depth-based reinforcement learning (RL) world for
effective orifice localization in a small search space. In our scheme, an RL
agent observes the centerline-to-surface distance and navigates through the LAA
centerline to localize the orifice. Thus, the search space is significantly
reduced facilitating improved localization. The proposed formulation could
result in high localization accuracy comparing to the expert-annotations in 98
CT images. Moreover, the localization process takes about 8 seconds which is 18
times more efficient than the existing method. Therefore, this can be a useful
aid to physicians during the preprocedural planning of LAAC.
</p>
<a href="http://arxiv.org/abs/1904.01241" target="_blank">arXiv:1904.01241</a> [<a href="http://arxiv.org/pdf/1904.01241" target="_blank">pdf</a>]

<h2>Contrastive Multiview Coding. (arXiv:1906.05849v5 [cs.CV] UPDATED)</h2>
<h3>Yonglong Tian, Dilip Krishnan, Phillip Isola</h3>
<p>Humans view the world through many sensory channels, e.g., the
long-wavelength light channel, viewed by the left eye, or the high-frequency
vibrations channel, heard by the right ear. Each view is noisy and incomplete,
but important factors, such as physics, geometry, and semantics, tend to be
shared between all views (e.g., a "dog" can be seen, heard, and felt). We
investigate the classic hypothesis that a powerful representation is one that
models view-invariant factors. We study this hypothesis under the framework of
multiview contrastive learning, where we learn a representation that aims to
maximize mutual information between different views of the same scene but is
otherwise compact. Our approach scales to any number of views, and is
view-agnostic. We analyze key properties of the approach that make it work,
finding that the contrastive loss outperforms a popular alternative based on
cross-view prediction, and that the more views we learn from, the better the
resulting representation captures underlying scene semantics. Our approach
achieves state-of-the-art results on image and video unsupervised learning
benchmarks. Code is released at: this http URL
</p>
<a href="http://arxiv.org/abs/1906.05849" target="_blank">arXiv:1906.05849</a> [<a href="http://arxiv.org/pdf/1906.05849" target="_blank">pdf</a>]

<h2>Mixture-of-Experts Variational Autoencoder for Clustering and Generating from Similarity-Based Representations on Single Cell Data. (arXiv:1910.07763v3 [cs.LG] UPDATED)</h2>
<h3>Andreas Kopf, Vincent Fortuin, Vignesh Ram Somnath, Manfred Claassen</h3>
<p>Clustering high-dimensional data, such as images or biological measurements,
is a long-standingproblem and has been studied extensively. Recently, Deep
Clustering has gained popularity due toits flexibility in fitting the specific
peculiarities of complex data. Here we introduce the Mixture-of-Experts
Similarity Variational Autoencoder (MoE-Sim-VAE), a novel generative clustering
model.The model can learn multi-modal distributions of high-dimensional data
and use these to generaterealistic data with high efficacy and efficiency.
MoE-Sim-VAE is based on a Variational Autoencoder(VAE), where the decoder
consists of a Mixture-of-Experts (MoE) architecture. This specific architecture
allows for various modes of the data to be automatically learned by means of
the experts.Additionally, we encourage the lower dimensional latent
representation of our model to follow aGaussian mixture distribution and to
accurately represent the similarities between the data points. Weassess the
performance of our model on the MNIST benchmark data set and challenging
real-worldtasks of clustering mouse organs from single-cell RNA-sequencing
measurements and defining cellsubpopulations from mass cytometry (CyTOF)
measurements on hundreds of different datasets.MoE-Sim-VAE exhibits superior
clustering performance on all these tasks in comparison to thebaselines as well
as competitor methods.
</p>
<a href="http://arxiv.org/abs/1910.07763" target="_blank">arXiv:1910.07763</a> [<a href="http://arxiv.org/pdf/1910.07763" target="_blank">pdf</a>]

<h2>Unlocking the Full Potential of Small Data with Diverse Supervision. (arXiv:1911.12911v2 [cs.CV] UPDATED)</h2>
<h3>Ziqi Pang, Zhiyuan Hu, Pavel Tokmakov, Yu-Xiong Wang, Martial Hebert</h3>
<p>Virtually all of deep learning literature relies on the assumption of large
amounts of available training data. Indeed, even the majority of few-shot
learning methods rely on a large set of "base classes" for pretraining. This
assumption, however, does not always hold. For some tasks, annotating a large
number of classes can be infeasible, and even collecting the images themselves
can be a challenge in some scenarios. In this paper, we study this problem and
call it "Small Data" setting, in contrast to "Big Data". To unlock the full
potential of small data, we propose to augment the models with annotations for
other related tasks, thus increasing their generalization abilities. In
particular, we use the richly annotated scene parsing dataset ADE20K to
construct our realistic Long-tail Recognition with Diverse Supervision (LRDS)
benchmark by splitting the object categories into head and tail based on their
distribution. Following the standard few-shot learning protocol, we use the
head classes for representation learning and the tail classes for evaluation.
Moreover, we further subsample the head categories and images to generate two
novel settings which we call "Scarce-Class" and "Scarce-Image", respectively
corresponding to the shortage of samples for rare classes and training images.
Finally, we analyze the effect of applying various additional supervision
sources under the proposed settings. Our experiments demonstrate that densely
labeling a small set of images can indeed largely remedy the small data
constraints.
</p>
<a href="http://arxiv.org/abs/1911.12911" target="_blank">arXiv:1911.12911</a> [<a href="http://arxiv.org/pdf/1911.12911" target="_blank">pdf</a>]

<h2>MetaCI: Meta-Learning for Causal Inference in a Heterogeneous Population. (arXiv:1912.03960v5 [cs.LG] UPDATED)</h2>
<h3>Ankit Sharma, Garima Gupta, Ranjitha Prasad, Arnab Chatterjee, Lovekesh Vig, Gautam Shroff</h3>
<p>Performing inference on data obtained through observational studies is
becoming extremely relevant due to the widespread availability of data in
fields such as healthcare, education, retail, etc. Furthermore, this data is
accrued from multiple homogeneous subgroups of a heterogeneous population, and
hence, generalizing the inference mechanism over such data is essential. We
propose the MetaCI framework with the goal of answering counterfactual
questions in the context of causal inference (CI), where the factual
observations are obtained from several homogeneous subgroups. While the CI
network is designed to generalize from factual to counterfactual distribution
in order to tackle covariate shift, MetaCI employs the meta-learning paradigm
to tackle the shift in data distributions between training and test phase due
to the presence of heterogeneity in the population, and due to drifts in the
target distribution, also known as concept shift. We benchmark the performance
of the MetaCI algorithm using the mean absolute percentage error over the
average treatment effect as the metric, and demonstrate that meta
initialization has significant gains compared to randomly initialized networks,
and other methods.
</p>
<a href="http://arxiv.org/abs/1912.03960" target="_blank">arXiv:1912.03960</a> [<a href="http://arxiv.org/pdf/1912.03960" target="_blank">pdf</a>]

<h2>Reinforcement Learning-based Visual Navigation with Information-Theoretic Regularization. (arXiv:1912.04078v5 [cs.RO] UPDATED)</h2>
<h3>Qiaoyun Wu, Kai Xu, Jun Wang, Mingliang Xu, Xiaoxi Gong, Dinesh Manocha</h3>
<p>To enhance the cross-target and cross-scene generalization of target-driven
visual navigation based on deep reinforcement learning (RL), we introduce an
information-theoretic regularization term into the RL objective. The
regularization maximizes the mutual information between navigation actions and
visual observation transforms of an agent, thus promoting more informed
navigation decisions. This way, the agent models the action-observation
dynamics by learning a variational generative model. Based on the model, the
agent generates (imagines) the next observation from its current observation
and navigation target. This way, the agent learns to understand the causality
between navigation actions and the changes in its observations, which allows
the agent to predict the next action for navigation by comparing the current
and the imagined next observations. Cross-target and cross-scene evaluations on
the AI2-THOR framework show that our method attains at least a $10\%$
improvement of average success rate over some state-of-the-art models. We
further evaluate our model in two real-world settings: navigation in unseen
indoor scenes from a discrete Active Vision Dataset (AVD) and continuous
real-world environments with a TurtleBot.We demonstrate that our navigation
model is able to successfully achieve navigation tasks in these scenarios.
Videos and models can be found in the supplementary material.
</p>
<a href="http://arxiv.org/abs/1912.04078" target="_blank">arXiv:1912.04078</a> [<a href="http://arxiv.org/pdf/1912.04078" target="_blank">pdf</a>]

<h2>FsNet: Feature Selection Network on High-dimensional Biological Data. (arXiv:2001.08322v3 [cs.LG] UPDATED)</h2>
<h3>Dinesh Singh, H&#xe9;ctor Climente-Gonz&#xe1;lez, Mathis Petrovich, Eiryo Kawakami, Makoto Yamada</h3>
<p>Biological data including gene expression data are generally high-dimensional
and require efficient, generalizable, and scalable machine-learning methods to
discover their complex nonlinear patterns. The recent advances in machine
learning can be attributed to deep neural networks (DNNs), which excel in
various tasks in terms of computer vision and natural language processing.
However, standard DNNs are not appropriate for high-dimensional datasets
generated in biology because they have many parameters, which in turn require
many samples. In this paper, we propose a DNN-based, nonlinear feature
selection method, called the feature selection network (FsNet), for
high-dimensional and small number of sample data. Specifically, FsNet comprises
a selection layer that selects features and a reconstruction layer that
stabilizes the training. Because a large number of parameters in the selection
and reconstruction layers can easily result in overfitting under a limited
number of samples, we use two tiny networks to predict the large, virtual
weight matrices of the selection and reconstruction layers. Experimental
results on several real-world, high-dimensional biological datasets demonstrate
the efficacy of the proposed method.
</p>
<a href="http://arxiv.org/abs/2001.08322" target="_blank">arXiv:2001.08322</a> [<a href="http://arxiv.org/pdf/2001.08322" target="_blank">pdf</a>]

<h2>Learning Test-time Data Augmentation for Image Retrieval with Reinforcement Learning. (arXiv:2002.01642v2 [cs.CV] UPDATED)</h2>
<h3>Osman Tursun, Simon Denman, Sridha Sridharan, Clinton Fookes</h3>
<p>Off-the-shelf convolutional neural network features achieve outstanding
results in many image retrieval tasks. However, their invariance is pre-defined
by the network architecture and training data. Existing image retrieval
approaches require fine-tuning or modification of the pre-trained networks to
adapt to the variations in the target data. In contrast, our method enhances
the invariance of off-the-shelf features by aggregating features extracted from
images augmented with learned test-time augmentations. The optimal ensemble of
test-time augmentations is learned automatically through reinforcement
learning. Our training is time and resources efficient, and learns a diverse
test-time augmentations. Experiment results on trademark retrieval (METU
trademark dataset) and landmark retrieval (Oxford5k and Paris6k scene datasets)
tasks show the learned ensemble of transformations is effective and
transferable. We also achieve state-of-the-art MAP@100 results on the METU
trademark dataset.
</p>
<a href="http://arxiv.org/abs/2002.01642" target="_blank">arXiv:2002.01642</a> [<a href="http://arxiv.org/pdf/2002.01642" target="_blank">pdf</a>]

<h2>On Contrastive Learning for Likelihood-free Inference. (arXiv:2002.03712v2 [stat.ML] UPDATED)</h2>
<h3>Conor Durkan, Iain Murray, George Papamakarios</h3>
<p>Likelihood-free methods perform parameter inference in stochastic simulator
models where evaluating the likelihood is intractable but sampling synthetic
data is possible. One class of methods for this likelihood-free problem uses a
classifier to distinguish between pairs of parameter-observation samples
generated using the simulator and pairs sampled from some reference
distribution, which implicitly learns a density ratio proportional to the
likelihood. Another popular class of methods fits a conditional distribution to
the parameter posterior directly, and a particular recent variant allows for
the use of flexible neural density estimators for this task. In this work, we
show that both of these approaches can be unified under a general contrastive
learning scheme, and clarify how they should be run and compared.
</p>
<a href="http://arxiv.org/abs/2002.03712" target="_blank">arXiv:2002.03712</a> [<a href="http://arxiv.org/pdf/2002.03712" target="_blank">pdf</a>]

<h2>Global Convergence of Deep Networks with One Wide Layer Followed by Pyramidal Topology. (arXiv:2002.07867v3 [cs.LG] UPDATED)</h2>
<h3>Quynh Nguyen, Marco Mondelli</h3>
<p>Recent works have shown that gradient descent can find a global minimum for
over-parameterized neural networks where the widths of all the hidden layers
scale polynomially with $N$ ($N$ being the number of training samples). In this
paper, we prove that, for deep networks, a single layer of width $N$ following
the input layer suffices to ensure a similar guarantee. In particular, all the
remaining layers are allowed to have constant widths, and form a pyramidal
topology. We show an application of our result to the widely used LeCun's
initialization and obtain an over-parameterization requirement for the single
wide layer of order $N^2.$
</p>
<a href="http://arxiv.org/abs/2002.07867" target="_blank">arXiv:2002.07867</a> [<a href="http://arxiv.org/pdf/2002.07867" target="_blank">pdf</a>]

<h2>Safe Imitation Learning via Fast Bayesian Reward Inference from Preferences. (arXiv:2002.09089v4 [cs.LG] UPDATED)</h2>
<h3>Daniel S. Brown, Russell Coleman, Ravi Srinivasan, Scott Niekum</h3>
<p>Bayesian reward learning from demonstrations enables rigorous safety and
uncertainty analysis when performing imitation learning. However, Bayesian
reward learning methods are typically computationally intractable for complex
control problems. We propose Bayesian Reward Extrapolation (Bayesian REX), a
highly efficient Bayesian reward learning algorithm that scales to
high-dimensional imitation learning problems by pre-training a low-dimensional
feature encoding via self-supervised tasks and then leveraging preferences over
demonstrations to perform fast Bayesian inference. Bayesian REX can learn to
play Atari games from demonstrations, without access to the game score and can
generate 100,000 samples from the posterior over reward functions in only 5
minutes on a personal laptop. Bayesian REX also results in imitation learning
performance that is competitive with or better than state-of-the-art methods
that only learn point estimates of the reward function. Finally, Bayesian REX
enables efficient high-confidence policy evaluation without having access to
samples of the reward function. These high-confidence performance bounds can be
used to rank the performance and risk of a variety of evaluation policies and
provide a way to detect reward hacking behaviors.
</p>
<a href="http://arxiv.org/abs/2002.09089" target="_blank">arXiv:2002.09089</a> [<a href="http://arxiv.org/pdf/2002.09089" target="_blank">pdf</a>]

<h2>Ground Texture Based Localization Using Compact Binary Descriptors. (arXiv:2002.11061v2 [cs.CV] UPDATED)</h2>
<h3>Jan Fabian Schmid, Stephan F. Simon, Rudolf Mester</h3>
<p>Ground texture based localization is a promising approach to achieve
high-accuracy positioning of vehicles. We present a self-contained method that
can be used for global localization as well as for subsequent local
localization updates, i.e. it allows a robot to localize without any knowledge
of its current whereabouts, but it can also take advantage of a prior pose
estimate to reduce computation time significantly. Our method is based on a
novel matching strategy, which we call identity matching, that is based on
compact binary feature descriptors. Identity matching treats pairs of features
as matches only if their descriptors are identical. While other methods for
global localization are faster to compute, our method reaches higher
localization success rates, and can switch to local localization after the
initial localization.
</p>
<a href="http://arxiv.org/abs/2002.11061" target="_blank">arXiv:2002.11061</a> [<a href="http://arxiv.org/pdf/2002.11061" target="_blank">pdf</a>]

<h2>PushNet: Efficient and Adaptive Neural Message Passing. (arXiv:2003.02228v4 [cs.LG] UPDATED)</h2>
<h3>Julian Busch, Jiaxing Pi, Thomas Seidl</h3>
<p>Message passing neural networks have recently evolved into a state-of-the-art
approach to representation learning on graphs. Existing methods perform
synchronous message passing along all edges in multiple subsequent rounds and
consequently suffer from various shortcomings: Propagation schemes are
inflexible since they are restricted to $k$-hop neighborhoods and insensitive
to actual demands of information propagation. Further, long-range dependencies
cannot be modeled adequately and learned representations are based on
correlations of fixed locality. These issues prevent existing methods from
reaching their full potential in terms of prediction performance. Instead, we
consider a novel asynchronous message passing approach where information is
pushed only along the most relevant edges until convergence. Our proposed
algorithm can equivalently be formulated as a single synchronous message
passing iteration using a suitable neighborhood function, thus sharing the
advantages of existing methods while addressing their central issues. The
resulting neural network utilizes a node-adaptive receptive field derived from
meaningful sparse node neighborhoods. In addition, by learning and combining
node representations over differently sized neighborhoods, our model is able to
capture correlations on multiple scales. We further propose variants of our
base model with different inductive bias. Empirical results are provided for
semi-supervised node classification on five real-world datasets following a
rigorous evaluation protocol. We find that our models outperform competitors on
all datasets in terms of accuracy with statistical significance. In some cases,
our models additionally provide faster runtime.
</p>
<a href="http://arxiv.org/abs/2003.02228" target="_blank">arXiv:2003.02228</a> [<a href="http://arxiv.org/pdf/2003.02228" target="_blank">pdf</a>]

<h2>Transformation-based Adversarial Video Prediction on Large-Scale Data. (arXiv:2003.04035v2 [cs.CV] UPDATED)</h2>
<h3>Pauline Luc, Aidan Clark, Sander Dieleman, Diego de Las Casas, Yotam Doron, Albin Cassirer, Karen Simonyan</h3>
<p>Recent breakthroughs in adversarial generative modeling have led to models
capable of producing video samples of high quality, even on large and complex
datasets of real-world video. In this work, we focus on the task of video
prediction, where given a sequence of frames extracted from a video, the goal
is to generate a plausible future sequence. We first improve the state of the
art by performing a systematic empirical study of discriminator decompositions
and proposing an architecture that yields faster convergence and higher
performance than previous approaches. We then analyze recurrent units in the
generator, and propose a novel recurrent unit which transforms its past hidden
state according to predicted motion-like features, and refines it to handle
dis-occlusions, scene changes and other complex behavior. We show that this
recurrent unit consistently outperforms previous designs. Our final model leads
to a leap in the state-of-the-art performance, obtaining a test set Frechet
Video Distance of 25.7, down from 69.2, on the large-scale Kinetics-600
dataset.
</p>
<a href="http://arxiv.org/abs/2003.04035" target="_blank">arXiv:2003.04035</a> [<a href="http://arxiv.org/pdf/2003.04035" target="_blank">pdf</a>]

<h2>Unpaired Image-to-Image Translation using Adversarial Consistency Loss. (arXiv:2003.04858v6 [cs.CV] UPDATED)</h2>
<h3>Yihao Zhao, Ruihai Wu, Hao Dong</h3>
<p>Unpaired image-to-image translation is a class of vision problems whose goal
is to find the mapping between different image domains using unpaired training
data. Cycle-consistency loss is a widely used constraint for such problems.
However, due to the strict pixel-level constraint, it cannot perform geometric
changes, remove large objects, or ignore irrelevant texture. In this paper, we
propose a novel adversarial-consistency loss for image-to-image translation.
This loss does not require the translated image to be translated back to be a
specific source image but can encourage the translated images to retain
important features of the source images and overcome the drawbacks of
cycle-consistency loss noted above. Our method achieves state-of-the-art
results on three challenging tasks: glasses removal, male-to-female
translation, and selfie-to-anime translation.
</p>
<a href="http://arxiv.org/abs/2003.04858" target="_blank">arXiv:2003.04858</a> [<a href="http://arxiv.org/pdf/2003.04858" target="_blank">pdf</a>]

<h2>Minimizing Energy Use of Mixed-Fleet Public Transit for Fixed-Route Service. (arXiv:2004.05146v2 [cs.AI] UPDATED)</h2>
<h3>Amutheezan Sivagnanam, Afiya Ayman, Michael Wilbur, Philip Pugliese, Abhishek Dubey, Aron Laszka</h3>
<p>Affordable public transit services are crucial for communities since they
enable residents to access employment, education, and other services.
Unfortunately, transit services that provide wide coverage tend to suffer from
relatively low utilization, which results in high fuel usage per passenger per
mile, leading to high operating costs and environmental impact. Electric
vehicles (EVs) can reduce energy costs and environmental impact, but most
public transit agencies have to employ them in combination with conventional,
internal-combustion engine vehicles due to the high upfront costs of EVs. To
make the best use of such a mixed fleet of vehicles, transit agencies need to
optimize route assignments and charging schedules, which presents a challenging
problem for large transit networks. We introduce a novel problem formulation to
minimize fuel and electricity use by assigning vehicles to transit trips and
scheduling them for charging, while serving an existing fixed-route transit
schedule. We present an integer program for optimal assignment and scheduling,
and we propose polynomial-time heuristic and meta-heuristic algorithms for
larger networks. We evaluate our algorithms on the public transit service of
Chattanooga, TN using operational data collected from transit vehicles. Our
results show that the proposed algorithms are scalable and can reduce energy
use and, hence, environmental impact and operational costs. For Chattanooga,
the proposed algorithms can save $145,635 in energy costs and 576.7 metric tons
of CO2 emission annually.
</p>
<a href="http://arxiv.org/abs/2004.05146" target="_blank">arXiv:2004.05146</a> [<a href="http://arxiv.org/pdf/2004.05146" target="_blank">pdf</a>]

<h2>Knowledge Distillation for Action Anticipation via Label Smoothing. (arXiv:2004.07711v2 [cs.CV] UPDATED)</h2>
<h3>Guglielmo Camporese, Pasquale Coscia, Antonino Furnari, Giovanni Maria Farinella, Lamberto Ballan</h3>
<p>Human capability to anticipate near future from visual observations and
non-verbal cues is essential for developing intelligent systems that need to
interact with people. Several research areas, such as human-robot interaction
(HRI), assisted living or autonomous driving need to foresee future events to
avoid crashes or help people. Egocentric scenarios are classic examples where
action anticipation is applied due to their numerous applications. Such
challenging task demands to capture and model domain's hidden structure to
reduce prediction uncertainty. Since multiple actions may equally occur in the
future, we treat action anticipation as a multi-label problem with missing
labels extending the concept of label smoothing. This idea resembles the
knowledge distillation process since useful information is injected into the
model during training. We implement a multi-modal framework based on long
short-term memory (LSTM) networks to summarize past observations and make
predictions at different time steps. We perform extensive experiments on
EPIC-Kitchens and EGTEA Gaze+ datasets including more than 2500 and 100 action
classes, respectively. The experiments show that label smoothing systematically
improves performance of state-of-the-art models for action anticipation.
</p>
<a href="http://arxiv.org/abs/2004.07711" target="_blank">arXiv:2004.07711</a> [<a href="http://arxiv.org/pdf/2004.07711" target="_blank">pdf</a>]

<h2>Two-Level Lattice Neural Network Architectures for Control of Nonlinear Systems. (arXiv:2004.09628v2 [cs.LG] UPDATED)</h2>
<h3>James Ferlez, Xiaowu Sun, Yasser Shoukry</h3>
<p>In this paper, we consider the problem of automatically designing a Rectified
Linear Unit (ReLU) Neural Network (NN) architecture (number of layers and
number of neurons per layer) with the guarantee that it is sufficiently
parametrized to control a nonlinear system. Whereas current state-of-the-art
techniques are based on hand-picked architectures or heuristic based search to
find such NN architectures, our approach exploits the given model of the system
to design an architecture; as a result, we provide a guarantee that the
resulting NN architecture is sufficient to implement a controller that
satisfies an achievable specification. Our approach exploits two basic ideas.
First, assuming that the system can be controlled by an unknown
Lipschitz-continuous state-feedback controller with some Lipschitz constant
upper-bounded by $K_\text{cont}$, we bound the number of affine functions
needed to construct a Continuous Piecewise Affine (CPWA) function that can
approximate the unknown Lipschitz-continuous controller. Second, we utilize the
authors' recent results on a novel NN architecture named as the Two-Level
Lattice (TLL) NN architecture, which was shown to be capable of implementing
any CPWA function just from the knowledge of the number of affine functions
that compromises this CPWA function.
</p>
<a href="http://arxiv.org/abs/2004.09628" target="_blank">arXiv:2004.09628</a> [<a href="http://arxiv.org/pdf/2004.09628" target="_blank">pdf</a>]

<h2>What Makes for Good Views for Contrastive Learning?. (arXiv:2005.10243v3 [cs.CV] UPDATED)</h2>
<h3>Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, Phillip Isola</h3>
<p>Contrastive learning between multiple views of the data has recently achieved
state of the art performance in the field of self-supervised representation
learning. Despite its success, the influence of different view choices has been
less studied. In this paper, we use theoretical and empirical analysis to
better understand the importance of view selection, and argue that we should
reduce the mutual information (MI) between views while keeping task-relevant
information intact. To verify this hypothesis, we devise unsupervised and
semi-supervised frameworks that learn effective views by aiming to reduce their
MI. We also consider data augmentation as a way to reduce MI, and show that
increasing data augmentation indeed leads to decreasing MI and improves
downstream classification accuracy. As a by-product, we achieve a new
state-of-the-art accuracy on unsupervised pre-training for ImageNet
classification ($73\%$ top-1 linear readout with a ResNet-50). In addition,
transferring our models to PASCAL VOC object detection and COCO instance
segmentation consistently outperforms supervised pre-training.
Code:this http URL
</p>
<a href="http://arxiv.org/abs/2005.10243" target="_blank">arXiv:2005.10243</a> [<a href="http://arxiv.org/pdf/2005.10243" target="_blank">pdf</a>]

<h2>A Performance-Explainability Framework to Benchmark Machine Learning Methods: Application to Multivariate Time Series Classifiers. (arXiv:2005.14501v5 [cs.LG] UPDATED)</h2>
<h3>Kevin Fauvel, V&#xe9;ronique Masson, &#xc9;lisa Fromont</h3>
<p>Our research aims to propose a new performance-explainability analytical
framework to assess and benchmark machine learning methods. The framework
details a set of characteristics that systematize the
performance-explainability assessment of existing machine learning methods. In
order to illustrate the use of the framework, we apply it to benchmark the
current state-of-the-art multivariate time series classifiers.
</p>
<a href="http://arxiv.org/abs/2005.14501" target="_blank">arXiv:2005.14501</a> [<a href="http://arxiv.org/pdf/2005.14501" target="_blank">pdf</a>]

<h2>Kinematics Based Visual Localization for Skid-Steering Robots: Algorithm and Theory. (arXiv:2006.04335v4 [cs.RO] UPDATED)</h2>
<h3>Xingxing Zuo, Mingming Zhang, Yiming Chen, Guoquan Huang, Yong Liu, Mingyang Li</h3>
<p>To build commercial robots, skid-steering mechanical design is of increased
popularity due to its manufacturing simplicity and unique mechanism. However,
these also cause significant challenges on software and algorithm design,
especially for pose estimation (i.e., determining the robot's rotation and
position), which is the prerequisite of autonomous navigation. While the
general localization algorithms have been extensively studied in research
communities, there are still fundamental problems that need to be resolved for
localizing skid-steering robots that change their orientation with a skid. To
tackle this problem, we propose a probabilistic sliding-window estimator
dedicated to skid-steering robots, using measurements from a monocular camera,
the wheel encoders, and optionally an inertial measurement unit (IMU).
Specifically, we explicitly model the kinematics of skid-steering robots by
both track instantaneous centers of rotation (ICRs) and correction factors,
which are capable of compensating for the complexity of track-to-terrain
interaction, the imperfectness of mechanical design, terrain conditions and
smoothness, and so on. To prevent performance reduction in robots' lifelong
missions, the time- and location- varying kinematic parameters are estimated
online along with pose estimation states in a tightly-coupled manner. More
importantly, we conduct in-depth observability analysis for different sensors
and design configurations in this paper, which provides us with theoretical
tools in making the correct choice when building real commercial robots. In our
experiments, we validate the proposed method by both simulation tests and
real-world experiments, which demonstrate that our method outperforms competing
methods by wide margins.
</p>
<a href="http://arxiv.org/abs/2006.04335" target="_blank">arXiv:2006.04335</a> [<a href="http://arxiv.org/pdf/2006.04335" target="_blank">pdf</a>]

<h2>Modeling Shared Responses in Neuroimaging Studies through MultiView ICA. (arXiv:2006.06635v3 [stat.ML] UPDATED)</h2>
<h3>Hugo Richard, Luigi Gresele, Aapo Hyv&#xe4;rinen, Bertrand Thirion, Alexandre Gramfort, Pierre Ablin</h3>
<p>Group studies involving large cohorts of subjects are important to draw
general conclusions about brain functional organization. However, the
aggregation of data coming from multiple subjects is challenging, since it
requires accounting for large variability in anatomy, functional topography and
stimulus response across individuals. Data modeling is especially hard for
ecologically relevant conditions such as movie watching, where the experimental
setup does not imply well-defined cognitive operations.

We propose a novel MultiView Independent Component Analysis (ICA) model for
group studies, where data from each subject are modeled as a linear combination
of shared independent sources plus noise. Contrary to most group-ICA
procedures, the likelihood of the model is available in closed form. We develop
an alternate quasi-Newton method for maximizing the likelihood, which is robust
and converges quickly. We demonstrate the usefulness of our approach first on
fMRI data, where our model demonstrates improved sensitivity in identifying
common sources among subjects. Moreover, the sources recovered by our model
exhibit lower between-session variability than other methods.On
magnetoencephalography (MEG) data, our method yields more accurate source
localization on phantom data. Applied on 200 subjects from the Cam-CAN dataset
it reveals a clear sequence of evoked activity in sensor and source space.

The code is freely available at https://github.com/hugorichard/multiviewica.
</p>
<a href="http://arxiv.org/abs/2006.06635" target="_blank">arXiv:2006.06635</a> [<a href="http://arxiv.org/pdf/2006.06635" target="_blank">pdf</a>]

<h2>Video Representation Learning with Visual Tempo Consistency. (arXiv:2006.15489v2 [cs.CV] UPDATED)</h2>
<h3>Ceyuan Yang, Yinghao Xu, Bo Dai, Bolei Zhou</h3>
<p>Visual tempo, which describes how fast an action goes, has shown its
potential in supervised action recognition. In this work, we demonstrate that
visual tempo can also serve as a self-supervision signal for video
representation learning. We propose to maximize the mutual information between
representations of slow and fast videos via hierarchical contrastive learning
(VTHCL). Specifically, by sampling the same instance at slow and fast frame
rates respectively, we can obtain slow and fast video frames which share the
same semantics but contain different visual tempos. Video representations
learned from VTHCL achieve the competitive performances under the
self-supervision evaluation protocol for action recognition on UCF-101 (82.1\%)
and HMDB-51 (49.2\%). Moreover, comprehensive experiments suggest that the
learned representations are generalized well to other downstream tasks
including action detection on AVA and action anticipation on Epic-Kitchen.
Finally, we propose Instance Correspondence Map (ICM) to visualize the shared
semantics captured by contrastive learning.
</p>
<a href="http://arxiv.org/abs/2006.15489" target="_blank">arXiv:2006.15489</a> [<a href="http://arxiv.org/pdf/2006.15489" target="_blank">pdf</a>]

<h2>Beyond accuracy: quantifying trial-by-trial behaviour of CNNs and humans by measuring error consistency. (arXiv:2006.16736v3 [cs.CV] UPDATED)</h2>
<h3>Robert Geirhos, Kristof Meding, Felix A. Wichmann</h3>
<p>A central problem in cognitive science and behavioural neuroscience as well
as in machine learning and artificial intelligence research is to ascertain
whether two or more decision makers (be they brains or algorithms) use the same
strategy. Accuracy alone cannot distinguish between strategies: two systems may
achieve similar accuracy with very different strategies. The need to
differentiate beyond accuracy is particularly pressing if two systems are near
ceiling performance, like Convolutional Neural Networks (CNNs) and humans on
visual object recognition. Here we introduce trial-by-trial error consistency,
a quantitative analysis for measuring whether two decision making systems
systematically make errors on the same inputs. Making consistent errors on a
trial-by-trial basis is a necessary condition for similar processing strategies
between decision makers. Our analysis is applicable to compare algorithms with
algorithms, humans with humans, and algorithms with humans. When applying error
consistency to object recognition we obtain three main findings: (1.)
Irrespective of architecture, CNNs are remarkably consistent with one another.
(2.) The consistency between CNNs and human observers, however, is little above
what can be expected by chance alone -- indicating that humans and CNNs are
likely implementing very different strategies. (3.) CORnet-S, a recurrent model
termed the "current best model of the primate ventral visual stream", fails to
capture essential characteristics of human behavioural data and behaves
essentially like a standard purely feedforward ResNet-50 in our analysis. Taken
together, error consistency analysis suggests that the strategies used by human
and machine vision are still very different -- but we envision our
general-purpose error consistency analysis to serve as a fruitful tool for
quantifying future progress.
</p>
<a href="http://arxiv.org/abs/2006.16736" target="_blank">arXiv:2006.16736</a> [<a href="http://arxiv.org/pdf/2006.16736" target="_blank">pdf</a>]

<h2>Multi-Task Variational Information Bottleneck. (arXiv:2007.00339v3 [cs.LG] UPDATED)</h2>
<h3>Weizhu Qian, Bowei Chen, Yichao Zhang, Guanghui Wen, Franck Gechter</h3>
<p>Multi-task learning (MTL) is an important subject in machine learning and
artificial intelligence. Its applications to computer vision, signal
processing, and speech recognition are ubiquitous. Although this subject has
attracted considerable attention recently, the performance and robustness of
the existing models to different tasks have not been well balanced. This
article proposes an MTL model based on the architecture of the variational
information bottleneck (VIB), which can provide a more effective latent
representation of the input features for the downstream tasks. Extensive
observations on three public data sets under adversarial attacks show that the
proposed model is competitive to the state-of-the-art algorithms concerning the
prediction accuracy. Experimental results suggest that combining the VIB and
the task-dependent uncertainties is a very effective way to abstract valid
information from the input features for accomplishing multiple tasks.
</p>
<a href="http://arxiv.org/abs/2007.00339" target="_blank">arXiv:2007.00339</a> [<a href="http://arxiv.org/pdf/2007.00339" target="_blank">pdf</a>]

<h2>Drone swarms in fire suppression activities. (arXiv:2007.00883v2 [cs.RO] UPDATED)</h2>
<h3>Elena Ausonio, Patrizia Bagnerini, Marco Ghio</h3>
<p>Recent huge technological development of Unmanned Aerial Vehicles (UAVs) can
provide breakthrough means of fighting wildland fires. We propose an innovative
forest firefighting system based on the use of a swarm of hundreds of UAVs able
to generate a continuous flow of extinguishing liquid on the fire front,
simulating rain effect. Automatic battery replacement and refilling of the
extinguishing liquid ensure the continuity of the action, and fire-resistant
materials protect drones exposed to possible high temperatures. We demonstrate
the validity of the approach in Mediterranean scrub first computing the
critical water flow rate according to the main factors involved in the
evolution of a fire, then estimating the number of linear meters of active fire
front that can be extinguished depending on the number of drones available and
the amount of extinguishing fluid carried. A fire propagation cellular automata
model is also employed to study the evolution of the fire. Simulation results
suggest that the proposed system can successfully integrate, or in case of
low-intensity and limited extent fires completely replace, current forest
firefighting techniques.
</p>
<a href="http://arxiv.org/abs/2007.00883" target="_blank">arXiv:2007.00883</a> [<a href="http://arxiv.org/pdf/2007.00883" target="_blank">pdf</a>]

<h2>Learning Object Depth from Camera Motion and Video Object Segmentation. (arXiv:2007.05676v3 [cs.CV] UPDATED)</h2>
<h3>Brent A. Griffin, Jason J. Corso</h3>
<p>Video object segmentation, i.e., the separation of a target object from
background in video, has made significant progress on real and challenging
videos in recent years. To leverage this progress in 3D applications, this
paper addresses the problem of learning to estimate the depth of segmented
objects given some measurement of camera motion (e.g., from robot kinematics or
vehicle odometry). We achieve this by, first, introducing a diverse, extensible
dataset and, second, designing a novel deep network that estimates the depth of
objects using only segmentation masks and uncalibrated camera movement. Our
data-generation framework creates artificial object segmentations that are
scaled for changes in distance between the camera and object, and our network
learns to estimate object depth even with segmentation errors. We demonstrate
our approach across domains using a robot camera to locate objects from the YCB
dataset and a vehicle camera to locate obstacles while driving.
</p>
<a href="http://arxiv.org/abs/2007.05676" target="_blank">arXiv:2007.05676</a> [<a href="http://arxiv.org/pdf/2007.05676" target="_blank">pdf</a>]

<h2>Explicit Regularisation in Gaussian Noise Injections. (arXiv:2007.07368v3 [stat.ML] UPDATED)</h2>
<h3>Alexander Camuto, Matthew Willetts, Umut &#x15e;im&#x15f;ekli, Stephen Roberts, Chris Holmes</h3>
<p>We study the regularisation induced in neural networks by Gaussian noise
injections (GNIs). Though such injections have been extensively studied when
applied to data, there have been few studies on understanding the regularising
effect they induce when applied to network activations. Here we derive the
explicit regulariser of GNIs, obtained by marginalising out the injected noise,
and show that it penalises functions with high-frequency components in the
Fourier domain; particularly in layers closer to a neural network's output. We
show analytically and empirically that such regularisation produces calibrated
classifiers with large classification margins.
</p>
<a href="http://arxiv.org/abs/2007.07368" target="_blank">arXiv:2007.07368</a> [<a href="http://arxiv.org/pdf/2007.07368" target="_blank">pdf</a>]

<h2>Value-Decomposition Multi-Agent Actor-Critics. (arXiv:2007.12306v4 [cs.AI] UPDATED)</h2>
<h3>Jianyu Su, Stephen Adams, Peter A. Beling</h3>
<p>The exploitation of extra state information has been an active research area
in multi-agent reinforcement learning (MARL). QMIX represents the joint
action-value using a non-negative function approximator and achieves the best
performance, by far, on multi-agent benchmarks, StarCraft II micromanagement
tasks. However, our experiments show that, in some cases, QMIX is incompatible
with A2C, a training paradigm that promotes algorithm training efficiency. To
obtain a reasonable trade-off between training efficiency and algorithm
performance, we extend value-decomposition to actor-critics that are compatible
with A2C and propose a novel actor-critic framework, value-decomposition
actor-critics (VDACs). We evaluate VDACs on the testbed of StarCraft II
micromanagement tasks and demonstrate that the proposed framework improves
median performance over other actor-critic methods. Furthermore, we use a set
of ablation experiments to identify the key factors that contribute to the
performance of VDACs.
</p>
<a href="http://arxiv.org/abs/2007.12306" target="_blank">arXiv:2007.12306</a> [<a href="http://arxiv.org/pdf/2007.12306" target="_blank">pdf</a>]

<h2>Explainability in Deep Reinforcement Learning. (arXiv:2008.06693v4 [cs.AI] UPDATED)</h2>
<h3>Alexandre Heuillet, Fabien Couthouis, Natalia D&#xed;az-Rodr&#xed;guez</h3>
<p>A large set of the explainable Artificial Intelligence (XAI) literature is
emerging on feature relevance techniques to explain a deep neural network (DNN)
output or explaining models that ingest image source data. However, assessing
how XAI techniques can help understand models beyond classification tasks, e.g.
for reinforcement learning (RL), has not been extensively studied. We review
recent works in the direction to attain Explainable Reinforcement Learning
(XRL), a relatively new subfield of Explainable Artificial Intelligence,
intended to be used in general public applications, with diverse audiences,
requiring ethical, responsible and trustable algorithms. In critical situations
where it is essential to justify and explain the agent's behaviour, better
explainability and interpretability of RL models could help gain scientific
insight on the inner workings of what is still considered a black box. We
evaluate mainly studies directly linking explainability to RL, and split these
into two categories according to the way the explanations are generated:
transparent algorithms and post-hoc explainaility. We also review the most
prominent XAI works from the lenses of how they could potentially enlighten the
further deployment of the latest advances in RL, in the demanding present and
future of everyday problems.
</p>
<a href="http://arxiv.org/abs/2008.06693" target="_blank">arXiv:2008.06693</a> [<a href="http://arxiv.org/pdf/2008.06693" target="_blank">pdf</a>]

<h2>Counterfactual Generation and Fairness Evaluation Using Adversarially Learned Inference. (arXiv:2009.08270v2 [cs.CV] UPDATED)</h2>
<h3>Saloni Dash, Amit Sharma</h3>
<p>Recent studies have reported biases in machine learning image classifiers,
especially against particular demographic groups. Counterfactual examples for
an input -- perturbations that change specific features but not others -- have
been shown to be useful for evaluating explainability and fairness of machine
learning models. However, generating counterfactual examples for images is
non-trivial due to the underlying causal structure governing the various
features of an image. To be meaningful, generated perturbations need to satisfy
constraints implied by the causal model. We present a method for generating
counterfactuals by incorporating a known causal graph structure in a
conditional variant of Adversarially Learned Inference (ALI). The proposed
approach learns causal relationships between the specified attributes of an
image and generates counterfactuals in accordance with these relationships. On
Morpho-MNIST and CelebA datasets, the method generates counterfactuals that can
change specified attributes and their causal descendants while keeping other
attributes constant. As an application, we apply the generated counterfactuals
from CelebA images to evaluate fairness biases in a classifier that predicts
attractiveness of a face.
</p>
<a href="http://arxiv.org/abs/2009.08270" target="_blank">arXiv:2009.08270</a> [<a href="http://arxiv.org/pdf/2009.08270" target="_blank">pdf</a>]

<h2>Towards image-based automatic meter reading in unconstrained scenarios: A robust and efficient approach. (arXiv:2009.10181v3 [cs.CV] UPDATED)</h2>
<h3>Rayson Laroca, Alessandra B. Araujo, Luiz A. Zanlorensi, Eduardo C. de Almeida, David Menotti</h3>
<p>Existing approaches for image-based Automatic Meter Reading (AMR) have been
evaluated on images captured in well-controlled scenarios. However, real-world
meter reading presents unconstrained scenarios that are way more challenging
due to dirt, various lighting conditions, scale variations, in-plane and
out-of-plane rotations, among other factors. In this work, we present an
end-to-end approach for AMR focusing on unconstrained scenarios. Our main
contribution is the insertion of a new stage in the AMR pipeline, called corner
detection and counter classification, which enables the counter region to be
rectified -- as well as the rejection of illegible/faulty meters -- prior to
the recognition stage. We also introduce a publicly available dataset, called
Copel-AMR, that contains 12,500 meter images acquired in the field by the
service company's employees themselves, including 2,500 images of faulty meters
or cases where the reading is illegible due to occlusions. Experimental
evaluation demonstrates that the proposed system, which has three networks
operating in a cascaded mode, outperforms six baselines in terms of recognition
rate while still being quite efficient. Moreover, as very few reading errors
are tolerated in real-world applications, we show that our AMR system achieves
impressive recognition rates (i.e., &gt; 99%) when rejecting readings made with
lower confidence values.
</p>
<a href="http://arxiv.org/abs/2009.10181" target="_blank">arXiv:2009.10181</a> [<a href="http://arxiv.org/pdf/2009.10181" target="_blank">pdf</a>]

<h2>Learning Self-Expression Metrics for Scalable and Inductive Subspace Clustering. (arXiv:2009.12875v2 [cs.LG] UPDATED)</h2>
<h3>Julian Busch, Evgeniy Faerman, Matthias Schubert, Thomas Seidl</h3>
<p>Subspace clustering has established itself as a state-of-the-art approach to
clustering high-dimensional data. In particular, methods relying on the
self-expressiveness property have recently proved especially successful.
However, they suffer from two major shortcomings: First, a quadratic-size
coefficient matrix is learned directly, preventing these methods from scaling
beyond small datasets. Secondly, the trained models are transductive and thus
cannot be used to cluster out-of-sample data unseen during training. Instead of
learning self-expression coefficients directly, we propose a novel metric
learning approach to learn instead a subspace affinity function using a siamese
neural network architecture. Consequently, our model benefits from a constant
number of parameters and a constant-size memory footprint, allowing it to scale
to considerably larger datasets. In addition, we can formally show that out
model is still able to exactly recover subspace clusters given an independence
assumption. The siamese architecture in combination with a novel geometric
classifier further makes our model inductive, allowing it to cluster
out-of-sample data. Additionally, non-linear clusters can be detected by simply
adding an auto-encoder module to the architecture. The whole model can then be
trained end-to-end in a self-supervised manner. This work in progress reports
promising preliminary results on the MNIST dataset. In the spirit of
reproducible research, me make all code publicly available. In future work we
plan to investigate several extensions of our model and to expand experimental
evaluation.
</p>
<a href="http://arxiv.org/abs/2009.12875" target="_blank">arXiv:2009.12875</a> [<a href="http://arxiv.org/pdf/2009.12875" target="_blank">pdf</a>]

<h2>Spatial Frequency Bias in Convolutional Generative Adversarial Networks. (arXiv:2010.01473v3 [cs.LG] UPDATED)</h2>
<h3>Mahyar Khayatkhoei, Ahmed Elgammal</h3>
<p>As the success of Generative Adversarial Networks (GANs) on natural images
quickly propels them into various real-life applications across different
domains, it becomes more and more important to clearly understand their
limitations. Specifically, understanding GANs' capability across the full
spectrum of spatial frequencies, i.e. beyond the low-frequency dominant
spectrum of natural images, is critical for assessing the reliability of GAN
generated data in any detail-sensitive application (e.g. denoising, filling and
super-resolution in medical and satellite images). In this paper, we show that
the ability of convolutional GANs to learn a distribution is significantly
affected by the spatial frequency of the underlying carrier signal, that is,
GANs have a bias against learning high spatial frequencies. Crucially, we show
that this bias is not merely a result of the scarcity of high frequencies in
natural images, rather, it is a systemic bias hindering the learning of high
frequencies regardless of their prominence in a dataset. Furthermore, we
explain why large-scale GANs' ability to generate fine details on natural
images does not exclude them from the adverse effects of this bias. Finally, we
propose a method for manipulating this bias with minimal computational
overhead. This method can be used to explicitly direct computational resources
towards any specific spatial frequency of interest in a dataset, extending the
flexibility of GANs.
</p>
<a href="http://arxiv.org/abs/2010.01473" target="_blank">arXiv:2010.01473</a> [<a href="http://arxiv.org/pdf/2010.01473" target="_blank">pdf</a>]

<h2>Characterising Bias in Compressed Models. (arXiv:2010.03058v2 [cs.LG] UPDATED)</h2>
<h3>Sara Hooker, Nyalleng Moorosi, Gregory Clark, Samy Bengio, Emily Denton</h3>
<p>The popularity and widespread use of pruning and quantization is driven by
the severe resource constraints of deploying deep neural networks to
environments with strict latency, memory and energy requirements. These
techniques achieve high levels of compression with negligible impact on
top-line metrics (top-1 and top-5 accuracy). However, overall accuracy hides
disproportionately high errors on a small subset of examples; we call this
subset Compression Identified Exemplars (CIE). We further establish that for
CIE examples, compression amplifies existing algorithmic bias. Pruning
disproportionately impacts performance on underrepresented features, which
often coincides with considerations of fairness. Given that CIE is a relatively
small subset but a great contributor of error in the model, we propose its use
as a human-in-the-loop auditing tool to surface a tractable subset of the
dataset for further inspection or annotation by a domain expert. We provide
qualitative and quantitative support that CIE surfaces the most challenging
examples in the data distribution for human-in-the-loop auditing.
</p>
<a href="http://arxiv.org/abs/2010.03058" target="_blank">arXiv:2010.03058</a> [<a href="http://arxiv.org/pdf/2010.03058" target="_blank">pdf</a>]

<h2>Block-term Tensor Neural Networks. (arXiv:2010.04963v2 [cs.LG] UPDATED)</h2>
<h3>Jinmian Ye, Guangxi Li, Di Chen, Haiqin Yang, Shandian Zhe, Zenglin Xu</h3>
<p>Deep neural networks (DNNs) have achieved outstanding performance in a wide
range of applications, e.g., image classification, natural language processing,
etc. Despite the good performance, the huge number of parameters in DNNs brings
challenges to efficient training of DNNs and also their deployment in low-end
devices with limited computing resources. In this paper, we explore the
correlations in the weight matrices, and approximate the weight matrices with
the low-rank block-term tensors. We name the new corresponding structure as
block-term tensor layers (BT-layers), which can be easily adapted to neural
network models, such as CNNs and RNNs. In particular, the inputs and the
outputs in BT-layers are reshaped into low-dimensional high-order tensors with
a similar or improved representation power. Sufficient experiments have
demonstrated that BT-layers in CNNs and RNNs can achieve a very large
compression ratio on the number of parameters while preserving or improving the
representation power of the original DNNs.
</p>
<a href="http://arxiv.org/abs/2010.04963" target="_blank">arXiv:2010.04963</a> [<a href="http://arxiv.org/pdf/2010.04963" target="_blank">pdf</a>]

<h2>Achilles Heels for AGI/ASI via Decision Theoretic Adversaries. (arXiv:2010.05418v2 [cs.AI] UPDATED)</h2>
<h3>Stephen Casper</h3>
<p>As progress in AI continues to advance at a rapid pace, it is crucial to know
how advanced systems will make choices and in what ways they may fail. Machines
can already outsmart humans in some domains, and understanding how to safely
build systems which may have capabilities at or above the human level is of
particular concern. One might suspect that superhumanly-intelligent systems
should be modeled as as something which humans, by definition, can't outsmart.
However, as a challenge to this assumption, this paper presents the Achilles
Heel hypothesis which states that highly-effective goal-oriented systems --
even ones that are potentially superintelligent -- may nonetheless have stable
decision theoretic delusions which cause them to make obviously irrational
decisions in adversarial settings. In a survey of relevant dilemmas and
paradoxes from the decision theory literature, a number of these potential
Achilles Heels are discussed in context of this hypothesis. Several novel
contributions are made involving the ways in which these weaknesses could be
implanted into a system.
</p>
<a href="http://arxiv.org/abs/2010.05418" target="_blank">arXiv:2010.05418</a> [<a href="http://arxiv.org/pdf/2010.05418" target="_blank">pdf</a>]

<h2>Maximum-Entropy Adversarial Data Augmentation for Improved Generalization and Robustness. (arXiv:2010.08001v2 [cs.LG] UPDATED)</h2>
<h3>Long Zhao, Ting Liu, Xi Peng, Dimitris Metaxas</h3>
<p>Adversarial data augmentation has shown promise for training robust deep
neural networks against unforeseen data shifts or corruptions. However, it is
difficult to define heuristics to generate effective fictitious target
distributions containing "hard" adversarial perturbations that are largely
different from the source distribution. In this paper, we propose a novel and
effective regularization term for adversarial data augmentation. We
theoretically derive it from the information bottleneck principle, which
results in a maximum-entropy formulation. Intuitively, this regularization term
encourages perturbing the underlying source distribution to enlarge predictive
uncertainty of the current model, so that the generated "hard" adversarial
perturbations can improve the model robustness during training. Experimental
results on three standard benchmarks demonstrate that our method consistently
outperforms the existing state of the art by a statistically significant
margin.
</p>
<a href="http://arxiv.org/abs/2010.08001" target="_blank">arXiv:2010.08001</a> [<a href="http://arxiv.org/pdf/2010.08001" target="_blank">pdf</a>]

<h2>Boosting High-Level Vision with Joint Compression Artifacts Reduction and Super-Resolution. (arXiv:2010.08919v2 [cs.CV] UPDATED)</h2>
<h3>Xiaoyu Xiang, Qian Lin, Jan P. Allebach</h3>
<p>Due to the limits of bandwidth and storage space, digital images are usually
down-scaled and compressed when transmitted over networks, resulting in loss of
details and jarring artifacts that can lower the performance of high-level
visual tasks. In this paper, we aim to generate an artifact-free
high-resolution image from a low-resolution one compressed with an arbitrary
quality factor by exploring joint compression artifacts reduction (CAR) and
super-resolution (SR) tasks. First, we propose a context-aware joint CAR and SR
neural network (CAJNN) that integrates both local and non-local features to
solve CAR and SR in one-stage. Finally, a deep reconstruction network is
adopted to predict high quality and high-resolution images. Evaluation on CAR
and SR benchmark datasets shows that our CAJNN model outperforms previous
methods and also takes 26.2% shorter runtime. Based on this model, we explore
addressing two critical challenges in high-level computer vision: optical
character recognition of low-resolution texts, and extremely tiny face
detection. We demonstrate that CAJNN can serve as an effective image
preprocessing method and improve the accuracy for real-scene text recognition
(from 85.30% to 85.75%) and the average precision for tiny face detection (from
0.317 to 0.611).
</p>
<a href="http://arxiv.org/abs/2010.08919" target="_blank">arXiv:2010.08919</a> [<a href="http://arxiv.org/pdf/2010.08919" target="_blank">pdf</a>]

<h2>Bidirectional Microrocker Bots Controlled via Neutral Position Offset. (arXiv:2010.11295v2 [cs.RO] UPDATED)</h2>
<h3>Tony Wang, DeaGyu Kim, Yifan Shi, Zhijian Hao, Azadeh Ansari</h3>
<p>The recent advancements in nanoscale 3D printing and microfabrication
techniques have reinvigorated research on microrobots. However, precise motion
control of the microrobots on biological environments using compact actuation
setups remains challenging to date. This work presents a novel control
mechanism and contact design that enables bidirectional steering via biasing
the neutral position of the microrobot. Equipped with rockers to contact the
substrate, the microrobot, hence microrocker bot, is capable of well-controlled
forward and backward movement on flat and non-flat biological surfaces. The
100um by 113um by 36um robots were 3D printed via two-photon lithography and
subsequently deposited with nickel thin films. Under a relatively small static
magnetic field, the microrocker bot tilts either forward or backward to align
the thin film magnetization direction with the magnetic field lines. When
combined with an oscillating magnetic field, the robot undergoes stick-slip
motion in the predisposed direction, dictated by the neutral position tilt. The
microrocker bots are further equipped with sharp mechanical tips that can be
selectively engaged. When the frequency and offset of the actuation sawtooth
waveform are optimized, the robot travels up to 100um/s (1 body length per
second) forward and backward showing very linear trajectories. Finally, to
prove the functionality of the microrocker bots in direct contact with
biological surfaces, we demonstrate the robot's ability to traverse forward and
backward on the surface of a Dracaena Fragrans leaf, and upend/engage on its
mechanical tip.
</p>
<a href="http://arxiv.org/abs/2010.11295" target="_blank">arXiv:2010.11295</a> [<a href="http://arxiv.org/pdf/2010.11295" target="_blank">pdf</a>]

<h2>Towards Safe Policy Improvement for Non-Stationary MDPs. (arXiv:2010.12645v2 [cs.LG] UPDATED)</h2>
<h3>Yash Chandak, Scott M. Jordan, Georgios Theocharous, Martha White, Philip S. Thomas</h3>
<p>Many real-world sequential decision-making problems involve critical systems
with financial risks and human-life risks. While several works in the past have
proposed methods that are safe for deployment, they assume that the underlying
problem is stationary. However, many real-world problems of interest exhibit
non-stationarity, and when stakes are high, the cost associated with a false
stationarity assumption may be unacceptable. We take the first steps towards
ensuring safety, with high confidence, for smoothly-varying non-stationary
decision problems. Our proposed method extends a type of safe algorithm, called
a Seldonian algorithm, through a synthesis of model-free reinforcement learning
with time-series analysis. Safety is ensured using sequential hypothesis
testing of a policy's forecasted performance, and confidence intervals are
obtained using wild bootstrap.
</p>
<a href="http://arxiv.org/abs/2010.12645" target="_blank">arXiv:2010.12645</a> [<a href="http://arxiv.org/pdf/2010.12645" target="_blank">pdf</a>]

<h2>Stabilizing Transformer-Based Action Sequence Generation For Q-Learning. (arXiv:2010.12698v2 [cs.LG] UPDATED)</h2>
<h3>Gideon Stein, Andrey Filchenkov, Arip Asadulaev</h3>
<p>Since the publication of the original Transformer architecture (Vaswani et
al. 2017), Transformers revolutionized the field of Natural Language
Processing. This, mainly due to their ability to understand timely dependencies
better than competing RNN-based architectures. Surprisingly, this architecture
change does not affect the field of Reinforcement Learning (RL), even though
RNNs are quite popular in RL, and time dependencies are very common in RL.
Recently, Parisotto et al. 2019) conducted the first promising research of
Transformers in RL. To support the findings of this work, this paper seeks to
provide an additional example of a Transformer-based RL method. Specifically,
the goal is a simple Transformer-based Deep Q-Learning method that is stable
over several environments. Due to the unstable nature of Transformers and RL,
an extensive method search was conducted to arrive at a final method that
leverages developments around Transformers as well as Q-learning. The proposed
method can match the performance of classic Q-learning on control environments
while showing potential on some selected Atari benchmarks. Furthermore, it was
critically evaluated to give additional insights into the relation between
Transformers and RL.
</p>
<a href="http://arxiv.org/abs/2010.12698" target="_blank">arXiv:2010.12698</a> [<a href="http://arxiv.org/pdf/2010.12698" target="_blank">pdf</a>]

<h2>Hierarchical clustering in particle physics through reinforcement learning. (arXiv:2011.08191v2 [cs.AI] UPDATED)</h2>
<h3>Johann Brehmer, Sebastian Macaluso, Duccio Pappadopulo, Kyle Cranmer</h3>
<p>Particle physics experiments often require the reconstruction of decay
patterns through a hierarchical clustering of the observed final-state
particles. We show that this task can be phrased as a Markov Decision Process
and adapt reinforcement learning algorithms to solve it. In particular, we show
that Monte-Carlo Tree Search guided by a neural policy can construct
high-quality hierarchical clusterings and outperform established greedy and
beam search baselines.
</p>
<a href="http://arxiv.org/abs/2011.08191" target="_blank">arXiv:2011.08191</a> [<a href="http://arxiv.org/pdf/2011.08191" target="_blank">pdf</a>]

<h2>Quantifying Uncertainty from Different Sources in Deep Neural Networks for Image Classification. (arXiv:2011.08712v3 [cs.CV] UPDATED)</h2>
<h3>Aria Khoshsirat</h3>
<p>Quantifying uncertainty in a model's predictions is important as it enables,
for example, the safety of an AI system to be increased by acting on the
model's output in an informed manner. We cannot expect a system to be 100%
accurate or perfect at its task, however, we can equip the system with some
tools to inform us if it is not certain about a prediction. This way, a second
check can be performed, or the task can be passed to a human specialist. This
is crucial for applications where the cost of an error is high, such as in
autonomous vehicle control, medical image analysis, financial estimations or
legal fields. Deep Neural Networks are powerful black box predictors that have
recently achieved impressive performance on a wide spectrum of tasks.
Quantifying predictive uncertainty in DNNs is a challenging and yet on-going
problem. Although there have been many efforts to equip NNs with tools to
estimate uncertainty, such as Monte Carlo Dropout, most of the previous methods
only focus on one of the three types of model, data or distributional
uncertainty. In this paper we propose a complete framework to capture and
quantify all of these three types of uncertainties in DNNs for image
classification. This framework includes an ensemble of CNNs for model
uncertainty, a supervised reconstruction auto-encoder to capture distributional
uncertainty and using the output of activation functions in the last layer of
the network, to capture data uncertainty. Finally we demonstrate the efficiency
of our method on popular image datasets for classification.
</p>
<a href="http://arxiv.org/abs/2011.08712" target="_blank">arXiv:2011.08712</a> [<a href="http://arxiv.org/pdf/2011.08712" target="_blank">pdf</a>]

<h2>Scale-covariant and scale-invariant Gaussian derivative networks. (arXiv:2011.14759v3 [cs.CV] UPDATED)</h2>
<h3>Tony Lindeberg</h3>
<p>This paper presents a hybrid approach between scale-space theory and deep
learning, where a deep learning architecture is constructed by coupling
parameterized scale-space operations in cascade. By sharing the learnt
parameters between multiple scale channels, and by using the transformation
properties of the scale-space primitives under scaling transformations, the
resulting network becomes provably scale covariant. By in addition performing
max pooling over the multiple scale channels, a resulting network architecture
for image classification also becomes provably scale invariant. We investigate
the performance of such networks on the MNISTLargeScale dataset, which contains
rescaled images from original MNIST over a factor 4 concerning training data
and over a factor of 16 concerning testing data. It is demonstrated that the
resulting approach allows for scale generalization, enabling good performance
for classifying patterns at scales not present in the training data.
</p>
<a href="http://arxiv.org/abs/2011.14759" target="_blank">arXiv:2011.14759</a> [<a href="http://arxiv.org/pdf/2011.14759" target="_blank">pdf</a>]

<h2>SAFCAR: Structured Attention Fusion for Compositional Action Recognition. (arXiv:2012.02109v2 [cs.CV] UPDATED)</h2>
<h3>Tae Soo Kim, Gregory D. Hager</h3>
<p>We present a general framework for compositional action recognition -- i.e.
action recognition where the labels are composed out of simpler components such
as subjects, atomic-actions and objects. The main challenge in compositional
action recognition is that there is a combinatorially large set of possible
actions that can be composed using basic components. However, compositionality
also provides a structure that can be exploited. To do so, we develop and test
a novel Structured Attention Fusion (SAF) self-attention mechanism to combine
information from object detections, which capture the time-series structure of
an action, with visual cues that capture contextual information. We show that
our approach recognizes novel verb-noun compositions more effectively than
current state of the art systems, and it generalizes to unseen action
categories quite efficiently from only a few labeled examples. We validate our
approach on the challenging Something-Else tasks from the
Something-Something-V2 dataset. We further show that our framework is flexible
and can generalize to a new domain by showing competitive results on the
Charades-Fewshot dataset.
</p>
<a href="http://arxiv.org/abs/2012.02109" target="_blank">arXiv:2012.02109</a> [<a href="http://arxiv.org/pdf/2012.02109" target="_blank">pdf</a>]

<h2>Comparison of Anomaly Detectors: Context Matters. (arXiv:2012.06260v2 [cs.LG] UPDATED)</h2>
<h3>V&#xed;t &#x160;kv&#xe1;ra, Jan Franc&#x16f;, Mat&#x11b;j Zorek, Tom&#xe1;&#x161; Pevn&#xfd;, V&#xe1;clav &#x160;m&#xed;dl</h3>
<p>Deep generative models are challenging the classical methods in the field of
anomaly detection nowadays. Every new method provides evidence of outperforming
its predecessors, often with contradictory results. The objective of this
comparison is twofold: comparison of anomaly detection methods of various
paradigms, and identification of sources of variability that can yield
different results. The methods were compared on popular tabular and image
datasets. While the one class support-vector machine (OC-SVM) had no rival on
the tabular datasets, the best results on the image data were obtained either
by a feature-matching GAN or a combination of variational autoencoder (VAE) and
OC-SVM, depending on the experimental conditions. The main sources of
variability that can influence the performance of the methods were identified
to be: the range of searched hyper-parameters, the methodology of model
selection, and the choice of the anomalous samples. All our code and results
are available for download.
</p>
<a href="http://arxiv.org/abs/2012.06260" target="_blank">arXiv:2012.06260</a> [<a href="http://arxiv.org/pdf/2012.06260" target="_blank">pdf</a>]

<h2>Data optimization for large batch distributed training of deep neural networks. (arXiv:2012.09272v2 [cs.LG] UPDATED)</h2>
<h3>Shubhankar Gahlot, Junqi Yin, Mallikarjun Shankar</h3>
<p>Distributed training in deep learning (DL) is common practice as data and
models grow. The current practice for distributed training of deep neural
networks faces the challenges of communication bottlenecks when operating at
scale, and model accuracy deterioration with an increase in global batch size.
Present solutions focus on improving message exchange efficiency as well as
implementing techniques to tweak batch sizes and models in the training
process. The loss of training accuracy typically happens because the loss
function gets trapped in a local minima. We observe that the loss landscape
minimization is shaped by both the model and training data and propose a data
optimization approach that utilizes machine learning to implicitly smooth out
the loss landscape resulting in fewer local minima. Our approach filters out
data points which are less important to feature learning, enabling us to speed
up the training of models on larger batch sizes to improved accuracy.
</p>
<a href="http://arxiv.org/abs/2012.09272" target="_blank">arXiv:2012.09272</a> [<a href="http://arxiv.org/pdf/2012.09272" target="_blank">pdf</a>]

<h2>Stabilizing Q Learning Via Soft Mellowmax Operator. (arXiv:2012.09456v2 [cs.LG] UPDATED)</h2>
<h3>Yaozhong Gan, Zhe Zhang, Xiaoyang Tan</h3>
<p>Learning complicated value functions in high dimensional state space by
function approximation is a challenging task, partially due to that the
max-operator used in temporal difference updates can theoretically cause
instability for most linear or non-linear approximation schemes. Mellowmax is a
recently proposed differentiable and non-expansion softmax operator that allows
a convergent behavior in learning and planning. Unfortunately, the performance
bound for the fixed point it converges to remains unclear, and in practice, its
parameter is sensitive to various domains and has to be tuned case by case.
Finally, the Mellowmax operator may suffer from oversmoothing as it ignores the
probability being taken for each action when aggregating them. In this paper,
we address all the above issues with an enhanced Mellowmax operator, named SM2
(Soft Mellowmax). Particularly, the proposed operator is reliable, easy to
implement, and has provable performance guarantee, while preserving all the
advantages of Mellowmax. Furthermore, we show that our SM2 operator can be
applied to the challenging multi-agent reinforcement learning scenarios,
leading to stable value function approximation and state of the art
performance.
</p>
<a href="http://arxiv.org/abs/2012.09456" target="_blank">arXiv:2012.09456</a> [<a href="http://arxiv.org/pdf/2012.09456" target="_blank">pdf</a>]

<h2>Cost-sensitive Hierarchical Clustering for Dynamic Classifier Selection. (arXiv:2012.09608v2 [cs.LG] UPDATED)</h2>
<h3>Meinolf Sellmann, Tapan Shah</h3>
<p>We consider the dynamic classifier selection (DCS) problem: Given an ensemble
of classifiers, we are to choose which classifier to use depending on the
particular input vector that we get to classify. The problem is a special case
of the general algorithm selection problem where we have multiple different
algorithms we can employ to process a given input. We investigate if a method
developed for general algorithm selection named cost-sensitive hierarchical
clustering (CSHC) is suited for DCS. We introduce some additions to the
original CSHC method for the special case of choosing a classification
algorithm and evaluate their impact on performance. We then compare with a
number of state-of-the-art dynamic classifier selection methods. Our
experimental results show that our modified CSHC algorithm compares favorably
</p>
<a href="http://arxiv.org/abs/2012.09608" target="_blank">arXiv:2012.09608</a> [<a href="http://arxiv.org/pdf/2012.09608" target="_blank">pdf</a>]

<h2>RainNet: A Large-Scale Dataset for Spatial Precipitation Downscaling. (arXiv:2012.09700v2 [cs.CV] UPDATED)</h2>
<h3>Xuanhong Chen, Kairui Feng, Naiyuan Liu, Yifan Lu, Zhengyan Tong, Bingbing Ni, Ziang Liu, Ning Lin</h3>
<p>Spatial Precipitation Downscaling is one of the most important problems in
the geo-science community. However, it still remains an unaddressed issue. Deep
learning is a promising potential solution for downscaling. In order to
facilitate the research on precipitation downscaling for deep learning, we
present the first REAL (non-simulated) Large-Scale Spatial Precipitation
Downscaling Dataset, RainNet, which contains 62,424 pairs of low-resolution and
high-resolution precipitation maps for 17 years. Contrary to simulated data,
this real dataset covers various types of real meteorological phenomena (e.g.,
Hurricane, Squall, etc.), and shows the physical characters - Temporal
Misalignment, Temporal Sparse and Fluid Properties - that challenge the
downscaling algorithms. In order to fully explore potential downscaling
solutions, we propose an implicit physical estimation framework to learn the
above characteristics. Eight metrics specifically considering the physical
property of the data set are raised, while fourteen models are evaluated on the
proposed dataset. Finally, we analyze the effectiveness and feasibility of
these models on precipitation downscaling task. The Dataset and Code will be
available at https://neuralchen.github.io/RainNet/.
</p>
<a href="http://arxiv.org/abs/2012.09700" target="_blank">arXiv:2012.09700</a> [<a href="http://arxiv.org/pdf/2012.09700" target="_blank">pdf</a>]

<h2>Infinite Nature: Perpetual View Generation of Natural Scenes from a Single Image. (arXiv:2012.09855v2 [cs.CV] UPDATED)</h2>
<h3>Andrew Liu, Richard Tucker, Varun Jampani, Ameesh Makadia, Noah Snavely, Angjoo Kanazawa</h3>
<p>We introduce the problem of perpetual view generation -- long-range
generation of novel views corresponding to an arbitrarily long camera
trajectory given a single image. This is a challenging problem that goes far
beyond the capabilities of current view synthesis methods, which work for a
limited range of viewpoints and quickly degenerate when presented with a large
camera motion. Methods designed for video generation also have limited ability
to produce long video sequences and are often agnostic to scene geometry. We
take a hybrid approach that integrates both geometry and image synthesis in an
iterative render, refine, and repeat framework, allowing for long-range
generation that cover large distances after hundreds of frames. Our approach
can be trained from a set of monocular video sequences without any manual
annotation. We propose a dataset of aerial footage of natural coastal scenes,
and compare our method with recent view synthesis and conditional video
generation baselines, showing that it can generate plausible scenes for much
longer time horizons over large camera trajectories compared to existing
methods. Please visit our project page at https://infinite-nature.github.io/.
</p>
<a href="http://arxiv.org/abs/2012.09855" target="_blank">arXiv:2012.09855</a> [<a href="http://arxiv.org/pdf/2012.09855" target="_blank">pdf</a>]

