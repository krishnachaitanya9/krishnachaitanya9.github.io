---
title: Latest Deep Learning Papers
date: 2020-11-11 21:00:01 +0800
featured-img: DL-logo
categories: [Deep_Learning]
tags: [Deep_Learning]
mathjax: Yes
---

<h1>Your arXiv Feed (180 Articles)</h1>
<h2>SALR: Sharpness-aware Learning Rates for Improved Generalization. (arXiv:2011.05348v1 [cs.LG])</h2>
<h3>Xubo Yue, Maher Nouiehed, Raed Al Kontar</h3>
<p>In an effort to improve generalization in deep learning, we propose SALR: a
sharpness-aware learning rate update technique designed to recover flat
minimizers. Our method dynamically updates the learning rate of gradient-based
optimizers based on the local sharpness of the loss function. This allows
optimizers to automatically increase learning rates at sharp valleys to
increase the chance of escaping them. We demonstrate the effectiveness of SALR
when adopted by various algorithms over a broad range of networks. Our
experiments indicate that SALR improves generalization, converges faster, and
drives solutions to significantly flatter regions.
</p>
<a href="http://arxiv.org/abs/2011.05348" target="_blank">arXiv:2011.05348</a> [<a href="http://arxiv.org/pdf/2011.05348" target="_blank">pdf</a>]

<h2>Discrete solution pools and noise-contrastive estimation for predict-and-optimize. (arXiv:2011.05354v1 [cs.LG])</h2>
<h3>Maxime Mulamba, Jayanta Mandi, Michelangelo Diligenti, Michele Lombardi, Victor Bucarey, Tias Guns</h3>
<p>Numerous real-life decision-making processes involve solving a combinatorial
optimization problem with uncertain input that can be estimated from historic
data. There is a growing interest in decision-focused learning methods, where
the loss function used for learning to predict the uncertain input uses the
outcome of solving the combinatorial problem over a set of predictions.
Different surrogate loss functions have been identified, often using a
continuous approximation of the combinatorial problem. However, a key
bottleneck is that to compute the loss, one has to solve the combinatorial
optimisation problem for each training instance in each epoch, which is
computationally expensive even in the case of continuous approximations.

We propose a different solver-agnostic method for decision-focused learning,
namely by considering a pool of feasible solutions as a discrete approximation
of the full combinatorial problem. Solving is now trivial through a single pass
over the solution pool. We design several variants of a noise-contrastive loss
over the solution pool, which we substantiate theoretically and empirically.
Furthermore, we show that by dynamically re-solving only a fraction of the
training instances each epoch, our method performs on par with the state of the
art, whilst drastically reducing the time spent solving, hence increasing the
feasibility of predict-and-optimize for larger problems.
</p>
<a href="http://arxiv.org/abs/2011.05354" target="_blank">arXiv:2011.05354</a> [<a href="http://arxiv.org/pdf/2011.05354" target="_blank">pdf</a>]

<h2>Selective Spatio-Temporal Aggregation Based Pose Refinement System: Towards Understanding Human Activities in Real-World Videos. (arXiv:2011.05358v1 [cs.CV])</h2>
<h3>Di Yang, Rui Dai, Yaohui Wang, Rupayan Mallick, Luca Minciullo, Gianpiero Francesca, Francois Bremond</h3>
<p>Taking advantage of human pose data for understanding human activities has
attracted much attention these days. However, state-of-the-art pose estimators
struggle in obtaining high-quality 2D or 3D pose data due to occlusion,
truncation and low-resolution in real-world un-annotated videos. Hence, in this
work, we propose 1) a Selective Spatio-Temporal Aggregation mechanism, named
SST-A, that refines and smooths the keypoint locations extracted by multiple
expert pose estimators, 2) an effective weakly-supervised self-training
framework which leverages the aggregated poses as pseudo ground-truth instead
of handcrafted annotations for real-world pose estimation. Extensive
experiments are conducted for evaluating not only the upstream pose refinement
but also the downstream action recognition performance on four datasets, Toyota
Smarthome, NTU-RGB+D, Charades, and Kinetics-50. We demonstrate that the
skeleton data refined by our Pose-Refinement system (SSTA-PRS) is effective at
boosting various existing action recognition models, which achieves competitive
or state-of-the-art performance.
</p>
<a href="http://arxiv.org/abs/2011.05358" target="_blank">arXiv:2011.05358</a> [<a href="http://arxiv.org/pdf/2011.05358" target="_blank">pdf</a>]

<h2>Learning Discrete Energy-based Models via Auxiliary-variable Local Exploration. (arXiv:2011.05363v1 [cs.LG])</h2>
<h3>Hanjun Dai, Rishabh Singh, Bo Dai, Charles Sutton, Dale Schuurmans</h3>
<p>Discrete structures play an important role in applications like program
language modeling and software engineering. Current approaches to predicting
complex structures typically consider autoregressive models for their
tractability, with some sacrifice in flexibility. Energy-based models (EBMs) on
the other hand offer a more flexible and thus more powerful approach to
modeling such distributions, but require partition function estimation. In this
paper we propose ALOE, a new algorithm for learning conditional and
unconditional EBMs for discrete structured data, where parameter gradients are
estimated using a learned sampler that mimics local search. We show that the
energy function and sampler can be trained efficiently via a new variational
form of power iteration, achieving a better trade-off between flexibility and
tractability. Experimentally, we show that learning local search leads to
significant improvements in challenging application domains. Most notably, we
present an energy model guided fuzzer for software testing that achieves
comparable performance to well engineered fuzzing engines like libfuzzer.
</p>
<a href="http://arxiv.org/abs/2011.05363" target="_blank">arXiv:2011.05363</a> [<a href="http://arxiv.org/pdf/2011.05363" target="_blank">pdf</a>]

<h2>Learning ODE Models with Qualitative Structure Using Gaussian Processes. (arXiv:2011.05364v1 [cs.LG])</h2>
<h3>Steffen Ridderbusch (1), Paul Goulart (1), Sina Ober-Bl&#xf6;baum (2) ((1) University of Oxford, (2) University of Paderborn)</h3>
<p>Recent advances in learning techniques have enabled the modelling of
dynamical systems for scientific and engineering applications directly from
data. However, in many contexts, explicit data collection is expensive and
learning algorithms must be data-efficient to be feasible. This suggests using
additional qualitative information about the system, which is often available
from prior experiments or domain knowledge. In this paper, we propose an
approach to learning the vector field of differential equations using sparse
Gaussian Processes that allows us to combine data and additional structural
information, like Lie Group symmetries and fixed points, as well as known input
transformations. We show that this combination improves extrapolation
performance and long-term behaviour significantly, while also reducing the
computational cost.
</p>
<a href="http://arxiv.org/abs/2011.05364" target="_blank">arXiv:2011.05364</a> [<a href="http://arxiv.org/pdf/2011.05364" target="_blank">pdf</a>]

<h2>Predicting Water Temperature Dynamics of Unmonitored Lakes with Meta Transfer Learning. (arXiv:2011.05369v1 [cs.LG])</h2>
<h3>Jared D. Willard, Jordan S. Read, Alison P. Appling, Samantha K. Oliver, Xiaowei Jia, Vipin Kumar</h3>
<p>Most environmental data come from a minority of well-observed sites. An
ongoing challenge in the environmental sciences is transferring knowledge from
monitored sites to unobserved sites. Here, we demonstrate a novel transfer
learning framework that accurately predicts temperature in unobserved lakes
(targets) by borrowing models from highly observed lakes (sources). This
method, Meta Transfer Learning (MTL), builds a meta-learning model to predict
transfer performance from candidate source models to targets using lake
attributes and candidates' past performance. We constructed source models at
145 well-observed lakes using calibrated process-based modeling (PB) and a
recently developed approach called process-guided deep learning (PGDL). We
applied MTL to either PB or PGDL source models (PB-MTL or PGDL-MTL,
respectively) to predict temperatures in 305 target lakes treated as unobserved
in the Upper Midwestern United States. We show significantly improved
performance relative to the uncalibrated process-based General Lake Model,
where the median RMSE for the target lakes is $2.52^{\circ}C$. PB-MTL yielded a
median RMSE of $2.43^{\circ}C$; PGDL-MTL yielded $2.16^{\circ}C$; and a
PGDL-MTL ensemble of nine sources per target yielded $1.88^{\circ}C$. For
sparsely observed target lakes, PGDL-MTL often outperformed PGDL models trained
on the target lakes themselves. Differences in maximum depth between the source
and target were consistently the most important predictors. Our approach
readily scales to thousands of lakes in the Midwestern United States,
demonstrating that MTL with meaningful predictor variables and high-quality
source models is a promising approach for many kinds of unmonitored systems and
environmental variables.
</p>
<a href="http://arxiv.org/abs/2011.05369" target="_blank">arXiv:2011.05369</a> [<a href="http://arxiv.org/pdf/2011.05369" target="_blank">pdf</a>]

<h2>Collaborative Augmented Reality on Smartphones via Life-long City-scale Maps. (arXiv:2011.05370v1 [cs.CV])</h2>
<h3>Lukas Platinsky, Michal Szabados, Filip Hlasek, Ross Hemsley, Luca Del Pero, Andrej Pancik, Bryan Baum, Hugo Grimmett, Peter Ondruska</h3>
<p>In this paper we present the first published end-to-end production
computer-vision system for powering city-scale shared augmented reality
experiences on mobile devices. In doing so we propose a new formulation for an
experience-based mapping framework as an effective solution to the key issues
of city-scale SLAM scalability, robustness, map updates and all-time
all-weather performance required by a production system. Furthermore, we
propose an effective way of synchronising SLAM systems to deliver seamless
real-time localisation of multiple edge devices at the same time. All this in
the presence of network latency and bandwidth limitations. The resulting system
is deployed and tested at scale in San Francisco where it delivers AR
experiences in a mapped area of several hundred kilometers. To foster further
development of this area we offer the data set to the public, constituting the
largest of this kind to date.
</p>
<a href="http://arxiv.org/abs/2011.05370" target="_blank">arXiv:2011.05370</a> [<a href="http://arxiv.org/pdf/2011.05370" target="_blank">pdf</a>]

<h2>Emergent Reciprocity and Team Formation from Randomized Uncertain Social Preferences. (arXiv:2011.05373v1 [cs.LG])</h2>
<h3>Bowen Baker</h3>
<p>Multi-agent reinforcement learning (MARL) has shown recent success in
increasingly complex fixed-team zero-sum environments. However, the real world
is not zero-sum nor does it have fixed teams; humans face numerous social
dilemmas and must learn when to cooperate and when to compete. To successfully
deploy agents into the human world, it may be important that they be able to
understand and help in our conflicts. Unfortunately, selfish MARL agents
typically fail when faced with social dilemmas. In this work, we show evidence
of emergent direct reciprocity, indirect reciprocity and reputation, and team
formation when training agents with randomized uncertain social preferences
(RUSP), a novel environment augmentation that expands the distribution of
environments agents play in. RUSP is generic and scalable; it can be applied to
any multi-agent environment without changing the original underlying game
dynamics or objectives. In particular, we show that with RUSP these behaviors
can emerge and lead to higher social welfare equilibria in both classic
abstract social dilemmas like Iterated Prisoner's Dilemma as well in more
complex intertemporal environments.
</p>
<a href="http://arxiv.org/abs/2011.05373" target="_blank">arXiv:2011.05373</a> [<a href="http://arxiv.org/pdf/2011.05373" target="_blank">pdf</a>]

<h2>Applications of Online Nonnegative Matrix Factorization to Image and Time-Series Data. (arXiv:2011.05384v1 [cs.LG])</h2>
<h3>Hanbaek Lyu, Georg Menz, Deanna Needell, Christopher Strohmeier</h3>
<p>Online nonnegative matrix factorization (ONMF) is a matrix factorization
technique in the online setting where data are acquired in a streaming fashion
and the matrix factors are updated each time. This enables factor analysis to
be performed concurrently with the arrival of new data samples. In this
article, we demonstrate how one can use online nonnegative matrix factorization
algorithms to learn joint dictionary atoms from an ensemble of correlated data
sets. We propose a temporal dictionary learning scheme for time-series data
sets, based on ONMF algorithms. We demonstrate our dictionary learning
technique in the application contexts of historical temperature data, video
frames, and color images.
</p>
<a href="http://arxiv.org/abs/2011.05384" target="_blank">arXiv:2011.05384</a> [<a href="http://arxiv.org/pdf/2011.05384" target="_blank">pdf</a>]

<h2>Learning for Integer-Constrained Optimization through Neural Networks with Limited Training. (arXiv:2011.05399v1 [cs.LG])</h2>
<h3>Zhou Zhou, Shashank Jere, Lizhong Zheng, Lingjia Liu</h3>
<p>In this paper, we investigate a neural network-based learning approach
towards solving an integer-constrained programming problem using very limited
training. To be specific, we introduce a symmetric and decomposed neural
network structure, which is fully interpretable in terms of the functionality
of its constituent components. By taking advantage of the underlying pattern of
the integer constraint, as well as of the affine nature of the objective
function, the introduced neural network offers superior generalization
performance with limited training, as compared to other generic neural network
structures that do not exploit the inherent structure of the integer
constraint. In addition, we show that the introduced decomposed approach can be
further extended to semi-decomposed frameworks. The introduced learning
approach is evaluated via the classification/symbol detection task in the
context of wireless communication systems where available training sets are
usually limited. Evaluation results demonstrate that the introduced learning
strategy is able to effectively perform the classification/symbol detection
task in a wide variety of wireless channel environments specified by the 3GPP
community.
</p>
<a href="http://arxiv.org/abs/2011.05399" target="_blank">arXiv:2011.05399</a> [<a href="http://arxiv.org/pdf/2011.05399" target="_blank">pdf</a>]

<h2>Self-supervised Learning of LiDAR Odometry for Robotic Applications. (arXiv:2011.05418v1 [cs.RO])</h2>
<h3>Julian Nubert, Shehryar Khattak, Marco Hutter</h3>
<p>Reliable robot pose estimation is a key building block of many robot autonomy
pipelines, with LiDAR localization being an active research domain. In this
work, a versatile self-supervised LiDAR odometry estimation method is
presented, in order to enable the efficient utilization of all available LiDAR
data while maintaining real-time performance. The proposed approach selectively
applies geometric losses during training, being cognizant of the amount of
information that can be extracted from scan points. In addition, no labeled or
ground-truth data is required, hence making the presented approach suitable for
pose estimation in applications where accurate ground-truth is difficult to
obtain. Furthermore, the presented network architecture is applicable to a wide
range of environments and sensor modalities without requiring any network or
loss function adjustments. The proposed approach is thoroughly tested for both
indoor and outdoor real-world applications through a variety of experiments
using legged, tracked and wheeled robots, demonstrating the suitability of
learning-based LiDAR odometry for complex robotic applications.
</p>
<a href="http://arxiv.org/abs/2011.05418" target="_blank">arXiv:2011.05418</a> [<a href="http://arxiv.org/pdf/2011.05418" target="_blank">pdf</a>]

<h2>Using GANs to Synthesise Minimum Training Data for Deepfake Generation. (arXiv:2011.05421v1 [cs.CV])</h2>
<h3>Simranjeet Singh, Rajneesh Sharma, Alan F. Smeaton</h3>
<p>There are many applications of Generative Adversarial Networks (GANs) in
fields like computer vision, natural language processing, speech synthesis, and
more. Undoubtedly the most notable results have been in the area of image
synthesis and in particular in the generation of deepfake videos. While
deepfakes have received much negative media coverage, they can be a useful
technology in applications like entertainment, customer relations, or even
assistive care. One problem with generating deepfakes is the requirement for a
lot of image training data of the subject which is not an issue if the subject
is a celebrity for whom many images already exist. If there are only a small
number of training images then the quality of the deepfake will be poor. Some
media reports have indicated that a good deepfake can be produced with as few
as 500 images but in practice, quality deepfakes require many thousands of
images, one of the reasons why deepfakes of celebrities and politicians have
become so popular. In this study, we exploit the property of a GAN to produce
images of an individual with variable facial expressions which we then use to
generate a deepfake. We observe that with such variability in facial
expressions of synthetic GAN-generated training images and a reduced quantity
of them, we can produce a near-realistic deepfake videos.
</p>
<a href="http://arxiv.org/abs/2011.05421" target="_blank">arXiv:2011.05421</a> [<a href="http://arxiv.org/pdf/2011.05421" target="_blank">pdf</a>]

<h2>Preference-Based Learning for User-Guided HZD Gait Generation on Bipedal Walking Robots. (arXiv:2011.05424v1 [cs.RO])</h2>
<h3>Maegan Tucker, Noel Csomay-Shanklin, Wen-Loong Ma, Aaron D. Ames</h3>
<p>This paper presents a framework that unifies control theory and machine
learning in the setting of bipedal locomotion. Traditionally, gaits are
generated through trajectory optimization methods and then realized
experimentally -- a process that often requires extensive tuning due to
differences between the models and hardware. In this work, the process of gait
realization via hybrid zero dynamics (HZD) based optimization problems is
formally combined with preference-based learning to systematically realize
dynamically stable walking. Importantly, this learning approach does not
require a carefully constructed reward function, but instead utilizes human
pairwise preferences. The power of the proposed approach is demonstrated
through two experiments on a planar biped AMBER-3M: the first with rigid point
feet, and the second with induced model uncertainty through the addition of
springs where the added compliance was not accounted for in the gait generation
or in the controller. In both experiments, the framework achieves stable,
robust, efficient, and natural walking in fewer than 50 iterations with no
reliance on a simulation environment. These results demonstrate a promising
step in the unification of control theory and learning.
</p>
<a href="http://arxiv.org/abs/2011.05424" target="_blank">arXiv:2011.05424</a> [<a href="http://arxiv.org/pdf/2011.05424" target="_blank">pdf</a>]

<h2>Debugging Tests for Model Explanations. (arXiv:2011.05429v1 [cs.CV])</h2>
<h3>Julius Adebayo, Michael Muelly, Ilaria Liccardi, Been Kim</h3>
<p>We investigate whether post-hoc model explanations are effective for
diagnosing model errors--model debugging. In response to the challenge of
explaining a model's prediction, a vast array of explanation methods have been
proposed. Despite increasing use, it is unclear if they are effective. To
start, we categorize \textit{bugs}, based on their source, into:~\textit{data,
model, and test-time} contamination bugs. For several explanation methods, we
assess their ability to: detect spurious correlation artifacts (data
contamination), diagnose mislabeled training examples (data contamination),
differentiate between a (partially) re-initialized model and a trained one
(model contamination), and detect out-of-distribution inputs (test-time
contamination). We find that the methods tested are able to diagnose a spurious
background bug, but not conclusively identify mislabeled training examples. In
addition, a class of methods, that modify the back-propagation algorithm are
invariant to the higher layer parameters of a deep network; hence, ineffective
for diagnosing model contamination. We complement our analysis with a human
subject study, and find that subjects fail to identify defective models using
attributions, but instead rely, primarily, on model predictions. Taken
together, our results provide guidance for practitioners and researchers
turning to explanations as tools for model debugging.
</p>
<a href="http://arxiv.org/abs/2011.05429" target="_blank">arXiv:2011.05429</a> [<a href="http://arxiv.org/pdf/2011.05429" target="_blank">pdf</a>]

<h2>Do You See What I See? Coordinating Multiple Aerial Cameras for Robot Cinematography. (arXiv:2011.05437v1 [cs.RO])</h2>
<h3>Arthur Bucker, Rogerio Bonatti, Sebastian Scherer</h3>
<p>Aerial cinematography is significantly expanding the capabilities of
film-makers. Recent progress in autonomous unmanned aerial vehicles (UAVs) has
further increased the potential impact of aerial cameras, with systems that can
safely track actors in unstructured cluttered environments. Professional
productions, however, require the use of multiple cameras simultaneously to
record different viewpoints of the same scene, which are edited into the final
footage either in real time or in post-production. Such extreme motion
coordination is particularly hard for unscripted action scenes, which are a
common use case of aerial cameras. In this work we develop a real-time
multi-UAV coordination system that is capable of recording dynamic targets
while maximizing shot diversity and avoiding collisions and mutual visibility
between cameras. We validate our approach in multiple cluttered environments of
a photo-realistic simulator, and deploy the system using two UAVs in real-world
experiments. We show that our coordination scheme has low computational cost
and takes only 1.17 ms on average to plan for a team of 3 UAVs over a 10 s time
horizon. Supplementary video: https://youtu.be/m2R3anv2ADE
</p>
<a href="http://arxiv.org/abs/2011.05437" target="_blank">arXiv:2011.05437</a> [<a href="http://arxiv.org/pdf/2011.05437" target="_blank">pdf</a>]

<h2>Fast & Slow Learning: Incorporating Synthetic Gradients in Neural Memory Controllers. (arXiv:2011.05438v1 [cs.CV])</h2>
<h3>Tharindu Fernando, Simon Denman, Sridha Sridharan, Clinton Fookes</h3>
<p>Neural Memory Networks (NMNs) have received increased attention in recent
years compared to deep architectures that use a constrained memory. Despite
their new appeal, the success of NMNs hinges on the ability of the
gradient-based optimiser to perform incremental training of the NMN
controllers, determining how to leverage their high capacity for knowledge
retrieval. This means that while excellent performance can be achieved when the
training data is consistent and well distributed, rare data samples are hard to
learn from as the controllers fail to incorporate them effectively during model
training. Drawing inspiration from the human cognition process, in particular
the utilisation of neuromodulators in the human brain, we propose to decouple
the learning process of the NMN controllers to allow them to achieve flexible,
rapid adaptation in the presence of new information. This trait is highly
beneficial for meta-learning tasks where the memory controllers must quickly
grasp abstract concepts in the target domain, and adapt stored knowledge. This
allows the NMN controllers to quickly determine which memories are to be
retained and which are to be erased, and swiftly adapt their strategy to the
new task at hand. Through both quantitative and qualitative evaluations on
multiple public benchmarks, including classification and regression tasks, we
demonstrate the utility of the proposed approach. Our evaluations not only
highlight the ability of the proposed NMN architecture to outperform the
current state-of-the-art methods, but also provide insights on how the proposed
augmentations help achieve such superior results. In addition, we demonstrate
the practical implications of the proposed learning strategy, where the
feedback path can be shared among multiple neural memory networks as a
mechanism for knowledge sharing.
</p>
<a href="http://arxiv.org/abs/2011.05438" target="_blank">arXiv:2011.05438</a> [<a href="http://arxiv.org/pdf/2011.05438" target="_blank">pdf</a>]

<h2>Perturbation-based exploration methods in deep reinforcement learning. (arXiv:2011.05446v1 [cs.LG])</h2>
<h3>Sneha Aenugu</h3>
<p>Recent research on structured exploration placed emphasis on identifying
novel states in the state space and incentivizing the agent to revisit them
through intrinsic reward bonuses. In this study, we question whether the
performance boost demonstrated through these methods is indeed due to the
discovery of structure in exploratory schedule of the agent or is the benefit
largely attributed to the perturbations in the policy and reward space
manifested in pursuit of structured exploration. In this study we investigate
the effect of perturbations in policy and reward spaces on the exploratory
behavior of the agent. We proceed to show that simple acts of perturbing the
policy just before the softmax layer and introduction of sporadic reward
bonuses into the domain can greatly enhance exploration in several domains of
the arcade learning environment. In light of these findings, we recommend
benchmarking any enhancements to structured exploration research against the
backdrop of noisy exploration.
</p>
<a href="http://arxiv.org/abs/2011.05446" target="_blank">arXiv:2011.05446</a> [<a href="http://arxiv.org/pdf/2011.05446" target="_blank">pdf</a>]

<h2>A Self-supervised Learning System for Object Detection in Videos Using Random Walks on Graphs. (arXiv:2011.05459v1 [cs.CV])</h2>
<h3>Juntao Tan, Changkyu Song, Abdeslam Boularias</h3>
<p>This paper presents a new self-supervised system for learning to detect novel
and previously unseen categories of objects in images. The proposed system
receives as input several unlabeled videos of scenes containing various
objects. The frames of the videos are segmented into objects using depth
information, and the segments are tracked along each video. The system then
constructs a weighted graph that connects sequences based on the similarities
between the objects that they contain. The similarity between two sequences of
objects is measured by using generic visual features, after automatically
re-arranging the frames in the two sequences to align the viewpoints of the
objects. The graph is used to sample triplets of similar and dissimilar
examples by performing random walks. The triplet examples are finally used to
train a siamese neural network that projects the generic visual features into a
low-dimensional manifold. Experiments on three public datasets, YCB-Video,
CORe50 and RGBD-Object, show that the projected low-dimensional features
improve the accuracy of clustering unknown objects into novel categories, and
outperform several recent unsupervised clustering techniques.
</p>
<a href="http://arxiv.org/abs/2011.05459" target="_blank">arXiv:2011.05459</a> [<a href="http://arxiv.org/pdf/2011.05459" target="_blank">pdf</a>]

<h2>Teaching deep learning causal effects improves predictive performance. (arXiv:2011.05466v1 [cs.LG])</h2>
<h3>Jia Li, Xiaowei Jia, Haoyu Yang, Vipin Kumar, Michael Steinbach, Gyorgy Simon</h3>
<p>Causal inference is a powerful statistical methodology for explanatory
analysis and individualized treatment effect (ITE) estimation, a prominent
causal inference task that has become a fundamental research problem. ITE
estimation, when performed naively, tends to produce biased estimates. To
obtain unbiased estimates, counterfactual information is needed, which is not
directly observable from data. Based on mature domain knowledge, reliable
traditional methods to estimate ITE exist. In recent years, neural networks
have been widely used in clinical studies. Specifically, recurrent neural
networks (RNN) have been applied to temporal Electronic Health Records (EHR)
data analysis. However, RNNs are not guaranteed to automatically discover
causal knowledge, correctly estimate counterfactual information, and thus
correctly estimate the ITE. This lack of correct ITE estimates can hinder the
performance of the model. In this work we study whether RNNs can be guided to
correctly incorporate ITE-related knowledge and whether this improves
predictive performance. Specifically, we first describe a Causal-Temporal
Structure for temporal EHR data; then based on this structure, we estimate
sequential ITE along the timeline, using sequential Propensity Score Matching
(PSM); and finally, we propose a knowledge-guided neural network methodology to
incorporate estimated ITE. We demonstrate on real-world and synthetic data
(where the actual ITEs are known) that the proposed methodology can
significantly improve the prediction performance of RNN.
</p>
<a href="http://arxiv.org/abs/2011.05466" target="_blank">arXiv:2011.05466</a> [<a href="http://arxiv.org/pdf/2011.05466" target="_blank">pdf</a>]

<h2>Multi-Label Classification Using Link Prediction. (arXiv:2011.05476v1 [cs.LG])</h2>
<h3>Seyed Amin Fadaee, Maryam Amir Haeri</h3>
<p>Solving classification with graph methods has gained huge popularity in
recent years. This is due to the fact that the data can be intuitively modeled
with graphs to utilize high level features to aid in solving the classification
problem. CULP which is short for Classification Using Link Prediction is a
graph-based classifier. This classifier utilizes the graph representation of
the data and transforms the problem to that of link prediction where we try to
find the link between an unlabeled node and the proper class node for it. CULP
proved to be highly accurate classifier and it has the power to predict the
labels in near constant time. A variant of the classification problem is
multi-label classification which tackles this problem for multi-label data
where an instance can have multiple labels associated to it. In this work, we
extend the CULP algorithm to address this problem. Our proposed extensions
conveys the powers of CULP and its intuitive representation of the data in to
the multi-label domain and in comparison to some of the cutting edge
multi-label classifiers, yield competitive results.
</p>
<a href="http://arxiv.org/abs/2011.05476" target="_blank">arXiv:2011.05476</a> [<a href="http://arxiv.org/pdf/2011.05476" target="_blank">pdf</a>]

<h2>ForestNet: Classifying Drivers of Deforestation in Indonesia using Deep Learning on Satellite Imagery. (arXiv:2011.05479v1 [cs.CV])</h2>
<h3>Jeremy Irvin, Hao Sheng, Neel Ramachandran, Sonja Johnson-Yu, Sharon Zhou, Kyle Story, Rose Rustowicz, Cooper Elsworth, Kemen Austin, Andrew Y. Ng</h3>
<p>Characterizing the processes leading to deforestation is critical to the
development and implementation of targeted forest conservation and management
policies. In this work, we develop a deep learning model called ForestNet to
classify the drivers of primary forest loss in Indonesia, a country with one of
the highest deforestation rates in the world. Using satellite imagery,
ForestNet identifies the direct drivers of deforestation in forest loss patches
of any size. We curate a dataset of Landsat 8 satellite images of known forest
loss events paired with driver annotations from expert interpreters. We use the
dataset to train and validate the models and demonstrate that ForestNet
substantially outperforms other standard driver classification approaches. In
order to support future research on automated approaches to deforestation
driver classification, the dataset curated in this study is publicly available
at https://stanfordmlgroup.github.io/projects/forestnet .
</p>
<a href="http://arxiv.org/abs/2011.05479" target="_blank">arXiv:2011.05479</a> [<a href="http://arxiv.org/pdf/2011.05479" target="_blank">pdf</a>]

<h2>A novel method for Causal Structure Discovery from EHR data, a demonstration on type-2 diabetes mellitus. (arXiv:2011.05489v1 [cs.LG])</h2>
<h3>Xinpeng Shen, Sisi Ma, Prashanthi Vemuri, M. Regina Castro, Pedro J. Caraballo, Gyorgy J. Simon</h3>
<p>Introduction: The discovery of causal mechanisms underlying diseases enables
better diagnosis, prognosis and treatment selection. Clinical trials have been
the gold standard for determining causality, but they are resource intensive,
sometimes infeasible or unethical. Electronic Health Records (EHR) contain a
wealth of real-world data that holds promise for the discovery of disease
mechanisms, yet the existing causal structure discovery (CSD) methods fall
short on leveraging them due to the special characteristics of the EHR data. We
propose a new data transformation method and a novel CSD algorithm to overcome
the challenges posed by these characteristics. Materials and methods: We
demonstrated the proposed methods on an application to type-2 diabetes
mellitus. We used a large EHR data set from Mayo Clinic to internally evaluate
the proposed transformation and CSD methods and used another large data set
from an independent health system, Fairview Health Services, as external
validation. We compared the performance of our proposed method to Fast Greedy
Equivalence Search (FGES), a state-of-the-art CSD method in terms of
correctness, stability and completeness. We tested the generalizability of the
proposed algorithm through external validation. Results and conclusions: The
proposed method improved over the existing methods by successfully
incorporating study design considerations, was robust in face of unreliable EHR
timestamps and inferred causal effect directions more correctly and reliably.
The proposed data transformation successfully improved the clinical correctness
of the discovered graph and the consistency of edge orientation across
bootstrap samples. It resulted in superior accuracy, stability, and
completeness.
</p>
<a href="http://arxiv.org/abs/2011.05489" target="_blank">arXiv:2011.05489</a> [<a href="http://arxiv.org/pdf/2011.05489" target="_blank">pdf</a>]

<h2>Unsupervised Learning of Dense Visual Representations. (arXiv:2011.05499v1 [cs.CV])</h2>
<h3>Pedro O. Pinheiro, Amjad Almahairi, Ryan Y. Benmaleck, Florian Golemo, Aaron Courville</h3>
<p>Contrastive self-supervised learning has emerged as a promising approach to
unsupervised visual representation learning. In general, these methods learn
global (image-level) representations that are invariant to different views
(i.e., compositions of data augmentation) of the same image. However, many
visual understanding tasks require dense (pixel-level) representations. In this
paper, we propose View-Agnostic Dense Representation (VADeR) for unsupervised
learning of dense representations. VADeR learns pixelwise representations by
forcing local features to remain constant over different viewing conditions.
Specifically, this is achieved through pixel-level contrastive learning:
matching features (that is, features that describes the same location of the
scene on different views) should be close in an embedding space, while
non-matching features should be apart. VADeR provides a natural representation
for dense prediction tasks and transfers well to downstream tasks. Our method
outperforms ImageNet supervised pretraining (and strong unsupervised baselines)
in multiple dense prediction tasks.
</p>
<a href="http://arxiv.org/abs/2011.05499" target="_blank">arXiv:2011.05499</a> [<a href="http://arxiv.org/pdf/2011.05499" target="_blank">pdf</a>]

<h2>Automatic Open-World Reliability Assessment. (arXiv:2011.05506v1 [cs.CV])</h2>
<h3>Mohsen Jafarzadeh, Touqeer Ahmad, Akshay Raj Dhamija, Chunchun Li, Steve Cruz, Terrance E. Boult</h3>
<p>Image classification in the open-world must handle out-of-distribution (OOD)
images. Systems should ideally reject OOD images, or they will map atop of
known classes and reduce reliability. Using open-set classifiers that can
reject OOD inputs can help. However, optimal accuracy of open-set classifiers
depend on the frequency of OOD data. Thus, for either standard or open-set
classifiers, it is important to be able to determine when the world changes and
increasing OOD inputs will result in reduced system reliability. However,
during operations, we cannot directly assess accuracy as there are no labels.
Thus, the reliability assessment of these classifiers must be done by human
operators, made more complex because networks are not 100% accurate, so some
failures are to be expected. To automate this process, herein, we formalize the
open-world recognition reliability problem and propose multiple automatic
reliability assessment policies to address this new problem using only the
distribution of reported scores/probability data. The distributional algorithms
can be applied to both classic classifiers with SoftMax as well as the
open-world Extreme Value Machine (EVM) to provide automated reliability
assessment. We show that all of the new algorithms significantly outperform
detection using the mean of SoftMax.
</p>
<a href="http://arxiv.org/abs/2011.05506" target="_blank">arXiv:2011.05506</a> [<a href="http://arxiv.org/pdf/2011.05506" target="_blank">pdf</a>]

<h2>Two-dimensional Bhattacharyya bound linear discriminant analysis with its applications. (arXiv:2011.05507v1 [cs.LG])</h2>
<h3>Yan-Ru Guo, Yan-Qin Bai, Chun-Na Li, Lan Bai, Yuan-Hai Shao</h3>
<p>Recently proposed L2-norm linear discriminant analysis criterion via the
Bhattacharyya error bound estimation (L2BLDA) is an effective improvement of
linear discriminant analysis (LDA) for feature extraction. However, L2BLDA is
only proposed to cope with vector input samples. When facing with
two-dimensional (2D) inputs, such as images, it will lose some useful
information, since it does not consider intrinsic structure of images. In this
paper, we extend L2BLDA to a two-dimensional Bhattacharyya bound linear
discriminant analysis (2DBLDA). 2DBLDA maximizes the matrix-based between-class
distance which is measured by the weighted pairwise distances of class means
and meanwhile minimizes the matrix-based within-class distance. The weighting
constant between the between-class and within-class terms is determined by the
involved data that makes the proposed 2DBLDA adaptive. In addition, the
criterion of 2DBLDA is equivalent to optimizing an upper bound of the
Bhattacharyya error. The construction of 2DBLDA makes it avoid the small sample
size problem while also possess robustness, and can be solved through a simple
standard eigenvalue decomposition problem. The experimental results on image
recognition and face image reconstruction demonstrate the effectiveness of the
proposed methods.
</p>
<a href="http://arxiv.org/abs/2011.05507" target="_blank">arXiv:2011.05507</a> [<a href="http://arxiv.org/pdf/2011.05507" target="_blank">pdf</a>]

<h2>A Quantum-Inspired Probabilistic Model for the Inverse Design of Meta-Structures. (arXiv:2011.05511v1 [cs.LG])</h2>
<h3>Yingtao Luo, Xuefeng Zhu</h3>
<p>In quantum mechanics, a norm squared wave function can be interpreted as the
probability density that describes the likelihood of a particle to be measured
in a given position or momentum. This statistical property is at the core of
the microcosmos. Meanwhile, machine learning inverse design of materials raised
intensive attention, resulting in various intelligent systems for matter
engineering. Here, inspired by quantum theory, we propose a probabilistic deep
learning paradigm for the inverse design of functional meta-structures. Our
probability-density-based neural network (PDN) can accurately capture all
plausible meta-structures to meet the desired performances. Local maxima in
probability density distribution correspond to the most likely candidates. We
verify this approach by designing multiple meta-structures for each targeted
transmission spectrum to enrich design choices.
</p>
<a href="http://arxiv.org/abs/2011.05511" target="_blank">arXiv:2011.05511</a> [<a href="http://arxiv.org/pdf/2011.05511" target="_blank">pdf</a>]

<h2>Zero-Shot Terrain Generalization for Visual Locomotion Policies. (arXiv:2011.05513v1 [cs.RO])</h2>
<h3>Alejandro Escontrela, George Yu, Peng Xu, Atil Iscen, Jie Tan</h3>
<p>Legged robots have unparalleled mobility on unstructured terrains. However,
it remains an open challenge to design locomotion controllers that can operate
in a large variety of environments. In this paper, we address this challenge of
automatically learning locomotion controllers that can generalize to a diverse
collection of terrains often encountered in the real world. We frame this
challenge as a multi-task reinforcement learning problem and define each task
as a type of terrain that the robot needs to traverse. We propose an end-to-end
learning approach that makes direct use of the raw exteroceptive inputs
gathered from a simulated 3D LiDAR sensor, thus circumventing the need for
ground-truth heightmaps or preprocessing of perception information. As a
result, the learned controller demonstrates excellent zero-shot generalization
capabilities and can navigate 13 different environments, including stairs,
rugged land, cluttered offices, and indoor spaces with humans.
</p>
<a href="http://arxiv.org/abs/2011.05513" target="_blank">arXiv:2011.05513</a> [<a href="http://arxiv.org/pdf/2011.05513" target="_blank">pdf</a>]

<h2>Probability-Density-Based Deep Learning Paradigm for the Fuzzy Design of Functional Metastructures. (arXiv:2011.05516v1 [cs.LG])</h2>
<h3>Ying-Tao Luo, Peng-Qi Li, Dong-Ting Li, Yu-Gui Peng, Zhi-Guo Geng, Shu-Huan Xie, Yong Li, Andrea Alu, Jie Zhu, Xue-Feng Zhu</h3>
<p>In quantum mechanics, a norm squared wave function can be interpreted as the
probability density that describes the likelihood of a particle to be measured
in a given position or momentum. This statistical property is at the core of
the fuzzy structure of microcosmos. Recently, hybrid neural structures raised
intense attention, resulting in various intelligent systems with far-reaching
influence. Here, we propose a probability-density-based deep learning paradigm
for the fuzzy design of functional meta-structures. In contrast to other
inverse design methods, our probability-density-based neural network can
efficiently evaluate and accurately capture all plausible meta-structures in a
high-dimensional parameter space. Local maxima in probability density
distribution correspond to the most likely candidates to meet the desired
performances. We verify this universally adaptive approach in but not limited
to acoustics by designing multiple meta-structures for each targeted
transmission spectrum, with experiments unequivocally demonstrating the
effectiveness and generalization of the inverse design.
</p>
<a href="http://arxiv.org/abs/2011.05516" target="_blank">arXiv:2011.05516</a> [<a href="http://arxiv.org/pdf/2011.05516" target="_blank">pdf</a>]

<h2>Energy consumption forecasting using a stacked nonparametric Bayesian approach. (arXiv:2011.05519v1 [cs.LG])</h2>
<h3>Dilusha Weeraddana, Nguyen Lu Dang Khoa, Lachlan O Neil, Weihong Wang, Chen Cai</h3>
<p>In this paper, the process of forecasting household energy consumption is
studied within the framework of the nonparametric Gaussian Process (GP), using
multiple short time series data. As we begin to use smart meter data to paint a
clearer picture of residential electricity use, it becomes increasingly
apparent that we must also construct a detailed picture and understanding of
consumer's complex relationship with gas consumption. Both electricity and gas
consumption patterns are highly dependent on various factors, and the intricate
interplay of these factors is sophisticated. Moreover, since typical gas
consumption data is low granularity with very few time points, naive
application of conventional time-series forecasting techniques can lead to
severe over-fitting. Given these considerations, we construct a stacked GP
method where the predictive posteriors of each GP applied to each task are used
in the prior and likelihood of the next level GP. We apply our model to a
real-world dataset to forecast energy consumption in Australian households
across several states. We compare intuitively appealing results against other
commonly used machine learning techniques. Overall, the results indicate that
the proposed stacked GP model outperforms other forecasting techniques that we
tested, especially when we have a multiple short time-series instances.
</p>
<a href="http://arxiv.org/abs/2011.05519" target="_blank">arXiv:2011.05519</a> [<a href="http://arxiv.org/pdf/2011.05519" target="_blank">pdf</a>]

<h2>Optimized Loss Functions for Object detection and Application on Nighttime Vehicle Detection. (arXiv:2011.05523v1 [cs.CV])</h2>
<h3>Shang Jiang, Haoran Qin, Bingli Zhang, Jieyu Zheng</h3>
<p>Loss functions is a crucial factor than affecting the detection precision in
object detection task. In this paper, we optimize both two loss functions for
classification and localization simultaneously. Firstly, by multiplying an
IoU-based coefficient by the standard cross entropy loss in classification loss
function, the correlation between localization and classification is
established. Compared to the existing studies, in which the correlation is only
applied to improve the localization accuracy for positive samples, this paper
utilizes the correlation to obtain the really hard negative samples and aims to
decrease the misclassified rate for negative samples. Besides, a novel
localization loss named MIoU is proposed by incorporating a Mahalanobis
distance between predicted box and target box, which eliminate the gradients
inconsistency problem in the DIoU loss, further improving the localization
accuracy. Finally, sufficient experiments for nighttime vehicle detection have
been done on two datasets. Our results show than when train with the proposed
loss functions, the detection performance can be outstandingly improved. The
source code and trained models are available at
https://github.com/therebellll/NegIoU-PosIoU-Miou.
</p>
<a href="http://arxiv.org/abs/2011.05523" target="_blank">arXiv:2011.05523</a> [<a href="http://arxiv.org/pdf/2011.05523" target="_blank">pdf</a>]

<h2>Proximal Policy Optimization via Enhanced Exploration Efficiency. (arXiv:2011.05525v1 [cs.LG])</h2>
<h3>Junwei Zhang, Zhenghao Zhang, Shuai Han, Shuai L&#xfc;</h3>
<p>Proximal policy optimization (PPO) algorithm is a deep reinforcement learning
algorithm with outstanding performance, especially in continuous control tasks.
But the performance of this method is still affected by its exploration
ability. For classical reinforcement learning, there are some schemes that make
exploration more full and balanced with data exploitation, but they can't be
applied in complex environments due to the complexity of algorithm. Based on
continuous control tasks with dense reward, this paper analyzes the assumption
of the original Gaussian action exploration mechanism in PPO algorithm, and
clarifies the influence of exploration ability on performance. Afterward,
aiming at the problem of exploration, an exploration enhancement mechanism
based on uncertainty estimation is designed in this paper. Then, we apply
exploration enhancement theory to PPO algorithm and propose the proximal policy
optimization algorithm with intrinsic exploration module (IEM-PPO) which can be
used in complex environments. In the experimental parts, we evaluate our method
on multiple tasks of MuJoCo physical simulator, and compare IEM-PPO algorithm
with curiosity driven exploration algorithm (ICM-PPO) and original algorithm
(PPO). The experimental results demonstrate that IEM-PPO algorithm needs longer
training time, but performs better in terms of sample efficiency and cumulative
reward, and has stability and robustness.
</p>
<a href="http://arxiv.org/abs/2011.05525" target="_blank">arXiv:2011.05525</a> [<a href="http://arxiv.org/pdf/2011.05525" target="_blank">pdf</a>]

<h2>On Polynomial Approximations for Privacy-Preserving and Verifiable ReLU Networks. (arXiv:2011.05530v1 [cs.LG])</h2>
<h3>Ramy E. Ali, Jinhyun So, A. Salman Avestimehr</h3>
<p>Outsourcing neural network inference tasks to an untrusted cloud raises data
privacy and integrity concerns. In order to address these challenges, several
privacy-preserving and verifiable inference techniques have been proposed based
on replacing the non-polynomial activation functions such as the rectified
linear unit (ReLU) function with polynomial activation functions. Such
techniques usually require the polynomial coefficients to be in a finite field.
Motivated by such requirements, several works proposed replacing the ReLU
activation function with the square activation function. In this work, we
empirically show that the square function is not the best second-degree
polynomial that can replace the ReLU function in deep neural networks. We
instead propose a second-degree polynomial activation function with a first
order term and empirically show that it can lead to much better models. Our
experiments on the CIFAR-$10$ dataset show that our proposed polynomial
activation function significantly outperforms the square activation function.
</p>
<a href="http://arxiv.org/abs/2011.05530" target="_blank">arXiv:2011.05530</a> [<a href="http://arxiv.org/pdf/2011.05530" target="_blank">pdf</a>]

<h2>Spoken Language Interaction with Robots: Research Issues and Recommendations, Report from the NSF Future Directions Workshop. (arXiv:2011.05533v1 [cs.RO])</h2>
<h3>Matthew Marge, Carol Espy-Wilson, Nigel Ward</h3>
<p>With robotics rapidly advancing, more effective human-robot interaction is
increasingly needed to realize the full potential of robots for society. While
spoken language must be part of the solution, our ability to provide spoken
language interaction capabilities is still very limited. The National Science
Foundation accordingly convened a workshop, bringing together speech, language,
and robotics researchers to discuss what needs to be done. The result is this
report, in which we identify key scientific and engineering advances needed.

Our recommendations broadly relate to eight general themes. First, meeting
human needs requires addressing new challenges in speech technology and user
experience design. Second, this requires better models of the social and
interactive aspects of language use. Third, for robustness, robots need
higher-bandwidth communication with users and better handling of uncertainty,
including simultaneous consideration of multiple hypotheses and goals. Fourth,
more powerful adaptation methods are needed, to enable robots to communicate in
new environments, for new tasks, and with diverse user populations, without
extensive re-engineering or the collection of massive training data. Fifth,
since robots are embodied, speech should function together with other
communication modalities, such as gaze, gesture, posture, and motion. Sixth,
since robots operate in complex environments, speech components need access to
rich yet efficient representations of what the robot knows about objects,
locations, noise sources, the user, and other humans. Seventh, since robots
operate in real time, their speech and language processing components must
also. Eighth, in addition to more research, we need more work on infrastructure
and resources, including shareable software modules and internal interfaces,
inexpensive hardware, baseline systems, and diverse corpora.
</p>
<a href="http://arxiv.org/abs/2011.05533" target="_blank">arXiv:2011.05533</a> [<a href="http://arxiv.org/pdf/2011.05533" target="_blank">pdf</a>]

<h2>Differentially Private Synthetic Data: Applied Evaluations and Enhancements. (arXiv:2011.05537v1 [cs.LG])</h2>
<h3>Lucas Rosenblatt, Xiaoyan Liu, Samira Pouyanfar, Eduardo de Leon, Anuj Desai, Joshua Allen</h3>
<p>Machine learning practitioners frequently seek to leverage the most
informative available data, without violating the data owner's privacy, when
building predictive models. Differentially private data synthesis protects
personal details from exposure, and allows for the training of differentially
private machine learning models on privately generated datasets. But how can we
effectively assess the efficacy of differentially private synthetic data? In
this paper, we survey four differentially private generative adversarial
networks for data synthesis. We evaluate each of them at scale on five standard
tabular datasets, and in two applied industry scenarios. We benchmark with
novel metrics from recent literature and other standard machine learning tools.
Our results suggest some synthesizers are more applicable for different privacy
budgets, and we further demonstrate complicating domain-based tradeoffs in
selecting an approach. We offer experimental learning on applied machine
learning scenarios with private internal data to researchers and practioners
alike. In addition, we propose QUAIL, an ensemble-based modeling approach to
generating synthetic data. We examine QUAIL's tradeoffs, and note circumstances
in which it outperforms baseline differentially private supervised learning
models under the same budget constraint.
</p>
<a href="http://arxiv.org/abs/2011.05537" target="_blank">arXiv:2011.05537</a> [<a href="http://arxiv.org/pdf/2011.05537" target="_blank">pdf</a>]

<h2>Learning Agile Locomotion Skills with a Mentor. (arXiv:2011.05541v1 [cs.RO])</h2>
<h3>Atil Iscen, George Yu, Alejandro Escontrela, Deepali Jain, Jie Tan, Ken Caluwaerts</h3>
<p>Developing agile behaviors for legged robots remains a challenging problem.
While deep reinforcement learning is a promising approach, learning truly agile
behaviors typically requires tedious reward shaping and careful curriculum
design. We formulate agile locomotion as a multi-stage learning problem in
which a mentor guides the agent throughout the training. The mentor is
optimized to place a checkpoint to guide the movement of the robot's center of
mass while the student (i.e. the robot) learns to reach these checkpoints. Once
the student can solve the task, we teach the student to perform the task
without the mentor. We evaluate our proposed learning system with a simulated
quadruped robot on a course consisting of randomly generated gaps and hurdles.
Our method significantly outperforms a single-stage RL baseline without a
mentor, and the quadruped robot can agilely run and jump across gaps and
obstacles. Finally, we present a detailed analysis of the learned behaviors'
feasibility and efficiency.
</p>
<a href="http://arxiv.org/abs/2011.05541" target="_blank">arXiv:2011.05541</a> [<a href="http://arxiv.org/pdf/2011.05541" target="_blank">pdf</a>]

<h2>End-to-End Chinese Landscape Painting Creation Using Generative Adversarial Networks. (arXiv:2011.05552v1 [cs.CV])</h2>
<h3>Alice Xue</h3>
<p>Current GAN-based art generation methods produce unoriginal artwork due to
their dependence on conditional input. Here, we propose Sketch-And-Paint GAN
(SAPGAN), the first model which generates Chinese landscape paintings from end
to end, without conditional input. SAPGAN is composed of two GANs: SketchGAN
for generation of edge maps, and PaintGAN for subsequent edge-to-painting
translation. Our model is trained on a new dataset of traditional Chinese
landscape paintings never before used for generative research. A 242-person
Visual Turing Test study reveals that SAPGAN paintings are mistaken as human
artwork with 55% frequency, significantly outperforming paintings from baseline
GANs. Our work lays a groundwork for truly machine-original art generation.
</p>
<a href="http://arxiv.org/abs/2011.05552" target="_blank">arXiv:2011.05552</a> [<a href="http://arxiv.org/pdf/2011.05552" target="_blank">pdf</a>]

<h2>TRAILER: Transformer-based Time-wise Long Term Relation Modeling for Citywide Traffic Flow Prediction. (arXiv:2011.05554v1 [cs.AI])</h2>
<h3>Hao Xue, Flora D Salim</h3>
<p>Traffic flow prediction is a crucial task in enabling efficient intelligent
transportation systems and smart cities. Although there has been rapid progress
in this area in the last few years, given the major advances of deep learning
techniques, it remains a challenging task because of the inherent periodic
characteristics of traffic flow sequence. To incorporate the periodicity in the
prediction process, existing methods have observed three components separately
as the input of prediction models, i.e., the closeness, period, and trend
components. The long term relation of these components has not been fully
addressed. In this paper, we present a novel architecture, TRAILER
(TRAnsformer-based tIme-wise Long tErm Relation modeling), to predict traffic
flows more effectively. First, we explicitly design a Transformer based long
term relation prediction module to model the long term relation and predict the
periodic relation to be used for the downstream task. Second, we propose a
consistency module at the target time interval, in order to model the
consistency of the predicted periodic relation and the relation inferred from
the predicted traffic flow tensor. Finally, based on the consistency module, we
introduce a consistency loss to stabilize the training process and further
improve the prediction performance. Through extensive experiments, we show the
superiority of the proposed method on three real-world datasets and the
effectiveness of each module in TRAILER.
</p>
<a href="http://arxiv.org/abs/2011.05554" target="_blank">arXiv:2011.05554</a> [<a href="http://arxiv.org/pdf/2011.05554" target="_blank">pdf</a>]

<h2>Intentonomy: a Dataset and Study towards Human Intent Understanding. (arXiv:2011.05558v1 [cs.CV])</h2>
<h3>Menglin Jia, Zuxuan Wu, Austin Reiter, Claire Cardie, Serge Belongie, Ser-Nam Lim</h3>
<p>An image is worth a thousand words, conveying information that goes beyond
the mere visual content therein. In this paper, we study the intent behind
social media images with an aim to analyze how visual information can
facilitate recognition of human intent. Towards this goal, we introduce an
intent dataset, Intentonomy, comprising 14K images covering a wide range of
everyday scenes. These images are manually annotated with 28 intent categories
derived from a social psychology taxonomy. We then systematically study
whether, and to what extent, commonly used visual information, i.e., object and
context, contribute to human motive understanding. Based on our findings, we
conduct further study to quantify the effect of attending to object and context
classes as well as textual information in the form of hashtags when training an
intent classifier. Our results quantitatively and qualitatively shed light on
how visual and textual information can produce observable effects when
predicting intent.
</p>
<a href="http://arxiv.org/abs/2011.05558" target="_blank">arXiv:2011.05558</a> [<a href="http://arxiv.org/pdf/2011.05558" target="_blank">pdf</a>]

<h2>Learning Bayes Filter Models for Tactile Localization. (arXiv:2011.05559v1 [cs.RO])</h2>
<h3>Tarik Kelestemur, Colin Keil, John P. Whitney, Robert Platt, Taskin Padir</h3>
<p>Localizing and tracking the pose of robotic grippers are necessary skills for
manipulation tasks. However, the manipulators with imprecise kinematic models
(e.g. low-cost arms) or manipulators with unknown world coordinates (e.g. poor
camera-arm calibration) cannot locate the gripper with respect to the world. In
these circumstances, we can leverage tactile feedback between the gripper and
the environment. In this paper, we present learnable Bayes filter models that
can localize robotic grippers using tactile feedback. We propose a novel
observation model that conditions the tactile feedback on visual maps of the
environment along with a motion model to recursively estimate the gripper's
location. Our models are trained in simulation with self-supervision and
transferred to the real world. Our method is evaluated on a tabletop
localization task in which the gripper interacts with objects. We report
results in simulation and on a real robot, generalizing over different sizes,
shapes, and configurations of the objects.
</p>
<a href="http://arxiv.org/abs/2011.05559" target="_blank">arXiv:2011.05559</a> [<a href="http://arxiv.org/pdf/2011.05559" target="_blank">pdf</a>]

<h2>Docking two multirotors in midair using relative vision measurements. (arXiv:2011.05565v1 [cs.RO])</h2>
<h3>Karan P. Jain, Minos Park, Mark W. Mueller</h3>
<p>Modular robots have been rising in popularity for a variety of applications,
and autonomous midair docking is a necessary task for real world deployment of
these robots. We present a state estimator based on the extended Kalman filter
for relative localization of one multirotor with respect to another using only
onboard sensors, specifically an inertial measurement unit and a camera-marker
pair. Acceleration and angular velocity measurements along with relative pose
measurements from a camera on the first multirotor looking at a marker on the
second multirotor are used to estimate the relative position and velocity of
the first multirotor with respect to the second, and the absolute attitude of
the first multirotor. We also present a control architecture to use these
onboard state estimates to control the first multirotor at a desired setpoint
with respect to the second. The performance of the estimator and control
architecture are experimentally validated by successfully and repeatably
performing midair docking -- a task that requires relative position precision
on the order of a centimeter.
</p>
<a href="http://arxiv.org/abs/2011.05565" target="_blank">arXiv:2011.05565</a> [<a href="http://arxiv.org/pdf/2011.05565" target="_blank">pdf</a>]

<h2>Challenges of Applying Deep Reinforcement Learning in Dynamic Dispatching. (arXiv:2011.05570v1 [cs.LG])</h2>
<h3>Hamed Khorasgani, Haiyan Wang, Chetan Gupta</h3>
<p>Dynamic dispatching aims to smartly allocate the right resources to the right
place at the right time. Dynamic dispatching is one of the core problems for
operations optimization in the mining industry. Theoretically, deep
reinforcement learning (RL) should be a natural fit to solve this problem.
However, the industry relies on heuristics or even human intuitions, which are
often short-sighted and sub-optimal solutions. In this paper, we review the
main challenges in using deep RL to address the dynamic dispatching problem in
the mining industry.
</p>
<a href="http://arxiv.org/abs/2011.05570" target="_blank">arXiv:2011.05570</a> [<a href="http://arxiv.org/pdf/2011.05570" target="_blank">pdf</a>]

<h2>Deja vu from the SVM Era: Example-based Explanations with Outlier Detection. (arXiv:2011.05577v1 [cs.LG])</h2>
<h3>Penny Chong, Yuval Elovici, Alexander Binder</h3>
<p>Understanding the features that contributed to a prediction is important for
high-stake tasks. In this work, we revisit the idea of a student network to
provide an example-based explanation for its prediction in two forms: i)
identify top-k most relevant prototype examples and ii) show evidence of
similarity between the prediction sample and each of the top-k prototypes. We
compare the prediction performance and the explanation performance for the
second type of explanation with the teacher network. In addition, we evaluate
the outlier detection performance of the network. We show that using
prototype-based students beyond similarity kernels deliver meaningful
explanations and promising outlier detection results, without compromising on
classification accuracy.
</p>
<a href="http://arxiv.org/abs/2011.05577" target="_blank">arXiv:2011.05577</a> [<a href="http://arxiv.org/pdf/2011.05577" target="_blank">pdf</a>]

<h2>Compression Boosts Differentially Private Federated Learning. (arXiv:2011.05578v1 [cs.LG])</h2>
<h3>Raouf Kerkouche, Gergely &#xc1;cs, Claude Castelluccia, Pierre Genev&#xe8;s</h3>
<p>Federated Learning allows distributed entities to train a common model
collaboratively without sharing their own data. Although it prevents data
collection and aggregation by exchanging only parameter updates, it remains
vulnerable to various inference and reconstruction attacks where a malicious
entity can learn private information about the participants' training data from
the captured gradients. Differential Privacy is used to obtain theoretically
sound privacy guarantees against such inference attacks by noising the
exchanged update vectors. However, the added noise is proportional to the model
size which can be very large with modern neural networks. This can result in
poor model quality. In this paper, compressive sensing is used to reduce the
model size and hence increase model quality without sacrificing privacy. We
show experimentally, using 2 datasets, that our privacy-preserving proposal can
reduce the communication costs by up to 95% with only a negligible performance
penalty compared to traditional non-private federated learning schemes.
</p>
<a href="http://arxiv.org/abs/2011.05578" target="_blank">arXiv:2011.05578</a> [<a href="http://arxiv.org/pdf/2011.05578" target="_blank">pdf</a>]

<h2>Recognizing More Emotions with Less Data Using Self-supervised Transfer Learning. (arXiv:2011.05585v1 [cs.LG])</h2>
<h3>Jonathan Boigne, Biman Liyanage, Ted &#xd6;strem</h3>
<p>We propose a novel transfer learning method for speech emotion recognition
allowing us to obtain promising results when only few training data is
available. With as low as 125 examples per emotion class, we were able to reach
a higher accuracy than a strong baseline trained on 8 times more data. Our
method leverages knowledge contained in pre-trained speech representations
extracted from models trained on a more general self-supervised task which
doesn't require human annotations, such as the wav2vec model. We provide
detailed insights on the benefits of our approach by varying the training data
size, which can help labeling teams to work more efficiently. We compare
performance with other popular methods on the IEMOCAP dataset, a
well-benchmarked dataset among the Speech Emotion Recognition (SER) research
community. Furthermore, we demonstrate that results can be greatly improved by
combining acoustic and linguistic knowledge from transfer learning. We align
acoustic pre-trained representations with semantic representations from the
BERT model through an attention-based recurrent neural network. Performance
improves significantly when combining both modalities and scales with the
amount of data. When trained on the full IEMOCAP dataset, we reach a new
state-of-the-art of 73.9% unweighted accuracy (UA).
</p>
<a href="http://arxiv.org/abs/2011.05585" target="_blank">arXiv:2011.05585</a> [<a href="http://arxiv.org/pdf/2011.05585" target="_blank">pdf</a>]

<h2>Accounting for Human Learning when Inferring Human Preferences. (arXiv:2011.05596v1 [cs.LG])</h2>
<h3>Harry Giles, Lawrence Chan</h3>
<p>Inverse reinforcement learning (IRL) is a common technique for inferring
human preferences from data. Standard IRL techniques tend to assume that the
human demonstrator is stationary, that is that their policy $\pi$ doesn't
change over time. In practice, humans interacting with a novel environment or
performing well on a novel task will change their demonstrations as they learn
more about the environment or task. We investigate the consequences of relaxing
this assumption of stationarity, in particular by modelling the human as
learning. Surprisingly, we find in some small examples that this can lead to
better inference than if the human was stationary. That is, by observing a
demonstrator who is themselves learning, a machine can infer more than by
observing a demonstrator who is noisily rational. In addition, we find evidence
that misspecification can lead to poor inference, suggesting that modelling
human learning is important, especially when the human is facing an unfamiliar
environment.
</p>
<a href="http://arxiv.org/abs/2011.05596" target="_blank">arXiv:2011.05596</a> [<a href="http://arxiv.org/pdf/2011.05596" target="_blank">pdf</a>]

<h2>A Nonconvex Framework for Structured Dynamic Covariance Recovery. (arXiv:2011.05601v1 [stat.ML])</h2>
<h3>Katherine Tsai, Mladen Kolar, Oluwasanmi Koyejo</h3>
<p>We propose a flexible yet interpretable model for high-dimensional data with
time-varying second order statistics, motivated and applied to functional
neuroimaging data. Motivated by the neuroscience literature, we factorize the
covariances into sparse spatial and smooth temporal components. While this
factorization results in both parsimony and domain interpretability, the
resulting estimation problem is nonconvex. To this end, we design a two-stage
optimization scheme with a carefully tailored spectral initialization, combined
with iteratively refined alternating projected gradient descent. We prove a
linear convergence rate up to a nontrivial statistical error for the proposed
descent scheme and establish sample complexity guarantees for the estimator. We
further quantify the statistical error for the multivariate Gaussian case.
Empirical results using simulated and real brain imaging data illustrate that
our approach outperforms existing baselines.
</p>
<a href="http://arxiv.org/abs/2011.05601" target="_blank">arXiv:2011.05601</a> [<a href="http://arxiv.org/pdf/2011.05601" target="_blank">pdf</a>]

<h2>Joint predictions of multi-modal ride-hailing demands: a deep multi-task multigraph learning-based approach. (arXiv:2011.05602v1 [cs.LG])</h2>
<h3>Jintao Ke, Siyuan Feng, Zheng Zhu, Hai Yang, Jieping Ye</h3>
<p>Ride-hailing platforms generally provide various service options to
customers, such as solo ride services, shared ride services, etc. It is
generally expected that demands for different service modes are correlated, and
the prediction of demand for one service mode can benefit from historical
observations of demands for other service modes. Moreover, an accurate joint
prediction of demands for multiple service modes can help the platforms better
allocate and dispatch vehicle resources. Although there is a large stream of
literature on ride-hailing demand predictions for one specific service mode,
little efforts have been paid towards joint predictions of ride-hailing demands
for multiple service modes. To address this issue, we propose a deep multi-task
multi-graph learning approach, which combines two components: (1) multiple
multi-graph convolutional (MGC) networks for predicting demands for different
service modes, and (2) multi-task learning modules that enable knowledge
sharing across multiple MGC networks. More specifically, two multi-task
learning structures are established. The first one is the regularized
cross-task learning, which builds cross-task connections among the inputs and
outputs of multiple MGC networks. The second one is the multi-linear
relationship learning, which imposes a prior tensor normal distribution on the
weights of various MGC networks. Although there are no concrete bridges between
different MGC networks, the weights of these networks are constrained by each
other and subject to a common prior distribution. Evaluated with the
for-hire-vehicle datasets in Manhattan, we show that our propose approach
outperforms the benchmark algorithms in prediction accuracy for different
ride-hailing modes.
</p>
<a href="http://arxiv.org/abs/2011.05602" target="_blank">arXiv:2011.05602</a> [<a href="http://arxiv.org/pdf/2011.05602" target="_blank">pdf</a>]

<h2>Decentralized Motion Planning for Multi-Robot Navigation using Deep Reinforcement Learning. (arXiv:2011.05605v1 [cs.RO])</h2>
<h3>Sivanathan Kandhasamy, Vinayagam Babu Kuppusamy, Tanmay Vilas Samak, Chinmay Vilas Samak</h3>
<p>This work presents a decentralized motion planning framework for addressing
the task of multi-robot navigation using deep reinforcement learning. A custom
simulator was developed in order to experimentally investigate the navigation
problem of 4 cooperative non-holonomic robots sharing limited state information
with each other in 3 different settings. The notion of decentralized motion
planning with common and shared policy learning was adopted, which allowed
robust training and testing of this approach in a stochastic environment since
the agents were mutually independent and exhibited asynchronous motion
behavior. The task was further aggravated by providing the agents with a sparse
observation space and requiring them to generate continuous action commands so
as to efficiently, yet safely navigate to their respective goal locations,
while avoiding collisions with other dynamic peers and static obstacles at all
times. The experimental results are reported in terms of quantitative measures
and qualitative remarks for both training and deployment phases.
</p>
<a href="http://arxiv.org/abs/2011.05605" target="_blank">arXiv:2011.05605</a> [<a href="http://arxiv.org/pdf/2011.05605" target="_blank">pdf</a>]

<h2>Sim-To-Real Transfer for Miniature Autonomous Car Racing. (arXiv:2011.05617v1 [cs.AI])</h2>
<h3>Yeong-Jia Roger Chu, Ting-Han Wei, Jin-Bo Huang, Yuan-Hao Chen, I-Chen Wu</h3>
<p>Sim-to-real, a term that describes where a model is trained in a simulator
then transferred to the real world, is a technique that enables faster deep
reinforcement learning (DRL) training. However, differences between the
simulator and the real world often cause the model to perform poorly in the
real world. Domain randomization is a way to bridge the sim-to-real gap by
exposing the model to a wide range of scenarios so that it can generalize to
real-world situations. However, following domain randomization to train an
autonomous car racing model with DRL can lead to undesirable outcomes. Namely,
a model trained with randomization tends to run slower; a higher completion
rate on the testing track comes at the expense of longer lap times. This paper
aims to boost the robustness of a trained race car model without compromising
racing lap times. For a training track and a testing track having the same
shape (and same optimal paths), but with different lighting, background, etc.,
we first train a model (teacher model) that overfits the training track, moving
along a near optimal path. We then use this model to teach a student model the
correct actions along with randomization. With our method, a model with 18.4\%
completion rate on the testing track is able to help teach a student model with
52\% completion. Moreover, over an average of 50 trials, the student is able to
finish a lap 0.23 seconds faster than the teacher. This 0.23 second gap is
significant in tight races, with lap times of about 10 to 12 seconds.
</p>
<a href="http://arxiv.org/abs/2011.05617" target="_blank">arXiv:2011.05617</a> [<a href="http://arxiv.org/pdf/2011.05617" target="_blank">pdf</a>]

<h2>Scribble-Supervised Semantic Segmentation by Random Walk on Neural Representation and Self-Supervision on Neural Eigenspa. (arXiv:2011.05621v1 [cs.CV])</h2>
<h3>Zhiyi Pan, Peng Jiang, Changhe Tu</h3>
<p>Scribble-supervised semantic segmentation has gained much attention recently
for its promising performance without high-quality annotations. Many approaches
have been proposed. Typically, they handle this problem to either introduce a
well-labeled dataset from another related task, turn to iterative refinement
and post-processing with the graphical model, or manipulate the scribble label.
This work aims to achieve semantic segmentation supervised by scribble label
directly without auxiliary information and other intermediate manipulation.
Specifically, we impose diffusion on neural representation by random walk and
consistency on neural eigenspace by self-supervision, which forces the neural
network to produce dense and consistent predictions over the whole dataset. The
random walk embedded in the network will compute a probabilistic transition
matrix, with which the neural representation diffused to be uniform. Moreover,
given the probabilistic transition matrix, we apply the self-supervision on its
eigenspace for consistency in the image's main parts. In addition to comparing
the common scribble dataset, we also conduct experiments on the modified
datasets that randomly shrink and even drop the scribbles on image objects. The
results demonstrate the superiority of the proposed method and are even
comparable to some full-label supervised ones. The code and datasets are
available at https://github.com/panzhiyi/RW-SS.
</p>
<a href="http://arxiv.org/abs/2011.05621" target="_blank">arXiv:2011.05621</a> [<a href="http://arxiv.org/pdf/2011.05621" target="_blank">pdf</a>]

<h2>Robust Reinforcement Learning for General Video Game Playing. (arXiv:2011.05622v1 [cs.AI])</h2>
<h3>Chengpeng Hu, Ziqi Wang, Tianye Shu, Yang Tao, Hao Tong, Julian Togelius, Xin Yao, Jialin Liu</h3>
<p>Reinforcement learning has successfully learned to play challenging board and
video games. However, its generalization ability remains under-explored. The
General Video Game AI Learning Competition aims at designing agents that are
capable of learning to play different games levels that were unseen during
training. This paper presents the games, entries and results of the 2020
General Video Game AI Learning Competition, held at the Sixteenth International
Conference on Parallel Problem Solving from Nature and the 2020 IEEE Conference
on Games. Three new games with sparse, periodic and dense rewards,
respectively, were designed for this competition and the test levels were
generated by adding minor perturbations to training levels or combining
training levels. In this paper, we also design a reinforcement learning agent,
called Arcane, for general video game playing. We assume that it is more likely
to observe similar local information in different levels rather than global
information. Therefore, instead of directly inputting a single, raw pixel-based
screenshot of current game screen, Arcane takes the encoded, transformed global
and local observations of the game screen as two simultaneous inputs, aiming at
learning local information for playing new levels. Two versions of Arcane,
using a stochastic or deterministic policy for decision-making during test,
both show robust performance on the game set of the 2020 General Video Game AI
Learning Competition.
</p>
<a href="http://arxiv.org/abs/2011.05622" target="_blank">arXiv:2011.05622</a> [<a href="http://arxiv.org/pdf/2011.05622" target="_blank">pdf</a>]

<h2>Self-supervised Segmentation via Background Inpainting. (arXiv:2011.05626v1 [cs.CV])</h2>
<h3>Isinsu Katircioglu, Helge Rhodin, Victor Constantin, J&#xf6;rg Sp&#xf6;rri, Mathieu Salzmann, Pascal Fua</h3>
<p>While supervised object detection and segmentation methods achieve impressive
accuracy, they generalize poorly to images whose appearance significantly
differs from the data they have been trained on. To address this when
annotating data is prohibitively expensive, we introduce a self-supervised
detection and segmentation approach that can work with single images captured
by a potentially moving camera. At the heart of our approach lies the
observation that object segmentation and background reconstruction are linked
tasks, and that, for structured scenes, background regions can be
re-synthesized from their surroundings, whereas regions depicting the moving
object cannot. We encode this intuition into a self-supervised loss function
that we exploit to train a proposal-based segmentation network. To account for
the discrete nature of the proposals, we develop a Monte Carlo-based training
strategy that allows the algorithm to explore the large space of object
proposals. We apply our method to human detection and segmentation in images
that visually depart from those of standard benchmarks and outperform existing
self-supervised methods.
</p>
<a href="http://arxiv.org/abs/2011.05626" target="_blank">arXiv:2011.05626</a> [<a href="http://arxiv.org/pdf/2011.05626" target="_blank">pdf</a>]

<h2>Exploratory Grasping: Asymptotically Optimal Algorithms for Grasping Challenging Polyhedral Objects. (arXiv:2011.05632v1 [cs.RO])</h2>
<h3>Michael Danielczuk, Ashwin Balakrishna, Daniel S. Brown, Shivin Devgon, Ken Goldberg</h3>
<p>There has been significant recent work on data-driven algorithms for learning
general-purpose grasping policies. However, these policies can consistently
fail to grasp challenging objects which are significantly out of the
distribution of objects in the training data or which have very few high
quality grasps. Motivated by such objects, we propose a novel problem setting,
Exploratory Grasping, for efficiently discovering reliable grasps on an unknown
polyhedral object via sequential grasping, releasing, and toppling. We
formalize Exploratory Grasping as a Markov Decision Process, study the
theoretical complexity of Exploratory Grasping in the context of reinforcement
learning and present an efficient bandit-style algorithm, Bandits for Online
Rapid Grasp Exploration Strategy (BORGES), which leverages the structure of the
problem to efficiently discover high performing grasps for each object stable
pose. BORGES can be used to complement any general-purpose grasping algorithm
with any grasp modality (parallel-jaw, suction, multi-fingered, etc) to learn
policies for objects in which they exhibit persistent failures. Simulation
experiments suggest that BORGES can significantly outperform both
general-purpose grasping pipelines and two other online learning algorithms and
achieves performance within 5% of the optimal policy within 1000 and 8000
timesteps on average across 46 challenging objects from the Dex-Net adversarial
and EGAD! object datasets, respectively. Initial physical experiments suggest
that BORGES can improve grasp success rate by 45% over a Dex-Net baseline with
just 200 grasp attempts in the real world. See https://tinyurl.com/exp-grasping
for supplementary material and videos.
</p>
<a href="http://arxiv.org/abs/2011.05632" target="_blank">arXiv:2011.05632</a> [<a href="http://arxiv.org/pdf/2011.05632" target="_blank">pdf</a>]

<h2>Semi-supervised Sparse Representation with Graph Regularization for Image Classification. (arXiv:2011.05648v1 [cs.CV])</h2>
<h3>Hongfeng Li</h3>
<p>Image classification is a challenging problem for computer in reality. Large
numbers of methods can achieve satisfying performances with sufficient labeled
images. However, labeled images are still highly limited for certain image
classification tasks. Instead, lots of unlabeled images are available and easy
to be obtained. Therefore, making full use of the available unlabeled data can
be a potential way to further improve the performance of current image
classification methods. In this paper, we propose a discriminative
semi-supervised sparse representation algorithm for image classification. In
the algorithm, the classification process is combined with the sparse coding to
learn a data-driven linear classifier. To obtain discriminative predictions,
the predicted labels are regularized with three graphs, i.e., the global
manifold structure graph, the within-class graph and the between-classes graph.
The constructed graphs are able to extract structure information included in
both the labeled and unlabeled data. Moreover, the proposed method is extended
to a kernel version for dealing with data that cannot be linearly classified.
Accordingly, efficient algorithms are developed to solve the corresponding
optimization problems. Experimental results on several challenging databases
demonstrate that the proposed algorithm achieves excellent performances
compared with related popular methods.
</p>
<a href="http://arxiv.org/abs/2011.05648" target="_blank">arXiv:2011.05648</a> [<a href="http://arxiv.org/pdf/2011.05648" target="_blank">pdf</a>]

<h2>Skeleton-based Relational Reasoning for Group Activity Analysis. (arXiv:2011.05653v1 [cs.CV])</h2>
<h3>Mauricio Perez, Jun Liu, Alex C. Kot</h3>
<p>Research on group activity recognition mostly leans on standard two-stream
approach (RGB and Optical Flow) as their input features. Few have explored
explicit pose information, with none using it directly to reason about the
individuals interactions. In this paper, we leverage the skeleton information
to learn the interactions between the individuals straight from it. With our
proposed method GIRN, multiple relationship types are inferred from independent
modules, that describe the relations between the joints pair-by-pair.
Additionally to the joints relations, we also experiment with previously
unexplored relationship between individuals and relevant objects (e.g.
volleyball). The individuals distinct relations are then merged through an
attention mechanism, that gives more importance to those more relevant for
distinguishing the group activity. We evaluate our method in the Volleyball
dataset, obtaining competitive results to the state-of-the-art, even though
using a single modality. Therefore demonstrating the potential of
skeleton-based approaches for modeling multi-person interactions.
</p>
<a href="http://arxiv.org/abs/2011.05653" target="_blank">arXiv:2011.05653</a> [<a href="http://arxiv.org/pdf/2011.05653" target="_blank">pdf</a>]

<h2>LMB Filter Based Tracking Allowing for Multiple Hypotheses in Object Reference Point Association*. (arXiv:2011.05657v1 [cs.RO])</h2>
<h3>Martin Herrmann, Aldi Piroli, Jan Strohbeck, Johannes M&#xfc;ller, Michael Buchholz</h3>
<p>Autonomous vehicles need precise knowledge on dynamic objects in their
surroundings. Especially in urban areas with many objects and possible
occlusions, an infrastructure system based on a multi-sensor setup can provide
the required environment model for the vehicles. Previously, we have published
a concept of object reference points (e.g. the corners of an object), which
allows for generic sensor "plug and play" interfaces and relatively cheap
sensors. This paper describes a novel method to additionally incorporate
multiple hypotheses for fusing the measurements of the object reference points
using an extension to the previously presented Labeled Multi-Bernoulli (LMB)
filter. In contrast to the previous work, this approach improves the tracking
quality in the cases where the correct association of the measurement and the
object reference point is unknown. Furthermore, this paper identifies options
based on physical models to sort out inconsistent and unfeasible associations
at an early stage in order to keep the method computationally tractable for
real-time applications. The method is evaluated on simulations as well as on
real scenarios. In comparison to comparable methods, the proposed approach
shows a considerable performance increase, especially the number of
non-continuous tracks is decreased significantly.
</p>
<a href="http://arxiv.org/abs/2011.05657" target="_blank">arXiv:2011.05657</a> [<a href="http://arxiv.org/pdf/2011.05657" target="_blank">pdf</a>]

<h2>Accelerating Grasp Exploration by Leveraging Learned Priors. (arXiv:2011.05661v1 [cs.RO])</h2>
<h3>Han Yu Li, Michael Danielczuk, Ashwin Balakrishna, Vishal Satish, Ken Goldberg</h3>
<p>The ability of robots to grasp novel objects has industry applications in
e-commerce order fulfillment and home service. Data-driven grasping policies
have achieved success in learning general strategies for grasping arbitrary
objects. However, these approaches can fail to grasp objects which have complex
geometry or are significantly outside of the training distribution. We present
a Thompson sampling algorithm that learns to grasp a given object with unknown
geometry using online experience. The algorithm leverages learned priors from
the Dexterity Network robot grasp planner to guide grasp exploration and
provide probabilistic estimates of grasp success for each stable pose of the
novel object. We find that seeding the policy with the Dex-Net prior allows it
to more efficiently find robust grasps on these objects. Experiments suggest
that the best learned policy attains an average total reward 64.5% higher than
a greedy baseline and achieves within 5.7% of an oracle baseline when evaluated
over 300,000 training runs across a set of 3000 object poses.
</p>
<a href="http://arxiv.org/abs/2011.05661" target="_blank">arXiv:2011.05661</a> [<a href="http://arxiv.org/pdf/2011.05661" target="_blank">pdf</a>]

<h2>Distill2Vec: Dynamic Graph Representation Learning with Knowledge Distillation. (arXiv:2011.05664v1 [cs.LG])</h2>
<h3>Stefanos Antaris, Dimitrios Rafailidis</h3>
<p>Dynamic graph representation learning strategies are based on different
neural architectures to capture the graph evolution over time. However, the
underlying neural architectures require a large amount of parameters to train
and suffer from high online inference latency, that is several model parameters
have to be updated when new data arrive online. In this study we propose
Distill2Vec, a knowledge distillation strategy to train a compact model with a
low number of trainable parameters, so as to reduce the latency of online
inference and maintain the model accuracy high. We design a distillation loss
function based on Kullback-Leibler divergence to transfer the acquired
knowledge from a teacher model trained on offline data, to a small-size student
model for online data. Our experiments with publicly available datasets show
the superiority of our proposed model over several state-of-the-art approaches
with relative gains up to 5% in the link prediction task. In addition, we
demonstrate the effectiveness of our knowledge distillation strategy, in terms
of number of required parameters, where Distill2Vec achieves a compression
ratio up to 7:100 when compared with baseline approaches. For reproduction
purposes, our implementation is publicly available at
https://stefanosantaris.github.io/Distill2Vec.
</p>
<a href="http://arxiv.org/abs/2011.05664" target="_blank">arXiv:2011.05664</a> [<a href="http://arxiv.org/pdf/2011.05664" target="_blank">pdf</a>]

<h2>Progressive Spatio-Temporal Graph Convolutional Network for Skeleton-Based Human Action Recognition. (arXiv:2011.05668v1 [cs.CV])</h2>
<h3>Negar Heidari, Alexandros Iosifidis</h3>
<p>Graph convolutional networks (GCNs) have been very successful in
skeleton-based human action recognition where the sequence of skeletons is
modeled as a graph. However, most of the GCN-based methods in this area train a
deep feed-forward network with a fixed topology that leads to high
computational complexity and restricts their application in low computation
scenarios. In this paper, we propose a method to automatically find a compact
and problem-specific topology for spatio-temporal graph convolutional networks
in a progressive manner. Experimental results on two widely used datasets for
skeleton-based human action recognition indicate that the proposed method has
competitive or even better classification performance compared to the
state-of-the-art methods with much lower computational complexity.
</p>
<a href="http://arxiv.org/abs/2011.05668" target="_blank">arXiv:2011.05668</a> [<a href="http://arxiv.org/pdf/2011.05668" target="_blank">pdf</a>]

<h2>A Hybrid Approach for 6DoF Pose Estimation. (arXiv:2011.05669v1 [cs.CV])</h2>
<h3>Rebecca K&#xf6;nig, Bertram Drost</h3>
<p>We propose a method for 6DoF pose estimation of rigid objects that uses a
state-of-the-art deep learning based instance detector to segment object
instances in an RGB image, followed by a point-pair based voting method to
recover the object's pose. We additionally use an automatic method selection
that chooses the instance detector and the training set as that with the
highest performance on the validation set. This hybrid approach leverages the
best of learning and classic approaches, using CNNs to filter highly
unstructured data and cut through the clutter, and a local geometric approach
with proven convergence for robust pose estimation. The method is evaluated on
the BOP core datasets where it significantly exceeds the baseline method and is
the best fast method in the BOP 2020 Challenge.
</p>
<a href="http://arxiv.org/abs/2011.05669" target="_blank">arXiv:2011.05669</a> [<a href="http://arxiv.org/pdf/2011.05669" target="_blank">pdf</a>]

<h2>FPGA: Fast Patch-Free Global Learning Framework for Fully End-to-End Hyperspectral Image Classification. (arXiv:2011.05670v1 [cs.CV])</h2>
<h3>Zhuo Zheng, Yanfei Zhong, Ailong Ma, Liangpei Zhang</h3>
<p>Deep learning techniques have provided significant improvements in
hyperspectral image (HSI) classification. The current deep learning based HSI
classifiers follow a patch-based learning framework by dividing the image into
overlapping patches. As such, these methods are local learning methods, which
have a high computational cost. In this paper, a fast patch-free global
learning (FPGA) framework is proposed for HSI classification. In FPGA, an
encoder-decoder based FCN is utilized to consider the global spatial
information by processing the whole image, which results in fast inference.
However, it is difficult to directly utilize the encoder-decoder based FCN for
HSI classification as it always fails to converge due to the insufficiently
diverse gradients caused by the limited training samples. To solve the
divergence problem and maintain the abilities of FCN of fast inference and
global spatial information mining, a global stochastic stratified sampling
strategy is first proposed by transforming all the training samples into a
stochastic sequence of stratified samples. This strategy can obtain diverse
gradients to guarantee the convergence of the FCN in the FPGA framework. For a
better design of FCN architecture, FreeNet, which is a fully end-to-end network
for HSI classification, is proposed to maximize the exploitation of the global
spatial information and boost the performance via a spectral attention based
encoder and a lightweight decoder. A lateral connection module is also designed
to connect the encoder and decoder, fusing the spatial details in the encoder
and the semantic features in the decoder. The experimental results obtained
using three public benchmark datasets suggest that the FPGA framework is
superior to the patch-based framework in both speed and accuracy for HSI
classification. Code has been made available at:
https://github.com/Z-Zheng/FreeNet.
</p>
<a href="http://arxiv.org/abs/2011.05670" target="_blank">arXiv:2011.05670</a> [<a href="http://arxiv.org/pdf/2011.05670" target="_blank">pdf</a>]

<h2>VStreamDRLS: Dynamic Graph Representation Learning with Self-Attention for Enterprise Distributed Video Streaming Solutions. (arXiv:2011.05671v1 [cs.AI])</h2>
<h3>Stefanos Antaris, Dimitrios Rafailidis</h3>
<p>Live video streaming has become a mainstay as a standard communication
solution for several enterprises worldwide. To efficiently stream high-quality
live video content to a large amount of offices, companies employ distributed
video streaming solutions which rely on prior knowledge of the underlying
evolving enterprise network. However, such networks are highly complex and
dynamic. Hence, to optimally coordinate the live video distribution, the
available network capacity between viewers has to be accurately predicted. In
this paper we propose a graph representation learning technique on weighted and
dynamic graphs to predict the network capacity, that is the weights of
connections/links between viewers/nodes. We propose VStreamDRLS, a graph neural
network architecture with a self-attention mechanism to capture the evolution
of the graph structure of live video streaming events. VStreamDRLS employs the
graph convolutional network (GCN) model over the duration of a live video
streaming event and introduces a self-attention mechanism to evolve the GCN
parameters. In doing so, our model focuses on the GCN weights that are relevant
to the evolution of the graph and generate the node representation,
accordingly. We evaluate our proposed approach on the link prediction task on
two real-world datasets, generated by enterprise live video streaming events.
The duration of each event lasted an hour. The experimental results demonstrate
the effectiveness of VStreamDRLS when compared with state-of-the-art
strategies. Our evaluation datasets and implementation are publicly available
at https://github.com/stefanosantaris/vstreamdrls
</p>
<a href="http://arxiv.org/abs/2011.05671" target="_blank">arXiv:2011.05671</a> [<a href="http://arxiv.org/pdf/2011.05671" target="_blank">pdf</a>]

<h2>Zero-Pair Image to Image Translation using Domain Conditional Normalization. (arXiv:2011.05680v1 [cs.CV])</h2>
<h3>Samarth Shukla, Andr&#xe9;s Romero, Luc Van Gool, Radu Timofte</h3>
<p>In this paper, we propose an approach based on domain conditional
normalization (DCN) for zero-pair image-to-image translation, i.e., translating
between two domains which have no paired training data available but each have
paired training data with a third domain. We employ a single generator which
has an encoder-decoder structure and analyze different implementations of
domain conditional normalization to obtain the desired target domain output.
The validation benchmark uses RGB-depth pairs and RGB-semantic pairs for
training and compares performance for the depth-semantic translation task. The
proposed approaches improve in qualitative and quantitative terms over the
compared methods, while using much fewer parameters. Code available at
https://github.com/samarthshukla/dcn
</p>
<a href="http://arxiv.org/abs/2011.05680" target="_blank">arXiv:2011.05680</a> [<a href="http://arxiv.org/pdf/2011.05680" target="_blank">pdf</a>]

<h2>Noise Conscious Training of Non Local Neural Network powered by Self Attentive Spectral Normalized Markovian Patch GAN for Low Dose CT Denoising. (arXiv:2011.05684v1 [cs.CV])</h2>
<h3>Sutanu Bera, Prabir Kumar Biswas</h3>
<p>The explosive rise of the use of Computer tomography (CT) imaging in medical
practice has heightened public concern over the patient's associated radiation
dose. However, reducing the radiation dose leads to increased noise and
artifacts, which adversely degrades the scan's interpretability. Consequently,
an advanced image reconstruction algorithm to improve the diagnostic
performance of low dose ct arose as the primary concern among the researchers,
which is challenging due to the ill-posedness of the problem. In recent times,
the deep learning-based technique has emerged as a dominant method for low dose
CT(LDCT) denoising. However, some common bottleneck still exists, which hinders
deep learning-based techniques from furnishing the best performance. In this
study, we attempted to mitigate these problems with three novel accretions.
First, we propose a novel convolutional module as the first attempt to utilize
neighborhood similarity of CT images for denoising tasks. Our proposed module
assisted in boosting the denoising by a significant margin. Next, we moved
towards the problem of non-stationarity of CT noise and introduced a new noise
aware mean square error loss for LDCT denoising. Moreover, the loss mentioned
above also assisted to alleviate the laborious effort required while training
CT denoising network using image patches. Lastly, we propose a novel
discriminator function for CT denoising tasks. The conventional vanilla
discriminator tends to overlook the fine structural details and focus on the
global agreement. Our proposed discriminator leverage self-attention and
pixel-wise GANs for restoring the diagnostic quality of LDCT images. Our method
validated on a publicly available dataset of the 2016 NIH-AAPM-Mayo Clinic Low
Dose CT Grand Challenge performed remarkably better than the existing state of
the art method.
</p>
<a href="http://arxiv.org/abs/2011.05684" target="_blank">arXiv:2011.05684</a> [<a href="http://arxiv.org/pdf/2011.05684" target="_blank">pdf</a>]

<h2>Invariant Deep Compressible Covariance Pooling for Aerial Scene Categorization. (arXiv:2011.05702v1 [cs.CV])</h2>
<h3>Shidong Wang, Yi Ren, Gerard Parr, Yu Guan, Ling Shao</h3>
<p>Learning discriminative and invariant feature representation is the key to
visual image categorization. In this article, we propose a novel invariant deep
compressible covariance pooling (IDCCP) to solve nuisance variations in aerial
scene categorization. We consider transforming the input image according to a
finite transformation group that consists of multiple confounding orthogonal
matrices, such as the D4 group. Then, we adopt a Siamese-style network to
transfer the group structure to the representation space, where we can derive a
trivial representation that is invariant under the group action. The linear
classifier trained with trivial representation will also be possessed with
invariance. To further improve the discriminative power of representation, we
extend the representation to the tensor space while imposing orthogonal
constraints on the transformation matrix to effectively reduce feature
dimensions. We conduct extensive experiments on the publicly released aerial
scene image data sets and demonstrate the superiority of this method compared
with state-of-the-art methods. In particular, with using ResNet architecture,
our IDCCP model can reduce the dimension of the tensor representation by about
98% without sacrificing accuracy (i.e., &lt;0.5%).
</p>
<a href="http://arxiv.org/abs/2011.05702" target="_blank">arXiv:2011.05702</a> [<a href="http://arxiv.org/pdf/2011.05702" target="_blank">pdf</a>]

<h2>EvidentialMix: Learning with Combined Open-set and Closed-set Noisy Labels. (arXiv:2011.05704v1 [cs.LG])</h2>
<h3>Ragav Sachdeva, Filipe R. Cordeiro, Vasileios Belagiannis, Ian Reid, Gustavo Carneiro</h3>
<p>The efficacy of deep learning depends on large-scale data sets that have been
carefully curated with reliable data acquisition and annotation processes.
However, acquiring such large-scale data sets with precise annotations is very
expensive and time-consuming, and the cheap alternatives often yield data sets
that have noisy labels. The field has addressed this problem by focusing on
training models under two types of label noise: 1) closed-set noise, where some
training samples are incorrectly annotated to a training label other than their
known true class; and 2) open-set noise, where the training set includes
samples that possess a true class that is (strictly) not contained in the set
of known training labels. In this work, we study a new variant of the noisy
label problem that combines the open-set and closed-set noisy labels, and
introduce a benchmark evaluation to assess the performance of training
algorithms under this setup. We argue that such problem is more general and
better reflects the noisy label scenarios in practice. Furthermore, we propose
a novel algorithm, called EvidentialMix, that addresses this problem and
compare its performance with the state-of-the-art methods for both closed-set
and open-set noise on the proposed benchmark. Our results show that our method
produces superior classification results and better feature representations
than previous state-of-the-art methods. The code is available at
https://github.com/ragavsachdeva/EvidentialMix.
</p>
<a href="http://arxiv.org/abs/2011.05704" target="_blank">arXiv:2011.05704</a> [<a href="http://arxiv.org/pdf/2011.05704" target="_blank">pdf</a>]

<h2>EGAD: Evolving Graph Representation Learning with Self-Attention and Knowledge Distillation for Live Video Streaming Events. (arXiv:2011.05705v1 [cs.LG])</h2>
<h3>Stefanos Antaris, Dimitrios Rafailidis, Sarunas Girdzijauskas</h3>
<p>In this study, we present a dynamic graph representation learning model on
weighted graphs to accurately predict the network capacity of connections
between viewers in a live video streaming event. We propose EGAD, a neural
network architecture to capture the graph evolution by introducing a
self-attention mechanism on the weights between consecutive graph convolutional
networks. In addition, we account for the fact that neural architectures
require a huge amount of parameters to train, thus increasing the online
inference latency and negatively influencing the user experience in a live
video streaming event. To address the problem of the high online inference of a
vast number of parameters, we propose a knowledge distillation strategy. In
particular, we design a distillation loss function, aiming to first pretrain a
teacher model on offline data, and then transfer the knowledge from the teacher
to a smaller student model with less parameters. We evaluate our proposed model
on the link prediction task on three real-world datasets, generated by live
video streaming events. The events lasted 80 minutes and each viewer exploited
the distribution solution provided by the company Hive Streaming AB. The
experiments demonstrate the effectiveness of the proposed model in terms of
link prediction accuracy and number of required parameters, when evaluated
against state-of-the-art approaches. In addition, we study the distillation
performance of the proposed model in terms of compression ratio for different
distillation strategies, where we show that the proposed model can achieve a
compression ratio up to 15:100, preserving high link prediction accuracy. For
reproduction purposes, our evaluation datasets and implementation are publicly
available at https://stefanosantaris.github.io/EGAD.
</p>
<a href="http://arxiv.org/abs/2011.05705" target="_blank">arXiv:2011.05705</a> [<a href="http://arxiv.org/pdf/2011.05705" target="_blank">pdf</a>]

<h2>Reinforcement Learning with Time-dependent Goals for Robotic Musicians. (arXiv:2011.05715v1 [cs.RO])</h2>
<h3>Thilo Fryen, Manfred Eppe, Phuong D.H. Nguyen, Timo Gerkmann, Stefan Wermter</h3>
<p>Reinforcement learning is a promising method to accomplish robotic control
tasks. The task of playing musical instruments is, however, largely unexplored
because it involves the challenge of achieving sequential goals - melodies -
that have a temporal dimension. In this paper, we address robotic musicianship
by introducing a temporal extension to goal-conditioned reinforcement learning:
Time-dependent goals. We demonstrate that these can be used to train a robotic
musician to play the theremin instrument. We train the robotic agent in
simulation and transfer the acquired policy to a real-world robotic
thereminist. Supplemental video: https://youtu.be/jvC9mPzdQN4
</p>
<a href="http://arxiv.org/abs/2011.05715" target="_blank">arXiv:2011.05715</a> [<a href="http://arxiv.org/pdf/2011.05715" target="_blank">pdf</a>]

<h2>Filtered Manifold Alignment. (arXiv:2011.05716v1 [cs.LG])</h2>
<h3>Stefan Dernbach, Don Towsley</h3>
<p>Domain adaptation is an essential task in transfer learning to leverage data
in one domain to bolster learning in another domain. In this paper, we present
a new semi-supervised manifold alignment technique based on a two-step approach
of projecting and filtering the source and target domains to low dimensional
spaces followed by joining the two spaces. Our proposed approach, filtered
manifold alignment (FMA), reduces the computational complexity of previous
manifold alignment techniques, is flexible enough to align domains with
completely disparate sets of feature and demonstrates state-of-the-art
classification accuracy on multiple benchmark domain adaptation tasks composed
of classifying real world image datasets.
</p>
<a href="http://arxiv.org/abs/2011.05716" target="_blank">arXiv:2011.05716</a> [<a href="http://arxiv.org/pdf/2011.05716" target="_blank">pdf</a>]

<h2>Generative Adversarial Network to Learn Valid Distributions of Robot Configurations for Inverse Kinematics and Constrained Motion Planning. (arXiv:2011.05717v1 [cs.RO])</h2>
<h3>Teguh Santoso Lembono, Emmanuel Pignat, Julius Jankowski, Sylvain Calinon</h3>
<p>In high dimensional robotic system, the manifold of the valid configuration
space often has complex shape, especially under constraints such as
end-effector orientation, static stability, and obstacles. We propose a
generative adversarial network approach to learn the distribution of valid
robot configurations. It can generate configurations that are close to the
constraint manifold. We present two applications of this method. First, by
learning the conditional distribution with respect to the desired end-effector
position, we can do fast inverse kinematics even for very high
degrees-of-freedom (DoF) systems. Then, it can be used to generate samples in
sampling based constrained motion planning algorithms to reduce the necessary
projection steps, speeding up computation. We validate the approach in
simulation using the 7-DoF Panda manipulator and the 28-DoF humanoid robot
Talos.
</p>
<a href="http://arxiv.org/abs/2011.05717" target="_blank">arXiv:2011.05717</a> [<a href="http://arxiv.org/pdf/2011.05717" target="_blank">pdf</a>]

<h2>Learning from THEODORE: A Synthetic Omnidirectional Top-View Indoor Dataset for Deep Transfer Learning. (arXiv:2011.05719v1 [cs.CV])</h2>
<h3>Tobias Scheck, Roman Seidel, Gangolf Hirtz</h3>
<p>Recent work about synthetic indoor datasets from perspective views has shown
significant improvements of object detection results with Convolutional Neural
Networks(CNNs). In this paper, we introduce THEODORE: a novel, large-scale
indoor dataset containing 100,000 high-resolution diversified fisheye images
with 14 classes. To this end, we create 3D virtual environments of living
rooms, different human characters and interior textures. Beside capturing
fisheye images from virtual environments we create annotations for semantic
segmentation, instance masks and bounding boxes for object detection tasks. We
compare our synthetic dataset to state of the art real-world datasets for
omnidirectional images. Based on MS COCO weights, we show that our dataset is
well suited for fine-tuning CNNs for object detection. Through a high
generalization of our models by means of image synthesis and domain
randomization, we reach an AP up to 0.84 for class person on High-Definition
Analytics dataset.
</p>
<a href="http://arxiv.org/abs/2011.05719" target="_blank">arXiv:2011.05719</a> [<a href="http://arxiv.org/pdf/2011.05719" target="_blank">pdf</a>]

<h2>A CNN-based Feature Space for Semi-supervised Incremental Learning in Assisted Living Applications. (arXiv:2011.05734v1 [cs.CV])</h2>
<h3>Tobias Scheck, Ana Perez Grassi, Gangolf Hirtz</h3>
<p>A Convolutional Neural Network (CNN) is sometimes confronted with objects of
changing appearance ( new instances) that exceed its generalization capability.
This requires the CNN to incorporate new knowledge, i.e., to learn
incrementally. In this paper, we are concerned with this problem in the context
of assisted living. We propose using the feature space that results from the
training dataset to automatically label problematic images that could not be
properly recognized by the CNN. The idea is to exploit the extra information in
the feature space for a semi-supervised labeling and to employ problematic
images to improve the CNN's classification model. Among other benefits, the
resulting semi-supervised incremental learning process allows improving the
classification accuracy of new instances by 40% as illustrated by extensive
experiments.
</p>
<a href="http://arxiv.org/abs/2011.05734" target="_blank">arXiv:2011.05734</a> [<a href="http://arxiv.org/pdf/2011.05734" target="_blank">pdf</a>]

<h2>DeepSim: Semantic similarity metrics for learned image registration. (arXiv:2011.05735v1 [cs.CV])</h2>
<h3>Steffen Czolbe, Oswin Krause, Aasa Feragen</h3>
<p>We propose a semantic similarity metric for image registration. Existing
metrics like euclidean distance or normalized cross-correlation focus on
aligning intensity values, giving difficulties with low intensity contrast or
noise. Our semantic approach learns dataset-specific features that drive the
optimization of a learning-based registration model. Comparing to existing
unsupervised and supervised methods across multiple image modalities and
applications, we achieve consistently high registration accuracy and faster
convergence than state of the art, and the learned invariance to noise gives
smoother transformations on low-quality images.
</p>
<a href="http://arxiv.org/abs/2011.05735" target="_blank">arXiv:2011.05735</a> [<a href="http://arxiv.org/pdf/2011.05735" target="_blank">pdf</a>]

<h2>Survey on 3D face reconstruction from uncalibrated images. (arXiv:2011.05740v1 [cs.CV])</h2>
<h3>Araceli Morales, Gemma Piella, Federico M. Sukno</h3>
<p>Recently, a lot of attention has been focused on the incorporation of 3D data
into face analysis and its applications. Despite providing a more accurate
representation of the face, 3D face images are more complex to acquire than 2D
pictures. As a consequence, great effort has been invested in developing
systems that reconstruct 3D faces from an uncalibrated 2D image. However, the
3D-from-2D face reconstruction problem is ill-posed, thus prior knowledge is
needed to restrict the solutions space. In this work, we review 3D face
reconstruction methods in the last decade, focusing on those that only use 2D
pictures captured under uncontrolled conditions. We present a classification of
the proposed methods based on the technique used to add prior knowledge,
considering three main strategies, namely, statistical model fitting,
photometry, and deep learning, and reviewing each of them separately. In
addition, given the relevance of statistical 3D facial models as prior
knowledge, we explain the construction procedure and provide a comprehensive
list of the publicly available 3D facial models. After the exhaustive study of
3D-from-2D face reconstruction approaches, we observe that the deep learning
strategy is rapidly growing since the last few years, matching its extension to
that of the widespread statistical model fitting. Unlike the other two
strategies, photometry-based methods have decreased in number since the
required strong assumptions cause the reconstructions to be of more limited
quality than those resulting from model fitting and deep learning methods. The
review also identifies current gaps and suggests avenues for future research.
</p>
<a href="http://arxiv.org/abs/2011.05740" target="_blank">arXiv:2011.05740</a> [<a href="http://arxiv.org/pdf/2011.05740" target="_blank">pdf</a>]

<h2>Behaviorally Diverse Traffic Simulation via Reinforcement Learning. (arXiv:2011.05741v1 [cs.LG])</h2>
<h3>Shinya Shiroshita, Shirou Maruyama, Daisuke Nishiyama, Mario Ynocente Castro, Karim Hamzaoui, Guy Rosman, Jonathan DeCastro, Kuan-Hui Lee, Adrien Gaidon</h3>
<p>Traffic simulators are important tools in autonomous driving development.
While continuous progress has been made to provide developers more options for
modeling various traffic participants, tuning these models to increase their
behavioral diversity while maintaining quality is often very challenging. This
paper introduces an easily-tunable policy generation algorithm for autonomous
driving agents. The proposed algorithm balances diversity and driving skills by
leveraging the representation and exploration abilities of deep reinforcement
learning via a distinct policy set selector. Moreover, we present an algorithm
utilizing intrinsic rewards to widen behavioral differences in the training. To
provide quantitative assessments, we develop two trajectory-based evaluation
metrics which measure the differences among policies and behavioral coverage.
We experimentally show the effectiveness of our methods on several challenging
intersection scenes.
</p>
<a href="http://arxiv.org/abs/2011.05741" target="_blank">arXiv:2011.05741</a> [<a href="http://arxiv.org/pdf/2011.05741" target="_blank">pdf</a>]

<h2>End-To-End Semi-supervised Learning for Differentiable Particle Filters. (arXiv:2011.05748v1 [cs.LG])</h2>
<h3>Hao Wen, Xiongjie Chen, Georgios Papagiannis, Conghui Hu, Yunpeng Li</h3>
<p>Recent advances in incorporating neural networks into particle filters
provide the desired flexibility to apply particle filters in large-scale
real-world applications. The dynamic and measurement models in this framework
are learnable through the differentiable implementation of particle filters.
Past efforts in optimising such models often require the knowledge of true
states which can be expensive to obtain or even unavailable in practice. In
this paper, in order to reduce the demand for annotated data, we present an
end-to-end learning objective based upon the maximisation of a
pseudo-likelihood function which can improve the estimation of states when
large portion of true states are unknown. We assess performance of the proposed
method in state estimation tasks in robotics with simulated and real-world
datasets.
</p>
<a href="http://arxiv.org/abs/2011.05748" target="_blank">arXiv:2011.05748</a> [<a href="http://arxiv.org/pdf/2011.05748" target="_blank">pdf</a>]

<h2>Finding Relevant Flood Images on Twitter using Content-based Filters. (arXiv:2011.05756v1 [cs.CV])</h2>
<h3>Bj&#xf6;rn Barz, Kai Schr&#xf6;ter, Ann-Christin Kra, Joachim Denzler</h3>
<p>The analysis of natural disasters such as floods in a timely manner often
suffers from limited data due to coarsely distributed sensors or sensor
failures. At the same time, a plethora of information is buried in an abundance
of images of the event posted on social media platforms such as Twitter. These
images could be used to document and rapidly assess the situation and derive
proxy-data not available from sensors, e.g., the degree of water pollution.
However, not all images posted online are suitable or informative enough for
this purpose. Therefore, we propose an automatic filtering approach using
machine learning techniques for finding Twitter images that are relevant for
one of the following information objectives: assessing the flooded area, the
inundation depth, and the degree of water pollution. Instead of relying on
textual information present in the tweet, the filter analyzes the image
contents directly. We evaluate the performance of two different approaches and
various features on a case-study of two major flooding events. Our image-based
filter is able to enhance the quality of the results substantially compared
with a keyword-based filter, improving the mean average precision from 23% to
53% on average.
</p>
<a href="http://arxiv.org/abs/2011.05756" target="_blank">arXiv:2011.05756</a> [<a href="http://arxiv.org/pdf/2011.05756" target="_blank">pdf</a>]

<h2>Simulating Autonomous Driving in Massive Mixed Urban Traffic. (arXiv:2011.05767v1 [cs.RO])</h2>
<h3>Yuanfu Luo, Panpan Cai, Yiyuan Lee, David Hsu</h3>
<p>Autonomous driving in an unregulated urban crowd is an outstanding challenge,
especially, in the presence of many aggressive, high-speed traffic
participants. This paper presents SUMMIT, a high-fidelity simulator that
facilitates the development and testing of crowd-driving algorithms. SUMMIT
simulates dense, unregulated urban traffic at any worldwide locations as
supported by the OpenStreetMap. The core of SUMMIT is a multi-agent motion
model, GAMMA, that models the behaviours of heterogeneous traffic agents, and a
real-time POMDP planner, Context-POMDP, that serves as a driving expert. SUMMIT
is built as an extension of CARLA and inherits from it the physical and visual
realism for autonomous driving simulation. SUMMIT supports a wide range of
applications, including perception, vehicle control or planning, and end-to-end
learning. We validate the realism of our motion model using its traffic motion
prediction accuracy on various real-world data sets. We also provide several
real-world benchmark scenarios to show that SUMMIT simulates complex, realistic
traffic behaviors, and Context-POMDP drives safely and efficiently in
challenging crowd-driving settings.
</p>
<a href="http://arxiv.org/abs/2011.05767" target="_blank">arXiv:2011.05767</a> [<a href="http://arxiv.org/pdf/2011.05767" target="_blank">pdf</a>]

<h2>Reinforcement Learning Experiments and Benchmark for Solving Robotic Reaching Tasks. (arXiv:2011.05782v1 [cs.RO])</h2>
<h3>Pierre Aumjaud, David McAuliffe, Francisco Javier Rodr&#xed;guez Lera, Philip Cardiff</h3>
<p>Reinforcement learning has shown great promise in robotics thanks to its
ability to develop efficient robotic control procedures through self-training.
In particular, reinforcement learning has been successfully applied to solving
the reaching task with robotic arms. In this paper, we define a robust,
reproducible and systematic experimental procedure to compare the performance
of various model-free algorithms at solving this task. The policies are trained
in simulation and are then transferred to a physical robotic manipulator. It is
shown that augmenting the reward signal with the Hindsight Experience Replay
exploration technique increases the average return of off-policy agents between
7 and 9 folds when the target position is initialised randomly at the beginning
of each episode.
</p>
<a href="http://arxiv.org/abs/2011.05782" target="_blank">arXiv:2011.05782</a> [<a href="http://arxiv.org/pdf/2011.05782" target="_blank">pdf</a>]

<h2>SPRITE: Stewart Platform Robot for Interactive Tabletop Engagement. (arXiv:2011.05786v1 [cs.RO])</h2>
<h3>Elaine Schaertl Short, Dale Short, Yifeng Fu, Maja Matari&#x107;</h3>
<p>We present the design of the Stewart Platform Robot for Interactive Tabletop
Engagement (SPRITE). This robot is designed for use in socially assistive
robotics, a field focusing on non-contact social interaction to help people
achieve goals relating to health, wellness, and education. We describe a series
of design goals for a tabletop, socially assistive robot, including expressive
movement, affective communication, a friendly, nonthreatening, and customizable
appearance, and a safe, robust, and easily-repaired mechanical design.
</p>
<a href="http://arxiv.org/abs/2011.05786" target="_blank">arXiv:2011.05786</a> [<a href="http://arxiv.org/pdf/2011.05786" target="_blank">pdf</a>]

<h2>Learned Equivariant Rendering without Transformation Supervision. (arXiv:2011.05787v1 [cs.CV])</h2>
<h3>Cinjon Resnick, Or Litany, Hugo Larochelle, Joan Bruna, Kyunghyun Cho</h3>
<p>We propose a self-supervised framework to learn scene representations from
video that are automatically delineated into objects and background. Our method
relies on moving objects being equivariant with respect to their transformation
across frames and the background being constant. After training, we can
manipulate and render the scenes in real time to create unseen combinations of
objects, transformations, and backgrounds. We show results on moving MNIST with
backgrounds.
</p>
<a href="http://arxiv.org/abs/2011.05787" target="_blank">arXiv:2011.05787</a> [<a href="http://arxiv.org/pdf/2011.05787" target="_blank">pdf</a>]

<h2>Interpretable and synergistic deep learning for visual explanation and statistical estimations of segmentation of disease features from medical images. (arXiv:2011.05791v1 [stat.ML])</h2>
<h3>Sambuddha Ghosal, Pratik Shah</h3>
<p>Deep learning (DL) models for disease classification or segmentation from
medical images are increasingly trained using transfer learning (TL) from
unrelated natural world images. However, shortcomings and utility of TL for
specialized tasks in the medical imaging domain remain unknown and are based on
assumptions that increasing training data will improve performance. We report
detailed comparisons, rigorous statistical analysis and comparisons of widely
used DL architecture for binary segmentation after TL with ImageNet
initialization (TII-models) with supervised learning with only medical
images(LMI-models) of macroscopic optical skin cancer, microscopic prostate
core biopsy and Computed Tomography (CT) DICOM images. Through visual
inspection of TII and LMI model outputs and their Grad-CAM counterparts, our
results identify several counter intuitive scenarios where automated
segmentation of one tumor by both models or the use of individual segmentation
output masks in various combinations from individual models leads to 10%
increase in performance. We also report sophisticated ensemble DL strategies
for achieving clinical grade medical image segmentation and model explanations
under low data regimes. For example; estimating performance, explanations and
replicability of LMI and TII models described by us can be used for situations
in which sparsity promotes better learning. A free GitHub repository of TII and
LMI models, code and more than 10,000 medical images and their Grad-CAM output
from this study can be used as starting points for advanced computational
medicine and DL research for biomedical discovery and applications.
</p>
<a href="http://arxiv.org/abs/2011.05791" target="_blank">arXiv:2011.05791</a> [<a href="http://arxiv.org/pdf/2011.05791" target="_blank">pdf</a>]

<h2>Regularization of Persistent Homology Gradient Computation. (arXiv:2011.05804v1 [cs.LG])</h2>
<h3>Padraig Corcoran, Bailin Deng</h3>
<p>Persistent homology is a method for computing the topological features
present in a given data. Recently, there has been much interest in the
integration of persistent homology as a computational step in neural networks
or deep learning. In order for a given computation to be integrated in such a
way, the computation in question must be differentiable. Computing the
gradients of persistent homology is an ill-posed inverse problem with
infinitely many solutions. Consequently, it is important to perform
regularization so that the solution obtained agrees with known priors. In this
work we propose a novel method for regularizing persistent homology gradient
computation through the addition of a grouping term. This has the effect of
helping to ensure gradients are defined with respect to larger entities and not
individual points.
</p>
<a href="http://arxiv.org/abs/2011.05804" target="_blank">arXiv:2011.05804</a> [<a href="http://arxiv.org/pdf/2011.05804" target="_blank">pdf</a>]

<h2>Dynamic Plane Convolutional Occupancy Networks. (arXiv:2011.05813v1 [cs.CV])</h2>
<h3>Stefan Lionar, Daniil Emtsev, Dusan Svilarkovic, Songyou Peng</h3>
<p>Learning-based 3D reconstruction using implicit neural representations has
shown promising progress not only at the object level but also in more
complicated scenes. In this paper, we propose Dynamic Plane Convolutional
Occupancy Networks, a novel implicit representation pushing further the quality
of 3D surface reconstruction. The input noisy point clouds are encoded into
per-point features that are projected onto multiple 2D dynamic planes. A
fully-connected network learns to predict plane parameters that best describe
the shapes of objects or scenes. To further exploit translational equivariance,
convolutional neural networks are applied to process the plane features. Our
method shows superior performance in surface reconstruction from unoriented
point clouds in ShapeNet as well as an indoor scene dataset. Moreover, we also
provide interesting observations on the distribution of learned dynamic planes.
</p>
<a href="http://arxiv.org/abs/2011.05813" target="_blank">arXiv:2011.05813</a> [<a href="http://arxiv.org/pdf/2011.05813" target="_blank">pdf</a>]

<h2>Duality-Induced Regularizer for Tensor Factorization Based Knowledge Graph Completion. (arXiv:2011.05816v1 [cs.LG])</h2>
<h3>Zhanqiu Zhang, Jianyu Cai, Jie Wang</h3>
<p>Tensor factorization based models have shown great power in knowledge graph
completion (KGC). However, their performance usually suffers from the
overfitting problem seriously. This motivates various regularizers---such as
the squared Frobenius norm and tensor nuclear norm regularizers---while the
limited applicability significantly limits their practical usage. To address
this challenge, we propose a novel regularizer---namely, DUality-induced
RegulArizer (DURA)---which is not only effective in improving the performance
of existing models but widely applicable to various methods. The major novelty
of DURA is based on the observation that, for an existing tensor factorization
based KGC model (primal), there is often another distance based KGC model
(dual) closely associated with it. Experiments show that DURA yields consistent
and significant improvements on benchmarks.
</p>
<a href="http://arxiv.org/abs/2011.05816" target="_blank">arXiv:2011.05816</a> [<a href="http://arxiv.org/pdf/2011.05816" target="_blank">pdf</a>]

<h2>FINO-Net: A Deep Multimodal Sensor Fusion Framework for Manipulation Failure Detection. (arXiv:2011.05817v1 [cs.RO])</h2>
<h3>Arda Inceoglu, Eren Erdal Aksoy, Abdullah Cihan Ak, Sanem Sariel</h3>
<p>Safe manipulation in unstructured environments for service robots is a
challenging problem. A failure detection system is needed to monitor and detect
unintended outcomes. We propose FINO-Net, a novel multimodal sensor fusion
based deep neural network to detect and identify manipulation failures. We also
introduce a multimodal dataset, containing 229 real-world manipulation data
recorded with a Baxter robot. Our network combines RGB, depth and audio
readings to effectively detect and classify failures. Results indicate that
fusing RGB with depth and audio modalities significantly improves the
performance. FINO-Net achieves 98.60% detection and 87.31% classification
accuracy on our novel dataset. Code and data are publicly available at
https://github.com/ardai/fino-net.
</p>
<a href="http://arxiv.org/abs/2011.05817" target="_blank">arXiv:2011.05817</a> [<a href="http://arxiv.org/pdf/2011.05817" target="_blank">pdf</a>]

<h2>Where to drive: free space detection with one fisheye camera. (arXiv:2011.05822v1 [cs.CV])</h2>
<h3>Tobias Scheck, Adarsh Mallandur, Christian Wiede, Gangolf Hirtz</h3>
<p>The development in the field of autonomous driving goes hand in hand with
ever new developments in the field of image processing and machine learning
methods. In order to fully exploit the advantages of deep learning, it is
necessary to have sufficient labeled training data available. This is
especially not the case for omnidirectional fisheye cameras. As a solution, we
propose in this paper to use synthetic training data based on Unity3D. A
five-pass algorithm is used to create a virtual fisheye camera. This synthetic
training data is evaluated for the application of free space detection for
different deep learning network architectures. The results indicate that
synthetic fisheye images can be used in deep learning context.
</p>
<a href="http://arxiv.org/abs/2011.05822" target="_blank">arXiv:2011.05822</a> [<a href="http://arxiv.org/pdf/2011.05822" target="_blank">pdf</a>]

<h2>Semi-Structured Deep Piecewise Exponential Models. (arXiv:2011.05824v1 [cs.LG])</h2>
<h3>Philipp Kopper, Sebastian P&#xf6;lsterl, Christian Wachinger, Bernd Bischl, Andreas Bender, David R&#xfc;gamer</h3>
<p>We propose a versatile framework for survival analysis that combines advanced
concepts from statistics with deep learning. The presented framework is based
on piecewise exponential models and thereby supports various survival tasks,
such as competing risks and multi-state modeling, and further allows for
estimation of time-varying effects and time-varying features. To also include
multiple data sources and higher-order interaction effects into the model, we
embed the model class in a neural network and thereby enable the simultaneous
estimation of both inherently interpretable structured regression inputs as
well as deep neural network components which can potentially process additional
unstructured data sources. A proof of concept is provided by using the
framework to predict Alzheimer's disease progression based on tabular and 3D
point cloud data and applying it to synthetic data.
</p>
<a href="http://arxiv.org/abs/2011.05824" target="_blank">arXiv:2011.05824</a> [<a href="http://arxiv.org/pdf/2011.05824" target="_blank">pdf</a>]

<h2>Neural Empirical Bayes: Source Distribution Estimation and its Applications to Simulation-Based Inference. (arXiv:2011.05836v1 [stat.ML])</h2>
<h3>Maxime Vandegar, Michael Kagan, Antoine Wehenkel, Gilles Louppe</h3>
<p>We revisit empirical Bayes in the absence of a tractable likelihood function,
as is typical in scientific domains relying on computer simulations. We
investigate how the empirical Bayesian can make use of neural density
estimators first to use all noise-corrupted observations to estimate a prior or
source distribution over uncorrupted samples, and then to perform
single-observation posterior inference using the fitted source distribution. We
propose an approach based on the direct maximization of the log-marginal
likelihood of the observations, examining both biased and de-biased estimators,
and comparing to variational approaches. We find that, up to symmetries, a
neural empirical Bayes approach recovers ground truth source distributions.
With the learned source distribution in hand, we show the applicability to
likelihood-free inference and examine the quality of the resulting posterior
estimates. Finally, we demonstrate the applicability of Neural Empirical Bayes
on an inverse problem from collider physics.
</p>
<a href="http://arxiv.org/abs/2011.05836" target="_blank">arXiv:2011.05836</a> [<a href="http://arxiv.org/pdf/2011.05836" target="_blank">pdf</a>]

<h2>Offline Learning of Counterfactual Perception as Prediction for Real-World Robotic Reinforcement Learning. (arXiv:2011.05857v1 [cs.RO])</h2>
<h3>Jun Jin, Daniel Graves, Cameron Haigh, Jun Luo, Martin Jagersand</h3>
<p>We propose a method for offline learning of counterfactual predictions to
address real world robotic reinforcement learning challenges. The proposed
method encodes action-oriented visual observations as several "what if"
questions learned offline from prior experience using reinforcement learning
methods. These "what if" questions counterfactually predict how
action-conditioned observation would evolve on multiple temporal scales if the
agent were to stick to its current action. We show that combining these offline
counterfactual predictions along with online in-situ observations (e.g. force
feedback) allows efficient policy learning with only a sparse terminal
(success/failure) reward. We argue that the learned predictions form an
effective representation of the visual task, and guide the online exploration
towards high-potential success interactions (e.g. contact-rich regions).
Experiments were conducted in both simulation and real-world scenarios for
evaluation. Our results demonstrate that it is practical to train a
reinforcement learning agent to perform real-world fine manipulation in about
half a day, without hand engineered perception systems or calibrated
instrumentation. Recordings of the real robot training can be found via
https://sites.google.com/view/realrl.
</p>
<a href="http://arxiv.org/abs/2011.05857" target="_blank">arXiv:2011.05857</a> [<a href="http://arxiv.org/pdf/2011.05857" target="_blank">pdf</a>]

<h2>DeepI2I: Enabling Deep Hierarchical Image-to-Image Translation by Transferring from GANs. (arXiv:2011.05867v1 [cs.CV])</h2>
<h3>Yaxing Wang, Lu Yu, Joost van de Weijer</h3>
<p>Image-to-image translation has recently achieved remarkable results. But
despite current success, it suffers from inferior performance when translations
between classes require large shape changes. We attribute this to the
high-resolution bottlenecks which are used by current state-of-the-art
image-to-image methods. Therefore, in this work, we propose a novel deep
hierarchical Image-to-Image Translation method, called DeepI2I. We learn a
model by leveraging hierarchical features: (a) structural information contained
in the shallow layers and (b) semantic information extracted from the deep
layers. To enable the training of deep I2I models on small datasets, we propose
a novel transfer learning method, that transfers knowledge from pre-trained
GANs. Specifically, we leverage the discriminator of a pre-trained GANs (i.e.
BigGAN or StyleGAN) to initialize both the encoder and the discriminator and
the pre-trained generator to initialize the generator of our model. Applying
knowledge transfer leads to an alignment problem between the encoder and
generator. We introduce an adaptor network to address this. On many-class
image-to-image translation on three datasets (Animal faces, Birds, and Foods)
we decrease mFID by at least 35% when compared to the state-of-the-art.
Furthermore, we qualitatively and quantitatively demonstrate that transfer
learning significantly improves the performance of I2I systems, especially for
small datasets. Finally, we are the first to perform I2I translations for
domains with over 100 classes.
</p>
<a href="http://arxiv.org/abs/2011.05867" target="_blank">arXiv:2011.05867</a> [<a href="http://arxiv.org/pdf/2011.05867" target="_blank">pdf</a>]

<h2>A Primal Approach to Constrained Policy Optimization: Global Optimality and Finite-Time Analysis. (arXiv:2011.05869v1 [cs.LG])</h2>
<h3>Tengyu Xu, Yingbin Liang, Guanghui Lan</h3>
<p>Safe reinforcement learning (SRL) problems are typically modeled as
constrained Markov Decision Process (CMDP), in which an agent explores the
environment to maximize the expected total reward and meanwhile avoids
violating certain constraints on a number of expected total costs. In general,
such SRL problems have nonconvex objective functions subject to multiple
nonconvex constraints, and hence are very challenging to solve, particularly to
provide a globally optimal policy. Many popular SRL algorithms adopt a
primal-dual structure which utilizes the updating of dual variables for
satisfying the constraints. In contrast, we propose a primal approach, called
constraint-rectified policy optimization (CRPO), which updates the policy
alternatingly between objective improvement and constraint satisfaction. CRPO
provides a primal-type algorithmic framework to solve SRL problems, where each
policy update can take any variant of policy optimization step. To demonstrate
the theoretical performance of CRPO, we adopt natural policy gradient (NPG) for
each policy update step and show that CRPO achieves an
$\mathcal{O}(1/\sqrt{T})$ convergence rate to the global optimal policy in the
constrained policy set and an $\mathcal{O}(1/\sqrt{T})$ error bound on
constraint satisfaction. This is the first finite-time analysis of SRL
algorithms with global optimality guarantee. Our empirical results demonstrate
that CRPO can outperform the existing primal-dual baseline algorithms
significantly.
</p>
<a href="http://arxiv.org/abs/2011.05869" target="_blank">arXiv:2011.05869</a> [<a href="http://arxiv.org/pdf/2011.05869" target="_blank">pdf</a>]

<h2>FAT: Training Neural Networks for Reliable Inference Under Hardware Faults. (arXiv:2011.05873v1 [cs.LG])</h2>
<h3>Ussama Zahid, Giulio Gambardella, Nicholas J. Fraser, Michaela Blott, Kees Vissers</h3>
<p>Deep neural networks (DNNs) are state-of-the-art algorithms for multiple
applications, spanning from image classification to speech recognition. While
providing excellent accuracy, they often have enormous compute and memory
requirements. As a result of this, quantized neural networks (QNNs) are
increasingly being adopted and deployed especially on embedded devices, thanks
to their high accuracy, but also since they have significantly lower compute
and memory requirements compared to their floating point equivalents. QNN
deployment is also being evaluated for safety-critical applications, such as
automotive, avionics, medical or industrial. These systems require functional
safety, guaranteeing failure-free behaviour even in the presence of hardware
faults. In general fault tolerance can be achieved by adding redundancy to the
system, which further exacerbates the overall computational demands and makes
it difficult to meet the power and performance requirements. In order to
decrease the hardware cost for achieving functional safety, it is vital to
explore domain-specific solutions which can exploit the inherent features of
DNNs. In this work we present a novel methodology called fault-aware training
(FAT), which includes error modeling during neural network (NN) training, to
make QNNs resilient to specific fault models on the device. Our experiments
show that by injecting faults in the convolutional layers during training,
highly accurate convolutional neural networks (CNNs) can be trained which
exhibits much better error tolerance compared to the original. Furthermore, we
show that redundant systems which are built from QNNs trained with FAT achieve
higher worse-case accuracy at lower hardware cost. This has been validated for
numerous classification tasks including CIFAR10, GTSRB, SVHN and ImageNet.
</p>
<a href="http://arxiv.org/abs/2011.05873" target="_blank">arXiv:2011.05873</a> [<a href="http://arxiv.org/pdf/2011.05873" target="_blank">pdf</a>]

<h2>Matrix Completion with Noise via Leveraged Sampling. (arXiv:2011.05885v1 [cs.LG])</h2>
<h3>Xinjian Huang, Weiwei Liu, Bo Du</h3>
<p>Many matrix completion methods assume that the data follows the uniform
distribution. To address the limitation of this assumption, Chen et al.
\cite{Chen20152999} propose to recover the matrix where the data follows the
specific biased distribution. Unfortunately, in most real-world applications,
the recovery of a data matrix appears to be incomplete, and perhaps even
corrupted information. This paper considers the recovery of a low-rank matrix,
where some observed entries are sampled in a \emph{biased distribution}
suitably dependent on \emph{leverage scores} of a matrix, and some observed
entries are uniformly corrupted. Our theoretical findings show that we can
provably recover an unknown $n\times n$ matrix of rank $r$ from just about
$O(nr\log^2 n)$ entries even when the few observed entries are corrupted with a
small amount of noisy information. Empirical studies verify our theoretical
results.
</p>
<a href="http://arxiv.org/abs/2011.05885" target="_blank">arXiv:2011.05885</a> [<a href="http://arxiv.org/pdf/2011.05885" target="_blank">pdf</a>]

<h2>Transferred Fusion Learning using Skipped Networks. (arXiv:2011.05895v1 [cs.CV])</h2>
<h3>Vinayaka R Kamath, Vishal S, Varun M</h3>
<p>Identification of an entity that is of interest is prominent in any
intelligent system. The visual intelligence of the model is enhanced when the
capability of recognition is added. Several methods such as transfer learning
and zero shot learning help to reuse the existing models or augment the
existing model to achieve improved performance at the task of object
recognition. Transferred fusion learning is one such mechanism that intends to
use the best of both worlds and build a model that is capable of outperforming
the models involved in the system. We propose a novel mechanism to amplify the
process of transfer learning by introducing a student architecture where the
networks learn from each other.
</p>
<a href="http://arxiv.org/abs/2011.05895" target="_blank">arXiv:2011.05895</a> [<a href="http://arxiv.org/pdf/2011.05895" target="_blank">pdf</a>]

<h2>Age Gap Reducer-GAN for Recognizing Age-Separated Faces. (arXiv:2011.05897v1 [cs.CV])</h2>
<h3>Daksha Yadav, Naman Kohli, Mayank Vatsa, Richa Singh, Afzel Noore</h3>
<p>In this paper, we propose a novel algorithm for matching faces with temporal
variations caused due to age progression. The proposed generative adversarial
network algorithm is a unified framework that combines facial age estimation
and age-separated face verification. The key idea of this approach is to learn
the age variations across time by conditioning the input image on the subject's
gender and the target age group to which the face needs to be progressed. The
loss function accounts for reducing the age gap between the original image and
generated face image as well as preserving the identity. Both visual fidelity
and quantitative evaluations demonstrate the efficacy of the proposed
architecture on different facial age databases for age-separated face
recognition.
</p>
<a href="http://arxiv.org/abs/2011.05897" target="_blank">arXiv:2011.05897</a> [<a href="http://arxiv.org/pdf/2011.05897" target="_blank">pdf</a>]

<h2>A decision-making tool to fine-tune abnormal levels in the complete blood count tests. (arXiv:2011.05900v1 [stat.ML])</h2>
<h3>Marta Avalos-Fernandez, Helene Touchais, Marcela Henriquez-Henriquez</h3>
<p>The complete blood count (CBC) performed by automated hematology analyzers is
one of the most ordered laboratory tests. It is a first-line tool for assessing
a patient's general health status, or diagnosing and monitoring disease
progression. When the analysis does not fit an expected setting, technologists
manually review a blood smear using a microscope. The International Consensus
Group for Hematology Review published in 2005 a set of criteria for reviewing
CBCs. Commonly, adjustments are locally needed to account for laboratory
resources and populations characteristics. Our objective is to provide a
decision support tool to identify which CBC variables are associated with
higher risks of abnormal smear and at which cutoff values. We propose a
cost-sensitive Lasso-penalized additive logistic regression combined with
stability selection. Using simulated and real CBC data, we demonstrate that our
tool correctly identify the true cutoff values, provided that there is enough
available data in their neighbourhood.
</p>
<a href="http://arxiv.org/abs/2011.05900" target="_blank">arXiv:2011.05900</a> [<a href="http://arxiv.org/pdf/2011.05900" target="_blank">pdf</a>]

<h2>Hamiltonian Q-Learning: Leveraging Importance-sampling for Data Efficient RL. (arXiv:2011.05927v1 [cs.LG])</h2>
<h3>Udari Madhushani, Biswadip Dey, Naomi Ehrich Leonard, Amit Chakraborty</h3>
<p>Model-free reinforcement learning (RL), in particular Q-learning is widely
used to learn optimal policies for a variety of planning and control problems.
However, when the underlying state-transition dynamics are stochastic and
high-dimensional, Q-learning requires a large amount of data and incurs a
prohibitively high computational cost. In this paper, we introduce Hamiltonian
Q-Learning, a data efficient modification of the Q-learning approach, which
adopts an importance-sampling based technique for computing the Q function. To
exploit stochastic structure of the state-transition dynamics, we employ
Hamiltonian Monte Carlo to update Q function estimates by approximating the
expected future rewards using Q values associated with a subset of next states.
Further, to exploit the latent low-rank structure of the dynamic system,
Hamiltonian Q-Learning uses a matrix completion algorithm to reconstruct the
updated Q function from Q value updates over a much smaller subset of
state-action pairs. By providing an efficient way to apply Q-learning in
stochastic, high-dimensional problems, the proposed approach broadens the scope
of RL algorithms for real-world applications, including classical control tasks
and environmental monitoring.
</p>
<a href="http://arxiv.org/abs/2011.05927" target="_blank">arXiv:2011.05927</a> [<a href="http://arxiv.org/pdf/2011.05927" target="_blank">pdf</a>]

<h2>Empirical Risk Minimization in the Non-interactive Local Model of Differential Privacy. (arXiv:2011.05934v1 [cs.LG])</h2>
<h3>Di Wang, Marco Gaboardi, Adam Smith, Jinhui Xu</h3>
<p>In this paper, we study the Empirical Risk Minimization (ERM) problem in the
non-interactive Local Differential Privacy (LDP) model. Previous research on
this problem \citep{smith2017interaction} indicates that the sample complexity,
to achieve error $\alpha$, needs to be exponentially depending on the
dimensionality $p$ for general loss functions. In this paper, we make two
attempts to resolve this issue by investigating conditions on the loss
functions that allow us to remove such a limit. In our first attempt, we show
that if the loss function is $(\infty, T)$-smooth, by using the Bernstein
polynomial approximation we can avoid the exponential dependency in the term of
$\alpha$. We then propose player-efficient algorithms with $1$-bit
communication complexity and $O(1)$ computation cost for each player. The error
bound of these algorithms is asymptotically the same as the original one. With
some additional assumptions, we also give an algorithm which is more efficient
for the server. In our second attempt, we show that for any $1$-Lipschitz
generalized linear convex loss function, there is an $(\epsilon, \delta)$-LDP
algorithm whose sample complexity for achieving error $\alpha$ is only linear
in the dimensionality $p$. Our results use a polynomial of inner product
approximation technique. Finally, motivated by the idea of using polynomial
approximation and based on different types of polynomial approximations, we
propose (efficient) non-interactive locally differentially private algorithms
for learning the set of k-way marginal queries and the set of smooth queries.
</p>
<a href="http://arxiv.org/abs/2011.05934" target="_blank">arXiv:2011.05934</a> [<a href="http://arxiv.org/pdf/2011.05934" target="_blank">pdf</a>]

<h2>LittleYOLO-SPP: A Delicate Real-Time Vehicle Detection Algorithm. (arXiv:2011.05940v1 [cs.CV])</h2>
<h3>Sri Jamiya S, Esther Rani P</h3>
<p>Vehicle detection in real-time is a challenging and important task. The
existing real-time vehicle detection lacks accuracy and speed. Real-time
systems must detect and locate vehicles during criminal activities like theft
of vehicle and road traffic violations with high accuracy. Detection of
vehicles in complex scenes with occlusion is also extremely difficult. In this
study, a lightweight model of deep neural network LittleYOLO-SPP based on the
YOLOv3-tiny network is proposed to detect vehicles effectively in real-time.
The YOLOv3-tiny object detection network is improved by modifying its feature
extraction network to increase the speed and accuracy of vehicle detection. The
proposed network incorporated Spatial pyramid pooling into the network, which
consists of different scales of pooling layers for concatenation of features to
enhance network learning capability. The Mean square error (MSE) and
Generalized IoU (GIoU) loss function for bounding box regression is used to
increase the performance of the network. The network training includes
vehicle-based classes from PASCAL VOC 2007,2012 and MS COCO 2014 datasets such
as car, bus, and truck. LittleYOLO-SPP network detects the vehicle in real-time
with high accuracy regardless of video frame and weather conditions. The
improved network achieves a higher mAP of 77.44% on PASCAL VOC and 52.95% mAP
on MS COCO datasets.
</p>
<a href="http://arxiv.org/abs/2011.05940" target="_blank">arXiv:2011.05940</a> [<a href="http://arxiv.org/pdf/2011.05940" target="_blank">pdf</a>]

<h2>Asymptotically Optimal Information-Directed Sampling. (arXiv:2011.05944v1 [stat.ML])</h2>
<h3>Johannes Kirschner, Tor Lattimore, Claire Vernade, Csaba Szepesv&#xe1;ri</h3>
<p>We introduce a computationally efficient algorithm for finite stochastic
linear bandits. The approach is based on the frequentist information-directed
sampling (IDS) framework, with an information gain potential that is derived
directly from the asymptotic regret lower bound. We establish frequentist
regret bounds, which show that the proposed algorithm is both asymptotically
optimal and worst-case rate optimal in finite time. Our analysis sheds light on
how IDS trades off regret and information to incrementally solve the
semi-infinite concave program that defines the optimal asymptotic regret. Along
the way, we uncover interesting connections towards a recently proposed
two-player game approach and the Bayesian IDS algorithm.
</p>
<a href="http://arxiv.org/abs/2011.05944" target="_blank">arXiv:2011.05944</a> [<a href="http://arxiv.org/pdf/2011.05944" target="_blank">pdf</a>]

<h2>$(f,\Gamma)$-Divergences: Interpolating between $f$-Divergences and Integral Probability Metrics. (arXiv:2011.05953v1 [stat.ML])</h2>
<h3>Jeremiah Birrell, Paul Dupuis, Markos A. Katsoulakis, Yannis Pantazis, Luc Rey-Bellet</h3>
<p>We develop a general framework for constructing new information-theoretic
divergences that rigorously interpolate between $f$-divergences and integral
probability metrics (IPMs), such as the Wasserstein distance. These new
divergences inherit features from IPMs, such as the ability to compare
distributions which are not absolute continuous, as well as from
$f$-divergences, for instance the strict concavity of their variational
representations and the ability to compare heavy-tailed distributions. When
combined, these features establish a divergence with improved convergence and
estimation properties for statistical learning applications. We demonstrate
their use in the training of generative adversarial networks (GAN) for
heavy-tailed data and also show they can provide improved performance over
gradient-penalized Wasserstein GAN in image generation.
</p>
<a href="http://arxiv.org/abs/2011.05953" target="_blank">arXiv:2011.05953</a> [<a href="http://arxiv.org/pdf/2011.05953" target="_blank">pdf</a>]

<h2>Real-Time Decentralized knowledge Transfer at the Edge. (arXiv:2011.05961v1 [cs.LG])</h2>
<h3>Orpaz Goldstein, Mohammad Kachuee, Dereck Shiell, Majid Sarrafzadeh</h3>
<p>Proliferation of edge networks creates islands of learning agents working on
local streams of data. Transferring knowledge between these agents in real-time
without exposing private data allows for collaboration to decrease learning
time, and increase model confidence. Incorporating knowledge from data that was
not seen by a local model creates an ability to debias a local model, or add to
classification abilities on data never before seen. Transferring knowledge in a
decentralized approach allows for models to retain their local insights, in
turn allowing for local flavors of a machine learning model. This approach
suits the decentralized architecture of edge networks, as a local edge node
will serve a community of learning agents that will likely encounter similar
data. We propose a method based on knowledge distillation for pairwise
knowledge transfer pipelines, and compare to other popular knowledge transfer
methods. Additionally, we test different scenarios of knowledge transfer
network construction and show the practicality of our approach. Based on our
experiments we show knowledge transfer using our model outperforms common
methods in a real time transfer scenario.
</p>
<a href="http://arxiv.org/abs/2011.05961" target="_blank">arXiv:2011.05961</a> [<a href="http://arxiv.org/pdf/2011.05961" target="_blank">pdf</a>]

<h2>Transformers for One-Shot Visual Imitation. (arXiv:2011.05970v1 [cs.LG])</h2>
<h3>Sudeep Dasari, Abhinav Gupta</h3>
<p>Humans are able to seamlessly visually imitate others, by inferring their
intentions and using past experience to achieve the same end goal. In other
words, we can parse complex semantic knowledge from raw video and efficiently
translate that into concrete motor control. Is it possible to give a robot this
same capability? Prior research in robot imitation learning has created agents
which can acquire diverse skills from expert human operators. However,
expanding these techniques to work with a single positive example during test
time is still an open challenge. Apart from control, the difficulty stems from
mismatches between the demonstrator and robot domains. For example, objects may
be placed in different locations (e.g. kitchen layouts are different in every
house). Additionally, the demonstration may come from an agent with different
morphology and physical appearance (e.g. human), so one-to-one action
correspondences are not available. This paper investigates techniques which
allow robots to partially bridge these domain gaps, using their past
experience. A neural network is trained to mimic ground truth robot actions
given context video from another agent, and must generalize to unseen task
instances when prompted with new videos during test time. We hypothesize that
our policy representations must be both context driven and dynamics aware in
order to perform these tasks. These assumptions are baked into the neural
network using the Transformers attention mechanism and a self-supervised
inverse dynamics loss. Finally, we experimentally determine that our method
accomplishes a $\sim 2$x improvement in terms of task success rate over prior
baselines in a suite of one-shot manipulation tasks.
</p>
<a href="http://arxiv.org/abs/2011.05970" target="_blank">arXiv:2011.05970</a> [<a href="http://arxiv.org/pdf/2011.05970" target="_blank">pdf</a>]

<h2>GRCNN: Graph Recognition Convolutional Neural Network for Synthesizing Programs from Flow Charts. (arXiv:2011.05980v1 [cs.CV])</h2>
<h3>Lin Cheng, Zijiang Yang</h3>
<p>Program synthesis is the task to automatically generate programs based on
user specification. In this paper, we present a framework that synthesizes
programs from flow charts that serve as accurate and intuitive specifications.
In order doing so, we propose a deep neural network called GRCNN that
recognizes graph structure from its image. GRCNN is trained end-to-end, which
can predict edge and node information of the flow chart simultaneously.
Experiments show that the accuracy rate to synthesize a program is 66.4%, and
the accuracy rates to recognize edge and nodes are 94.1% and 67.9%,
respectively. On average, it takes about 60 milliseconds to synthesize a
program.
</p>
<a href="http://arxiv.org/abs/2011.05980" target="_blank">arXiv:2011.05980</a> [<a href="http://arxiv.org/pdf/2011.05980" target="_blank">pdf</a>]

<h2>A Benchmark and Evaluation of Non-Rigid Structure from Motion. (arXiv:1801.08388v3 [cs.CV] UPDATED)</h2>
<h3>Sebastian Hoppe Nesgaard Jensen, Mads Emil Brix Doest, Henrik Aanaes, Alessio Del Bue</h3>
<p>Non-Rigid structure from motion (NRSfM), is a long standing and central
problem in computer vision and its solution is necessary for obtaining 3D
information from multiple images when the scene is dynamic. A main issue
regarding the further development of this important computer vision topic, is
the lack of high quality data sets. We here address this issue by presenting a
data set created for this purpose, which is made publicly available, and
considerably larger than the previous state of the art. To validate the
applicability of this data set, and provide an investigation into the state of
the art of NRSfM, including potential directions forward, we here present a
benchmark and a scrupulous evaluation using this data set. This benchmark
evaluates 18 different methods with available code that reasonably spans the
state of the art in sparse NRSfM. This new public data set and evaluation
protocol will provide benchmark tools for further development in this
challenging field.
</p>
<a href="http://arxiv.org/abs/1801.08388" target="_blank">arXiv:1801.08388</a> [<a href="http://arxiv.org/pdf/1801.08388" target="_blank">pdf</a>]

<h2>Dynamically Sacrificing Accuracy for Reduced Computation: Cascaded Inference Based on Softmax Confidence. (arXiv:1805.10982v2 [cs.LG] UPDATED)</h2>
<h3>Konstantin Berestizshevsky, Guy Even</h3>
<p>We study the tradeoff between computational effort and classification
accuracy in a cascade of deep neural networks. During inference, the user sets
the acceptable accuracy degradation which then automatically determines
confidence thresholds for the intermediate classifiers. As soon as the
confidence threshold is met, inference terminates immediately without having to
compute the output of the complete network. Confidence levels are derived
directly from the softmax outputs of intermediate classifiers, as we do not
train special decision functions. We show that using a softmax output as a
confidence measure in a cascade of deep neural networks leads to a reduction of
15%-50% in the number of MAC operations while degrading the classification
accuracy by roughly 1%. Our method can be easily incorporated into pre-trained
non-cascaded architectures, as we exemplify on ResNet. Our main contribution is
a method that dynamically adjusts the tradeoff between accuracy and computation
without retraining the model.
</p>
<a href="http://arxiv.org/abs/1805.10982" target="_blank">arXiv:1805.10982</a> [<a href="http://arxiv.org/pdf/1805.10982" target="_blank">pdf</a>]

<h2>Analysis of the Generalization Error: Empirical Risk Minimization over Deep Artificial Neural Networks Overcomes the Curse of Dimensionality in the Numerical Approximation of Black-Scholes Partial Differential Equations. (arXiv:1809.03062v3 [cs.LG] UPDATED)</h2>
<h3>Julius Berner, Philipp Grohs, Arnulf Jentzen</h3>
<p>The development of new classification and regression algorithms based on
empirical risk minimization (ERM) over deep neural network hypothesis classes,
coined deep learning, revolutionized the area of artificial intelligence,
machine learning, and data analysis. In particular, these methods have been
applied to the numerical solution of high-dimensional partial differential
equations with great success. Recent simulations indicate that deep
learning-based algorithms are capable of overcoming the curse of dimensionality
for the numerical solution of Kolmogorov equations, which are widely used in
models from engineering, finance, and the natural sciences. The present paper
considers under which conditions ERM over a deep neural network hypothesis
class approximates the solution of a $d$-dimensional Kolmogorov equation with
affine drift and diffusion coefficients and typical initial values arising from
problems in computational finance up to error $\varepsilon$. We establish that,
with high probability over draws of training samples, such an approximation can
be achieved with both the size of the hypothesis class and the number of
training samples scaling only polynomially in $d$ and $\varepsilon^{-1}$. It
can be concluded that ERM over deep neural network hypothesis classes overcomes
the curse of dimensionality for the numerical solution of linear Kolmogorov
equations with affine coefficients.
</p>
<a href="http://arxiv.org/abs/1809.03062" target="_blank">arXiv:1809.03062</a> [<a href="http://arxiv.org/pdf/1809.03062" target="_blank">pdf</a>]

<h2>Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning. (arXiv:1811.12808v3 [cs.LG] UPDATED)</h2>
<h3>Sebastian Raschka</h3>
<p>The correct use of model evaluation, model selection, and algorithm selection
techniques is vital in academic machine learning research as well as in many
industrial settings. This article reviews different techniques that can be used
for each of these three subtasks and discusses the main advantages and
disadvantages of each technique with references to theoretical and empirical
studies. Further, recommendations are given to encourage best yet feasible
practices in research and applications of machine learning. Common methods such
as the holdout method for model evaluation and selection are covered, which are
not recommended when working with small datasets. Different flavors of the
bootstrap technique are introduced for estimating the uncertainty of
performance estimates, as an alternative to confidence intervals via normal
approximation if bootstrapping is computationally feasible. Common
cross-validation techniques such as leave-one-out cross-validation and k-fold
cross-validation are reviewed, the bias-variance trade-off for choosing k is
discussed, and practical tips for the optimal choice of k are given based on
empirical evidence. Different statistical tests for algorithm comparisons are
presented, and strategies for dealing with multiple comparisons such as omnibus
tests and multiple-comparison corrections are discussed. Finally, alternative
methods for algorithm selection, such as the combined F-test 5x2
cross-validation and nested cross-validation, are recommended for comparing
machine learning algorithms when datasets are small.
</p>
<a href="http://arxiv.org/abs/1811.12808" target="_blank">arXiv:1811.12808</a> [<a href="http://arxiv.org/pdf/1811.12808" target="_blank">pdf</a>]

<h2>Efficient Attention: Attention with Linear Complexities. (arXiv:1812.01243v9 [cs.CV] UPDATED)</h2>
<h3>Zhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi, Hongsheng Li</h3>
<p>Dot-product attention has wide applications in computer vision and natural
language processing. However, its memory and computational costs grow
quadratically with the input size. Such growth prohibits its application on
high-resolution inputs. To remedy this drawback, this paper proposes a novel
efficient attention mechanism equivalent to dot-product attention but with
substantially less memory and computational costs. Its resource efficiency
allows more widespread and flexible integration of attention modules into a
network, which leads to better accuracies. Empirical evaluations demonstrated
the effectiveness of its advantages. Efficient attention modules brought
significant performance boosts to object detectors and instance segmenters on
MS-COCO 2017. Further, the resource efficiency democratizes attention to
complex models, where high costs prohibit the use of dot-product attention. As
an exemplar, a model with efficient attention achieved state-of-the-art
accuracies for stereo depth estimation on the Scene Flow dataset. Code is
available at https://github.com/cmsflash/efficient-attention.
</p>
<a href="http://arxiv.org/abs/1812.01243" target="_blank">arXiv:1812.01243</a> [<a href="http://arxiv.org/pdf/1812.01243" target="_blank">pdf</a>]

<h2>Simultaneous lesion and neuroanatomy segmentation in Multiple Sclerosis using deep neural networks. (arXiv:1901.07419v3 [cs.CV] UPDATED)</h2>
<h3>Richard McKinley, Rik Wepfer, Fabian Aschwanden, Lorenz Grunder, Raphaela Muri, Christian Rummel, Rajeev Verma, Christian Weisstanner, Mauricio Reyes, Anke Salmen, Andrew Chan, Franca Wagner, Roland Wiest</h3>
<p>Segmentation of white matter lesions and deep grey matter structures is an
important task in the quantification of magnetic resonance imaging in multiple
sclerosis. In this paper we explore segmentation solutions based on
convolutional neural networks (CNNs) for providing fast, reliable segmentations
of lesions and grey-matter structures in multi-modal MR imaging, and the
performance of these methods when applied to out-of-centre data.

We trained two state-of-the-art fully convolutional CNN architectures on the
2016 MSSEG training dataset, which was annotated by seven independent human
raters: a reference implementation of a 3D Unet, and a more recently proposed
3D-to-2D architecture (DeepSCAN). We then retrained those methods on a larger
dataset from a single centre, with and without labels for other brain
structures. We quantified changes in performance owing to dataset shift, and
changes in performance by adding the additional brain-structure labels. We also
compared performance with freely available reference methods.

Both fully-convolutional CNN methods substantially outperform other
approaches in the literature when trained and evaluated in cross-validation on
the MSSEG dataset, showing agreement with human raters in the range of human
inter-rater variability. Both architectures showed drops in performance when
trained on single-centre data and tested on the MSSEG dataset. When trained
with the addition of weak anatomical labels derived from Freesurfer, the
performance of the 3D Unet degraded, while the performance of the DeepSCAN net
improved. Overall, the DeepSCAN network predicting both lesion and anatomical
labels was the best-performing network examined.
</p>
<a href="http://arxiv.org/abs/1901.07419" target="_blank">arXiv:1901.07419</a> [<a href="http://arxiv.org/pdf/1901.07419" target="_blank">pdf</a>]

<h2>Improving performance of deep learning models with axiomatic attribution priors and expected gradients. (arXiv:1906.10670v2 [cs.LG] UPDATED)</h2>
<h3>Gabriel Erion, Joseph D. Janizek, Pascal Sturmfels, Scott Lundberg, Su-In Lee</h3>
<p>Recent research has demonstrated that feature attribution methods for deep
networks can themselves be incorporated into training; these attribution priors
optimize for a model whose attributions have certain desirable properties --
most frequently, that particular features are important or unimportant. These
attribution priors are often based on attribution methods that are not
guaranteed to satisfy desirable interpretability axioms, such as completeness
and implementation invariance. Here, we introduce attribution priors to
optimize for higher-level properties of explanations, such as smoothness and
sparsity, enabled by a fast new attribution method formulation called expected
gradients that satisfies many important interpretability axioms. This improves
model performance on many real-world tasks where previous attribution priors
fail. Our experiments show that the gains from combining higher-level
attribution priors with expected gradients attributions are consistent across
image, gene expression, and health care data sets. We believe this work
motivates and provides the necessary tools to support the widespread adoption
of axiomatic attribution priors in many areas of applied machine learning. The
implementations and our results have been made freely available to academic
communities.
</p>
<a href="http://arxiv.org/abs/1906.10670" target="_blank">arXiv:1906.10670</a> [<a href="http://arxiv.org/pdf/1906.10670" target="_blank">pdf</a>]

<h2>Adaptive Sequential Experiments with Unknown Information Arrival Processes. (arXiv:1907.00107v4 [cs.LG] UPDATED)</h2>
<h3>Yonatan Gur, Ahmadreza Momeni</h3>
<p>Sequential experiments are often characterized by an exploration-exploitation
tradeoff that is captured by the multi-armed bandit (MAB) framework. This
framework has been studied and applied, typically when at each time epoch
feedback is received only on the action that was selected at that epoch.
However, in many practical settings additional information may become available
between decision steps. We introduce a generalized MAB formulation, which
considers a broad class of distributions that are informative about mean
rewards, and allows observations from these distributions to arrive at
arbitrary and a priori unknown times. We characterize the minimax complexity of
this family of problems as a function of the information arrival process, and
identify how salient characteristics of this process impact policy design and
achievable performance. We establish that: (i) upper confidence bound and
posterior sampling policies possess natural robustness with respect to the
information arrival process without any adjustments, which uncovers a novel
property of these popular families of policies and further lends credence to
their appeal; and (ii) policies with exogenous exploration rate do not possess
such robustness. For such policies, we devise a novel virtual time indices
method for dynamically controlling the effective exploration rate to attain the
best performance that is achievable when the information arrival process is a
priori known. When the relation of auxiliary data to rewards is unknown, we
characterize necessary and sufficient conditions under which auxiliary
information still allows performance improvement, and devise new policies based
on upper confidence bound that uniformly guarantee rate optimality. We use data
from a large media site to analyze the value that may be captured in practice
by leveraging auxiliary information for designing content recommendations.
</p>
<a href="http://arxiv.org/abs/1907.00107" target="_blank">arXiv:1907.00107</a> [<a href="http://arxiv.org/pdf/1907.00107" target="_blank">pdf</a>]

<h2>Learning GPLVM with arbitrary kernels using the unscented transformation. (arXiv:1907.01867v2 [stat.ML] UPDATED)</h2>
<h3>Daniel Augusto R. M. A. de Souza, Diego Mesquita, C&#xe9;sar Lincoln C. Mattos, Jo&#xe3;o Paulo P. Gomes</h3>
<p>Gaussian Process Latent Variable Model (GPLVM) is a flexible framework to
handle uncertain inputs in Gaussian Processes (GPs) and incorporate GPs as
components of larger graphical models. Nonetheless, the standard GPLVM
variational inference approach is tractable only for a narrow family of kernel
functions. The most popular implementations of GPLVM circumvent this limitation
using quadrature methods, which may become a computational bottleneck even for
relatively low dimensions. For instance, the widely employed Gauss-Hermite
quadrature has exponential complexity on the number of dimensions. In this
work, we propose using the unscented transformation instead. Overall, this
method presents comparable, if not better, performance than offthe-shelf
solutions to GPLVM and its computational complexity scales only linearly on
dimension. In contrast to Monte Carlo methods, our approach is deterministic
and works well with quasi-Newton methods, such as the
Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm. We illustrate the
applicability of our method with experiments on dimensionality reduction and
multistep-ahead prediction with uncertainty propagation.
</p>
<a href="http://arxiv.org/abs/1907.01867" target="_blank">arXiv:1907.01867</a> [<a href="http://arxiv.org/pdf/1907.01867" target="_blank">pdf</a>]

<h2>Decentralized Deep Learning with Arbitrary Communication Compression. (arXiv:1907.09356v3 [cs.LG] UPDATED)</h2>
<h3>Anastasia Koloskova, Tao Lin, Sebastian U. Stich, Martin Jaggi</h3>
<p>Decentralized training of deep learning models is a key element for enabling
data privacy and on-device learning over networks, as well as for efficient
scaling to large compute clusters. As current approaches suffer from limited
bandwidth of the network, we propose the use of communication compression in
the decentralized training context. We show that Choco-SGD $-$ recently
introduced and analyzed for strongly-convex objectives only $-$ converges under
arbitrary high compression ratio on general non-convex functions at the rate
$O\bigl(1/\sqrt{nT}\bigr)$ where $T$ denotes the number of iterations and $n$
the number of workers. The algorithm achieves linear speedup in the number of
workers and supports higher compression than previous state-of-the art methods.
We demonstrate the practical performance of the algorithm in two key scenarios:
the training of deep learning models (i) over distributed user devices,
connected by a social network and (ii) in a datacenter (outperforming
all-reduce time-wise).
</p>
<a href="http://arxiv.org/abs/1907.09356" target="_blank">arXiv:1907.09356</a> [<a href="http://arxiv.org/pdf/1907.09356" target="_blank">pdf</a>]

<h2>A Domain-Knowledge-Aided Deep Reinforcement Learning Approach for Flight Control Design. (arXiv:1908.06884v2 [cs.AI] UPDATED)</h2>
<h3>Hyo-Sang Shin, Shaoming He, Antonios Tsourdos</h3>
<p>This paper aims to examine the potential of using the emerging deep
reinforcement learning techniques in flight control. Instead of learning from
scratch, we suggest to leverage domain knowledge available in learning to
improve learning efficiency and generalisability. More specifically, the
proposed approach fixes the autopilot structure as typical three-loop autopilot
and deep reinforcement learning is utilised to learn the autopilot gains. To
solve the flight control problem, we then formulate a Markovian decision
process with a proper reward function that enable the application of
reinforcement learning theory. Another type of domain knowledge is exploited
for defining the reward function, by shaping reference inputs in consideration
of important control objectives and using the shaped reference inputs in the
reward function. The state-of-the-art deep deterministic policy gradient
algorithm is utilised to learn an action policy that maps the observed states
to the autopilot gains. Extensive empirical numerical simulations are performed
to validate the proposed computational control algorithm.
</p>
<a href="http://arxiv.org/abs/1908.06884" target="_blank">arXiv:1908.06884</a> [<a href="http://arxiv.org/pdf/1908.06884" target="_blank">pdf</a>]

<h2>NL-LinkNet: Toward Lighter but More Accurate Road Extraction with Non-Local Operations. (arXiv:1908.08223v3 [cs.LG] UPDATED)</h2>
<h3>Yooseung Wang, Junghoon Seo, Taegyun Jeon</h3>
<p>Road extraction from very high resolution satellite (VHR) images is one of
the most important topics in the field of remote sensing. In this paper, we
propose an efficient Non-Local LinkNet with non-local blocks that can grasp
relations between global features. This enables each spatial feature point to
refer to all other contextual information and results in more accurate road
segmentation. In detail, our single model without any post-processing like CRF
refinement, performed better than any other published state-of-the-art ensemble
model in the official DeepGlobe Challenge. Moreover, our NL-LinkNet beat the
D-LinkNet, the winner of the DeepGlobe challenge, with 43 \% less parameters,
less giga floating-point operations per seconds (GFLOPs) and shorter training
convergence time. We also present empirical analyses on the proper usages of
non-local blocks for the baseline model.
</p>
<a href="http://arxiv.org/abs/1908.08223" target="_blank">arXiv:1908.08223</a> [<a href="http://arxiv.org/pdf/1908.08223" target="_blank">pdf</a>]

<h2>Goodness-of-fit tests on manifolds. (arXiv:1909.05229v2 [stat.ML] UPDATED)</h2>
<h3>Alexander Shapiro, Yao Xie, Rui Zhang</h3>
<p>We develop a general theory for the goodness-of-fit test to non-linear
models. In particular, we assume that the observations are noisy samples of a
submanifold defined by a \yao{sufficiently smooth non-linear map}. The
observation noise is additive Gaussian. Our main result shows that the
"residual" of the model fit, by solving a non-linear least-square problem,
follows a (possibly noncentral) $\chi^2$ distribution. The parameters of the
$\chi^2$ distribution are related to the model order and dimension of the
problem. We further present a method to select the model orders sequentially.
We demonstrate the broad application of the general theory in machine learning
and signal processing, including determining the rank of low-rank (possibly
complex-valued) matrices and tensors from noisy, partial, or indirect
observations, determining the number of sources in signal demixing, and
potential applications in determining the number of hidden nodes in neural
networks.
</p>
<a href="http://arxiv.org/abs/1909.05229" target="_blank">arXiv:1909.05229</a> [<a href="http://arxiv.org/pdf/1909.05229" target="_blank">pdf</a>]

<h2>Multi-agent Interactive Prediction under Challenging Driving Scenarios. (arXiv:1909.10737v4 [cs.RO] UPDATED)</h2>
<h3>Weihao Xuan, Ruijie Ren</h3>
<p>In order to drive safely on the road, autonomous vehicle is expected to
predict future outcomes of its surrounding environment and react properly. In
fact, many researchers have been focused on solving behavioral prediction
problems for autonomous vehicles. However, very few of them consider
multi-agent prediction under challenging driving scenarios such as urban
environment. In this paper, we proposed a prediction method that is able to
predict various complicated driving scenarios where heterogeneous road
entities, signal lights, and static map information are taken into account.
Moreover, the proposed multi-agent interactive prediction (MAIP) system is
capable of simultaneously predicting any number of road entities while
considering their mutual interactions. A case study of a simulated challenging
urban intersection scenario is provided to demonstrate the performance and
capability of the proposed prediction system.
</p>
<a href="http://arxiv.org/abs/1909.10737" target="_blank">arXiv:1909.10737</a> [<a href="http://arxiv.org/pdf/1909.10737" target="_blank">pdf</a>]

<h2>Robust Training with Ensemble Consensus. (arXiv:1910.09792v3 [cs.LG] UPDATED)</h2>
<h3>Jisoo Lee, Sae-Young Chung</h3>
<p>Since deep neural networks are over-parameterized, they can memorize noisy
examples. We address such a memorization issue in the presence of label noise.
From the fact that deep neural networks cannot generalize to neighborhoods of
memorized features, we hypothesize that noisy examples do not consistently
incur small losses on the network under a certain perturbation. Based on this,
we propose a novel training method called Learning with Ensemble Consensus
(LEC) that prevents overfitting to noisy examples by removing them based on the
consensus of an ensemble of perturbed networks. One of the proposed LECs, LTEC
outperforms the current state-of-the-art methods on noisy MNIST, CIFAR-10, and
CIFAR-100 in an efficient manner.
</p>
<a href="http://arxiv.org/abs/1910.09792" target="_blank">arXiv:1910.09792</a> [<a href="http://arxiv.org/pdf/1910.09792" target="_blank">pdf</a>]

<h2>Deterministic tensor completion with hypergraph expanders. (arXiv:1910.10692v2 [stat.ML] UPDATED)</h2>
<h3>Kameron Decker Harris, Yizhe Zhu</h3>
<p>We provide a novel analysis of low-rank tensor completion based on hypergraph
expanders. As a proxy for rank, we minimize the max-quasinorm of the tensor,
which generalizes the max-norm for matrices. Our analysis is deterministic and
shows that the number of samples required to approximately recover an order-$t$
tensor with at most $n$ entries per dimension is linear in $n$, under the
assumption that the rank and order of the tensor are $O(1)$. As steps in our
proof, we find an improved expander mixing lemma for a $t$-partite, $t$-uniform
regular hypergraph model, and prove several new properties about tensor
max-quasinorm. To the best of our knowledge, this is the first deterministic
analysis of tensor completion. We develop a practical algorithm that solves a
relaxed version of the max-quasinorm minimization problem, and we demonstrate
its efficacy with numerical experiments.
</p>
<a href="http://arxiv.org/abs/1910.10692" target="_blank">arXiv:1910.10692</a> [<a href="http://arxiv.org/pdf/1910.10692" target="_blank">pdf</a>]

<h2>On the convergence of projective-simulation-based reinforcement learning in Markov decision processes. (arXiv:1910.11914v2 [cs.LG] UPDATED)</h2>
<h3>Jens Clausen, Walter L. Boyajian, Lea M. Trenkwalder, Vedran Dunjko, Hans J. Briegel</h3>
<p>In recent years, the interest in leveraging quantum effects for enhancing
machine learning tasks has significantly increased. Many algorithms speeding up
supervised and unsupervised learning were established. The first framework in
which ways to exploit quantum resources specifically for the broader context of
reinforcement learning were found is projective simulation. Projective
simulation presents an agent-based reinforcement learning approach designed in
a manner which may support quantum walk-based speed-ups. Although classical
variants of projective simulation have been benchmarked against common
reinforcement learning algorithms, very few formal theoretical analyses have
been provided for its performance in standard learning scenarios. In this
paper, we provide a detailed formal discussion of the properties of this model.
Specifically, we prove that one version of the projective simulation model,
understood as a reinforcement learning approach, converges to optimal behavior
in a large class of Markov decision processes. This proof shows that a
physically-inspired approach to reinforcement learning can guarantee to
converge.
</p>
<a href="http://arxiv.org/abs/1910.11914" target="_blank">arXiv:1910.11914</a> [<a href="http://arxiv.org/pdf/1910.11914" target="_blank">pdf</a>]

<h2>Insights into Ordinal Embedding Algorithms: A Systematic Evaluation. (arXiv:1912.01666v5 [cs.LG] UPDATED)</h2>
<h3>Leena Chennuru Vankadara, Siavash Haghiri, Michael Lohaus, Faiz Ul Wahab, Ulrike von Luxburg</h3>
<p>The objective of ordinal embedding is to find a Euclidean representation of a
set of abstract items, using only answers to triplet comparisons of the form
"Is item $i$ closer to the item $j$ or item $k$?". In recent years, numerous
algorithms have been proposed to solve this problem. However, there does not
exist a fair and thorough assessment of these embedding methods and therefore
several key questions remain unanswered: Which algorithms scale better with
increasing sample size or dimension? Which ones perform better when the
embedding dimension is small or few triplet comparisons are available? In our
paper, we address these questions and provide the first comprehensive and
systematic empirical evaluation of existing algorithms as well as a new neural
network approach. In the large triplet regime, we find that simple, relatively
unknown, non-convex methods consistently outperform all other algorithms,
including elaborate approaches based on neural networks or landmark approaches.
This finding can be explained by our insight that many of the non-convex
optimization approaches do not suffer from local optima. In the low triplet
regime, our neural network approach is either competitive or significantly
outperforms all the other methods. Our comprehensive assessment is enabled by
our unified library of popular embedding algorithms that leverages GPU
resources and allows for fast and accurate embeddings of millions of data
points.
</p>
<a href="http://arxiv.org/abs/1912.01666" target="_blank">arXiv:1912.01666</a> [<a href="http://arxiv.org/pdf/1912.01666" target="_blank">pdf</a>]

<h2>ScanRefer: 3D Object Localization in RGB-D Scans using Natural Language. (arXiv:1912.08830v3 [cs.CV] UPDATED)</h2>
<h3>Dave Zhenyu Chen, Angel X. Chang, Matthias Nie&#xdf;ner</h3>
<p>We introduce the task of 3D object localization in RGB-D scans using natural
language descriptions. As input, we assume a point cloud of a scanned 3D scene
along with a free-form description of a specified target object. To address
this task, we propose ScanRefer, learning a fused descriptor from 3D object
proposals and encoded sentence embeddings. This fused descriptor correlates
language expressions with geometric features, enabling regression of the 3D
bounding box of a target object. We also introduce the ScanRefer dataset,
containing 51,583 descriptions of 11,046 objects from 800 ScanNet scenes.
ScanRefer is the first large-scale effort to perform object localization via
natural language expression directly in 3D.
</p>
<a href="http://arxiv.org/abs/1912.08830" target="_blank">arXiv:1912.08830</a> [<a href="http://arxiv.org/pdf/1912.08830" target="_blank">pdf</a>]

<h2>A Model Predictive Approach for Online Mobile Manipulation of Nonholonomic Objects using Learned Dynamics. (arXiv:1912.09565v3 [cs.RO] UPDATED)</h2>
<h3>Roya Sabbagh Novin, Amir Yazdani, Andrew Merryweather, Tucker Hermans</h3>
<p>A particular type of assistive robots designed for physical interaction with
objects could play an important role assisting with mobility and fall
prevention in healthcare facilities. Autonomous mobile manipulation presents a
hurdle prior to safely using robots in real life applications. In this article,
we introduce a mobile manipulation framework based on model predictive control
using learned dynamics models of objects. We focus on the specific problem of
manipulating legged objects such as those commonly found in healthcare
environments and personal dwellings (e.g. walkers, tables, chairs). We describe
a probabilistic method for autonomous learning of an approximate dynamics model
for these objects. In this method, we learn dynamic parameters using a small
dataset consisting of force and motion data from interactions between the robot
and object. Moreover, we account for multiple manipulation strategies by
formulating the manipulation planning as a mixed-integer convex optimization.
The proposed framework considers the hybrid control system comprised of i)
choosing which leg to grasp, and ii) control of continuous applied forces for
manipulation. We formalize our algorithm based on model predictive control to
compensate for modeling errors and find an optimal path to manipulate the
object from one configuration to another. We show results for several objects
with various wheel configurations. Simulation and physical experiments show
that the obtained dynamics models are sufficiently accurate for safe and
collision-free manipulation. When combined with the proposed manipulation
planning algorithm, the robot successfully moves the object to a desired pose
while avoiding collision.
</p>
<a href="http://arxiv.org/abs/1912.09565" target="_blank">arXiv:1912.09565</a> [<a href="http://arxiv.org/pdf/1912.09565" target="_blank">pdf</a>]

<h2>A comparison of Vector Symbolic Architectures. (arXiv:2001.11797v3 [cs.AI] UPDATED)</h2>
<h3>Kenny Schlegel, Peer Neubert, Peter Protzel</h3>
<p>Vector Symbolic Architectures combine a high-dimensional vector space with a
set of carefully designed operators in order to perform symbolic computations
with large numerical vectors. Major goals are the exploitation of their
representational power and ability to deal with fuzziness and ambiguity. Over
the past years, several VSA implementations have been proposed. The available
implementations differ in the underlying vector space and the particular
implementations of the required VSA operators - with important ramifications
for the properties of these architectures. For example, not every VSA is
equally well suited to address each task, including complete incompatibility.
This paper provides an overview of eleven available VSA implementations and
discusses their commonalities and differences in the underlying vector space,
bundling, and binding/unbinding operations. We create a taxonomy of available
binding/unbinding operations and show an important ramification for non
self-inverse binding operations using an example from analogical reasoning. A
main contribution is the experimental comparison of the available
implementations in order to evaluate (1) the capacity of bundles, (2) the
approximation quality of non-exact unbinding operations, (3) the influence of
combining binding and bundling operations on the query answering performance,
and (4) the performance on two example applications: visual place and language
recognition. An overall good performance is shown by the HRR VSA in the
frequency domain. However, its non-self-inverse binding mechanism can
negatively influence its applicability, e.g. to analogical reasoning. We expect
this systematization and comparison to be relevant for development and
evaluation of new VSAs, but most importantly, to support the selection of an
appropriate VSA for a particular task. The implementations are available in
form of a MATLAB toolbox.
</p>
<a href="http://arxiv.org/abs/2001.11797" target="_blank">arXiv:2001.11797</a> [<a href="http://arxiv.org/pdf/2001.11797" target="_blank">pdf</a>]

<h2>Extrapolation Towards Imaginary $0$-Nearest Neighbour and Its Improved Convergence Rate. (arXiv:2002.03054v2 [stat.ML] UPDATED)</h2>
<h3>Akifumi Okuno, Hidetoshi Shimodaira</h3>
<p>$k$-nearest neighbour ($k$-NN) is one of the simplest and most widely-used
methods for supervised classification, that predicts a query's label by taking
weighted ratio of observed labels of $k$ objects nearest to the query. The
weights and the parameter $k \in \mathbb{N}$ regulate its bias-variance
trade-off, and the trade-off implicitly affects the convergence rate of the
excess risk for the $k$-NN classifier; several existing studies considered
selecting optimal $k$ and weights to obtain faster convergence rate. Whereas
$k$-NN with non-negative weights has been developed widely, it was also proved
that negative weights are essential for eradicating the bias terms and
attaining optimal convergence rate. In this paper, we propose a novel
multiscale $k$-NN (MS-$k$-NN), that extrapolates unweighted $k$-NN estimators
from several $k \ge 1$ values to $k=0$, thus giving an imaginary 0-NN
estimator. Our method implicitly computes optimal real-valued weights that are
adaptive to the query and its neighbour points. We theoretically prove that the
MS-$k$-NN attains the improved rate, which coincides with the existing optimal
rate under some conditions.
</p>
<a href="http://arxiv.org/abs/2002.03054" target="_blank">arXiv:2002.03054</a> [<a href="http://arxiv.org/pdf/2002.03054" target="_blank">pdf</a>]

<h2>Deep Domain Adaptive Object Detection: a Survey. (arXiv:2002.06797v3 [cs.CV] UPDATED)</h2>
<h3>Wanyi Li, Fuyu Li, Yongkang Luo, Peng Wang, Jia sun</h3>
<p>Deep learning (DL) based object detection has achieved great progress. These
methods typically assume that large amount of labeled training data is
available, and training and test data are drawn from an identical distribution.
However, the two assumptions are not always hold in practice. Deep domain
adaptive object detection (DDAOD) has emerged as a new learning paradigm to
address the above mentioned challenges. This paper aims to review the
state-of-the-art progress on deep domain adaptive object detection approaches.
Firstly, we introduce briefly the basic concepts of deep domain adaptation.
Secondly, the deep domain adaptive detectors are classified into five
categories and detailed descriptions of representative methods in each category
are provided. Finally, insights for future research trend are presented.
</p>
<a href="http://arxiv.org/abs/2002.06797" target="_blank">arXiv:2002.06797</a> [<a href="http://arxiv.org/pdf/2002.06797" target="_blank">pdf</a>]

<h2>DASNet: Dual attentive fully convolutional siamese networks for change detection of high resolution satellite images. (arXiv:2003.03608v2 [cs.CV] UPDATED)</h2>
<h3>Jie Chen, Ziyang Yuan, Jian Peng, Li Chen, Haozhe Huang, Jiawei Zhu, Yu Liu, Haifeng Li</h3>
<p>Change detection is a basic task of remote sensing image processing. The
research objective is to identity the change information of interest and filter
out the irrelevant change information as interference factors. Recently, the
rise of deep learning has provided new tools for change detection, which have
yielded impressive results. However, the available methods focus mainly on the
difference information between multitemporal remote sensing images and lack
robustness to pseudo-change information. To overcome the lack of resistance of
current methods to pseudo-changes, in this paper, we propose a new method,
namely, dual attentive fully convolutional Siamese networks (DASNet) for change
detection in high-resolution images. Through the dual-attention mechanism,
long-range dependencies are captured to obtain more discriminant feature
representations to enhance the recognition performance of the model. Moreover,
the imbalanced sample is a serious problem in change detection, i.e. unchanged
samples are much more than changed samples, which is one of the main reasons
resulting in pseudo-changes. We put forward the weighted double margin
contrastive loss to address this problem by punishing the attention to
unchanged feature pairs and increase attention to changed feature pairs. The
experimental results of our method on the change detection dataset (CDD) and
the building change detection dataset (BCDD) demonstrate that compared with
other baseline methods, the proposed method realizes maximum improvements of
2.1\% and 3.6\%, respectively, in the F1 score. Our Pytorch implementation is
available at https://github.com/lehaifeng/DASNet.
</p>
<a href="http://arxiv.org/abs/2003.03608" target="_blank">arXiv:2003.03608</a> [<a href="http://arxiv.org/pdf/2003.03608" target="_blank">pdf</a>]

<h2>3D Object Detection from a Single Fisheye Image Without a Single Fisheye Training Image. (arXiv:2003.03759v2 [cs.CV] UPDATED)</h2>
<h3>Elad Plaut, Erez Ben Yaacov, Bat El Shlomo</h3>
<p>Existing monocular 3D object detection methods have been demonstrated on
rectilinear perspective images and fail in images with alternative projections
such as those acquired by fisheye cameras. Previous works on object detection
in fisheye images have focused on 2D object detection, partly due to the lack
of 3D datasets of such images. In this work, we show how to use existing
monocular 3D object detection models, trained only on rectilinear images, to
detect 3D objects in images from fisheye cameras, without using any fisheye
training data. We outperform the only existing method for monocular 3D object
detection in panoramas on a benchmark of synthetic data, despite the fact that
the existing method trains on the target non-rectilinear projection whereas we
train only on rectilinear images. We also experiment with an internal dataset
of real fisheye images.
</p>
<a href="http://arxiv.org/abs/2003.03759" target="_blank">arXiv:2003.03759</a> [<a href="http://arxiv.org/pdf/2003.03759" target="_blank">pdf</a>]

<h2>An Adversarial Objective for Scalable Exploration. (arXiv:2003.06082v4 [cs.RO] UPDATED)</h2>
<h3>Bernadette Bucher, Karl Schmeckpeper, Nikolai Matni, Kostas Daniilidis</h3>
<p>Model-based curiosity combines active learning approaches to optimal sampling
with the information gain based incentives for exploration presented in the
curiosity literature. Existing model-based curiosity methods look to
approximate prediction uncertainty with approaches which struggle to scale to
many prediction-planning pipelines used in robotics tasks. We address these
scalability issues with an adversarial curiosity method minimizing a score
given by a discriminator network. This discriminator is optimized jointly with
a prediction model and enables our active learning approach to sample sequences
of observations and actions which result in predictions considered the least
realistic by the discriminator. We demonstrate progressively increasing
advantages as compute is restricted of our adversarial curiosity approach over
leading model-based exploration strategies in simulated environments. We
further demonstrate the ability of our adversarial curiosity method to scale to
a robotic manipulation prediction-planning pipeline where we improve sample
efficiency and prediction performance for a domain transfer problem.
</p>
<a href="http://arxiv.org/abs/2003.06082" target="_blank">arXiv:2003.06082</a> [<a href="http://arxiv.org/pdf/2003.06082" target="_blank">pdf</a>]

<h2>Self-Supervised Discovering of Interpretable Features for Reinforcement Learning. (arXiv:2003.07069v3 [cs.CV] UPDATED)</h2>
<h3>Wenjie Shi, Gao Huang, Shiji Song, Zhuoyuan Wang, Tingyu Lin, Cheng Wu</h3>
<p>Deep reinforcement learning (RL) has recently led to many breakthroughs on a
range of complex control tasks. However, the agent's decision-making process is
generally not transparent. The lack of interpretability hinders the
applicability of RL in safety-critical scenarios. While several methods have
attempted to interpret vision-based RL, most come without detailed explanation
for the agent's behavior. In this paper, we propose a self-supervised
interpretable framework, which can discover interpretable features to enable
easy understanding of RL agents even for non-experts. Specifically, a
self-supervised interpretable network (SSINet) is employed to produce
fine-grained attention masks for highlighting task-relevant information, which
constitutes most evidence for the agent's decisions. We verify and evaluate our
method on several Atari 2600 games as well as Duckietown, which is a
challenging self-driving car simulator environment. The results show that our
method renders empirical evidences about how the agent makes decisions and why
the agent performs well or badly, especially when transferred to novel scenes.
Overall, our method provides valuable insight into the internal decision-making
process of vision-based RL. In addition, our method does not use any external
labelled data, and thus demonstrates the possibility to learn high-quality mask
through a self-supervised manner, which may shed light on new paradigms for
label-free vision learning such as self-supervised segmentation and detection.
</p>
<a href="http://arxiv.org/abs/2003.07069" target="_blank">arXiv:2003.07069</a> [<a href="http://arxiv.org/pdf/2003.07069" target="_blank">pdf</a>]

<h2>Online stochastic gradient descent on non-convex losses from high-dimensional inference. (arXiv:2003.10409v3 [stat.ML] UPDATED)</h2>
<h3>Gerard Ben Arous, Reza Gheissari, Aukosh Jagannath</h3>
<p>Stochastic gradient descent (SGD) is a popular algorithm for optimization
problems arising in high-dimensional inference tasks. Here one produces an
estimator of an unknown parameter from independent samples of data by
iteratively optimizing a loss function. This loss function is random and often
non-convex. We study the performance of the simplest version of SGD, namely
online SGD, from a random start in the setting where the parameter space is
high-dimensional.

We develop nearly sharp thresholds for the number of samples needed for
consistent estimation as one varies the dimension. Our thresholds depend only
on an intrinsic property of the population loss which we call the information
exponent. In particular, our results do not assume uniform control on the loss
itself, such as convexity or uniform derivative bounds. The thresholds we
obtain are polynomial in the dimension and the precise exponent depends
explicitly on the information exponent. As a consequence of our results, we
find that except for the simplest tasks, almost all of the data is used simply
in the initial search phase to obtain non-trivial correlation with the ground
truth. Upon attaining non-trivial correlation, the descent is rapid and
exhibits law of large numbers type behaviour.

We illustrate our approach by applying it to a wide set of inference tasks
such as phase retrieval, parameter estimation for generalized linear models,
spiked matrix models, and spiked tensor models, as well as for supervised
learning for single-layer networks with general activation functions.
</p>
<a href="http://arxiv.org/abs/2003.10409" target="_blank">arXiv:2003.10409</a> [<a href="http://arxiv.org/pdf/2003.10409" target="_blank">pdf</a>]

<h2>A Unified Theory of Decentralized SGD with Changing Topology and Local Updates. (arXiv:2003.10422v2 [cs.LG] UPDATED)</h2>
<h3>Anastasia Koloskova, Nicolas Loizou, Sadra Boreiri, Martin Jaggi, Sebastian U. Stich</h3>
<p>Decentralized stochastic optimization methods have gained a lot of attention
recently, mainly because of their cheap per iteration cost, data locality, and
their communication-efficiency. In this paper we introduce a unified
convergence analysis that covers a large variety of decentralized SGD methods
which so far have required different intuitions, have different applications,
and which have been developed separately in various communities.

Our algorithmic framework covers local SGD updates and synchronous and
pairwise gossip updates on adaptive network topology. We derive universal
convergence rates for smooth (convex and non-convex) problems and the rates
interpolate between the heterogeneous (non-identically distributed data) and
iid-data settings, recovering linear convergence rates in many special cases,
for instance for over-parametrized models. Our proofs rely on weak assumptions
(typically improving over prior work in several aspects) and recover (and
improve) the best known complexity results for a host of important scenarios,
such as for instance coorperative SGD and federated averaging (local SGD).
</p>
<a href="http://arxiv.org/abs/2003.10422" target="_blank">arXiv:2003.10422</a> [<a href="http://arxiv.org/pdf/2003.10422" target="_blank">pdf</a>]

<h2>Temporal Shift GAN for Large Scale Video Generation. (arXiv:2004.01823v2 [cs.CV] UPDATED)</h2>
<h3>Andres Munoz, Mohammadreza Zolfaghari, Max Argus, Thomas Brox</h3>
<p>Video generation models have become increasingly popular in the last few
years, however the standard 2D architectures used today lack natural
spatio-temporal modelling capabilities. In this paper, we present a network
architecture for video generation that models spatio-temporal consistency
without resorting to costly 3D architectures. The architecture facilitates
information exchange between neighboring time points, which improves the
temporal consistency of both the high level structure as well as the low-level
details of the generated frames. The approach achieves state-of-the-art
quantitative performance, as measured by the inception score on the UCF-101
dataset as well as better qualitative results. We also introduce a new
quantitative measure (S3) that uses downstream tasks for evaluation. Moreover,
we present a new multi-label dataset MaisToy, which enables us to evaluate the
generalization of the model.
</p>
<a href="http://arxiv.org/abs/2004.01823" target="_blank">arXiv:2004.01823</a> [<a href="http://arxiv.org/pdf/2004.01823" target="_blank">pdf</a>]

<h2>Position-based Scaled Gradient for Model Quantization and Pruning. (arXiv:2005.11035v4 [cs.CV] UPDATED)</h2>
<h3>Jangho Kim, KiYoon Yoo, Nojun Kwak</h3>
<p>We propose the position-based scaled gradient (PSG) that scales the gradient
depending on the position of a weight vector to make it more
compression-friendly. First, we theoretically show that applying PSG to the
standard gradient descent (GD), which is called PSGD, is equivalent to the GD
in the warped weight space, a space made by warping the original weight space
via an appropriately designed invertible function. Second, we empirically show
that PSG acting as a regularizer to a weight vector is favorable for model
compression domains such as quantization and pruning. PSG reduces the gap
between the weight distributions of a full-precision model and its compressed
counterpart. This enables the versatile deployment of a model either as an
uncompressed mode or as a compressed mode depending on the availability of
resources. The experimental results on CIFAR-10/100 and ImageNet datasets show
the effectiveness of the proposed PSG in both domains of pruning and
quantization even for extremely low bits. The code is released in Github.
</p>
<a href="http://arxiv.org/abs/2005.11035" target="_blank">arXiv:2005.11035</a> [<a href="http://arxiv.org/pdf/2005.11035" target="_blank">pdf</a>]

<h2>Revisiting Parameter Sharing In Multi-Agent Deep Reinforcement Learning. (arXiv:2005.13625v5 [cs.LG] UPDATED)</h2>
<h3>Justin K Terry, Nathaniel Grammel, Ananth Hari, Luis Santos, Benjamin Black</h3>
<p>"Nonstationarity" is a fundamental problem in cooperative multi-agent
reinforcement learning (MARL). It results from every agent's policy changing
during learning, while being part of the environment from the perspective of
other agents. This causes information to inherently oscillate between agents
during learning, greatly slowing convergence. We use the MAILP model of
information transfer during multi-agent learning to show that increasing
centralization during learning arbitrarily mitigates the slowing of convergence
due to nonstationarity. The most centralized case of learning is parameter
sharing, an uncommonly used MARL method, specific to environments with
homogeneous agents. It bootstraps single-agent reinforcement learning (RL)
methods and learns an identical policy for each agent. We experimentally
replicate our theoretical result of increased learning centralization leading
to better performance. We further apply parameter sharing to 8 more modern
single-agent deep RL methods for the first time, achieving up to 44 times more
average reward in 16% as many episodes compared to previous parameter sharing
experiments. We finally give a formal proof of a set of methods that allow
parameter sharing to serve in environments with heterogeneous agents.
</p>
<a href="http://arxiv.org/abs/2005.13625" target="_blank">arXiv:2005.13625</a> [<a href="http://arxiv.org/pdf/2005.13625" target="_blank">pdf</a>]

<h2>Statistical Guarantees for Regularized Neural Networks. (arXiv:2006.00294v2 [cs.LG] UPDATED)</h2>
<h3>Mahsa Taheri, Fang Xie, Johannes Lederer</h3>
<p>Neural networks have become standard tools in the analysis of data, but they
lack comprehensive mathematical theories. For example, there are very few
statistical guarantees for learning neural networks from data, especially for
classes of estimators that are used in practice or at least similar to such. In
this paper, we develop a general statistical guarantee for estimators that
consist of a least-squares term and a regularizer. We then exemplify this
guarantee with $\ell_1$-regularization, showing that the corresponding
prediction error increases at most sub-linearly in the number of layers and at
most logarithmically in the total number of parameters. Our results establish a
mathematical basis for regularized estimation of neural networks, and they
deepen our mathematical understanding of neural networks and deep learning more
generally.
</p>
<a href="http://arxiv.org/abs/2006.00294" target="_blank">arXiv:2006.00294</a> [<a href="http://arxiv.org/pdf/2006.00294" target="_blank">pdf</a>]

<h2>Impact-Aware Task-Space Quadratic-Programming Control. (arXiv:2006.01987v2 [cs.RO] UPDATED)</h2>
<h3>Yuquan Wang, Niels Dehio, Arnaud Tanguy, Abderrahmane Kheddar</h3>
<p>Generating on-purpose impacts with rigid robots is challenging as they may
lead to severe hardware failures due to abrupt changes in the velocities and
torques. Without dedicated hardware and controllers, robots typically operate
at a near-zero velocity in the vicinity of contacts. We assume knowing how much
of impact the hardware can absorb and focus solely on the controller aspects.
The novelty of our approach is twofold: (i) it uses the task-space inverse
dynamics formalism that we extend by seamlessly integrating impact tasks; (ii)
it does not require separate models with switches or a reset map to operate the
robot undergoing impact tasks. Our main idea lies in integrating post-impact
states prediction and impact-aware inequality constraints as part of our
existing general-purpose whole-body controller. To achieve such prediction, we
formulate task-space impacts and its spreading along the kinematic tree of a
floating-base robot with subsequent joint velocity and torque jumps. As a
result, the feasible solution set accounts for various constraints due to
expected impacts. In a multi-contact situation of under-actuated legged robots
subject to multiple impacts, we also enforce standing stability margins. By
design, our controller does not require precise knowledge of impact location
and timing. We assessed our formalism with the humanoid robot HRP-4, generating
maximum contact velocities, neither breaking established contacts nor damaging
the hardware.
</p>
<a href="http://arxiv.org/abs/2006.01987" target="_blank">arXiv:2006.01987</a> [<a href="http://arxiv.org/pdf/2006.01987" target="_blank">pdf</a>]

<h2>Learnability with Indirect Supervision Signals. (arXiv:2006.08791v2 [cs.LG] UPDATED)</h2>
<h3>Kaifu Wang, Qiang Ning, Dan Roth</h3>
<p>Learning from indirect supervision signals is important in real-world AI
applications when, often, gold labels are missing or too costly. In this paper,
we develop a unified theoretical framework for multi-class classification when
the supervision is provided by a variable that contains nonzero mutual
information with the gold label. The nature of this problem is determined by
(i) the transition probability from the gold labels to the indirect supervision
variables and (ii) the learner's prior knowledge about the transition. Our
framework relaxes assumptions made in the literature, and supports learning
with unknown, non-invertible and instance-dependent transitions. Our theory
introduces a novel concept called \emph{separation}, which characterizes the
learnability and generalization bounds. We also demonstrate the application of
our framework via concrete novel results in a variety of learning scenarios
such as learning with superset annotations and joint supervision signals.
</p>
<a href="http://arxiv.org/abs/2006.08791" target="_blank">arXiv:2006.08791</a> [<a href="http://arxiv.org/pdf/2006.08791" target="_blank">pdf</a>]

<h2>What Do Neural Networks Learn When Trained With Random Labels?. (arXiv:2006.10455v2 [stat.ML] UPDATED)</h2>
<h3>Hartmut Maennel, Ibrahim Alabdulmohsin, Ilya Tolstikhin, Robert J. N. Baldock, Olivier Bousquet, Sylvain Gelly, Daniel Keysers</h3>
<p>We study deep neural networks (DNNs) trained on natural image data with
entirely random labels. Despite its popularity in the literature, where it is
often used to study memorization, generalization, and other phenomena, little
is known about what DNNs learn in this setting. In this paper, we show
analytically for convolutional and fully connected networks that an alignment
between the principal components of network parameters and data takes place
when training with random labels. We study this alignment effect by
investigating neural networks pre-trained on randomly labelled image data and
subsequently fine-tuned on disjoint datasets with random or real labels. We
show how this alignment produces a positive transfer: networks pre-trained with
random labels train faster downstream compared to training from scratch even
after accounting for simple effects, such as weight scaling. We analyze how
competing effects, such as specialization at later layers, may hide the
positive transfer. These effects are studied in several network architectures,
including VGG16 and ResNet18, on CIFAR10 and ImageNet.
</p>
<a href="http://arxiv.org/abs/2006.10455" target="_blank">arXiv:2006.10455</a> [<a href="http://arxiv.org/pdf/2006.10455" target="_blank">pdf</a>]

<h2>A polynomial-time algorithm for learning nonparametric causal graphs. (arXiv:2006.11970v2 [stat.ML] UPDATED)</h2>
<h3>Ming Gao, Yi Ding, Bryon Aragam</h3>
<p>We establish finite-sample guarantees for a polynomial-time algorithm for
learning a nonlinear, nonparametric directed acyclic graphical (DAG) model from
data. The analysis is model-free and does not assume linearity, additivity,
independent noise, or faithfulness. Instead, we impose a condition on the
residual variances that is closely related to previous work on linear models
with equal variances. Compared to an optimal algorithm with oracle knowledge of
the variable ordering, the additional cost of the algorithm is linear in the
dimension $d$ and the number of samples $n$. Finally, we compare the proposed
algorithm to existing approaches in a simulation study.
</p>
<a href="http://arxiv.org/abs/2006.11970" target="_blank">arXiv:2006.11970</a> [<a href="http://arxiv.org/pdf/2006.11970" target="_blank">pdf</a>]

<h2>Learning to Combine Top-Down and Bottom-Up Signals in Recurrent Neural Networks with Attention over Modules. (arXiv:2006.16981v2 [cs.LG] UPDATED)</h2>
<h3>Sarthak Mittal, Alex Lamb, Anirudh Goyal, Vikram Voleti, Murray Shanahan, Guillaume Lajoie, Michael Mozer, Yoshua Bengio</h3>
<p>Robust perception relies on both bottom-up and top-down signals. Bottom-up
signals consist of what's directly observed through sensation. Top-down signals
consist of beliefs and expectations based on past experience and short-term
memory, such as how the phrase `peanut butter and~...' will be completed. The
optimal combination of bottom-up and top-down information remains an open
question, but the manner of combination must be dynamic and both context and
task dependent. To effectively utilize the wealth of potential top-down
information available, and to prevent the cacophony of intermixed signals in a
bidirectional architecture, mechanisms are needed to restrict information flow.
We explore deep recurrent neural net architectures in which bottom-up and
top-down signals are dynamically combined using attention. Modularity of the
architecture further restricts the sharing and communication of information.
Together, attention and modularity direct information flow, which leads to
reliable performance improvements in perceptual and language tasks, and in
particular improves robustness to distractions and noisy data. We demonstrate
on a variety of benchmarks in language modeling, sequential image
classification, video prediction and reinforcement learning that the
\emph{bidirectional} information flow can improve results over strong
baselines.
</p>
<a href="http://arxiv.org/abs/2006.16981" target="_blank">arXiv:2006.16981</a> [<a href="http://arxiv.org/pdf/2006.16981" target="_blank">pdf</a>]

<h2>Labeling of Multilingual Breast MRI Reports. (arXiv:2007.03028v3 [cs.CV] UPDATED)</h2>
<h3>Chen-Han Tsai, Nahum Kiryati, Eli Konen, Miri Sklair-Levy, Arnaldo Mayer</h3>
<p>Medical reports are an essential medium in recording a patient's condition
throughout a clinical trial. They contain valuable information that can be
extracted to generate a large labeled dataset needed for the development of
clinical tools. However, the majority of medical reports are stored in an
unregularized format, and a trained human annotator (typically a doctor) must
manually assess and label each case, resulting in an expensive and time
consuming procedure. In this work, we present a framework for developing a
multilingual breast MRI report classifier using a custom-built language
representation called LAMBR. Our proposed method overcomes practical challenges
faced in clinical settings, and we demonstrate improved performance in
extracting labels from medical reports when compared with conventional
approaches.
</p>
<a href="http://arxiv.org/abs/2007.03028" target="_blank">arXiv:2007.03028</a> [<a href="http://arxiv.org/pdf/2007.03028" target="_blank">pdf</a>]

<h2>Learning RGB-D Feature Embeddings for Unseen Object Instance Segmentation. (arXiv:2007.15157v2 [cs.RO] UPDATED)</h2>
<h3>Yu Xiang, Christopher Xie, Arsalan Mousavian, Dieter Fox</h3>
<p>Segmenting unseen objects in cluttered scenes is an important skill that
robots need to acquire in order to perform tasks in new environments. In this
work, we propose a new method for unseen object instance segmentation by
learning RGB-D feature embeddings from synthetic data. A metric learning loss
function is utilized to learn to produce pixel-wise feature embeddings such
that pixels from the same object are close to each other and pixels from
different objects are separated in the embedding space. With the learned
feature embeddings, a mean shift clustering algorithm can be applied to
discover and segment unseen objects. We further improve the segmentation
accuracy with a new two-stage clustering algorithm. Our method demonstrates
that non-photorealistic synthetic RGB and depth images can be used to learn
feature embeddings that transfer well to real-world images for unseen object
instance segmentation.
</p>
<a href="http://arxiv.org/abs/2007.15157" target="_blank">arXiv:2007.15157</a> [<a href="http://arxiv.org/pdf/2007.15157" target="_blank">pdf</a>]

<h2>A Benchmark for Studying Diabetic Retinopathy: Segmentation, Grading, and Transferability. (arXiv:2008.09772v3 [cs.CV] UPDATED)</h2>
<h3>Yi Zhou, Boyang Wang, Lei Huang, Shanshan Cui, Ling Shao</h3>
<p>People with diabetes are at risk of developing an eye disease called diabetic
retinopathy (DR). This disease occurs when high blood glucose levels cause
damage to blood vessels in the retina. Computer-aided DR diagnosis is a
promising tool for early detection of DR and severity grading, due to the great
success of deep learning. However, most current DR diagnosis systems do not
achieve satisfactory performance or interpretability for ophthalmologists, due
to the lack of training data with consistent and fine-grained annotations. To
address this problem, we construct a large fine-grained annotated DR dataset
containing 2,842 images (FGADR). This dataset has 1,842 images with pixel-level
DR-related lesion annotations, and 1,000 images with image-level labels graded
by six board-certified ophthalmologists with intra-rater consistency. The
proposed dataset will enable extensive studies on DR diagnosis. We set up three
benchmark tasks for evaluation: 1. DR lesion segmentation; 2. DR grading by
joint classification and segmentation; 3. Transfer learning for ocular
multi-disease identification. Moreover, a novel inductive transfer learning
method is introduced for the third task. Extensive experiments using different
state-of-the-art methods are conducted on our FGADR dataset, which can serve as
baselines for future research.
</p>
<a href="http://arxiv.org/abs/2008.09772" target="_blank">arXiv:2008.09772</a> [<a href="http://arxiv.org/pdf/2008.09772" target="_blank">pdf</a>]

<h2>Adaptive Local Structure Consistency based Heterogeneous Remote Sensing Change Detection. (arXiv:2008.12958v2 [cs.CV] UPDATED)</h2>
<h3>Lin Lei, Yuli Sun, Gangyao Kuang</h3>
<p>Change detection of heterogeneous remote sensing images is an important and
challenging topic in remote sensing for emergency situation resulting from
nature disaster. Due to the different imaging mechanisms of heterogeneous
sensors, it is difficult to directly compare the images. To address this
challenge, we explore an unsupervised change detection method based on adaptive
local structure consistency (ALSC) between heterogeneous images in this letter,
which constructs an adaptive graph representing the local structure for each
patch in one image domain and then projects this graph to the other image
domain to measure the change level. This local structure consistency exploits
the fact that the heterogeneous images share the same structure information for
the same ground object, which is imaging modality-invariant. To avoid the
leakage of heterogeneous data, the pixelwise change image is calculated in the
same image domain by graph projection. Experiment results demonstrate the
effectiveness of the proposed ALSC based change detection method by comparing
with some state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2008.12958" target="_blank">arXiv:2008.12958</a> [<a href="http://arxiv.org/pdf/2008.12958" target="_blank">pdf</a>]

<h2>Recurrent Graph Tensor Networks. (arXiv:2009.08727v4 [cs.LG] UPDATED)</h2>
<h3>Yao Lei Xu, Danilo P. Mandic</h3>
<p>Recurrent Neural Networks (RNNs) are among the most successful machine
learning models for sequence modelling. In this paper, we show that the
modelling of hidden states in RNNs can be approximated through a multi-linear
graph filter, which describes the directional flow of temporal information. The
so derived multi-linear graph filter is then generalized to a tensor network
form to improve its modelling power, resulting in a novel Recurrent Graph
Tensor Network (RGTN). To validate the expressive power of the derived network,
several variants of RGTN models were proposed and employed for the task of
time-series forecasting, demonstrating superior properties in terms of
convergence, performance, and complexity. By leveraging the multi-modal nature
of tensor networks, RGTN models were shown to out-perform a standard RNN by 23%
in terms of mean-squared-error while using up to 86% less parameters.
Therefore, by combining the expressive power of tensor networks with a suitable
graph filter, we show that the proposed RGTN models can out-perform a classical
RNN at a drastically lower parameter complexity, especially in the multi-modal
setting.
</p>
<a href="http://arxiv.org/abs/2009.08727" target="_blank">arXiv:2009.08727</a> [<a href="http://arxiv.org/pdf/2009.08727" target="_blank">pdf</a>]

<h2>GRAC: Self-Guided and Self-Regularized Actor-Critic. (arXiv:2009.08973v2 [cs.LG] UPDATED)</h2>
<h3>Lin Shao, Yifan You, Mengyuan Yan, Qingyun Sun, Jeannette Bohg</h3>
<p>Deep reinforcement learning (DRL) algorithms have successfully been
demonstrated on a range of challenging decision making and control tasks. One
dominant component of recent deep reinforcement learning algorithms is the
target network which mitigates the divergence when learning the Q function.
However, target networks can slow down the learning process due to delayed
function updates. Our main contribution in this work is a self-regularized
TD-learning method to address divergence without requiring a target network.
Additionally, we propose a self-guided policy improvement method by combining
policy-gradient with zero-order optimization to search for actions associated
with higher Q-values in a broad neighborhood. This makes learning more robust
to local noise in the Q function approximation and guides the updates of our
actor network. Taken together, these components define GRAC, a novel
self-guided and self-regularized actor critic algorithm. We evaluate GRAC on
the suite of OpenAI gym tasks, achieving or outperforming state of the art in
every environment tested.
</p>
<a href="http://arxiv.org/abs/2009.08973" target="_blank">arXiv:2009.08973</a> [<a href="http://arxiv.org/pdf/2009.08973" target="_blank">pdf</a>]

<h2>Randomized fast no-loss expert system to play tic tac toe like a human. (arXiv:2009.11225v2 [cs.AI] UPDATED)</h2>
<h3>Aditya Jyoti Paul</h3>
<p>This paper introduces a blazingly fast, no-loss expert system for Tic Tac Toe
using Decision Trees called T3DT, that tries to emulate human gameplay as
closely as possible. It does not make use of any brute force, minimax or
evolutionary techniques, but is still always unbeatable. In order to make the
gameplay more human-like, randomization is prioritized and T3DT randomly
chooses one of the multiple optimal moves at each step. Since it does not need
to analyse the complete game tree at any point, T3DT is exceptionally faster
than any brute force or minimax algorithm, this has been shown theoretically as
well as empirically from clock-time analyses in this paper. T3DT also doesn't
need the data sets or the time to train an evolutionary model, making it a
practical no-loss approach to play Tic Tac Toe.
</p>
<a href="http://arxiv.org/abs/2009.11225" target="_blank">arXiv:2009.11225</a> [<a href="http://arxiv.org/pdf/2009.11225" target="_blank">pdf</a>]

<h2>Multi-Loss Sub-Ensembles for Accurate Classification with Uncertainty Estimation. (arXiv:2010.01917v2 [cs.LG] UPDATED)</h2>
<h3>Omer Achrack, Ouriel Barzilay, Raizy Kellerman</h3>
<p>Deep neural networks (DNNs) have made a revolution in numerous fields during
the last decade. However, in tasks with high safety requirements, such as
medical or autonomous driving applications, providing an assessment of the
models reliability can be vital. Uncertainty estimation for DNNs has been
addressed using Bayesian methods, providing mathematically founded models for
reliability assessment. These model are computationally expensive and generally
impractical for many real-time use cases. Recently, non-Bayesian methods were
proposed to tackle uncertainty estimation more efficiently. We propose an
efficient method for uncertainty estimation in DNNs achieving high accuracy. We
simulate the notion of multi-task learning on single-task problems by producing
parallel predictions from similar models differing by their loss. This
multi-loss approach allows one-phase training for single-task learning with
uncertainty estimation. We keep our inference time relatively low by leveraging
the advantage proposed by the Deep-Sub-Ensembles method. The novelty of this
work resides in the proposed accurate variational inference with a simple and
convenient training procedure, while remaining competitive in terms of
computational time. We conduct experiments on SVHN, CIFAR10, CIFAR100 as well
as Image-Net using different architectures. Our results show improved accuracy
on the classification task and competitive results on several uncertainty
measures.
</p>
<a href="http://arxiv.org/abs/2010.01917" target="_blank">arXiv:2010.01917</a> [<a href="http://arxiv.org/pdf/2010.01917" target="_blank">pdf</a>]

<h2>Energy-based Out-of-distribution Detection. (arXiv:2010.03759v3 [cs.LG] UPDATED)</h2>
<h3>Weitang Liu, Xiaoyun Wang, John D. Owens, Yixuan Li</h3>
<p>Determining whether inputs are out-of-distribution (OOD) is an essential
building block for safely deploying machine learning models in the open world.
However, previous methods relying on the softmax confidence score suffer from
overconfident posterior distributions for OOD data. We propose a unified
framework for OOD detection that uses an energy score. We show that energy
scores better distinguish in- and out-of-distribution samples than the
traditional approach using the softmax scores. Unlike softmax confidence
scores, energy scores are theoretically aligned with the probability density of
the inputs and are less susceptible to the overconfidence issue. Within this
framework, energy can be flexibly used as a scoring function for any
pre-trained neural classifier as well as a trainable cost function to shape the
energy surface explicitly for OOD detection. On a CIFAR-10 pre-trained
WideResNet, using the energy score reduces the average FPR (at TPR 95%) by
18.03% compared to the softmax confidence score. With energy-based training,
our method outperforms the state-of-the-art on common benchmarks.
</p>
<a href="http://arxiv.org/abs/2010.03759" target="_blank">arXiv:2010.03759</a> [<a href="http://arxiv.org/pdf/2010.03759" target="_blank">pdf</a>]

<h2>Autonomous Person-Specific Following Robot. (arXiv:2010.08017v2 [cs.RO] UPDATED)</h2>
<h3>Wesley P. Chan, Sina Radmard, Zhao Quan Hew, Jon Morris, Elizabeth Croft, H.F. Machiel Van der Loos</h3>
<p>Following a specific user is a desired or even required capability for
service robots in many human-robot collaborative applications. However, most
existing person-following robots follow people without knowledge of who it is
following. In this paper, we proposed an identity-specific person tracker,
capable of tracking and identifying nearby people, to enable person-specific
following. Our proposed method uses a Sequential Nearest Neighbour with
Thresholding Selection algorithm we devised to fuse together an anonymous
person tracker and a face recogniser. Experiment results comparing our proposed
method with alternative approaches showed that our method achieves better
performance in tracking and identifying people, as well as improved robot
performance in following a target individual.
</p>
<a href="http://arxiv.org/abs/2010.08017" target="_blank">arXiv:2010.08017</a> [<a href="http://arxiv.org/pdf/2010.08017" target="_blank">pdf</a>]

<h2>Tensor-based Intrinsic Subspace Representation Learning for Multi-view Clustering. (arXiv:2010.09193v5 [cs.LG] UPDATED)</h2>
<h3>Qinghai Zheng, Jihua Zhu, Zhongyu Li, Haoyu Tang, Shuangxun Ma</h3>
<p>As a hot research topic, many multi-view clustering methods are proposed in
recent years. Nevertheless, most existing algorithms merely take the consensus
information among different views into consideration for clustering. Actually,
it may hinder the multi-view clustering performance in real-life applications,
since different views usually contain diverse statistic properties. To address
this problem, we propose a novel Tensor-based Intrinsic Subspace Representation
Learning (TISRL) for multi-view clustering in this paper. Concretely, the rank
preserving decomposition is proposed firstly to effectively deal with the
diverse statistic information contained in different views. Then, to achieve
the intrinsic subspace representation, the tensor-singular value decomposition
based low-rank tensor constraint is also utilized in our method. It can be seen
that the specific information of multi-view data is fully investigated by the
rank preserving decomposition, and the high-order correlations of multi-view
data are also mined by the low-rank tensor constraint. The objective function
can be optimized by an augmented Lagrangian multiplier based alternating
direction minimization algorithm. Experimental results on nine common used
real-world multi-view datasets illustrate the superiority of TISRL.
</p>
<a href="http://arxiv.org/abs/2010.09193" target="_blank">arXiv:2010.09193</a> [<a href="http://arxiv.org/pdf/2010.09193" target="_blank">pdf</a>]

<h2>Parameter Norm Growth During Training of Transformers. (arXiv:2010.09697v2 [cs.LG] UPDATED)</h2>
<h3>William Merrill, Vivek Ramanujan, Yoav Goldberg, Roy Schwartz, Noah Smith</h3>
<p>The capacity of neural networks like the widely adopted transformer is known
to be very high. Evidence is emerging that they learn successfully due to
inductive bias in the training routine, typically some variant of gradient
descent (GD). To better understand this bias, we study the tendency of
transformer parameters to grow in magnitude during training. We find, both
theoretically and empirically, that, in certain contexts, GD increases the
parameter $L_2$ norm up to a threshold that itself increases with training-set
accuracy. This means increasing training accuracy over time enables the norm to
increase. Empirically, we show that the norm grows continuously over
pretraining for T5 (Raffel et al., 2019). We show that pretrained T5
approximates a semi-discretized network with saturated activation functions.
Such "saturated" networks are known to have a reduced capacity compared to the
original network family that can be described in automata-theoretic terms. This
suggests saturation is a new characterization of an inductive bias implicit in
GD that is of particular interest for NLP. While our experiments focus on
transformers, our theoretical analysis extends to other architectures with
similar formal properties, such as feedforward ReLU networks.
</p>
<a href="http://arxiv.org/abs/2010.09697" target="_blank">arXiv:2010.09697</a> [<a href="http://arxiv.org/pdf/2010.09697" target="_blank">pdf</a>]

<h2>Robust & Asymptotically Locally Optimal UAV-Trajectory Generation Based on Spline Subdivision. (arXiv:2010.09904v2 [cs.RO] UPDATED)</h2>
<h3>Ruiqi Ni, Teseo Schneider, Daniele Panozzo, Zherong Pan, Xifeng Gao</h3>
<p>Generating locally optimal UAV-trajectories is challenging due to the
non-convex constraints of collision avoidance and actuation limits. We present
the first local, optimization-based UAV-trajectory generator that
simultaneously guarantees validity and asymptotic optimality. Validity: Given a
feasible initial guess, our algorithm guarantees the satisfaction of all
constraints throughout the process of optimization. Asymptotic Optimality: We
use a conservative piecewise approximation of the trajectory with automatically
adjustable resolution of its discretization. The trajectory converges under
refinement to the first-order stationary point of the exact non-convex
programming problem. Our method has additional practical advantages including
joint optimality in terms of trajectory and time-allocation, and robustness to
challenging environments as demonstrated in our experiments.
</p>
<a href="http://arxiv.org/abs/2010.09904" target="_blank">arXiv:2010.09904</a> [<a href="http://arxiv.org/pdf/2010.09904" target="_blank">pdf</a>]

<h2>GDN: A Coarse-To-Fine (C2F) Representation for End-To-End 6-DoF Grasp Detection. (arXiv:2010.10695v4 [cs.RO] UPDATED)</h2>
<h3>Kuang-Yu Jeng, Yueh-Cheng Liu, Zhe Yu Liu, Jen-Wei Wang, Ya-Liang Chang, Hung-Ting Su, Winston H. Hsu</h3>
<p>We proposed an end-to-end grasp detection network, Grasp Detection Network
(GDN), cooperated with a novel coarse-to-fine (C2F) grasp representation design
to detect diverse and accurate 6-DoF grasps based on point clouds. Compared to
previous two-stage approaches which sample and evaluate multiple grasp
candidates, our architecture is at least 20 times faster. It is also 8% and 40%
more accurate in terms of the success rate in single object scenes and the
complete rate in clutter scenes, respectively. Our method shows superior
results among settings with different number of views and input points.
Moreover, we propose a new AP-based metric which considers both rotation and
transition errors, making it a more comprehensive evaluation tool for grasp
detection models.
</p>
<a href="http://arxiv.org/abs/2010.10695" target="_blank">arXiv:2010.10695</a> [<a href="http://arxiv.org/pdf/2010.10695" target="_blank">pdf</a>]

<h2>A Data Set and a Convolutional Model for Iconography Classification in Paintings. (arXiv:2010.11697v2 [cs.CV] UPDATED)</h2>
<h3>Federico Milani, Piero Fraternali</h3>
<p>Iconography in art is the discipline that studies the visual content of
artworks to determine their motifs and themes andto characterize the way these
are represented. It is a subject of active research for a variety of purposes,
including the interpretation of meaning, the investigation of the origin and
diffusion in time and space of representations, and the study of influences
across artists and art works. With the proliferation of digital archives of art
images, the possibility arises of applying Computer Vision techniques to the
analysis of art images at an unprecedented scale, which may support iconography
research and education. In this paper we introduce a novel paintings data set
for iconography classification and present the quantitativeand qualitative
results of applying a Convolutional Neural Network (CNN) classifier to the
recognition of the iconography of artworks. The proposed classifier achieves
good performances (71.17% Precision, 70.89% Recall, 70.25% F1-Score and 72.73%
Average Precision) in the task of identifying saints in Christian religious
paintings, a task made difficult by the presence of classes with very similar
visual features. Qualitative analysis of the results shows that the CNN focuses
on the traditional iconic motifs that characterize the representation of each
saint and exploits such hints to attain correct identification. The ultimate
goal of our work is to enable the automatic extraction, decomposition, and
comparison of iconography elements to support iconographic studies and
automatic art work annotation.
</p>
<a href="http://arxiv.org/abs/2010.11697" target="_blank">arXiv:2010.11697</a> [<a href="http://arxiv.org/pdf/2010.11697" target="_blank">pdf</a>]

<h2>Multi-Graph Tensor Networks. (arXiv:2010.13209v2 [cs.LG] UPDATED)</h2>
<h3>Yao Lei Xu, Kriton Konstantinidis, Danilo P. Mandic</h3>
<p>The irregular and multi-modal nature of numerous modern data sources poses
serious challenges for traditional deep learning algorithms. To this end,
recent efforts have generalized existing algorithms to irregular domains
through graphs, with the aim to gain additional insights from data through the
underlying graph topology. At the same time, tensor-based methods have
demonstrated promising results in bypassing the bottlenecks imposed by the
Curse of Dimensionality. In this paper, we introduce a novel Multi-Graph Tensor
Network (MGTN) framework, which exploits both the ability of graphs to handle
irregular data sources and the compression properties of tensor networks in a
deep learning setting. The potential of the proposed framework is demonstrated
through an MGTN based deep Q agent for Foreign Exchange (FOREX) algorithmic
trading. By virtue of the MGTN, a FOREX currency graph is leveraged to impose
an economically meaningful structure on this demanding task, resulting in a
highly superior performance against three competing models and at a drastically
lower complexity.
</p>
<a href="http://arxiv.org/abs/2010.13209" target="_blank">arXiv:2010.13209</a> [<a href="http://arxiv.org/pdf/2010.13209" target="_blank">pdf</a>]

<h2>Malicious Requests Detection with Improved Bidirectional Long Short-term Memory Neural Networks. (arXiv:2010.13285v3 [cs.LG] UPDATED)</h2>
<h3>Wenhao Li, Bincheng Zhang, Jiajie Zhang</h3>
<p>Extension of comments: The experimental results are self-contradictory. The
results of RNN-IDS[22] in Table 1. FPR = 100% = FP/(FP+TN), so TN should be 0.
recall = 100% = TP/(TP+FN), so FN should be 0. Accuray =
(FP+FN)/(TP+FP+FN+TN)=FP/(TP+FP), given that TN=FN=0.Accuray=precision, so
FP/(TP+FP)=TP/(TP+FP), so FP=TP. Therefore, precision=TP/(TP+FP)=50%, given
that FP=TP. But table 1 shows that precision is 0.6967.

Abstract: Detecting and intercepting malicious requests are one of the most
widely used ways against attacks in the network security. Most existing
detecting approaches, including matching blacklist characters and machine
learning algorithms have all shown to be vulnerable to sophisticated attacks.
To address the above issues, a more general and rigorous detection method is
required. In this paper, we formulate the problem of detecting malicious
requests as a temporal sequence classification problem, and propose a novel
deep learning model namely Convolutional Neural Network-Bidirectional Long
Short-term Memory-Convolutional Neural Network (CNN-BiLSTM-CNN). By connecting
the shadow and deep feature maps of the convolutional layers, the malicious
feature extracting ability is improved on more detailed functionality.
Experimental results on HTTP dataset CSIC 2010 have demonstrated the
effectiveness of the proposed method when compared with the state-of-the-arts.
</p>
<a href="http://arxiv.org/abs/2010.13285" target="_blank">arXiv:2010.13285</a> [<a href="http://arxiv.org/pdf/2010.13285" target="_blank">pdf</a>]

<h2>Proceedings of the AI-HRI Symposium at AAAI-FSS 2020. (arXiv:2010.13830v3 [cs.RO] UPDATED)</h2>
<h3>Shelly Bagchi, Jason R. Wilson, Muneeb I. Ahmad, Christian Dondrup, Zhao Han, Justin W. Hart, Matteo Leonetti, Katrin Lohan, Ross Mead, Emmanuel Senft, Jivko Sinapov, Megan L. Zimmerman</h3>
<p>The Artificial Intelligence (AI) for Human-Robot Interaction (HRI) Symposium
has been a successful venue of discussion and collaboration since 2014. In that
time, the related topic of trust in robotics has been rapidly growing, with
major research efforts at universities and laboratories across the world.
Indeed, many of the past participants in AI-HRI have been or are now involved
with research into trust in HRI. While trust has no consensus definition, it is
regularly associated with predictability, reliability, inciting confidence, and
meeting expectations. Furthermore, it is generally believed that trust is
crucial for adoption of both AI and robotics, particularly when transitioning
technologies from the lab to industrial, social, and consumer applications.
However, how does trust apply to the specific situations we encounter in the
AI-HRI sphere? Is the notion of trust in AI the same as that in HRI? We see a
growing need for research that lives directly at the intersection of AI and HRI
that is serviced by this symposium. Over the course of the two-day meeting, we
propose to create a collaborative forum for discussion of current efforts in
trust for AI-HRI, with a sub-session focused on the related topic of
explainable AI (XAI) for HRI.
</p>
<a href="http://arxiv.org/abs/2010.13830" target="_blank">arXiv:2010.13830</a> [<a href="http://arxiv.org/pdf/2010.13830" target="_blank">pdf</a>]

<h2>Graph Contrastive Learning with Augmentations. (arXiv:2010.13902v2 [cs.LG] UPDATED)</h2>
<h3>Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, Yang Shen</h3>
<p>Generalizable, transferrable, and robust representation learning on
graph-structured data remains a challenge for current graph neural networks
(GNNs). Unlike what has been developed for convolutional neural networks (CNNs)
for image data, self-supervised learning and pre-training are less explored for
GNNs. In this paper, we propose a graph contrastive learning (GraphCL)
framework for learning unsupervised representations of graph data. We first
design four types of graph augmentations to incorporate various priors. We then
systematically study the impact of various combinations of graph augmentations
on multiple datasets, in four different settings: semi-supervised,
unsupervised, and transfer learning as well as adversarial attacks. The results
show that, even without tuning augmentation extents nor using sophisticated GNN
architectures, our GraphCL framework can produce graph representations of
similar or better generalizability, transferrability, and robustness compared
to state-of-the-art methods. We also investigate the impact of parameterized
graph augmentation extents and patterns, and observe further performance gains
in preliminary experiments. Our codes are available at
https://github.com/Shen-Lab/GraphCL.
</p>
<a href="http://arxiv.org/abs/2010.13902" target="_blank">arXiv:2010.13902</a> [<a href="http://arxiv.org/pdf/2010.13902" target="_blank">pdf</a>]

<h2>Geometric Fabrics for the Acceleration-based Design of Robotic Motion. (arXiv:2010.14750v3 [cs.RO] UPDATED)</h2>
<h3>Mandy Xie, Karl Van Wyk, Anqi Li, Muhammad Asif Rana, Dieter Fox, Byron Boots, Nathan Ratliff</h3>
<p>This paper describes the pragmatic design and construction of geometric
fabrics for shaping a robot's task-independent nominal behavior, capturing
behavioral components such as obstacle avoidance, joint limit avoidance,
redundancy resolution, global navigation heuristics, etc. Geometric fabrics
constitute the most concrete incarnation of a new mathematical formulation for
reactive behavior called optimization fabrics. Fabrics generalize recent work
on Riemannian Motion Policies (RMPs); they add provable stability guarantees
and improve design consistency while promoting the intuitive acceleration-based
principles of modular design that make RMPs successful. We describe a suite of
mathematical modeling tools that practitioners can employ in practice and
demonstrate both how to mitigate system complexity by constructing behaviors
layer-wise and how to employ these tools to design robust,
strongly-generalizing, policies that solve practical problems one would expect
to find in industry applications. Our system exhibits intelligent global
navigation behaviors expressed entirely as fabrics with zero planning or state
machine governance.
</p>
<a href="http://arxiv.org/abs/2010.14750" target="_blank">arXiv:2010.14750</a> [<a href="http://arxiv.org/pdf/2010.14750" target="_blank">pdf</a>]

<h2>LIFI: Towards Linguistically Informed Frame Interpolation. (arXiv:2010.16078v3 [cs.CV] UPDATED)</h2>
<h3>Aradhya Neeraj Mathur, Devansh Batra, Yaman Kumar, Rajiv Ratn Shah, Roger Zimmermann, Amanda Stent</h3>
<p>In this work, we explore a new problem of frame interpolation for speech
videos. Such content today forms the major form of online communication. We try
to solve this problem by using several deep learning video generation
algorithms to generate the missing frames. We also provide examples where
computer vision models despite showing high performance on conventional
non-linguistic metrics fail to accurately produce faithful interpolation of
speech. With this motivation, we provide a new set of linguistically-informed
metrics specifically targeted to the problem of speech videos interpolation. We
also release several datasets to test computer vision video generation models
of their speech understanding.
</p>
<a href="http://arxiv.org/abs/2010.16078" target="_blank">arXiv:2010.16078</a> [<a href="http://arxiv.org/pdf/2010.16078" target="_blank">pdf</a>]

<h2>Two-Level K-FAC Preconditioning for Deep Learning. (arXiv:2011.00573v2 [cs.LG] UPDATED)</h2>
<h3>Nikolaos Tselepidis, Jonas Kohler, Antonio Orvieto</h3>
<p>In the context of deep learning, many optimization methods use gradient
covariance information in order to accelerate the convergence of Stochastic
Gradient Descent. In particular, starting with Adagrad, a seemingly endless
line of research advocates the use of diagonal approximations of the so-called
empirical Fisher matrix in stochastic gradient-based algorithms, with the most
prominent one arguably being Adam. However, in recent years, several works cast
doubt on the theoretical basis of preconditioning with the empirical Fisher
matrix, and it has been shown that more sophisticated approximations of the
actual Fisher matrix more closely resemble the theoretically well-motivated
Natural Gradient Descent. One particularly successful variant of such methods
is the so-called K-FAC optimizer, which uses a Kronecker-factored
block-diagonal Fisher approximation as preconditioner. In this work, drawing
inspiration from two-level domain decomposition methods used as preconditioners
in the field of scientific computing, we extend K-FAC by enriching it with
off-diagonal (i.e. global) curvature information in a computationally efficient
way. We achieve this by adding a coarse-space correction term to the
preconditioner, which captures the global Fisher information matrix at a
coarser scale. We present a small set of experimental results suggesting
improved convergence behaviour of our proposed method.
</p>
<a href="http://arxiv.org/abs/2011.00573" target="_blank">arXiv:2011.00573</a> [<a href="http://arxiv.org/pdf/2011.00573" target="_blank">pdf</a>]

<h2>A Formally Verified Fail-Operational Safety Concept for Automated Driving. (arXiv:2011.00892v2 [cs.RO] UPDATED)</h2>
<h3>Yuting Fu, Andrei Terechko, Jan Friso Groote, Arash Khabbaz Saberi</h3>
<p>Modern Automated Driving (AD) systems rely on safety measures to handle
faults and to bring vehicle to a safe state. To eradicate lethal road
accidents, car manufacturers are constantly introducing new perception as well
as control systems. Contemporary automotive design and safety engineering best
practices are suitable for analyzing system components in isolation, whereas
today's highly complex and interdependent AD systems require novel approach to
ensure resilience to multi-point failures. We present a holistic safety concept
unifying advanced safety measures for handling multiple-point faults. Our
proposed approach enables designers to focus on more pressing issues such as
handling fault-free hazardous behavior associated with system performance
limitations. To verify our approach, we developed an executable model of the
safety concept in the formal specification language mCRL2. The model behavior
is governed by a four-mode degradation policy controlling distributed
processors, redundant communication networks, and virtual machines. To keep the
vehicle as safe as possible our degradation policy can reduce driving comfort
or AD system's availability using additional low-cost driving channels. We
formalized five safety requirements in the modal mu-calculus and proved them
against our mCRL2 model, which is intractable to accomplish exhaustively using
traditional road tests or simulation techniques. In conclusion, our formally
proven safety concept defines a holistic design pattern for designing AD
systems.
</p>
<a href="http://arxiv.org/abs/2011.00892" target="_blank">arXiv:2011.00892</a> [<a href="http://arxiv.org/pdf/2011.00892" target="_blank">pdf</a>]

<h2>Attention Beam: An Image Captioning Approach. (arXiv:2011.01753v2 [cs.CV] UPDATED)</h2>
<h3>Anubhav Shrimal, Tanmoy Chakraborty</h3>
<p>The aim of image captioning is to generate textual description of a given
image. Though seemingly an easy task for humans, it is challenging for machines
as it requires the ability to comprehend the image (computer vision) and
consequently generate a human-like description for the image (natural language
understanding). In recent times, encoder-decoder based architectures have
achieved state-of-the-art results for image captioning. Here, we present a
heuristic of beam search on top of the encoder-decoder based architecture that
gives better quality captions on three benchmark datasets: Flickr8k, Flickr30k
and MS COCO.
</p>
<a href="http://arxiv.org/abs/2011.01753" target="_blank">arXiv:2011.01753</a> [<a href="http://arxiv.org/pdf/2011.01753" target="_blank">pdf</a>]

<h2>Hi-UCD: A Large-scale Dataset for Urban Semantic Change Detection in Remote Sensing Imagery. (arXiv:2011.03247v3 [cs.CV] UPDATED)</h2>
<h3>Shiqi Tian, Zhuo Zheng, Ailong Ma, Yanfei Zhong</h3>
<p>With the acceleration of the urban expansion, urban change detection (UCD),
as a significant and effective approach, can provide the change information
with respect to geospatial objects for dynamical urban analysis. However,
existing datasets suffer from three bottlenecks: (1) lack of high spatial
resolution images; (2) lack of semantic annotation; (3) lack of long-range
multi-temporal images. In this paper, we propose a large scale benchmark
dataset, termed Hi-UCD. This dataset uses aerial images with a spatial
resolution of 0.1 m provided by the Estonia Land Board, including three-time
phases, and semantically annotated with nine classes of land cover to obtain
the direction of ground objects change. It can be used for detecting and
analyzing refined urban changes. We benchmark our dataset using some classic
methods in binary and multi-class change detection. Experimental results show
that Hi-UCD is challenging yet useful. We hope the Hi-UCD can become a strong
benchmark accelerating future research.
</p>
<a href="http://arxiv.org/abs/2011.03247" target="_blank">arXiv:2011.03247</a> [<a href="http://arxiv.org/pdf/2011.03247" target="_blank">pdf</a>]

<h2>Sample-efficient Reinforcement Learning in Robotic Table Tennis. (arXiv:2011.03275v2 [cs.RO] UPDATED)</h2>
<h3>Jonas Tebbe, Lukas Krauch, Yapeng Gao, Andreas Zell</h3>
<p>Reinforcement learning (RL) has recently shown impressive success in various
computer games and simulations. Most of these successes are based on numerous
episodes to be learned from. For typical robotic applications, however, the
number of feasible attempts is very limited. In this paper we present a
sample-efficient RL algorithm applied to the example of a table tennis robot.
In table tennis every stroke is different, of varying placement, speed and
spin. Therefore, an accurate return has be found depending on a
high-dimensional continuous state space. To make learning in few trials
possible the method is embedded into our robot system. In this way we can use a
one-step environment. The state space depends on the ball at hitting time
(position, velocity, spin) and the action is the racket state (orientation,
velocity) at hitting. An actor-critic based deterministic policy gradient
algorithm was developed for accelerated learning. Our approach shows
competitive performance in both simulation and on the real robot in different
challenging scenarios. Accurate results are always obtained within under 200
episodes of training. The video presenting our experiments is available at
https://youtu.be/uRAtdoL6Wpw.
</p>
<a href="http://arxiv.org/abs/2011.03275" target="_blank">arXiv:2011.03275</a> [<a href="http://arxiv.org/pdf/2011.03275" target="_blank">pdf</a>]

<h2>Occlusion-Aware Search for Object Retrieval in Clutter. (arXiv:2011.03334v2 [cs.RO] UPDATED)</h2>
<h3>Wissam Bejjani, Wisdom C. Agboh, Mehmet R. Dogar, Matteo Leonetti</h3>
<p>We address the manipulation task of retrieving a target object from a
cluttered shelf. When the target object is hidden, the robot must search
through the clutter for retrieving it. Solving this task requires reasoning
over the likely locations of the target object. It also requires physics
reasoning over multi-object interactions and future occlusions. In this work,
we present a data-driven approach for generating occlusion-aware actions in
closed-loop. We present a hybrid planner that explores likely states generated
from a learned distribution over the location of the target object. The search
is guided by a heuristic trained with reinforcement learning to evaluate
occluded observations. We evaluate our approach in different environments with
varying clutter densities and physics parameters. The results validate that our
approach can search and retrieve a target object in different physics
environments, while only being trained in simulation. It achieves near
real-time behaviour with a success rate exceeding 88%.
</p>
<a href="http://arxiv.org/abs/2011.03334" target="_blank">arXiv:2011.03334</a> [<a href="http://arxiv.org/pdf/2011.03334" target="_blank">pdf</a>]

<h2>Noise2Sim -- Similarity-based Self-Learning for Image Denoising. (arXiv:2011.03384v2 [cs.LG] UPDATED)</h2>
<h3>Chuang Niu, Ge Wang</h3>
<p>The key idea behind denoising methods is to perform a mean/averaging
operation, either locally or non-locally. An observation on classic denoising
methods is that non-local mean (NLM) outcomes are typically superior to locally
denoised results. Despite achieving the best performance in image denoising,
the supervised deep denoising methods require paired noise-clean data which are
often unavailable. To address this challenge, Noise2Noise methods are based on
the fact that paired noise-clean images can be replaced by paired noise-noise
images which are easier to collect. However, in many scenarios the collection
of paired noise-noise images are still impractical. To bypass labeled images,
Noise2Void methods predict masked pixels from their surroundings in a single
noisy image only. It is pitiful that neither Noise2Noise nor Noise2Void methods
utilize self-similarities in an image as NLM methods do, while
self-similarities/symmetries play a critical role in modern sciences. Here we
propose Noise2Sim, an NLM-inspired self-learning method for image denoising.
Specifically, Noise2Sim leverages self-similarities of image patches and learns
to map between the center pixels of similar patches for self-consistent image
denoising. Our statistical analysis shows that Noise2Sim tends to be equivalent
to Noise2Noise under mild conditions. To accelerate the process of finding
similar image patches, we design an efficient two-step procedure to provide
data for Noise2Sim training, which can be iteratively conducted if needed.
Extensive experiments demonstrate the superiority of Noise2Sim over Noise2Noise
and Noise2Void on common benchmark datasets.
</p>
<a href="http://arxiv.org/abs/2011.03384" target="_blank">arXiv:2011.03384</a> [<a href="http://arxiv.org/pdf/2011.03384" target="_blank">pdf</a>]

<h2>Mapless-Planner: A Robust and Fast Planning Framework for Aggressive Autonomous Flight without Map Fusion. (arXiv:2011.03975v2 [cs.RO] UPDATED)</h2>
<h3>Jialin Ji, Zhepei Wang, Yingjian Wang, Chao Xu, Fei Gao</h3>
<p>Maintaining a map online is resource-consuming while a robust navigation
system usually needs environment abstraction via a well-fused map. In this
paper, we propose a mapless planner which directly conducts such abstraction on
the unfused sensor data. A limited-memory data structure with a reliable
proximity query algorithm is proposed for maintaining raw historical
information. A sampling-based scheme is designed to extract the free-space
skeleton. A smart waypoint selection strategy enables to generate high-quality
trajectories within the resultant flight corridors. Our planner differs from
other mapless ones in that it can abstract and exploit the environment
information efficiently. The online replan consistency and success rate are
both significantly improved against conventional mapless methods.
</p>
<a href="http://arxiv.org/abs/2011.03975" target="_blank">arXiv:2011.03975</a> [<a href="http://arxiv.org/pdf/2011.03975" target="_blank">pdf</a>]

<h2>An HVS-Oriented Saliency Map Prediction Modeling. (arXiv:2011.04076v2 [cs.CV] UPDATED)</h2>
<h3>Qiang Li</h3>
<p>Visual attention is one of the most significant characteristics for selecting
and understanding the outside world. The nature complex scenes, including
larger redundancy and human vision, can't be processing all information
simultaneously because of the information bottleneck. The visual system mainly
focuses on dominant parts of the scenes to reduce the input visual redundancy
information. It's commonly known as visual attention prediction or visual
saliency map. This paper proposes a new saliency prediction architecture
inspired by human low-level visual cortex function. The model considered the
opponent color channel, wavelet energy map, and contrast sensitivity function
for extract image features and maximum approach to real visual neural network
function in the brain. The proposed model is evaluated several datasets,
including MIT1003, MIT300, TORONTO, and SID4VAM to explain its efficiency. The
proposed model results are quantitatively and qualitatively compared to other
state-of-the-art salience prediction models and their achieved out-performing
of visual saliency prediction.
</p>
<a href="http://arxiv.org/abs/2011.04076" target="_blank">arXiv:2011.04076</a> [<a href="http://arxiv.org/pdf/2011.04076" target="_blank">pdf</a>]

<h2>EDEN: Multimodal Synthetic Dataset of Enclosed GarDEN Scenes. (arXiv:2011.04389v2 [cs.CV] UPDATED)</h2>
<h3>Hoang-An Le, Thomas Mensink, Partha Das, Sezer Karaoglu, Theo Gevers</h3>
<p>Multimodal large-scale datasets for outdoor scenes are mostly designed for
urban driving problems. The scenes are highly structured and semantically
different from scenarios seen in nature-centered scenes such as gardens or
parks. To promote machine learning methods for nature-oriented applications,
such as agriculture and gardening, we propose the multimodal synthetic dataset
for Enclosed garDEN scenes (EDEN). The dataset features more than 300K images
captured from more than 100 garden models. Each image is annotated with various
low/high-level vision modalities, including semantic segmentation, depth,
surface normals, intrinsic colors, and optical flow. Experimental results on
the state-of-the-art methods for semantic segmentation and monocular depth
prediction, two important tasks in computer vision, show positive impact of
pre-training deep networks on our dataset for unstructured natural scenes. The
dataset and related materials will be available at
https://lhoangan.github.io/eden.
</p>
<a href="http://arxiv.org/abs/2011.04389" target="_blank">arXiv:2011.04389</a> [<a href="http://arxiv.org/pdf/2011.04389" target="_blank">pdf</a>]

<h2>StylePredict: Machine Theory of Mind for Human Driver Behavior From Trajectories. (arXiv:2011.04816v2 [cs.RO] UPDATED)</h2>
<h3>Rohan Chandra, Aniket Bera, Dinesh Manocha</h3>
<p>Studies have shown that autonomous vehicles (AVs) behave conservatively in a
traffic environment composed of human drivers and do not adapt to local
conditions and socio-cultural norms. It is known that socially aware AVs can be
designed if there exist a mechanism to understand the behaviors of human
drivers. We present a notion of Machine Theory of Mind (M-ToM) to infer the
behaviors of human drivers by observing the trajectory of their vehicles. Our
M-ToM approach, called StylePredict, is based on trajectory analysis of
vehicles, which has been investigated in robotics and computer vision.
StylePredict mimics human ToM to infer driver behaviors, or styles, using a
computational mapping between the extracted trajectory of a vehicle in traffic
and the driver behaviors using graph-theoretic techniques, including spectral
analysis and centrality functions. We use StylePredict to analyze driver
behavior in different cultures in the USA, China, India, and Singapore, based
on traffic density, heterogeneity, and conformity to traffic rules and observe
an inverse correlation between longitudinal (overspeeding) and lateral
(overtaking, lane-changes) driving styles.
</p>
<a href="http://arxiv.org/abs/2011.04816" target="_blank">arXiv:2011.04816</a> [<a href="http://arxiv.org/pdf/2011.04816" target="_blank">pdf</a>]

<h2>Neural Network Compression Via Sparse Optimization. (arXiv:2011.04868v2 [cs.LG] UPDATED)</h2>
<h3>Tianyi Chen, Bo Ji, Yixin Shi, Tianyu Ding, Biyi Fang, Sheng Yi, Xiao Tu</h3>
<p>The compression of deep neural networks (DNNs) to reduce inference cost
becomes increasingly important to meet realistic deployment requirements of
various applications. There have been a significant amount of work regarding
network compression, while most of them are heuristic rule-based or typically
not friendly to be incorporated into varying scenarios. On the other hand,
sparse optimization yielding sparse solutions naturally fits the compression
requirement, but due to the limited study of sparse optimization in stochastic
learning, its extension and application onto model compression is rarely well
explored. In this work, we propose a model compression framework based on the
recent progress on sparse stochastic optimization. Compared to existing model
compression techniques, our method is effective and requires fewer extra
engineering efforts to incorporate with varying applications, and has been
numerically demonstrated on benchmark compression tasks. Particularly, we
achieve up to 7.2 and 2.9 times FLOPs reduction with the same level of
evaluation accuracy on VGG16 for CIFAR10 and ResNet50 for ImageNet compared to
the baseline heavy models, respectively.
</p>
<a href="http://arxiv.org/abs/2011.04868" target="_blank">arXiv:2011.04868</a> [<a href="http://arxiv.org/pdf/2011.04868" target="_blank">pdf</a>]

<h2>Multi-modal, multi-task, multi-attention (M3) deep learning detection of reticular pseudodrusen: towards automated and accessible classification of age-related macular degeneration. (arXiv:2011.05142v2 [cs.CV] UPDATED)</h2>
<h3>Qingyu Chen, Tiarnan D. L. Keenan, Alexis Allot, Yifan Peng, Elvira Agr&#xf3;n, Amitha Domalpally, Caroline C. W. Klaver, Daniel T. Luttikhuizen, Marcus H. Colyer, Catherine A. Cukras, Henry E. Wiley, M. Teresa Magone, Chantal Cousineau-Krieger, Wai T. Wong, Yingying Zhu, Emily Y. Chew, Zhiyong Lu (for the AREDS2 Deep Learning Research Group)</h3>
<p>Objective Reticular pseudodrusen (RPD), a key feature of age-related macular
degeneration (AMD), are poorly detected by human experts on standard color
fundus photography (CFP) and typically require advanced imaging modalities such
as fundus autofluorescence (FAF). The objective was to develop and evaluate the
performance of a novel 'M3' deep learning framework on RPD detection. Materials
and Methods A deep learning framework M3 was developed to detect RPD presence
accurately using CFP alone, FAF alone, or both, employing &gt;8000 CFP-FAF image
pairs obtained prospectively (Age-Related Eye Disease Study 2). The M3
framework includes multi-modal (detection from single or multiple image
modalities), multi-task (training different tasks simultaneously to improve
generalizability), and multi-attention (improving ensembled feature
representation) operation. Performance on RPD detection was compared with
state-of-the-art deep learning models and 13 ophthalmologists; performance on
detection of two other AMD features (geographic atrophy and pigmentary
abnormalities) was also evaluated. Results For RPD detection, M3 achieved area
under receiver operating characteristic (AUROC) 0.832, 0.931, and 0.933 for CFP
alone, FAF alone, and both, respectively. M3 performance on CFP was very
substantially superior to human retinal specialists (median F1-score 0.644
versus 0.350). External validation (on Rotterdam Study, Netherlands)
demonstrated high accuracy on CFP alone (AUROC 0.965). The M3 framework also
accurately detected geographic atrophy and pigmentary abnormalities (AUROC
0.909 and 0.912, respectively), demonstrating its generalizability. Conclusion
This study demonstrates the successful development, robust evaluation, and
external validation of a novel deep learning framework that enables accessible,
accurate, and automated AMD diagnosis and prognosis.
</p>
<a href="http://arxiv.org/abs/2011.05142" target="_blank">arXiv:2011.05142</a> [<a href="http://arxiv.org/pdf/2011.05142" target="_blank">pdf</a>]

<h2>Advances in Human-Robot Handshaking. (arXiv:2008.11695v1 [cs.RO] CROSS LISTED)</h2>
<h3>Vignesh Prasad, Ruth Stock-Homburg, Jan Peters</h3>
<p>The use of social, anthropomorphic robots to support humans in various
industries has been on the rise. During Human-Robot Interaction (HRI),
physically interactive non-verbal behaviour is key for more natural
interactions. Handshaking is one such natural interaction used commonly in many
social contexts. It is one of the first non-verbal interactions which takes
place and should, therefore, be part of the repertoire of a social robot. In
this paper, we explore the existing state of Human-Robot Handshaking and
discuss possible ways forward for such physically interactive behaviours.
</p>
<a href="http://arxiv.org/abs/2008.11695" target="_blank">arXiv:2008.11695</a> [<a href="http://arxiv.org/pdf/2008.11695" target="_blank">pdf</a>]

<h2>Scalable Unsupervised Multi-Criteria Trajectory Segmentation and Driving Preference Mining. (arXiv:2011.03331v1 [cs.RO] CROSS LISTED)</h2>
<h3>Florian Barth, Stefan Funke, Tobias Skovgaard Jepsen, Claudius Proissl</h3>
<p>We present analysis techniques for large trajectory data sets that aim to
provide a semantic understanding of trajectories reaching beyond them being
point sequences in time and space. The presented techniques use a driving
preference model w.r.t. road segment traversal costs, e.g., travel time and
distance, to analyze and explain trajectories.

In particular, we present trajectory mining techniques that can (a) find
interesting points within a trajectory indicating, e.g., a via-point, and (b)
recover the driving preferences of a driver based on their chosen trajectory.
We evaluate our techniques on the tasks of via-point identification and
personalized routing using a data set of more than 1 million vehicle
trajectories collected throughout Denmark during a 3-year period. Our
techniques can be implemented efficiently and are highly parallelizable,
allowing them to scale to millions or billions of trajectories.
</p>
<a href="http://arxiv.org/abs/2011.03331" target="_blank">arXiv:2011.03331</a> [<a href="http://arxiv.org/pdf/2011.03331" target="_blank">pdf</a>]

<h2>What Did You Think Would Happen? Explaining Agent Behaviour Through Intended Outcomes. (arXiv:2011.05064v1 [cs.AI] CROSS LISTED)</h2>
<h3>Herman Yau, Chris Russell, Simon Hadfield,</h3>
<p>We present a novel form of explanation for Reinforcement Learning, based
around the notion of intended outcome. These explanations describe the outcome
an agent is trying to achieve by its actions. We provide a simple proof that
general methods for post-hoc explanations of this nature are impossible in
traditional reinforcement learning. Rather, the information needed for the
explanations must be collected in conjunction with training the agent. We
derive approaches designed to extract local explanations based on intention for
several variants of Q-function approximation and prove consistency between the
explanations and the Q-values learned. We demonstrate our method on multiple
reinforcement learning problems, and provide code to help researchers
introspecting their RL environments and algorithms.
</p>
<a href="http://arxiv.org/abs/2011.05064" target="_blank">arXiv:2011.05064</a> [<a href="http://arxiv.org/pdf/2011.05064" target="_blank">pdf</a>]

