---
title: Latest Deep Learning Papers
date: 2021-03-12 21:00:01 +0800
featured-img: DL-logo
categories: [Deep_Learning]
tags: [Deep_Learning]
mathjax: Yes
---

<h1>Your arXiv Feed (79 Articles)</h1>
<h2>Face Images as Jigsaw Puzzles: Compositional Perception of Human Faces for Machines Using Generative Adversarial Networks. (arXiv:2103.06331v1 [cs.CV])</h2>
<h3>Mahla Abdolahnejad, Peter Xiaoping Liu</h3>
<p>An important goal in human-robot-interaction (HRI) is for machines to achieve
a close to human level of face perception. One of the important differences
between machine learning and human intelligence is the lack of
compositionality. This paper introduces a new scheme to enable generative
adversarial networks to learn the distribution of face images composed of
smaller parts. This results in a more flexible machine face perception and
easier generalization to outside training examples. We demonstrate that this
model is able to produce realistic high-quality face images by generating and
piecing together the parts. Additionally, we demonstrate that this model learns
the relations between the facial parts and their distributions. Therefore, the
specific facial parts are interchangeable between generated face images.
</p>
<a href="http://arxiv.org/abs/2103.06331" target="_blank">arXiv:2103.06331</a> [<a href="http://arxiv.org/pdf/2103.06331" target="_blank">pdf</a>]

<h2>Continual Semantic Segmentation via Repulsion-Attraction of Sparse and Disentangled Latent Representations. (arXiv:2103.06342v1 [cs.CV])</h2>
<h3>Umberto Michieli, Pietro Zanuttigh</h3>
<p>Deep neural networks suffer from the major limitation of catastrophic
forgetting old tasks when learning new ones. In this paper we focus on class
incremental continual learning in semantic segmentation, where new categories
are made available over time while previous training data is not retained. The
proposed continual learning scheme shapes the latent space to reduce forgetting
whilst improving the recognition of novel classes. Our framework is driven by
three novel components which we also combine on top of existing techniques
effortlessly. First, prototypes matching enforces latent space consistency on
old classes, constraining the encoder to produce similar latent representation
for previously seen classes in the subsequent steps. Second, features
sparsification allows to make room in the latent space to accommodate novel
classes. Finally, contrastive learning is employed to cluster features
according to their semantics while tearing apart those of different classes.
Extensive evaluation on the Pascal VOC2012 and ADE20K datasets demonstrates the
effectiveness of our approach, significantly outperforming state-of-the-art
methods.
</p>
<a href="http://arxiv.org/abs/2103.06342" target="_blank">arXiv:2103.06342</a> [<a href="http://arxiv.org/pdf/2103.06342" target="_blank">pdf</a>]

<h2>Hiding Leader's Identity in Leader-Follower Navigation through Multi-Agent Reinforcement Learning. (arXiv:2103.06359v1 [cs.RO])</h2>
<h3>Ankur Deka, Wenhao Luo, Huao Li, Michael Lewis, Katia Sycara</h3>
<p>Leader-follower navigation is a popular class of multi-robot algorithms where
a leader robot leads the follower robots in a team. The leader has specialized
capabilities or mission critical information (e.g. goal location) that the
followers lack which makes the leader crucial for the mission's success.
However, this also makes the leader a vulnerability - an external adversary who
wishes to sabotage the robot team's mission can simply harm the leader and the
whole robot team's mission would be compromised. Since robot motion generated
by traditional leader-follower navigation algorithms can reveal the identity of
the leader, we propose a defense mechanism of hiding the leader's identity by
ensuring the leader moves in a way that behaviorally camouflages it with the
followers, making it difficult for an adversary to identify the leader. To
achieve this, we combine Multi-Agent Reinforcement Learning, Graph Neural
Networks and adversarial training. Our approach enables the multi-robot team to
optimize the primary task performance with leader motion similar to follower
motion, behaviorally camouflaging it with the followers. Our algorithm
outperforms existing work that tries to hide the leader's identity in a
multi-robot team by tuning traditional leader-follower control parameters with
Classical Genetic Algorithms. We also evaluated human performance in inferring
the leader's identity and found that humans had lower accuracy when the robot
team used our proposed navigation algorithm.
</p>
<a href="http://arxiv.org/abs/2103.06359" target="_blank">arXiv:2103.06359</a> [<a href="http://arxiv.org/pdf/2103.06359" target="_blank">pdf</a>]

<h2>Structure-From-Motion and RGBD Depth Fusion. (arXiv:2103.06366v1 [cs.CV])</h2>
<h3>Akash Chandrashekar, John Papadakis, Andrew Willis, Jamie Gantert</h3>
<p>This article describes a technique to augment a typical RGBD sensor by
integrating depth estimates obtained via Structure-from-Motion (SfM) with
sensor depth measurements. Limitations in the RGBD depth sensing technology
prevent capturing depth measurements in four important contexts: (1) distant
surfaces (&gt;5m), (2) dark surfaces, (3) brightly lit indoor scenes and (4)
sunlit outdoor scenes. SfM technology computes depth via multi-view
reconstruction from the RGB image sequence alone. As such, SfM depth estimates
do not suffer the same limitations and may be computed in all four of the
previously listed circumstances. This work describes a novel fusion of RGBD
depth data and SfM-estimated depths to generate an improved depth stream that
may be processed by one of many important downstream applications such as
robotic localization and mapping, as well as object recognition and tracking.
</p>
<a href="http://arxiv.org/abs/2103.06366" target="_blank">arXiv:2103.06366</a> [<a href="http://arxiv.org/pdf/2103.06366" target="_blank">pdf</a>]

<h2>PANTHER: Perception-Aware Trajectory Planner in Dynamic Environments. (arXiv:2103.06372v1 [cs.RO])</h2>
<h3>Jesus Tordesillas, Jonathan P. How</h3>
<p>This paper presents PANTHER, a real-time perception-aware (PA) trajectory
planner in dynamic environments. PANTHER plans trajectories that avoid dynamic
obstacles while also keeping them in the sensor field of view (FOV) and
minimizing the blur to aid in object tracking. The rotation and translation of
the UAV are jointly optimized, which allows PANTHER to fully exploit the
differential flatness of multirotors. Real-time performance is achieved by
implicitly imposing this constraint through the Hopf fibration. PANTHER is able
to keep the obstacles inside the FOV 7.4 and 1.5 times more than non-PA
approaches and PA approaches that decouple translation and yaw, respectively.
The projected velocity (and hence the blur) is reduced by 30%. Our
recently-derived MINVO basis is used to impose low-conservative collision
avoidance constraints in position and velocity space. Finally, extensive
hardware experiments in unknown dynamic environments with all the computation
running onboard are presented, with velocities of up to 5.8 m/s, and with
relative velocities (with respect to the obstacles) of up to 6.3 m/s. The only
sensors used are an IMU, a forward-facing depth camera, and a downward-facing
monocular camera.
</p>
<a href="http://arxiv.org/abs/2103.06372" target="_blank">arXiv:2103.06372</a> [<a href="http://arxiv.org/pdf/2103.06372" target="_blank">pdf</a>]

<h2>Holistic 3D Scene Understanding from a Single Image with Implicit Representation. (arXiv:2103.06422v1 [cs.CV])</h2>
<h3>Cheng Zhang, Zhaopeng Cui, Yinda Zhang, Bing Zeng, Marc Pollefeys, Shuaicheng Liu</h3>
<p>We present a new pipeline for holistic 3D scene understanding from a single
image, which could predict object shape, object pose, and scene layout. As it
is a highly ill-posed problem, existing methods usually suffer from inaccurate
estimation of both shapes and layout especially for the cluttered scene due to
the heavy occlusion between objects. We propose to utilize the latest deep
implicit representation to solve this challenge. We not only propose an
image-based local structured implicit network to improve the object shape
estimation, but also refine 3D object pose and scene layout via a novel
implicit scene graph neural network that exploits the implicit local object
features. A novel physical violation loss is also proposed to avoid incorrect
context between objects. Extensive experiments demonstrate that our method
outperforms the state-of-the-art methods in terms of object shape, scene layout
estimation, and 3D object detection.
</p>
<a href="http://arxiv.org/abs/2103.06422" target="_blank">arXiv:2103.06422</a> [<a href="http://arxiv.org/pdf/2103.06422" target="_blank">pdf</a>]

<h2>Robust 2D/3D Vehicle Parsing in CVIS. (arXiv:2103.06432v1 [cs.CV])</h2>
<h3>Hui Miao, Feixiang Lu, Zongdai Liu, Liangjun Zhang, Dinesh Manocha, Bin Zhou</h3>
<p>We present a novel approach to robustly detect and perceive vehicles in
different camera views as part of a cooperative vehicle-infrastructure system
(CVIS). Our formulation is designed for arbitrary camera views and makes no
assumptions about intrinsic or extrinsic parameters. First, to deal with
multi-view data scarcity, we propose a part-assisted novel view synthesis
algorithm for data augmentation. We train a part-based texture inpainting
network in a self-supervised manner. Then we render the textured model into the
background image with the target 6-DoF pose. Second, to handle various camera
parameters, we present a new method that produces dense mappings between image
pixels and 3D points to perform robust 2D/3D vehicle parsing. Third, we build
the first CVIS dataset for benchmarking, which annotates more than 1540 images
(14017 instances) from real-world traffic scenarios. We combine these novel
algorithms and datasets to develop a robust approach for 2D/3D vehicle parsing
for CVIS. In practice, our approach outperforms SOTA methods on 2D detection,
instance segmentation, and 6-DoF pose estimation, by 4.5%, 4.3%, and 2.9%,
respectively. More details and results are included in the supplement. To
facilitate future research, we will release the source code and the dataset on
GitHub.
</p>
<a href="http://arxiv.org/abs/2103.06432" target="_blank">arXiv:2103.06432</a> [<a href="http://arxiv.org/pdf/2103.06432" target="_blank">pdf</a>]

<h2>Hierarchical Bayesian Model for the Transfer of Knowledge on Spatial Concepts based on Multimodal Information. (arXiv:2103.06442v1 [cs.RO])</h2>
<h3>Yoshinobu Hagiwara, Keishiro Taguchi, Satoshi Ishibushi, Akira Taniguchi, Tadahiro Taniguchi</h3>
<p>This paper proposes a hierarchical Bayesian model based on spatial concepts
that enables a robot to transfer the knowledge of places from experienced
environments to a new environment. The transfer of knowledge based on spatial
concepts is modeled as the calculation process of the posterior distribution
based on the observations obtained in each environment with the parameters of
spatial concepts generalized to environments as prior knowledge. We conducted
experiments to evaluate the generalization performance of spatial knowledge for
general places such as kitchens and the adaptive performance of spatial
knowledge for unique places such as `Emma's room' in a new environment. In the
experiments, the accuracies of the proposed method and conventional methods
were compared in the prediction task of location names from an image and a
position, and the prediction task of positions from a location name. The
experimental results demonstrated that the proposed method has a higher
prediction accuracy of location names and positions than the conventional
method owing to the transfer of knowledge.
</p>
<a href="http://arxiv.org/abs/2103.06442" target="_blank">arXiv:2103.06442</a> [<a href="http://arxiv.org/pdf/2103.06442" target="_blank">pdf</a>]

<h2>Where is your place, Visual Place Recognition?. (arXiv:2103.06443v1 [cs.RO])</h2>
<h3>Sourav Garg, Tobias Fischer, Michael Milford</h3>
<p>Visual Place Recognition (VPR) is often characterized as being able to
recognize the same place despite significant changes in appearance and
viewpoint. VPR is a key component of Spatial Artificial Intelligence, enabling
robotic platforms and intelligent augmentation platforms such as augmented
reality devices to perceive and understand the physical world. In this paper,
we observe that there are three "drivers" that impose requirements on spatially
intelligent agents and thus VPR systems: 1) the particular agent including its
sensors and computational resources, 2) the operating environment of this
agent, and 3) the specific task that the artificial agent carries out. In this
paper, we characterize and survey key works in the VPR area considering those
drivers, including their place representation and place matching choices. We
also provide a new definition of VPR based on the visual overlap -- akin to
spatial view cells in the brain -- that enables us to find similarities and
differences to other research areas in the robotics and computer vision fields.
We identify numerous open challenges and suggest areas that require more
in-depth attention in future works.
</p>
<a href="http://arxiv.org/abs/2103.06443" target="_blank">arXiv:2103.06443</a> [<a href="http://arxiv.org/pdf/2103.06443" target="_blank">pdf</a>]

<h2>Self-Supervised Motion Retargeting with Safety Guarantee. (arXiv:2103.06447v1 [cs.RO])</h2>
<h3>Sungjoon Choi, Min Jae Song, Hyemin Ahn, Joohyung Kim</h3>
<p>In this paper, we present self-supervised shared latent embedding (S3LE), a
data-driven motion retargeting method that enables the generation of natural
motions in humanoid robots from motion capture data or RGB videos. While it
requires paired data consisting of human poses and their corresponding robot
configurations, it significantly alleviates the necessity of time-consuming
data-collection via novel paired data generating processes. Our self-supervised
learning procedure consists of two steps: automatically generating paired data
to bootstrap the motion retargeting, and learning a projection-invariant
mapping to handle the different expressivity of humans and humanoid robots.
Furthermore, our method guarantees that the generated robot pose is
collision-free and satisfies position limits by utilizing nonparametric
regression in the shared latent space. We demonstrate that our method can
generate expressive robotic motions from both the CMU motion capture database
and YouTube videos.
</p>
<a href="http://arxiv.org/abs/2103.06447" target="_blank">arXiv:2103.06447</a> [<a href="http://arxiv.org/pdf/2103.06447" target="_blank">pdf</a>]

<h2>Full Page Handwriting Recognition via Image to Sequence Extraction. (arXiv:2103.06450v1 [cs.CV])</h2>
<h3>Sumeet S. Singh, Sergey Karayev</h3>
<p>We present a Neural Network based Handwritten Text Recognition (HTR) model
architecture that can be trained to recognize full pages of handwritten or
printed text without image segmentation. Being based on an Image to Sequence
architecture, it can be trained to extract text present in an image and
sequence it correctly without imposing any constraints on language, shape of
characters or orientation and layout of text and non-text. The model can also
be trained to generate auxiliary markup related to formatting, layout and
content. We use character level token vocabulary, thereby supporting proper
nouns and terminology of any subject. The model achieves a new state-of-art in
full page recognition on the IAM dataset and when evaluated on scans of real
world handwritten free form test answers - a dataset beset with curved and
slanted lines, drawings, tables, math, chemistry and other symbols - it
performs better than all commercially available HTR APIs. It is deployed in
production as part of a commercial web application.
</p>
<a href="http://arxiv.org/abs/2103.06450" target="_blank">arXiv:2103.06450</a> [<a href="http://arxiv.org/pdf/2103.06450" target="_blank">pdf</a>]

<h2>Dynamic Grasping with a "Soft" Drone: From Theory to Practice. (arXiv:2103.06465v1 [cs.RO])</h2>
<h3>Joshua Fishman, Samuel Ubellacker, Nathan Hughes, Luca Carlone</h3>
<p>Rigid grippers used in existing aerial manipulators require precise
positioning to achieve successful grasps and transmit large contact forces that
may destabilize the drone. This limits the speed during grasping and prevents
"dynamic grasping", where the drone attempts to grasp an object while moving.
On the other hand, biological systems (e.g., birds) rely on compliant and soft
parts to dampen contact forces and compensate for grasping inaccuracy, enabling
impressive feats. This paper presents the first prototype of a soft drone -- a
quadrotor where traditional (i.e., rigid) landing gears are replaced with a
soft tendon-actuated gripper to enable aggressive grasping. We provide three
key contributions. First, we describe our soft drone prototype, including
electro-mechanical design, software infrastructure, and fabrication. Second, we
review the set of algorithms we use for trajectory optimization and control of
the drone and the soft gripper; the algorithms combine state-of-the-art
techniques for quadrotor control (i.e., an adaptive geometric controller) with
advanced soft robotics models (i.e., a quasi-static finite element model).
Finally, we evaluate our soft drone in physics simulations (using SOFA and
Unity) and in real tests in a motion-capture room. Our drone is able to
dynamically grasp objects of unknown shape where baseline approaches fail. Our
physical prototype ensures consistent performance, achieving 91.7% successful
grasps across 23 trials. We showcase dynamic grasping results in the video
attachment.
</p>
<a href="http://arxiv.org/abs/2103.06465" target="_blank">arXiv:2103.06465</a> [<a href="http://arxiv.org/pdf/2103.06465" target="_blank">pdf</a>]

<h2>Imitation learning for variable speed motion generation over multiple actions. (arXiv:2103.06466v1 [cs.RO])</h2>
<h3>Yuki Saigusa, Ayumu Sasagawa, Sho Sakaino, Toshiaki Tsuji</h3>
<p>Robot motion generation methods using machine learning have been studied in
recent years. Bilateral controlbased imitation learning can imitate human
motions using force information. By means of this method, variable speed motion
generation that considers physical phenomena such as the inertial force and
friction can be achieved. Previous research demonstrated that the complex
relationship between the force and speed can be learned by using a neural
network model. However, the previous study only focused on a simple
reciprocating motion. To learn the complex relationship between the force and
speed more accurately, it is necessary to learn multiple actions using many
joints. In this paper, we propose a variable speed motion generation method for
multiple motions. We considered four types of neural network models for the
motion generation and determined the best model for multiple motions at
variable speeds. Subsequently, we used the best model to evaluate the
reproducibility of the task completion time for the input completion time
command. The results revealed that the proposed method could change the task
completion time according to the specified completion time command in multiple
motions.
</p>
<a href="http://arxiv.org/abs/2103.06466" target="_blank">arXiv:2103.06466</a> [<a href="http://arxiv.org/pdf/2103.06466" target="_blank">pdf</a>]

<h2>Pavement Distress Detection and Segmentation using YOLOv4 and DeepLabv3 on Pavements in the Philippines. (arXiv:2103.06467v1 [cs.CV])</h2>
<h3>James-Andrew Sarmiento</h3>
<p>Road transport infrastructure is critical for safe, fast, economical, and
reliable mobility within the whole country that is conducive to a productive
society. However, roads tend to deteriorate over time due to natural causes in
the environment and repeated traffic loads. Pavement Distress (PD) detection is
essential in monitoring the current conditions of the public roads to enable
targeted rehabilitation and preventive maintenance. Nonetheless, distress
detection surveys are still done via manual inspection for developing countries
such as the Philippines. This study proposed the use of deep learning for two
ways of recording pavement distresses from 2D RGB images - detection and
segmentation. YOLOv4 is used for pavement distress detection while DeepLabv3 is
employed for pavement distress segmentation on a small dataset of pavement
images in the Philippines. This study aims to provide a basis to potentially
spark solutions in building a cheap, scalable, and automated end-to-end
solution for PD detection in the country.
</p>
<a href="http://arxiv.org/abs/2103.06467" target="_blank">arXiv:2103.06467</a> [<a href="http://arxiv.org/pdf/2103.06467" target="_blank">pdf</a>]

<h2>Robust High-speed Running for Quadruped Robots via Deep Reinforcement Learning. (arXiv:2103.06484v1 [cs.RO])</h2>
<h3>Guillaume Bellegarda, Quan Nguyen</h3>
<p>Deep reinforcement learning has emerged as a popular and powerful way to
develop locomotion controllers for quadruped robots. Common approaches have
largely focused on learning actions directly in joint space, or learning to
modify and offset foot positions produced by trajectory generators. Both
approaches typically require careful reward shaping and training for millions
of time steps, and with trajectory generators introduce human bias into the
resulting control policies. In this paper, we instead explore learning foot
positions in Cartesian space, which we track with impedance control, for a task
of running as fast as possible subject to environmental disturbances. Compared
with other action spaces, we observe less needed reward shaping, much improved
sample efficiency, the emergence of natural gaits such as galloping and
bounding, and ease of sim-to-sim transfer. Policies can be learned in only a
few million time steps, even for challenging tasks of running over rough
terrain with loads of over 100% of the nominal quadruped mass. Training occurs
in PyBullet, and we perform a sim-to-sim transfer to Gazebo, where our
quadruped is able to run at over 4 m/s without a load, and 3.5 m/s with a 10 kg
load, which is over 83% of the nominal quadruped mass. Video results can be
found at https://youtu.be/roE1vxpEWfw.
</p>
<a href="http://arxiv.org/abs/2103.06484" target="_blank">arXiv:2103.06484</a> [<a href="http://arxiv.org/pdf/2103.06484" target="_blank">pdf</a>]

<h2>DAFAR: Detecting Adversaries by Feedback-Autoencoder Reconstruction. (arXiv:2103.06487v1 [cs.CV])</h2>
<h3>Haowen Liu, Ping Yi, Hsiao-Ying Lin, Jie Shi</h3>
<p>Deep learning has shown impressive performance on challenging perceptual
tasks. However, researchers found deep neural networks vulnerable to
adversarial examples. Since then, many methods are proposed to defend against
or detect adversarial examples, but they are either attack-dependent or shown
to be ineffective with new attacks.

We propose DAFAR, a feedback framework that allows deep learning models to
detect adversarial examples in high accuracy and universality. DAFAR has a
relatively simple structure, which contains a target network, a plug-in
feedback network and an autoencoder-based detector. The key idea is to capture
the high-level features extracted by the target network, and then reconstruct
the input using the feedback network. These two parts constitute a feedback
autoencoder. It transforms the imperceptible-perturbation attack on the target
network directly into obvious reconstruction-error attack on the feedback
autoencoder. Finally the detector gives an anomaly score and determines whether
the input is adversarial according to the reconstruction errors. Experiments
are conducted on MNIST and CIFAR-10 data-sets. Experimental results show that
DAFAR is effective against popular and arguably most advanced attacks without
losing performance on legitimate samples, with high accuracy and universality
across attack methods and parameters.
</p>
<a href="http://arxiv.org/abs/2103.06487" target="_blank">arXiv:2103.06487</a> [<a href="http://arxiv.org/pdf/2103.06487" target="_blank">pdf</a>]

<h2>Read Like Humans: Autonomous, Bidirectional and Iterative Language Modeling for Scene Text Recognition. (arXiv:2103.06495v1 [cs.CV])</h2>
<h3>Shancheng Fang, Hongtao Xie, Yuxin Wang, Zhendong Mao, Yongdong Zhang</h3>
<p>Linguistic knowledge is of great benefit to scene text recognition. However,
how to effectively model linguistic rules in end-to-end deep networks remains a
research challenge. In this paper, we argue that the limited capacity of
language models comes from: 1) implicitly language modeling; 2) unidirectional
feature representation; and 3) language model with noise input.
Correspondingly, we propose an autonomous, bidirectional and iterative ABINet
for scene text recognition. Firstly, the autonomous suggests to block gradient
flow between vision and language models to enforce explicitly language
modeling. Secondly, a novel bidirectional cloze network (BCN) as the language
model is proposed based on bidirectional feature representation. Thirdly, we
propose an execution manner of iterative correction for language model which
can effectively alleviate the impact of noise input. Additionally, based on the
ensemble of iterative predictions, we propose a self-training method which can
learn from unlabeled images effectively. Extensive experiments indicate that
ABINet has superiority on low-quality images and achieves state-of-the-art
results on several mainstream benchmarks. Besides, the ABINet trained with
ensemble self-training shows promising improvement in realizing human-level
recognition. Code is available at https://github.com/FangShancheng/ABINet.
</p>
<a href="http://arxiv.org/abs/2103.06495" target="_blank">arXiv:2103.06495</a> [<a href="http://arxiv.org/pdf/2103.06495" target="_blank">pdf</a>]

<h2>3D Human Pose, Shape and Texture from Low-Resolution Images and Videos. (arXiv:2103.06498v1 [cs.CV])</h2>
<h3>Xiangyu Xu, Hao Chen, Francesc Moreno-Noguer, Laszlo A. Jeni, Fernando De la Torre</h3>
<p>3D human pose and shape estimation from monocular images has been an active
research area in computer vision. Existing deep learning methods for this task
rely on high-resolution input, which however, is not always available in many
scenarios such as video surveillance and sports broadcasting. Two common
approaches to deal with low-resolution images are applying super-resolution
techniques to the input, which may result in unpleasant artifacts, or simply
training one model for each resolution, which is impractical in many realistic
applications.

To address the above issues, this paper proposes a novel algorithm called
RSC-Net, which consists of a Resolution-aware network, a Self-supervision loss,
and a Contrastive learning scheme. The proposed method is able to learn 3D body
pose and shape across different resolutions with one single model. The
self-supervision loss enforces scale-consistency of the output, and the
contrastive learning scheme enforces scale-consistency of the deep features. We
show that both these new losses provide robustness when learning in a
weakly-supervised manner. Moreover, we extend the RSC-Net to handle
low-resolution videos and apply it to reconstruct textured 3D pedestrians from
low-resolution input. Extensive experiments demonstrate that the RSC-Net can
achieve consistently better results than the state-of-the-art methods for
challenging low-resolution images.
</p>
<a href="http://arxiv.org/abs/2103.06498" target="_blank">arXiv:2103.06498</a> [<a href="http://arxiv.org/pdf/2103.06498" target="_blank">pdf</a>]

<h2>Level-aware Haze Image Synthesis by Self-Supervised Content-Style Disentanglement. (arXiv:2103.06501v1 [cs.CV])</h2>
<h3>Chi Zhang, Zihang Lin, Liheng Xu, Zongliang Li, Le Wang, Yuehu Liu, Gaofeng Meng, Li Li, Nanning Zheng</h3>
<p>The key procedure of haze image translation through adversarial training lies
in the disentanglement between the feature only involved in haze synthesis,
i.e.style feature, and the feature representing the invariant semantic content,
i.e. content feature. Previous methods separate content feature apart by
utilizing it to classify haze image during the training process. However, in
this paper we recognize the incompleteness of the content-style disentanglement
in such technical routine. The flawed style feature entangled with content
information inevitably leads the ill-rendering of the haze images. To address,
we propose a self-supervised style regression via stochastic linear
interpolation to reduce the content information in style feature. The ablative
experiments demonstrate the disentangling completeness and its superiority in
level-aware haze image synthesis. Moreover, the generated haze data are applied
in the testing generalization of vehicle detectors. Further study between
haze-level and detection performance shows that haze has obvious impact on the
generalization of the vehicle detectors and such performance degrading level is
linearly correlated to the haze-level, which, in turn, validates the
effectiveness of the proposed method.
</p>
<a href="http://arxiv.org/abs/2103.06501" target="_blank">arXiv:2103.06501</a> [<a href="http://arxiv.org/pdf/2103.06501" target="_blank">pdf</a>]

<h2>Instance Segmentation GNNs for One-Shot Conformal Tracking at the LHC. (arXiv:2103.06509v1 [cs.CV])</h2>
<h3>Savannah Thais, Gage DeZoort</h3>
<p>3D instance segmentation remains a challenging problem in computer vision.
Particle tracking at colliders like the LHC can be conceptualized as an
instance segmentation task: beginning from a point cloud of hits in a particle
detector, an algorithm must identify which hits belong to individual particle
trajectories and extract track properties. Graph Neural Networks (GNNs) have
shown promising performance on standard instance segmentation tasks. In this
work we demonstrate the applicability of instance segmentation GNN
architectures to particle tracking; moreover, we re-imagine the traditional
Cartesian space approach to track-finding and instead work in a conformal
geometry that allows the GNN to identify tracks and extract parameters in a
single shot.
</p>
<a href="http://arxiv.org/abs/2103.06509" target="_blank">arXiv:2103.06509</a> [<a href="http://arxiv.org/pdf/2103.06509" target="_blank">pdf</a>]

<h2>Fast-Tracker 2.0: Improving Autonomy of Aerial Tracking with Active Vision and Human Location Regression. (arXiv:2103.06522v1 [cs.RO])</h2>
<h3>Neng Pan, Ruibin Zhang, Tiankai Yang, Chao Xu, Fei Gao</h3>
<p>In recent years, several progressive works promote the development of aerial
tracking. One of the representative works is our previous work Fast-tracker
which is applicable to various challenging tracking scenarios. However, it
suffers from two main drawbacks: 1) the over simplification in target detection
by using artificial markers and 2) the contradiction between simultaneous
target and environment perception with limited onboard vision. In this paper,
we upgrade the target detection in Fast-tracker to detect and localize a human
target based on deep learning and non-linear regression to solve the former
problem. For the latter one, we equip the quadrotor system with 360 degree
active vision on a customized gimbal camera. Furthermore, we improve the
tracking trajectory planning in Fast-tracker by incorporating an
occlusion-aware mechanism that generates observable tracking trajectories.
Comprehensive real-world tests confirm the proposed system's robustness and
real-time capability. Benchmark comparisons with Fast-tracker validate that the
proposed system presents better tracking performance even when performing more
difficult tracking tasks.
</p>
<a href="http://arxiv.org/abs/2103.06522" target="_blank">arXiv:2103.06522</a> [<a href="http://arxiv.org/pdf/2103.06522" target="_blank">pdf</a>]

<h2>DualPoseNet: Category-level 6D Object Pose and Size Estimation using Dual Pose Network with Refined Learning of Pose Consistency. (arXiv:2103.06526v1 [cs.CV])</h2>
<h3>Jiehong Lin, Zewei Wei, Zhihao Li, Songcen Xu, Kui Jia, Yuanqing Li</h3>
<p>Category-level 6D object pose and size estimation is to predict 9
degrees-of-freedom (9DoF) pose configurations of rotation, translation, and
size for object instances observed in single, arbitrary views of cluttered
scenes. It extends previous related tasks with learning of the two additional
rotation angles. This seemingly small difference poses technical challenges due
to the learning and prediction in the full rotation space of SO(3). In this
paper, we propose a new method of Dual Pose Network with refined learning of
pose consistency for this task, shortened as DualPoseNet. DualPoseNet stacks
two parallel pose decoders on top of a shared pose encoder, where the implicit
decoder predicts object poses with a working mechanism different from that of
the explicit one; they thus impose complementary supervision on the training of
pose encoder. We construct the encoder based on spherical convolutions, and
design a module of Spherical Fusion wherein for a better embedding of
pose-sensitive features from the appearance and shape observations. Given no
the testing CAD models, it is the novel introduction of the implicit decoder
that enables the refined pose prediction during testing, by enforcing the
predicted pose consistency between the two decoders using a self-adaptive loss
term. Thorough experiments on the benchmark 9DoF object pose datasets of
CAMERA25 and REAL275 confirm efficacy of our designs. DualPoseNet outperforms
existing methods with a large margin in the regime of high precision.
</p>
<a href="http://arxiv.org/abs/2103.06526" target="_blank">arXiv:2103.06526</a> [<a href="http://arxiv.org/pdf/2103.06526" target="_blank">pdf</a>]

<h2>Triple-cooperative Video Shadow Detection. (arXiv:2103.06533v1 [cs.CV])</h2>
<h3>Zhihao Chen, Liang Wan, Lei Zhu, Jia Shen, Huazhu Fu, Wennan Liu, Jing Qin</h3>
<p>Shadow detection in a single image has received significant research interest
in recent years. However, much fewer works have been explored in shadow
detection over dynamic scenes. The bottleneck is the lack of a well-established
dataset with high-quality annotations for video shadow detection. In this work,
we collect a new video shadow detection dataset, which contains 120 videos with
11, 685 frames, covering 60 object categories, varying lengths, and different
motion/lighting conditions. All the frames are annotated with a high-quality
pixel-level shadow mask. To the best of our knowledge, this is the first
learning-oriented dataset for video shadow detection. Furthermore, we develop a
new baseline model, named triple-cooperative video shadow detection network
(TVSD-Net). It utilizes triple parallel networks in a cooperative manner to
learn discriminative representations at intra-video and inter-video levels.
Within the network, a dual gated co-attention module is proposed to constrain
features from neighboring frames in the same video, while an auxiliary
similarity loss is introduced to mine semantic information between different
videos. Finally, we conduct a comprehensive study on ViSha, evaluating 12
state-of-the-art models (including single image shadow detectors, video object
segmentation, and saliency detection methods). Experiments demonstrate that our
model outperforms SOTA competitors.
</p>
<a href="http://arxiv.org/abs/2103.06533" target="_blank">arXiv:2103.06533</a> [<a href="http://arxiv.org/pdf/2103.06533" target="_blank">pdf</a>]

<h2>Calibrated and Partially Calibrated Semi-Generalized Homographies. (arXiv:2103.06535v1 [cs.CV])</h2>
<h3>Snehal Bhayani, Torsten Sattler, Daniel Barath, Patrik Beliansky, Janne Heikkila, Zuzana Kukelova</h3>
<p>In this paper, we propose the first minimal solutions for estimating the
semi-generalized homography given a perspective and a generalized camera. The
proposed solvers use five 2D-2D image point correspondences induced by a scene
plane. One of them assumes the perspective camera to be fully calibrated, while
the other solver estimates the unknown focal length together with the absolute
pose parameters. This setup is particularly important in structure-from-motion
and image-based localization pipelines, where a new camera is localized in each
step with respect to a set of known cameras and 2D-3D correspondences might not
be available. As a consequence of a clever parametrization and the elimination
ideal method, our approach only needs to solve a univariate polynomial of
degree five or three. The proposed solvers are stable and efficient as
demonstrated by a number of synthetic and real-world experiments.
</p>
<a href="http://arxiv.org/abs/2103.06535" target="_blank">arXiv:2103.06535</a> [<a href="http://arxiv.org/pdf/2103.06535" target="_blank">pdf</a>]

<h2>Affect2MM: Affective Analysis of Multimedia Content Using Emotion Causality. (arXiv:2103.06541v1 [cs.CV])</h2>
<h3>Trisha Mittal, Puneet Mathur, Aniket Bera, Dinesh Manocha</h3>
<p>We present Affect2MM, a learning method for time-series emotion prediction
for multimedia content. Our goal is to automatically capture the varying
emotions depicted by characters in real-life human-centric situations and
behaviors. We use the ideas from emotion causation theories to computationally
model and determine the emotional state evoked in clips of movies. Affect2MM
explicitly models the temporal causality using attention-based methods and
Granger causality. We use a variety of components like facial features of
actors involved, scene understanding, visual aesthetics, action/situation
description, and movie script to obtain an affective-rich representation to
understand and perceive the scene. We use an LSTM-based learning model for
emotion perception. To evaluate our method, we analyze and compare our
performance on three datasets, SENDv1, MovieGraphs, and the LIRIS-ACCEDE
dataset, and observe an average of 10-15% increase in the performance over SOTA
methods for all three datasets.
</p>
<a href="http://arxiv.org/abs/2103.06541" target="_blank">arXiv:2103.06541</a> [<a href="http://arxiv.org/pdf/2103.06541" target="_blank">pdf</a>]

<h2>Programming Robot Behaviors with Execution Management Functions. (arXiv:2103.06545v1 [cs.RO])</h2>
<h3>Martin Molina, Pablo Santamaria, Abraham Carrera</h3>
<p>The control architecture of autonomous robots can be developed by programming
and integrating multiple software components that individually control separate
behaviors. This approach requires additional mechanisms to coordinate their
concurrent execution. This paper presents a programming method for such
components that has been designed to facilitate their coordinated execution.
Each component is programmed as a module that controls a separate robot
behavior together with a set of functions for execution management. The details
of this proposal are formulated in the form of a ROS-based software library
called \textit{behaviorlib}. This solution has been used to program general
behavior controllers that have been successfully reused to build multiple
applications in aerial robotics.
</p>
<a href="http://arxiv.org/abs/2103.06545" target="_blank">arXiv:2103.06545</a> [<a href="http://arxiv.org/pdf/2103.06545" target="_blank">pdf</a>]

<h2>Integrated Age Estimation Mechanism. (arXiv:2103.06546v1 [cs.CV])</h2>
<h3>Fan Li, Yongming Li, Pin Wang, Jie Xiao, Fang Yan, Xinke Li</h3>
<p>Machine-learning-based age estimation has received lots of attention.
Traditional age estimation mechanism focuses estimation age error, but ignores
that there is a deviation between the estimated age and real age due to
disease. Pathological age estimation mechanism the author proposed before
introduces age deviation to solve the above problem and improves classification
capability of the estimated age significantly. However,it does not consider the
age estimation error of the normal control (NC) group and results in a larger
error between the estimated age and real age of NC group. Therefore, an
integrated age estimation mechanism based on Decision-Level fusion of error and
deviation orientation model is proposed to solve the problem.Firstly, the
traditional age estimation and pathological age estimation mechanisms are
weighted together.Secondly, their optimal weights are obtained by minimizing
mean absolute error (MAE) between the estimated age and real age of normal
people. In the experimental section, several representative age-related
datasets are used for verification of the proposed method. The results show
that the proposed age estimation mechanism achieves a good tradeoff effect of
age estimation. It not only improves the classification ability of the
estimated age, but also reduces the age estimation error of the NC group. In
general, the proposed age estimation mechanism is effective. Additionally, the
mechanism is a framework mechanism that can be used to construct different
specific age estimation algorithms, contributing to relevant research.
</p>
<a href="http://arxiv.org/abs/2103.06546" target="_blank">arXiv:2103.06546</a> [<a href="http://arxiv.org/pdf/2103.06546" target="_blank">pdf</a>]

<h2>PREPRINT: Comparison of deep learning and hand crafted features for mining simulation data. (arXiv:2103.06552v1 [cs.CV])</h2>
<h3>Theodoros Georgiou, Sebastian Schmitt, Thomas B&#xe4;ck, Nan Pu, Wei Chen, Michael Lew</h3>
<p>Computational Fluid Dynamics (CFD) simulations are a very important tool for
many industrial applications, such as aerodynamic optimization of engineering
designs like cars shapes, airplanes parts etc. The output of such simulations,
in particular the calculated flow fields, are usually very complex and hard to
interpret for realistic three-dimensional real-world applications, especially
if time-dependent simulations are investigated. Automated data analysis methods
are warranted but a non-trivial obstacle is given by the very large
dimensionality of the data. A flow field typically consists of six measurement
values for each point of the computational grid in 3D space and time (velocity
vector values, turbulent kinetic energy, pressure and viscosity). In this paper
we address the task of extracting meaningful results in an automated manner
from such high dimensional data sets. We propose deep learning methods which
are capable of processing such data and which can be trained to solve relevant
tasks on simulation data, i.e. predicting drag and lift forces applied on an
airfoil. We also propose an adaptation of the classical hand crafted features
known from computer vision to address the same problem and compare a large
variety of descriptors and detectors. Finally, we compile a large dataset of 2D
simulations of the flow field around airfoils which contains 16000 flow fields
with which we tested and compared approaches. Our results show that the deep
learning-based methods, as well as hand crafted feature based approaches, are
well-capable to accurately describe the content of the CFD simulation output on
the proposed dataset.
</p>
<a href="http://arxiv.org/abs/2103.06552" target="_blank">arXiv:2103.06552</a> [<a href="http://arxiv.org/pdf/2103.06552" target="_blank">pdf</a>]

<h2>WenLan: Bridging Vision and Language by Large-Scale Multi-Modal Pre-Training. (arXiv:2103.06561v1 [cs.CV])</h2>
<h3>Yuqi Huo, Manli Zhang, Guangzhen Liu, Haoyu Lu, Yizhao Gao, Guoxing Yang, Jingyuan Wen, Heng Zhang, Baogui Xu, Weihao Zheng, Zongzheng Xi, Yueqian Yang, Anwen Hu, Jinming Zhao, Ruichen Li, Yida Zhao, Liang Zhang, Yuqing Song, Xin Hong, Wanqing Cui, Danyang Hou, Yingyan Li, Junyi Li, Peiyu Liu, Zheng Gong, Chuhao Jin, Yuchong Sun, Shizhe Chen, Zhiwu Lu, Zhicheng Dou, Qin Jin, Yanyan Lan, Wayne Xin Zhao, Ruihua Song, Ji-Rong Wen</h3>
<p>Multi-modal pre-training models have been intensively explored to bridge
vision and language in recent years. However, most of them explicitly model the
cross-modal interaction between image-text pairs, by assuming that there exists
strong semantic correlation between the text and image modalities. Since this
strong assumption is often invalid in real-world scenarios, we choose to
implicitly model the cross-modal correlation for large-scale multi-modal
pre-training, which is the focus of the Chinese project `WenLan' led by our
team. Specifically, with the weak correlation assumption over image-text pairs,
we propose a two-tower pre-training model within the cross-modal contrastive
learning (CMCL) framework. Unlike OpenAI CLIP that adopts a simple contrastive
learning method, we devise a more advanced algorithm by adapting the latest
method MoCo into the cross-modal scenario. By building a large queue-based
dictionary, our CMCL can incorporate more negative samples in limited GPU
resources. We further construct a large Chinese multi-source image-text dataset
called RUC-CAS-WenLan for pre-training our CMCL model. Extensive experiments
demonstrate that the pre-trained CMCL model outperforms both UNITER and OpenAI
CLIP on various downstream tasks.
</p>
<a href="http://arxiv.org/abs/2103.06561" target="_blank">arXiv:2103.06561</a> [<a href="http://arxiv.org/pdf/2103.06561" target="_blank">pdf</a>]

<h2>PointFlow: Flowing Semantics Through Points for Aerial Image Segmentation. (arXiv:2103.06564v1 [cs.CV])</h2>
<h3>Xiangtai Li, Hao He, Xia Li, Duo Li, Guangliang Cheng, Jianping Shi, Lubin Weng, Yunhai Tong, Zhouchen Lin</h3>
<p>Aerial Image Segmentation is a particular semantic segmentation problem and
has several challenging characteristics that general semantic segmentation does
not have. There are two critical issues: The one is an extremely
foreground-background imbalanced distribution, and the other is multiple small
objects along with the complex background. Such problems make the recent dense
affinity context modeling perform poorly even compared with baselines due to
over-introduced background context. To handle these problems, we propose a
point-wise affinity propagation module based on the Feature Pyramid Network
(FPN) framework, named PointFlow. Rather than dense affinity learning, a sparse
affinity map is generated upon selected points between the adjacent features,
which reduces the noise introduced by the background while keeping efficiency.
In particular, we design a dual point matcher to select points from the salient
area and object boundaries, respectively. Experimental results on three
different aerial segmentation datasets suggest that the proposed method is more
effective and efficient than state-of-the-art general semantic segmentation
methods. Especially, our methods achieve the best speed and accuracy trade-off
on three aerial benchmarks. Further experiments on three general semantic
segmentation datasets prove the generality of our method. Code will be provided
in (https: //github.com/lxtGH/PFSegNets).
</p>
<a href="http://arxiv.org/abs/2103.06564" target="_blank">arXiv:2103.06564</a> [<a href="http://arxiv.org/pdf/2103.06564" target="_blank">pdf</a>]

<h2>Preprint: Norm Loss: An efficient yet effective regularization method for deep neural networks. (arXiv:2103.06583v1 [cs.CV])</h2>
<h3>Theodoros Georgiou, Sebastian Schmitt, Thomas B&#xe4;ck, Wei Chen, Michael Lew</h3>
<p>Convolutional neural network training can suffer from diverse issues like
exploding or vanishing gradients, scaling-based weight space symmetry and
covariant-shift. In order to address these issues, researchers develop weight
regularization methods and activation normalization methods. In this work we
propose a weight soft-regularization method based on the Oblique manifold. The
proposed method uses a loss function which pushes each weight vector to have a
norm close to one, i.e. the weight matrix is smoothly steered toward the
so-called Oblique manifold. We evaluate our method on the very popular
CIFAR-10, CIFAR-100 and ImageNet 2012 datasets using two state-of-the-art
architectures, namely the ResNet and wide-ResNet. Our method introduces
negligible computational overhead and the results show that it is competitive
to the state-of-the-art and in some cases superior to it. Additionally, the
results are less sensitive to hyperparameter settings such as batch size and
regularization factor.
</p>
<a href="http://arxiv.org/abs/2103.06583" target="_blank">arXiv:2103.06583</a> [<a href="http://arxiv.org/pdf/2103.06583" target="_blank">pdf</a>]

<h2>Privacy-preserving Object Detection. (arXiv:2103.06587v1 [cs.CV])</h2>
<h3>Peiyang He, Charlie Griffin, Krzysztof Kacprzyk, Artjom Joosen, Michael Collyer, Aleksandar Shtedritski, Yuki M. Asano</h3>
<p>Privacy considerations and bias in datasets are quickly becoming
high-priority issues that the computer vision community needs to face. So far,
little attention has been given to practical solutions that do not involve
collection of new datasets. In this work, we show that for object detection on
COCO, both anonymizing the dataset by blurring faces, as well as swapping faces
in a balanced manner along the gender and skin tone dimension, can retain
object detection performances while preserving privacy and partially balancing
bias.
</p>
<a href="http://arxiv.org/abs/2103.06587" target="_blank">arXiv:2103.06587</a> [<a href="http://arxiv.org/pdf/2103.06587" target="_blank">pdf</a>]

<h2>Controlled Gaussian Process Dynamical Models with Application to Robotic Cloth Manipulation. (arXiv:2103.06615v1 [cs.RO])</h2>
<h3>Fabio Amadio, Juan Antonio Delgado-Guerrero, Adri&#xe0; Colom&#xe9;, Carme Torras</h3>
<p>Over the last years, robotic cloth manipulation has gained relevance within
the research community. While significant advances have been made in robotic
manipulation of rigid objects, the manipulation of non-rigid objects such as
cloth garments is still a challenging problem. The uncertainty on how cloth
behaves often requires the use of model-based approaches. However, cloth models
have a very high dimensionality. Therefore, it is difficult to find a middle
point between providing a manipulator with a dynamics model of cloth and
working with a state space of tractable dimensionality. For this reason, most
cloth manipulation approaches in literature perform static or quasi-static
manipulation. In this paper, we propose a variation of Gaussian Process
Dynamical Models (GPDMs) to model cloth dynamics in a low-dimensional manifold.
GPDMs project a high-dimensional state space into a smaller dimension latent
space which is capable of keeping the dynamic properties. Using such approach,
we add control variables to the original formulation. In this way, it is
possible to take into account the robot commands exerted on the cloth dynamics.
We call this new version Controlled Gaussian Process Dynamical Model (C-GPDM).
Moreover, we propose an alternative kernel representation for the model,
characterized by a richer parameterization than the one employed in the
majority of previous GPDM realizations. The modeling capacity of our proposal
has been tested in a simulated scenario, where C-GPDM proved to be capable of
generalizing over a considerably wide range of movements and correctly
predicting the cloth oscillations generated by previously unseen sequences of
control actions.
</p>
<a href="http://arxiv.org/abs/2103.06615" target="_blank">arXiv:2103.06615</a> [<a href="http://arxiv.org/pdf/2103.06615" target="_blank">pdf</a>]

<h2>MagFace: A Universal Representation for Face Recognition and Quality Assessment. (arXiv:2103.06627v1 [cs.CV])</h2>
<h3>Qiang Meng, Shichao Zhao, Zhida Huang, Feng Zhou</h3>
<p>The performance of face recognition system degrades when the variability of
the acquired faces increases. Prior work alleviates this issue by either
monitoring the face quality in pre-processing or predicting the data
uncertainty along with the face feature. This paper proposes MagFace, a
category of losses that learn a universal feature embedding whose magnitude can
measure the quality of the given face. Under the new loss, it can be proven
that the magnitude of the feature embedding monotonically increases if the
subject is more likely to be recognized. In addition, MagFace introduces an
adaptive mechanism to learn a wellstructured within-class feature distributions
by pulling easy samples to class centers while pushing hard samples away. This
prevents models from overfitting on noisy low-quality samples and improves face
recognition in the wild. Extensive experiments conducted on face recognition,
quality assessments as well as clustering demonstrate its superiority over
state-of-the-arts. The code is available at
https://github.com/IrvingMeng/MagFace.
</p>
<a href="http://arxiv.org/abs/2103.06627" target="_blank">arXiv:2103.06627</a> [<a href="http://arxiv.org/pdf/2103.06627" target="_blank">pdf</a>]

<h2>Generalized Contrastive Optimization of Siamese Networks for Place Recognition. (arXiv:2103.06638v1 [cs.CV])</h2>
<h3>Mar&#xed;a Leyva-Vallina, Nicola Strisciuglio, Nicolai Petkov</h3>
<p>Visual place recognition is a challenging task in computer vision and a key
component of camera-based localization and navigation systems. Recently,
Convolutional Neural Networks (CNNs) achieved high results and good
generalization capabilities. They are usually trained using pairs or triplets
of images labeled as either similar or dissimilar, in a binary fashion. In
practice, the similarity between two images is not binary, but rather
continuous. Furthermore, training these CNNs is computationally complex and
involves costly pair and triplet mining strategies.

We propose a Generalized Contrastive loss (GCL) function that relies on image
similarity as a continuous measure, and use it to train a siamese CNN.
Furthermore, we propose three techniques for automatic annotation of image
pairs with labels indicating their degree of similarity, and deploy them to
re-annotate the MSLS, TB-Places, and 7Scenes datasets.

We demonstrate that siamese CNNs trained using the GCL function and the
improved annotations consistently outperform their binary counterparts. Our
models trained on MSLS outperform the state-of-the-art methods, including
NetVLAD, and generalize well on the Pittsburgh, TokyoTM and Tokyo 24/7
datasets. Furthermore, training a siamese network using the GCL function does
not require complex pair mining. We release the source code at
https://github.com/marialeyvallina/generalized_contrastive_loss.
</p>
<a href="http://arxiv.org/abs/2103.06638" target="_blank">arXiv:2103.06638</a> [<a href="http://arxiv.org/pdf/2103.06638" target="_blank">pdf</a>]

<h2>Deep Graph Matching under Quadratic Constraint. (arXiv:2103.06643v1 [cs.CV])</h2>
<h3>Quankai Gao, Fudong Wang, Nan Xue, Jin-Gang Yu, Gui-Song Xia</h3>
<p>Recently, deep learning based methods have demonstrated promising results on
the graph matching problem, by relying on the descriptive capability of deep
features extracted on graph nodes. However, one main limitation with existing
deep graph matching (DGM) methods lies in their ignorance of explicit
constraint of graph structures, which may lead the model to be trapped into
local minimum in training. In this paper, we propose to explicitly formulate
pairwise graph structures as a \textbf{quadratic constraint} incorporated into
the DGM framework. The quadratic constraint minimizes the pairwise structural
discrepancy between graphs, which can reduce the ambiguities brought by only
using the extracted CNN features.

Moreover, we present a differentiable implementation to the quadratic
constrained-optimization such that it is compatible with the unconstrained deep
learning optimizer. To give more precise and proper supervision, a
well-designed false matching loss against class imbalance is proposed, which
can better penalize the false negatives and false positives with less
overfitting. Exhaustive experiments demonstrate that our method achieves
state-of-the-art performance on real-world datasets.
</p>
<a href="http://arxiv.org/abs/2103.06643" target="_blank">arXiv:2103.06643</a> [<a href="http://arxiv.org/pdf/2103.06643" target="_blank">pdf</a>]

<h2>Real-Time Surface Fitting to RGBD Sensor Data. (arXiv:2103.06644v1 [cs.CV])</h2>
<h3>John Papadakis, Andrew R. Willis</h3>
<p>This article describes novel approaches to quickly estimate planar surfaces
from RGBD sensor data. The approach manipulates the standard algebraic fitting
equations into a form that allows many of the needed regression variables to be
computed directly from the camera calibration information. As such, much of the
computational burden required by a standard algebraic surface fit can be
pre-computed. This provides a significant time and resource savings, especially
when many surface fits are being performed which is often the case when RGBD
point-cloud data is being analyzed for normal estimation, curvature estimation,
polygonization or 3D segmentation applications. Using an integral image
implementation, the proposed approaches show a significant increase in
performance compared to the standard algebraic fitting approaches.
</p>
<a href="http://arxiv.org/abs/2103.06644" target="_blank">arXiv:2103.06644</a> [<a href="http://arxiv.org/pdf/2103.06644" target="_blank">pdf</a>]

<h2>Temporal Action Segmentation from Timestamp Supervision. (arXiv:2103.06669v1 [cs.CV])</h2>
<h3>Zhe Li, Yazan Abu Farha, Juergen Gall</h3>
<p>Temporal action segmentation approaches have been very successful recently.
However, annotating videos with frame-wise labels to train such models is very
expensive and time consuming. While weakly supervised methods trained using
only ordered action lists require much less annotation effort, the performance
is still much worse than fully supervised approaches. In this paper, we
introduce timestamp supervision for the temporal action segmentation task.
Timestamps require a comparable annotation effort to weakly supervised
approaches, and yet provide a more supervisory signal. To demonstrate the
effectiveness of timestamp supervision, we propose an approach to train a
segmentation model using only timestamps annotations. Our approach uses the
model output and the annotated timestamps to generate frame-wise labels by
detecting the action changes. We further introduce a confidence loss that
forces the predicted probabilities to monotonically decrease as the distance to
the timestamps increases. This ensures that all and not only the most
distinctive frames of an action are learned during training. The evaluation on
four datasets shows that models trained with timestamps annotations achieve
comparable performance to the fully supervised approaches.
</p>
<a href="http://arxiv.org/abs/2103.06669" target="_blank">arXiv:2103.06669</a> [<a href="http://arxiv.org/pdf/2103.06669" target="_blank">pdf</a>]

<h2>Have I been here before? Learning to Close the Loop with LiDAR Data in Graph-Based SLAM. (arXiv:2103.06713v1 [cs.RO])</h2>
<h3>Tim-Lukas Habich, Marvin Stuede, Mathieu Labb&#xe9;, Svenja Spindeldreier</h3>
<p>This work presents an extension of graph-based SLAM methods to exploit the
potential of 3D laser scans for loop detection. Every high-dimensional point
cloud is replaced by a compact global descriptor, whereby a trained detector
decides whether a loop exists. Searching for loops is performed locally in a
variable space to consider the odometry drift. Since closing a wrong loop has
fatal consequences, an extensive verification is performed before acceptance.
The proposed algorithm is implemented as an extension of the widely used
state-of-the-art library RTAB-Map, and several experiments show the
improvement: During SLAM with a mobile service robot in changing indoor and
outdoor campus environments, our approach improves RTAB-Map regarding total
number of closed loops. Especially in the presence of significant environmental
changes, which typically lead to failure, localization becomes possible by our
extension. Experiments with a car in traffic (KITTI benchmark) show the general
applicability of our approach. These results are comparable to the
state-of-the-art LiDAR method LOAM. The developed ROS package is freely
available.
</p>
<a href="http://arxiv.org/abs/2103.06713" target="_blank">arXiv:2103.06713</a> [<a href="http://arxiv.org/pdf/2103.06713" target="_blank">pdf</a>]

<h2>Duplex Contextual Relation Network for Polyp Segmentation. (arXiv:2103.06725v1 [cs.CV])</h2>
<h3>Zijin Yin, Kongming Liang, Zhanyu Ma, Jun Guo</h3>
<p>Polyp segmentation is of great importance in the early diagnosis and
treatment of colorectal cancer. Since polyps vary in their shape, size, color,
and texture, accurate polyp segmentation is very challenging. One promising way
to mitigate the diversity of polyps is to model the contextual relation for
each pixel such as using attention mechanism. However, previous methods only
focus on learning the dependencies between the position within an individual
image and ignore the contextual relation across different images. In this
paper, we propose Duplex Contextual Relation Network (DCRNet) to capture both
within-image and cross-image contextual relations. Specifically, we first
design Interior Contextual-Relation Module to estimate the similarity between
each position and all the positions within the same image. Then Exterior
Contextual-Relation Module is incorporated to estimate the similarity between
each position and the positions across different images. Based on the above two
types of similarity, the feature at one position can be further enhanced by the
contextual region embedding within and across images. To store the
characteristic region embedding from all the images, a memory bank is designed
and operates as a queue. Therefore, the proposed method can relate similar
features even though they come from different images. We evaluate the proposed
method on the EndoScene, Kvasir-SEG and the recently released large-scale
PICCOLO dataset. Experimental results show that the proposed DCRNet outperforms
the state-of-the-art methods in terms of the widely-used evaluation metrics.
</p>
<a href="http://arxiv.org/abs/2103.06725" target="_blank">arXiv:2103.06725</a> [<a href="http://arxiv.org/pdf/2103.06725" target="_blank">pdf</a>]

<h2>Swarm Robots in Agriculture. (arXiv:2103.06732v1 [cs.RO])</h2>
<h3>Daniel Albiero, Angel Pontin Garcia, Claudio Kiyoshi Umezu, Rodrigo Leme de Paulo</h3>
<p>Agricultural mechanization is an area of knowledge that has evolved a lot
over the past century, its main actors being agricultural tractors that, in 100
years, have increased their powers by 3,300%. This evolution has resulted in an
exponential increase in the field capacity of such machines. However, it has
also generated negative results such as excessive consumption of fossil fuel,
excessive weight on the soil, very high operating costs, and millionaire
acquisition value. This paper aims to present an antiparadigmatic alternative
in this area. It is proposing a swarm of small electric robotic tractors that
together have the same field capacity as a large tractor with an internal
combustion engine. A comparison of costs and field capacity between a 270 kW
tractor and a swarm of ten swarm tractors of 24 kW each was carried out. The
result demonstrated a wide advantage for the small robot team. It was also
proposed the preliminary design of an electric swarm robot tractor. Finally,
research challenges were suggested to operationalize such a proposal, calling
on the Brazilian Robotics Research Community to elaborate a roadmap for
research in the area of swarm robot for mechanized agricultural operations.
</p>
<a href="http://arxiv.org/abs/2103.06732" target="_blank">arXiv:2103.06732</a> [<a href="http://arxiv.org/pdf/2103.06732" target="_blank">pdf</a>]

<h2>Visibility-aware Trajectory Optimization with Application to Aerial Tracking. (arXiv:2103.06742v1 [cs.RO])</h2>
<h3>Qianhao Wang, Yuman Gao, Jialin Ji, Chao Xu, Fei Gao</h3>
<p>The visibility of targets determines performance and even success rate of
various applications, such as active slam, exploration, and target tracking.
Therefore, it is crucial to take the visibility of targets into explicit
account in trajectory planning. In this paper, we propose a general metric for
target visibility, considering observation distance and angle as well as
occlusion effect. We formulate this metric into a differentiable visibility
cost function, with which spatial trajectory and yaw can be jointly optimized.
Furthermore, this visibility-aware trajectory optimization handles dynamic
feasibility of position and yaw simultaneously. To validate that our method is
practical and generic, we integrate it into a customized quadrotor tracking
system. The experimental results show that our visibility-aware planner
performs more robustly and observes targets better. In order to benefit related
researches, we release our code to the public.
</p>
<a href="http://arxiv.org/abs/2103.06742" target="_blank">arXiv:2103.06742</a> [<a href="http://arxiv.org/pdf/2103.06742" target="_blank">pdf</a>]

<h2>ChallenCap: Monocular 3D Capture of Challenging Human Performances using Multi-Modal References. (arXiv:2103.06747v1 [cs.CV])</h2>
<h3>Yannan He, Anqi Pang, Xin Chen, Han Liang, Minye Wu, Yuexin Ma, Lan Xu</h3>
<p>Capturing challenging human motions is critical for numerous applications,
but it suffers from complex motion patterns and severe self-occlusion under the
monocular setting. In this paper, we propose ChallenCap -- a template-based
approach to capture challenging 3D human motions using a single RGB camera in a
novel learning-and-optimization framework, with the aid of multi-modal
references. We propose a hybrid motion inference stage with a generation
network, which utilizes a temporal encoder-decoder to extract the motion
details from the pair-wise sparse-view reference, as well as a motion
discriminator to utilize the unpaired marker-based references to extract
specific challenging motion characteristics in a data-driven manner. We further
adopt a robust motion optimization stage to increase the tracking accuracy, by
jointly utilizing the learned motion details from the supervised multi-modal
references as well as the reliable motion hints from the input image reference.
Extensive experiments on our new challenging motion dataset demonstrate the
effectiveness and robustness of our approach to capture challenging human
motions.
</p>
<a href="http://arxiv.org/abs/2103.06747" target="_blank">arXiv:2103.06747</a> [<a href="http://arxiv.org/pdf/2103.06747" target="_blank">pdf</a>]

<h2>Automatic Social Distance Estimation From Images: Performance Evaluation, Test Benchmark, and Algorithm. (arXiv:2103.06759v1 [cs.CV])</h2>
<h3>Mert Seker, Anssi M&#xe4;nnist&#xf6;, Alexandros Iosifidis, Jenni Raitoharju</h3>
<p>The COVID-19 virus has caused a global pandemic since March 2020. The World
Health Organization (WHO) has provided guidelines on how to reduce the spread
of the virus and one of the most important measures is social distancing.
Maintaining a minimum of one meter distance from other people is strongly
suggested to reduce the risk of infection. This has created a strong interest
in monitoring the social distances either as a safety measure or to study how
the measures have affected human behavior and country-wise differences in this.
The need for automatic social distance estimation algorithms is evident, but
there is no suitable test benchmark for such algorithms. Collecting images with
measured ground-truth pair-wise distances between all the people using
different camera settings is cumbersome. Furthermore, performance evaluation
for social distance estimation algorithms is not straightforward and there is
no widely accepted evaluation protocol. In this paper, we provide a dataset of
varying images with measured pair-wise social distances under different camera
positionings and focal length values. We suggest a performance evaluation
protocol and provide a benchmark to easily evaluate social distance estimation
algorithms. We also propose a method for automatic social distance estimation.
Our method takes advantage of object detection and human pose estimation. It
can be applied on any single image as long as focal length and sensor size
information are known. The results on our benchmark are encouraging with 92%
human detection rate and only 28.9% average error in distance estimation among
the detected people.
</p>
<a href="http://arxiv.org/abs/2103.06759" target="_blank">arXiv:2103.06759</a> [<a href="http://arxiv.org/pdf/2103.06759" target="_blank">pdf</a>]

<h2>Unknown Object Segmentation from Stereo Images. (arXiv:2103.06796v1 [cs.CV])</h2>
<h3>Maximilian Durner, Wout Boerdijk, Martin Sundermeyer, Werner Friedl, Zoltan-Csaba Marton, Rudolph Triebel</h3>
<p>Although instance-aware perception is a key prerequisite for many autonomous
robotic applications, most of the methods only partially solve the problem by
focusing solely on known object categories. However, for robots interacting in
dynamic and cluttered environments, this is not realistic and severely limits
the range of potential applications. Therefore, we propose a novel object
instance segmentation approach that does not require any semantic or geometric
information of the objects beforehand. In contrast to existing works, we do not
explicitly use depth data as input, but rely on the insight that slight
viewpoint changes, which for example are provided by stereo image pairs, are
often sufficient to determine object boundaries and thus to segment objects.
Focusing on the versatility of stereo sensors, we employ a transformer-based
architecture that maps directly from the pair of input images to the object
instances. This has the major advantage that instead of a noisy, and
potentially incomplete depth map as an input, on which the segmentation is
computed, we use the original image pair to infer the object instances and a
dense depth map. In experiments in several different application domains, we
show that our Instance Stereo Transformer (INSTR) algorithm outperforms current
state-of-the-art methods that are based on depth maps. Training code and
pretrained models will be made available.
</p>
<a href="http://arxiv.org/abs/2103.06796" target="_blank">arXiv:2103.06796</a> [<a href="http://arxiv.org/pdf/2103.06796" target="_blank">pdf</a>]

<h2>Coming Down to Earth: Satellite-to-Street View Synthesis for Geo-Localization. (arXiv:2103.06818v1 [cs.CV])</h2>
<h3>Aysim Toker, Qunjie Zhou, Maxim Maximov, Laura Leal-Taix&#xe9;</h3>
<p>The goal of cross-view image based geo-localization is to determine the
location of a given street view image by matching it against a collection of
geo-tagged satellite images. This task is notoriously challenging due to the
drastic viewpoint and appearance differences between the two domains. We show
that we can address this discrepancy explicitly by learning to synthesize
realistic street views from satellite inputs. Following this observation, we
propose a novel multi-task architecture in which image synthesis and retrieval
are considered jointly. The rationale behind this is that we can bias our
network to learn latent feature representations that are useful for retrieval
if we utilize them to generate images across the two input domains. To the best
of our knowledge, ours is the first approach that creates realistic street
views from satellite images and localizes the corresponding query street-view
simultaneously in an end-to-end manner. In our experiments, we obtain
state-of-the-art performance on the CVUSA and CVACT benchmarks. Finally, we
show compelling qualitative results for satellite-to-street view synthesis.
</p>
<a href="http://arxiv.org/abs/2103.06818" target="_blank">arXiv:2103.06818</a> [<a href="http://arxiv.org/pdf/2103.06818" target="_blank">pdf</a>]

<h2>SMPLicit: Topology-aware Generative Model for Clothed People. (arXiv:2103.06871v1 [cs.CV])</h2>
<h3>Enric Corona, Albert Pumarola, Guillem Aleny&#xe0;, Gerard Pons-Moll, Francesc Moreno-Noguer</h3>
<p>In this paper we introduce SMPLicit, a novel generative model to jointly
represent body pose, shape and clothing geometry. In contrast to existing
learning-based approaches that require training specific models for each type
of garment, SMPLicit can represent in a unified manner different garment
topologies (e.g. from sleeveless tops to hoodies and to open jackets), while
controlling other properties like the garment size or tightness/looseness. We
show our model to be applicable to a large variety of garments including
T-shirts, hoodies, jackets, shorts, pants, skirts, shoes and even hair. The
representation flexibility of SMPLicit builds upon an implicit model
conditioned with the SMPL human body parameters and a learnable latent space
which is semantically interpretable and aligned with the clothing attributes.
The proposed model is fully differentiable, allowing for its use into larger
end-to-end trainable systems. In the experimental section, we demonstrate
SMPLicit can be readily used for fitting 3D scans and for 3D reconstruction in
images of dressed people. In both cases we are able to go beyond state of the
art, by retrieving complex garment geometries, handling situations with
multiple clothing layers and providing a tool for easy outfit editing. To
stimulate further research in this direction, we will make our code and model
publicly available at this http URL
</p>
<a href="http://arxiv.org/abs/2103.06871" target="_blank">arXiv:2103.06871</a> [<a href="http://arxiv.org/pdf/2103.06871" target="_blank">pdf</a>]

<h2>Fast and Accurate Model Scaling. (arXiv:2103.06877v1 [cs.CV])</h2>
<h3>Piotr Doll&#xe1;r, Mannat Singh, Ross Girshick</h3>
<p>In this work we analyze strategies for convolutional neural network scaling;
that is, the process of scaling a base convolutional network to endow it with
greater computational complexity and consequently representational power.
Example scaling strategies may include increasing model width, depth,
resolution, etc. While various scaling strategies exist, their tradeoffs are
not fully understood. Existing analysis typically focuses on the interplay of
accuracy and flops (floating point operations). Yet, as we demonstrate, various
scaling strategies affect model parameters, activations, and consequently
actual runtime quite differently. In our experiments we show the surprising
result that numerous scaling strategies yield networks with similar accuracy
but with widely varying properties. This leads us to propose a simple fast
compound scaling strategy that encourages primarily scaling model width, while
scaling depth and resolution to a lesser extent. Unlike currently popular
scaling strategies, which result in about $O(s)$ increase in model activation
w.r.t. scaling flops by a factor of $s$, the proposed fast compound scaling
results in close to $O(\sqrt{s})$ increase in activations, while achieving
excellent accuracy. This leads to comparable speedups on modern memory-limited
hardware (e.g., GPU, TPU). More generally, we hope this work provides a
framework for analyzing and selecting scaling strategies under various
computational constraints.
</p>
<a href="http://arxiv.org/abs/2103.06877" target="_blank">arXiv:2103.06877</a> [<a href="http://arxiv.org/pdf/2103.06877" target="_blank">pdf</a>]

<h2>Diverse Semantic Image Synthesis via Probability Distribution Modeling. (arXiv:2103.06878v1 [cs.CV])</h2>
<h3>Zhentao Tan, Menglei Chai, Dongdong Chen, Jing Liao, Qi Chu, Bin Liu, Gang Hua, Nenghai Yu</h3>
<p>Semantic image synthesis, translating semantic layouts to photo-realistic
images, is a one-to-many mapping problem. Though impressive progress has been
recently made, diverse semantic synthesis that can efficiently produce
semantic-level multimodal results, still remains a challenge. In this paper, we
propose a novel diverse semantic image synthesis framework from the perspective
of semantic class distributions, which naturally supports diverse generation at
semantic or even instance level. We achieve this by modeling class-level
conditional modulation parameters as continuous probability distributions
instead of discrete values, and sampling per-instance modulation parameters
through instance-adaptive stochastic sampling that is consistent across the
network. Moreover, we propose prior noise remapping, through linear
perturbation parameters encoded from paired references, to facilitate
supervised training and exemplar-based instance style control at test time.
Extensive experiments on multiple datasets show that our method can achieve
superior diversity and comparable quality compared to state-of-the-art methods.
Code will be available at \url{https://github.com/tzt101/INADE.git}
</p>
<a href="http://arxiv.org/abs/2103.06878" target="_blank">arXiv:2103.06878</a> [<a href="http://arxiv.org/pdf/2103.06878" target="_blank">pdf</a>]

<h2>CoMoGAN: continuous model-guided image-to-image translation. (arXiv:2103.06879v1 [cs.CV])</h2>
<h3>Fabio Pizzati, Pietro Cerri, Raoul de Charette</h3>
<p>CoMoGAN is a continuous GAN relying on the unsupervised reorganization of the
target data on a functional manifold. To that matter, we introduce a new
Functional Instance Normalization layer and residual mechanism, which together
disentangle image content from position on target manifold. We rely on naive
physics-inspired models to guide the training while allowing private
model/translations features. CoMoGAN can be used with any GAN backbone and
allows new types of image translation, such as cyclic image translation like
timelapse generation, or detached linear translation. On all datasets and
metrics, it outperforms the literature. Our code is available at
this http URL .
</p>
<a href="http://arxiv.org/abs/2103.06879" target="_blank">arXiv:2103.06879</a> [<a href="http://arxiv.org/pdf/2103.06879" target="_blank">pdf</a>]

<h2>Multi-Kernel Prediction Networks for Denoising of Burst Images. (arXiv:1902.05392v2 [cs.CV] UPDATED)</h2>
<h3>Talmaj Marin&#x10d;, Vignesh Srinivasan, Serhan G&#xfc;l, Cornelius Hellge, Wojciech Samek</h3>
<p>In low light or short-exposure photography the image is often corrupted by
noise. While longer exposure helps reduce the noise, it can produce blurry
results due to the object and camera motion. The reconstruction of a noise-less
image is an ill posed problem. Recent approaches for image denoising aim to
predict kernels which are convolved with a set of successively taken images
(burst) to obtain a clear image. We propose a deep neural network based
approach called Multi-Kernel Prediction Networks (MKPN) for burst image
denoising. MKPN predicts kernels of not just one size but of varying sizes and
performs fusion of these different kernels resulting in one kernel per pixel.
The advantages of our method are two fold: (a) the different sized kernels help
in extracting different information from the image which results in better
reconstruction and (b) kernel fusion assures retaining of the extracted
information while maintaining computational efficiency. Experimental results
reveal that MKPN outperforms state-of-the-art on our synthetic datasets with
different noise levels.
</p>
<a href="http://arxiv.org/abs/1902.05392" target="_blank">arXiv:1902.05392</a> [<a href="http://arxiv.org/pdf/1902.05392" target="_blank">pdf</a>]

<h2>Centralized Cooperation for Connected and Automated Vehicles at Intersections by Proximal Policy Optimization. (arXiv:1912.08410v2 [cs.RO] UPDATED)</h2>
<h3>Yang Guan, Yangang Ren, Shengbo Eben Li, Qi Sun, Laiquan Luo, Keqiang Li</h3>
<p>Connected vehicles will change the modes of future transportation management
and organization, especially at an intersection without traffic light.
Centralized coordination methods globally coordinate vehicles approaching the
intersection from all sections by considering their states altogether. However,
they need substantial computation resources since they own a centralized
controller to optimize the trajectories for all approaching vehicles in
real-time. In this paper, we propose a centralized coordination scheme of
automated vehicles at an intersection without traffic signals using
reinforcement learning (RL) to address low computation efficiency suffered by
current centralized coordination methods. We first propose an RL training
algorithm, model accelerated proximal policy optimization (MA-PPO), which
incorporates a prior model into proximal policy optimization (PPO) algorithm to
accelerate the learning process in terms of sample efficiency. Then we present
the design of state, action and reward to formulate centralized coordination as
an RL problem. Finally, we train a coordinate policy in a simulation setting
and compare computing time and traffic efficiency with a coordination scheme
based on model predictive control (MPC) method. Results show that our method
spends only 1/400 of the computing time of MPC and increase the efficiency of
the intersection by 4.5 times.
</p>
<a href="http://arxiv.org/abs/1912.08410" target="_blank">arXiv:1912.08410</a> [<a href="http://arxiv.org/pdf/1912.08410" target="_blank">pdf</a>]

<h2>Human in Events: A Large-Scale Benchmark for Human-centric Video Analysis in Complex Events. (arXiv:2005.04490v4 [cs.CV] UPDATED)</h2>
<h3>Weiyao Lin, Huabin Liu, Shizhan Liu, Yuxi Li, Rui Qian, Tao Wang, Hongkai Xiong, Ning Xu, Guo-Jun Qi, Nicu Sebe</h3>
<p>Along with the development of modern smart cities, human-centric video
analysis has been encountering the challenge of analyzing diverse and complex
events in real scenes. A complex event relates to dense crowds, anomalous, or
collective behaviors. However, limited by the scale of existing video datasets,
few human analysis approaches have reported their performance on such complex
events. To this end, we present a new large-scale dataset, named
Human-in-Events or HiEve (Human-centric video analysis in complex Events), for
the understanding of human motions, poses, and actions in a variety of
realistic events, especially in crowd and complex events. It contains a record
number of poses (&gt;1M), the largest number of action instances (&gt;56k) under
complex events, as well as one of the largest numbers of trajectories lasting
for longer time (with an average trajectory length of &gt;480 frames). Based on
this dataset, we present an enhanced pose estimation baseline by utilizing the
potential of action information to guide the learning of more powerful 2D pose
features. We demonstrate that the proposed method is able to boost the
performance of existing pose estimation pipelines on our HiEve dataset.
Furthermore, we conduct extensive experiments to benchmark recent video
analysis approaches together with our baseline methods, demonstrating that
HiEve is a challenging dataset for human-centric video analysis. We expect that
the dataset will advance the development of cutting-edge techniques in
human-centric analysis and the understanding of complex events. The dataset is
available at this http URL
</p>
<a href="http://arxiv.org/abs/2005.04490" target="_blank">arXiv:2005.04490</a> [<a href="http://arxiv.org/pdf/2005.04490" target="_blank">pdf</a>]

<h2>A Sliced Wasserstein Loss for Neural Texture Synthesis. (arXiv:2006.07229v4 [cs.CV] UPDATED)</h2>
<h3>Eric Heitz, Kenneth Vanhoey, Thomas Chambon, Laurent Belcour</h3>
<p>We address the problem of computing a textural loss based on the statistics
extracted from the feature activations of a convolutional neural network
optimized for object recognition (e.g. VGG-19). The underlying mathematical
problem is the measure of the distance between two distributions in feature
space. The Gram-matrix loss is the ubiquitous approximation for this problem
but it is subject to several shortcomings. Our goal is to promote the Sliced
Wasserstein Distance as a replacement for it. It is theoretically
proven,practical, simple to implement, and achieves results that are visually
superior for texture synthesis by optimization or training generative neural
networks.
</p>
<a href="http://arxiv.org/abs/2006.07229" target="_blank">arXiv:2006.07229</a> [<a href="http://arxiv.org/pdf/2006.07229" target="_blank">pdf</a>]

<h2>MirrorNet: Bio-Inspired Camouflaged Object Segmentation. (arXiv:2007.12881v3 [cs.CV] UPDATED)</h2>
<h3>Jinnan Yan, Trung-Nghia Le, Khanh-Duy Nguyen, Minh-Triet Tran, Thanh-Toan Do, Tam V. Nguyen</h3>
<p>Camouflaged objects are generally difficult to be detected in their natural
environment even for human beings. In this paper, we propose a novel
bio-inspired network, named the MirrorNet, that leverages both instance
segmentation and mirror stream for the camouflaged object segmentation.
Differently from existing networks for segmentation, our proposed network
possesses two segmentation streams: the main stream and the mirror stream
corresponding with the original image and its flipped image, respectively. The
output from the mirror stream is then fused into the main stream's result for
the final camouflage map to boost up the segmentation accuracy. Extensive
experiments conducted on the public CAMO dataset demonstrate the effectiveness
of our proposed network. Our proposed method achieves 89% in accuracy,
outperforming the state-of-the-arts.

Project Page: https://sites.google.com/view/ltnghia/research/camo
</p>
<a href="http://arxiv.org/abs/2007.12881" target="_blank">arXiv:2007.12881</a> [<a href="http://arxiv.org/pdf/2007.12881" target="_blank">pdf</a>]

<h2>Predicted Composite Signed-Distance Fields for Real-Time Motion Planning in Dynamic Environments. (arXiv:2008.00969v2 [cs.RO] UPDATED)</h2>
<h3>Mark Nicholas Finean, Wolfgang Merkt, Ioannis Havoutis</h3>
<p>We present a novel framework for motion planning in dynamic environments that
accounts for the predicted trajectories of moving objects in the scene. We
explore the use of composite signed-distance fields in motion planning and
detail how they can be used to generate signed-distance fields (SDFs) in
real-time to incorporate predicted obstacle motions. We benchmark our approach
of using composite SDFs against performing exact SDF calculations on the
workspace occupancy grid. Our proposed technique generates predictions
substantially faster and typically exhibits an 81--97% reduction in time for
subsequent predictions. We integrate our framework with GPMP2 to demonstrate a
full implementation of our approach in real-time, enabling a 7-DoF Panda arm to
smoothly avoid a moving robot.
</p>
<a href="http://arxiv.org/abs/2008.00969" target="_blank">arXiv:2008.00969</a> [<a href="http://arxiv.org/pdf/2008.00969" target="_blank">pdf</a>]

<h2>OpenBot: Turning Smartphones into Robots. (arXiv:2008.10631v2 [cs.RO] UPDATED)</h2>
<h3>Matthias M&#xfc;ller, Vladlen Koltun</h3>
<p>Current robots are either expensive or make significant compromises on
sensory richness, computational power, and communication capabilities. We
propose to leverage smartphones to equip robots with extensive sensor suites,
powerful computational abilities, state-of-the-art communication channels, and
access to a thriving software ecosystem. We design a small electric vehicle
that costs $50 and serves as a robot body for standard Android smartphones. We
develop a software stack that allows smartphones to use this body for mobile
operation and demonstrate that the system is sufficiently powerful to support
advanced robotics workloads such as person following and real-time autonomous
navigation in unstructured environments. Controlled experiments demonstrate
that the presented approach is robust across different smartphones and robot
bodies. A video of our work is available at
https://www.youtube.com/watch?v=qc8hFLyWDOM
</p>
<a href="http://arxiv.org/abs/2008.10631" target="_blank">arXiv:2008.10631</a> [<a href="http://arxiv.org/pdf/2008.10631" target="_blank">pdf</a>]

<h2>Each Part Matters: Local Patterns Facilitate Cross-view Geo-localization. (arXiv:2008.11646v2 [cs.CV] UPDATED)</h2>
<h3>Tingyu Wang, Zhedong Zheng, Chenggang Yan, Jiyong Zhang, Yaoqi Sun, Bolun Zheng, Yi Yang</h3>
<p>Cross-view geo-localization is to spot images of the same geographic target
from different platforms, e.g., drone-view cameras and satellites. It is
challenging in the large visual appearance changes caused by extreme viewpoint
variations. Existing methods usually concentrate on mining the fine-grained
feature of the geographic target in the image center, but underestimate the
contextual information in neighbor areas. In this work, we argue that neighbor
areas can be leveraged as auxiliary information, enriching discriminative clues
for geolocalization. Specifically, we introduce a simple and effective deep
neural network, called Local Pattern Network (LPN), to take advantage of
contextual information in an end-to-end manner. Without using extra part
estimators, LPN adopts a square-ring feature partition strategy, which provides
the attention according to the distance to the image center. It eases the part
matching and enables the part-wise representation learning. Owing to the
square-ring partition design, the proposed LPN has good scalability to rotation
variations and achieves competitive results on three prevailing benchmarks,
i.e., University-1652, CVUSA and CVACT. Besides, we also show the proposed LPN
can be easily embedded into other frameworks to further boost performance.
</p>
<a href="http://arxiv.org/abs/2008.11646" target="_blank">arXiv:2008.11646</a> [<a href="http://arxiv.org/pdf/2008.11646" target="_blank">pdf</a>]

<h2>Semantic MapNet: Building Allocentric Semantic Maps and Representations from Egocentric Views. (arXiv:2010.01191v3 [cs.CV] UPDATED)</h2>
<h3>Vincent Cartillier, Zhile Ren, Neha Jain, Stefan Lee, Irfan Essa, Dhruv Batra</h3>
<p>We study the task of semantic mapping - specifically, an embodied agent (a
robot or an egocentric AI assistant) is given a tour of a new environment and
asked to build an allocentric top-down semantic map ("what is where?") from
egocentric observations of an RGB-D camera with known pose (via localization
sensors). Towards this goal, we present SemanticMapNet (SMNet), which consists
of: (1) an Egocentric Visual Encoder that encodes each egocentric RGB-D frame,
(2) a Feature Projector that projects egocentric features to appropriate
locations on a floor-plan, (3) a Spatial Memory Tensor of size floor-plan
length x width x feature-dims that learns to accumulate projected egocentric
features, and (4) a Map Decoder that uses the memory tensor to produce semantic
top-down maps. SMNet combines the strengths of (known) projective camera
geometry and neural representation learning. On the task of semantic mapping in
the Matterport3D dataset, SMNet significantly outperforms competitive baselines
by 4.01-16.81% (absolute) on mean-IoU and 3.81-19.69% (absolute) on Boundary-F1
metrics. Moreover, we show how to use the neural episodic memories and
spatio-semantic allocentric representations build by SMNet for subsequent tasks
in the same space - navigating to objects seen during the tour("Find chair") or
answering questions about the space ("How many chairs did you see in the
house?"). Project page: https://vincentcartillier.github.io/smnet.html.
</p>
<a href="http://arxiv.org/abs/2010.01191" target="_blank">arXiv:2010.01191</a> [<a href="http://arxiv.org/pdf/2010.01191" target="_blank">pdf</a>]

<h2>Inverse Dynamics vs. Forward Dynamics in Direct Transcription Formulations for Trajectory Optimization. (arXiv:2010.05359v2 [cs.RO] UPDATED)</h2>
<h3>Henrique Ferrolho, Vladimir Ivan, Wolfgang Merkt, Ioannis Havoutis, Sethu Vijayakumar</h3>
<p>Benchmarks of state-of-the-art rigid-body dynamics libraries report better
performance solving the inverse dynamics problem than the forward alternative.
Those benchmarks encouraged us to question whether that computational advantage
would translate to direct transcription, where calculating rigid-body dynamics
and their derivatives accounts for a significant share of computation time. In
this work, we implement an optimization framework where both approaches for
enforcing the system dynamics are available. We evaluate the performance of
each approach for systems of varying complexity, for domains with rigid
contacts. Our tests reveal that formulations using inverse dynamics converge
faster, require less iterations, and are more robust to coarse problem
discretization. These results indicate that inverse dynamics should be
preferred to enforce the nonlinear system dynamics in simultaneous methods,
such as direct transcription.
</p>
<a href="http://arxiv.org/abs/2010.05359" target="_blank">arXiv:2010.05359</a> [<a href="http://arxiv.org/pdf/2010.05359" target="_blank">pdf</a>]

<h2>Learn to Navigate Maplessly with Varied LiDAR Configurations: A Support Point-Based Approach. (arXiv:2010.10209v2 [cs.RO] UPDATED)</h2>
<h3>Wei Zhang, Ning Liu, Yunfeng Zhang</h3>
<p>Deep reinforcement learning (DRL) demonstrates great potential in mapless
navigation domain. However, such a navigation model is normally restricted to a
fixed configuration of the range sensor because its input format is fixed. In
this paper, we propose a DRL model that can address range data obtained from
different range sensors with different installation positions. Our model first
extracts the goal-directed features from each obstacle point. Subsequently, it
chooses global obstacle features from all point-feature candidates and uses
these features for the final decision. As only a few points are used to support
the final decision, we refer to these points as support points and our approach
as support point-based navigation (SPN). Our model can handle data from
different LiDAR setups and demonstrates good performance in simulation and
real-world experiments. Moreover, it shows great potential in crowded scenarios
with small obstacles when using a high-resolution LiDAR.
</p>
<a href="http://arxiv.org/abs/2010.10209" target="_blank">arXiv:2010.10209</a> [<a href="http://arxiv.org/pdf/2010.10209" target="_blank">pdf</a>]

<h2>Search-based Kinodynamic Motion Planning for Omnidirectional Quadruped Robots. (arXiv:2011.00806v2 [cs.RO] UPDATED)</h2>
<h3>Pei Wang, Xiaoyu Zhou, Qingteng Zhao, Jun Wu, Qiuguo Zhu</h3>
<p>Autonomous navigation has played an increasingly significant role in
quadruped robot system. However, most existing works on quadruped robots
navigation using traditional search-based or sample-based methods do not
consider the kinodynamic characteristics of quadruped robots, generating
kinodynamically infeasible parts, that are difficult to track. In this paper,
we introduce a complete navigation system considering the omnidirectional
abilities of quadruped robots. First, we use kinodynamic path finding method to
obtain smooth, dynamically feasible, time-optimal initial paths and add
collision cost as a soft constraint to ensure safety. Then the trajectory is
refined by the timed elastic band (TEB) method based on the omnidirectional
model of quadruped robots. The superior performance of our work is demonstrated
through simulating and real-world experiments on our quadruped robot Jueying
Mini.
</p>
<a href="http://arxiv.org/abs/2011.00806" target="_blank">arXiv:2011.00806</a> [<a href="http://arxiv.org/pdf/2011.00806" target="_blank">pdf</a>]

<h2>Sim-to-Real Learning of All Common Bipedal Gaits via Periodic Reward Composition. (arXiv:2011.01387v2 [cs.RO] UPDATED)</h2>
<h3>Jonah Siekmann, Yesh Godse, Alan Fern, Jonathan Hurst</h3>
<p>We study the problem of realizing the full spectrum of bipedal locomotion on
a real robot with sim-to-real reinforcement learning (RL). A key challenge of
learning legged locomotion is describing different gaits, via reward functions,
in a way that is intuitive for the designer and specific enough to reliably
learn the gait across different initial random seeds or hyperparameters. A
common approach is to use reference motions (e.g. trajectories of joint
positions) to guide learning. However, finding high-quality reference motions
can be difficult and the trajectories themselves narrowly constrain the space
of learned motion. At the other extreme, reference-free reward functions are
often underspecified (e.g. move forward) leading to massive variance in policy
behavior, or are the product of significant reward-shaping via trial-and-error,
making them exclusive to specific gaits. In this work, we propose a
reward-specification framework based on composing simple probabilistic periodic
costs on basic forces and velocities. We instantiate this framework to define a
parametric reward function with intuitive settings for all common bipedal gaits
- standing, walking, hopping, running, and skipping. Using this function we
demonstrate successful sim-to-real transfer of the learned gaits to the bipedal
robot Cassie, as well as a generic policy that can transition between all of
the two-beat gaits.
</p>
<a href="http://arxiv.org/abs/2011.01387" target="_blank">arXiv:2011.01387</a> [<a href="http://arxiv.org/pdf/2011.01387" target="_blank">pdf</a>]

<h2>Monitoring and Diagnosability of Perception Systems. (arXiv:2011.07010v4 [cs.RO] UPDATED)</h2>
<h3>Pasquale Antonante, David I. Spivak, Luca Carlone</h3>
<p>Perception is a critical component of high-integrity applications of robotics
and autonomous systems, such as self-driving vehicles. In these applications,
failure of perception systems may put human life at risk, and a broad adoption
of these technologies requires the development of methodologies to guarantee
and monitor safe operation. Despite the paramount importance of perception
systems, currently there is no formal approach for system-level monitoring. In
this work, we propose a mathematical model for runtime monitoring and fault
detection and identification in perception systems. Towards this goal, we draw
connections with the literature on diagnosability in multiprocessor systems,
and generalize it to account for modules with heterogeneous outputs that
interact over time. The resulting temporal diagnostic graphs (i) provide a
framework to reason over the consistency of perception outputs -- across
modules and over time -- thus enabling fault detection, (ii) allow us to
establish formal guarantees on the maximum number of faults that can be
uniquely identified in a given perception system, and (iii) enable the design
of efficient algorithms for fault identification. We demonstrate our monitoring
system, dubbed PerSyS, in realistic simulations using the LGSVL self-driving
simulator and the Apollo Auto autonomy software stack, and show that PerSyS is
able to detect failures in challenging scenarios (including scenarios that have
caused self-driving car accidents in recent years), and is able to correctly
identify faults while entailing a minimal computation overhead (&lt; 5 ms on a
single-core CPU).
</p>
<a href="http://arxiv.org/abs/2011.07010" target="_blank">arXiv:2011.07010</a> [<a href="http://arxiv.org/pdf/2011.07010" target="_blank">pdf</a>]

<h2>Robust Quadruped Jumping via Deep Reinforcement Learning. (arXiv:2011.07089v2 [cs.RO] UPDATED)</h2>
<h3>Guillaume Bellegarda, Quan Nguyen</h3>
<p>In this paper we consider a general task of jumping varying distances and
heights for a quadrupedal robot in noisy environments, such as off of uneven
terrain and with variable robot dynamics parameters. To accurately jump in such
conditions, we propose a framework using deep reinforcement learning to
leverage the complex solution of nonlinear trajectory optimization for
quadrupedal jumping. While the standalone optimization limits jumping to
take-off from flat ground and requires accurate assumption of robot dynamics,
our proposed approach improves the robustness to allow jumping off of
significantly uneven terrain with variable robot dynamical parameters. Through
our method, the quadruped is able to jump distances of up to 1 m and heights of
up to 0.4 m, while being robust to environment noise of foot disturbances of up
to 0.1 m in height as well as with 5% variability of its body mass and inertia.
This behavior is learned through just a few thousand simulated jumps in
PyBullet, and we perform a sim-to-sim transfer to Gazebo. Video results can be
found at https://youtu.be/jkzvL2o3g-s.
</p>
<a href="http://arxiv.org/abs/2011.07089" target="_blank">arXiv:2011.07089</a> [<a href="http://arxiv.org/pdf/2011.07089" target="_blank">pdf</a>]

<h2>PLOP: Learning without Forgetting for Continual Semantic Segmentation. (arXiv:2011.11390v3 [cs.CV] UPDATED)</h2>
<h3>Arthur Douillard, Yifu Chen, Arnaud Dapogny, Matthieu Cord</h3>
<p>Deep learning approaches are nowadays ubiquitously used to tackle computer
vision tasks such as semantic segmentation, requiring large datasets and
substantial computational power. Continual learning for semantic segmentation
(CSS) is an emerging trend that consists in updating an old model by
sequentially adding new classes. However, continual learning methods are
usually prone to catastrophic forgetting. This issue is further aggravated in
CSS where, at each step, old classes from previous iterations are collapsed
into the background. In this paper, we propose Local POD, a multi-scale pooling
distillation scheme that preserves long- and short-range spatial relationships
at feature level. Furthermore, we design an entropy-based pseudo-labelling of
the background w.r.t. classes predicted by the old model to deal with
background shift and avoid catastrophic forgetting of the old classes. Our
approach, called PLOP, significantly outperforms state-of-the-art methods in
existing CSS scenarios, as well as in newly proposed challenging benchmarks.
</p>
<a href="http://arxiv.org/abs/2011.11390" target="_blank">arXiv:2011.11390</a> [<a href="http://arxiv.org/pdf/2011.11390" target="_blank">pdf</a>]

<h2>PCT: Point Cloud Transformer. (arXiv:2012.09688v2 [cs.CV] UPDATED)</h2>
<h3>Meng-Hao Guo, Jun-Xiong Cai, Zheng-Ning Liu, Tai-Jiang Mu, Ralph R. Martin, Shi-Min Hu</h3>
<p>The irregular domain and lack of ordering make it challenging to design deep
neural networks for point cloud processing. This paper presents a novel
framework named Point Cloud Transformer(PCT) for point cloud learning. PCT is
based on Transformer, which achieves huge success in natural language
processing and displays great potential in image processing. It is inherently
permutation invariant for processing a sequence of points, making it
well-suited for point cloud learning. To better capture local context within
the point cloud, we enhance input embedding with the support of farthest point
sampling and nearest neighbor search. Extensive experiments demonstrate that
the PCT achieves the state-of-the-art performance on shape classification, part
segmentation and normal estimation tasks.
</p>
<a href="http://arxiv.org/abs/2012.09688" target="_blank">arXiv:2012.09688</a> [<a href="http://arxiv.org/pdf/2012.09688" target="_blank">pdf</a>]

<h2>Towards Coordinated Robot Motions: End-to-End Learning of Motion Policies on Transform Trees. (arXiv:2012.13457v2 [cs.RO] UPDATED)</h2>
<h3>M. Asif Rana, Anqi Li, Dieter Fox, Sonia Chernova, Byron Boots, Nathan Ratliff</h3>
<p>Generating robot motion that fulfills multiple tasks simultaneously is
challenging due to the geometric constraints imposed by the robot. In this
paper, we propose to solve multi-task problems through learning structured
policies from human demonstrations. Our structured policy is inspired by
RMPflow, a framework for combining subtask policies on different spaces. The
policy structure provides the user an interface to 1) specifying the spaces
that are directly relevant to the completion of the tasks, and 2) designing
policies for certain tasks that do not need to be learned. We derive an
end-to-end learning objective function that is suitable for the multi-task
problem, emphasizing the deviation of motions on task spaces. Furthermore, the
motion generated from the learned policy class is guaranteed to be stable. We
validate the effectiveness of our proposed learning framework through
qualitative and quantitative evaluations on three robotic tasks on a 7-DOF
Rethink Sawyer robot.
</p>
<a href="http://arxiv.org/abs/2012.13457" target="_blank">arXiv:2012.13457</a> [<a href="http://arxiv.org/pdf/2012.13457" target="_blank">pdf</a>]

<h2>A novel shape matching descriptor for real-time hand gesture recognition. (arXiv:2101.03923v2 [cs.CV] UPDATED)</h2>
<h3>Michalis Lazarou, Bo Li, Tania Stathaki</h3>
<p>The current state-of-the-art hand gesture recognition methodologies heavily
rely in the use of machine learning. However there are scenarios that machine
learning cannot be applied successfully, for example in situations where data
is scarce. This is the case when one-to-one matching is required between a
query and a dataset of hand gestures where each gesture represents a unique
class. In situations where learning algorithms cannot be trained, classic
computer vision techniques such as feature extraction can be used to identify
similarities between objects. Shape is one of the most important features that
can be extracted from images, however the most accurate shape matching
algorithms tend to be computationally inefficient for real-time applications.
In this work we present a novel shape matching methodology for real-time hand
gesture recognition. Extensive experiments were carried out comparing our
method with other shape matching methods with respect to accuracy and
computational complexity using our own collected hand gesture dataset and a
modified version of the MPEG-7 dataset.%that is widely used for comparing 2D
shape matching algorithms. Our method outperforms the other methods and
provides a good combination of accuracy and computational efficiency for
real-time applications.
</p>
<a href="http://arxiv.org/abs/2101.03923" target="_blank">arXiv:2101.03923</a> [<a href="http://arxiv.org/pdf/2101.03923" target="_blank">pdf</a>]

<h2>LLA: Loss-aware Label Assignment for Dense Pedestrian Detection. (arXiv:2101.04307v2 [cs.CV] UPDATED)</h2>
<h3>Zheng Ge, Jianfeng Wang, Xin Huang, Songtao Liu, Osamu Yoshie</h3>
<p>Label assignment has been widely studied in general object detection because
of its great impact on detectors' performance. However, none of these works
focus on label assignment in dense pedestrian detection. In this paper, we
propose a simple yet effective assigning strategy called Loss-aware Label
Assignment (LLA) to boost the performance of pedestrian detectors in crowd
scenarios. LLA first calculates classification (cls) and regression (reg)
losses between each anchor and ground-truth (GT) pair. A joint loss is then
defined as the weighted summation of cls and reg losses as the assigning
indicator. Finally, anchors with top K minimum joint losses for a certain GT
box are assigned as its positive anchors. Anchors that are not assigned to any
GT box are considered negative. Loss-aware label assignment is based on an
observation that anchors with lower joint loss usually contain richer semantic
information and thus can better represent their corresponding GT boxes.
Experiments on CrowdHuman and CityPersons show that such a simple label
assigning strategy can boost MR by 9.53% and 5.47% on two famous one-stage
detectors - RetinaNet and FCOS, respectively, demonstrating the effectiveness
of LLA.
</p>
<a href="http://arxiv.org/abs/2101.04307" target="_blank">arXiv:2101.04307</a> [<a href="http://arxiv.org/pdf/2101.04307" target="_blank">pdf</a>]

<h2>Pruning and Quantization for Deep Neural Network Acceleration: A Survey. (arXiv:2101.09671v2 [cs.CV] UPDATED)</h2>
<h3>Tailin Liang, John Glossner, Lei Wang, Shaobo Shi</h3>
<p>Deep neural networks have been applied in many applications exhibiting
extraordinary abilities in the field of computer vision. However, complex
network architectures challenge efficient real-time deployment and require
significant computation resources and energy costs. These challenges can be
overcome through optimizations such as network compression. Network compression
can often be realized with little loss of accuracy. In some cases accuracy may
even improve. This paper provides a survey on two types of network compression:
pruning and quantization. Pruning can be categorized as static if it is
performed offline or dynamic if it is performed at run-time. We compare pruning
techniques and describe criteria used to remove redundant computations. We
discuss trade-offs in element-wise, channel-wise, shape-wise, filter-wise,
layer-wise and even network-wise pruning. Quantization reduces computations by
reducing the precision of the datatype. Weights, biases, and activations may be
quantized typically to 8-bit integers although lower bit width implementations
are also discussed including binary neural networks. Both pruning and
quantization can be used independently or combined. We compare current
techniques, analyze their strengths and weaknesses, present compressed network
accuracy results on a number of frameworks, and provide practical guidance for
compressing networks.
</p>
<a href="http://arxiv.org/abs/2101.09671" target="_blank">arXiv:2101.09671</a> [<a href="http://arxiv.org/pdf/2101.09671" target="_blank">pdf</a>]

<h2>Deepfake Video Detection Using Convolutional Vision Transformer. (arXiv:2102.11126v3 [cs.CV] UPDATED)</h2>
<h3>Deressa Wodajo, Solomon Atnafu</h3>
<p>The rapid advancement of deep learning models that can generate and synthesis
hyper-realistic videos known as Deepfakes and their ease of access to the
general public have raised concern from all concerned bodies to their possible
malicious intent use. Deep learning techniques can now generate faces, swap
faces between two subjects in a video, alter facial expressions, change gender,
and alter facial features, to list a few. These powerful video manipulation
methods have potential use in many fields. However, they also pose a looming
threat to everyone if used for harmful purposes such as identity theft,
phishing, and scam. In this work, we propose a Convolutional Vision Transformer
for the detection of Deepfakes. The Convolutional Vision Transformer has two
components: Convolutional Neural Network (CNN) and Vision Transformer (ViT).
The CNN extracts learnable features while the ViT takes in the learned features
as input and categorizes them using an attention mechanism. We trained our
model on the DeepFake Detection Challenge Dataset (DFDC) and have achieved 91.5
percent accuracy, an AUC value of 0.91, and a loss value of 0.32. Our
contribution is that we have added a CNN module to the ViT architecture and
have achieved a competitive result on the DFDC dataset.
</p>
<a href="http://arxiv.org/abs/2102.11126" target="_blank">arXiv:2102.11126</a> [<a href="http://arxiv.org/pdf/2102.11126" target="_blank">pdf</a>]

<h2>Deep Active Shape Model for Face Alignment and Pose Estimation in Mobile Environment. (arXiv:2103.00119v2 [cs.CV] UPDATED)</h2>
<h3>Ali Pourramezan Fard, Hojjat Abdollahi, Mohammad Mahoor</h3>
<p>Active Shape Model (ASM) is a statistical model of object shapes that
represents a target structure. ASM can guide machine learning algorithms to fit
a set of points representing an object (e.g., face) onto an image. This paper
presents a lightweight Convolutional Neural Network (CNN) architecture with a
loss function being assisted by ASM for face alignment and estimating head pose
in the wild. We use ASM to first guide the network towards learning the
smoother distribution of the facial landmark points. Then, during the training
process, inspired by the transfer learning, we gradually harden the regression
problem and lead the network towards learning the original landmark points
distribution. We define multi-tasks in our loss function that are responsible
for detecting facial landmark points, as well as estimating face pose. Learning
multiple correlated tasks simultaneously builds synergy and improves the
performance of individual tasks. We compare the performance of our proposed
CNN, ASMNet with MobileNetV2 (which is about 2 times bigger ASMNet) in both
face alignment and pose estimation tasks. Experimental results on challenging
datasets show that by using the proposed ASM assisted loss function, ASMNet
performance is comparable with MobileNetV2 in face alignment task. Besides, for
face pose estimation, ASMNet performs much better than MobileNetV2. Moreover,
overall ASMNet achieves an acceptable performance for facial landmark points
detection and pose estimation while having a significantly smaller number of
parameters and floating-point operations comparing to many CNN-based proposed
models.
</p>
<a href="http://arxiv.org/abs/2103.00119" target="_blank">arXiv:2103.00119</a> [<a href="http://arxiv.org/pdf/2103.00119" target="_blank">pdf</a>]

<h2>DONeRF: Towards Real-Time Rendering of Neural Radiance Fields using Depth Oracle Networks. (arXiv:2103.03231v2 [cs.CV] UPDATED)</h2>
<h3>Thomas Neff, Pascal Stadlbauer, Mathias Parger, Andreas Kurz, Chakravarty R. Alla Chaitanya, Anton Kaplanyan, Markus Steinberger</h3>
<p>The recent research explosion around implicit neural representations, such as
NeRF, shows that there is immense potential for implicitly storing high-quality
scene and lighting information in neural networks. However, one major
limitation preventing the use of NeRF in interactive and real-time rendering
applications is the prohibitive computational cost of excessive network
evaluations along each view ray, requiring dozens of petaFLOPS when aiming for
real-time rendering on consumer hardware. In this work, we take a step towards
bringing neural representations closer to practical rendering of synthetic
content in interactive and real-time applications, such as games and virtual
reality. We show that the number of samples required for each view ray can be
significantly reduced when local samples are placed around surfaces in the
scene. To this end, we propose a depth oracle network, which predicts ray
sample locations for each view ray with a single network evaluation. We show
that using a classification network around logarithmically discretized and
spherically warped depth values is essential to encode surface locations rather
than directly estimating depth. The combination of these techniques leads to
DONeRF, a dual network design with a depth oracle network as a first step and a
locally sampled shading network for ray accumulation. With our design, we
reduce the inference costs by up to 48x compared to NeRF. Using an
off-the-shelf inference API in combination with simple compute kernels, we are
the first to render raymarching-based neural representations at interactive
frame rates (15 frames per second at 800x800) on a single GPU. At the same
time, since we focus on the important parts of the scene around surfaces, we
achieve equal or better quality compared to NeRF to enable interactive
high-quality rendering.
</p>
<a href="http://arxiv.org/abs/2103.03231" target="_blank">arXiv:2103.03231</a> [<a href="http://arxiv.org/pdf/2103.03231" target="_blank">pdf</a>]

<h2>OPANAS: One-Shot Path Aggregation Network Architecture Search for Object Detection. (arXiv:2103.04507v3 [cs.CV] UPDATED)</h2>
<h3>Tingting Liang, Yongtao Wang, Zhi Tang, Guosheng Hu, Haibin Ling</h3>
<p>Recently, neural architecture search (NAS) has been exploited to design
feature pyramid networks (FPNs) and achieved promising results for visual
object detection. Encouraged by the success, we propose a novel One-Shot Path
Aggregation Network Architecture Search (OPANAS) algorithm, which significantly
improves both searching efficiency and detection accuracy. Specifically, we
first introduce six heterogeneous information paths to build our search space,
namely top-down, bottom-up, fusing-splitting, scale-equalizing, skip-connect
and none. Second, we propose a novel search space of FPNs, in which each FPN
candidate is represented by a densely-connected directed acyclic graph (each
node is a feature pyramid and each edge is one of the six heterogeneous
information paths). Third, we propose an efficient one-shot search method to
find the optimal path aggregation architecture, that is, we first train a
super-net and then find the optimal candidate with an evolutionary algorithm.
Experimental results demonstrate the efficacy of the proposed OPANAS for object
detection: (1) OPANAS is more efficient than state-of-the-art methods (e.g.,
NAS-FPN and Auto-FPN), at significantly smaller searching cost (e.g., only 4
GPU days on MS-COCO); (2) the optimal architecture found by OPANAS
significantly improves main-stream detectors including RetinaNet, Faster R-CNN
and Cascade R-CNN, by 2.3-3.2 % mAP comparing to their FPN counterparts; and
(3) a new state-of-the-art accuracy-speed trade-off (52.2 % mAP at 7.6 FPS) at
smaller training costs than comparable state-of-the-arts. Code will be released
at https://github.com/VDIGPKU/OPANAS.
</p>
<a href="http://arxiv.org/abs/2103.04507" target="_blank">arXiv:2103.04507</a> [<a href="http://arxiv.org/pdf/2103.04507" target="_blank">pdf</a>]

<h2>Beyond Max-Margin: Class Margin Equilibrium for Few-shot Object Detection. (arXiv:2103.04612v2 [cs.CV] UPDATED)</h2>
<h3>Bohao Li, Boyu Yang, Chang Liu, Feng Liu, Rongrong Ji, Qixiang Ye</h3>
<p>Few-shot object detection has made substantial progressby representing novel
class objects using the feature representation learned upon a set of base class
objects. However,an implicit contradiction between novel class classification
and representation is unfortunately ignored. On the one hand, to achieve
accurate novel class classification, the distributions of either two base
classes must be far away fromeach other (max-margin). On the other hand, to
precisely represent novel classes, the distributions of base classes should be
close to each other to reduce the intra-class distance of novel classes
(min-margin). In this paper, we propose a class margin equilibrium (CME)
approach, with the aim to optimize both feature space partition and novel class
reconstruction in a systematic way. CME first converts the few-shot detection
problem to the few-shot classification problem by using a fully connected layer
to decouple localization features. CME then reserves adequate margin space for
novel classes by introducing simple-yet-effective class margin loss during
feature learning. Finally, CME pursues margin equilibrium by disturbing the
features of novel class instances in an adversarial min-max fashion.
Experiments on Pascal VOC and MS-COCO datasets show that CME significantly
improves upon two baseline detectors (up to 3 ∼ 5% in average), achieving
state-of-the-art performance. Code is available at
https://github.com/Bohao-Lee/CME .
</p>
<a href="http://arxiv.org/abs/2103.04612" target="_blank">arXiv:2103.04612</a> [<a href="http://arxiv.org/pdf/2103.04612" target="_blank">pdf</a>]

<h2>A Scavenger Hunt for Service Robots. (arXiv:2103.05225v2 [cs.RO] UPDATED)</h2>
<h3>Harel Yedidsion, Jennifer Suriadinata, Zifan Xu, Stefan Debruyn, Peter Stone</h3>
<p>Creating robots that can perform general-purpose service tasks in a
human-populated environment has been a longstanding grand challenge for AI and
Robotics research. One particularly valuable skill that is relevant to a wide
variety of tasks is the ability to locate and retrieve objects upon request.
This paper models this skill as a Scavenger Hunt (SH) game, which we formulate
as a variation of the NP-hard stochastic traveling purchaser problem. In this
problem, the goal is to find a set of objects as quickly as possible, given
probability distributions of where they may be found. We investigate the
performance of several solution algorithms for the SH problem, both in
simulation and on a real mobile robot. We use Reinforcement Learning (RL) to
train an agent to plan a minimal cost path, and show that the RL agent can
outperform a range of heuristic algorithms, achieving near optimal performance.
In order to stimulate research on this problem, we introduce a publicly
available software stack and associated website that enable users to upload
scavenger hunts which robots can download, perform, and learn from to
continually improve their performance on future hunts.
</p>
<a href="http://arxiv.org/abs/2103.05225" target="_blank">arXiv:2103.05225</a> [<a href="http://arxiv.org/pdf/2103.05225" target="_blank">pdf</a>]

<h2>FAST-Dynamic-Vision: Detection and Tracking Dynamic Objects with Event and Depth Sensing. (arXiv:2103.05903v2 [cs.RO] UPDATED)</h2>
<h3>Botao He, Haojia Li, Siyuan Wu, Dong Wang, Zhiwei Zhang, Qianli Dong, Chao Xu, Fei Gao</h3>
<p>The development of aerial autonomy has enabled aerial robots to fly agilely
in complex environments. However, dodging fast-moving objects in flight remains
a challenge, limiting the further application of unmanned aerial vehicles
(UAVs). The bottleneck of solving this problem is the accurate perception of
rapid dynamic objects. Recently, event cameras have shown great potential in
solving this problem. This paper presents a complete perception system
including ego-motion compensation, object detection, and trajectory prediction
for fast-moving dynamic objects with low latency and high precision. Firstly,
we propose an accurate ego-motion compensation algorithm by considering both
rotational and translational motion for more robust object detection. Then, for
dynamic object detection, an event camera-based efficient regression algorithm
is designed. Finally, we propose an optimizationbased approach that
asynchronously fuses event and depth cameras for trajectory prediction.
Extensive real-world experiments and benchmarks are performed to validate our
framework. Moreover, our code will be released to benefit related researches.
</p>
<a href="http://arxiv.org/abs/2103.05903" target="_blank">arXiv:2103.05903</a> [<a href="http://arxiv.org/pdf/2103.05903" target="_blank">pdf</a>]

<h2>Dynamical Pose Estimation. (arXiv:2103.06182v2 [cs.CV] UPDATED)</h2>
<h3>Heng Yang, Chris Doran, Jean-Jacques Slotine</h3>
<p>We study the problem of aligning two sets of 3D geometric primitives given
known correspondences. Our first contribution is to show that this primitive
alignment framework unifies five perception problems including point cloud
registration, primitive (mesh) registration, category-level 3D registration,
absolution pose estimation (APE), and category-level APE. Our second
contribution is to propose DynAMical Pose estimation (DAMP), the first general
and practical algorithm to solve primitive alignment problem by simulating
rigid body dynamics arising from virtual springs and damping, where the springs
span the shortest distances between corresponding primitives. Our third
contribution is to apply DAMP to the five perception problems in simulated and
real datasets and demonstrate (i) DAMP always converges to the globally optimal
solution in the first three problems with 3D-3D correspondences; (ii) although
DAMP sometimes converges to suboptimal solutions in the last two problems with
2D-3D correspondences, with a simple scheme for escaping local minima, DAMP
almost always succeeds. Our last contribution is to demystify the surprising
empirical performance of DAMP and formally prove a global convergence result in
the case of point cloud registration by charactering local stability of the
equilibrium points of the underlying dynamical system.
</p>
<a href="http://arxiv.org/abs/2103.06182" target="_blank">arXiv:2103.06182</a> [<a href="http://arxiv.org/pdf/2103.06182" target="_blank">pdf</a>]

