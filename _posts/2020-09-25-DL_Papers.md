---
title: Latest Deep Learning Papers
date: 2020-12-07 21:00:01 +0800
featured-img: DL-logo
categories: [Deep_Learning]
tags: [Deep_Learning]
mathjax: Yes
---

<h1>Your arXiv Feed (288 Articles)</h1>
<h2>Weight Update Skipping: Reducing Training Time for Artificial Neural Networks. (arXiv:2012.02792v1 [cs.LG])</h2>
<h3>Pooneh Safayenikoo, Ismail Akturk</h3>
<p>Artificial Neural Networks (ANNs) are known as state-of-the-art techniques in
Machine Learning (ML) and have achieved outstanding results in data-intensive
applications, such as recognition, classification, and segmentation. These
networks mostly use deep layers of convolution or fully connected layers with
many filters in each layer, demanding a large amount of data and tunable
hyperparameters to achieve competitive accuracy. As a result, storage,
communication, and computational costs of training (in particular training
time) become limiting factors to scale them up. In this paper, we propose a new
training methodology for ANNs that exploits the observation of improvement of
accuracy shows temporal variations which allow us to skip updating weights when
the variation is minuscule. During such time windows, we keep updating bias
which ensures the network still trains and avoids overfitting; however, we
selectively skip updating weights (and their time-consuming computations). Such
a training approach virtually achieves the same accuracy with considerably less
computational cost, thus lower training time. We propose two methods for
updating weights and evaluate them by analyzing four state-of-the-art models,
AlexNet, VGG-11, VGG-16, ResNet-18 on CIFAR datasets. On average, our two
proposed methods called WUS and WUS+LR reduced the training time (compared to
the baseline) by 54%, and 50%, respectively on CIFAR-10; and 43% and 35% on
CIFAR-100, respectively.
</p>
<a href="http://arxiv.org/abs/2012.02792" target="_blank">arXiv:2012.02792</a> [<a href="http://arxiv.org/pdf/2012.02792" target="_blank">pdf</a>]

<h2>Adaptive Explicit Kernel Minkowski Weighted K-means. (arXiv:2012.02805v1 [cs.LG])</h2>
<h3>Amir Aradnia, Maryam Amir Haeri, Mohammad Mehdi Ebadzadeh</h3>
<p>The K-means algorithm is among the most commonly used data clustering
methods. However, the regular K-means can only be applied in the input space
and it is applicable when clusters are linearly separable. The kernel K-means,
which extends K-means into the kernel space, is able to capture nonlinear
structures and identify arbitrarily shaped clusters. However, kernel methods
often operate on the kernel matrix of the data, which scale poorly with the
size of the matrix or suffer from the high clustering cost due to the
repetitive calculations of kernel values. Another issue is that algorithms
access the data only through evaluations of $K(x_i, x_j)$, which limits many
processes that can be done on data through the clustering task. This paper
proposes a method to combine the advantages of the linear and nonlinear
approaches by using driven corresponding approximate finite-dimensional feature
maps based on spectral analysis. Applying approximate finite-dimensional
feature maps were only discussed in the Support Vector Machines (SVM) problems
before. We suggest using this method in kernel K-means era as alleviates
storing huge kernel matrix in memory, further calculating cluster centers more
efficiently and access the data explicitly in feature space. These explicit
feature maps enable us to access the data in the feature space explicitly and
take advantage of K-means extensions in that space. We demonstrate our Explicit
Kernel Minkowski Weighted K-mean (Explicit KMWK-mean) method is able to be more
adopted and find best-fitting values in new space by applying additional
Minkowski exponent and feature weights parameter. Moreover, it can reduce the
impact of concentration on nearest neighbour search by suggesting investigate
among other norms instead of Euclidean norm, includes Minkowski norms and
fractional norms (as an extension of the Minkowski norms with p&lt;1).
</p>
<a href="http://arxiv.org/abs/2012.02805" target="_blank">arXiv:2012.02805</a> [<a href="http://arxiv.org/pdf/2012.02805" target="_blank">pdf</a>]

<h2>Learning summary features of time series for likelihood free inference. (arXiv:2012.02807v1 [stat.ML])</h2>
<h3>Pedro L. C. Rodrigues, Alexandre Gramfort</h3>
<p>There has been an increasing interest from the scientific community in using
likelihood-free inference (LFI) to determine which parameters of a given
simulator model could best describe a set of experimental data. Despite
exciting recent results and a wide range of possible applications, an important
bottleneck of LFI when applied to time series data is the necessity of defining
a set of summary features, often hand-tailored based on domain knowledge. In
this work, we present a data-driven strategy for automatically learning summary
features from univariate time series and apply it to signals generated from
autoregressive-moving-average (ARMA) models and the Van der Pol Oscillator. Our
results indicate that learning summary features from data can compete and even
outperform LFI methods based on hand-crafted values such as autocorrelation
coefficients even in the linear case.
</p>
<a href="http://arxiv.org/abs/2012.02807" target="_blank">arXiv:2012.02807</a> [<a href="http://arxiv.org/pdf/2012.02807" target="_blank">pdf</a>]

<h2>Cross-Modal Generalization: Learning in Low Resource Modalities via Meta-Alignment. (arXiv:2012.02813v1 [cs.LG])</h2>
<h3>Paul Pu Liang, Peter Wu, Liu Ziyin, Louis-Philippe Morency, Ruslan Salakhutdinov</h3>
<p>The natural world is abundant with concepts expressed via visual, acoustic,
tactile, and linguistic modalities. Much of the existing progress in multimodal
learning, however, focuses primarily on problems where the same set of
modalities are present at train and test time, which makes learning in
low-resource modalities particularly difficult. In this work, we propose
algorithms for cross-modal generalization: a learning paradigm to train a model
that can (1) quickly perform new tasks in a target modality (i.e.
meta-learning) and (2) doing so while being trained on a different source
modality. We study a key research question: how can we ensure generalization
across modalities despite using separate encoders for different source and
target modalities? Our solution is based on meta-alignment, a novel method to
align representation spaces using strongly and weakly paired cross-modal data
while ensuring quick generalization to new tasks across different modalities.
We study this problem on 3 classification tasks: text to image, image to audio,
and text to speech. Our results demonstrate strong performance even when the
new target modality has only a few (1-10) labeled samples and in the presence
of noisy labels, a scenario particularly prevalent in low-resource modalities.
</p>
<a href="http://arxiv.org/abs/2012.02813" target="_blank">arXiv:2012.02813</a> [<a href="http://arxiv.org/pdf/2012.02813" target="_blank">pdf</a>]

<h2>Encoding the latent posterior of Bayesian Neural Networks for uncertainty quantification. (arXiv:2012.02818v1 [cs.CV])</h2>
<h3>Gianni Franchi, Andrei Bursuc, Emanuel Aldea, Severine Dubuisson, Isabelle Bloch</h3>
<p>Bayesian neural networks (BNNs) have been long considered an ideal, yet
unscalable solution for improving the robustness and the predictive uncertainty
of deep neural networks. While they could capture more accurately the posterior
distribution of the network parameters, most BNN approaches are either limited
to small networks or rely on constraining assumptions such as parameter
independence. These drawbacks have enabled prominence of simple, but
computationally heavy approaches such as Deep Ensembles, whose training and
testing costs increase linearly with the number of networks. In this work we
aim for efficient deep BNNs amenable to complex computer vision architectures,
e.g. ResNet50 DeepLabV3+, and tasks, e.g. semantic segmentation, with fewer
assumptions on the parameters. We achieve this by leveraging variational
autoencoders (VAEs) to learn the interaction and the latent distribution of the
parameters at each network layer. Our approach, Latent-Posterior BNN (LP-BNN),
is compatible with the recent BatchEnsemble method, leading to highly efficient
({in terms of computation and} memory during both training and testing)
ensembles. LP-BNN s attain competitive results across multiple metrics in
several challenging benchmarks for image classification, semantic segmentation
and out-of-distribution detection.
</p>
<a href="http://arxiv.org/abs/2012.02818" target="_blank">arXiv:2012.02818</a> [<a href="http://arxiv.org/pdf/2012.02818" target="_blank">pdf</a>]

<h2>MPG: A Multi-ingredient Pizza Image Generator with Conditional StyleGANs. (arXiv:2012.02821v1 [cs.CV])</h2>
<h3>Fangda Han, Guoyao Hao, Ricardo Guerrero, Vladimir Pavlovic</h3>
<p>Multilabel conditional image generation is a challenging problem in computer
vision. In this work we propose Multi-ingredient Pizza Generator (MPG), a
conditional Generative Neural Network (GAN) framework for synthesizing
multilabel images. We design MPG based on a state-of-the-art GAN structure
called StyleGAN2, in which we develop a new conditioning technique by enforcing
intermediate feature maps to learn scalewise label information. Because of the
complex nature of the multilabel image generation problem, we also regularize
synthetic image by predicting the corresponding ingredients as well as
encourage the discriminator to distinguish between matched image and mismatched
image. To verify the efficacy of MPG, we test it on Pizza10, which is a
carefully annotated multi-ingredient pizza image dataset. MPG can successfully
generate photo-realist pizza images with desired ingredients. The framework can
be easily extend to other multilabel image generation scenarios.
</p>
<a href="http://arxiv.org/abs/2012.02821" target="_blank">arXiv:2012.02821</a> [<a href="http://arxiv.org/pdf/2012.02821" target="_blank">pdf</a>]

<h2>Deep Learning for Human Mobility: a Survey on Data and Models. (arXiv:2012.02825v1 [cs.LG])</h2>
<h3>Massimiliano Luca, Gianni Barlacchi, Bruno Lepri, Luca Pappalardo</h3>
<p>The study of human mobility is crucial due to its impact on several aspects
of our society, such as disease spreading, urban planning, well-being,
pollution, and more. The proliferation of digital mobility data, such as phone
records, GPS traces, and social media posts, combined with the outstanding
predictive power of artificial intelligence, triggered the application of deep
learning to human mobility. In particular, the literature is focusing on three
tasks: next-location prediction, i.e., predicting an individual's future
locations; crowd flow prediction, i.e., forecasting flows on a geographic
region; and trajectory generation, i.e., generating realistic individual
trajectories. Existing surveys focus on single tasks, data sources, mechanistic
or traditional machine learning approaches, while a comprehensive description
of deep learning solutions is missing. This survey provides: (i) basic notions
on mobility and deep learning; (ii) a review of data sources and public
datasets; (iii) a description of deep learning models and (iv) a discussion
about relevant open challenges. Our survey is a guide to the leading deep
learning solutions to next-location prediction, crowd flow prediction, and
trajectory generation. At the same time, it helps deep learning scientists and
practitioners understand the fundamental concepts and the open challenges of
the study of human mobility.
</p>
<a href="http://arxiv.org/abs/2012.02825" target="_blank">arXiv:2012.02825</a> [<a href="http://arxiv.org/pdf/2012.02825" target="_blank">pdf</a>]

<h2>Orientation Matters: 6-DoF Autonomous Camera Movement for Minimally Invasive Surgery. (arXiv:2012.02836v1 [cs.RO])</h2>
<h3>Alaa Eldin Abdelaal, Nancy Hong, Apeksha Avinash, Divya Budihal, Maram Sakr, Gregory D. Hager, Septimiu E. Salcudean</h3>
<p>We propose a new method for six-degree-of-freedom (6-DoF) autonomous camera
movement for minimally invasive surgery, which, unlike previous methods, takes
into account both the position and orientation information from structures in
the surgical scene. In addition to locating the camera for a good view of the
manipulated object, our autonomous camera takes into account workspace
constraints, including the horizon and safety constraints. We developed a
simulation environment to test our method on the "wire chaser" surgical
training task from validated training curricula in conventional laparoscopy and
robot-assisted surgery. Furthermore, we propose, for the first time, the
application of the proposed autonomous camera method in video-based surgical
skill assessment, an area where videos are typically recorded using fixed
cameras. In a study with N=30 human subjects, we show that video examination of
the autonomous camera view as it tracks the ring motion over the wire leads to
more accurate user error (ring touching the wire) detection than when using a
fixed camera view, or camera movement with a fixed orientation. Our preliminary
work suggests that there are potential benefits to autonomous camera
positioning informed by scene orientation, and this can direct designers of
automated endoscopes and surgical robotic systems, especially when using
chip-on-tip cameras that can be wristed for 6-DoF motion.
</p>
<a href="http://arxiv.org/abs/2012.02836" target="_blank">arXiv:2012.02836</a> [<a href="http://arxiv.org/pdf/2012.02836" target="_blank">pdf</a>]

<h2>One-bit feedback is sufficient for upper confidence bound policies. (arXiv:2012.02876v1 [cs.LG])</h2>
<h3>Daniel Vial, Sanjay Shakkottai, R. Srikant</h3>
<p>We consider a variant of the traditional multi-armed bandit problem in which
each arm is only able to provide one-bit feedback during each pull based on its
past history of rewards. Our main result is the following: given an upper
confidence bound policy which uses full-reward feedback, there exists a coding
scheme for generating one-bit feedback, and a corresponding decoding scheme and
arm selection policy, such that the ratio of the regret achieved by our policy
and the regret of the full-reward feedback policy asymptotically approaches
one.
</p>
<a href="http://arxiv.org/abs/2012.02876" target="_blank">arXiv:2012.02876</a> [<a href="http://arxiv.org/pdf/2012.02876" target="_blank">pdf</a>]

<h2>Discovering Underground Maps from Fashion. (arXiv:2012.02897v1 [cs.CV])</h2>
<h3>Utkarsh Mall, Kavita Bala, Tamara Berg, Kristen Grauman</h3>
<p>The fashion sense -- meaning the clothing styles people wear -- in a
geographical region can reveal information about that region. For example, it
can reflect the kind of activities people do there, or the type of crowds that
frequently visit the region (e.g., tourist hot spot, student neighborhood,
business center). We propose a method to automatically create underground
neighborhood maps of cities by analyzing how people dress. Using publicly
available images from across a city, our method finds neighborhoods with a
similar fashion sense and segments the map without supervision. For 37 cities
worldwide, we show promising results in creating good underground maps, as
evaluated using experiments with human judges and underground map benchmarks
derived from non-image data. Our approach further allows detecting distinct
neighborhoods (what is the most unique region of LA?) and answering analogy
questions between cities (what is the "Downtown LA" of Bogota?).
</p>
<a href="http://arxiv.org/abs/2012.02897" target="_blank">arXiv:2012.02897</a> [<a href="http://arxiv.org/pdf/2012.02897" target="_blank">pdf</a>]

<h2>Learning Interpretable Concept-Based Models with Human Feedback. (arXiv:2012.02898v1 [cs.LG])</h2>
<h3>Isaac Lage, Finale Doshi-Velez</h3>
<p>Machine learning models that first learn a representation of a domain in
terms of human-understandable concepts, then use it to make predictions, have
been proposed to facilitate interpretation and interaction with models trained
on high-dimensional data. However these methods have important limitations: the
way they define concepts are not inherently interpretable, and they assume that
concept labels either exist for individual instances or can easily be acquired
from users. These limitations are particularly acute for high-dimensional
tabular features. We propose an approach for learning a set of transparent
concept definitions in high-dimensional tabular data that relies on users
labeling concept features instead of individual instances. Our method produces
concepts that both align with users' intuitive sense of what a concept means,
and facilitate prediction of the downstream label by a transparent machine
learning model. This ensures that the full model is transparent and intuitive,
and as predictive as possible given this constraint. We demonstrate with
simulated user feedback on real prediction problems, including one in a
clinical domain, that this kind of direct feedback is much more efficient at
learning solutions that align with ground truth concept definitions than
alternative transparent approaches that rely on labeling instances or other
existing interaction mechanisms, while maintaining similar predictive
performance.
</p>
<a href="http://arxiv.org/abs/2012.02898" target="_blank">arXiv:2012.02898</a> [<a href="http://arxiv.org/pdf/2012.02898" target="_blank">pdf</a>]

<h2>Automated Calibration of Mobile Cameras for 3D Reconstruction of Mechanical Pipes. (arXiv:2012.02899v1 [cs.CV])</h2>
<h3>Reza Maalek, Derek Lichti</h3>
<p>This manuscript provides a new framework for calibration of optical
instruments, in particular mobile cameras, using large-scale circular black and
white target fields. New methods were introduced for (i) matching targets
between images; (ii) adjusting the systematic eccentricity error of target
centers; and (iii) iteratively improving the calibration solution through a
free-network self-calibrating bundle adjustment. It was observed that the
proposed target matching effectively matched circular targets in 270 mobile
phone images from a complete calibration laboratory with robustness to Type II
errors. The proposed eccentricity adjustment, which requires only camera
projective matrices from two views, behaved synonymous to available closed-form
solutions, which require several additional object space target information a
priori. Finally, specifically for the case of the mobile devices, the
calibration parameters obtained using our framework was found superior compared
to in-situ calibration for estimating the 3D reconstructed radius of a
mechanical pipe (approximately 45% improvement).
</p>
<a href="http://arxiv.org/abs/2012.02899" target="_blank">arXiv:2012.02899</a> [<a href="http://arxiv.org/pdf/2012.02899" target="_blank">pdf</a>]

<h2>Joint Estimation of Image Representations and their Lie Invariants. (arXiv:2012.02903v1 [cs.AI])</h2>
<h3>Christine Allen-Blanchette, Kostas Daniilidis</h3>
<p>Images encode both the state of the world and its content. The former is
useful for tasks such as planning and control, and the latter for
classification. The automatic extraction of this information is challenging
because of the high-dimensionality and entangled encoding inherent to the image
representation. This article introduces two theoretical approaches aimed at the
resolution of these challenges. The approaches allow for the interpolation and
extrapolation of images from an image sequence by joint estimation of the image
representation and the generators of the sequence dynamics. In the first
approach, the image representations are learned using probabilistic PCA
\cite{tipping1999probabilistic}. The linear-Gaussian conditional distributions
allow for a closed form analytical description of the latent distributions but
assumes the underlying image manifold is a linear subspace. In the second
approach, the image representations are learned using probabilistic nonlinear
PCA which relieves the linear manifold assumption at the cost of requiring a
variational approximation of the latent distributions. In both approaches, the
underlying dynamics of the image sequence are modelled explicitly to
disentangle them from the image representations. The dynamics themselves are
modelled with Lie group structure which enforces the desirable properties of
smoothness and composability of inter-image transformations.
</p>
<a href="http://arxiv.org/abs/2012.02903" target="_blank">arXiv:2012.02903</a> [<a href="http://arxiv.org/pdf/2012.02903" target="_blank">pdf</a>]

<h2>A Knowledge Driven Approach to Adaptive Assistance Using Preference Reasoning and Explanation. (arXiv:2012.02904v1 [cs.RO])</h2>
<h3>Jason R. Wilson, Leilani Gilpin, Irina Rabkina</h3>
<p>There is a need for socially assistive robots (SARs) to provide transparency
in their behavior by explaining their reasoning. Additionally, the reasoning
and explanation should represent the user's preferences and goals. To work
towards satisfying this need for interpretable reasoning and representations,
we propose the robot uses Analogical Theory of Mind to infer what the user is
trying to do and uses the Hint Engine to find an appropriate assistance based
on what the user is trying to do. If the user is unsure or confused, the robot
provides the user with an explanation, generated by the Explanation
Synthesizer. The explanation helps the user understand what the robot inferred
about the user's preferences and why the robot decided to provide the
assistance it gave. A knowledge-driven approach provides transparency to
reasoning about preferences, assistance, and explanations, thereby facilitating
the incorporation of user feedback and allowing the robot to learn and adapt to
the user.
</p>
<a href="http://arxiv.org/abs/2012.02904" target="_blank">arXiv:2012.02904</a> [<a href="http://arxiv.org/pdf/2012.02904" target="_blank">pdf</a>]

<h2>Driver Glance Classification In-the-wild: Towards Generalization Across Domains and Subjects. (arXiv:2012.02906v1 [cs.CV])</h2>
<h3>Sandipan Banerjee, Ajjen Joshi, Jay Turcot, Bryan Reimer, Taniya Mishra</h3>
<p>Distracted drivers are dangerous drivers. Equipping advanced driver
assistance systems (ADAS) with the ability to detect driver distraction can
help prevent accidents and improve driver safety. In order to detect driver
distraction, an ADAS must be able to monitor their visual attention. We propose
a model that takes as input a patch of the driver's face along with a crop of
the eye-region and classifies their glance into 6 coarse regions-of-interest
(ROIs) in the vehicle. We demonstrate that an hourglass network, trained with
an additional reconstruction loss, allows the model to learn stronger
contextual feature representations than a traditional encoder-only
classification module. To make the system robust to subject-specific variations
in appearance and behavior, we design a personalized hourglass model tuned with
an auxiliary input representing the driver's baseline glance behavior. Finally,
we present a weakly supervised multi-domain training regimen that enables the
hourglass to jointly learn representations from different domains (varying in
camera type, angle), utilizing unlabeled samples and thereby reducing
annotation cost.
</p>
<a href="http://arxiv.org/abs/2012.02906" target="_blank">arXiv:2012.02906</a> [<a href="http://arxiv.org/pdf/2012.02906" target="_blank">pdf</a>]

<h2>Depth estimation on embedded computers for robot swarms in forest. (arXiv:2012.02907v1 [cs.RO])</h2>
<h3>Chaoyue Niu, Danesh Tarapore, Klaus-Peter Zauner</h3>
<p>Robot swarms to date are not prepared for autonomous navigation such as path
planning and obstacle detection in forest floor, unable to achieve low-cost.
The development of depth sensing and embedded computing hardware paves the way
for swarm of terrestrial robots. The goal of this research is to improve this
situation by developing low cost vision system for small ground robots to
rapidly perceive terrain. We develop two depth estimation models and evaluate
their performance on Raspberry Pi 4 and Jetson Nano in terms of accuracy,
runtime and model size of depth estimation models, as well as memory
consumption, power draw, temperature, and cost of above two embedded on-board
computers. Our research demonstrated that auto-encoder network deployed on
Raspberry Pi 4 runs at a power consumption of 3.4 W, memory consumption of
about 200 MB, and mean runtime of 13 ms. This can be to meet our requirement
for low-cost swarm of robots. Moreover, our analysis also indicated multi-scale
deep network performs better for predicting depth map from blurred RGB images
caused by camera motion. This paper mainly describes depth estimation models
trained on our own dataset recorded in forest, and their performance on
embedded on-board computers.
</p>
<a href="http://arxiv.org/abs/2012.02907" target="_blank">arXiv:2012.02907</a> [<a href="http://arxiv.org/pdf/2012.02907" target="_blank">pdf</a>]

<h2>Knowledge Distillation Thrives on Data Augmentation. (arXiv:2012.02909v1 [cs.CV])</h2>
<h3>Huan Wang, Suhas Lohit, Michael Jones, Yun Fu</h3>
<p>Knowledge distillation (KD) is a general deep neural network training
framework that uses a teacher model to guide a student model. Many works have
explored the rationale for its success, however, its interplay with data
augmentation (DA) has not been well recognized so far. In this paper, we are
motivated by an interesting observation in classification: KD loss can benefit
from extended training iterations while the cross-entropy loss does not. We
show this disparity arises because of data augmentation: KD loss can tap into
the extra information from different input views brought by DA. By this
explanation, we propose to enhance KD via a stronger data augmentation scheme
(e.g., mixup, CutMix). Furthermore, an even stronger new DA approach is
developed specifically for KD based on the idea of active learning. The
findings and merits of the proposed method are validated by extensive
experiments on CIFAR-100, Tiny ImageNet, and ImageNet datasets. We can achieve
improved performance simply by using the original KD loss combined with
stronger augmentation schemes, compared to existing state-of-the-art methods,
which employ more advanced distillation losses. In addition, when our
approaches are combined with more advanced distillation losses, we can advance
the state-of-the-art performance even more. On top of the encouraging
performance, this paper also sheds some light on explaining the success of
knowledge distillation. The discovered interplay between KD and DA may inspire
more advanced KD algorithms.
</p>
<a href="http://arxiv.org/abs/2012.02909" target="_blank">arXiv:2012.02909</a> [<a href="http://arxiv.org/pdf/2012.02909" target="_blank">pdf</a>]

<h2>Cosine-Pruned Medial Axis: A new method for isometric equivariant and noise-free medial axis extraction. (arXiv:2012.02910v1 [cs.CV])</h2>
<h3>Diego Pati&#xf1;o, John Branch</h3>
<p>We present the CPMA, a new method for medial axis pruning with noise
robustness and equivariance to isometric transformations. Our method leverages
the discrete cosine transform to create smooth versions of a shape $\Omega$. We
use the smooth shapes to compute a score function $\scorefunction$ that filters
out spurious branches from the medial axis. We extensively compare the CPMA
with state-of-the-art pruning methods and highlight our method's noise
robustness and isometric equivariance. We found that our pruning approach
achieves competitive results and yields stable medial axes even in scenarios
with significant contour perturbations.
</p>
<a href="http://arxiv.org/abs/2012.02910" target="_blank">arXiv:2012.02910</a> [<a href="http://arxiv.org/pdf/2012.02910" target="_blank">pdf</a>]

<h2>Multi-head Knowledge Distillation for Model Compression. (arXiv:2012.02911v1 [cs.CV])</h2>
<h3>Huan Wang, Suhas Lohit, Michael Jones, Yun Fu</h3>
<p>Several methods of knowledge distillation have been developed for neural
network compression. While they all use the KL divergence loss to align the
soft outputs of the student model more closely with that of the teacher, the
various methods differ in how the intermediate features of the student are
encouraged to match those of the teacher. In this paper, we propose a
simple-to-implement method using auxiliary classifiers at intermediate layers
for matching features, which we refer to as multi-head knowledge distillation
(MHKD). We add loss terms for training the student that measure the
dissimilarity between student and teacher outputs of the auxiliary classifiers.
At the same time, the proposed method also provides a natural way to measure
differences at the intermediate layers even though the dimensions of the
internal teacher and student features may be different. Through several
experiments in image classification on multiple datasets we show that the
proposed method outperforms prior relevant approaches presented in the
literature.
</p>
<a href="http://arxiv.org/abs/2012.02911" target="_blank">arXiv:2012.02911</a> [<a href="http://arxiv.org/pdf/2012.02911" target="_blank">pdf</a>]

<h2>iGibson, a Simulation Environment for Interactive Tasks in Large RealisticScenes. (arXiv:2012.02924v1 [cs.AI])</h2>
<h3>Bokui Shen, Fei Xia, Chengshu Li, Roberto Mart&#xed;n-Mart&#xed;n, Linxi Fan, Guanzhi Wang, Shyamal Buch, Claudia D&#x27;Arpino, Sanjana Srivastava, Lyne P. Tchapmi, Micael E. Tchapmi, Kent Vainio, Li Fei-Fei, Silvio Savarese</h3>
<p>We present iGibson, a novel simulation environment to develop robotic
solutions for interactive tasks in large-scale realistic scenes. Our
environment contains fifteen fully interactive home-sized scenes populated with
rigid and articulated objects. The scenes are replicas of 3D scanned real-world
homes, aligning the distribution of objects and layout to that of the real
world. iGibson integrates several key features to facilitate the study of
interactive tasks: i) generation of high-quality visual virtual sensor signals
(RGB, depth, segmentation, LiDAR, flow, among others), ii) domain randomization
to change the materials of the objects (both visual texture and dynamics)
and/or their shapes, iii) integrated sampling-based motion planners to generate
collision-free trajectories for robot bases and arms, and iv) intuitive
human-iGibson interface that enables efficient collection of human
demonstrations. Through experiments, we show that the full interactivity of the
scenes enables agents to learn useful visual representations that accelerate
the training of downstream manipulation tasks. We also show that iGibson
features enable the generalization of navigation agents, and that the
human-iGibson interface and integrated motion planners facilitate efficient
imitation learning of simple human demonstrated behaviors. iGibson is
open-sourced with comprehensive examples and documentation. For more
information, visit our project website: this http URL
</p>
<a href="http://arxiv.org/abs/2012.02924" target="_blank">arXiv:2012.02924</a> [<a href="http://arxiv.org/pdf/2012.02924" target="_blank">pdf</a>]

<h2>Cirrus: A Long-range Bi-pattern LiDAR Dataset. (arXiv:2012.02938v1 [cs.CV])</h2>
<h3>Ze Wang, Sihao Ding, Ying Li, Jonas Fenn, Sohini Roychowdhury, Andreas Wallin, Lane Martin, Scott Ryvola, Guillermo Sapiro, Qiang Qiu</h3>
<p>In this paper, we introduce Cirrus, a new long-range bi-pattern LiDAR public
dataset for autonomous driving tasks such as 3D object detection, critical to
highway driving and timely decision making. Our platform is equipped with a
high-resolution video camera and a pair of LiDAR sensors with a 250-meter
effective range, which is significantly longer than existing public datasets.
We record paired point clouds simultaneously using both Gaussian and uniform
scanning patterns. Point density varies significantly across such a long range,
and different scanning patterns further diversify object representation in
LiDAR. In Cirrus, eight categories of objects are exhaustively annotated in the
LiDAR point clouds for the entire effective range. To illustrate the kind of
studies supported by this new dataset, we introduce LiDAR model adaptation
across different ranges, scanning patterns, and sensor devices. Promising
results show the great potential of this new dataset to the robotics and
computer vision communities.
</p>
<a href="http://arxiv.org/abs/2012.02938" target="_blank">arXiv:2012.02938</a> [<a href="http://arxiv.org/pdf/2012.02938" target="_blank">pdf</a>]

<h2>Neurosymbolic AI for Situated Language Understanding. (arXiv:2012.02947v1 [cs.AI])</h2>
<h3>Nikhil Krishnaswamy, James Pustejovsky</h3>
<p>In recent years, data-intensive AI, particularly the domain of natural
language processing and understanding, has seen significant progress driven by
the advent of large datasets and deep neural networks that have sidelined more
classic AI approaches to the field. These systems can apparently demonstrate
sophisticated linguistic understanding or generation capabilities, but often
fail to transfer their skills to situations they have not encountered before.
We argue that computational situated grounding provides a solution to some of
these learning challenges by creating situational representations that both
serve as a formal model of the salient phenomena, and contain rich amounts of
exploitable, task-appropriate data for training new, flexible computational
models. Our model reincorporates some ideas of classic AI into a framework of
neurosymbolic intelligence, using multimodal contextual modeling of interactive
situations, events, and object properties. We discuss how situated grounding
provides diverse data and multiple levels of modeling for a variety of AI
learning challenges, including learning how to interact with object
affordances, learning semantics for novel structures and configurations, and
transferring such learned knowledge to new objects and situations.
</p>
<a href="http://arxiv.org/abs/2012.02947" target="_blank">arXiv:2012.02947</a> [<a href="http://arxiv.org/pdf/2012.02947" target="_blank">pdf</a>]

<h2>Deep Multi-task Learning for Depression Detection and Prediction in Longitudinal Data. (arXiv:2012.02950v1 [cs.LG])</h2>
<h3>Guansong Pang, Ngoc Thien Anh Pham, Emma Baker, Rebecca Bentley, Anton van den Hengel</h3>
<p>Depression is among the most prevalent mental disorders, affecting millions
of people of all ages globally. Machine learning techniques have shown
effective in enabling automated detection and prediction of depression for
early intervention and treatment. However, they are challenged by the relative
scarcity of instances of depression in the data. In this work we introduce a
novel deep multi-task recurrent neural network to tackle this challenge, in
which depression classification is jointly optimized with two auxiliary tasks,
namely one-class metric learning and anomaly ranking. The auxiliary tasks
introduce an inductive bias that improves the classification model's
generalizability on small depression samples. Further, unlike existing studies
that focus on learning depression signs from static data without considering
temporal dynamics, we focus on longitudinal data because i) temporal changes in
personal development and family environment can provide critical cues for
psychiatric disorders and ii) it may enable us to predict depression before the
illness actually occurs. Extensive experimental results on child depression
data show that our model is able to i) achieve nearly perfect performance in
depression detection and ii) accurately predict depression 2-4 years before the
clinical diagnosis, substantially outperforming seven competing methods.
</p>
<a href="http://arxiv.org/abs/2012.02950" target="_blank">arXiv:2012.02950</a> [<a href="http://arxiv.org/pdf/2012.02950" target="_blank">pdf</a>]

<h2>FloodNet: A High Resolution Aerial Imagery Dataset for Post Flood Scene Understanding. (arXiv:2012.02951v1 [cs.CV])</h2>
<h3>Maryam Rahnemoonfar, Tashnim Chowdhury, Argho Sarkar, Debvrat Varshney, Masoud Yari, Robin Murphy</h3>
<p>Visual scene understanding is the core task in making any crucial decision in
any computer vision system. Although popular computer vision datasets like
Cityscapes, MS-COCO, PASCAL provide good benchmarks for several tasks (e.g.
image classification, segmentation, object detection), these datasets are
hardly suitable for post disaster damage assessments. On the other hand,
existing natural disaster datasets include mainly satellite imagery which have
low spatial resolution and a high revisit period. Therefore, they do not have a
scope to provide quick and efficient damage assessment tasks. Unmanned Aerial
Vehicle(UAV) can effortlessly access difficult places during any disaster and
collect high resolution imagery that is required for aforementioned tasks of
computer vision. To address these issues we present a high resolution UAV
imagery, FloodNet, captured after the hurricane Harvey. This dataset
demonstrates the post flooded damages of the affected areas. The images are
labeled pixel-wise for semantic segmentation task and questions are produced
for the task of visual question answering. FloodNet poses several challenges
including detection of flooded roads and buildings and distinguishing between
natural water and flooded water. With the advancement of deep learning
algorithms, we can analyze the impact of any disaster which can make a precise
understanding of the affected areas. In this paper, we compare and contrast the
performances of baseline methods for image classification, semantic
segmentation, and visual question answering on our dataset.
</p>
<a href="http://arxiv.org/abs/2012.02951" target="_blank">arXiv:2012.02951</a> [<a href="http://arxiv.org/pdf/2012.02951" target="_blank">pdf</a>]

<h2>Multi Scale Temporal Graph Networks For Skeleton-based Action Recognition. (arXiv:2012.02970v1 [cs.CV])</h2>
<h3>Tingwei Li, Ruiwen Zhang, Qing Li</h3>
<p>Graph convolutional networks (GCNs) can effectively capture the features of
related nodes and improve the performance of the model. More attention is paid
to employing GCN in Skeleton-Based action recognition. But existing methods
based on GCNs have two problems. First, the consistency of temporal and spatial
features is ignored for extracting features node by node and frame by frame. To
obtain spatiotemporal features simultaneously, we design a generic
representation of skeleton sequences for action recognition and propose a novel
model called Temporal Graph Networks (TGN). Secondly, the adjacency matrix of
the graph describing the relation of joints is mostly dependent on the physical
connection between joints. To appropriately describe the relations between
joints in the skeleton graph, we propose a multi-scale graph strategy, adopting
a full-scale graph, part-scale graph, and core-scale graph to capture the local
features of each joint and the contour features of important joints.
Experiments were carried out on two large datasets and results show that TGN
with our graph strategy outperforms state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2012.02970" target="_blank">arXiv:2012.02970</a> [<a href="http://arxiv.org/pdf/2012.02970" target="_blank">pdf</a>]

<h2>Machine learning for public policy: Do we need to sacrifice accuracy to make models fair?. (arXiv:2012.02972v1 [cs.LG])</h2>
<h3>Kit T. Rodolfa, Hemank Lamba, Rayid Ghani</h3>
<p>Growing applications of machine learning in policy settings have raised
concern for fairness implications, especially for racial minorities, but little
work has studied the practical trade-offs between fairness and accuracy in
real-world settings. This empirical study fills this gap by investigating the
accuracy cost of mitigating disparities across several policy settings,
focusing on the common context of using machine learning to inform benefit
allocation in resource-constrained programs across education, mental health,
criminal justice, and housing safety. In each setting, explicitly focusing on
achieving equity and using our proposed post-hoc disparity mitigation methods,
fairness was substantially improved without sacrificing accuracy, challenging
the commonly held assumption that reducing disparities either requires
accepting an appreciable drop in accuracy or the development of novel, complex
methods.
</p>
<a href="http://arxiv.org/abs/2012.02972" target="_blank">arXiv:2012.02972</a> [<a href="http://arxiv.org/pdf/2012.02972" target="_blank">pdf</a>]

<h2>A Review of Designs and Applications of Echo State Networks. (arXiv:2012.02974v1 [cs.LG])</h2>
<h3>Chenxi Sun, Moxian Song, Shenda Hong, Hongyan Li</h3>
<p>Recurrent Neural Networks (RNNs) have demonstrated their outstanding ability
in sequence tasks and have achieved state-of-the-art in wide range of
applications, such as industrial, medical, economic and linguistic. Echo State
Network (ESN) is simple type of RNNs and has emerged in the last decade as an
alternative to gradient descent training based RNNs. ESN, with a strong
theoretical ground, is practical, conceptually simple, easy to implement. It
avoids non-converging and computationally expensive in the gradient descent
methods. Since ESN was put forward in 2002, abundant existing works have
promoted the progress of ESN, and the recently introduced Deep ESN model opened
the way to uniting the merits of deep learning and ESNs. Besides, the
combinations of ESNs with other machine learning models have also overperformed
baselines in some applications. However, the apparent simplicity of ESNs can
sometimes be deceptive and successfully applying ESNs needs some experience.
Thus, in this paper, we categorize the ESN-based methods to basic ESNs,
DeepESNs and combinations, then analyze them from the perspective of
theoretical studies, network designs and specific applications. Finally, we
discuss the challenges and opportunities of ESNs by summarizing the open
questions and proposing possible future works.
</p>
<a href="http://arxiv.org/abs/2012.02974" target="_blank">arXiv:2012.02974</a> [<a href="http://arxiv.org/pdf/2012.02974" target="_blank">pdf</a>]

<h2>Design and Implementation of Path Trackers for Ackermann Drive based Vehicles. (arXiv:2012.02978v1 [cs.RO])</h2>
<h3>Adarsh Patnaik, Manthan Patel, Vibhakar Mohta, Het Shah, Shubh Agrawal, Aditya Rathore, Ritwik Malik, Debashish Chakravarty, Ranjan Bhattacharya</h3>
<p>This article is an overview of the various literature on path tracking
methods and their implementation in simulation and realistic operating
environments.The scope of this study includes analysis, implementation,tuning,
and comparison of some selected path tracking methods commonly used in practice
for trajectory tracking in autonomous vehicles. Many of these methods are
applicable at low speed due to the linear assumption for the system model, and
hence, some methods are also included that consider nonlinearities present in
lateral vehicle dynamics during high-speed navigation. The performance
evaluation and comparison of tracking methods are carried out on realistic
simulations and a dedicated instrumented passenger car, Mahindra e2o, to get a
performance idea of all the methods in realistic operating conditions and
develop tuning methodologies for each of the methods. It has been observed that
our model predictive control-based approach is able to perform better compared
to the others in medium velocity ranges.
</p>
<a href="http://arxiv.org/abs/2012.02978" target="_blank">arXiv:2012.02978</a> [<a href="http://arxiv.org/pdf/2012.02978" target="_blank">pdf</a>]

<h2>Spatially-Adaptive Pixelwise Networks for Fast Image Translation. (arXiv:2012.02992v1 [cs.CV])</h2>
<h3>Tamar Rott Shaham, Michael Gharbi, Richard Zhang, Eli Shechtman, Tomer Michaeli</h3>
<p>We introduce a new generator architecture, aimed at fast and efficient
high-resolution image-to-image translation. We design the generator to be an
extremely lightweight function of the full-resolution image. In fact, we use
pixel-wise networks; that is, each pixel is processed independently of others,
through a composition of simple affine transformations and nonlinearities. We
take three important steps to equip such a seemingly simple function with
adequate expressivity. First, the parameters of the pixel-wise networks are
spatially varying so they can represent a broader function class than simple
1x1 convolutions. Second, these parameters are predicted by a fast
convolutional network that processes an aggressively low-resolution
representation of the input; Third, we augment the input image with a
sinusoidal encoding of spatial coordinates, which provides an effective
inductive bias for generating realistic novel high-frequency image content. As
a result, our model is up to 18x faster than state-of-the-art baselines. We
achieve this speedup while generating comparable visual quality across
different image resolutions and translation domains.
</p>
<a href="http://arxiv.org/abs/2012.02992" target="_blank">arXiv:2012.02992</a> [<a href="http://arxiv.org/pdf/2012.02992" target="_blank">pdf</a>]

<h2>Attention-Driven Dynamic Graph Convolutional Network for Multi-Label Image Recognition. (arXiv:2012.02994v1 [cs.CV])</h2>
<h3>Jin Ye, Junjun He, Xiaojiang Peng, Wenhao Wu, Yu Qiao</h3>
<p>Recent studies often exploit Graph Convolutional Network (GCN) to model label
dependencies to improve recognition accuracy for multi-label image recognition.
However, constructing a graph by counting the label co-occurrence possibilities
of the training data may degrade model generalizability, especially when there
exist occasional co-occurrence objects in test images. Our goal is to eliminate
such bias and enhance the robustness of the learnt features. To this end, we
propose an Attention-Driven Dynamic Graph Convolutional Network (ADD-GCN) to
dynamically generate a specific graph for each image. ADD-GCN adopts a Dynamic
Graph Convolutional Network (D-GCN) to model the relation of content-aware
category representations that are generated by a Semantic Attention Module
(SAM). Extensive experiments on public multi-label benchmarks demonstrate the
effectiveness of our method, which achieves mAPs of 85.2%, 96.0%, and 95.5% on
MS-COCO, VOC2007, and VOC2012, respectively, and outperforms current
state-of-the-art methods with a clear margin. All codes can be found at
https://github.com/Yejin0111/ADD-GCN.
</p>
<a href="http://arxiv.org/abs/2012.02994" target="_blank">arXiv:2012.02994</a> [<a href="http://arxiv.org/pdf/2012.02994" target="_blank">pdf</a>]

<h2>Fixed Priority Global Scheduling from a Deep Learning Perspective. (arXiv:2012.03002v1 [cs.AI])</h2>
<h3>Hyunsung Lee, Michel Wang, Honguk Woo</h3>
<p>Deep Learning has been recently recognized as one of the feasible solutions
to effectively address combinatorial optimization problems, which are often
considered important yet challenging in various research domains. In this work,
we first present how to adopt Deep Learning for real-time task scheduling
through our preliminary work upon fixed priority global scheduling (FPGS)
problems. We then briefly discuss possible generalizations of Deep Learning
adoption for several realistic and complicated FPGS scenarios, e.g., scheduling
tasks with dependency, mixed-criticality task scheduling. We believe that there
are many opportunities for leveraging advanced Deep Learning technologies to
improve the quality of scheduling in various system configurations and problem
scenarios.
</p>
<a href="http://arxiv.org/abs/2012.03002" target="_blank">arXiv:2012.03002</a> [<a href="http://arxiv.org/pdf/2012.03002" target="_blank">pdf</a>]

<h2>ProMask: Probability Mask for Skeleton Detection. (arXiv:2012.03003v1 [cs.CV])</h2>
<h3>Xiuxiu Bai, Lele Ye, Zhe Liu</h3>
<p>Detecting object skeletons in natural images presents challenging, due to
varied object scales, the complexity of backgrounds and various noises. The
skeleton is a highly compressing shape representation, which can bring some
essential advantages but cause the difficulties of detection. This skeleton
line occupies a rare proportion of an image and is overly sensitive to spatial
position. Inspired by these issues, we propose the ProMask, which is a novel
skeleton detection model. The ProMask includes the probability mask and vector
router. The skeleton probability mask representation explicitly encodes
skeletons with segmentation signals, which can provide more supervised
information to learn and pay more attention to ground-truth skeleton pixels.
Moreover, the vector router module possesses two sets of orthogonal basis
vectors in a two-dimensional space, which can dynamically adjust the predicted
skeleton position. We evaluate our method on the well-known skeleton datasets,
realizing the better performance than state-of-the-art approaches. Especially,
ProMask significantly outperforms the competitive DeepFlux by 6.2% on the
challenging SYM-PASCAL dataset. We consider that our proposed skeleton
probability mask could serve as a solid baseline for future skeleton detection,
since it is very effective and it requires about 10 lines of code.
</p>
<a href="http://arxiv.org/abs/2012.03003" target="_blank">arXiv:2012.03003</a> [<a href="http://arxiv.org/pdf/2012.03003" target="_blank">pdf</a>]

<h2>MFES-HB: Efficient Hyperband with Multi-Fidelity Quality Measurements. (arXiv:2012.03011v1 [cs.LG])</h2>
<h3>Yang Li, Yu Shen, Jiawei Jiang, Jinyang Gao, Ce Zhang, Bin Cui</h3>
<p>Hyperparameter optimization (HPO) is a fundamental problem in automatic
machine learning (AutoML). However, due to the expensive evaluation cost of
models (e.g., training deep learning models or training models on large
datasets), vanilla Bayesian optimization (BO) is typically computationally
infeasible. To alleviate this issue, Hyperband (HB) utilizes the early stopping
mechanism to speed up configuration evaluations by terminating those
badly-performing configurations in advance. This leads to two kinds of quality
measurements: (1) many low-fidelity measurements for configurations that get
early-stopped, and (2) few high-fidelity measurements for configurations that
are evaluated without being early stopped. The state-of-the-art HB-style
method, BOHB, aims to combine the benefits of both BO and HB. Instead of
sampling configurations randomly in HB, BOHB samples configurations based on a
BO surrogate model, which is constructed with the high-fidelity measurements
only. However, the scarcity of high-fidelity measurements greatly hampers the
efficiency of BO to guide the configuration search. In this paper, we present
MFES-HB, an efficient Hyperband method that is capable of utilizing both the
high-fidelity and low-fidelity measurements to accelerate the convergence of
HPO tasks. Designing MFES-HB is not trivial as the low-fidelity measurements
can be biased yet informative to guide the configuration search. Thus we
propose to build a Multi- Fidelity Ensemble Surrogate (MFES) based on the
generalized Product of Experts framework, which can integrate useful
information from multi-fidelity measurements effectively. The empirical studies
on the real-world AutoML tasks demonstrate that MFES-HB can achieve 3.3-8.9x
speedups over the state-of-the-art approach - BOHB.
</p>
<a href="http://arxiv.org/abs/2012.03011" target="_blank">arXiv:2012.03011</a> [<a href="http://arxiv.org/pdf/2012.03011" target="_blank">pdf</a>]

<h2>CIA-SSD: Confident IoU-Aware Single-Stage Object Detector From Point Cloud. (arXiv:2012.03015v1 [cs.CV])</h2>
<h3>Wu Zheng, Weiliang Tang, Sijin Chen, Li Jiang, Chi-Wing Fu</h3>
<p>Existing single-stage detectors for locating objects in point clouds often
treat object localization and category classification as separate tasks, so the
localization accuracy and classification confidence may not well align. To
address this issue, we present a new single-stage detector named the Confident
IoU-Aware Single-Stage object Detector (CIA-SSD). First, we design the
lightweight Spatial-Semantic Feature Aggregation module to adaptively fuse
high-level abstract semantic features and low-level spatial features for
accurate predictions of bounding boxes and classification confidence. Also, the
predicted confidence is further rectified with our designed IoU-aware
confidence rectification module to make the confidence more consistent with the
localization accuracy. Based on the rectified confidence, we further formulate
the Distance-variant IoU-weighted NMS to obtain smoother regressions and avoid
redundant predictions. We experiment CIA-SSD on 3D car detection in the KITTI
test set and show that it attains top performance in terms of the official
ranking metric (moderate AP 80.28%) and above 32 FPS inference speed,
outperforming all prior single-stage detectors. The code is available at
https://github.com/Vegeta2020/CIA-SSD.
</p>
<a href="http://arxiv.org/abs/2012.03015" target="_blank">arXiv:2012.03015</a> [<a href="http://arxiv.org/pdf/2012.03015" target="_blank">pdf</a>]

<h2>A three layer neural network can represent any discontinuous multivariate function. (arXiv:2012.03016v1 [cs.LG])</h2>
<h3>Vugar Ismailov</h3>
<p>In 1987, Hecht-Nielsen showed that any continuous multivariate function could
be implemented by a certain type three-layer neural network. This result was
very much discussed in neural network literature. In this paper we prove that
not only continuous functions but also all discontinuous functions can be
implemented by such neural networks.
</p>
<a href="http://arxiv.org/abs/2012.03016" target="_blank">arXiv:2012.03016</a> [<a href="http://arxiv.org/pdf/2012.03016" target="_blank">pdf</a>]

<h2>Depth estimation from 4D light field videos. (arXiv:2012.03021v1 [cs.CV])</h2>
<h3>Takahiro Kinoshita, Satoshi Ono</h3>
<p>Depth (disparity) estimation from 4D Light Field (LF) images has been a
research topic for the last couple of years. Most studies have focused on depth
estimation from static 4D LF images while not considering temporal information,
i.e., LF videos. This paper proposes an end-to-end neural network architecture
for depth estimation from 4D LF videos. This study also constructs a
medium-scale synthetic 4D LF video dataset that can be used for training deep
learning-based methods. Experimental results using synthetic and real-world 4D
LF videos show that temporal information contributes to the improvement of
depth estimation accuracy in noisy regions. Dataset and code is available at:
https://mediaeng-lfv.github.io/LFV_Disparity_Estimation
</p>
<a href="http://arxiv.org/abs/2012.03021" target="_blank">arXiv:2012.03021</a> [<a href="http://arxiv.org/pdf/2012.03021" target="_blank">pdf</a>]

<h2>Efficient Volumetric Mapping Using Depth Completion With Uncertainty for Robotic Navigation. (arXiv:2012.03023v1 [cs.RO])</h2>
<h3>Marija Popovic, Florian Thomas, Sotiris Papatheodorou, Nils Funk, Teresa Vidal-Calleja, Stefan Leutenegger</h3>
<p>In robotic applications, a key requirement for safe and efficient motion
planning is the ability to map obstacle-free space in unknown, cluttered 3D
environments. However, commodity-grade RGB-D cameras commonly used for sensing
fail to register valid depth values on shiny, glossy, bright, or distant
surfaces, leading to missing data in the map. To address this issue, we propose
a framework leveraging probabilistic depth completion as an additional input
for spatial mapping. We introduce a deep learning architecture providing
uncertainty estimates for the depth completion of RGB-D images. Our pipeline
exploits the inferred missing depth values and depth uncertainty to complement
raw depth images and improve the speed and quality of free space mapping.
Evaluations on synthetic data show that our approach maps significantly more
correct free space with relatively low error when compared against using raw
data alone in different indoor environments; thereby producing more complete
maps that can be directly used for robotic navigation tasks. The performance of
our framework is validated using real-world data.
</p>
<a href="http://arxiv.org/abs/2012.03023" target="_blank">arXiv:2012.03023</a> [<a href="http://arxiv.org/pdf/2012.03023" target="_blank">pdf</a>]

<h2>ParaNet: Deep Regular Representation for 3D Point Clouds. (arXiv:2012.03028v1 [cs.CV])</h2>
<h3>Qijian Zhang, Junhui Hou, Yue Qian, Juyong Zhang, Ying He</h3>
<p>Although convolutional neural networks have achieved remarkable success in
analyzing 2D images/videos, it is still non-trivial to apply the well-developed
2D techniques in regular domains to the irregular 3D point cloud data. To
bridge this gap, we propose ParaNet, a novel end-to-end deep learning
framework, for representing 3D point clouds in a completely regular and nearly
lossless manner. To be specific, ParaNet converts an irregular 3D point cloud
into a regular 2D color image, named point geometry image (PGI), where each
pixel encodes the spatial coordinates of a point. In contrast to conventional
regular representation modalities based on multi-view projection and
voxelization, the proposed representation is differentiable and reversible.
Technically, ParaNet is composed of a surface embedding module, which
parameterizes 3D surface points onto a unit square, and a grid resampling
module, which resamples the embedded 2D manifold over regular dense grids. Note
that ParaNet is unsupervised, i.e., the training simply relies on
reference-free geometry constraints. The PGIs can be seamlessly coupled with a
task network established upon standard and mature techniques for 2D
images/videos to realize a specific task for 3D point clouds. We evaluate
ParaNet over shape classification and point cloud upsampling, in which our
solutions perform favorably against the existing state-of-the-art methods. We
believe such a paradigm will open up many possibilities to advance the progress
of deep learning-based point cloud processing and understanding.
</p>
<a href="http://arxiv.org/abs/2012.03028" target="_blank">arXiv:2012.03028</a> [<a href="http://arxiv.org/pdf/2012.03028" target="_blank">pdf</a>]

<h2>Understanding Bird's-Eye View Semantic HD-Maps Using an Onboard Monocular Camera. (arXiv:2012.03040v1 [cs.CV])</h2>
<h3>Yigit Baran Can, Alexander Liniger, Ozan Unal, Danda Paudel, Luc Van Gool</h3>
<p>Autonomous navigation requires scene understanding of the action-space to
move or anticipate events. For planner agents moving on the ground plane, such
as autonomous vehicles, this translates to scene understanding in the
bird's-eye view. However, the onboard cameras of autonomous cars are
customarily mounted horizontally for a better view of the surrounding. In this
work, we study scene understanding in the form of online estimation of semantic
bird's-eye-view HD-maps using the video input from a single onboard camera. We
study three key aspects of this task, image-level understanding, BEV level
understanding, and the aggregation of temporal information. Based on these
three pillars we propose a novel architecture that combines these three
aspects. In our extensive experiments, we demonstrate that the considered
aspects are complementary to each other for HD-map understanding. Furthermore,
the proposed architecture significantly surpasses the current state-of-the-art.
</p>
<a href="http://arxiv.org/abs/2012.03040" target="_blank">arXiv:2012.03040</a> [<a href="http://arxiv.org/pdf/2012.03040" target="_blank">pdf</a>]

<h2>Self-Supervised Visual Representation Learning from Hierarchical Grouping. (arXiv:2012.03044v1 [cs.CV])</h2>
<h3>Xiao Zhang, Michael Maire</h3>
<p>We create a framework for bootstrapping visual representation learning from a
primitive visual grouping capability. We operationalize grouping via a contour
detector that partitions an image into regions, followed by merging of those
regions into a tree hierarchy. A small supervised dataset suffices for training
this grouping primitive. Across a large unlabeled dataset, we apply this
learned primitive to automatically predict hierarchical region structure. These
predictions serve as guidance for self-supervised contrastive feature learning:
we task a deep network with producing per-pixel embeddings whose pairwise
distances respect the region hierarchy. Experiments demonstrate that our
approach can serve as state-of-the-art generic pre-training, benefiting
downstream tasks. We additionally explore applications to semantic region
search and video-based object instance tracking.
</p>
<a href="http://arxiv.org/abs/2012.03044" target="_blank">arXiv:2012.03044</a> [<a href="http://arxiv.org/pdf/2012.03044" target="_blank">pdf</a>]

<h2>BayLIME: Bayesian Local Interpretable Model-Agnostic Explanations. (arXiv:2012.03058v1 [cs.AI])</h2>
<h3>Xingyu Zhao, Xiaowei Huang, Valentin Robu, David Flynn</h3>
<p>A key impediment to the use of AI is the lacking of transparency, especially
in safety/security critical applications. The black-box nature of AI systems
prevents humans from direct explanations on how the AI makes predictions, which
stimulated Explainable AI (XAI) -- a research field that aims at improving the
trust and transparency of AI systems. In this paper, we introduce a novel XAI
technique, BayLIME, which is a Bayesian modification of the widely used XAI
approach LIME. BayLIME exploits prior knowledge to improve the consistency in
repeated explanations of a single prediction and also the robustness to kernel
settings. Both theoretical analysis and extensive experiments are conducted to
support our conclusions.
</p>
<a href="http://arxiv.org/abs/2012.03058" target="_blank">arXiv:2012.03058</a> [<a href="http://arxiv.org/pdf/2012.03058" target="_blank">pdf</a>]

<h2>A Survey on Deep Learning with Noisy Labels: How to train your model when you cannot trust on the annotations?. (arXiv:2012.03061v1 [cs.LG])</h2>
<h3>Filipe R. Cordeiro, Gustavo Carneiro</h3>
<p>Noisy Labels are commonly present in data sets automatically collected from
the internet, mislabeled by non-specialist annotators, or even specialists in a
challenging task, such as in the medical field. Although deep learning models
have shown significant improvements in different domains, an open issue is
their ability to memorize noisy labels during training, reducing their
generalization potential. As deep learning models depend on correctly labeled
data sets and label correctness is difficult to guarantee, it is crucial to
consider the presence of noisy labels for deep learning training. Several
approaches have been proposed in the literature to improve the training of deep
learning models in the presence of noisy labels. This paper presents a survey
on the main techniques in literature, in which we classify the algorithm in the
following groups: robust losses, sample weighting, sample selection,
meta-learning, and combined approaches. We also present the commonly used
experimental setup, data sets, and results of the state-of-the-art models.
</p>
<a href="http://arxiv.org/abs/2012.03061" target="_blank">arXiv:2012.03061</a> [<a href="http://arxiv.org/pdf/2012.03061" target="_blank">pdf</a>]

<h2>Different Approaches Towards Vertical Track Irregularity Prediction -- A Comparative Study. (arXiv:2012.03062v1 [cs.LG])</h2>
<h3>Yutao Chen, Yu Zhang, Fei Yang</h3>
<p>Railway systems require regular manual maintenance, a large part of which is
dedicated to track deformation inspection. Such deformation might severely
impact trains' runtime security, whereas such inspections remain costly as for
both finance and manpower. Therefore, a more precise, efficient and automated
approach to detect potential railway track deformation is in urgent needs. In
this paper, we proposed an applicational framework for predicting vertical
track irregularities. Our researches are based on large-scale real-world
datasets produced by several operating railways in China. We explored several
different sampling methods and compared traditional machine learning algorithms
for time-series prediction with popular deep learning techniques. Different
ensemble learning methods are also employed for further optimization. The
conclusion is reached that neural networks turn out to be the most performant
and accurate.
</p>
<a href="http://arxiv.org/abs/2012.03062" target="_blank">arXiv:2012.03062</a> [<a href="http://arxiv.org/pdf/2012.03062" target="_blank">pdf</a>]

<h2>FAIROD: Fairness-aware Outlier Detection. (arXiv:2012.03063v1 [cs.LG])</h2>
<h3>Shubhranshu Shekhar, Neil Shah, Leman Akoglu</h3>
<p>Fairness and Outlier Detection (OD) are closely related, as it is exactly the
goal of OD to spot rare, minority samples in a given population. When being a
minority (as defined by protected variables, e.g. race/ethnicity/sex/age) does
not reflect positive-class membership (e.g. criminal/fraud), however, OD
produces unjust outcomes. Surprisingly, fairness-aware OD has been almost
untouched in prior work, as fair machine learning literature mainly focus on
supervised settings. Our work aims to bridge this gap. Specifically, we develop
desiderata capturing well-motivated fairness criteria for OD, and
systematically formalize the fair OD problem. Further, guided by our
desiderata, we propose FairOD, a fairness-aware outlier detector, which has the
following, desirable properties: FairOD (1) does not employ disparate treatment
at test time, (2) aims to flag equal proportions of samples from all groups
(i.e. obtain group fairness, via statistical parity), and (3) strives to flag
truly high-risk fraction of samples within each group. Extensive experiments on
a diverse set of synthetic and real world datasets show that FairOD produces
outcomes that are fair with respect to protected variables, while performing
comparable to (and in some cases, even better than) fairness-agnostic detectors
in terms of detection performance.
</p>
<a href="http://arxiv.org/abs/2012.03063" target="_blank">arXiv:2012.03063</a> [<a href="http://arxiv.org/pdf/2012.03063" target="_blank">pdf</a>]

<h2>Dynamic Neural Radiance Fields for Monocular 4D Facial Avatar Reconstruction. (arXiv:2012.03065v1 [cs.CV])</h2>
<h3>Guy Gafni, Justus Thies, Michael Zollh&#xf6;fer, Matthias Nie&#xdf;ner</h3>
<p>We present dynamic neural radiance fields for modeling the appearance and
dynamics of a human face. Digitally modeling and reconstructing a talking human
is a key building-block for a variety of applications. Especially, for
telepresence applications in AR or VR, a faithful reproduction of the
appearance including novel viewpoints or head-poses is required. In contrast to
state-of-the-art approaches that model the geometry and material properties
explicitly, or are purely image-based, we introduce an implicit representation
of the head based on scene representation networks. To handle the dynamics of
the face, we combine our scene representation network with a low-dimensional
morphable model which provides explicit control over pose and expressions. We
use volumetric rendering to generate images from this hybrid representation and
demonstrate that such a dynamic neural scene representation can be learned from
monocular input data only, without the need of a specialized capture setup. In
our experiments, we show that this learned volumetric representation allows for
photo-realistic image generation that surpasses the quality of state-of-the-art
video-based reenactment methods.
</p>
<a href="http://arxiv.org/abs/2012.03065" target="_blank">arXiv:2012.03065</a> [<a href="http://arxiv.org/pdf/2012.03065" target="_blank">pdf</a>]

<h2>Multi-target normal behaviour models for wind farm condition monitoring. (arXiv:2012.03074v1 [cs.LG])</h2>
<h3>Angela Meyer</h3>
<p>The trend towards larger wind turbines and remote locations of wind farms
fuels the demand for automated condition monitoring and condition-based
maintenance strategies that can reduce the operating cost and avoid unplanned
downtime. Normal behaviour modelling has been introduced to automatically
detect anomalous deviations from normal operation based on the turbine's SCADA
data. A growing number of machine learning models and related threshold values
of the normal behaviour of turbine subsystems are being developed by wind farm
managers to this end. However, these models need to be kept track of, be
maintained and require frequent updates. Every additional model increases the
overall lifetime management effort in practice. This research explores and
benchmarks multi-target machine learning models as a new approach to capturing
a wind turbine's normal behaviour. We present an overview of multi-target
regression methods and motivate their application and benefits in wind turbine
condition monitoring. As a second contribution, we evaluate and benchmark the
performance of multi-target normal behaviour models in a wind turbine case
study. We find that multi-target models are advantageous in comparison to
single-target modelling in that they can substantially reduce the lifecycle
management effort of normal behaviour models without compromising on the
accuracy of the models. Finally, we also outline some areas of future research.
</p>
<a href="http://arxiv.org/abs/2012.03074" target="_blank">arXiv:2012.03074</a> [<a href="http://arxiv.org/pdf/2012.03074" target="_blank">pdf</a>]

<h2>Quantifying Aleatoric and Epistemic Uncertainty Using Density Estimation in Latent Space. (arXiv:2012.03082v1 [cs.LG])</h2>
<h3>Janis Postels, Hermann Blum, Cesar Cadena, Roland Siegwart, Luc Van Gool, Federico Tombari</h3>
<p>The distribution of a neural network's latent representations has been
successfully used to detect Out-of-Distribution (OOD) data. Since OOD detection
denotes a popular benchmark for epistemic uncertainty estimates, this raises
the question of a deeper correlation. This work investigates whether the
distribution of latent representations indeed contains information about the
uncertainty associated with the predictions of a neural network. Prior work
identifies epistemic uncertainty with the surprise, thus the negative
log-likelihood, of observing a particular latent representation, which we
verify empirically. Moreover, we demonstrate that the output-conditional
distribution of hidden representations allows quantifying aleatoric uncertainty
via the entropy of the predictive distribution. We analyze epistemic and
aleatoric uncertainty inferred from the representations of different layers and
conclude with the exciting finding that the hidden repesentations of a
deterministic neural network indeed contain information about its uncertainty.
We verify our findings on both classification and regression models.
</p>
<a href="http://arxiv.org/abs/2012.03082" target="_blank">arXiv:2012.03082</a> [<a href="http://arxiv.org/pdf/2012.03082" target="_blank">pdf</a>]

<h2>Graph Mixture Density Networks. (arXiv:2012.03085v1 [cs.LG])</h2>
<h3>Federico Errica, Davide Bacciu, Alessio Micheli</h3>
<p>We introduce the Graph Mixture Density Network, a new family of machine
learning models that can fit multimodal output distributions conditioned on
arbitrary input graphs. By combining ideas from mixture models and graph
representation learning, we address a broad class of challenging regression
problems that rely on structured data. Our main contribution is the design and
evaluation of our method on large stochastic epidemic simulations conditioned
on random graphs. We show that there is a significant improvement in the
likelihood of an epidemic outcome when taking into account both multimodality
and structure. In addition, we investigate how to \textit{implicitly} retain
structural information in node representations by computing the distance
between distributions of adjacent nodes, and the technique is tested on two
structure reconstruction tasks with very good accuracy. Graph Mixture Density
Networks open appealing research opportunities in the study of
structure-dependent phenomena that exhibit non-trivial conditional output
distributions.
</p>
<a href="http://arxiv.org/abs/2012.03085" target="_blank">arXiv:2012.03085</a> [<a href="http://arxiv.org/pdf/2012.03085" target="_blank">pdf</a>]

<h2>MyFood: A Food Segmentation and Classification System to Aid Nutritional Monitoring. (arXiv:2012.03087v1 [cs.CV])</h2>
<h3>Charles N. C. Freitas, Filipe R. Cordeiro, Valmir Macario</h3>
<p>The absence of food monitoring has contributed significantly to the increase
in the population's weight. Due to the lack of time and busy routines, most
people do not control and record what is consumed in their diet. Some solutions
have been proposed in computer vision to recognize food images, but few are
specialized in nutritional monitoring. This work presents the development of an
intelligent system that classifies and segments food presented in images to
help the automatic monitoring of user diet and nutritional intake. This work
shows a comparative study of state-of-the-art methods for image classification
and segmentation, applied to food recognition. In our methodology, we compare
the FCN, ENet, SegNet, DeepLabV3+, and Mask RCNN algorithms. We build a dataset
composed of the most consumed Brazilian food types, containing nine classes and
a total of 1250 images. The models were evaluated using the following metrics:
Intersection over Union, Sensitivity, Specificity, Balanced Precision, and
Positive Predefined Value. We also propose an system integrated into a mobile
application that automatically recognizes and estimates the nutrients in a
meal, assisting people with better nutritional monitoring. The proposed
solution showed better results than the existing ones in the market. The
dataset is publicly available at the following link
this http URL
</p>
<a href="http://arxiv.org/abs/2012.03087" target="_blank">arXiv:2012.03087</a> [<a href="http://arxiv.org/pdf/2012.03087" target="_blank">pdf</a>]

<h2>Understanding Interpretability by generalized distillation in Supervised Classification. (arXiv:2012.03089v1 [cs.LG])</h2>
<h3>Adit Agarwal, Dr. K.K. Shukla, Arjan Kuijper, Anirban Mukhopadhyay</h3>
<p>The ability to interpret decisions taken by Machine Learning (ML) models is
fundamental to encourage trust and reliability in different practical
applications. Recent interpretation strategies focus on human understanding of
the underlying decision mechanisms of the complex ML models. However, these
strategies are restricted by the subjective biases of humans. To dissociate
from such human biases, we propose an interpretation-by-distillation
formulation that is defined relative to other ML models. We generalize the
distillation technique for quantifying interpretability, using an
information-theoretic perspective, removing the role of ground-truth from the
definition of interpretability. Our work defines the entropy of supervised
classification models, providing bounds on the entropy of Piece-Wise Linear
Neural Networks (PWLNs), along with the first theoretical bounds on the
interpretability of PWLNs. We evaluate our proposed framework on the MNIST,
Fashion-MNIST and Stanford40 datasets and demonstrate the applicability of the
proposed theoretical framework in different supervised classification
scenarios.
</p>
<a href="http://arxiv.org/abs/2012.03089" target="_blank">arXiv:2012.03089</a> [<a href="http://arxiv.org/pdf/2012.03089" target="_blank">pdf</a>]

<h2>Semantic Segmentation of Medium-Resolution Satellite Imagery using Conditional Generative Adversarial Networks. (arXiv:2012.03093v1 [cs.CV])</h2>
<h3>Aditya Kulkarni, Tharun Mohandoss, Daniel Northrup, Ernest Mwebaze, Hamed Alemohammad</h3>
<p>Semantic segmentation of satellite imagery is a common approach to identify
patterns and detect changes around the planet. Most of the state-of-the-art
semantic segmentation models are trained in a fully supervised way using
Convolutional Neural Network (CNN). The generalization property of CNN is poor
for satellite imagery because the data can be very diverse in terms of
landscape types, image resolutions, and scarcity of labels for different
geographies and seasons. Hence, the performance of CNN doesn't translate well
to images from unseen regions or seasons. Inspired by Conditional Generative
Adversarial Networks (CGAN) based approach of image-to-image translation for
high-resolution satellite imagery, we propose a CGAN framework for land cover
classification using medium-resolution Sentinel-2 imagery. We find that the
CGAN model outperforms the CNN model of similar complexity by a significant
margin on an unseen imbalanced test dataset.
</p>
<a href="http://arxiv.org/abs/2012.03093" target="_blank">arXiv:2012.03093</a> [<a href="http://arxiv.org/pdf/2012.03093" target="_blank">pdf</a>]

<h2>RLOC: Terrain-Aware Legged Locomotion using Reinforcement Learning and Optimal Control. (arXiv:2012.03094v1 [cs.RO])</h2>
<h3>Siddhant Gangapurwala, Mathieu Geisert, Romeo Orsolino, Maurice Fallon, Ioannis Havoutis</h3>
<p>We present a unified model-based and data-driven approach for quadrupedal
planning and control to achieve dynamic locomotion over uneven terrain. We
utilize on-board proprioceptive and exteroceptive feedback to map sensory
information and desired base velocity commands into footstep plans using a
reinforcement learning (RL) policy trained in simulation over a wide range of
procedurally generated terrains. When ran online, the system tracks the
generated footstep plans using a model-based controller. We evaluate the
robustness of our method over a wide variety of complex terrains. It exhibits
behaviors which prioritize stability over aggressive locomotion. Additionally,
we introduce two ancillary RL policies for corrective whole-body motion
tracking and recovery control. These policies account for changes in physical
parameters and external perturbations. We train and evaluate our framework on a
complex quadrupedal system, ANYmal version B, and demonstrate transferability
to a larger and heavier robot, ANYmal C, without requiring retraining.
</p>
<a href="http://arxiv.org/abs/2012.03094" target="_blank">arXiv:2012.03094</a> [<a href="http://arxiv.org/pdf/2012.03094" target="_blank">pdf</a>]

<h2>Parallel Blockwise Knowledge Distillation for Deep Neural Network Compression. (arXiv:2012.03096v1 [cs.LG])</h2>
<h3>Cody Blakeney, Xiaomin Li, Yan Yan, Ziliang Zong</h3>
<p>Deep neural networks (DNNs) have been extremely successful in solving many
challenging AI tasks in natural language processing, speech recognition, and
computer vision nowadays. However, DNNs are typically computation intensive,
memory demanding, and power hungry, which significantly limits their usage on
platforms with constrained resources. Therefore, a variety of compression
techniques (e.g. quantization, pruning, and knowledge distillation) have been
proposed to reduce the size and power consumption of DNNs. Blockwise knowledge
distillation is one of the compression techniques that can effectively reduce
the size of a highly complex DNN. However, it is not widely adopted due to its
long training time. In this paper, we propose a novel parallel blockwise
distillation algorithm to accelerate the distillation process of sophisticated
DNNs. Our algorithm leverages local information to conduct independent
blockwise distillation, utilizes depthwise separable layers as the efficient
replacement block architecture, and properly addresses limiting factors (e.g.
dependency, synchronization, and load balancing) that affect parallelism. The
experimental results running on an AMD server with four Geforce RTX 2080Ti GPUs
show that our algorithm can achieve 3x speedup plus 19% energy savings on VGG
distillation, and 3.5x speedup plus 29% energy savings on ResNet distillation,
both with negligible accuracy loss. The speedup of ResNet distillation can be
further improved to 3.87 when using four RTX6000 GPUs in a distributed cluster.
</p>
<a href="http://arxiv.org/abs/2012.03096" target="_blank">arXiv:2012.03096</a> [<a href="http://arxiv.org/pdf/2012.03096" target="_blank">pdf</a>]

<h2>Obstacle avoidance and path finding for mobile robot navigation. (arXiv:2012.03105v1 [cs.RO])</h2>
<h3>Poojith Kotikalapudi, Vinayak Elangovan</h3>
<p>This paper investigates different methods to detect obstacles ahead of a
robot using a camera in the robot, an aerial camera, and an ultrasound sensor.
We also explored various efficient path finding methods for the robot to
navigate to the target source. Single and multi-iteration angle-based
navigation algorithms were developed. The theta-based path finding algorithms
were compared with the Dijkstra Algorithm and their performance were analyzed.
</p>
<a href="http://arxiv.org/abs/2012.03105" target="_blank">arXiv:2012.03105</a> [<a href="http://arxiv.org/pdf/2012.03105" target="_blank">pdf</a>]

<h2>When Do Curricula Work?. (arXiv:2012.03107v1 [cs.LG])</h2>
<h3>Xiaoxia Wu, Ethan Dyer, Behnam Neyshabur</h3>
<p>Inspired by human learning, researchers have proposed ordering examples
during training based on their difficulty. Both curriculum learning, exposing a
network to easier examples early in training, and anti-curriculum learning,
showing the most difficult examples first, have been suggested as improvements
to the standard i.i.d. training. In this work, we set out to investigate the
relative benefits of ordered learning. We first investigate the \emph{implicit
curricula} resulting from architectural and optimization bias and find that
samples are learned in a highly consistent order. Next, to quantify the benefit
of \emph{explicit curricula}, we conduct extensive experiments over thousands
of orderings spanning three kinds of learning: curriculum, anti-curriculum, and
random-curriculum -- in which the size of the training dataset is dynamically
increased over time, but the examples are randomly ordered. We find that for
standard benchmark datasets, curricula have only marginal benefits, and that
randomly ordered samples perform as well or better than curricula and
anti-curricula, suggesting that any benefit is entirely due to the dynamic
training set size. Inspired by common use cases of curriculum learning in
practice, we investigate the role of limited training time budget and noisy
data in the success of curriculum learning. Our experiments demonstrate that
curriculum, but not anti-curriculum can indeed improve the performance either
with limited training time budget or in existence of noisy data.
</p>
<a href="http://arxiv.org/abs/2012.03107" target="_blank">arXiv:2012.03107</a> [<a href="http://arxiv.org/pdf/2012.03107" target="_blank">pdf</a>]

<h2>Generating Synthetic Multispectral Satellite Imagery from Sentinel-2. (arXiv:2012.03108v1 [cs.CV])</h2>
<h3>Tharun Mohandoss, Aditya Kulkarni, Daniel Northrup, Ernest Mwebaze, Hamed Alemohammad</h3>
<p>Multi-spectral satellite imagery provides valuable data at global scale for
many environmental and socio-economic applications. Building supervised machine
learning models based on these imagery, however, may require ground reference
labels which are not available at global scale. Here, we propose a generative
model to produce multi-resolution multi-spectral imagery based on Sentinel-2
data. The resulting synthetic images are indistinguishable from real ones by
humans. This technique paves the road for future work to generate labeled
synthetic imagery that can be used for data augmentation in data scarce regions
and applications.
</p>
<a href="http://arxiv.org/abs/2012.03108" target="_blank">arXiv:2012.03108</a> [<a href="http://arxiv.org/pdf/2012.03108" target="_blank">pdf</a>]

<h2>Spectral Distribution aware Image Generation. (arXiv:2012.03110v1 [cs.CV])</h2>
<h3>Steffen Jung, Margret Keuper</h3>
<p>Recent advances in deep generative models for photo-realistic images have led
to high quality visual results. Such models learn to generate data from a given
training distribution such that generated images can not be easily
distinguished from real images by the human eye. Yet, recent work on the
detection of such fake images pointed out that they are actually easily
distinguishable by artifacts in their frequency spectra. In this paper, we
propose to generate images according to the frequency distribution of the real
data by employing a spectral discriminator. The proposed discriminator is
lightweight, modular and works stably with different commonly used GAN losses.
We show that the resulting models can better generate images with realistic
frequency spectra, which are thus harder to detect by this cue.
</p>
<a href="http://arxiv.org/abs/2012.03110" target="_blank">arXiv:2012.03110</a> [<a href="http://arxiv.org/pdf/2012.03110" target="_blank">pdf</a>]

<h2>LandCoverNet: A global benchmark land cover classification training dataset. (arXiv:2012.03111v1 [cs.CV])</h2>
<h3>Hamed Alemohammad, Kevin Booth</h3>
<p>Regularly updated and accurate land cover maps are essential for monitoring
14 of the 17 Sustainable Development Goals. Multispectral satellite imagery
provide high-quality and valuable information at global scale that can be used
to develop land cover classification models. However, such a global application
requires a geographically diverse training dataset. Here, we present
LandCoverNet, a global training dataset for land cover classification based on
Sentinel-2 observations at 10m spatial resolution. Land cover class labels are
defined based on annual time-series of Sentinel-2, and verified by consensus
among three human annotators.
</p>
<a href="http://arxiv.org/abs/2012.03111" target="_blank">arXiv:2012.03111</a> [<a href="http://arxiv.org/pdf/2012.03111" target="_blank">pdf</a>]

<h2>GpuShareSat: a SAT solver using the GPU for clause sharing. (arXiv:2012.03119v1 [cs.AI])</h2>
<h3>Nicolas Prevot</h3>
<p>We describe a SAT solver using both the GPU (CUDA) and the CPU with a new
clause exchange strategy. The CPU runs a classic multithreaded CDCL SAT solver.
EachCPU thread exports all the clauses it learns to the GPU. The GPU makes a
heavy usage of bitwise operations. It notices when a clause would have been
used by a CPU thread and notifies that thread, in which case it imports that
clause. This relies on the GPU repeatedly testing millions of clauses against
hundreds of assignments. All the clauses are tested independantly from each
other (which allows the GPU massively parallel approach), but against all the
assignments at once, using bitwise operations. This allows CPU threads to only
import clauses which would have been useful for them. Our solver is based upon
glucose-syrup. Experiments show that this leads to a strong performance
improvement, with 22 more instances solved on the SAT 2020 competition than
glucose-syrup.
</p>
<a href="http://arxiv.org/abs/2012.03119" target="_blank">arXiv:2012.03119</a> [<a href="http://arxiv.org/pdf/2012.03119" target="_blank">pdf</a>]

<h2>It's All Around You: Range-Guided Cylindrical Network for 3D Object Detection. (arXiv:2012.03121v1 [cs.CV])</h2>
<h3>Meytal Rapoport-Lavie, Dan Raviv</h3>
<p>Modern perception systems in the field of autonomous driving rely on 3D data
analysis. LiDAR sensors are frequently used to acquire such data due to their
increased resilience to different lighting conditions. Although rotating LiDAR
scanners produce ring-shaped patterns in space, most networks analyze their
data using an orthogonal voxel sampling strategy. This work presents a novel
approach for analyzing 3D data produced by 360-degree depth scanners, utilizing
a more suitable coordinate system, which is aligned with the scanning pattern.
Furthermore, we introduce a novel notion of range-guided convolutions, adapting
the receptive field by distance from the ego vehicle and the object's scale.
Our network demonstrates powerful results on the nuScenes challenge, comparable
to current state-of-the-art architectures. The backbone architecture introduced
in this work can be easily integrated onto other pipelines as well.
</p>
<a href="http://arxiv.org/abs/2012.03121" target="_blank">arXiv:2012.03121</a> [<a href="http://arxiv.org/pdf/2012.03121" target="_blank">pdf</a>]

<h2>Recent Developments in Boolean Matrix Factorization. (arXiv:2012.03127v1 [cs.LG])</h2>
<h3>Pauli Miettinen, Stefan Neumann</h3>
<p>The goal of Boolean Matrix Factorization (BMF) is to approximate a given
binary matrix as the product of two low-rank binary factor matrices, where the
product of the factor matrices is computed under the Boolean algebra. While the
problem is computationally hard, it is also attractive because the binary
nature of the factor matrices makes them highly interpretable. In the last
decade, BMF has received a considerable amount of attention in the data mining
and formal concept analysis communities and, more recently, the machine
learning and the theory communities also started studying BMF. In this survey,
we give a concise summary of the efforts of all of these communities and raise
some open questions which in our opinion require further investigation.
</p>
<a href="http://arxiv.org/abs/2012.03127" target="_blank">arXiv:2012.03127</a> [<a href="http://arxiv.org/pdf/2012.03127" target="_blank">pdf</a>]

<h2>YieldNet: A Convolutional Neural Network for Simultaneous Corn and Soybean Yield Prediction Based on Remote Sensing Data. (arXiv:2012.03129v1 [cs.CV])</h2>
<h3>Saeed Khaki, Hieu Pham, Lizhi Wang</h3>
<p>Large scale crop yield estimation is, in part, made possible due to the
availability of remote sensing data allowing for the continuous monitoring of
crops throughout its growth state. Having this information allows stakeholders
the ability to make real-time decisions to maximize yield potential. Although
various models exist that predict yield from remote sensing data, there
currently does not exist an approach that can estimate yield for multiple crops
simultaneously, and thus leads to more accurate predictions. A model that
predicts yield of multiple crops and concurrently considers the interaction
between multiple crop's yield. We propose a new model called YieldNet which
utilizes a novel deep learning framework that uses transfer learning between
corn and soybean yield predictions by sharing the weights of the backbone
feature extractor. Additionally, to consider the multi-target response
variable, we propose a new loss function. Numerical results demonstrate that
our proposed method accurately predicts yield from one to four months before
the harvest, and is competitive to other state-of-the-art approaches.
</p>
<a href="http://arxiv.org/abs/2012.03129" target="_blank">arXiv:2012.03129</a> [<a href="http://arxiv.org/pdf/2012.03129" target="_blank">pdf</a>]

<h2>Rejoinder: New Objectives for Policy Learning. (arXiv:2012.03130v1 [stat.ML])</h2>
<h3>Nathan Kallus</h3>
<p>I provide a rejoinder for discussion of "More Efficient Policy Learning via
Optimal Retargeting" to appear in the Journal of the American Statistical
Association with discussion by Oliver Dukes and Stijn Vansteelandt; Sijia Li,
Xiudi Li, and Alex Luedtkeand; and Muxuan Liang and Yingqi Zhao.
</p>
<a href="http://arxiv.org/abs/2012.03130" target="_blank">arXiv:2012.03130</a> [<a href="http://arxiv.org/pdf/2012.03130" target="_blank">pdf</a>]

<h2>Learning Poisson systems and trajectories of autonomous systems via Poisson neural networks. (arXiv:2012.03133v1 [cs.LG])</h2>
<h3>Pengzhan Jin, Zhen Zhang, Ioannis G. Kevrekidis, George Em Karniadakis</h3>
<p>We propose the Poisson neural networks (PNNs) to learn Poisson systems and
trajectories of autonomous systems from data. Based on the Darboux-Lie theorem,
the phase flow of a Poisson system can be written as the composition of (1) a
coordinate transformation, (2) an extended symplectic map and (3) the inverse
of the transformation. In this work, we extend this result to the unknotted
trajectories of autonomous systems. We employ structured neural networks with
physical priors to approximate the three aforementioned maps. We demonstrate
through several simulations that PNNs are capable of handling very accurately
several challenging tasks, including the motion of a particle in the
electromagnetic potential, the nonlinear Schr{\"o}dinger equation, and pixel
observations of the two-body problem.
</p>
<a href="http://arxiv.org/abs/2012.03133" target="_blank">arXiv:2012.03133</a> [<a href="http://arxiv.org/pdf/2012.03133" target="_blank">pdf</a>]

<h2>Deep Archimedean Copulas. (arXiv:2012.03137v1 [cs.LG])</h2>
<h3>Chun Kai Ling, Fei Fang, J. Zico Kolter</h3>
<p>A central problem in machine learning and statistics is to model joint
densities of random variables from data. Copulas are joint cumulative
distribution functions with uniform marginal distributions and are used to
capture interdependencies in isolation from marginals. Copulas are widely used
within statistics, but have not gained traction in the context of modern deep
learning. In this paper, we introduce ACNet, a novel differentiable neural
network architecture that enforces structural properties and enables one to
learn an important class of copulas--Archimedean Copulas. Unlike Generative
Adversarial Networks, Variational Autoencoders, or Normalizing Flow methods,
which learn either densities or the generative process directly, ACNet learns a
generator of the copula, which implicitly defines the cumulative distribution
function of a joint distribution. We give a probabilistic interpretation of the
network parameters of ACNet and use this to derive a simple but efficient
sampling algorithm for the learned copula. Our experiments show that ACNet is
able to both approximate common Archimedean Copulas and generate new copulas
which may provide better fits to data.
</p>
<a href="http://arxiv.org/abs/2012.03137" target="_blank">arXiv:2012.03137</a> [<a href="http://arxiv.org/pdf/2012.03137" target="_blank">pdf</a>]

<h2>Biclustering and Boolean Matrix Factorization in Data Streams. (arXiv:2012.03138v1 [cs.LG])</h2>
<h3>Stefan Neumann, Pauli Miettinen</h3>
<p>We study the clustering of bipartite graphs and Boolean matrix factorization
in data streams. We consider a streaming setting in which the vertices from the
left side of the graph arrive one by one together with all of their incident
edges. We provide an algorithm that, after one pass over the stream, recovers
the set of clusters on the right side of the graph using sublinear space; to
the best of our knowledge, this is the first algorithm with this property. We
also show that after a second pass over the stream, the left clusters of the
bipartite graph can be recovered and we show how to extend our algorithm to
solve the Boolean matrix factorization problem (by exploiting the
correspondence of Boolean matrices and bipartite graphs). We evaluate an
implementation of the algorithm on synthetic data and on real-world data. On
real-world datasets the algorithm is orders of magnitudes faster than a static
baseline algorithm while providing quality results within a factor 2 of the
baseline algorithm. Our algorithm scales linearly in the number of edges in the
graph. Finally, we analyze the algorithm theoretically and provide sufficient
conditions under which the algorithm recovers a set of planted clusters under a
standard random graph model.
</p>
<a href="http://arxiv.org/abs/2012.03138" target="_blank">arXiv:2012.03138</a> [<a href="http://arxiv.org/pdf/2012.03138" target="_blank">pdf</a>]

<h2>Selective Eye-gaze Augmentation To Enhance Imitation Learning In Atari Games. (arXiv:2012.03145v1 [cs.LG])</h2>
<h3>Chaitanya Thammineni, Hemanth Manjunatha, Ehsan T. Esfahani</h3>
<p>This paper presents the selective use of eye-gaze information in learning
human actions in Atari games. Vast evidence suggests that our eye movement
convey a wealth of information about the direction of our attention and mental
states and encode the information necessary to complete a task. Based on this
evidence, we hypothesize that selective use of eye-gaze, as a clue for
attention direction, will enhance the learning from demonstration. For this
purpose, we propose a selective eye-gaze augmentation (SEA) network that learns
when to use the eye-gaze information. The proposed network architecture
consists of three sub-networks: gaze prediction, gating, and action prediction
network. Using the prior 4 game frames, a gaze map is predicted by the gaze
prediction network which is used for augmenting the input frame. The gating
network will determine whether the predicted gaze map should be used in
learning and is fed to the final network to predict the action at the current
frame. To validate this approach, we use publicly available Atari Human
Eye-Tracking And Demonstration (Atari-HEAD) dataset consists of 20 Atari games
with 28 million human demonstrations and 328 million eye-gazes (over game
frames) collected from four subjects. We demonstrate the efficacy of selective
eye-gaze augmentation in comparison with state of the art Attention Guided
Imitation Learning (AGIL), Behavior Cloning (BC). The results indicate that the
selective augmentation approach (the SEA network) performs significantly better
than the AGIL and BC. Moreover, to demonstrate the significance of selective
use of gaze through the gating network, we compare our approach with the random
selection of the gaze. Even in this case, the SEA network performs
significantly better validating the advantage of selectively using the gaze in
demonstration learning.
</p>
<a href="http://arxiv.org/abs/2012.03145" target="_blank">arXiv:2012.03145</a> [<a href="http://arxiv.org/pdf/2012.03145" target="_blank">pdf</a>]

<h2>Adaptive Weighted Discriminator for Training Generative Adversarial Networks. (arXiv:2012.03149v1 [cs.LG])</h2>
<h3>Vasily Zadorozhnyy, Qiang Cheng, Qiang Ye</h3>
<p>Generative adversarial network (GAN) has become one of the most important
neural network models for classical unsupervised machine learning. A variety of
discriminator loss functions have been developed to train GAN's discriminators
and they all have a common structure: a sum of real and fake losses that only
depends on the actual and generated data respectively. One challenge associated
with an equally weighted sum of two losses is that the training may benefit one
loss but harm the other, which we show causes instability and mode collapse. In
this paper, we introduce a new family of discriminator loss functions that
adopts a weighted sum of real and fake parts, which we call adaptive weighted
loss functions or aw-loss functions. Using the gradients of the real and fake
parts of the loss, we can adaptively choose weights to train a discriminator in
the direction that benefits the GAN's stability. Our method can be potentially
applied to any discriminator model with a loss that is a sum of the real and
fake parts. Experiments validated the effectiveness of our loss functions on an
unconditional image generation task, improving the baseline results by a
significant margin on CIFAR-10, STL-10, and CIFAR-100 datasets in Inception
Scores and FID.
</p>
<a href="http://arxiv.org/abs/2012.03149" target="_blank">arXiv:2012.03149</a> [<a href="http://arxiv.org/pdf/2012.03149" target="_blank">pdf</a>]

<h2>Automatic sampling and training method for wood-leaf classification based on tree terrestrial point cloud. (arXiv:2012.03152v1 [cs.CV])</h2>
<h3>Zichu Liu, Qing Zhang, Pei Wang, Yaxin Li, Jingqian Sun</h3>
<p>Terrestrial laser scanning technology provides an efficient and accuracy
solution for acquiring three-dimensional information of plants. The leaf-wood
classification of plant point cloud data is a fundamental step for some
forestry and biological research. An automatic sampling and training method for
classification was proposed based on tree point cloud data. The plane fitting
method was used for selecting leaf sample points and wood sample points
automatically, then two local features were calculated for training and
classification by using support vector machine (SVM) algorithm. The point cloud
data of ten trees were tested by using the proposed method and a manual
selection method. The average correct classification rate and kappa coefficient
are 0.9305 and 0.7904, respectively. The results show that the proposed method
had better efficiency and accuracy comparing to the manual selection method.
</p>
<a href="http://arxiv.org/abs/2012.03152" target="_blank">arXiv:2012.03152</a> [<a href="http://arxiv.org/pdf/2012.03152" target="_blank">pdf</a>]

<h2>Any-Width Networks. (arXiv:2012.03153v1 [cs.CV])</h2>
<h3>Thanh Vu, Marc Eder, True Price, Jan-Michael Frahm</h3>
<p>Despite remarkable improvements in speed and accuracy, convolutional neural
networks (CNNs) still typically operate as monolithic entities at inference
time. This poses a challenge for resource-constrained practical applications,
where both computational budgets and performance needs can vary with the
situation. To address these constraints, we propose the Any-Width Network
(AWN), an adjustable-width CNN architecture and associated training routine
that allow for fine-grained control over speed and accuracy during inference.
Our key innovation is the use of lower-triangular weight matrices which
explicitly address width-varying batch statistics while being naturally suited
for multi-width operations. We also show that this design facilitates an
efficient training routine based on random width sampling. We empirically
demonstrate that our proposed AWNs compare favorably to existing methods while
providing maximally granular control during inference.
</p>
<a href="http://arxiv.org/abs/2012.03153" target="_blank">arXiv:2012.03153</a> [<a href="http://arxiv.org/pdf/2012.03153" target="_blank">pdf</a>]

<h2>Distributed Multi-agent Meta Learning for Trajectory Design in Wireless Drone Networks. (arXiv:2012.03158v1 [cs.LG])</h2>
<h3>Ye Hu, Mingzhe Chen, Walid Saad, H. Vincent Poor, Shuguang Cui</h3>
<p>In this paper, the problem of the trajectory design for a group of
energy-constrained drones operating in dynamic wireless network environments is
studied. In the considered model, a team of drone base stations (DBSs) is
dispatched to cooperatively serve clusters of ground users that have dynamic
and unpredictable uplink access demands. In this scenario, the DBSs must
cooperatively navigate in the considered area to maximize coverage of the
dynamic requests of the ground users. This trajectory design problem is posed
as an optimization framework whose goal is to find optimal trajectories that
maximize the fraction of users served by all DBSs. To find an optimal solution
for this non-convex optimization problem under unpredictable environments, a
value decomposition based reinforcement learning (VDRL) solution coupled with a
meta-training mechanism is proposed. This algorithm allows the DBSs to
dynamically learn their trajectories while generalizing their learning to
unseen environments. Analytical results show that, the proposed VD-RL algorithm
is guaranteed to converge to a local optimal solution of the non-convex
optimization problem. Simulation results show that, even without meta-training,
the proposed VD-RL algorithm can achieve a 53.2% improvement of the service
coverage and a 30.6% improvement in terms of the convergence speed, compared to
baseline multi-agent algorithms. Meanwhile, the use of meta-learning improves
the convergence speed of the VD-RL algorithm by up to 53.8% when the DBSs must
deal with a previously unseen task.
</p>
<a href="http://arxiv.org/abs/2012.03158" target="_blank">arXiv:2012.03158</a> [<a href="http://arxiv.org/pdf/2012.03158" target="_blank">pdf</a>]

<h2>Conditional Generative Adversarial Networks for Optimal Path Planning. (arXiv:2012.03166v1 [cs.RO])</h2>
<h3>Nachuan Ma, Jiankun Wang, Max Q.-H. Meng</h3>
<p>Path planning plays an important role in autonomous robot systems. Effective
understanding of the surrounding environment and efficient generation of
optimal collision-free path are both critical parts for solving path planning
problem. Although conventional sampling-based algorithms, such as the
rapidly-exploring random tree (RRT) and its improved optimal version (RRT*),
have been widely used in path planning problems because of their ability to
find a feasible path in even complex environments, they fail to find an optimal
path efficiently. To solve this problem and satisfy the two aforementioned
requirements, we propose a novel learning-based path planning algorithm which
consists of a novel generative model based on the conditional generative
adversarial networks (CGAN) and a modified RRT* algorithm (denoted by
CGANRRT*). Given the map information, our CGAN model can generate an efficient
possibility distribution of feasible paths, which can be utilized by the
CGAN-RRT* algorithm to find the optimal path with a non-uniform sampling
strategy. The CGAN model is trained by learning from ground truth maps, each of
which is generated by putting all the results of executing RRT algorithm 50
times on one raw map. We demonstrate the efficient performance of this CGAN
model by testing it on two groups of maps and comparing CGAN-RRT* algorithm
with conventional RRT* algorithm.
</p>
<a href="http://arxiv.org/abs/2012.03166" target="_blank">arXiv:2012.03166</a> [<a href="http://arxiv.org/pdf/2012.03166" target="_blank">pdf</a>]

<h2>Design of an Optoelectronically Innervated Gripper for Rigid-Soft Interactive Grasping. (arXiv:2012.03168v1 [cs.RO])</h2>
<h3>Linhan Yang, Xudong Han, Weijie Guo, Zixin Zhang, Fang Wan, Jia Pan, Chaoyang Song</h3>
<p>Over the past few decades, efforts have been made towards robust robotic
grasping, and therefore dexterous manipulation. The soft gripper has shown
their potential in robust grasping due to their inherent properties-low,
control complexity, and high adaptability. However, the deformation of the soft
gripper when interacting with objects bring inaccuracy of grasped objects,
which causes instability for robust grasping and further manipulation. In this
paper, we present an omni-directional adaptive soft finger that can sense
deformation based on embedded optical fibers and the application of machine
learning methods to interpret transmitted light intensities. Furthermore, to
use tactile information provided by a soft finger, we design a low-cost and
multi degrees of freedom gripper to conform to the shape of objects actively
and optimize grasping policy, which is called Rigid-Soft Interactive Grasping.
Two main advantages of this grasping policy are provided: one is that a more
robust grasping could be achieved through an active adaptation; the other is
that the tactile information collected could be helpful for further
manipulation.
</p>
<a href="http://arxiv.org/abs/2012.03168" target="_blank">arXiv:2012.03168</a> [<a href="http://arxiv.org/pdf/2012.03168" target="_blank">pdf</a>]

<h2>Food Classification with Convolutional Neural Networks and Multi-Class Linear Discernment Analysis. (arXiv:2012.03170v1 [cs.CV])</h2>
<h3>Joshua Ball</h3>
<p>Convolutional neural networks (CNNs) have been successful in representing the
fully-connected inferencing ability perceived to be seen in the human brain:
they take full advantage of the hierarchy-style patterns commonly seen in
complex data and develop more patterns using simple features. Countless
implementations of CNNs have shown how strong their ability is to learn these
complex patterns, particularly in the realm of image classification. However,
the cost of getting a high performance CNN to a so-called "state of the art"
level is computationally costly. Even when using transfer learning, which
utilize the very deep layers from models such as MobileNetV2, CNNs still take a
great amount of time and resources. Linear discriminant analysis (LDA), a
generalization of Fisher's linear discriminant, can be implemented in a
multi-class classification method to increase separability of class features
while not needing a high performance system to do so for image classification.
Similarly, we also believe LDA has great promise in performing well. In this
paper, we discuss our process of developing a robust CNN for food
classification as well as our effective implementation of multi-class LDA and
prove that (1) CNN is superior to LDA for image classification and (2) why LDA
should not be left out of the races for image classification, particularly for
binary cases.
</p>
<a href="http://arxiv.org/abs/2012.03170" target="_blank">arXiv:2012.03170</a> [<a href="http://arxiv.org/pdf/2012.03170" target="_blank">pdf</a>]

<h2>Robust Deep AUC Maximization: A New Surrogate Loss and Empirical Studies on Medical Image Classification. (arXiv:2012.03173v1 [cs.LG])</h2>
<h3>Zhuoning Yuan, Yan Yan, Milan Sonka, Tianbao Yang</h3>
<p>Deep AUC Maximization (DAM) is a paradigm for learning a deep neural network
by maximizing the AUC score of the model on a dataset. Most previous works of
AUC maximization focus on the perspective of optimization by designing
efficient stochastic algorithms, and studies on generalization performance of
DAM on difficult tasks are missing. In this work, we aim to make DAM more
practical for interesting real-world applications (e.g., medical image
classification). First, we propose a new margin-based surrogate loss function
for the AUC score (named as the AUC margin loss). It is more robust than the
commonly used AUC square loss, while enjoying the same advantage in terms of
large-scale stochastic optimization. Second, we conduct empirical studies of
our DAM method on difficult medical image classification tasks, namely
classification of chest x-ray images for identifying many threatening diseases
and classification of images of skin lesions for identifying melanoma. Our DAM
method has achieved great success on these difficult tasks, i.e., the 1st place
on Stanford CheXpert competition (by the paper submission date) and Top 1% rank
(rank 33 out of 3314 teams) on Kaggle 2020 Melanoma classification competition.
We also conduct extensive ablation studies to demonstrate the advantages of the
new AUC margin loss over the AUC square loss on benchmark datasets. To the best
of our knowledge, this is the first work that makes DAM succeed on large-scale
medical image datasets.
</p>
<a href="http://arxiv.org/abs/2012.03173" target="_blank">arXiv:2012.03173</a> [<a href="http://arxiv.org/pdf/2012.03173" target="_blank">pdf</a>]

<h2>Counting Substructures with Higher-Order Graph Neural Networks: Possibility and Impossibility Results. (arXiv:2012.03174v1 [cs.LG])</h2>
<h3>Behrooz Tahmasebi, Stefanie Jegelka</h3>
<p>While massage passing based Graph Neural Networks (GNNs) have become
increasingly popular architectures for learning with graphs, recent works have
revealed important shortcomings in their expressive power. In response, several
higher-order GNNs have been proposed, which substantially increase the
expressive power, but at a large computational cost. Motivated by this gap, we
introduce and analyze a new recursive pooling technique of local neighborhoods
that allows different tradeoffs of computational cost and expressive power.
First, we show that this model can count subgraphs of size $k$, and thereby
overcomes a known limitation of low-order GNNs. Second, we prove that, in
several cases, the proposed algorithm can greatly reduce computational
complexity compared to the existing higher-order $k$-GNN and Local Relational
Pooling (LRP) networks. We also provide a (near) matching information-theoretic
lower bound for graph representations that can provably count subgraphs, and
discuss time complexity lower bounds as well.
</p>
<a href="http://arxiv.org/abs/2012.03174" target="_blank">arXiv:2012.03174</a> [<a href="http://arxiv.org/pdf/2012.03174" target="_blank">pdf</a>]

<h2>Maximum Entropy Subspace Clustering Network. (arXiv:2012.03176v1 [cs.CV])</h2>
<h3>Zhihao Peng, Yuheng Jia, Hui Liu, Junhui Hou, Qingfu Zhang</h3>
<p>Deep subspace clustering network (DSC-Net) and its numerous variants have
achieved impressive performance for subspace clustering, in which an
auto-encoder non-linearly maps input data into a latent space, and a fully
connected layer named self-expressiveness module is introduced between the
encoder and the decoder to learn an affinity matrix. However, the adopted
regularization on the affinity matrix (e.g., sparse, Tikhonov, or low-rank) is
still insufficient to drive the learning of an ideal affinity matrix, thus
limiting their performance. In addition, in DSC-Net, the self-expressiveness
module and the auto-encoder module are tightly coupled, making the training of
the DSC-Net non-trivial. To this end, in this paper, we propose a novel deep
learning-based clustering method named Maximum Entropy Subspace Clustering
Network (MESC-Net). Specifically, MESC-Net maximizes the learned affinity
matrix's entropy to encourage it to exhibit an ideal affinity matrix structure.
We theoretically prove that the affinity matrix driven by MESC-Net obeys the
block-diagonal property, and experimentally show that its elements
corresponding to the same subspace are uniformly and densely distributed, which
gives better clustering performance. Moreover, we explicitly decouple the
auto-encoder module and the self-expressiveness module. Extensive quantitative
and qualitative results on commonly used benchmark datasets validate MESC-Net
significantly outperforms state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2012.03176" target="_blank">arXiv:2012.03176</a> [<a href="http://arxiv.org/pdf/2012.03176" target="_blank">pdf</a>]

<h2>Probabilistic Federated Learning of Neural Networks Incorporated with Global Posterior Information. (arXiv:2012.03178v1 [cs.LG])</h2>
<h3>Peng Xiao, Samuel Cheng</h3>
<p>In federated learning, models trained on local clients are distilled into a
global model. Due to the permutation invariance arises in neural networks, it
is necessary to match the hidden neurons first when executing federated
learning with neural networks. Through the Bayesian nonparametric framework,
Probabilistic Federated Neural Matching (PFNM) matches and fuses local neural
networks so as to adapt to varying global model size and the heterogeneity of
the data. In this paper, we propose a new method which extends the PFNM with a
Kullback-Leibler (KL) divergence over neural components product, in order to
make inference exploiting posterior information in both local and global
levels. We also show theoretically that The additional part can be seamlessly
concatenated into the match-and-fuse progress. Through a series of simulations,
it indicates that our new method outperforms popular state-of-the-art federated
learning methods in both single communication round and additional
communication rounds situation.
</p>
<a href="http://arxiv.org/abs/2012.03178" target="_blank">arXiv:2012.03178</a> [<a href="http://arxiv.org/pdf/2012.03178" target="_blank">pdf</a>]

<h2>Representaciones del aprendizaje reutilizando los gradientes de la retropropagacion. (arXiv:2012.03188v1 [cs.LG])</h2>
<h3>Roberto Reyes-Ochoa, Servando Lopez-Aguayo</h3>
<p>This work proposes an algorithm for taking advantage of backpropagation
gradients to determine feature importance at different stages of training.
Additionally, we propose a way to represent the learning process qualitatively.
Experiments were performed over the Wisconsin cancer dataset provided by
sklearn, and results showed an interesting convergence of the so called
"learning gradients" towards the most important features.

---

Este trabajo propone el algoritmo de gradientes de aprendizaje para encontrar
significado en las entradas de una red neuronal. Ademas, se propone una manera
de evaluarlas por orden de importancia y representar el proceso de aprendizaje
a traves de las etapas de entrenamiento. Los resultados obtenidos utilizan como
referencia el conjunto de datos acerca de tumores malignos y benignos en
Wisconsin. Esta referencia sirvio para detectar un patron en las variables mas
importantes del modelo gracias, asi como su evolucion temporal.
</p>
<a href="http://arxiv.org/abs/2012.03188" target="_blank">arXiv:2012.03188</a> [<a href="http://arxiv.org/pdf/2012.03188" target="_blank">pdf</a>]

<h2>A Data-driven Human Responsibility Management System. (arXiv:2012.03190v1 [cs.AI])</h2>
<h3>Xuejiao Tang, Jiong Qiu, Ruijun Chen, Wenbin Zhang, Vasileios Iosifidis, Zhen Liu, Wei Meng, Mingli Zhang, Ji Zhang</h3>
<p>An ideal safe workplace is described as a place where staffs fulfill
responsibilities in a well-organized order, potential hazardous events are
being monitored in real-time, as well as the number of accidents and relevant
damages are minimized. However, occupational-related death and injury are still
increasing and have been highly attended in the last decades due to the lack of
comprehensive safety management. A smart safety management system is therefore
urgently needed, in which the staffs are instructed to fulfill responsibilities
as well as automating risk evaluations and alerting staffs and departments when
needed. In this paper, a smart system for safety management in the workplace
based on responsibility big data analysis and the internet of things (IoT) are
proposed. The real world implementation and assessment demonstrate that the
proposed systems have superior accountability performance and improve the
responsibility fulfillment through real-time supervision and self-reminder.
</p>
<a href="http://arxiv.org/abs/2012.03190" target="_blank">arXiv:2012.03190</a> [<a href="http://arxiv.org/pdf/2012.03190" target="_blank">pdf</a>]

<h2>Computer Stereo Vision for Autonomous Driving. (arXiv:2012.03194v1 [cs.CV])</h2>
<h3>Rui Fan, Li Wang, Mohammud Junaid Bocus, Ioannis Pitas</h3>
<p>For a long time, autonomous cars were found only in science fiction movies
and series but now they are becoming a reality. The opportunity to pick such a
vehicle at your garage forecourt is closer than you think. As an important
component of autonomous systems, autonomous car perception has had a big leap
with recent advances in parallel computing architectures, such as OpenMP for
multi-threading CPUs and OpenCL for GPUs. With the use of tiny but full-feature
embedded supercomputers, computer stereo vision has been prevalently applied in
autonomous cars for depth perception. The two key aspects of computer stereo
vision are speed and accuracy. They are two desirable but conflicting
properties -- the algorithms with better disparity accuracy usually have higher
computational complexity. Therefore, the main aim of developing a computer
stereo vision algorithm for resource-limited hardware is to improve the
trade-off between speed and accuracy. In this chapter, we first introduce the
autonomous car system, from the hardware aspect to the software aspect. We then
discuss four autonomous car perception functionalities, including: 1) visual
feature detection, description and matching, 2) 3D information acquisition, 3)
object detection/recognition and 4) semantic image segmentation. Finally, we
introduce the principles of computer stereo vision and parallel computing.
</p>
<a href="http://arxiv.org/abs/2012.03194" target="_blank">arXiv:2012.03194</a> [<a href="http://arxiv.org/pdf/2012.03194" target="_blank">pdf</a>]

<h2>Depth Completion using Piecewise Planar Model. (arXiv:2012.03195v1 [cs.CV])</h2>
<h3>Yiran Zhong, Yuchao Dai, Hongdong Li</h3>
<p>A depth map can be represented by a set of learned bases and can be
efficiently solved in a closed form solution. However, one issue with this
method is that it may create artifacts when colour boundaries are inconsistent
with depth boundaries. In fact, this is very common in a natural image. To
address this issue, we enforce a more strict model in depth recovery: a
piece-wise planar model. More specifically, we represent the desired depth map
as a collection of 3D planar and the reconstruction problem is formulated as
the optimization of planar parameters. Such a problem can be formulated as a
continuous CRF optimization problem and can be solved through particle based
method (MP-PBP) \cite{Yamaguchi14}. Extensive experimental evaluations on the
KITTI visual odometry dataset show that our proposed methods own high
resistance to false object boundaries and can generate useful and visually
pleasant 3D point clouds.
</p>
<a href="http://arxiv.org/abs/2012.03195" target="_blank">arXiv:2012.03195</a> [<a href="http://arxiv.org/pdf/2012.03195" target="_blank">pdf</a>]

<h2>Online Adaptation for Consistent Mesh Reconstruction in the Wild. (arXiv:2012.03196v1 [cs.CV])</h2>
<h3>Xueting Li, Sifei Liu, Shalini De Mello, Kihwan Kim, Xiaolong Wang, Ming-Hsuan Yang, Jan Kautz</h3>
<p>This paper presents an algorithm to reconstruct temporally consistent 3D
meshes of deformable object instances from videos in the wild. Without
requiring annotations of 3D mesh, 2D keypoints, or camera pose for each video
frame, we pose video-based reconstruction as a self-supervised online
adaptation problem applied to any incoming test video. We first learn a
category-specific 3D reconstruction model from a collection of single-view
images of the same category that jointly predicts the shape, texture, and
camera pose of an image. Then, at inference time, we adapt the model to a test
video over time using self-supervised regularization terms that exploit
temporal consistency of an object instance to enforce that all reconstructed
meshes share a common texture map, a base shape, as well as parts. We
demonstrate that our algorithm recovers temporally consistent and reliable 3D
structures from videos of non-rigid objects including those of animals captured
in the wild -- an extremely challenging task rarely addressed before.
</p>
<a href="http://arxiv.org/abs/2012.03196" target="_blank">arXiv:2012.03196</a> [<a href="http://arxiv.org/pdf/2012.03196" target="_blank">pdf</a>]

<h2>DGGAN: Depth-image Guided Generative Adversarial Networks for Disentangling RGB and Depth Images in 3D Hand Pose Estimation. (arXiv:2012.03197v1 [cs.CV])</h2>
<h3>Liangjian Chen, Shih-Yao Lin, Yusheng Xie, Yen-Yu Lin, Wei Fan, Xiaohui Xie</h3>
<p>Estimating3D hand poses from RGB images is essentialto a wide range of
potential applications, but is challengingowing to substantial ambiguity in the
inference of depth in-formation from RGB images. State-of-the-art estimators
ad-dress this problem by regularizing3D hand pose estimationmodels during
training to enforce the consistency betweenthe predicted3D poses and the
ground-truth depth maps.However, these estimators rely on both RGB images and
thepaired depth maps during training. In this study, we proposea conditional
generative adversarial network (GAN) model,called Depth-image Guided GAN
(DGGAN), to generate re-alistic depth maps conditioned on the input RGB image,
anduse the synthesized depth maps to regularize the3D handpose estimation
model, therefore eliminating the need forground-truth depth maps. Experimental
results on multiplebenchmark datasets show that the synthesized depth
mapsproduced by DGGAN are quite effective in regularizing thepose estimation
model, yielding new state-of-the-art resultsin estimation accuracy, notably
reducing the mean3D end-point errors (EPE) by4.7%,16.5%, and6.8%on the RHD,STB
and MHP datasets, respectively.
</p>
<a href="http://arxiv.org/abs/2012.03197" target="_blank">arXiv:2012.03197</a> [<a href="http://arxiv.org/pdf/2012.03197" target="_blank">pdf</a>]

<h2>Estimating Vector Fields from Noisy Time Series. (arXiv:2012.03199v1 [stat.ML])</h2>
<h3>Harish S. Bhat, Majerle Reeves, Ramin Raziperchikolaei</h3>
<p>While there has been a surge of recent interest in learning differential
equation models from time series, methods in this area typically cannot cope
with highly noisy data. We break this problem into two parts: (i) approximating
the unknown vector field (or right-hand side) of the differential equation, and
(ii) dealing with noise. To deal with (i), we describe a neural network
architecture consisting of tensor products of one-dimensional neural shape
functions. For (ii), we propose an alternating minimization scheme that
switches between vector field training and filtering steps, together with
multiple trajectories of training data. We find that the neural shape function
architecture retains the approximation properties of dense neural networks,
enables effective computation of vector field error, and allows for graphical
interpretability, all for data/systems in any finite dimension $d$. We also
study the combination of either our neural shape function method or existing
differential equation learning methods with alternating minimization and
multiple trajectories. We find that retrofitting any learning method in this
way boosts the method's robustness to noise. While in their raw form the
methods struggle with 1% Gaussian noise, after retrofitting, they learn
accurate vector fields from data with 10% Gaussian noise.
</p>
<a href="http://arxiv.org/abs/2012.03199" target="_blank">arXiv:2012.03199</a> [<a href="http://arxiv.org/pdf/2012.03199" target="_blank">pdf</a>]

<h2>Fever Basketball: A Complex, Flexible, and Asynchronized Sports Game Environment for Multi-agent Reinforcement Learning. (arXiv:2012.03204v1 [cs.AI])</h2>
<h3>Hangtian Jia, Yujing Hu, Yingfeng Chen, Chunxu Ren, Tangjie Lv, Changjie Fan, Chongjie Zhang</h3>
<p>The development of deep reinforcement learning (DRL) has benefited from the
emergency of a variety type of game environments where new challenging problems
are proposed and new algorithms can be tested safely and quickly, such as Board
games, RTS, FPS, and MOBA games. However, many existing environments lack
complexity and flexibility and assume the actions are synchronously executed in
multi-agent settings, which become less valuable. We introduce the Fever
Basketball game, a novel reinforcement learning environment where agents are
trained to play basketball game. It is a complex and challenging environment
that supports multiple characters, multiple positions, and both the
single-agent and multi-agent player control modes. In addition, to better
simulate real-world basketball games, the execution time of actions differs
among players, which makes Fever Basketball a novel asynchronized environment.
We evaluate commonly used multi-agent algorithms of both independent learners
and joint-action learners in three game scenarios with varying difficulties,
and heuristically propose two baseline methods to diminish the extra
non-stationarity brought by asynchronism in Fever Basketball Benchmarks.
Besides, we propose an integrated curricula training (ICT) framework to better
handle Fever Basketball problems, which includes several game-rule based
cascading curricula learners and a coordination curricula switcher focusing on
enhancing coordination within the team. The results show that the game remains
challenging and can be used as a benchmark environment for studies like
long-time horizon, sparse rewards, credit assignment, and non-stationarity,
etc. in multi-agent settings.
</p>
<a href="http://arxiv.org/abs/2012.03204" target="_blank">arXiv:2012.03204</a> [<a href="http://arxiv.org/pdf/2012.03204" target="_blank">pdf</a>]

<h2>Temporal-Aware Self-Supervised Learning for 3D Hand Pose and Mesh Estimation in Videos. (arXiv:2012.03205v1 [cs.CV])</h2>
<h3>Liangjian Chen, Shih-Yao Lin, Yusheng Xie, Yen-Yu Lin, Xiaohui Xie</h3>
<p>Estimating 3D hand pose directly from RGB imagesis challenging but has gained
steady progress recently bytraining deep models with annotated 3D poses.
Howeverannotating 3D poses is difficult and as such only a few 3Dhand pose
datasets are available, all with limited samplesizes. In this study, we propose
a new framework of training3D pose estimation models from RGB images without
usingexplicit 3D annotations, i.e., trained with only 2D informa-tion. Our
framework is motivated by two observations: 1)Videos provide richer information
for estimating 3D posesas opposed to static images; 2) Estimated 3D poses
oughtto be consistent whether the videos are viewed in the for-ward order or
reverse order. We leverage these two obser-vations to develop a self-supervised
learning model calledtemporal-aware self-supervised network (TASSN). By
en-forcing temporal consistency constraints, TASSN learns 3Dhand poses and
meshes from videos with only 2D keypointposition annotations. Experiments show
that our modelachieves surprisingly good results, with 3D estimation ac-curacy
on par with the state-of-the-art models trained with3D annotations,
highlighting the benefit of the temporalconsistency in constraining 3D
prediction models.
</p>
<a href="http://arxiv.org/abs/2012.03205" target="_blank">arXiv:2012.03205</a> [<a href="http://arxiv.org/pdf/2012.03205" target="_blank">pdf</a>]

<h2>MVHM: A Large-Scale Multi-View Hand Mesh Benchmark for Accurate 3D Hand Pose Estimation. (arXiv:2012.03206v1 [cs.CV])</h2>
<h3>Liangjian Chen, Shih-Yao Lin, Yusheng Xie, Yen-Yu Lin, Xiaohui Xie</h3>
<p>Estimating 3D hand poses from a single RGB image is challenging because depth
ambiguity leads the problem ill-posed. Training hand pose estimators with 3D
hand mesh annotations and multi-view images often results in significant
performance gains. However, existing multi-view datasets are relatively small
with hand joints annotated by off-the-shelf trackers or automated through model
predictions, both of which may be inaccurate and can introduce biases.
Collecting a large-scale multi-view 3D hand pose images with accurate mesh and
joint annotations is valuable but strenuous. In this paper, we design a spin
match algorithm that enables a rigid mesh model matching with any target mesh
ground truth. Based on the match algorithm, we propose an efficient pipeline to
generate a large-scale multi-view hand mesh (MVHM) dataset with accurate 3D
hand mesh and joint labels. We further present a multi-view hand pose
estimation approach to verify that training a hand pose estimator with our
generated dataset greatly enhances the performance. Experimental results show
that our approach achieves the performance of 0.990 in
$\text{AUC}_{\text{20-50}}$ on the MHP dataset compared to the previous
state-of-the-art of 0.939 on this dataset. Our datasset is public available.
\footnote{\url{https://github.com/Kuzphi/MVHM}} Our datasset is available
at~\href{https://github.com/Kuzphi/MVHM}{\color{blue}{https://github.com/Kuzphi/MVHM}}.
</p>
<a href="http://arxiv.org/abs/2012.03206" target="_blank">arXiv:2012.03206</a> [<a href="http://arxiv.org/pdf/2012.03206" target="_blank">pdf</a>]

<h2>MOCA: A Modular Object-Centric Approach for Interactive Instruction Following. (arXiv:2012.03208v1 [cs.AI])</h2>
<h3>Kunal Pratap Singh, Suvaansh Bhambri, Byeonghwi Kim, Roozbeh Mottaghi, Jonghyun Choi</h3>
<p>Performing simple household tasks based on language directives is very
natural to humans, yet it remains an open challenge for an AI agent. Recently,
an `interactive instruction following' task has been proposed to foster
research in reasoning over long instruction sequences that requires object
interactions in a simulated environment. It involves solving open problems in
vision, language and navigation literature at each step. To address this
multifaceted problem, we propose a modular architecture that decouples the task
into visual perception and action policy, and name it as MOCA, a Modular
Object-Centric Approach. We evaluate our method on the ALFRED benchmark and
empirically validate that it outperforms prior arts by significant margins in
all metrics with good generalization performance (high success rate in unseen
environments). Our code is available at https://github.com/gistvision/moca.
</p>
<a href="http://arxiv.org/abs/2012.03208" target="_blank">arXiv:2012.03208</a> [<a href="http://arxiv.org/pdf/2012.03208" target="_blank">pdf</a>]

<h2>Skeleon-Based Typing Style Learning For Person Identification. (arXiv:2012.03212v1 [cs.CV])</h2>
<h3>Lior Gelberg, David Mendlovic, Dan Raviv</h3>
<p>We present a novel architecture for person identification based on
typing-style, constructed of adaptive non-local spatio-temporal graph
convolutional network. Since type style dynamics convey meaningful information
that can be useful for person identification, we extract the joints positions
and then learn their movements' dynamics. Our non-local approach increases our
model's robustness to noisy input data while analyzing joints locations instead
of RGB data provides remarkable robustness to alternating environmental
conditions, e.g., lighting, noise, etc. We further present two new datasets for
typing style based person identification task and extensive evaluation that
displays our model's superior discriminative and generalization abilities, when
compared with state-of-the-art skeleton-based models.
</p>
<a href="http://arxiv.org/abs/2012.03212" target="_blank">arXiv:2012.03212</a> [<a href="http://arxiv.org/pdf/2012.03212" target="_blank">pdf</a>]

<h2>TornadoAggregate: Accurate and Scalable Federated Learning via the Ring-Based Architecture. (arXiv:2012.03214v1 [cs.LG])</h2>
<h3>Jin-woo Lee, Jaehoon Oh, Sungsu Lim, Se-Young Yun, Jae-Gil Lee</h3>
<p>Federated learning has emerged as a new paradigm of collaborative machine
learning; however, many prior studies have used global aggregation along a star
topology without much consideration of the communication scalability or the
diurnal property relied on clients' local time variety. In contrast, ring
architecture can resolve the scalability issue and even satisfy the diurnal
property by iterating nodes without an aggregation. Nevertheless, such
ring-based algorithms can inherently suffer from the high-variance problem. To
this end, we propose a novel algorithm called TornadoAggregate that improves
both accuracy and scalability by facilitating the ring architecture. In
particular, to improve the accuracy, we reformulate the loss minimization into
a variance reduction problem and establish three principles to reduce variance:
Ring-Aware Grouping, Small Ring, and Ring Chaining. Experimental results show
that TornadoAggregate improved the test accuracy by up to 26.7% and achieved
near-linear scalability.
</p>
<a href="http://arxiv.org/abs/2012.03214" target="_blank">arXiv:2012.03214</a> [<a href="http://arxiv.org/pdf/2012.03214" target="_blank">pdf</a>]

<h2>Spatiotemporal tomography based on scattered multiangular signals and its application for resolving evolving clouds using moving platforms. (arXiv:2012.03223v1 [cs.CV])</h2>
<h3>Roi Ronen (1), Yoav Y. Schechner (1), Eshkol Eytan (2) ((1) Viterbi Faculty of Electrical Engineering, Technion - Israel Institute of Technology, Haifa, Israel, (2) Department of Earth and Planetary Sciences, The Weizmann Institute of Science, Rehovot, Israel)</h3>
<p>We derive computed tomography (CT) of a time-varying volumetric translucent
object, using a small number of moving cameras. We particularly focus on
passive scattering tomography, which is a non-linear problem. We demonstrate
the approach on dynamic clouds, as clouds have a major effect on Earth's
climate. State of the art scattering CT assumes a static object. Existing 4D CT
methods rely on a linear image formation model and often on significant priors.
In this paper, the angular and temporal sampling rates needed for a proper
recovery are discussed. If these rates are used, the paper leads to a
representation of the time-varying object, which simplifies 4D CT tomography.
The task is achieved using gradient-based optimization. We demonstrate this in
physics-based simulations and in an experiment that had yielded real-world
data.
</p>
<a href="http://arxiv.org/abs/2012.03223" target="_blank">arXiv:2012.03223</a> [<a href="http://arxiv.org/pdf/2012.03223" target="_blank">pdf</a>]

<h2>Benefit of deep learning with non-convex noisy gradient descent: Provable excess risk bound and superiority to kernel methods. (arXiv:2012.03224v1 [stat.ML])</h2>
<h3>Taiji Suzuki, Shunta Akiyama</h3>
<p>Establishing a theoretical analysis that explains why deep learning can
outperform shallow learning such as kernel methods is one of the biggest issues
in the deep learning literature. Towards answering this question, we evaluate
excess risk of a deep learning estimator trained by a noisy gradient descent
with ridge regularization on a mildly overparameterized neural network, and
discuss its superiority to a class of linear estimators that includes neural
tangent kernel approach, random feature model, other kernel methods, $k$-NN
estimator and so on. We consider a teacher-student regression model, and
eventually show that any linear estimator can be outperformed by deep learning
in a sense of the minimax optimal rate especially for a high dimension setting.
The obtained excess bounds are so-called fast learning rate which is faster
than $O(1/\sqrt{n})$ that is obtained by usual Rademacher complexity analysis.
This discrepancy is induced by the non-convex geometry of the model and the
noisy gradient descent used for neural network training provably reaches a near
global optimal solution even though the loss landscape is highly non-convex.
Although the noisy gradient descent does not employ any explicit or implicit
sparsity inducing regularization, it shows a preferable generalization
performance that dominates linear estimators.
</p>
<a href="http://arxiv.org/abs/2012.03224" target="_blank">arXiv:2012.03224</a> [<a href="http://arxiv.org/pdf/2012.03224" target="_blank">pdf</a>]

<h2>Appendix for the Motion Primitives-based Path Planning for Fast and Agile Exploration Method. (arXiv:2012.03228v1 [cs.RO])</h2>
<h3>Mihir Dharmadhikari, Tung Dang, Kostas Alexis</h3>
<p>This manuscript presents enhancements on our motion-primitives exploration
path planning method for agile exploration using aerial robots. The method now
further integrates a global planning layer to facilitate reliable large-scale
exploration. The implemented bifurcation between local and global planning
allows for efficient exploration combined with the ability to plan within very
large environments, while also ensuring safe and timely return-to-home. A new
set of simulation studies and experimental results are presented to demonstrate
the new improvements and enhancements. The method is available open source as a
Robot Operating System (ROS) package.
</p>
<a href="http://arxiv.org/abs/2012.03228" target="_blank">arXiv:2012.03228</a> [<a href="http://arxiv.org/pdf/2012.03228" target="_blank">pdf</a>]

<h2>Amortized Q-learning with Model-based Action Proposals for Autonomous Driving on Highways. (arXiv:2012.03234v1 [cs.LG])</h2>
<h3>Branka Mirchevska, Maria H&#xfc;gle, Gabriel Kalweit, Moritz Werling, Joschka Boedecker</h3>
<p>Well-established optimization-based methods can guarantee an optimal
trajectory for a short optimization horizon, typically no longer than a few
seconds. As a result, choosing the optimal trajectory for this short horizon
may still result in a sub-optimal long-term solution. At the same time, the
resulting short-term trajectories allow for effective, comfortable and provable
safe maneuvers in a dynamic traffic environment. In this work, we address the
question of how to ensure an optimal long-term driving strategy, while keeping
the benefits of classical trajectory planning. We introduce a Reinforcement
Learning based approach that coupled with a trajectory planner, learns an
optimal long-term decision-making strategy for driving on highways. By online
generating locally optimal maneuvers as actions, we balance between the
infinite low-level continuous action space, and the limited flexibility of a
fixed number of predefined standard lane-change actions. We evaluated our
method on realistic scenarios in the open-source traffic simulator SUMO and
were able to achieve better performance than the 4 benchmark approaches we
compared against, including a random action selecting agent, greedy agent,
high-level, discrete actions agent and an IDM-based SUMO-controlled agent.
</p>
<a href="http://arxiv.org/abs/2012.03234" target="_blank">arXiv:2012.03234</a> [<a href="http://arxiv.org/pdf/2012.03234" target="_blank">pdf</a>]

<h2>Cross-Layer Distillation with Semantic Calibration. (arXiv:2012.03236v1 [cs.CV])</h2>
<h3>Defang Chen, Jian-Ping Mei, Yuan Zhang, Can Wang, Zhe Wang, Yan Feng, Chun Chen</h3>
<p>Recently proposed knowledge distillation approaches based on feature-map
transfer validate that intermediate layers of a teacher model can serve as
effective targets for training a student model to obtain better generalization
ability. Existing studies mainly focus on particular representation forms for
knowledge transfer between manually specified pairs of teacher-student
intermediate layers. However, semantics of intermediate layers may vary in
different networks and manual association of layers might lead to negative
regularization caused by semantic mismatch between certain teacher-student
layer pairs. To address this problem, we propose Semantic Calibration for
Cross-layer Knowledge Distillation (SemCKD), which automatically assigns proper
target layers of the teacher model for each student layer with an attention
mechanism. With a learned attention distribution, each student layer distills
knowledge contained in multiple layers rather than a single fixed intermediate
layer from the teacher model for appropriate cross-layer supervision in
training. Consistent improvements over state-of-the-art approaches are observed
in extensive experiments with various network architectures for teacher and
student models, demonstrating the effectiveness and flexibility of the proposed
attention based soft layer association mechanism for cross-layer distillation.
</p>
<a href="http://arxiv.org/abs/2012.03236" target="_blank">arXiv:2012.03236</a> [<a href="http://arxiv.org/pdf/2012.03236" target="_blank">pdf</a>]

<h2>Capturing Delayed Feedback in Conversion Rate Prediction via Elapsed-Time Sampling. (arXiv:2012.03245v1 [cs.LG])</h2>
<h3>Jia-Qi Yang, Xiang Li, Shuguang Han, Tao Zhuang, De-Chuan Zhan, Xiaoyi Zeng, Bin Tong</h3>
<p>Capturing Delayed Feedback in Conversion Rate Prediction via Elapsed-Time
Sampling Hide abstract Conversion rate (CVR) prediction is one of the most
critical tasks for digital display advertising. Commercial systems often
require to update models in an online learning manner to catch up with the
evolving data distribution. However, conversions usually do not happen
immediately after a user click. This may result in inaccurate labeling, which
is called delayed feedback problem. In previous studies, delayed feedback
problem is handled either by waiting positive label for a long period of time,
or by consuming the negative sample on its arrival and then insert a positive
duplicate when a conversion happens later. Indeed, there is a trade-off between
waiting for more accurate labels and utilizing fresh data, which is not
considered in existing works. To strike a balance in this trade-off, we propose
Elapsed-Time Sampling Delayed Feedback Model (ES-DFM), which models the
relationship between the observed conversion distribution and the true
conversion distribution. Then we optimize the expectation of true conversion
distribution via importance sampling under the elapsed-time sampling
distribution. We further estimate the importance weight for each instance,
which is used as the weight of loss function in CVR prediction. To demonstrate
the effectiveness of ES-DFM, we conduct extensive experiments on a public data
and a private industrial dataset. Experimental results confirm that our method
consistently outperforms the previous state-of-the-art results.
</p>
<a href="http://arxiv.org/abs/2012.03245" target="_blank">arXiv:2012.03245</a> [<a href="http://arxiv.org/pdf/2012.03245" target="_blank">pdf</a>]

<h2>Towards Better Object Detection in Scale Variation with Adaptive Feature Selection. (arXiv:2012.03265v1 [cs.CV])</h2>
<h3>Zehui Gong, Dong Li</h3>
<p>It is a common practice to exploit pyramidal feature representation to tackle
the problem of scale variation in object instances. However, most of them still
predict the objects in a certain range of scales based solely or mainly on a
single-level representation, yielding inferior detection performance. To this
end, we propose a novel adaptive feature selection module (AFSM), to
automatically learn the way to fuse multi-level representations in the channel
dimension, in a data-driven manner. It significantly improves the performance
of the detectors that have a feature pyramid structure, while introducing
nearly free inference overhead. Moreover, we propose a class-aware sampling
mechanism to tackle the class imbalance problem, which automatically assigns
the sampling weight to each of the images during training, according to the
number of objects in each class. Experimental results demonstrate the
effectiveness of the proposed method, with 83.04\% mAP at 15.96 FPS on the VOC
dataset, and 39.48\% AP on the VisDrone-DET validation subset, respectively,
outperforming other state-of-the-art detectors considerably. The code will be
available soon at https://github.com/ZeHuiGong/AFSM.git.
</p>
<a href="http://arxiv.org/abs/2012.03265" target="_blank">arXiv:2012.03265</a> [<a href="http://arxiv.org/pdf/2012.03265" target="_blank">pdf</a>]

<h2>Accurate and Fast Federated Learning via Combinatorial Multi-Armed Bandits. (arXiv:2012.03270v1 [cs.LG])</h2>
<h3>Taehyeon Kim, Sangmin Bae, Jin-woo Lee, Seyoung Yun</h3>
<p>Federated learning has emerged as an innovative paradigm of collaborative
machine learning. Unlike conventional machine learning, a global model is
collaboratively learned while data remains distributed over a tremendous number
of client devices, thus not compromising user privacy. However, several
challenges still remain despite its glowing popularity; above all, the global
aggregation in federated learning involves the challenge of biased model
averaging and lack of prior knowledge in client sampling, which, in turn, leads
to high generalization error and slow convergence rate, respectively. In this
work, we propose a novel algorithm called FedCM that addresses the two
challenges by utilizing prior knowledge with multi-armed bandit based client
sampling and filtering biased models with combinatorial model averaging. Based
on extensive evaluations using various algorithms and representative
heterogeneous datasets, we showed that FedCM significantly outperformed the
state-of-the-art algorithms by up to 37.25% and 4.17 times, respectively, in
terms of generalization accuracy and convergence rate.
</p>
<a href="http://arxiv.org/abs/2012.03270" target="_blank">arXiv:2012.03270</a> [<a href="http://arxiv.org/pdf/2012.03270" target="_blank">pdf</a>]

<h2>FedSemi: An Adaptive Federated Semi-Supervised Learning Framework. (arXiv:2012.03292v1 [cs.LG])</h2>
<h3>Zewei Long, Liwei Che, Yaqing Wang, Muchao Ye, Junyu Luo, Jinze Wu, Houping Xiao, Fenglong Ma</h3>
<p>Federated learning (FL) has emerged as an effective technique to co-training
machine learning models without actually sharing data and leaking privacy.
However, most existing FL methods focus on the supervised setting and ignore
the utilization of unlabeled data. Although there are a few existing studies
trying to incorporate unlabeled data into FL, they all fail to maintain
performance guarantees or generalization ability in various settings. In this
paper, we tackle the federated semi-supervised learning problem from the
insight of data regularization and analyze the new-raised difficulties. We
propose FedSemi, a novel, adaptive, and general framework, which firstly
introduces the consistency regularization into FL using a teacher-student
model. We further propose a new metric to measure the divergence of local model
layers. Based on the divergence, FedSemi can automatically select layer-level
parameters to be uploaded to the server in an adaptive manner. Through
extensive experimental validation of our method in four datasets, we show that
our method achieves performance gain under the IID setting and three Non-IID
settings compared to state-of-the-art baselines.
</p>
<a href="http://arxiv.org/abs/2012.03292" target="_blank">arXiv:2012.03292</a> [<a href="http://arxiv.org/pdf/2012.03292" target="_blank">pdf</a>]

<h2>Contrastive Divergence Learning is a Time Reversal Adversarial Game. (arXiv:2012.03295v1 [cs.LG])</h2>
<h3>Omer Yair, Tomer Michaeli</h3>
<p>Contrastive divergence (CD) learning is a classical method for fitting
unnormalized statistical models to data samples. Despite its wide-spread use,
the convergence properties of this algorithm are still not well understood. The
main source of difficulty is an unjustified approximation which has been used
to derive the gradient of the loss. In this paper, we present an alternative
derivation of CD that does not require any approximation and sheds new light on
the objective that is actually being optimized by the algorithm. Specifically,
we show that CD is an adversarial learning procedure, where a discriminator
attempts to classify whether a Markov chain generated from the model has been
time-reversed. Thus, although predating generative adversarial networks (GANs)
by more than a decade, CD is, in fact, closely related to these techniques. Our
derivation settles well with previous observations, which have concluded that
CD's update steps cannot be expressed as the gradients of any fixed objective
function. In addition, as a byproduct, our derivation reveals a simple
correction that can be used as an alternative to Metropolis-Hastings rejection,
which is required when the underlying Markov chain is inexact (\eg when using
Langevin dynamics with a large step).
</p>
<a href="http://arxiv.org/abs/2012.03295" target="_blank">arXiv:2012.03295</a> [<a href="http://arxiv.org/pdf/2012.03295" target="_blank">pdf</a>]

<h2>Pedestrian Behavior Prediction via Multitask Learning and Categorical Interaction Modeling. (arXiv:2012.03298v1 [cs.CV])</h2>
<h3>Amir Rasouli, Mohsen Rohani, Jun Luo</h3>
<p>Pedestrian behavior prediction is one of the major challenges for intelligent
driving systems. Pedestrians often exhibit complex behaviors influenced by
various contextual elements. To address this problem, we propose a multitask
learning framework that simultaneously predicts trajectories and actions of
pedestrians by relying on multimodal data. Our method benefits from 1) a hybrid
mechanism to encode different input modalities independently allowing them to
develop their own representations, and jointly to produce a representation for
all modalities using shared parameters; 2) a novel interaction modeling
technique that relies on categorical semantic parsing of the scenes to capture
interactions between target pedestrians and their surroundings; and 3) a dual
prediction mechanism that uses both independent and shared decoding of
multimodal representations. Using public pedestrian behavior benchmark datasets
for driving, PIE and JAAD, we highlight the benefits of multitask learning for
behavior prediction and show that our model achieves state-of-the-art
performance and improves trajectory and action prediction by up to 22% and 6%
respectively. We further investigate the contributions of the proposed
processing and interaction modeling techniques via extensive ablation studies.
</p>
<a href="http://arxiv.org/abs/2012.03298" target="_blank">arXiv:2012.03298</a> [<a href="http://arxiv.org/pdf/2012.03298" target="_blank">pdf</a>]

<h2>Deep Transfer Learning for Industrial Automation: A Review and Discussion of New Techniques for Data-Driven Machine Learning. (arXiv:2012.03301v1 [cs.LG])</h2>
<h3>Benjamin Maschler, Michael Weyrich</h3>
<p>In this article, the concepts of transfer and continual learning are
introduced. The ensuing review reveals promising approaches for industrial deep
transfer learning, utilizing methods of both classes of algorithms. In the
field of computer vision, it is already state-of-the-art. In others, e.g. fault
prediction, it is barely starting. However, over all fields, the abstract
differentiation between continual and transfer learning is not benefitting
their practical use. In contrast, both should be brought together to create
robust learning algorithms fulfilling the industrial automation sector's
requirements. To better describe these requirements, base use cases of
industrial transfer learning are introduced.
</p>
<a href="http://arxiv.org/abs/2012.03301" target="_blank">arXiv:2012.03301</a> [<a href="http://arxiv.org/pdf/2012.03301" target="_blank">pdf</a>]

<h2>TediGAN: Text-Guided Diverse Image Generation and Manipulation. (arXiv:2012.03308v1 [cs.CV])</h2>
<h3>Weihao Xia, Yujiu Yang, Jing-Hao Xue, Baoyuan Wu</h3>
<p>In this work, we propose TediGAN, a novel framework for multi-modal image
generation and manipulation with textual descriptions. The proposed method
consists of three components: StyleGAN inversion module, visual-linguistic
similarity learning, and instance-level optimization. The inversion module is
to train an image encoder to map real images to the latent space of a
well-trained StyleGAN. The visual-linguistic similarity is to learn the
text-image matching by mapping the image and text into a common embedding
space. The instance-level optimization is for identity preservation in
manipulation. Our model can provide the lowest effect guarantee, and produce
diverse and high-quality images with an unprecedented resolution at 1024. Using
a control mechanism based on style-mixing, our TediGAN inherently supports
image synthesis with multi-modal inputs, such as sketches or semantic labels
with or without instance (text or real image) guidance. To facilitate
text-guided multi-modal synthesis, we propose the Multi-Modal CelebA-HQ, a
large-scale dataset consisting of real face images and corresponding semantic
segmentation map, sketch, and textual descriptions. Extensive experiments on
the introduced dataset demonstrate the superior performance of our proposed
method. Code and data are available at https://github.com/weihaox/TediGAN.
</p>
<a href="http://arxiv.org/abs/2012.03308" target="_blank">arXiv:2012.03308</a> [<a href="http://arxiv.org/pdf/2012.03308" target="_blank">pdf</a>]

<h2>PAC-Learning for Strategic Classification. (arXiv:2012.03310v1 [cs.LG])</h2>
<h3>Ravi Sundaram, Anil Vullikanti, Haifeng Xu, Fan Yao</h3>
<p>Machine learning (ML) algorithms may be susceptible to being gamed by
individuals with knowledge of the algorithm (a.k.a. Goodhart's law). Such
concerns have motivated a surge of recent work on strategic classification
where each data point is a self-interested agent and may strategically
manipulate his features to induce a more desirable classification outcome for
himself. Previous works assume agents have homogeneous preferences and all
equally prefer the positive label. This paper generalizes strategic
classification to settings where different data points may have different
preferences over the classification outcomes. Besides a richer model, this
generalization allows us to include evasion attacks in adversarial ML also as a
special case of our model where positive [resp. negative] data points prefer
the negative [resp. positive] label, and thus for the first time allows
strategic and adversarial learning to be studied under the same framework.

We introduce the strategic VC-dimension (SVC), which captures the
PAC-learnability of a hypothesis class in our general strategic setup. SVC
generalizes the notion of adversarial VC-dimension (AVC) introduced recently by
Cullina et al. arXiv:1806.0147(1). We then instantiate our framework for
arguably the most basic hypothesis class, i.e., linear classifiers. We fully
characterize the statistical learnability of linear classifiers by pining down
its SVC and the computational tractability by pining down the complexity of the
empirical risk minimization problem. Our bound of SVC for linear classifiers
also strictly generalizes the AVC bound for linear classifiers in
arXiv:1806.0147(1). Finally, we briefly study the power of randomization in our
strategic classification setup. We show that randomization may strictly
increase the accuracy in general, but will not help in the special case of
adversarial classification under evasion attacks.
</p>
<a href="http://arxiv.org/abs/2012.03310" target="_blank">arXiv:2012.03310</a> [<a href="http://arxiv.org/pdf/2012.03310" target="_blank">pdf</a>]

<h2>Efficient Human Pose Estimation with Depthwise Separable Convolution and Person Centroid Guided Joint Grouping. (arXiv:2012.03316v1 [cs.CV])</h2>
<h3>Jie Ou, Hong Wu</h3>
<p>In this paper, we propose efficient and effective methods for 2D human pose
estimation. A new ResBlock is proposed based on depthwise separable convolution
and is utilized instead of the original one in Hourglass network. It can be
further enhanced by replacing the vanilla depthwise convolution with a mixed
depthwise convolution. Based on it, we propose a bottom-up multi-person pose
estimation method. A rooted tree is used to represent human pose by introducing
person centroid as the root which connects to all body joints directly or
hierarchically. Two branches of sub-networks are used to predict the centroids,
body joints and their offsets to their parent nodes. Joints are grouped by
tracing along their offsets to the closest centroids. Experimental results on
the MPII human dataset and the LSP dataset show that both our single-person and
multi-person pose estimation methods can achieve competitive accuracies with
low computational costs.
</p>
<a href="http://arxiv.org/abs/2012.03316" target="_blank">arXiv:2012.03316</a> [<a href="http://arxiv.org/pdf/2012.03316" target="_blank">pdf</a>]

<h2>Global Unifying Intrinsic Calibration for Spinning and Solid-State LiDARs. (arXiv:2012.03321v1 [cs.RO])</h2>
<h3>Jiunn-Kai Huang, Chenxi Feng, Madhav Achar, Maani Ghaffari, Jessy W. Grizzle</h3>
<p>Sensor calibration, which can be intrinsic or extrinsic, is an essential step
to achieve the measurement accuracy required for modern perception and
navigation systems deployed on autonomous robots. To date, intrinsic
calibration models for spinning LiDARs have been based on hypothesized based on
their physical mechanisms, resulting in anywhere from three to ten parameters
to be estimated from data, while no phenomenological models have yet been
proposed for solid-state LiDARs. Instead of going down that road, we propose to
abstract away from the physics of a LiDAR type (spinning vs solid-state, for
example), and focus on the spatial geometry of the point cloud generated by the
sensor. By modeling the calibration parameters as an element of a special
matrix Lie Group, we achieve a unifying view of calibration for different types
of LiDARs. We further prove mathematically that the proposed model is
well-constrained (has a unique answer) given four appropriately orientated
targets. The proof provides a guideline for target positioning in the form of a
tetrahedron. Moreover, an existing Semidefinite programming global solver for
SE(3) can be modified to compute efficiently the optimal calibration
parameters. For solid state LiDARs, we illustrate how the method works in
simulation. For spinning LiDARs, we show with experimental data that the
proposed matrix Lie Group model performs equally well as physics-based models
in terms of reducing the P2P distance, while being more robust to noise.
</p>
<a href="http://arxiv.org/abs/2012.03321" target="_blank">arXiv:2012.03321</a> [<a href="http://arxiv.org/pdf/2012.03321" target="_blank">pdf</a>]

<h2>Improving Auto-Encoders' self-supervised image classification using pseudo-labelling via data augmentation and the perceptual loss. (arXiv:2012.03322v1 [cs.CV])</h2>
<h3>Aymene Mohammed Bouayed, Karim Atif, Rachid Deriche, Abdelhakim Saim</h3>
<p>In this paper, we introduce a novel method to pseudo-label unlabelled images
and train an Auto-Encoder to classify them in a self-supervised manner that
allows for a high accuracy and consistency across several datasets. The
proposed method consists of first applying a randomly sampled set of data
augmentation transformations to each training image. As a result, each initial
image can be considered as a pseudo-label to its corresponding augmented ones.
Then, an Auto-Encoder is used to learn the mapping between each set of the
augmented images and its corresponding pseudo-label. Furthermore, the
perceptual loss is employed to take into consideration the existing
dependencies between the pixels in the same neighbourhood of an image. This
combination encourages the encoder to output richer encodings that are highly
informative of the input's class. Consequently, the Auto-Encoder's performance
on unsupervised image classification is improved both in termes of stability
and accuracy becoming more uniform and more consistent across all tested
datasets. Previous state-of-the-art accuracy on the MNIST, CIFAR-10 and SVHN
datasets is improved by 0.3\%, 3.11\% and 9.21\% respectively.
</p>
<a href="http://arxiv.org/abs/2012.03322" target="_blank">arXiv:2012.03322</a> [<a href="http://arxiv.org/pdf/2012.03322" target="_blank">pdf</a>]

<h2>Neural Online Graph Exploration. (arXiv:2012.03345v1 [cs.LG])</h2>
<h3>Ioannis Chiotellis, Daniel Cremers</h3>
<p>Can we learn how to explore unknown spaces efficiently? To answer this
question, we study the problem of Online Graph Exploration, the online version
of the Traveling Salesperson Problem. We reformulate graph exploration as a
reinforcement learning problem and apply Direct Future Prediction (Dosovitskiy
and Koltun, 2016) to solve it. As the graph is discovered online, the
corresponding Markov Decision Process entails a dynamic state space, namely the
observable graph and a dynamic action space, namely the nodes forming the
graph's frontier. To the best of our knowledge, this is the first attempt to
solve online graph exploration in a data-driven way. We conduct experiments on
six data sets of procedurally generated graphs and three real city road
networks. We demonstrate that our agent can learn strategies superior to many
well known graph traversal algorithms, confirming that exploration can be
learned.
</p>
<a href="http://arxiv.org/abs/2012.03345" target="_blank">arXiv:2012.03345</a> [<a href="http://arxiv.org/pdf/2012.03345" target="_blank">pdf</a>]

<h2>Rethinking FUN: Frequency-Domain Utilization Networks. (arXiv:2012.03357v1 [cs.CV])</h2>
<h3>Kfir Goldberg, Stav Shapiro, Elad Richardson, Shai Avidan</h3>
<p>The search for efficient neural network architectures has gained much focus
in recent years, where modern architectures focus not only on accuracy but also
on inference time and model size. Here, we present FUN, a family of novel
Frequency-domain Utilization Networks. These networks utilize the inherent
efficiency of the frequency-domain by working directly in that domain,
represented with the Discrete Cosine Transform. Using modern techniques and
building blocks such as compound-scaling and inverted-residual layers we
generate a set of such networks allowing one to balance between size, latency
and accuracy while outperforming competing RGB-based models. Extensive
evaluations verifies that our networks present strong alternatives to previous
approaches. Moreover, we show that working in frequency domain allows for
dynamic compression of the input at inference time without any explicit change
to the architecture.
</p>
<a href="http://arxiv.org/abs/2012.03357" target="_blank">arXiv:2012.03357</a> [<a href="http://arxiv.org/pdf/2012.03357" target="_blank">pdf</a>]

<h2>Select, Label, and Mix: Learning Discriminative Invariant Feature Representations for Partial Domain Adaptation. (arXiv:2012.03358v1 [cs.CV])</h2>
<h3>Aadarsh Sahoo, Rameswar Panda, Rogerio Feris, Kate Saenko, Abir Das</h3>
<p>Partial domain adaptation which assumes that the unknown target label space
is a subset of the source label space has attracted much attention in computer
vision. Despite recent progress, existing methods often suffer from three key
problems: negative transfer, lack of discriminability and domain invariance in
the latent space. To alleviate the above issues, we develop a novel 'Select,
Label, and Mix' (SLM) framework that aims to learn discriminative invariant
feature representations for partial domain adaptation. First, we present a
simple yet efficient "select" module that automatically filters out the outlier
source samples to avoid negative transfer while aligning distributions across
both domains. Second, the "label" module iteratively trains the classifier
using both the labeled source domain data and the generated pseudo-labels for
the target domain to enhance the discriminability of the latent space. Finally,
the "mix" module utilizes domain mixup regularization jointly with the other
two modules to explore more intrinsic structures across domains leading to a
domain-invariant latent space for partial domain adaptation. Extensive
experiments on several benchmark datasets demonstrate the superiority of our
proposed framework over state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2012.03358" target="_blank">arXiv:2012.03358</a> [<a href="http://arxiv.org/pdf/2012.03358" target="_blank">pdf</a>]

<h2>Self-Training for Class-Incremental Semantic Segmentation. (arXiv:2012.03362v1 [cs.CV])</h2>
<h3>Lu Yu, Xialei Liu, Joost van de Weijer</h3>
<p>We study incremental learning for semantic segmentation where when learning
new classes we have no access to the labeled data of previous tasks. When
incrementally learning new classes, deep neural networks suffer from
catastrophic forgetting of previous learned knowledge. To address this problem,
we propose to apply a self-training approach that leverages unlabeled data,
which is used for rehearsal of previous knowledge. Additionally, conflict
reduction is proposed to resolve the conflicts of pseudo labels generated from
both the old and new models. We show that maximizing self-entropy can further
improve results by smoothing the overconfident predictions. The experiments
demonstrate state-of-the-art results: obtaining a relative gain of up to 114%
on Pascal-VOC 2012 and 8.5% on the more challenging ADE20K compared to previous
state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2012.03362" target="_blank">arXiv:2012.03362</a> [<a href="http://arxiv.org/pdf/2012.03362" target="_blank">pdf</a>]

<h2>Unsupervised Regionalization of Particle-resolved Aerosol Mixing State Indices on the Global Scale. (arXiv:2012.03365v1 [cs.LG])</h2>
<h3>Zhonghua Zheng, Joseph Ching, Jeffrey H. Curtis, Yu Yao, Peng Xu, Matthew West, Nicole Riemer</h3>
<p>The aerosol mixing state significantly affects the climate and health impacts
of atmospheric aerosol particles. Simplified aerosol mixing state assumptions,
common in Earth System models, can introduce errors in the prediction of these
aerosol impacts. The aerosol mixing state index, a metric to quantify aerosol
mixing state, is a convenient measure for quantifying these errors. Global
estimates of aerosol mixing state indices have recently become available via
supervised learning models, but require regionalization to ease spatiotemporal
analysis. Here we developed a simple but effective unsupervised learning
approach to regionalize predictions of global aerosol mixing state indices. We
used the monthly average of aerosol mixing state indices global distribution as
the input data. Grid cells were then clustered into regions by the k-means
algorithm without explicit spatial information as input. This approach resulted
in eleven regions over the globe with specific spatial aggregation patterns.
Each region exhibited a unique distribution of mixing state indices and aerosol
compositions, showing the effectiveness of the unsupervised regionalization
approach. This study defines "aerosol mixing state zones" that could be useful
for atmospheric science research.
</p>
<a href="http://arxiv.org/abs/2012.03365" target="_blank">arXiv:2012.03365</a> [<a href="http://arxiv.org/pdf/2012.03365" target="_blank">pdf</a>]

<h2>Visual Aware Hierarchy Based Food Recognition. (arXiv:2012.03368v1 [cs.CV])</h2>
<h3>Runyu Mao, Jiangpeng He, Zeman Shao, Sri Kalyan Yarlagadda, Fengqing Zhu</h3>
<p>Food recognition is one of the most important components in image-based
dietary assessment. However, due to the different complexity level of food
images and inter-class similarity of food categories, it is challenging for an
image-based food recognition system to achieve high accuracy for a variety of
publicly available datasets. In this work, we propose a new two-step food
recognition system that includes food localization and hierarchical food
classification using Convolutional Neural Networks (CNNs) as the backbone
architecture. The food localization step is based on an implementation of the
Faster R-CNN method to identify food regions. In the food classification step,
visually similar food categories can be clustered together automatically to
generate a hierarchical structure that represents the semantic visual relations
among food categories, then a multi-task CNN model is proposed to perform the
classification task based on the visual aware hierarchical structure. Since the
size and quality of dataset is a key component of data driven methods, we
introduce a new food image dataset, VIPER-FoodNet (VFN) dataset, consists of 82
food categories with 15k images based on the most commonly consumed foods in
the United States. A semi-automatic crowdsourcing tool is used to provide the
ground-truth information for this dataset including food object bounding boxes
and food object labels. Experimental results demonstrate that our system can
significantly improve both classification and recognition performance on 4
publicly available datasets and the new VFN dataset.
</p>
<a href="http://arxiv.org/abs/2012.03368" target="_blank">arXiv:2012.03368</a> [<a href="http://arxiv.org/pdf/2012.03368" target="_blank">pdf</a>]

<h2>Proactive Pseudo-Intervention: Causally Informed Contrastive Learning For Interpretable Vision Models. (arXiv:2012.03369v1 [cs.CV])</h2>
<h3>Dong Wang, Yuewei Yang, Chenyang Tao, Fanjie Kong, Ricardo Henao, Lawrence Carin</h3>
<p>Deep neural networks have shown significant promise in comprehending complex
visual signals, delivering performance on par or even superior to that of human
experts. However, these models often lack a mechanism for interpreting their
predictions, and in some cases, particularly when the sample size is small,
existing deep learning solutions tend to capture spurious correlations that
compromise model generalizability on unseen inputs. In this work, we propose a
contrastive causal representation learning strategy that leverages proactive
interventions to identify causally-relevant image features, called Proactive
Pseudo-Intervention (PPI). This approach is complemented with a causal salience
map visualization module, i.e., Weight Back Propagation (WBP), that identifies
important pixels in the raw input image, which greatly facilitates the
interpretability of predictions. To validate its utility, our model is
benchmarked extensively on both standard natural images and challenging medical
image datasets. We show this new contrastive causal representation learning
model consistently improves model performance relative to competing solutions,
particularly for out-of-domain predictions or when dealing with data
integration from heterogeneous sources. Further, our causal saliency maps are
more succinct and meaningful relative to their non-causal counterparts.
</p>
<a href="http://arxiv.org/abs/2012.03369" target="_blank">arXiv:2012.03369</a> [<a href="http://arxiv.org/pdf/2012.03369" target="_blank">pdf</a>]

<h2>A Weighted Solution to SVM Actionability and Interpretability. (arXiv:2012.03372v1 [cs.LG])</h2>
<h3>Samuel Marc Denton, Ansaf Salleb-Aouissi</h3>
<p>Research in machine learning has successfully developed algorithms to build
accurate classification models. However, in many real-world applications, such
as healthcare, customer satisfaction, and environment protection, we want to be
able to use the models to decide what actions to take.

We investigate the concept of actionability in the context of Support Vector
Machines. Actionability is as important as interpretability or explainability
of machine learning models, an ongoing and important research topic.
Actionability is the task that gives us ways to act upon machine learning
models and their predictions.

This paper finds a solution to the question of actionability on both linear
and non-linear SVM models. Additionally, we introduce a way to account for
weighted actions that allow for more change in certain features than others. We
propose a gradient descent solution on the linear, RBF, and polynomial kernels,
and we test the effectiveness of our models on both synthetic and real
datasets. We are also able to explore the model's interpretability through the
lens of actionability.
</p>
<a href="http://arxiv.org/abs/2012.03372" target="_blank">arXiv:2012.03372</a> [<a href="http://arxiv.org/pdf/2012.03372" target="_blank">pdf</a>]

<h2>Art Style Classification with Self-Trained Ensemble of AutoEncoding Transformations. (arXiv:2012.03377v1 [cs.CV])</h2>
<h3>Akshay Joshi, Ankit Agrawal, Sushmita Nair</h3>
<p>The artistic style of a painting is a rich descriptor that reveals both
visual and deep intrinsic knowledge about how an artist uniquely portrays and
expresses their creative vision. Accurate categorization of paintings across
different artistic movements and styles is critical for large-scale indexing of
art databases. However, the automatic extraction and recognition of these
highly dense artistic features has received little to no attention in the field
of computer vision research. In this paper, we investigate the use of deep
self-supervised learning methods to solve the problem of recognizing complex
artistic styles with high intra-class and low inter-class variation. Further,
we outperform existing approaches by almost 20% on a highly class imbalanced
WikiArt dataset with 27 art categories. To achieve this, we train the EnAET
semi-supervised learning model (Wang et al., 2019) with limited annotated data
samples and supplement it with self-supervised representations learned from an
ensemble of spatial and non-spatial transformations.
</p>
<a href="http://arxiv.org/abs/2012.03377" target="_blank">arXiv:2012.03377</a> [<a href="http://arxiv.org/pdf/2012.03377" target="_blank">pdf</a>]

<h2>Brain Co-Processors: Using AI to Restore and Augment Brain Function. (arXiv:2012.03378v1 [cs.AI])</h2>
<h3>Rajesh P. N. Rao</h3>
<p>Brain-computer interfaces (BCIs) use decoding algorithms to control
prosthetic devices based on brain signals for restoration of lost function.
Computer-brain interfaces (CBIs), on the other hand, use encoding algorithms to
transform external sensory signals into neural stimulation patterns for
restoring sensation or providing sensory feedback for closed-loop prosthetic
control. In this article, we introduce brain co-processors, devices that
combine decoding and encoding in a unified framework using artificial
intelligence (AI) to supplement or augment brain function. Brain co-processors
can be used for a range of applications, from inducing Hebbian plasticity for
rehabilitation after brain injury to reanimating paralyzed limbs and enhancing
memory. A key challenge is simultaneous multi-channel neural decoding and
encoding for optimization of external behavioral or task-related goals. We
describe a new framework for developing brain co-processors based on artificial
neural networks, deep learning and reinforcement learning. These "neural
co-processors" allow joint optimization of cost functions with the nervous
system to achieve desired behaviors. By coupling artificial neural networks
with their biological counterparts, neural co-processors offer a new way of
restoring and augmenting the brain, as well as a new scientific tool for brain
research. We conclude by discussing the potential applications and ethical
implications of brain co-processors.
</p>
<a href="http://arxiv.org/abs/2012.03378" target="_blank">arXiv:2012.03378</a> [<a href="http://arxiv.org/pdf/2012.03378" target="_blank">pdf</a>]

<h2>Learning to Rearrange Deformable Cables, Fabrics, and Bags with Goal-Conditioned Transporter Networks. (arXiv:2012.03385v1 [cs.RO])</h2>
<h3>Daniel Seita, Pete Florence, Jonathan Tompson, Erwin Coumans, Vikas Sindhwani, Ken Goldberg, Andy Zeng</h3>
<p>Rearranging and manipulating deformable objects such as cables, fabrics, and
bags is a long-standing challenge in robotic manipulation. The complex dynamics
and high-dimensional configuration spaces of deformables, compared to rigid
objects, make manipulation difficult not only for multi-step planning, but even
for goal specification. Goals cannot be as easily specified as rigid object
poses, and may involve complex relative spatial relations such as "place the
item inside the bag". In this work, we develop a suite of simulated benchmarks
with 1D, 2D, and 3D deformable structures, including tasks that involve
image-based goal-conditioning and multi-step deformable manipulation. We
propose embedding goal-conditioning into Transporter Networks, a recently
proposed model architecture for learning robotic manipulation that rearranges
deep features to infer displacements that can represent pick and place actions.
We demonstrate that goal-conditioned Transporter Networks enable agents to
manipulate deformable structures into flexibly specified configurations without
test-time visual anchors for target locations. We also significantly extend
prior results using Transporter Networks for manipulating deformable objects by
testing on tasks with 2D and 3D deformables. Supplementary material is
available at https://berkeleyautomation.github.io/bags/.
</p>
<a href="http://arxiv.org/abs/2012.03385" target="_blank">arXiv:2012.03385</a> [<a href="http://arxiv.org/pdf/2012.03385" target="_blank">pdf</a>]

<h2>On Infusing Reachability-Based Safety Assurance within Planning Frameworks for Human-Robot Vehicle Interactions. (arXiv:2012.03390v1 [cs.RO])</h2>
<h3>Karen Leung, Edward Schmerling, Mengxuan Zhang, Mo Chen, John Talbot, J. Christian Gerdes, Marco Pavone</h3>
<p>Action anticipation, intent prediction, and proactive behavior are all
desirable characteristics for autonomous driving policies in interactive
scenarios. Paramount, however, is ensuring safety on the road -- a key
challenge in doing so is accounting for uncertainty in human driver actions
without unduly impacting planner performance. This paper introduces a
minimally-interventional safety controller operating within an autonomous
vehicle control stack with the role of ensuring collision-free interaction with
an externally controlled (e.g., human-driven) counterpart while respecting
static obstacles such as a road boundary wall. We leverage reachability
analysis to construct a real-time (100Hz) controller that serves the dual role
of (i) tracking an input trajectory from a higher-level planning algorithm
using model predictive control, and (ii) assuring safety by maintaining the
availability of a collision-free escape maneuver as a persistent constraint
regardless of whatever future actions the other car takes. A full-scale
steer-by-wire platform is used to conduct traffic weaving experiments wherein
two cars, initially side-by-side, must swap lanes in a limited amount of time
and distance, emulating cars merging onto/off of a highway. We demonstrate
that, with our control stack, the autonomous vehicle is able to avoid collision
even when the other car defies the planner's expectations and takes dangerous
actions, either carelessly or with the intent to collide, and otherwise
deviates minimally from the planned trajectory to the extent required to
maintain safety.
</p>
<a href="http://arxiv.org/abs/2012.03390" target="_blank">arXiv:2012.03390</a> [<a href="http://arxiv.org/pdf/2012.03390" target="_blank">pdf</a>]

<h2>CompFeat: Comprehensive Feature Aggregation for Video Instance Segmentation. (arXiv:2012.03400v1 [cs.CV])</h2>
<h3>Yang Fu, Linjie Yang, Ding Liu, Thomas S. Huang, Humphrey Shi</h3>
<p>Video instance segmentation is a complex task in which we need to detect,
segment, and track each object for any given video. Previous approaches only
utilize single-frame features for the detection, segmentation, and tracking of
objects and they suffer in the video scenario due to several distinct
challenges such as motion blur and drastic appearance change. To eliminate
ambiguities introduced by only using single-frame features, we propose a novel
comprehensive feature aggregation approach (CompFeat) to refine features at
both frame-level and object-level with temporal and spatial context
information. The aggregation process is carefully designed with a new attention
mechanism which significantly increases the discriminative power of the learned
features. We further improve the tracking capability of our model through a
siamese design by incorporating both feature similarities and spatial
similarities. Experiments conducted on the YouTube-VIS dataset validate the
effectiveness of proposed CompFeat. Our code will be available at
https://github.com/SHI-Labs/CompFeat-for-Video-Instance-Segmentation.
</p>
<a href="http://arxiv.org/abs/2012.03400" target="_blank">arXiv:2012.03400</a> [<a href="http://arxiv.org/pdf/2012.03400" target="_blank">pdf</a>]

<h2>The Neural Coding Framework for Learning Generative Models. (arXiv:2012.03405v1 [cs.LG])</h2>
<h3>Alexander Ororbia, Daniel Kifer</h3>
<p>Neural generative models can be used to learn complex probability
distributions from data, to sample from them, and to produce probability
density estimates. We propose a novel neural generative model inspired by the
theory of predictive processing in the brain. According to predictive
processing theory, the neurons in the brain form a hierarchy in which neurons
in one level form expectations about sensory inputs from another level. These
neurons update their local models based on the differences between their
expectations and the observed signals. In a similar way, artificial neurons in
our generative model predict what neighboring neurons will do, and adjust their
parameters based on how well the predictions matched reality. This neural
generative model performs very well in practice. On a variety of benchmark
datasets and metrics, it either remains competitive with or significantly
outperforms other generative models with similar functionality (such as the
popular variational auto-encoder).
</p>
<a href="http://arxiv.org/abs/2012.03405" target="_blank">arXiv:2012.03405</a> [<a href="http://arxiv.org/pdf/2012.03405" target="_blank">pdf</a>]

<h2>PMP-Net: Point Cloud Completion by Learning Multi-step Point Moving Paths. (arXiv:2012.03408v1 [cs.CV])</h2>
<h3>Xin Wen, Peng Xiang, Zhizhong Han, Yan-Pei Cao, Pengfei Wan, Wen Zheng, Yu-Shen Liu</h3>
<p>The task of point cloud completion aims to predict the missing part for an
incomplete 3D shape. A widely used strategy is to generate a complete point
cloud from the incomplete one. However, the unordered nature of point clouds
will degrade the generation of high-quality 3D shapes, as the detailed topology
and structure of discrete points are hard to be captured by the generative
process only using a latent code. In this paper, we address the above problem
by reconsidering the completion task from a new perspective, where we formulate
the prediction as a point cloud deformation process. Specifically, we design a
novel neural network, named PMP-Net, to mimic the behavior of an earth mover.
It moves move each point of the incomplete input to complete the point cloud,
where the total distance of point moving paths (PMP) should be shortest.
Therefore, PMP-Net predicts a unique point moving path for each point according
to the constraint of total point moving distances. As a result, the network
learns a strict and unique correspondence on point-level, which can capture the
detailed topology and structure relationships between the incomplete shape and
the complete target, and thus improves the quality of the predicted complete
shape. We conduct comprehensive experiments on Completion3D and PCN datasets,
which demonstrate our advantages over the state-of-the-art point cloud
completion methods.
</p>
<a href="http://arxiv.org/abs/2012.03408" target="_blank">arXiv:2012.03408</a> [<a href="http://arxiv.org/pdf/2012.03408" target="_blank">pdf</a>]

<h2>Mapping Network States Using Connectivity Queries. (arXiv:2012.03413v1 [cs.LG])</h2>
<h3>Alexander Rodr&#xed;guez, Bijaya Adhikari, Andr&#xe9;s D. Gonz&#xe1;lez, Charles Nicholson, Anil Vullikanti, B. Aditya Prakash</h3>
<p>Can we infer all the failed components of an infrastructure network, given a
sample of reachable nodes from supply nodes? One of the most critical
post-disruption processes after a natural disaster is to quickly determine the
damage or failure states of critical infrastructure components. However, this
is non-trivial, considering that often only a fraction of components may be
accessible or observable after a disruptive event. Past work has looked into
inferring failed components given point probes, i.e. with a direct sample of
failed components. In contrast, we study the harder problem of inferring failed
components given partial information of some `serviceable' reachable nodes and
a small sample of point probes, being the first often more practical to obtain.
We formulate this novel problem using the Minimum Description Length (MDL)
principle, and then present a greedy algorithm that minimizes MDL cost
effectively. We evaluate our algorithm on domain-expert simulations of real
networks in the aftermath of an earthquake. Our algorithm successfully identify
failed components, especially the critical ones affecting the overall system
performance.
</p>
<a href="http://arxiv.org/abs/2012.03413" target="_blank">arXiv:2012.03413</a> [<a href="http://arxiv.org/pdf/2012.03413" target="_blank">pdf</a>]

<h2>Vehicular Cooperative Perception Through Action Branching and Federated Reinforcement Learning. (arXiv:2012.03414v1 [cs.LG])</h2>
<h3>Mohamed K. Abdel-Aziz, Cristina Perfecto, Sumudu Samarakoon, Mehdi Bennis, Walid Saad</h3>
<p>Cooperative perception plays a vital role in extending a vehicle's sensing
range beyond its line-of-sight. However, exchanging raw sensory data under
limited communication resources is infeasible. Towards enabling an efficient
cooperative perception, vehicles need to address the following fundamental
question: What sensory data needs to be shared?, at which resolution?, and with
which vehicles? To answer this question, in this paper, a novel framework is
proposed to allow reinforcement learning (RL)-based vehicular association,
resource block (RB) allocation, and content selection of cooperative perception
messages (CPMs) by utilizing a quadtree-based point cloud compression
mechanism. Furthermore, a federated RL approach is introduced in order to speed
up the training process across vehicles. Simulation results show the ability of
the RL agents to efficiently learn the vehicles' association, RB allocation,
and message content selection while maximizing vehicles' satisfaction in terms
of the received sensory information. The results also show that federated RL
improves the training process, where better policies can be achieved within the
same amount of time compared to the non-federated approach.
</p>
<a href="http://arxiv.org/abs/2012.03414" target="_blank">arXiv:2012.03414</a> [<a href="http://arxiv.org/pdf/2012.03414" target="_blank">pdf</a>]

<h2>Boosting Image Super-Resolution Via Fusion of Complementary Information Captured by Multi-Modal Sensors. (arXiv:2012.03417v1 [cs.CV])</h2>
<h3>Fan Wang, Jiangxin Yang, Yanlong Cao, Yanpeng Cao, Michael Ying Yang</h3>
<p>Image Super-Resolution (SR) provides a promising technique to enhance the
image quality of low-resolution optical sensors, facilitating better-performing
target detection and autonomous navigation in a wide range of robotics
applications. It is noted that the state-of-the-art SR methods are typically
trained and tested using single-channel inputs, neglecting the fact that the
cost of capturing high-resolution images in different spectral domains varies
significantly. In this paper, we attempt to leverage complementary information
from a low-cost channel (visible/depth) to boost image quality of an expensive
channel (thermal) using fewer parameters. To this end, we first present an
effective method to virtually generate pixel-wise aligned visible and thermal
images based on real-time 3D reconstruction of multi-modal data captured at
various viewpoints. Then, we design a feature-level multispectral fusion
residual network model to perform high-accuracy SR of thermal images by
adaptively integrating co-occurrence features presented in multispectral
images. Experimental results demonstrate that this new approach can effectively
alleviate the ill-posed inverse problem of image SR by taking into account
complementary information from an additional low-cost channel, significantly
outperforming state-of-the-art SR approaches in terms of both accuracy and
efficiency.
</p>
<a href="http://arxiv.org/abs/2012.03417" target="_blank">arXiv:2012.03417</a> [<a href="http://arxiv.org/pdf/2012.03417" target="_blank">pdf</a>]

<h2>Sobolev Wasserstein GAN. (arXiv:2012.03420v1 [cs.LG])</h2>
<h3>Minkai Xu, Zhiming Zhou, Guansong Lu, Jian Tang, Weinan Zhang, Yong Yu</h3>
<p>Wasserstein GANs (WGANs), built upon the Kantorovich-Rubinstein (KR) duality
of Wasserstein distance, is one of the most theoretically sound GAN models.
However, in practice it does not always outperform other variants of GANs. This
is mostly due to the imperfect implementation of the Lipschitz condition
required by the KR duality. Extensive work has been done in the community with
different implementations of the Lipschitz constraint, which, however, is still
hard to satisfy the restriction perfectly in practice. In this paper, we argue
that the strong Lipschitz constraint might be unnecessary for optimization.
Instead, we take a step back and try to relax the Lipschitz constraint.
Theoretically, we first demonstrate a more general dual form of the Wasserstein
distance called the Sobolev duality, which relaxes the Lipschitz constraint but
still maintains the favorable gradient property of the Wasserstein distance.
Moreover, we show that the KR duality is actually a special case of the Sobolev
duality. Based on the relaxed duality, we further propose a generalized WGAN
training scheme named Sobolev Wasserstein GAN (SWGAN), and empirically
demonstrate the improvement of SWGAN over existing methods with extensive
experiments.
</p>
<a href="http://arxiv.org/abs/2012.03420" target="_blank">arXiv:2012.03420</a> [<a href="http://arxiv.org/pdf/2012.03420" target="_blank">pdf</a>]

<h2>Learning Graph Neural Networks with Approximate Gradient Descent. (arXiv:2012.03429v1 [cs.LG])</h2>
<h3>Qunwei Li, Shaofeng Zou, Wenliang Zhong</h3>
<p>The first provably efficient algorithm for learning graph neural networks
(GNNs) with one hidden layer for node information convolution is provided in
this paper. Two types of GNNs are investigated, depending on whether labels are
attached to nodes or graphs. A comprehensive framework for designing and
analyzing convergence of GNN training algorithms is developed. The algorithm
proposed is applicable to a wide range of activation functions including ReLU,
Leaky ReLU, Sigmod, Softplus and Swish. It is shown that the proposed algorithm
guarantees a linear convergence rate to the underlying true parameters of GNNs.
For both types of GNNs, sample complexity in terms of the number of nodes or
the number of graphs is characterized. The impact of feature dimension and GNN
structure on the convergence rate is also theoretically characterized.
Numerical experiments are further provided to validate our theoretical
analysis.
</p>
<a href="http://arxiv.org/abs/2012.03429" target="_blank">arXiv:2012.03429</a> [<a href="http://arxiv.org/pdf/2012.03429" target="_blank">pdf</a>]

<h2>Interpreting Deep Neural Networks with Relative Sectional Propagation by Analyzing Comparative Gradients and Hostile Activations. (arXiv:2012.03434v1 [cs.CV])</h2>
<h3>Woo-Jeoung Nam, jaesik choi, Seong-Whan Lee</h3>
<p>The clear transparency of Deep Neural Networks (DNNs) is hampered by complex
internal structures and nonlinear transformations along deep hierarchies. In
this paper, we propose a new attribution method, Relative Sectional Propagation
(RSP), for fully decomposing the output predictions with the characteristics of
class-discriminative attributions and clear objectness. We carefully revisit
some shortcomings of backpropagation-based attribution methods, which are
trade-off relations in decomposing DNNs. We define hostile factor as an element
that interferes with finding the attributions of the target and propagate it in
a distinguishable way to overcome the non-suppressed nature of activated
neurons. As a result, it is possible to assign the bi-polar relevance scores of
the target (positive) and hostile (negative) attributions while maintaining
each attribution aligned with the importance. We also present the purging
techniques to prevent the decrement of the gap between the relevance scores of
the target and hostile attributions during backward propagation by eliminating
the conflicting units to channel attribution map. Therefore, our method makes
it possible to decompose the predictions of DNNs with clearer
class-discriminativeness and detailed elucidations of activation neurons
compared to the conventional attribution methods. In a verified experimental
environment, we report the results of the assessments: (i) Pointing Game, (ii)
mIoU, and (iii) Model Sensitivity with PASCAL VOC 2007, MS COCO 2014, and
ImageNet datasets. The results demonstrate that our method outperforms existing
backward decomposition methods, including distinctive and intuitive
visualizations.
</p>
<a href="http://arxiv.org/abs/2012.03434" target="_blank">arXiv:2012.03434</a> [<a href="http://arxiv.org/pdf/2012.03434" target="_blank">pdf</a>]

<h2>Low-Rank Tensor Recovery with Euclidean-Norm-Induced Schatten-p Quasi-Norm Regularization. (arXiv:2012.03436v1 [cs.LG])</h2>
<h3>Jicong Fan, Lijun Ding, Chengrun Yang, Madeleine Udell</h3>
<p>The nuclear norm and Schatten-$p$ quasi-norm of a matrix are popular rank
proxies in low-rank matrix recovery. Unfortunately, computing the nuclear norm
or Schatten-$p$ quasi-norm of a tensor is NP-hard, which is a pity for low-rank
tensor completion (LRTC) and tensor robust principal component analysis
(TRPCA). In this paper, we propose a new class of rank regularizers based on
the Euclidean norms of the CP component vectors of a tensor and show that these
regularizers are monotonic transformations of tensor Schatten-$p$ quasi-norm.
This connection enables us to minimize the Schatten-$p$ quasi-norm in LRTC and
TRPCA implicitly. The methods do not use the singular value decomposition and
hence scale to big tensors. Moreover, the methods are not sensitive to the
choice of initial rank and provide an arbitrarily sharper rank proxy for
low-rank tensor recovery compared to nuclear norm. We provide theoretical
guarantees in terms of recovery error for LRTC and TRPCA, which show relatively
smaller $p$ of Schatten-$p$ quasi-norm leads to tighter error bounds.
Experiments using LRTC and TRPCA on synthetic data and natural images verify
the effectiveness and superiority of our methods compared to baseline methods.
</p>
<a href="http://arxiv.org/abs/2012.03436" target="_blank">arXiv:2012.03436</a> [<a href="http://arxiv.org/pdf/2012.03436" target="_blank">pdf</a>]

<h2>MFST: A Python OpenFST Wrapper With Support for Custom Semirings and Jupyter Notebooks. (arXiv:2012.03437v1 [cs.LG])</h2>
<h3>Matthew Francis-Landau</h3>
<p>This paper introduces mFST, a new Python library for working with
Finite-State Machines based on OpenFST. mFST is a thin wrapper for OpenFST and
exposes all of OpenFST's methods for manipulating FSTs. Additionally, mFST is
the only Python wrapper for OpenFST that exposes OpenFST's ability to define a
custom semirings. This makes mFST ideal for developing models that involve
learning the weights on a FST or creating neuralized FSTs. mFST has been
designed to be easy to get started with and has been previously used in
homework assignments for a NLP class as well in projects for integrating FSTs
and neural networks. In this paper, we exhibit mFST API and how to use mFST to
build a simple neuralized FST with PyTorch.
</p>
<a href="http://arxiv.org/abs/2012.03437" target="_blank">arXiv:2012.03437</a> [<a href="http://arxiv.org/pdf/2012.03437" target="_blank">pdf</a>]

<h2>Selective Pseudo-Labeling with Reinforcement Learning for Semi-Supervised Domain Adaptation. (arXiv:2012.03438v1 [cs.CV])</h2>
<h3>Bingyu Liu, Yuhong Guo, Jieping Ye, Weihong Deng</h3>
<p>Recent domain adaptation methods have demonstrated impressive improvement on
unsupervised domain adaptation problems. However, in the semi-supervised domain
adaptation (SSDA) setting where the target domain has a few labeled instances
available, these methods can fail to improve performance. Inspired by the
effectiveness of pseudo-labels in domain adaptation, we propose a reinforcement
learning based selective pseudo-labeling method for semi-supervised domain
adaptation. It is difficult for conventional pseudo-labeling methods to balance
the correctness and representativeness of pseudo-labeled data. To address this
limitation, we develop a deep Q-learning model to select both accurate and
representative pseudo-labeled instances. Moreover, motivated by large margin
loss's capacity on learning discriminative features with little data, we
further propose a novel target margin loss for our base model training to
improve its discriminability. Our proposed method is evaluated on several
benchmark datasets for SSDA, and demonstrates superior performance to all the
comparison methods.
</p>
<a href="http://arxiv.org/abs/2012.03438" target="_blank">arXiv:2012.03438</a> [<a href="http://arxiv.org/pdf/2012.03438" target="_blank">pdf</a>]

<h2>Hyperspectral Classification Based on Lightweight 3-D-CNN With Transfer Learning. (arXiv:2012.03439v1 [cs.CV])</h2>
<h3>Haokui Zhang, Ying Li, Yenan Jiang, Peng Wang, Qiang Shen, Chunhua Shen</h3>
<p>Recently, hyperspectral image (HSI) classification approaches based on deep
learning (DL) models have been proposed and shown promising performance.
However, because of very limited available training samples and massive model
parameters, DL methods may suffer from overfitting. In this paper, we propose
an end-to-end 3-D lightweight convolutional neural network (CNN) (abbreviated
as 3-D-LWNet) for limited samples-based HSI classification. Compared with
conventional 3-D-CNN models, the proposed 3-D-LWNet has a deeper network
structure, less parameters, and lower computation cost, resulting in better
classification performance. To further alleviate the small sample problem, we
also propose two transfer learning strategies: 1) cross-sensor strategy, in
which we pretrain a 3-D model in the source HSI data sets containing a greater
number of labeled samples and then transfer it to the target HSI data sets and
2) cross-modal strategy, in which we pretrain a 3-D model in the 2-D RGB image
data sets containing a large number of samples and then transfer it to the
target HSI data sets. In contrast to previous approaches, we do not impose
restrictions over the source data sets, in which they do not have to be
collected by the same sensors as the target data sets. Experiments on three
public HSI data sets captured by different sensors demonstrate that our model
achieves competitive performance for HSI classification compared to several
state-of-the-art methods
</p>
<a href="http://arxiv.org/abs/2012.03439" target="_blank">arXiv:2012.03439</a> [<a href="http://arxiv.org/pdf/2012.03439" target="_blank">pdf</a>]

<h2>Variational Autoencoders for Learning Nonlinear Dynamics of Physical Systems. (arXiv:2012.03448v1 [cs.LG])</h2>
<h3>Ryan Lopez, Paul J. Atzberger</h3>
<p>We develop data-driven methods for incorporating physical information for
priors to learn parsimonious representations of nonlinear systems arising from
parameterized PDEs and mechanics. Our approach is based on Variational
Autoencoders (VAEs) for learning from observations nonlinear state space
models. We develop ways to incorporate geometric and topological priors through
general manifold latent space representations. We investigate the performance
of our methods for learning low dimensional representations for the nonlinear
Burgers equation and constrained mechanical systems.
</p>
<a href="http://arxiv.org/abs/2012.03448" target="_blank">arXiv:2012.03448</a> [<a href="http://arxiv.org/pdf/2012.03448" target="_blank">pdf</a>]

<h2>Efficient Heuristic Generation for Robot Path Planning with Recurrent Generative Model. (arXiv:2012.03449v1 [cs.RO])</h2>
<h3>Zhaoting Li, Jiankun Wang, Max Q.-H. Meng</h3>
<p>Robot path planning is difficult to solve due to the contradiction between
optimality of results and complexity of algorithms, even in 2D environments. To
find an optimal path, the algorithm needs to search all the state space, which
costs a lot of computation resource. To address this issue, we present a novel
recurrent generative model (RGM) which generates efficient heuristic to reduce
the search efforts of path planning algorithm. This RGM model adopts the
framework of general generative adversarial networks (GAN), which consists of a
novel generator that can generate heuristic by refining the outputs recurrently
and two discriminators that check the connectivity and safety properties of
heuristic. We test the proposed RGM module in various 2D environments to
demonstrate its effectiveness and efficiency. The results show that the RGM
successfully generates appropriate heuristic in both seen and new unseen maps
with a high accuracy, demonstrating the good generalization ability of this
model. We also compare the rapidly-exploring random tree star (RRT*) with
generated heuristic and the conventional RRT* in four different maps, showing
that the generated heuristic can guide the algorithm to find both initial and
optimal solution in a faster and more efficient way.
</p>
<a href="http://arxiv.org/abs/2012.03449" target="_blank">arXiv:2012.03449</a> [<a href="http://arxiv.org/pdf/2012.03449" target="_blank">pdf</a>]

<h2>Stronger Calibration Lower Bounds via Sidestepping. (arXiv:2012.03454v1 [cs.LG])</h2>
<h3>Mingda Qiao, Gregory Valiant</h3>
<p>We consider an online binary prediction setting where a forecaster observes a
sequence of $T$ bits one by one. Before each bit is revealed, the forecaster
predicts the probability that the bit is $1$. The forecaster is called
well-calibrated if for each $p \in [0, 1]$, among the $n_p$ bits for which the
forecaster predicts probability $p$, the actual number of ones, $m_p$, is
indeed equal to $p \cdot n_p$. The calibration error, defined as $\sum_p |m_p -
p n_p|$, quantifies the extent to which the forecaster deviates from being
well-calibrated. It has long been known that an $O(T^{2/3})$ calibration error
is achievable even when the bits are chosen adversarially, and possibly based
on the previous predictions. However, little is known on the lower bound side,
except an $\Omega(\sqrt{T})$ bound that follows from the trivial example of
independent fair coin flips.

In this paper, we prove an $\Omega(T^{0.528})$ bound on the calibration
error, which is the first super-$\sqrt{T}$ lower bound for this setting to the
best of our knowledge. The technical contributions of our work include two
lower bound techniques, early stopping and sidestepping, which circumvent the
obstacles that have previously hindered strong calibration lower bounds. We
also propose an abstraction of the prediction setting, termed the
Sign-Preservation game, which may be of independent interest. This game has a
much smaller state space than the full prediction setting and allows simpler
analyses. The $\Omega(T^{0.528})$ lower bound follows from a general reduction
theorem that translates lower bounds on the game value of Sign-Preservation
into lower bounds on the calibration error.
</p>
<a href="http://arxiv.org/abs/2012.03454" target="_blank">arXiv:2012.03454</a> [<a href="http://arxiv.org/pdf/2012.03454" target="_blank">pdf</a>]

<h2>TP-TIO: A Robust Thermal-Inertial Odometry with Deep ThermalPoint. (arXiv:2012.03455v1 [cs.RO])</h2>
<h3>Shibo Zhao, Peng Wang, Hengrui Zhang, Zheng Fang, Sebastian Scherer</h3>
<p>To achieve robust motion estimation in visually degraded environments,
thermal odometry has been an attraction in the robotics community. However,
most thermal odometry methods are purely based on classical feature extractors,
which is difficult to establish robust correspondences in successive frames due
to sudden photometric changes and large thermal noise. To solve this problem,
we propose ThermalPoint, a lightweight feature detection network specifically
tailored for producing keypoints on thermal images, providing notable
anti-noise improvements compared with other state-of-the-art methods. After
that, we combine ThermalPoint with a novel radiometric feature tracking method,
which directly makes use of full radiometric data and establishes reliable
correspondences between sequential frames. Finally, taking advantage of an
optimization-based visual-inertial framework, a deep feature-based
thermal-inertial odometry (TP-TIO) framework is proposed and evaluated
thoroughly in various visually degraded environments. Experiments show that our
method outperforms state-of-the-art visual and laser odometry methods in
smoke-filled environments and achieves competitive accuracy in normal
environments.
</p>
<a href="http://arxiv.org/abs/2012.03455" target="_blank">arXiv:2012.03455</a> [<a href="http://arxiv.org/pdf/2012.03455" target="_blank">pdf</a>]

<h2>VideoMix: Rethinking Data Augmentation for Video Classification. (arXiv:2012.03457v1 [cs.CV])</h2>
<h3>Sangdoo Yun, Seong Joon Oh, Byeongho Heo, Dongyoon Han, Jinhyung Kim</h3>
<p>State-of-the-art video action classifiers often suffer from overfitting. They
tend to be biased towards specific objects and scene cues, rather than the
foreground action content, leading to sub-optimal generalization performances.
Recent data augmentation strategies have been reported to address the
overfitting problems in static image classifiers. Despite the effectiveness on
the static image classifiers, data augmentation has rarely been studied for
videos. For the first time in the field, we systematically analyze the efficacy
of various data augmentation strategies on the video classification task. We
then propose a powerful augmentation strategy VideoMix. VideoMix creates a new
training video by inserting a video cuboid into another video. The ground truth
labels are mixed proportionally to the number of voxels from each video. We
show that VideoMix lets a model learn beyond the object and scene biases and
extract more robust cues for action recognition. VideoMix consistently
outperforms other augmentation baselines on Kinetics and the challenging
Something-Something-V2 benchmarks. It also improves the weakly-supervised
action localization performance on THUMOS'14. VideoMix pretrained models
exhibit improved accuracies on the video detection task (AVA).
</p>
<a href="http://arxiv.org/abs/2012.03457" target="_blank">arXiv:2012.03457</a> [<a href="http://arxiv.org/pdf/2012.03457" target="_blank">pdf</a>]

<h2>Deep Neural Network Training without Multiplications. (arXiv:2012.03458v1 [cs.LG])</h2>
<h3>Tsuguo Mogami</h3>
<p>Is multiplication really necessary for deep neural networks? Here we propose
just adding two IEEE754 floating-point numbers with an integer-add instruction
in place of a floating-point multiplication instruction. We show that ResNet
can be trained using this operation with competitive classification accuracy.
Our proposal did not require any methods to solve instability and decrease in
accuracy, which is common in low-precision training. In some settings, we may
obtain equal accuracy to the baseline FP32 result. This method will enable
eliminating the multiplications in deep neural-network training and inference.
</p>
<a href="http://arxiv.org/abs/2012.03458" target="_blank">arXiv:2012.03458</a> [<a href="http://arxiv.org/pdf/2012.03458" target="_blank">pdf</a>]

<h2>PFA-GAN: Progressive Face Aging with Generative Adversarial Network. (arXiv:2012.03459v1 [cs.CV])</h2>
<h3>Zhizhong Huang, Shouzhen Chen, Junping Zhang, Hongming Shan</h3>
<p>Face aging is to render a given face to predict its future appearance, which
plays an important role in the information forensics and security field as the
appearance of the face typically varies with age. Although impressive results
have been achieved with conditional generative adversarial networks (cGANs),
the existing cGANs-based methods typically use a single network to learn
various aging effects between any two different age groups. However, they
cannot simultaneously meet three essential requirements of face aging --
including image quality, aging accuracy, and identity preservation -- and
usually generate aged faces with strong ghost artifacts when the age gap
becomes large. Inspired by the fact that faces gradually age over time, this
paper proposes a novel progressive face aging framework based on generative
adversarial network (PFA-GAN) to mitigate these issues. Unlike the existing
cGANs-based methods, the proposed framework contains several sub-networks to
mimic the face aging process from young to old, each of which only learns some
specific aging effects between two adjacent age groups. The proposed framework
can be trained in an end-to-end manner to eliminate accumulative artifacts and
blurriness. Moreover, this paper introduces an age estimation loss to take into
account the age distribution for an improved aging accuracy, and proposes to
use the Pearson correlation coefficient as an evaluation metric measuring the
aging smoothness for face aging methods. Extensively experimental results
demonstrate superior performance over existing (c)GANs-based methods, including
the state-of-the-art one, on two benchmarked datasets. The source code is
available at~\url{https://github.com/Hzzone/PFA-GAN}.
</p>
<a href="http://arxiv.org/abs/2012.03459" target="_blank">arXiv:2012.03459</a> [<a href="http://arxiv.org/pdf/2012.03459" target="_blank">pdf</a>]

<h2>Reprogramming Language Models for Molecular Representation Learning. (arXiv:2012.03460v1 [cs.LG])</h2>
<h3>Ria Vinod, Pin-Yu Chen, Payel Das</h3>
<p>Recent advancements in transfer learning have made it a promising approach
for domain adaptation via transfer of learned representations. This is
especially when relevant when alternate tasks have limited samples of
well-defined and labeled data, which is common in the molecule data domain.
This makes transfer learning an ideal approach to solve molecular learning
tasks. While Adversarial reprogramming has proven to be a successful method to
repurpose neural networks for alternate tasks, most works consider source and
alternate tasks within the same domain. In this work, we propose a new
algorithm, Representation Reprogramming via Dictionary Learning (R2DL), for
adversarially reprogramming pretrained language models for molecular learning
tasks, motivated by leveraging learned representations in massive state of the
art language models. The adversarial program learns a linear transformation
between a dense source model input space (language data) and a sparse target
model input space (e.g., chemical and biological molecule data) using a k-SVD
solver to approximate a sparse representation of the encoded data, via
dictionary learning. R2DL achieves the baseline established by state of the art
toxicity prediction models trained on domain-specific data and outperforms the
baseline in a limited training-data setting, thereby establishing avenues for
domain-agnostic transfer learning for tasks with molecule data.
</p>
<a href="http://arxiv.org/abs/2012.03460" target="_blank">arXiv:2012.03460</a> [<a href="http://arxiv.org/pdf/2012.03460" target="_blank">pdf</a>]

<h2>Attention-based Saliency Hashing for Ophthalmic Image Retrieval. (arXiv:2012.03466v1 [cs.CV])</h2>
<h3>Jiansheng Fang, Yanwu Xu, Xiaoqing Zhang, Yan Hu, Jiang Liu</h3>
<p>Deep hashing methods have been proved to be effective for the large-scale
medical image search assisting reference-based diagnosis for clinicians.
However, when the salient region plays a maximal discriminative role in
ophthalmic image, existing deep hashing methods do not fully exploit the
learning ability of the deep network to capture the features of salient regions
pointedly. The different grades or classes of ophthalmic images may be share
similar overall performance but have subtle differences that can be
differentiated by mining salient regions. To address this issue, we propose a
novel end-to-end network, named Attention-based Saliency Hashing (ASH), for
learning compact hash-code to represent ophthalmic images. ASH embeds a
spatial-attention module to focus more on the representation of salient regions
and highlights their essential role in differentiating ophthalmic images.
Benefiting from the spatial-attention module, the information of salient
regions can be mapped into the hash-code for similarity calculation. In the
training stage, we input the image pairs to share the weights of the network,
and a pairwise loss is designed to maximize the discriminability of the
hash-code. In the retrieval stage, ASH obtains the hash-code by inputting an
image with an end-to-end manner, then the hash-code is used to similarity
calculation to return the most similar images. Extensive experiments on two
different modalities of ophthalmic image datasets demonstrate that the proposed
ASH can further improve the retrieval performance compared to the
state-of-the-art deep hashing methods due to the huge contributions of the
spatial-attention module.
</p>
<a href="http://arxiv.org/abs/2012.03466" target="_blank">arXiv:2012.03466</a> [<a href="http://arxiv.org/pdf/2012.03466" target="_blank">pdf</a>]

<h2>NCGNN: Node-level Capsule Graph Neural Network. (arXiv:2012.03476v1 [cs.LG])</h2>
<h3>Rui Yang, Wenrui Dai, Chenglin Li, Junni Zou, Hongkai Xiong</h3>
<p>Message passing has evolved as an effective tool for designing Graph Neural
Networks (GNNs). However, most existing works naively sum or average all the
neighboring features to update node representations, which suffers from the
following limitations: (1) lack of interpretability to identify crucial node
features for GNN's prediction; (2) over-smoothing issue where repeated
averaging aggregates excessive noise, making features of nodes in different
classes over-mixed and thus indistinguishable. In this paper, we propose the
Node-level Capsule Graph Neural Network (NCGNN) to address these issues with an
improved message passing scheme. Specifically, NCGNN represents nodes as groups
of capsules, in which each capsule extracts distinctive features of its
corresponding node. For each node-level capsule, a novel dynamic routing
procedure is developed to adaptively select appropriate capsules for
aggregation from a subgraph identified by the designed graph filter.
Consequently, as only the advantageous capsules are aggregated and harmful
noise is restrained, over-mixing features of interacting nodes in different
classes tends to be avoided to relieve the over-smoothing issue. Furthermore,
since the graph filter and the dynamic routing identify a subgraph and a subset
of node features that are most influential for the prediction of the model,
NCGNN is inherently interpretable and exempt from complex post-hoc
explanations. Extensive experiments on six node classification benchmarks
demonstrate that NCGNN can well address the over-smoothing issue and
outperforms the state of the arts by producing better node embeddings for
classification.
</p>
<a href="http://arxiv.org/abs/2012.03476" target="_blank">arXiv:2012.03476</a> [<a href="http://arxiv.org/pdf/2012.03476" target="_blank">pdf</a>]

<h2>Meta Ordinal Regression Forest For Learning with Unsure Lung Nodules. (arXiv:2012.03480v1 [cs.CV])</h2>
<h3>Yiming Lei, Haiping Zhu, Junping Zhang, Hongming Shan</h3>
<p>Deep learning-based methods have achieved promising performance in early
detection and classification of lung nodules, most of which discard unsure
nodules and simply deal with a binary classification -- malignant vs benign.
Recently, an unsure data model (UDM) was proposed to incorporate those unsure
nodules by formulating this problem as an ordinal regression, showing better
performance over traditional binary classification. To further explore the
ordinal relationship for lung nodule classification, this paper proposes a meta
ordinal regression forest (MORF), which improves upon the state-of-the-art
ordinal regression method, deep ordinal regression forest (DORF), in three
major ways. First, MORF can alleviate the biases of the predictions by making
full use of deep features while DORF needs to fix the composition of decision
trees before training. Second, MORF has a novel grouped feature selection (GFS)
module to re-sample the split nodes of decision trees. Last, combined with GFS,
MORF is equipped with a meta learning-based weighting scheme to map the
features selected by GFS to tree-wise weights while DORF assigns equal weights
for all trees. Experimental results on the LIDC-IDRI dataset demonstrate
superior performance over existing methods, including the state-of-the-art
DORF.
</p>
<a href="http://arxiv.org/abs/2012.03480" target="_blank">arXiv:2012.03480</a> [<a href="http://arxiv.org/pdf/2012.03480" target="_blank">pdf</a>]

<h2>Rethinking Learnable Tree Filter for Generic Feature Transform. (arXiv:2012.03482v1 [cs.CV])</h2>
<h3>Lin Song, Yanwei Li, Zhengkai Jiang, Zeming Li, Xiangyu Zhang, Hongbin Sun, Jian Sun, Nanning Zheng</h3>
<p>The Learnable Tree Filter presents a remarkable approach to model
structure-preserving relations for semantic segmentation. Nevertheless, the
intrinsic geometric constraint forces it to focus on the regions with close
spatial distance, hindering the effective long-range interactions. To relax the
geometric constraint, we give the analysis by reformulating it as a Markov
Random Field and introduce a learnable unary term. Besides, we propose a
learnable spanning tree algorithm to replace the original non-differentiable
one, which further improves the flexibility and robustness. With the above
improvements, our method can better capture long-range dependencies and
preserve structural details with linear complexity, which is extended to
several vision tasks for more generic feature transform. Extensive experiments
on object detection/instance segmentation demonstrate the consistent
improvements over the original version. For semantic segmentation, we achieve
leading performance (82.1% mIoU) on the Cityscapes benchmark without
bells-and-whistles. Code is available at
https://github.com/StevenGrove/LearnableTreeFilterV2.
</p>
<a href="http://arxiv.org/abs/2012.03482" target="_blank">arXiv:2012.03482</a> [<a href="http://arxiv.org/pdf/2012.03482" target="_blank">pdf</a>]

<h2>Learning to Separate Clusters of Adversarial Representations for Robust Adversarial Detection. (arXiv:2012.03483v1 [cs.LG])</h2>
<h3>Byunggill Joe, Jihun Hamm, Sung Ju Hwang, Sooel Son, Insik Shin</h3>
<p>Although deep neural networks have shown promising performances on various
tasks, they are susceptible to incorrect predictions induced by imperceptibly
small perturbations in inputs. A large number of previous works proposed to
detect adversarial attacks. Yet, most of them cannot effectively detect them
against adaptive whitebox attacks where an adversary has the knowledge of the
model and the defense method. In this paper, we propose a new probabilistic
adversarial detector motivated by a recently introduced non-robust feature. We
consider the non-robust features as a common property of adversarial examples,
and we deduce it is possible to find a cluster in representation space
corresponding to the property. This idea leads us to probability estimate
distribution of adversarial representations in a separate cluster, and leverage
the distribution for a likelihood based adversarial detector.
</p>
<a href="http://arxiv.org/abs/2012.03483" target="_blank">arXiv:2012.03483</a> [<a href="http://arxiv.org/pdf/2012.03483" target="_blank">pdf</a>]

<h2>An Approach to Intelligent Pneumonia Detection and Integration. (arXiv:2012.03487v1 [cs.AI])</h2>
<h3>Bonaventure F. P. Dossou, Alena Iureva, Sayali R. Rajhans, Vamsi S. Pidikiti</h3>
<p>Each year, over 2.5 million people, most of them in developed countries, die
from pneumonia [1]. Since many studies have proved pneumonia is successfully
treatable when timely and correctly diagnosed, many of diagnosis aids have been
developed, with AI-based methods achieving high accuracies [2]. However,
currently, the usage of AI in pneumonia detection is limited, in particular,
due to challenges in generalizing a locally achieved result. In this report, we
propose a roadmap for creating and integrating a system that attempts to solve
this challenge. We also address various technical, legal, ethical, and
logistical issues, with a blueprint of possible solutions.
</p>
<a href="http://arxiv.org/abs/2012.03487" target="_blank">arXiv:2012.03487</a> [<a href="http://arxiv.org/pdf/2012.03487" target="_blank">pdf</a>]

<h2>Multi-agent Policy Optimization with Approximatively Synchronous Advantage Estimation. (arXiv:2012.03488v1 [cs.LG])</h2>
<h3>Lipeng Wan, Xuwei Song, Xuguang Lan, Nanning Zheng</h3>
<p>Cooperative multi-agent tasks require agents to deduce their own
contributions with shared global rewards, known as the challenge of credit
assignment. General methods for policy based multi-agent reinforcement learning
to solve the challenge introduce differentiate value functions or advantage
functions for individual agents. In multi-agent system, polices of different
agents need to be evaluated jointly. In order to update polices synchronously,
such value functions or advantage functions also need synchronous evaluation.
However, in current methods, value functions or advantage functions use
counter-factual joint actions which are evaluated asynchronously, thus suffer
from natural estimation bias. In this work, we propose the approximatively
synchronous advantage estimation. We first derive the marginal advantage
function, an expansion from single-agent advantage function to multi-agent
system. Further more, we introduce a policy approximation for synchronous
advantage estimation, and break down the multi-agent policy optimization
problem into multiple sub-problems of single-agent policy optimization. Our
method is compared with baseline algorithms on StarCraft multi-agent
challenges, and shows the best performance on most of the tasks.
</p>
<a href="http://arxiv.org/abs/2012.03488" target="_blank">arXiv:2012.03488</a> [<a href="http://arxiv.org/pdf/2012.03488" target="_blank">pdf</a>]

<h2>Generative Adversarial Network based Heuristics for Sampling-based Path Planning. (arXiv:2012.03490v1 [cs.RO])</h2>
<h3>Tianyi Zhang, Jiankun Wang, Max Q.-H. Meng</h3>
<p>Sampling-based path planning is a popular methodology for robot path
planning. With a uniform sampling strategy to explore the state space, a
feasible path can be found without the complex geometric modeling of the
configuration space. However, the quality of initial solution is not guaranteed
and the convergence speed to the optimal solution is slow. In this paper, we
present a novel image-based path planning algorithm to overcome these
limitations. Specifically, a generative adversarial network (GAN) is designed
to take the environment map (denoted as RGB image) as the input without other
preprocessing works. The output is also an RGB image where the promising region
(where a feasible path probably exists) is segmented. This promising region is
utilized as a heuristic to achieve nonuniform sampling for the path planner. We
conduct a number of simulation experiments to validate the effectiveness of the
proposed method, and the results demonstrate that our method performs much
better in terms of the quality of initial solution and the convergence speed to
the optimal solution. Furthermore, apart from the environments similar to the
training set, our method also works well on the environments which are very
different from the training set.
</p>
<a href="http://arxiv.org/abs/2012.03490" target="_blank">arXiv:2012.03490</a> [<a href="http://arxiv.org/pdf/2012.03490" target="_blank">pdf</a>]

<h2>Adaptive Local Bayesian Optimization Over Multiple Discrete Variables. (arXiv:2012.03501v1 [cs.LG])</h2>
<h3>Taehyeon Kim, Jaeyeon Ahn, Nakyil Kim, Seyoung Yun</h3>
<p>In the machine learning algorithms, the choice of the hyperparameter is often
an art more than a science, requiring labor-intensive search with expert
experience. Therefore, automation on hyperparameter optimization to exclude
human intervention is a great appeal, especially for the black-box functions.
Recently, there have been increasing demands of solving such concealed tasks
for better generalization, though the task-dependent issue is not easy to
solve. The Black-Box Optimization challenge (NeurIPS 2020) required competitors
to build a robust black-box optimizer across different domains of standard
machine learning problems. This paper describes the approach of team KAIST OSI
in a step-wise manner, which outperforms the baseline algorithms by up to
+20.39%. We first strengthen the local Bayesian search under the concept of
region reliability. Then, we design a combinatorial kernel for a Gaussian
process kernel. In a similar vein, we combine the methodology of Bayesian and
multi-armed bandit,(MAB) approach to select the values with the consideration
of the variable types; the real and integer variables are with Bayesian, while
the boolean and categorical variables are with MAB. Empirical evaluations
demonstrate that our method outperforms the existing methods across different
tasks.
</p>
<a href="http://arxiv.org/abs/2012.03501" target="_blank">arXiv:2012.03501</a> [<a href="http://arxiv.org/pdf/2012.03501" target="_blank">pdf</a>]

<h2>Semi-supervised Soil Moisture Prediction through Graph Neural Networks. (arXiv:2012.03506v1 [cs.LG])</h2>
<h3>Anoushka Vyas, Sambaran Bandyopadhyay</h3>
<p>Recent improvement and availability of remote satellite and IoT data offers
interesting and diverse applications of artificial intelligence in precision
agriculture. Soil moisture is an important component of multiple agricultural
and food supply chain practices. It measures the amount of water stored in
various depth of soil. Existing data driven approaches for soil moisture
prediction use conventional models which fail to capture the dynamic dependency
of soil moisture values in near-by locations over time. In this work, we
propose to convert the problem of soil moisture prediction as a semi-supervised
learning on temporal graphs. We propose a dynamic graph neural network which
can use the dependency of related locations over a region to predict soil
moisture. However, unlike social or information networks, graph structure is
not explicitly given for soil moisture prediction. Hence, we incorporate the
problem of graph structure learning in the framework of dynamic GNN. Our
algorithm, referred as DGLR, provides an end-to-end learning which can predict
soil moisture over multiple locations in a region over time and also update the
graph structure in between. Our solution achieves state-of-the-art results on
real-world soil moisture datasets compared to existing machine learning
approaches.
</p>
<a href="http://arxiv.org/abs/2012.03506" target="_blank">arXiv:2012.03506</a> [<a href="http://arxiv.org/pdf/2012.03506" target="_blank">pdf</a>]

<h2>A review of possible effects of cognitive biases on the interpretation of rule-based machine learning models. (arXiv:1804.02969v6 [stat.ML] UPDATED)</h2>
<h3>Tom&#xe1;&#x161; Kliegr, &#x160;t&#x11b;p&#xe1;n Bahn&#xed;k, Johannes F&#xfc;rnkranz</h3>
<p>While the interpretability of machine learning models is often equated with
their mere syntactic comprehensibility, we think that interpretability goes
beyond that, and that human interpretability should also be investigated from
the point of view of cognitive science. In particular, the goal of this paper
is to discuss to what extent cognitive biases may affect human understanding of
interpretable machine learning models, in particular of logical rules
discovered from data. Twenty cognitive biases are covered, as are possible
debiasing techniques that can be adopted by designers of machine learning
algorithms and software. Our review transfers results obtained in cognitive
psychology to the domain of machine learning, aiming to bridge the current gap
between these two areas. It needs to be followed by empirical studies
specifically focused on the machine learning domain.
</p>
<a href="http://arxiv.org/abs/1804.02969" target="_blank">arXiv:1804.02969</a> [<a href="http://arxiv.org/pdf/1804.02969" target="_blank">pdf</a>]

<h2>Applications of Common Entropy for Causal Inference. (arXiv:1807.10399v2 [stat.ML] UPDATED)</h2>
<h3>Murat Kocaoglu, Sanjay Shakkottai, Alexandros G. Dimakis, Constantine Caramanis, Sriram Vishwanath</h3>
<p>We study the problem of discovering the simplest latent variable that can
make two observed discrete variables conditionally independent. The minimum
entropy required for such a latent is known as common entropy in information
theory. We extend this notion to Renyi common entropy by minimizing the Renyi
entropy of the latent variable. To efficiently compute common entropy, we
propose an iterative algorithm that can be used to discover the trade-off
between the entropy of the latent variable and the conditional mutual
information of the observed variables. We show two applications of common
entropy in causal inference: First, under the assumption that there are no
low-entropy mediators, it can be used to distinguish causation from spurious
correlation among almost all joint distributions on simple causal graphs with
two observed variables. Second, common entropy can be used to improve
constraint-based methods such as PC or FCI algorithms in the small-sample
regime, where these methods are known to struggle. We propose a modification to
these constraint-based methods to assess if a separating set found by these
algorithms is valid using common entropy. We finally evaluate our algorithms on
synthetic and real data to establish their performance.
</p>
<a href="http://arxiv.org/abs/1807.10399" target="_blank">arXiv:1807.10399</a> [<a href="http://arxiv.org/pdf/1807.10399" target="_blank">pdf</a>]

<h2>MIST: Multiple Instance Spatial Transformer Network. (arXiv:1811.10725v5 [cs.CV] UPDATED)</h2>
<h3>Baptiste Angles, Yuhe Jin, Simon Kornblith, Andrea Tagliasacchi, Kwang Moo Yi</h3>
<p>We propose a deep network that can be trained to tackle image reconstruction
and classification problems that involve detection of multiple object
instances, without any supervision regarding their whereabouts. The network
learns to extract the most significant top-K patches, and feeds these patches
to a task-specific network -- e.g., auto-encoder or classifier -- to solve a
domain specific problem. The challenge in training such a network is the
non-differentiable top-K selection process. To address this issue, we lift the
training optimization problem by treating the result of top-K selection as a
slack variable, resulting in a simple, yet effective, multi-stage training. Our
method is able to learn to detect recurrent structures in the training dataset
by learning to reconstruct images. It can also learn to localize structures
when only knowledge on the occurrence of the object is provided, and in doing
so it outperforms the state-of-the-art.
</p>
<a href="http://arxiv.org/abs/1811.10725" target="_blank">arXiv:1811.10725</a> [<a href="http://arxiv.org/pdf/1811.10725" target="_blank">pdf</a>]

<h2>Learning to Design Circuits. (arXiv:1812.02734v5 [cs.LG] UPDATED)</h2>
<h3>Hanrui Wang, Jiacheng Yang, Hae-Seung Lee, Song Han</h3>
<p>Analog IC design relies on human experts to search for parameters that
satisfy circuit specifications with their experience and intuitions, which is
highly labor intensive, time consuming and suboptimal. Machine learning is a
promising tool to automate this process. However, supervised learning is
difficult for this task due to the low availability of training data: 1)
Circuit simulation is slow, thus generating large-scale dataset is
time-consuming; 2) Most circuit designs are propitiatory IPs within individual
IC companies, making it expensive to collect large-scale datasets. We propose
Learning to Design Circuits (L2DC) to leverage reinforcement learning that
learns to efficiently generate new circuits data and to optimize circuits. We
fix the schematic, and optimize the parameters of the transistors automatically
by training an RL agent with no prior knowledge about optimizing circuits.
After iteratively getting observations, generating a new set of transistor
parameters, getting a reward, and adjusting the model, L2DC is able to optimize
circuits. We evaluate L2DC on two transimpedance amplifiers. Trained for a day,
our RL agent can achieve comparable or better performance than human experts
trained for a quarter. It first learns to meet hard-constraints (eg. gain,
bandwidth), and then learns to optimize good-to-have targets (eg. area, power).
Compared with grid search-aided human design, L2DC can achieve
$\mathbf{250}\boldsymbol{\times}$ higher sample efficiency with comparable
performance. Under the same runtime constraint, the performance of L2DC is also
better than Bayesian Optimization.
</p>
<a href="http://arxiv.org/abs/1812.02734" target="_blank">arXiv:1812.02734</a> [<a href="http://arxiv.org/pdf/1812.02734" target="_blank">pdf</a>]

<h2>Learning Multiplication-free Linear Transformations. (arXiv:1812.03412v2 [cs.LG] UPDATED)</h2>
<h3>Cristian Rusu</h3>
<p>In this paper, we propose several dictionary learning algorithms for sparse
representations that also impose specific structures on the learned
dictionaries such that they are numerically efficient to use: reduced number of
addition/multiplications and even avoiding multiplications altogether. We base
our work on factorizations of the dictionary in highly structured basic
building blocks (binary orthonormal, scaling and shear transformations) for
which we can write closed-form solutions to the optimization problems that we
consider. We show the effectiveness of our methods on image data where we can
compare against well-known numerically efficient transforms such as the fast
Fourier and the fast discrete cosine transforms.
</p>
<a href="http://arxiv.org/abs/1812.03412" target="_blank">arXiv:1812.03412</a> [<a href="http://arxiv.org/pdf/1812.03412" target="_blank">pdf</a>]

<h2>Synthetic Data Generators: Sequential and Private. (arXiv:1902.03468v4 [cs.LG] UPDATED)</h2>
<h3>Olivier Bousquet, Roi Livni, Shay Moran</h3>
<p>We study the sample complexity of private synthetic data generation over an
unbounded sized class of statistical queries, and show that any class that is
privately proper PAC learnable admits a private synthetic data generator
(perhaps non-efficient). Previous work on synthetic data generators focused on
the case that the query class $\mathcal{D}$ is finite and obtained sample
complexity bounds that scale logarithmically with the size $|\mathcal{D}|$.
Here we construct a private synthetic data generator whose sample complexity is
independent of the domain size, and we replace finiteness with the assumption
that $\mathcal{D}$ is privately PAC learnable (a formally weaker task, hence we
obtain equivalence between the two tasks).
</p>
<a href="http://arxiv.org/abs/1902.03468" target="_blank">arXiv:1902.03468</a> [<a href="http://arxiv.org/pdf/1902.03468" target="_blank">pdf</a>]

<h2>Robot Learning via Human Adversarial Games. (arXiv:1903.00636v2 [cs.RO] UPDATED)</h2>
<h3>Jiali Duan, Qian Wang, Lerrel Pinto, C.-C. Jay Kuo, Stefanos Nikolaidis</h3>
<p>Much work in robotics has focused on "human-in-the-loop" learning techniques
that improve the efficiency of the learning process. However, these algorithms
have made the strong assumption of a cooperating human supervisor that assists
the robot. In reality, human observers tend to also act in an adversarial
manner towards deployed robotic systems. We show that this can in fact improve
the robustness of the learned models by proposing a physical framework that
leverages perturbations applied by a human adversary, guiding the robot towards
more robust models. In a manipulation task, we show that grasping success
improves significantly when the robot trains with a human adversary as compared
to training in a self-supervised manner.
</p>
<a href="http://arxiv.org/abs/1903.00636" target="_blank">arXiv:1903.00636</a> [<a href="http://arxiv.org/pdf/1903.00636" target="_blank">pdf</a>]

<h2>MoGlow: Probabilistic and controllable motion synthesis using normalising flows. (arXiv:1905.06598v3 [cs.LG] UPDATED)</h2>
<h3>Gustav Eje Henter, Simon Alexanderson, Jonas Beskow</h3>
<p>Data-driven modelling and synthesis of motion is an active research area with
applications that include animation, games, and social robotics. This paper
introduces a new class of probabilistic, generative, and controllable
motion-data models based on normalising flows. Models of this kind can describe
highly complex distributions, yet can be trained efficiently using exact
maximum likelihood, unlike GANs or VAEs. Our proposed model is autoregressive
and uses LSTMs to enable arbitrarily long time-dependencies. Importantly, is is
also causal, meaning that each pose in the output sequence is generated without
access to poses or control inputs from future time steps; this absence of
algorithmic latency is important for interactive applications with real-time
motion control. The approach can in principle be applied to any type of motion
since it does not make restrictive, task-specific assumptions regarding the
motion or the character morphology. We evaluate the models on motion-capture
datasets of human and quadruped locomotion. Objective and subjective results
show that randomly-sampled motion from the proposed method outperforms
task-agnostic baselines and attains a motion quality close to recorded motion
capture.
</p>
<a href="http://arxiv.org/abs/1905.06598" target="_blank">arXiv:1905.06598</a> [<a href="http://arxiv.org/pdf/1905.06598" target="_blank">pdf</a>]

<h2>Quantifying the alignment of graph and features in deep learning. (arXiv:1905.12921v2 [cs.LG] UPDATED)</h2>
<h3>Yifan Qian, Paul Expert, Tom Rieu, Pietro Panzarasa, Mauricio Barahona</h3>
<p>We show that the classification performance of Graph Convolutional Networks
is related to the alignment between features, graph and ground truth, which we
quantify using a subspace alignment measure corresponding to the Frobenius norm
of the matrix of pairwise chordal distances between three subspaces associated
with features, graph and ground truth. The proposed measure is based on the
principal angles between subspaces and has both spectral and geometrical
interpretations. We showcase the relationship between the subspace alignment
measure and the classification performance through the study of limiting cases
of Graph Convolutional Networks as well as systematic randomizations of both
features and graph structure applied to a constructive example and several
examples of citation networks of different origin. The analysis also reveals
the relative importance of the graph and features for classification purposes.
</p>
<a href="http://arxiv.org/abs/1905.12921" target="_blank">arXiv:1905.12921</a> [<a href="http://arxiv.org/pdf/1905.12921" target="_blank">pdf</a>]

<h2>Beyond Product Quantization: Deep Progressive Quantization for Image Retrieval. (arXiv:1906.06698v3 [cs.CV] UPDATED)</h2>
<h3>Lianli Gao, Xiaosu Zhu, Jingkuan Song, Zhou Zhao, Heng Tao Shen</h3>
<p>Product Quantization (PQ) has long been a mainstream for generating an
exponentially large codebook at very low memory/time cost. Despite its success,
PQ is still tricky for the decomposition of high-dimensional vector space, and
the retraining of model is usually unavoidable when the code length changes. In
this work, we propose a deep progressive quantization (DPQ) model, as an
alternative to PQ, for large scale image retrieval. DPQ learns the quantization
codes sequentially and approximates the original feature space progressively.
Therefore, we can train the quantization codes with different code lengths
simultaneously. Specifically, we first utilize the label information for
guiding the learning of visual features, and then apply several quantization
blocks to progressively approach the visual features. Each quantization block
is designed to be a layer of a convolutional neural network, and the whole
framework can be trained in an end-to-end manner. Experimental results on the
benchmark datasets show that our model significantly outperforms the
state-of-the-art for image retrieval. Our model is trained once for different
code lengths and therefore requires less computation time. Additional ablation
study demonstrates the effect of each component of our proposed model. Our code
is released at https://github.com/cfm-uestc/DPQ.
</p>
<a href="http://arxiv.org/abs/1906.06698" target="_blank">arXiv:1906.06698</a> [<a href="http://arxiv.org/pdf/1906.06698" target="_blank">pdf</a>]

<h2>Deep Recurrent Quantization for Generating Sequential Binary Codes. (arXiv:1906.06699v3 [cs.CV] UPDATED)</h2>
<h3>Jingkuan Song, Xiaosu Zhu, Lianli Gao, Xin-Shun Xu, Wu Liu, Heng Tao Shen</h3>
<p>Quantization has been an effective technology in ANN (approximate nearest
neighbour) search due to its high accuracy and fast search speed. To meet the
requirement of different applications, there is always a trade-off between
retrieval accuracy and speed, reflected by variable code lengths. However, to
encode the dataset into different code lengths, existing methods need to train
several models, where each model can only produce a specific code length. This
incurs a considerable training time cost, and largely reduces the flexibility
of quantization methods to be deployed in real applications. To address this
issue, we propose a Deep Recurrent Quantization (DRQ) architecture which can
generate sequential binary codes. To the end, when the model is trained, a
sequence of binary codes can be generated and the code length can be easily
controlled by adjusting the number of recurrent iterations. A shared codebook
and a scalar factor is designed to be the learnable weights in the deep
recurrent quantization block, and the whole framework can be trained in an
end-to-end manner. As far as we know, this is the first quantization method
that can be trained once and generate sequential binary codes. Experimental
results on the benchmark datasets show that our model achieves comparable or
even better performance compared with the state-of-the-art for image retrieval.
But it requires significantly less number of parameters and training times. Our
code is published online: https://github.com/cfm-uestc/DRQ.
</p>
<a href="http://arxiv.org/abs/1906.06699" target="_blank">arXiv:1906.06699</a> [<a href="http://arxiv.org/pdf/1906.06699" target="_blank">pdf</a>]

<h2>Multi-Armed Bandits with Fairness Constraints for Distributing Resources to Human Teammates. (arXiv:1907.00313v3 [cs.AI] UPDATED)</h2>
<h3>Houston Claure, Yifang Chen, Jignesh Modi, Malte Jung, Stefanos Nikolaidis</h3>
<p>How should a robot that collaborates with multiple people decide upon the
distribution of resources (e.g. social attention, or parts needed for an
assembly)? People are uniquely attuned to how resources are distributed. A
decision to distribute more resources to one team member than another might be
perceived as unfair with potentially detrimental effects for trust. We
introduce a multi-armed bandit algorithm with fairness constraints, where a
robot distributes resources to human teammates of different skill levels. In
this problem, the robot does not know the skill level of each human teammate,
but learns it by observing their performance over time. We define fairness as a
constraint on the minimum rate that each human teammate is selected throughout
the task. We provide theoretical guarantees on performance and perform a
large-scale user study, where we adjust the level of fairness in our algorithm.
Results show that fairness in resource distribution has a significant effect on
users' trust in the system.
</p>
<a href="http://arxiv.org/abs/1907.00313" target="_blank">arXiv:1907.00313</a> [<a href="http://arxiv.org/pdf/1907.00313" target="_blank">pdf</a>]

<h2>Quantum-inspired canonical correlation analysis for exponentially large dimensional data. (arXiv:1907.03236v2 [cs.LG] UPDATED)</h2>
<h3>Naoko Koide-Majima, Kei Majima</h3>
<p>Canonical correlation analysis (CCA) is a technique to find statistical
dependencies between a pair of multivariate data. However, its application to
high dimensional data is limited due to the resulting time complexity. While
the conventional CCA algorithm requires polynomial time, we have developed an
algorithm that approximates CCA with computational time proportional to the
logarithm of the input dimensionality using quantum-inspired computation. The
computational efficiency and approximation performance of the proposed
quantum-inspired CCA (qiCCA) algorithm are experimentally demonstrated.
Furthermore, the fast computation of qiCCA allows us to directly apply CCA even
after nonlinearly mapping raw input data into very high dimensional spaces.
Experiments performed using a benchmark dataset demonstrated that, by mapping
the raw input data into the high dimensional spaces with second-order
monomials, the proposed qiCCA extracted more correlations than linear CCA and
was comparable to deep CCA and kernel CCA. These results suggest that qiCCA is
considerably useful and quantum-inspired computation has the potential to
unlock a new field in which exponentially large dimensional data can be
analyzed.
</p>
<a href="http://arxiv.org/abs/1907.03236" target="_blank">arXiv:1907.03236</a> [<a href="http://arxiv.org/pdf/1907.03236" target="_blank">pdf</a>]

<h2>Accelerating Large-Scale Inference with Anisotropic Vector Quantization. (arXiv:1908.10396v5 [cs.LG] UPDATED)</h2>
<h3>Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha, Felix Chern, Sanjiv Kumar</h3>
<p>Quantization based techniques are the current state-of-the-art for scaling
maximum inner product search to massive databases. Traditional approaches to
quantization aim to minimize the reconstruction error of the database points.
Based on the observation that for a given query, the database points that have
the largest inner products are more relevant, we develop a family of
anisotropic quantization loss functions. Under natural statistical assumptions,
we show that quantization with these loss functions leads to a new variant of
vector quantization that more greatly penalizes the parallel component of a
datapoint's residual relative to its orthogonal component. The proposed
approach achieves state-of-the-art results on the public benchmarks available
at \url{ann-benchmarks.com}.
</p>
<a href="http://arxiv.org/abs/1908.10396" target="_blank">arXiv:1908.10396</a> [<a href="http://arxiv.org/pdf/1908.10396" target="_blank">pdf</a>]

<h2>InceptionTime: Finding AlexNet for Time Series Classification. (arXiv:1909.04939v3 [cs.LG] UPDATED)</h2>
<h3>Hassan Ismail Fawaz, Benjamin Lucas, Germain Forestier, Charlotte Pelletier, Daniel F. Schmidt, Jonathan Weber, Geoffrey I. Webb, Lhassane Idoumghar, Pierre-Alain Muller, Fran&#xe7;ois Petitjean</h3>
<p>This paper brings deep learning at the forefront of research into Time Series
Classification (TSC). TSC is the area of machine learning tasked with the
categorization (or labelling) of time series. The last few decades of work in
this area have led to significant progress in the accuracy of classifiers, with
the state of the art now represented by the HIVE-COTE algorithm. While
extremely accurate, HIVE-COTE cannot be applied to many real-world datasets
because of its high training time complexity in O(N2 * T4) for a dataset with N
time series of length T. For example, it takes HIVE-COTE more than 8 days to
learn from a small dataset with N = 1500 time series of short length T = 46.
Meanwhile deep learning has received enormous attention because of its high
accuracy and scalability. Recent approaches to deep learning for TSC have been
scalable, but less accurate than HIVE-COTE. We introduce InceptionTime - an
ensemble of deep Convolutional Neural Network (CNN) models, inspired by the
Inception-v4 architecture. Our experiments show that InceptionTime is on par
with HIVE-COTE in terms of accuracy while being much more scalable: not only
can it learn from 1,500 time series in one hour but it can also learn from 8M
time series in 13 hours, a quantity of data that is fully out of reach of
HIVE-COTE.
</p>
<a href="http://arxiv.org/abs/1909.04939" target="_blank">arXiv:1909.04939</a> [<a href="http://arxiv.org/pdf/1909.04939" target="_blank">pdf</a>]

<h2>Pose Neural Fabrics Search. (arXiv:1909.07068v4 [cs.CV] UPDATED)</h2>
<h3>Sen Yang, Wankou Yang, Zhen Cui</h3>
<p>Neural Architecture Search (NAS) technologies have emerged in many domains to
jointly learn the architectures and weights of the neural network. However,
most existing NAS works claim they are task-specific and focus only on
optimizing a single architecture to replace a human-designed neural network, in
fact, their search processes are almost independent of domain knowledge of the
tasks. In this paper, we propose Pose Neural Fabrics Search (PoseNFS). We
explore a new solution for NAS and human pose estimation task: part-specific
neural architecture search, which can be seen as a variant of multi-task
learning. Firstly, we design a new neural architecture search space, Cell-based
Neural Fabric (CNF), to learn micro as well as macro neural architecture using
a differentiable search strategy. Then, we view locating human keypoints as
multiple disentangled prediction sub-tasks, and then use prior knowledge of
body structure as guidance to search for multiple part-specific neural
architectures for different human parts. After search, all these part-specific
CNFs have distinct micro and macro architecture parameters. The results show
that such knowledge-guided NAS-based architectures have obvious performance
improvements to a hand-designed part-based baseline model. The experiments on
MPII and MS-COCO datasets demonstrate that PoseNFS\footnote{Code is available
at \url{https://github.com/yangsenius/PoseNFS}} can achieve comparable
performance to some efficient and state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/1909.07068" target="_blank">arXiv:1909.07068</a> [<a href="http://arxiv.org/pdf/1909.07068" target="_blank">pdf</a>]

<h2>Striving for Simplicity and Performance in Off-Policy DRL: Output Normalization and Non-Uniform Sampling. (arXiv:1910.02208v4 [cs.LG] UPDATED)</h2>
<h3>Che Wang, Yanqiu Wu, Quan Vuong, Keith Ross</h3>
<p>We aim to develop off-policy DRL algorithms that not only exceed
state-of-the-art performance but are also simple and minimalistic. For standard
continuous control benchmarks, Soft Actor-Critic (SAC), which employs entropy
maximization, currently provides state-of-the-art performance. We first
demonstrate that the entropy term in SAC addresses action saturation due to the
bounded nature of the action spaces, with this insight, we propose a
streamlined algorithm with a simple normalization scheme or with inverted
gradients. We show that both approaches can match SAC's sample efficiency
performance without the need of entropy maximization, we then propose a simple
non-uniform sampling method for selecting transitions from the replay buffer
during training. Extensive experimental results demonstrate that our proposed
sampling scheme leads to state of the art sample efficiency on challenging
continuous control tasks. We combine all of our findings into one simple
algorithm, which we call Streamlined Off Policy with Emphasizing Recent
Experience, for which we provide robust public-domain code.
</p>
<a href="http://arxiv.org/abs/1910.02208" target="_blank">arXiv:1910.02208</a> [<a href="http://arxiv.org/pdf/1910.02208" target="_blank">pdf</a>]

<h2>Deep Weakly-supervised Anomaly Detection. (arXiv:1910.13601v3 [cs.LG] UPDATED)</h2>
<h3>Guansong Pang, Chunhua Shen, Huidong Jin, Anton van den Hengel</h3>
<p>Anomaly detection is typically posited as an unsupervised learning task in
the literature due to the prohibitive cost and difficulty to obtain large-scale
labeled anomaly data, but this ignores the fact that a very small number
(e.g.,, a few dozens) of labeled anomalies can often be made available with
small/trivial cost in many real-world anomaly detection applications. To
leverage such labeled anomaly data, we study an important anomaly detection
problem termed weakly-supervised anomaly detection, in which, in addition to a
large amount of unlabeled data, a limited number of labeled anomalies are
available during modeling. Learning with the small labeled anomaly data enables
anomaly-informed modeling, which helps identify anomalies of interest and
address the notorious high false positives in unsupervised anomaly detection.
However, the problem is especially challenging, since (i) the limited amount of
labeled anomaly data often, if not always, cannot cover all types of anomalies
and (ii) the unlabeled data is often dominated by normal instances but has
anomaly contamination. We address the problem by formulating it as a pairwise
relation prediction task. Particularly, our approach defines a two-stream
ordinal regression neural network to learn the relation of randomly sampled
instance pairs, i.e., whether the instance pair contains two labeled anomalies,
one labeled anomaly, or just unlabeled data instances. The resulting model
effectively leverages both the labeled and unlabeled data to substantially
augment the training data and learn well-generalized representations of
normality and abnormality. Comprehensive empirical results on 40 real-world
datasets show that our approach (i) significantly outperforms four
state-of-the-art methods in detecting both of the known and previously unseen
anomalies and (ii) is substantially more data-efficient.
</p>
<a href="http://arxiv.org/abs/1910.13601" target="_blank">arXiv:1910.13601</a> [<a href="http://arxiv.org/pdf/1910.13601" target="_blank">pdf</a>]

<h2>Local AdaAlter: Communication-Efficient Stochastic Gradient Descent with Adaptive Learning Rates. (arXiv:1911.09030v2 [cs.LG] UPDATED)</h2>
<h3>Cong Xie, Oluwasanmi Koyejo, Indranil Gupta, Haibin Lin</h3>
<p>When scaling distributed training, the communication overhead is often the
bottleneck. In this paper, we propose a novel SGD variant with reduced
communication and adaptive learning rates. We prove the convergence of the
proposed algorithm for smooth but non-convex problems. Empirical results show
that the proposed algorithm significantly reduces the communication overhead,
which, in turn, reduces the training time by up to 30% for the 1B word dataset.
</p>
<a href="http://arxiv.org/abs/1911.09030" target="_blank">arXiv:1911.09030</a> [<a href="http://arxiv.org/pdf/1911.09030" target="_blank">pdf</a>]

<h2>Scaling Out-of-Distribution Detection for Real-World Settings. (arXiv:1911.11132v2 [cs.CV] UPDATED)</h2>
<h3>Dan Hendrycks, Steven Basart, Mantas Mazeika, Mohammadreza Mostajabi, Jacob Steinhardt, Dawn Song</h3>
<p>Detecting out-of-distribution examples is important for safety-critical
machine learning applications such as medical screening and self-driving cars.
However, existing research mainly focuses on simple small-scale settings. To
set the stage for more realistic out-of-distribution detection, we depart from
small-scale settings and explore large-scale multiclass and multi-label
settings with high-resolution images and hundreds of classes. To make future
work in real-world settings possible, we also create a new benchmark for
anomaly segmentation by introducing the Combined Anomalous Object Segmentation
benchmark. Our novel benchmark combines two datasets for anomaly segmentation
that incorporate both realism and anomaly diversity. Using both real images and
those from a simulated driving environment, we ensure the background context
and a wide variety of anomalous objects are naturally integrated, unlike
before. We conduct extensive experiments in these more realistic settings for
out-of-distribution detection and find that a surprisingly simple detector
based on the maximum logit outperforms prior methods in all the large-scale
multi-class, multi-label, and segmentation tasks we consider, establishing a
new baseline for future work. These results, along with our new anomaly
segmentation benchmark, open the door to future research in out-of-distribution
detection.
</p>
<a href="http://arxiv.org/abs/1911.11132" target="_blank">arXiv:1911.11132</a> [<a href="http://arxiv.org/pdf/1911.11132" target="_blank">pdf</a>]

<h2>A case study of Consistent Vehicle Routing Problem with Time Windows. (arXiv:1912.05929v3 [cs.AI] UPDATED)</h2>
<h3>Hern&#xe1;n Lespay, Karol Suchan</h3>
<p>We develop a heuristic for the Consistent Vehicle Routing Problem with Time
Windows (ConVRPTW), which is motivated by a real-world application at a food
company's distribution center. Besides standard VRPTW restrictions, ConVRPTW
assigns each customer just one driver to fulfill their orders during the whole
multi-period planning horizon. For each driver and period, a route is sought to
serve all their customers with positive demand. For each customer, the number
of periods between consecutive orders and the ordered quantities are highly
irregular. This causes difficulties in the daily routing, negatively impacting
the service level of the company. Similar problems have been studied as ConVRP,
where the number of drivers is fixed a priori, and only the total travel time
is minimized. Moreover, the clients present no time window constraints, but the
visits should be scheduled with a small arrival time variation. In our model,
the objective is to minimize the number of drivers. We impose hard time windows
but do not consider time consistency in more detail. We compare solutions given
by the heuristic with solutions of a MILP model on a set of small artificial
instances and solutions used by the food company on real-world instances. The
results show the effectiveness of the heuristic. For the company, we obtain
significant improvements in the routing plans, with a lower number of vehicles
and a higher rate of orders delivered within the prescribed time window.
</p>
<a href="http://arxiv.org/abs/1912.05929" target="_blank">arXiv:1912.05929</a> [<a href="http://arxiv.org/pdf/1912.05929" target="_blank">pdf</a>]

<h2>Contextually Plausible and Diverse 3D Human Motion Prediction. (arXiv:1912.08521v4 [cs.LG] UPDATED)</h2>
<h3>Sadegh Aliakbarian, Fatemeh Sadat Saleh, Lars Petersson, Stephen Gould, Mathieu Salzmann</h3>
<p>We tackle the task of diverse 3D human motion prediction, that is,
forecasting multiple plausible future 3D poses given a sequence of observed 3D
poses. In this context, a popular approach consists of using a Conditional
Variational Autoencoder (CVAE). However, existing approaches that do so either
fail to capture the diversity in human motion, or generate diverse but
semantically implausible continuations of the observed motion. In this paper,
we address both of these problems by developing a new variational framework
that accounts for both diversity and context of the generated future motion. To
this end, and in contrast to existing approaches, we condition the sampling of
the latent variable that acts as source of diversity on the representation of
the past observation, thus encouraging it to carry relevant information. Our
experiments demonstrate that our approach yields motions not only of higher
quality while retaining diversity, but also that preserve the contextual
information contained in the observed 3D pose sequence.
</p>
<a href="http://arxiv.org/abs/1912.08521" target="_blank">arXiv:1912.08521</a> [<a href="http://arxiv.org/pdf/1912.08521" target="_blank">pdf</a>]

<h2>Unsupervised Learning of Graph Hierarchical Abstractions with Differentiable Coarsening and Optimal Transport. (arXiv:1912.11176v2 [cs.LG] UPDATED)</h2>
<h3>Tengfei Ma, Jie Chen</h3>
<p>Hierarchical abstractions are a methodology for solving large-scale graph
problems in various disciplines. Coarsening is one such approach: it generates
a pyramid of graphs whereby the one in the next level is a structural summary
of the prior one. With a long history in scientific computing, many coarsening
strategies were developed based on mathematically driven heuristics. Recently,
resurgent interests exist in deep learning to design hierarchical methods
learnable through differentiable parameterization. These approaches are paired
with downstream tasks for supervised learning. In practice, however, supervised
signals (e.g., labels) are scarce and are often laborious to obtain. In this
work, we propose an unsupervised approach, coined OTCoarsening, with the use of
optimal transport. Both the coarsening matrix and the transport cost matrix are
parameterized, so that an optimal coarsening strategy can be learned and
tailored for a given set of graphs. We demonstrate that the proposed approach
produces meaningful coarse graphs and yields competitive performance compared
with supervised methods for graph classification and regression.
</p>
<a href="http://arxiv.org/abs/1912.11176" target="_blank">arXiv:1912.11176</a> [<a href="http://arxiv.org/pdf/1912.11176" target="_blank">pdf</a>]

<h2>Learning 3D Human Shape and Pose from Dense Body Parts. (arXiv:1912.13344v2 [cs.CV] UPDATED)</h2>
<h3>Hongwen Zhang, Jie Cao, Guo Lu, Wanli Ouyang, Zhenan Sun</h3>
<p>Reconstructing 3D human shape and pose from monocular images is challenging
despite the promising results achieved by the most recent learning-based
methods. The commonly occurred misalignment comes from the facts that the
mapping from images to the model space is highly non-linear and the
rotation-based pose representation of body models is prone to result in the
drift of joint positions. In this work, we investigate learning 3D human shape
and pose from dense correspondences of body parts and propose a
Decompose-and-aggregate Network (DaNet) to address these issues. DaNet adopts
the dense correspondence maps, which densely build a bridge between 2D pixels
and 3D vertices, as intermediate representations to facilitate the learning of
2D-to-3D mapping. The prediction modules of DaNet are decomposed into one
global stream and multiple local streams to enable global and fine-grained
perceptions for the shape and pose predictions, respectively. Messages from
local streams are further aggregated to enhance the robust prediction of the
rotation-based poses, where a position-aided rotation feature refinement
strategy is proposed to exploit spatial relationships between body joints.
Moreover, a Part-based Dropout (PartDrop) strategy is introduced to drop out
dense information from intermediate representations during training,
encouraging the network to focus on more complementary body parts as well as
neighboring position features. The efficacy of the proposed method is validated
on both indoor and real-world datasets including Human3.6M, UP3D, COCO, and
3DPW, showing that our method could significantly improve the reconstruction
performance in comparison with previous state-of-the-art methods. Our code is
publicly available at https://hongwenzhang.github.io/dense2mesh .
</p>
<a href="http://arxiv.org/abs/1912.13344" target="_blank">arXiv:1912.13344</a> [<a href="http://arxiv.org/pdf/1912.13344" target="_blank">pdf</a>]

<h2>Multi-site fMRI Analysis Using Privacy-preserving Federated Learning and Domain Adaptation: ABIDE Results. (arXiv:2001.05647v3 [cs.LG] UPDATED)</h2>
<h3>Xiaoxiao Li, Yufeng Gu, Nicha Dvornek, Lawrence Staib, Pamela Ventola, James S. Duncan</h3>
<p>Deep learning models have shown their advantage in many different tasks,
including neuroimage analysis. However, to effectively train a high-quality
deep learning model, the aggregation of a significant amount of patient
information is required. The time and cost for acquisition and annotation in
assembling, for example, large fMRI datasets make it difficult to acquire large
numbers at a single site. However, due to the need to protect the privacy of
patient data, it is hard to assemble a central database from multiple
institutions. Federated learning allows for population-level models to be
trained without centralizing entities' data by transmitting the global model to
local entities, training the model locally, and then averaging the gradients or
weights in the global model. However, some studies suggest that private
information can be recovered from the model gradients or weights. In this work,
we address the problem of multi-site fMRI classification with a
privacy-preserving strategy. To solve the problem, we propose a federated
learning approach, where a decentralized iterative optimization algorithm is
implemented and shared local model weights are altered by a randomization
mechanism. Considering the systemic differences of fMRI distributions from
different sites, we further propose two domain adaptation methods in this
federated learning formulation. We investigate various practical aspects of
federated model optimization and compare federated learning with alternative
training strategies. Overall, our results demonstrate that it is promising to
utilize multi-site data without data sharing to boost neuroimage analysis
performance and find reliable disease-related biomarkers. Our proposed pipeline
can be generalized to other privacy-sensitive medical data analysis problems.
</p>
<a href="http://arxiv.org/abs/2001.05647" target="_blank">arXiv:2001.05647</a> [<a href="http://arxiv.org/pdf/2001.05647" target="_blank">pdf</a>]

<h2>Multi-task Reinforcement Learning with a Planning Quasi-Metric. (arXiv:2002.03240v3 [cs.LG] UPDATED)</h2>
<h3>Vincent Micheli, Karthigan Sinnathamby, Fran&#xe7;ois Fleuret</h3>
<p>We introduce a new reinforcement learning approach combining a planning
quasi-metric (PQM) that estimates the number of steps required to go from any
state to another, with task-specific "aimers" that compute a target state to
reach a given goal. This decomposition allows the sharing across tasks of a
task-agnostic model of the quasi-metric that captures the environment's
dynamics and can be learned in a dense and unsupervised manner. We achieve
multiple-fold training speed-up compared to recently published methods on the
standard bit-flip problem and in the MuJoCo robotic arm simulator.
</p>
<a href="http://arxiv.org/abs/2002.03240" target="_blank">arXiv:2002.03240</a> [<a href="http://arxiv.org/pdf/2002.03240" target="_blank">pdf</a>]

<h2>Self-Supervised Joint Encoding of Motion and Appearance for First Person Action Recognition. (arXiv:2002.03982v2 [cs.CV] UPDATED)</h2>
<h3>Mirco Planamente, Andrea Bottino, Barbara Caputo</h3>
<p>Wearable cameras are becoming more and more popular in several applications,
increasing the interest of the research community in developing approaches for
recognizing actions from the first-person point of view. An open challenge in
egocentric action recognition is that videos lack detailed information about
the main actor's pose and thus tend to record only parts of the movement when
focusing on manipulation tasks. Thus, the amount of information about the
action itself is limited, making crucial the understanding of the manipulated
objects and their context. Many previous works addressed this issue with
two-stream architectures, where one stream is dedicated to modeling the
appearance of objects involved in the action, and another to extracting motion
features from optical flow. In this paper, we argue that learning features
jointly from these two information channels is beneficial to capture the
spatio-temporal correlations between the two better. To this end, we propose a
single stream architecture able to do so, thanks to the addition of a
self-supervised block that uses a pretext motion prediction task to intertwine
motion and appearance knowledge. Experiments on several publicly available
databases show the power of our approach.
</p>
<a href="http://arxiv.org/abs/2002.03982" target="_blank">arXiv:2002.03982</a> [<a href="http://arxiv.org/pdf/2002.03982" target="_blank">pdf</a>]

<h2>Contrastive Similarity Matching for Supervised Learning. (arXiv:2002.10378v5 [cs.LG] UPDATED)</h2>
<h3>Shanshan Qin, Nayantara Mudur, Cengiz Pehlevan</h3>
<p>We propose a novel biologically-plausible solution to the credit assignment
problem motivated by observations in the ventral visual pathway and trained
deep neural networks. In both, representations of objects in the same category
become progressively more similar, while objects belonging to different
categories become less similar. We use this observation to motivate a
layer-specific learning goal in a deep network: each layer aims to learn a
representational similarity matrix that interpolates between previous and later
layers. We formulate this idea using a contrastive similarity matching
objective function and derive from it deep neural networks with feedforward,
lateral, and feedback connections, and neurons that exhibit
biologically-plausible Hebbian and anti-Hebbian plasticity. Contrastive
similarity matching can be interpreted as an energy-based learning algorithm,
but with significant differences from others in how a contrastive function is
constructed.
</p>
<a href="http://arxiv.org/abs/2002.10378" target="_blank">arXiv:2002.10378</a> [<a href="http://arxiv.org/pdf/2002.10378" target="_blank">pdf</a>]

<h2>Curriculum By Smoothing. (arXiv:2003.01367v4 [cs.LG] UPDATED)</h2>
<h3>Samarth Sinha, Animesh Garg, Hugo Larochelle</h3>
<p>Convolutional Neural Networks (CNNs) have shown impressive performance in
computer vision tasks such as image classification, detection, and
segmentation. Moreover, recent work in Generative Adversarial Networks (GANs)
has highlighted the importance of learning by progressively increasing the
difficulty of a learning task [26]. When learning a network from scratch, the
information propagated within the network during the earlier stages of training
can contain distortion artifacts due to noise which can be detrimental to
training. In this paper, we propose an elegant curriculum based scheme that
smoothes the feature embedding of a CNN using anti-aliasing or low-pass
filters. We propose to augment the train-ing of CNNs by controlling the amount
of high frequency information propagated within the CNNs as training
progresses, by convolving the output of a CNN feature map of each layer with a
Gaussian kernel. By decreasing the variance of the Gaussian kernel, we
gradually increase the amount of high-frequency information available within
the network for inference. As the amount of information in the feature maps
increases during training, the network is able to progressively learn better
representations of the data. Our proposed augmented training scheme
significantly improves the performance of CNNs on various vision tasks without
either adding additional trainable parameters or an auxiliary regularization
objective. The generality of our method is demonstrated through empirical
performance gains in CNN architectures across four different tasks: transfer
learning, cross-task transfer learning, and generative models.
</p>
<a href="http://arxiv.org/abs/2003.01367" target="_blank">arXiv:2003.01367</a> [<a href="http://arxiv.org/pdf/2003.01367" target="_blank">pdf</a>]

<h2>Train-by-Reconnect: Decoupling Locations of Weights from their Values. (arXiv:2003.02570v6 [cs.LG] UPDATED)</h2>
<h3>Yushi Qiu, Reiji Suda</h3>
<p>What makes untrained deep neural networks (DNNs) different from the trained
performant ones? By zooming into the weights in well-trained DNNs, we found it
is the location of weights that hold most of the information encoded by the
training. Motivated by this observation, we hypothesize that weights in
stochastic gradient-based method trained DNNs can be separated into two
dimensions: the locations of weights and their exact values. To assess our
hypothesis, we propose a novel method named Lookahead Permutation (LaPerm) to
train DNNs by reconnecting the weights. We empirically demonstrate the
versatility of LaPerm while producing extensive evidence to support our
hypothesis: when the initial weights are random and dense, our method
demonstrates speed and performance similar to or better than that of regular
optimizers, e.g., Adam; when the initial weights are random and sparse (many
zeros), our method changes the way neurons connect and reach accuracy
comparable to that of a well-trained fully initialized network; when the
initial weights share a single value, our method finds weight agnostic neural
network with far better-than-chance accuracy.
</p>
<a href="http://arxiv.org/abs/2003.02570" target="_blank">arXiv:2003.02570</a> [<a href="http://arxiv.org/pdf/2003.02570" target="_blank">pdf</a>]

<h2>DIBS: Diversity inducing Information Bottleneck in Model Ensembles. (arXiv:2003.04514v2 [cs.LG] UPDATED)</h2>
<h3>Samarth Sinha, Homanga Bharadhwaj, Anirudh Goyal, Hugo Larochelle, Animesh Garg, Florian Shkurti</h3>
<p>Although deep learning models have achieved state-of-the-art performance on a
number of vision tasks, generalization over high dimensional multi-modal data,
and reliable predictive uncertainty estimation are still active areas of
research. Bayesian approaches including Bayesian Neural Nets (BNNs) do not
scale well to modern computer vision tasks, as they are difficult to train, and
have poor generalization under dataset-shift. This motivates the need for
effective ensembles which can generalize and give reliable uncertainty
estimates. In this paper, we target the problem of generating effective
ensembles of neural networks by encouraging diversity in prediction. We
explicitly optimize a diversity inducing adversarial loss for learning the
stochastic latent variables and thereby obtain diversity in the output
predictions necessary for modeling multi-modal data. We evaluate our method on
benchmark datasets: MNIST, CIFAR100, TinyImageNet and MIT Places 2, and
compared to the most competitive baselines show significant improvements in
classification accuracy, under a shift in the data distribution and in
out-of-distribution detection. Code will be released in this url
https://github.com/rvl-lab-utoronto/dibs
</p>
<a href="http://arxiv.org/abs/2003.04514" target="_blank">arXiv:2003.04514</a> [<a href="http://arxiv.org/pdf/2003.04514" target="_blank">pdf</a>]

<h2>Rethinking Object Detection in Retail Stores. (arXiv:2003.08230v3 [cs.CV] UPDATED)</h2>
<h3>Yuanqiang Cai, Longyin Wen, Libo Zhang, Dawei Du, Weiqiang Wang</h3>
<p>The convention standard for object detection uses a bounding box to represent
each individual object instance. However, it is not practical in the
industry-relevant applications in the context of warehouses due to severe
occlusions among groups of instances of the same categories. In this paper, we
propose a new task, ie, simultaneously object localization and counting,
abbreviated as Locount, which requires algorithms to localize groups of objects
of interest with the number of instances. However, there does not exist a
dataset or benchmark designed for such a task. To this end, we collect a
large-scale object localization and counting dataset with rich annotations in
retail stores, which consists of 50,394 images with more than 1.9 million
object instances in 140 categories. Together with this dataset, we provide a
new evaluation protocol and divide the training and testing subsets to fairly
evaluate the performance of algorithms for Locount, developing a new benchmark
for the Locount task. Moreover, we present a cascaded localization and counting
network as a strong baseline, which gradually classifies and regresses the
bounding boxes of objects with the predicted numbers of instances enclosed in
the bounding boxes, trained in an end-to-end manner. Extensive experiments are
conducted on the proposed dataset to demonstrate its significance and the
analysis discussions on failure cases are provided to indicate future
directions. Dataset is available at
https://isrc.iscas.ac.cn/gitlab/research/locount-dataset.
</p>
<a href="http://arxiv.org/abs/2003.08230" target="_blank">arXiv:2003.08230</a> [<a href="http://arxiv.org/pdf/2003.08230" target="_blank">pdf</a>]

<h2>Classification of Chinese Handwritten Numbers with Labeled Projective Dictionary Pair Learning. (arXiv:2003.11700v3 [cs.CV] UPDATED)</h2>
<h3>Rasool Ameri, Ali Alameer, Saideh Ferdowsi, Kianoush Nazarpour, Vahid Abolghasemi</h3>
<p>Dictionary learning is a cornerstone of image classification. We set out to
address a longstanding challenge in using dictionary learning for
classification; that is to simultaneously maximise the discriminability and
sparse-representability power of the learned dictionaries. Upon this premise,
we designed class-specific dictionaries incorporating three factors:
discriminability, sparsity and classification error. We integrated these
metrics into a unified cost function and adopted a new feature space, i.e.,
histogram of oriented gradients (HOG), to generate the dictionary atoms. The
rationale of using HOG features for designing the dictionaries is their
strength in describing fine details of crowded images. The results of applying
the proposed method in the classification of Chinese handwritten numbers
demonstrated enhanced classification performance $(\sim98\%)$ compared to
state-of-the-art deep learning techniques (i.e., SqueezeNet, GoogLeNet and
MobileNetV2), but with a fraction of parameters. Furthermore, combination of
the HOG features with dictionary learning enhances the accuracy by $11\%$
compared to the case where only pixel domain data are used. These results were
supported when the proposed method was applied to both Arabic and English
handwritten number databases.
</p>
<a href="http://arxiv.org/abs/2003.11700" target="_blank">arXiv:2003.11700</a> [<a href="http://arxiv.org/pdf/2003.11700" target="_blank">pdf</a>]

<h2>Multi-Task Reinforcement Learning with Soft Modularization. (arXiv:2003.13661v2 [cs.LG] UPDATED)</h2>
<h3>Ruihan Yang, Huazhe Xu, Yi Wu, Xiaolong Wang</h3>
<p>Multi-task learning is a very challenging problem in reinforcement learning.
While training multiple tasks jointly allow the policies to share parameters
across different tasks, the optimization problem becomes non-trivial: It
remains unclear what parameters in the network should be reused across tasks,
and how the gradients from different tasks may interfere with each other. Thus,
instead of naively sharing parameters across tasks, we introduce an explicit
modularization technique on policy representation to alleviate this
optimization issue. Given a base policy network, we design a routing network
which estimates different routing strategies to reconfigure the base network
for each task. Instead of directly selecting routes for each task, our
task-specific policy uses a method called soft modularization to softly combine
all the possible routes, which makes it suitable for sequential tasks. We
experiment with various robotics manipulation tasks in simulation and show our
method improves both sample efficiency and performance over strong baselines by
a large margin.
</p>
<a href="http://arxiv.org/abs/2003.13661" target="_blank">arXiv:2003.13661</a> [<a href="http://arxiv.org/pdf/2003.13661" target="_blank">pdf</a>]

<h2>Assisted Learning: A Framework for Multi-Organization Learning. (arXiv:2004.00566v5 [cs.LG] UPDATED)</h2>
<h3>Xun Xian, Xinran Wang, Jie Ding, Reza Ghanadan</h3>
<p>In an increasing number of AI scenarios, collaborations among different
organizations or agents (e.g., human and robots, mobile units) are often
essential to accomplish an organization-specific mission. However, to avoid
leaking useful and possibly proprietary information, organizations typically
enforce stringent security constraints on sharing modeling algorithms and data,
which significantly limits collaborations. In this work, we introduce the
Assisted Learning framework for organizations to assist each other in
supervised learning tasks without revealing any organization's algorithm, data,
or even task. An organization seeks assistance by broadcasting task-specific
but nonsensitive statistics and incorporating others' feedback in one or more
iterations to eventually improve its predictive performance. Theoretical and
experimental studies, including real-world medical benchmarks, show that
Assisted Learning can often achieve near-oracle learning performance as if data
and training processes were centralized.
</p>
<a href="http://arxiv.org/abs/2004.00566" target="_blank">arXiv:2004.00566</a> [<a href="http://arxiv.org/pdf/2004.00566" target="_blank">pdf</a>]

<h2>Optimistic Agent: Accurate Graph-Based Value Estimation for More Successful Visual Navigation. (arXiv:2004.03222v2 [cs.CV] UPDATED)</h2>
<h3>Mahdi Kazemi Moghaddam, Qi Wu, Ehsan Abbasnejad, Javen Qinfeng Shi</h3>
<p>We humans can impeccably search for a target object, given its name only,
even in an unseen environment. We argue that this ability is largely due to
three main reasons: the incorporation of prior knowledge (or experience), the
adaptation of it to the new environment using the observed visual cues and most
importantly optimistically searching without giving up early. This is currently
missing in the state-of-the-art visual navigation methods based on
Reinforcement Learning (RL). In this paper, we propose to use externally
learned prior knowledge of the relative object locations and integrate it into
our model by constructing a neural graph. In order to efficiently incorporate
the graph without increasing the state-space complexity, we propose our
Graph-based Value Estimation (GVE) module. GVE provides a more accurate
baseline for estimating the Advantage function in actor-critic RL algorithm.
This results in reduced value estimation error and, consequently, convergence
to a more optimal policy. Through empirical studies, we show that our agent,
dubbed as the optimistic agent, has a more realistic estimate of the state
value during a navigation episode which leads to a higher success rate. Our
extensive ablation studies show the efficacy of our simple method which
achieves the state-of-the-art results measured by the conventional visual
navigation metrics, e.g. Success Rate (SR) and Success weighted by Path Length
(SPL), in AI2THOR environment.
</p>
<a href="http://arxiv.org/abs/2004.03222" target="_blank">arXiv:2004.03222</a> [<a href="http://arxiv.org/pdf/2004.03222" target="_blank">pdf</a>]

<h2>Mean-Variance Policy Iteration for Risk-Averse Reinforcement Learning. (arXiv:2004.10888v4 [cs.LG] UPDATED)</h2>
<h3>Shangtong Zhang, Bo Liu, Shimon Whiteson</h3>
<p>We present a mean-variance policy iteration (MVPI) framework for risk-averse
control in a discounted infinite horizon MDP optimizing the variance of a
per-step reward random variable. MVPI enjoys great flexibility in that any
policy evaluation method and risk-neutral control method can be dropped in for
risk-averse control off the shelf, in both on- and off-policy settings. This
flexibility reduces the gap between risk-neutral control and risk-averse
control and is achieved by working on a novel augmented MDP directly. We
propose risk-averse TD3 as an example instantiating MVPI, which outperforms
vanilla TD3 and many previous risk-averse control methods in challenging Mujoco
robot simulation tasks under a risk-aware performance metric. This risk-averse
TD3 is the first to introduce deterministic policies and off-policy learning
into risk-averse reinforcement learning, both of which are key to the
performance boost we show in Mujoco domains.
</p>
<a href="http://arxiv.org/abs/2004.10888" target="_blank">arXiv:2004.10888</a> [<a href="http://arxiv.org/pdf/2004.10888" target="_blank">pdf</a>]

<h2>Short-term Load Forecasting Based on Hybrid Strategy Using Warm-start Gradient Tree Boosting. (arXiv:2005.11478v2 [cs.LG] UPDATED)</h2>
<h3>Yuexin Zhang, Jiahong Wang</h3>
<p>A deep-learning-based hybrid strategy for short-term load forecasting is
presented. The strategy proposes a novel tree-based ensemble method Warm-start
Gradient Tree Boosting (WGTB). Current strategies either ensemble submodels of
a single type, which fail to take advantage of the statistical strengths of
different inference models. Or they simply sum the outputs from completely
different inference models, which doesn't maximize the potential of ensemble.
Inspired by the bias-variance trade-off, WGTB is proposed and tailored to the
great disparity among different inference models on accuracy, volatility and
linearity. The complete strategy integrates four different inference models of
different capacities. WGTB then ensembles their outputs by a warm-start and a
hybrid of bagging and boosting, which lowers bias and variance concurrently. It
is validated on two real datasets from State Grid Corporation of China of
hourly resolution. The result demonstrates the effectiveness of the proposed
strategy that hybridizes the statistical strengths of both low-bias and
low-variance inference models.
</p>
<a href="http://arxiv.org/abs/2005.11478" target="_blank">arXiv:2005.11478</a> [<a href="http://arxiv.org/pdf/2005.11478" target="_blank">pdf</a>]

<h2>Revisiting Street-to-Aerial View Image Geo-localization and Orientation Estimation. (arXiv:2005.11592v2 [cs.CV] UPDATED)</h2>
<h3>Sijie Zhu, Taojiannan Yang, Chen Chen</h3>
<p>Street-to-aerial image geo-localization, which matches a query street-view
image to the GPS-tagged aerial images in a reference set, has attracted
increasing attention recently. In this paper, we revisit this problem and point
out the ignored issue about image alignment information. We show that the
performance of a simple Siamese network is highly dependent on the alignment
setting and the comparison of previous works can be unfair if they have
different assumptions. Instead of focusing on the feature extraction under the
alignment assumption, we show that improvements in metric learning techniques
significantly boost the performance regardless of the alignment. Without
leveraging the alignment information, our pipeline outperforms previous works
on both panorama and cropped datasets. Furthermore, we conduct visualization to
help understand the learned model and the effect of alignment information using
Grad-CAM. With our discovery on the approximate rotation-invariant activation
maps, we propose a novel method to estimate the orientation/alignment between a
pair of cross-view images with unknown alignment information. It achieves
state-of-the-art results on the CVUSA dataset.
</p>
<a href="http://arxiv.org/abs/2005.11592" target="_blank">arXiv:2005.11592</a> [<a href="http://arxiv.org/pdf/2005.11592" target="_blank">pdf</a>]

<h2>Deep Context-Aware Novelty Detection. (arXiv:2006.01168v2 [cs.LG] UPDATED)</h2>
<h3>Ellen Rushe, Brian Mac Namee</h3>
<p>A common assumption of novelty detection is that the distribution of both
"normal" and "novel" data are static. This, however, is often not the case -
for example scenarios where data evolves over time or scenarios in which the
definition of normal and novel depends on contextual information, both leading
to changes in these distributions. This can lead to significant difficulties
when attempting to train a model on datasets where the distribution of normal
data in one scenario is similar to that of novel data in another scenario. In
this paper we propose a context-aware approach to novelty detection for deep
autoencoders to address these difficulties. We create a semi-supervised network
architecture that utilises auxiliary labels to reveal contextual information
and allow the model to adapt to a variety of contexts in which the definitions
of normal and novel change. We evaluate our approach on both image data and
real world audio data displaying these characteristics and show that the
performance of individually trained models can be achieved in a single model.
</p>
<a href="http://arxiv.org/abs/2006.01168" target="_blank">arXiv:2006.01168</a> [<a href="http://arxiv.org/pdf/2006.01168" target="_blank">pdf</a>]

<h2>An Ergodic Measure for Active Learning From Equilibrium. (arXiv:2006.03552v2 [cs.RO] UPDATED)</h2>
<h3>Ian Abraham, Ahalya Prabhakar, Todd D. Murphey</h3>
<p>This paper develops KL-Ergodic Exploration from Equilibrium
($\text{KL-E}^3$), a method for robotic systems to integrate stability into
actively generating informative measurements through ergodic exploration.
Ergodic exploration enables robotic systems to indirectly sample from
informative spatial distributions globally, avoiding local optima, and without
the need to evaluate the derivatives of the distribution against the robot
dynamics. Using hybrid systems theory, we derive a controller that allows a
robot to exploit equilibrium policies (i.e., policies that solve a task) while
allowing the robot to explore and generate informative data using an ergodic
measure that can extend to high-dimensional states. We show that our method is
able to maintain Lyapunov attractiveness with respect to the equilibrium task
while actively generating data for learning tasks such, as Bayesian
optimization, model learning, and off-policy reinforcement learning. In each
example, we show that our proposed method is capable of generating an
informative distribution of data while synthesizing smooth control signals. We
illustrate these examples using simulated systems and provide simplification of
our method for real-time online learning in robotic systems.
</p>
<a href="http://arxiv.org/abs/2006.03552" target="_blank">arXiv:2006.03552</a> [<a href="http://arxiv.org/pdf/2006.03552" target="_blank">pdf</a>]

<h2>BanditPAM: Almost Linear Time $k$-Medoids Clustering via Multi-Armed Bandits. (arXiv:2006.06856v2 [cs.LG] UPDATED)</h2>
<h3>Mo Tiwari, Martin Jinye Zhang, James Mayclin, Sebastian Thrun, Chris Piech, Ilan Shomorony</h3>
<p>Clustering is a ubiquitous task in data science. Compared to the commonly
used $k$-means clustering, $k$-medoids clustering requires the cluster centers
to be actual data points and support arbitrary distance metrics, which permits
greater interpretability and the clustering of structured objects. Current
state-of-the-art $k$-medoids clustering algorithms, such as Partitioning Around
Medoids (PAM), are iterative and are quadratic in the dataset size $n$ for each
iteration, being prohibitively expensive for large datasets. We propose
BanditPAM, a randomized algorithm inspired by techniques from multi-armed
bandits, that reduces the complexity of each PAM iteration from $O(n^2)$ to
$O(n \log n)$ and returns the same results with high probability, under
assumptions on the data that often hold in practice. As such, BanditPAM matches
state-of-the-art clustering loss while reaching solutions much faster. We
empirically validate our results on several large real-world datasets,
including a coding exercise submissions dataset, the 10x Genomics 68k PBMC
single-cell RNA sequencing dataset, and the MNIST handwritten digits dataset.
In these experiments, we observe that BanditPAM returns the same results as
state-of-the-art PAM-like algorithms up to 4x faster while performing up to
200x fewer distance computations. The improvements demonstrated by BanditPAM
enable $k$-medoids clustering on a wide range of applications, including
identifying cell types in large-scale single-cell data and providing scalable
feedback for students learning computer science online. We also release highly
optimized Python and C++ implementations of our algorithm.
</p>
<a href="http://arxiv.org/abs/2006.06856" target="_blank">arXiv:2006.06856</a> [<a href="http://arxiv.org/pdf/2006.06856" target="_blank">pdf</a>]

<h2>Differentially Private Stochastic Coordinate Descent. (arXiv:2006.07272v3 [cs.LG] UPDATED)</h2>
<h3>Georgios Damaskinos, Celestine Mendler-D&#xfc;nner, Rachid Guerraoui, Nikolaos Papandreou, Thomas Parnell</h3>
<p>In this paper we tackle the challenge of making the stochastic coordinate
descent algorithm differentially private. Compared to the classical gradient
descent algorithm where updates operate on a single model vector and controlled
noise addition to this vector suffices to hide critical information about
individuals, stochastic coordinate descent crucially relies on keeping
auxiliary information in memory during training. This auxiliary information
provides an additional privacy leak and poses the major challenge addressed in
this work. Driven by the insight that under independent noise addition, the
consistency of the auxiliary information holds in expectation, we present
DP-SCD, the first differentially private stochastic coordinate descent
algorithm. We analyze our new method theoretically and argue that decoupling
and parallelizing coordinate updates is essential for its utility. On the
empirical side we demonstrate competitive performance against the popular
stochastic gradient descent alternative (DP-SGD) while requiring significantly
less tuning.
</p>
<a href="http://arxiv.org/abs/2006.07272" target="_blank">arXiv:2006.07272</a> [<a href="http://arxiv.org/pdf/2006.07272" target="_blank">pdf</a>]

<h2>Geo-PIFu: Geometry and Pixel Aligned Implicit Functions for Single-view Human Reconstruction. (arXiv:2006.08072v2 [cs.CV] UPDATED)</h2>
<h3>Tong He, John Collomosse, Hailin Jin, Stefano Soatto</h3>
<p>We propose Geo-PIFu, a method to recover a 3D mesh from a monocular color
image of a clothed person. Our method is based on a deep implicit
function-based representation to learn latent voxel features using a
structure-aware 3D U-Net, to constrain the model in two ways: first, to resolve
feature ambiguities in query point encoding, second, to serve as a coarse human
shape proxy to regularize the high-resolution mesh and encourage global shape
regularity. We show that, by both encoding query points and constraining global
shape using latent voxel features, the reconstruction we obtain for clothed
human meshes exhibits less shape distortion and improved surface details
compared to competing methods. We evaluate Geo-PIFu on a recent human mesh
public dataset that is $10 \times$ larger than the private commercial dataset
used in PIFu and previous derivative work. On average, we exceed the state of
the art by $42.7\%$ reduction in Chamfer and Point-to-Surface Distances, and
$19.4\%$ reduction in normal estimation errors.
</p>
<a href="http://arxiv.org/abs/2006.08072" target="_blank">arXiv:2006.08072</a> [<a href="http://arxiv.org/pdf/2006.08072" target="_blank">pdf</a>]

<h2>GALI: Generalized Adversarially Learned Inference. (arXiv:2006.08089v2 [cs.LG] UPDATED)</h2>
<h3>Yatin Dandi, Homanga Bharadhwaj, Abhishek Kumar, Piyush Rai</h3>
<p>Allowing effective inference of latent vectors while training GANs can
greatly increase their applicability in various downstream tasks. Recent
approaches, such as ALI and BiGAN frameworks, develop methods of inference of
latent variables in GANs by adversarially training an image generator along
with an encoder to match two joint distributions of image and latent vector
pairs. We generalize these approaches to incorporate multiple layers of
feedback on reconstructions, self-supervision, and other forms of supervision
based on prior or learned knowledge about the desired solutions. We achieve
this by modifying the discriminator's objective to correctly identify more than
two joint distributions of tuples of an arbitrary number of random variables
consisting of images, latent vectors, and other variables generated through
auxiliary tasks, such as reconstruction and inpainting or as outputs of
suitable pre-trained models. We design a non-saturating maximization objective
for the generator-encoder pair and prove that the resulting adversarial game
corresponds to a global optimum that simultaneously matches all the
distributions. Within our proposed framework, we introduce a novel set of
techniques for providing self-supervised feedback to the model based on
properties, such as patch-level correspondence and cycle consistency of
reconstructions. Through comprehensive experiments, we demonstrate the
efficacy, scalability, and flexibility of the proposed approach for a variety
of tasks.
</p>
<a href="http://arxiv.org/abs/2006.08089" target="_blank">arXiv:2006.08089</a> [<a href="http://arxiv.org/pdf/2006.08089" target="_blank">pdf</a>]

<h2>Depth Uncertainty in Neural Networks. (arXiv:2006.08437v3 [stat.ML] UPDATED)</h2>
<h3>Javier Antor&#xe1;n, James Urquhart Allingham, Jos&#xe9; Miguel Hern&#xe1;ndez-Lobato</h3>
<p>Existing methods for estimating uncertainty in deep learning tend to require
multiple forward passes, making them unsuitable for applications where
computational resources are limited. To solve this, we perform probabilistic
reasoning over the depth of neural networks. Different depths correspond to
subnetworks which share weights and whose predictions are combined via
marginalisation, yielding model uncertainty. By exploiting the sequential
structure of feed-forward networks, we are able to both evaluate our training
objective and make predictions with a single forward pass. We validate our
approach on real-world regression and image classification tasks. Our approach
provides uncertainty calibration, robustness to dataset shift, and accuracies
competitive with more computationally expensive baselines.
</p>
<a href="http://arxiv.org/abs/2006.08437" target="_blank">arXiv:2006.08437</a> [<a href="http://arxiv.org/pdf/2006.08437" target="_blank">pdf</a>]

<h2>Intra-Processing Methods for Debiasing Neural Networks. (arXiv:2006.08564v2 [cs.LG] UPDATED)</h2>
<h3>Yash Savani, Colin White, Naveen Sundar Govindarajulu</h3>
<p>As deep learning models become tasked with more and more decisions that
impact human lives, such as criminal recidivism, loan repayment, and face
recognition for law enforcement, bias is becoming a growing concern. Debiasing
algorithms are typically split into three paradigms: pre-processing,
in-processing, and post-processing. However, in computer vision or natural
language applications, it is common to start with a large generic model and
then fine-tune to a specific use-case. Pre- or in-processing methods would
require retraining the entire model from scratch, while post-processing methods
only have black-box access to the model, so they do not leverage the weights of
the trained model. Creating debiasing algorithms specifically for this
fine-tuning use-case has largely been neglected.

In this work, we initiate the study of a new paradigm in debiasing research,
intra-processing, which sits between in-processing and post-processing methods.
Intra-processing methods are designed specifically to debias large models which
have been trained on a generic dataset and fine-tuned on a more specific task.
We show how to repurpose existing in-processing methods for this use-case, and
we also propose three baseline algorithms: random perturbation, layerwise
optimization, and adversarial fine-tuning. All of our techniques can be used
for all popular group fairness measures such as equalized odds or statistical
parity difference. We evaluate these methods across three popular datasets from
the AIF360 toolkit, as well as on the CelebA faces dataset. Our code is
available at https://github.com/abacusai/intraprocessing_debiasing.
</p>
<a href="http://arxiv.org/abs/2006.08564" target="_blank">arXiv:2006.08564</a> [<a href="http://arxiv.org/pdf/2006.08564" target="_blank">pdf</a>]

<h2>Predicting Livelihood Indicators from Crowdsourced Street Level Images. (arXiv:2006.08661v5 [cs.CV] UPDATED)</h2>
<h3>Jihyeon Lee, Dylan Grosz, Burak Uzkent, Sicheng Zeng, Marshall Burke, David Lobell, Stefano Ermon</h3>
<p>Major decisions from governments and other large organizations rely on
measurements of the populace's well-being, but making such measurements at a
broad scale is expensive and thus infrequent in much of the developing world.
We propose an inexpensive, scalable, and interpretable approach to predict key
livelihood indicators from public crowd-sourced street-level imagery. Such
imagery can be cheaply collected and more frequently updated compared to
traditional surveying methods, while containing plausibly relevant information
for a range of livelihood indicators. We propose two approaches to learn from
the street-level imagery: (1) a method that creates multi-household cluster
representations by detecting informative objects and (2) a graph-based approach
that captures the relationships between images. By visualizing what features
are important to a model and how they are used, we can help end-user
organizations understand the models and offer an alternate approach for index
estimation that uses cheaply obtained roadway features. By comparing our
results against ground data collected in nationally-representative household
surveys, we demonstrate the performance of our approach in accurately
predicting indicators of poverty, population, and health and its scalability by
testing in two different countries, India and Kenya.
</p>
<a href="http://arxiv.org/abs/2006.08661" target="_blank">arXiv:2006.08661</a> [<a href="http://arxiv.org/pdf/2006.08661" target="_blank">pdf</a>]

<h2>Differentiable Augmentation for Data-Efficient GAN Training. (arXiv:2006.10738v4 [cs.CV] UPDATED)</h2>
<h3>Shengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, Song Han</h3>
<p>The performance of generative adversarial networks (GANs) heavily
deteriorates given a limited amount of training data. This is mainly because
the discriminator is memorizing the exact training set. To combat it, we
propose Differentiable Augmentation (DiffAugment), a simple method that
improves the data efficiency of GANs by imposing various types of
differentiable augmentations on both real and fake samples. Previous attempts
to directly augment the training data manipulate the distribution of real
images, yielding little benefit; DiffAugment enables us to adopt the
differentiable augmentation for the generated samples, effectively stabilizes
training, and leads to better convergence. Experiments demonstrate consistent
gains of our method over a variety of GAN architectures and loss functions for
both unconditional and class-conditional generation. With DiffAugment, we
achieve a state-of-the-art FID of 6.80 with an IS of 100.8 on ImageNet 128x128
and 2-4x reductions of FID given 1,000 images on FFHQ and LSUN. Furthermore,
with only 20% training data, we can match the top performance on CIFAR-10 and
CIFAR-100. Finally, our method can generate high-fidelity images using only 100
images without pre-training, while being on par with existing transfer learning
algorithms. Code is available at
https://github.com/mit-han-lab/data-efficient-gans.
</p>
<a href="http://arxiv.org/abs/2006.10738" target="_blank">arXiv:2006.10738</a> [<a href="http://arxiv.org/pdf/2006.10738" target="_blank">pdf</a>]

<h2>Multi-Class Uncertainty Calibration via Mutual Information Maximization-based Binning. (arXiv:2006.13092v4 [cs.LG] UPDATED)</h2>
<h3>Kanil Patel, William Beluch, Bin Yang, Michael Pfeiffer, Dan Zhang</h3>
<p>Post-hoc multi-class calibration is a common approach for providing
high-quality confidence estimates of deep neural network predictions. Recent
work has shown that widely used scaling methods underestimate their calibration
error, while alternative Histogram Binning (HB) methods often fail to preserve
classification accuracy. When classes have small prior probabilities, HB also
faces the issue of severe sample-inefficiency after the conversion into K
one-vs-rest class-wise calibration problems. The goal of this paper is to
resolve the identified issues of HB in order to provide calibrated confidence
estimates using only a small holdout calibration dataset for bin optimization
while preserving multi-class ranking accuracy. From an information-theoretic
perspective, we derive the I-Max concept for binning, which maximizes the
mutual information between labels and quantized logits. This concept mitigates
potential loss in ranking performance due to lossy quantization, and by
disentangling the optimization of bin edges and representatives allows
simultaneous improvement of ranking and calibration performance. To improve the
sample efficiency and estimates from a small calibration set, we propose a
shared class-wise (sCW) calibration strategy, sharing one calibrator among
similar classes (e.g., with similar class priors) so that the training sets of
their class-wise calibration problems can be merged to train the single
calibrator. The combination of sCW and I-Max binning outperforms the state of
the art calibration methods on various evaluation metrics across different
benchmark datasets and models, using a small calibration set (e.g., 1k samples
for ImageNet).
</p>
<a href="http://arxiv.org/abs/2006.13092" target="_blank">arXiv:2006.13092</a> [<a href="http://arxiv.org/pdf/2006.13092" target="_blank">pdf</a>]

<h2>Randomized Block-Diagonal Preconditioning for Parallel Learning. (arXiv:2006.13591v2 [cs.LG] UPDATED)</h2>
<h3>Celestine Mendler-D&#xfc;nner, Aurelien Lucchi</h3>
<p>We study preconditioned gradient-based optimization methods where the
preconditioning matrix has block-diagonal form. Such a structural constraint
comes with the advantage that the update computation is block-separable and can
be parallelized across multiple independent tasks. Our main contribution is to
demonstrate that the convergence of these methods can significantly be improved
by a randomization technique which corresponds to repartitioning coordinates
across tasks during the optimization procedure. We provide a theoretical
analysis that accurately characterizes the expected convergence gains of
repartitioning and validate our findings empirically on various traditional
machine learning tasks. From an implementation perspective, block-separable
models are well suited for parallelization and, when shared memory is
available, randomization can be implemented on top of existing methods very
efficiently to improve convergence.
</p>
<a href="http://arxiv.org/abs/2006.13591" target="_blank">arXiv:2006.13591</a> [<a href="http://arxiv.org/pdf/2006.13591" target="_blank">pdf</a>]

<h2>Policy Improvement via Imitation of Multiple Oracles. (arXiv:2007.00795v2 [cs.LG] UPDATED)</h2>
<h3>Ching-An Cheng, Andrey Kolobov, Alekh Agarwal</h3>
<p>Despite its promise, reinforcement learning's real-world adoption has been
hampered by the need for costly exploration to learn a good policy. Imitation
learning (IL) mitigates this shortcoming by using an oracle policy during
training as a bootstrap to accelerate the learning process. However, in many
practical situations, the learner has access to multiple suboptimal oracles,
which may provide conflicting advice in a state. The existing IL literature
provides a limited treatment of such scenarios. Whereas in the single-oracle
case, the return of the oracle's policy provides an obvious benchmark for the
learner to compete against, neither such a benchmark nor principled ways of
outperforming it are known for the multi-oracle setting. In this paper, we
propose the state-wise maximum of the oracle policies' values as a natural
baseline to resolve conflicting advice from multiple oracles. Using a reduction
of policy optimization to online learning, we introduce a novel IL algorithm
MAMBA, which can provably learn a policy competitive with this benchmark. In
particular, MAMBA optimizes policies by using a gradient estimator in the style
of generalized advantage estimation (GAE). Our theoretical analysis shows that
this design makes MAMBA robust and enables it to outperform the oracle policies
by a larger margin than the IL state of the art, even in the single-oracle
case. In an evaluation against standard policy gradient with GAE and
AggreVaTe(D), we showcase MAMBA's ability to leverage demonstrations both from
a single and from multiple weak oracles, and significantly speed up policy
optimization.
</p>
<a href="http://arxiv.org/abs/2007.00795" target="_blank">arXiv:2007.00795</a> [<a href="http://arxiv.org/pdf/2007.00795" target="_blank">pdf</a>]

<h2>Deep Learning for Anomaly Detection: A Review. (arXiv:2007.02500v3 [cs.LG] UPDATED)</h2>
<h3>Guansong Pang, Chunhua Shen, Longbing Cao, Anton van den Hengel</h3>
<p>Anomaly detection, a.k.a. outlier detection or novelty detection, has been a
lasting yet active research area in various research communities for several
decades. There are still some unique problem complexities and challenges that
require advanced approaches. In recent years, deep learning enabled anomaly
detection, i.e., deep anomaly detection, has emerged as a critical direction.
This paper surveys the research of deep anomaly detection with a comprehensive
taxonomy, covering advancements in three high-level categories and 11
fine-grained categories of the methods. We review their key intuitions,
objective functions, underlying assumptions, advantages and disadvantages, and
discuss how they address the aforementioned challenges. We further discuss a
set of possible future opportunities and new perspectives on addressing the
challenges.
</p>
<a href="http://arxiv.org/abs/2007.02500" target="_blank">arXiv:2007.02500</a> [<a href="http://arxiv.org/pdf/2007.02500" target="_blank">pdf</a>]

<h2>Graph Convolutional Networks for Graphs Containing Missing Features. (arXiv:2007.04583v2 [cs.LG] UPDATED)</h2>
<h3>Hibiki Taguchi, Xin Liu, Tsuyoshi Murata</h3>
<p>Graph Convolutional Network (GCN) has experienced great success in graph
analysis tasks. It works by smoothing the node features across the graph. The
current GCN models overwhelmingly assume that the node feature information is
complete. However, real-world graph data are often incomplete and containing
missing features. Traditionally, people have to estimate and fill in the
unknown features based on imputation techniques and then apply GCN. However,
the process of feature filling and graph learning are separated, resulting in
degraded and unstable performance. This problem becomes more serious when a
large number of features are missing. We propose an approach that adapts GCN to
graphs containing missing features. In contrast to traditional strategy, our
approach integrates the processing of missing features and graph learning
within the same neural network architecture. Our idea is to represent the
missing data by Gaussian Mixture Model (GMM) and calculate the expected
activation of neurons in the first hidden layer of GCN, while keeping the other
layers of the network unchanged. This enables us to learn the GMM parameters
and network weight parameters in an end-to-end manner. Notably, our approach
does not increase the computational complexity of GCN and it is consistent with
GCN when the features are complete. We demonstrate through extensive
experiments that our approach significantly outperforms the imputation-based
methods in node classification and link prediction tasks. We show that the
performance of our approach for the case with a low level of missing features
is even superior to GCN for the case with complete features.
</p>
<a href="http://arxiv.org/abs/2007.04583" target="_blank">arXiv:2007.04583</a> [<a href="http://arxiv.org/pdf/2007.04583" target="_blank">pdf</a>]

<h2>LORCK: Learnable Object-Resembling Convolution Kernels. (arXiv:2007.05103v2 [cs.CV] UPDATED)</h2>
<h3>Elizaveta Lazareva, Oleg Rogov, Olga Shegai, Denis Larionov, Dmitry V. Dylov</h3>
<p>Segmentation of certain hollow organs, such as the bladder, is especially
hard to automate due to their complex geometry, vague intensity gradients in
the soft tissues, and a tedious manual process of the data annotation routine.
Yet, accurate localization of the walls and the cancer regions in the
radiologic images of such organs is an essential step in oncology. To address
this issue, we propose a new class of hollow kernels that learn to 'mimic' the
contours of the segmented organ, effectively replicating its shape and
structural complexity. We train a series of the U-Net-like neural networks
using the proposed kernels and demonstrate the superiority of the idea in
various spatio-temporal convolution scenarios. Specifically, the dilated
hollow-kernel architecture outperforms state-of-the-art spatial segmentation
models, whereas the addition of temporal blocks with, e.g., Bi-LSTM,
establishes a new multi-class baseline for the bladder segmentation challenge.
Our spatio-temporal model based on the hollow kernels reaches the mean dice
scores of 0.936, 0.736, and 0.712 for the bladder's inner wall, the outer wall,
and the tumor regions, respectively. The results pave the way towards other
domain-specific deep learning applications where the shape of the segmented
object could be used to form a proper convolution kernel for boosting the
segmentation outcome.
</p>
<a href="http://arxiv.org/abs/2007.05103" target="_blank">arXiv:2007.05103</a> [<a href="http://arxiv.org/pdf/2007.05103" target="_blank">pdf</a>]

<h2>Semi-supervised Learning for Aggregated Multilayer Graphs Using Diffuse Interface Methods and Fast Matrix Vector Products. (arXiv:2007.05239v2 [cs.LG] UPDATED)</h2>
<h3>Kai Bergermann, Martin Stoll, Toni Volkmer</h3>
<p>We generalize a graph-based multiclass semi-supervised classification
technique based on diffuse interface methods to multilayer graphs. Besides the
treatment of various applications with an inherent multilayer structure, we
present a very flexible approach that interprets high-dimensional data in a
low-dimensional multilayer graph representation. Highly efficient numerical
methods involving the spectral decomposition of the corresponding differential
graph operators as well as fast matrix-vector products based on the
nonequispaced fast Fourier transform (NFFT) enable the rapid treatment of large
and high-dimensional data sets. We perform various numerical tests putting a
special focus on image segmentation. In particular, we test the performance of
our method on data sets with up to 10 million nodes per layer as well as up to
104 dimensions resulting in graphs with up to 52 layers. While all presented
numerical experiments can be run on an average laptop computer, the linear
dependence per iteration step of the runtime on the network size makes the
algorithm scalable to even larger and higher-dimensional problems.
</p>
<a href="http://arxiv.org/abs/2007.05239" target="_blank">arXiv:2007.05239</a> [<a href="http://arxiv.org/pdf/2007.05239" target="_blank">pdf</a>]

<h2>Closed-Form Factorization of Latent Semantics in GANs. (arXiv:2007.06600v3 [cs.CV] UPDATED)</h2>
<h3>Yujun Shen, Bolei Zhou</h3>
<p>A rich set of interpretable dimensions has been shown to emerge in the latent
space of the Generative Adversarial Networks (GANs) trained for synthesizing
images. In order to identify such latent dimensions for image editing, previous
methods typically annotate a collection of synthesized samples and train linear
classifiers in the latent space. However, they require a clear definition of
the target attribute as well as the corresponding manual annotations, limiting
their applications in practice. In this work, we examine the internal
representation learned by GANs to reveal the underlying variation factors in an
unsupervised manner. In particular, we take a closer look into the generation
mechanism of GANs and further propose a closed-form factorization algorithm for
latent semantic discovery by directly decomposing the pre-trained weights. With
a lightning-fast implementation, our approach is capable of not only finding
semantically meaningful dimensions comparably to the state-of-the-art
supervised methods, but also resulting in far more versatile concepts across
multiple GAN models trained on a wide range of datasets.
</p>
<a href="http://arxiv.org/abs/2007.06600" target="_blank">arXiv:2007.06600</a> [<a href="http://arxiv.org/pdf/2007.06600" target="_blank">pdf</a>]

<h2>Active Learning under Label Shift. (arXiv:2007.08479v2 [cs.LG] UPDATED)</h2>
<h3>Eric Zhao, Anqi Liu, Animashree Anandkumar, Yisong Yue</h3>
<p>We address the problem of active learning under label shift: when the class
proportions of source and target domains differ. We introduce a "medial
distribution" to incorporate a tradeoff between importance weighting and
class-balanced sampling and propose their combined usage in active learning.
Our method is known as Mediated Active Learning under Label Shift (MALLS). It
balances the bias from class-balanced sampling and the variance from importance
weighting. We prove sample complexity and generalization guarantees for MALLS
which show active learning reduces asymptotic sample complexity even under
arbitrary label shift. We empirically demonstrate MALLS scales to
high-dimensional datasets and can reduce the sample complexity of active
learning by 60% in deep active learning tasks.
</p>
<a href="http://arxiv.org/abs/2007.08479" target="_blank">arXiv:2007.08479</a> [<a href="http://arxiv.org/pdf/2007.08479" target="_blank">pdf</a>]

<h2>Referring Expression Comprehension: A Survey of Methods and Datasets. (arXiv:2007.09554v2 [cs.CV] UPDATED)</h2>
<h3>Yanyuan Qiao, Chaorui Deng, Qi Wu</h3>
<p>Referring expression comprehension (REC) aims to localize a target object in
an image described by a referring expression phrased in natural language.
Different from the object detection task that queried object labels have been
pre-defined, the REC problem only can observe the queries during the test. It
thus more challenging than a conventional computer vision problem. This task
has attracted a lot of attention from both computer vision and natural language
processing community, and several lines of work have been proposed, from
CNN-RNN model, modular network to complex graph-based model. In this survey, we
first examine the state of the art by comparing modern approaches to the
problem. We classify methods by their mechanism to encode the visual and
textual modalities. In particular, we examine the common approach of joint
embedding images and expressions to a common feature space. We also discuss
modular architectures and graph-based models that interface with structured
graph representation. In the second part of this survey, we review the datasets
available for training and evaluating REC systems. We then group results
according to the datasets, backbone models, settings so that they can be fairly
compared. Finally, we discuss promising future directions for the field, in
particular the compositional referring expression comprehension that requires
longer reasoning chain to address.
</p>
<a href="http://arxiv.org/abs/2007.09554" target="_blank">arXiv:2007.09554</a> [<a href="http://arxiv.org/pdf/2007.09554" target="_blank">pdf</a>]

<h2>TinyTL: Reduce Memory, Not Parameters for Efficient On-Device Learning. (arXiv:2007.11622v3 [cs.CV] UPDATED)</h2>
<h3>Han Cai, Chuang Gan, Ligeng Zhu, Song Han</h3>
<p>On-device learning enables edge devices to continually adapt the AI models to
new data, which requires a small memory footprint to fit the tight memory
constraint of edge devices. Existing work solves this problem by reducing the
number of trainable parameters. However, this doesn't directly translate to
memory saving since the major bottleneck is the activations, not parameters. In
this work, we present Tiny-Transfer-Learning (TinyTL) for memory-efficient
on-device learning. TinyTL freezes the weights while only learns the bias
modules, thus no need to store the intermediate activations. To maintain the
adaptation capacity, we introduce a new memory-efficient bias module, the lite
residual module, to refine the feature extractor by learning small residual
feature maps adding only 3.8% memory overhead. Extensive experiments show that
TinyTL significantly saves the memory (up to 6.5x) with little accuracy loss
compared to fine-tuning the full network. Compared to fine-tuning the last
layer, TinyTL provides significant accuracy improvements (up to 33.8%) with
little memory overhead. Furthermore, combined with feature extractor
adaptation, TinyTL provides 7.5-12.9x memory saving without sacrificing
accuracy compared to fine-tuning the full Inception-V3.
</p>
<a href="http://arxiv.org/abs/2007.11622" target="_blank">arXiv:2007.11622</a> [<a href="http://arxiv.org/pdf/2007.11622" target="_blank">pdf</a>]

<h2>Distributional Reinforcement Learning via Moment Matching. (arXiv:2007.12354v2 [cs.LG] UPDATED)</h2>
<h3>Thanh Tang Nguyen, Sunil Gupta, Svetha Venkatesh</h3>
<p>We consider the problem of learning a set of probability distributions from
the empirical Bellman dynamics in distributional reinforcement learning (RL), a
class of state-of-the-art methods that estimate the distribution, as opposed to
only the expectation, of the total return. We formulate a method that learns a
finite set of statistics from each return distribution via neural networks, as
in (Bellemare, Dabney, and Munos 2017; Dabney et al. 2018b). Existing
distributional RL methods however constrain the learned statistics to
\emph{predefined} functional forms of the return distribution which is both
restrictive in representation and difficult in maintaining the predefined
statistics. Instead, we learn \emph{unrestricted} statistics, i.e.,
deterministic (pseudo-)samples, of the return distribution by leveraging a
technique from hypothesis testing known as maximum mean discrepancy (MMD),
which leads to a simpler objective amenable to backpropagation. Our method can
be interpreted as implicitly matching all orders of moments between a return
distribution and its Bellman target. We establish sufficient conditions for the
contraction of the distributional Bellman operator and provide finite-sample
analysis for the deterministic samples in distribution approximation.
Experiments on the suite of Atari games show that our method outperforms the
standard distributional RL baselines and sets a new record in the Atari games
for non-distributed agents.
</p>
<a href="http://arxiv.org/abs/2007.12354" target="_blank">arXiv:2007.12354</a> [<a href="http://arxiv.org/pdf/2007.12354" target="_blank">pdf</a>]

<h2>CSER: Communication-efficient SGD with Error Reset. (arXiv:2007.13221v3 [cs.LG] UPDATED)</h2>
<h3>Cong Xie, Shuai Zheng, Oluwasanmi Koyejo, Indranil Gupta, Mu Li, Haibin Lin</h3>
<p>The scalability of Distributed Stochastic Gradient Descent (SGD) is today
limited by communication bottlenecks. We propose a novel SGD variant:
Communication-efficient SGD with Error Reset, or CSER. The key idea in CSER is
first a new technique called "error reset" that adapts arbitrary compressors
for SGD, producing bifurcated local models with periodic reset of resulting
local residual errors. Second we introduce partial synchronization for both the
gradients and the models, leveraging advantages from them. We prove the
convergence of CSER for smooth non-convex problems. Empirical results show that
when combined with highly aggressive compressors, the CSER algorithms
accelerate the distributed training by nearly 10x for CIFAR-100, and by 4.5x
for ImageNet.
</p>
<a href="http://arxiv.org/abs/2007.13221" target="_blank">arXiv:2007.13221</a> [<a href="http://arxiv.org/pdf/2007.13221" target="_blank">pdf</a>]

<h2>Neural Temporal Point Processes For Modelling Electronic Health Records. (arXiv:2007.13794v2 [cs.LG] UPDATED)</h2>
<h3>Joseph Enguehard, Dan Busbridge, Adam Bozson, Claire Woodcock, Nils Y. Hammerla</h3>
<p>The modelling of Electronic Health Records (EHRs) has the potential to drive
more efficient allocation of healthcare resources, enabling early intervention
strategies and advancing personalised healthcare. However, EHRs are challenging
to model due to their realisation as noisy, multi-modal data occurring at
irregular time intervals. To address their temporal nature, we treat EHRs as
samples generated by a Temporal Point Process (TPP), enabling us to model what
happened in an event with when it happened in a principled way. We gather and
propose neural network parameterisations of TPPs, collectively referred to as
Neural TPPs. We perform evaluations on synthetic EHRs as well as on a set of
established benchmarks. We show that TPPs significantly outperform their
non-TPP counterparts on EHRs. We also show that an assumption of many Neural
TPPs, that the class distribution is conditionally independent of time, reduces
performance on EHRs. Finally, our proposed attention-based Neural TPP performs
favourably compared to existing models, whilst aligning with real world
interpretability requirements, an important step towards a component of
clinical decision support systems.
</p>
<a href="http://arxiv.org/abs/2007.13794" target="_blank">arXiv:2007.13794</a> [<a href="http://arxiv.org/pdf/2007.13794" target="_blank">pdf</a>]

<h2>Online neural connectivity estimation with ensemble stimulation. (arXiv:2007.13911v2 [cs.LG] UPDATED)</h2>
<h3>Anne Draelos, Eva A. Naumann, John M. Pearson</h3>
<p>One of the primary goals of systems neuroscience is to relate the structure
of neural circuits to their function, yet patterns of connectivity are
difficult to establish when recording from large populations in behaving
organisms. Many previous approaches have attempted to estimate functional
connectivity between neurons using statistical modeling of observational data,
but these approaches rely heavily on parametric assumptions and are purely
correlational. Recently, however, holographic photostimulation techniques have
made it possible to precisely target selected ensembles of neurons, offering
the possibility of establishing direct causal links. Here, we propose a method
based on noisy group testing that drastically increases the efficiency of this
process in sparse networks. By stimulating small ensembles of neurons, we show
that it is possible to recover binarized network connectivity with a number of
tests that grows only logarithmically with population size under minimal
statistical assumptions. Moreover, we prove that our approach, which reduces to
an efficiently solvable convex optimization problem, can be related to
Variational Bayesian inference on the binary connection weights, and we derive
rigorous bounds on the posterior marginals. This allows us to extend our method
to the streaming setting, where continuously updated posteriors allow for
optional stopping, and we demonstrate the feasibility of inferring connectivity
for networks of up to tens of thousands of neurons online. Finally, we show how
our work can be theoretically linked to compressed sensing approaches, and
compare results for connectivity inference in different settings.
</p>
<a href="http://arxiv.org/abs/2007.13911" target="_blank">arXiv:2007.13911</a> [<a href="http://arxiv.org/pdf/2007.13911" target="_blank">pdf</a>]

<h2>Central object segmentation by deep learning for fruits and other roundish objects. (arXiv:2008.01251v2 [cs.CV] UPDATED)</h2>
<h3>Motohisa Fukuda, Takashi Okuno, Shinya Yuki</h3>
<p>We present CROP (Central Roundish Object Painter), which identifies and
paints the object at the center of an RGB image. Primarily CROP works for
roundish fruits in various illumination conditions, but surprisingly, it could
also deal with images of other organic or inorganic materials, or ones by
optical and electron microscopes, although CROP was trained solely by 172
images of fruits. The method involves image segmentation by deep learning, and
the architecture of the neural network is a deeper version of the original
U-Net. This technique could provide us with a means of automatically collecting
statistical data of fruit growth in farms. As an example, we describe our
experiment of processing 510 time series photos automatically to collect the
data on the size and the position of the target fruit. Our trained neural
network CROP and the above automatic programs are available on GitHub with
user-friendly interface programs.
</p>
<a href="http://arxiv.org/abs/2008.01251" target="_blank">arXiv:2008.01251</a> [<a href="http://arxiv.org/pdf/2008.01251" target="_blank">pdf</a>]

<h2>Towards Sample Efficient Agents through Algorithmic Alignment. (arXiv:2008.03229v4 [cs.AI] UPDATED)</h2>
<h3>Mingxuan Li, Michael L. Littman</h3>
<p>In this work, we propose and explore Deep Graph Value Network (DeepGV) as a
promising method to work around sample complexity in deep
reinforcement-learning agents using a message-passing mechanism. The main idea
is that the agent should be guided by structured non-neural-network algorithms
like dynamic programming. According to recent advances in algorithmic
alignment, neural networks with structured computation procedures can be
trained efficiently. We demonstrate the potential of graph neural network in
supporting sample efficient learning by showing that Deep Graph Value Network
can outperform unstructured baselines by a large margin in solving the Markov
Decision Process (MDP). We believe this would open up a new avenue for
structured agent design. See
https://github.com/drmeerkat/Deep-Graph-Value-Network for the code.
</p>
<a href="http://arxiv.org/abs/2008.03229" target="_blank">arXiv:2008.03229</a> [<a href="http://arxiv.org/pdf/2008.03229" target="_blank">pdf</a>]

<h2>Joint Policy Search for Multi-agent Collaboration with Imperfect Information. (arXiv:2008.06495v5 [cs.LG] UPDATED)</h2>
<h3>Yuandong Tian, Qucheng Gong, Tina Jiang</h3>
<p>To learn good joint policies for multi-agent collaboration with imperfect
information remains a fundamental challenge. While for two-player zero-sum
games, coordinate-ascent approaches (optimizing one agent's policy at a time,
e.g., self-play) work with guarantees, in multi-agent cooperative setting they
often converge to sub-optimal Nash equilibrium. On the other hand, directly
modeling joint policy changes in imperfect information game is nontrivial due
to complicated interplay of policies (e.g., upstream updates affect downstream
state reachability). In this paper, we show global changes of game values can
be decomposed to policy changes localized at each information set, with a novel
term named policy-change density. Based on this, we propose Joint Policy
Search(JPS) that iteratively improves joint policies of collaborative agents in
imperfect information games, without re-evaluating the entire game. On
multi-agent collaborative tabular games, JPS is proven to never worsen
performance and can improve solutions provided by unilateral approaches (e.g,
CFR), outperforming algorithms designed for collaborative policy learning (e.g.
BAD). Furthermore, for real-world games, JPS has an online form that naturally
links with gradient updates. We test it to Contract Bridge, a 4-player
imperfect-information game where a team of $2$ collaborates to compete against
the other. In its bidding phase, players bid in turn to find a good contract
through a limited information channel. Based on a strong baseline agent that
bids competitive bridge purely through domain-agnostic self-play, JPS improves
collaboration of team players and outperforms WBridge5, a championship-winning
software, by $+0.63$ IMPs (International Matching Points) per board over 1k
games, substantially better than previous SoTA ($+0.41$ IMPs/b) under
Double-Dummy evaluation.
</p>
<a href="http://arxiv.org/abs/2008.06495" target="_blank">arXiv:2008.06495</a> [<a href="http://arxiv.org/pdf/2008.06495" target="_blank">pdf</a>]

<h2>EGO-Planner: An ESDF-free Gradient-based Local Planner for Quadrotors. (arXiv:2008.08835v2 [cs.RO] UPDATED)</h2>
<h3>Xin Zhou, Zhepei Wang, Hongkai Ye, Chao Xu, Fei Gao</h3>
<p>Gradient-based planners are widely used for quadrotor local planning, in
which a Euclidean Signed Distance Field (ESDF) is crucial for evaluating
gradient magnitude and direction. Nevertheless, computing such a field has much
redundancy since the trajectory optimization procedure only covers a very
limited subspace of the ESDF updating range. In this paper, an ESDF-free
gradient-based planning framework is proposed, which significantly reduces
computation time. The main improvement is that the collision term in the
penalty function is formulated by comparing the colliding trajectory with a
collision-free guiding path. The resulting obstacle information will be stored
only if the trajectory hits new obstacles, making the planner only extract
necessary obstacle information. Then, we lengthen the time allocation if
dynamical feasibility is violated. An anisotropic curve fitting algorithm is
introduced to adjust higher-order derivatives of the trajectory while
maintaining the original shape. Benchmark comparisons and real-world
experiments verify its robustness and high-performance. The source code is
released as ROS packages.
</p>
<a href="http://arxiv.org/abs/2008.08835" target="_blank">arXiv:2008.08835</a> [<a href="http://arxiv.org/pdf/2008.08835" target="_blank">pdf</a>]

<h2>Memory-based Jitter: Improving Visual Recognition on Long-tailed Data with Diversity In Memory. (arXiv:2008.09809v4 [cs.CV] UPDATED)</h2>
<h3>Jialun Liu, Jingwei Zhang, Wenhui Li, Chi Zhang, Yifan Sun</h3>
<p>This paper considers deep visual recognition on long-tailed data. To be
general, we consider two applied scenarios, \ie, deep classification and deep
metric learning. Under the long-tailed data distribution, the majority classes
(\ie, tail classes) only occupy relatively few samples and are prone to lack of
within-class diversity. A radical solution is to augment the tail classes with
higher diversity. To this end, we introduce a simple and reliable method named
Memory-based Jitter (MBJ). We observe that during training, the deep model
constantly changes its parameters after every iteration, yielding the
phenomenon of \emph{weight jitters}. Consequentially, given a same image as the
input, two historical editions of the model generate two different features in
the deeply-embedded space, resulting in \emph{feature jitters}. Using a memory
bank, we collect these (model or feature) jitters across multiple training
iterations and get the so-called Memory-based Jitter. The accumulated jitters
enhance the within-class diversity for the tail classes and consequentially
improves long-tailed visual recognition. With slight modifications, MBJ is
applicable for two fundamental visual recognition tasks, \emph{i.e.}, deep
image classification and deep metric learning (on long-tailed data). Extensive
experiments on five long-tailed classification benchmarks and two deep metric
learning benchmarks demonstrate significant improvement. Moreover, the achieved
performance are on par with the state of the art on both tasks.
</p>
<a href="http://arxiv.org/abs/2008.09809" target="_blank">arXiv:2008.09809</a> [<a href="http://arxiv.org/pdf/2008.09809" target="_blank">pdf</a>]

<h2>Seesaw Loss for Long-Tailed Instance Segmentation. (arXiv:2008.10032v2 [cs.CV] UPDATED)</h2>
<h3>Jiaqi Wang, Wenwei Zhang, Yuhang Zang, Yuhang Cao, Jiangmiao Pang, Tao Gong, Kai Chen, Ziwei Liu, Chen Change Loy, Dahua Lin</h3>
<p>Instance segmentation has witnessed a remarkable progress on class-balanced
benchmarks. However, they fail to perform as accurately in real-world
scenarios, where the category distribution of objects naturally comes with a
long tail. Instances of head classes dominate a long-tailed dataset and they
serve as negative samples of tail categories. The overwhelming gradients of
negative samples on tail classes lead to a biased learning process for
classifiers. Consequently, objects of tail categories are more likely to be
misclassified as backgrounds or head categories. To tackle this problem, we
propose Seesaw Loss to dynamically re-balance gradients of positive and
negative samples for each category, with two complementary factors, i.e.,
mitigation factor and compensation factor. The mitigation factor reduces
punishments to tail categories w.r.t. the ratio of cumulative training
instances between different categories. Meanwhile, the compensation factor
increases the penalty of misclassified instances to avoid false positives of
tail categories. We conduct extensive experiments on Seesaw Loss with
mainstream frameworks and different data sampling strategies. With a simple
end-to-end training pipeline, Seesaw Loss obtains significant gains over
Cross-Entropy Loss, and achieves state-of-the-art performance on LVIS dataset
without bells and whistles.
</p>
<a href="http://arxiv.org/abs/2008.10032" target="_blank">arXiv:2008.10032</a> [<a href="http://arxiv.org/pdf/2008.10032" target="_blank">pdf</a>]

<h2>Canonical 3D Deformer Maps: Unifying parametric and non-parametric methods for dense weakly-supervised category reconstruction. (arXiv:2008.12709v2 [cs.CV] UPDATED)</h2>
<h3>David Novotny, Roman Shapovalov, Andrea Vedaldi</h3>
<p>We propose the Canonical 3D Deformer Map, a new representation of the 3D
shape of common object categories that can be learned from a collection of 2D
images of independent objects. Our method builds in a novel way on concepts
from parametric deformation models, non-parametric 3D reconstruction, and
canonical embeddings, combining their individual advantages. In particular, it
learns to associate each image pixel with a deformation model of the
corresponding 3D object point which is canonical, i.e. intrinsic to the
identity of the point and shared across objects of the category. The result is
a method that, given only sparse 2D supervision at training time, can, at test
time, reconstruct the 3D shape and texture of objects from single views, while
establishing meaningful dense correspondences between object instances. It also
achieves state-of-the-art results in dense 3D reconstruction on public
in-the-wild datasets of faces, cars, and birds.
</p>
<a href="http://arxiv.org/abs/2008.12709" target="_blank">arXiv:2008.12709</a> [<a href="http://arxiv.org/pdf/2008.12709" target="_blank">pdf</a>]

<h2>Beyond variance reduction: Understanding the true impact of baselines on policy optimization. (arXiv:2008.13773v2 [cs.LG] UPDATED)</h2>
<h3>Wesley Chung, Valentin Thomas, Marlos C. Machado, Nicolas Le Roux</h3>
<p>Policy gradients methods are a popular and effective choice to train
reinforcement learning agents in complex environments. The variance of the
stochastic policy gradient is often seen as a key quantity to determine the
effectiveness of the algorithm. Baselines are a common addition to reduce the
variance of the gradient, but previous works have hardly ever considered other
effects baselines may have on the optimization process. Using simple examples,
we find that baselines modify the optimization dynamics even when the variance
is the same. In certain cases, a baseline with lower variance may even be worse
than another with higher variance. Furthermore, we find that the choice of
baseline can affect the convergence of natural policy gradient, where certain
baselines may lead to convergence to a suboptimal policy for any stepsize. Such
behaviour emerges when sampling is constrained to be done using the current
policy and we show how decoupling the sampling policy from the current policy
guarantees convergence for a much wider range of baselines. More broadly, this
work suggests that a more careful treatment of stochasticity in the updates --
beyond the immediate variance -- is necessary to understand the optimization
process of policy gradient algorithms.
</p>
<a href="http://arxiv.org/abs/2008.13773" target="_blank">arXiv:2008.13773</a> [<a href="http://arxiv.org/pdf/2008.13773" target="_blank">pdf</a>]

<h2>Devil's in the Details: Aligning Visual Clues for Conditional Embedding in Person Re-Identification. (arXiv:2009.05250v2 [cs.CV] UPDATED)</h2>
<h3>Fufu Yu, Xinyang Jiang, Yifei Gong, Shizhen Zhao, Xiaowei Guo, Wei-Shi Zheng, Feng Zheng, Xing Sun</h3>
<p>Although Person Re-Identification has made impressive progress, difficult
cases like occlusion, change of view-pointand similar clothing still bring
great challenges. Besides overall visual features, matching and comparing
detailed information is also essential for tackling these challenges. This
paper proposes two key recognition patterns to better utilize the detail
information of pedestrian images, that most of the existing methods are unable
to satisfy. Firstly, Visual Clue Alignment requires the model to select and
align decisive regions pairs from two images for pair-wise comparison, while
existing methods only align regions with predefined rules like high feature
similarity or same semantic labels. Secondly, the Conditional Feature Embedding
requires the overall feature of a query image to be dynamically adjusted based
on the gallery image it matches, while most of the existing methods ignore the
reference images. By introducing novel techniques including correspondence
attention module and discrepancy-based GCN, we propose an end-to-end ReID
method that integrates both patterns into a unified framework, called
CACE-Net((C)lue(A)lignment and (C)onditional (E)mbedding). The experiments show
that CACE-Net achieves state-of-the-art performance on three public datasets.
</p>
<a href="http://arxiv.org/abs/2009.05250" target="_blank">arXiv:2009.05250</a> [<a href="http://arxiv.org/pdf/2009.05250" target="_blank">pdf</a>]

<h2>A Survey of FPGA-Based Robotic Computing. (arXiv:2009.06034v2 [cs.RO] UPDATED)</h2>
<h3>Zishen Wan, Bo Yu, Thomas Yuang Li, Jie Tang, Yuhao Zhu, Yu Wang, Arijit Raychowdhury, Shaoshan Liu</h3>
<p>Recent researches on robotics have shown significant improvement, spanning
from algorithms, mechanics to hardware architectures. Robotics, including
manipulators, legged robots, drones, and autonomous vehicles, are now widely
applied in diverse scenarios. However, the high computation and data complexity
of robotic algorithms pose great challenges to its applications. On the one
hand, CPU platform is flexible to handle multiple robotic tasks. GPU platform
has higher computational capacities and easy-touse development frameworks, so
they have been widely adopted in several applications. On the other hand,
FPGA-based robotic accelerators are becoming increasingly competitive
alternatives, especially in latency-critical and power-limited scenarios. With
specialized designed hardware logic and algorithm kernels, FPGA-based
accelerators can surpass CPU and GPU in performance and energy efficiency. In
this paper, we give an overview of previous work on FPGA-based robotic
accelerators covering different stages of the robotic system pipeline. An
analysis of software and hardware optimization techniques and main technical
issues is presented, along with some commercial and space applications, to
serve as a guide for future work.
</p>
<a href="http://arxiv.org/abs/2009.06034" target="_blank">arXiv:2009.06034</a> [<a href="http://arxiv.org/pdf/2009.06034" target="_blank">pdf</a>]

<h2>Constrained Labeling for Weakly Supervised Learning. (arXiv:2009.07360v2 [cs.LG] UPDATED)</h2>
<h3>Chidubem Arachie, Bert Huang</h3>
<p>Curation of large fully supervised datasets has become one of the major
roadblocks for machine learning. Weak supervision provides an alternative to
supervised learning by training with cheap, noisy, and possibly correlated
labeling functions from varying sources. The key challenge in weakly supervised
learning is combining the different weak supervision signals while navigating
misleading correlations in their errors. In this paper, we propose a simple
data-free approach for combining weak supervision signals by defining a
constrained space for the possible labels of the weak signals and training with
a random labeling within this constrained space. Our method is efficient and
stable, converging after a few iterations of gradient descent. We prove
theoretical conditions under which the worst-case error of the randomized label
decreases with the rank of the linear constraints. We show experimentally that
our method outperforms other weak supervision methods on various text- and
image-classification tasks.
</p>
<a href="http://arxiv.org/abs/2009.07360" target="_blank">arXiv:2009.07360</a> [<a href="http://arxiv.org/pdf/2009.07360" target="_blank">pdf</a>]

<h2>Collaborative Group Learning. (arXiv:2009.07712v2 [cs.LG] UPDATED)</h2>
<h3>Shaoxiong Feng, Hongshen Chen, Xuancheng Ren, Zhuoye Ding, Kan Li, Xu Sun</h3>
<p>Collaborative learning has successfully applied knowledge transfer to guiding
a pool of small student networks towards robust local minima. However, previous
approaches typically struggle with drastically aggravated student
homogenization and rapidly growing computational complexity when the number of
students rises. In this paper, we propose Collaborative Group Learning, an
efficient framework that aims to maximize student population without
sacrificing generalization performance and computational efficiency. First,
each student is established by randomly routing on a modular neural network,
which is not only parameter-efficient but also facilitates flexible knowledge
communication between students due to random levels of representation sharing
and branching. Second, to resist homogenization and further reduce the
computational cost, students first compose diverse feature sets by exploiting
the inductive bias from sub-sets of training data, and then aggregate and
distill supplementary knowledge by choosing a random sub-group of students at
each time step. Empirical evaluations on both image and text tasks indicate
that our method significantly outperforms various state-of-the-art
collaborative approaches whilst enhancing computational efficiency.
</p>
<a href="http://arxiv.org/abs/2009.07712" target="_blank">arXiv:2009.07712</a> [<a href="http://arxiv.org/pdf/2009.07712" target="_blank">pdf</a>]

<h2>A Comprehensive Review for MRF and CRF Approaches in Pathology Image Analysis. (arXiv:2009.13721v2 [cs.CV] UPDATED)</h2>
<h3>Chen Li, Yixin Li, Kai Wang, Md Mamunur Rahaman, Xiaoyan Li, Changhao Sun, Hao Chen, Xinran Wu, Hong Zhang, Qian Wang</h3>
<p>Pathology image analysis is an essential procedure for clinical diagnosis of
many diseases. To boost the accuracy and objectivity of detection, nowadays, an
increasing number of computer-aided diagnosis (CAD) system is proposed. Among
these methods, random field models play an indispensable role in improving the
analysis performance. In this review, we present a comprehensive overview of
pathology image analysis based on the markov random fields (MRFs) and
conditional random fields (CRFs), which are two popular random field models.
Firstly, we introduce the background of two random fields and pathology images.
Secondly, we summarize the basic mathematical knowledge of MRFs and CRFs from
modelling to optimization. Then, a thorough review of the recent research on
the MRFs and CRFs of pathology images analysis is presented. Finally, we
investigate the popular methodologies in the related works and discuss the
method migration among CAD field.
</p>
<a href="http://arxiv.org/abs/2009.13721" target="_blank">arXiv:2009.13721</a> [<a href="http://arxiv.org/pdf/2009.13721" target="_blank">pdf</a>]

<h2>Robustness Analysis of Neural Networks via Efficient Partitioning with Applications in Control Systems. (arXiv:2010.00540v2 [cs.LG] UPDATED)</h2>
<h3>Michael Everett, Golnaz Habibi, Jonathan P. How</h3>
<p>Neural networks (NNs) are now routinely implemented on systems that must
operate in uncertain environments, but the tools for formally analyzing how
this uncertainty propagates to NN outputs are not yet commonplace. Computing
tight bounds on NN output sets (given an input set) provides a measure of
confidence associated with the NN decisions and is essential to deploy NNs on
safety-critical systems. Recent works approximate the propagation of sets
through nonlinear activations or partition the uncertainty set to provide a
guaranteed outer bound on the set of possible NN outputs. However, the bound
looseness causes excessive conservatism and/or the computation is too slow for
online analysis. This paper unifies propagation and partition approaches to
provide a family of robustness analysis algorithms that give tighter bounds
than existing works for the same amount of computation time (or reduced
computational effort for a desired accuracy level). Moreover, we provide new
partitioning techniques that are aware of their current bound estimates and
desired boundary shape (e.g., lower bounds, weighted $\ell_\infty$-ball, convex
hull), leading to further improvements in the computation-tightness tradeoff.
The paper demonstrates the tighter bounds and reduced conservatism of the
proposed robustness analysis framework with examples from model-free RL and
forward kinematics learning.
</p>
<a href="http://arxiv.org/abs/2010.00540" target="_blank">arXiv:2010.00540</a> [<a href="http://arxiv.org/pdf/2010.00540" target="_blank">pdf</a>]

<h2>Understanding Approximate Fisher Information for Fast Convergence of Natural Gradient Descent in Wide Neural Networks. (arXiv:2010.00879v3 [stat.ML] UPDATED)</h2>
<h3>Ryo Karakida, Kazuki Osawa</h3>
<p>Natural Gradient Descent (NGD) helps to accelerate the convergence of
gradient descent dynamics, but it requires approximations in large-scale deep
neural networks because of its high computational cost. Empirical studies have
confirmed that some NGD methods with approximate Fisher information converge
sufficiently fast in practice. Nevertheless, it remains unclear from the
theoretical perspective why and under what conditions such heuristic
approximations work well. In this work, we reveal that, under specific
conditions, NGD with approximate Fisher information achieves the same fast
convergence to global minima as exact NGD. We consider deep neural networks in
the infinite-width limit, and analyze the asymptotic training dynamics of NGD
in function space via the neural tangent kernel. In the function space, the
training dynamics with the approximate Fisher information are identical to
those with the exact Fisher information, and they converge quickly. The fast
convergence holds in layer-wise approximations; for instance, in block diagonal
approximation where each block corresponds to a layer as well as in block
tri-diagonal and K-FAC approximations. We also find that a unit-wise
approximation achieves the same fast convergence under some assumptions. All of
these different approximations have an isotropic gradient in the function
space, and this plays a fundamental role in achieving the same convergence
properties in training. Thus, the current study gives a novel and unified
theoretical foundation with which to understand NGD methods in deep learning.
</p>
<a href="http://arxiv.org/abs/2010.00879" target="_blank">arXiv:2010.00879</a> [<a href="http://arxiv.org/pdf/2010.00879" target="_blank">pdf</a>]

<h2>On the linearity of large non-linear models: when and why the tangent kernel is constant. (arXiv:2010.01092v2 [cs.LG] UPDATED)</h2>
<h3>Chaoyue Liu, Libin Zhu, Mikhail Belkin</h3>
<p>The goal of this work is to shed light on the remarkable phenomenon of
transition to linearity of certain neural networks as their width approaches
infinity. We show that the transition to linearity of the model and,
equivalently, constancy of the (neural) tangent kernel (NTK) result from the
scaling properties of the norm of the Hessian matrix of the network as a
function of the network width. We present a general framework for understanding
the constancy of the tangent kernel via Hessian scaling applicable to the
standard classes of neural networks. Our analysis provides a new perspective on
the phenomenon of constant tangent kernel, which is different from the widely
accepted "lazy training". Furthermore, we show that the transition to linearity
is not a general property of wide neural networks and does not hold when the
last layer of the network is non-linear. It is also not necessary for
successful optimization by gradient descent.
</p>
<a href="http://arxiv.org/abs/2010.01092" target="_blank">arXiv:2010.01092</a> [<a href="http://arxiv.org/pdf/2010.01092" target="_blank">pdf</a>]

<h2>LEGAN: Disentangled Manipulation of Directional Lighting and Facial Expressions by Leveraging Human Perceptual Judgements. (arXiv:2010.01464v2 [cs.CV] UPDATED)</h2>
<h3>Sandipan Banerjee, Ajjen Joshi, Prashant Mahajan, Sneha Bhattacharya, Survi Kyal, Taniya Mishra</h3>
<p>Building facial analysis systems that generalize to extreme variations in
lighting and facial expressions is a challenging problem that can potentially
be alleviated using natural-looking synthetic data. Towards that, we propose
LEGAN, a novel synthesis framework that leverages perceptual quality judgments
for jointly manipulating lighting and expressions in face images, without
requiring paired training data. LEGAN disentangles the lighting and expression
subspaces and performs transformations in the feature space before upscaling to
the desired output image. The fidelity of the synthetic image is further
refined by integrating a perceptual quality estimation model, trained with face
images rendered using multiple synthesis methods and their crowd-sourced
naturalness ratings, into the LEGAN framework as an auxiliary discriminator.
Using objective metrics like FID and LPIPS, LEGAN is shown to generate higher
quality face images when compared with popular GAN models like StarGAN and
StarGAN-v2 for lighting and expression synthesis. We also conduct a perceptual
study using images synthesized by LEGAN and other GAN models and show the
correlation between our quality estimation and visual fidelity. Finally, we
demonstrate the effectiveness of LEGAN as training data augmenter for
expression recognition and face verification tasks.
</p>
<a href="http://arxiv.org/abs/2010.01464" target="_blank">arXiv:2010.01464</a> [<a href="http://arxiv.org/pdf/2010.01464" target="_blank">pdf</a>]

<h2>Robust Multi-class Feature Selection via $l_{2,0}$-Norm Regularization Minimization. (arXiv:2010.03728v3 [cs.LG] UPDATED)</h2>
<h3>Zhenzhen Sun, Yuanlong Yu</h3>
<p>Feature selection is an important data pre-processing in data mining and
machine learning, which can reduce feature size without deteriorating model's
performance. Recently, sparse regression based feature selection methods have
received considerable attention due to their good performance. However, because
the $l_{2,0}$-norm regularization term is non-convex, this problem is very hard
to solve. In this paper, unlike most of the other methods which only solve the
approximate problem, a novel method based on homotopy iterative hard threshold
(HIHT) is proposed to solve the $l_{2,0}$-norm regularization least square
problem directly for multi-class feature selection, which can produce exact
row-sparsity solution for the weights matrix. What'more, in order to reduce the
computational time of HIHT, an acceleration version of HIHT (AHIHT) is derived.
Extensive experiments on eight biological datasets show that the proposed
method can achieve higher classification accuracy (ACC) with fewest number of
selected features (No.fea) comparing with the approximate convex counterparts
and state-of-the-art feature selection methods. The robustness of
classification accuracy to the regularization parameter and the number of
selected feature are also exhibited.
</p>
<a href="http://arxiv.org/abs/2010.03728" target="_blank">arXiv:2010.03728</a> [<a href="http://arxiv.org/pdf/2010.03728" target="_blank">pdf</a>]

<h2>DiffTune: Optimizing CPU Simulator Parameters with Learned Differentiable Surrogates. (arXiv:2010.04017v2 [cs.LG] UPDATED)</h2>
<h3>Alex Renda, Yishen Chen, Charith Mendis, Michael Carbin</h3>
<p>CPU simulators are useful tools for modeling CPU execution behavior. However,
they suffer from inaccuracies due to the cost and complexity of setting their
fine-grained parameters, such as the latencies of individual instructions. This
complexity arises from the expertise required to design benchmarks and
measurement frameworks that can precisely measure the values of parameters at
such fine granularity. In some cases, these parameters do not necessarily have
a physical realization and are therefore fundamentally approximate, or even
unmeasurable.

In this paper we present DiffTune, a system for learning the parameters of
x86 basic block CPU simulators from coarse-grained end-to-end measurements.
Given a simulator, DiffTune learns its parameters by first replacing the
original simulator with a differentiable surrogate, another function that
approximates the original function; by making the surrogate differentiable,
DiffTune is then able to apply gradient-based optimization techniques even when
the original function is non-differentiable, such as is the case with CPU
simulators. With this differentiable surrogate, DiffTune then applies
gradient-based optimization to produce values of the simulator's parameters
that minimize the simulator's error on a dataset of ground truth end-to-end
performance measurements. Finally, the learned parameters are plugged back into
the original simulator.

DiffTune is able to automatically learn the entire set of
microarchitecture-specific parameters within the Intel x86 simulation model of
llvm-mca, a basic block CPU simulator based on LLVM's instruction scheduling
model. DiffTune's learned parameters lead llvm-mca to an average error that not
only matches but lowers that of its original, expert-provided parameter values.
</p>
<a href="http://arxiv.org/abs/2010.04017" target="_blank">arXiv:2010.04017</a> [<a href="http://arxiv.org/pdf/2010.04017" target="_blank">pdf</a>]

<h2>Fast Convergence of Langevin Dynamics on Manifold: Geodesics meet Log-Sobolev. (arXiv:2010.05263v2 [cs.LG] UPDATED)</h2>
<h3>Xiao Wang, Qi Lei, Ioannis Panageas</h3>
<p>Sampling is a fundamental and arguably very important task with numerous
applications in Machine Learning. One approach to sample from a high
dimensional distribution $e^{-f}$ for some function $f$ is the Langevin
Algorithm (LA). Recently, there has been a lot of progress in showing fast
convergence of LA even in cases where $f$ is non-convex, notably [53], [39] in
which the former paper focuses on functions $f$ defined in $\mathbb{R}^n$ and
the latter paper focuses on functions with symmetries (like matrix completion
type objectives) with manifold structure. Our work generalizes the results of
[53] where $f$ is defined on a manifold $M$ rather than $\mathbb{R}^n$. From
technical point of view, we show that KL decreases in a geometric rate whenever
the distribution $e^{-f}$ satisfies a log-Sobolev inequality on $M$.
</p>
<a href="http://arxiv.org/abs/2010.05263" target="_blank">arXiv:2010.05263</a> [<a href="http://arxiv.org/pdf/2010.05263" target="_blank">pdf</a>]

<h2>Structural Forecasting for Tropical Cyclone Intensity Prediction: Providing Insight with Deep Learning. (arXiv:2010.05783v3 [cs.LG] UPDATED)</h2>
<h3>Trey McNeely, Niccol&#xf2; Dalmasso, Kimberly M. Wood, Ann B. Lee</h3>
<p>Tropical cyclone (TC) intensity forecasts are ultimately issued by human
forecasters. The human in-the-loop pipeline requires that any forecasting
guidance must be easily digestible by TC experts if it is to be adopted at
operational centers like the National Hurricane Center. Our proposed framework
leverages deep learning to provide forecasters with something neither
end-to-end prediction models nor traditional intensity guidance does: a
powerful tool for monitoring high-dimensional time series of key physically
relevant predictors and the means to understand how the predictors relate to
one another and to short-term intensity changes.
</p>
<a href="http://arxiv.org/abs/2010.05783" target="_blank">arXiv:2010.05783</a> [<a href="http://arxiv.org/pdf/2010.05783" target="_blank">pdf</a>]

<h2>Can Federated Learning Save The Planet?. (arXiv:2010.06537v4 [cs.LG] UPDATED)</h2>
<h3>Xinchi Qiu, Titouan Parcolle, Daniel J. Beutel, Taner Topal, Akhil Mathur, Nicholas D. Lane</h3>
<p>Despite impressive results, deep learning-based technologies also raise
severe privacy and environmental concerns induced by the training procedure
often conducted in data centers. In response, alternatives to centralized
training such as Federated Learning (FL) have emerged. Perhaps unexpectedly, FL
in particular is starting to be deployed at a global scale by companies that
must adhere to new legal demands and policies originating from governments and
the civil society for privacy protection. However, the potential environmental
impact related to FL remains unclear and unexplored. This paper offers the
first-ever systematic study of the carbon footprint of FL. First, we propose a
rigorous model to quantify the carbon footprint, hence facilitating the
investigation of the relationship between FL design and carbon emissions. Then,
we compare the carbon footprint of FL to traditional centralized learning.
Finally, we highlight and connect the reported results to the future challenges
and trends in FL to reduce its environmental impact, including algorithms
efficiency, hardware capabilities, and stronger industry transparency.
</p>
<a href="http://arxiv.org/abs/2010.06537" target="_blank">arXiv:2010.06537</a> [<a href="http://arxiv.org/pdf/2010.06537" target="_blank">pdf</a>]

<h2>Probabilistic Time Series Forecasting with Structured Shape and Temporal Diversity. (arXiv:2010.07349v2 [stat.ML] UPDATED)</h2>
<h3>Vincent Le Guen, Nicolas Thome</h3>
<p>Probabilistic forecasting consists in predicting a distribution of possible
future outcomes. In this paper, we address this problem for non-stationary time
series, which is very challenging yet crucially important. We introduce the
STRIPE model for representing structured diversity based on shape and time
features, ensuring both probable predictions while being sharp and accurate.
STRIPE is agnostic to the forecasting model, and we equip it with a
diversification mechanism relying on determinantal point processes (DPP). We
introduce two DPP kernels for modeling diverse trajectories in terms of shape
and time, which are both differentiable and proved to be positive
semi-definite. To have an explicit control on the diversity structure, we also
design an iterative sampling mechanism to disentangle shape and time
representations in the latent space. Experiments carried out on synthetic
datasets show that STRIPE significantly outperforms baseline methods for
representing diversity, while maintaining accuracy of the forecasting model. We
also highlight the relevance of the iterative sampling scheme and the
importance to use different criteria for measuring quality and diversity.
Finally, experiments on real datasets illustrate that STRIPE is able to
outperform state-of-the-art probabilistic forecasting approaches in the best
sample prediction.
</p>
<a href="http://arxiv.org/abs/2010.07349" target="_blank">arXiv:2010.07349</a> [<a href="http://arxiv.org/pdf/2010.07349" target="_blank">pdf</a>]

<h2>Importance Reweighting for Biquality Learning. (arXiv:2010.09621v2 [cs.LG] UPDATED)</h2>
<h3>Pierre Nodet, Vincent Lemaire, Alexis Bondu, Antoine Cornu&#xe9;jols</h3>
<p>The field of Weakly Supervised Learning (WSL) has recently seen a surge of
popularity, with numerous papers addressing different types of "supervision
deficiencies", namely: poor quality, non adaptability, and insufficient
quantity of labels. Regarding quality, label noise can be of different types,
including completely-at-random, at-random or even not-at-random. All these
kinds of label noise are addressed separately in the literature, leading to
highly specialized approaches. This paper proposes an original, encompassing,
view of Weakly Supervised Learning, which results in the design of generic
approaches capable of dealing with any kind of label noise. For this purpose,
an alternative setting called "Biquality data" is used. It assumes that a small
trusted dataset of correctly labeled examples is available, in addition to an
untrusted dataset of noisy examples. In this paper, we propose a new
reweigthing scheme capable of identifying noncorrupted examples in the
untrusted dataset. This allows one to learn classifiers using both datasets.
Extensive experiments that simulate several types of label noise and that vary
the quality and quantity of untrusted examples, demonstrate that the proposed
approach outperforms baselines and state-of-the-art approaches.
</p>
<a href="http://arxiv.org/abs/2010.09621" target="_blank">arXiv:2010.09621</a> [<a href="http://arxiv.org/pdf/2010.09621" target="_blank">pdf</a>]

<h2>An explainable deep vision system for animal classification and detection in trail-camera images with automatic post-deployment retraining. (arXiv:2010.11472v2 [cs.CV] UPDATED)</h2>
<h3>Golnaz Moallem (1), Don D. Pathirage (1), Joel Reznick (1), James Gallagher (2), Hamed Sari-Sarraf (1) ((1) Applied Vision Lab Texas Tech University (2) Texas Parks and Wildlife Department)</h3>
<p>This paper introduces an automated vision system for animal detection in
trail-camera images taken from a field under the administration of the Texas
Parks and Wildlife Department. As traditional wildlife counting techniques are
intrusive and labor intensive to conduct, trail-camera imaging is a
comparatively non-intrusive method for capturing wildlife activity. However,
given the large volume of images produced from trail-cameras, manual analysis
of the images remains time-consuming and inefficient. We implemented a
two-stage deep convolutional neural network pipeline to find animal-containing
images in the first stage and then process these images to detect birds in the
second stage. The animal classification system classifies animal images with
overall 93% sensitivity and 96% specificity. The bird detection system achieves
better than 93% sensitivity, 92% specificity, and 68% average
Intersection-over-Union rate. The entire pipeline processes an image in less
than 0.5 seconds as opposed to an average 30 seconds for a human labeler. We
also addressed post-deployment issues related to data drift for the animal
classification system as image features vary with seasonal changes. This system
utilizes an automatic retraining algorithm to detect data drift and update the
system. We introduce a novel technique for detecting drifted images and
triggering the retraining procedure. Two statistical experiments are also
presented to explain the prediction behavior of the animal classification
system. These experiments investigate the cues that steers the system towards a
particular decision. Statistical hypothesis testing demonstrates that the
presence of an animal in the input image significantly contributes to the
system's decisions.
</p>
<a href="http://arxiv.org/abs/2010.11472" target="_blank">arXiv:2010.11472</a> [<a href="http://arxiv.org/pdf/2010.11472" target="_blank">pdf</a>]

<h2>Deep Reinforcement Learning with Stacked Hierarchical Attention for Text-based Games. (arXiv:2010.11655v2 [cs.LG] UPDATED)</h2>
<h3>Yunqiu Xu, Meng Fang, Ling Chen, Yali Du, Joey Tianyi Zhou, Chengqi Zhang</h3>
<p>We study reinforcement learning (RL) for text-based games, which are
interactive simulations in the context of natural language. While different
methods have been developed to represent the environment information and
language actions, existing RL agents are not empowered with any reasoning
capabilities to deal with textual games. In this work, we aim to conduct
explicit reasoning with knowledge graphs for decision making, so that the
actions of an agent are generated and supported by an interpretable inference
procedure. We propose a stacked hierarchical attention mechanism to construct
an explicit representation of the reasoning process by exploiting the structure
of the knowledge graph. We extensively evaluate our method on a number of
man-made benchmark games, and the experimental results demonstrate that our
method performs better than existing text-based agents.
</p>
<a href="http://arxiv.org/abs/2010.11655" target="_blank">arXiv:2010.11655</a> [<a href="http://arxiv.org/pdf/2010.11655" target="_blank">pdf</a>]

<h2>XLVIN: eXecuted Latent Value Iteration Nets. (arXiv:2010.13146v2 [cs.LG] UPDATED)</h2>
<h3>Andreea Deac, Petar Veli&#x10d;kovi&#x107;, Ognjen Milinkovi&#x107;, Pierre-Luc Bacon, Jian Tang, Mladen Nikoli&#x107;</h3>
<p>Value Iteration Networks (VINs) have emerged as a popular method to
incorporate planning algorithms within deep reinforcement learning, enabling
performance improvements on tasks requiring long-range reasoning and
understanding of environment dynamics. This came with several limitations,
however: the model is not incentivised in any way to perform meaningful
planning computations, the underlying state space is assumed to be discrete,
and the Markov decision process (MDP) is assumed fixed and known. We propose
eXecuted Latent Value Iteration Networks (XLVINs), which combine recent
developments across contrastive self-supervised learning, graph representation
learning and neural algorithmic reasoning to alleviate all of the above
limitations, successfully deploying VIN-style models on generic environments.
XLVINs match the performance of VIN-like models when the underlying MDP is
discrete, fixed and known, and provides significant improvements to model-free
baselines across three general MDP setups.
</p>
<a href="http://arxiv.org/abs/2010.13146" target="_blank">arXiv:2010.13146</a> [<a href="http://arxiv.org/pdf/2010.13146" target="_blank">pdf</a>]

<h2>EDNet: Efficient Disparity Estimation with Cost Volume Combination and Attention-based Spatial Residual. (arXiv:2010.13338v3 [cs.CV] UPDATED)</h2>
<h3>Songyan Zhang, Zhicheng Wang, Qiang Wang, Jinshuo Zhang, Gang Wei, Xiaowen Chu</h3>
<p>Existing state-of-the-art disparity estimation works mostly leverage the 4D
concatenation volume and construct a very deep 3D convolution neural network
(CNN) for disparity regression, which is inefficient due to the high memory
consumption and slow inference speed. In this paper, we propose a network named
EDNet for efficient disparity estimation. Firstly, we construct a combined
volume which incorporates contextual information from the squeezed
concatenation volume and feature similarity measurement from the correlation
volume. The combined volume can be next aggregated by 2D convolutions which are
faster and require less memory than 3D convolutions. Secondly, we propose an
attention-based spatial residual module to generate attention-aware residual
features. The attention mechanism is applied to provide intuitive spatial
evidence about inaccurate regions with the help of error maps at multiple
scales and thus improve the residual learning efficiency. Extensive experiments
on the Scene Flow and KITTI datasets show that EDNet outperforms the previous
3D CNN based works and achieves state-of-the-art performance with significantly
faster speed and less memory consumption.
</p>
<a href="http://arxiv.org/abs/2010.13338" target="_blank">arXiv:2010.13338</a> [<a href="http://arxiv.org/pdf/2010.13338" target="_blank">pdf</a>]

<h2>Fusion Models for Improved Visual Captioning. (arXiv:2010.15251v2 [cs.CV] UPDATED)</h2>
<h3>Marimuthu Kalimuthu, Aditya Mogadala, Marius Mosbach, Dietrich Klakow</h3>
<p>Visual captioning aims to generate textual descriptions given images or
videos. Traditionally, image captioning models are trained on human annotated
datasets such as Flickr30k and MS-COCO, which are limited in size and
diversity. This limitation hinders the generalization capabilities of these
models while also rendering them liable to making mistakes. Language models
can, however, be trained on vast amounts of freely available unlabelled data
and have recently emerged as successful language encoders and coherent text
generators. Meanwhile, several unimodal and multimodal fusion techniques have
been proven to work well for natural language generation and automatic speech
recognition. Building on these recent developments, and with the aim of
improving the quality of generated captions, the contribution of our work in
this paper is two-fold: First, we propose a generic multimodal model fusion
framework for caption generation as well as emendation where we utilize
different fusion strategies to integrate a pretrained Auxiliary Language Model
(AuxLM) within the traditional encoder-decoder visual captioning frameworks.
Next, we employ the same fusion strategies to integrate a pretrained Masked
Language Model (MLM), namely BERT, with a visual captioning model, viz. Show,
Attend, and Tell, for emending both syntactic and semantic errors in captions.
Our caption emendation experiments on three benchmark image captioning
datasets, viz. Flickr8k, Flickr30k, and MSCOCO, show improvements over the
baseline, indicating the usefulness of our proposed multimodal fusion
strategies. Further, we perform a preliminary qualitative analysis on the
emended captions and identify error categories based on the type of
corrections.
</p>
<a href="http://arxiv.org/abs/2010.15251" target="_blank">arXiv:2010.15251</a> [<a href="http://arxiv.org/pdf/2010.15251" target="_blank">pdf</a>]

<h2>Two-Level K-FAC Preconditioning for Deep Learning. (arXiv:2011.00573v3 [cs.LG] UPDATED)</h2>
<h3>Nikolaos Tselepidis, Jonas Kohler, Antonio Orvieto</h3>
<p>In the context of deep learning, many optimization methods use gradient
covariance information in order to accelerate the convergence of Stochastic
Gradient Descent. In particular, starting with Adagrad, a seemingly endless
line of research advocates the use of diagonal approximations of the so-called
empirical Fisher matrix in stochastic gradient-based algorithms, with the most
prominent one arguably being Adam. However, in recent years, several works cast
doubt on the theoretical basis of preconditioning with the empirical Fisher
matrix, and it has been shown that more sophisticated approximations of the
actual Fisher matrix more closely resemble the theoretically well-motivated
Natural Gradient Descent. One particularly successful variant of such methods
is the so-called K-FAC optimizer, which uses a Kronecker-factored
block-diagonal Fisher approximation as preconditioner. In this work, drawing
inspiration from two-level domain decomposition methods used as preconditioners
in the field of scientific computing, we extend K-FAC by enriching it with
off-diagonal (i.e. global) curvature information in a computationally efficient
way. We achieve this by adding a coarse-space correction term to the
preconditioner, which captures the global Fisher information matrix at a
coarser scale. We present a small set of experimental results suggesting
improved convergence behaviour of our proposed method.
</p>
<a href="http://arxiv.org/abs/2011.00573" target="_blank">arXiv:2011.00573</a> [<a href="http://arxiv.org/pdf/2011.00573" target="_blank">pdf</a>]

<h2>An Efficiency-boosting Client Selection Scheme for Federated Learning with Fairness Guarantee. (arXiv:2011.01783v3 [cs.LG] UPDATED)</h2>
<h3>Tiansheng Huang, Weiwei Lin, Wentai Wu, Ligang He, Keqin Li, Albert Y.Zomaya</h3>
<p>The issue of potential privacy leakage during centralized AI's model training
has drawn intensive concern from the public. A Parallel and Distributed
Computing (or PDC) scheme, termed Federated Learning (FL), has emerged as a new
paradigm to cope with the privacy issue by allowing clients to perform model
training locally, without the necessity to upload their personal sensitive
data. In FL, the number of clients could be sufficiently large, but the
bandwidth available for model distribution and re-upload is quite limited,
making it sensible to only involve part of the volunteers to participate in the
training process. The client selection policy is critical to an FL process in
terms of training efficiency, the final model's quality as well as fairness. In
this paper, we will model the fairness guaranteed client selection as a
Lyapunov optimization problem and then a C2MAB-based method is proposed for
estimation of the model exchange time between each client and the server, based
on which we design a fairness guaranteed algorithm termed RBCS-F for
problem-solving. The regret of RBCS-F is strictly bounded by a finite constant,
justifying its theoretical feasibility. Barring the theoretical results, more
empirical data can be derived from our real training experiments on public
datasets.
</p>
<a href="http://arxiv.org/abs/2011.01783" target="_blank">arXiv:2011.01783</a> [<a href="http://arxiv.org/pdf/2011.01783" target="_blank">pdf</a>]

<h2>A Comprehensive Study of Replay-based Class Incremental Learning Algorithms. (arXiv:2011.01844v2 [cs.LG] UPDATED)</h2>
<h3>Eden Belouadah, Adrian Popescu, Ioannis Kanellos</h3>
<p>The ability of artificial agents to increment their capabilities when
confronted with new data is an open challenge in artificial intelligence. The
main challenge faced in such cases is catastrophic forgetting, i.e., the
tendency of neural networks to underfit past data when new ones are ingested. A
first group of approaches tackles forgetting by increasing deep model capacity
to accommodate new knowledge. A second type of approaches fix the deep model
size and introduce a mechanism whose objective is to ensure a good compromise
between stability and plasticity of the model. While the first type of
algorithms were compared thoroughly, this is not the case for methods which
exploit a fixed size model. Here, we focus on the latter, place them in a
common conceptual and experimental framework and propose the following
contributions: (1) define six desirable properties of incremental learning
algorithms and analyze them according to these properties, (2) introduce a
unified formalization of the class-incremental learning problem, (3) propose a
common evaluation framework which is more thorough than existing ones in terms
of number of datasets, size of datasets, size of bounded memory and number of
incremental states, (4) investigate the usefulness of herding for past
exemplars selection, (5) provide experimental evidence that it is possible to
obtain competitive performance without the use of knowledge distillation to
tackle catastrophic forgetting and (6) facilitate reproducibility by
integrating all tested methods in a common open-source repository. The main
experimental finding is that none of the existing algorithms achieves the best
results in all evaluated settings. Important differences arise notably if a
bounded memory of past classes is allowed or not.
</p>
<a href="http://arxiv.org/abs/2011.01844" target="_blank">arXiv:2011.01844</a> [<a href="http://arxiv.org/pdf/2011.01844" target="_blank">pdf</a>]

<h2>Channel Pruning via Multi-Criteria based on Weight Dependency. (arXiv:2011.03240v2 [cs.CV] UPDATED)</h2>
<h3>Yangchun Yan, Chao Li, Rongzuo Guo, Kang Yang, Yongjun Xu</h3>
<p>Channel pruning has demonstrated its effectiveness in compressing ConvNets.
In many prior arts, the importance of an output feature map is only determined
by its associated filter. However, these methods ignore a small part of weights
in the next layer which disappear as the feature map is removed. They ignore
the dependency of the weights, so that, a part of weights are pruned without
being evaluated. In addition, many pruning methods use only one criterion for
evaluation, and find a sweet-spot of pruning structure and accuracy in a
trial-and-error fashion, which can be time-consuming. To address the above
issues, we proposed a channel pruning algorithm via multi-criteria based on
weight dependency, CPMC, which can compress a variety of models efficiently. We
design the importance of the feature map in three aspects, including its
associated weight value, computational cost and parameter quantity. Use the
phenomenon of weight dependency, We get the importance by assessing its
associated filter and the corresponding partial weights of the next layer. Then
we use global normalization to achieve cross-layer comparison. Our method can
compress various CNN models, including VGGNet, ResNet and DenseNet, on various
image classification datasets. Extensive experiments have shown CPMC
outperforms the others significantly.
</p>
<a href="http://arxiv.org/abs/2011.03240" target="_blank">arXiv:2011.03240</a> [<a href="http://arxiv.org/pdf/2011.03240" target="_blank">pdf</a>]

<h2>The Cost of Privacy in Generalized Linear Models: Algorithms and Minimax Lower Bounds. (arXiv:2011.03900v2 [stat.ML] UPDATED)</h2>
<h3>T. Tony Cai, Yichen Wang, Linjun Zhang</h3>
<p>We propose differentially private algorithms for parameter estimation in both
low-dimensional and high-dimensional sparse generalized linear models (GLMs) by
constructing private versions of projected gradient descent. We show that the
proposed algorithms are nearly rate-optimal by characterizing their statistical
performance and establishing privacy-constrained minimax lower bounds for GLMs.
The lower bounds are obtained via a novel technique, which is based on Stein's
Lemma and generalizes the tracing attack technique for privacy-constrained
lower bounds. This lower bound argument can be of independent interest as it is
applicable to general parametric models. Simulated and real data experiments
are conducted to demonstrate the numerical performance of our algorithms.
</p>
<a href="http://arxiv.org/abs/2011.03900" target="_blank">arXiv:2011.03900</a> [<a href="http://arxiv.org/pdf/2011.03900" target="_blank">pdf</a>]

<h2>Bait and Switch: Online Training Data Poisoning of Autonomous Driving Systems. (arXiv:2011.04065v2 [cs.LG] UPDATED)</h2>
<h3>Naman Patel, Prashanth Krishnamurthy, Siddharth Garg, Farshad Khorrami</h3>
<p>We show that by controlling parts of a physical environment in which a
pre-trained deep neural network (DNN) is being fine-tuned online, an adversary
can launch subtle data poisoning attacks that degrade the performance of the
system. While the attack can be applied in general to any perception task, we
consider a DNN based traffic light classifier for an autonomous car that has
been trained in one city and is being fine-tuned online in another city. We
show that by injecting environmental perturbations that do not modify the
traffic lights themselves or ground-truth labels, the adversary can cause the
deep network to learn spurious concepts during the online learning phase. The
attacker can leverage the introduced spurious concepts in the environment to
cause the model's accuracy to degrade during operation; therefore, causing the
system to malfunction.
</p>
<a href="http://arxiv.org/abs/2011.04065" target="_blank">arXiv:2011.04065</a> [<a href="http://arxiv.org/pdf/2011.04065" target="_blank">pdf</a>]

<h2>Deep Multimodal Fusion by Channel Exchanging. (arXiv:2011.05005v2 [cs.CV] UPDATED)</h2>
<h3>Yikai Wang, Wenbing Huang, Fuchun Sun, Tingyang Xu, Yu Rong, Junzhou Huang</h3>
<p>Deep multimodal fusion by using multiple sources of data for classification
or regression has exhibited a clear advantage over the unimodal counterpart on
various applications. Yet, current methods including aggregation-based and
alignment-based fusion are still inadequate in balancing the trade-off between
inter-modal fusion and intra-modal processing, incurring a bottleneck of
performance improvement. To this end, this paper proposes
Channel-Exchanging-Network (CEN), a parameter-free multimodal fusion framework
that dynamically exchanges channels between sub-networks of different
modalities. Specifically, the channel exchanging process is self-guided by
individual channel importance that is measured by the magnitude of
Batch-Normalization (BN) scaling factor during training. The validity of such
exchanging process is also guaranteed by sharing convolutional filters yet
keeping separate BN layers across modalities, which, as an add-on benefit,
allows our multimodal architecture to be almost as compact as a unimodal
network. Extensive experiments on semantic segmentation via RGB-D data and
image translation through multi-domain input verify the effectiveness of our
CEN compared to current state-of-the-art methods. Detailed ablation studies
have also been carried out, which provably affirm the advantage of each
component we propose. Our code is available at https://github.com/yikaiw/CEN.
</p>
<a href="http://arxiv.org/abs/2011.05005" target="_blank">arXiv:2011.05005</a> [<a href="http://arxiv.org/pdf/2011.05005" target="_blank">pdf</a>]

<h2>Hamiltonian Q-Learning: Leveraging Importance-sampling for Data Efficient RL. (arXiv:2011.05927v2 [cs.LG] UPDATED)</h2>
<h3>Udari Madhushani, Biswadip Dey, Naomi Ehrich Leonard, Amit Chakraborty</h3>
<p>Model-free reinforcement learning (RL), in particular Q-learning is widely
used to learn optimal policies for a variety of planning and control problems.
However, when the underlying state-transition dynamics are stochastic and
high-dimensional, Q-learning requires a large amount of data and incurs a
prohibitively high computational cost. In this paper, we introduce Hamiltonian
Q-Learning, a data efficient modification of the Q-learning approach, which
adopts an importance-sampling based technique for computing the Q function. To
exploit stochastic structure of the state-transition dynamics, we employ
Hamiltonian Monte Carlo to update Q function estimates by approximating the
expected future rewards using Q values associated with a subset of next states.
Further, to exploit the latent low-rank structure of the dynamic system,
Hamiltonian Q-Learning uses a matrix completion algorithm to reconstruct the
updated Q function from Q value updates over a much smaller subset of
state-action pairs. By providing an efficient way to apply Q-learning in
stochastic, high-dimensional problems, the proposed approach broadens the scope
of RL algorithms for real-world applications, including classical control tasks
and environmental monitoring.
</p>
<a href="http://arxiv.org/abs/2011.05927" target="_blank">arXiv:2011.05927</a> [<a href="http://arxiv.org/pdf/2011.05927" target="_blank">pdf</a>]

<h2>The Role of Edge Robotics As-a-Service in Monitoring COVID-19 Infection. (arXiv:2011.08482v2 [cs.RO] UPDATED)</h2>
<h3>Haimiao Mo, Shuai Ding (Member, IEEE), Shanlin Yang, Xi Zheng (Member, IEEE), Athanasios V. Vasilakos</h3>
<p>Deep learning technology has been widely used in edge computing. However,
pandemics like covid-19 require deep learning capabilities at mobile devices
(detect respiratory rate using mobile robotics or conduct CT scan using a
mobile scanner), which are severely constrained by the limited storage and
computation resources at the device level. To solve this problem, we propose a
three-tier architecture, including robot layers, edge layers, and cloud layers.
We adopt this architecture to design a non-contact respiratory monitoring
system to break down respiratory rate calculation tasks. Experimental results
of respiratory rate monitoring show that the proposed approach in this paper
significantly outperforms other approaches. It is supported by computation time
costs with 2.26 ms per frame, 27.48 ms per frame, 0.78 seconds for convolution
operation, similarity calculation, processing one-minute length respiratory
signals, respectively. And the computation time costs of our three-tier
architecture are less than that of edge+cloud architecture and cloud
architecture. Moreover, we use our three-tire architecture for CT image
diagnosis task decomposition. The evaluation of a CT image dataset of COVID-19
proves that our three-tire architecture is useful for resolving tasks on deep
learning networks by edge equipment. There are broad application scenarios in
smart hospitals in the future.
</p>
<a href="http://arxiv.org/abs/2011.08482" target="_blank">arXiv:2011.08482</a> [<a href="http://arxiv.org/pdf/2011.08482" target="_blank">pdf</a>]

<h2>Beyond Pinball Loss: Quantile Methods for Calibrated Uncertainty Quantification. (arXiv:2011.09588v2 [cs.LG] UPDATED)</h2>
<h3>Youngseog Chung, Willie Neiswanger, Ian Char, Jeff Schneider</h3>
<p>Among the many ways of quantifying uncertainty in a regression setting,
specifying the full quantile function is attractive, as quantiles are amenable
to interpretation and evaluation. A model that predicts the true conditional
quantiles for each input, at all quantile levels, presents a correct and
efficient representation of the underlying uncertainty. To achieve this, many
current quantile-based methods focus on optimizing the so-called pinball loss.
However, this loss restricts the scope of applicable regression models, limits
the ability to target many desirable properties (e.g. calibration, sharpness,
centered intervals), and may produce poor conditional quantiles. In this work,
we develop new quantile methods that address these shortcomings. In particular,
we propose methods that can apply to any class of regression model, allow for
selecting a Pareto-optimal trade-off between calibration and sharpness,
optimize for calibration of centered intervals, and produce more accurate
conditional quantiles. We provide a thorough experimental evaluation of our
methods, which includes a high dimensional uncertainty quantification task in
nuclear fusion.
</p>
<a href="http://arxiv.org/abs/2011.09588" target="_blank">arXiv:2011.09588</a> [<a href="http://arxiv.org/pdf/2011.09588" target="_blank">pdf</a>]

<h2>Multimodal Learning for Hateful Memes Detection. (arXiv:2011.12870v3 [cs.CV] UPDATED)</h2>
<h3>Yi Zhou, Zhenhao Chen</h3>
<p>Memes are used for spreading ideas through social networks. Although most
memes are created for humor, some memes become hateful under the combination of
pictures and text. Automatically detecting the hateful memes can help reduce
their harmful social impact. Unlike the conventional multimodal tasks, where
the visual and textual information is semantically aligned, the challenge of
hateful memes detection lies in its unique multimodal information. The image
and text in memes are weakly aligned or even irrelevant, which requires the
model to understand the content and perform reasoning over multiple modalities.
In this paper, we focus on multimodal hateful memes detection and propose a
novel method that incorporates the image captioning process into the memes
detection process. We conduct extensive experiments on multimodal meme datasets
and illustrated the effectiveness of our approach. Our model achieves promising
results on the Hateful Memes Detection Challenge.
</p>
<a href="http://arxiv.org/abs/2011.12870" target="_blank">arXiv:2011.12870</a> [<a href="http://arxiv.org/pdf/2011.12870" target="_blank">pdf</a>]

<h2>Functional Time Series Forecasting: Functional Singular Spectrum Analysis Approaches. (arXiv:2011.13077v2 [stat.ML] UPDATED)</h2>
<h3>Jordan Trinka, Hossein Haghbin, Mehdi Maadooliat</h3>
<p>In this paper, we propose two nonparametric methods used in the forecasting
of functional time-dependent data, namely functional singular spectrum analysis
recurrent forecasting and vector forecasting. Both algorithms utilize the
results of functional singular spectrum analysis and past observations in order
to predict future data points where recurrent forecasting predicts one function
at a time and the vector forecasting makes predictions using functional
vectors. We compare our forecasting methods to a gold standard algorithm used
in the prediction of functional, time-dependent data by way of simulation and
real data and we find our techniques do better for periodic stochastic
processes.
</p>
<a href="http://arxiv.org/abs/2011.13077" target="_blank">arXiv:2011.13077</a> [<a href="http://arxiv.org/pdf/2011.13077" target="_blank">pdf</a>]

<h2>SelfText Beyond Polygon: Unconstrained Text Detection with Box Supervision and Dynamic Self-Training. (arXiv:2011.13307v2 [cs.CV] UPDATED)</h2>
<h3>Weijia Wu, Enze Xie, Ruimao Zhang, Wenhai Wang, Guan Pang, Zhen Li, Hong Zhou, Ping Luo</h3>
<p>Although a polygon is a more accurate representation than an upright bounding
box for text detection, the annotations of polygons are extremely expensive and
challenging. Unlike existing works that employ fully-supervised training with
polygon annotations, we propose a novel text detection system termed SelfText
Beyond Polygon (SBP) with Bounding Box Supervision (BBS) and Dynamic Self
Training (DST), where training a polygon-based text detector with only a
limited set of upright bounding box annotations. For BBS, we firstly utilize
the synthetic data with character-level annotations to train a Skeleton
Attention Segmentation Network (SASN). Then the box-level annotations are
adopted to guide the generation of high-quality polygon-liked pseudo labels,
which can be used to train any detectors. In this way, our method achieves the
same performance as text detectors trained with polygon annotations (i.e., both
are 85.0% F-score for PSENet on ICDAR2015 ). For DST, through dynamically
removing the false alarms, it is able to leverage limited labeled data as well
as massive unlabeled data to further outperform the expensive baseline. We hope
SBP can provide a new perspective for text detection to save huge labeling
costs. Code is available at: github.com/weijiawu/SBP.
</p>
<a href="http://arxiv.org/abs/2011.13307" target="_blank">arXiv:2011.13307</a> [<a href="http://arxiv.org/pdf/2011.13307" target="_blank">pdf</a>]

<h2>Reducing Discrimination in Learning Algorithms for Social Good in Sociotechnical Systems. (arXiv:2011.13988v2 [cs.LG] UPDATED)</h2>
<h3>Katelyn Morrison</h3>
<p>Sociotechnical systems within cities are now equipped with machine learning
algorithms in hopes to increase efficiency and functionality by modeling and
predicting trends. Machine learning algorithms have been applied in these
domains to address challenges such as balancing the distribution of bikes
throughout a city and identifying demand hotspots for ride sharing drivers.
However, these algorithms applied to challenges in sociotechnical systems have
exacerbated social inequalities due to previous bias in data sets or the lack
of data from marginalized communities. In this paper, I will address how smart
mobility initiatives in cities use machine learning algorithms to address
challenges. I will also address how these algorithms unintentionally
discriminate against features such as socioeconomic status to motivate the
importance of algorithmic fairness. Using the bike sharing program in
Pittsburgh, PA, I will present a position on how discrimination can be
eliminated from the pipeline using Bayesian Optimization.
</p>
<a href="http://arxiv.org/abs/2011.13988" target="_blank">arXiv:2011.13988</a> [<a href="http://arxiv.org/pdf/2011.13988" target="_blank">pdf</a>]

<h2>Scaling down Deep Learning. (arXiv:2011.14439v3 [cs.LG] UPDATED)</h2>
<h3>Sam Greydanus</h3>
<p>Though deep learning models have taken on commercial and political relevance,
many aspects of their training and operation remain poorly understood. This has
sparked interest in "science of deep learning" projects, many of which are run
at scale and require enormous amounts of time, money, and electricity. But how
much of this research really needs to occur at scale? In this paper, we
introduce MNIST-1D: a minimalist, low-memory, and low-compute alternative to
classic deep learning benchmarks. The training examples are 20 times smaller
than MNIST examples yet they differentiate more clearly between linear,
nonlinear, and convolutional models which attain 32, 68, and 94% accuracy
respectively (these models obtain 94, 99+, and 99+% on MNIST). Then we present
example use cases which include measuring the spatial inductive biases of
lottery tickets, observing deep double descent, and metalearning an activation
function.
</p>
<a href="http://arxiv.org/abs/2011.14439" target="_blank">arXiv:2011.14439</a> [<a href="http://arxiv.org/pdf/2011.14439" target="_blank">pdf</a>]

<h2>Inductive Biases for Deep Learning of Higher-Level Cognition. (arXiv:2011.15091v2 [cs.LG] UPDATED)</h2>
<h3>Anirudh Goyal, Yoshua Bengio</h3>
<p>A fascinating hypothesis is that human and animal intelligence could be
explained by a few principles (rather than an encyclopedic list of heuristics).
If that hypothesis was correct, we could more easily both understand our own
intelligence and build intelligent machines. Just like in physics, the
principles themselves would not be sufficient to predict the behavior of
complex systems like brains, and substantial computation might be needed to
simulate human-like intelligence. This hypothesis would suggest that studying
the kind of inductive biases that humans and animals exploit could help both
clarify these principles and provide inspiration for AI research and
neuroscience theories. Deep learning already exploits several key inductive
biases, and this work considers a larger list, focusing on those which concern
mostly higher-level and sequential conscious processing. The objective of
clarifying these particular principles is that they could potentially help us
build AI systems benefiting from humans' abilities in terms of flexible
out-of-distribution and systematic generalization, which is currently an area
where a large gap exists between state-of-the-art machine learning and human
intelligence.
</p>
<a href="http://arxiv.org/abs/2011.15091" target="_blank">arXiv:2011.15091</a> [<a href="http://arxiv.org/pdf/2011.15091" target="_blank">pdf</a>]

<h2>Open-Ended Multi-Modal Relational Reason for Video Question Answering. (arXiv:2012.00822v2 [cs.AI] UPDATED)</h2>
<h3>Haozheng Luo, Ruiyang Qin</h3>
<p>People with visual impairments urgently need helps, not only on the basic
tasks such as guiding and retrieving objects , but on the advanced tasks like
picturing the new environments. More than a guiding dog, they might want some
devices which are able to provide linguistic interaction. Building on various
research literature, we aim to conduct a research on the interaction between
the robot agent and visual impaired people. The robot agent, applied VQA
techniques, is able to analyze the environment, process and understand the
pronouncing questions, and provide feedback to the human user. In this paper,
we are going to discuss the related questions about this kind of interaction,
the techniques we used in this work, and how we conduct our research.
</p>
<a href="http://arxiv.org/abs/2012.00822" target="_blank">arXiv:2012.00822</a> [<a href="http://arxiv.org/pdf/2012.00822" target="_blank">pdf</a>]

<h2>Algebraically-Informed Deep Networks (AIDN): A Deep Learning Approach to Represent Algebraic Structures. (arXiv:2012.01141v2 [cs.LG] UPDATED)</h2>
<h3>Mustafa Hajij, Ghada Zamzmi, Matthew Dawson, Greg Muller</h3>
<p>One of the central problems in the interface of deep learning and mathematics
is that of building learning systems that can automatically uncover underlying
mathematical laws from observed data. In this work, we make one step towards
building a bridge between algebraic structures and deep learning, and introduce
\textbf{AIDN}, \textit{Algebraically-Informed Deep Networks}. \textbf{AIDN} is
a deep learning algorithm to represent any finitely-presented algebraic object
with a set of deep neural networks. The deep networks obtained via
\textbf{AIDN} are \textit{algebraically-informed} in the sense that they
satisfy the algebraic relations of the presentation of the algebraic structure
that serves as the input to the algorithm. Our proposed network can robustly
compute linear and non-linear representations of most finitely-presented
algebraic structures such as groups, associative algebras, and Lie algebras. We
evaluate our proposed approach and demonstrate its applicability to algebraic
and geometric objects that are significant in low-dimensional topology. In
particular, we study solutions for the Yang-Baxter equations and their
applications on braid groups. Further, we study the representations of the
Temperley-Lieb algebra. Finally, we show, using the Reshetikhin-Turaev
construction, how our proposed deep learning approach can be utilized to
construct new link invariants. We believe the proposed approach would tread a
path toward a promising future research in deep learning applied to algebraic
and geometric structures.
</p>
<a href="http://arxiv.org/abs/2012.01141" target="_blank">arXiv:2012.01141</a> [<a href="http://arxiv.org/pdf/2012.01141" target="_blank">pdf</a>]

<h2>Siamese Basis Function Networks for Defect Classification. (arXiv:2012.01338v3 [cs.CV] UPDATED)</h2>
<h3>Tobias Schlagenhauf, Faruk Yildirim, Benedikt Br&#xfc;ckner, J&#xfc;rgen Fleischer</h3>
<p>Defect classification on metallic surfaces is considered a critical issue
since substantial quantities of steel and other metals are processed by the
manufacturing industry on a daily basis. The authors propose a new approach
where they introduce the usage of so called Siamese Kernels in a Basis Function
Network to create the Siamese Basis Function Network (SBF-Network). The
underlying idea is to classify by comparison using similarity scores. This
classification is reinforced through efficient deep learning based feature
extraction methods. First, a center image is assigned to each Siamese Kernel.
The Kernels are then trained to generate encodings in a way that enables them
to distinguish their center from other images in the dataset. Using this
approach the authors created some kind of class-awareness inside the Siamese
Kernels. To classify a given image, each Siamese Kernel generates a feature
vector for its center as well as the given image. These vectors represent
encodings of the respective images in a lower-dimensional space. The distance
between each pair of encodings is then computed using the cosine distance
together with radial basis functions. The distances are fed into a multilayer
neural network to perform the classification. With this approach the authors
achieved outstanding results on the state of the art NEU surface defect
dataset.
</p>
<a href="http://arxiv.org/abs/2012.01338" target="_blank">arXiv:2012.01338</a> [<a href="http://arxiv.org/pdf/2012.01338" target="_blank">pdf</a>]

<h2>Differential Morphed Face Detection Using Deep Siamese Networks. (arXiv:2012.01541v2 [cs.CV] UPDATED)</h2>
<h3>Sobhan Soleymani, Baaria Chaudhary, Ali Dabouei, Jeremy Dawson, Nasser M. Nasrabadi</h3>
<p>Although biometric facial recognition systems are fast becoming part of
security applications, these systems are still vulnerable to morphing attacks,
in which a facial reference image can be verified as two or more separate
identities. In border control scenarios, a successful morphing attack allows
two or more people to use the same passport to cross borders. In this paper, we
propose a novel differential morph attack detection framework using a deep
Siamese network. To the best of our knowledge, this is the first research work
that makes use of a Siamese network architecture for morph attack detection. We
compare our model with other classical and deep learning models using two
distinct morph datasets, VISAPP17 and MorGAN. We explore the embedding space
generated by the contrastive loss using three decision making frameworks using
Euclidean distance, feature difference and a support vector machine classifier,
and feature concatenation and a support vector machine classifier.
</p>
<a href="http://arxiv.org/abs/2012.01541" target="_blank">arXiv:2012.01541</a> [<a href="http://arxiv.org/pdf/2012.01541" target="_blank">pdf</a>]

<h2>Learning Hyperbolic Representations for Unsupervised 3D Segmentation. (arXiv:2012.01644v2 [cs.CV] UPDATED)</h2>
<h3>Joy Hsu, Jeffrey Gu, Gong-Her Wu, Wah Chiu, Serena Yeung</h3>
<p>There exists a need for unsupervised 3D segmentation on complex volumetric
data, particularly when annotation ability is limited or discovery of new
categories is desired. Using the observation that much of 3D volumetric data is
innately hierarchical, we propose learning effective representations of 3D
patches for unsupervised segmentation through a variational autoencoder (VAE)
with a hyperbolic latent space and a proposed gyroplane convolutional layer,
which better models the underlying hierarchical structure within a 3D image. We
also introduce a hierarchical triplet loss and multi-scale patch sampling
scheme to embed relationships across varying levels of granularity. We
demonstrate the effectiveness of our hyperbolic representations for
unsupervised 3D segmentation on a hierarchical toy dataset, BraTS whole tumor
dataset, and cryogenic electron microscopy data.
</p>
<a href="http://arxiv.org/abs/2012.01644" target="_blank">arXiv:2012.01644</a> [<a href="http://arxiv.org/pdf/2012.01644" target="_blank">pdf</a>]

<h2>Sample-efficient L0-L2 constrained structure learning of sparse Ising models. (arXiv:2012.01744v2 [stat.ML] UPDATED)</h2>
<h3>Antoine Dedieu, Miguel L&#xe1;zaro-Gredilla, Dileep George</h3>
<p>We consider the problem of learning the underlying graph of a sparse Ising
model with $p$ nodes from $n$ i.i.d. samples. The most recent and best
performing approaches combine an empirical loss (the logistic regression loss
or the interaction screening loss) with a regularizer (an L1 penalty or an L1
constraint). This results in a convex problem that can be solved separately for
each node of the graph. In this work, we leverage the cardinality constraint L0
norm, which is known to properly induce sparsity, and further combine it with
an L2 norm to better model the non-zero coefficients. We show that our proposed
estimators achieve an improved sample complexity, both (a) theoretically -- by
reaching new state-of-the-art upper bounds for recovery guarantees -- and (b)
empirically -- by showing sharper phase transitions between poor and full
recovery for graph topologies studied in the literature -- when compared to
their L1-based counterparts.
</p>
<a href="http://arxiv.org/abs/2012.01744" target="_blank">arXiv:2012.01744</a> [<a href="http://arxiv.org/pdf/2012.01744" target="_blank">pdf</a>]

<h2>Folding and Unfolding on Metagraphs. (arXiv:2012.01759v2 [cs.AI] UPDATED)</h2>
<h3>Ben Goertzel</h3>
<p>Typed metagraphs are defined as hypergraphs with types assigned to hyperedges
and their targets, and the potential to have targets of hyperedges connect to
whole links as well as targets. Directed typed metagraphs (DTMGs) are
introduced via partitioning the targets of each edge in a typed metagraph into
input, output and lateral sets; one can then look at "metapaths" in which
edges' output-sets are linked to other edges' input-sets. An initial algebra
approach to DTMGs is presented, including introduction of constructors for
building up DTMGs and laws regarding relationships among multiple ways of using
these constructors. A menagerie of useful morphism types is then defined on
DTMGs (catamorphisms, anamorphisms, histomorphisms, futumorphisms,
hylomorphisms, chronomorphisms, metamorphisms and metachronomorphisms),
providing a general abstract framework for formulating a broad variety of
metagraph operations. Deterministic and stochastic processes on typed
metagraphs are represented in terms of forests of DTMGs defined over a common
TMG, where the various morphisms can be straightforwardly extended to these
forests. The framework outlined can be applied to realistic metagraphs
involving complexities like dependent and probabilistic types, multidimensional
values and dynamic processing including insertion and deletion of edges.
</p>
<a href="http://arxiv.org/abs/2012.01759" target="_blank">arXiv:2012.01759</a> [<a href="http://arxiv.org/pdf/2012.01759" target="_blank">pdf</a>]

<h2>Autonomous Navigation with Mobile Robots using Deep Learning and the Robot Operating System. (arXiv:2012.02417v2 [cs.RO] UPDATED)</h2>
<h3>Anh Nguyen, Quang Tran</h3>
<p>Autonomous navigation is a long-standing field of robotics research, which
provides an essential capability for mobile robots to execute a series of tasks
on the same environments performed by human everyday. In this chapter, we
present a set of algorithms to train and deploy deep networks for autonomous
navigation of mobile robots using the Robot Operation System (ROS). We describe
three main steps to tackle this problem: i) collecting data in simulation
environments using ROS and Gazebo; ii) designing deep network for autonomous
navigation, and iii) deploying the learned policy on mobile robots in both
simulation and real-world. Theoretically, we present deep learning
architectures for robust navigation in normal environments (e.g., man-made
houses, roads) and complex environments (e.g., collapsed cities, or natural
caves). We further show that the use of visual modalities such as RGB, Lidar,
and point cloud is essential to improve the autonomy of mobile robots. Our
project website and demonstration video can be found at
https://sites.google.com/site/autonomousnavigationros.
</p>
<a href="http://arxiv.org/abs/2012.02417" target="_blank">arXiv:2012.02417</a> [<a href="http://arxiv.org/pdf/2012.02417" target="_blank">pdf</a>]

<h2>Rethinking movie genre classification with fine-grained semantic clustering. (arXiv:2012.02639v2 [cs.CV] UPDATED)</h2>
<h3>Edward Fish, Andrew Gilbert, Jon Weinbren</h3>
<p>Movie genre classification is an active research area in machine learning.
However, due to the limited labels available, there can be large semantic
variations between movies within a single genre definition. We expand these
'coarse' genre labels by identifying 'fine-grained' semantic information within
the multi-modal content of movies. By leveraging pre-trained 'expert' networks,
we learn the influence of different combinations of modes for multi-label genre
classification. Using a contrastive loss, we continue to fine-tune this
'coarse' genre classification network to identify high-level intertextual
similarities between the movies across all genre labels. This leads to a more
'fine-grained' and detailed clustering, based on semantic similarities while
still retaining some genre information. Our approach is demonstrated on a newly
introduced multi-modal 37,866,450 frame, 8,800 movie trailer dataset,
MMX-Trailer-20, which includes pre-computed audio, location, motion, and image
embeddings.
</p>
<a href="http://arxiv.org/abs/2012.02639" target="_blank">arXiv:2012.02639</a> [<a href="http://arxiv.org/pdf/2012.02639" target="_blank">pdf</a>]

<h2>Super-Selfish: Self-Supervised Learning on Images with PyTorch. (arXiv:2012.02706v2 [cs.CV] UPDATED)</h2>
<h3>Nicolas Wagner, Anirban Mukhopadhyay</h3>
<p>Super-Selfish is an easy to use PyTorch framework for image-based
self-supervised learning. Features can be learned with 13 algorithms that span
from simple classification to more complex state of theart contrastive pretext
tasks. The framework is easy to use and allows for pretraining any PyTorch
neural network with only two lines of code. Simultaneously, full flexibility is
maintained through modular design choices. The code can be found at
https://github.com/MECLabTUDA/Super_Selfish and installed using pip install
super-selfish.
</p>
<a href="http://arxiv.org/abs/2012.02706" target="_blank">arXiv:2012.02706</a> [<a href="http://arxiv.org/pdf/2012.02706" target="_blank">pdf</a>]

<h2>From Videos to URLs: A Multi-Browser Guide To Extract User's Behavior with Optical Character Recognition. (arXiv:1811.06193v2 [cs.CV] CROSS LISTED)</h2>
<h3>Mojtaba Heidarysafa, James Reed, Kamran Kowsari, April Celeste R.Leviton, Janet I. Warren, Donald E. Brown</h3>
<p>Tracking users' activities on the World Wide Web (WWW) allows researchers to
analyze each user's internet behavior as time passes and for the amount of time
spent on a particular domain. This analysis can be used in research design, as
researchers may access to their participant's behaviors while browsing the web.
Web search behavior has been a subject of interest because of its real-world
applications in marketing, digital advertisement, and identifying potential
threats online. In this paper, we present an image-processing based method to
extract domains which are visited by a participant over multiple browsers
during a lab session. This method could provide another way to collect users'
activities during an online session given that the session recorder collected
the data. The method can also be used to collect the textual content of
web-pages that an individual visits for later analysis
</p>
<a href="http://arxiv.org/abs/1811.06193" target="_blank">arXiv:1811.06193</a> [<a href="http://arxiv.org/pdf/1811.06193" target="_blank">pdf</a>]

<h2>Online Model Selection: a Rested Bandit Formulation. (arXiv:2012.03522v1 [stat.ML])</h2>
<h3>Leonardo Cella, Claudio Gentile, Massimiliano Pontil</h3>
<p>Motivated by a natural problem in online model selection with bandit
information, we introduce and analyze a best arm identification problem in the
rested bandit setting, wherein arm expected losses decrease with the number of
times the arm has been played. The shape of the expected loss functions is
similar across arms, and is assumed to be available up to unknown parameters
that have to be learned on the fly. We define a novel notion of regret for this
problem, where we compare to the policy that always plays the arm having the
smallest expected loss at the end of the game. We analyze an arm elimination
algorithm whose regret vanishes as the time horizon increases. The actual rate
of convergence depends in a detailed way on the postulated functional form of
the expected losses. Unlike known model selection efforts in the recent bandit
literature, our algorithm exploits the specific structure of the problem to
learn the unknown parameters of the expected loss function so as to identify
the best arm as quickly as possible. We complement our analysis with a lower
bound, indicating strengths and limitations of the proposed solution.
</p>
<a href="http://arxiv.org/abs/2012.03522" target="_blank">arXiv:2012.03522</a> [<a href="http://arxiv.org/pdf/2012.03522" target="_blank">pdf</a>]

<h2>Why Unsupervised Deep Networks Generalize. (arXiv:2012.03531v1 [cs.LG])</h2>
<h3>Anita de Mello Koch, Ellen de Mello Koch, Robert de Mello Koch</h3>
<p>Promising resolutions of the generalization puzzle observe that the actual
number of parameters in a deep network is much smaller than naive estimates
suggest. The renormalization group is a compelling example of a problem which
has very few parameters, despite the fact that naive estimates suggest
otherwise. Our central hypothesis is that the mechanisms behind the
renormalization group are also at work in deep learning, and that this leads to
a resolution of the generalization puzzle. We show detailed quantitative
evidence that proves the hypothesis for an RBM, by showing that the trained RBM
is discarding high momentum modes. Specializing attention mainly to
autoencoders, we give an algorithm to determine the network's parameters
directly from the learning data set. The resulting autoencoder almost performs
as well as one trained by deep learning, and it provides an excellent initial
condition for training, reducing training times by a factor between 4 and 100
for the experiments we considered. Further, we are able to suggest a simple
criterion to decide if a given problem can or can not be solved using a deep
network.
</p>
<a href="http://arxiv.org/abs/2012.03531" target="_blank">arXiv:2012.03531</a> [<a href="http://arxiv.org/pdf/2012.03531" target="_blank">pdf</a>]

<h2>Teaching reproducible research for medical students and postgraduate pharmaceutical scientists. (arXiv:2012.03554v1 [stat.ML])</h2>
<h3>Andreas D. Meid</h3>
<p>In many academic settings, medical students start their scientific work
already during their studies. Like at our institution, they often work in
interdisciplinary teams with more or less experienced (postgraduate)
researchers of pharmaceutical sciences, natural sciences in general, or
biostatistics. All of them should be taught good research practices as an
integral part of their education, especially in terms of statistical analysis.
This includes reproducibility as a central aspect of modern research.
Acknowledging that even educators might be unfamiliar with necessary aspects of
a perfectly reproducible workflow, I agreed to give a lecture series on
reproducible research (RR) for medical students and postgraduate pharmacists
involved in several areas of clinical research. Thus, I designed a piloting
lecture series to highlight definitions of RR, reasons for RR, potential merits
of RR, and ways to work accordingly. In trying to actually reproduce a
published analysis, I encountered several practical obstacles. In this article,
I focus on this working example to emphasize the manifold facets of RR, to
provide possible explanations and solutions, and argue that harmonized
curricula for (quantitative) clinical researchers should include RR principles.
I therefore hope these experiences are helpful to raise awareness among
educators and students. RR working habits are not only beneficial for ourselves
or our students, but also for other researchers within an institution, for
scientific partners, for the scientific community, and eventually for the
public profiting from research findings.
</p>
<a href="http://arxiv.org/abs/2012.03554" target="_blank">arXiv:2012.03554</a> [<a href="http://arxiv.org/pdf/2012.03554" target="_blank">pdf</a>]

<h2>LCS Graph Kernel Based on Wasserstein Distance in Longest Common Subsequence Metric Space. (arXiv:2012.03612v1 [cs.LG])</h2>
<h3>Jianming Huang, Zhongxi Fang, Hiroyuki Kasai</h3>
<p>For graph classification tasks, many methods use a common strategy to
aggregate information of vertex neighbors. Although this strategy provides an
efficient means of extracting graph topological features, it brings excessive
amounts of information that might greatly reduce its accuracy when dealing with
large-scale neighborhoods. Learning graphs using paths or walks will not suffer
from this difficulty, but many have low utilization of each path or walk, which
might engender information loss and high computational costs. To solve this, we
propose a graph kernel using a longest common subsequence (LCS kernel) to
compute more comprehensive similarity between paths and walks, which resolves
substructure isomorphism difficulties. We also combine it with optimal
transport theory to extract more in-depth features of graphs. Furthermore, we
propose an LCS metric space and apply an adjacent point merge operation to
reduce its computational costs. Finally, we demonstrate that our proposed
method outperforms many state-of-the-art graph kernel methods.
</p>
<a href="http://arxiv.org/abs/2012.03612" target="_blank">arXiv:2012.03612</a> [<a href="http://arxiv.org/pdf/2012.03612" target="_blank">pdf</a>]

<h2>Explainable Artificial Intelligence: How Subsets of the Training Data Affect a Prediction. (arXiv:2012.03625v1 [stat.ML])</h2>
<h3>Andreas Brands&#xe6;ter, Ingrid K. Glad</h3>
<p>There is an increasing interest in and demand for interpretations and
explanations of machine learning models and predictions in various application
areas. In this paper, we consider data-driven models which are already
developed, implemented and trained. Our goal is to interpret the models and
explain and understand their predictions. Since the predictions made by
data-driven models rely heavily on the data used for training, we believe
explanations should convey information about how the training data affects the
predictions. To do this, we propose a novel methodology which we call Shapley
values for training data subset importance. The Shapley value concept
originates from coalitional game theory, developed to fairly distribute the
payout among a set of cooperating players. We extend this to subset importance,
where a prediction is explained by treating the subsets of the training data as
players in a game where the predictions are the payouts. We describe and
illustrate how the proposed method can be useful and demonstrate its
capabilities on several examples. We show how the proposed explanations can be
used to reveal biasedness in models and erroneous training data. Furthermore,
we demonstrate that when predictions are accurately explained in a known
situation, then explanations of predictions by simple models correspond to the
intuitive explanations. We argue that the explanations enable us to perceive
more of the inner workings of the algorithms, and illustrate how models
producing similar predictions can be based on very different parts of the
training data. Finally, we show how we can use Shapley values for subset
importance to enhance our training data acquisition, and by this reducing
prediction error.
</p>
<a href="http://arxiv.org/abs/2012.03625" target="_blank">arXiv:2012.03625</a> [<a href="http://arxiv.org/pdf/2012.03625" target="_blank">pdf</a>]

<h2>Passive Approach for the K-means Problem on Streaming Data. (arXiv:2012.03628v1 [stat.ML])</h2>
<h3>Arkaitz Bidaurrazaga, Aritz P&#xe9;rez, Marco Cap&#xf3;</h3>
<p>Currently the amount of data produced worldwide is increasing beyond measure,
thus a high volume of unsupervised data must be processed continuously. One of
the main unsupervised data analysis is clustering. In streaming data scenarios,
the data is composed by an increasing sequence of batches of samples where the
concept drift phenomenon may happen. In this paper, we formally define the
Streaming $K$-means(S$K$M) problem, which implies a restart of the error
function when a concept drift occurs. We propose a surrogate error function
that does not rely on concept drift detection. We proof that the surrogate is a
good approximation of the S$K$M error. Hence, we suggest an algorithm which
minimizes this alternative error each time a new batch arrives. We present some
initialization techniques for streaming data scenarios as well. Besides
providing theoretical results, experiments demonstrate an improvement of the
converged error for the non-trivial initialization methods.
</p>
<a href="http://arxiv.org/abs/2012.03628" target="_blank">arXiv:2012.03628</a> [<a href="http://arxiv.org/pdf/2012.03628" target="_blank">pdf</a>]

<h2>Stochastic Gradient Descent with Large Learning Rate. (arXiv:2012.03636v1 [stat.ML])</h2>
<h3>Kangqiao Liu, Liu Ziyin, Masahito Ueda</h3>
<p>As a simple and efficient optimization method in deep learning, stochastic
gradient descent (SGD) has attracted tremendous attention. In the vanishing
learning rate regime, SGD is now relatively well understood, and the majority
of theoretical approaches to SGD set their assumptions in the continuous-time
limit. However, the continuous-time predictions are unlikely to reflect the
experimental observations well because the practice often runs in the large
learning rate regime, where the training is faster and the generalization of
models are often better. In this paper, we propose to study the basic
properties of SGD and its variants in the non-vanishing learning rate regime.
The focus is on deriving exactly solvable results and relating them to
experimental observations. The main contributions of this work are to derive
the stable distribution for discrete-time SGD in a quadratic loss function with
and without momentum. Examples of applications of the proposed theory
considered in this work include the approximation error of variants of SGD, the
effect of mini-batch noise, the escape rate from a sharp minimum, and and the
stationary distribution of a few second order methods.
</p>
<a href="http://arxiv.org/abs/2012.03636" target="_blank">arXiv:2012.03636</a> [<a href="http://arxiv.org/pdf/2012.03636" target="_blank">pdf</a>]

<h2>DiffPrune: Neural Network Pruning with Deterministic Approximate Binary Gates and $L_0$ Regularization. (arXiv:2012.03653v1 [stat.ML])</h2>
<h3>Yaniv Shulman</h3>
<p>Modern neural network architectures typically have many millions of
parameters and can be pruned significantly without substantial loss in
effectiveness which demonstrates they are over-parameterized. The contribution
of this work is two-fold. The first is a method for approximating a
multivariate Bernoulli random variable by means of a deterministic and
differentiable transformation of any real-valued multivariate random variable.
The second is a method for model selection by element-wise multiplication of
parameters with approximate binary gates that may be computed deterministically
or stochastically and take on exact zero values. Sparsity is encouraged by the
inclusion of a surrogate regularization to the $L_0$ loss. Since the method is
differentiable it enables straightforward and efficient learning of model
architectures by an empirical risk minimization procedure with stochastic
gradient descent and theoretically enables conditional computation during
training. The method also supports any arbitrary group sparsity over parameters
or activations and therefore offers a framework for unstructured or flexible
structured model pruning. To conclude experiments are performed to demonstrate
the effectiveness of the proposed approach.
</p>
<a href="http://arxiv.org/abs/2012.03653" target="_blank">arXiv:2012.03653</a> [<a href="http://arxiv.org/pdf/2012.03653" target="_blank">pdf</a>]

<h2>Nonnegative Matrix Factorization with Toeplitz Penalty. (arXiv:2012.03694v1 [stat.ML])</h2>
<h3>Matthew Corsetti, Ernest Fokou&#xe9;</h3>
<p>Nonnegative Matrix Factorization (NMF) is an unsupervised learning algorithm
that produces a linear, parts-based approximation of a data matrix. NMF
constructs a nonnegative low rank basis matrix and a nonnegative low rank
matrix of weights which, when multiplied together, approximate the data matrix
of interest using some cost function. The NMF algorithm can be modified to
include auxiliary constraints which impose task-specific penalties or
restrictions on the cost function of the matrix factorization. In this paper we
propose a new NMF algorithm that makes use of non-data dependent auxiliary
constraints which incorporate a Toeplitz matrix into the multiplicative
updating of the basis and weight matrices. We compare the facial recognition
performance of our new Toeplitz Nonnegative Matrix Factorization (TNMF)
algorithm to the performance of the Zellner Nonnegative Matrix Factorization
(ZNMF) algorithm which makes use of data-dependent auxiliary constraints. We
also compare the facial recognition performance of the two aforementioned
algorithms with the performance of several preexisting constrained NMF
algorithms that have non-data-dependent penalties. The facial recognition
performances are evaluated using the Cambridge ORL Database of Faces and the
Yale Database of Faces.
</p>
<a href="http://arxiv.org/abs/2012.03694" target="_blank">arXiv:2012.03694</a> [<a href="http://arxiv.org/pdf/2012.03694" target="_blank">pdf</a>]

<h2>Autoencoding Variational Autoencoder. (arXiv:2012.03715v1 [cs.LG])</h2>
<h3>A. Taylan Cemgil, Sumedh Ghaisas, Krishnamurthy Dvijotham, Sven Gowal, Pushmeet Kohli</h3>
<p>Does a Variational AutoEncoder (VAE) consistently encode typical samples
generated from its decoder? This paper shows that the perhaps surprising answer
to this question is `No'; a (nominally trained) VAE does not necessarily
amortize inference for typical samples that it is capable of generating. We
study the implications of this behaviour on the learned representations and
also the consequences of fixing it by introducing a notion of self consistency.
Our approach hinges on an alternative construction of the variational
approximation distribution to the true posterior of an extended VAE model with
a Markov chain alternating between the encoder and the decoder. The method can
be used to train a VAE model from scratch or given an already trained VAE, it
can be run as a post processing step in an entirely self supervised way without
access to the original training data. Our experimental analysis reveals that
encoders trained with our self-consistency approach lead to representations
that are robust (insensitive) to perturbations in the input introduced by
adversarial attacks. We provide experimental results on the ColorMnist and
CelebA benchmark datasets that quantify the properties of the learned
representations and compare the approach with a baseline that is specifically
trained for the desired property.
</p>
<a href="http://arxiv.org/abs/2012.03715" target="_blank">arXiv:2012.03715</a> [<a href="http://arxiv.org/pdf/2012.03715" target="_blank">pdf</a>]

<h2>Joint Optimization of an Autoencoder for Clustering and Embedding. (arXiv:2012.03740v1 [stat.ML])</h2>
<h3>Ahc&#xe8;ne Boubekki, Michael Kampffmeyer, Ulf Brefeld, Robert Jenssen</h3>
<p>Incorporating k-means-like clustering techniques into (deep) autoencoders
constitutes an interesting idea as the clustering may exploit the learned
similarities in the embedding to compute a non-linear grouping of data at-hand.
Unfortunately, the resulting contributions are often limited by ad-hoc choices,
decoupled optimization problems and other issues. We present a
theoretically-driven deep clustering approach that does not suffer from these
limitations and allows for joint optimization of clustering and embedding. The
network in its simplest form is derived from a Gaussian mixture model and can
be incorporated seamlessly into deep autoencoders for state-of-the-art
performance.
</p>
<a href="http://arxiv.org/abs/2012.03740" target="_blank">arXiv:2012.03740</a> [<a href="http://arxiv.org/pdf/2012.03740" target="_blank">pdf</a>]

<h2>Continuum Limit of Lipschitz Learning on Graphs. (arXiv:2012.03772v1 [cs.LG])</h2>
<h3>Tim Roith, Leon Bungert</h3>
<p>Tackling semi-supervised learning problems with graph-based methods have
become a trend in recent years since graphs can represent all kinds of data and
provide a suitable framework for studying continuum limits, e.g., of
differential operators. A popular strategy here is $p$-Laplacian learning,
which poses a smoothness condition on the sought inference function on the set
of unlabeled data. For $p&lt;\infty$ continuum limits of this approach were
studied using tools from $\Gamma$-convergence. For the case $p=\infty$, which
is referred to as Lipschitz learning, continuum limits of the related
infinity-Laplacian equation were studied using the concept of viscosity
solutions.

In this work, we prove continuum limits of Lipschitz learning using
$\Gamma$-convergence. In particular, we define a sequence of functionals which
approximate the largest local Lipschitz constant of a graph function and prove
$\Gamma$-convergence in the $L^\infty$-topology to the supremum norm of the
gradient as the graph becomes denser. Furthermore, we show compactness of the
functionals which implies convergence of minimizers. In our analysis we allow a
varying set of labeled data which converges to a general closed set in the
Hausdorff distance. We apply our results to nonlinear ground states and, as a
by-product, prove convergence of graph distance functions to geodesic distance
functions.
</p>
<a href="http://arxiv.org/abs/2012.03772" target="_blank">arXiv:2012.03772</a> [<a href="http://arxiv.org/pdf/2012.03772" target="_blank">pdf</a>]

<h2>A PAC-Bayesian Perspective on Structured Prediction with Implicit Loss Embeddings. (arXiv:2012.03780v1 [cs.LG])</h2>
<h3>Th&#xe9;ophile Cantelobre, Benjamin Guedj, Mar&#xed;a P&#xe9;rez-Ortiz, John Shawe-Taylor</h3>
<p>Many practical machine learning tasks can be framed as Structured prediction
problems, where several output variables are predicted and considered
interdependent. Recent theoretical advances in structured prediction have
focused on obtaining fast rates convergence guarantees, especially in the
Implicit Loss Embedding (ILE) framework. PAC-Bayes has gained interest recently
for its capacity of producing tight risk bounds for predictor distributions.
This work proposes a novel PAC-Bayes perspective on the ILE Structured
prediction framework. We present two generalization bounds, on the risk and
excess risk, which yield insights into the behavior of ILE predictors. Two
learning algorithms are derived from these bounds. The algorithms are
implemented and their behavior analyzed, with source code available at
\url{https://github.com/theophilec/PAC-Bayes-ILE-Structured-Prediction}.
</p>
<a href="http://arxiv.org/abs/2012.03780" target="_blank">arXiv:2012.03780</a> [<a href="http://arxiv.org/pdf/2012.03780" target="_blank">pdf</a>]

<h2>A predictive model for kidney transplant graft survival using machine learning. (arXiv:2012.03787v1 [cs.LG])</h2>
<h3>Eric S. Pahl, W. Nick Street, Hans J. Johnson, Alan I. Reed</h3>
<p>Kidney transplantation is the best treatment for end-stage renal failure
patients. The predominant method used for kidney quality assessment is the Cox
regression-based, kidney donor risk index. A machine learning method may
provide improved prediction of transplant outcomes and help decision-making. A
popular tree-based machine learning method, random forest, was trained and
evaluated with the same data originally used to develop the risk index (70,242
observations from 1995-2005). The random forest successfully predicted an
additional 2,148 transplants than the risk index with equal type II error rates
of 10%. Predicted results were analyzed with follow-up survival outcomes up to
240 months after transplant using Kaplan-Meier analysis and confirmed that the
random forest performed significantly better than the risk index (p&lt;0.05). The
random forest predicted significantly more successful and longer-surviving
transplants than the risk index. Random forests and other machine learning
models may improve transplant decisions.
</p>
<a href="http://arxiv.org/abs/2012.03787" target="_blank">arXiv:2012.03787</a> [<a href="http://arxiv.org/pdf/2012.03787" target="_blank">pdf</a>]

<h2>Perfect density models cannot guarantee anomaly detection. (arXiv:2012.03808v1 [cs.LG])</h2>
<h3>Charline Le Lan, Laurent Dinh</h3>
<p>Thanks to the tractability of their likelihood, some deep generative models
show promise for seemingly straightforward but important applications like
anomaly detection, uncertainty estimation, and active learning. However, the
likelihood values empirically attributed to anomalies conflict with the
expectations these proposed applications suggest. In this paper, we take a
closer look at the behavior of distribution densities and show that these
quantities carry less meaningful information than previously thought, beyond
estimation issues or the curse of dimensionality. We conclude that the use of
these likelihoods for out-of-distribution detection relies on strong and
implicit hypotheses, and highlight the necessity of explicitly formulating
these assumptions for reliable anomaly detection.
</p>
<a href="http://arxiv.org/abs/2012.03808" target="_blank">arXiv:2012.03808</a> [<a href="http://arxiv.org/pdf/2012.03808" target="_blank">pdf</a>]

<h2>CycleQSM: Unsupervised QSM Deep Learning using Physics-Informed CycleGAN. (arXiv:2012.03842v1 [cs.CV])</h2>
<h3>Gyutaek Oh, Hyokyoung Bae, Hyun-Seo Ahn, Sung-Hong Park, Jong Chul Ye</h3>
<p>Quantitative susceptibility mapping (QSM) is a useful magnetic resonance
imaging (MRI) technique which provides spatial distribution of magnetic
susceptibility values of tissues. QSMs can be obtained by deconvolving the
dipole kernel from phase images, but the spectral nulls in the dipole kernel
make the inversion ill-posed. In recent times, deep learning approaches have
shown a comparable QSM reconstruction performance as the classic approaches,
despite the fast reconstruction time. Most of the existing deep learning
methods are, however, based on supervised learning, so matched pairs of input
phase images and the ground-truth maps are needed. Moreover, it was reported
that the supervised learning often leads to underestimated QSM values. To
address this, here we propose a novel unsupervised QSM deep learning method
using physics-informed cycleGAN, which is derived from optimal transport
perspective. In contrast to the conventional cycleGAN, our novel cycleGAN has
only one generator and one discriminator thanks to the known dipole kernel.
Experimental results confirm that the proposed method provides more accurate
QSM maps compared to the existing deep learning approaches, and provide
competitive performance to the best classical approaches despite the ultra-fast
reconstruction.
</p>
<a href="http://arxiv.org/abs/2012.03842" target="_blank">arXiv:2012.03842</a> [<a href="http://arxiv.org/pdf/2012.03842" target="_blank">pdf</a>]

<h2>Nonnegative Matrix Factorization with Zellner Penalty. (arXiv:2012.03889v1 [stat.ML])</h2>
<h3>Matthew Corsetti, Ernest Fokou&#xe9;</h3>
<p>Nonnegative matrix factorization (NMF) is a relatively new unsupervised
learning algorithm that decomposes a nonnegative data matrix into a
parts-based, lower dimensional, linear representation of the data. NMF has
applications in image processing, text mining, recommendation systems and a
variety of other fields. Since its inception, the NMF algorithm has been
modified and explored by numerous authors. One such modification involves the
addition of auxiliary constraints to the objective function of the
factorization. The purpose of these auxiliary constraints is to impose
task-specific penalties or restrictions on the objective function. Though many
auxiliary constraints have been studied, none have made use of data-dependent
penalties. In this paper, we propose Zellner nonnegative matrix factorization
(ZNMF), which uses data-dependent auxiliary constraints. We assess the facial
recognition performance of the ZNMF algorithm and several other well-known
constrained NMF algorithms using the Cambridge ORL database.
</p>
<a href="http://arxiv.org/abs/2012.03889" target="_blank">arXiv:2012.03889</a> [<a href="http://arxiv.org/pdf/2012.03889" target="_blank">pdf</a>]

